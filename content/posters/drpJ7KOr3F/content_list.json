[{"type": "text", "text": "LLMs Can Evolve Continually on Modality for X-Modal Reasoning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiazuo $\\mathbf{Y}\\mathbf{u}^{1}$ , Haomiao Xiong1, Lu Zhang1,\u2217, Haiwen Diao1, Yunzhi Zhuge1, Lanqing $\\mathbf{Hong^{2}}$ , Dong Wang1, Huchuan $\\mathbf{L}\\mathbf{u}^{1}$ , You $\\mathbf{He^{3}}$ , Long Chen4 ", "page_idx": 0}, {"type": "text", "text": "1Dalian University of Technology, 2Huawei Noah\u2019s Ark Lab 3Tsinghua University, 4The Hong Kong University of Science and Technology yujiazuo@mail.dlut.edu.cn, zhangluu@dlut.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multimodal Large Language Models (MLLMs) have gained significant attention due to their impressive capabilities in multimodal understanding. However, existing methods rely heavily on extensive modal-specific pretraining and joint-modal tuning, leading to significant computational burdens when expanding to new modalities. In this paper, we propose PathWeave, a flexible and scalable framework with modal-path switching and expansion abilities that enables MLLMs to continually evolve on modalities for $\\mathbb{X}_{}$ -modal reasoning. We leverage the concept of Continual Learning and develop an incremental training strategy atop pre-trained MLLMs, enabling their expansion to new modalities using uni-modal data, without executing joint-modal pretraining. In detail, a novel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and cross-modal adapters are seamlessly integrated to facilitate efficient modality alignment and collaboration. Additionally, an MoEbased gating module is applied between two types of adapters to further enhance the multimodal interaction. To investigate the proposed method, we establish a challenging benchmark called Continual Learning of Modality (MCL), which consists of high-quality QA data from five distinct modalities: image, video, audio, depth and point cloud. Extensive experiments demonstrate the effectiveness of the proposed AnA framework on learning plasticity and memory stability during continual learning. Furthermore, PathWeave performs comparably to state-of-the-art MLLMs while concurrently reducing parameter training burdens by $98.73\\%$ . Our code locates at https://github.com/JiazuoYu/PathWeave. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With recent advances in artificial intelligence, Large Language Models (LLMs) have demonstrated impressive capacities in language understanding and reasoning. The success of LLMs [1, 2, 3, 4] has spurred researchers to develop Multimodal LLMs (MLLMs) by integrating additional input for multimodal tasks, such as image-text understanding [5, 6, 7], audio recognition [8, 9] and 3D question answering [10, 11]. Aided by large-scale image-text paired data from the Internet [12, 13, 6, 14, 5], vision LLMs have become a thriving area in the research community. The typical framework comprises a visual encoder, a frozen or trainable LLM, and a projection module for vision-language alignment. Through stepwisely pretraining on large-scale image-text pairs and instruction tuning on specific datasets, vision LLMs exhibit promising generalization abilities on downstream applications such as detection [15], grounding [16, 17], and captioning [6, 14]. Subsequently, the LLM-based framework and training pipeline of vision LLMs serve as the basis and drive the extension to other modalities, including video [18, 19], audio [9, 8], and point cloud [11, 10]. However, these modalspecific LLMs that inject single-modal data into language models struggle to tackle the challenge of perceiving different modalities like us humans. ", "page_idx": 0}, {"type": "image", "img_path": "drpJ7KOr3F/tmp/8d5f35323ce6a1fd89e048474f875f8288fe85a26c6a1cb44068577fe3dda8cd.jpg", "img_caption": ["Figure 1: Comparisons of Different Multimodal LLMs: (a) The normal multimodal methods [21, 23, 22] require unified sampling across multi-modal. (b) Our proposed incremental MLLMs learns each modality sequentially without joint-modal datasets. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address this issue, recent approaches [20, 21, 22, 23] extend the architecture and training strategies of modal-specific MLLMs, and try to integrate multiple modalities into a unified system. Some early attempts [22, 23] utilize specific projection modules to align image, video, and audio encoders into a frozen LLM. However, a complex training process is usually required to enhance cross-modal alignment, involving separate pretraining on uni-modal data and joint fine-tuning on multimodal data. Subsequent attempts try to enhance the scalability of MLLMs by unifying the architecture and simplifying the training process. For instance, X-InstructBLIP [20] proposes a unified projection architecture for all modalities and constructs high-quality instruction tuning data to simplify modalspecific customization and pretraining. OneLLM [21] leverages a unified encoder and projection module and introduces an incremental pretraining strategy to achieve parameter unification for a wide range of modalities. While effective, most approaches still rely on joint-modal optimization that is high-resource demanding (see Figure 1 (a)). When expanded to new modalities, the models have to re-access all the historical data and repeat the complete training process, limiting the continual extension of MLLMs. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose PathWeave $\\nLeftrightarrow$ , a flexible and scalable framework with modal-path switching and expansion capabilities that enables MLLMs to continually evolve on modality for $\\mathbb{X}$ -modal reasoning. PathWeave leverages the concept of Continual Learning (CL) and forms an incremental training pipeline on uni-modal data, eliminating the necessity for joint-modal pretraining or finetuning. To this end, we employ a pre-trained vision LLM [20] as the interface and propose a novel Adapterin-Adapter (AnA) framework, allowing efficient extension and alignment for other modalities. We set two types of adapters in AnA, uni-modal and cross-modal, and seamlessly incorporate them to boost modality alignment and collaboration during incremental learning. Specifically, the uni-modal adapters are progressively added to the interface and optimized on the corresponding modality data, which will be frozen once trained. Meanwhile, we insert in-adapters into the previous unimodal adapters to form cross-modal adapters, allowing the effective integration between historical knowledge and ongoing modality. Additionally, an MoE-based gating module is implemented between uni-modal and cross-modal adapters to further enhance multimodal collaboration. As shown in Figure 1 (b), our PathWeave can be flexibly implemented on the pretrained MLLMs and efficiently expand to more modalities in an incremental manner. ", "page_idx": 1}, {"type": "text", "text": "To evaluate the proposed PathWeave, we establish a challenging benchmark, namely Continual Learning of multi-Modality (MCL). It consists of data from five distinct modalities: image, video, depth, audio, and point cloud. In our setting, the modalities data are incrementally fed to the MLLMs. Thus, we leverage the commonly-used metrics from [20, 21] to investigate the precision on newly learned modalities. Furthermore, we introduce a metric to measure the forgetting rate in MCL to demonstrate the effectiveness of the proposed AnA strategy on historical modality memorization. Finally, we conduct extensive experiments to compare with state-of-the-art continual learning approaches, demonstrating that PathWeave is effective at incorporating multimodal data in an incremental manner. Moreover, our method achieves comparable performance with state-of-the-art MLLMs without requiring joint-modal pretraining or fine-tuning. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are summarized as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We present an efficient and scalable framework, PathWeave, which enables MLLM to progressively expand on multiple modalities, without the need for joint-modal pretraining. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a novel adapter-in-adapter framework that seamlessly integrates uni-modal and cross-modal adapters to enhance modality alignment and interaction during incremental learning. \u2022 We establish a challenging MCL benchmark with well-defined evaluation metrics. Extensive results demonstrate the effectiveness of PathWeave on modality plasticity and memorization during continual learning. Furthermore, PathWeave performs on par with state-of-the-art MLLMs while reducing parameter training burdens by at least $98.73\\%$ . ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multimodal Large Language Models. In recent years, researchers have been exploring the potential of LLMs in multimodal perceptions, such as visual question answering [5, 14] and captioning [6, 24]. This leads to the rapid development of Multimodal LLMs [6, 5, 21, 22]. For example, LLaVA [5] utilizes a simple linear layer to project visual information into language space, enduing LLMs the ability to perceive natural scenes. Subsequently, several methods attempt to expand the supported modalities of LLMs by modifying architecture designs or training strategies. For instance, XLLM [22] and Chatbridge [23] use modal-specific modules to extract features for multiple modalities and exploit modal-specific projection layers for multimodal alignment on a frozen LLM. However, a complex training process is usually required to enhance cross-modal alignment, which involves separate pretraining on uni-modal data and joint instruction tuning on multimodal data. Later, XInstructBLIP [20] proposes a unified projection architecture (Q-former) for all modalities and collects large-scale, high-quality instruction tuning data to eliminate the need for uni-modal pretraining. OneLLM [21] explores parameter unification by introducing a unified encoder and projection module for a wide range of modalities. Although an incremental pretraining strategy is proposed to alleviate the high resource demand of cross-modal alignment, OneLLM still relies on cross-modal finetuning on large-scale instruction datasets. In contrast to these methods, we incorporate the continual learning concept into MLLMs and propose an incremental training strategy to allow MLLMs\u2019 modal expansion by finetuning on uni-modal data, without requiring joint-modal pretraining or finetuning. Among these approaches, X-InstructBLIP [20] is highly related to our method, as it separately tunes Q-former to align multimodal into a uniform system. However, our method designs an adapter-based expansible framework that significantly reduces the parameter training burdens by at least $98.73\\%$ . ", "page_idx": 2}, {"type": "text", "text": "Continual Learning in Foundational Models. Continual Learning (CL) has been applied to large foundational models [25, 26, 27, 28], allowing them to continually acquire new knowledge. To address the forgetting issue in CL, significant efforts [29] have been made, including data replay, regularization constraints, and dynamic frameworks. Data replay-based methods [30, 31, 32, 33, 34] retain the historical data in a memory bank and mix them with new data to execute the general training process. However, the redundant historical data would incur increasing resource demand during lifelong learning. Regularization-based methods add explicit regularization terms on weights [35, 36, 37] or data [38, 39, 40, 41] to achieve a balance between historical and new tasks, which are usually used as an auxiliary trick in data-replay or dynamic methods. In contrast, dynamic methods [28, 26, 42, 43, 44] exhibit impressive expansible abilities by incrementally adding new parameters into a shared interface. Recently, the dynamic frameworks have been combined with efficient tuning techniques to achieve efficient, cost-friendly continual learning on visual-textual domain [28, 27, 26]. This inspires us to eliminate joint-modal pertaining from MLLMs by developing an efficient, scalable framework where new modalities are incrementally involved by accessing unimodal data. To this end, we propose an adapter-in-adapter framework, which incorporates uni-modal and cross-modal adapters for efficient modality alignment and collaboration. ", "page_idx": 2}, {"type": "text", "text": "Transfer learning. In the realm of Natural Language Progressing (NLP), fine-tuning large-scale models (e.g., 175B GPT-3 [4]) imposes significant burdens in both parameter complexity and time consumption. As a result, transfer learning methods [45, 46, 47, 48] have gained significant attention to facilitate the efficient adaption of LLMs on downstream applications. The techniques usually activate a small set of parameters on the frozen models while achieving comparable performance with fully-finetuned approaches. Among these methods, LoRA [45] reduces the trainable parameters through low-rank matrix decomposition, leading to the generalization of the pre-trained model on diverse downstream tasks. The success of LoRA further promotes the development of parameterefficient transfer learning of MLLMs [5, 49, 50] and uni-modal continual learning approaches [27, 26, 51]. However, these methods cannot be directly applied to fix the proposed MCL task due to the significant variations in modality spaces. In this paper, we propose a modality continual learning ", "page_idx": 2}, {"type": "image", "img_path": "drpJ7KOr3F/tmp/2652d920ae91559acb88daaa076e13f3a77023e45eaa4ad44f4ff4c916478fbf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overall framework of PathWeave. We start from a pretrained vision LLM [20] and progressively expand new modalities on it without acquiring historical data. Given input samples from modality $\\pmb{m}$ , we first exploit a frozen encoder $\\left(E_{m}\\right)$ for feature extraction and leverage $\\mathrm{Q}-$ Former to achieve multimodal alignment with LLMs. Then, the Adapter-in-Adapter (AnA) module is implemented in Q-Former to achieve flexible modal-path switching and expansion. In detail, the uni-modal adapters $({\\mathcal{A}}^{m})$ are implemented in parallel to facilitate new modal plasticity, which will be frozen once trained. While the cross-modal adapters $(\\hat{\\mathcal{A}}^{m})$ are formed by inserting a set of in-adapters $(\\{\\mathcal{F}_{i}^{m}\\}_{i=1}^{m-1})$ into the learned uni-adapters to enhance the collaboration of historical knowledge. Additionally, an MoE-based gating module $(\\mathcal{G}^{m})$ is implemented among uni-adapters to adaptively multimodal integration in input space. ", "page_idx": 3}, {"type": "text", "text": "method that incorporates adapter-based dynamic architecture on a frozen LLM, allowing efficient adaption and flexible expansion of new modalities in an incremental manner. ", "page_idx": 3}, {"type": "text", "text": "3 PathWeave ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Continual learning can empower large-scale foundation modals to constantly acquire new knowledge without accessing the entire historical data. We introduce this concept into MLLMs to form an incremental training strategy on uni-modal data called Continual Learning on Modality (MCL), eliminating the necessity of modal-specific pertaining and joint-modal datasets. Given a set of $M$ modalities $\\{\\mathcal{M}^{m}\\}_{m=1}^{M}$ , we enforce LLMs to sequentially access and learn on each modality for question answering. Here, each modality ${\\mathcal{M}}^{m}$ contains $N^{m}$ datasets, which can be represented as $\\mathcal{M}^{m}\\,=\\,\\{\\mathcal{D}_{i}^{m}\\}_{i=1}^{N^{m}}$ . More specifically, $\\mathcal{D}_{i}^{m}\\,=\\,\\{i_{i}^{m},s_{i}^{m},\\pmb{\\sigma}_{i}^{m}\\}$ denotes the $i{-}t h$ data of the $m{-}t h$ modality ${\\mathcal{M}}^{m}$ , in which , and are text instruction, modality samples, and answering, respectively. ", "page_idx": 3}, {"type": "text", "text": "3.2 Framework Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This work presents PathWeave, an efficient and extensible framework that empowers MLLMs to constantly evolve on modalities, without requiring modal-specific pretraining. Considering the complicity of training MLLMs from scratch, we start from a pretrained vision LLM and align other modalities in an incremental manner. The overall framework of PathWeave is illustrated in Figure 2. Specifically, we build the PathWeave on $\\boldsymbol{\\mathrm{X}}$ -InstructBLIP [20], providing a unified Q-Former architecture for various modalities. Given the samples from $m{-}t h$ modality, a modal-specific encoder $E_{m}$ pretrained on the corresponding modality is first exploited for feature extraction. Then, the $\\mathrm{Q}-$ Former $Q$ takes the input of modality feature, learnable query $q_{m}$ , and instruction embedding $I_{m}$ for multimodal alignment on a frozen LLM. It is worth noting that the initial modality $\\mathcal{M}^{0}$ is predefined as images, as we leverage the pretrained X-InstructBLIP to facilitate the alignment of subsequent modalities. As a result, the entire parameter of the encoder, Q-Former, and LLM will be frozen during continual learning. To achieve continual learning on modalities, we propose Adapter-in-Adapter (AnA), a dynamically expansible framework atop MLLMs, enabling the efficient integration of new modalities by executing uni-modal instruction tuning. The AnA consists of uni-modal and crossmodal adapters to boost modality alignment and collaboration along the modality sequence. In detail, the uni-modal adapters $({\\mathcal{A}}^{m})$ are implemented in parallel in Q-Former to efficiently adapt to new modalities, which will be frozen once trained to \u201cmemorize\u201d the historical modalities. Meanwhile, the cross-modal adapters $(\\hat{\\mathcal{A}}^{m})$ are constructed by inserting a set of in-adapters $(\\{\\mathcal{F}_{i}^{m}\\}_{i=1}^{m-1})$ into previously learned uni-adapters to acquire their knowledge for ongoing modality, which will be removed accordingly when testing former modalities. Furthermore, an MoE-based gating module is implemented between uni-adapter and cross-adapted for further multimodal integration. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.3 Adapter-in-Adapter ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "X-InstructBLIP [20] utilizes Q-Former as a unified framework to extend MLLMs\u2019 capabilities on more diverse modality reasoning, eliminating the need for modal-specific pretraining. However, instruction tuning on uni-modal data is implemented on separated Q-Formers, which leads to significant computational costs and parameter burdens when integrating more modalities. Recently, some attempts [26, 27] have demonstrated that adapters with few parameters can enhance the adaption of foundation modal on downstream tasks. Inspired by this, we leverage an effective transfer learning technique, LoRA [45], to serve as the basic unit of our AnA framework, enabling the efficient adaption of subsequent modalities during incremental learning. ", "page_idx": 4}, {"type": "text", "text": "Uni-modal Adapters. Given the current modality ${\\mathcal{M}}^{m}$ , we implement uni-modal adapters ${\\mathcal{A}}^{m}$ in the pretrained Q-Former for new modal alignment. The adapters ${\\mathcal{A}}^{m}$ are inserted into different linear layers $l$ of pretrained model in parallel. The output of layer $l$ with adapters ${\\mathcal{A}}^{m}$ can be expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{y}_{l}^{m}=Q_{l}(\\mathbf{x}_{l}^{m})+\\mathcal{A}_{l}^{m}(\\mathbf{x}_{l}^{m}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{x}_{l}^{m}$ and $\\mathbf{}{\\boldsymbol{\\mathbf{y}}}_{l}^{m}$ are the input and output embedding of $l{-}t h$ layer when aligning $m{-}t h$ modality. $\\mathcal{A}_{l}^{m}$ is the adapter of $m{-}t h$ modality in $l$ layer, and $\\bar{\\mathcal{A}^{m}(\\pmb{x})}=\\mathcal{F}_{u}^{m}\\bar{(\\mathcal{F}_{d}^{m}(\\pmb{x}))}$ , where ${\\mathcal{F}}_{u}$ and $\\mathcal{F}_{d}$ are the up and down projection of adapter. The uni-modal adapters are effective at acquiring modal-specific knowledge. Besides, the parallel architecture of adapters endows our system with the capabilities to flexibly switch and expand to diverse modalities. ", "page_idx": 4}, {"type": "text", "text": "Cross-modal Adapters. The uni-modal adapters are effective at preserving the uni-modal knowledge and alleviating the forgetting issue in long-term learning. Based on it, we introduce a modal-special in-adapter module $(\\mathcal{F}_{i}^{m})$ to form a cross-modal adapter $\\grave{(A^{m})}$ , which can help the ongoing modality learn previous knowledge and encourage inter-modality collaboration. Specifically, the in-adapters are inserted into the previously learned uni-modal adapters to effectively acquire the learned knowledge without reactivating their parameters. Then, the output of $l{-}t h$ layer ${\\bf y}_{l}^{m}$ after adding In-Adapter $\\mathcal{F}_{i}^{m}$ can be reformulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{y}_{l}^{m}=Q_{l}(\\pmb{x}_{l}^{m})+\\sum_{i=1}^{m-1}\\hat{\\mathcal{A}}_{l}^{i}(\\pmb{x}_{l}^{m})+\\mathcal{A}_{l}^{m}(\\pmb{x}_{l}^{m}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\mathcal{A}}^{i}(\\pmb{x})\\,=\\,\\mathcal{F}_{u}^{i}(\\mathcal{F}_{i}^{m}(\\mathcal{F}_{d}^{i}(\\pmb{x}))),i\\,\\in\\,[1,m-1]$ represents the cross-modal adapters for current modality ${\\mathcal{M}}^{m}$ . $\\mathcal{F}_{i}^{m}$ is the in-adapter that is inserted into $i{-}t h$ frozen uni-adapters $A^{i}$ , which is a single linear layer with the dimension of adapters\u2019 low rank. The uni-modal and cross-modal adapters collaborate to facilitate the new modality alignment and cross-modal integration during incremental learning. Furthermore, the proposed in-adapter serves as a plug-and-play module that will not affect the performance of previously learned adapters, thereby effectively alleviating the modality forgetting. ", "page_idx": 4}, {"type": "text", "text": "MoE-based Gating. Cross-modal adapters rely on in-adapters to effectively leverage historical knowledge to boost the alignment of ongoing modality. However, the output of cross-modal and uni-modal adapters are treated equally in the original Q-Former. Considering the significant gap between distinct modalities, this simple integration strategy might pose performance degradation affected by the interfering information from other modalities. To address this issue, we propose an MoE-based gating module between cross-modal and uni-modal adapters for adaptive multimodal integration. Our MoE-based gating ${\\mathcal{G}}^{m}$ automatically assigns weights of paths ${\\mathcal P}^{m}$ of different cross-modal adapters and uni-modal adapter to produce outcomes tailored to each modality ${\\mathcal{M}}^{m}$ . The paths $\\{\\mathcal{P}^{m}\\}_{m=1}^{M}$ include the previous cross-modal adapters with the current in-adapter and current uni-modal adapter. Therefore, each linear\u2019s output ${\\mathfrak{y}}^{m}$ after adding MoE-based gating ${\\mathcal{G}}^{m}$ in AnA module can be computed as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\pmb y}_{l}^{m}=Q_{l}({\\pmb x}_{l}^{m})+\\sum_{i=1}^{m}W_{i}^{m}\\mathcal{P}_{i}({\\pmb x}_{l}^{m}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $W^{m}=\\{W_{i}^{m}\\}_{i=1}^{N_{E}}$ represents the gating weights assigned by ${\\mathcal{G}}^{m}$ , dictating the contribution of each adapter\u2019s path ${\\mathcal P}^{m}$ . The gating weights are then computed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nW^{m}=S o f t m a x({\\mathcal G}^{m}({\\pmb x}^{m})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\mathcal{G}}^{m}$ projects each token of embeddings $\\boldsymbol{x}$ to a 1-D vector indicating each modality\u2019s likelihood of functioning. It is worth noting that we do not set the Topk hyper-parameter here. By default, the knowledge of each modality will provide a reference for the current modality. The $S o\\dot{f}t m a x(\\cdot)$ function normalizes these weights to emphasize the modality-branch contribution. Finally, the output $\\mathbf{}{\\boldsymbol{\\mathbf{y}}}_{l}^{m}$ of AnA with MoE-based gating can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{y}_{l}^{m}=Q_{l}^{m}(\\pmb{x}_{l}^{m})+\\sum_{i=1}^{m-1}W^{i}\\hat{\\mathcal{A}}^{i}(\\pmb{x}_{l}^{m})+W^{m}\\mathcal{A}^{m}(\\pmb{x}_{l}^{m}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Continual Learning on Modality ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "MCL Benchmark. We establish a challenging benchmark, Continual Learning on Modality (MCL), which consists of multimodal high-quality QA data to evaluate the effectiveness of our method on continual uni-modal finetuning. These datasets are collected from five distinct modalities: image, video, depth, audio and point cloud. Based on this benchmark, our PathWeave is trained and tested along the multimodal sequence without requiring modal-specific pretraining or joint-modal finetuning. More details of the dataset list and size for each modality are illustrated in Table A6 of the Appendix. ", "page_idx": 5}, {"type": "text", "text": "MCL Metrics. We formulate the metrics from two aspects to evaluate the proposed MCL strategy on multimodal reasoning. On the one hand, we use the general metrics from MLLMs [20, 21] to investigate the model\u2019s overall performance on learned new modalities. On the other hand, we modify the conventional metrics of continual learning to verify the performance of our method on \u201ccatastrophic forgetting\u201d. Specifically, for each modality and dataset, suppose $S_{m,i}^{n}$ represents the evaluation score on $n$ -th datasets of modality $\\mathcal{M}^{i}$ after training on modality ${\\mathcal{M}}^{m}$ . We redefine the forgetting rate [28] to measure the degree of forgetting $F_{m}$ on all old modalities after each modality stage $m$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{m}=\\frac{1}{m}\\sum_{i=0}^{m-1}F_{m,i}^{N_{i}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $F_{m,i}^{N_{i}}$ is the average forgetting across $N_{i}$ datasets of modality $i$ after modality $m$ training, and $N_{i}$ is the number of datasets in modality $i$ . And the $F_{m,i}^{N_{i}}$ are defined: ", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{m,i}^{N_{i}}=\\frac{1}{N^{i}}\\sum_{n=1}^{N_{i}}\\underset{0\\leq j<m}{\\operatorname*{max}}\\big(S_{j,i}^{n}\\big)-S_{m,i}^{n}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In addition, we define the forgetting $\\hat{F}_{i}^{n}$ for the $n$ -th dataset in modality $i$ during the training of all modalities: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{F}_{i}^{n}=\\frac{1}{M}\\sum_{m=1}^{M}\\operatorname*{max}_{0\\le j<m}(S_{j,i}^{n})-S_{m,i}^{n}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To measure the overall performance on learned modalities, we further report the average scores of across $N_{m}$ datasets of modality $m$ after training on $m$ modality, it can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nT_{m}=\\frac{1}{N_{m}}\\sum_{n=1}^{N_{m}}{S_{m,m}^{n}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "And the performance on learned modalities $\\hat{T}_{i}^{n}$ for the $n$ -th dataset in modality $i$ can be expressed as $\\hat{T}_{i}^{n}=S_{i,i}^{n}$ . ", "page_idx": 5}, {"type": "table", "img_path": "drpJ7KOr3F/tmp/62355e48b7ed3350560a6108878e822f50c56d83f747f68551ad2919f8f4f930.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "drpJ7KOr3F/tmp/ac40424bbad418fcba365df28f51685926c1fc4d7156f54047f4677cc263560a.jpg", "table_caption": ["Table 1: Comparison with other CL methods on each modalities of in-domain datasets. We label the best and second methods with bold and underline styles. The top gray block indicates the upper-bound scores $T_{m}$ of transfer learning capability to adapt the new modality. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 2: Comparison with other CL methods on the performance of each in-domain datasets. We label the best and second methods with bold and underline styles. The top gray block indicates the upper-bound scores $\\hat{T}_{i}^{n}$ of transfer learning capability to adapt the new modality. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our method is built on the LAVIS library\u2019s framework [52] atop the Vicuna v1.1 7b [3]. The input preprocessing method remains consistent with X-InstructBLIP [20]. We optimize our model on $4\\!\\times\\!\\mathrm{A}800$ GPUs (80GB) using AdamW [53] with $\\beta_{1}=0.9$ , $\\beta_{2}=0.999$ , and a weight decay of 0.05. Our initial pre-trained model is the image modality model of X-InstructBLIP [20]. During training, the unified incremental module, consisting of Q-former and LLM projection, is continuously trained in the order of image, video, audio, depth, and 3D modalities. During testing, the learnable query and modality encoder are kept modality-specific. The CL methods compared below maintain consistent settings with our method. More details are provided in the Appendix A.2. ", "page_idx": 6}, {"type": "text", "text": "5.2 Comparison with State-of-the-art Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Transfer Learning on New Modality. As shown in Table 1 and 2, we conduct experiments on existing traditional CL methods under our proposed MCL setting. We report the average expansion capability for each modality, which is represented as $T_{m}$ and indicates the scalability in the new modality. The inference datasets are in-domain, which is involved in model training, and additional results of out-of-domain are provided in the supplementary material. Continual-FT, which refers to continuous learning of each modality without incorporating anti-forgetting strategies, exhibits the best expansion ability due to fine-tuning all parameters but inevitably leads to catastrophic forgetting. In contrast, the methods of L2 Reg&WE [25], WISE-FT [54] and EProj [28] effectively alleviate forgetting by parameter regularization and ensemble, but it is difficult for them to transfer new modality. As shown in Table 1, when performing transfer learning on new modalities with significant data distribution gaps from the images, these methods under-perform ours by at least 38 points on the Audio modality and 66 points on the 3D modality. Furthermore, as shown in Table 2, our method surpasses the current best methods by over 29 points in the average transfer learning metrics across in-domain datasets. This demonstrates that our approach can effectively prevent forgetting while flexibly extending to new modalities with substantial data distribution differences. ", "page_idx": 6}, {"type": "text", "text": "Alleviate Forgetting of Previous Knowledge. We also present the average forgetting rate $F_{m}$ of historical modality knowledge after training each modality $m$ , as shown in the $F_{m}$ columns of Table 1 and 2. The results show that continually full finetuning pre-trained modal suffers from catastrophic forgetting. WiSE-FT [54] and L2 Reg&WE [25] achieve some effectiveness in combating forgetting via parameter regularization and ensemble. However, the constraint of parameters limits their transfer learning on new modalities. In contrast, the EProj [28] and our method achieve anti-forgetting by freezing model parameters. However, the scalability of the EProj [28] is significantly lower than our method, especially in the audio and 3D modes. It indicates that our method achieves an optimal balance between anti-forgetting and effective expansion compared to other methods. ", "page_idx": 6}, {"type": "table", "img_path": "drpJ7KOr3F/tmp/3c8187a8ce94268a5fcbf72c297b48f06de31d1ce7a73fe4ad6334152801143c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "drpJ7KOr3F/tmp/4b4e3474fe11b43c231b2b669dc49ad9e8dcb8a7c79c861662e3e4f19c90e30f.jpg", "table_caption": ["Table 3: Comparison with state-of-the-art methods on training parameters, data requirements and some performance. \u201cAll Modal\u201d indicates whether fine-tuning on all modality datasets is included. \u201c\u2020\u201d represents the same hyperparameters and training settings of different methods for fair comparison. "], "table_footnote": ["Table 4: Ablation study of different parts for the influence of the each modalities\u2019 performance. We label the best and second methods with bold and underline styles. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Comparison with Existing MLLMs. Table 3 shows the comparison between our approach and state-of-the-art multimodal QA methods in terms of training parameters, required data, training times, GPU usage, and relevant multimodal QA metrics. Among these methods, we unify the settings to ensure fairness in the Times and GPU metrics by only training on the instruction tuning stage, setting all batchsize to 4, and keeping the LLMs of BLIP-based X-LLM and ChatBridge frozen. It can be seen that our method demonstrates a significant advantage in parameter efficiency compared to X-InsructBLIP [20] and OneLLM [21], reducing parameter training burdens by at least $98.73\\%$ . Moreover, compared with OneLLM [21], X-LLM [22], and ChatBridge [23], our approach does not necessitate pre-training and instruction tuning with all joint-modal datasets to adapt to multimodal language reasoning tasks. Our method offers flexible scalability and requires considerably less training data than other methods. The results of the three QA tasks involving video, audio, and 3D, as shown in Table 3, indicate that our approach maintains flexibility without significantly compromising model performance. More experiments are provided in the Table A11 of Appendix. ", "page_idx": 7}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Ablation Study of the In-Adapter and MoE-based gating. We conduct detailed ablation studies on different parts of the proposed method, as shown in Table 4 and 5. Table 4 shows the average performance $T_{m}$ of transfer learning in each modality. It can be seen that our final method demonstrates increasingly significant performance improvements compared to others when faced with continual modality changes. For instance, as we further extend to depth and 3D modalities, the collaborative synergy between MoE-based gating and In-Adapter becomes increasingly apparent. In addition, Table 5 demonstrates that compared to directly using the incremental adapter method, our approach improves the average performance of transfer learning across all datasets by 4.3 points. When removing the In-Adapter or MoE-based gating, the model\u2019s transfer learning performance of transfer learning across all datasets decreases by at least 1.1 points and 4.0 points. It indicates the effectiveness of our proposed In-Adapter and MoE-based gating, which enhance inter-modal interactions and modulate cross-modal knowledge. ", "page_idx": 7}, {"type": "text", "text": "Analysis of the Benefit from Previous Modalities. Figure 3 presents the ablation study on the ability to transfer learning based on different knowledge of modalities. As shown in Figure 3 (a), our method enhances the scalability of audio modality after incorporating additional video modality training. It indicates that our designed method can extract knowledge from the other adapter to enhance the learning of the current modality. In addition, when more than one modality is additionally introduced, our method can still enhance new generalization by modulating inter-modal knowledge and finetuning frozen knowledge with In-Adapter, as shown in Figure 3 (b) and (c). It demonstrates that our method can enhance the adapting to new modalities by knowledge learned from other modalities. ", "page_idx": 7}, {"type": "table", "img_path": "drpJ7KOr3F/tmp/aa85d1f984745994e01a6e448d268701d4493428d42ab9938b03844737457c09.jpg", "table_caption": ["Table 5: Ablation study of different parts for the influence of the each dataset\u2019s performance. We label the best and second methods with bold and underline styles. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "drpJ7KOr3F/tmp/e91496aaeefe16582afbcec4beaee979d3387a9321a7bb7677479ed094f0aba7.jpg", "img_caption": ["Figure 3: Ablation study of the $\\hat{T}_{i}^{n}$ performance for the $n$ -th dataset in modality $i$ , which benefits from knowledge of different modalities. \u201cBased on I-V-A-D\u201d represents training point modality based on our pre-trained PathWeave that is trained in the sequence of image, video, audio, and depth. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.4 Qualitative Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Figure 4 shows the qualitative results of our method for inference on each modality after continual training is completed. We show our final model can (a) understand visual content in images, (b) leverage temporal information in videos, (c) scene understanding using depth maps, (d) do creative writing based on audio content, and (e) understand the details of 3D shapes. More qualitative results are provided in Figure A5 of the Appendix. ", "page_idx": 8}, {"type": "image", "img_path": "drpJ7KOr3F/tmp/0625fdf8ac1364f797a8aefed7030b5e7ed757bb348fce97213c800f1295a419.jpg", "img_caption": ["(a) Image modality (b) Video modality (c) Audio modality (d) Depth modality (e) 3D modality Figure 4: Qualitative results of our method on each modality after continuous training. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Conclusion and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We propose a flexible and scalable framework for multi-modal language reasoning that enables MLLMs to continually expand on multiple modalities without joint-modal datasets. We introduce an incremental Adapter-in-Adapter (AnA) strategy, incorporating two types of adapters to enhance modality plasticity and collaboration during expanding on other modalities. Moreover, we design an MoE-based gating module to further enhance multi-modal integration by modulating the output space of different modalities. Extensive experimental results in our proposed benchmark demonstrate the superiority of our method over previous arts in terms of modality alignment and memorization. ", "page_idx": 8}, {"type": "text", "text": "A limitation of this paper is that we only explored the extension of five modalities and do not cover all modal information in real-world scenarios. Furthermore, the implicit interaction between the modalities in our method cannot accomplish cross-modal joint language reasoning tasks in an incremental manner. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by National Natural Science Foundation of China under Grant 62206039, 62293544, and the Fundamental Research Funds for the Central Universities (DUT24RC(3)025). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[3] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%^{*}$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023.   \n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[5] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \n[6] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[7] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024.   \n[8] Xingjian Du, Zhesong Yu, Jiaju Lin, Bilei Zhu, and Qiuqiang Kong. Joint music and language attention models for zero-shot music tagging. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1126\u20131130. IEEE, 2024.   \n[9] Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang. Pengi: An audio language model for audio tasks. Advances in Neural Information Processing Systems, 36:18090\u201318108, 2023.   \n[10] Dingning Liu, Xiaoshui Huang, Yuenan Hou, Zhihui Wang, Zhenfei Yin, Yongshun Gong, Peng Gao, and Wanli Ouyang. Uni3d-llm: Unifying point cloud perception, generation and editing with large language models. arXiv preprint arXiv:2402.03327, 2024.   \n[11] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:20482\u201320494, 2023.   \n[12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.   \n[13] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. Advances in Neural Information Processing Systems, 36, 2024.   \n[14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024.   \n[15] Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, and Xuansong Xie. Multi-modal instruction tuned llms with fine-grained visual perception. arXiv preprint arXiv:2403.02969, 2024.   \n[16] Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David F Fouhey, and Joyce Chai. Llm-grounder: Open-vocabulary 3d visual grounding with large language model as an agent. arXiv preprint arXiv:2309.12311, 2023.   \n[17] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2998\u20133009, 2023.   \n[18] Jiawen Zhu, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Bin Luo, Huchuan Lu, Yifeng Geng, and Xuansong Xie. Tracking with human-intent reasoning. arXiv preprint arXiv:2312.17448, 2023.   \n[19] Hui Yang, Lekha Chaisorn, Yunlong Zhao, Shi-Yong Neo, and Tat-Seng Chua. Videoqa: question answering on news video. In Proceedings of the eleventh ACM international conference on Multimedia, pages 632\u2013641, 2003.   \n[20] Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong, and Juan Carlos Niebles. X-instructblip: A framework for aligning x-modal instructionaware representations to llms and emergent cross-modal reasoning. arXiv preprint arXiv:2311.18799, 2023.   \n[21] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language. arXiv preprint arXiv:2312.03700, 2023.   \n[22] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. arXiv preprint arXiv:2305.04160, 2023.   \n[23] Zijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen, Shuai Shao, Xinxin Zhu, Zehuan Yuan, and Jing Liu. Chatbridge: Bridging modalities with large language model as a language catalyst. arXiv preprint arXiv:2305.16103, 2023.   \n[24] Peipei Song, Dan Guo, Xun Yang, Shengeng Tang, and Meng Wang. Emotional video captioning with vision-based emotion interpretation network. IEEE Transactions on Image Processing, 2024.   \n[25] Zangwei Zheng, Mingyuan Ma, Kai Wang, Ziheng Qin, Xiangyu Yue, and Yang You. Preventing zero-shot transfer degradation in continual learning of vision-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19125\u201319136, 2023.   \n[26] Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang, Huchuan Lu, and You He. Boosting continual learning of vision-language models via mixture-of-experts adapters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23219\u201323230, 2024.   \n[27] Ying Shen, Zhiyang Xu, Qifan Wang, Yu Cheng, Wenpeng Yin, and Lifu Huang. Multimodal instruction tuning with conditional mixture of lora. arXiv preprint arXiv:2402.15896, 2024.   \n[28] Jinghan He, Haiyun Guo, Ming Tang, and Jinqiao Wang. Continual instruction tuning for large multimodal models. arXiv preprint arXiv:2311.16206, 2023.   \n[29] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366\u20133385, 2021.   \n[30] David Isele and Akansel Cosgun. Selective experience replay for lifelong learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.   \n[31] Frantzeska Lavda, Jason Ramapuram, Magda Gregorova, and Alexandros Kalousis. Continual classification learning using generative models. arXiv preprint arXiv:1810.10612, 2018.   \n[32] David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017.   \n[33] Oleksiy Ostapenko, Timothee Lesort, Pau Rodr\u00edguez, Md Rifat Arefin, Arthur Douillard, Irina Rish, and Laurent Charlin. Continual learning with foundation models: An empirical study of latent replay. In Conference on lifelong learning agents, pages 60\u201391. PMLR, 2022.   \n[34] Sylvestre-Alvise Rebuff,i Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001\u20132010, 2017.   \n[35] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. Advances in neural information processing systems, 30, 2017.   \n[36] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International conference on machine learning, pages 3987\u20133995. PMLR, 2017.   \n[37] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.   \n[38] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. Learning without memorizing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5138\u20135146, 2019.   \n[39] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In Computer vision\u2013ECCV 2020: 16th European conference, Glasgow, UK, August 23\u201328, 2020, proceedings, part XX 16, pages 86\u2013102. Springer, 2020.   \n[40] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 831\u2013839, 2019.   \n[41] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935\u20132947, 2017.   \n[42] Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3366\u20133375, 2017.   \n[43] Arthur Douillard, Alexandre Ram\u00e9, Guillaume Couairon, and Matthieu Cord. Dytox: Transformers for continual learning with dynamic token expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9285\u20139295, 2022.   \n[44] Zhiyuan Hu, Yunsheng Li, Jiancheng Lyu, Dashan Gao, and Nuno Vasconcelos. Dense network expansion for class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11858\u201311867, 2023.   \n[45] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[46] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. Advances in Neural Information Processing Systems, 35:12991\u201313005, 2022.   \n[47] Haiwen Diao, Bo Wan, Ying Zhang, Xu Jia, Huchuan Lu, and Long Chen. Unipt: Universal parallel tuning for transfer learning with efficient parameter and memory. arXiv preprint arXiv:2308.14316, 2023.   \n[48] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709\u2013727. Springer, 2022.   \n[49] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023.   \n[50] Jinjin Xu, Liwu Xu, Yuzhe Yang, Xiang Li, Yanchun Xie, Yi-Jie Huang, and Yaqian Li. u-llava: Unifying multi-modal tasks via large language model. arXiv preprint arXiv:2311.05348, 2023.   \n[51] Yukun Li, Guansong Pang, Wei Suo, Chenchen Jing, Yuling Xi, Lingqiao Liu, Hao Chen, Guoqiang Liang, and Peng Wang. Coleclip: Open-domain continual learning via joint task prompt and vocabulary learning. arXiv preprint arXiv:2403.10245, 2024.   \n[52] Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven CH Hoi. Lavis: A library for language-vision intelligence. arXiv preprint arXiv:2209.09019, 2022.   \n[53] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[54] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust finetuning of zero-shot models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7959\u20137971, 2022.   \n[55] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual $12\\mathrm{m}$ : Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3558\u20133568, 2021.   \n[56] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.   \n[57] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 119\u2013132, 2019.   \n[58] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.   \n[59] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. Advances in Neural Information Processing Systems, 36, 2024.   \n[60] Karol J Piczak. Esc: Dataset for environmental sound classification. In Proceedings of the 23rd ACM international conference on Multimedia, pages 1015\u20131018, 2015.   \n[61] Samuel Lipping, Parthasaarathy Sudarsanam, Konstantinos Drossos, and Tuomas Virtanen. Clotho-aqa: A crowdsourced dataset for audio question answering. In 2022 30th European Signal Processing Conference (EUSIPCO), pages 1140\u20131144. IEEE, 2022.   \n[62] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 736\u2013740. IEEE, 2020.   \n[63] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Computer Vision\u2013ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V 12, pages 746\u2013760. Springer, 2012.   \n[64] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 567\u2013576, 2015.   \n[65] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1912\u20131920, 2015.   \n[66] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10786\u201310796, 2021.   \n[67] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[68] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011.   \n[69] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32\u201373, 2017.   \n[70] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. Aokvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146\u2013162. Springer, 2022.   \n[71] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195\u20133204, 2019.   \n[72] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947\u2013952. IEEE, 2019.   \n[73] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.   \n[74] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645\u20131653, 2017.   \n[75] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley, Yuexian Zou, and Wenwu Wang. Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research. arXiv preprint arXiv:2303.17395, 2023.   \n[76] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738, 2021.   \n[77] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19358\u201319369, 2023.   \n[78] Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, and Furu Wei. Beats: Audio pre-training with acoustic tokenizers. arXiv preprint arXiv:2212.09058, 2022.   \n[79] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709, 2019.   \n[80] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 190\u2013200, 2011. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Dataset Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We summarize the multimodal-text dataset in Table A6 for modality continue learning. For depth-text pairs, we adopt the DPT model pre-trained on ominidata [66] to generate depth maps. The source dataset is a subset of CC3M [58], around 0.5M image-text pairs and 50K image-text pairs random sampled from LLaVA-150K [5]. ", "page_idx": 14}, {"type": "text", "text": "LLaVA data includes multiple rounds of dialogue. To align with our training process, we randomly select one round as a training sample. This selection method also applies when creating the validation set, where these samples remain fixed and do not change during testing. ", "page_idx": 14}, {"type": "table", "img_path": "drpJ7KOr3F/tmp/15da89e2bebd4d3ccc4e131230c357fe240768783a98120e4759c0117d3fe759.jpg", "table_caption": [], "table_footnote": ["Table A6: Datasets for continually uni-modal finetuning. Our datasets are extensions of XInstructBLIP [20], in contrast, we additionally included depth data and removed inaccessible video data WebVid2M [76]. \\* represent data we generated ourselves. "], "page_idx": 14}, {"type": "text", "text": "A.2 Training & Evaluation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table A7 records the detailed hyper-parameters we used during the training and testing process. It is worth noting that the training of our method on each modal data is continuous. The encoders for image, video, and depth are set to EVA-CLIP-ViT-G/14 [77]. The audio and 3D encoders are $\\mathrm{BEATs}_{\\mathrm{iter3+}}$ [78] and ULIP-2, respectively. ", "page_idx": 14}, {"type": "text", "text": "When using WiSE-FT [54] and L2 Reg&WE [25] methods for training, in order to be as consistent as possible with the original approach, we update the weights of the Q-Former and LLM projection layer in each inner epoch (for WiSE-FT, we set update coefficient $\\alpha$ as 0.8). For example, when we train on Audio modal data, the total training iteration is set to 65000, and 5000 iterations per inner epoch, then the number of weight updates is 13 times in the current situation. ", "page_idx": 14}, {"type": "text", "text": "During modality backward testing for methods in Table 1, we keep the encoder and Q-Former queries consistent with the test modality. We utilize the same instruct prompts as X-InstructBLIP [20] during training and testing. ", "page_idx": 14}, {"type": "table", "img_path": "drpJ7KOr3F/tmp/f1ce36b89cfe5ab918f842dfde199ac40d96c8e5ce1df70e9aa82dba0754778e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table A7: Hyper-parameters for modality continue learning. We keep all the learning rate decrease from 1e-5 and cosine annealing strategy with 0.5 decay weight. The warm-up phase starts from 1e-8 and lasts for 1000 iterations for all modality training. ", "page_idx": 14}, {"type": "text", "text": "A.3 Complete Raw Data ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table A8 records all the original data of the methods compared in Table 1. We highlight the transfer learning performance in new modality of each method with green color. ", "page_idx": 14}, {"type": "table", "img_path": "drpJ7KOr3F/tmp/b267a622769d65f63626794cf248a270f73a1dec6c146af340a713f74b143bc2.jpg", "table_caption": [], "table_footnote": ["Video modalityAudio modality Table A8: Raw data records of all compared CL methods in all modalities. "], "page_idx": 15}, {"type": "text", "text": "A.4 Additional Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As shown in Table A9 and Table A10, we conduct experiments to analyze the performance on outof-domain data in addition to the in-domain experiments. Our method shows robust generalization while maintaining anti-forgetting performance on out-of-domain data. Specifically, compared with the full-finetune method, our average accuracy only decrease 0.33 points, while achieving 31.34 points anti-forgetting capability. At the same time, with the same powerful anti-forgetting ability as EProj [28], the generalization of our method between different modalities improves 18.95 points. ", "page_idx": 16}, {"type": "table", "img_path": "drpJ7KOr3F/tmp/53b626a18aa3ed5cfe17720d1bb16f1c5e3cbcb6e3cb55e2707634039aef390d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table A9: Comparison with other CL methods on each modality of out-of-domain datasets. We label the best and second methods with bold and underline styles. The top block indicates the upper-bound scores $T_{m}$ of transfer learning capability to adapt the new modality. ", "page_idx": 16}, {"type": "table", "img_path": "drpJ7KOr3F/tmp/c1b4e1c753de15f4f1fc70361004cb94bf27908c6110827227c94e940f0b160e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table A10: Comparison with other CL methods on the performance of each out-of-domain dataset. We label the best and second methods with bold and underline styles. The top block indicates the upper-bound scores $\\hat{T}_{i}^{n}$ of transfer learning capability to adapt the new modality. ", "page_idx": 16}, {"type": "text", "text": "We quantitatively compare the results of our method and other multi-modal large language models that support multiple modalities in Table A11. Compared with other MLLMs, we achieve a better trade-off between model performance and the number of supported modalities with fewer learnable parameters and less training data. ", "page_idx": 16}, {"type": "table", "img_path": "drpJ7KOr3F/tmp/dc9f1c0104d10b473315bca0207056b0d620da564623ec0b8e17db52664c9ad3.jpg", "table_caption": ["Table A11: Comparison with state-of-the-art methods on metrics of different datasets. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "In addition, we provide more qualitative results on each modality in Figure A5. ", "page_idx": 16}, {"type": "text", "text": "A.5 More training details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "All modalities are trained by an Autoregressive CE loss. The detailed hyperparameter settings for each modality are shown in Table A12 of the attached PDF. We will provide further details and descriptions of the loss and hyperparameters in the paper to ensure better clarity and flow. ", "page_idx": 16}, {"type": "image", "img_path": "drpJ7KOr3F/tmp/fd2c2799eebf11949da2e856410ec597407bfb38b222b607e8e9c59358fb01cd.jpg", "img_caption": ["Figure A5: More qualitative results of our method on each modality after continuous training. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "drpJ7KOr3F/tmp/06b9d8efacec31dabdcf1418c4853eaa4e64d7e806588bdf7f5dbb230f3c0134.jpg", "table_caption": ["Table A12: More details of hyperparameters used on each of the datasets. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have claimed the contributions in both abstract and introduction. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We have discussed the limitations in Conclusion and Discussion ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have explained the methodology. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have provided the implementation details including training and testing. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We will release the code once acceptance. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provided the details accprdingly. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: We set the parameters following previous methods without strictly adjusting the parameters. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We claimed the resource demand. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We follow the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No social impacts exist in this work. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We used the public benchmarks as previous methods. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have credited the related benchmark. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: No new assets. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We used published benchmarks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our research didn\u2019t contain this issue. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]