[{"type": "text", "text": "User-item fairness tradeoffs in recommendations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sophie Greenwood Sudalakshmee Chiniah Computer Science Operations Research and Information Engineering Cornell Tech Cornell Tech ", "page_idx": 0}, {"type": "text", "text": "Nikhil Garg Operations Research and Information Engineering Cornell Tech ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the basic recommendation paradigm, the most (predicted) relevant item is recommended to each user. This may result in some items receiving lower exposure than they \u201cshould\u201d; to counter this, several algorithmic approaches have been developed to ensure item fairness. These approaches necessarily degrade recommendations for some users to improve outcomes for items, leading to user fairness concerns. In turn, a recent line of work has focused on developing algorithms for multi-sided fairness, to jointly optimize user fairness, item fairness, and overall recommendation quality. This induces the question: what is the tradeoff between these objectives, and what are the characteristics of (multi-objective) optimal solutions? Theoretically, we develop a model of recommendations with user and item fairness objectives and characterize the solutions of fairness-constrained optimization. We identify two phenomena: (a) when user preferences are diverse, there is \u201cfree\u201d item and user fairness; and (b) users whose preferences are misestimated can be especially disadvantaged by item fairness constraints. Empirically, we prototype a recommendation system for preprints on arXiv and implement our framework, measuring the phenomena in practice and showing how these phenomena inform the design of markets with recommendation systems-intermediated matching. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recommendation systems are employed throughout modern online platforms to suggest items (media, songs, books, products, or jobs) to users (viewers, listeners, readers, consumers, or job seekers). The platform learns user preferences and shows each user personalized recommendations. One recommendation paradigm is to simply show the user the items they most prefer. However, this approach may result in disparately poor outcomes for some items, which may not be most preferred by any user [42]. For example, in our empirical application in prototyping a recommender system for arXiv prepints, we find that on average more than $47\\%$ of papers have less than a $0.0001\\%$ probability of being recommended to any user, even when the number of users and items are the same. Thus, many algorithmic techniques have been proposed to improve item fairness in recommendation [3, 33, 46, 48]. However, by not solely optimizing for user engagement, these techniques impose a cost both to overall recommendation quality [3] and especially for some individual users more than others. ", "page_idx": 0}, {"type": "text", "text": "Accordingly, algorithms have recently been introduced to address the problem of two-sided fairness (or multi-sided fairness), in which the platform aims to balance user fairness, item fairness, and overall recommendation quality [11, 12, 45]. These algorithms formalize the desired balance in terms of an optimization problem \u2013 for example, maximizing the difference between overall recommendation quality and unfairness penalties [11], or the overall recommendation quality subject to fairness constraints [12]. The relative importance of user and item fairness is described by the relative strength of the respective unfairness penalties or slack in the fairness constraints. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, an open question is, what are the implications of such algorithms on the recommendations, i.e., what do multi-sided constraints do, and what is the price of (multi-sided) fairness? More specifically, are there settings in which we can simultaneously maximize all objectives, \u201cfor free?,\u201d as opposed to there being large tradeoffs as commonly emphasized? Are some users or items \u2013 for example, those new to the platform \u2013 more affected than others? How do the answers to the above questions depend on the context and, in real-world settings, do \u201cfairness\u201d constraints substantially affect recommendation characteristics? Such real-world considerations are essential for platform designers to understand. In fact, a recent survey and critique of the fair ranking literature advocated for such grounded analyses to understand algorithmic implications and tradeoffs [38], as opposed to black-box deployment of fairness algorithms. ", "page_idx": 1}, {"type": "text", "text": "Answering such questions is challenging. Theoretically, multi-sided fairness is cast as an optimization problem, and conceptually characterizing optimization solutions is often intractable. For example, given an arbitrary utility matrix and a constraint on the exposure provided to each item, it is not tractable to calculate recommendations in closed form: the solution depends on global structure, users may not receive their most preferred items, and items may not be recommended to the users who most prefer them. Then, once phenomena are theoretically identified, empirically verifying phenomena requires specifying a recommendation setting and measuring user-item utilities as a function of their characteristics. Given these challenges, our contributions are as follows. ", "page_idx": 1}, {"type": "text", "text": "Theoretical framework to characterize solutions of multi-sided fair recommendations. We formulate a concave optimization problem in which user fairness is formalized as an objective on the minimum normalized utility provided to each user \u2013 and item fairness constraints determine the problem\u2019s feasible region, through the solutions of another concave optimization. This formulation qualitatively captures standard multi-objective approaches for user-item fairness [5, 12]. We show that when item fairness constraints are maximal, the solutions to this optimization problem (recommendation probabilities for each user) have a sparse structure that can be characterized as a function of the problem inputs (e.g., estimated utilities for each user-item pair, or slack given in the item fairness constraint). ", "page_idx": 1}, {"type": "text", "text": "Conceptual insights on the price of fairness. We use our theoretical framework to characterize user-item fairness tradeoffs. We identify two phenomena: (a) \u201cfree fairness\u201d as a function of user preference diversity: if users have sufficiently diverse preferences, imposing item fairness constraints can have large benefits to individual items with little cost to users, i.e., there is a small price of fairness. (b) \u201cReinforced disparate effects\u201d due to preference uncertainty. Of course, users for whom the platform has poor preference estimation (e.g., \u201ccold start\u201d users on whom the platform has no data) typically receive more inaccurate recommendations; we show that this effect may be worsened with item fairness constraints, in a worst case sense: when a user\u2019s preferences are uncertain, item-fair recommendation algorithms will recommend them the globally least preferred items \u2013 even when attempting to maximize the minimum user utility. ", "page_idx": 1}, {"type": "text", "text": "Empirical measurement. Finally, we use real data to prototype a recommendation engine for new arXiv preprints and use this system to measure the above phenomena in practice. For example, we find that more homogeneous groups of users have steeper user-item fairness tradeoffs \u2013 as theoretically predicted, diverse user preferences decrease the price of item fairness. Furthermore, we find that the \u201cprice of misestimation\u201d is high (users for whom less training data is available receive poor recommendations), but on average item fairness constraints do not increase this cost. ", "page_idx": 1}, {"type": "text", "text": "Putting things together, we show that the real-world effects of user-item recommendation fairness constraints heavily depend on the empirical context. In some cases, \u201citem fairness\u201d comes for free, with little cost to users. In others, deploying such an algorithm may lead to especially poor recommendations for some users, in ways that cannot be mechanically addressed by adding user fairness terms. We urge designers of fair recommendation systems in practice to develop such evaluations to measure such individual-level effects, and for researchers to further characterize the potential implications of such algorithms. Our code is available at this repository. ", "page_idx": 1}, {"type": "text", "text": "2 Formal Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our setup is characterized by user and item (estimated) utilities, recommendation optimization with user and item fairness desiderata, and evaluation of the effects of such desiderata. (As discussed below, some of these modeling choices are made for concreteness, and our results extend beyond these specific choices). ", "page_idx": 2}, {"type": "text", "text": "Users, items, and utilities. There is a finite population of $m$ users and $n$ items. Let $w_{i j}>0$ be the utility of recommending item $j$ to user $i$ ; this could represent a click-through rate or purchase probability. We suppose that user-item utilities are symmetric: each of the user $i$ and item $j$ receives utility $w_{i j}$ from being recommended item $j$ . ", "page_idx": 2}, {"type": "text", "text": "Let $\\Delta_{n-1}$ denote the simplex in $\\mathbb{R}^{n}$ . The platform\u2019s task is to choose a recommendation policy $\\rho\\,\\in\\,\\Delta_{n-1}^{m}$ . For each user $i$ , the platform will recommend one item to user $i$ selected randomly according to the distribution $\\rho_{i}$ . Given a recommendation policy $\\rho$ , user $i$ \u2019s expected utility from using the platform is $\\sum_{j}\\rho_{i j}w_{i j}$ ; item $j$ \u2019s expected utility is $\\sum_{i}\\rho_{i j}w_{i j}$ , where $\\bar{\\sum_{j}\\rho_{i j}}=1$ for each user $i$ . ", "page_idx": 2}, {"type": "text", "text": "Fairness desiderata. We suppose that the platform uses as a benchmark, for each user or item, the best thing that it could do for that agent if it ignored the utilities of others. Thus, given a recommendation policy $\\rho$ , let $U_{i}(\\rho,w)$ be user $i$ \u2019s utility from $\\rho$ , normalized by the utility $\\operatorname*{max}_{j}w_{i j}$ they would receive from being recommended their best match. Let $I_{j}(\\rho,w)$ be item $j$ \u2019s utility from $\\rho$ , normalized by the utility $j$ receives if it is recommended to every user, $\\textstyle\\sum_{i}w_{i j}$ . The normalized utilities are: ", "page_idx": 2}, {"type": "equation", "text": "$$\nU_{i}(\\rho,w)=\\frac{\\sum_{j}\\rho_{i j}w_{i j}}{\\operatorname*{max}_{j}w_{i j}},\\quad I_{j}(\\rho,w)=\\frac{\\sum_{i}\\rho_{i j}w_{i j}}{\\sum_{i}w_{i j}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The normalizations capture that recommendations should not be affected by scaling utilities, or be distorted by a user who is not satisfied with any item or an item that is generally undesirable to users. ", "page_idx": 2}, {"type": "text", "text": "Given a recommendation policy $\\rho$ , user fairness is quantified as the minimum normalized user utility $U_{\\mathrm{min}}\\left(\\rho,w\\right)=\\mathrm{min}_{i}\\,U_{i}(\\bar{\\rho,w})$ , and item fairness analogously as $I_{\\mathrm{min}}\\left(\\rho,w\\right)=\\mathrm{min}_{j}\\,I_{j}(\\rho,w)$ . ", "page_idx": 2}, {"type": "text", "text": "Multi-objective recommendation optimization. We suppose that the platform seeks to satisfy its fairness desiderata as follows. At the extremes, the platform could choose a maximally fair solution for one side, ignoring the other. Denote the optimal user fair utility as $U_{\\mathrm{min}}^{*}\\left(w\\right):=\\operatorname*{max}_{\\rho}U_{\\mathrm{min}}\\left(\\rho,w\\right)$ (achieved by giving each user their favorite item deterministically), and the optimal item fair utility as $I_{\\operatorname*{min}}^{*}\\left(w\\right):=\\operatorname*{max}_{\\rho}I_{\\operatorname*{min}}\\left(\\rho,w\\right)$ . ", "page_idx": 2}, {"type": "text", "text": "Finally, we cast the two-sided fair optimization \u2013 for the optimal $\\gamma.$ -constrained user fair solution \u2013 as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{U_{\\mathrm{min}}^{*}\\left(\\gamma,w\\right)=\\underset{\\rho}{\\operatorname*{max}}}&{U_{\\mathrm{min}}\\left(\\rho,w\\right)}\\\\ {\\mathrm{subject\\,to}}&{I_{\\mathrm{min}}\\left(\\rho,w\\right)\\geq\\gamma I_{\\mathrm{min}}^{*}\\left(w\\right)\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "i.e., we maximize the minimum normalized user utility, subject to the minimum normalized item utility being at least a fraction $\\gamma$ of the optimal item fair solution. ", "page_idx": 2}, {"type": "text", "text": "Research questions: price of fairness and misestimation. We can now define the price of item fairness on user fairness $\\pi_{U|I}^{F}$ (\u201cprice of fairness\u201d) as the decrease in user fairness with maximal item fairness constraints: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{U|I}^{F}(w):=\\frac{U_{\\mathrm{min}}^{*}\\left(w\\right)-U_{\\mathrm{min}}^{*}\\left(\\gamma=1,w\\right)}{U_{\\mathrm{min}}^{*}\\left(w\\right)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We ask how $\\pi_{U|I}^{F}$ changes with the utility matrix $w$ . We note that while this question (in terms of solutions to Problem (1)) can be simply stated, as we detail in Section 3, finding a closed form expression for $U_{\\operatorname*{min}}^{*}\\left(1,w\\right)$ \u2013 the optimal minimum normalized user utility given item fairness constraints \u2013 in terms of $w$ is theoretically challenging. ", "page_idx": 2}, {"type": "text", "text": "Similarly, we investigate the price of misestimation. Let $\\hat{w}_{i j}$ denote the platform\u2019s estimate of the utility of recommending $i$ to $j$ ; let $\\hat{\\rho}(\\gamma)$ be a policy that solves the optimization problem above with misestimated utilities, that is, $\\hat{\\rho}(\\gamma)$ attains $U_{\\operatorname*{min}}^{*}\\left(\\gamma,\\hat{w}\\right)$ .1 Then we define the price of misestimation on user utility (\u201cprice of misestimation\u201d) $\\pi_{U}^{M}$ as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{U}^{M}(\\gamma,w,\\hat{w}):=\\frac{U_{\\operatorname*{min}}^{*}\\left(\\gamma,w\\right)-U_{\\operatorname*{min}}\\left(\\hat{\\rho}(\\gamma),w\\right)}{U_{\\operatorname*{min}}^{*}\\left(\\gamma,w\\right)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "that is, the decrease in true user fairness due to optimizing with estimated utilities. In particular, in Section 5 we will examine whether item fairness exacerbates the price of misestimation by comparing $\\pi_{U}^{M}(0,w,\\hat{w})$ and $\\pi_{U}^{M}(1,w,\\hat{w})$ , particularly in the case of cold start users. ", "page_idx": 3}, {"type": "text", "text": "Discussion. We note several modeling choices. First, we assume that utility $w_{i j}$ is shared \u2013 both the item $j$ and user $i$ benefit equally from a successful recommendation. This choice captures, e.g., purchase or click-through rates. It also helps us isolate effects due to fairness constraints and effects across items and users, as opposed to misaligned utilities. We expect the price of fairness to increase \u2013 and potentially be arbitrarily high \u2013 with such misaligned utilities. We further justify and relax the shared utility assumption in Appendix A. Second, we quantify fairness through the minimum normalized utility. Normalization is standard in related algorithmic work, such as [45], and prevents solutions in which an item that provides utility $\\epsilon$ to every user needs to be recommended to every user to equalize utilities. We use individual egalitarian fairness (minimum utility over individuals) instead of group fairness to capture settings in which group identity is not available, and systems in which individual-level disparities may be widespread; egalitarian fairness is widespread in algorithmic fairness [2, 17, 41, 43]. In Appendix A we show that our empirical findings extend to other measures of individual fairness. Finally, in the user-item fairness tradeoff problem (Problem 1) we used an item fairness constraint rather than adding an item fairness term to the objective; these two approaches can be thought of conceptually as dual to one another, and so we expect similar properties to hold in either formulation. ", "page_idx": 3}, {"type": "text", "text": "3 Theoretical framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To determine how the price of fairness $\\pi_{U|I}^{F}$ depends on utility matrix $w$ , we need to compute $U_{\\operatorname*{min}}^{*}\\left(1,w\\right)$ and $U_{\\mathrm{min}}^{*}\\left(w\\right)$ . It turns out that $U_{\\mathrm{min}}^{*}\\left(w\\right)$ is easy to describe: without item fairness constraints, the optimal recommendation policy deterministically recommends each user their most preferred item. Each user attains the maximum possible normalized utility of 1, and $U_{\\operatorname*{min}}^{*}\\left(w\\right)=1$ . ", "page_idx": 3}, {"type": "text", "text": "However, with an item fairness constraint, we can no longer select each user\u2019s recommendation policies independently. Thus characterizing $U_{\\operatorname*{min}}^{*}\\left(1,w\\right)$ is much more complicated. Recall that $U_{\\operatorname*{min}}^{*}\\left(1,w\\right)$ is the minimum normalized user utility of the optimal user-fair recommendation subject to maximal item fairness constraints. Plugging into Problem (1) and expanding the definitions, we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{U_{\\mathrm{min}}^{*}\\left(1,w\\right)=\\underset{\\rho\\in\\Delta_{n-1}^{m}}{\\operatorname*{max}}}&{\\underset{i}{\\operatorname*{min}}\\,\\frac{\\sum_{j}w_{i j}\\rho_{i j}}{\\operatorname*{max}_{j}w_{i j}}}\\\\ {\\mathrm{subject~to}}&{\\rho\\in\\arg\\underset{\\phi\\in\\Delta_{n-1}^{m}}{\\operatorname*{max}}\\,\\underset{j}{\\operatorname*{min}}\\,\\frac{\\sum_{i}w_{i j}\\phi_{i j}}{\\sum_{i}w_{i j}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thus we need to find closed-form solutions in terms of the utilities $w$ to a non-linear concave program, in which both the objective and the constraint depend on $w$ , and indeed even determining the constraint requires solving another non-linear concave program. ", "page_idx": 3}, {"type": "text", "text": "In Proposition 1, we develop a framework to solve this problem, which we later apply to show our main results Theorems 3 and 4. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Suppose that for a set of recommendation policies $S\\subseteq\\Delta_{n-1}^{m}$ , (i) $\\boldsymbol{S}$ can be described by a finite set of linear constraints (ii) There exists an optimal solution $\\rho^{*}$ to Problem (2) such that $\\rho^{*}\\in S$ (iii) $\\rho^{*}$ is the unique feasible solution to Problem (2) in $\\boldsymbol{S}$ ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Then, finding an optimal solution $\\rho^{*}$ to Problem (2) can be reduced to solving a linear program $\\mathcal{L}$ . ", "page_idx": 4}, {"type": "text", "text": "With Proposition 1, the key technical challenges become: (a) given utility matrix $w$ , finding $\\boldsymbol{S}$ satisfying the above conditions; (b) given $\\boldsymbol{S}$ and $w$ , finding a closed-form expression for $\\rho^{*}$ ; (c) given a closed form for $\\rho^{*}$ in terms of $w$ , reasoning about the properties of item fair solution $U_{\\operatorname*{min}}^{*}\\left(1,w\\right)$ . ", "page_idx": 4}, {"type": "text", "text": "To do this, for our main results, we construct $\\boldsymbol{S}$ as a set of recommendations with a particular symmetric structure. Suppose users come in $K$ types, where a user of type $k$ shares a utility vector. Then, consider $S_{\\mathrm{symm}}=\\{\\rho:\\rho_{i}=\\rho_{i^{\\prime}}$ if $w_{i}=w_{i^{\\prime}}\\}\\subseteq\\Delta_{n-1}^{m}$ , the set of policies where users of the same type are given the same recommendation probabilities. Note that, in the extreme case, each user is their own type. Furthermore, let $\\rho_{k}$ denote the recommendation policy for type $k$ , for $\\rho\\in S_{\\mathrm{symm}}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 2. ${\\cal S}_{s y m m}$ satisfies conditions $(i)$ and (ii) in Proposition 1. Furthermore, solutions $\\rho\\in S_{s y m m}$ to Problem (2) have a sparse structure: ", "page_idx": 4}, {"type": "text", "text": "\u2022 If $\\rho$ is a basic feasible solution to the linear program $\\mathcal{L}$ in Proposition 1, then there are at most $n+K-1$ type-item pairs $(k,j)$ such that $\\rho_{k j}>0$ (out of $n K$ possible pairs). \u2022 If $\\rho$ is also optimal, then there are at most $K-1$ items that are ever recommended to more than one type of user, i.e., where $\\rho_{k j},\\rho_{k^{\\prime}j}>0\\,f o r\\,k\\ne k^{\\prime}$ . ", "page_idx": 4}, {"type": "text", "text": "For our main results, we will leverage the sparsity structure in Proposition 2 to show, with further restrictions on utility matrix $w$ , that $\\mathcal{S}_{\\mathrm{symm}}$ also satisfies $(i i i)$ . We will then derive closed-form expressions for that solution, and show how it changes as a function of $w$ . ", "page_idx": 4}, {"type": "text", "text": "The sparsity structure in Proposition 2 is also interesting in its own right. Without item fairness constraints, solutions will be highly sparse: each user will be recommended their most preferred item deterministically; each other item will have a recommendation probability of zero. Once we add item fairness constraints, solutions do not necessarily remain sparse. However, Proposition 2 shows that sparse solutions still arise under item fairness constraints in settings with symmetric solutions. ", "page_idx": 4}, {"type": "text", "text": "4 User preference diversity and fairness tradeoffs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now use the theoretical framework developed in the above section to understand the effect of the structure of user utilities on the price of fairness. In particular, we identify \u201cfree fairness,\u201d i.e., the price of fairness is low, when preferences are sufficiently diverse. ", "page_idx": 4}, {"type": "text", "text": "Example 1. For intuition as to why user diversity affects the price of fairness, consider the following example. Suppose we have $n=2$ items and $m$ users, and that half the users have utility $\\epsilon$ for the first item and $1-\\epsilon$ for the second item. Then, the recommender can simply give each user their favorite item (the user optimal solution), and this solution simultaneously maximizes user and item fairness as well as total user and item utility. ", "page_idx": 4}, {"type": "text", "text": "On the other hand, suppose all users have the same preferences: each user has utility $1-\\epsilon$ for the first item and utility $\\epsilon$ for the second item, for $\\epsilon>0$ that is small. Then, any recommendation probability given to the second item comes at a cost to users who receive that item instead of the first item; however, (normalized) item fairness would require that the second item receives $\\epsilon$ as much utility as the first item. Below, we show that this results in a tradeoff even between linear normalized user and item utilities, where we account for the fact that the second item is on average less preferred by users. ", "page_idx": 4}, {"type": "text", "text": "Since all users have the same preferences for items, using Proposition 2 it is sufficient to consider them receiving the same recommendation probabilities $\\rho_{1}$ for the first item and $\\rho_{2}=1-\\rho_{1}$ for the second. Bounding minimum item utility $I_{\\mathrm{min}}$ by the utility of the second item, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\nI_{\\mathrm{min}}\\,\\le I_{2}(\\rho)=\\frac{\\sum_{i}\\rho_{2}\\epsilon}{m\\epsilon}=\\rho_{2}\\triangleq1-\\rho_{1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For a given recommendation probability $\\rho_{1}$ , the minimum normalized user utility is ", "page_idx": 4}, {"type": "equation", "text": "$$\nU_{\\mathrm{min}}=(1-\\epsilon)\\rho_{1}+\\epsilon\\rho_{2}=\\epsilon+(1-2\\epsilon)\\rho_{1}\\le\\rho_{1}+\\epsilon.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Rearranging, we get that ", "page_idx": 4}, {"type": "equation", "text": "$$\nU_{\\mathrm{min}}\\,-\\epsilon\\le\\rho_{1}\\le1-I_{\\mathrm{min}}\\implies U_{\\mathrm{min}}+I_{\\mathrm{min}}\\le1+\\epsilon.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Thus the minimum normalized user utility and the minimum normalized item utility essentially follow a negative linear relationship \u2013 guaranteeing the second item even $\\epsilon$ as much utility as the first item results in a linear cost to users. \u25a1 ", "page_idx": 4}, {"type": "text", "text": "We now formalize and generalize this example, when the level of heterogeneity in the population can be captured by a single parameter. Consider the following utility matrix structure. Let $v_{1}>$ $v_{2}>...>v_{n}>0$ . Suppose that there are two user types: a user $i$ either has utility $w_{i j}\\,=\\,v_{j}$ or $w_{i j}=v_{n-j+1}$ , and say that user $i$ is of type 1 or 2 respectively. In words, the two types have opposite preferences, but the preferences can otherwise be generic and be for any number of items. Now, the direct solution by symmetry for the example no longer directly holds \u2013 the items in the middle, not necessarily preferred by either user type, may be binding in terms of item fairness constraints. ", "page_idx": 5}, {"type": "text", "text": "Let $\\alpha$ be the proportion of type 1 users in the population, out of a fixed population of $m$ users. Parameter $\\alpha$ thus controls the population heterogeneity; if $\\alpha$ is near 0 or 1, the population is highly homogeneous, dominated by users of the same type. If $\\begin{array}{r}{\\bar{\\alpha}=\\frac{1}{2}}\\end{array}$ , the population is split evenly between the two types and is highly heterogeneous. Since we parametrize $w$ by $\\alpha$ , we may write the price of fairness as, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{U|I}^{F}(\\alpha):=\\frac{U_{\\mathrm{min}}^{*}\\left(\\alpha\\right)-U_{\\mathrm{min}}^{*}\\left(\\gamma=1,\\alpha\\right)}{U_{\\mathrm{min}}^{*}\\left(\\alpha\\right)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Given this structure, Theorem 3 states that the price of fairness $\\pi_{U|I}^{F}(\\alpha)$ increases in the homogeneity of the users \u2013 heterogeneous user populations are less affected by incorporating item fairness constraints. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. $\\pi_{U|I}^{F}(\\alpha)$ is decreasing in $\\alpha$ for $0<\\alpha\\leq1/2$ , and increasing in $\\alpha$ for $1/2\\le\\alpha<1$ . ", "page_idx": 5}, {"type": "text", "text": "Proof sketch for Theorem 3. We show that when in a population with two opposing types as described above, the sparsity condition in Proposition 2 yields a unique solution, for which we can find a closed form and express in terms of $\\alpha$ . We then evaluate $U_{\\mathrm{min}}\\left(\\rho,\\alpha\\right)$ at this solution to find $U_{\\operatorname*{min}}^{*}\\left(1,\\alpha\\right)$ , and show that this is indeed increasing. The full proof is in Appendix D. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "5 Uncertainty and fairness tradeoffs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A basic fact \u2013 often ignored in fair recommendation \u2013 is that recommendations are made with (mis)estimated utilities. Platforms do not have full knowledge of user preferences, especially those new to the platform. Of course, recommendations under misestimated utilities may be poor; here, we show that adding item fairness constraints may worsen the cost of this misestimation even further. ", "page_idx": 5}, {"type": "text", "text": "Intuitively \u2013 for a new user for whom the platform has no data \u2013 the platform would estimate the user\u2019s preferences as the average of preferences of existing users (e.g., in a Bayesian fashion). Thus, without item fairness considerations, it would show the user generally popular items. However, the new user\u2019s preferences are generally estimated as \u201cweaker\u201d than the preferences of others for any given item (since it averages preferences of users who may either like or dislike any given item). Thus, with fairness constraints, the optimization is incentivized to show the user otherwise unpopular items, since all the user\u2019s estimated preferences are weaker. Liu and Burke [29] for example develop an algorithm for item fairness where users with weaker preferences are explicitly leveraged in this way. ", "page_idx": 5}, {"type": "text", "text": "For a given item fairness level $\\gamma$ , true utility matrix $w$ , and estimated utility matrix $\\hat{w}$ , let $\\hat{\\rho}(\\gamma)$ be a recommendation policy that solves the recommendation problem (Problem 1) with respect to the misestimated utilities, that is, $\\hat{\\rho}$ solves $U_{\\operatorname*{min}}^{*}\\left(\\gamma,\\hat{w}\\right)$ . Recall that we define the price of misestimation ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{U}^{M}(\\gamma,w,\\hat{w})=\\frac{U_{\\mathrm{min}}^{*}\\left(\\gamma,w\\right)-U_{\\mathrm{min}}\\left(\\hat{\\rho}(\\gamma),w\\right)}{U_{\\mathrm{min}}^{*}\\left(\\gamma,w\\right)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which represents the relative decrease in minimum normalized user utility as a result of misestimation.   \nItem fairness worsens the price of misestimation if $\\pi_{U}^{M}(\\gamma=1,w,\\hat{w})\\stackrel{\\cdot}{>}\\pi_{U}^{M}(\\gamma=0,w,\\hat{w})$ . ", "page_idx": 5}, {"type": "text", "text": "We now formalize the above argument, building on the analysis in the previous section. As in Section 4, suppose that there are 2 types of users, with opposing preferences (i.e., with values $v_{1},v_{2}...,v_{n}$ and $v_{n},v_{n-1},...,v_{1}$ , respectively) \u2013 and the platform has correctly estimated these preferences. However, now, these two types only make up a proportion $\\beta$ of the population each. ", "page_idx": 5}, {"type": "text", "text": "Now, we suppose that there is a fraction $1-\\beta$ of the user population who are \u201cnew\u201d users. We assume that these users are drawn from the same distribution as the remaining users, but the platform does not know their preferences. It thus constructs a prior by averaging over the known users\u2019 preferences \u2013 (mis)estimating the users\u2019 utility for each item $j$ as $\\textstyle{\\frac{v_{j}+v_{n-j+1}}{2}}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4. If $\\beta>{\\frac{1}{n}}$ and $w$ and $\\hat{w}$ are as described above, then fairness constraints can arbitrarily worsen the price of misestimation. ", "page_idx": 6}, {"type": "text", "text": "\u2022 The price of misestimation without fairness constraints is low: for all $\\{v_{j}\\}$ , there is $a$ recommendation policy $\\hat{\\rho}$ that solves the misestimated problem $U_{\\operatorname*{min}}^{*}\\left(0,\\hat{w}\\right)$ so that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi_{U}^{M}(0,w,\\hat{w})\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "\u2022 The price of misestimation with fairness constraints can be arbitrarily large: $\\forall\\epsilon$ , there exists $\\{v_{j}\\}$ and a recommendation policy $\\hat{\\rho}$ that solves the problem $U_{\\operatorname*{min}}^{*}\\left(1,\\hat{w}\\right)$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi_{U}^{M}(1,w,\\hat{w})>1-\\epsilon.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof sketch. The main task is to find the price of misestimation with fairness constraints, which requires computing $U_{\\operatorname*{min}}^{*}\\left(1,w\\right)$ when users\u2019 values are correctly estimated, and computing $U_{\\operatorname*{min}}^{*}\\left(1,\\hat{w}\\right)$ when users\u2019 values are incorrectly estimated. To find $U_{\\operatorname*{min}}^{*}\\left(1,w\\right)$ , note that $w$ is a population with two opposing types of users, so we may leverage the insights of Theorem 3. To find $U_{\\operatorname*{min}}^{*}\\left(1,\\hat{w}\\right)$ we again use the framework in Section 3, showing that in the setting of this theorem, we can find an optimal policy $\\rho^{*}$ for $U_{\\operatorname*{min}}^{*}\\left(1,\\hat{w}\\right)$ in a set $S^{\\prime}\\subseteq\\bar{S}_{\\mathrm{symm}}$ of policies with an additional column-symmetry property. We use an analogue of the sparsity result in Proposition 2 to show that there is a unique feasible solution ${\\hat{\\rho}}\\in S^{\\prime}$ and obtain a closed form expression for $\\rho^{*}$ , and find an upper bound for $U_{\\operatorname*{min}}\\left(\\hat{\\rho},\\hat{w}\\right)=U_{\\operatorname*{min}}^{*}\\left(1,\\hat{w}\\right)$ . In fact, we show that as long as $\\beta>{\\frac{1}{n}}$ , under $\\hat{\\rho}$ the mis-estimated users will never be recommended their most preferred items. The full proof is in Appendix E. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "The idea follows the above intuition: without fairness constraints and assuming cold start users follow the same distribution as existing users, the platform\u2019s price of misestimation is low because the platform treats cold start users as the average of the existing population. Fairness constraints, however, can make this cost arbitrarily high, as in expectation cold start users relatively enjoy items other users do not. Such effects suggest that a more careful treatment of uncertainty and fairness together is necessary for recommendation algorithms. ", "page_idx": 6}, {"type": "text", "text": "6 Empirical findings: arXiv recommendation engine ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We prototype a recommender for preprints on arXiv, to illustrate our conceptual findings. We consider the cold start setting for items (papers), when they are newly uploaded to arXiv and so only have metadata and paper text but no associated interaction or citation data. For users (readers), we use as data the papers that they have shared on arXiv to estimate their preferences. ", "page_idx": 6}, {"type": "text", "text": "Empirical setup. We use data from arXiv and Semantic Scholar [1, 23]. As training for user preferences, we consider 139,308 CS papers by 178,260 distinct authors before 2020; as the items to be recommended, we consider the 14,307 papers uploaded to arXiv in 2020. We apply two natural language processing-based models \u2013 TF-IDF [26] and the sentence transformer model SPECTER [13] \u2013 to textual features such as the paper\u2019s abstract (for both items and the user\u2019s historical papers) to generate embeddings for all papers in the training set. We use these embeddings to compute similarity scores (utility matrices) for users and items. To compute the similarity score (utility) between a user (an author of at least one paper before 2020) and an item (a paper uploaded in 2020), we compute the cosine similarity between the embedding of each of the user\u2019s pre-2020 papers and the item\u2019s embedding. We then use either the mean or the max similarity amongst the pre-2020 papers and the item; the max similarity score may more effectively capture a user\u2019s diverse interests [18, 39]. We then generate recommendations for each user, at various levels of user and item fairness constraints. ", "page_idx": 6}, {"type": "text", "text": "To validate our recommendation approach, we use citation data from Semantic Scholar [23] to determine for each user and each paper published in 2020 whether the user cites that paper in their post-2020 work. We then examine how well the user-item similarity score generated by our recommendation engine predicts the presence of a citation. We find that our recommendations effectively predict whether a user cites a paper with a high score after 2020. In Table 1 we show the results of a logistic regression between each similarity score and the presence of a citation, and observe that the coefficient on the score is significantly positive for each model. We generally find that the max score models are more predictive of future citations. Appendix B includes details and evaluations; our computational experiments in the main text use the max score, TF-IDF model. ", "page_idx": 6}, {"type": "table", "img_path": "ZOZjMs3JTs/tmp/a7c7350cfcace261e582e2842424ea113ba459f01055ce0d4811aad71a0d3503.jpg", "table_caption": ["Table 1: Logistic regression results for predicting whether user $i$ cites paper $j$ from the similarity score $w_{i j}$ for each model. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/daf0260f85218f5606e2c92d06511332236bfb006c3d8601eb8ab973b773168b.jpg", "img_caption": ["(a) Homogeneous versus diverse users "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/05f44c15a840c56debd68f2fedc8021be16ec7f11294a9c3a5b3c582c4adb8d9.jpg", "img_caption": ["(b) With and without misestimation "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 1: Empirical (using our arXiv recommender) tradeoff between the minimum user (Y axis) and item (X axis) utility $\\boldsymbol{\\cdot}$ is the fraction of the best possible minimum normalized item utility $I_{\\mathrm{min}}^{*}$ we guarantee). (a) Illustrating Theorem 3 empirically \u2013 homogeneous populations have a higher price of fairness. Empirically, however, the price of fairness is small except with strict item fairness constraints $\\gamma\\rightarrow1$ . (b) For a set of users, holding other users fixed, the cost to the worst-off user of misestimating their preferences at varying levels of fairness. Empirically, the cost of misestimation is high, but is not worsened with item fairness constraints, as in the worst case analysis of Theorem 4. ", "page_idx": 7}, {"type": "text", "text": "6.1 Empirical results. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We examine the tradeoffs between item and user fairness and the effect of misestimation on this tradeoff empirically. We work with a dataset of all 14,307 papers from 2020 and 1,000 randomly selected authors who published papers prior to 2020. The recommendation engine described above produces similarity scores between each author and paper in this dataset; we take $w$ to be these similarity scores. For a given value of $\\gamma$ , to compute $U_{\\operatorname*{min}}^{*}\\left(\\gamma\\right)$ we use the cvxpy implementation of the convex optimization algorithm SCS [36]. ", "page_idx": 7}, {"type": "text", "text": "User-item tradeoffs as function of user diversity. Figure 3a shows the tradeoff between user fairness and item fairness in a random population and in a population of homogeneous users. To generate a tradeoff curve for the heterogeneous population, we sampled 20 random papers and 40 random authors to form $w$ , and computed $U_{\\mathrm{min}}^{*}\\left(\\gamma,w\\right)$ for 50 values of $\\gamma$ between 0 and 1. To generate tradeoff curves from homogeneous user populations, we clustered all 1,000 users into 25 clusters using the $k$ -means algorithm. For a single curve, to form $w$ we sampled 20 random papers and all authors in one random cluster, which has 40 users in expectation. We run 10 experiments and plot the mean of $U_{\\operatorname*{min}}^{*}\\left(\\gamma,w\\right)$ at each value of $\\gamma$ across the 10 curves as well as two std. error bars for the mean. ", "page_idx": 7}, {"type": "text", "text": "Figure 3a demonstrates that in real data, for moderate item fairness guarantees $(0\\leq\\gamma\\leq0.9)$ , on average there is a fairly low cost to user fairness, but as we approach optimal item fairness $(\\gamma\\rightarrow1)$ ), the tradeoff becomes very steep. Furthermore, Figure 3a shows that item fairness tends to impose a higher cost to user fairness in more homogeneous populations, as in Theorem 3. ", "page_idx": 7}, {"type": "text", "text": "Price of misestimation. Figure 3b shows how user fairness is affected by item fairness guarantees in the presence of misestimation. For sets of 20 random papers and 40 random authors, we select a set $10\\bar{\\%}$ of these users at random and treat them as if we did not have any data for them, estimating these users\u2019 utility for item $j$ as the average utility for item $j$ for the other $90\\%$ of the users $\\hat{w}$ in the notation of Theorem 4). For 50 values of $\\gamma$ between 0 and 1, we compute $U_{\\operatorname*{min}}^{*}\\left(1,\\hat{w}\\right)$ for the misestimated utility matrix. We also compute, counterfactually, this quantity if the utility matrix had been correctly estimated, $U_{\\operatorname*{min}}^{*}\\left(1,w\\right)$ . We plot the true minimum normalized user utility for the misestimated group under the recommendation policies that attain $U_{\\operatorname*{min}}^{*}\\left(1,w\\right)$ and $U_{\\operatorname*{min}}^{*}\\left(1,\\hat{w}\\right)$ . We run 10 experiments and plot the mean value of the minimum normalized user utility and two std. error bars. ", "page_idx": 8}, {"type": "text", "text": "In Figure 3b, near $\\gamma=0$ , the price of misestimation \u2013 the gap between the two curves \u2013 is higher than the cost of misestimation near $\\gamma=1$ . This results because the item fairness constraint barely changes the (already low) utility of these users when their preferences are misestimated, but substantially changes their utility when their preferences are correctly estimated. While theoretically, item fairness constraints can arbitrarily increase the cost of misestimation, in practice, on average they do not affect this cost \u2013 the cost of misestimation without item fairness is already high. ", "page_idx": 8}, {"type": "text", "text": "7 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "There is a large literature on (item) fair recommendation and ranking [3, 33, 46, 48, 49] and, more recently, on multi-sided user-item fair recommendation [5, 10\u201312, 16, 37, 45]. This literature is primarily algorithmic: for a given formulation of user and item utility (and other desiderata), how do we devise an efficient algorithm for multiple objectives or constraints? In contrast, our goal is primarily conceptual, to aid algorithm designers in choosing when to use such algorithms: for example, by explaining when we might expect the tradeoff to be especially sharp, and to understand the cost to cold start users in particular. For example, we theoretically analyze recommendations from a constrained optimization-based approach akin to that in Basu et al. [5], in terms of the implications of such an approach on recommendations. ", "page_idx": 8}, {"type": "text", "text": "Several papers observe related phenomena to the ones we study, especially empirically. Wang and Joachims [45] develop an optimization algorithm to be fair to users (in terms of group fairness to demographic groups) and items (similar to our normalized utility metric). Theoretically, they show that there is a tradeoff between user and item utility metrics, and further empirically show how fairness interacts with the diversity of items shown to each user. While their focus is also primarily algorithmic, they do show that there is a fundamental tension between item and user fairness. In concurrent work, Kleinberg and Meister [24] also theoretically demonstrate and characterize this tension. They focus on the cost to individual users caused by imposing maximal item fairness constraints \u2013 similar to how we define price of fairness \u2013 and determine the relationship between a cost level and the proportion of users in the worst-case recommendation setting who experience that cost. In constrast, we examine the cost to the single worst-off user as a function of properties of the recommendation setting such as user diversity and recommender mis-estimation. Rahmani et al. [40] demonstrate a similar tension between item and user fairness in empirical data. Liu and Burke [29] do not examine user fairness, but observe that one can mitigate the cost of item fairness in multi-sided recommendations by recommending to users with weaker preferences for items that may otherwise be less preferred. Subsequently, Farastu et al. [16] examine which users bear the cost of item fairness, pointing to Liu and Burke [29] to argue that the cost to users of item fairness constraints disproportionately falls on users with flexible preferences, creating an incentive for users to misrepresent their preferences as more rigid than reality. We build on these arguments by theoretically analyzing the cost of item fairness on (individual) users, especially the users most affected, as a function of the estimated user preference matrix. ", "page_idx": 8}, {"type": "text", "text": "In focusing on conceptual phenomena, our work is related to work analyzing the price of fairness and efficiency-equity tradeoffs in various settings beyond recommendations. Bertsimas et al. [8] first defined the price of fairness as the normalized decrease in the utility of an algorithmic outcome after adding fairness considerations, and develop general bounds on the price of fairness in an array of optimization scenarios. This concept has been subsequently applied in a variety of domains, including auction theory [22], fair division and resource allocation [6, 30, 44], and computer networking [47]. Barre et al. [3] apply a similar concept in assortment optimization, examining the cost of item visibility constraints on the revenue of a platform. Most similarly, Chen et al. [12] define the price of fairness in the setting of multi-sided fairness as the cost on platform revenue of including both fairness constraints; in contrast we define the price of fairness as the cost to user fairness of imposing item fairness constraints in order to capture the interplay between user and item fairness. The authors then show that this price of fairness on the revenue depends on objective misalignment \u2013 the difference in fairness between the item/user utility required by the constraints, and the item/user utility in a revenueoptimal solution. We study how the price of item fairness on user fairness depends on user preference diversity \u2013 the agreement between users\u2019 utilities. These concepts are related: user preference diversity may cause objective alignment. Moreover, they also consider the problem of unknown preferences: they examine how to algorithmically impose fairness constraints when preferences are unknown, while we address the question of whether fairness constraints disproportionately harm users with unknown preferences. Finally, our result that diverse population preferences can mitigate the price of fairness resembles the result of Bastani et al. [4] that greedy contextual bandits can perform well without exploration if there is sufficient contextual diversity. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Finally, there is a large literature on other tradeoffs in recommendations and ranking: engagement versus value, diversity, strategic behavior, and over-time dynamics [9, 14, 15, 18\u201321, 25, 27, 28, 31, 32, 34, 39]. In the context of set recommendations when users consume their favorite item out of the multiple recommended, e.g., Peng et al. [39] show that there is a minimal utility-diversity tradeoff, and Besbes et al. [9] show that there is a minimal exploration-exploitation tradeoff. ", "page_idx": 9}, {"type": "text", "text": "8 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this project, we investigate the relationship between user fairness and item fairness in recommendation settings. We first develop a theoretical framework to enable us to solve for the price of fairness for many population settings, and develop a recommendation engine using real data to allow us to investigate user-item fairness tradeoffs in practice. Our work informs the design of fair recommendation systems as follows: (1) first, it emphasizes the beneftis of a diverse user population, and suggests that item fairness constraints should not be imposed on sub-markets (sub-groups), but instead on the entire population together. (2) It cautions designers to be especially mindful of effects on individual users (especially cold start users), who may receive disproportionately poor recommendations with item fairness constraints, even in the presence of a user fairness objective. Our empirical analysis supports our theoretical analysis\u2014the user base diversity affects the severity of user-side effects of imposing item fairness; however, the price of misestimating user utility is already high without item fairness constraints, and so imposing such constraints does not have additional effects. Such results speak to the importance of instance-specific analyses, cf. [38]: one cannot make general statements about the specific effects of item-fairness constraints on users (or vice versa) outside of a specific context, though we identify two relevant phenomena (user diversity and misestimation) that modulate these effects. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Our theoretical analysis explores these fairness tradeoffs in a fairly restricted setting. First, we assume that users are only recommended a single item; future work should investigate how the price of fairness changes as the number of recommended items increases. We do not expect our theoretical framework to easily extend to other definitions of fairness; however, we extend our computational arXiv experiments to other definitions of fairness in Appendix A. These extended experiments also show that diverse user preferences reduce fairness tradeoffs; an interesting direction for future work is to theoretically characterize user-item fairness tradeoffs under other definitions of fairness. Furthermore, in practice platforms are unwilling to maximize the worst-off user\u2019s item or user fairness at the expense of the entire platform\u2019s utility; the problem is really one of balancing user and item fairness with overall platform performance. Algorithms to optimize these multisided problems are explored in other work [12], but it would be interesting to develop qualitative observations about user-item fairness tradeoffs in the presence of a total utility constraint. Finally, we show our Theorems 3 and 4 in a limited context with only two or three types of users. However, the theoretical framework developed in Section 3 is significantly more general. It is would be interesting to apply this framework to other population structures such as when users do not have perfectly opposite preferences and where there are more than three groups of users. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "SG is supported by a fellowship from the Cornell University Department of Computer Science and an NSERC PGS-D fellowship [587665]. NG is supported by NSF CAREER IIS-2339427, and Cornell Tech Urban Tech Hub, Meta, and Amazon research awards. The authors would like to thank Sidhika Balachandar, Erica Chiang, Evan Dong, Omar El Housni, Meena Jagadeesan, Jon Kleinberg, Kevin Leyton-Brown, Michela Meister, Chidozie Onyeze, Kenny Peng, Emma Pierson, and Manish Raghavan for valuable conversations and insights, as well as the NeurIPS 2024 reviewers for helpful feedback. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] arXiv.org submitters. arXiv dataset, 2024. URL https://www.kaggle.com/dsv/7548853.   \n[2] Arash Asadpour and Amin Saberi. An approximation algorithm for max-min fair allocation of indivisible goods. In Proceedings of the Thirty-Ninth Annual ACM Symposium on Theory of Computing, STOC \u201907, page 114\u2013121, New York, NY, USA, 2007. Association for Computing Machinery. ISBN 9781595936318. doi: 10.1145/1250790.1250808. URL https://doi.org/ 10.1145/1250790.1250808.   \n[3] Theo Barre, Omar El Housni, Marouane Ibn Brahim, Andrea Lodi, and Danny Segev. Assortment optimization with visibility constraints, 2024. URL https://arxiv.org/abs/2307. 13656.   \n[4] Hamsa Bastani, Mohsen Bayati, and Khashayar Khosravi. Mostly exploration-free algorithms for contextual bandits. Management Science, 67, 07 2020. doi: 10.1287/mnsc.2020.3605.   \n[5] Kinjal Basu, Cyrus DiCiccio, Heloise Logan, and Noureddine El Karoui. A framework for fairness in two-sided marketplaces, 2020. URL https://arxiv.org/abs/2006.12756.   \n[6] Xiaohui Bei, Xinhang Lu, Pasin Manurangsi, and Warut Suksompong. The price of fairness for indivisible goods. Theory of Computing Systems, 65:1069 \u2013 1093, 2019. URL https: //api.semanticscholar.org/CorpusID:152282391. [7] Dimitris Bertsimas and John Tsitsiklis. Introduction to Linear Optimization. 1998.   \n[8] Dimitris Bertsimas, Vivek F. Farias, and Nikolaos Trichakis. The price of fairness. Operational Research, 59:17\u201331, 2011. [9] Omar Besbes, Yash Kanoria, and Akshit Kumar. The fault in our recommendations: On the perils of optimizing the measurable, 2024. URL https://arxiv.org/abs/2405.03948.   \n[10] Robin Burke. Multisided fairness for recommendation, 2017. URL https://arxiv.org/ abs/1707.00093.   \n[11] Robin Burke, Nasim Sonboli, and Aldo Ordonez-Gauger. Balanced neighborhoods for multi-sided fairness in recommendation. In Sorelle A. Friedler and Christo Wilson, editors, Proceedings of the 1st Conference on Fairness, Accountability and Transparency, volume 81 of Proceedings of Machine Learning Research, pages 202\u2013214. PMLR, 23\u201324 Feb 2018. URL https://proceedings.mlr.press/v81/burke18a.html.   \n[12] Qinyi Chen, Jason Liang, Negin Golrezaei, and Djallel Bouneffouf, 2024. URL https: //arxiv.org/abs/2306.10050.   \n[13] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S. Weld. SPECTER: Document-level representation learning using citation-informed transformers. In ACL, 2020.   \n[14] Jessica Dai, Bailey Flanigan, Meena Jagadeesan, Nika Haghtalab, and Chara Podimata. Can probabilistic feedback drive user impacts in online platforms? In International Conference on Artificial Intelligence and Statistics, pages 2512\u20132520. PMLR, 2024.   \n[15] Sarah Dean, Evan Dong, Meena Jagadeesan, and Liu Leqi. Accounting for AI and users shaping one another: The role of mathematical models, 2024. URL https://arxiv.org/abs/2404. 12366. ", "page_idx": 10}, {"type": "text", "text": "[16] Paresha Farastu, Nicholas Mattei, and Robin Burke. Who pays? personalization, bossiness and the cost of fairness, 2022. URL https://arxiv.org/abs/2209.04043. ", "page_idx": 11}, {"type": "text", "text": "[17] Naveen Garg, Telikepalli Kavitha, Amit Kumar, Kurt Mehlhorn, and Juli\u00e1n Mestre. Assigning papers to referees. Algorithmica, 58(1):119\u2013136, September 2010. ISSN 0178-4617. ", "page_idx": 11}, {"type": "text", "text": "[18] Wenshuo Guo, Karl Krauth, Michael Jordan, and Nikhil Garg. The stereotyping problem in collaboratively filtered recommender systems. In Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, pages 1\u201310, 2021. ", "page_idx": 11}, {"type": "text", "text": "[19] Andreas Haupt, Dylan Hadfield-Menell, and Chara Podimata. Recommending to strategic users, 2023. URL https://arxiv.org/abs/2302.06559. ", "page_idx": 11}, {"type": "text", "text": "[20] Daniel Huttenlocher, Hannah Li, Liang Lyu, Asuman Ozdaglar, and James Siderius. Matching of users and creators in two-sided markets with departures, 2023. URL https://arxiv.org/ abs/2401.00313. ", "page_idx": 11}, {"type": "text", "text": "[21] Meena Jagadeesan, Nikhil Garg, and Jacob Steinhardt. Supply-side equilibria in recommender systems. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 11}, {"type": "text", "text": "[22] Mariusz Kaleta. Price of fairness on networked auctions. J. Appl. Math., 2014:860747:1\u2013 860747:7, 2014. URL https://api.semanticscholar.org/CorpusID:38538799. ", "page_idx": 11}, {"type": "text", "text": "[23] Rodney Michael Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David W. Graham, F.Q. Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler Murray, Christopher Newell, Smita R Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, A. Tanaka, Alex D Wade, Linda M. Wagner, Lucy Lu Wang, Christopher Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine van Zuylen, and Daniel S. Weld. The Semantic Scholar Open Data platform. ArXiv, abs/2301.10140, 2023. URL https://api.semanticscholar. org/CorpusID:256194545. ", "page_idx": 11}, {"type": "text", "text": "[24] Jon Kleinberg and Michela Meister. Modeling amplification in a network of speakers and listeners. Private correspondence, 2024. ", "page_idx": 11}, {"type": "text", "text": "[25] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. The challenge of understanding what users want: Inconsistent preferences and engagement optimization. Management Science, 2023. ", "page_idx": 11}, {"type": "text", "text": "[26] Jure Leskovec, Anand Rajaraman, and Jeffrey David Ullman. Mining of Massive Datasets. Cambridge University Press, 2011. ", "page_idx": 11}, {"type": "text", "text": "[27] David Liu, Jackie Baek, and Tina Eliassi-Rad. When collaborative filtering is not collaborative: Unfairness of PCA for recommendations, 2023. URL https://arxiv.org/abs/2310. 09687. ", "page_idx": 11}, {"type": "text", "text": "[28] Lydia T Liu, Nikhil Garg, and Christian Borgs. Strategic ranking. In International Conference on Artificial Intelligence and Statistics, pages 2489\u20132518. PMLR, 2022. ", "page_idx": 11}, {"type": "text", "text": "[29] Weiwen Liu and Robin Burke. Personalizing fairness-aware re-ranking. In FATRec Workshop on Responsible Recommendation, page to appear, 2018. ", "page_idx": 11}, {"type": "text", "text": "[30] Zhi Liu and Nikhil Garg. Redesigning service level agreements: Equity and efficiency in city government operations. In ACM Conference on Economics and Computation, 2024. ", "page_idx": 11}, {"type": "text", "text": "[31] Thomas Ma, Michael S Bernstein, Ramesh Johari, and Nikhil Garg. Balancing producer fairness and efficiency via Bayesian rating system design. In International AAAI Conference on Web and Social Media, ICWSM \u201825, 2025. ", "page_idx": 11}, {"type": "text", "text": "[32] Vahideh Manshadi, Scott Rodilitz, Daniela Saban, and Akshaya Suresh. Redesigning VolunteerMatch\u2019s ranking algorithm: Toward more equitable access to volunteers. Available at SSRN 4497747, 2023. ", "page_idx": 11}, {"type": "text", "text": "[33] Rishabh Mehrotra, James McInerney, Hugues Bouchard, Mounia Lalmas, and Fernando Diaz. Towards a fair marketplace: Counterfactual evaluation of the trade-off between relevance, fairness & satisfaction in recommendation systems. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM \u201918, page 2243\u20132251, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450360142. doi: 10.1145/3269206.3272027. URL https://doi.org/10.1145/3269206.3272027. ", "page_idx": 12}, {"type": "text", "text": "[34] Smitha Milli, Emma Pierson, and Nikhil Garg. Choosing the right weights: Balancing value, strategy, and noise in recommender systems, 2023. URL https://arxiv.org/abs/2305. 17428. ", "page_idx": 12}, {"type": "text", "text": "[35] Herv\u00e9 Moulin. Fair Division and Collective Welfare. MIT Press, 2003. ", "page_idx": 12}, {"type": "text", "text": "[36] Brendan O\u2019Donoghue, Eric Chu, Neal Parikh, and Stephen Boyd. Conic optimization via operator splitting and homogeneous self-dual embedding. Journal of Optimization Theory and Applications, 169(3):1042\u20131068, June 2016. URL http://stanford.edu/\\~boyd/papers/ scs.html. ", "page_idx": 12}, {"type": "text", "text": "[37] Gourab K Patro, Arpita Biswas, Niloy Ganguly, Krishna P. Gummadi, and Abhijnan Chakraborty. FairRec: Two-sided fairness for personalized recommendations in two-sided platforms. In Proceedings of The Web Conference 2020, WWW \u201920, page 1194\u20131204, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450370233. doi: 10.1145/3366423. 3380196. URL https://doi.org/10.1145/3366423.3380196. ", "page_idx": 12}, {"type": "text", "text": "[38] Gourab K Patro, Lorenzo Porcaro, Laura Mitchell, Qiuyue Zhang, Meike Zehlike, and Nikhil Garg. Fair ranking: A critical review, challenges, and future directions. In Proceedings of the 2022 ACM conference on fairness, accountability, and transparency, pages 1929\u20131942, 2022. doi: 10.1145/3531146.353323. URL https://doi.org/10.1145/3531146.353323. ", "page_idx": 12}, {"type": "text", "text": "[39] Kenny Peng, Manish Raghavan, Emma Pierson, Jon Kleinberg, and Nikhil Garg. Reconciling the accuracy-diversity trade-off in recommendations. In Proceedings of the ACM on Web Conference 2024, WWW \u201924, page 1318\u20131329, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400701719. doi: 10.1145/3589334.3645625. URL https://doi.org/10.1145/3589334.3645625. ", "page_idx": 12}, {"type": "text", "text": "[40] Hossein A. Rahmani, Yashar Deldjoo, Ali Tourani, and Mohammadmehdi Naghiaei. The unfairness of active users and popularity bias in point-of-interest recommendation. In International Workshop on Algorithmic Bias in Search and Recommendation, 2022. doi: 10. 1007/978-3-031-09316-6_6. URL https://doi.org/10.1007/978-3-031-09316-6_6. ", "page_idx": 12}, {"type": "text", "text": "[41] John Rawls. A Theory of Justice: Original Edition. Harvard University Press, 1971. ISBN 9780674880108. URL http://www.jstor.org/stable/j.ctvjf9z6v. ", "page_idx": 12}, {"type": "text", "text": "[42] Ashudeep Singh and Thorsten Joachims. Fairness of exposure in rankings. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD \u201918, page 2219\u20132228, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450355520. doi: 10.1145/3219819.3220088. URL https://doi.org/10.1145/ 3219819.3220088. ", "page_idx": 12}, {"type": "text", "text": "[43] Ivan Stelmakh, Nihar B. Shah, and Aarti Singh. Peerreview4all: Fair and accurate reviewer assignment in peer review. In Aur\u00e9lien Garivier and Satyen Kale, editors, Proceedings of the 30th International Conference on Algorithmic Learning Theory, volume 98 of Proceedings of Machine Learning Research, pages 828\u2013856. PMLR, 22\u201324 Mar 2019. URL https:// proceedings.mlr.press/v98/stelmakh19a.html. ", "page_idx": 12}, {"type": "text", "text": "[44] Zhongzheng Tang, Chenhao Wang, and Mengqi Zhang. Price of fairness in budget division for egalitarian social welfare. In International Conference on Combinatorial Optimization and Applications, volume 12577 of Lecture Notes in Computer Science, pages 594\u2013607. Springer, 2020. doi: 10.1007/978-3-030-64843-5_40. URL https://doi.org/10.1007/ 978-3-030-64843-5_40. ", "page_idx": 12}, {"type": "text", "text": "[45] Lequn Wang and Thorsten Joachims. User fairness, item fairness, and diversity for rankings in two-sided markets. In Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval, ICTIR \u201921, page 23\u201341, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450386111. doi: 10.1145/3471158.3472260. URL https://doi.org/10.1145/3471158.3472260.   \n[46] Lequn Wang, Yiwei Bai, Wen Sun, and Thorsten Joachims. Fairness of exposure in stochastic bandits. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 10686\u201310696. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/ v139/wang21b.html.   \n[47] Yong Xiao, Jianwei Huang, Chau Yuen, and Luiz A. Dasilva. Fairness and efficiency tradeoffs for user cooperation in distributed wireless networks. 2013 Proceedings IEEE INFOCOM, pages 285\u2013289, 2013. doi: 10.1109/INFCOM.2013.6566780. URL https://ieeexplore. ieee.org/document/6566780.   \n[48] Chen Xu, Sirui Chen, Jun Xu, Weiran Shen, Xiao Zhang, Gang Wang, and Zhenhua Dong. P-MMF: Provider max-min fairness re-ranking in recommender system. In Proceedings of the ACM Web Conference 2023, WWW \u201923, page 3701\u20133711, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450394161. doi: 10.1145/3543507.3583296. URL https://doi.org/10.1145/3543507.3583296.   \n[49] Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Megahed, and Ricardo Baeza-Yates. FA\\*IR: A fair top-k ranking algorithm. In Proceedings of the 2017 ACM Conference on Information and Knowledge Management, CIKM \u201917, page 1569\u20131578, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450349185. doi: 10.1145/3132847.3132938. URL https://doi.org/10.1145/3132847.3132938. ", "page_idx": 13}, {"type": "text", "text": "A Extensions", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Alternative definitions of fairness ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Nash Welfare The first alternative definition of fairness we consider is Nash welfare [35], ", "page_idx": 14}, {"type": "equation", "text": "$$\nU_{\\mathrm{NW}}(\\boldsymbol{\\rho})=\\sum_{i}\\log U_{i}(\\boldsymbol{\\rho}),\\quad I_{\\mathrm{NW}}(\\boldsymbol{\\rho})=\\sum_{j}\\log I_{j}(\\boldsymbol{\\rho}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This measure of fairness is more holistic than egalitarian fairness as it accounts for the utilities of all users (items). In Figure 2, we show the results of repeating the experiment in Figure 1 with Nash Welfare fairness. We see that the trade-off between user and item fairness is steeper for homogeneous populations of users than for uniformly random populations. While the curves appear more concave than before, it is important to notice that we replaced $\\gamma$ with $1/\\gamma$ in the item fairness constraint of the optimization problem due to the Nash welfare being negative (as detailed in Figure 2), so that $\\gamma-$ while still capturing constraint strength \u2013 has a slightly different interpretation than previously. When this $\\gamma$ is nearly 1, we do see an increase in the price of mis-estimation. ", "page_idx": 14}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/d98a4fb7711fbde79a45804d9dff7301dd0ae27dd28075911fb0d30f7c22fc94.jpg", "img_caption": ["(a) Homogeneous versus diverse users "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/af13dd9565fc5e318bfeaef3bda8e6908033dfd04847d1e89d31fb383ec458a3.jpg", "img_caption": ["(b) With and without misestimation "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 2: We repeat the experiment of Figure 1 from the original paper but replace max-min fairness with Nash welfare fairness. That is, in the objective we replace $U_{\\mathrm{min}}$ with the user Nash welfare $U_{\\mathrm{NW}}$ . We must be more careful with the item fairness constraint: we know that the normalized utilities satisfy $0\\leq U_{i},I_{j}\\leq1$ , so $U_{\\mathrm{NW}},I_{\\mathrm{NW}}<0$ . This means that in Problem 1 we must replace the item fairness constraint $I_{\\mathrm{min}}\\left(\\rho\\right)\\ge\\gamma I_{\\mathrm{min}}^{*}$ with the constraint $I_{\\mathrm{NW}}(\\rho)\\,\\geq\\,(1/\\gamma)I_{\\mathrm{NW}}^{*}$ . When $\\gamma=0$ , this corresponds to $I_{\\mathrm{NW}}(\\rho)\\ge-\\infty$ ; when $\\gamma=1$ , this corresponds to $I_{\\mathrm{min}}\\left(\\rho\\right)\\ge I_{\\mathrm{min}}^{*}$ . Thus as before, when $\\gamma=0$ there is effectively no item fairness constraint, and $\\gamma=1$ constrains item fairness to be maximal. ", "page_idx": 14}, {"type": "text", "text": "Sum of $k{\\bf-m i n}$ The second alternative definition of fairness we consider is the sum of the $k$ - minimum user or item utilities, which is a generalization of egalitarian fairness $\\;k=1$ ) that measures the utility of a size- ${\\cdot k}$ set of worst-off entities, ", "page_idx": 14}, {"type": "equation", "text": "$$\nU_{k-\\operatorname*{min}}(\\rho)=\\operatorname*{min}_{i_{1}\\neq\\ldots\\neq i_{k}}\\sum_{\\ell=1}^{k}U_{i_{\\ell}}(\\rho),\\quad I_{k-\\operatorname*{min}}(\\rho)=\\operatorname*{min}_{j_{1}\\neq\\ldots\\neq j_{k}}\\sum_{\\ell=1}^{k}I_{j_{\\ell}}(\\rho).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In Figure 3, we show the results of repeating the experiment in Figure 1 with max-sum- $k$ -min fairness. We again observe that the trade-off between user and item fairness is steeper for homogeneous populations of users than for uniformly random populations. At already very high levels of item fairness \u2013 between $\\gamma\\approx0.9$ and $\\gamma=1$ \u2013 there appears to be an increase in the price of misestimation. ", "page_idx": 14}, {"type": "text", "text": "A.2 Alternative item utility models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In our theoretical and empirical results, we use a symmetric utility model, where an item\u2019s utility for being recommended to a user is the same as the user\u2019s utility for the recommendation. Formally, in general item $j$ has utility $w_{i j}^{I}$ for being recommended to user $i$ , while user $i$ may have a different $w_{i j}^{U}$ .f oBr etlhoawt ,r ewceo dmismceunsds atthieo nr.a tIino noaulre t fhoero trheitisc aml oadnedl , eamnpdi rsihcoalw r tehsautl ttsh iasb aosvseu, mwpet itoono k $w_{i j}=$ $w_{i j}^{I}=w_{i j}^{\\bar{U}}$   \nrelaxed in our theoretical and empirical results. ", "page_idx": 14}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/d52987f68030190dc93215bd85bd1f6b440ca1becbe9dd0d9c58deba11e7b3ab.jpg", "img_caption": ["(a) Homogeneous versus diverse users "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/b72854b96cfbcb5b52be004a65d9c851a2109da20b00192a481e21dcf939e6c5.jpg", "img_caption": ["(b) With and without misestimation "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 3: We repeat the experiment of Figure 1 from the original paper but replace max-min fairness with max-sum- $k$ -min fairness for $k=3$ . In the optimization in Problem 1, we replace $U_{\\mathrm{min}}$ and $I_{\\mathrm{min}}$ with $U_{k-\\mathrm{min}}$ and $I_{k-\\mathrm{min}}$ respectively. ", "page_idx": 15}, {"type": "text", "text": "The symmetric utility model we use \u2013 which corresponds to the \u201cmarket share\u201d item utility model in Chen et al. [12] \u2013 is motivated by platforms in which both users and items derive utility from a successful recommendation. This is reasonable in settings in which not only does a user want to be recommended relevant items, but an item\u2019s producer wants it to be recommended to users for which the item is especially relevant. Concretely, in the case of readers receiving recommendations for academic pre-prints, the readers prefer papers that they will engage with, and authors prefer readers that will engage with their work. in an online marketplace producers want customers who will purchase their product to be shown the item; in a social media setting, content creators prefer their content to appear to users who will appreciate it and thus engage with future content. Our symmetric utility model captures this basic structure behind producer preferences in many cases. ", "page_idx": 15}, {"type": "text", "text": "Note that since users can only receive a limited number of recommendations, our assumption does not eliminate the tension between user and item utility: an individual user wants recommendations that maximize her total utility across all items, while an individual item wants the platform to produce recommendations that maximize its total utility across all users. A concrete example of this is that under this model a low-quality item will want to be recommended to the user most likely to click on it, but that user won\u2019t want to be recommended the low-quality item. This assumption does, however, imply that the platform-wide item utility and platform-wide user utility are equivalent \u2013 for recommendations $\\rho_{i j}$ , these are both $\\textstyle\\sum_{j}\\sum_{i}w_{i j}{\\bar{\\rho}}_{i j}$ . ", "page_idx": 15}, {"type": "text", "text": "One generalization of the symmetric utility model is where each user and item receives utility proportional to some shared recommendation quality $w_{i j}$ . Formally, each user $i$ receives a utility of $a_{i}w_{i j}$ and each item $j$ receives a utility of $b_{j}w_{i j}$ from recommending $j$ to $i$ , for $a_{i},b_{j}>0$ . For example, different values of $a_{i}$ could capture different levels of baseline interest in items among users. Our theoretical results still hold in this more general setting, since these coefficients will cancel out when we normalize the utilities. ", "page_idx": 15}, {"type": "text", "text": "Another common item utility model is exposure [11, 37], where items receive utility only from being recommended, and are ambivalent to which user it is shown to. Formally, this corresponds to taking $w_{i j}^{I}=1$ for all $i,j$ .3 Intuitively, with exposure-based item utility, the item fairness constraint will cause the users\u2019 recommendation policies move from being concentrated on the most popular items, to a more uniform distribution over items. If the users in the population have diverse preferences, each item will already have an approximately uniform probability of being recommended, so that imposing item fairness constraints has a low cost. ", "page_idx": 15}, {"type": "text", "text": "In Figures 4 and 5 we examine the robustness of our empirical results in 3a and 3b respectively as we interpolate the item utility model between symmetry and exposure. In Figure 5 we see that user preference diversity indeed still improves the user-item fairness tradeoff when we change the item utility model. We also again see that the price of mis-estimation does not appear to increase with item fairness. ", "page_idx": 15}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/5a51354fe5c5a380df576c16700cdea305e4f12208a75b3e5860ba3ee6d1f57f.jpg", "img_caption": ["Figure 4: Robustness of empirical findings without symmetry assumption: user-item fairness tradeoffs and diversity as item utilities become less correlated with user utilities. Here, we take the user utilities $w^{U}$ to be derived from the arXiv recommendation engine similarity scores as in Figure 3a. For each plot the item utilities are a linear interpolation between the users\u2019 utilities and exposure. Formally, $w_{i j}^{I}=\\Delta\\cdot1+(1-\\Delta)\\cdot w_{i j}^{U}$ . When $\\Delta=0$ , item and user utilities agree; when $\\Delta=1$ items derive utility solely from exposure. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/abce32209697598be5217a0c29ede4bfeab1b11d149ba39ffa91a9f29905bb4f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 5: Robustness of empirical findings without symmetry assumption: item fairness constraints still do not increase the price of mis-estimation empirically. Here, we take the user utilities $w^{U}$ to be derived from the arXiv recommendation engine similarity scores as in Figure 3b. For each plot the item utilities are a linear interpolation between the users\u2019 utilities and exposure. Formally, wiIj = \u2206\u00b7 1 + (1 \u2212\u2206) \u00b7 wiUj. When $\\Delta=0$ , item and user utilities agree; when $\\Delta=1$ items derive utility solely from exposure. ", "page_idx": 16}, {"type": "text", "text": "B arXiv recommender empirical details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We prototype a recommender for preprints on arXiv, to illustrate our conceptual findings. We consider the cold start setting for items (papers), when they are newly uploaded to arXiv and so only have metadata but no associated interaction or citation data. For users (readers), we use as data the papers that they have published on arXiv in the past; we assume authors with identical names are the same author. We implement various natural language processing-based methods on the abstract text (for both items and the user\u2019s historical papers) to generate similarity scores (utility matrices) for users and items. We use the similarity scores to generate recommendations for each user, at various levels of user and item fairness constraints. We validate our approach by analyzing how citations correlate with the similarity score. ", "page_idx": 16}, {"type": "text", "text": "B.1 Dataset and computation details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The original dataset was sourced from the public ArXiv Dataset available on Kaggle,4 containing 1,796,911 articles. This dataset covers a wide range of scientific categories. Each entry in the dataset is characterized by features which include: ", "page_idx": 16}, {"type": "text", "text": "\u2022 ID: A unique identifier for each entry.   \n\u2022 Authors: Names and affiliations of the authors.   \n\u2022 Title: The title of the paper.   \n\u2022 Categories: The scientific categories for the paper.   \n\u2022 Abstract: A brief summary of the paper.   \n\u2022 Update Date: The date of the latest update. ", "page_idx": 16}, {"type": "text", "text": "We further join this data with citation data from the Semantic Scholar API [23].5 For each paper, we have both the papers that it cites and the papers that cite it. ", "page_idx": 17}, {"type": "text", "text": "We focus exclusively on entries classified under the \u2018Computer Science (CS)\u2019 category. We extract entries where the primary category designation was \u2018CS\u2019, and remove null and duplicate paper ID values. This filtering results in 177,323 entries. ", "page_idx": 17}, {"type": "text", "text": "This entire empirical workflow was run on a machine with 64 CPUs, 1 TB RAM, and 14 TB (nonSSD) disk. The estimated time was about 20 hours per week for 3 months, and the longest individual run was approximately 12 hours. ", "page_idx": 17}, {"type": "text", "text": "Train and test data As training (to construct embeddings for users), we consider papers that those users published up to the end of 2019. Papers in 2020 are in the test set (the set available to be recommended). Papers after 2020 are used to evaluate recommendations (did the user cite a paper published in 2020). ", "page_idx": 17}, {"type": "text", "text": "The training dataset has 139,308 papers, and the test dataset has 14,307 papers, with the remaining being post-2020 papers. The training dataset has 178,260 distinct users with an average of approximately 2 papers per user. Figures 6a and 6b show the distribution of research paper subcategories between the train and test datasets. Figure 7 below shows the distribution of the number of papers per user in the training set on a logarithmic scale for the y-axis. The $\\mathbf{X}$ -axis represents the number of papers per user, ranging from 0 to over 250. ", "page_idx": 17}, {"type": "text", "text": "Splitting the training and test data temporally both reflects practice and allows us to evaluate the recommendation model\u2019s effectiveness in predicting future citations. As detailed below, the model\u2019s success was measured by whether papers our model would have recommended to users in 2020 were in fact cited in their subsequent works. For further details, see below Section B.3 on the model evaluation. ", "page_idx": 17}, {"type": "text", "text": "B.2 Recommendation models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Each recommendation approach has two design dimensions: (a) how we generate embeddings for each paper (papers to be recommended, and papers uploaded by users that will be used to construct user embeddings), and (b) how similarity scores are constructed once we have an embedding for each paper. Below, we evaluate each approach by their effectiveness in recommending papers that users are likely to cite in their future works. ", "page_idx": 17}, {"type": "text", "text": "(a) Generating embeddings for each paper We use text-based analyses on the abstracts of each paper to generate paper embeddings. The first approach is TF-IDF, while the second employs Sentence Transformers. ", "page_idx": 17}, {"type": "text", "text": "TF-IDF. The first preprocessing step involved removing stopwords\u2014common words with little informational value, such as \u201cand\u201d and \u201cthe\u201d\u2014to reduce noise and emphasize meaningful content. Next, a TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer is applied to count term frequencies and scale them according to their rarity across the dataset, highlighting unique and informative words. The resulting frequency vector for each abstract is used as its embedding. ", "page_idx": 17}, {"type": "text", "text": "Sentence Transformers. The author-based recommendation model using Sentence Transformers leverages contextual embeddings from sentence-level representations. The preprocessing involves tokenizing the text (title, abstract, and categories) and generating embeddings using a pre-trained Sentence Transformer model, specifically the AllenAI SPECTER model [13],6 available on Hugging Face. ", "page_idx": 17}, {"type": "text", "text": "(b) Constructing similarity scores for each user-paper pair. After constructing embeddings, we have an embedding for each paper in the potential recommendation set, and for each paper authored by a user in the training set. We construct user-paper similarity scores as follows. First, we compute the cosine similarity between each recommendation set embedding and each user\u2019s papers\u2019 embeddings. Then, the similarity score between each user and the recommendations set paper is constructed using one of the following approaches: ", "page_idx": 17}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/ccd22bcedbb0768c5e7316ebc3b1b91f1fe0204a91df6e4b6aaf8f71fd48fc52.jpg", "img_caption": ["(a) Distribution of research paper publications in the train dataset over time "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/90d64b5787b2c7f45b36e59d7fc7e93d365e831793105fecbeb0bead933ec1ef.jpg", "img_caption": ["(b) Distribution of research paper publications in the test dataset over time ", "Figure 6: Distribution of research paper publications over time "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Mean score. We take the mean of the cosine similarities between the recommendation set paper and each of the papers by the user in the training set. This approach is equivalent to constructing a user embedding as the mean embedding of their uploaded papers. ", "page_idx": 18}, {"type": "text", "text": "Max score. The mean similarity score has been recognized as not capturing diverse interests that a user may have [18, 39] \u2013 for example, for a user who has published papers in two different subject areas, a paper should be recommended to them if it matches either of their interests, as opposed to the average of those interests. Thus, we also construct user-paper similarity scores via the max similarity between any of the user\u2019s training set papers, and the recommended set paper. ", "page_idx": 18}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/e171850a2978b2145ce67b9ac7bf2323182f25aed86c275c235752126881e39d.jpg", "img_caption": ["Figure 7: Distribution of the number of papers per user in the training set on a logarithmic scale "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Note that we also experimented with using dot products instead of cosine similarity; results are similar and omitted. ", "page_idx": 19}, {"type": "text", "text": "B.3 Model evaluation method ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We evaluate each of the four approaches (TF-IDF and sentence transformers, each with the max or the mean scores) using citation data for 1,128 users (authors) and 14,307 papers. For each user-paper pair, we use the following as outcome data: ", "page_idx": 19}, {"type": "text", "text": "User cites paper in the future: Is the paper cited by the user in the future? ", "page_idx": 19}, {"type": "text", "text": "Paper cites user: Does the paper cite the user already? We note that this data is technically available at the time of a hypothetical recommendation for a new paper, and so in theory could be used by a recommender. Thus, although metric does not directly measure the future behavior of the user, it helps in understanding the contextual alignment (determined just by natural language processing of the abstracts) of the recommended papers with the user\u2019s previous research or interests. Importantly, these references are not part of the data used to generate the similarity score. ", "page_idx": 19}, {"type": "text", "text": "In both cases, we consider the presence of citations as signifying a good recommendation. ", "page_idx": 19}, {"type": "text", "text": "For each recommendation approach, we calculate the relationship between this citation data and the following similarity score measures. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Similarity score: The raw cosine similarity score.   \n\u2022 Score percentile: The percentile rank of the similarity score, where percentile is calculated for each user.   \n\u2022 Normalized score: The similarity score is normalized by subtracting the mean and dividing by the standard deviation of scores for each user. ", "page_idx": 19}, {"type": "text", "text": "A successful recommendation approach would have a strong relationship between text-based similarity scores and future citation outcomes. ", "page_idx": 19}, {"type": "text", "text": "B.4 Evaluation results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We evaluate each of the 4 approaches. As summarized below, we find that the best performing approach is using the TF-IDF vectorizer to construct abstract embeddings, and then using the max similarity scores between any of their papers in the training set and each paper in the recommended set. We find that this approach is effective at predicting future citations by the user, especially for a cold start recommender that uses only abstract textual information, and for such a sparse outcome as citations. ", "page_idx": 19}, {"type": "table", "img_path": "ZOZjMs3JTs/tmp/4d742a776f7bcdc08b94aa297f4aba6183984dab26abaf8d14c9a92c47f7c801.jpg", "table_caption": ["Table 2: Average similarity measures in the Max score, TF-IDF model, conditioned on citation presence for different types of citations. "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/f9195c8ec2dbe7063e63f98f9aec67d17deaaf15b852faa6f4ad4dd92b03211c.jpg", "img_caption": ["(a) Density plot of similarity scores grouped by citation presence. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/5fae23d8a658308c737b02f250ebe0e05efa4709e734593bd5d8191a4b7044ce.jpg", "img_caption": ["(b) Density plot of similarity score percentiles grouped by citation presence. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 8: $\\mathrm{Pr}$ (score|User cites paper), the distribution of the score for a user-paper pair, conditional on whether the user cites the paper in the future, for the Max score, TF-IDF model. ", "page_idx": 20}, {"type": "table", "img_path": "ZOZjMs3JTs/tmp/33bd6939803b04fd73a0430fcec2c81e14c222c299df922472d22fee3fda5fd0.jpg", "table_caption": [], "table_footnote": ["Table 3: Logistic regression results for predicting whether user $i$ cites paper $j$ from the similarity score $w_{i j}$ , for the Max score, TF-IDF model "], "page_idx": 20}, {"type": "text", "text": "Max score, TF-IDF Table 2 shows the similarity measures described above averaged over all users and test papers, conditioned on whether or not a citation occurred between the user and paper (whether the user cites the paper, or vice versa). Figure 8 illustrates the distribution of similarity scores for papers that were cited (orange) versus those that were not cited (blue) by the user in the future. The density for cited papers is higher at higher levels of similarity scores compared to non-cited papers. Table 3 performing logistic regression between the user-paper score and whether the user cites the paper with a bias term (\u2018User cites paper $\\sim1\\!+\\!\\operatorname{score}^{\\prime})$ ), for each score measure. All measures suggest that our text-based recommendation scores are effective for predicting whether the user will cite the given paper. For example, Figure 8b shows that $\\mathrm{Pr}$ (Highest score bin|Author cites paper) is more than five times $\\mathrm{Pr}$ (Highest score bin|Author does not cite paper). ", "page_idx": 20}, {"type": "table", "img_path": "ZOZjMs3JTs/tmp/c41ae59c840579e0116e39bbabf6d1406d929e1fe2b053e499082773174b2891.jpg", "table_caption": ["Citation Type No/Yes Similarity Score Score Percentile Normalized Score "], "table_footnote": ["Table 4: Average similarity measures in the Mean score, TF-IDF model, conditioned on citation presence for different types of citations. "], "page_idx": 20}, {"type": "text", "text": "Mean score, TF-IDF Table 4 shows the similarity measures described above averaged over all users and test papers, conditioned on whether or not a citation occurred between the user and paper (whether the author cites the paper, or vice versa). Figure 9 illustrates the distribution of similarity scores for papers that were cited (orange) versus those that were not cited (blue) by the user in ", "page_idx": 20}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/8845a352c44565b363f628eae4fc53ec0e206876b4e5fcd3d4da46c7edd668d5.jpg", "img_caption": ["(a) Density plot of similarity scores grouped by citation presence. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/8b31b05608a51b6495135700086a290db74a8fde2d8ffaadb6c9a049a28292f2.jpg", "img_caption": ["(b) Density plot of similarity score percentiles grouped by citation presence. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 9: $\\mathrm{Pr}$ (score|Author cites paper), the distribution of the score for a user-paper pair, conditional on whether the user cites the paper in the future, for the Mean score, TF-IDF model. ", "page_idx": 21}, {"type": "table", "img_path": "ZOZjMs3JTs/tmp/1cc73adecea419b4d7663e94c83d85ff91f9505ab27399ccfbe1e05e0d4cd970.jpg", "table_caption": [], "table_footnote": ["Table 5: Logistic regression results for predicting whether user $i$ cites paper $j$ from the similarity score $w_{i j}$ \u201e for the Mean score, TF-IDF model "], "page_idx": 21}, {"type": "text", "text": "the future. The density for cited papers is higher at higher levels of similarity scores compared to non-cited papers. Table 5 performing logistic regression between the user-paper score and whether the user cites the paper with a bias term (\u2018User cites paper $\\sim1+$ score\u2019), for each score measure. ", "page_idx": 21}, {"type": "table", "img_path": "ZOZjMs3JTs/tmp/d19a2a83e1ff9f74a2d9c837bfaf81c91c04d1290f3ca820e8a45899d7b98efa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 6: Average similarity measures in the Max score, Sentence transformer model, conditioned on citation presence for different types of citations. ", "page_idx": 21}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/af430f090d0126fe2c083d36d7dd299b66edb79c102bf7ecc1ca66b2fe1f9878.jpg", "img_caption": ["(a) Density plot of similarity scores grouped by citation presence. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/37bb1ce3cb649d865718e8244ec064f054cd71c3df2cdaba0510745de7cfa6a9.jpg", "img_caption": ["(b) Density plot of similarity score percentiles grouped by citation presence. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 10: $\\mathrm{Pr}$ (score|Author cites paper), the distribution of the score for a user-paper pair, conditional on whether the user cites the paper in the future, for the Max score, Sentence transformer model. ", "page_idx": 21}, {"type": "table", "img_path": "ZOZjMs3JTs/tmp/cb097a8aa7596586119baeaac21d3f69356ed9be0e66dd10cdf6f61e9422cfe3.jpg", "table_caption": [], "table_footnote": ["Table 7: Logistic regression results for predicting whether user $i$ cites paper $j$ from the similarity score $w_{i j}$ \u201e for the Max score, Sentence transformer model "], "page_idx": 22}, {"type": "text", "text": "Max score, Sentence transformer Table 6 shows the similarity measures described above averaged over all users and test papers, conditioned on whether or not a citation occurred between the user and paper (whether the user cites the paper, or vice versa). Figure 10 illustrates the distribution of similarity scores for papers that were cited (orange) versus those that were not cited (blue) by the user in the future. The density for cited papers is higher at higher levels of similarity scores compared to non-cited papers. Table 7 shows the results of performing logistic regression between the user-paper score and whether the user cites the paper with a bias term (\u2018User cites paper $\\sim1+$ score\u2019), for each score measure. ", "page_idx": 22}, {"type": "table", "img_path": "ZOZjMs3JTs/tmp/fbdf4d2ab37e0301ecf0862c2c062d1a04df99accef93881e4b696d533f75de5.jpg", "table_caption": ["Table 8: Average similarity measures in the Mean score, Sentence transformer model, conditioned on citation presence for different types of citations. "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/90e21bb9da74bd25a63458dccdde9f72f70be8ef31493d5cd6e3b790955928f4.jpg", "img_caption": ["(a) Density plot of similarity scores grouped by citation presence. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "ZOZjMs3JTs/tmp/b315897e37e25030b9a660a2f1d1125325f3c549aad15a8a4de5d63396e32f9f.jpg", "img_caption": ["(b) Density plot of similarity score percentiles grouped by citation presence. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 11: $\\mathrm{Pr}$ (score|User cites paper), the distribution of the score for a user-paper pair, conditional on whether the user cites the paper in the future, for the Mean score, Sentence transformer model. ", "page_idx": 22}, {"type": "table", "img_path": "ZOZjMs3JTs/tmp/f3bd75880330cf1af21bf103dad4bd1456c9fb5b1b3e25155da12064a8e487aa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 9: Logistic regression results for predicting whether user $i$ cites paper $j$ from the similarity score $w_{i j}$ \u201e for the Mean score, Sentence transformer model ", "page_idx": 22}, {"type": "text", "text": "Mean score, Sentence transformer Table 8 shows the similarity measures described above averaged over all users and test papers, conditioned on whether or not a citation occurred between the user and paper (whether the user cites the paper, or vice versa). Figure 11 illustrates the distribution of similarity scores for papers that were cited (orange) versus those that were not cited (blue) by the user in the future. The density for cited papers is higher at higher levels of similarity scores compared to non-cited papers. Table 9 shows the results of performing logistic regression between the user-paper score and whether the user cites the paper with a bias term (\u2018User cites paper $\\sim1+\\mathrm{score}^{\\circ}$ ), for each score measure. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Altogether, each model performs fairly well; the model using sentence transformer embeddings and the max similarity score among each author\u2019s papers appears to perform the best overall. ", "page_idx": 23}, {"type": "text", "text": "C Proof of Section 3 results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we provide complete proofs for Proposition 1 and Proposition 2. In this and all other proofs in the appendix, we ignore the dependence on the utility matrix $w$ in the user and item utility and fairness functions when clear. ", "page_idx": 23}, {"type": "text", "text": "The following result will be broadly useful throughout the rest of the proofs. ", "page_idx": 23}, {"type": "text", "text": "Lemma 1. Since $w_{i j}>0$ for all $j$ , $I_{\\mathrm{min}}^{*}>0$ . ", "page_idx": 23}, {"type": "text", "text": "First, recall the statement of Proposition 1. ", "page_idx": 23}, {"type": "text", "text": "Proposition 1. Suppose that for a set of recommendation policies $S\\subseteq\\Delta_{n-1}^{m}$ , ", "page_idx": 23}, {"type": "text", "text": "(i) $\\boldsymbol{S}$ can be described by a finite set of linear constraints (ii) There exists an optimal solution $\\rho^{*}$ to Problem (2) such that $\\rho^{*}\\in S$ (iii) $\\rho^{*}$ is the unique feasible solution to Problem (2) in $\\boldsymbol{S}$ ", "page_idx": 23}, {"type": "text", "text": "Then, finding an optimal solution $\\rho^{*}$ to Problem (2) can be reduced to solving a linear program $\\mathcal{L}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. We first need the following result, which states that the feasible set of 1 can be expressed as a linear program. We will prove this result below. ", "page_idx": 23}, {"type": "text", "text": "Lemma 2. Let $\\mathcal{F}$ denote the feasible set of Problem $^{\\,l}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{F}\\triangleq\\arg\\operatorname*{max}_{\\rho\\in\\Delta_{n-1}^{m}}I_{\\mathrm{min}}\\left(\\rho\\right)\\triangleq\\arg\\operatorname*{max}_{\\rho\\in\\Delta_{n-1}^{m}}\\operatorname*{min}_{j}I_{j}(\\rho).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then $\\mathcal{F}$ is the solution set of the linear program ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{F}=\\arg\\underset{\\rho\\in\\Delta_{n-1}^{m},\\lambda}{\\operatorname*{max}}}&{\\lambda}\\\\ {s u b j e c t\\;t o}&{I_{j}(\\rho)=\\lambda\\quad\\forall j.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, condition (iii) on $\\boldsymbol{S}$ implies that there is a unique $\\rho^{*}$ in $S\\cap{\\mathcal{F}}$ . Thus ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\boldsymbol{\\rho}^{*}=\\mathcal{F}\\cap\\mathcal{S}=\\arg\\operatorname*{max}_{\\boldsymbol{\\rho}\\in\\Delta_{n-1}^{m}\\cap\\mathcal{S},\\boldsymbol{\\lambda}}}&{\\boldsymbol{\\lambda}}\\\\ {\\mathrm{subject}\\;\\mathrm{to}}&{I_{j}(\\boldsymbol{\\rho})=\\boldsymbol{\\lambda}\\quad\\forall j,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can add $\\boldsymbol{S}$ as a constraint in Problem 4 because we know that there is at least one solution to Problem 4 in $\\boldsymbol{S}$ \u2013 namely, $\\rho^{*}-\\mathbf{s}\\mathrm{o}$ this additional constraint will not change the optimal objective value, but will constrain the set of optimal solutions to be in $\\boldsymbol{S}$ , as desired. ", "page_idx": 23}, {"type": "text", "text": "By condition (ii), $\\rho^{*}$ is an optimal solution for Problem 1; by condition (i), $\\boldsymbol{S}$ can be described by a finite set of linear constraints, and thus Problem 5 is a linear program; call this $\\mathcal{L}$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "We now show Lemma 2. ", "page_idx": 23}, {"type": "text", "text": "Lemma 2. Let $\\mathcal{F}$ denote the feasible set of Problem $^{\\,l}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{F}\\triangleq\\arg\\operatorname*{max}_{\\rho\\in\\Delta_{n-1}^{m}}I_{\\operatorname*{min}}\\left(\\rho\\right)\\triangleq\\arg\\operatorname*{max}_{\\rho\\in\\Delta_{n-1}^{m}}\\operatorname*{min}_{j}I_{j}(\\rho).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then $\\mathcal{F}$ is the solution set of the linear program ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{F}=\\arg\\underset{\\rho\\in\\Delta_{n-1}^{m},\\lambda}{\\operatorname*{max}}}&{\\lambda}\\\\ {s u b j e c t\\;t o}&{I_{j}(\\rho)=\\lambda\\quad\\forall j.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. To show this, we prove by contradiction that every optimal policy gives each item $j$ the same normalized utility; that is, we show that if the policy $\\rho$ is an optimal solution of Problem 3, then there is some $\\lambda$ such that for all $j$ , $I_{j}(\\rho)=\\lambda$ , and in this case $\\lambda=\\operatorname*{min}_{j}I_{j}(\\rho)$ . This implies that any solution $\\rho$ to Problem 3 is a feasible solution of 4. Since these two problems have the same objective, and the additional constraint in Problem 4 does not eliminate any solutions, the two problems have the same solution set. ", "page_idx": 24}, {"type": "text", "text": "To show that all items have the same normalized utility at the optimum, we suppose the contrary and produce a contradiction. Suppose that the policy $\\rho$ maximizes $\\operatorname*{min}_{j}I_{j}(\\rho)$ , but there is some $j$ such that $I_{j}(\\rho)>I_{\\mathrm{min}}^{*}$ . We will show that this means we can construct $\\rho^{\\prime}$ such that $I_{\\mathrm{min}}\\left(\\rho^{\\prime}\\right)>I_{\\mathrm{min}}^{*}$ . ", "page_idx": 24}, {"type": "text", "text": "First, note that it is impossible that $\\rho_{i j}\\,=\\,0$ for all $i$ , otherwise $I_{\\mathrm{min}}^{*}\\,=\\,I_{j}(\\rho)\\,=\\,0$ . However, by Lemma 1, $I_{\\mathrm{min}}^{*}>0$ . Thus for some $i$ , $\\rho_{i j}>0$ . ", "page_idx": 24}, {"type": "text", "text": "Define $\\mathcal{I}=\\{j^{\\prime}:I_{j^{\\prime}}(\\rho)=I_{\\mathrm{min}}^{*}\\}$ , and let $j^{\\prime}\\in\\mathcal{I}$ . Since $\\rho_{i j}>0$ and $\\textstyle\\sum_{k}\\rho_{i k}=1$ , $\\rho_{i j^{\\prime}}<1$ . Pick $\\epsilon>0$ such that $\\rho_{i j}^{\\prime}:=\\rho_{i j}-\\epsilon>0$ , $\\rho_{i j^{\\prime}}^{\\prime}:=\\rho_{i j^{\\prime}}+\\epsilon<1$ , and $I_{j}(\\rho^{\\prime})>I_{\\mathrm{min}}^{\\overline{{{*}}}}$ . Since $I_{j}$ is linear in the recommendation policy, this is always possible. ", "page_idx": 24}, {"type": "text", "text": "Now, repeat this process for all $j^{\\prime}\\,\\in\\,{\\mathcal{I}}$ , to obtain a final recommendation policy $\\rho^{\\prime}$ in which $I_{\\ell}(\\rho^{\\prime})>I_{\\mathrm{min}}^{*}$ for all $\\ell$ and thus $I_{\\mathrm{min}}\\left(\\rho^{\\prime}\\right)>I_{\\mathrm{min}}^{*}$ . This contradicts the optimality of $I_{\\mathrm{min}}^{*}$ . ", "page_idx": 24}, {"type": "text", "text": "Now, we prove Proposition 2. ", "page_idx": 24}, {"type": "text", "text": "Proposition 2. ${\\cal S}_{s y m m}$ satisfies conditions $(i)$ and (ii) in Proposition 1. Furthermore, solutions $\\rho\\in S_{s y m m}$ to Problem (2) have a sparse structure: ", "page_idx": 24}, {"type": "text", "text": "\u2022 If $\\rho$ is a basic feasible solution to the linear program $\\mathcal{L}$ in Proposition 1, then there are at most $n+K-1$ type-item pairs $(k,j)$ such that $\\rho_{k j}>0$ (out of $n K$ possible pairs). \u2022 If $\\rho$ is also optimal, then there are at most $K-1$ items that are ever recommended to more than one type of user, i.e., where $\\rho_{k j},\\rho_{k^{\\prime}j}>0$ for $k\\neq k^{\\prime}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. We will first prove that $\\mathcal{S}_{\\mathrm{symm}}$ satisfies conditions (i) and (ii), and then show sparsity. ", "page_idx": 24}, {"type": "text", "text": "Part 1. Recall that for fixed $w$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n{S_{\\mathrm{symm}}=\\{\\rho:\\rho_{i}=\\rho_{i^{\\prime}}\\,\\mathrm{if}\\,w_{i}=w_{i^{\\prime}}\\}=\\bigcap_{i,i^{\\prime}:w_{i}=w_{i^{\\prime}}}\\{\\rho:\\rho_{i}=\\rho_{i^{\\prime}}\\}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is a finite set of linear constraints, so condition (i) holds. ", "page_idx": 24}, {"type": "text", "text": "Now, we show condition (ii), that always there is some $\\phi^{*}\\in S_{\\mathrm{symm}}$ that is an optimal solution to Problem 1. Let $\\rho$ be an arbitrary optimal solution to Problem 1. Then $\\rho$ is also feasible: $I_{\\mathrm{min}}\\left(\\rho\\right)=$ I\u2217min . ", "page_idx": 24}, {"type": "text", "text": "We first define a piece of convenient notation: if two users $i,i^{\\prime}$ share the same utility vector $w_{i}=w_{i^{\\prime}}$ , we say that they have the same type $\\tau(i)=\\tau(i^{\\prime})$ . If $\\tau(i)=\\tau(i^{\\prime}):=\\tau$ , we write $w_{i j}=w_{i^{\\prime}j}:=w_{\\tau j}$ ", "page_idx": 24}, {"type": "text", "text": "Let ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\phi_{i j}=\\frac{1}{|\\{i^{\\prime}:\\tau(i^{\\prime})=\\tau(i)\\}|}\\sum_{\\substack{i^{\\prime}:\\tau(i^{\\prime})=\\tau(i)}}\\rho_{i^{\\prime}j}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $i,j$ . We must show that: ", "page_idx": 24}, {"type": "text", "text": "\u2022 $\\phi_{i j}=\\phi_{i^{\\prime}j}$ for all $i,i^{\\prime}$ where $\\tau(i)=\\tau(i^{\\prime})$ , \u2022 $\\phi$ is a valid set of recommendation probabilities, \u2022 $\\phi$ is feasible: $I_{\\mathrm{min}}\\left(\\phi\\right)=I_{\\mathrm{min}}^{*}$ , and \u2022 $\\phi$ is optimal: $U_{\\mathrm{min}}\\left(\\phi\\right)\\geq U_{\\mathrm{min}}\\left(\\rho\\right)=U_{\\mathrm{min}}^{*}$ . ", "page_idx": 24}, {"type": "text", "text": "Clearly $\\phi_{i j}=\\phi_{i^{\\prime}j}$ for all $i,i^{\\prime}$ where $\\tau(i)=\\tau(i^{\\prime})$ , since $\\phi_{i j}$ depends only on $i$ through $\\tau(i)$ . For each $i$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{j}\\phi_{i j}=\\frac{1}{|\\{i^{\\prime}:\\tau(i^{\\prime})=\\tau(i)\\}|}\\sum_{i^{\\prime}:\\tau(i^{\\prime})=\\tau(i)}\\sum_{j}\\rho_{i^{\\prime}j}=\\frac{1}{|\\{i^{\\prime}:\\tau(i^{\\prime})=\\tau(i)\\}|}\\sum_{i^{\\prime}:\\tau(i^{\\prime})=\\tau(i)}1=1.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, for all $\\begin{array}{r}{i,j,0\\leq\\operatorname*{min}_{r,s}\\rho_{r s}\\leq\\phi_{i j}\\leq\\operatorname*{max}_{r,s}\\rho_{r s}\\leq1.}\\end{array}$ . Thus $\\phi\\in\\Delta_{n-1}^{m}$ . ", "page_idx": 25}, {"type": "text", "text": "To show that $I_{\\mathrm{min}}\\left(\\phi\\right)=I_{\\mathrm{min}}^{*}$ , notice that intuitively, when we move from $\\rho$ to $\\phi$ , we redistribute the probability mass assigned to an item $j$ among users of the same type. Since all of these users generate the same value for item $j$ , there should be no change in item $j$ \u2019s expected utility. Formally, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{\\mathrm{min}}\\left(\\phi\\right)=\\displaystyle\\operatorname*{min}_{j}\\frac{\\sum_{\\tau}w_{\\tau j}\\sum_{i:\\tau(i)=\\tau}\\phi_{i j}}{\\sum_{\\tau}w_{\\tau j}\\sum_{i:\\tau(i)=\\tau}1}}\\\\ &{\\qquad\\quad\\ =\\displaystyle\\operatorname*{min}_{j}\\frac{\\sum_{\\tau}w_{\\tau j}\\sum_{i:\\tau(i)=\\tau}\\rho_{i j}}{\\sum_{\\tau}w_{\\tau j}\\sum_{i:\\tau(i)=\\tau}1}}\\\\ &{\\qquad\\quad\\ =I_{\\mathrm{min}}\\left(\\rho\\right)}\\\\ &{\\qquad\\quad\\ =I_{\\mathrm{min}}^{*}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, to show that $U_{\\mathrm{min}}\\left(\\phi\\right)\\ge U_{\\mathrm{min}}\\left(\\rho\\right)$ , we observe that if users with the same values are given different recommendation probabilities, some user will be worst off and have expected value lower than the average expected value over all users in that type. By averaging, we bring all users\u2019 expected values to the average expected value across the type, increasing the expected value for the previously worst off user. Formally, for each type $\\tau$ , let $\\begin{array}{r}{i(\\bar{\\tau)}\\,\\bar{\\in}\\,\\mathrm{arg}\\,\\mathrm{min}_{i:\\bar{\\tau}(i)=\\tau}\\,\\bar{U_{i}}(\\rho)}\\end{array}$ . Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\implies\\frac{\\sum_{j=1}^{n}w_{\\tau j}\\rho_{i(\\tau)j}}{\\operatorname*{max}_{j}v_{j}}\\le\\frac{\\sum_{j=1}^{n}w_{\\tau j}\\rho_{i^{\\prime}j}}{\\operatorname*{max}_{j}v_{j}}\\mathrm{~for~all~}i^{\\prime}:\\tau(i^{\\prime})=\\tau}\\\\ &{\\implies|\\{i^{\\prime}:\\tau(i^{\\prime})=\\tau\\}|\\frac{\\sum_{j=1}^{n}w_{\\tau j}\\rho_{i(\\tau)j}}{\\operatorname*{max}_{j}v_{j}}\\le\\sum_{i^{\\prime}:\\tau(i^{\\prime})=\\tau}\\frac{\\sum_{j=1}^{n}w_{\\tau j}\\rho_{i^{\\prime}j}}{\\operatorname*{max}_{j}v_{j}}}\\\\ &{\\implies\\frac{\\sum_{j=1}^{n}w_{\\tau j}\\rho_{i(\\tau)j}}{\\operatorname*{max}_{j}v_{j}}\\le\\frac{\\sum_{j=1}^{n}w_{\\tau j}\\phi_{i^{\\prime}j}}{\\operatorname*{max}_{j}v_{j}}\\mathrm{~for~all~}i^{\\prime}:\\tau(i^{\\prime})=\\tau}\\\\ &{\\implies\\underset{i:\\tau(i)=\\tau}{\\operatorname*{min}}U_{i}(\\rho)\\le\\underset{i:\\tau(i)=\\tau}{\\operatorname*{min}}U_{i}(\\phi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "(definition of $\\phi$ ) ", "page_idx": 25}, {"type": "text", "text": "Then ", "page_idx": 25}, {"type": "equation", "text": "$$\nU_{\\operatorname*{min}}\\left(\\rho\\right)=\\operatorname*{min}_{\\tau}\\operatorname*{min}_{i:\\tau(i)=\\tau}U_{i}(\\rho)\\le\\operatorname*{min}_{\\tau}\\operatorname*{min}_{i:\\tau(i)=\\tau}U_{i}(\\phi)=U_{\\operatorname*{min}}\\left(\\phi\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and we have shown that $U_{\\mathrm{min}}\\left(\\phi\\right)\\ge U_{\\mathrm{min}}\\left(\\rho\\right)$ . ", "page_idx": 25}, {"type": "text", "text": "Part 2. Now we show the sparsity of solutions to the linear program $\\mathcal{L}$ in Proposition 1, that is, Problem 5 defined above in the proof of Proposition 1. Given that $\\rho\\in S_{\\mathrm{symm}}$ , we can rewrite the linear program as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi=\\arg\\operatorname*{max}_{\\rho\\in\\Delta_{n-1}^{K},\\lambda}}&{\\lambda}\\\\ {\\mathrm{subject~to}}&{I_{j}(\\rho)=\\lambda\\quad\\forall j,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which is the same as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi=\\arg\\operatorname*{max}_{\\rho,\\lambda}}&{\\lambda}\\\\ {\\mathrm{subject~to}}&{I_{j}(\\rho)=\\lambda\\quad\\forall j,}\\\\ &{\\displaystyle\\sum_{j}\\rho_{k j}=1\\quad\\forall k,}\\\\ &{\\rho_{k j}\\geq0\\quad\\forall k,j,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $I_{j}(\\rho)$ is shorthand for $I_{j}(\\rho^{\\prime})$ where $\\rho^{\\prime}\\in\\Delta_{n-1}^{m}$ is $\\rho\\in\\Delta_{n-1}^{K}$ expressed in terms of users rather than types. This is a linear program with $n K+1$ variables, and $n+K$ equality constraints. Then for any basic feasible solution $\\rho$ of Problem 5, there must be $n K+1$ linearly independent active constraints (see Definition 2.9 in Bertsimas and Tsitsiklis [7]). This means that of the $n K$ constraints $\\{\\rho_{k j}\\,\\ge\\,0\\}_{k,j}$ , at least $n K+1-(n+K)$ must be binding. Equivalently, there can be at most ${\\dot{n}}K^{\\stackrel{*}{}}-(n{\\dot{K}}\\stackrel{*}{+}1-(n+K))=n+K-1$ values of $k,j$ such that $\\rho_{k j}>0$ . ", "page_idx": 25}, {"type": "text", "text": "If $\\rho$ is also an optimal solution, then by Lemma 1 there must be some $k$ such that $\\rho_{k j}>0$ for each $j$ , which takes up $n$ non-zero values. Only $K-1$ non-zero values remain to be assigned, so at most $K-1$ items $j$ have $\\rho_{k j},\\rho_{k^{\\prime}j}>0$ for two types $k,k^{\\prime}$ . \u53e3 ", "page_idx": 25}, {"type": "text", "text": "D Proof of Theorem 3 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Recall Theorem 3 and its setting. We let $v_{1}>v_{2}>\\ldots>v_{n}$ and suppose that there are two user types. A user $i$ either has value $w_{i j}=v_{j}$ for all $j$ or $w_{i j}=v_{n-j+1}$ for all $j$ , corresponding to types 1 and 2 respectively. That is, the two types have opposite preferences. We let $\\alpha$ be the proportion of type 1 users in the population, out of a fixed population of $m$ users. ", "page_idx": 26}, {"type": "text", "text": "Theorem 3. $\\pi_{U|I}^{F}(\\alpha)$ is decreasing in $\\alpha$ for $0<\\alpha\\leq1/2$ , and increasing in $\\alpha$ for $1/2\\le\\alpha<1$ . ", "page_idx": 26}, {"type": "text", "text": "D.1 Main proof ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. The proof will proceed in two parts. First, we will reduce the problem to finding a solution for a linear program using the framework from Proposition 1. Then, we will find a solution to the linear program. In this proof, we will use a series of intermediate results, which we prove in the following section. The first such result is Lemma 3. ", "page_idx": 26}, {"type": "text", "text": "Lemma 3. For all $0<\\alpha<1$ , $U_{\\operatorname*{min}}^{*}\\left(\\alpha\\right)=1$ . ", "page_idx": 26}, {"type": "text", "text": "By Lemma 3, $U_{\\operatorname*{min}}^{*}\\left(\\alpha\\right):=U_{\\operatorname*{min}}^{*}$ is constant in $\\alpha$ , so $\\pi_{U|I}^{F}(\\alpha)\\,=\\,(U_{\\mathrm{min}}^{*}-U_{\\mathrm{min}}^{*}\\,(1,\\!\\alpha))\\big/U_{\\mathrm{min}}^{*}$ and it is enough to show $U_{\\operatorname*{min}}^{*}\\left(1,\\alpha\\right)$ increases in $\\alpha$ . ", "page_idx": 26}, {"type": "text", "text": "Recall the definition of $\\mathcal{S}_{\\mathrm{symm}}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\{\\rho:\\rho_{i}=\\rho_{i^{\\prime}}\\,\\,\\mathrm{if}\\,\\,w_{i}=w_{i^{\\prime}}\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By Proposition 2, we know that $\\ensuremath{S_{\\mathrm{symm}}}$ is a satisfies conditions (i) and (ii) on $\\boldsymbol{S}$ for Proposition 1. Thus by Proposition 1, it is sufficient to find $\\phi$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi\\in\\arg\\operatorname*{max}_{\\rho\\in\\mathcal{S}\\cap\\Delta_{n-1}^{m},\\lambda}}&{\\lambda}\\\\ {\\mathrm{subject~to}}&{I_{j}(\\rho)=\\lambda\\quad\\forall j,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and show that this $\\phi$ is unique, where the linear program is derived from the proof of Proposition 1. ", "page_idx": 26}, {"type": "text", "text": "Since each user in our population only has one of two possible value vectors, $w_{i}\\,=\\,(v_{1},...,v_{n})$ or $w_{i}\\,=\\,(v_{n},...,v_{1})$ , we can identify $\\ensuremath{S_{\\mathrm{symm}}}$ with $\\Delta_{n-1}^{2}$ , and identify $\\rho\\,\\in\\,S_{\\mathrm{symm}}$ with a pair $x,y$ Expanding the expression for $I_{j}(\\rho)$ and making explicit the dependence on $\\alpha$ , we see that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{j}(\\rho,\\alpha)=\\frac{\\sum_{i}\\rho_{i j}w_{i j}(\\alpha)}{\\sum_{i}w_{i j}(\\alpha)}}\\\\ &{\\qquad\\qquad=\\frac{\\sum_{i:\\tau(i)=1}\\rho_{i j}v_{j}+\\sum_{i:\\tau(i)=2}\\rho_{i j}v_{n-j+1}}{\\sum_{i:\\tau(i)=1}v_{j}+\\sum_{i:\\tau(i)=2}v_{n-j+1}}}\\\\ &{\\qquad\\qquad=\\frac{m\\alpha x_{j}v_{j}+m(1-\\alpha)y_{j}v_{n-j+1}}{m\\alpha v_{j}+m(1-\\alpha)v_{n-j+1}}}\\\\ &{\\qquad\\qquad=q_{j}(\\alpha)x_{j}+(1-q_{j}(\\alpha))y_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ", "page_idx": 26}, {"type": "equation", "text": "$$\nq_{j}(\\alpha):=\\frac{\\alpha v_{j}}{\\alpha v_{j}+(1-\\alpha)v_{n-j+1}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that for all $j$ and $0<\\alpha<1$ , we have $0<q_{j},1-q_{j}<1$ . We can thus simplify ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi=\\arg\\operatorname*{max}_{x,y\\in\\Delta_{n-1}^{2},\\lambda}}&{\\lambda}\\\\ {\\mathrm{subject}\\tan}&{q_{j}(\\alpha)x_{j}+(1-q_{j}(\\alpha))y_{j}=\\lambda}&{\\forall j,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "First, we will show uniqueness and find a closed form to Problem 6 in Lemmas 4 and 5. Then, we will use this closed forms to show that $U_{\\operatorname*{min}}^{*}\\left(1,\\alpha\\right)$ is increasing for $\\alpha<1/2$ . By symmetry, this implies that $U_{\\operatorname*{min}}^{*}\\left(1,\\alpha\\right)$ must be decreasing for $\\alpha>1/2$ and we will be done. ", "page_idx": 26}, {"type": "text", "text": "Lemma 4. Problem 6 has a unique solution $(x,y,\\lambda)$ . Moreover, the solution is sparse: there is some $1\\leq t\\leq n$ such that for $j>t$ , $x_{j}=0$ , and for $j<t$ , $y_{j}=0$ . ", "page_idx": 26}, {"type": "text", "text": "Thus, to solve our original problem (Problem 1) we merely need to evaluate the user fairness $U_{\\mathrm{min}}$ of $x,y$ solving Problem 6 and show that this is increasing. We will do this in the remainder of the proof. For each $\\alpha$ , let $x,y$ be the solution to the corresponding Problem 6, and define7 ", "page_idx": 27}, {"type": "equation", "text": "$$\nt(\\alpha):=\\operatorname*{max}\\{j:x_{j}(\\alpha)>0\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma 5. Let $0<\\alpha<1,t:=t(\\alpha)$ . Define ", "page_idx": 27}, {"type": "equation", "text": "$$\nL_{t}=\\sum_{j<t}{\\frac{1}{q_{j}}},\\quad R_{t}=\\sum_{j>t}{\\frac{1}{1-q_{j}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then ", "page_idx": 27}, {"type": "equation", "text": "$$\nI_{\\operatorname*{min}}^{*}=\\frac{1}{1+q_{t}L_{t}+(1-q_{t})R_{t}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\nx_{j}=\\left\\{\\begin{array}{l l}{\\frac{I_{\\mathrm{min}}^{*}}{q_{j}},}&{j<t}\\\\ {1-\\frac{L_{t}}{1+q_{t}L_{t}+(1-q_{t})R_{t}},}&{j=t\\,,\\quad y_{j}=\\left\\{\\begin{array}{l l}{0,}&{j<t}\\\\ {1-\\frac{R_{t}}{1+q_{t}L_{t}+(1-q_{t})R_{t}},}&{j=t\\,.}\\\\ {\\frac{I_{\\mathrm{min}}^{*}}{1-q_{j}},}&{j>t}\\end{array}\\right.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that for $t\\neq t(\\alpha),x,y$ above may not be valid probability vectors. ", "page_idx": 27}, {"type": "text", "text": "Lemma 6. For $0\\,<\\,\\alpha\\,<\\,1/2,$ , if $\\rho^{*}$ is the optimal recommendation policy, then $U_{i}(\\rho^{*},\\alpha)\\ge$ $U_{i^{\\prime}}(\\rho^{*},\\alpha)$ for $\\tau(i)=1$ , $\\tau(i^{\\prime})=2$ . ", "page_idx": 27}, {"type": "text", "text": "By Lemma 6, $U_{\\mathrm{min}}^{*}\\left(\\alpha\\right)=U_{i}(\\alpha)$ for $i$ with $\\tau(i)=2$ . Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{U_{\\mathrm{min}}^{*}\\left(1,\\alpha\\right)=U_{i}\\left(\\alpha\\right)}}\\\\ &{=\\frac{1}{\\operatorname*{max}_{j}v_{j}}\\sum_{j=1}^{n}y_{j}v_{n-j+1}}\\\\ &{=\\frac{1}{\\operatorname*{max}_{j}v_{j}}\\left(y_{t(\\alpha)}v_{n-t(\\alpha)+1}+\\sum_{j>t(\\alpha)}y_{j}v_{n-j+1}\\right)}\\\\ &{=\\frac{1}{\\operatorname*{max}_{j}v_{j}}\\left(\\left(1-\\sum_{j>t(\\alpha)}\\frac{I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)}{1-q_{j}(\\alpha)}\\right)v_{n-t(\\alpha)+1}+\\sum_{j>t(\\alpha)}\\frac{I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)}{1-q_{j}(\\alpha)}v_{n-j+1}\\right)}\\\\ &{=\\frac{1}{\\operatorname*{max}_{j}v_{j}}\\left(v_{n-t(\\alpha)+1}+\\sum_{j>t(\\alpha)}\\frac{I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)}{1-q_{j}(\\alpha)}(v_{n-j+1}-v_{n-t(\\alpha)+1})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The following results will enable us to conclude that $U_{\\operatorname*{min}}^{*}\\left(1,\\alpha\\right)$ is increasing for $\\alpha<1/2$ . ", "page_idx": 27}, {"type": "text", "text": "Lemma 7. $t(\\alpha)$ is weakly increasing. ", "page_idx": 27}, {"type": "text", "text": "Lemma 8. $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)$ is increasing for $\\alpha\\leq1/2$ . ", "page_idx": 27}, {"type": "text", "text": "Lemma 9. $q_{j}(\\alpha)$ strictly increases as $\\alpha$ increases and strictly decreases as $j$ increases. ", "page_idx": 27}, {"type": "text", "text": "Since $t(\\alpha)$ is increasing and $v_{j}$ decreases in $j$ , $v_{n-t(\\alpha)+1}$ is increasing in $\\alpha$ . Also, $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)$ is increasing for $\\alpha<1/2$ , and $q_{j}(\\alpha)$ is increasing, so $\\frac{I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)}{1\\!-\\!q_{j}\\left(\\alpha\\right)}$ is increasing. For $j>t(\\alpha)$ , $v_{n-j+1}-$ $v_{n-t(\\alpha)+1}>0$ . Thus, $U_{\\operatorname*{min}}^{*}\\left(1,\\alpha\\right)$ is increasing. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "D.2 Supporting lemma proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We will first prove Lemma 9, as it is a simple result that we will use extensively. ", "page_idx": 28}, {"type": "text", "text": "Lemma 9. $q_{j}(\\alpha)$ strictly increases as $\\alpha$ increases and strictly decreases as $j$ increases. ", "page_idx": 28}, {"type": "text", "text": "Proof. Recall the definition of $q_{j}(\\alpha)$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\nq_{j}(\\alpha)=\\frac{\\alpha v_{j}}{\\alpha v_{j}+(1-\\alpha)v_{n-j+1}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We can rewrite this as ", "page_idx": 28}, {"type": "equation", "text": "$$\nq_{j}(\\alpha)=\\frac{1}{1+\\frac{(1-\\alpha)v_{n-j+1}}{\\alpha v_{j}}}=\\frac{1}{1+(\\frac{1}{\\alpha}-1)\\frac{v_{n-j+1}}{v_{j}}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which is strictly increasing in $\\alpha$ since $v_{n-j+1}/v_{j}>0$ . ", "page_idx": 28}, {"type": "text", "text": "Since $v_{j}$ is decreasing in $j$ $\\dot{},\\,v_{n-j+1}/v_{j}$ is increasing in $j$ . Then since $1/\\alpha>1$ , $\\begin{array}{r}{\\left(\\frac{1}{\\alpha}-1\\right)\\frac{v_{n-j+1}}{v_{j}}}\\end{array}$ is increasing, so $q_{j}(\\alpha)$ is decreasing. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Lemma 3. For all $0<\\alpha<1$ , $U_{\\operatorname*{min}}^{*}\\left(\\alpha\\right)=1$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. Simply put, the argument below says that since there are no total utility or item fairness constraints, we can simply maximize utility for each user individually. When we do this, each user gets a utility ratio of 1. Formally, recall that ", "page_idx": 28}, {"type": "equation", "text": "$$\nU_{\\mathrm{min}}^{*}\\left(\\alpha\\right)=\\operatorname*{max}_{\\rho\\in\\Delta_{n-1}^{m}}\\operatorname*{min}_{i}\\frac{\\sum_{j=1}\\rho_{i j}w_{i j}(\\alpha)}{\\operatorname*{max}_{j}w_{i j}(\\alpha)}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We can pick $x_{i}$ for each user $i$ independently, so this problem becomes ", "page_idx": 28}, {"type": "equation", "text": "$$\nU_{\\mathrm{min}}^{*}\\left(\\alpha\\right)=\\operatorname*{min}_{i}\\frac{1}{\\operatorname*{max}_{j}w_{i j}}\\operatorname*{max}_{\\rho_{i}\\in\\Delta_{n-1}}\\sum_{j=1}x_{i j}w_{i j}(\\alpha).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We know that $\\textstyle\\sum_{j=1}\\rho_{i j}w_{i j}$ is maximized by putting all probability mass of $\\rho_{i}$ on the coordinate of $w_{i}$ with the highest value, yielding expected value $\\operatorname*{max}_{j}w_{i j}$ . Then ", "page_idx": 28}, {"type": "equation", "text": "$$\nU_{\\operatorname*{min}}^{*}\\left(\\alpha\\right)=\\operatorname*{min}_{i}\\frac{\\operatorname*{max}_{j}w_{i j}}{\\operatorname*{max}_{j}w_{i j}}=1.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma 4. Problem $^{6}$ has a unique solution $(x,y,\\lambda)$ . Moreover, the solution is sparse: there is some $1\\leq t\\leq n$ such that for $j>t$ , $x_{j}=0$ , and for $j<t,$ , $y_{j}=0$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. Let $\\alpha$ be fixed; we will suppress the dependence on $\\alpha$ for the remainded of the proof. Recall that Problem 6 is a linear program, and we can therefore rely on known facts about the structure of solutions to linear programs to prove this result. ", "page_idx": 28}, {"type": "text", "text": "Part 1 First, we show that at least $n-1$ of $\\{x_{j},y_{j}\\}_{j}$ must be zero in every basic feasible solution. By Proposition 2, since $K=2$ , at most $n+2-1=n+1$ of $\\{x_{j},y_{j}\\}_{j}$ can be non-zero and thus at least $n-1$ of these are zero. ", "page_idx": 28}, {"type": "text", "text": "Part 2 Now, we show that every optimal basic feasible solution has the form above. ", "page_idx": 28}, {"type": "text", "text": "If $x,y,\\lambda$ define an optimal basic feasible solution and $x_{j}=0$ , then if $i>j$ , $x_{i}=0$ . To see this, suppose this is not the case, that is, there is some $j<i$ such that $x_{i}:=c>0$ but $x_{j}=0$ . We will show that we can form $x^{\\prime},y^{\\prime}$ with $\\operatorname*{min}_{j}I_{j}(x^{\\prime},y^{\\prime})>\\lambda$ , which means that the policy $\\rho^{\\prime}$ formed by giving recommendation policy $x^{\\prime}$ to all users of type 1 and recommendation policy $y^{\\prime}$ to all users of type 2, is a better solution to Problem 1 than the policy $\\rho$ defined by $x,y$ , which means $\\rho$ is not optimal and we have a contradiction. ", "page_idx": 28}, {"type": "text", "text": "Since $x,y$ are feasible, we must have $I_{i}(x,y)=I_{j}(x^{\\prime},y^{\\prime})=\\lambda$ , so ", "page_idx": 29}, {"type": "equation", "text": "$$\nq_{i}c+(1-q_{i})y_{i}=(1-q_{j})y_{j}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Define $x^{\\prime},y^{\\prime}$ as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{\\ell}^{\\prime}=\\left\\{\\!\\!\\begin{array}{l l}{c-\\epsilon_{1},}&{\\ell=j}\\\\ {0,}&{\\ell=i}\\\\ {x_{\\ell}+\\frac{\\epsilon_{1}}{n-2},}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right.,}\\\\ {y_{\\ell}^{\\prime}=\\left\\{\\!\\!\\begin{array}{l l}{y_{j}-\\left(\\frac{q_{i}c}{1-q_{i}}+\\epsilon_{2}\\right),}&{\\ell=j}\\\\ {y_{i}+\\left(\\frac{q_{i}c}{1-q_{i}}+\\epsilon_{2}\\right),}&{\\ell=i}\\\\ {y_{\\ell},}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Intuitively, to define $x^{\\prime}$ we move all probability mass away from $i$ to $j$ . To define $y^{\\prime}$ , we move sufficient mass from $j$ to $i$ to offset the decrease in value to item $i$ from changing $x$ to $x^{\\prime}$ . In order to strictly increase $I_{\\ell}$ for all items $\\ell$ , in $x^{\\prime}$ we also move $\\epsilon_{1}$ mass to the other items. ", "page_idx": 29}, {"type": "text", "text": "These will be valid probability vectors for $\\epsilon_{1},\\epsilon_{2}>0$ small enough; for $x^{\\prime}$ this is clear. For $y^{\\prime}$ , we must show that $\\begin{array}{r}{y_{j}>\\frac{q_{i}c}{1-q_{i}}}\\end{array}$ 1q\u2212icq , and yi + $\\begin{array}{r}{y_{i}+\\frac{q_{i}c}{1-q_{i}}<1}\\end{array}$ . Rearranging Equation 7 and noticing that $q_{j}>q_{i}$ by Lemma 9, ", "page_idx": 29}, {"type": "equation", "text": "$$\ny_{i}+\\frac{q_{i}c}{1-q_{i}}=\\frac{(1-q_{j})y_{j}}{1-q_{i}}<y_{j}\\leq1.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Moreover, ", "page_idx": 29}, {"type": "equation", "text": "$$\n(q_{j}>q_{i})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{y_{j}-\\displaystyle\\frac{q_{i}c}{1-q_{i}}=\\displaystyle\\frac{q_{i}c+(1-q_{i})y_{i}}{1-q_{j}}-\\frac{q_{i}c}{1-q_{i}}}}\\\\ {{\\displaystyle\\qquad\\qquad>\\displaystyle\\frac{q_{i}c+(1-q_{i})y_{i}}{1-q_{i}}-\\frac{q_{i}c}{1-q_{i}}}}\\\\ {{\\displaystyle\\qquad\\qquad=y_{i}.}}\\\\ {{\\displaystyle\\qquad\\qquad\\geq0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, we can show that $I_{\\mathrm{min}}\\left(x^{\\prime},y^{\\prime}\\right)>I_{\\mathrm{min}}\\left(x,y\\right)=I_{\\mathrm{min}}^{*}$ , which is a contradiction. To do this, it is sufficient to show that $I_{\\ell}(x^{\\prime},y^{\\prime})>I_{\\ell}(x,y)$ for each $\\ell$ . ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{I_{i}(x^{\\prime},y^{\\prime})=(1-q_{i})\\displaystyle\\left(y_{i}+\\frac{q_{i}c}{1-q_{i}}+\\epsilon_{2}\\right)}}\\\\ {{=(1-q_{i})y_{i}+q_{i}\\lambda+\\epsilon_{2}}}\\\\ {{=I_{i}(x,y)+\\epsilon_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "\u2022 $\\ell=j$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{I_{j}(x^{\\prime},y^{\\prime})=q_{j}(\\lambda-\\epsilon_{1})+(1-q_{j})\\left(y_{j}-\\frac{q_{i}\\lambda}{1-q_{i}}-\\epsilon_{2}\\right)}}\\\\ {{\\qquad\\,=(1-q_{j})y_{j}+\\lambda\\left(q_{j}-q_{i}\\frac{1-q_{j}}{1-q_{i}}\\right)-\\epsilon_{1}q_{j}-\\epsilon_{2}(1-q_{j})}}\\\\ {{\\qquad\\,=I_{j}(x,y)+\\lambda\\left(q_{j}-q_{i}\\frac{1-q_{j}}{1-q_{i}}\\right)-\\epsilon_{1}q_{j}-\\epsilon_{2}(1-q_{j})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "$\\begin{array}{r}{q_{j}>q_{i},\\,q_{j}-q_{i}\\frac{1-q_{j}}{1-q_{i}}>0}\\end{array}$ i11\u2212\u2212qqji > 0, and thus Ij(x\u2032, y\u2032) > Ij(x, y) if \u03f51, \u03f52 are small enough. ", "page_idx": 29}, {"type": "text", "text": "Since we did not use the fact that $\\alpha<1/2$ here, a symmetric argument shows that we cannot have $y_{j}=0$ , $y_{i}>0$ , and $i<j$ , ", "page_idx": 29}, {"type": "text", "text": "Let $k$ be the number of indices $j$ such that $x_{j}~>~0$ . The above arguments together imply that $x_{j}=0,j>k$ . There are then at least $n-1\\bar{-}\\left(n-k\\right)=k-1$ indices $j$ such that $y_{j}=0$ ; the argument above implies that these zeroes are on the lowest possible indices, that is, $y_{j}=0,j<k$ . Thus if we take $t=k$ , we have that for $j>t$ , $x_{j}=0$ , and for $j<t$ , $y_{j}=0$ . ", "page_idx": 29}, {"type": "text", "text": "Part 3 Finally, we show that there is only a single optimal solution, which is a basic feasible solution of this form. ", "page_idx": 30}, {"type": "text", "text": "Suppose that $x^{\\prime},y^{\\prime}$ is another optimal basic feasible solution; we will show that $x^{\\prime}=x,y^{\\prime}=y$ .   \nDenote $t(x,y)=\\operatorname*{max}\\{j:x_{j}>0\\}$ . ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{j}x_{j}=I_{j}(x,y)}\\\\ &{\\qquad=I_{\\mathrm{min}}\\left(x,y\\right)}\\\\ &{\\qquad=I_{\\mathrm{min}}\\left(x^{\\prime},y^{\\prime}\\right)}\\\\ &{\\qquad=I_{j}(x^{\\prime},y^{\\prime})}\\\\ &{\\qquad=q_{j}x_{j}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "so $x_{j}=x_{j}^{\\prime}$ .   \n\u2013 Similarly for $j>t$ , $x_{j}=x_{j}^{\\prime}=0$ , and $(1-q_{j})y_{j}=I_{j}(x,y)=I_{\\mathrm{min}}\\left(x,y\\right)=I_{\\mathrm{min}}\\left(x^{\\prime},y^{\\prime}\\right)=I_{j}(x^{\\prime},y^{\\prime})=(1-q_{j})y_{j}^{\\prime},$ so $y_{j}^{\\prime}=y_{j}$ .   \n\u2013 Finally, this means $\\begin{array}{r}{y_{t}=1-\\sum_{j>t}y_{j}=1-\\sum_{j>t}y_{j}^{\\prime}=y_{t}^{\\prime}}\\end{array}$ , and $\\begin{array}{r}{x_{t}=1-\\sum_{j<t}x_{j}=}\\end{array}$ $\\begin{array}{r}{1-\\sum_{j<t}x_{j}^{\\prime}=x_{t}^{\\prime}.}\\end{array}$ ", "page_idx": 30}, {"type": "text", "text": "\u2022 Otherwise, without loss of generality let $t(x^{\\prime},y^{\\prime})>t(x,y):=t$ . For $j<t$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\nq_{j}x_{j}=I_{j}(x,y)=I_{j}(x^{\\prime},y^{\\prime})=q_{j}x_{j}^{\\prime}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Furthermore, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle q_{t}x_{t}+(1-q_{t})y_{t}=I_{t}(x,y)=I_{t}(x^{\\prime},y^{\\prime})=q_{t}x_{t}^{\\prime}}\\\\ {\\displaystyle\\iff q_{t}\\left(1-\\sum_{j<t}x_{j}\\right)+(1-q_{t})y_{t}=q_{t}x_{t}^{\\prime}}\\\\ {\\displaystyle\\iff q_{t}\\left(1-\\sum_{j<t}x_{j}^{\\prime}\\right)+(1-q_{t})y_{t}=q_{t}x_{t}^{\\prime}}\\\\ {\\displaystyle\\iff(1-q_{t})y_{t}=q_{t}\\left(\\sum_{j\\leq t}x_{t}^{\\prime}-1\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $q_{t},1\\mathrm{~-~}q_{t}\\;>\\;0$ , and $\\begin{array}{r}{y_{t}\\;\\geq\\;0,\\sum_{j\\leq t}x_{t}^{\\prime}\\;\\leq\\;1}\\end{array}$ , it must be the case that $y_{t}\\;=\\;0$ and $\\textstyle\\sum_{j\\leq t}x_{t}^{\\prime}=1$ . However, $y_{t}>0$ by definition of $t$ , and we have a contradiction. ", "page_idx": 30}, {"type": "text", "text": "This means that there is only one optimal basic feasible solution, and thus there is only one solution to the linear program. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Lemma 5. Let $0<\\alpha<1,t:=t(\\alpha)$ . Define ", "page_idx": 30}, {"type": "equation", "text": "$$\nL_{t}=\\sum_{j<t}{\\frac{1}{q_{j}}},\\quad R_{t}=\\sum_{j>t}{\\frac{1}{1-q_{j}}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then ", "page_idx": 30}, {"type": "equation", "text": "$$\nI_{\\operatorname*{min}}^{*}=\\frac{1}{1+q_{t}L_{t}+(1-q_{t})R_{t}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\nx_{j}=\\left\\{\\begin{array}{l l}{\\frac{I_{\\mathrm{min}}^{*}}{q_{j}},}&{j<t}\\\\ {1-\\frac{L_{t}}{1+q_{t}L_{t}+(1-q_{t})R_{t}},}&{j=t\\,,\\quad y_{j}=\\left\\{\\begin{array}{l l}{0,}&{j<t}\\\\ {1-\\frac{R_{t}}{1+q_{t}L_{t}+(1-q_{t})R_{t}},}&{j=t\\,.}\\\\ {\\frac{I_{\\mathrm{min}}^{*}}{1-q_{j}},}&{j>t}\\end{array}\\right.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Fix $0\\,<\\,\\alpha\\,<\\,1$ . This result mainly follows from the fact that $\\textstyle\\sum_{j}x_{j}\\,=\\,\\sum_{j}y_{j}\\,=\\,1$ , and   \n$I_{j}(x^{*},y^{*})=I_{j^{\\prime}}(x^{*},y^{*})$ for all $j,j^{\\prime}$ , as well as the sparse solution structure shown in Lemma 4.   \nWe know that the optimal solution satisfies $I_{\\mathrm{min}}^{*}=I_{j}=I_{j^{\\prime}}$ for all $j,j^{\\prime}$ . We also already know that   \nfor $j>t$ , $x_{j}=0$ , and for $j<t,y_{j}=0$ .   \n$\\begin{array}{r}{\\mathrm{For}\\;j<t,I_{\\mathrm{min}}^{*}=I_{j}=x_{j}q_{j}+0\\implies x_{j}=\\frac{I_{\\mathrm{min}}^{*}}{q_{j}}.}\\end{array}$ qj   \nFor j > t, I\u2217min = Ij = 0 + yj(1 \u2212qj) =\u21d2 yj = 1Im\u2212iqnj .   \nFor $j=t$ , we know $I_{\\operatorname*{min}}^{*}=I_{t}=q_{t}x_{t}+(1-q_{t})y_{t}$ . Then for $j<t$ , $\\begin{array}{r}{x_{j}=\\frac{1}{q_{j}}(q_{t}x_{t}+(1-q_{t})y_{t})}\\end{array}$ , so   \n$1-x_{t}=\\sum_{j<t}x_{j}=\\sum_{j<t}{\\frac{1}{q_{j}}}{\\big(}q_{t}x_{t}+(1-q_{t})y_{t}{\\big)}={\\big(}q_{t}x_{t}+(1-q_{t})y_{t}{\\big)}\\sum_{j<t}{\\frac{1}{q_{t}}}={\\big(}q_{t}x_{t}+(1-q_{t})y_{t}{\\big)}L_{t}$ . (8) ", "page_idx": 31}, {"type": "text", "text": "Similarly, $\\begin{array}{r}{y_{j}=\\frac{1}{1-q_{j}}\\big(q_{t}x_{t}+(1-q_{t})y_{t}\\big)}\\end{array}$ so ", "page_idx": 31}, {"type": "equation", "text": "$$\n1-y_{t}=\\sum_{j>t}{\\frac{1}{1-q_{j}}}{\\big(}q_{t}x_{t}+(1-q_{t})y_{t}{\\big)}={\\big(}q_{t}x_{t}+(1-q_{t})y_{t}{\\big)}R_{t}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining equations 8 and 9, we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{L_{t}}{R_{t}}(1-y_{t})=1-x_{t}\\implies x_{t}=1-\\frac{L_{t}}{R_{t}}(1-y_{t})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Substituting equation 10 into equation 9, we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-y_{t}=\\left(q_{t}\\left(1-\\cfrac{L_{t}}{R_{t}}(1-y_{t})\\right)+(1-q_{t})y_{t}\\right)R_{t}}\\\\ &{\\iff1+q_{t}L_{t}-q_{t}R_{t}=y_{t}(1+q_{t}L_{t}+(1-q_{t})R_{t})}\\\\ &{\\implies y_{t}=\\cfrac{1+q_{t}L_{t}-q_{t}R_{t}}{1+q_{t}L_{t}+(1-q_{t})R_{t}}=1-\\cfrac{R_{t}}{1+q_{t}L_{t}+(1-q_{t})R_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then ", "page_idx": 31}, {"type": "equation", "text": "$$\nx_{t}=1-\\frac{L_{t}}{R_{t}}(1-y_{t})=1-\\frac{L_{t}}{1+q_{t}L_{t}+(1-q_{t})R_{t}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Finally, we see that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{I_{\\mathrm{min}}^{*}=q_{t}x_{t}+(1-q_{t})y_{t}}}\\\\ {{\\displaystyle=q_{t}\\left(1-\\frac{L_{t}}{1+q_{t}L_{t}+(1-q_{t})R_{t}}\\right)+(1-q_{t})\\left(1-\\frac{R_{t}}{1+q_{t}L_{t}+(1-q_{t})R_{t}}\\right)}}\\\\ {{\\displaystyle=1-\\frac{q_{t}L_{t}+(1-q_{t})R_{t}}{1+q_{t}L_{t}+(1-q_{t})R_{t}}}}\\\\ {{\\displaystyle=\\frac{1}{1+q_{t}L_{t}+(1-q_{t})R_{t}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma 6. For $0\\,<\\,\\alpha\\,<\\,1/2,$ , if $\\rho^{*}$ is the optimal recommendation policy, then $U_{i}(\\rho^{*},\\alpha)\\ge$ $U_{i^{\\prime}}(\\rho^{*},\\alpha)$ for $\\tau(i)=1$ , $\\tau(i^{\\prime})=2$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. Intuitively, the type with a larger proportion in the population must shoulder the burden of ensuring item fairness to mediocre items, and thus users from this type will have a lower recommendation probability on the items they really like compared to the smaller group. ", "page_idx": 31}, {"type": "text", "text": "Formally, let $i,i^{\\prime}$ be users such that $\\tau(i)=1$ , $\\tau(i^{\\prime})=2$ . For $\\alpha\\leq1/2,j<t(\\alpha)$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{x_{j}-y_{n-j+1}=\\frac{1}{q_{j}(\\alpha)}-\\frac{1}{1-q_{j}(\\alpha)}}&{}\\\\ {=\\frac{\\alpha v_{j}+(1-\\alpha)v_{n-j+1}}{\\alpha v_{j}}-\\frac{\\alpha v_{n-j+1}-(1-\\alpha)v_{j}}{(1-\\alpha)v_{j}}}\\\\ &{=\\frac{1}{v_{j}}\\left(v_{j}+\\frac{1-\\alpha}{\\alpha}v_{n-j+1}-v_{j}-\\frac{\\alpha}{1-\\alpha}v_{n-j+1}\\right)}\\\\ &{=\\frac{v_{n-j+1}}{v_{j}}\\left(\\frac{1-\\alpha}{\\alpha}-\\frac{\\alpha}{1-\\alpha}\\right)}\\\\ &{=\\frac{v_{n-j+1}}{v_{j}}\\frac{1-2\\alpha}{\\alpha(1-\\alpha)}}\\\\ &{>0}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n(0<\\alpha\\le1/2)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "So ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\underset{p\\in\\mathcal{X}}{\\operatorname*{max}}\\ v_{j}\\right)\\cdot\\left(U_{i}-U_{i^{\\prime}}\\right)}\\\\ &{=\\sum_{\\substack{v\\in\\mathcal{X}}}\\sum_{\\substack{x\\in\\mathcal{X}}}-\\sum_{\\substack{v\\in\\mathcal{X}}}v_{i-j+1}y_{j}}\\\\ &{\\quad\\times\\sum_{\\substack{v\\in\\mathcal{X}}}\\left(\\sum_{j\\in\\mathcal{X}}\\|\\partial_{x_{i}}\\|\\partial_{x_{i}}\\|\\partial_{x_{i}}\\|\\right)-\\sum_{\\substack{v\\in\\mathcal{X}}}\\nabla_{j}y_{i-1}}\\\\ &{=\\sum_{\\substack{v\\in\\mathcal{X}}}v_{j}(x_{j}-y_{j-j+1})-\\sum_{\\substack{v\\in\\mathcal{X}}}v_{j}y_{i-j+1}}\\\\ &{\\geq\\sum_{\\substack{x\\in\\mathcal{X}}}v_{i}(x_{j}-y_{j-j+1})-\\sum_{\\substack{v\\in\\mathcal{X}}}v_{j}y_{i-j+1}}\\\\ &{\\geq v_{i}\\sum_{\\substack{x\\in\\mathcal{X}}}\\sum_{\\substack{x\\in\\mathcal{X}}}\\jmath_{x-j+1}-v_{i}\\sum_{\\substack{v\\in\\mathcal{X}}}\\jmath_{v-j+1}}\\\\ &{\\geq v_{i}-v_{i}\\sum_{\\substack{x\\in\\mathcal{X}}}\\jmath_{x-j+1}-v_{i}\\sum_{\\substack{v=\\mathcal{X}}}\\jmath_{v-j+1}}\\\\ &{=v_{i}-v_{i}}\\\\ &{=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "(collecting coefficients of $v_{j}$ ) ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\sum_{j\\leq t}x_{j}=1)}\\\\ {~~}\\\\ {(\\sum_{j\\leq t}y_{j}=1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $\\operatorname*{max}_{j}v_{j}>0$ , we have that $U_{i}\\geq U_{i^{\\prime}}$ . ", "page_idx": 32}, {"type": "text", "text": "Lemma 7. $t(\\alpha)$ is weakly increasing. ", "page_idx": 32}, {"type": "text", "text": "Proof. Suppose not, that is, there are some $\\alpha<\\alpha^{\\prime}$ such that $t:=t(\\alpha)>t(\\alpha^{\\prime}):=t^{\\prime}$ . We will split this problem into several cases based on which of $\\alpha,\\alpha^{\\prime}$ gives a solution with higher item fairness, and show the implications of decreasing $t$ on the closed-form solutions from Lemma 5, eventually reaching a contradiction in each case. ", "page_idx": 32}, {"type": "text", "text": "Recall from Lemma 9 that $\\begin{array}{r}{q_{j}(\\alpha)=\\frac{\\alpha v_{j}}{\\alpha v_{j}+(1-\\alpha)v_{n-j+1}}}\\end{array}$ is increasing in $\\alpha$ , so $q_{j}(\\alpha)<q_{j}(\\alpha^{\\prime})$ for all $j$ . ", "page_idx": 32}, {"type": "text", "text": "First, let $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)<I_{\\mathrm{min}}^{*}\\left(\\alpha^{\\prime}\\right)$ . If $t=n$ then for $j<n$ , $y_{j}=0$ , and thus $y_{n}=1$ . So ", "page_idx": 32}, {"type": "equation", "text": "$$\nI_{\\operatorname*{min}}^{*}\\left(\\alpha\\right)=\\left(1-q_{n}(\\alpha)\\right)>\\left(1-q_{n}(\\alpha^{\\prime})\\right)\\geq\\left(1-q_{n}(\\alpha^{\\prime})\\right)y_{n}^{\\prime}=I_{\\operatorname*{min}}^{*}\\left(\\alpha^{\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "since $y_{n}^{\\prime}\\leq1$ , and we have a contradiction. Otherwise if $t<n$ , let $j>t$ . Then ", "page_idx": 32}, {"type": "equation", "text": "$$\n1-q_{j}(\\alpha))y_{j}=I_{\\operatorname*{min}}^{*}(\\alpha)<I_{\\operatorname*{min}}^{*}(\\alpha^{\\prime})=(1-q_{j}(\\alpha^{\\prime}))y_{j}^{\\prime}\\iff\\frac{y_{j}}{y_{j}^{\\prime}}<\\frac{1-q_{j}(\\alpha^{\\prime})}{1-q_{j}(\\alpha)}<1\\implies y_{j}<y_{j}^{\\prime}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then $\\begin{array}{r}{y_{t}=1-\\sum_{j>t}y_{j}>1-\\sum_{j>t}y_{j}^{\\prime}\\geq y_{t}^{\\prime}}\\end{array}$ . So ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{t}(\\alpha)x_{t}+(1-q_{t}(\\alpha))y_{t}<(1-q_{t}(\\alpha^{\\prime}))y_{t}^{\\prime}<(1-q_{t}(\\alpha^{\\prime}))y_{t}}\\\\ &{\\implies q_{t}(\\alpha)x_{t}<(1-q_{t}(\\alpha^{\\prime}))y_{t}-(1-q_{t}(\\alpha))y_{t}}\\\\ &{\\implies0\\leq q_{t}(\\alpha)x_{t}<y_{t}(q_{t}(\\alpha)-q_{t}(\\alpha^{\\prime}))<0}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and we have a contradiction. ", "page_idx": 33}, {"type": "text", "text": "Now, let $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)\\ge I_{\\mathrm{min}}^{*}\\left(\\alpha^{\\prime}\\right)$ instead. If $t^{\\prime}=1$ then $x_{1}^{\\prime}=1$ and ", "page_idx": 33}, {"type": "equation", "text": "$$\nI_{\\operatorname*{min}}^{*}\\left(\\alpha^{\\prime}\\right)=q_{1}(\\alpha^{\\prime})>q_{1}(\\alpha)\\geq q_{1}(\\alpha)x_{1}=I_{\\operatorname*{min}}^{*}\\left(\\alpha\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and we have a contradiction. Otherwise if $t^{\\prime}>1$ , then for $j<t^{\\prime}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\nq_{j}(\\alpha)x_{j}=I_{\\mathrm{min}}^{*}\\,(\\alpha)\\geq I_{\\mathrm{min}}^{*}\\,(\\alpha^{\\prime})=q_{j}(\\alpha^{\\prime})x_{j}^{\\prime}\\implies\\frac{x_{j}}{x_{j}^{\\prime}}\\geq\\frac{q_{j}(\\alpha^{\\prime})}{q_{j}(\\alpha)}>1\\implies x_{j}>x_{j}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then $\\begin{array}{r}{x_{t^{\\prime}}^{\\prime}=1-\\sum_{j<t^{\\prime}}x_{j}^{\\prime}>1-\\sum_{j<t^{\\prime}}x_{j}\\geq x_{t^{\\prime}}}\\end{array}$ . So ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{t^{\\prime}}(\\alpha^{\\prime})x_{t^{\\prime}}^{\\prime}+(1-q_{t^{\\prime}}(\\alpha^{\\prime}))y_{t^{\\prime}}^{\\prime}=I_{\\operatorname*{min}}^{*}\\left(\\alpha^{\\prime}\\right)\\leq I_{\\operatorname*{min}}^{*}\\left(\\alpha\\right)=q_{t^{\\prime}}(\\alpha)x_{t}}\\\\ &{\\implies(1-q_{t^{\\prime}}(\\alpha^{\\prime}))y_{t^{\\prime}}^{\\prime}\\leq q_{t^{\\prime}}(\\alpha)x_{t^{\\prime}}-q_{t^{\\prime}}(\\alpha^{\\prime})x_{t^{\\prime}}^{\\prime}<q_{t^{\\prime}}(\\alpha)x_{t^{\\prime}}-q_{t^{\\prime}}(\\alpha^{\\prime})x_{t^{\\prime}}}\\\\ &{\\implies0\\leq(1-q_{t^{\\prime}}(\\alpha^{\\prime}))y_{t^{\\prime}}^{\\prime}<x_{t^{\\prime}}(q_{t^{\\prime}}(\\alpha)-q_{t^{\\prime}}(\\alpha^{\\prime}))\\leq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and we have a contradiction. Thus all cases result in a contradiction. ", "page_idx": 33}, {"type": "text", "text": "Lemma 8. $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)$ is increasing for $\\alpha\\leq1/2$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. We first need the following Lemma, which shows that $\\alpha\\leq1/2$ implies that $t(\\alpha)$ must be an index at most halfway through the list of items. ", "page_idx": 33}, {"type": "text", "text": "Lemma 10. If $\\alpha\\leq1/2$ , then $t(\\alpha)\\leq(n+1)/2$ . ", "page_idx": 33}, {"type": "text", "text": "Now, for $1\\leq t\\leq n$ , define ${\\mathcal{A}}(t)=\\{\\alpha:t(\\alpha)=t\\}$ . Since $t(\\alpha)$ is weakly increasing in $\\alpha$ by Lemma 7, $\\boldsymbol{A}(t)$ is an interval in $(0,1)$ , and $[A(1),{\\mathcal{A}}(2),...,{\\mathcal{A}}(n)]$ forms a consecutive partition of $(0,1)$ . We will now show that $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)$ is increasing on each of the first $\\lfloor(n+1)/2\\rfloor$ intervals, which by Lemma 10 contain all $\\alpha\\in(0,1/2]$ . ", "page_idx": 33}, {"type": "text", "text": "Lemma 11. $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)$ is increasing on $A(t)$ for $t\\le(n+1)/2$ . ", "page_idx": 33}, {"type": "text", "text": "Finally, we show that this implies $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)$ is increasing on $0<\\alpha\\leq1/2$ . Notice that $I_{j}(x,y,\\alpha)$ is continuous in $\\alpha$ for each fixed $x,y$ , so $\\begin{array}{r}{I_{\\mathrm{min}}\\left(x,y,\\alpha\\right)=\\mathrm{min}_{j}\\,I_{j}(x,y,\\alpha)}\\end{array}$ is continuous in $\\alpha$ for each fixed $x,y$ . Recall that $\\begin{array}{r}{I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)=\\operatorname*{max}_{x,y\\in\\Delta_{n-1}}I_{\\mathrm{min}}\\left(x,y,\\alpha\\right)}\\end{array}$ . As the supremum of a set of continuous functions on a compact set must itself be continuous , this means that $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)$ must be continuous in $\\alpha$ , since $\\Delta_{n-1}\\times\\Delta_{n-1}$ is compact. Since $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)$ is continuous and is increasing on each interval $\\{\\mathcal{A}(t)\\}_{t\\leq(n+1)/2}$ by Lemma 11, $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)$ must be increasing on $\\textstyle\\bigcup_{t\\leq(n+1)/2}{\\mathcal{A}}(t)$ . By Lemma 10, $\\bigcup_{t\\le(n+1)/2}\\mathcal{A}(t)\\supset(0,1/2]$ . So $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)$ is increasing on $(0,1/2]$ . \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Now, we show Lemmas 10 and 11. ", "page_idx": 33}, {"type": "text", "text": "Lemma 10. If $\\alpha\\leq1/2$ , then $t(\\alpha)\\leq(n+1)/2$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. By Lemma 7, it is enough to show that for $\\alpha=1/2,t(\\alpha)\\leq(n+1)/2$ . ", "page_idx": 33}, {"type": "text", "text": "Suppose that $\\alpha=1/2$ and we have $x,y$ of the form in Lemma 5 such that $t:=t(\\alpha)>(n+1)/2$ . ", "page_idx": 33}, {"type": "text", "text": "If $n$ is even, take $t^{\\prime}=n/2$ in the definitions of $x_{j},y_{j}$ in Lemma 5; if $n$ is odd, take $t^{\\prime}=(n+1)/2$ . We now verify that this results in $x^{\\prime},y^{\\prime}$ that are a feasible solution to Problem 6 and that satisfy $x_{j}^{\\prime}=y_{n-j+1}^{\\prime}$ . This is simple to see after observing that when $\\alpha=1/2$ , $q_{j}=1-q_{n-j+1}$ , so for even $n$ , $\\begin{array}{r}{L_{t^{\\prime}}=R_{t^{\\prime}}+\\frac{1}{1-q_{t^{\\prime}}}}\\end{array}$ , and for odd $n,L_{t^{\\prime}}=R_{t^{\\prime}}$ , and simplifying. ", "page_idx": 33}, {"type": "text", "text": "Since by assumption $t(\\alpha)$ yields the optimal solution, $I_{\\mathrm{min}}\\left(x,y\\right)>I_{\\mathrm{min}}\\left(x^{\\prime},y^{\\prime}\\right)$ . By the construction of solutions in Lemma 5, for all $j$ we have $I_{j}(x^{\\prime},y^{\\prime})=I_{\\mathrm{min}}\\left(x^{\\prime},y^{\\prime}\\right)$ and $\\dot{I_{j}}(x,y)=I_{\\mathrm{min}}\\left(x,y\\right)$ . Then for $j<t$ , $q_{j}x_{j}=I_{j}(x,y)>I_{j}(x^{\\prime},y^{\\prime})=q_{j}x_{j}^{\\prime}$ , so $x_{j}>x_{j}^{\\prime}$ . Then since $I_{t^{\\prime}}(x,y)>I_{t^{\\prime}}(x^{\\prime},y^{\\prime})$ , ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{t^{\\prime}}x_{t^{\\prime}}>q_{t^{\\prime}}x_{t^{\\prime}}^{\\prime}+(1-q_{t^{\\prime}})y_{t^{\\prime}}^{\\prime}}\\\\ &{\\qquad=q_{t^{\\prime}}\\left(1-\\displaystyle\\sum_{j<\\ell^{\\prime}}x_{j}^{\\prime}\\right)+(1-q_{t^{\\prime}})y_{t^{\\prime}}^{\\prime}}\\\\ &{\\qquad>q_{t^{\\prime}}\\left(1-\\displaystyle\\sum_{j<\\ell^{\\prime}}x_{j}\\right)+(1-q_{t^{\\prime}})y_{t^{\\prime}}^{\\prime}}\\\\ &{\\qquad>q_{t^{\\prime}}\\left(1-\\displaystyle\\sum_{j<\\ell^{\\prime}}x_{j}\\right)}\\\\ &{\\qquad\\geq q_{t^{\\prime}}x_{t^{\\prime}}^{\\prime}}\\\\ &{\\qquad\\geq q_{t^{\\prime}}x_{t^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which is a contradiction. ", "page_idx": 34}, {"type": "text", "text": "Lemma 11. $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)$ is increasing on $A(t)$ for $t\\le(n+1)/2$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. Recall that for fixed $t$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\nI_{\\operatorname*{min}}^{*}\\left(\\alpha\\right)=\\frac{1}{1+q_{t}(\\alpha)L_{t}(\\alpha)+(1-q_{t}(\\alpha))R_{t}(\\alpha)}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "It is sufficient to show that $q_{t}L_{t}+(1-q_{t})R_{t}$ is decreasing. ", "page_idx": 34}, {"type": "text", "text": "We first expand the expression: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle q_{t}(\\alpha)L_{t}(\\alpha)+(1-q_{t}(\\alpha))R_{t}(\\alpha)}}\\\\ {{\\displaystyle~~=\\sum_{j<t}\\frac{q_{t}(\\alpha)}{q_{j}(\\alpha)}+\\sum_{j>t}\\frac{1-q_{t}(\\alpha)}{1-q_{j}(\\alpha)}}}\\\\ {{\\displaystyle~~=\\sum_{j<t}\\frac{q_{t}(\\alpha)}{q_{j}(\\alpha)}+\\sum_{j>n-t}\\frac{1-q_{t}(\\alpha)}{1-q_{j}(\\alpha)}+\\sum_{t<j<n-t}\\frac{1-q_{t}(\\alpha)}{1-q_{j}(\\alpha)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let $t<j\\leq n-t+1$ . Then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{g(\\alpha):=\\cfrac{1-q_{t}(\\alpha)}{1-q_{j}(\\alpha)}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}\\cfrac{\\alpha v_{j}+(1-\\alpha)v_{n-j+1}}{(1-\\alpha)v_{n-j+1}}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "So ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\prime(\\alpha)=\\frac{v_{n-t+1}}{v_{n-j+1}}\\frac{\\left(v_{j}-v_{n-j+1}\\right)\\left(v_{n-t+1}+\\alpha\\left(v_{t}-\\alpha\\right)v_{n-t+1}\\right)\\right)-\\left(v_{t}-v_{n-t+1}\\right)\\left(v_{n-j+1}+\\alpha\\left(v_{j}-v_{n-t+1}\\right)\\right)}{\\left(v_{n-t+1}+\\alpha\\left(v_{t}-v_{n-t+1}\\right)\\right)^{2}}\\frac{\\left(v_{n-j+1}\\right)-v_{n-j+1}}{2}\\frac{\\left(v_{n-t+1}\\right)^{2}}{2}\\frac{\\left(v_{n-1}+v_{n-j+1}\\right)\\left(v_{n-2}+v_{n-t+1}\\right)}{2}}}\\\\ &{}&{\\propto\\left(v_{j}-v_{n-j+1}\\right)\\!\\left(v_{n-t+1}+\\alpha\\!\\left(v_{t}-v_{n-t+1}\\right)\\right)-\\left(v_{t}-v_{n-t+1}\\right)\\!\\left(v_{n-j+1}+\\alpha\\left(v_{j}-v_{n-j+1}\\right)\\right)}\\\\ &{}&{=\\left(v_{j}-v_{n-j+1}\\right)\\!v_{n-t+1}-\\left(v_{t}-v_{n-t+1}\\right)\\!v_{n-j+1}}\\\\ &{}&{=v_{j}v_{n-t+1}-v_{t}v_{n-j+1}}\\\\ &{}&{\\le v_{t}v_{n-j+1}-v_{t}v_{n-j+1}}\\\\ &{}&{=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "So $g$ is a decreasing function of $\\alpha$ . Now, consider ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle\\sum_{j<t}}\\frac{q_{t}(\\alpha)}{q_{j}(\\alpha)}+\\sum_{j>n-t}\\frac{1-q_{t}(\\alpha)}{1-q_{j}(\\alpha)}}}\\\\ {{{\\displaystyle=\\sum_{j=1}^{t-1}\\frac{q_{t}(\\alpha)}{q_{j}(\\alpha)}+\\sum_{j=n-t+2}^{n}\\frac{1-q_{t}(\\alpha)}{1-q_{j}(\\alpha)}}}}\\\\ {{{\\displaystyle=\\sum_{j=1}^{t-1}\\frac{q_{t}(\\alpha)}{q_{j}(\\alpha)}+\\sum_{i=1}^{t-1}\\frac{1-q_{t}(\\alpha)}{1-q_{n-i+1}(\\alpha)}}}}\\\\ {{{\\displaystyle=\\sum_{j=1}^{t-1}\\frac{q_{t}(\\alpha)}{q_{j}(\\alpha)}+\\frac{1-q_{t}(\\alpha)}{1-q_{n-j+1}(\\alpha)}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n(i=n-j+1)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then for $1\\leq j\\leq t-1$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{t}(\\alpha):=\\frac{q(\\alpha)}{q(\\alpha)}+\\frac{1-q(\\alpha)}{1-q_{n}-j_{+1}(\\alpha)}}\\\\ &{\\quad=\\cfrac{\\alpha\\nu}{\\alpha\\nu_{t}+(1-\\alpha)\\nu_{n-t+1}}\\cdot\\cfrac{\\alpha\\nu_{j}+(1-\\alpha)\\nu_{n-j+1}}{\\alpha\\nu_{j}}}\\\\ &{\\quad\\quad+\\cfrac{(1-\\alpha)\\nu_{n-t+1}}{(1-\\alpha)\\nu_{n-t+1}}\\cdot\\cfrac{\\alpha\\nu_{n-j+1}+(1-\\alpha)\\nu_{j}}{(1-\\alpha)\\nu_{j}}}\\\\ &{\\quad=\\cfrac{\\nu_{0}(\\alpha)+(1-\\alpha)\\nu_{n-j+1})+\\nu_{n-t+1}(\\alpha\\nu_{n-j+1}+(1-\\alpha)\\nu_{j})}{\\nu_{j}(\\alpha\\nu_{t}+(1-\\alpha)\\nu_{n-t+1})}}\\\\ &{\\quad=1+\\frac{(1-\\alpha)\\nu_{0}\\nu_{n-j+1}+\\alpha\\nu_{n-t+1}\\nu_{n-j+1}}{\\nu_{j}(\\alpha\\nu_{t}+(1-\\alpha)\\nu_{n-t+1})}}\\\\ &{\\quad=1+\\frac{v_{n-j+1}}{v_{j}}\\cdot\\cfrac{(1-\\alpha)\\nu_{n+t}+\\alpha\\nu_{n-t+1}}{\\alpha\\nu_{j}+(1-\\alpha)\\nu_{n-t+1}}}\\\\ &{\\quad=1+\\frac{v_{n-j+1}}{v_{j}}\\cdot\\cfrac{\\alpha\\nu_{n+t+1}+\\alpha\\nu_{n-t+1}}{v_{j}+(1-\\alpha)\\nu_{n-t+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "So ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{h_{t}^{\\prime}(\\alpha)=\\frac{v_{n-j+1}}{v_{j}}}}\\\\ &{}&{\\cdot\\left(\\upsilon_{n-t+1}-\\upsilon_{t}\\right)(\\upsilon_{n-t+1}+\\alpha(\\upsilon_{t}-\\upsilon_{n-t+1}))-(\\upsilon_{t}-\\upsilon_{n-t+1})(\\upsilon_{t}+\\alpha(\\upsilon_{n-t+1}-\\upsilon_{t}))}\\\\ &{}&{\\quad\\left(\\upsilon_{n-t+1}+\\alpha(\\upsilon_{t}-\\upsilon_{n-t+1})\\right)^{2}}\\\\ &{}&{\\propto-(\\upsilon_{t}-\\upsilon_{n-t+1})(\\upsilon_{n-t+1}+\\alpha(\\upsilon_{t}-\\upsilon_{n-t+1}))-(\\upsilon_{t}-\\upsilon_{n-t+1})(\\upsilon_{t}+\\alpha(\\upsilon_{n-t+1}-\\upsilon_{t}))}\\\\ &{}&{=-(\\upsilon_{t}-\\upsilon_{n-t+1})(\\upsilon_{n-t+1}+\\alpha(\\upsilon_{t}-\\upsilon_{n-t+1})+\\upsilon_{t}+\\alpha(\\upsilon_{n-t+1}-\\upsilon_{t}))}\\\\ &{}&{=-(\\upsilon_{t}-\\upsilon_{n-t+1})(\\upsilon_{t}+\\upsilon_{n-t+1})}\\\\ &{}&{\\le-(\\upsilon_{t}-\\upsilon_{n-t+1})}\\\\ &{}&{=\\alpha\\circ}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and $h_{t}$ is decreasing in $\\alpha$ for $1\\leq j\\leq t\\!-\\!1$ . Then $\\textstyle\\sum_{j=1}^{t-1}h_{t}(\\alpha)$ is decreasing, and $q_{t}L_{t}+(1\\!-\\!q_{t})R_{t}=$ $\\begin{array}{r}{\\sum_{j=1}^{t-1}h_{t}(\\alpha)+\\sum_{j=t+1}^{n-t+1}g_{t}(\\alpha)}\\end{array}$ is decreasing, and $I_{\\mathrm{min}}^{*}\\left(\\alpha\\right)$ is increasing. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "E Proof of Theorem 4 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Recall Theorem 4. ", "page_idx": 35}, {"type": "text", "text": "Theorem 4. If $\\beta>{\\frac{1}{n}}$ and $w$ and $\\hat{w}$ are as described above, then fairness constraints can arbitrarily worsen the price of misestimation. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The price of misestimation without fairness constraints is low: for all $\\{v_{j}\\}$ , there is $a$ recommendation policy $\\hat{\\rho}$ that solves the misestimated problem $U_{\\operatorname*{min}}^{*}\\left(0,\\hat{w}\\right)$ so that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\pi_{U}^{M}(0,w,\\hat{w})\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "\u2022 The price of misestimation with fairness constraints can be arbitrarily large: \u2200\u03f5, there exists $\\{v_{j}\\}$ and a recommendation policy $\\hat{\\rho}$ that solves the problem $U_{\\operatorname*{min}}^{*}\\left(1,\\hat{w}\\right)$ such that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\pi_{U}^{M}(1,w,\\hat{w})>1-\\epsilon.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "E.1 Main proof ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proof. We first find a worst-case price of misestimation with fairness constraints, and then find the price of misestimation without fairness constraints. ", "page_idx": 36}, {"type": "text", "text": "Item fairness constraints with misestimation. First, notice that in the misestimated value matrix $\\hat{w}$ there are three distinct user value types, and thus we can identify $\\mathcal{S}_{\\mathrm{symm}}$ with $\\Delta_{n-1}^{3}$ 3n\u22121 and a policy $\\rho\\in S_{\\mathrm{symm}}$ with vectors $x,y,z$ , where $x$ is the recommendation policy for users with value $\\hat{w}_{i j}=v_{j}$ , $y$ is the recommendation policy for users with value $\\hat{w}_{i j}=v_{n-j+1}$ , and $z$ is the recommendation policy for the mis-estimated users. ", "page_idx": 36}, {"type": "text", "text": "Let ", "page_idx": 36}, {"type": "equation", "text": "$$\nS^{\\prime}=\\{x,y,z\\in S_{\\mathrm{symm}}:x_{j}=y_{n-j+1},z_{j}=z_{n-j+1}\\;{\\mathrm{for}}\\;{\\mathrm{all}}\\;j\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since $y$ is completely determined by $x$ , we can identify $S^{\\prime}$ with $\\Delta_{n-1}^{2}$ and can identify $x,y,z\\in S^{\\prime}$ simply by $x,z$ . ", "page_idx": 36}, {"type": "text", "text": "Lemma 12. If a policy $\\rho=(x,y,z)\\in S_{\\mathrm{symm}}$ solves $U_{\\mathrm{min}}\\left(\\rho\\right)=U_{\\mathrm{min}}^{*}\\left(1\\right)\\!,$ , then there is some policy $\\rho^{\\prime}=x^{\\prime},z^{\\prime}\\in S^{\\prime}$ that solves $U_{\\mathrm{min}}\\left(\\rho^{\\prime}\\right)=U_{\\mathrm{min}}^{*}\\left(1\\right.$ ). ", "page_idx": 36}, {"type": "text", "text": "Applying the framework in Proposition 1 to this proof with $S=S^{\\prime}$ and using $\\hat{w}$ as the user and item values, we reduce the problem to finding ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi=\\arg\\operatorname*{max}_{x,z\\in\\Delta_{n-1}^{2},\\lambda}}&{\\lambda}\\\\ {\\mathrm{subject~to}}&{I_{j}(x,z)=\\lambda\\quad\\forall j,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and showing that this $\\phi$ is unique. ", "page_idx": 36}, {"type": "text", "text": "First, we use an analogue of the sparsity result Proposition 2 to show that the optimal $x,z$ has the following sparse structure. ", "page_idx": 36}, {"type": "text", "text": "Lemma 13. Let $x,z$ be an optimal basic feasible solution to the linear program Problem 11. There is some $j\\leq\\textstyle{\\frac{n+1}{2}}$ such that for all $j^{\\prime}>j$ , $x_{j^{\\prime}}=0$ and for all $j^{\\prime}<j$ , $z_{j^{\\prime}}=z_{n-j^{\\prime}+1}=0$ . Let the pivot index $t$ denote the minimum such $j$ . ", "page_idx": 36}, {"type": "text", "text": "This sparse structure implies uniqueness. ", "page_idx": 36}, {"type": "text", "text": "Lemma 14. Problem 11 has a unique optimal solution. ", "page_idx": 36}, {"type": "text", "text": "We can now describe what this solution $(x,z)$ looks like. ", "page_idx": 36}, {"type": "text", "text": "Lemma 15. Let $(x,z)$ be the optimal solution to Problem $^{12}$ , and let t be the pivot element of $(x,z)$ ; suppose that $\\textstyle t\\neq{\\frac{n+1}{2}}$ . Define ", "page_idx": 36}, {"type": "equation", "text": "$$\nL_{t}:=\\sum_{j<t}\\frac{1}{q_{j}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\lambda=\\frac{2\\beta q_{t}+1/2(1-2\\beta)}{1+q_{t}L_{t}+1/2(n-2t)},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\nz_{j}=\\left\\{\\frac{0,}{\\frac{1}{2}\\left(1-(n-2t)\\frac{\\lambda}{1-2\\beta}\\right),}\\right.\\ \\ j\\in\\left\\{t,n-t+1\\right\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Finally, ", "page_idx": 36}, {"type": "equation", "text": "$$\nx_{j}=\\left\\{\\begin{array}{l l}{\\frac{\\lambda}{2\\beta q_{j}},}&{j<t}\\\\ {1-\\frac{\\lambda}{2\\beta}L_{t},}&{j=t\\;.}\\\\ {0,}&{j>t}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "If $\\begin{array}{r}{\\dot{\\boldsymbol{\\tau}}_{t}=\\frac{n+1}{2}}\\end{array}$ , then x remains the same, but $z_{t}=1$ and ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\lambda=\\frac{2\\beta q_{t}+(1-2\\beta)}{1+q_{t}L_{t}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "If we try $t=1$ and obtain $z_{1}<0$ above, we can conclude that $t>1$ . Suppose $t=1$ . Then $L_{t}=0$ , so ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\lambda=\\frac{2\\beta q_{1}+1/2(1-2\\beta)}{1+1/2(n-2)}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbf{\\dot{\\rho}}_{1}=\\frac{1}{2}\\left(1-(n-2)\\frac{\\lambda}{1-2\\beta}\\right)=\\frac{1}{2}\\left(1-(n-2)\\frac{1}{1-2\\beta}\\frac{2\\beta q_{1}+1/2(1-2\\beta)}{n/2}\\right)=\\frac{1}{n}-\\frac{n-2}{n}\\frac{2\\beta q_{1}-1}{1-2\\beta}\\mathbf{\\dot{\\rho}}_{1}-\\mathbf{\\dot{\\rho}}_{2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then $t=1$ and $z_{1}<0$ if and only if ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{n}-\\frac{n-2}{n}\\frac{2\\beta q_{1}}{1-2\\beta}<0\\iff\\frac{v_{n}}{v_{1}}<\\frac{n-2}{1/2\\beta-1}-1.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "since q1 = $\\begin{array}{r}{q_{1}=\\frac{v_{1}}{v_{1}+v_{n}}}\\end{array}$ . If $\\beta>{\\frac{1}{n}}$ , then ", "page_idx": 37}, {"type": "equation", "text": "$$\n{\\frac{n-2}{1/2\\beta-1}}-1>{\\frac{n-2}{n/2-1}}-1=2{\\frac{n-2}{n-2}}-1=1.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "So if $\\beta>{\\frac{1}{n}}$ , $t>1$ and $z_{1}=z_{n}=0$ . In this case, regardless of whether a mis-estimated user has $w_{i1}=v_{1}$ or $w_{i n}=v_{1}$ , that user will never be recommended their most preferred item. ", "page_idx": 37}, {"type": "text", "text": "Moreover, no matter the true type of the mis-estimated user, ", "page_idx": 37}, {"type": "equation", "text": "$$\nU_{i}(x,z)={\\frac{1}{v_{1}}}\\left(\\sum_{1\\leq j\\leq n}v_{j}z_{j}\\right)={\\frac{1}{v_{1}}}\\left(\\sum_{1<j<n}v_{j}z_{j}\\right)<{\\frac{v_{2}}{v_{1}}}\\left(\\sum_{1<j<n}z_{j}\\right)={\\frac{v_{2}}{v_{1}}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus if $\\begin{array}{r}{v_{2}<\\frac{v_{1}}{n}\\epsilon}\\end{array}$ , then $\\begin{array}{r}{U_{i}(x,z)<\\frac{\\epsilon}{n}}\\end{array}$ . ", "page_idx": 37}, {"type": "text", "text": "Taking $\\hat{\\rho}$ to be the recommendation probabilities induced by this pair $x,z$ , this implies that $U_{\\mathrm{min}}\\left(\\hat{\\rho}\\right)<$ $\\epsilon/n$ . ", "page_idx": 37}, {"type": "text", "text": "Item fairness constraints without mis-estimation. This becomes precisely the problem we solve in Theorem 3; if the two types occur in equal proportion in the mis-estimated group as well as the correctly estimated group, then $\\alpha=1/2$ . ", "page_idx": 37}, {"type": "text", "text": "If $\\alpha=1/2$ , then by Lemma 17 with $\\beta=1$ , $\\textstyle t={\\lfloor{\\frac{n+1}{2}}\\rfloor}$ , and we can show that ", "page_idx": 37}, {"type": "equation", "text": "$$\nL_{t}={\\left\\{\\begin{array}{l l}{R_{t}-{\\frac{1}{q_{t}}},}&{n{\\mathrm{~even}}}\\\\ {R_{t},}&{n{\\mathrm{odd}}}\\end{array}\\right.}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By Lemma 5 we know that ", "page_idx": 37}, {"type": "equation", "text": "$$\nI_{\\operatorname*{min}}^{*}=\\frac{1}{1+q_{t}L_{t}+(1-q_{t})R_{t}},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "so if $n$ is even, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\nI_{\\mathrm{min}}^{*}=\\frac{1}{1+q_{t}(R_{t}-\\frac{1}{q_{t}})+(1-q_{t})R_{t}}=\\frac{1}{R_{t}}=\\frac{1}{\\sum_{j>t}\\frac{1}{1-q_{j}}}>\\frac{1}{2(n-(n/2))}=\\frac{1}{n}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we use the fact from Lemma 16 that if $\\begin{array}{r}{j>\\frac{n+1}{2}}\\end{array}$ , then $q_{j}<\\textstyle{\\frac{1}{2}}$ . Likewise if $n$ is odd, ", "page_idx": 37}, {"type": "equation", "text": "$$\nI_{\\mathrm{min}}^{*}=\\frac{1}{1+q_{t}R_{t}+(1-q_{t})R_{t}}=\\frac{1}{1+R_{t}}=\\frac{1}{1+\\sum_{j>t}\\frac{1}{1-q_{j}}}>\\frac{1}{1+2(n-(n+1)/2)}=\\frac{1}{n}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then a user $i$ of type 1 will have normalized utility will be ", "page_idx": 37}, {"type": "equation", "text": "$$\nU_{i}(x,y)\\geq\\frac{x_{1}v_{1}}{v_{1}}=\\frac{I_{\\operatorname*{min}}^{*}}{q_{1}}=I_{\\operatorname*{min}}^{*}\\frac{v_{1}+v_{n}}{v_{n}}\\geq I_{\\operatorname*{min}}^{*}>\\frac{1}{n}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Similarly a user of type 2 will have utility ", "page_idx": 37}, {"type": "equation", "text": "$$\nU_{i}(x,y)\\geq\\frac{y_{n}v_{1}}{v_{1}}=\\frac{I_{\\operatorname*{min}}^{*}}{1-q_{n}}=I_{\\operatorname*{min}}^{*}\\,\\frac{v_{1}+v_{n}}{v_{n}}\\geq I_{\\operatorname*{min}}^{*}>\\frac{1}{n}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This means that $U_{\\operatorname*{min}}^{*}\\left(1\\right)>1/n$ . ", "page_idx": 37}, {"type": "text", "text": "Price of estimation with item fairness constraints. The two arguments above imply that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{U}^{M}(1,w,\\hat{w})=1-\\displaystyle\\frac{U_{\\mathrm{min}}\\left(\\hat{\\rho},w\\right)}{U_{\\mathrm{min}}^{*}\\left(1,w\\right)}}\\\\ &{\\qquad\\qquad\\qquad>1-\\displaystyle\\frac{\\epsilon/n}{1/n}}\\\\ &{\\qquad\\qquad\\qquad=1-\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Price of estimation without item fairness constraints. In this case an optimal recommendation policy may choose each user\u2019s policy individually without regard to item utilities. Given the misestimated values, it is optimal to give mis-estimated users the item $j^{*}$ or $n-j^{*}+1$ that maximizes $\\textstyle{\\frac{v_{j}+v_{n-j+1}}{2}}$ deterministically. It is thus also optimal to recommend the item $j^{*}$ and $n-j^{*}+1$ each with probability $1/2$ . This yields expected value ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j}\\frac{v_{j}+v_{n-j+1}}{2}\\geq\\frac{v_{1}+v_{n}}{2}>\\frac{v_{1}}{2},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and thus a mis-estimated user $i$ will receive a normalized value of $U_{i}(\\rho^{\\prime})=1/2$ . ", "page_idx": 38}, {"type": "text", "text": "The optimal utility of each user if their preferences were correctly estimated is 1, so the price of misestimation is $1/2$ . \u53e3 ", "page_idx": 38}, {"type": "text", "text": "E.2 Supporting lemma proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "First, we show a fact that will be helpful later on. ", "page_idx": 38}, {"type": "text", "text": "Lemma 16. If $\\begin{array}{r}{j<\\frac{n+1}{2}}\\end{array}$ , then $\\begin{array}{r}{q_{j}>1/2;\\,i f\\,j>\\frac{n+1}{2},}\\end{array}$ , then $q_{j}<1/2.$ . If $\\textstyle j={\\frac{n+1}{2}},q_{j}=1/2$ . ", "page_idx": 38}, {"type": "text", "text": "Proof. If $\\begin{array}{r}{j<\\frac{n+1}{2}}\\end{array}$ , then $v_{j}>v_{n-j+1}$ and ", "page_idx": 38}, {"type": "equation", "text": "$$\nq_{j}=\\frac{v_{j}}{v_{j}+v_{n-j+1}}=\\frac{1}{1+\\frac{v_{n-j+1}}{v_{j}}}>\\frac{1}{1+1}=\\frac{1}{2};\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "if $\\begin{array}{r}{j<\\frac{n+1}{2}}\\end{array}$ , then $v_{j}<v_{n-j+1}$ and a symmetric argument holds. ", "page_idx": 38}, {"type": "text", "text": "In the remainder of this section, we will use $\\operatorname{rev}(x)$ to denote the reverse of a vector $x$ , that is, $\\operatorname{rev}(x)_{j}=x_{n-j+1}$ for all $j$ . ", "page_idx": 38}, {"type": "text", "text": "Lemma 12. If a policy $\\rho=(x,y,z)\\in S_{\\mathrm{symm}}$ solves $U_{\\mathrm{min}}\\left(\\rho\\right)=U_{\\mathrm{min}}^{*}\\left(1\\right)\\!,$ , then there is some policy $\\rho^{\\prime}=x^{\\prime},z^{\\prime}\\in\\partial^{\\prime}$ that solves $\\dot{U}_{\\mathrm{min}}\\left(\\rho^{\\prime}\\right)=\\dot{U}_{\\mathrm{min}}^{*}\\left(1\\right.$ ). ", "page_idx": 38}, {"type": "text", "text": "Proof. Suppose $(x,y,z):=\\rho$ is an optimal solution to $U_{\\mathrm{min}}^{*}\\left(1\\right)$ ", "page_idx": 38}, {"type": "text", "text": "First, if $U_{\\mathrm{min}}^{*}\\left(1\\right)=U_{i}(\\rho)$ for some $i$ in the first or second type of user ", "page_idx": 38}, {"type": "text", "text": "Then clearly $\\rho^{\\prime}\\in S^{\\prime}$ , and $\\begin{array}{r}{\\sum_{j}x_{j}^{\\prime}=\\sum_{j}y_{j}^{\\prime}=\\sum_{j}z_{j}^{\\prime}=1,\\,0\\leq x_{j}^{\\prime},y_{j}^{\\prime},z_{j}^{\\prime}\\leq1\\mathrm{\\;for\\;all\\;}j.}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "Furthermore, for users $i,i^{\\prime}$ of the first and second types respectively with correctly estimated types, ", "page_idx": 38}, {"type": "equation", "text": "$$\nU_{i}(\\rho^{\\prime})=\\sum_{j=1}^{n}x_{j}^{\\prime}v_{j}=\\sum_{j=1}^{n}\\frac12(x_{j}+y_{n-j+1})v_{j}=\\frac{U_{i}(\\rho)+U_{i^{\\prime}}(\\rho)}{2}\\geq\\operatorname*{min}\\{U_{i}(\\rho),U_{i^{\\prime}}(\\rho)\\}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and by an analogous argument, ", "page_idx": 38}, {"type": "equation", "text": "$$\nU_{i^{\\prime}}\\geq\\operatorname*{min}\\{U_{i}(\\rho),U_{i^{\\prime}}(\\rho)\\}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Moreover, the estimated normalized utility of a mis-estimated user $i$ , will be ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{U_{i}(\\rho^{\\prime})=\\displaystyle\\sum_{j=1}^{n}z_{j}^{\\prime}\\frac{v_{j}+v_{n-j+1}}{2}}}\\\\ {{\\qquad=\\displaystyle\\frac{1}{2}\\sum_{j=1}^{n}z_{j}\\frac{v_{j}+v_{n-j+1}}{2}+\\frac{1}{2}\\sum_{j=1}^{n}z_{n-j+1}\\frac{v_{j}+v_{n-j+1}}{2}}}\\\\ {{\\qquad=\\displaystyle\\frac{1}{2}(U_{i}(\\rho^{\\prime})+U_{i}(\\rho^{\\prime}))}}\\\\ {{\\qquad=U_{i}(\\rho^{\\prime}){}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "So $\\begin{array}{r}{\\operatorname*{min}_{i}U_{i}(\\rho^{\\prime})\\ge\\operatorname*{min}_{i}U_{i}(\\rho).}\\end{array}$ . ", "page_idx": 39}, {"type": "text", "text": "Finally, for any $j$ , we can expand the definition of $I_{j}$ to see that ", "page_idx": 39}, {"type": "equation", "text": "$$\nI_{j}(\\rho^{\\prime})=\\frac12(I_{j}(\\rho)+I_{n-j+1}(\\rho))=I_{\\mathrm{min}}^{*}\\,.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus $\\rho^{\\prime}$ is also an optimal solution to Problem 1. ", "page_idx": 39}, {"type": "text", "text": "Lemma 13. Let $x,z$ be an optimal basic feasible solution to the linear program Problem 11. There is some $j\\leq\\textstyle{\\frac{n+1}{2}}$ such that for all $j^{\\prime}>j$ , $x_{j^{\\prime}}=0$ and for all $j^{\\prime}<j$ , $z_{j^{\\prime}}=z_{n-j^{\\prime}+1}=0$ . Let the pivot index $t$ denote the minimum such $j$ . ", "page_idx": 39}, {"type": "text", "text": "Proof. Part 1. While it is tempting attempt to directly apply the sparsity result in Proposition 2 here, we will need a slightly stronger result to ensure that there are no $j$ such that $x_{j}=z_{j}=0$ . First, we must show that $x_{j}>0$ and $y_{j}>0$ are almost mutually exclusive. ", "page_idx": 39}, {"type": "text", "text": "Lemma 17. Let $(x,z)$ be an optimal solution to Problem 11. For $\\textstyle j>{\\frac{n+1}{2}}$ , $x_{j}=0$ . ", "page_idx": 39}, {"type": "text", "text": "In particular, if we let $\\textstyle h=\\lfloor{\\frac{n+1}{2}}\\rfloor$ indicate the index halfway through the set of items, this means that we can simplify the problem above to ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\arg_{\\underset{z_{1},\\ldots,z_{h},0}{\\operatorname*{max}}}}&{\\lambda}\\\\ {\\displaystyle\\mathrm{subject~to}}&{I_{j}(x,z)=\\lambda,1\\leq j\\leq h}\\\\ &{\\displaystyle\\sum_{j\\leq h}x_{j}=\\sum_{j\\leq h}z_{j}+\\sum_{h<j\\leq n}z_{n-j+1}=1}\\\\ &{x_{j},z_{j}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Now this is a linear program with $2h\\!+\\!1$ variables and $h\\!+\\!2$ constraints, so by an argument analogous to Problem 12, we find that every basic feasible solution $x_{1:h},z_{1:h}$ must share $h-1$ zeros. This means that there is a single index $t$ such that $x_{t}>0,z_{t}>0$ ; if there were more than one, by the pigeonhole principle there must be some $t^{\\prime}\\leq h$ such that $x_{t^{\\prime}}=z_{t^{\\prime}}=x_{n-t^{\\prime}+1}=0$ , and $I_{t^{\\prime}}(x,z)=0$ . Then ${\\bar{I}}_{\\operatorname*{min}}\\left({\\bar{x}},z\\right)=0$ ; however, by Lemma 1, $I_{\\operatorname*{min}}\\left(x,z\\right)=I_{\\operatorname*{min}}^{*}>0$ , so we have a contradiction. ", "page_idx": 39}, {"type": "text", "text": "In terms of $x,z$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{j}(x,z)=\\frac{\\beta v_{j}x_{j}+\\beta v_{n-j+1}x_{n-j+1}+\\left(1-2\\beta\\right)\\frac{v_{j}+v_{n-j+1}}{2}z_{j}}{\\beta v_{j}+\\beta v_{n-j+1}+\\left(1-2\\beta\\right)\\frac{v_{j}+v_{n-j+1}}{2}}}\\\\ &{\\qquad=\\frac{\\beta v_{j}x_{j}+\\beta v_{n-j+1}x_{n-j+1}+\\left(1-2\\beta\\right)\\frac{v_{j}+v_{n-j+1}}{2}z_{j}}{\\frac{v_{j}+v_{n-j+1}}{2}}}\\\\ &{\\qquad=2\\beta\\left(\\frac{v_{j}}{v_{j}+v_{n-j+1}}x_{j}+\\frac{v_{n-j+1}}{v_{j}+v_{n-j+1}}x_{n-j+1}\\right)+(1-2\\beta)z_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Let qj =vj+vn\u2212j+1 . Note that $q_{j}$ does not depend on $\\beta$ , and $q_{j}$ is decreasing in $j$ . Then ", "page_idx": 40}, {"type": "equation", "text": "$$\nI_{j}(x,y,z)=2\\beta\\left(q_{j}x_{j}+(1-q_{j})y_{j}\\right)+(1-2\\beta)z_{j}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Part 2. Moreover, the set of $j$ such that $x_{j}=0$ is contiguous; similarly for $z_{j}$ . ", "page_idx": 40}, {"type": "text", "text": "We prove this by contradiction, showing that if this set is not contiguous, we can move around probability mass until we achieve greater than optimal item fairness. Let $i<j\\le h$ , and let $x,z$ be an optimal solution to Problem 12. We need to show that if $x_{i}=0$ then $x_{j}=0$ , and if $z_{i}>0$ , then $z_{j}>0$ . ", "page_idx": 40}, {"type": "text", "text": "Suppose first that $x_{i}=0$ and $x_{j}>0$ . Notice that since $x,z$ are optimal, ", "page_idx": 40}, {"type": "equation", "text": "$$\n(1-2\\beta)z_{i}=I_{i}(x,z)=I_{j}(x,z)=2\\beta q_{j}x_{j}+(1-2\\beta)z_{j}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Define ", "page_idx": 40}, {"type": "equation", "text": "$$\nx_{\\ell}^{\\prime}=\\left\\{{\\!x_{j}-\\epsilon\\!,}\\atop{x_{i}+\\frac{\\epsilon}{h-1}},\\begin{array}{l}{{\\ell=i}}\\\\ {{\\ell=j}}\\\\ {{x_{\\ell}+\\frac{\\epsilon}{h-1}},}\\end{array}}\\right.,\\quad z_{\\ell}^{\\prime}=\\left\\{{z_{j},\\atop z_{\\ell}}\\right.,\\begin{array}{l}{{\\ell=i}}\\\\ {{\\ell=j}}\\\\ {{z_{\\ell},}}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for $\\epsilon$ small enough that $x^{\\prime}\\in\\Delta_{h-1}$ . Now for all $\\ell\\,\\notin\\,\\{i,j\\},\\,I_{\\ell}(x^{\\prime},z^{\\prime})\\,>\\,I_{\\ell}(x,z)$ , since we have increased the probability mass on $x_{\\ell}$ while leaving $z_{\\ell}$ unchanged. Furthermore, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{i}(x^{\\prime},z^{\\prime})=2\\beta q_{i}\\left(x_{j}-\\epsilon\\right)+(1-2\\beta)z_{j}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ =2\\beta(q_{i}-q_{j})x_{j}+2\\beta q_{j}x_{j}+(1-2\\beta)z_{j}-2\\beta q_{i}\\epsilon}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ =2\\beta(q_{i}-q_{j})x_{j}+(1-2\\beta)z_{i}-2\\beta q_{i}\\epsilon}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ =I_{i}(x,z)+2\\beta(q_{i}-q_{j})x_{j}-2\\beta q_{i}\\epsilon}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ >I_{i}(x,z)}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "as long as $q_{i}\\epsilon<(q_{i}-q_{j})x_{j}$ , which we can ensure by taking $\\epsilon$ small enough, since $q_{i}>q_{j}$ . Moreover, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{j}(x^{\\prime},z^{\\prime})=2\\beta q_{j}\\left(x_{i}+\\cfrac{\\epsilon}{h-1}\\right)+(1-2\\beta)z_{i}}\\\\ &{\\phantom{\\frac{1}{1}}=2\\beta q_{j}\\left(x_{i}+\\cfrac{\\epsilon}{h-1}\\right)+2\\beta q_{j}x_{j}+(1-2\\beta)z_{j}}\\\\ &{\\phantom{\\frac{1}{1}}>2\\beta q_{j}x_{j}+(1-2\\beta)z_{j}}\\\\ &{=I_{j}(x,z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus ", "page_idx": 40}, {"type": "equation", "text": "$$\nI_{\\operatorname*{min}}\\left(x^{\\prime},z^{\\prime}\\right)=\\operatorname*{min}_{\\ell}I_{\\ell}(x^{\\prime},z^{\\prime})>\\operatorname*{min}_{\\ell}I_{\\ell}(x,z)=I_{\\operatorname*{min}}\\left(x,z\\right)=I_{\\operatorname*{min}}^{*}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and we have a contradiction. ", "page_idx": 40}, {"type": "text", "text": "Similarly, suppose that $z_{i}>0$ and $z_{j}=0$ . Then since $x,z$ is optimal, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n2\\beta q_{i}x_{i}+(1-2\\beta)z_{i}=I_{i}(x,z)=I_{j}(x,z)=2\\beta q_{j}x_{j}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Define ", "page_idx": 40}, {"type": "equation", "text": "$$\nz_{\\ell}^{\\prime}=\\left\\{{z_{i}-\\epsilon,\\atop z_{j}+\\frac{\\epsilon}{h-1}},\\begin{array}{l}{\\ell=j}\\\\ {\\ell=i}\\\\ {z_{\\ell}+\\frac{\\epsilon}{h-1},}\\end{array}}\\right.,\\quad x_{\\ell}^{\\prime}=\\left\\{\\begin{array}{l l}{x_{j}-\\delta,}&{\\ell=j}\\\\ {x_{i}+\\delta,}&{\\ell=i}\\\\ {x_{\\ell},}&{\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "$\\begin{array}{r}{\\delta=\\frac{1-2\\beta}{2\\beta q_{i}}z_{i}>0}\\end{array}$ and taking $\\epsilon$ small enough that $z^{\\prime}\\in\\Delta_{h-1}$ and $\\epsilon<z_{i}(1-q_{j}/q_{i})$ . This implies that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\delta=\\frac{1-2\\beta}{2\\beta q_{i}}z_{i}<\\frac{(1-2\\beta)(z_{i}-\\epsilon)}{2\\beta q_{j}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{i}+\\delta=x_{i}+\\frac{1-2\\beta}{2\\beta q_{i}}z_{i}}\\\\ &{\\phantom{x x}=\\frac{1}{2\\beta q_{i}}(2\\beta q_{i}x_{i}+(1-2\\beta)z_{i})}\\\\ &{\\phantom{x x}=\\frac{1}{2\\beta q_{i}}2\\beta q_{i}x_{i}+(1-2\\beta)z_{i})}\\\\ &{\\phantom{x x}=\\frac{1}{2\\beta q_{i}}2\\beta q_{j}x_{j}}\\\\ &{\\phantom{x x}=\\frac{q_{j}}{q_{i}}x_{j}}\\\\ &{\\phantom{x x}<x_{j}}\\\\ &{\\phantom{x x}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{j}-\\delta>x_{j}-\\frac{(1-2\\beta)\\left(z_{i}-\\epsilon\\right)}{2\\beta q_{j}}}\\\\ &{\\phantom{x x}=\\frac{1}{2\\beta q_{j}}(2\\beta q_{j}x_{j}-(1-2\\beta)(z_{i}-\\epsilon))}\\\\ &{\\phantom{x x}>\\frac{1}{2\\beta q_{j}}(2\\beta q_{j}x_{j}-(1-2\\beta)z_{i})}\\\\ &{\\phantom{x x}=\\frac{1}{2\\beta q_{j}}(2\\beta q_{j}x_{j}-(2\\beta q_{j}x_{j}-2\\beta q_{i}x_{i}))}\\\\ &{\\phantom{x x}=\\frac{1}{2\\beta q_{j}}(2\\beta q_{j}x_{j}-(2\\beta q_{j}x_{j}-2\\beta q_{i}x_{i}))}\\\\ &{\\phantom{x x}=\\frac{1}{2\\beta q_{j}}2\\beta q_{i}x_{i}}\\\\ &{\\phantom{x x}>0}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and $x_{i}+\\delta>x_{i}\\geq0,x_{j}-\\delta<x_{j}\\leq1$ , so $x^{\\prime}\\in\\Delta_{h-1}$ . ", "page_idx": 41}, {"type": "text", "text": "Now as above $I_{\\ell}(x^{\\prime},z^{\\prime})>I_{\\mathrm{min}}\\left(x,z\\right)$ for all $\\ell\\notin\\{i,j\\}$ , since there is strictly more probability mass on $z_{\\ell}$ while $x_{\\ell}$ remains the same. Furthermore, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{i}(x^{\\prime},z^{\\prime})-I_{i}(x,z)=2\\beta q_{i}(x_{i}+\\delta)+(1-2\\beta)(z_{j}+\\displaystyle\\frac{\\epsilon}{h-1})-(2\\beta q_{i}x_{i}+(1-2\\beta)z_{i})}\\\\ &{\\phantom{I_{i}(x^{\\prime},z^{\\prime})-}>2\\beta q_{i}(x_{i}+\\delta)-(2\\beta q_{i}x_{i}+(1-2\\beta)z_{i})}\\\\ &{\\phantom{I_{i}(x^{\\prime},z^{\\prime})-}=2\\beta q_{i}\\delta-(1-2\\beta)z_{i}}\\\\ &{\\phantom{I_{i}(x^{\\prime},z^{\\prime})-}=2\\beta q_{i}\\displaystyle\\frac{(1-2\\delta)}{2\\beta q_{i}}z_{i}-(1-2\\beta)z_{i}}\\\\ &{\\phantom{I_{i}(x^{\\prime},z^{\\prime})-}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{j}(x^{\\prime},z^{\\prime})-I_{j}(x,z)=2\\beta q_{j}(x_{j}-\\delta)+(1-2\\beta)(z_{i}-\\epsilon)-2\\beta q_{j}x_{j}}\\\\ &{\\hphantom{I_{j}(x^{\\prime},z^{\\prime})-I_{j}(x,z)=2}=(1-2\\beta)(z_{i}-\\epsilon)-2\\beta q_{j}\\delta}\\\\ &{\\hphantom{I_{j}(x^{\\prime},z^{\\prime})-I_{j}(x,z)=2\\beta q_{j}\\delta}>(1-2\\beta)(z_{i}-\\epsilon)}\\\\ &{\\hphantom{I_{j}(x^{\\prime},z^{\\prime})-I_{j}(x,z)=2\\beta q_{j}\\delta}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then ", "page_idx": 41}, {"type": "equation", "text": "$$\nI_{\\operatorname*{min}}\\left(x^{\\prime},z^{\\prime}\\right)=\\operatorname*{min}_{\\ell}I_{\\ell}(x^{\\prime},z^{\\prime})>\\operatorname*{min}_{\\ell}I_{\\ell}(x,z)=I_{\\operatorname*{min}}\\left(x,z\\right)=I_{\\operatorname*{min}}^{*}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and we have found $x^{\\prime},z^{\\prime}$ achieving a higher-than-optimal $I_{\\mathrm{min}}$ , which is a contradiction. ", "page_idx": 41}, {"type": "text", "text": "Lemma 17. Let $(x,z)$ be an optimal solution to Problem 11. For $\\begin{array}{r}{j>\\frac{n+1}{2}}\\end{array}$ , $x_{j}=0$ ", "page_idx": 41}, {"type": "text", "text": "Proof. We prove this by contradiction. Suppose that for $\\textstyle j>{\\frac{n+1}{2}}$ n2+ 1, xj > 0. Then define x\u2032 such that ", "page_idx": 41}, {"type": "equation", "text": "$$\nx_{\\ell}^{\\prime}={\\binom{0,}{x_{n-j+1}+(x_{j}-\\epsilon)}}\\quad\\ell=j}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $0<\\epsilon<(2q_{n-j+1}-1)x_{j}$ . Since $\\begin{array}{r}{j>\\frac{n+1}{2}}\\end{array}$ n2+ 1, qn\u2212j+1 > 1/2 by Lemma 16, so such \u03f5 exists. Then for all $\\ell\\notin\\{j,n-j+1\\}$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{\\ell}(x^{\\prime},z)=2\\beta\\left(q_{\\ell}x_{\\ell}^{\\prime}+(1-q_{\\ell})x_{n-\\ell+1}^{\\prime}\\right)+(1-2\\beta)z_{\\ell}}\\\\ &{\\qquad\\qquad>2\\beta\\left(q_{\\ell}x_{\\ell}+(1-q_{\\ell})x_{n-\\ell+1}\\right)+(1-2\\beta)z_{\\ell}}\\\\ &{\\qquad\\qquad=I_{\\ell}(x,z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Since $\\epsilon<(2q_{n-j+1}-1)x_{j}$ , we can rearrange to see that $(1-q_{n-j+1})x_{j}<q_{n-j+1}(x_{j}-\\epsilon)$ . This implies that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{n-j+1}(x^{\\prime},z)=2\\beta\\left(q_{n-j+1}(x_{n-j+1}+(x_{j}-\\epsilon))+q_{j}\\cdot0\\right)+(1-2\\beta)z_{n-j+1}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=2\\beta\\left(q_{n-j+1}x_{n-j+1}+q_{n-j+1}(x_{j}-\\epsilon)\\right)+(1-2\\beta)z_{n-j+1}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{I_{j}(x^{\\prime},z)=2\\beta\\left(q_{j}\\cdot0+(1-q_{j})\\cdot(x_{n-j+1}+(x_{j}-\\epsilon))\\right)+(1-2\\beta)z_{j}}&{}\\\\ {2\\beta\\left(q_{n-j+1}x_{n-j+1}+q_{n-j+1}(x_{j}-\\epsilon)\\right)+(1-2\\beta)z_{j}}&{(q_{n-j+1}=1-q_{j})}\\\\ {>2\\beta\\left(q_{n-j+1}x_{n-j+1}+(1-q_{n-j+1})x_{j}\\right)+(1-2\\beta)z_{j}}&{}\\\\ {=2\\beta\\left((1-q_{j})x_{n-j+1}+q_{j}x_{j}\\right)+(1-2\\beta)z_{j}}&{(q_{n-j+1}=1-q_{j})}\\\\ {=I_{j}(x,z)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus we have found a solution $x^{\\prime},z$ that satisfies ", "page_idx": 42}, {"type": "equation", "text": "$$\nI_{\\operatorname*{min}}\\left(x^{\\prime},z\\right)=\\operatorname*{min}_{j}I_{j}(x^{\\prime},z)>\\operatorname*{min}_{j}I_{j}(x,z)=I_{\\operatorname*{min}}\\left(x,z\\right)=I_{\\operatorname*{min}}^{*}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and we have a contradiction. ", "page_idx": 42}, {"type": "text", "text": "Lemma 14. Problem $_{l l}$ has a unique optimal solution. ", "page_idx": 42}, {"type": "text", "text": "Proof. Suppose $(x,z)$ and $(x^{\\prime},z^{\\prime})$ are both optimal basic feasible solutions to Problem 12. Let $t$ be the pivot index of $(x,z)$ and $t^{\\prime}$ be the pivot index of $(x^{\\prime},z^{\\prime})$ , and suppose without loss of generality that $t\\le t^{\\prime}$ . ", "page_idx": 42}, {"type": "text", "text": "First, if $t<t^{\\prime}$ , then for all $j<t$ , $z_{j}=z_{j}^{\\prime}=0$ , and ", "page_idx": 42}, {"type": "equation", "text": "$$\n2\\beta q_{j}x_{j}=I_{j}(x,z)=I_{\\mathrm{min}}\\left(x,z\\right)=I_{\\mathrm{min}}\\left(x^{\\prime},z^{\\prime}\\right)=I_{j}(x^{\\prime},z^{\\prime})=2\\beta q_{j}x_{j}^{\\prime}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "so $x_{j}=x_{j}^{\\prime}$ . If $j>t,x_{j}=0$ . For $j=t,\\,z_{t}^{\\prime}=0$ and ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\beta q_{t}x_{t}+(1-2\\beta)z_{t}=2\\beta q_{t}x_{t}^{\\prime}}\\\\ &{\\implies2\\beta q_{t}\\left(1-\\displaystyle\\sum_{j<t}x_{j}\\right)+(1-2\\beta)z_{t}=2\\beta q_{t}x_{t}^{\\prime}}\\\\ &{\\implies2\\beta q_{t}\\left(1-\\displaystyle\\sum_{j<t}x_{j}^{\\prime}\\right)+(1-2\\beta)z_{t}=2\\beta q_{t}x_{t}^{\\prime}}\\\\ &{\\implies(1-2\\beta)z_{t}=2\\beta q_{t}\\left(\\displaystyle\\sum_{j\\leq t}x_{j}^{\\prime}-1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "But $(1-2\\beta)z_{t}\\geq0$ , so $\\textstyle\\sum_{j\\leq t}x_{j}^{\\prime}=1$ and for $j>t,x_{j}^{\\prime}=0$ . This implies that $t^{\\prime}\\leq t$ , which is a contradiction, so it is not possible for $t^{\\prime}>t$ . ", "page_idx": 42}, {"type": "text", "text": "Now, if $t=t^{\\prime}$ , then again for all $j<t$ , $z_{j}=z_{j}^{\\prime}=0$ , and $x_{j}=x_{j}^{\\prime}$ . For $t<j\\le h$ , $x_{j}=x_{j}^{\\prime}=0$ and if $t<h$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n(1-2\\beta)z_{j}=I_{j}(x,z)=I_{\\mathrm{min}}\\left(x,z\\right)=I_{\\mathrm{min}}\\left(x^{\\prime},z^{\\prime}\\right)=I_{j}(x^{\\prime},z^{\\prime})=(1-2\\beta)z_{j}^{\\prime}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "so $z_{j}=z_{j}^{\\prime}$ . Finally, ", "page_idx": 43}, {"type": "equation", "text": "$$\nx_{t}=1-\\sum_{j<t}x_{j}=1-\\sum_{j<t}x_{j}^{\\prime}=x_{t}^{\\prime}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and ", "page_idx": 43}, {"type": "equation", "text": "$$\nz_{t}=\\left\\{\\begin{array}{l l}{\\frac{1}{2}(1-2\\sum_{t<j\\leq h}z_{j})=\\frac{1}{2}(1-2\\sum_{t<j\\leq h}z_{j}^{\\prime})=z_{t}^{\\prime},}&{n\\mathrm{~even}}\\\\ {\\frac{1}{2}(1-2\\sum_{t<j<h}z_{j}-z_{h})=\\frac{1}{2}(1-2\\sum_{t<j<h}z_{j}^{\\prime}-z_{h})=z_{t}^{\\prime},}&{n\\mathrm{~odd}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "If $t=h$ , then $z_{t}=1=z_{t}^{\\prime}$ . Thus $\\left(x,z\\right)=\\left(x^{\\prime},z^{\\prime}\\right)$ . ", "page_idx": 43}, {"type": "text", "text": "Thus there is only one optimal basic feasible solution to Problem 12, and thus only one optimal solution by the fundamental theorem of linear programming. ", "page_idx": 43}, {"type": "text", "text": "Lemma 15. Let $(x,z)$ be the optimal solution to Problem $^{12}$ , and let t be the pivot element of $(x,z)$ ; suppose that $\\textstyle t\\neq{\\frac{n+1}{2}}$ . Define ", "page_idx": 43}, {"type": "equation", "text": "$$\nL_{t}:=\\sum_{j<t}\\frac{1}{q_{j}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\lambda=\\frac{2\\beta q_{t}+1/2(1-2\\beta)}{1+q_{t}L_{t}+1/2(n-2t)},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "equation", "text": "$$\nz_{j}=\\left\\{\\frac{0,}{\\frac{1}{2}\\left(1-(n-2t)\\frac{\\lambda}{1-2\\beta}\\right),}\\right.\\ \\ j\\in\\left\\{t,n-t+1\\right\\}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Finally, ", "page_idx": 43}, {"type": "equation", "text": "$$\nx_{j}=\\left\\{\\begin{array}{l l}{\\frac{\\lambda}{2\\beta q_{j}},}&{j<t}\\\\ {1-\\frac{\\lambda}{2\\beta}L_{t},}&{j=t\\;.}\\\\ {0,}&{j>t}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "$\\textstyle I f t={\\frac{n+1}{2}}$ , then $x$ remains the same, but $z_{t}=1$ and ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\lambda=\\frac{2\\beta q_{t}+(1-2\\beta)}{1+q_{t}L_{t}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. For all j < t, \u03bb = 2\u03b2qjxj, so xj =2\u03b2\u03bbq . Moreover, ", "page_idx": 43}, {"type": "equation", "text": "$$\nx_{t}=1-\\sum_{j<t}x_{j}=1-\\sum_{j<t}\\frac{\\lambda}{2\\beta q_{j}}=1-\\frac{\\lambda}{2\\beta}L_{t}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Suppose that $n$ is odd and that $\\begin{array}{r}{t=h=\\frac{n+1}{2}}\\end{array}$ . Then $z_{t}=z_{h}$ must be 1 since for all $j<h\\ z_{j}=0$ . Moreover, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\lambda=2\\beta q_{t}x_{t}+(1-2\\beta)z_{t}=2\\beta q_{t}\\left(1-\\frac{\\lambda}{2\\beta}L_{t}\\right)+(1-2\\beta)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "so, rearranging, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\lambda=\\frac{2\\beta q_{t}+(1-2\\beta)}{1+q_{t}L_{t}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Otherwise, $t<j\\leq h,\\lambda=(1-2\\beta)z_{j}$ , s $\\begin{array}{r}{\\sigma\\,z_{j}=\\frac{\\lambda}{1-2\\beta}}\\end{array}$ . Then since $\\begin{array}{r}{\\sum_{j\\leq h}z_{j}+\\sum_{h<j\\leq n}z_{n-j+1}=1,}\\end{array}$ , ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{z_{t}=\\displaystyle\\frac{1}{2}\\left(1-\\sum_{j<t}z_{j}-\\sum_{t<j\\leq h}z_{j}-\\sum_{h<j<n-t+1}z_{n-j+1}-\\sum_{j>n-t+1}z_{n-j+1}\\right)}}\\\\ {{\\mathrm{~}=\\displaystyle\\frac{1}{2}\\left(1-\\sum_{t<j\\leq h}z_{j}-\\sum_{h<j<n-t+1}z_{n-j+1}\\right)}}\\\\ {{\\mathrm{~}=\\displaystyle\\frac{1}{2}\\left(1-(n-2t)\\frac{\\lambda}{1-2\\beta}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Moreover, ", "text_level": 1, "page_idx": 44}, {"type": "equation", "text": "$$\n\\lambda=2\\beta q_{t}x_{t}+(1-2\\beta)z_{t}=2\\beta q_{t}\\left(1-\\frac{\\lambda}{2\\beta}L_{t}\\right)+(1-2\\beta)\\frac{1}{2}\\left(1-(n-2t)\\frac{\\lambda}{1-2\\beta}\\right).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Rearranging, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\lambda(1+q_{t}L_{t}+1/2(n-2t))=2\\beta q_{t}+1/2(1-2\\beta),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "so ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\lambda=\\frac{2\\beta q_{t}+1/2(1-2\\beta)}{1+q_{t}L_{t}+1/2(n-2t)}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We believe the abstract and the introduction provide an accurate overview of the contents of this paper. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 45}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: We discuss limitations in Section 8. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 45}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: For each of our main theorems, we provide a detailed description of the modeling assumptions prior to the theorem. We provide complete proofs of each theorem in Appendices C, D, and E. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We provide our code and thoroughly detail our experimental setting. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: In the introduction, we provide a link to a GitHub repository containing the code we used to train our recommendation engine and run our experiments. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 47}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide a detailed description of the training and evaluation of our recommendation engine in Appendix B. We describe the training procedure in Appendix B.2, and the evaluation in Appendix B.3. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 47}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: In Appendix B we provide a thorough evaluation of our recommendation engine including; quantities are reported with $P$ -values and standard deviations where appropriate. We also show error bars on the experimental plots and discuss how they were obtained. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 47}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 48}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We provide this information in Appendix B.1. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 48}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 48}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We discuss this in Section 8; since our paper is investigating potential negative social impacts of technology, we anticipate a positive social impact. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 48}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 49}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 49}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We cite the models and datasets we use and provide license information in Appendix B. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We release a code to train and analyze the recommendation system that we develop, and provide documentation with the code. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 50}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 50}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}]