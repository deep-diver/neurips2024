{"importance": "This paper is crucial for researchers in AI safety and security, especially those working with open-source LLMs.  It **highlights a novel threat model**\u2014embedding space attacks\u2014that can bypass existing safety mechanisms more efficiently than previously known methods. The findings are relevant to current trends in adversarial machine learning and **open up new avenues for research** in LLM robustness and unlearning.", "summary": "Open-source LLMs are vulnerable to embedding space attacks, which efficiently bypass safety mechanisms and enable data extraction, even after unlearning.", "takeaways": ["Embedding space attacks are a novel and effective way to compromise the safety of open-source LLMs.", "These attacks can extract sensitive information from models, even after supposedly successful unlearning.", "The research identifies a critical gap in current AI safety research and opens up new research directions."], "tldr": "Current research focuses on discrete input manipulations to assess the robustness of Large Language Models (LLMs). However, this approach is insufficient for open-source models which offer complete access to their internal mechanisms. This paper introduces a new threat model: embedding space attacks which directly manipulate the continuous embedding representation of input tokens. These attacks bypass safety mechanisms and trigger harmful behaviors more efficiently than other existing methods. This approach raises serious concerns regarding the security and privacy of open-source LLMs. \nThe study demonstrates that embedding space attacks are highly effective in removing safety alignment in various open-source LLMs. Furthermore, it reveals the ability of embedding space attacks to extract seemingly deleted information during unlearning, demonstrating the limitations of current unlearning techniques. This threat model extends to data extraction, as the study shows that these attacks can recover pretraining data. **The findings highlight the vulnerability of open-source LLMs and emphasize the need for improved safety measures**.", "affiliation": "Technical University of Munich", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "CLxcLPfARc/podcast.wav"}