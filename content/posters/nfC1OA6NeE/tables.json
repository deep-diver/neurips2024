[{"figure_path": "nfC1OA6NeE/tables/tables_43_1.jpg", "caption": "Table 1: Scaling with the batch size: Effect of adapting AdamW hyperparameters.", "description": "This table presents the results of an experiment investigating the effect of scaling the batch size on the performance of the AdamW optimizer.  The base setting (indicated by '.') uses a batch size of 256. The subsequent rows show the results when the batch size is increased to 1024 (4 times larger).  Each row also displays how different hyperparameters (learning rate \u03b7, \u03b22, weight decay \u03bb) were adjusted following the proposed scaling rules from the paper. The final two columns report the test accuracy achieved on two different network architectures, VGG11 and ResNet34, for CIFAR-10 image classification. The results show marginal improvements in test accuracy when the hyperparameters are scaled according to the proposed rules, suggesting the validity of the theory.", "section": "F.8 Increasing weight decay with the batch size"}]