[{"type": "text", "text": "Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yanfang Ling Sun Yat-sen University lingyf3@mail2.sysu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Jiyong Li Sun Yat-sen University lijy373@mail2.sysu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Lingbo Li InfMind Technology Ltd lingbo@infmind.ai ", "page_idx": 0}, {"type": "text", "text": "Shangsong Liang\u2217 Sun Yat-sen University liangshangsong@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent methods are proposed to improve performance of domain adaptation by inferring domain index under an adversarial variational bayesian framework, where domain index is unavailable. However, existing methods typically assume that the global domain indices are sampled from a vanilla gaussian prior, overlooking the inherent structures among different domains. To address this challenge, we propose a Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing(GMDI) algorithm. GMDI employs a Gaussian Mixture Model for domain indices, with the number of component distributions in the \u201cdomain-themes\u201d space adaptively determined by a Chinese Restaurant Process. By dynamically adjusting the mixtures at the domain indices level, GMDI significantly improves domain adaptation performance. Our theoretical analysis demonstrates that GMDI achieves a more stringent evidence lower bound, closer to the log-likelihood. For classification, GMDI outperforms all approaches, and surpasses the state-of-the-art method, VDI, by up to $3.4\\%$ , reaching $99.3\\%$ . For regression, GMDI reduces MSE by up to $21\\%$ (from 3.160 to 2.493), achieving the lowest errors among all methods. Source code is publicly available from https://github.com/lingyf3/GMDI. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning models often suffer from performance degradation when applied to new domains that differ from their training domains, a phenomenon known as domain shift [21, 8, 28, 15]. Domain Adaptation (DA) [4, 44, 49, 42, 6, 1, 37, 32, 38, 13] seeks to mitigate this issue by producing domaininvariant features, thereby enhancing generalization from source to target domains [23, 19, 12, 39, 45]. ", "page_idx": 0}, {"type": "text", "text": "Recent research has explored the use of domain identity and domain index to improve domaininvariant data encoding and enhance domain adaptation performance [36, 40, 41]. Domain identity [41], a one-hot discrete variable vector, differentiates between domains, whereas domain index [41], a real-valued continuous variable vector, captures domain semantics. Due to the limited information in the discrete domain identity vector, research has increasingly focused on the domain index. Current approaches to incorporating domain index in domain adaptation include: (1) Directly using existing additional information in the dataset as the domain index [36, 40], which is impractical for datasets lacking such indices [22, 29], and (2) Treating the domain index as a latent variable to be inferred [26, 41]. However, these methods typically model the domain indices with a simple Gaussian distribution, limiting the domain indices space and thus hindering adaptation to diverse target domains, resulting in suboptimal performance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address the aforementioned issues, we propose a Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing (GMDI) algorithm. The proposed adversarial Bayesian algorithm assumes that domain indices follow a mixture of Gaussian distributions, with the number of mixture components dynamically determined by a Chinese Restaurant Process. As shown in Figure 1, a single Gaussian distribution struggles to adequately fti the domain indices, neglecting the inherent structures among different domains. This observation motivates us to model domain indices from different domains collectively as a Gaussian mixture distribution. To the best of our knowledge, we are the first to model domain indices as a mixture of Gaussian distributions to address the aforementioned challenges. Inspired by [3], the latent space of the mixture is defined as the \u201cdomain-themes\u201d space. The mixtures of distributions provide a higher level of flexibility in a larger latent space, thereby increasing the capability to adapt to various target domains with domain shift. Our theoretical analysis demonstrates that GMDI achieves a more rigorous evidence lower bound, and that maximizing this bound along with adversarial loss effectively infers optimal domain indices. Extensive experimental results validate the significant effectiveness of GMDI. ", "page_idx": 1}, {"type": "image", "img_path": "Grd7yzFm5V/tmp/b5a22f22d7c1a9d61d1b6b6fd539a00b8863f8e491c5bd6643ca45e8d2cd4dc1.jpg", "img_caption": ["Figure 1: Illustration of domain indices modeled by different distributions. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our key contributions are summarized as: (1) Our proposed GMDI is the first one to consider the entire distributions of domain indices in the \u201cdomain-themes\u201d space following a mixture of Gaussian distributions, and dynamically determining the number of components in the mixture with the Chinese Restaurant Process. (2) Our detailed theoretical analysis demonstrates that training with GMDI\u2019s superior evidence lower bound together with adversarial loss can yield optimal and more interpretable domain indices. (3) Extensive experiments on classification and regression tasks showcase the strong domain index modeling capability of GMDI, significantly outperforming the state-of-the-art. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Adversarial domain adaptation. There exists a substantial body of work on domain adaptation [4, 44, 49, 42, 6, 1, 37, 32, 38]. They focus on generating domain-invariant data encoding by aligning the distributions of source and target domains to adapt to target domains. This alignment is achieved by directly matching the statistics of distributions [25, 24] or by employing adversarial loss [33, 31], which encourages domain confusion through adversarial objective with a discriminator. Adversarial domain adaptation is widely used due to its integration with deep learning, strong theoretical foundation [7], and superior performance. Various different types of adversarial losses have been explored: [35] uses an inverted label GAN loss, [5] utilizes a minimax loss, and [34] employs a cross-entropy loss against the uniform distribution. Typically, the discriminators in these models rely on domain identity, which contains limited information, to align data encoding distributions. [20] and [10] also pay attention to domain identity. Our work, however, focuses on domain index, providing a more detailed representation of domains. ", "page_idx": 1}, {"type": "text", "text": "Domain adaptation related to domain indices. Recently, there has been growing interest in using continuous domain index, which contain richer and more interpretable information, to enhance domain adaptation performance. [36] use the rotation angle of images as the domain index for the Rotating MNIST dataset and patients\u2019 ages as the domain index for Healthcare Datasets. Their theoretical analysis demonstrates the value of utilizing domain indices to generate domain-invariant features. [40] employ graph node embeddings as domain indices to achieve domain adaptation in graph-relational domains. These methods assume that domain indices are available. However, in practice, domain indices are not always accessible [22, 29]. [26] generates features representing the similarity between different domains but do not formally define the domain index. [41] formally define the domain index and treat it as a latent variable to be inferred. Although [41] takes steps towards Bayesian approximation to parameter distributions, it only assumes a single domain index distribution, limiting its capability to adapt to diverse target domains effectively. In contrast, we address this issue by representing the domain index with a dynamically updated mixture model. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We aim at unsupervised domain adaptation: given $N$ domains with different domain shifts, each domain has a domain identity $w\\in\\mathcal{W}=[N]\\triangleq\\{1,...,N\\}$ , and each domain contains $D_{w}$ data points. Similar to the conventional unsupervised domain adaptation setting, the $N$ domains are divided into source domains with labeled data $\\mathcal{D}^{S}=\\{(\\pmb{x}_{i}^{s},y_{i}^{s},w_{i}^{\\bar{s}})\\}_{i=1}^{n_{s}}$ and target domains with unlabeled data $\\mathbf{\\mathcal{D}}^{T}=\\{(\\mathbf{\\boldsymbol{x}}_{i}^{t},\\boldsymbol{w}_{i}^{t})\\}_{i=1}^{n_{t}}$ . A foundational element that builds up our research problem is the diverse domain shifts [14] between different target domains and source domains. For source domains, the complexity of each target domain varies, which motivates us to dynamically infer domain indices in the \u201cdomain-themes\u201d space and model them with dynamic Gaussian Mixture Model. We aim to (1) predict the label $\\bar{\\{y_{i}^{t}\\}}_{i=1}^{n_{t}}$ of target domain data, and (2) infer local domain index $\\pmb{u}_{w}\\in\\mathbb{R}^{B_{u}}$ and global domain index $\\pmb{\\theta}_{w}\\in\\mathbb{R}^{B_{\\theta}}$ in the dynamic \u201cdomain-themes\u201d space. The summary of the notations is presented in Appendix $\\textbf{J}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Domain index. The domain index, distinct from domain identity $w$ , represents domain semantics, thus empowering it to significantly enhance domain adaptation performance. As per its definition [36, 41], the domain index satisfies the following : (1) To acquire domain-invariant data encoding $_{z}$ , the global domain index $\\pmb{\\theta}$ must remain independent of data encoding $_{z}$ , i.e., $\\pmb{\\theta}$ \u22a5\u22a5 $_{z}$ or equivalently $\\bar{p}(z\\mid\\theta)=p(z)$ . (2) Effectively representing data point $\\textbf{\\em x}$ while averting the occurrence of collapsing. (3) Ensuring optimal performance of downstream tasks utilizing the data encoding $_{z}$ learned by the encoder under the aforementioned constraints, and necessitating the maintenance of sensitivity to labels. ", "page_idx": 2}, {"type": "text", "text": "Variational domain index. In circumstances where the domain index may not be readily accessible, the Variational Domain Index (VDI) [41] is a Bayesian approach to infer the domain index $\\pmb{\\theta}$ and $\\textbf{\\em u}$ as latent variables. VDI factorizes the generative model $p(\\mathbf{\\boldsymbol{x}},y,\\mathbf{\\boldsymbol{u}},\\theta,z\\mid\\varepsilon)$ as: ", "page_idx": 2}, {"type": "equation", "text": "$$\np(x,y,u,\\theta,z\\mid\\varepsilon)=p(\\theta\\mid\\varepsilon)p(u\\mid\\theta)p(x\\mid u)p(z\\mid x,u,\\theta)p(y\\mid z)\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\varepsilon$ denotes the parameters for the prior probability distribution of the domain index $\\pmb{\\theta}$ . As shown in Equation 1, VDI stands capable of significantly enhancing domain adaptation proficiency by leveraging the inferred domain index for generating data encoding $_{z}$ . Note that the independence between the domain index $\\pmb{\\theta}$ and data encoding $_{z}$ , i.e., $p(z\\ \\mid\\ \\theta)\\;=\\;p(z)$ , does not contradict $p(z~\\vert~x,u,\\theta)$ , given the existence of multiple pathways between the domain index $\\pmb{\\theta}$ and data encoding $_{\\textit{z}}$ . Compared to VDI, which treats the distribution of domain index as a single Gaussian, we focus on a dynamic mixture of Gaussian distributions. ", "page_idx": 2}, {"type": "text", "text": "Chinese Restaurant Process(CRP). The Dirichlet Process (DP) is a classical method used for clustering. However, DP is difficult to construct directly, we apply the Chinese Restaurant Process(CRP) [16, 17, 18, 46] to implement it. Since similar domains have similar domain indices, the clusters formed by the domain indices correspond one-to-one with the components in mixture of Gaussian distributions. CRP can be employed to determine which cluster a domain belongs to (i.e., which component distribution domain index corresponds to). Specially, CRP is able to dynamically and adaptively determine the number of mixture components. CRP operates as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(v=k)=\\left\\{\\frac{n_{k}}{N-1+\\alpha}\\;\\mathrm{if}\\;\\mathrm{the}\\;\\mathrm{cluster}\\;k\\;\\mathrm{exists}\\,,\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $n_{k}$ is the number of domain contained in cluster $k$ , and parameter $\\alpha$ is the concentration parameter of the CRP. A larger $\\alpha$ implies a tendency to generate more domain clusters. ", "page_idx": 2}, {"type": "image", "img_path": "Grd7yzFm5V/tmp/1155785a4db60cb8a10a6dd00b409e5b531c72f964e7732127a7f06df1faa439.jpg", "img_caption": ["Figure 2: The schematic diagram of domain index distributions. It shows the inference of variational Gaussianshaped distributions for the global domain index, representing domain semantics. The process involves ranking candidate distributions in the \u201cdomain-themes\u201d space, selecting the highest probability one, and deriving the local domain index from it. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4 Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Overview of GMDI ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We propose GMDI in order to infer more interpretable domain indices and thereby improve domain adaptation performance. Our model is constructed in three steps: First, in generate process, we model global domain indices as a dynamic Gaussian mixture model, with local indices generated from global domain index. Second, in inference process, we build structured variational inference to approximate the posterior of the latent variables. Finally, we train the model using an evidence lower bound with robust theoretical guarantees and an adversarial loss. Under this framework, GMDI has several significant advantages: (1) With global domain indices following a dynamic mixture of Gaussian distributions adaptively determined by CRP, it provides a higher level of flexibility in a larger latent space. (2) The evidence lower bound of our GMDI is more stringent, leading to more interpretable and optimal domain indices. The overview of GMDI is presented in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "4.2 Mixture of domain index distributions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Similar to VDI, GMDI (Figure 3 (right)) also considers the intermediate latent variable of local domain index $\\textbf{\\em u}$ . The local domain index $\\textbf{\\em u}$ contains instance-level information, meaning that each data point has a unique local domain index. In contrast, the global domain index $\\pmb{\\theta}$ contains domainlevel information, indicating that all data points within the same domain share the same global domain index. In VDI, with the local domain index $\\textbf{\\em u}$ derived from the global domain index $\\pmb{\\theta}$ , the data distribution $p(y,x\\mid\\varepsilon)$ is expressed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(y,\\pmb{x}\\mid\\pmb{\\varepsilon})=\\int p(\\pmb{\\theta}\\mid\\pmb{\\varepsilon})p(\\pmb{u}\\mid\\pmb{\\theta})p(\\pmb{x}\\mid\\pmb{u})p(\\pmb{z}\\mid\\pmb{x},\\pmb{u},\\pmb{\\theta})p(y\\mid\\pmb{z})\\,d\\pmb{z}d\\pmb{u}d\\pmb{\\theta}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\varepsilon$ denotes the parameters for the prior probability distribution of the global domain index $\\pmb{\\theta}$ . ", "page_idx": 3}, {"type": "text", "text": "With the setting of unsupervised domain adaptation, domains are not i.i.d, existing domain shift between domains. It implies that there may be substantial differences in distribution between domains. This leads to a problem that disparate target domains require a more significant degree of adaptation. Although we compute a distribution for domain index $\\pmb{\\theta}$ to enhance the capability of domain adaptation, it may not be effective enough to aid in adapting to a diversity of different target domains. Therefore, if local domain index $\\textbf{\\em u}$ are adapted from a simple Gaussian distribution of global domain index $\\pmb{\\theta}$ , it may play a small role in improving the performance of domain adaptation. ", "page_idx": 3}, {"type": "text", "text": "To better model global domain index and thus enhance the effectiveness of domain adaptation, we propose to maintain a mixture of dynamically updated global domain index $\\pmb{\\theta}$ distributions in the \u201cdomain-themes\u201d space. Intuitively, similar domains have similar global domain indices, implying that the mixture of global domain index distributions is associated with a cluster of similar domains. The process of adapting local domain indices from global domain indices is illustrated in Figure 2. Specifically, we consider a Gaussian Mixture Model (GMM) as the mixture of global domain index distributions. For each distinct domain, we first rank the candidate global domain index distributions in \u201cdomain-themes\u201d space and select the distribution with the highest probability. We then derive the local domain index $\\textbf{\\em u}$ from the global domain index $\\pmb{\\theta}$ . Therefore, the global domain index is designed to be dynamical GMM. ", "page_idx": 3}, {"type": "image", "img_path": "Grd7yzFm5V/tmp/5695a33c267909ecb91a726f9ab387a5fb32610e82f65b4fca64b07ae62a8b39.jpg", "img_caption": ["Figure 3: The probabilistic graphical model of VDI (left) and GMDI (right). Edge type \"- - -\" denotes the independence between global domain index $\\pmb{\\theta}$ and data encoding $_{\\mathscr{L}}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Let $v$ denotes the latent categorical variable indicating the assignment of a domain to a cluster, which is equivalent to selecting components in a mixture distribution. Based on the definition of $v$ , we derive the updated representation of the distribution $p(y,x\\mid\\varepsilon)$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\np(y,x\\mid\\varepsilon)=\\int p(v)p(\\theta^{v}\\mid\\varepsilon)p(u\\mid\\theta^{v})p(x\\mid u)p(z\\mid x,u,\\theta^{v})p(y\\mid z)\\,d z d u d\\theta d v\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The component distribution $\\theta^{v}$ is selected from the mixture distribution of $\\pmb{\\theta}$ , and afterward, the local domain index $\\textbf{\\em u}$ is obtained from global domain index ${\\pmb\\theta}^{v}$ for generating domain-invariant data encoding $_{\\textit{z}}$ . Equation 3 and Equation 4 both represent the factorization of the distribution $p(y,x\\mid\\varepsilon)$ . $\\pmb{\\theta}$ in Equation 3 is the global domain index. While GMDI models the global domain index $\\pmb{\\theta}$ as a mixture of Gaussian distributions, $\\pmb\\theta^{v}$ in Equation 4 indicates the $v\\cdot$ -th component of the mixture distribution of $\\pmb{\\theta}$ with the prior $p(v)$ . Compared to the single distribution, the mixture of global domain index distributions adequately model the domain index of different domains, enhancing the effectiveness of domain adaptation in the face of varying degrees or even significant domain shifts. However, a remaining challenge is determining the number of components in the mixture distributions, especially when there are numerous domains, or even possibly infinite ones. ", "page_idx": 4}, {"type": "text", "text": "4.3 Generative process of GMDI ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In extreme cases, there may be infinite domains. Due to CRP\u2019s flexibility in dynamically determining the number of domain indices mixture components, we employ CRP to determine which cluster a domain belongs to (i.e., which component distribution domain index corresponds to). Specifically, we define the prior for domain indices cluster as a CRP, where the generation of new domain indices clusters is controlled by parameter $\\alpha$ . Thus, the probability of a domain belonging to cluster $k$ is calculated by Equation 2. Since CRP is an infinite mixture model, it is able to easily adapted to an infinite number of domains. ", "page_idx": 4}, {"type": "text", "text": "Mixture of domain indices need a stick-breaking representation of CRP to obtain component weights. Stick-breaking representation indicating an infinite construction, considering the Dirichlet prior with parameter $\\alpha$ , each element in the probability vector $\\pmb{\\pi}=[\\pi_{1},\\pi_{2},...]$ is non-negative and the sum of the elements is 1: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\beta_{k}\\mid\\alpha\\sim\\operatorname{Beta}(1,\\alpha)\\mathrm{~for~}k{=}1,...,\\infty\\mathrm{~,~}}\\\\ {\\displaystyle\\pi_{k}=\\beta_{k}\\prod_{l=1}^{k-1}(1-\\beta_{l})\\mathrm{~for~}k{=}1,...,\\infty\\mathrm{~.~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Equation 6 is equivalent to the weights implied by CRP. Based on Equation 6 and Equation 4, the generative process of GMDI is as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle v\\mid\\pi\\sim\\mathrm{Categorical}_{\\infty}(\\pi)\\,,}}\\\\ {{\\displaystyle\\theta^{v=k}\\sim{\\mathcal N}(\\mu_{k},\\sigma_{k}^{2})\\,,}}\\\\ {{\\displaystyle u\\mid\\theta^{v=k}\\sim p(u\\mid\\theta^{v=k})\\,,}}\\\\ {{\\displaystyle x\\mid u\\sim p(x\\mid u)\\,,}}\\\\ {{\\displaystyle z\\mid x,u,\\theta^{v}\\sim p(z\\mid x,u,\\theta^{v})\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{\\mu}_{k}$ and $\\pmb{\\sigma}_{k}^{2}$ are mean vector and semi-positive covariance matrix of the $k$ -th component in dynamic Gaussian mixture of domain indices, respectively. Figure 3 illustrates the generative process of VDI with a single distribution and GMDI with a mixture of distributions for domain indices. Since CRP is computationally intensive. To improve computational efficiency, we consider the stickbreaking construction to transform the infinite Gaussian mixture of domain indices into a finite one. It can be achieved by directly specifying an upper bound $K$ for the number of components in Gaussian mixture of domain indices. Selecting an appropriate $K$ allows to effectively reduce computational overhead. The finite version of the generative process of GMDI is available in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Accordingly, the generative model can be factorized as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\np(x,y,u,\\theta,z,v,\\beta\\mid\\alpha)=p(\\beta\\mid\\alpha)p(v\\mid\\beta)p(\\theta^{v})p(u\\mid\\theta^{v})p(x\\mid u)p(z\\mid x,u,\\theta^{v})p(y\\mid z)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The predictor $p(y~\\vert~z)$ is a categorical distribution for classification tasks and a Gaussian distribution for regression tasks. ", "page_idx": 5}, {"type": "text", "text": "4.4 Evidence Lower Bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The exact posterior of all latent variables, i.e., $p(\\pmb{u},\\pmb{\\theta},z,v,\\beta\\mid\\pmb{x})$ is intractable, variational inference is used to approximate the posterior. Compared to the Monte Carlo sampling, variational inference allows both uncertainty quantification and computational efficiency. We employ structured variational inference to approximate the exact posterior, factorizing the approximate posterior $q(u,\\pmb\\theta,z,v,\\beta\\mid$ $\\textbf{\\em x}$ ): ", "page_idx": 5}, {"type": "equation", "text": "$$\nq(u,\\pmb{\\theta},z,v,\\beta\\mid\\pmb{x})=q(\\beta;\\gamma)q(v;\\eta)q(u\\mid x;\\psi_{u})q(\\pmb{\\theta}^{v}\\mid u;\\psi_{\\theta})q(z\\mid x,u,\\pmb{\\theta}^{v};\\psi_{z})\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\gamma,\\beta,\\psi_{u},\\psi_{\\theta}$ and $\\psi_{z}$ respectively represent the parameters of the variational distributions $q(\\beta),q(v),q(\\boldsymbol{\\textbf{u}}|\\boldsymbol{\\textbf{x}}),q(\\theta^{v}\\mid\\boldsymbol{\\textbf{u}})$ and $q(z\\bar{\\mid}\\;x,\\bar{{\\boldsymbol{u}}},\\theta^{v})$ . ", "page_idx": 5}, {"type": "text", "text": "We train GMDI by maximizing the evidence lower bound(ELBO) to obtain the optimal variational distributions which best approximate exact posterior distributions. Section 5 demonstrates that our proposed GMDI has a more stringent evidence lower bound. Based on generative and inference process of GMDI, we caluculate the ELBO as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{ELBO}}=\\mathbb{E}_{q(u,\\theta^{v},z\\mid x;\\phi)q(v;\\eta)}[\\log p(y\\vert z)]+\\mathbb{E}_{q(u\\mid x;\\psi_{u})}[\\log p(x\\vert u)]}\\\\ &{\\quad\\quad\\quad+\\mathbb{E}_{q(v;\\eta)q(\\beta;\\gamma)q(u\\mid x;\\psi_{u})q(\\theta^{v}\\mid u;\\psi_{\\theta})}[\\log p(u\\vert\\theta^{v})]-\\mathrm{KL}[q(\\beta;\\gamma)\\vert\\vert p(\\beta)]}\\\\ &{\\quad\\quad\\quad-\\mathbb{E}_{q(\\beta;\\gamma)}[\\mathrm{KL}[q(v;\\eta)\\vert\\vert p(v\\vert\\beta;\\psi_{v})]]-\\mathbb{E}_{q(u\\mid x;\\psi_{u})q(v;\\eta)}[\\mathrm{KL}[q(\\theta^{v}\\mid u;\\psi_{\\theta})\\vert\\vert p(\\theta^{v})]]}\\\\ &{\\quad\\quad\\quad-\\mathbb{E}_{q(v;\\eta)q(u,\\theta^{v}\\mid x;\\xi)}[\\mathrm{KL}[q(z\\mid x,u,\\theta^{v};\\psi_{z})\\vert\\vert p(z\\mid x,u,\\theta^{v})]]}\\\\ &{\\quad\\quad\\quad-\\mathbb{E}_{q(u\\mid x;\\psi_{u})}[\\log q(u\\mid x;\\psi_{u})]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\phi$ and $\\xi$ represent the parameters of the variational distributions $q(\\pmb{u},\\pmb{\\theta}^{v},z|\\pmb{x})$ and $q(\\pmb{u},\\pmb{\\theta}^{v}|\\pmb{x})$ , respectively, and KL [\u00b7||\u00b7] is the Kullback\u2013Leibler divergence. ", "page_idx": 5}, {"type": "text", "text": "4.5 Adversarial loss with a discriminator ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To ensure the independence between global domain index $\\pmb{\\theta}$ and data encoding $_{z}$ as defined, we follow VDI [41] by training an additional discriminator $\\mathrm{D}$ with an adversarial loss. As we prove in Section 5 that the independence between global domain index $\\pmb{\\theta}$ and data encoding $_{\\textit{z}}$ relies on the independence between domain identity $w$ and data encoding $_{z}$ , the adversarial loss is simplified to discriminate the domain identity $w$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{D}}=\\mathbb{E}_{p(w,\\mathbf{x})}\\mathbb{E}_{q(z|\\mathbf{x};\\psi_{z})}[\\log\\mathrm{D}(w|z)]\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.6 Objective function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Combining Equation 14 and Equation 15, the final objective of GMDI is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{GMDI}}=\\operatorname*{max}_{D}\\operatorname*{min}_{\\boldsymbol{D}}\\mathcal{L}_{\\mathrm{ELBO}}-\\lambda*\\mathcal{L}_{\\mathrm{D}}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda$ denotes the hyper-parameter that balances two terms. Since the exact posterior of all latent variables is intractable, we propose to use a structured variational inference method to approximate the exact posterior. More details can be viewed in the appendix B. ", "page_idx": 5}, {"type": "text", "text": "Variational distribution of $\\beta$ . To derive the optimal variational distribution of $\\beta$ , we only consider the terms related to $\\beta$ in $\\mathcal{L}_{\\mathrm{ELBO}}$ , then we can get the posterior $q(\\beta_{k};\\gamma_{k})=B e t a(\\beta_{k};\\dot{\\gamma}_{k,1},\\gamma_{k,2})$ with parameters $\\gamma_{k,1}=1+\\eta_{k}$ and $\\begin{array}{r}{\\gamma_{k,2}=\\alpha+\\sum_{i=k+1}^{K}\\pmb{\\eta}_{i}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Variational distribution of $v$ . Similarly, the variational posterior of $v$ can be calculated as a Categorical distribution $q(v;\\pmb{\\eta})=C a t e g o r i c a l_{K}(v;\\pmb{\\eta})$ , where the pareameters can be updated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\eta_{k}\\ \\mathrm{{o}}\\mathbb{E}_{q(\\beta;\\gamma)}[\\pi]+\\mathbb{E}_{q(u|x;\\psi_{u})q(\\theta^{v}|u;\\psi_{\\theta})}[\\log p(u|\\theta^{v})]-\\mathbb{E}_{q(u|x;\\psi_{u})}[\\mathrm{{KL}}[q(\\theta^{v}|u;\\psi_{\\theta})||p(\\theta^{v})]]}\\\\ &{\\qquad\\qquad-\\ \\mathbb{E}_{q(u,\\theta^{v}|x;\\xi)}[\\mathrm{{KL}}[q(z|u,\\theta,x;\\psi_{z})||p(z|u,\\theta,x)]]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\sum_{k=1}^{K}\\eta_{k}=1}\\end{array}$ and $\\begin{array}{r}{q(\\beta;\\gamma)=\\prod_{k=1}^{K-1}q(\\beta_{k};\\gamma_{k}).}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Variational distribution of $_{\\theta,u}$ and $_{\\textit{z}}$ . With assuming that the latent parameters are sampled from Gaussian, we have the following forms: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(\\pmb{\\theta}^{v}|\\mathbf{{u}};\\pmb{\\psi}_{\\theta})=\\mathcal{N}(\\pmb{\\mu}_{\\theta},\\pmb{\\sigma}_{\\theta}^{2})\\,,}\\\\ {q(\\mathbf{{u}}|\\pmb{x};\\pmb{\\psi}_{u})=\\mathcal{N}(\\pmb{\\mu}_{u},\\pmb{\\sigma}_{u}^{2})\\,,}\\\\ {q(z|\\pmb{x},\\mathbf{{u}},\\pmb{\\theta}^{v};\\pmb{\\psi}_{z})=\\mathcal{N}(\\pmb{\\mu}_{z},\\pmb{\\sigma}_{z}^{2})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\pmb{\\mu}$ and $\\sigma^{2}$ are mean vector and semi-positive covariance matrix of Gaussian distribution. The parameters are updated by gradient descend. Specifically, we follow VDI [41] by using Earth Mover\u2019s Distance (EMD)[30] and Multi-Dimensional Scaling (MDS)[2] to infer $\\pmb\\theta^{v}$ from $\\textbf{\\em u}$ . ", "page_idx": 6}, {"type": "text", "text": "5 Theory ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we provide significant theoretical guarantees for our GMDI method. First, we give the upper bound for ELBO and adversial loss respectively. Second, we prove the upper bound of the whole loss with mutual information and entropy only. Moreover, we show that the upper bound can be achieved when the conditions are satisfied. Finally, we proove the significant result that our ELBO is better than the VDI, which means that a mixture of Gaussian prior can get better results. See Appendix C for detailed proof. ", "page_idx": 6}, {"type": "text", "text": "Lemma 1 The ELBO of $p(\\pmb{x},y)$ is bounded by the following formula with the Mutual Information, the Entropy and the KL-divergence: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p(\\mathbf{x},y)}[\\mathcal{L}_{\\mathrm{ELBO}}(p(\\mathbf{x},y))]\\leq\\!I(y;z)+I(\\mathbf{x};\\mathbf{u},\\theta,z,v)-(H(\\mathbf{x})+H(y))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\,\\mathbb{E}_{q(\\mathbf{x},\\mathbf{u},\\theta,z,v)}[\\mathrm{KL}[q(\\mathbf{x}|u,\\theta,v,z)||p(\\mathbf{x}|u,\\theta,v,z)]]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\,\\mathrm{KL}[q(\\mathbf{u},\\theta,v,z|x)||p(\\mathbf{u},\\theta,v,z)]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The main difference between and Lemma 1 in GMDI and Lemma 4.1 in VDI [41] is the last two KL terms and the inclusion of $v$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 2 (Information Decomposition of the Adversarial Loss $[4l]\\jmath{W e}$ can decompose the global maximum of adversial loss as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{D}\\mathbb{E}_{p(w,x)}\\mathbb{E}_{q(z|x)}[\\log\\mathrm{D}(w|z)]=I(z;\\pmb{\\theta})+I(z;w|\\pmb{\\theta})-H(w)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The global minimum of the function is achieved if and only if $I(z;\\theta)=0$ and $I(z;w|\\pmb\\theta)=0$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 The upper bound of the objective function can be decomposed as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{GMDI}}\\leq I(y;z)+I(x;u,\\theta,z,v)-I(z;\\theta)-I(z;w|\\theta)-\\left(H(x)+H(y)-H(w)\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The main difference between Theorem 1 in GMDI and Theorem 4.1 in VDI [41] is the inclusion of $v$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 The global optimum is achieved if and only if: $(I)I(z;\\pmb\\theta)~=~I(z;w|\\pmb\\theta)~=~0,$ , $(2)I(y;z)$ and $I(x;u,\\pmb\\theta,z,v)$ are maximized, $(3)\\mathrm{KL}[q({\\pmb u},{\\pmb\\theta},v,z|{\\pmb x})||p({\\pmb u},{\\pmb\\theta},v,z)]\\ =\\ 0$ and $\\mathrm{KL}[q(\\pmb{x}|\\pmb{u},\\pmb{\\theta},\\upsilon,z)||p(\\pmb{x}|\\pmb{u},\\pmb{\\theta},\\upsilon,z)]=0.$ . ", "page_idx": 6}, {"type": "text", "text": "The main difference between Theorem 2 in GMDI and Theorem 4.2 in VDI [41] is that $I(x;u,\\pmb\\theta,z,v)$ , which includes $v$ , needs to be maximized, and the two KL divergences should equal zero. ", "page_idx": 6}, {"type": "table", "img_path": "Grd7yzFm5V/tmp/e3b8c337ca5b4dc5bb2967d188c73ae51dabc1fdadc1ae1bcc93c08926fc289b.jpg", "table_caption": ["Table 1: Accuracy on binary classification tasks (Circle, DG-15, and $D G{-}6O$ ) and 4-way classification task (CompCars). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Theorem 3 Assuming the ELBO and objective of VDI are $\\mathcal{L}_{\\mathrm{VDI-ELBO}}$ and $\\mathcal{L}_{\\mathrm{VDI}}$ respectively, where domain indices are sampled from a simple Gaussian prior, we can prove that our objective achieves a more stringent evidence lower bound which is closer to the log-likelihood, and also a tighter upper bound of the objective: L $\\cdot\\mathrm{{VDI-ELBO}}\\leq\\mathcal{L}_{\\mathrm{{ELBO}}}\\leq\\log p(\\mathbf{x},y)$ and $\\mathcal{L}_{\\mathrm{VDI}}\\le\\mathcal{L}_{\\mathrm{GMDI}}$ . ", "page_idx": 7}, {"type": "text", "text": "6 Experimental Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We verify the effectiveness of GMDI via experimental comparison and analysis. In particular, we answer three research questions: (RQ1) Can the performance of GMDI for domain adaptation outperform baselines? (RQ2) How effective is the global domain indices inferred by GMDI? (RQ3) How does the number of mixture components K affect results? Additional experimental results are available in Appendix K. ", "page_idx": 7}, {"type": "text", "text": "6.1 Experimental setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets. We compare GMDI with existing DA methods on the following datasets (see Appendix H and Appendix I for more details): Circle [36] is used for binary classification task. DG-15 and DG60 [40] are synthetic datasets used for binary classification task. TPT-48 [40] dataset is a real-world dataset used for regression task. W $(6)\\rightarrow\\mathrm{E}$ (42): Adapting models from the 6 states in the west to the 42 states in the east. $\\mathrm{~N~}(24)\\rightarrow\\mathrm{~S~}$ (24): Adapting models from the 24 states in the north to the 24 states in the south. level-1 target domains: one hop away from the closest source domain. level-2 target domains: two hops away from the closest source domain. level-3 target domains: more than two hops away from the closest source domain. CompCars [43] dataset is a real-world dataset for 4-way classification task. ", "page_idx": 7}, {"type": "text", "text": "Baselines. To evaluate our proposed GMDI, we compare it against eight state-of-the-art domain adaptation methods: Domain Adversarial Neural Networks (DANN) [4], Adversarial Discriminative Domain Adaptation (ADDA) [5], Conditional Domain Adaptation Neural Networks (CDANN) [48], Margin Disparity Discrepancy (MDD) [47], SENTRY [27], Domain to Vector (D2V) [26], and Variational Domain Index (VDI) [41]. Additionally, we include the results for models trained and tested only on the source domain (Source-only). Note that D2V is not applicable to regression tasks, so its results are not reported on the TPT-48 dataset. Moreover, since our proposed GMDI focuses on inferring domain indices when they are unavailable, whereas [36] and [40] assume domain indices ", "page_idx": 7}, {"type": "table", "img_path": "Grd7yzFm5V/tmp/d644a070481c7ec9f43de6c70b73dc38599f1a8169a2f2ad7a2ea472d4ce76d0.jpg", "table_caption": ["Table 2: MSE for various DA methods in both tasks W $(6)\\rightarrow\\mathrm{E}$ (42) and $\\mathrm{~N~}(24)\\rightarrow\\mathrm{~S~}$ (24) on TPT-48. We report the average MSE of all domains as well as more detailed average MSE of level-1, level-2, level-3 target domains, respectively. Note that there is only one single DA model per column. We mark the best result with bold face. "], "table_footnote": ["are available, they are not applicable to our setting. Detailed explanations of these algorithms can be found in the respective references. "], "page_idx": 8}, {"type": "text", "text": "6.2 Results and discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "RQ1: Performance on classification and regression tasks. (1) Circle, DG-15 and DG-60. The results in Table 1 show that, on all three datasets, the performance of baselines other than D2V and VDI is only marginally better or worse than random guess (accuracy of $50\\%$ ). This is likely due to the complex relationships between domains within the datasets, making it difficult to adapt to target domains. Additionally, Source-only performs poorly due to overftiting. Compared to VDI, our GMDI improves accuracy by up to $3.4\\%$ , achieving very high accuracy (over $96.5\\%$ ). This improvement is attributed to proposal of modeling the global domain index as the mixture distributions (e.g., Figure 8). (2) TPT-48. In Table 2, we report the mean square error (MSE) of the evaluated methods on TPT-48. In both $\\mathrm{E}\\left(6\\right)\\rightarrow\\mathrm{W}$ (42) and $\\mathrm{~N~}(24)\\rightarrow\\mathrm{~S~}$ (24) regression tasks, all methods except DANN, SENTRY, and VDI performed worse than Source-only, indicating the occurrence of negative transfer. In contrast, GMDI significantly reduced the MSE compared to VDI, with average MSE decreases of $16\\%$ and $21\\%$ , respectively. (3) CompCars. The results in Table 1 show that our method achieves the best classification accuracy. All domain adaptation methods improved to varying degrees compared to Source-only, but our method achieved the highest increase in accuracy, with an improvement of up to $5\\%$ . In Figure 7(left), the data encoding generated by VDI are clustered together, indicating a mixture of points from different class labels. In contrast, in Figure 7(right), the data encoding of GMDI are separated by class label, demonstrating that GMDI can better distinguish points by class label. Across all datasets, GMDI significantly outperforms baselines, with minimum accuracy of $96.5\\;\\%$ on synthetic datasets , while MSE is reduced by at least ${\\bf16}\\,\\%$ on TPT-48 dataset. ", "page_idx": 8}, {"type": "text", "text": "RQ2: Effectiveness of inferred domain indices. Note that our proposed GMDI have no access to ground-truth domain indices $\\pmb{\\theta}$ during traning. To evaluate the effectiveness of GMDI in inferring domain indices, we compare the inferred domain indices with the ground-truth domain indices and calculate MSE between them. As shown in Figure 5, for nearly all 30 domains on Circle dataset, the MSE of the domain indices inferred by GMDI is significantly lower than that inferred by VDI. On TPT-48 dataset, the domain indices for $\\mathrm{E}\\left(6\\right)\\rightarrow\\mathrm{W}$ (42) and $\\mathrm{~N~}(24)\\rightarrow\\mathrm{~S~}$ (24) regression tasks correspond to longitude and latitude of 48 states. Therefore, we use longitude and latitude as the ground-truth domain indices and calculate the corresponding MSE. In Figure 4, it is evident that the MSE of the domain indices inferred by GMDI is still substantially lower than that of VDI. Although the CompCars lacks ground-truth domain indices, the data encoding visualization of VDI and GMDI(Figure 7) show that data encoding generated by GMDI form more distinct clusters compared to VDI. It indirectly indicates the effectiveness of the domain indices inferred by GMDI, demonstrating the considerable impact of modeling the global domain indices as Gaussian Mixture Model. On all datasets, the domain indices inferred by GMDI outperform those by VDI, owing to the dynamic mixture of domain indices distributions. ", "page_idx": 8}, {"type": "text", "text": "RQ3: Number of mixture components. We utilize the stick-breaking representation of the CRP to improve computational efficiency, setting an upper bound $K$ on the number of components in GMM. To study the impact of the number of components on domain adaptation performance, we report classification accuracy on CompCars, more complex dataset for different values of K. For other 4 datasets, the best results are achieved with $K=2$ , while for CompCars, the best performance is obtained with $K=3$ . The results in Figure 6 show that accuracy is the lowest when $K=1$ , suggesting that maintaining a single domain index distribution is insufficient for diverse target domains. The choice of $K$ is related to the concentration parameter of CRP and the dataset; the larger the concentration parameter and the more complex the dataset, the larger the value of $K$ should be. ", "page_idx": 8}, {"type": "image", "img_path": "Grd7yzFm5V/tmp/08ae70c3f6e0cd5e5cac319bb74e529b0472f646b78932c61b3f9c42023f3c02.jpg", "img_caption": ["Figure 5: MSE of domain indices on Figure 6: Accuracy $(\\%)$ on CompCars Circle dataset. with different K. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "Grd7yzFm5V/tmp/9a639068bd7d6be50f8caf46acc591c3b048b36c69148d56b16fd94b1b82bc84.jpg", "img_caption": ["Figure 7: t-SNE visualization of data encoding on CompCars dataset. Colors indicating different domains $\\{2,3,4\\}$ . Left: data encoding generated by VDI. Right: data encoding generated by GMDI. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose GMDI, a novel Gaussian Mixture Domain-Indexing algorithm, to address the challenge of inferring domain indices when they are unavailable. Unlike existing methods that assume global domain indices are sampled from a single static Gaussian, GMDI is the first one to utilize a mixture of dynamic Gaussians. The number of mixture components is determined adaptively by the Chinese Restaurant Process, enhancing the flexibility and effectiveness of domain adaptation. Our theoretical analysis confirms that GMDI achieves a more stringent evidence lower bound, closer to the log-likelihood. Extensive experiments validate the effectiveness of GMDI in inferring domain indices and highlight its potential practical applications. Specifically, for classification tasks, GMDI outperforms all approaches, and surpasses the state-of-the-art method, VDI, by up to $3.4\\%$ , reaching $99.3\\%$ . For regression tasks, GMDI reduces MSE by at least $16\\%$ (from 2.496 to 2.087) and by $21\\%$ (from 3.160 to 2.493), achieving the lowest errors among all methods. Despite these advantages, GMDI still relies on the availability of domain identities and cannot infer them as latent variables. Future work will focus on developing algorithms capable of inferring domain indices together with domain identities to further enhance the robustness and applicability of our approach. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was partly supported by the Fundamental Research Funds for the Central Universities, Sun Yat-sen University (67000-31610047). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Shuanghao Bai, Min Zhang, Wanqi Zhou, Siteng Huang, Zhirong Luan, Donglin Wang, and Badong Chen. Prompt-based distribution alignment for unsupervised domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 729\u2013737, 2024.   \n[2] Ingwer Borg and Patrick JF Groenen. Modern multidimensional scaling: Theory and applications. Springer Science & Business Media, 2005.   \n[3] Li Fei-Fei and Pietro Perona. A bayesian hierarchical model for learning natural scene categories. In 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR\u201905), volume 2, pages 524\u2013531. IEEE, 2005.   \n[4] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180\u20131189, 2015.   \n[5] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of machine learning research, 17(59):1\u201335, 2016.   \n[6] Saurabh Garg, Nick Erickson, James Sharpnack, Alex Smola, Sivaraman Balakrishnan, and Zachary Chase Lipton. Rlsbench: Domain adaptation under relaxed label shift. In International Conference on Machine Learning, pages 10879\u201310928. PMLR, 2023.   \n[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \n[8] Huan He, Owen Queen, Teddy Koker, Consuelo Cuevas, Theodoros Tsiligkaridis, and Marinka Zitnik. Domain adaptation for time series under feature and label shifts. In International Conference on Machine Learning, pages 12746\u201312774. PMLR, 2023.   \n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[10] Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, Hongfu Liu, and Ye Wang. Extrapolative continuous-time bayesian neural network for fast training-free test-time adaptation. Advances in Neural Information Processing Systems, 35:36000\u201336013, 2022.   \n[11] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.   \n[12] Sang-Yeong Jo and Sung Whan Yoon. Poem: polarization of embeddings for domain-invariant representations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 8150\u20138158, 2023.   \n[13] Jiyong Li, Dilshod Azizov, LI Yang, and Shangsong Liang. Contrastive continual learning with importance sampling and prototype-instance relation distillation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 13554\u201313562, 2024.   \n[14] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Lingyu Duan. Uncertainty modeling for out-of-distribution generalization. In International Conference on Learning Representations, 2023.   \n[15] Zijian Li, Ruichu Cai, Guangyi Chen, Boyang Sun, Zhifeng Hao, and Kun Zhang. Subspace identification for multi-source domain adaptation. Advances in Neural Information Processing Systems, 36, 2024.   \n[16] Shangsong Liang, Emine Yilmaz, and Evangelos Kanoulas. Dynamic clustering of streaming short documents. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 995\u20131004, 2016.   \n[17] Shangsong Liang, Zhaochun Ren, Emine Yilmaz, and Evangelos Kanoulas. Collaborative user clustering for short text streams. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.   \n[18] Shangsong Liang, Emine Yilmaz, and Evangelos Kanoulas. Collaboratively tracking interests for user clustering in streams of short texts. IEEE Transactions on Knowledge and Data Engineering, 31:257\u2013272, 2018.   \n[19] Chen-Hao Liao, Wen-Cheng Chen, Hsuan-Tung Liu, Yi-Ren Yeh, Min-Chun Hu, and Chu-Song Chen. Domain invariant vision transformer learning for face anti-spoofing. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 6098\u20136107, 2023.   \n[20] Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, and Xing Xie. Out-of-distribution representation learning for time series classification. arXiv preprint arXiv:2209.07027, 2022.   \n[21] Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, and Yi Yang. Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2507\u20132516, 2019.   \n[22] Toshihiko Matsuura and Tatsuya Harada. Domain generalization using a mixture of multiple latent domains. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11749\u201311756, 2020.   \n[23] A Tuan Nguyen, Toan Tran, Yarin Gal, and Atilim Gunes Baydin. Domain invariant representation learning with domain density transformations. Advances in Neural Information Processing Systems, 34:5264\u20135275, 2021.   \n[24] Le Thanh Nguyen-Meidine, Atif Belal, Madhu Kiran, Jose Dolz, Louis-Antoine Blais-Morin, and Eric Granger. Unsupervised multi-target domain adaptation through knowledge distillation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 1339\u20131347, 2021.   \n[25] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1406\u20131415, 2019.   \n[26] Xingchao Peng, Yichen Li, and Kate Saenko. Domain2vec: Domain embedding for unsupervised domain adaptation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VI 16, pages 756\u2013774. Springer, 2020.   \n[27] Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8558\u20138567, 2021.   \n[28] Sanqing Qu, Tianpei Zou, Florian R\u00f6hrbein, Cewu Lu, Guang Chen, Dacheng Tao, and Changjun Jiang. Upcycling models under domain and category shift. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20019\u201320028, 2023.   \n[29] Sylvestre-Alvise Rebuff,i Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in neural information processing systems, 30, 2017.   \n[30] Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover\u2019s distance as a metric for image retrieval. International journal of computer vision, 40:99\u2013121, 2000.   \n[31] Lianghe Shi and Weiwei Liu. Adversarial self-training improves robustness and generalization for gradual domain adaptation. Advances in Neural Information Processing Systems, 36, 2024.   \n[32] Kunpeng Song, Ligong Han, Bingchen Liu, Dimitris Metaxas, and Ahmed Elgammal. Stylegan-fusion: Diffusion guided domain adaptation of image generators. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5453\u20135463, 2024.   \n[33] Tao Sun, Cheng Lu, and Haibin Ling. Domain adaptation with adversarial training on penultimate activations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 9935\u20139943, 2023.   \n[34] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pages 4068\u20134076, 2015.   \n[35] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167\u20137176, 2017.   \n[36] Hao Wang, Hao He, and Dina Katabi. Continuously indexed domain adaptation. In International Conference on Machine Learning, pages 9898\u20139907. PMLR, 2020.   \n[37] Thomas Westfechtel, Hao-Wei Yeh, Dexuan Zhang, and Tatsuya Harada. Gradual source domain expansion for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1946\u20131955, 2024.   \n[38] Yi Wu, Ziqiang Li, Chaoyue Wang, Heliang Zheng, Shanshan Zhao, Bin Li, and Dacheng Tao. Domain re-modulation for few-shot generative domain adaptation. Advances in Neural Information Processing Systems, 36, 2024.   \n[39] Ruicheng Xian, Honglei Zhuang, Zhen Qin, Hamed Zamani, Jing Lu, Ji Ma, Kai Hui, Han Zhao, Xuanhui Wang, and Michael Bendersky. Learning list-level domain-invariant representations for ranking. Advances in Neural Information Processing Systems, 36, 2023.   \n[40] Zihao Xu, Hao He, Guang-He Lee, Bernie Wang, and Hao Wang. Graph-relational domain adaptation. In International Conference on Learning Representations, 2022.   \n[41] Zihao Xu, Guang-Yuan Hao, Hao He, and Hao Wang. Domain-indexing variational bayes: Interpretable domain index for domain adaptation. In International Conference on Learning Representations, 2023.   \n[42] Jinyu Yang, Jingjing Liu, Ning Xu, and Junzhou Huang. Tvt: Transferable vision transformer for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 520\u2013530, 2023.   \n[43] Linjie Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang. A large-scale car dataset for fine-grained categorization and verification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3973\u20133981, 2015.   \n[44] Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Universal domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2720\u20132729, 2019.   \n[45] Zhongqi Yue, Qianru Sun, and Hanwang Zhang. Make the u in uda matter: Invariant consistency learning for unsupervised domain adaptation. Advances in Neural Information Processing Systems, 36, 2024.   \n[46] Qiang Zhang, Jinyuan Fang, Zaiqiao Meng, Shangsong Liang, and Emine Yilmaz. Variational continual bayesian meta-learning. Advances in Neural Information Processing Systems, 34:24556\u201324568, 2021.   \n[47] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In International conference on machine learning, pages 7404\u20137413. PMLR, 2019.   \n[48] Mingmin Zhao, Shichao Yue, Dina Katabi, Tommi S Jaakkola, and Matt T Bianchi. Learning sleep stages from radio signals: A conditional adversarial architecture. In International Conference on Machine Learning, pages 4100\u20134109. PMLR, 2017.   \n[49] Jinjing Zhu, Haotian Bai, and Lin Wang. Patch-mix transformer for unsupervised domain adaptation: A game perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3561\u20133571, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Finite Stick-Breaking Construction of CRP ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The infinite Chinese Restaurant Process (CRP) requires substantial computational overhead. To leverage CRP with lower computational cost, we use the stick-breaking construction to construct it. We set an upper bound on the number of Gaussian mixture components, eliminating the need for a varying number of mixture components. The finite stick-breaking construction of CRP is given as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{}}&{{}}&{{\\beta_{k}\\ |\\ \\alpha\\sim\\mathrm{Beta}(1,\\alpha)\\ \\ \\mathrm{for}\\ k{=}1,...,K-1\\ ,}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{\\pi_{k}=\\beta_{k}\\displaystyle\\prod_{l=1}^{k-1}(1-\\beta_{l})\\ \\ \\mathrm{for}\\ k{=}1,...,K-1\\ ,}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{\\pi_{K}=\\displaystyle\\prod_{l=1}^{K-1}(1-\\beta_{l})\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\pmb{\\pi}=\\mathrm{stickbreak}(\\pmb{\\theta})$ is the prior parameters of the $K$ -dimensional category variable $v$ . Considering the finite stick-breaking construction of CRP mentioned above, we rewrite the generative process of GMDI: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\left.v\\right|\\,\\pi\\sim\\mathrm{Categorical}_{K}(\\pi)\\,,}}\\\\ {{\\theta^{v=k}\\sim{\\mathcal N}(\\mu_{k},\\sigma_{k}^{2})\\,,}}\\\\ {{u\\mid\\theta^{v=k}\\sim p(u\\mid\\theta^{v=k})\\,,}}\\\\ {{x\\mid u\\sim p(x\\mid u)\\,,}}\\\\ {{z\\mid x,u,\\theta^{v}\\sim p(z\\mid x,u,\\theta^{v})\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\pmb{\\mu}_{k}$ and $\\pmb{\\sigma}_{k}^{2}$ are mean vector and semi-positive covariance matrix of the $k$ -th component in GMM, respectively. ", "page_idx": 13}, {"type": "text", "text": "B Derivation for Variational Posterior ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Firstly, we factorize the generative model as follows: ", "page_idx": 13}, {"type": "text", "text": "$p(x,y,u,\\theta,z,v,\\beta\\mid\\alpha)=p(\\beta\\mid\\alpha)p(v\\mid\\beta)p(\\theta^{v})p(u\\mid\\theta^{v})p(x\\mid u)p(z\\mid x,u,\\theta^{v})p(y\\mid z)\\,.$ (23) During the inference process, we need to infer the latent variables. Since the target posterior distributions are intractable, we employ the technique of approximate variational inference. In this framework, we design variational distributions for these latent variables, aiming to approximate their true underlying posterior distributions: ", "page_idx": 13}, {"type": "text", "text": "$q(u,\\pmb{\\theta},z,v,\\beta\\mid\\pmb{x})=q(\\beta;\\gamma)q(v;\\eta)q(u\\mid x;\\psi_{u})q(\\pmb{\\theta}^{v}\\mid u;\\psi_{\\theta})q(z\\mid x,u,\\pmb{\\theta}^{v};\\psi_{z})\\,.$ Thus we calculate the ELBO as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log(x,y\\vert\\alpha)=\\log\\int_{\\mathcal{P}}(x,y,u,\\theta,z,v,\\beta)\\,|\\alpha|\\,d\\alpha d\\alpha d\\theta d\\alpha d\\beta}\\\\ &{=\\log\\int_{\\mathcal{P}}(\\mathbf{u},\\theta,z,v,\\beta)\\,|\\alpha\\geq\\frac{p}{\\alpha p}\\frac{|x,y|\\alpha,\\theta,z,v,\\beta\\,|\\alpha)}{p!}\\,d\\alpha d\\alpha d\\beta d\\alpha d\\alpha d\\alpha d\\beta}\\\\ &{=\\log\\mathbb{E}_{\\alpha}\\bigg[\\frac{p}{\\alpha p}\\frac{\\langle x,y,u,\\theta,z,v,\\beta\\,|\\alpha\\rangle}{p!}\\bigg]}\\\\ &{\\geq\\mathbb{E}_{\\alpha}\\bigg[\\log\\frac{p(x,y,u,\\theta,z,v,\\beta\\,|\\alpha)}{q!}\\bigg]}\\\\ &{=\\mathbb{E}_{\\alpha}\\bigg[\\log\\frac{p(x,y,u,\\theta,z,v,\\beta\\,|\\alpha)}{q!}\\bigg]}\\\\ &{=\\mathbb{E}_{\\alpha}\\bigg[\\log\\frac{p(x,y)(x)(y,u,\\theta,w)}{q!(x,y)(y)(y)(y)(y)(z)}\\bigg]^{\\alpha}\\bigg]}\\\\ &{=\\mathbb{E}_{\\alpha}\\bigg[\\log\\frac{p(x,y)(y)(x,y)(u,\\theta,w))}{q!}\\bigg[\\log\\frac{p}{\\alpha p}\\big(\\log\\frac{|x|\\alpha\\rangle}{q!}\\sqrt{\\alpha!\\alpha!\\beta!\\big|x,\\theta,w^{\\prime}\\!\\big|\\alpha}\\bigg]\\bigg]}\\\\ &{=\\mathbb{E}_{\\alpha}(\\log\\frac{p(x,y)(y)(y)(y)(z)}{q!})+\\mathbb{E}_{\\alpha}(\\log\\vert\\log(\\frac{|x|\\alpha\\rangle}{q!})\\big\\vert\\log(\\frac{1}{\\alpha}\\log(\\frac{|x|\\alpha\\rangle}{q!})\\bigg]}\\\\ &{\\quad+\\mathbb{E}_{\\alpha}(\\log(\\theta)(x)\\vert\\alpha\\rangle\\sin(\\theta^{\\prime}\\,\\log(\\frac{|x|\\alpha\\rangle}{q!})\\log(\\frac{|x|\\alpha\\rangle}{q!})\\bigg]}\\\\ &{\\quad-\\mathbb{E}_{\\alpha}[\\log(\\frac{|x|\\alpha\\rangle}{q!})\\big\\Vert\\mathcal{P}\\big(\\log(1/\\alpha)\\big)\\big\\Vert\\mathrm{H}[\\alpha]}\\\\ &{=\\mathbb{E}_{\\alpha}(\\log(\\theta)(x,y)(\\log(1/\\alpha)\\log(p(x)\\big|\\alpha)\\big|\\alpha}\\\\ &{\\quad-\\mathbb{E}_{\\alpha}(\\log(\\theta)(x,y)(\\log(1/\\alpha) \n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the inequality is given by applying Jensen\u2019s inequality. Besides, we apply the adversarial loss to ensure the independence between global domain index and data encoding. The adversarial loss with a discriminator $\\mathrm{D}$ is designed as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{D}}=\\mathbb{E}_{p(w,\\mathbf{x})}\\mathbb{E}_{q(\\boldsymbol{z}|\\mathbf{x};\\boldsymbol{\\psi}_{\\boldsymbol{z}})}[\\log\\mathrm{D}(w\\mid\\boldsymbol{z})]\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Our final objective is derived as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{GMDI}}=\\operatorname*{max}_{D}\\operatorname*{min}_{D}\\mathcal{L}_{\\mathrm{ELBO}}-\\lambda*\\mathcal{L}_{\\mathrm{D}}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By optimizing the objective function, we can calculate all the optimal variational distributions of latent variables as follows: ", "page_idx": 14}, {"type": "text", "text": "Variational distribution of $\\beta$ . To obtain the variational posterior of the latent variable $\\beta$ , We only need to consider the related terms in $\\mathcal{L}_{\\mathrm{GMDI}}$ . Note that the adversarial loss $\\mathcal{L}_{\\mathrm{D}}$ is independent of the target latent variable, with all terms pertaining to $\\beta$ encompassed within the ELBO loss, which can be formulated as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F(q(\\beta_{k};\\gamma_{k}))=\\mathbb{E}_{q(v;\\eta)}\\mathbb{E}_{q(\\beta_{k};\\gamma_{k})}[\\log p(\\beta_{k})+\\log p(v)]-\\mathbb{E}_{q(\\beta_{k};\\gamma_{k})}[\\log p(\\beta_{k})]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Subsequently, by differentiating the function and setting the derivative equal to zero: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial q(\\beta_{k};\\gamma_{k})}F(q(\\beta_{k};\\gamma_{k}))}\\\\ &{=\\displaystyle\\int q(\\boldsymbol{v};\\boldsymbol{\\eta})q(\\beta_{k};\\gamma_{\\setminus k})\\left[\\frac{\\partial}{\\partial q(\\beta_{k};\\gamma_{k})}\\int q(\\beta_{k};\\gamma_{k})[\\log p(\\beta_{k})+\\log p(\\boldsymbol{v}\\mid\\beta)-\\log q(\\beta_{k};\\gamma_{k})]d\\beta_{k}\\right]d\\beta_{k}}\\\\ &{=\\displaystyle\\int q(\\boldsymbol{v};\\boldsymbol{\\eta})q(\\beta_{k};\\gamma_{\\setminus k})[\\log p(\\beta_{k})+\\log p(\\boldsymbol{v}\\mid\\beta)-\\log q(\\beta_{k};\\gamma_{k})]d\\beta_{\\setminus k}d\\boldsymbol{v}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $q(\\beta_{k};\\gamma_{\\setminus k})$ is the variational posterior of $\\beta$ without $\\beta_{k}$ , we derive the final variational distribution of $\\beta_{k}$ as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log q(\\beta_{k};\\gamma_{k})\\propto\\mathbb{E}_{q(v;\\eta)}\\mathbb{E}_{q(\\beta_{k};\\gamma_{k})}|\\log p(\\beta_{k})+\\log p(v\\mid\\beta)|}\\\\ &{\\qquad\\qquad=\\log p(\\beta_{k})+\\mathbb{E}_{q(v;\\eta)}\\bigg[\\mathbb{E}_{q(\\beta_{k};\\gamma_{k})}\\bigg[\\displaystyle\\sum_{k=1}^{K-1}\\Big(v_{k}\\log\\beta_{k}+\\displaystyle\\sum_{l=1}^{k-1}v_{k}\\log(1-\\beta_{l})\\Big)}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\sum_{j=1}^{K}v_{k}\\log(1-\\beta_{j})\\bigg]\\Bigg]}\\\\ &{\\qquad\\qquad\\propto\\log p(\\beta_{k})+\\mathbb{E}_{q(v;\\eta)}\\bigg[v_{k}\\log\\beta_{k}+\\displaystyle\\sum_{j=k+1}^{K-1}v_{j}\\log(1-\\beta_{k})+v_{K}\\log(1-\\beta_{k})\\bigg]}\\\\ &{\\qquad\\qquad\\qquad=\\log p(\\beta_{k})+\\eta_{k}\\log\\beta_{k}+\\displaystyle\\sum_{j=k+1}^{K}\\eta_{j}\\log(1-\\beta_{k})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since we assume that $p(\\beta_{k})\\sim B e t a(\\cdot;1,\\alpha)$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log p(\\beta_{k})\\propto(\\alpha-1)\\log(1-\\beta_{k})\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus we can derive the optimal variational posterior of $\\beta_{k}$ as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log q(\\beta_{k};\\gamma_{k})\\propto\\eta_{k}\\log\\beta_{k}+(\\alpha-1+\\sum_{j=k+1}^{K}\\eta_{j})\\log(1-\\beta_{k})\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is also a Beta distribution $\\mathrm{Beta}(\\beta_{k};\\gamma_{k,1},\\gamma_{k,2})$ with parameters: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma_{k,1}=1+\\eta_{k}\\,,\\quad\\gamma_{k,2}=\\alpha+\\sum_{j=k+1}^{K}\\eta_{j}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Variational distribution of $v$ . Since $v$ is a category variable, we assume its variational posterior to be a categorical distribution parameterized by $\\eta$ as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(v;\\pmb{\\eta})=\\mathrm{Categorical}_{K}(v;\\pmb{\\eta})\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, we only consider the terms related to $v$ in $\\mathcal{L}_{\\mathrm{GMDI}}$ to derive the optimal varational posterior, which is calculated as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\boldsymbol{F}(\\boldsymbol{q}(\\boldsymbol{v};\\boldsymbol{\\eta}))=-\\mathbb{E}_{\\boldsymbol{q}(\\boldsymbol{\\beta};\\boldsymbol{\\gamma})}[\\mathrm{KL}[\\boldsymbol{q}(\\boldsymbol{v};\\boldsymbol{\\eta})||\\boldsymbol{p}(\\boldsymbol{v}|\\boldsymbol{\\beta})]]-\\mathbb{E}_{\\boldsymbol{q}(\\boldsymbol{u}|\\boldsymbol{x};\\boldsymbol{\\psi}_{\\boldsymbol{u}})\\boldsymbol{q}(\\boldsymbol{v};\\boldsymbol{\\eta})}[\\mathrm{KL}[\\boldsymbol{q}(\\boldsymbol{\\theta}^{\\boldsymbol{v}}|\\boldsymbol{u};\\boldsymbol{\\psi}_{\\boldsymbol{\\theta}})||\\boldsymbol{p}(\\boldsymbol{\\theta}^{\\boldsymbol{v}})]]}\\\\ &{\\quad\\quad+\\mathbb{E}_{\\boldsymbol{q}(\\boldsymbol{v};\\boldsymbol{\\eta})\\boldsymbol{q}(\\boldsymbol{u}|\\boldsymbol{x};\\boldsymbol{\\psi}_{\\boldsymbol{u}})\\boldsymbol{q}(\\boldsymbol{\\theta}^{\\boldsymbol{v}}|\\boldsymbol{u};\\boldsymbol{\\psi}_{\\boldsymbol{\\theta}})}[\\log{p}(\\boldsymbol{u}|\\boldsymbol{\\theta}^{\\boldsymbol{v}})]}\\\\ &{\\quad\\quad-\\mathbb{E}_{\\boldsymbol{q}(\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v}|\\boldsymbol{x};\\boldsymbol{\\xi})}[\\mathrm{KL}[\\boldsymbol{q}(\\boldsymbol{z}|\\boldsymbol{x},\\boldsymbol{u},\\boldsymbol{\\beta},\\boldsymbol{v};\\boldsymbol{\\psi}_{\\boldsymbol{z}})]|\\boldsymbol{p}(\\boldsymbol{z}|\\boldsymbol{x},\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v})]]}\\\\ &{\\quad\\quad\\displaystyle=\\sum_{k=1}^{K}\\left\\{\\eta_{k}\\mathbb{E}_{\\boldsymbol{q}(\\boldsymbol{\\beta};\\boldsymbol{\\gamma})}[\\log{p}(\\boldsymbol{v}|\\boldsymbol{\\beta})]-\\eta_{k}\\log{\\eta_{k}}-\\eta_{k}\\mathbb{E}_{\\boldsymbol{q}(\\boldsymbol{u}|\\boldsymbol{x};\\boldsymbol{\\psi}_{\\boldsymbol{u}})}[\\mathrm{KL}[\\boldsymbol{q}(\\boldsymbol{\\theta}^{\\boldsymbol{v}}|\\boldsymbol{u};\\boldsymbol{\\psi}_{\\boldsymbol{\\theta}})||\\boldsymbol{p}(\\boldsymbol{\\theta}^{\\boldsymbol{v}})]]\\right.}\\\\ &{\\quad\\quad\\left.+\\eta_{k}\\mathbb{E}_{\\boldsymbol{q}(\\boldsymbol{u}|\\boldsymbol{x};\\boldsymbol{\\psi}_{\\boldsymbol{u}})\\boldsymbol{q}(\\boldsymbol{\\theta}^{\\boldsymbol{v}}|\\boldsymbol{u};\\boldsymbol{\\psi}_{\\boldsymbol{\\theta}})}[\\log{p}(\\boldsymbol{u}|\\boldsymbol{\\theta}^\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By taking the derivative function of $F(q(v;\\eta))$ with respective to zero: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial\\eta_{k}}F(q(v;\\eta))=\\mathbb{E}_{q(\\beta;\\gamma)}[\\log p(v|\\beta)]-\\log\\eta_{k}-1-\\mathbb{E}_{q(u|x;\\psi_{u})}[\\mathrm{KL}[q(\\theta^{v}|u;\\psi_{\\theta})||p(\\theta^{v})]]}\\\\ &{\\qquad\\qquad\\qquad+\\,\\mathbb{E}_{q(u|x;\\psi_{u})q(\\theta^{v}|u;\\psi_{\\theta})}[\\log p(u|\\theta^{v})]}\\\\ &{\\qquad\\qquad\\qquad-\\,\\mathbb{E}_{q(u,\\theta^{v}|x;\\xi)}[\\mathrm{KL}[q(z|x,u,\\theta^{v};\\psi_{z})||p(z|x,u,\\theta^{v})]]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we can finally have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\eta_{k}\\propto\\!\\!\\mathbb{E}_{q(\\beta;\\gamma)}[\\pmb{\\pi}]+\\mathbb{E}_{q(\\pmb{u}|\\pmb{x};\\psi_{u})q(\\pmb{\\theta}^{v}|\\pmb{u};\\psi_{\\theta})}[\\log p(\\pmb{u}|\\pmb{\\theta}^{v})]}\\\\ &{~~~~~~~~~~~~-\\mathbb{E}_{q(\\pmb{u}|\\pmb{x};\\psi_{u})}[\\mathrm{KL}[q(\\pmb{\\theta}^{v}|\\pmb{u};\\psi_{\\theta})||p(\\pmb{\\theta}^{v})]]}\\\\ &{~~~~~~~~~~~~-\\mathbb{E}_{q(\\pmb{u},\\pmb{\\theta}^{v}|\\pmb{x};\\xi)}[\\mathrm{KL}[q(\\pmb{z}|\\pmb{u},\\pmb{\\theta},\\pmb{x};\\psi_{z})||p(\\pmb{z}|\\pmb{u},\\pmb{\\theta},\\pmb{x})]]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\sum_{k=1}^{K}\\eta_{k}=1}\\end{array}$ and $\\begin{array}{r}{q(\\beta;\\gamma)=\\prod_{k=1}^{K-1}q(\\beta_{k};\\gamma_{k})}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Variational distribution of $_{\\theta,u}$ and $_{\\textit{z}}$ . The distribution of $\\pmb{\\theta}$ is assumed to include a mixture of components. The distribution of the latent variable $\\pmb{\\theta}$ is assumed to be a mixture of a series of distributions, while the latent variables $\\textbf{\\em u}$ and $_{z}$ can be regarded as following conditional Gaussian distributions. Since it is highly intractable to precisely compute the posterior distributions of these latent variables, we employ variational Gaussian distributions to approximate the posterior distribution for each component of $\\pmb{\\theta}$ , as well as the conditional posterior distributions for $\\textbf{\\em u}$ and $_{z}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{q(\\pmb{\\theta}^{v=k}|\\pmb{u};\\pmb{\\psi}_{\\theta})=\\mathcal{N}(\\pmb{\\mu}_{\\theta}^{k},\\pmb{\\Lambda}_{\\theta}^{k})\\,,}\\\\ &{}&{q(\\pmb{u}|\\pmb{x};\\pmb{\\psi}_{u})=\\mathcal{N}(\\pmb{\\mu}_{u},\\pmb{\\Lambda}_{u})\\,,}\\\\ &{}&{q(z|\\pmb{x},\\pmb{u},\\pmb{\\theta}^{v};\\pmb{\\psi}_{z})=\\mathcal{N}(\\pmb{\\mu}_{z},\\pmb{\\Lambda}_{z})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\pmb{\\mu}$ and $\\Lambda$ are mean vector and semi-positive covariance matrix of Gaussian distribution. All these variational distributions can be updated by maximizing $\\mathcal{L}_{\\mathrm{GMDI}}$ with back propagation. ", "page_idx": 15}, {"type": "text", "text": "C Theory Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The proof process of the lemmas and theorems are partially based on VDI [41]. ", "page_idx": 15}, {"type": "text", "text": "Lemma C.1 The ELBO of $p(\\pmb{x},y)$ is bounded by the following formula with the Mutual Information, the Entropy and the KL-divergence: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p(x,y)}[\\mathcal{L}_{\\mathrm{ELBO}}(p(\\pmb{x},y))]\\leq\\!I(y;z)+I(\\pmb{x};\\pmb{u},\\pmb{\\theta},z,v)-(H(\\pmb{x})+H(y))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\mathbb{E}_{q(\\pmb{x},\\pmb{u},\\pmb{\\theta},z,v)}[\\mathrm{KL}[q(\\pmb{x}|\\pmb{u},\\pmb{\\theta},v,z)||p(\\pmb{x}|\\pmb{u},\\pmb{\\theta},v,z)]]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\mathrm{KL}[q(\\pmb{u},\\pmb{\\theta},v,z|\\pmb{x})||p(\\pmb{u},\\pmb{\\theta},z,v)]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The main difference between and Lemma C.1 in GMDI and Lemma B.1 in VDI [41] is the last two KL terms and the inclusion of $v$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. In order to give an upper bound of $p(\\pmb{x},y)$ , we first calculate the ELBO as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log p(\\boldsymbol{x},\\boldsymbol{y})=\\log\\int p(\\boldsymbol{x},\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z,\\boldsymbol{y})d z d u d\\theta^{v}d v d\\beta}\\\\ &{\\quad\\quad\\quad\\quad=\\log\\int\\frac{p(\\boldsymbol{x},\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z,\\boldsymbol{y})\\,\\ast\\,q(\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z\\mid\\boldsymbol{x})}{q(\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z\\mid\\boldsymbol{x})}d z d u d\\theta^{v}d v d\\beta}\\\\ &{\\quad\\quad\\quad=\\log\\mathbb{E}_{q}\\left[\\frac{p(\\boldsymbol{x},\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z,\\boldsymbol{y})}{q(\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z\\mid\\boldsymbol{x})}\\right]}\\\\ &{\\quad\\quad\\quad\\geq\\mathbb{E}_{q}\\left[\\log\\frac{p(\\boldsymbol{x},\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z,\\boldsymbol{y})}{q(\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z\\mid\\boldsymbol{x})}\\right]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{q}\\left[\\log\\frac{p(\\boldsymbol{y}|z)p(\\boldsymbol{x}|u,\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z)p(\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z)}{q(\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z\\mid\\boldsymbol{x})}\\right]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{q}\\left[\\log p(\\boldsymbol{y}|z)\\right]+\\mathbb{E}_{q}\\bigl[\\log p(\\boldsymbol{x}|u,\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z)\\bigr]-\\mathrm{KL}[p(\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z)\\bigr]|q(\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},z\\mid\\boldsymbol{x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Accordingly, we have the following ELBO objective: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\boldsymbol{\\mathrm{\\scriptscriptstyleELBO}}}(p(\\boldsymbol{x},\\boldsymbol{y}))=\\mathbb{E}_{q}[\\log p(\\boldsymbol{y}|\\boldsymbol{z})]+\\mathbb{E}_{q}[\\log p(\\boldsymbol{x}|\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},\\boldsymbol{z})]-\\mathrm{KL}[p(\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},\\boldsymbol{z})||q(\\boldsymbol{u},\\boldsymbol{\\theta},\\boldsymbol{v},\\boldsymbol{\\beta},\\boldsymbol{z}|\\boldsymbol{x})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here we aim at giving a formula only including Mutual Information, Entropy and KL-divergence to bound the objective. To achieve this, we first calculate the upper bound of $\\mathbb{E}_{q}\\log p(y|z)$ , with denoting $r(\\pmb{x},y,z)=p(\\pmb{x},y)q(z|\\pmb{x})$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p(x,y)}\\mathbb{E}_{q}[\\log p(y|z)]=\\mathbb{E}_{p(x,y)q(z|x)}[\\log p(y|z)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{r(x,y,z)}[\\log p(y|z)]}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\mathbb{E}_{r(y,z)}[\\log p(y|z)]}\\\\ &{\\qquad\\qquad\\quad\\leq\\mathbb{E}_{r(y,z)}[\\log r(y|z)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{r(y,z)}[\\log\\frac{r(y|z)}{p(y)}]+\\mathbb{E}_{r(y,z)}[\\log p(y)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=I(y|z)-H(y)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When $\\begin{array}{l c l}{p(y|z)}&{=}&{r(y|z)}\\end{array}$ , the maximum of $\\mathbb{E}_{q}[\\log p(y|z)]$ is achieved and then we have max $\\mathbb{E}_{q}[\\log p(y|z)]=I(y;z)-H(y)$ . ", "page_idx": 16}, {"type": "text", "text": "Secondly, we need to give an upper bound of $\\mathbb{E}_{q}[\\log p(\\mathbf{\\boldsymbol{x}}|\\mathbf{\\boldsymbol{u}},\\pmb{\\theta},\\boldsymbol{v},\\beta,z)]$ . For convenient, we denote the joint distribution $s(\\pmb{x},\\pmb{u},\\pmb{\\theta},\\upsilon,\\beta,z)=p(\\pmb{x})\\dot{q}(\\pmb{u},\\pmb{\\theta},\\upsilon,\\beta,z)$ , then we can calculate as follows: \u6b63 ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{p(x,y)}\\mathbb{E}_{q}[\\log p(x|u,\\theta,v,\\beta,z)]=\\mathbb{E}_{p(x)}\\mathbb{E}_{q}[\\log p(x|u,\\theta,v,\\beta,z)]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{p(x)}\\mathbb{E}_{q}[\\log\\frac{q(x|u,\\theta,v,\\beta,z)}{p(x)}\\frac{p(x)p(x|u,\\theta,v,\\beta,z)}{q(x|u,\\theta,v,\\beta,z)}]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{p(x)}\\mathbb{E}_{q}[\\log\\frac{q(x|u,\\theta,v,\\beta,z)}{p(x)}]+\\mathbb{E}_{p(x)}\\mathbb{E}_{q}[\\log p(x)]+\\mathbb{E}_{p(x)}\\mathbb{E}_{q}[\\log\\frac{p(x|u,\\theta,v,\\beta,z)}{q(x|u,\\theta,v,\\beta,z)}]}\\\\ &{\\quad\\quad\\quad=I_{s}(x;u,\\theta,v,\\beta,z)+H(x)-\\mathrm{KL}[q(x|u,\\theta,v,\\beta,z)||p(x|u,\\theta,v,\\beta,z)]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, we apply these two bounds on the Eq. 41 and then we can proove the Lemma C.1: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p(x,y)}[\\mathcal{L}_{\\mathrm{ELBO}}(p(\\pmb{x},y))]\\leq\\!I(y;z)+I(\\pmb{x};\\pmb{u},\\pmb{\\theta},z,v)-(H(\\pmb{x})+H(y))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\mathbb{E}_{q(\\pmb{x},\\pmb{u},\\pmb{\\theta},z,v)}[\\mathrm{KL}[q(\\pmb{x}|\\pmb{u},\\pmb{\\theta},v,z)||p(\\pmb{x}|\\pmb{u},\\pmb{\\theta},v,z)]]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\mathrm{KL}[q(\\pmb{u},\\pmb{\\theta},v,z|\\pmb{x})||p(\\pmb{u},\\pmb{\\theta},z,v)]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma C.2 (Information Decomposition of the Adversarial Loss $I4I J$ We can decompose the global maximum of adversial loss as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{D}\\mathbb{E}_{p(w,\\pmb{x})}\\mathbb{E}_{q(\\pmb{z}|\\pmb{x})}[\\log\\mathrm{D}(w|\\pmb{z})]=I(\\pmb{z};\\pmb{\\theta},v)+I(\\pmb{z},w|\\pmb{\\theta},v)-H(w)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The global minimum of the function is achieved if and only $\\!\\!\\!/f I(z;\\theta)=0$ and $I(z,w|\\pmb\\theta)=0$ ", "page_idx": 16}, {"type": "text", "text": "Proof. With denoting $t(w,\\pmb{x},\\pmb{\\theta},z)=p(w,\\pmb{x})q(z|\\pmb{x})q(\\pmb{\\theta},v|w)$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{p(w,x)}\\mathbb{E}_{q(z|x)}\\bigl[\\log\\mathrm{D}(w|z)\\bigr]=\\mathbb{E}_{t(w,z)}\\bigl[\\log\\mathrm{D}(w|z)\\bigr]\\leq\\mathbb{E}_{t(w,z)}\\bigl[\\log t(w|z)\\bigr]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $\\mathbb{E}_{p(w,\\pmb{x})}\\mathbb{E}_{q(\\pmb{z}|\\pmb{x})}[\\log\\mathrm{D}(w|\\pmb{z})]$ achieves the maximum when $t(w,\\pmb{x},\\pmb{\\theta},z)=D(w|z)$ . To futher analyze the joint distribution $t(w,\\pmb{\\theta},z):=q(z|\\pmb{\\theta},v,w)q(\\pmb{\\theta},v|w)p(w)$ , we assume that there is a function $v(w)$ mapping $w$ to a group of domain-related weights $v$ , then we can have: ", "page_idx": 17}, {"type": "equation", "text": "$$\np(z|\\theta,v,w)=p(z|\\theta,v(w),w)=p(z|\\theta^{v(w)})=p(z|w)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Accordingly, we can factorize the joint distribution $t(w,\\pmb{\\theta},v,z)=q(z|w)q(\\pmb{\\theta}^{v}|w)p(w)$ . Therefore, we can factorize $I(z;\\theta,v,w)$ into two different styles with the chain rule for mutual information: ", "page_idx": 17}, {"type": "text", "text": "$I(z;\\pmb\\theta,v)+I(z,w|\\pmb\\theta,v)=I(z;\\pmb\\theta,v,w)=I(z;w)+I_{q}(z,\\pmb\\theta,v|w)\\,,$ where $I_{q}(z,\\pmb\\theta,v|w)=0$ due to the chain rule. That means: ", "page_idx": 17}, {"type": "equation", "text": "$$\nI(z;w)=I(z;\\pmb\\theta,v)+I(z,w|\\pmb\\theta,v)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "And we also have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{t(w,z)}[\\log t(w|z)]=\\mathbb{E}_{t(w,z)}[\\log\\frac{t(w|z)}{q(w)}]+\\mathbb{E}_{t(w,z)}[\\log q(w)]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=I(w;z)-H(w)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{D}\\mathbb{E}_{p(w,x)}\\mathbb{E}_{q(z|x)}[\\log{\\mathrm{D}(w|z)}]=I(w;z)-H(w)=I(z;\\theta,v)+I(z,w|\\theta,v)-H(w)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Accordingly, min maxD $\\begin{array}{r}{\\mathbb{E}_{p(w,\\mathbf{x})}\\mathbb{E}_{q(z|\\mathbf{x})}[\\log\\mathrm{D}(w|z)]=0}\\end{array}$ if and only if $I(z;\\pmb\\theta,v)=I(z,w|\\pmb\\theta,v)=$ 0 due to the fact that $I(\\cdot)\\geq0$ . ", "page_idx": 17}, {"type": "text", "text": "Theorem C.1 The upper bound of the objective function can be decomposed as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{GMDI}}\\leq I(y;z)+I(x;u,\\theta,z,v)-I(z;\\theta)-I(z;w|\\theta)-\\left(H(x)+H(y)-H(w)\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The main difference between Theorem C.1 in GMDI and Theorem B.1 in VDI [41] is the inclusion of $v$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. To proove the theorem, we apply the Lemma C.1 and C.2 to directly get the final upper bound: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{p(x,y)}[\\mathcal{L}_{\\mathrm{ELBO}}(p(x,y))]-\\underset{D}{\\mathrm{max}}\\,\\mathbb{E}_{p(w,x)}\\mathbb{E}_{q(z\\mid x)}[\\log\\mathrm{D}(w\\vert z)]}\\\\ &{\\le I(y;z)+I(x;u,\\theta,z,v)-I(z;\\theta)-I(z;w\\vert\\theta)-\\left(H(x)+H(y)-H(w)\\right)}\\\\ &{\\quad-\\,\\mathbb{E}_{q(x,u,\\theta,z,v)}[\\mathrm{KL}[q(x|u,\\theta,v,z)\\vert\\vert p(x|u,\\theta,v,z)]]-\\mathrm{KL}[q(u,\\theta,v,z\\vert x)\\vert\\vert p(u,\\theta,z,v)]}\\\\ &{\\le I(y;z)+I(x;u,\\theta,z,v)-I(z;\\theta)-I(z;w\\vert\\theta)-\\left(H(x)+H(y)-H(w)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second equality holds when all the terms of KL-divergence are equal to zero. ", "page_idx": 17}, {"type": "text", "text": "Theorem C.2 The global optimum is achieved if and only if: $(l)I(z;\\theta)\\ =\\ I(z;w|\\theta)\\ =$ 0, $(2)I(y;z)$ and $I(x;u,\\pmb\\theta,z,v)$ are maximized, $(3)\\mathrm{KL}[q({\\pmb u},{\\pmb\\theta},v,z|{\\pmb x})||p({\\pmb u},{\\pmb\\theta},v,z)]\\ =\\ 0$ and $\\begin{array}{r}{\\mathrm{KL}\\big[q(\\pmb{x}|\\pmb{u},\\pmb{\\theta},\\upsilon,z)||p(\\pmb{x}|\\pmb{u},\\pmb{\\theta},\\upsilon,z)\\big]=0.}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "The main difference between Theorem C.2 in GMDI and Theorem B.2 in VDI [41] is that $I(x;u,\\pmb\\theta,z,v)$ , which includes $v$ , needs to be maximized, and the two KL divergences should equal zero. ", "page_idx": 17}, {"type": "text", "text": "Proof. The theorem can be prooved by observing the conditions from Lemma C.1, C.2 and Theorem C.1. ", "page_idx": 17}, {"type": "text", "text": "Theorem C.3 Assuming the ELBO and objective of VDI are $\\mathcal{L}_{\\mathrm{VDI-ELBO}}$ and $\\mathcal{L}_{\\mathrm{VDI}}$ respectively, where domain indices are sampled from a simple Gaussian prior, we can prove that our objective achieves a more stringent evidence lower bound which is closer to the log-likelihood, and also $a$ tighter upper bound of the objective: $\\mathcal{L}_{\\mathrm{VDI-ELBO}}\\leq\\mathcal{L}_{\\mathrm{ELBO}}\\leq\\log p(\\mathbf{x},y)$ and $\\mathcal{L}_{\\mathrm{VDI}}\\le\\mathcal{L}_{\\mathrm{GMDI}}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. To compare our objective loss with the VDI\u2019s, we first list the ELBO loss of VDI here and provide an upper bound: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{VDI-ELBO}}(p(\\pmb{x},\\pmb{y}))=\\mathbb{E}_{q}[\\log p(\\pmb{y}|\\pmb{z})]+\\mathbb{E}_{q}[\\log p(\\pmb{x}|\\pmb{u},\\hat{\\pmb{\\theta}},\\pmb{z})]-\\mathrm{KL}[p(\\pmb{u},\\hat{\\pmb{\\theta}},\\pmb{z})||q(\\pmb{u},\\hat{\\pmb{\\theta}},\\pmb{z}|\\pmb{x})]}\\\\ &{\\quad\\quad\\quad\\quad\\leq I(\\pmb{y};\\pmb{z})+I(\\pmb{x};\\pmb{u},\\hat{\\pmb{\\theta}},\\pmb{z},\\pmb{v})-(H(\\pmb{x})+H(\\pmb{y}))}\\\\ &{\\quad\\quad\\quad\\quad\\quad-\\mathbb{E}_{q(\\pmb{x},\\pmb{u},\\hat{\\pmb{\\theta}},\\pmb{z})}[\\mathrm{KL}[q(\\pmb{x}|\\pmb{u},\\hat{\\pmb{\\theta}},\\pmb{z})||p(\\pmb{x}|\\pmb{u},\\hat{\\pmb{\\theta}},\\pmb{z})]]}\\\\ &{\\quad\\quad\\quad\\quad\\quad-\\mathrm{KL}[q(\\pmb{u},\\hat{\\pmb{\\theta}},\\pmb{z}|\\pmb{x})||p(\\pmb{u},\\hat{\\pmb{\\theta}},\\pmb{z})]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can observe that the most significant difference is the prior distribution, which mainly affects the term of KL-divergence. To further analyze, it is obvious that when $\\pmb{\\theta}^{v=k}=\\hat{\\pmb{\\theta}}$ , VDI is a special case of our proposed method GMDI. Hence we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\nm a x\\mathcal{L}_{\\mathrm{VDI-ELBO}}(p(\\pmb{x},y))\\leq\\operatorname*{max}\\mathcal{L}_{\\mathrm{GMDI-ELBO}}(p(\\pmb{x},y))\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Further more, we can notice that the adversarial loss is independent of the prior distribution of global indices and we can have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{VDI}}=\\operatorname*{max}\\underset{\\boldsymbol{D}}{\\operatorname*{min}}\\,\\mathcal{L}_{\\mathrm{VDI-ELBO}}(p(\\boldsymbol{x},\\boldsymbol{y}))-\\lambda*\\mathcal{L}_{\\mathrm{D}}}\\\\ &{\\qquad\\leq\\operatorname*{max}\\underset{\\boldsymbol{D}}{\\operatorname*{min}}\\,\\mathcal{L}_{\\mathrm{GMDI-ELBO}}(p(\\boldsymbol{x},\\boldsymbol{y}))-\\lambda*\\mathcal{L}_{\\mathrm{D}}}\\\\ &{\\qquad=\\mathcal{L}_{\\mathrm{GMDI}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D Visualization of Inferred Domain Indices for Circle ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Figure 8 shows the inferred domain indices for Circle dataset. GMDI\u2019s inferred indices have a correlation of 0.99 with true indices, even though GMDI does not have access to true indices during training. ", "page_idx": 18}, {"type": "image", "img_path": "Grd7yzFm5V/tmp/305bd14836fc2ba9992e953deac4725e798709508795502e4940c43b3a75f1c3.jpg", "img_caption": ["Figure 8: Inferred domain indices (reduced to 1 dimension by PCA) with true domain indices for dataset Circle. GMDI\u2019s inferred indices have a correlation of 0.99 with true indices, even though GMDI does not have access to true indices during training. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "E Architecture and Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Architecture ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The latent variables are estimated by neural networks. Specifically, for the local domain index $\\textbf{\\em u}$ , we employ ResNet-18 [9] to approximate its posterior on CompCars dataset, while using multi-layer perceptrons for the other datasets. Additionally, all neural networks are implemented as multi-layer perceptrons. We use the features obtained from the ResNet-18 as inputs to our model. Furthermore, all inputs are uniformly normalized. ", "page_idx": 18}, {"type": "text", "text": "We implement our model based on the code of VDI[41]. We appreciate the authors for making their code publicly available. We run experiments on a single machine using 1 NVIDIA GeForce RTX $2080\\,\\mathrm{Ti}$ with 11GB memory, 56 Intel Xeon CPUs (E5-2680 v4 $@$ 2.40GHz). It takes about 30 minutes to train GMDI with $K{=}2$ on synthetic datasets , 4 hours on TPT-48, and 6 hours on CompCars with $K{=}3$ . ", "page_idx": 18}, {"type": "text", "text": "E.2 Hyperparameters ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We set the maximum number of mixture components $K$ from 2,3, and the concentration parameter $\\alpha$ to 1 throughout the experiments. Except for $D G{-}l5$ and $D G{-}6O$ datasets, which have a batch size of 32, all other datasets use a batch size of 16. Our model is trained with 20 to 100 warmup steps, learning rates ranging from $1\\times10^{-5}$ to $1\\times10^{-3}$ , and $\\lambda$ ranging from 0.1 to 1. ", "page_idx": 19}, {"type": "text", "text": "F Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our model has the potential to be applied to various domain shift problems, which also implies the possibility of unintended negative consequences. However, we have not identified any specific societal harms associated with our model. If used maliciously, it could lead to negative impacts. ", "page_idx": 19}, {"type": "text", "text": "G Pseudo Codes ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The procedure of our proposed model GMDI is summarized by the pesuedo codes in Algorithm G. ", "page_idx": 19}, {"type": "text", "text": "Algorithm 1 Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing   \nInput: Dataset $\\mathcal{D}^{S}$ and $\\mathcal{D}^{T}$ , maximum number of mixture components $K$ , concentration parameter $\\alpha$ , learning rate $\\zeta$ .   \n1: Initialize parameters: $\\psi=\\{\\psi_{u},\\psi_{\\theta},\\psi_{z}\\}\\,;\\eta_{k},\\forall k=1,...,K;$   \n2: repeat   \n3: Update $\\gamma_{k}$ with $\\gamma_{k,1}=1+\\eta_{k}$ and $\\begin{array}{r}{\\gamma_{k,2}=\\alpha+\\sum_{i=k+1}^{K}\\eta_{i}}\\end{array}$ , $\\forall k=1,...,K$ ;   \n4: Update $\\eta$ with Equation 37;   \n5: Sample $\\textbf{\\em u}$ from Equation39;   \n6: Sample $\\pmb{\\theta}$ based on Equation38;   \n7: Sample $_{z}$ according to Equation 40;   \n8: Update $\\psi\\leftarrow\\psi+\\zeta\\nabla_{\\psi}\\mathcal{L}_{\\mathrm{GMDI}}$ .   \n9: until converge ", "page_idx": 19}, {"type": "text", "text": "H Dataset Summary ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 3 [41] summarizes the statistics for all the datasets used in our experiments. ", "page_idx": 19}, {"type": "table", "img_path": "Grd7yzFm5V/tmp/a5925a350c0725fe889035ecadeab2c99c58aac6d05e9740f15b9adfe261e46d.jpg", "table_caption": ["Table 3: Summary of statistics and settings in different datasets. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "I Dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "I.1 Circle ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure 9 [41] visualizes the detailed information of Circle dataset. It contains 30 domains and is used for binary classification task. The data points in the Circle are arranged in a semicircular shape, with each domain occupying a different section of the semicircle. There is a decision boundary that separates the different labels. We use the first six domains as the source domains and the remaining 24 domains as the target domains. ", "page_idx": 19}, {"type": "image", "img_path": "Grd7yzFm5V/tmp/92c7fa20db7e902963906b99a7a8284ff30aa974b2e8bef193b4b27e6358a728.jpg", "img_caption": ["Figure 9: The Circle dataset with 30 domains. Left: Different colors indicate ground-truth domain indices. The first 6 domains (in the green box) are source domains. Right: Ground-truth labels for Circle, with red dots and blue crosses as positive and negative data points, respectively. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "I.2 DG-15 and DG-60 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "DG-15 and DG-60 datasets containing 15 and 60 domains, respectively. Both used for binary classification task. Adjacent domains in the datasets have similar decision boundaries. For these two datasets, we select 6 connected domains as the source domains, while the remaining domains serve as the target domains. ", "page_idx": 20}, {"type": "text", "text": "I.3 TPT-48 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Figure 10 [40] visualizes the detailed information of TPT-48 dataset. It contains the monthly average temperatures of 48 contiguous states in the United States from 2008 to 2019. Our regression task is to predict the average temperatures for the next 6 months using the average temperatures of the first 6 months. We divide this task into two finer-grained regression tasks with different adaptation directions: ", "page_idx": 20}, {"type": "text", "text": "\u2022 W $^7\\left(6\\right)\\rightarrow\\mathrm{E}$ (42): Adapting models from the 6 states in the west to the 42 states in the east. \u2022 $\\mathrm{~N~}(24)\\rightarrow\\mathrm{~S~}$ (24): Adapting models from the 24 states in the north to the 24 states in the south. ", "page_idx": 20}, {"type": "text", "text": "To better verify performance, the target domains in the above two regression tasks are divided into three groups based on their distance from the closest source domains. level-1 target domains: one hop away from the closest source domain. level-2 target domains: two hops away from the closest source domain. level-3 target domains: more than two hops away from the closest source domain. ", "page_idx": 20}, {"type": "image", "img_path": "Grd7yzFm5V/tmp/aedfe524ed987f7a5e7a3601bf2c8a2217bdde8f133ba58f591a9e3a5e62f411.jpg", "img_caption": ["Figure 10: Domain graphs for the two adaptation tasks on TPT-48, with black nodes indicating source domains and white nodes indicating target domains. Left: Adaptation from the 24 states in the east to the 24 states in the west. Right: Adaptation from the 24 states in the north to the 24 states in the south. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "I.4 CompCars ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "CompCars dataset includes three attributes: car types, viewpoints, and years of manufacture (YOMs). We select a subset of CompCars with 4 types (MPV, SUV, sedan and hatchback), 5 viewpoints (front(F), rear (R), side (S), front-side (FS), and rear-side (RS)), and 6 YOMs(2009, 2010, 2011, 2012, 2013, 2014) for our experiments. This subset is divided into 30 domains(5 viewpoints $\\times\\,6$ YOMs) based on viewpoints and YOMs. The car types are used as labels for prediction. The first domain, which has the front view and YOM 2009, is treated as the source domain, while the remaining 29 domains are target domains. ", "page_idx": 21}, {"type": "text", "text": "J Notation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table 4 provides a summary of the notations used in this paper. ", "page_idx": 21}, {"type": "text", "text": "Table 4: Summary of notations. ", "page_idx": 21}, {"type": "table", "img_path": "Grd7yzFm5V/tmp/dc83630084335957a1984e9834b5a1c5233af40ee850c2aff1ceb4b9a3a984e1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "K Additional Experimental Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To evaluate the impact and computational cost of the CRP, we conduct ablation and computational cost experiments.We implement GMDI w/o CRP using Gumbel-Softmax [11]. The number of components for GMDI w/o CRP is set to the upper bound $K$ of GMDI, and the hyperparameter temperature $\\tau$ for Gumbel-Softmax ranges from 0.1 to 50 (with the best performance reported). \"Total time\" refers to the total training duration, which concludes when the loss converges. ", "page_idx": 21}, {"type": "table", "img_path": "Grd7yzFm5V/tmp/17b42e9965d69d5a503d0243216ecd22991dd97565db56d02bbbbc3555d1066b.jpg", "table_caption": ["Table 5: The results of the ablation and computational cost experiments on TPT-48. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "Grd7yzFm5V/tmp/0ed61c6f9f056aaa71f5a7d0726d04fa22fd6eb967b9c1fa06159abc9ee81754.jpg", "table_caption": ["Table 6: The results of the ablation and computational cost experiments on CompCars. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "The experimental results are shown in Table 5 and Table 6. We find that although the proposed GMDI has a longer \"Time per epoch\" compared to GMDI w/o CRP, it converges faster due to the flexible number of components adaptively controlled by CRP. Therefore, the \"Total time\" is roughly the same as GMDI w/o CRP. On the $T P T{\\cdot}48(\\mathrm{W}{\\cdot}{\\mathrm{>}}\\mathrm{E})$ dataset, due to faster convergence, the \"Total time\" of GMDI is less than that of GMDI w/o CRP and is even comparable to VDI. In all three datasets, the performance of GMDI w/o CRP is worse than that of GMDI. On the two TPT-48 datasets, compared to the baseline VDI, GMDI w/o CRP reduces MSE by $1\\%$ and $3\\%$ , whereas GMDI reduces MSE by $16\\%$ and $21\\%$ , surpassing GMDI w/o CRP. On the CompCars dataset, GMDI\u2019s accuracy is higher than that of GMDI w/o CRP. These results indicate that although using a fixed-component GMM is simpler, the computational costs are roughly equivalent to using CRP, but the performance is inferior to CRP, demonstrating the significance of CRP in GMDI. ", "page_idx": 22}, {"type": "text", "text": "Additionally, compared to VDI, which models the domain index as a single Gaussian distribution, GMDI\u2019s computational costs are only slightly higher, yet its performance is superior. For large-scale datasets with numerous domains, modeling the domain index as a simple single Gaussian distribution may result in poor performance due to the dataset\u2019s complexity. The experimental results indicate that GMDI has broad applicability. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Abstract and Section 1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: See Section 7. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Codes and dataset to reproduce the experiments are included in the supplemental material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Data and code will be publicly available once the paper is accepted. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Experimental training details are provided in Appendix Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We report performance mean after running experiments for multiple times with different seeds. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The amount and the type of computing resource used in our experiments are described in Appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our research is conducted with the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Citations for the existing assets (code, data and baseline models) are provided in Section 6 and Appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]