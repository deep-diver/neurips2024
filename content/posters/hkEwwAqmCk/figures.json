[{"figure_path": "hkEwwAqmCk/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Traditional DAOD methods optimize the backbone adversarially. (b) Domain-agnostic adapter is inserted into the frozen visual encoder to learn domain-invariant knowledge. (c) Domain-aware adapter can simultaneously capture the domain-specific knowledge from the discarded feature. (d) The mAP(%) comparison on the Cross-Weather Adaptation. Compared with original VLM, domain-agnostic adapter brings significant improvement to the source domain but limited improvement to the source domain, while domain-aware adapter brings significant improvement to both source domain and target domain.", "description": "This figure shows a comparison of three different approaches to domain adaptive object detection (DAOD): traditional methods, domain-agnostic adapter, and domain-aware adapter.  Traditional methods fine-tune the entire backbone, leading to potential overfitting.  The domain-agnostic adapter inserts a module into a frozen visual encoder to learn domain-invariant features, but it may discard useful domain-specific information. The domain-aware adapter, in contrast, captures both domain-invariant and domain-specific knowledge, resulting in better performance, as shown in the mAP comparison on a Cross-Weather Adaptation task.", "section": "1 Introduction"}, {"figure_path": "hkEwwAqmCk/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the proposed (a) DA-Ada for DAOD and the architecture of (b) the i-th domain-aware adapter module (c) the visual-guided textual adapter.", "description": "This figure shows the overall architecture of the proposed DA-Ada framework for domain adaptive object detection (DAOD).  Part (a) presents a schematic of the entire system, illustrating how the DA-Ada modules are integrated into a visual encoder, and the interaction with textual encoders for improved cross-domain generalization. Parts (b) and (c) detail the DA-Ada module and the Visual-guided Textual Adapter (VTA), respectively.  The DA-Ada module is a key component that exploits both domain-invariant and domain-specific knowledge, enhancing the robustness and generalization ability of the detector. The VTA further enhances the discriminative power of the detection head by leveraging visual features and textual information.", "section": "3 Methodology"}, {"figure_path": "hkEwwAqmCk/figures/figures_5_1.jpg", "caption": "Figure 3: Comparison between (a) DA-Pro and (b) Visual-guided textual adapter.", "description": "This figure compares two approaches for adapting visual-language models to domain adaptive object detection.  (a) shows DA-Pro, which tunes only the textual encoder using learnable textual descriptions. (b) shows the proposed Visual-guided Textual Adapter (VTA), which leverages both domain-invariant and domain-specific knowledge from the visual encoder to enhance the textual encoder's discriminative power, leading to improved detection performance on the target domain.", "section": "3.4 Visual-guided Textual Adapter (VTA)"}, {"figure_path": "hkEwwAqmCk/figures/figures_9_1.jpg", "caption": "Figure 4: Detection comparison on the Cross-Weather adaptation scenario. We visualize the ground truth (a), the detection boxes of SOTA DA-Pro [31](c) and our methods (b)(d). mAP: mean Average Precision on the example image", "description": "This figure compares the object detection results of the proposed DA-Ada model against the state-of-the-art DA-Pro model and a baseline on a sample image from the Cross-Weather dataset (Cityscapes \u2192 Foggy Cityscapes).  Subfigures (a.1), (b.1), (c.1), and (d.1) provide zoomed-in views of a specific region of the image, highlighting the differences in detection accuracy. DA-Ada demonstrates superior performance, achieving a higher mean Average Precision (mAP) value compared to DA-Pro and the baseline. This showcases DA-Ada\u2019s improved ability to handle challenging weather conditions.", "section": "4.4 Detection Visualization"}, {"figure_path": "hkEwwAqmCk/figures/figures_9_2.jpg", "caption": "Figure 2: Overview of the proposed (a) DA-Ada for DAOD and the architecture of (b) the i-th domain-aware adapter module (c) the visual-guided textual adapter.", "description": "This figure shows the proposed DA-Ada architecture for domain adaptive object detection (DAOD).  (a) provides a high-level overview of how DA-Ada is integrated into a visual language model (VLM) for DAOD. (b) details the structure of a single domain-aware adapter module (DA-Ada), which consists of a domain-invariant adapter (DIA) and a domain-specific adapter (DSA).  (c) illustrates the visual-guided textual adapter (VTA), which leverages the output of DA-Ada to improve the textual encoder's discriminative ability. The figure highlights the key components and their interactions to achieve improved domain adaptation performance.", "section": "3 Methodology"}, {"figure_path": "hkEwwAqmCk/figures/figures_18_1.jpg", "caption": "Figure 4: Detection comparison on the Cross-Weather adaptation scenario. We visualize the ground truth (a), the detection boxes of SOTA DA-Pro [31](c) and our methods (b)(d). mAP: mean Average Precision on the example image", "description": "This figure compares the object detection results of the proposed DA-Ada method against the state-of-the-art DA-Pro method and a baseline method on the Cross-Weather dataset.  It visually demonstrates the improved performance of DA-Ada by showing ground truth bounding boxes alongside the detection bounding boxes generated by each method. The zoomed-in sections highlight the differences in detection accuracy, particularly in challenging conditions such as fog.", "section": "4.4 Detection Visualization"}]