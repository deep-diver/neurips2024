[{"type": "text", "text": "Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Thalaiyasingam Ajanthan ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Evan Markou Australian National University evan.markou@anu.edu.au ", "page_idx": 0}, {"type": "text", "text": "Australian National University & Amazon thalaiyasingam.ajanthan@anu.edu.au ", "page_idx": 0}, {"type": "text", "text": "Stephen Gould Australian National University stephen.gould@anu.edu.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural Collapse (NC) is a recently observed phenomenon in neural networks that characterises the solution space of the final classifier layer when trained until zero training loss. Specifically, NC suggests that the final classifier layer converges to a Simplex Equiangular Tight Frame (ETF), which maximally separates the weights corresponding to each class. By duality, the penultimate layer feature means also converge to the same simplex ETF. Since this simple symmetric structure is optimal, our idea is to utilise this property to improve convergence speed. Specifically, we introduce the notion of nearest simplex ETF geometry for the penultimate layer features at any given training iteration, by formulating it as a Riemannian optimisation. Then, at each iteration, the classifier weights are implicitly set to the nearest simplex ETF by solving this inner-optimisation, which is encapsulated within a declarative node to allow backpropagation. Our experiments on synthetic and real-world architectures for classification tasks demonstrate that our approach accelerates convergence and enhances training stability1. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While modern deep neural networks (DNNs) have demonstrated remarkable success in solving diverse machine learning problems [22, 34, 38], the fundamental mechanisms underlying their training process remain elusive. In recent years, considerable research efforts have focused on delineating the optimisation trajectory and characterising the solution space resulting from the optimisation process in training neural networks [72, 17, 49, 41]. One such finding is that gradient descent algorithms, when combined with certain loss functions, introduce an implicit bias that often favours max-margin solutions, influencing the learned representations and decision boundaries. [44, 57, 33, 27, 21, 54, 70, 31, 49]. ", "page_idx": 0}, {"type": "text", "text": "In this vein, Neural Collapse (NC) is a recently observed phenomenon in neural networks that characterises the solution space of the final classifier layer in both balanced [50, 76, 74, 28, 48, 63, 43, 32] and imbalanced dataset settings [19, 59, 5]. Specifically, NC suggests that the final classifier layer converges to a Simplex Equiangular Tight Frame (ETF), which maximally separates the weights corresponding to each class, and by duality, the penultimate layer feature means converge to the classifier weights, i.e., to the simplex ETF (formal definitions are provided in Appendix A). This simple, symmetric structure is shown to be the only set of optimal solutions for a variety of loss functions when the features are also assumed to be free parameters, i.e., Unconstrained Feature ", "page_idx": 0}, {"type": "text", "text": "Models (UFMs) [32, 19, 74, 76, 28, 75]. Nevertheless, even in realistic large-scale deep networks, this phenomenon is observed when trained to convergence, even after attaining zero training error. ", "page_idx": 1}, {"type": "text", "text": "Since we can characterise the optimal solution space for the classifier layer, a natural extension is to leverage the simplex ETF structure of the classifier weights to improve training. To this end, researchers have tried fixing the classifier weights to a canonical simplex ETF, effectively reducing the number of trainable parameters [76]. However, in practice, this approach does not improve the convergence speed as the backbone network still needs to do the heavy lifting of matching feature means to the chosen fixed simplex ETF. ", "page_idx": 1}, {"type": "text", "text": "In this work, we introduce a mechanism for finding the nearest simplex ETF to the features at any given training iteration. Specifically, the nearest simplex ETF is determined by solving a Riemannian optimisation problem. Therefore, our classifier weights are dynamically updated based on the penultimate layer feature means at each iteration, i.e., implicitly defined rather than trained using gradient descent. Additionally, by constructing this inner-optimisation problem as a deep declarative node [23], we allow gradients to propagate through the Riemannian optimisation facilitating end-toend learning. Our whole framework significantly speeds up convergence to a NC solution compared to the fixed simplex ETF and conventional learnable classifier approaches. We demonstrate the effectiveness of our approach on synthetic UFMs and standard image classification experiments. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We introduce the notion of the nearest simplex ETF geometry given the penultimate layer features. Instead of selecting a predetermined simplex ETF (canonical or random), we implicitly fix the classifier as the solution to a Riemannian optimisation problem.   \n2. To establish end-to-end learning, we encapsulate the Riemannian optimisation problem of determining the nearest simplex ETF geometry within a declarative node. This allows for efficient backpropagation throughout the network.   \n3. We demonstrate that our method achieves an optimal neural collapse solution more rapidly compared to fixed simplex ETF methods or conventional training approaches, where a learned linear classifier is employed. Additionally, our method ensures training stability by markedly reducing variance in network performance. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Neural Collapse and Simplex ETFs. Zhu et al. [76] proposed fixing classifier weights to a simplex ETF, reducing parameters while maintaining performance. Simplex ETFs effectively tackle imbalanced learning, as demonstrated by Yang et al. [67], where they fix the target classifier to an arbitrary simplex ETF, relying on the network\u2019s over-parameterisation to adapt. Similarly, Yang et al. [68] addressed class incremental learning by fixing the target classifier to a simplex ETF. They advocate adjusting prototype means towards the simplex ETF using a convex combination, smoothly guiding backbone features into the targeted simplex ETF. However, these methods did not yield any benefits regarding convergence speed. The work most relevant to ours is that of Peifeng et al. [51], who argued about the significance of feature directions, particularly in long-tailed learning scenarios. They compared their method against a fixed simplex ETF target, formulating their problem to enable the network to learn feature direction through a rotation matrix. Additionally, they efficiently addressed their optimisation using trivialisation techniques [39, 40]. However, they did not demonstrate any improvements in convergence speed over the fixed simplex ETF, achieving only a minimal increase in test accuracy. Fixing a classifier is not a recent concept, as it has been proposed prior to the emergence of neural collapse [52, 58, 30]. Most notably, Pernici et al. [52] demonstrated improved convergence speed by fixing the classifier to a simplex structure only on ImageNet while maintaining comparable performance on smaller-scale datasets. In contrast, our method shows superior convergence speed compared to both a fixed simplex ETF and a learned classifier across both small and large-scale datasets. ", "page_idx": 1}, {"type": "text", "text": "Optimisation on Smooth Manifolds. Our optimisation problem involves orthogonality constraints, characterised by the Stiefel manifold [2, 11]. Due to the nonlinearity of these constraints, efficiently solving such problems requires leveraging Riemannian geometry [18]. A multitude of works are dedicated to solving such problems by either transforming existing classical optimisation techniques into Riemannian equivalent algorithms [1, 73, 20, 64, 55] or by carefully designing penalty functions to address equivalent unconstrained problems [66, 65]. In our approach, we opt for a retraction-based Riemannian optimisation algorithm [1] to optimally handle orthogonality constraints. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Implicit Differentiable Optimisation. In a neural network setting, end-to-end architectures are commonplace. To backpropagate solutions to optimisation problems, we rely on machinery from implicit differentiation. Pioneering works [4, 3] demonstrated efficient gradient backpropagation when dealing with solutions of convex optimisation problems. This concept was independently introduced as a generalised version by Gould et al. [23, 24] to encompass any twice-differentiable optimisation problem. A key advantage of Deep Declarative Networks (DDNs) lies in their ability to efficiently solve problems at any scale by leveraging the problem\u2019s underlying structure [25]. Our setting involves utilising an equality-constrained declarative node to efficiently backpropagate through the network. ", "page_idx": 2}, {"type": "text", "text": "3 Optimising Towards the Nearest Simplex ETF ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce our method to determine the nearest simplex ETF geometry and detail how we can dynamically steer the training algorithm to converge towards this particular solution. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let us first introduce key notation that will be useful when formulating our optimisation problem. ", "page_idx": 2}, {"type": "text", "text": "Simplex ETF. Mathematically, a general simplex ETF is a collection of points in $\\mathbb{R}^{C}$ specified by the columns of a matrix ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\cal M}=\\alpha\\sqrt{\\frac{C}{C-1}}{\\cal U}\\left({\\cal I}_{C}-\\frac{1}{C}{\\bf1}_{C}{\\bf1}_{C}^{\\top}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\alpha\\,\\in\\,\\mathbb{R}_{+}$ denotes an arbitrary scale factor, $\\mathbf{1}_{C}$ is the $C$ -dimensional vector of ones, and $U\\in\\mathbb{R}^{d\\times C}$ (with $d\\geq C,$ ) represents a semi-orthogonal matrix $(U^{\\top}U=I_{C})$ ). Note that there are many simplex ETFs in $\\mathbb{R}^{C}$ as the rotation $U$ varies, and $_M$ is rank-deficient. Additionally, the standard simplex ETF with unit Frobenius norm is defined as: $\\begin{array}{r}{{\\tilde{M}}=\\frac{1}{\\sqrt{C-1}}\\left(I_{C}-\\frac{1}{C}\\mathbf{1}_{C}\\mathbf{1}_{C}^{\\top}\\right)}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Mean of Features. Consider a classification dataset $\\mathcal{D}=\\{(\\pmb{x}_{i},y_{i})~|~i=1,\\ldots,N\\}$ where the data $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathcal{X}$ and labels $y_{i}\\in\\mathcal{Y}=\\{1,...\\,,C\\}$ . Suppose, $n_{c}$ is the number of samples correspond to label $c$ , then $\\begin{array}{r}{\\sum_{c=1}^{C}n_{c}=N}\\end{array}$ . Let us consider a scenario where we have a collection of features defined as, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{H}\\triangleq[h_{c,i}:1\\leq c\\leq C,1\\leq i\\leq n_{c}]\\in\\mathbb{R}^{d\\times N}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, each feature may originate from a nonlinear compound mapping of input data through a neural network, denoted as, $h_{y_{i},i}\\,=\\,\\phi_{\\pmb{\\theta}}(\\pmb{x}_{i})$ for the data sample $({\\pmb x}_{i},y_{i})$ . Now, for the final layer, our decision variables (weights and biases) are represented as $W\\triangleq[\\pmb{w}_{1},\\dots,\\pmb{w}_{C}]^{\\top}\\in\\mathbb{R}^{C\\times d}$ , and $\\pmb{b}\\in\\mathbb{R}^{C}$ , and the logits for the $i$ -th sample is computed as, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\psi_{\\pmb{\\Theta}}(\\pmb{x}_{i})=\\pmb{W}h_{y_{i},i}+\\pmb{b}\\,,\\qquad\\mathrm{where}\\quad h_{y_{i},i}=\\phi_{\\pmb{\\theta}}(\\pmb{x}_{i})\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In UFMs, the features are assumed to be free variables, which serves as a rough approximation for neural networks and helps derive theoretical guarantees. Additionally, we define the global mean and per-class mean of the features $\\{h_{c,i}\\}$ as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nh_{G}\\triangleq\\frac{1}{N}\\sum_{c=1}^{C}\\sum_{i=1}^{n_{c}}h_{c,i}\\ ,\\quad\\bar{h}_{c}\\triangleq\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}h_{c,i}\\ ,\\quad(1\\leq c\\leq C)\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and the globally centred feature mean matrix as, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\bar{\\pmb{H}}\\triangleq[\\bar{\\pmb{h}}_{1}-\\pmb{h}_{G},\\dots,\\bar{\\pmb{h}}_{C}-\\pmb{h}_{G}]\\in\\mathbb{R}^{d\\times C}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Finally, we scale the feature mean matrix to have unit Frobenius norm, i.e., $\\tilde{\\pmb{H}}=\\bar{\\pmb{H}}/\\|\\bar{\\pmb{H}}\\|_{F}$ which will be used in formulation below. ", "page_idx": 2}, {"type": "text", "text": "3.2 Nearest Simplex ETF through Riemannian Optimisation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Once we obtain the feature means, our objective is to calculate the nearest simplex ETF based on these means and subsequently adjust the classifier weights $W$ to align with this particular simplex ETF. The rationale is to identify and establish a simplex ETF that closely corresponds to the feature means at any given iteration. This approach aims to expedite convergence during the training process by providing the algorithm with a starting point that is closer to an optimal solution rather than requiring it to learn a simplex ETF direction or converge towards an arbitrary one. ", "page_idx": 3}, {"type": "text", "text": "To find the nearest simplex ETF geometry, we solve the following Riemannian optimisation problem ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{U\\in S t_{C}^{d}}\\left\\|\\tilde{H}-U\\tilde{M}\\right\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $S t_{C}^{d}\\,=\\,\\{{\\pmb X}\\,\\in\\,\\mathbb{R}^{d\\times C}\\,:\\,{\\pmb X}^{\\top}{\\pmb X}\\,=\\,{\\pmb I}_{C}\\}$ . Here, $\\tilde{M}$ is the standard simplex ETF with unit Frobenius norm, and the set of the orthogonality constraints $S t_{C}^{d}$ forms a compact Stiefel manifold [2, 11] embedded in a Euclidean space. ", "page_idx": 3}, {"type": "text", "text": "3.3 Proximal Problem ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The solution to the Riemannian optimisation problem, denoted as $U^{\\star}$ , is not unique since a component of $U^{\\star}$ lies in the null space of $\\tilde{M}$ . As simplex ETFs reside in $\\left(C-1\\right)$ -dimensional space, the matrix $\\tilde{M}$ is rank-one deficient. Consequently, we are faced with a family of solutions, leading to challenges in training stability, as we may oscillate between multiple simplex ETF directions. We address this issue by introducing a proximal term to the problem\u2019s objective function. This guarantees the uniqueness of the solution and stabilises the training process, ensuring that our problem converges to a solution closer to the previous one. ", "page_idx": 3}, {"type": "text", "text": "So, the original problem in Equation 6 is transformed into: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{U\\in S t_{C}^{d}}\\left\\|\\tilde{H}-U\\tilde{M}\\right\\|_{F}^{2}+\\frac{\\delta}{2}\\Big\\|U-U_{\\mathrm{prox}}\\Big\\|_{F}^{2}\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $U_{\\mathrm{prox}}$ represents the proximal target simplex ETF direction, and $\\delta>0$ serves as the proximal coefficient, handling the trade-off between achieving the optimal solution\u2019s proximity to the feature means and its proximity to a given simplex ETF direction. In fact, one can perceive our problem formulation in Equation 7 as a generalisation to a predetermined fixed simplex ETF solution. This is evident when considering that if we significantly increase $\\delta$ , the optimal direction $U^{\\star}$ would converge towards the fixed proximal direction $U_{\\mathrm{prox}}$ . ", "page_idx": 3}, {"type": "text", "text": "3.4 General Learning Setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our problem formulation, following the general deep neural network architecture in Equation 3, can be seen as a bilevel optimisation problem as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{\\Theta}{\\mathrm{minimize~}}\\mathcal{L}(\\mathcal{D};\\Theta,U^{\\star})\\triangleq-\\frac{1}{N}\\sum_{i=1}^{N}\\log\\left(\\frac{\\exp\\left(\\psi_{\\Theta}(\\boldsymbol{x}_{i},U^{\\star})_{\\boldsymbol{y}_{i}}\\right)}{\\sum_{j=1}^{C}\\exp\\left(\\psi_{\\Theta}(\\boldsymbol{x}_{i},U^{\\star})_{j}\\right)}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$U^{\\star}\\in\\underset{U\\in S t_{C}^{d}}{\\arg\\operatorname*{min}}\\left\\|\\tilde{H}-U\\tilde{M}\\right\\|_{F}^{2}+\\frac{\\delta}{2}\\left\\|U-U_{\\mathrm{prox}}\\right\\|_{F}^{2}.$ ", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\psi_{\\Theta}(\\pmb{x}_{i},\\pmb{U}^{\\star})=\\tau M\\pmb{U}^{\\star}(\\pmb{h}_{i}-\\pmb{h}_{G})$ with $h_{i}=\\phi_{\\pmb{\\theta}}(\\pmb{x}_{i})$ . Here, $\\psi$ denotes the logits, where the classifier weights are set as $W=M U^{\\star}$ , and the bias is set to $\\pmb{b}=-M\\pmb{U}^{\\star}\\pmb{h}_{G}$ to account for feature centring. Furthermore, $_M$ is the standard simplex ETF, M\u02dc is its normalised version, and H\u02dc is the normalised centred feature matrix. The temperature parameter $\\tau>0$ controls the lower bound of the cross-entropy loss when dealing with normalised features, as defined in [69, Theorem 1]. ", "page_idx": 3}, {"type": "text", "text": "3.5 Handling Stochastic Updates ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In practice, we use stochastic gradient descent updates, and, as such, adjustments to our computations are necessary. With each gradient update now based on a mini-batch, we implement two key changes. First, rather than directly optimising the problem of finding the nearest simplex ETF geometry concerning the feature means of the mini-batch, we introduce an exponential moving average operation during the computation of the feature means. This operation accumulates statistics and enhances training stability throughout iterations. Formally, at time step $t$ , we have the following equation, where $\\alpha\\in\\mathbb{R}$ represents the smoothing factor: ", "page_idx": 3}, {"type": "image", "img_path": "z4FaPUslma/tmp/f5c2e937a2319a49e0bc9458a399697cbaea3b7f05c7f1d6a81bed0dccc92caf.jpg", "img_caption": ["Figure 1: Schematic of our proposed architecture for optimising towards the nearest simplex ETF. The classifier weights $W=U^{\\star}M$ are an implicit function of the CNN features $\\pmb{H}$ . Note that the parameters of the CNN are updated via two gradient paths from the loss function $\\mathcal{L}$ , a direct path (top) and an indirect path through $U^{\\star}$ (bottom). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{H}_{t}=\\alpha\\tilde{H}_{\\mathrm{batch}}+(1-\\alpha)\\tilde{H}_{t-1}\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Second, we employ stratified batch sampling to guarantee that all class labels are represented in the mini-batch. This ensures that we avoid degenerate solutions when finding the nearest simplex ETF geometry, as our optimisation problem requires input feature means for all $C$ classes. In cases where the number of classes exceeds the chosen batch size, we compute the per-class feature mean for the class labels present in the given batch. For the remaining class labels, we set their feature mean as the global mean of the batch. We repeat this process for each training iteration until we have sampled examples belonging to the missing class labels. At that point, we update the feature mean of those missing class labels with the new feature statistics. We reserve this method only for cases where the batch size is smaller than the number of labels since it can introduce instability during early iterations. ", "page_idx": 4}, {"type": "text", "text": "3.6 Deep Declarative Layer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We can backpropagate through the Riemannian optimisation problem to update the feature means using a declarative node [23]. Then, the features are updated from both the loss and the feature means through auto-differentiation. The motivation for developing the DDN layer lies in recognising that, despite the presence of a proximal term, abrupt and sudden changes to the classifier may occur as the features are updated. These changes can pose challenges for backpropagation, potentially disrupting the stability and convergence of the training process. Incorporating an additional stream of gradients through the feature means to account for such changes, as depicted in Figure 1, assists in stabilising the feature updates during backpropagation. ", "page_idx": 4}, {"type": "text", "text": "To efficiently backpropagate through the optimisation problem, we employ techniques described in Gould et al. [23] utilising the implicit function theorem to compute the gradients. In our case, we have a scalar objective function $f:\\dot{\\mathbb{R}}^{d\\times C}\\rightarrow\\mathbb{R}$ , and a matrix constraint function $J:\\mathbb{R}^{d\\times C}\\rightarrow\\mathbb{R}^{C\\times C}$ . Since we have matrix variables, we use vectorisation techniques [46] to avoid numerically dealing with tensor gradients. More specifically, we have the following: ", "page_idx": 4}, {"type": "text", "text": "Proposition 1 (Following directly from Proposition 4.5 in Gould et al. [23]). Consider the optimisation problem in Equation 7. Assume that the solution exists and that the objective function $f$ and the constraint function $J$ are twice differentiable in the neighbourhood of the solution. If the rank(A) = C(C+1) and $\\pmb{G}$ is non-singular then: ", "page_idx": 4}, {"type": "text", "text": "where, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{D y(U)=G^{-1}A^{\\top}(A G^{-1}A^{\\top})^{-1}(A G^{-1}B)-G^{-1}B\\;,}\\\\ &{}&\\\\ &{}&{A=\\mathrm{rvech}(D_{U}J(\\tilde{H},U))\\in\\mathbb R^{\\frac{C(C+1)}{2}\\times d C}\\;,\\;\\;\\;\\;}\\\\ &{}&{B=\\mathrm{rvec}(D_{\\tilde{H}U}^{2}\\,f(\\tilde{H},U))\\in\\mathbb R^{d C\\times d C}\\;,\\;\\;\\;\\;}\\\\ &{}&{G=\\mathrm{rvec}(D_{U U}^{2}\\,f(\\tilde{H},U))-\\Lambda:\\mathrm{rvech}(D_{U U}^{2}\\,J(\\tilde{H},U))\\in\\mathbb R^{d C\\times d C}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, the double dot product symbol $(:)$ denotes a tensor contraction on appropriate indices between the Lagrange multiplier matrix $\\pmb{\\Lambda}$ and $a$ fourth-order tensor Hessian. Also, rvec(\u00b7) and rvech(\u00b7) refer to the row-major vectorisation and half-vectorisation operations, respectively. To find the Lagrange multiplier matrix $\\mathbf{A}\\in\\mathbb{R}^{C\\times\\frac{C+1}{2}}$ , we solve the following equation where we have vectorised the matrix as $\\bar{\\lambda}\\in\\mathbb{R}^{\\frac{C(C+1)}{2}}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda^{\\top}A=D_{U}\\,f(\\tilde{H},U)\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Alternatively, for a more efficient computation of the identity $\\pmb{G}$ , we can utilise the embedded gradient field method as defined in Birtea et al. $I^{g},$ , 10]. Therefore, we obtain: ", "page_idx": 5}, {"type": "equation", "text": "$$\nG=\\mathrm{rvec}(D_{U U}^{2}\\,f(\\tilde{H},U))-I_{d}\\otimes\\Sigma(U)\\in\\mathbb{R}^{d C\\times d C}\\;,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\Sigma(U)=\\frac{1}{2}\\Big(D_{U}\\,f(\\tilde{H},U)^{\\top}U+U^{\\top}D_{U}\\,f(\\tilde{H},U)\\Big),}\\end{array}$ and $\\otimes$ here denotes Kronecker product.   \nA detailed derivation of each identity in the proposition can be found in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In our experiments, we perform feature normalisation onto a hypersphere, a common practice in training neural networks, which improves representation and enhances model performance [71, 26, 53, 42, 61, 62, 12, 16, 35]. We find that combining classifier weight normalisation with feature normalisation accelerates convergence [69]. Given that simplex ETFs are inherently normalised, we include classifier weight normalisation in our standard training procedure to ensure fair method comparisons. ", "page_idx": 5}, {"type": "text", "text": "Experimental Setup. In this study, we conduct experiments on three model variants. First, the standard method involves training a model with learnable classifier weights, following conventional practice. Second, in the fixed ETF method, we set the classifier to a predefined simplex ETF. In all experiments, we choose the simplex ETF with canonical direction. In Appendix C, we also include additional experiments for fixed simplex ETFs with random directions generated from a Haar measure [47]. Last, our implicit ETF method, where we set the classifier weights on-the-fly as the simplex ETF closest to the current feature means. ", "page_idx": 5}, {"type": "text", "text": "We repeat experiments on each method five times with distinct random seeds and report the median values alongside their respective ranges. For reproducibility and to streamline hyperparameter tuning, we employed Automatic Gradient Descent (AGD) [6]. Following the authors\u2019 recommendation, we set the gain/momentum parameter to 10 to expedite convergence, aligning it with other widely used optimisers like Adam [36] and SGD. Our experiments on real datasets run for 200 epochs with batch size 256; for the UFM analysis, we run 2000 iterations. ", "page_idx": 5}, {"type": "text", "text": "Our method underwent rigorous evaluation across various UFM sizes and real model architectures trained on actual datasets, including CIFAR10 [37], CIFAR100 [37], STL10 [14], and ImageNet1000 [15], implemented on ResNet [29] and VGG [56] architectures. More specifically, we trained CIFAR10 on ResNet18 and VGG13, CIFAR100 and STL10 on ResNet50 and VGG13, and ImageNet1000 on ResNet50. The input images were preprocessed pixel-wise by subtracting the mean and dividing by the standard deviation. Additionally, standard data augmentation techniques were applied, including random horizontal filps, rotations, and crops. All experiments were conducted using Nvidia RTX3090 and A100 GPUs. ", "page_idx": 5}, {"type": "text", "text": "Hyperparameter Selection and Riemannian Initialisation Schemes. We solve the Riemannian optimisation problem defined in Equation 7 using a Riemannian Trust-Region method [1] from pyManopt [60]. We maintain a proximal coefficient $\\delta$ set to $10^{-3}$ consistently across all experiments. It is worth mentioning that algorithm convergence is robust to the precise value of $\\delta$ . In our problem, determining values for $U_{\\mathrm{init}}$ and $U_{\\mathrm{prox}}$ is crucial. We explored several methods to initialise these parameters. One approach involved setting both towards the canonical simplex ETF direction. This means initialising them as a partial orthogonal matrix where the first $C$ rows and columns form an identity matrix while the remaining $d-C$ rows are fliled with zeros. Another approach is to initialise both of them as random orthogonal matrices from classical compact groups, selected according to a Haar measure [47]. In the end, the approach that yielded the most stable results at initialisation was to employ either of the aforementioned initialisation methods to solve the original problem without the proximal term in Equation 6. We then used the obtained $U^{\\star}$ to initialise both $U_{\\mathrm{init}}$ and $U_{\\mathrm{prox}}$ for the problem in Equation 7. This process was carried out only for the first gradient update of the first epoch. In subsequent iterations, we update these parameters to the $U^{\\star}$ obtained from the previous time step. Importantly, the proximal term is held fixed during each Riemannian optimisation. ", "page_idx": 5}, {"type": "image", "img_path": "z4FaPUslma/tmp/0441fe97ebd6cba9a4a89651ba101abde71a0b8b7e0ba98fe7e52bfc95b2a58c.jpg", "img_caption": ["Figure 2: UFM-10 results. In all plots, the $\\mathbf{X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Regarding the calculation of the exponential moving average of the feature means, we have found that employing a decay policy on the smoothing factor $\\alpha$ yields optimal results. Specifically, we set $\\alpha=2/(T+\\bar{1})$ , where $T$ represents the number of iterations. Additionally, we include a thresholding value of $10^{-4}$ , such that if $\\alpha$ falls below this threshold, we fix $\\alpha$ to be equal to the threshold. This precaution ensures that $\\alpha$ does not diminish throughout the iterations, thereby guaranteeing that the newly calculated feature means contribute sufficient statistics to the exponential moving average. ", "page_idx": 6}, {"type": "text", "text": "Finally, in our experiments, we set the temperature parameter $\\tau$ to five. This choice aligns with the findings discussed by Yaras et al. [69], highlighting the influence of the temperature parameter value on the extent of neural collapse statistics with normalised features. ", "page_idx": 6}, {"type": "text", "text": "Unconstrained Feature Models (UFMs). Our experiments on UFMs, which provide a controlled setting for evaluating the effectiveness of our method, are done using the following configurations: ", "page_idx": 6}, {"type": "text", "text": "\u2022 UFM-10: a 10-class UFM containing 1000 features with a dimension of 512.   \n\u2022 UFM-100: a 100-class UFM containing 5000 features with a dimension of 1024.   \n\u2022 UFM-200: a 200-class UFM containing 5000 features, with a dimension of 1024.   \n\u2022 UFM-1000: a 1000-class UFM containing 10000 features, with a dimension of 1024. ", "page_idx": 6}, {"type": "text", "text": "Results. We present the results for the synthetic UFM-10 case in Figure 2. The CE loss plot demonstrates that fixing the classifier weights to a simplex ETF achieves the theoretical lower bound of Yaras et al. [69, Thm. 1], indicating the attainment of a globally optimal solution. We also visualise the average cosine margin per epoch and the cosine margin distributions of each example at the end of training, defined in Zhou et al. [74]. The neural collapse metrics, NC1 and $N C3$ , which measure the features\u2019 within-class variability, and the self-duality alignment between the feature means and the classifier weights [76], are also plotted. Last, we depict the absolute difference of the classifier and feature means norms to illustrate their convergence towards equinorms, as described in Papyan et al. [50]. A comprehensive description of the metrics can be found in Appendix A. Collectively, the plots indicate the superior performance of our method in achieving a neural collapse (NC) solution faster than other approaches. In Figure 3, we demonstrate under the UFM setting that as we increase the number of classes, our method maintains constant performance and converges at the same rate, while the fixed ETF and the standard approach require more time to reach the interpolation threshold. ", "page_idx": 6}, {"type": "table", "img_path": "z4FaPUslma/tmp/7dd120e84c453982905b4089e401173a9e5d99c711b220bc66a869117c5f0d1e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "z4FaPUslma/tmp/944b8350bac34be4c0bbd56654d581fe510a7b8de19dc8e60d85104a208aeda2.jpg", "table_caption": ["Table 1: Train top-1 accuracy results presented as a median with indices indicating the range of values from five random seeds. Best results are bolded. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Numerical results for the top-1 train and test accuracy are reported in Tables 1 and 2, respectively. The results are provided for snapshots taken at epoch 50 and epoch 200. It is evident that our method achieves a faster convergence speed compared to the competitive methods while ultimately converging to the same performance level. Additionally, it is noteworthy that our method exhibits the smallest degree of variability across different runs, as indicated by the range values provided. Finally, in Figure 4, we present qualitative results that confirm our solution\u2019s ability to converge much faster and reach peak performance earlier than the standard and fixed ETF methods on ImageNet. It\u2019s important to note that the standard method with AGD is reported to converge to the same testing accuracy $(65.5\\%)$ at epoch 350, as shown in Bernstein et al. [6, Figure 4]. At epoch 200, the authors exhibit a testing accuracy of approximately $51\\%$ . Since we have increased the gain parameter on AGD compared to the results reported in the original paper, we report a final $60.67\\%$ testing accuracy for the standard method, whereas our method reaches peak convergence at approximately epoch 80. We note that the ImageNet results reported in Tables 1 and 2, as well as Figure 4, are generated solely by solving the Riemannian optimisation problem without considering its gradient stream on the feature updates, due to computational constraints. We discuss the computational requirements of our method in Section 5. We also present qualitative results for all the other datasets and architectures in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "5 Discussion: Limitations and Future Directions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our method involves two gradient streams updating the features, as depicted in Figure 1. Interestingly, empirical observations on small-scale datasets (see Figure 15) indicate that even without the backpropagation through the DDN layer, the performance remains comparable, rendering the gradient calculation of the DDN layer optional. In Figure 15c, we observe a strong impact of the DDN layer gradient on the atomic feature level, with more features reaching the theoretical simplex ETF margin by the end of training. To reach a consensus on the exact effect of the DDN gradient on the learning process, further experiments on large-scale datasets are needed. However, on large-scale datasets with large $d$ and $C$ , such as ImageNet, computing the backward pass of the Riemannian optimisation is challenging due to the memory inefficiency of the current implementation of DDN gradients. This limitation is an area we aim to address in future work. Note that in all other experiments, we use the full gradient computations, including both direct and indirect components, through the DDN layer. We summarise the GPU memory requirements for each method across various datasets in Table 3. ", "page_idx": 7}, {"type": "table", "img_path": "z4FaPUslma/tmp/92ef07573dbcf83858eeacc456e7dbde3cb1c663676076128fd72ed2f5226800.jpg", "table_caption": ["Table 2: Test top-1 accuracy results presented as a median with indices indicating the range of values from five random seeds. Best results are bolded. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "z4FaPUslma/tmp/277a940a6d6d040d3d18ff55425e70480d4da827a1802543949ff4311e32f657.jpg", "img_caption": ["Figure 4: ImageNet results on ResNet-50. In all plots, the ${\\bf X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Our discussion so far has focused on convergence speed in terms of the number of epochs required for the network to converge. However, it is also important to consider the time required per epoch. In our case, as training progresses, the time taken by the Riemannian optimization quickly becomes almost negligible compared to the network\u2019s total forward pass time, while it approaches the standard and fixed ETF training forward times, as shown in Figure 5a. However, DDN gradient computation increases considerably when the feature dimension $d$ and the number of classes $C$ increase and starts to dominate the runtime for large datasets such as ImageNet. Nevertheless, for ImageNet, we do not compute the DDN gradients and still outperform other methods. We plan to explore ways to expedite the DDN forward and backward pass in future work. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we introduced a novel method for determining the nearest simplex ETF to the penultimate features of a neural network and utilising it as our target classifier at each iteration. This contrasts with previous approaches, which either fix to a specific simplex ETF or allow the network ", "page_idx": 8}, {"type": "table", "img_path": "z4FaPUslma/tmp/9c0db09ae825f823b084836704395dc20cca70c28209654c7321b0a7611de117.jpg", "table_caption": ["Table 3: GPU memory (in Gigabytes) during training. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "z4FaPUslma/tmp/a884f6808f1dfb51d9e96a2c09b2cb15f16670f319d858e56c57dfe05dbfbf31.jpg", "img_caption": ["(a) Forward pass times in milliseconds. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "z4FaPUslma/tmp/d1202887adef811bc9295f0e4feda904aacbce2242bb1e2ab7fff83222b56e4b.jpg", "img_caption": ["(b) Forward and backward times in (log) millisecs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 5: CIFAR100 computational cost results on ResNet-50. In (a), we plot the forward pass time for each method. For the implicit ETF method, which has dynamic computation times, we also include the mean and median time values. In (b), we plot the computational cost for each forward and backward pass across methods. For the implicit ETF forward pass, we have taken its median time. The notation is as follows: $\\mathrm{S/F=S}$ tandard Forward Pass, $S/\\mathbf{B}=\\mathbf{\\rho},$ Standard Backward Pass, $\\mathrm{F/F=}$ Fixed ETF Forward Pass, $\\mathrm{F/B=}$ Fixed ETF Backward Pass, $\\mathrm{I/F=}$ Implicit ETF Forward Pass, and I/B $=$ Implicit ETF Backward Pass. ", "page_idx": 9}, {"type": "text", "text": "to learn it through gradient descent. Our method involves solving a Riemannian optimisation problem facilitated by a deep declarative node, enabling backpropagation through this process. ", "page_idx": 9}, {"type": "text", "text": "We demonstrated that our approach enhances convergence speed across various datasets and architectures while also reducing variability stemming from different random initialisations. By defining the optimal structure of the classifier and efficiently leveraging its rotation invariance property to find the one closest to the backbone features, we anticipate that our method will facilitate the creation of new architectures and the utilisation of new datasets without necessitating specific learning or tuning of the classifier\u2019s structure. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] P.-A. Absil, C. G. Baker, and K. A. Gallivan. Trust-region methods on Riemannian manifolds. Foundations of Computational Mathematics, 7(3):303\u2013330, 2007. doi: 10.1007/ s10208-005-0179-9.   \n[2] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University Press, 2008. ISBN 9780691132983. URL http://www.jstor.org/ stable/j.ctt7smmk.   \n[3] Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter. Differentiable convex optimization layers. Advances in neural information processing systems, 32, 2019.   \n[4] Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. In International Conference on Machine Learning, pp. 136\u2013145. PMLR, 2017. [5] Tina Behnia, Ganesh Ramachandra Kini, Vala Vakilian, and Christos Thrampoulidis. On the implicit geometry of cross-entropy parameterizations for label-imbalanced data. In International Conference on Artificial Intelligence and Statistics, pp. 10815\u201310838. PMLR, 2023.   \n[6] Jeremy Bernstein, Chris Mingard, Kevin Huang, Navid Azizan, and Yisong Yue. Automatic Gradient Descent: Deep Learning without Hyperparameters. arXiv:2304.05187, 2023. [7] Petre Birtea and Dan Coma\u02d8nescu. Geometrical dissipation for dynamical systems. Communications in Mathematical Physics, 316:375\u2013394, 2012.   \n[8] Petre Birtea and Dan Com\u02d8anescu. Hessian operators on constraint manifolds. Journal of Nonlinear Science, 25:1285\u20131305, 2015.   \n[9] Petre Birtea, Ioan Ca\u00b8su, and Dan Com\u02d8anescu. First order optimality conditions and steepest descent algorithm on orthogonal stiefel manifolds. Optim. Lett., 13(8):1773\u20131791, November 2019.   \n[10] Petre Birtea, Ioan Ca\u00b8su, and Dan Com\u02d8anescu. Second order optimality on orthogonal stiefel manifolds. Bulletin des Sciences Math\u00e9matiques, 161:102868, 2020. ISSN 0007-4497. doi: https://doi.org/10.1016/j.bulsci.2020.102868. URL https://www.sciencedirect.com/ science/article/pii/S0007449720300385.   \n[11] Nicolas Boumal. An introduction to optimization on smooth manifolds. Cambridge University Press, 2023.   \n[12] Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. Deep networks from the principle of rate reduction. arXiv preprint arXiv:2010.14765, 2020.   \n[13] Lingling He Changqing Xu and Zerong Lin. Commutation matrices and commutation tensors. Linear and Multilinear Algebra, 68(9):1721\u20131742, 2020. doi: 10.1080/03081087.2018.1556242. URL https://doi.org/10.1080/03081087.2018.1556242.   \n[14] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Geoffrey Gordon, David Dunson, and Miroslav Dud\u00edk (eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research, pp. 215\u2013223, Fort Lauderdale, FL, USA, 11\u201313 Apr 2011. PMLR. URL https://proceedings.mlr.press/v15/coates11a.html.   \n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248\u2013255. IEEE, 2009. URL https://ieeexplore.ieee.org/ abstract/document/5206848/.   \n[16] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4690\u20134699, 2019.   \n[17] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pp. 1675\u20131685. PMLR, 2019.   \n[18] Alan Edelman, Tom\u00e1s A Arias, and Steven T Smith. The geometry of algorithms with orthogonality constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303\u2013353, 1998.   \n[19] Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences, 118(43):e2103091118, 2021. doi: 10.1073/pnas.2103091118. URL https://www.pnas.org/doi/abs/10.1073/pnas.2103091118.   \n[20] Bin Gao, Xin Liu, Xiaojun Chen, and Ya-xiang Yuan. A new first-order algorithmic framework for optimization problems with orthogonality constraints. SIAM Journal on Optimization, 28 (1):302\u2013332, 2018.   \n[21] Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/ paper_files/paper/2019/file/f39ae9ff3a81f499230c4126e01f421b-Paper.pdf.   \n[22] Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge, MA, USA, 2016. http://www.deeplearningbook.org.   \n[23] S. Gould, R. Hartley, and D. Campbell. Deep declarative networks. IEEE Transactions on Pattern Analysis & Machine Intelligence, 44(08):3988\u20134004, aug 2022. ISSN 1939-3539. doi: 10.1109/TPAMI.2021.3059462.   \n[24] Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison Guo. On differentiating parameterized argmin and argmax problems with application to bi-level optimization. arXiv preprint arXiv:1607.05447, 2016.   \n[25] Stephen Gould, Dylan Campbell, Itzik Ben-Shabat, Chamin Hewa Koneputugodage, and Zhiwei Xu. Exploiting problem structure in deep declarative networks: Two case studies. arXiv preprint arXiv:2202.12404, 2022.   \n[26] Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt. Dissecting supervised contrastive learning. In International Conference on Machine Learning, pp. 3821\u20133830. PMLR, 2021.   \n[27] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. Advances in neural information processing systems, 31, 2018.   \n[28] X.Y. Han, Vardan Papyan, and David L. Donoho. Neural collapse under MSE loss: Proximity to and dynamics on the central path. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $\\equiv$ w1UbdvWH_R3.   \n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR \u201916, pp. 770\u2013778. IEEE, June 2016. doi: 10.1109/CVPR.2016.90. URL http://ieeexplore.ieee.org/document/7780459.   \n[30] Elad Hoffer, Itay Hubara, and Daniel Soudry. Fix your classifier: the marginal value of training the last weight layer. arXiv preprint arXiv:1801.04540, 2018.   \n[31] Meena Jagadeesan, Ilya Razenshteyn, and Suriya Gunasekar. Inductive bias of multi-channel linear convolutional networks with bounded weight norm. In Conference on Learning Theory, pp. 2276\u20132325. PMLR, 2022.   \n[32] Wenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, and Weijie J Su. An unconstrained layer-peeled perspective on neural collapse. arXiv preprint arXiv:2110.02796, 2021.   \n[33] Ziwei Ji, Miroslav Dud\u00edk, Robert E. Schapire, and Matus Telgarsky. Gradient descent follows the regularization path for general losses. In Jacob Abernethy and Shivani Agarwal (eds.), Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pp. 2109\u20132136. PMLR, 09\u201312 Jul 2020. URL https:// proceedings.mlr.press/v125/ji20a.html.   \n[34] John M. Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Andy Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with alphafold. Nature, 596:583 \u2013 589, 2021. URL https://api.semanticscholar. org/CorpusID:235959867.   \n[35] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:18661\u201318673, 2020.   \n[36] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), San Diega, CA, USA, 2015.   \n[37] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[38] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436\u2013444, 2015.   \n[39] Mario Lezcano Casado. Trivializations for gradient-based optimization on manifolds. Advances in Neural Information Processing Systems, 32, 2019.   \n[40] Mario Lezcano-Casado and David Mart\u0131nez-Rubio. Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group. In International Conference on Machine Learning, pp. 3794\u20133803. PMLR, 2019.   \n[41] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. Advances in neural information processing systems, 31, 2018.   \n[42] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 212\u2013220, 2017.   \n[43] Jianfeng Lu and Stefan Steinerberger. Neural collapse under cross-entropy loss. Applied and Computational Harmonic Analysis, 59:224\u2013241, 2022.   \n[44] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id $\\cdot$ SJeLIgBKPS.   \n[45] Jan R. Magnus and H. Neudecker. The elimination matrix: Some lemmas and applications. SIAM Journal on Algebraic Discrete Methods, 1(4):422\u2013449, 1980. doi: 10.1137/0601049. URL https://doi.org/10.1137/0601049.   \n[46] Jan R. Magnus and Heinz Neudecker. Matrix differential calculus with applications in statistics and econometrics / Jan Rudolph Magnus and Heinz Neudecker. Wiley Series in Probability and Statistics. Wiley, Hoboken, NJ, third edition. edition, 2019. ISBN 1-119-54121-2.   \n[47] Francesco Mezzadri. How to generate random matrices from the classical compact groups. arXiv preprint math-ph/0609050, 2006.   \n[48] Dustin G. Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. CoRR, abs/2011.11619, 2020. URL https://arxiv.org/abs/2011.11619.   \n[49] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.   \n[50] Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Science, 117 (40):24652\u201324663, October 2020. doi: 10.1073/pnas.2015509117.   \n[51] Gao Peifeng, Qianqian Xu, Peisong Wen, Zhiyong Yang, Huiyang Shao, and Qingming Huang. Feature directions matter: Long-tailed learning via rotated balanced representation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 27542\u201327563. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/peifeng23a.html.   \n[52] Federico Pernici, Matteo Bruni, Claudio Baecchi, and Alberto Del Bimbo. Regular polytope networks. IEEE Transactions on Neural Networks and Learning Systems, 33(9):4373\u20134387, 2021.   \n[53] Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. L2-constrained softmax loss for discriminative face verification. arXiv preprint arXiv:1703.09507, 2017.   \n[54] Ohad Shamir. Gradient methods never overfti on separable data. Journal of Machine Learning Research, 22(85):1\u201320, 2021.   \n[55] Jonathan W. Siegel. Accelerated optimization with orthogonality constraints. Journal of Computational Mathematics, 39(2):207\u2013226, 2020. ISSN 1991-7139. doi: https://doi.org/10. 4208/jcm.1911-m2018-0242. URL http://global-sci.org/intro/article_detail/ jcm/18372.html.   \n[56] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.   \n[57] Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id $\\cdot$ r1q7n9gAb.   \n[58] Korawat Tanwisuth, Xinjie Fan, Huangjie Zheng, Shujian Zhang, Hao Zhang, Bo Chen, and Mingyuan Zhou. A prototype-oriented framework for unsupervised domain adaptation. Advances in Neural Information Processing Systems, 34:17194\u201317208, 2021.   \n[59] Christos Thrampoulidis, Ganesh Ramachandra Kini, Vala Vakilian, and Tina Behnia. Imbalance trouble: Revisiting neural-collapse geometry. Advances in Neural Information Processing Systems, 35:27225\u201327238, 2022.   \n[60] J. Townsend, N. Koep, and S. Weichwald. PyManopt: a Python toolbox for optimization on manifolds using automatic differentiation. Journal of Machine Learning Research, 17(137): 1\u20135, 2016. URL https://www.pymanopt.org.   \n[61] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5265\u20135274, 2018.   \n[62] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International conference on machine learning, pp. 9929\u20139939. PMLR, 2020.   \n[63] E Weinan and Stephan Wojtowytsch. On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers. In Mathematical and Scientific Machine Learning, pp. 270\u2013290. PMLR, 2022.   \n[64] Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality constraints. Mathematical Programming, 142(1):397\u2013434, 2013.   \n[65] Nachuan Xiao and Xin Liu. Solving optimization problems over the stiefel manifold by smooth exact penalty function. arXiv preprint arXiv:2110.08986, 2021.   \n[66] Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. Dissolving constraints for riemannian optimization. Mathematics of Operations Research, 49(1):366\u2013397, 2024.   \n[67] Yibo Yang, Shixiang Chen, Xiangtai Li, Liang Xie, Zhouchen Lin, and Dacheng Tao. Inducing neural collapse in imbalanced learning: Do we really need a learnable classifier at the end of deep neural network? In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=A6EmxI3_Xc.   \n[68] Yibo Yang, Haobo Yuan, Xiangtai Li, Jianlong Wu, Lefei Zhang, Zhouchen Lin, Philip H.S. Torr, Bernard Ghanem, and Dacheng Tao. Neural collapse terminus: A unified solution for class incremental learning and its variants. arXiv pre-print, 2023.   \n[69] Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, and Qing Qu. Neural collapse with normalized features: A geometric analysis over the riemannian manifold. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 11547\u201311560. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 4b3cc0d1c897ebcf71aca92a4a26ac83-Paper-Conference.pdf.   \n[70] Chong You, Zhihui Zhu, Qing Qu, and Yi Ma. Robust recovery via implicit bias of discrepant learning rates for double over-parameterization. Advances in Neural Information Processing Systems, 33:17733\u201317744, 2020.   \n[71] Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discriminative representations via the principle of maximal coding rate reduction. Advances in Neural Information Processing Systems, 33:9422\u20139434, 2020.   \n[72] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id $\\equiv$ Sy8gdB9xx.   \n[73] Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Conference on learning theory, pp. 1617\u20131638. PMLR, 2016.   \n[74] Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu. On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features. In International Conference on Machine Learning, pp. 27179\u201327202. PMLR, 2022.   \n[75] Jinxin Zhou, Chong You, Xiao Li, Kangning Liu, Sheng Liu, Qing Qu, and Zhihui Zhu. Are all losses created equal: A neural collapse perspective. Advances in Neural Information Processing Systems, 35:31697\u201331710, 2022.   \n[76] Zhihui Zhu, Tianyu DING, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=KRODJAa6pzE. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Background ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Following the definitions of the global mean and class mean of the penultimate-layer features $\\{h_{c,i}\\}$ in Equation 4, here we introduce the within-class and between-class covariances, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}_{W}\\triangleq\\frac{1}{N}\\sum_{c=1}^{C}\\sum_{i=1}^{n_{c}}\\left(\\pmb{h}_{c,i}-\\bar{\\pmb{h}}_{c}\\right)\\left(\\pmb{h}_{c,i}-\\bar{\\pmb{h}}_{c}\\right)^{\\top}\\ ,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\pmb\\Sigma}_{\\cal B}\\triangleq\\frac{1}{C}\\sum_{c=1}^{C}\\left(\\bar{h}_{c}-h_{G}\\right)\\left(\\bar{h}_{c}-h_{G}\\right)^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We proceed by expanding on the four key properties of the last-layer activations and classifiers, as empirically observed by Papyan et al. [50] at the terminal phase of training (TPT), where we have achieved zero classification error and continue towards zero loss. ", "page_idx": 15}, {"type": "text", "text": "NC1 Variability Collapse: Throughout training, feature activation variability diminishes as they converge towards their respective class means. ", "page_idx": 15}, {"type": "equation", "text": "$$\nN C1\\triangleq\\frac{1}{C}\\,\\mathrm{Tr}\\left(\\Sigma_{W}\\Sigma_{B}^{\\dagger}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\dagger$ denotes the Moore\u2013Penrose inverse. ", "page_idx": 15}, {"type": "text", "text": "NC2 Convergence to Simplex ETF: The class-mean activation vectors, centred around their global mean, converge to uniform norms while simultaneously maintaining equal-sized and maximally separable angles between them2. ", "page_idx": 15}, {"type": "equation", "text": "$$\nN C2\\triangleq\\left\\|\\frac{{\\pmb W}{\\pmb W}^{\\top}}{\\|{\\pmb W}{\\pmb W}^{\\top}\\|_{F}}-\\frac{1}{\\sqrt{C-1}}\\left({\\pmb I}_{C}-\\frac{1}{C}{\\pmb1}_{C}{\\bf1}_{C}^{\\top}\\right)\\right\\|_{F}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "NC3 Convergence to Self-duality: The feature class-means and linear classifiers eventually align in a dual vector space up to some scaling. ", "page_idx": 15}, {"type": "equation", "text": "$$\nN C3\\triangleq\\left\\Vert\\frac{W\\bar{H}}{\\Vert W\\bar{H}\\Vert_{F}}-\\frac{1}{\\sqrt{C-1}}\\left(I_{C}-\\frac{1}{C}{\\bf1}_{C}{\\bf1}_{C}^{\\top}\\right)\\right\\Vert_{F}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "NC4 Simplification to Nearest Class-Center (NCC): The network classifier tends to select the class whose mean is closest (in Euclidean distance) to a given deepnet activation. ", "page_idx": 15}, {"type": "equation", "text": "$$\nN C4\\triangleq\\underset{c^{\\prime}}{\\arg\\operatorname*{max}}\\langle\\pmb{w}_{c^{\\prime}},\\pmb{h}\\rangle+b_{c^{\\prime}}\\rightarrow\\underset{c^{\\prime}}{\\arg\\operatorname*{min}}\\,\\|\\pmb{h}-\\bar{\\pmb{h}}_{c^{\\prime}}\\|_{2}\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since the NC2 and NC3 metrics involve normalised matrices, it is not immediately evident whether the linear classifier and the class-mean activations are equinorm. Consequently, we introduce the following definition: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\qquad W\\bar{H}\\mathrm{{\\cdotEquinorm}}=|W_{\\mathrm{equinorm}}-\\bar{H}_{\\mathrm{equinorm}}|\\;,}\\\\ &{\\bar{H}_{\\mathrm{equinorm}}=\\frac{\\mathrm{std}_{c}(\\|\\bar{h}_{c}-h_{G}\\|_{2})}{\\mathrm{avg}_{c}(\\|\\bar{h}_{c}-h_{G}\\|_{2})}\\;\\mathrm{and}\\;W_{\\mathrm{equinorm}}=\\frac{\\mathrm{std}_{c}(\\|w_{c}\\|_{2})}{\\mathrm{avg}_{c}(\\|w_{c}\\|_{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, to measure the extent of the variability collapse of each feature separately, we define the cosine margin metric as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nC M_{c,i}=\\cos\\theta_{c,i;j}-\\operatorname*{max}_{j\\neq c}\\cos\\theta_{c,i;j},\\quad\\mathrm{where~}\\cos\\theta_{c,i;j}=\\frac{\\langle w_{j}-w_{G},h_{c,i}-h_{G}\\rangle}{\\|w_{j}-w_{G}\\|_{2}\\|h_{c,i}-h_{G}\\|_{2}}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that the maximal angle for a simplex ETF vector collection $\\{v_{c}\\}_{c=1}^{C}$ are defined as such: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{\\langle v_{c},v_{c^{\\prime}}\\rangle}{\\|v_{c}\\|_{2}\\|v_{c^{\\prime}}\\|_{2}}}={\\binom{1,}{-{\\frac{1}{C-1}}}},\\qquad\\operatorname{for}c=c^{\\prime}~.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we can calculate the theoretical simplex ETF cosine margin as $\\frac{C}{C-1}$ ", "page_idx": 15}, {"type": "text", "text": "2Mathematically, we have defined NC2 as the collapse of the linear classifiers to a simplex ETF. ", "page_idx": 15}, {"type": "text", "text": "B DDN Gradients ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The original problem we are trying to solve, as defined in Section 3.2, is the following: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{U\\in S t_{C}^{d}}\\left\\|\\tilde{H}-U\\tilde{M}\\right\\|_{F}^{2}\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "or equivalently expanded as such: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{y(U)\\in\\underset{U\\in\\mathbb{R}^{d\\times C}}{\\mathrm{arg\\,min}}\\quad}&{\\Tilde{H}:\\Tilde{H}-\\frac{2}{\\sqrt{C-1}}\\Tilde{H}:U\\Tilde{M}+\\frac{1}{C-1}U:U\\Tilde{M}\\,,}\\\\ {\\mathrm{subject\\,}\\quad}&{U^{\\top}U=I_{C}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, we denote the double-dot operator : as the Frobenius inner product, i.e., $A\\ :\\ B\\ =$ $\\textstyle\\sum_{i=1}^{m}\\sum_{j=1}^{n}A_{i j}B_{i j}=\\operatorname{Tr}(A^{\\top}B)$ . ", "page_idx": 16}, {"type": "text", "text": "To compute the first and second-order gradients of the objective function and constraints, respectively, we need to consider matrices as variables. Utilising matrix differentials and vectorised derivatives, as defined in [46], simplifies the computation of second-order objective derivatives and all constraint derivatives. For the objective function, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle d\\,f\\bigl(\\tilde{H},U\\bigr)=-\\frac{2}{\\sqrt{C-1}}\\tilde{H}:d U\\tilde{M}+\\frac{1}{C-1}\\biggl(d U:U\\tilde{M}+U:d U\\tilde{M}\\biggr)}\\\\ {\\displaystyle\\qquad=\\frac{2}{\\sqrt{C-1}}\\tilde{H}\\tilde{M}:d U+\\frac{2}{C-1}U\\tilde{M}:d U}\\\\ {\\displaystyle\\qquad=\\biggl(\\frac{2}{\\sqrt{C-1}}\\tilde{H}\\tilde{M}+\\frac{2}{C-1}U\\tilde{M}\\biggr):d U}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\implies D_{U}f(\\Tilde{\\pmb{H}},\\pmb{U})=\\frac{2}{\\sqrt{C-1}}\\Tilde{\\pmb{H}}\\Tilde{\\pmb{M}}+\\frac{2}{C-1}U\\Tilde{M}\\in\\mathbb{R}^{d\\times C}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad d\\,D_{U}(\\tilde{H},U)=\\frac{2}{C-1}d U\\tilde{M}}\\\\ &{\\implies d\\mathrm{rvec}\\,D_{U}(\\tilde{H},U)=\\frac{2}{C-1}\\mathrm{rvec}(d U\\tilde{M})=\\frac{2}{C-1}\\bigg(I_{d}\\otimes\\tilde{M}\\bigg)d\\mathrm{rvec}\\,U}\\\\ &{\\implies\\mathrm{rvec}(D_{U U}^{2}f(\\tilde{H},U))=\\frac{2}{C-1}\\bigg(I_{d}\\otimes\\tilde{M}\\bigg)\\in\\mathbb{R}^{d C\\times d C}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we have defined here the row-major vectorisation method, i.e., $\\operatorname{rvec}(\\mathbf{A})=\\operatorname{vec}(\\mathbf{A}^{\\top})$ , and we have used the property: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{rvec}(A B C)=(A\\otimes C^{\\top})\\operatorname{rvec}(B)\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A similar proof follows for the second-order partial gradient of the objective. We omit the proof here and just declare the result: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\cal B}\\triangleq\\mathrm{rvec}(D_{\\tilde{H}U}^{2}f(\\tilde{H},U))=-\\frac{2}{\\sqrt{C-1}}\\bigg(I_{d}\\otimes\\tilde{M}\\bigg)\\in\\mathbb{R}^{d C\\times d C}\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Regarding the gradients of the constraint function, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\nd J(\\tilde{H},U)=d(U^{\\top}U-I_{C})=(d U)^{\\top}U+U^{\\top}d U\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\\\\\\\\\\\\\\\\\}\\mathrm{~\\~\\\\\\\\\\\\}\\mathrm{~av}{\\mathrm{~(}A\\mathrm{,}v{\\mathrm{~'~}}\\mathrm{~=~}\\mathrm{sc}\\mathrm{~'~}\\mathrm{~=~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~av}{\\mathrm{~'~}}\\mathrm{~)~}}\\mathrm{~\\\\\\\\\\\\\\}\\mathrm{~~\\\\\\}\\mathrm{~~\\\\\\\\}}\\\\ &{\\implies d\\mathrm{rvec}{\\mathrm{~\\}}J(\\tilde{H},U)=\\mathrm{rvec}{((d U)^{\\top}U)}+\\mathrm{rvec}{(U^{\\top}d U)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\ \\ =(I_{C}\\otimes U^{\\top})\\,d\\mathrm{rvec}{U^{\\top}}+\\big(U^{\\top}\\otimes I_{C}\\big)\\,d\\mathrm{rvec}{U}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\ =(I_{C}\\otimes U^{\\top})K_{d C}\\,d\\mathrm{rvec}{U}+\\big(U^{\\top}\\otimes I_{C}\\big)\\,d\\mathrm{rvec}{U}}\\\\ &{\\qquad\\qquad\\qquad\\ \\ =K_{C C}(U^{\\top}\\otimes I_{C})\\,d\\mathrm{rvec}{U}+\\big(U^{\\top}\\otimes I_{C}\\big)\\,d\\mathrm{rvec}{U}}\\\\ &{\\qquad\\qquad\\qquad\\ \\ =(K_{C C}+I_{C})\\,(U^{\\top}\\otimes I_{C})\\,d\\mathrm{rvec}{U}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\ =(K_{C C}+I_{C})\\,(U\\otimes I_{C})^{\\top}\\,d\\mathrm{rvec}\\,U}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~\\\\\\\\\\\\}}\\\\ &{\\implies\\mathrm{rvec}{(D_{U}J(\\tilde{H},U))}=(K_{C C}+I_{C})\\,(U\\otimes I_{C})^{\\top}\\,\\mathrm{e~}\\mathbb{R}^{C C\\times d C}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we have defined the commutation matrix [46, 13] as $K_{m n}$ , as a matrix which satisfies the two following identities for any given $A\\in\\mathbb{R}^{m\\times n}$ and $B\\in\\mathbb{R}^{r\\times q}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{K_{m n}\\mathrm{vec}(\\pmb{A})=\\mathrm{vec}(\\pmb{A}^{\\top})\\;,}\\\\ {K_{r m}(\\pmb{A}\\otimes\\pmb{B})\\pmb{K}_{n q}=\\pmb{B}\\otimes\\pmb{A}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The gradient in Equation 24 contains redundant constraints because of the symmetrical nature of the orthogonality constraints. To retain only the non-redundant constraints, we must undertake a half-vectorisation procedure. Given that we already possess the fully vectorised gradients (which are simpler to compute in this scenario), we require an elimination matrix, $L_{C}\\in\\mathbb{R}^{\\frac{\\tilde{C}(C+1)}{2}\\times C^{2}}$ , such that ", "page_idx": 17}, {"type": "equation", "text": "$$\nL_{C}\\operatorname{rvec}(A)=\\operatorname{rvech}(A)~.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can define an elimination matrix explicitly as follows [45]: ", "page_idx": 17}, {"type": "equation", "text": "$$\nL_{n}=\\sum_{i\\geq j}u_{i j}\\mathrm{vec}(E_{i j})^{\\top}=\\sum_{i\\geq j}(u_{i j}\\otimes e_{j}^{\\top}\\otimes e_{i}^{\\top})\\;,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{u}_{i j}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{in~position}\\,(j-1)n+i-\\frac{1}{2}j(j-1)}\\\\ {0}&{\\mathrm{elsewhere}}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, ", "page_idx": 17}, {"type": "equation", "text": "$$\nA\\triangleq\\operatorname{rvech}(D_{U}J(\\tilde{H},U))=L_{C}{\\big(}K_{C C}+I_{C^{2}}{\\big)}(U\\otimes I_{C})^{\\top}\\in\\mathbb{R}^{\\frac{C(C+1)}{2}\\times d C}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We have the following useful Corollary for the second-order gradient of the constraint function. ", "page_idx": 17}, {"type": "text", "text": "Corollary 1 (Magnus & Neudecker [46]). Let $\\phi$ be a twice differentiable real-valued function of an $n\\times q$ matrix $\\mathbf{\\deltaX}$ . Then, the following two relationships hold between the second differential and the Hessian matrix of $\\phi$ at $\\mathbf{\\deltaX}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nd^{2}\\phi(X)=\\operatorname{Tr}(A(d X)^{\\top}B d X)\\iff H\\phi(X)=\\frac{1}{2}(A^{\\top}\\otimes B+A\\otimes B^{\\top})\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "With the above corollary established, we can have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad J(\\tilde{H},U)=U^{\\top}U-I_{C}}\\\\ &{\\quad\\,d J(\\tilde{H},U)=(d U)^{\\top}U+U^{\\top}d U}\\\\ &{\\quad\\,d^{2}J(\\tilde{H},U)=(d U)^{\\top}d U+(d U)^{\\top}d U=2(d U)^{\\top}d U}\\\\ &{\\,d^{2}J(\\tilde{H},U)_{i j}=2e_{i}^{\\top}(d U)^{\\top}(d U)e_{j}=2\\operatorname{Tr}(e_{j}e_{i}^{\\top}(d U)^{\\top}d U)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and hence, we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{rvec}\\left(D_{U U}^{2}J(\\tilde{H},U)_{i j}\\right)=I_{d}\\otimes(e_{i}e_{j}^{\\top})+I_{d}\\otimes(e_{j}e_{i}^{\\top})\\in\\mathbb{R}^{d C\\times d C}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, we repeat the process with the elimination matrix to eliminate the redundant constraints. However, while this is one way to compute the derivative, a more efficient approach exists, namely the embedded gradient vector field method, which we outline in the following subsection. For a complete overview of the method, we recommend readers to follow through the works of Birtea et al. [9, 10], Birtea & Coma\u02d8nescu [8, 7]. ", "page_idx": 18}, {"type": "text", "text": "Finally, the gradients for the proximal problem in Equation 7 are straightforward to compute, with the only change in gradients being the added proximal terms in: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{U}f(\\tilde{H},U)=\\displaystyle\\frac{2}{K-1}U\\tilde{M}-\\frac{2}{\\sqrt{K-1}}\\tilde{H}\\tilde{M}+\\delta(U-U_{\\mathrm{prox}})\\in\\mathbb{R}^{d\\times C}\\;,}\\\\ {\\mathrm{rvec}(D_{U U}^{2}f(\\tilde{H},U))=\\displaystyle\\frac{2}{K-1}\\bigg(I_{d}\\otimes\\tilde{M}\\bigg)+\\delta I_{d C}\\in\\mathbb{R}^{d C\\times d C}\\;.\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.1 Implicit Formulation of Lagrange Multipliers on Differentiable Manifolds ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "An issue arises in Proposition 1 with the expression for $\\boldsymbol{G}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nG=\\mathrm{rvec}(D_{U U}^{2}\\,f(\\tilde{H},U))-\\Lambda:\\mathrm{rvech}(D_{U U}^{2}\\,J(\\tilde{H},U))\\in\\mathbb{R}^{d C\\times d C}\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where both the calculation of the Lagrange multiplier matrix $\\Lambda$ (solved via a linear system) and the construction of the fourth-order tensor representing the second-order derivatives of the constraint function are complex and challenging. However, by recognising the manifold structure of the problem, we can reformulate Equation 33 in a simpler and more computationally efficient way. The embedded gradient vector field method offers such a solution [8]. ", "page_idx": 18}, {"type": "text", "text": "For a general Riemannian manifold $(\\mathcal{M},g)$ , we can define the Gram matrix for the smooth functions $f_{1},\\dots,f_{s},h_{1},\\dots,h_{r}:(\\mathcal{M},g)\\rightarrow\\mathbb{R}$ as follows, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Gram}_{(h_{1},\\ldots,h_{r})}^{(f_{1},\\ldots,f_{s})}\\triangleq\\left[\\begin{array}{c c c}{\\langle\\nabla h_{1},\\nabla f_{1}\\rangle}&{\\cdot\\cdot}&{\\langle\\nabla h_{r},\\nabla f_{1}\\rangle}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\langle\\nabla h_{1},\\nabla f_{s}\\rangle}&{\\ldots}&{\\langle\\nabla h_{r},\\nabla f_{s}\\rangle}\\end{array}\\right]\\in\\mathbb{R}^{s\\times r}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In our problem, we are working with a compact Stiefel manifold, which is an embedded submanifold of $\\mathbb{R}^{d\\times C}$ , and we identify the isomorphism (via vec) between $\\mathbb{R}^{d\\times C}$ and $\\mathbb{R}^{d C}$ . A Stiefel manifold $S t_{C}^{d}=\\{\\pmb{U}\\in\\mathbb{R}^{d\\times C}\\mathrm{~}|\\mathrm{~}\\pmb{U}^{\\dagger}\\pmb{U}=\\pmb{I}_{C}\\}$ can be characterised by a set of constraint functions, $j_{s},j_{p q}:\\mathbb{R}^{d C}\\rightarrow\\mathbb{R}$ , as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{j_{s}({\\boldsymbol{u}})=\\displaystyle\\frac{1}{2}\\|{\\boldsymbol{u}}_{s}\\|^{2},\\quad}}&{{1\\leq s\\leq C\\,,}}\\\\ {{\\ j_{p q}({\\boldsymbol{u}})=\\langle\\boldsymbol{u}_{p},\\boldsymbol{u}_{q}\\rangle,\\quad}}&{{1\\leq p<q\\leq C\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We consider a smooth cost function $\\tilde{f}:S t_{C}^{d}\\to\\mathbb{R}$ and define $f:\\mathbb{R}^{d\\times C}\\rightarrow\\mathbb{R}$ as a smooth extension $\\tilde{f}$ $\\mathbb{R}_{r e g}^{d C}\\,\\subset\\,\\mathbb{R}^{d C}$ formed with the regular leaves of the constrained function   \n$j:\\mathbb{R}^{d C}\\,\\rightarrow\\,\\mathbb{R}^{\\frac{C(C+1)}{2}}$   \nform [9]: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\partial f(\\pmb{u})=\\nabla f(\\pmb{u})-\\sum_{1\\leq s\\leq C}\\sigma_{s}(\\pmb{u})\\nabla j_{s}(\\pmb{u})-\\sum_{1\\leq p<q\\leq C}\\sigma_{p q}(\\pmb{u})\\nabla j_{p q}(\\pmb{u})\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\sigma_{s},\\sigma_{p q}$ are the Lagrange multiplier functions defined as such [7]: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{s}(\\boldsymbol{u})=\\frac{\\operatorname*{det}\\big(\\mathrm{Gram}_{(j_{1},\\dots,j_{s-1},j_{s},j_{s+1},\\dots,j_{C},j_{12},\\dots,j_{C-1,C})}^{(j_{1},\\dots,j_{s-1},j_{s},j_{s+1},\\dots,j_{C},j_{12},\\dots,j_{C-1,C})}(\\boldsymbol{u})\\big)}{\\operatorname*{det}\\big(\\mathrm{Gram}_{(j_{1},\\dots,j_{C},j_{12},\\dots,j_{C-1,C})}^{(j_{1},\\dots,j_{C},j_{12},\\dots,j_{C-1,C})}(\\boldsymbol{u})\\big)}\\triangleq\\frac{\\operatorname*{det}\\big(\\mathrm{Gram}_{s}(\\boldsymbol{u})\\big)}{\\operatorname*{det}\\big(\\mathrm{Gram}(\\boldsymbol{u})\\big)}\\,,}\\\\ &{\\sigma_{p q}(\\boldsymbol{u})=\\frac{\\operatorname*{det}\\big(\\mathrm{Gram}_{(j_{1},\\dots,j_{C},j_{12},\\dots,j_{P q},j_{p q+1},\\dots,j_{C-1,C})}^{(j_{1},\\dots,j_{C},j_{12},\\dots,j_{C-1,C})}(\\boldsymbol{u})\\big)}{\\operatorname*{det}\\big(\\mathrm{Gram}_{(j_{1},\\dots,j_{C},j_{12},\\dots,j_{C-1,C})}^{(j_{1},\\dots,j_{C},j_{12},\\dots,j_{C-1,C})}(\\boldsymbol{u})\\big)}\\triangleq\\frac{\\operatorname*{det}\\big(\\mathrm{Gram}_{p q}(\\boldsymbol{u})\\big)}{\\operatorname*{det}\\big(\\mathrm{Gram}(\\boldsymbol{u})\\big)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The Gram matrix in the denominator of the Lagrange multiplier functions can be defined as a block matrix as follows, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Gram}({\\pmb u})=\\left[\\begin{array}{c c}{A}&{C^{\\top}}\\\\ {C}&{B}\\end{array}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{A}=\\left[\\begin{array}{c c c}{\\langle\\nabla j_{1},\\nabla j_{1}\\rangle}&{\\hdots}&{\\langle\\nabla j_{C},\\nabla j_{1}\\rangle}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\langle\\nabla j_{1},\\nabla j_{C}\\rangle}&{\\hdots}&{\\langle\\nabla j_{C},\\nabla j_{C}\\rangle}\\end{array}\\right],}\\\\ &{\\boldsymbol{B}=\\left[\\begin{array}{c c c}{\\langle\\nabla j_{12},\\nabla j_{12}\\rangle}&{\\hdots}&{\\langle\\nabla j_{C-1,C},\\nabla j_{12}\\rangle}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\langle\\nabla j_{12},\\nabla j_{C-1,C}\\rangle}&{\\hdots}&{\\langle\\nabla j_{C-1,C},\\nabla j_{C-1,C}\\rangle}\\end{array}\\right],}\\\\ &{\\boldsymbol{C}=\\left[\\begin{array}{c c c}{\\langle\\nabla j_{1},\\nabla j_{12}\\rangle}&{\\hdots}&{\\langle\\nabla j_{C},\\nabla j_{12}\\rangle}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\langle\\nabla j_{1},\\nabla j_{C-1,C}\\rangle}&{\\hdots}&{\\langle\\nabla j_{C},\\nabla j_{C-1,C}\\rangle}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For $\\mathrm{Gram}_{s}(u)$ and ${\\mathrm{Gram}}_{p q}(\\pmb{u})$ , we can similarly define block decompositions by replacing the appropriate column of the $\\bar{\\mathrm{Gram}}(u)$ based on the index. For instance, to define $\\mathrm{Gram}_{s}(u)$ , we replace the column $s$ with $[\\langle\\nabla f(\\pmb{u}),\\nabla j_{i}(\\pmb{u})\\rangle]_{i=1}^{C-1,C}$ ", "page_idx": 19}, {"type": "text", "text": "It has been proved [9] that if $U\\in S t_{C}^{d}$ is a critical point of the function $\\tilde{f}$ , i.e., $\\partial f(\\pmb{u})=0$ , then $\\sigma_{s}(\\pmb{u}),\\sigma_{p q}(\\pmb{\\dot{u}})$ become the classical Lagrange multipliers. ", "page_idx": 19}, {"type": "text", "text": "Proposition 2 (Lagrange multiplier functions for Stiefel Manifolds). The Lagrange multiplier functions are described as a function of the constraint functions in Equation 35 in the following way: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\sigma_{s}}({\\pmb u})=\\langle\\nabla f({\\pmb u}),\\nabla j_{s}({\\pmb u})\\rangle=\\left\\langle\\frac{\\partial f}{\\partial{\\pmb u}_{s}}({\\pmb u}),{\\pmb u}_{s}\\right\\rangle\\,,}}\\\\ {{\\displaystyle{\\sigma_{p q}}({\\pmb u})=\\frac{1}{2}\\langle\\nabla f({\\pmb u}),\\nabla j_{p q}({\\pmb u})\\rangle=\\frac{1}{2}\\left(\\left\\langle\\frac{\\partial f}{\\partial{\\pmb u}_{q}}({\\pmb u}),{\\pmb u}_{p}\\right\\rangle\\,+\\left\\langle\\frac{\\partial f}{\\partial{\\pmb u}_{p}}({\\pmb u}),{\\pmb u}_{q}\\right\\rangle\\right)\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We take the definition of the Lagrange multiplier functions in Equation 37. Starting with the denominator, we observe that the Gram matrix is of the form: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Gram}(u)=\\left[\\begin{array}{c c}{{I_{C}}}&{{\\bf0}}\\\\ {{\\bf0}}&{{2I_{\\frac{C(C-1)}{2}}}}\\end{array}\\right]\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "since for each block matrix $(A,B,C)$ of the Gramian, we get for each of their elements: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bullet\\,\\,\\forall A_{s,r}\\colon\\,\\langle\\nabla j_{s}(\\pmb{u}),\\nabla j_{r}(\\pmb{u})\\rangle=\\delta_{s r}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bullet\\ \\forall B_{\\gamma\\tau,\\alpha\\beta}\\colon\\left\\langle\\nabla j_{\\gamma\\tau}(\\pmb{u}),\\nabla j_{\\alpha\\beta}(\\pmb{u})\\right\\rangle=2\\delta_{\\gamma\\alpha}\\delta_{\\tau\\beta}+2\\delta_{\\tau\\alpha}\\delta_{\\gamma\\beta}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bullet\\,\\,\\forall C_{s,\\alpha\\beta}\\colon\\,\\langle\\nabla j_{s}(\\pmb{u}),\\nabla j_{\\alpha\\beta}(\\pmb{u})\\rangle=2\\delta_{s\\alpha}\\delta_{s\\beta}=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\delta_{i j}$ is defined as the Kronecker delta symbol, i.e., $\\delta_{i j}=[i=j]$ . ", "page_idx": 20}, {"type": "text", "text": "Thus, the determinant of this Gram matrix is: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{det}(\\operatorname{Gram}(u))=\\operatorname*{det}\\left[\\begin{array}{c c}{I_{C}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{2I_{\\frac{C(C-1)}{2}}}\\end{array}\\right]=\\operatorname*{det}(I_{C})\\operatorname*{det}\\left(2I_{\\frac{C(C-1)}{2}}\\right)=2^{\\frac{C(C-1)}{2}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, let us turn our focus to the numerator of the Lagrange multiplier function $\\sigma_{s}(\\pmb{u})$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{det}(\\operatorname{Gram}_{s}(u))=\\operatorname*{det}\\left[\\begin{array}{c c c c c c c}{1}&{\\cdots}&{\\langle\\nabla f(u),\\nabla j_{1}(u)\\rangle}&{\\cdots}&{0}&{0}&{\\cdots}&{0}\\\\ {\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{\\cdots}&{\\langle\\nabla f(u),\\nabla j_{s}(u)\\rangle}&{\\cdots}&{0}&{0}&{\\cdots}&{0}\\\\ {\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{\\cdots}&{\\langle\\nabla f(u),\\nabla j_{C}(u)\\rangle}&{\\cdots}&{1}&{0}&{\\cdots}&{0}\\\\ {0}&{\\cdots}&{\\langle\\nabla f(u),\\nabla j_{12}(u)\\rangle}&{\\cdots}&{0}&{2}&{\\cdots}&{0}\\\\ {\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{\\cdots}&{\\langle\\nabla f(u),\\nabla j_{C-1,C}(u)\\rangle}&{\\cdots}&{0}&{0}&{\\cdots}&{\\vdots}\\\\ {0}&{\\cdots}&{\\langle\\nabla f(u),\\nabla j_{C-1,C}(u)\\rangle}&{\\cdots}&{0}&{\\cdots}&{0}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\langle\\nabla f(\\pmb{u}),\\nabla j_{s}(\\pmb{u})\\rangle\\cdot\\operatorname*{det}\\left[\\begin{array}{c c}{\\pmb{I}_{C-1}}&{\\pmb{0}}\\\\ {\\pmb{0}}&{2\\pmb{I}_{\\frac{C(C-1)}{2}}}\\end{array}\\right]}\\\\ &{=2^{\\frac{C(C-1)}{2}}\\langle\\nabla f(\\pmb{u}),\\nabla j_{s}(\\pmb{u})\\rangle\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, for $\\sigma_{p q}(\\pmb{u})$ , we get: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{det}(\\mathrm{Gram}_{p q}({\\pmb u}))=\\langle\\nabla f({\\pmb u}),\\nabla j_{p q}({\\pmb u})\\rangle\\cdot\\operatorname*{det}\\left[\\begin{array}{c c}{{\\pmb I}_{C}}&{{\\pmb0}}\\\\ {{\\pmb0}}&{{2\\pmb I}_{\\frac{C(C-1)}{2}-1}}\\end{array}\\right]}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=2^{\\frac{C(C-1)}{2}-1}\\langle\\nabla f({\\pmb u}),\\nabla j_{p q}({\\pmb u})\\rangle\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, we get the result by combining the determinants\u2019 results for each Lagrange multiplier function and computing the gradients of the constraint functions. ", "page_idx": 20}, {"type": "text", "text": "Similarly to the gradient vector field, we can show that the Hessian for a constraint manifold (e.g., Stiefel manifold, $\\mathrm{Hess}\\tilde{f}(\\pmb{u}):T_{\\pmb{u}}S t_{C}^{d}\\times T_{\\pmb{u}}S t_{C}^{d}\\rightarrow\\mathbb{R})$ ) can be given as such [10]: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{Hess}\\tilde{f}(\\pmb{u})=\\left(\\mathrm{Hess}f(\\pmb{u})-\\displaystyle\\sum_{1\\leq s\\leq C}\\sigma_{s}(\\pmb{u})\\mathrm{Hess}j_{s}(\\pmb{u})\\right.}\\\\ &{}&{\\left.-\\displaystyle\\sum_{1\\leq p<q\\leq C}\\sigma_{p q}(\\pmb{u})\\mathrm{Hess}j_{p q}(\\pmb{u})\\right)_{|T_{\\pmb{u}}S t_{C}^{d}\\times T_{\\pmb{u}}S t_{C}^{d}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can transform the results into a matrix form in the following way. First, we denote, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla f(\\pmb{U})\\triangleq\\mathrm{vec}^{-1}(\\nabla f(\\pmb{u}))\\in\\mathbb{R}^{d\\times C};\\quad\\partial f(\\pmb{U})\\triangleq\\mathrm{vec}^{-1}(\\partial f(\\pmb{u}))\\in\\mathbb{R}^{d\\times C}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We then introduce a symmetric matrix $\\Sigma(\\pmb{U})\\,\\triangleq\\,[\\sigma_{p q}(\\pmb{u})]\\,\\in\\,\\mathbb{R}^{C\\times C}$ , where we also include the Lagrange multiplier function\u2019s symmetrical component. For the Stiefel manifold, the matrix form of the embedded gradient vector field (after some simple computation) is given by, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\partial f(\\pmb{U})=\\nabla f(\\pmb{U})-\\pmb{U}\\Sigma(\\pmb{U})\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\Sigma(U)=\\frac{1}{2}\\left(\\nabla f(U)^{\\top}U+U^{\\top}\\nabla f(U)\\right)\\!.}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "Regarding the Hessian in Equation 44, to write it in matrix form, we first need to compute the Hessian matrices of the constraint functions as follows3: ", "page_idx": 21}, {"type": "equation", "text": "$$\n[\\mathrm{Hess}j_{s}(U)]=\\left[\\begin{array}{c c c c c}{\\mathbf{0}_{d}}&{\\ldots}&{\\mathbf{0}_{d}}&{\\ldots}&{\\mathbf{0}_{d}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\mathbf{0}_{d}}&{\\ldots}&{I_{d}}&{\\ldots}&{\\mathbf{0}_{d}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\mathbf{0}_{d}}&{\\ldots}&{\\mathbf{0}_{d}}&{\\ldots}&{\\mathbf{0}_{d}}\\end{array}\\right]=I_{d}\\otimes\\left(e_{s}\\otimes e_{s}^{\\top}\\right);\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n[\\operatorname{Hess}j_{p q}(U)]=\\left[\\begin{array}{l l l l l l l}{0_{d}}&{\\dots}&{0_{d}}&{\\dots}&{0_{d}}&{\\dots}&{0_{d}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0_{d}}&{\\dots}&{0_{d}}&{\\dots}&{I_{d}}&{\\dots}&{0_{d}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0_{d}}&{\\dots}&{I_{d}}&{\\dots}&{0_{d}}&{\\dots}&{0_{d}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0_{d}}&{\\dots}&{\\mathbf{0}_{d}}&{\\dots}&{\\mathbf{0}_{d}}&{\\dots}&{\\mathbf{0}_{d}}\\end{array}\\right]=I_{d}\\otimes\\left(e_{p}\\otimes e_{q}^{\\top}+e_{q}\\otimes e_{p}^{\\top}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, the matrix form of the Hessian of the cost function $\\tilde{f}:S t_{C}^{d}\\to\\mathbb{R}$ is given by, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{Hess}\\tilde{f}(U)=(\\mathrm{Hess}f(U)-I_{d}\\otimes\\Sigma(U))_{\\mid T_{U}S t_{C}^{d}\\times T_{U}S t_{C}^{d}}\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where from there, we can re-define the expression $\\pmb{G}$ in Equation 33 as follows, ", "page_idx": 21}, {"type": "equation", "text": "$$\nG=\\mathrm{rvec}(D_{U U}^{2}\\,f(\\tilde{H},U))-I_{d}\\otimes\\Sigma(U)\\in\\mathbb{R}^{d C\\times d C}\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C Additional Experimental Results ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "z4FaPUslma/tmp/e93bf2980a7f0fc4483316fa8d3a8c4a4d5705de6cfc315998fd073b94607380.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 6: CIFAR10 results on ResNet-18. In all plots, the ${\\bf X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}$ -axis denotes the number of training examples. ", "page_idx": 21}, {"type": "image", "img_path": "z4FaPUslma/tmp/9b6329293c1d4ab2605784ec826a460bd449c5212ccdfa5243f4eed15b6b3427.jpg", "img_caption": ["Figure 7: CIFAR10 results on VGG-13. In all plots, the ${\\bf X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}_{\\mathrm{~}}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "z4FaPUslma/tmp/964c71debf794dc2c8fbcc31b87be82fa0a6df88efc57fb23ab5c1c72ece6dd0.jpg", "img_caption": ["Figure 8: CIFAR100 results on ResNet-50. In all plots, the ${\\bf X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "z4FaPUslma/tmp/015e081992f11113266e46c319599cb710763822180230f8135d4ac2e4b86dc3.jpg", "img_caption": ["Figure 9: CIFAR100 results on VGG-13. In all plots, the ${\\bf X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "z4FaPUslma/tmp/38cddc1860ad24fc8982baf67c3fb0d7b58cf59b0b695f726d9018c0b113ca7a.jpg", "img_caption": ["Figure 10: STL10 results on ResNet-50. In all plots, the $\\mathbf{X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "z4FaPUslma/tmp/5ae0b251331cdc6ad5bada9e7e72e47cc26237b832f84ced11e15943ccc9f520.jpg", "img_caption": ["Figure 11: STL10 results on VGG-13. In all plots, the $\\mathbf{X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "z4FaPUslma/tmp/03019294599db7bdaffb8e2dc454f8f88c22398ffd845fa4197154108de7bec3.jpg", "img_caption": ["Figure 12: UFM-100 results. In all plots, the $\\mathbf{X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}_{\\mathrm{}}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "z4FaPUslma/tmp/e77e07908c0030a4348a7faaab615d1fe133f95bd50c3eb2a84370009898cb41.jpg", "img_caption": ["Figure 13: UFM-200 results. In all plots, the $\\mathbf{X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}_{\\mathrm{}}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "z4FaPUslma/tmp/917160efa419f2e2fb62f9485ad53868d789fd65982321af71dd03bdd1b4258a.jpg", "img_caption": ["Figure 14: UFM-1000 results. In all plots, the $\\mathbf{X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}_{\\mathrm{}}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "z4FaPUslma/tmp/ef8ed2319fb1a477b32b36ae84f9d6bfc312893be29492f7eeac8bc8d2ec4260.jpg", "img_caption": ["Figure 15: CIFAR10 results on VGG-13, comparing the implicit ETF method in two scenarios: one where the DDN gradient is computed and included in the SGD update, and another where the DDN gradient computation is omitted from the update. In all plots, the x-axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "z4FaPUslma/tmp/f401432350c0552b4d5b5804359b67db43d0fcf09b2d2a926227ae6859a83784.jpg", "img_caption": ["(a) Forward pass times in milliseconds. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "z4FaPUslma/tmp/5349efc27b16bc8535293013d4bfae10112fe1c47de0b93393d7c52cb0854ddf.jpg", "img_caption": ["(b) Forward and backward times in (log) millisecs. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 16: CIFAR10 computational cost results on ResNet-18. In (a), we plot the forward pass time for each method. For the implicit ETF method, which has dynamic computation times, we also include the mean and median time values. In (b), we plot the computational cost for each forward and backward pass across methods. For the implicit ETF forward pass, we have taken its median time. The notation is as follows: $S/\\mathrm{F}=\\mathsf{S}$ tandard Forward Pass, $S/\\mathbf{B}=1$ Standard Backward Pass, $\\mathrm{F/F=}$ Fixed ETF Forward Pass, $\\mathrm{F/B=}$ Fixed ETF Backward Pass, $\\mathrm{I/F=}$ Implicit ETF Forward Pass, and I/B $=$ Implicit ETF Backward Pass. ", "page_idx": 26}, {"type": "image", "img_path": "z4FaPUslma/tmp/93b9e0c29d8416203e08cc092a252402f241358afef89162f94bc367cea76d7a.jpg", "img_caption": ["Figure 17: CIFAR10 results on ResNet-18, where in standard no-norm we do not perform feature and weight normalisation. In all plots, the $\\mathbf{X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "z4FaPUslma/tmp/f7c42f457a562caab3882c02010eb6e8d5c1725f56e3d376fd0ac861c2118d61.jpg", "img_caption": ["Figure 18: CIFAR10 results on ResNet-18, where in fixed random ETF we choose a random orthogonal direction instead of the canonical one. In all plots, the $\\mathbf{X}$ -axis represents the number of epochs, except for plot (c), where the $\\mathbf{X}$ -axis denotes the number of training examples. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We introduced a novel method leveraging the simplex ETF structure and the neural collapse phenomenon to enhance training convergence and stability in neural networks, and we provided experimental results supporting our claims. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We discussed the limitations of our approach in Section 5 of the paper. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not propose new theoretical results. We proposed a new method and its mathematical formulation as well as the appropriate mathematical background. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All the necessary details required for reproducing the results are presented in the paper. Also, the code will be made publicly available upon acceptance. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Due to anonymity reasons, the code is not included in the submission. However, it will be made available in a GitHub repository upon acceptance. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All the specific implementation details can be found in Section 4 of the paper and the appendix. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have tested our results using different random seeds, and we present the median values along with the range. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have specified the GPUs that were used for our experiments. ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] Justification: The paper conforms with the NeurIPS code of ethics. ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: There is no societal impact of the work performed in this paper. ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not pose such risks. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Yes, every model architecture and dataset used were cited. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}]