[{"figure_path": "o8m4RM5mBk/figures/figures_2_1.jpg", "caption": "Figure 2: (a) Visualization of ViT attention. Although ViT behaves well on the source domain (miniImageNet), on target domains ViT tends to (1) wrongly focus on the CLS token while omitting the input images, and (2) dispersively focus on a large range of noisy image regions. (b) We quantitatively plot (top) the attention value of the CLS token and (bottom) the density of attention maps on image tokens for all images in each domain. The black curve of miniImageNet is always below other curves of target domains, verifying the ineffectiveness of target-domain attention. Therefore, we interpret the temperature adjustment as a remedy for the ineffective target-domain attention.", "description": "This figure visualizes and quantifies the attention mechanism of Vision Transformers (ViTs) in cross-domain few-shot learning.  Panel (a) shows heatmaps of attention weights for different datasets, highlighting that ViTs trained on miniImageNet (source domain) struggle to focus effectively on relevant image regions in target domains.  Instead, they often focus disproportionately on the classification (CLS) token and/or noisy image regions. Panel (b) provides a quantitative analysis. It graphs the attention weight on the CLS token and the density of attention across image tokens for each dataset. The consistent drop in attention to image features in the target domains, compared to miniImageNet, supports the claim that the temperature adjustment in the attention mechanism alleviates this issue by addressing ineffective target domain attention.", "section": "2.2 Interpretation: Attention Temperature Remedies Target-Domain Attentions"}, {"figure_path": "o8m4RM5mBk/figures/figures_4_1.jpg", "caption": "Figure 2: (a) Visualization of ViT attention. Although ViT behaves well on the source domain (miniImageNet), on target domains ViT tends to (1) wrongly focus on the CLS token while omitting the input images, and (2) dispersively focus on a large range of noisy image regions. (b) We quantitatively plot (top) the attention value of the CLS token and (bottom) the density of attention maps on image tokens for all images in each domain. The black curve of miniImageNet is always below other curves of target domains, verifying the ineffectiveness of target-domain attention. Therefore, we interpret the temperature adjustment as a remedy for the ineffective target-domain attention.", "description": "This figure visualizes and quantifies the attention mechanism of Vision Transformers (ViTs) in cross-domain few-shot learning. Part (a) shows heatmaps of attention weights, highlighting that ViTs trained on a source domain struggle to focus on relevant image regions in the target domains; instead, they inappropriately focus on the CLS token or noisy regions. Part (b) provides quantitative analysis by plotting attention values on the CLS token and attention density on image tokens.  The consistently lower values for miniImageNet (source domain) compared to target domains supports the claim that target-domain attention is ineffective, and the temperature adjustment is a remedy.", "section": "2.2 Interpretation: Attention Temperature Remedies Target-Domain Attentions"}, {"figure_path": "o8m4RM5mBk/figures/figures_5_1.jpg", "caption": "Figure 4: During the source-domain training, we propose to randomly abandon the attention network by multiplying a temperature of 0 to the attention in each block respectively, which resists the learning of the query-key attention parameters and enhances the non-query-key parts.", "description": "This figure illustrates the proposed method for source-domain training.  It shows a ViT block with the attention mechanism. A binary variable, \u03c4, is introduced to randomly drop out (set to 0) or keep (set to 1) the attention network.  By randomly setting \u03c4 to 0, the learning of query-key attention parameters is resisted and the non-query-key parts are enhanced, improving the model's transferability in cross-domain few-shot learning.", "section": "3 Method"}, {"figure_path": "o8m4RM5mBk/figures/figures_8_1.jpg", "caption": "Figure 5: (a) Visualization of attention maps of our model. We can see images activated only on the CLS token in Fig. 2a are now correctly activated, and the model can focus on the meaningful and concentrated regions, verifying the improved attention networks. (b) By evaluating the image-token-attention density and the CLS-token-attention value of our model, we can see these criteria are non-distinguishable between the source and target domains (compared with Fig. 2b), indicating attention networks\u2019 transferability against domain gaps is improved.", "description": "This figure visualizes and quantitatively analyzes the attention maps of the improved model. (a) shows that the model now correctly attends to image features instead of only the CLS token, focusing on meaningful regions. (b) demonstrates that the attention values and densities are consistent across source and target domains, highlighting improved transferability.", "section": "4.5 Verification of Improved Attention"}, {"figure_path": "o8m4RM5mBk/figures/figures_9_1.jpg", "caption": "Figure 6: (a) Domain similarity of self-attention outputs consistently increases. (b) The difference in accuracy between different attention choices is narrowed. (c) A high probability of abandoning query-key attention can help the transferability. (d) All blocks need to be put into Attention Abandonment.", "description": "This figure presents an ablation study on the proposed method, showing the impact of different components on domain similarity and accuracy. (a) shows that domain similarity consistently improves with the proposed method. (b) shows that the accuracy gap between different attention mechanisms is reduced with the proposed method. (c) shows the effect of the probability of abandoning query-key attention on accuracy, indicating a high probability helps transferability. (d) demonstrates the necessity of including all blocks in the attention abandonment process.  The results highlight the effectiveness of the proposed approach in improving cross-domain transferability.", "section": "4.5 Verification of Improved Attention"}, {"figure_path": "o8m4RM5mBk/figures/figures_14_1.jpg", "caption": "Figure 7: Samples of source domain miniImagenet dataset (left) and target domain datasets (right), from top to bottom correspond to CropDiseases, EuroSAT, ISIC2018, and ChestX. We can see large domain gaps between source and target domains.", "description": "This figure shows sample images from the source domain (miniImageNet) and four target domains used in the cross-domain few-shot learning experiments.  The source domain contains images of diverse objects (animals, landscapes, objects), while each target domain shows a specific image category: CropDiseases (plant diseases), EuroSAT (satellite imagery), ISIC2018 (skin lesions), and ChestX (chest X-rays). The visual differences highlight the significant domain gaps that the proposed method addresses.", "section": "A.1 Dataset Description"}, {"figure_path": "o8m4RM5mBk/figures/figures_15_1.jpg", "caption": "Figure 8: Average target domain accuracy vs. temperature. The red point represents the temperature is 1.0. If the attention is good enough, the attention adjustment will be trivial. Our method shows less reliance on the attention adjustments compared with the baseline method and achieves the best performance when the temperature is 1.0 in most blocks. This indicates the attention produced by our method is improved.", "description": "This figure shows the average target domain accuracy for both the proposed method and the baseline method across different temperature values (from 0.1 to 2.0) for each of the 12 blocks in the ViT architecture. The red point on each curve indicates a temperature of 1.0 (no temperature adjustment).  The results show that the proposed method is less sensitive to temperature adjustments than the baseline, indicating that the attention mechanism in the proposed method is more effective and requires less fine-tuning.", "section": "A.2.2 The Effectiveness of Our Attention"}]