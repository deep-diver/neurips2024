[{"figure_path": "bPOaHf8OcX/tables/tables_8_1.jpg", "caption": "Table 1: Multi-view video generation. Best in bold.", "description": "This table presents a quantitative comparison of the proposed Vivid-ZOO model against a baseline method (MVDream + IP-AnimateDiff) for multi-view video generation.  The metrics used are Frechet Video Distance (FVD), which measures video quality, and CLIP score, which assesses the alignment between generated videos and text prompts.  The 'Overall' column shows the percentage improvement of Vivid-ZOO over the baseline.  Lower FVD scores indicate better video quality, and higher CLIP scores indicate better text-prompt alignment.  Vivid-ZOO demonstrates a significant improvement in overall performance compared to the baseline, achieving a 72% improvement based on the evaluation metrics.", "section": "4.1 Qualitative and quantitative results"}, {"figure_path": "bPOaHf8OcX/tables/tables_8_2.jpg", "caption": "Table 2: The ablation study results. The overall performance is assessed by a user study using paired comparison [3, 15].", "description": "This table presents the results of an ablation study evaluating the contributions of different components of the proposed Vivid-ZOO model.  The overall performance is measured using a paired comparison user study, where participants compared the results of various model configurations. The configurations include the baseline (w/o MS w SD), variants with temporal module removed (w/o MT w TM LoRA), and variants missing either the 3D-2D alignment or the 2D-3D alignment layers.  The final row shows the performance of the full Vivid-ZOO model. The percentage improvement in \"Overall\" indicates a relative ranking of model performance compared to the baseline.", "section": "4.2 Ablation study and discussions"}, {"figure_path": "bPOaHf8OcX/tables/tables_21_1.jpg", "caption": "Table 1: Multi-view video generation. Best in bold.", "description": "This table presents a quantitative comparison of the proposed Vivid-ZOO model against a baseline method (MVDream + IP-AnimateDiff) on the task of multi-view video generation.  The metrics used are Fr\u00e9chet Video Distance (FVD), which measures the visual quality and temporal consistency, CLIP score, evaluating the alignment between generated videos and text prompts, and an overall score combining the two metrics. Lower FVD values and higher CLIP and overall scores indicate better performance. The table highlights the significant improvement achieved by Vivid-ZOO compared to the baseline, showing substantial gains in both video quality and text alignment.", "section": "4 Experiments"}, {"figure_path": "bPOaHf8OcX/tables/tables_21_2.jpg", "caption": "Table II: Training settings", "description": "This table lists the hyperparameters and hardware configurations used for training the Vivid-ZOO model.  It details settings such as the type of noise scheduler, the number of timesteps, beta values, optimizer, learning rate, batch size, and the computational resources used (CPU and GPU). These parameters are crucial to understanding the model's training process and reproducibility of the results.", "section": "Experiments"}, {"figure_path": "bPOaHf8OcX/tables/tables_21_3.jpg", "caption": "Table 1: Multi-view video generation. Best in bold.", "description": "This table presents a quantitative comparison of the proposed Vivid-ZOO model against a baseline method (MVDream + IP-AnimateDiff) for multi-view video generation.  The metrics used are Frechet Video Distance (FVD), which measures video quality, CLIP score, which assesses the alignment between the generated video and the input text prompt, and an overall score representing a combination of video quality and text alignment.  Lower FVD values are better, while higher CLIP and overall scores are better. The table shows that Vivid-ZOO significantly outperforms the baseline in both video quality and overall performance.", "section": "4 Experiments"}]