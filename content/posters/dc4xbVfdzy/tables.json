[{"figure_path": "dc4xbVfdzy/tables/tables_6_1.jpg", "caption": "Table 1: Overall Performance. M, M-E, and M-R denotes the medium, medium-expert, and medium-replay, respectively. The results of the baselines marked with \u2020 are cited from their original papers. We report the mean and standard deviation of the normalized score with four random seeds. Bold and underline indicate the highest score and second-highest score, respectively.", "description": "This table presents the overall performance comparison of Decision Mamba (DM) against various baseline offline reinforcement learning methods across different datasets (medium, medium-expert, and medium-replay).  The performance is measured by the normalized score, averaged over four runs with different random seeds.  The best and second-best results for each dataset are highlighted.", "section": "4.2 Overall Results"}, {"figure_path": "dc4xbVfdzy/tables/tables_6_2.jpg", "caption": "Table 2: Extensive Results. E, U, and U-D denotes the expert, umazed, and umazed-diverse.", "description": "This table presents a comprehensive comparison of the performance of Decision Mamba (DM) against various baseline methods across different datasets with varying levels of difficulty.  The datasets are categorized by their difficulty (Expert, Umazed, Umazed-Diverse) and the results are represented by average normalized scores.  This allows for a robust evaluation of DM's performance in offline reinforcement learning across diverse scenarios and quality levels of data.", "section": "4.2 Overall Results"}, {"figure_path": "dc4xbVfdzy/tables/tables_7_1.jpg", "caption": "Table 3: Ablation Results. \"w/o MG/PSER/ILO\" represents removing the module of multi-grained feature extraction, the progressive self-evolution regularization, and inverse learning objectives, respectively. Best results are marked in bold.", "description": "This table presents the ablation study results for the Decision Mamba model. It shows the impact of removing each of three key components: the multi-grained feature extraction, progressive self-evolution regularization, and inverse learning objective.  The table compares the average performance across multiple metrics (Halfcheetah, Hopper, Walker, Avg) and different dataset difficulties (M, M-E, M-R) by showing the performance drop when each component is removed.  The results demonstrate the importance of each component to the model's overall performance.", "section": "4.3 Ablation Study"}, {"figure_path": "dc4xbVfdzy/tables/tables_8_1.jpg", "caption": "Table 4: The effects of \u03b2 in PSER.", "description": "This table presents the ablation study results focusing on the impact of the hyperparameter \u03b2 in the Progressive Self-Evolution Regularization (PSER) strategy.  Different values of \u03b2 were tested (1, 0.75, 0.5, 0.25, and 0), representing different levels of reliance on past knowledge for refining the target labels during training. The table shows the average normalized scores achieved on the Halfcheetah, Hopper, and Walker MuJoCo tasks, across three data splits (M, M-E, M-R) for each \u03b2 value.  The results demonstrate the influence of \u03b2 on model performance and optimal balance between trusting the original label and relying on past predictions.", "section": "4.5 The Effects of \u03b2 in PSER"}, {"figure_path": "dc4xbVfdzy/tables/tables_13_1.jpg", "caption": "Table 5: Task-specific Hyperparameters.", "description": "This table lists the hyperparameters used for each task in the experiments.  It includes the learning rate, weight decay, context length, return-to-go values, total training steps, and the parameters \u03b2k and \u03b2min used in the progressive self-evolution regularization (PSER) strategy.  These hyperparameters were tuned for optimal performance on each specific task.", "section": "4.1 Settings"}, {"figure_path": "dc4xbVfdzy/tables/tables_14_1.jpg", "caption": "Table 1: Overall Performance. M, M-E, and M-R denotes the medium, medium-expert, and medium-replay, respectively. The results of the baselines marked with \u2020 are cited from their original papers. We report the mean and standard deviation of the normalized score with four random seeds. Bold and underline indicate the highest score and second-highest score, respectively.", "description": "This table presents the overall performance of the Decision Mamba model and several baseline offline reinforcement learning methods across various datasets.  The performance is measured by the normalized score, averaged over four random seeds, with standard deviations reported.  The table distinguishes between medium, medium-expert, and medium-replay datasets, reflecting different levels of data quality.  Bold and underlined entries highlight the top-performing methods for each dataset.", "section": "4.2 Overall Results"}]