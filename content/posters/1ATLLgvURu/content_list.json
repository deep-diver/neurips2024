[{"type": "text", "text": "Learning-Augmented Priority Queues ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziyad Benomar   \nENSAE, Ecole Polytechnique, FairPlay joint team   \nziyad.benomar@ensae.fr ", "page_idx": 0}, {"type": "text", "text": "Christian Coester Department of Computer Science University of Oxford, UK christian.coester@cs.ox.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Priority queues are one of the most fundamental and widely used data structures in computer science. Their primary objective is to efficiently support the insertion of new elements with assigned priorities and the extraction of the highest priority element. In this study, we investigate the design of priority queues within the learning-augmented framework, where algorithms use potentially inaccurate predictions to enhance their worst-case performance. We examine three prediction models spanning different use cases, and we show how the predictions can be leveraged to enhance the performance of priority queue operations. Moreover, we demonstrate the optimality of our solution and discuss some possible applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Priority queues are an essential abstract data type in computer science [Jaiswal, 1968, Brodal, 2013] whose objective is to enable the swift insertion of new elements and access or deletion of the highest priority element. Their applications span a wide range of problems within computer science and beyond. They play a crucial role in sorting [Williams, 1964, Thorup, 2007], in various graph algorithms such as Dijkstra\u2019s shortest path algorithm [Chen et al., 2007] or computing minimum spanning trees [Chazelle, 2000], in operating systems for scheduling and load balancing [Sharma et al., 2022], in networking protocols for managing data transmission packets [Moon et al., 2000], in discrete simulations for efficient event processing based on occurrence time [Goh and Thng, 2004], and in implementing hierarchical clustering algorithms [Day and Edelsbrunner, 1984, Olson, 1995]. ", "page_idx": 0}, {"type": "text", "text": "Various data structures can be used to implement priority queues, each offering distinct advantages and tradeoffs [Brodal, 2013]. However, it is established that a priority queue with $n$ elements cannot guarantee $o(\\log n)$ time for all the required operations [Brodal and Okasaki, 1996]. This limitation can be surpassed within the learning augmented framework [Mitzenmacher and Vassilvitskii, 2022], where the algorithms can benefit from machine-learned or expert advice to improve their worst-case performance. We propose in this work learning-augmented implementations of priority queues in three different prediction models, detailed in the next section. ", "page_idx": 0}, {"type": "text", "text": "1.1 Problem definition ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A priority queue is a dynamic data structure where each element $x$ is assigned a key $u$ from a totally ordered universe $(u,<)$ , determining its priority. The standard operations of priority queues are: ", "page_idx": 0}, {"type": "text", "text": "(i) $\\operatorname{FindMin}()$ : returns the element with the smallest key without removing it, (ii) $\\mathrm{{ExtractMin}()}$ : removes and returns the element with the smallest key, (iii) $\\mathrm{Insert}(x,u)$ : adds a new element $x$ to the priority queue with key $u$ , (iv) DecreaseKey $(x,v)$ : decreases the key of an element $x$ to $v$ . ", "page_idx": 0}, {"type": "text", "text": "The elements of the priority queue can be accessed via their keys in $O(1)$ time using a HashMap. Hence, the focus is on establishing efficient algorithms for key storage and organization, facilitating the execution of priority queue operations. For any key $u\\in\\mathcal{U}$ and subset $\\mathcal{V}\\subset\\mathcal{U}$ , we denote by $r(u,\\mathcal{V})$ the rank of $u$ in $\\nu$ , defined as the number of keys in $\\nu$ that are smaller than or equal to $u$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\nr(u,\\mathcal{V})=\\#\\{v\\in\\mathcal{V}:v\\leq u\\}\\;.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The difficulty lies in designing data structures offering adequate tradeoffs between the complexities of the operations listed above. This paper explores how using predictions can allow us to overcome the limitations of traditional priority queues. We examine three types of predictions. ", "page_idx": 1}, {"type": "text", "text": "Dirty comparisons. In the first prediction model, comparing two keys $(u,v)\\in\\mathcal{U}^{2}$ is slow or costly. However, the algorithm can query a prediction of the comparison $(u<v)$ . This prediction serves as a rapid or inexpensive, but possibly inaccurate, method of comparing elements of $\\boldsymbol{\\mathcal{U}}$ , termed a dirty comparison and denoted by $(u\\hat{<}v)$ . Conversely, the true outcome of $(u<v)$ is referred to as a clean comparison. For all $u\\in\\mathcal{U}$ and $\\mathcal{V}\\subseteq\\mathcal{U}$ , we denote by $\\eta(u,\\nu)$ the number of inaccurate dirty comparisons between $u$ and elements of $\\nu$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\eta(u,\\mathcal{V})=\\#\\{v\\in\\mathcal{V}\\ :\\ \\mathbb{1}(u\\stackrel{\\sim}{<}v)\\not=\\mathbb{1}(u<v)\\}\\ .\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "This prediction model was introduced in [Bai and Coester, 2023] for sorting, but it has a broader significance in comparison-based problems, such as search [Borgstrom and Kosaraju, 1993, Nowak, 2009, Tschopp et al., 2011], ranking [Wauthier et al., 2013, Shah and Wainwright, 2018, Heckel et al., 2018], and the design of comparison-based ML algorithms [Haghiri et al., 2017, 2018, Ghoshdastidar et al., 2019, Perrot et al., 2020, Meister and Nietert, 2021]. Particularly, it has great theoretical importance for priority queues, which have been extensively studied in the comparison-based framework [Gonnet and Munro, 1986, Brodal and Okasaki, 1996, Edelkamp and Wegener, 2000]. Comparisonbased models are often used, for example, when the preferences are determined by human subjects. Assigning numerical scores in these cases is inexact and prone to errors, while pairwise comparisons are absolute and more robust [David, 1963]. Within such a setting, dirty comparisons can be obtained by a binary classifier, and used to minimize human inference yielding clean comparisons, which are time-consuming and might incur additional costs. ", "page_idx": 1}, {"type": "text", "text": "Pointer predictions. In this second model, upon the addition of a new key $u$ to the priority queue $\\mathcal{Q}$ , the algorithm receives a prediction $\\widehat{\\mathrm{Pred}}(u,\\boldsymbol{Q})\\in\\boldsymbol{Q}$ of the predecessor of $u$ , which is the largest key belonging to $\\mathcal{Q}$ and smaller than $u$ . Before $u$ is inserted, the ranks of $u$ and its true predecessor in $\\mathcal{Q}$ are equal, hence we define the prediction error as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\vec{\\eta}(u,\\mathcal{Q})=\\vert r(u,\\mathcal{Q})-r(\\widehat{\\mathrm{Pred}}(u,\\mathcal{Q}),\\mathcal{Q})\\vert\\;.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In priority queue implementations, a HashMap preserves a pointer from each inserted key to its corresponding position in the priority queue. Consequently, $\\widehat{\\mathrm{Pred}}(u,\\boldsymbol{Q})$ provides direct access to the predicted predecessor\u2019s position. For example, this prediction model finds applications in scenarios where concurrent machines have access to the priority queue [Sundell and Tsigas, 2005, Shavit and Lotan, 2000, Lind\u00e9n and Jonsson, 2013]. Each machine can estimate, within the elements it has previously inserted, which one precedes the next element it intends to insert. However, this estimation might not be accurate, as other concurrent machines may have inserted additional elements. ", "page_idx": 1}, {"type": "text", "text": "Rank predictions. The last setting assumes that the priority queue is used in a process where a finite number $N$ of distinct keys will be inserted, i.e., the priority queue is not used indefinitely, but $N$ is unknown to the algorithm. Upon the insertion of any new key $u_{i}$ , the algorithm receives a prediction $\\widehat{R}(u_{i})$ of the rank of $u_{i}$ among all the $N$ keys $\\{u_{j}\\}_{j\\in[N]}$ . Denoting by $R(u_{i})=r(u_{i},\\{u_{j}\\}_{j\\in[N]})$ the true rank, the prediction error of $u_{i}$ is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\eta^{\\Delta}(u_{i})=|R(u_{i})-\\widehat{R}(u_{i})|\\ .\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The same prediction model was explored in [McCauley et al., 2024] for the online list labeling problem. Bai and Coester [2023] investigate a similar model for the sorting problem, but in an offline setting where the $N$ elements to sort and the predictions are accessible to the algorithm from the start. ", "page_idx": 1}, {"type": "text", "text": "Note that $N$ counts the distinct keys added with Insert or DecreaseKey operations. An arbitrarily large number of DecreaseKey operations thus can make $N$ arbitrarily large although the total number of insertions is reduced. However, with a lazy implementation of DecreaseKey, we can assume without loss of generality that $N$ is at most quadratic in the total number of insertions. Indeed, it is possible to omit executing the DecreaseKey operations when queried, and only store the new elements\u2019 keys to update. Then, at the first ExtractMin operation, all the element\u2019s keys are updated by executing for each one only the last queried DecreaseKey operation involving it. Denoting by $k$ the total number of insertions, there are at most $k$ ExtractMin operations, and for each of them, there are at most $k$ DecreaseKey operations executed. The total number of effectively executed DecreaseKey operations is therefore $\\b{O}(k^{2})$ . In particular, this implies that a complexity of ${\\cal O}(\\log N)$ is also logarithmic in the total number of insertions. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.2 Our results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first investigate augmenting binary heaps with predictions. To leverage dirty comparisons, we first design a randomized binary search algorithm to find the position of an element $u$ in a sorted list $L$ . We prove that it terminates using ${\\bar{O}}(\\log\\left|L\\right|)$ dirty comparisons and $O(\\log\\eta(u,L))$ clean comparisons in expectation. Subsequently, we use this result to establish an insertion algorithm in binary heaps using $O(\\log\\log n)$ dirty comparisons and reducing the number of clean comparisons to $O(\\log\\log\\eta(u,\\mathcal{Q}))$ . However, ExtractMin still mandates ${\\cal O}(\\log n)$ clean comparisons. In the two other prediction models, binary heaps and other heap implementations of priority queues appear unsuitable, as the positions of the keys are not determined solely by their ranks. ", "page_idx": 2}, {"type": "text", "text": "Consequently, in Section 3, we shift to using skip lists. We devise randomized insertion algorithms requiring, in expectation, only $O(\\log\\vec{\\eta}(u,\\mathcal{Q}))$ time and comparisons in the pointer prediction model, $O(\\log n)$ time and $O(\\log\\eta(u,\\mathcal{Q}))$ clean comparisons in the dirty comparison model, and $O(\\log\\log N+\\log\\operatorname*{max}_{i\\in[N]}\\eta^{\\Delta}(u_{i}))$ time and $O(\\log\\operatorname*{max}_{i\\in[N]}\\eta^{\\Delta}(u_{i}))$ comparisons in the rank prediction model, where we use in the latter an auxiliary van Emde Boas (vEB) tree [van Emde Boas et al., 1976]. Across the three prediction models, FindMin and ExtractMin only necessitate $O(1)$ time, and the complexity of DecreaseKey aligns with that of insertion. Finally, we prove in Theorem 3.4 the optimality of our data structure. Table 1.2 summarizes the complexities of our learningaugmented priority queue (LAPQ) in the three prediction models compared to standard priority queue implementations. The complexity of FindMin is $O(1)$ for all the listed priority queues. ", "page_idx": 2}, {"type": "table", "img_path": "1ATLLgvURu/tmp/e1d419ab8ad24456c628fbf2cd43a025ead88eac764d0e6bc6c67f2211f34da7.jpg", "table_caption": ["Table 1: Number of comparisons per operation used by different priority queues. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Our learning-augmented data structure enables additional operations beyond those of priority queues, such as the maximum priority queue operations FindMax, ExtractMax, and IncreaseKey with analogous complexities, and removing an arbitrary key $u$ from the priority queue, finding its predecessor or successor in expected $O(1)$ time. ", "page_idx": 2}, {"type": "text", "text": "Furthermore, we show in Section 4.1 that it can be used for sorting, yielding the same guarantees as the learning-augmented sorting algorithms presented in [Bai and Coester, 2023] for the positional predictions model with displacement error, and for the dirty comparison model. In the second model, our priority queue offers even stronger guarantees, as it maintains the elements sorted at any time even if the insertion order is adversarial, while the algorithm of [Bai and Coester, 2023] requires a random insertion order to achieve a sorted list by the end within the complexity guarantees. We also show how the learning-augmented priority queue can be used to accelerate Dijkstra\u2019s algorithm. ", "page_idx": 2}, {"type": "text", "text": "Finally, in Section 5, we compare the performance of our priority queue using predictions with binary and Fibonacci heaps when used for sorting and for Dijkstra\u2019s algorithm on both real-world city maps and synthetic graphs. The experimental results confirm our theoretical findings, showing that adequately using predictions significantly reduces the complexity of priority queue operations. ", "page_idx": 2}, {"type": "text", "text": "1.3 Related work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we briefly discuss related works on learning-augmented algorithms and priority queues.   \nFor a more extensive review of related work, please refer to Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Learning-augmented algorithms. Learning-augmented algorithms, introduced in [Lykouris and Vassilvtiskii, 2018, Purohit et al., 2018], have captured increasing interest over the last years, as they allow breaking longstanding limitations in many algorithm design problems. Assuming that the decision-maker is provided with potentially incorrect predictions regarding unknown parameters of the problem, learning-augmented algorithms must be capable of leveraging these predictions if they are accurate (consistency), while keeping the worst-case performance without advice even if the predictions are arbitrarily bad or adversarial (robustness). While many fundamental online problems were studied in this setting (see Appendix A), the design of data structures with predictions remains relatively underexplored. The seminal paper by [Kraska et al., 2018] shows how predictions can be used to optimize space usage. Another study by [Lin et al., 2022] demonstrates that the runtime of binary search trees can be optimized by incorporating predictions of item access frequency. Recent papers have extended this prediction model to other data structures, such as dictionaries [Zeynali et al., 2024] and skip lists [Fu et al., 2024]. The prediction models we study in the current paper deviate from the latter, and are more related to those considered respectively in [Bai and Coester, 2023] for sorting, and [McCauley et al., 2024] for online list labeling. An overview of the growing body of work on learning-augmented algorithms (also known as algorithms with predictions) is maintained at [Lindermayr and Megow, 2022]. ", "page_idx": 3}, {"type": "text", "text": "Priority queues implementations. Binary heaps, introduced by Williams [1964], are one of the first efficient implementations of priority queues. They allow all the operations in $O(\\log n)$ time, where $n$ is the number of items in the queue. A first improvement was introduced with Binomial heaps [Vuillemin, 1978], reducing the amortized time of insertion to $O(1)$ . A breakthrough came later with Fibonacci heaps [Fredman and Tarjan, 1987], which allow all the operations in constant amortized time, except for ExtractMin, which takes $O(\\log n)$ time. However, Fibonacci heaps are known to be slow in practice [Larkin et al., 2014], and other implementations with weaker theoretical guarantees such as binary heaps are often preferred. Another possible implementation uses skip lists [Pugh, 1990], which are probabilistic data structures, guaranteeing in expectation a constant time for FindMin and ExtractMin, and ${\\cal O}(\\log n)$ time for Insert and DecreaseKey. ", "page_idx": 3}, {"type": "text", "text": "2 Heap priority queues ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A common implementation of priority queues uses binary heaps, enabling all operations in ${\\cal O}(\\log n)$ time. Binary heaps maintain a balanced binary tree structure, where all depth levels are fully filled, except possibly for the last one, to which we refer in all this section as the leaf level. Moreover, it satisfies the heap property, i.e., any key is smaller than all its children. To maintain these two structure properties, when a new element is added, it is first inserted in the leftmost empty position in the leaf level, and then repeatedly swapped with its parent until the heap property is restored. ", "page_idx": 3}, {"type": "text", "text": "2.1 Insertion in the comparison-based model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Insertion in a binary heap can be accomplished using only $O(\\log\\log n)$ comparisons, albeit ${\\cal O}(\\log n)$ time, by doing a binary search of the new element\u2019s position along the path from the leftmost empty position in the leaf level to the root, which is a sorted list of size $O(\\log n)$ . To improve the insertion complexity with dirty comparisons, we first tackle the search problem in this setting. ", "page_idx": 3}, {"type": "text", "text": "Search with dirty comparisons. Consider a sorted list $\\boldsymbol{L}=\\left(v_{1},\\ldots,v_{k}\\right)$ and a target $u$ , the position of $u$ in $L$ can be found using binary search with $O(\\log k)$ comparisons. Extending ideas from Bai and Coester [2023] and Lykouris and Vassilvtiskii [2018], this complexity can be reduced using dirty comparisons. Indeed, we can obtain an estimated position $\\widehat{r}(u,L)$ through a binary search with dirty comparisons, followed by an exponential search with clean comparisons, starting from $\\widehat{r}(u,L)$ to find the exact position $r(u,L)$ . However, the positions of inaccurate dirty comparisons can be adversarially chosen to compromise the algorithm. This can be addressed by introducing randomness to the dirty search phase. We refer to the randomized binary search as the algorithm that proceeds similarly to the binary search, but whenever the search is reduced to an array $\\{v_{i},\\ldots,v_{j}\\}$ , instead of comparing $u$ to the pivot $v_{m}$ with index $\\begin{array}{r}{m=i+\\lfloor\\frac{j-i}{2}\\rfloor}\\end{array}$ , it compares $u$ to a pivot with an index chosen uniformly at random in the range $\\begin{array}{r}{\\{i+\\lceil\\frac{j-i}{4}\\rceil,\\dotsc,j-\\lceil\\frac{j-i}{4}\\rceil\\}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Theorem 2.1. $A$ dirty randomized binary search followed by a clean exponential search finds the target\u2019s position using $O(\\log k)$ dirty comparisons and $O(\\log\\eta(u,L))$ clean comparisons in expectation. ", "page_idx": 4}, {"type": "text", "text": "Randomized insertion in a binary heap. In a binary heap $\\mathcal{Q}$ , any new element $u$ is always inserted along the path of size $O(\\log n)$ from the root to the leftmost empty position in the leaf level. If all the inaccurate dirty comparisons are chosen along this path, then the insertion would require $O(\\log\\eta(u,\\boldsymbol{\\mathcal{Q}}))$ clean comparisons by Theorem 2.1. This complexity can be reduced further by randomizing the choice of the root-leaf path where $u$ is inserted, as explained in Algorithm 1. ", "page_idx": 4}, {"type": "table", "img_path": "1ATLLgvURu/tmp/2cca0272b8a28aba86500391c7396fdd95aeaaba9f9ba751e2905c5ab37b6b59.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Theorem 2.2. The insertion algorithm 1 requires ${\\cal O}(\\log n)$ time, $O(\\log\\log n)$ dirty comparisons and $O(\\log\\log\\eta(u,\\mathcal{Q}))$ clean comparisons in expectation. ", "page_idx": 4}, {"type": "text", "text": "2.2 Limitations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The previous theorem demonstrates that accurate predictions can reduce the number of clean comparisons for insertion in a binary heap. However, for the ExtractMin operation, when the minimum key, which is the root of the tree, is deleted, its two children are compared and the smallest is placed in the root position, and this process repeats recursively, with each new empty position filled by comparing both of its children, requiring necessarily $O(\\log n)$ clean comparisons in total to ensure the heap priority remains intact. Improving the efficiency of ExtractMin using dirty comparisons would therefore require bringing major modifications to the binary heap\u2019s structure. ", "page_idx": 4}, {"type": "text", "text": "Similar difficulties arise when attempting to enhance ExtractMin using dirty comparisons or the other prediction models in different heap implementations, such as Binomial or Fibonacci heaps. Consequently, we explore in the next section another priority queue implementation, using skip lists, which allows for an easier and more efficient exploitation of the predictions. ", "page_idx": 4}, {"type": "text", "text": "3 Skip lists ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A priority queue can be implemented naively by maintaining a dynamic sorted linked list of keys. This guarantees constant time for ExtractMin, but $O(n)$ time for insertion. Skip lists (see Figure 1) offer a solution to this inefficiency, by maintaining multiple levels of linked lists, with higher levels containing fewer elements and acting as shortcuts to lower levels, facilitating faster search and insertion in expected $O(\\log n)$ time. In all subsequent discussions concerning linked lists or skip lists, it is assumed that they are doubly linked, having both predecessor and successor pointers between elements. ", "page_idx": 4}, {"type": "text", "text": "The first level in a skip list is an ordinary linked list containing all the elements, which we denote by $v_{1},\\ldots,v_{n}$ . Every higher level is constructed by including each element from the previous level independently with probability $p$ , typically set to $1/2$ . For any key $v_{i}$ in the skip list, we define its height $h(v_{i})$ as the number of levels where it appears, which is an independent geometric random variable with parameter $p$ . A number $2h(v_{i})$ of pointers are associated with $v_{i}$ , giving access to the previous and next element in each level $\\ell\\,\\in\\,[h(v_{i})]$ , denoted respectively by $\\mathrm{Prev}(v_{i},\\ell)$ and $\\mathrm{Next}(v_{i},\\ell)$ . Using a HashMap, these pointers can be accessed in $O(1)$ time via the key value $v_{i}$ . For convenience, we consider that the skip list contains two additional keys $v_{0}=-\\infty$ and $v_{n+1}=\\infty$ , corresponding respectively to the head and the NIL value. Both have a height equal to the maximum height in the queue $h(v_{0})=h(v_{n+1})=\\operatorname*{max}_{i\\in[n]}h(v_{i})$ . ", "page_idx": 4}, {"type": "image", "img_path": "1ATLLgvURu/tmp/a06a80d5f38bdbb1a3f88afd2c6671de78500f023cf050a390d120152691d383.jpg", "img_caption": ["Figure 1: A skip list with keys $v_{1}<...<v_{9}\\in\\mathcal{U}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Since the expected height of keys in the skip list is $1/p$ , deleting any key only requires a constant time in expectation, by updating its associated pointers, along with those of its predecessors and successors in the levels where it appears. In particular, FindMin and ExtractMin take $O(1)$ time, and DecreaseKey can be performed by deleting the element and reinserting it with the new key, yielding the same complexity as insertion. Furthermore, by the same arguments, inserting a new key $u$ next to a given key $v_{i}$ in the skip list can be done in expected constant time. ", "page_idx": 5}, {"type": "text", "text": "Therefore, implementing efficient Insert and DecreaseKey operations for skip lists with predictions is reduced to designing efficient search algorithms to find the predecessor of a target key $u$ in the skip list, i.e., the largest key $v_{i}$ in the skip list that is smaller than $u$ . In all the following, we denote by $\\mathcal{Q}$ a skip list containing $n$ keys $v_{1}\\leq\\ldots\\leq v_{n}\\in\\mathcal{U}$ , and $u\\in\\mathcal{U}$ the target key. As explained in Appendix C.1, we can assume without loss of generality that the keys $(u,v_{1},\\ldots,v_{n})$ are pairwise distinct. In the following, we present separately for each model an insertion algorithm leveraging the predictions. ", "page_idx": 5}, {"type": "text", "text": "3.1 Pointer prediction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given a pointer prediction $v_{j}=\\widehat{\\mathrm{Pred}}(u,\\mathcal{Q})$ , we describe below an algorithm for finding the true predecessor of $u$ starting from the position of the key $v_{j}$ , then inserting $u$ . We assume in the algorithm that $v_{j}\\leq u$ . If $v_{j}>u$ , then the algorithm can be easily adapted by reversing the search direction. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 2: ExpSearchInsertion $(\\mathcal{Q},v_{j},u)$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "\u25b7Bottom-Up search \u25b7Top-Down search ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 is inspired by the classical exponential search in arrays. The first phase consists of a bottom-up search, expanding the size of the search interval by moving to upper levels until finding a key $w\\in\\mathcal{Q}$ satisfying $w\\ \\bar{\\leq}\\ u<\\mathrm{Next}(w,h(w))$ . The second phase conducts a top-down search from level $h(w)$ downward, refining the search until locating the position of $u$ . It is worth noting that the classical search algorithm in skip lists, denoted by Search $(\\mathcal{Q},u)$ , corresponds precisely to the top-down search, starting from the head of the skip list instead of $w$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1. Augmented with pointer predictions, a skip list allows FindMin and ExtractMin in expected $O(1)$ time, and $\\operatorname{Insert}(u)$ in expected $O(\\log\\vec{\\eta}(u,\\mathcal{Q}))$ time using Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "3.2 Dirty comparisons ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We devise in this section a search algorithm using dirty and clean comparisons. Algorithm 3 first estimates the position of $u$ with a dirty top-down search starting from the head, then performs a clean exponential search starting from the estimated position to find the true position. ", "page_idx": 6}, {"type": "table", "img_path": "1ATLLgvURu/tmp/8a825cd352711926636c2a5f6841a1d737a853ca42c76bd606242c1446dc78ec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "The dirty search concludes within ${\\cal O}(\\log n)$ steps, and Theorem 3.1 guarantees that the exponential search terminates within $O(\\log|r(\\hat{w},\\mathcal{Q})-r(u,\\mathcal{Q})|)$ steps. Combining these results and relating the distance between $u$ and $\\hat{w}$ in $\\mathcal{Q}$ to the prediction error $\\eta(u,\\mathcal{Q})$ , we derive the following theorem. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.2. Augmented with dirty comparisons, a skip list allows FindMin and ExtractMin in $O(1)$ expected time, and Insert $(u)$ with Algorithm $3$ in $O(\\log n)$ expected time, using $O(\\log n)$ dirty comparisons and $O(\\log\\eta(u,\\boldsymbol{Q}))$ clean comparisons in expectation. ", "page_idx": 6}, {"type": "text", "text": "3.3 Rank predictions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the rank prediction model, each $\\operatorname{Insert}(u)$ request is accompanied by a prediction $\\widehat{R}(u)$ of the rank of $u$ among all the distinct keys already in, or to be inserted into the priority queue. If the predictions are accurate and the total number $N$ of distinct keys to be inserted is known, the problem reduces to designing a priority queue with integer keys in $[N]$ , taking as keys the ranks $(R_{i})_{i\\in[N]}$ . This problem can be addressed using a van Emde Boas (vEB) tree over $[N]$ [van Emde Boas et al., 1976], which requires $O(\\log\\log N)$ time for insertion, deletion, finding the minimum or maximum, and finding the predecessor or successor of any element, guaranteeing in particular $O(\\log\\log N)$ time for all priority queue operations. More details on its structure can be found in Appendix C.5. ", "page_idx": 6}, {"type": "text", "text": "To leverage rank predictions, we use an auxiliary vEB tree $\\tau$ along with the skip list $\\mathcal{Q}$ . All the insertion and deletion operations are made simultaneously on $\\tau$ and $\\mathcal{Q}$ . However, the priorities used in $\\tau$ are the predicted ranks $\\{\\widehat{R}(u_{i})\\}_{i\\in[N]}$ . Whenever a new key $u_{i}$ is to be added, Algorithm 4 inserts it first in $\\tau$ at position $\\widehat{R}(u_{i})$ , gets its predecessor $\\hat{w}$ in $\\tau$ , i.e., the element in $\\tau$ with the largest predicted rank smaller than or equal to $\\widehat{R}(u_{i})$ , then uses $\\hat{w}$ as a pointer prediction to find the position of $u_{i}$ in $\\mathcal{Q}$ . If the predecessor is not u nique, the algorithm chooses an arbitrary one. ", "page_idx": 6}, {"type": "table", "img_path": "1ATLLgvURu/tmp/10c62bcb50f84744ddeee8552e6c364d9a596d9559fdbb0f556df4dfe5663d69.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "We explain in Appendix C.5 how the data structure can be adapted when $N$ is unknown and the rank predictions are not necessarily in $[N]$ , and we prove the following theorem, giving both the runtime and comparison complexities of the priority queue operations using this data structure. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3. If $\\widehat{R}(u_{i})=O(N)$ for all $i\\in[N]$ , then there is a data structure allowing FindMin and ExtractMin in $O(1)$ amortized time, and Insert in $O(\\log\\log N+\\log\\operatorname*{max}_{i\\in[N]}\\eta^{\\Delta}(u_{i}))$ amortized time using $O(\\log\\operatorname*{max}_{i\\in[N]}\\eta^{\\Delta}(u_{i}))$ comparisons in expectation. ", "page_idx": 6}, {"type": "text", "text": "In contrast to other prediction models, the complexity of inserting $u_{i}$ is not impacted only by $\\eta^{\\Delta}(u_{i})$ , but by the maximum error over all keys $\\{u_{j}\\}_{j\\in[N]}$ . This occurs because the exponential search conducted in Algorithm 4 starts from the key $\\hat{w}\\in\\hat{\\mathcal{Z}}$ , whose error also affects insertion performance. A similar behavior is observed in the online list labeling problem [McCauley et al., 2024], where the bounds provided by the authors also depend on the maximum prediction error for insertion. ", "page_idx": 6}, {"type": "text", "text": "With perfect predictions, the number of comparisons for insertion becomes constant, and its runtime $O(\\log\\log N)$ . It is not clear if the runtime of all the priority queue operations can be reduced to $O(1)$ with perfect predictions. Indeed, the problem in that case is reduced to a priority queue with all the keys in $[N]$ . The best-known solution to this problem is a randomized priority queue, by Thorup [2007], supporting all operations in $O({\\sqrt{\\log\\log N}})$ time. However, in our approach, we use vEB trees beyond the classical priority queue operations, as we also require fast access to the predecessor of any element. A data structure supporting all these operations solves the dynamic predecessor problem, for which vEB trees are optimal [P\u02d8atra\u00b8scu and Thorup, 2006]. Reducing the runtime of insertion below $O(\\log\\log N)$ would therefore require omitting the use of predecessor queries. ", "page_idx": 7}, {"type": "text", "text": "3.4 Lower bounds ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As explained earlier, ExtractMin requires only $O(1)$ expected time in skip lists. Furthermore, we presented insertion algorithms for the three prediction models and provided upper bounds on their complexities. The following theorem establishes lower bounds on the complexities of ExtractMin and Insert for any priority queue augmented with any of the three prediction types. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.4. For each of the three prediction models, the following lower bounds hold. ", "page_idx": 7}, {"type": "text", "text": "(i) Dirty comparisons: no data structure $\\mathcal{Q}$ can support ExtractMin with $O(1)$ clean comparisons and Insert $(u)$ with $o(\\log\\eta(u,\\boldsymbol{Q}))$ ) clean comparisons in expectation.   \n(ii) Pointer predictions: no data structure $\\mathcal{Q}$ can support ExtractMin with $O(1)$ comparisons and Insert $(u)$ with $o(\\log\\vec{\\eta}(u,\\mathcal{Q}))$ comparisons in expectation.   \n(iii) Rank predictions: no data structure $\\mathcal{Q}$ can support ExtractMin with $O(1)$ comparisons and $\\mathrm{Insert}(u_{i})$ with $o(\\log\\operatorname*{max}_{i\\in[N]}\\eta^{\\Delta}(u_{i}))$ comparisons in expectation, for all $i\\in[N]$ . ", "page_idx": 7}, {"type": "text", "text": "These lower bounds with Theorems 3.2 and 3.1 prove the tightness of our priority queue in the dirty comparison and the pointer prediction models. In the rank prediction model, the comparison complexities proved in Theorem 3.3 are optimal, whereas the runtimes are only optimal up to an additional $O(\\log\\log N)$ term. In particular, they are optimal if the maximal error is at least $\\Omega(\\log N)$ . ", "page_idx": 7}, {"type": "text", "text": "4 Applications ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Sorting algorithm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our learning-augmented priority queue can be used for sorting a sequence $A=(a_{1},\\ldots,a_{n})$ , by first inserting all the elements, then repeatedly extracting the minimum until the priority queue is empty. We compare below the performance of this sorting algorithm to those of [Bai and Coester, 2023]. ", "page_idx": 7}, {"type": "text", "text": "Dirty comparison model. Denoting by $\\eta_{i}\\,=\\,\\eta(a_{i},A)$ , Bai and Coester [2023] prove a sorting algorithm using $O(n\\log n)$ time, $O(n\\log n)$ dirty comparisons, and $O(\\sum_{i}\\log(\\eta_{i}+2))$ clean comparisons. Theorem 3.2 yields the same guarantees with our learning-augmented priority queue. Moreover, our learning-augmented priority queue is a skip list, maintaining elements in sorted order at any time, even if the elements are revealed online and the insertion order is adversarial, while in [Bai and Coester, 2023], it is crucial that the insertion order is chosen uniformly at random. ", "page_idx": 7}, {"type": "text", "text": "Positional predictions. In their second prediction model, they assume that the algorithm is given offline access to predictions $\\{\\widehat{R}(a_{i})\\}_{i\\in[n]}$ of the relative ranks $\\{R(a_{i})\\}_{i\\in[n]}$ of the $n$ elements to sort, and they study two different error measures. The rank prediction error $\\eta_{i}^{\\Delta}=|R(a_{i})-\\widehat{R}(a_{i})|$ matches their definition of displacement error, for which they prove a sorting algorithm in $O(\\sum_{i}\\log(\\eta_{i}^{\\Delta}+2))$ time. The same bound can be deduced using our results in the pointer prediction model. Further discussion on this claim can be found in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "Online rank predictions. If $n$ is unknown to the algorithm, and the elements $a_{1},\\ldots,a_{n}$ along with their predicted ranks are revealed online, possibly in an adversarial order, then by Theorem 3.3, the total runtime of our priority queue for maintaining all the inserted elements sorted at any time is $O(n\\log\\log n+n\\log\\operatorname*{max}_{i}\\bar{\\eta_{i}^{\\Delta}})$ , and the number of comparisons used is $O(n\\log\\operatorname*{max}_{i}\\eta_{i}^{\\bar{\\Delta}})$ . No analogous result is demonstrated in [Bai and Coester, 2023] in this setting. ", "page_idx": 7}, {"type": "text", "text": "4.2 Dijkstra\u2019s shortest path algorithm ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Consider a run of Dijkstra\u2019s algorithm on a directed positively weighted graph $G$ with $n$ nodes and $m$ edges. The elements inserted into the priority queue are the nodes of the graph, and the corresponding keys are their tentative distances to the source, which are updated over time. During the algorithm\u2019s execution, at most $m+1$ distinct keys $\\{d_{i}\\}_{i\\in[m+1]}$ are inserted into the priority queue. Given online predictions $(\\widehat{R}(d_{i}))_{i\\in[m+1]}$ of their relative ranks $(R(d_{i}))_{i\\in[m+1]}$ , the total runtime using our priority queue augmented with rank predictions is ", "page_idx": 8}, {"type": "equation", "text": "$$\nO\\big(m\\log\\log n+m\\log\\operatorname*{max}_{i\\in[m+1]}|R(d_{i})-\\widehat{R}(d_{i})|\\big)\\ .\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In contrast, the shortest path algorithm of Lattanzi et al. [2023] (which also works for negative edges) has a linear dependence on a similar error measure. Even with arbitrary error, our guarantee is never worse than the $O(m\\log n)$ runtime with binary heaps. Using Fibonacci heaps results in an $O(n\\log n+m)$ s hpeerref $\\begin{array}{r}{m=o\\big(\\frac{n\\log n}{\\log\\log n}\\big)}\\end{array}$ r iafc tpircee,d iecvtieonn sc oarmep oafr ehdi gtoh  qbiunaalirtyy .h eHaopws,e avse rs, uitp ipso rktneod wbny  tohuart experiments. ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we empirically evaluate the performance of our learning-augmented priority queue (LAPQ) by comparing it with Binary and Fibonacci heaps. We use two standard benchmarks for this evaluation: sorting and Dijkstra\u2019s algorithm. For the sorting benchmark, we also compare our results with those from Bai and Coester [2023]. For Dijkstra\u2019s algorithm, we assess performance on both real city maps and synthetic random graphs. In all the experiments, each data point represents the average result from 30 independent runs. Additional experiments and a detailed discussion on the prediction models and the obtained results can be found in Appendix F. The code used for conducting the experiments is available at github.com/Ziyad-Benomar/Learning-augmented-priority-queues. ", "page_idx": 8}, {"type": "text", "text": "Sorting. We compare sorting using our LAPQ with the algorithms of Bai and Coester [2023] under their same experimental settings. Given a sequence $A=(a_{1},\\ldots,a_{n})$ , we evaluate the complexity of sorting it with predictions in the class and the decay setting. In the first, $A$ is divided into $c$ classes $((t_{k-1},t_{k}])_{k\\in[c]}$ , where $0=t_{0}\\leq t_{1}\\leq.\\,.\\,.\\leq t_{c}=n$ are uniformly random thresholds. The predicted rank of any item $a_{i}$ with $t_{k}\\leq i<t_{k+1}$ is sampled uniformly at random within $(t_{k},t_{k+1}]$ . In the decay setting, the ranking is initially accurate but degrades over time. Each time step, one item\u2019s predicted position is perturbed by 1, either left or right, uniformly at random. ", "page_idx": 8}, {"type": "text", "text": "In both settings, we test the LAPQ with the three prediction models. First, assuming that the rank predictions are given offline, we use pointer predictions as explained in Appendix E. In the second case, the elements to insert along with their predicted ranks are revealed online in a uniformly random order. Finally, we test the dirty comparison setting with the dirty order $(a_{i}\\widehat{<}a_{j})=(\\widehat{R}(a_{i})\\dot{<}\\widehat{R}(a_{j}))$ . ", "page_idx": 8}, {"type": "image", "img_path": "1ATLLgvURu/tmp/251f775bb39e56f7d6b5161abab738753df072760c0531b5c4bc498487a96064.jpg", "img_caption": ["Figure 2: Sorting in the class setting "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "1ATLLgvURu/tmp/50529f74270bf43a8cd81dd9364ae1a7346ef443c7b5701fe93527aeb574ced4.jpg", "img_caption": ["Figure 3: Sorting in the decay setting "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figures 2 and 3 show the obtained results respectively in the class and the decay setting for $n\\bar{\\in}\\{10^{4},10^{5}\\}$ . In the class setting with offline predictions, the LAPQ slightly outperforms the Double-Hoover and Displacement sort algorithms of Bai and Coester [2023], which were shown to outperform classical sorting algorithms. In the decay setting, the LAPQ matches the performance of the Displacement sort, but is slightly outperformed by the Double-Hoover sort. With online predictions, although the problem is harder, LAPQ\u2019s performance remains comparable to the previous algorithms. In both settings, the LAPQ with offline predictions, online predictions, and dirty comparisons all yield better performance than binary or Fibonacci heaps, even with predictions that are not highly accurate. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Dijkstra\u2019s algorithm. Consider a graph $G=(V,E)$ with $n$ nodes and $m$ edges, and a source node $s\\in V$ . In the first predictions setting, we pick a random node $\\hat{s}$ and run Dijkstra\u2019s algorithm with $\\hat{s}$ as the source, memorizing all the keys $\\hat{D}=(\\hat{d}_{1},\\dotsc,\\hat{d}_{m})$ inserted into the priority queue. In subsequent runs of the algorithm with different sources, when a key $d_{i}$ is to be inserted, we augment the insertion with the rank prediction $\\widehat{R}(d_{i})=r(d_{i},\\hat{D})$ . We call these key rank predictions. This model aims at exploiting the topology a nd uniformity of city maps. As computing shortest paths from any source necessitates traversing all graph edges, keys inserted into the priority queue\u2014partial sums of edge lengths\u2014are likely to exhibit some degree of similarity even if the algorithm is executed from different sources. Notably, this prediction model offers an explicit method for computing predictions, readily applicable in real-world scenarios. ", "page_idx": 9}, {"type": "text", "text": "In the second setting, we consider rank predictions of the nodes in $G$ , ordered by their distances to $s$ . As Dijkstra\u2019s algorithm explores a new node $x\\in V$ , it receives a prediction ${\\widehat{r}}(x)$ of its rank. The node $x$ is then inserted with a key $d_{i}$ , to which we assign the prediction $\\widehat{R}(d_{i})=\\widehat{r}(x)$ . Unlike the previous experimental settings, we initially have predictions of the nodes\u2019 ranks, which we extend to predictions of the keys\u2019 ranks. Similarly to the sorting experiments, we consider class and decay perturbations of the node ranks. ", "page_idx": 9}, {"type": "text", "text": "In the context of searching the shortest path, rank predictions in the class setting can be derived from subdividing the city into multiple smaller areas. Each class corresponds to a specific area, facilitating the ordering of areas from closest to furthest relative to the source. However, comparing the distances from the source to the nodes in the same class might be inaccurate. On the other hand, the decay setting simulates modifications to shortest paths, such as rural works or new route constructions, by adding or removing edges from the graph. These alterations may affect the ranks of a limited number of nodes, which corresponds to the time steps in the decay setting. ", "page_idx": 9}, {"type": "text", "text": "We present below the experiment results obtained with the maps of Paris and London. More experiments with additional city maps and synthetic graphs are in Appendix F. The city maps were obtained using the Python library Osmnx [Boeing, 2017]. Figures 4 and 5 respectively illustrate the results in the class and the decay settings with node rank predictions. In both figures, for each city, we present the numbers of comparisons used for the same task by a binary and Fibonacci heap, and the number of comparisons used when the priority queue is augmented with key rank predictions. ", "page_idx": 9}, {"type": "image", "img_path": "1ATLLgvURu/tmp/3f07a287fa4070bbc28d922796c60fe778caa6e89741086c07c14511f67b28ed.jpg", "img_caption": ["Figure 4: Shortest path, class predictions "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "1ATLLgvURu/tmp/4611ad8729ae969d0068e4836701ce32400431e15bf66098385a2b5e2eac6dd4.jpg", "img_caption": ["Figure 5: Shortest path, decay predictions "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "In both settings, the performance of the LAPQ substantially improves with the quality of the predictions, and notably, key rank predictions yield almost the same performance as perfect node rank predictions, affirming our intuition on the similarity between the keys inserted in runs of Dijkstra\u2019s algorithm starting from different sources. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Keerti Anand, Rong Ge, and Debmalya Panigrahi. Customizing ml predictions for online algorithms. In International Conference on Machine Learning, pages 303\u2013313. PMLR, 2020. ", "page_idx": 9}, {"type": "text", "text": "Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online matching problems with machine learned advice. Advances in Neural Information Processing Systems, 33: 7933\u20137944, 2020.   \nAntonios Antoniadis, Christian Coester, Marek Eli\u00e1s, Adam Polak, and Bertrand Simon. Learningaugmented dynamic power management with multiple states via new ski rental bounds. Advances in Neural Information Processing Systems, 34:16714\u201316726, 2021.   \nAntonios Antoniadis, Joan Boyar, Marek Eli\u00e1s, Lene Monrad Favrholdt, Ruben Hoeksma, Kim S Larsen, Adam Polak, and Bertrand Simon. Paging with succinct predictions. In International Conference on Machine Learning, pages 952\u2013968. PMLR, 2023a.   \nAntonios Antoniadis, Christian Coester, Marek Eli\u00e1\u0161, Adam Polak, and Bertrand Simon. Online metric algorithms with untrusted predictions. ACM Transactions on Algorithms, 19(2):1\u201334, 2023b.   \nXingjian Bai and Christian Coester. Sorting with predictions. Advances in Neural Information Processing Systems, 36, 2023.   \nEtienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning augmented algorithms. Advances in Neural Information Processing Systems, 33:20083\u201320094, 2020.   \nNikhil Bansal, Christian Coester, Ravi Kumar, Manish Purohit, and Erik Vee. Learning-augmented weighted paging. In Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA, pages 67\u201389. SIAM, 2022.   \nDmitry Basin, Edward Bortnikov, Anastasia Braginsky, Guy Golan-Gueta, Eshcar Hillel, Idit Keidar, and Moshe Sulamy. Kiwi: A key-value map for scalable real-time analytics. In Proceedings of the 22Nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 357\u2013369, 2017.   \nMohammadHossein Bateni, Prathamesh Dharangutte, Rajesh Jayaram, and Chen Wang. Metric clustering and mst with strong and weak distance oracles. arXiv preprint arXiv:2310.15863, 2023.   \nZiyad Benomar and Vianney Perchet. Advice querying under budget constraint for online algorithms. Advances in Neural Information Processing Systems, 36, 2024a.   \nZiyad Benomar and Vianney Perchet. Non-clairvoyant scheduling with partial predictions. In Forty-first International Conference on Machine Learning, 2024b.   \nZiyad Benomar, Chaima Ghribi, Elie Cali, Alexander Hinsen, and Benedikt Jahnel. Agent-based modeling and simulation for malware spreading in d2d networks. AAMAS $^{\\circ2}$ , page 91\u201399, Richland, SC, 2022a. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450392136.   \nZiyad Benomar, Chaima Ghribi, Elie Cali, Alexander Hinsen, Benedikt Jahnel, and Jean-Philippe Wary. Multi-agent simulations for virus propagation in D2D $^{5G+}$ networks. 2022b.   \nZiyad Benomar, Evgenii Chzhen, Nicolas Schreuder, and Vianney Perchet. Addressing bias in online selection with limited budget of comparisons. arXiv preprint arXiv:2303.09205, 2023.   \nAditya Bhaskara, Ashok Cutkosky, Ravi Kumar, and Manish Purohit. Logarithmic regret from sublinear hints. Advances in Neural Information Processing Systems, 34:28222\u201328232, 2021.   \nGeoff Boeing. Osmnx: New methods for acquiring, constructing, analyzing, and visualizing complex street networks. Computers, environment and urban systems, 65:126\u2013139, 2017.   \nRyan S Borgstrom and S Rao Kosaraju. Comparison-based search in the presence of errors. In Proceedings of the twenty-fifth annual ACM symposium on Theory of computing, pages 130\u2013136, 1993.   \nGerth St\u00f8lting Brodal. Worst-case efficient priority queues. In Proceedings of the Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201996, page 52\u201358, USA, 1996. Society for Industrial and Applied Mathematics. ISBN 0898713668.   \nGerth St\u00f8lting Brodal. A survey on priority queues. In Space-Efficient Data Structures, Streams, and Algorithms: Papers in Honor of J. Ian Munro on the Occasion of His 66th Birthday, pages 150\u2013163. Springer, 2013.   \nGerth St\u00f8lting Brodal and Chris Okasaki. Optimal purely functional priority queues. Journal of Functional Programming, 6(6):839\u2013857, 1996.   \nGerth St\u00f8lting Brodal, George Lagogiannis, and Robert E Tarjan. Strict fibonacci heaps. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing, pages 1177\u20131184, 2012.   \nBernard Chazelle. A minimum spanning tree algorithm with inverse-ackermann type complexity. Journal of the ACM (JACM), 47(6):1028\u20131047, 2000.   \nJustin Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental graph algorithms via learned predictions. In International Conference on Machine Learning, pages 3583\u20133602. PMLR, 2022.   \nMo Chen, Rezaul Alam Chowdhury, Vijaya Ramachandran, David Lan Roche, and Lingling Tong. Priority queues and dijkstra\u2019s algorithm. 2007.   \nJakub Chlkedowski, Adam Polak, Bartosz Szabucki, and Konrad Tomasz .Zolna. Robust learningaugmented caching: An experimental study. In International Conference on Machine Learning, pages 1920\u20131930. PMLR, 2021.   \nNicolas Christianson, Junxuan Shen, and Adam Wierman. Optimal robustness-consistency tradeoffs for learning-augmented metrical task systems. In International Conference on Artificial Intelligence and Statistics, pages 9377\u20139399. PMLR, 2023.   \nClark Allan Crane. Linear lists and priority queues as balanced binary trees. Stanford University, 1972.   \nHerbert Aron David. The method of paired comparisons, volume 12. London, 1963.   \nWilliam HE Day and Herbert Edelsbrunner. Efficient algorithms for agglomerative hierarchical clustering methods. Journal of classification, 1(1):7\u201324, 1984.   \nIlias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Ali Vakilian, and Nikos Zarifis. Learning online algorithms with distributional advice. In International Conference on Machine Learning, pages 2687\u20132696. PMLR, 2021.   \nMichael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster matchings via learned duals. Advances in neural information processing systems, 34:10393\u201310406, 2021.   \nFranziska Eberle, Felix Hommelsheim, Alexander Lindermayr, Zhenwei Liu, Nicole Megow, and Jens Schl\u00f6ter. Accelerating matroid optimization through fast imprecise oracles. arXiv preprint arXiv:2402.02774, 2024.   \nStefan Edelkamp and Ingo Wegener. On the performance of weak-heapsort. In Annual Symposium on Theoretical Aspects of Computer Science, pages 254\u2013266. Springer, 2000.   \nBennett Eisenberg. On the expectation of the maximum of iid geometric random variables. Statistics & Probability Letters, 78(2):135\u2013143, 2008.   \nMichael L Fredman and Robert Endre Tarjan. Fibonacci heaps and their uses in improved network optimization algorithms. Journal of the ACM (JACM), 34(3):596\u2013615, 1987.   \nChunkai Fu, Jung Hoon Seo, and Samson Zhou. Learning-augmented skip lists. arXiv preprint arXiv:2402.10457, 2024.   \nAnna Gambin and Adam Malinowski. Randomized meldable priority queues. In International Conference on Current Trends in Theory and Practice of Computer Science, pages 344\u2013349. Springer, 1998.   \nTingjian Ge and Stan Zdonik. A skip-list approach for efficiently processing forecasting queries. Proceedings of the VLDB Endowment, 1(1):984\u2013995, 2008.   \nDebarghya Ghoshdastidar, Micha\u00ebl Perrot, and Ulrike von Luxburg. Foundations of comparison-based hierarchical clustering. Advances in neural information processing systems, 32, 2019.   \nCatherine Gloaguen and Elie Cali. Cost estimation of a fixed network deployment over an urban territory. Annals of Telecommunications, 73:367\u2013380, 2018.   \nCatherine Gloaguen, Frank Fleischer, Hendrik Schmidt, and Volker Schmidt. Fitting of stochastic telecommunication network models via distance measures and monte\u2013carlo tests. Telecommunication Systems, 31:353\u2013377, 2006.   \nRick Siow Mong Goh and Ian Li-Jin Thng. Dsplay: An efficient dynamic priority queue structure for discrete event simulation. In Proceedings of the SimTecT Simulation Technology and Training Conference. Citeseer, 2004.   \nSreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice. In International Conference on Machine Learning, pages 2319\u20132327. PMLR, 2019.   \nGaston H Gonnet and J Ian Munro. Heaps on heaps. SIAM Journal on Computing, 15(4):964\u2013971, 1986.   \nSiavash Haghiri, Debarghya Ghoshdastidar, and Ulrike von Luxburg. Comparison-based nearest neighbor search. In Artificial Intelligence and Statistics, pages 851\u2013859. PMLR, 2017.   \nSiavash Haghiri, Damien Garreau, and Ulrike Luxburg. Comparison-based random forests. In International Conference on Machine Learning, pages 1871\u20131880. PMLR, 2018.   \nReinhard Heckel, Max Simchowitz, Kannan Ramchandran, and Martin Wainwright. Approximate ranking from pairwise comparisons. In International Conference on Artificial Intelligence and Statistics, pages 1057\u20131066. PMLR, 2018.   \nYih-Chun Hu, Adrian Perrig, and David B Johnson. Efficient security mechanisms for routing protocolsa. In Ndss. Citeseer, 2003.   \nSungjin Im, Ravi Kumar, Aditya Petety, and Manish Purohit. Parsimonious learning-augmented caching. In International Conference on Machine Learning, pages 9588\u20139601. PMLR, 2022.   \nNarendra Kumar Jaiswal. Priority queues, volume 50. Academic press New York, 1968.   \nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index structures. In Proceedings of the 2018 international conference on management of data, pages 489\u2013504, 2018.   \nDaniel H Larkin, Siddhartha Sen, and Robert E Tarjan. A back-to-basics empirical study of priority queues. In 2014 Proceedings of the Sixteenth Workshop on Algorithm Engineering and Experiments (ALENEX), pages 61\u201372. SIAM, 2014.   \nAlexandra Anna Lassota, Alexander Lindermayr, Nicole Megow, and Jens Schl\u00f6ter. Minimalistic predictions to schedule jobs with online precedence constraints. In International Conference on Machine Learning, pages 18563\u201318583. PMLR, 2023.   \nSilvio Lattanzi, Ola Svensson, and Sergei Vassilvitskii. Speeding up bellman ford via minimum violation permutations. In International Conference on Machine Learning, ICML 2023, 2023.   \nRhyd Lewis. A comparison of dijkstra\u2019s algorithm using fibonacci heaps, binary heaps, and selfbalancing binary trees. arXiv preprint arXiv:2303.10034, 2023.   \nHonghao Lin, Tian Luo, and David Woodruff. Learning augmented binary search trees. In International Conference on Machine Learning, pages 13431\u201313440. PMLR, 2022.   \nJonatan Lind\u00e9n and Bengt Jonsson. A skiplist-based concurrent priority queue with minimal memory contention. In Principles of Distributed Systems: 17th International Conference, OPODIS 2013, Nice, France, December 16-18, 2013. Proceedings 17, pages 206\u2013220. Springer, 2013.   \nAlexander Lindermayr and Nicole Megow. Algorithms with predictions. https:// algorithms-with-predictions.github.io, 2022.   \nThodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned advice. In International Conference on Machine Learning, pages 3296\u20133305. PMLR, 2018.   \nJessica Maghakian, Russell Lee, Mohammad Hajiesmaili, Jian Li, Ramesh Sitaraman, and Zhenhua Liu. Applied online algorithms with heterogeneous predictors. In International Conference on Machine Learning, pages 23484\u201323497. PMLR, 2023.   \nSamuel McCauley, Ben Moseley, Aidin Niaparast, and Shikha Singh. Online list labeling with predictions. Advances in Neural Information Processing Systems, 36, 2024.   \nMichela Meister and Sloan Nietert. Learning with comparison feedback: Online estimation of sample statistics. In Algorithmic Learning Theory, pages 983\u20131001. PMLR, 2021.   \nNadav Merlis, Hugo Richard, Flore Sentenac, Corentin Odic, Mathieu Molina, and Vianney Perchet. On preemption and learning in stochastic scheduling. In International Conference on Machine Learning, pages 24478\u201324516. PMLR, 2023.   \nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. Communications of the ACM, 65(7):33\u201335, 2022.   \nSung-Whan Moon, Jennifer Rexford, and Kang G Shin. Scalable hardware priority queue architectures for high-speed packet switches. IEEE Transactions on computers, 49(11):1215\u20131227, 2000.   \nRobert Nowak. Noisy generalized binary search. Advances in neural information processing systems, 22, 2009.   \nClark F Olson. Parallel algorithms for hierarchical clustering. Parallel computing, 21(8):1313\u20131325, 1995.   \nMihai Pa\u02d8tra\u00b8scu and Mikkel Thorup. Time-space trade-offs for predecessor search. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pages 232\u2013240, 2006.   \nMicha\u00ebl Perrot, Pascal Esser, and Debarghya Ghoshdastidar. Near-optimal comparison based clustering. Advances in Neural Information Processing Systems, 33:19388\u201319399, 2020.   \nWilliam Pugh. Skip lists: a probabilistic alternative to balanced trees. Communications of the ACM, 33(6):668\u2013676, 1990.   \nManish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions. Advances in Neural Information Processing Systems, 31, 2018.   \nRobert R\u00f6nngren and Rassul Ayani. A comparative study of parallel and sequential priority queue algorithms. ACM Transactions on Modeling and Computer Simulation (TOMACS), 7(2):157\u2013209, 1997.   \nKarim Ahmed Abdel Sadek and Marek Elias. Algorithms for caching and MTS with reduced number of predictions. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ QuIiLSktO4.   \nNihar B Shah and Martin J Wainwright. Simple, robust and optimal ranking from pairwise comparisons. Journal of machine learning research, 18(199):1\u201338, 2018.   \nAvinash Sharma, Dankan Gowda, Anil Sharma, S Kumaraswamy, MR Arun, et al. Priority queueing model-based iot middleware for load balancing. In 2022 6th International Conference on Intelligent Computing and Control Systems (ICICCS), pages 425\u2013430. IEEE, 2022.   \nNir Shavit and Itay Lotan. Skiplist-based concurrent priority queues. In Proceedings 14th International Parallel and Distributed Processing Symposium. IPDPS 2000, pages 263\u2013268. IEEE, 2000.   \nYongho Shin, Changyeol Lee, Gukryeol Lee, and Hyung-Chan An. Improved learning-augmented algorithms for the multi-option ski rental problem via best-possible competitive analysis. arXiv preprint arXiv:2302.06832, 2023.   \nSandeep Silwal, Sara Ahmadian, Andrew Nystrom, Andrew McCallum, Deepak Ramachandran, and Seyed Mehran Kazemi. Kwikbucks: Correlation clustering with cheap-weak and expensive-strong signals. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=p0JSSa1AuV.   \nH\u00e5kan Sundell and Philippas Tsigas. Fast and lock-free concurrent priority queues for multi-thread systems. Journal of Parallel and Distributed Computing, 65(5):609\u2013627, 2005.   \nMikkel Thorup. Equivalence between priority queues and sorting. Journal of the ACM (JACM), 54 (6):28\u2013es, 2007.   \nDominique Tschopp, Suhas Diggavi, Payam Delgosha, and Soheil Mohajer. Randomized algorithms for comparison-based search. Advances in Neural Information Processing Systems, 24, 2011.   \nPeter van Emde Boas. Preserving order in a forest in less than logarithmic time and linear space. Information processing letters, 6(3):80\u201382, 1977.   \nPeter van Emde Boas, Robert Kaas, and Erik Zijlstra. Design and implementation of an efficient priority queue. Mathematical systems theory, 10(1):99\u2013127, 1976.   \nJean Vuillemin. A data structure for manipulating priority queues. Communications of the ACM, 21 (4):309\u2013315, 1978.   \nFabian Wauthier, Michael Jordan, and Nebojsa Jojic. Efficient ranking from pairwise comparisons. In International Conference on Machine Learning, pages 109\u2013117. PMLR, 2013.   \nJ. W. J. Williams. Algorithm 232: Heapsort. Communications of the ACM, 7(6):347\u2013348, 1964.   \nAli Zeynali, Shahin Kamali, and Mohammad Hajiesmaili. Robust learning-augmented dictionaries. arXiv preprint arXiv:2402.09687, 2024.   \nDeli Zhang and Damian Dechev. A lock-free priority queue design based on multi-dimensional linked lists. IEEE Transactions on Parallel and Distributed Systems, 27(3):613\u2013626, 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Learning-Augmented Priority Queues ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Appendix ", "page_idx": 15}, {"type": "text", "text": "A Extended related work", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Learning-augmented algorithms Learning-augmented algorithms, introduced in [Lykouris and Vassilvtiskii, 2018, Purohit et al., 2018], have captured increasing interest over the last years, as they allow breaking longstanding limitations in many algorithm design problems. Assuming that the decision-maker is provided with potentially incorrect predictions regarding unknown parameters of the problem, learning-augmented algorithms must be capable of leveraging these predictions if they are accurate (consistency), while keeping the worst-case performance without advice even if the predictions are arbitrarily bad or adversarial (robustness). Many fundamental online problems were studied in this setting, such as ski rental [Gollapudi and Panigrahi, 2019, Anand et al., 2020, Bamas et al., 2020, Diakonikolas et al., 2021, Antoniadis et al., 2021, Maghakian et al., 2023, Shin et al., 2023], scheduling [Purohit et al., 2018, Merlis et al., 2023, Lassota et al., 2023, Benomar and Perchet, 2024b], matching [Antoniadis et al., 2020, Dinitz et al., 2021, Chen et al., 2022], and caching [Lykouris and Vassilvtiskii, 2018, Chlkedowski et al., 2021, Bansal et al., 2022, Antoniadis et al., 2023b,a, Christianson et al., 2023]. Data structures can also be improved within this framework. However, this remains underexplored compared to online algorithms. The seminal paper by Kraska et al. [2018] shows how predictions can be used to optimize space usage. Another study by [Lin et al., 2022] demonstrates that the runtime of binary search trees can be enhanced by incorporating predictions of item access frequency. Recent papers have extended this prediction model to other data structures, such as dictionaries [Zeynali et al., 2024] and skip lists [Fu et al., 2024]. The prediction models we study deviate from the latter, and are more related to those considered respectively in [Bai and Coester, 2023] for sorting, and [McCauley et al., 2024] for online list labeling. An overview of the growing body of work on learning-augmented algorithms (also known as algorithms with predictions) is maintained at [Lindermayr and Megow, 2022]. ", "page_idx": 15}, {"type": "text", "text": "Dirty comparisons The dirty and clean comparison model is also related to the prediction querying model, gaining a growing interest in the study of learning-augmented algorithms, where the decisionmaker decides when to query predictions and for which items [Im et al., 2022, Benomar and Perchet, 2024a, Sadek and Elias, 2024]. In particular, having free-weak and costly-strong oracles has been studied in [Silwal et al., 2023] for correlation clustering, in [Bateni et al., 2023] for clustering and computing minimum spanning trees in a metric space, and in [Eberle et al., 2024] for matroid optimization. Another related setting involves the algorithm observing partial information online, which it can then use to decide whether to query a costly hint about the current item. This has been explored in contexts such as online linear optimization [Bhaskara et al., 2021] and the multicolor secretary problem [Benomar et al., 2023]. ", "page_idx": 15}, {"type": "text", "text": "Priority queues Binary heaps, introduced by Williams [1964], are one of the first efficient implementations of priority queues. They allow FindMin in constant time, and the other operations in $O(\\log n)$ time, where $n$ is the number of items in the queue. Shortly after, other heap-based implementations with similar guarantees were designed, such as leftist heaps [Crane, 1972] and randomized meldable priority queues [Gambin and Malinowski, 1998]. A new idea was introduced in [Vuillemin, 1978] with Binomial heaps, where instead of having a single tree storing the items, a binomial heap is a collection of $\\Theta(\\log n)$ trees with exponentially growing sizes, all satisfying the heap property. They allow insertion in constant amortized time, and $O(\\log n)$ time for ExtractMin and DecreaseKey. A breakthrough came with Fibonacci heaps [Fredman and Tarjan, 1987], which allow all the operations in constant amortized time, except for ExtractMin, which takes ${\\cal O}(\\log n)$ time. It was shown later, in works such as [Brodal, 1996, Brodal et al., 2012], that the same guarantees can be achieved in the worst-case, not only in amortized time. Although they offer very good theoretical guarantees, Fibonacci heaps are known to be slow in practice [Larkin et al., 2014, Lewis, ", "page_idx": 15}, {"type": "text", "text": "2023], and other implementations with weaker theoretical guarantees such as binary heaps are often preferred. We refer the interested reader to the detailed survey on priority queues by Brodal [2013]. ", "page_idx": 16}, {"type": "text", "text": "Skip lists A skip list [Pugh, 1990] is a probabilistic data structure, based on classical linked lists, having shortcut pointers allowing fast access between non-adjacent elements. Due to the simplicity of their implementation and their strong performance, skip lists have many applications [Hu et al., 2003, Ge and Zdonik, 2008, Basin et al., 2017]. In particular, they can be used to implement priority queues [R\u00f6nngren and Ayani, 1997], guaranteeing in expectation a constant time for FindMin and ExtractMin, and ${\\cal O}(\\log n)$ time for Insert and DecreaseKey. They show particularly good performance compared to other implementations in the case of concurrent priority queues, where multiple users or machines can make requests to the priority queue [Shavit and Lotan, 2000, Lind\u00e9n and Jonsson, 2013, Zhang and Dechev, 2015]. ", "page_idx": 16}, {"type": "text", "text": "B Heap priority queues ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Exponential search and randomized binary search ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The insertion algorithm we propose for binary heaps combines the classical exponential search and our randomized adaptation of the binary search algorithm. We provide a brief overview of the exponential search and its runtime, followed by an analysis of the runtime upper bound for our randomized binary search algorithm. ", "page_idx": 16}, {"type": "text", "text": "Exponential search. The exponential search of a target $u$ in a sorted list $L=\\left(v_{1},\\ldots,v_{k}\\right)$ starting from index $i~\\in~[k]$ consists in first comparing $u$ to $v_{i}$ , and if $v_{i}~<~u$ then $u$ is compared to $v_{i+1},v_{i+2},v_{i+4},.\\dotsc$ until an integer $\\ell$ is found satisfying $v_{i+2^{\\ell}}<u\\leq v_{i+2^{\\ell+1}}$ , a binary search is then conducted between indices $i+2^{\\ell}$ and $i+2^{\\ell+1}$ to find the position of $u$ . If the first comparison shows instead that $u<v_{i}$ , then $u$ is compared to $v_{i-1},v_{i-2},v_{i-4},....$ Denoting by $j$ the true position of $u$ in $L$ , the exponential search terminates in $O(\\log|i-j|)$ time. ", "page_idx": 16}, {"type": "text", "text": "Randomized binary search (RBS). Consider a sorted list $L=(v_{1},\\ldots,v_{k})\\in\\mathcal{U}^{k}$ and a target $u\\in\\mathcal{U}$ . When the search of $u$ in $L$ is reduced to the sub-list $(v_{i},\\ldots,v_{j})$ , RBS samples an index ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell\\sim\\mathcal{U}\\mathrm{niform}\\left(\\{i+\\lceil\\frac{j-i}{4}\\rceil,\\dots,j-\\lceil\\frac{j-i}{4}\\rceil\\}\\right)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "if $u=v_{\\ell}$ then algorithm terminates, if $u<v_{\\ell}$ then the search is reduced to the sub-list $(v_{i},\\ldots,v_{\\ell-1})$ , otherwise it is reduced to $(v_{\\ell+1},\\ldots,v_{j})$ . This process iterates until the obtained sub-list has a size of 1, the position of $u$ is then immediately deduced by comparing it to the single element in the sub-list. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.1. RBS in a list of size $k$ terminates in $O(\\log k)$ time with probability 1. ", "page_idx": 16}, {"type": "text", "text": "Proof. Let us denote by $S_{t}$ the size of the sub-list to which the search is reduced after $t$ steps, and $T=\\operatorname*{min}\\{t\\geq1:S_{t}=1\\}$ . For convenience, we consider that $S_{t}=1$ for all $t>T$ . It holds that $S_{0}=n$ , and for all $t\\geq1$ , if $S_{t}=j-i+1\\geq2$ then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{t+1}\\leq\\operatorname*{max}(j-\\ell,\\ell-i)\\leq j-i-\\lceil\\frac{j-i}{4}\\rceil\\leq\\frac{3}{4}(j-i)\\leq\\frac{3}{4}S_{t}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "On the other hand, if $S_{t}=1$ then $S_{t+1}=1$ . We deduce that $\\begin{array}{r}{S_{t+1}\\leq\\operatorname*{max}(1,\\frac{3}{4}S_{t})}\\end{array}$ with probability 1, hence $S_{t}\\le\\operatorname*{max}(1,(3/4)^{t}k)$ for all $t\\geq1$ . Therefore, it holds almost surely that ", "page_idx": 16}, {"type": "equation", "text": "$$\nT\\leq\\lceil\\log_{4/3}k\\rceil\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. The algorithm described in Theorem 2.1 runs RBS with dirty comparisons to obtain an estimated position $\\widehat{r}(u,L)$ of $u$ in $L$ , then it conducts a clean exponential search starting from $\\widehat{r}(u,L)$ to find the exact position $r(u,L)$ . Lemma B.1 guarantees that the dirty RBS uses $O(\\log k)$ comparisons, while the clean exponential search terminates in expectation after $O(\\log|r(u,L)-\\widehat{r}(\\dot{u},L)|)$ comparisons. Therefore, for demonstrating Theorem 2.1, it suffices to prove that $\\mathbb{E}[\\log|r(u,L)-\\widehat{r}(u,L)|]=$ $O(\\log\\eta(u,L))$ . ", "page_idx": 16}, {"type": "text", "text": "To lighten the expressions, we write $\\eta$ instead of $\\eta(u,L)$ in the rest of the proof. Let $(i_{0},j_{0})=(1,n)$ , and $\\left(i_{t},j_{t}\\right)$ the indices delimiting the sub-list of $L$ to which the RBS is reduced after $t$ steps for all $t\\geq1$ . Denoting by $t^{*}+1$ the first step where a dirty comparison is inaccurate, it holds that $\\widehat{r}(u,L),r(u,L)\\in\\{i_{t^{*}},j_{t^{*}}\\}$ . Indeed, the first $t^{*}$ dirty comparisons are all accurate, thus the estimated a nd the true positions of $u$ in $L$ are both in $\\{i_{t^{*}},j_{t^{*}}\\}$ . Therefore, $|r(u,L)-\\widehat{r}(u,L)|\\,\\leq\\,S_{t^{*}}\\,-\\,1$ , where $S_{t}=j_{t}-i_{t}+1$ is the size of the sub-list delimited by indices $\\left(i_{t},j_{t}\\right)$ , which yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\log|r(u,L)-\\widehat{r}(u,L)|\\leq\\log(S_{t^{*}})\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We focus in the remainder on bounding $\\mathbb{E}[\\log(S_{t^{*}})]$ . For this, we use a proof scheme similar to that of Lemmas A.4 and A.5 in Bai and Coester [2023]. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\log(S_{t^{*}})]\\leq\\mathbb{E}[\\lceil\\log(S_{t^{*}})\\rceil]}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{m\\geq1}^{\\infty}m\\,\\mathrm{Pr}(\\lceil\\log(S_{t^{*}})\\rceil=m)}\\\\ &{\\qquad\\qquad\\leq\\log(\\eta+1)+\\displaystyle\\sum_{m=\\lceil\\log(\\eta+1)\\rceil}^{\\infty}m\\,\\mathrm{Pr}(\\lceil\\log(S_{t^{*}})\\rceil=m)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and for all $m\\geq1$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(\\lceil\\log(S_{t^{*}})\\rceil=m)=\\operatorname*{Pr}(S_{t^{*}}\\in(2^{m-1},2^{m}])}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{t=1}^{\\infty}\\operatorname*{Pr}(S_{t}\\in(2^{m-1},2^{m}],t=t^{*})}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{t=1}^{\\infty}\\operatorname*{Pr}(S_{t}\\in(2^{m-1},2^{m}])\\operatorname*{Pr}(t=t^{*}\\mid S_{t}\\in(2^{m-1},2^{m}])\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the sum above, for any $t\\geq1$ , the probability that $t=t^{*}$ is the probability that the next sampled pivot index $\\ell_{t}$ is in the set $\\mathcal{F}(u,L)=\\overline{{\\{v\\in L:\\mathbb{1}(u\\stackrel{}{<}\\ v)\\neq\\mathbb{1}(u<\\stackrel{}{v})\\}}}$ , which has a cardinal $\\eta$ . Thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(t=t^{*}\\mid S_{t})=\\operatorname*{Pr}(\\ell_{t}\\in{\\mathcal{F}}(u,L)\\mid S_{t})={\\frac{{\\#}({\\mathcal{F}}\\cap\\{i_{t},\\dots,j_{t}\\})}{S_{t}}}\\leq{\\frac{\\eta}{S_{t}}}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It follows that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(\\lceil\\log(S_{t^{*}})\\rceil=m)\\leq\\displaystyle\\frac{\\eta}{2^{m-1}}\\sum_{t=1}^{\\infty}\\operatorname*{Pr}(S_{t}\\in(2^{m-1},2^{m}])}\\\\ &{\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{\\eta}{2^{m-1}}\\mathbb{E}[\\#\\{t\\ge1:S_{t}\\in(2^{m-1},2^{m}]\\}]}\\\\ &{\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{3\\eta}{2^{m-1}}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality is an immediate consequence of the identity $S_{t+1}\\,\\leq\\,\\frac{3}{4}S_{t+1}$ proved in Lemma B.1, which shows in particular that $S_{t+3}<S_{t}/2$ , i.e. the after at most 3 steps, the size of the search sub-list is divided by two, hence $(S_{t})_{t}$ falls into the interval $(2^{m-1},2^{m}]$ at most three times. Substituting into (6) gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\log S_{t^{*}}]\\leq\\log(\\eta+1)+3\\eta\\cdot\\underset{m=\\lceil\\log(\\eta+1)\\rceil}{\\bigotimes}\\frac{m}{2^{m-1}}}\\\\ &{\\quad\\quad\\quad\\quad=\\log(\\eta+1)+3\\eta\\cdot O\\left(\\frac{\\log\\eta}{\\eta}\\right)}\\\\ &{\\quad\\quad\\quad\\quad=O(\\log\\eta)\\,\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, by (5), the expected number of comparisons used during the clean exponential search is at most $O(\\mathbb{E}[\\log|r(u,L)-\\hat{r}(u,L)|])=O(\\log\\bar{\\eta})$ , which concludes the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.3 Proof of Theorem 2.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Consider a binary heap $\\mathcal{Q}$ containing $n$ elements, with positions randomly filled in the leaf level. Even with this randomization, $\\mathcal{Q}$ is still balanced, and the length of any path from an empty position in the leaf level to the root has a size $O(\\log n)$ . Denoting by $L$ the random insertion path chosen in Algorithm 1, constructing $L$ and storing it requires $O(\\log n)$ time and space. By Theorem 2.1, the randomized search in Algorithm 1 uses $O(\\log\\log n)$ dirty comparisons, and the exponential search uses an expected number of $O(\\mathbb{E}[\\log\\eta(u,L)])$ clean comparisons. Inserting $u$ and then swapping it up until its position does not use any comparison and requires a ${\\cal O}(\\log n)$ time. Therefore, to prove the theorem, it suffices to demonstrate that $\\bar{\\mathbb{E}}[\\log\\eta(u,L)]\\stackrel{\\cdot}{=}O(\\log\\log\\eta(u,\\mathcal{Q}))$ . ", "page_idx": 18}, {"type": "text", "text": "We assume that the key $u$ to be inserted can be chosen by an oblivious adversary, unaware of the randomization outcome. This means that the internal state of $\\mathcal{Q}$ remains private at any time. Let $\\mathcal{F}(u,\\mathcal{Q})=\\{v\\in\\mathcal{Q}:\\mathbb{1}(u\\stackrel{\\wedge}{<}v)\\neq\\mathbb{1}(u<v)\\}$ , which has a cardinal of $\\eta(u,\\mathcal{Q})$ . Enumerating the binary heap levels starting  from the root, each level $\\ell$ except for the last one, denoted $\\ell_{\\mathrm{max}}$ , contains exactly $2^{\\ell\\bar{-}1}$ elements $v_{1}^{\\ell^{-}},\\ldots,v_{2^{\\ell-1}}^{\\ell}$ . A key $v_{i}^{\\ell}$ in level $\\ell$ has a probability $1/2^{\\ell-1}$ of belonging to $L$ . Denoting by $\\xi_{i}^{\\ell}=\\mathbb{1}(v_{i}^{\\ell}\\in\\mathcal{F}(u,\\mathbf{\\bar{Q}}))$ , the expected number of keys in $L$ belonging to $\\textstyle{\\mathcal{F}}(u,{\\mathcal{Q}})$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\eta(u,L)]=\\sum_{\\ell=1}^{\\ell_{\\mathrm{max}}}\\sum_{i=1}^{2^{\\ell-1}}\\frac{\\xi_{i}^{\\ell}}{2^{\\ell-1}}=\\sum_{\\ell=1}^{\\ell_{\\mathrm{max}}}\\frac{1}{2^{\\ell-1}}\\left(\\sum_{i=1}^{2^{\\ell-1}}\\xi_{i}^{\\ell}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Given that $\\begin{array}{r}{\\sum_{\\ell=1}^{\\ell_{\\mathrm{max}}}\\sum_{i=1}^{2^{\\ell-1}}\\xi_{i}^{\\ell}=\\eta(u,\\mathcal{Q})\\leq2^{\\lceil\\log(\\eta(u,\\mathcal{Q})+1)\\rceil}-1}\\end{array}$ , the expression above is maximized under this constraint for the instance $\\bar{\\xi}_{i}^{\\ell}=\\mathbb{1}(\\ell\\leq\\lceil\\log(\\eta(u,\\mathcal{Q})+1)\\rceil)$ . Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\eta(u,L)]\\leq\\sum_{\\ell}\\frac{1}{2^{\\ell-1}}\\left(\\sum_{i=1}^{2^{\\ell-1}}\\bar{\\xi}_{i}^{\\ell}\\right)=\\lceil\\log(\\eta(u,\\mathcal{Q})+1)\\rceil\\ .\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, Jensen\u2019s inequality and the concavity of log yield ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\log\\eta(u,L)]\\leq\\log\\mathbb{E}[\\eta(u,L)]=O(\\log\\log\\eta(u,\\mathcal{Q}))\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which gives the result. ", "page_idx": 18}, {"type": "text", "text": "C Skip lists ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Unique keys in the priority queue ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The keys of the priority queue can be considered pairwise distinct by grouping elements with the same key together in a collection, for example, a HashSet. This collection can be accessible in $O(1)$ time via the key using a HashMap. When a new item $x$ with priority $u$ is to be inserted, the algorithms first checks if the key $u$ is already in the priority queue, if that is the case then $x$ is simply added to the collection corresponding to $u$ in $O(1)$ time. Otherwise, the key $u$ must first be inserted into its correct position. ", "page_idx": 18}, {"type": "text", "text": "With such implementation, when an ExtractMin operation is called, if multiple elements correspond to the minimum key, then the algorithm can for example retrieve an arbitrary one of them, of the first inserted one, depending on the use case. ", "page_idx": 18}, {"type": "text", "text": "C.2 Expected maximum of i.i.d. geometric random variables ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Before presenting the insertion algorithms in the three prediction models, we present an upper bound from [Eisenberg, 2008] on the expected maximum of i.i.d. geometric random variables with parameter $p$ . This Lemma will be useful in the analysis of our algorithms, as the heights of elements in the skip list are i.i.d. geometric random variables. The following Lemma is an immediate consequence of ", "page_idx": 18}, {"type": "text", "text": "Lemma C.1 ([Eisenberg, 2008]). If $X_{1},\\ldots,X_{m}$ are i.i.d. random variables following a geometric random distribution with parameter $p$ , then, denoting by $q=1-p,$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\operatorname*{max}_{i\\in[m]}X_{i}]\\leq1+\\frac{1}{\\log(1/q)}\\sum_{k=1}^{m}\\frac{1}{k}=O(\\log_{1/q}m)\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C.3 Pointer prediction model ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 3.1. The key $w$ found at the end of the algorithm is the processor of $u$ in $\\mathcal{Q}$ . Inserting $u$ next to $w$ only requires expected $O(1)$ time. Thus, we demonstrate in the following that Algorithm 2, starting from a key $v_{j}\\in\\mathcal{Q}$ , finds the predecessor of $u$ in $O(\\log|r(v_{j})-r(u)|)$ expected time, where $r(v)$ denotes the rank $r(v,\\mathcal{Q})$ of $v$ in $\\mathcal{Q}$ . In particular, for $v_{j}={\\widehat{\\mathrm{Pred}}}(u,\\mathcal{Q})$ , we obtain the claim of the theorem. ", "page_idx": 19}, {"type": "text", "text": "We assume in the proof that $v_{j}<u$ , i.e. the exponential search goes from left to right. Let $h^{*}(v_{j},u)$ be the maximum height of all elements in $\\mathcal{Q}$ between $u$ and $v_{j}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nh^{*}(v_{j},u)=\\operatorname*{max}\\{h(v):v\\in\\mathcal{Q}\\mathrm{~such~that~}v_{j}\\leq v\\leq u\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The number of elements between $v_{j}$ and $u$ , with $v_{j}$ included, is $|r(u)-r(v_{j})|+1$ , and the heights of all the elements in $\\mathcal{Q}$ are independent geometric random variables with parameter $p$ , thus Lemma C.1 gives that $\\mathbb{E}[h^{*}(v_{j},u)]=O\\bar{(}\\mathrm{log}\\,|r(\\bar{u)}-r(v_{j})|)$ . ", "page_idx": 19}, {"type": "text", "text": "The key $w^{*}$ found at the end of the Bottom-Up search is the last element, going from $v_{j}$ to $u$ , having a height of $h^{*}(v_{j},u)$ . Indeed, in the Bottom-Up search, whenever the algorithm reaches a new key, it moves to the maximum level to which the key belongs, the height of $w^{*}$ is therefore necessarily the maximum height of all the keys between $v_{j}$ and $w^{*}$ , i.e. $h(w^{*})=h^{*}(v_{j},w^{*})$ . Since the Bottom-Up search stops at key $w^{*}$ , then $\\mathrm{Next}(w^{*},h(\\bar{w^{*}}))>u$ , which means that there is no key in $\\mathcal{Q}$ between $w^{*}$ and $u$ having a height more than $h(w^{*})-1$ . ", "page_idx": 19}, {"type": "text", "text": "The number of comparisons made in this phase is therefore at most the number of comparisons needed to reach level $h^{*}(v_{j},u)+1$ starting from $v_{j}$ using the Bottom-Up search. We consider the hypothetical setting where the skip list is infinite to the right, the expected number of comparisons to reach level $h^{*}(v_{j},u)+1$ , in this case, is an upper bound on the expected number of comparisons needed in the Bottom-Up phase of Algorithm 2, as the algorithm also terminates if the end of the skip-list is reached. Let $T(\\ell)$ be the expected number of comparisons made in the bottom-up search to reach level $\\ell$ in an infinite skip list. After each comparison made in the bottom-up search, it is possible to go at least one level up with probability $p$ , while the algorithm can only move horizontally to the right with probability $1-p$ . This induces the inequality ", "page_idx": 19}, {"type": "equation", "text": "$$\nT(\\ell)\\leq1+p T(\\ell-1)+(1-p)T(\\ell)\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which yields ", "page_idx": 19}, {"type": "equation", "text": "$$\nT(\\ell)\\leq\\frac{1}{p}+T(\\ell-1)~.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given that $T(1)\\,=\\,0$ , we have for $\\ell\\,\\geq\\,1$ that $\\begin{array}{r}{T(\\ell)\\,\\le\\,\\frac{\\ell-1}{p}}\\end{array}$ , and we deduce that the expected number of comparisons made by the algorithm during the Bottom-Up search is at most $\\begin{array}{r}{\\frac{\\mathbb{E}[h^{\\ast}(v_{j},u)]}{p}=}\\end{array}$ $O(\\log|r(v_{j})-r(u)|)$ . ", "page_idx": 19}, {"type": "text", "text": "In the Top-Down search described in the second phase, the path traversed by the algorithm is exactly the inverse of the Bottom-Up search from the predecessor of $u$ to $w^{*}$ . The same arguments as the analysis of the first phase give that the Top-Down search terminates after $O(\\log|r(v_{j})-r(u)|)$ comparisons in expectation, which concludes the proof. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C.4 Dirty comparison model ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 3.2. Let $\\textstyle{\\mathcal{F}}(u,{\\mathcal{Q}})$ the set of keys in $\\mathcal{Q}$ whose dirty comparisons with $u$ are inaccurate ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\mathcal{F}}(u,{\\mathcal{Q}})=\\{v\\in{\\mathcal{Q}}:\\mathbb{1}(u\\stackrel{\\frown}{<}v)\\neq\\mathbb{1}(u<v)\\}\\ .\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The prediction error $\\eta(u,\\mathcal{Q})$ defined in (2) is the cardinal of $\\textstyle{\\mathcal{F}}(u,{\\mathcal{Q}})$ . Let ", "page_idx": 19}, {"type": "equation", "text": "$$\nh^{*}(\\mathcal{F}(u,\\mathcal{Q}))=\\operatorname*{max}\\{h(v):v\\in\\mathcal{F}(u,\\mathcal{Q})\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "be the maximal height of elements in $\\textstyle{\\mathcal{F}}(u,{\\mathcal{Q}})$ . The search algorithm Search $(\\mathcal{Q},u)$ with dirty comparisons starts from the highest level at the head of the skip list, and then goes down the different levels until finding the predicted position $\\hat{w}$ of $u$ . This is the classical search algorithm in skip lists, and it is known to require $O(\\log n)$ comparisons to terminate. This can also be deduced from the analysis of the exponential search described in Algorithm 2, as it corresponds to the Top-Down search starting from the head of the skip list. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Before level $h^{*}({\\mathcal{F}}(u,{\\mathcal{Q}}))$ is reached in ${\\mathrm{Search}}(\\mathcal{Q},u)$ , all the dirty comparisons are accurate. Denoting by $v^{\\prime}$ the last key in $\\mathcal{Q}$ visited in a level higher than $h^{*}({\\mathcal{F}}(u,{\\mathcal{Q}}))$ during this search, and $v^{\\prime\\prime}=\\mathrm{\\bar{Next}}(\\mathcal{Q},v^{\\prime},h^{*}(\\mathcal{F}(u,\\mathcal{Q})))$ , it holds that both keys $\\hat{w}$ and $w$ are between $v^{\\prime}$ and $v^{\\prime\\prime}$ , and there is no key in $\\mathcal{Q}$ between $v^{\\prime}$ and $v^{\\prime\\prime}$ with height more than $h^{*}(\\mathcal{F}(u,\\mathcal{Q}))-1$ . ", "page_idx": 20}, {"type": "text", "text": "In particular, the maximal key height between $\\hat{w}$ and $w$ is at most $h^{*}(\\mathcal{F}(u,\\boldsymbol{\\mathcal{Q}}))-1$ . We showed in the proof of Theorem 3.1 that the number of comparisons and runtime of ExpSearchInsertion $(\\mathcal{Q},v_{j},u)$ is linear with the maximal height of keys in $\\mathcal{Q}$ that are between $v_{j}$ and $u$ . Using this result with $\\hat{w}$ instead of $v_{j}$ gives that ExpSearchInsertion $(\\varphi,\\hat{w},u)$ finds the position of $u$ using $O(h^{\\ast}(\\mathcal{F}(u,\\mathcal{Q})))$ clean comparisons. Finally, since $h^{*}({\\mathcal{F}}(u,{\\mathcal{Q}}))$ is the maximum of a number $\\eta(u,\\mathcal{Q})$ of i.i.d. geometric random variables with parameter $p$ , Lemma C.1 gives that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[h^{*}({\\mathcal{F}}(u,{\\mathcal{Q}}))]=O(\\log\\eta(u,{\\mathcal{Q}}))\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which proves the theorem. ", "page_idx": 20}, {"type": "text", "text": "C.5 Rank prediction model ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To leverage rank predictions, as explained in Section 3.3, we use an auxiliary van Emde Boas (vEB) tree van Emde Boas [1977]. We describe in the following the structure ad complexity of vEB trees on $[N]$ , and we explain how they can be adapted in the case where $N$ is unknown. ", "page_idx": 20}, {"type": "text", "text": "C.5.1 Van Emde Boas (vEB) trees ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "A vEB tree over an interval $\\{i+1,\\ldots,i+m\\}$ has a root with $\\sqrt{m}$ children, each being the root of a smaller vEB tree over a sub-interval $\\{i\\{k\\sqrt{m}\\!+\\!1,\\ldots,i\\!+\\!(k\\!+\\!1)\\sqrt{m}\\}$ for some $k\\in\\{0,\\bar{.}\\,.\\,.\\,,\\sqrt{m}{-}1\\}$ . The tree leaves are either empty or contain elements with the corresponding key, stored together in a collection, and internal nodes carry binary information indicating whether or not the subtree they root contains at least one element. Denoting by $H(m)$ the height of a vEB tree of size $m$ , it holds that $H(m)=1+H({\\sqrt{m}})$ , which yields that $H(m)=O(\\log\\log m)$ , enabling efficient implementation of the operations listed below in $O(\\log\\log m)$ time: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Insert $(x,k)$ : insert a new element $x$ with key $k\\in[m]$ in the tree,   \n\u2022 $\\mathrm{{Delete}}(x,k)$ : Delete the element/key pair $(x,k)$ ,   \n\u2022 Predecessor $(k)$ : return the element in the tree with the largest key smaller than or equal to $k$ ,   \n\u2022 Successor $(k)$ : return the element in the tree with the smallest key larger than or equal to $k$ , \u2022 $\\mathrm{{ExtractMin}()}$ : removes and returns the element with the smallest key. ", "page_idx": 20}, {"type": "text", "text": "Other operations such as FindMin, FindMax, or Lookup $(k)$ are supported in $O(1)$ time. These runtimes, however, require knowing the maximal key value $m$ from the beginning, as it is used for constructing $\\tau_{m}$ . ", "page_idx": 20}, {"type": "text", "text": "C.5.2 Dynamic size vEB trees ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "If the maximal key value $\\bar{R}$ is unknown, we argue that the operations listed above can be supported in amortized $O(\\log\\log{\\bar{R}})$ time. Given a vEB tree $\\tau_{m}$ of size $m$ , if a new key $k\\in\\{m+1,\\ldots,2m\\}$ is to be inserted, we construct an empty vEB tree $\\mathcal{T}_{2m}$ of size $2m$ in $O(m)$ time, then repeatedly extract the elements with minimal key from $\\tau_{m}$ and insert them in $\\mathcal{T}_{2m}$ with the same key. Each ExtractMin operation in $\\tau_{m}$ and insertion in $\\ensuremath{\\tau_{2m}}$ requires $O(\\log\\log m)$ time. Therefore, constructing $\\mathcal{T}_{2m}$ and inserting all the elements from $\\tau_{m}$ takes $O(m\\log\\log m)$ time. ", "page_idx": 20}, {"type": "text", "text": "This observation can be used to define a vEB with dynamic size. First, we construct a vEB tree with an initial constant size $R_{0}$ . If at some point the size of the vEB tree is $m\\geq R_{0}$ and a new key $k>m$ is to be inserted, then we iterate the size doubling process described before until the size of the vEB tree is at least $k$ . At any time step, denoting by R\u00af the maximal key value inserted in the vEB tree, and letting $i\\geq1$ such that $2^{i-1}R_{0}\\overset{\\cdot}{\\leq}\\bar{R}<2^{i}\\check{R}_{0}$ , the size of the tree has been doubled up to this step $i$ times to cover all the keys. The total time for resizing the vEB tree is at most proportional to ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{j=0}^{i-1}2^{j}R_{0}\\log\\log(2^{j}R_{0})\\le\\Big(\\displaystyle\\sum_{j=0}^{i-1}2^{j}\\Big)R_{0}\\log\\log\\bar{R}}}\\\\ &{}&{\\le2^{i}R_{0}\\log\\log\\bar{R}}\\\\ &{}&{\\le2\\bar{R}\\log\\log\\bar{R}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, if $N$ is the total number of elements inserted into the vEB tree and $\\bar{R}$ the maximum key value, we can neglect the cost of resizing by considering that each insertion requires an amortized time of $\\begin{array}{r}{O((1+\\frac{\\bar{R}}{N})\\log\\log\\bar{R})}\\end{array}$ . The runtime of all the other operations is $O(\\log\\log{\\bar{R}})$ . In particular, if $\\bar{R}=O(N)$ , then all the operations run in $O(\\log\\log N)$ amortized time. ", "page_idx": 21}, {"type": "text", "text": "C.5.3 Priority queue with rank predictions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Consider the setting where $N$ is unknown and all the predicted ranks, revealed online, satisfy $R(u_{i})=O(N)$ . The priority queue we consider is a skip list $\\mathcal{Q}$ with an auxiliary dynamic vEB tree $\\tau$ . For insertions, we use Algorithm 4. For ExtractMin, we first extract the minimum $u_{\\mathrm{{min}}}$ from $\\mathcal{Q}$ in $O(1)$ time, then we delete it from the corresponding position $\\widehat{R}(u_{\\mathrm{min}})$ in $\\tau$ in $O(\\log\\log N)$ time. Deleting an arbitrary key $u$ from $\\mathcal{Q}$ can be done in the same way, by removing it from $\\mathcal{Q}$ in expected $O(1)$ time and then deleting it from the position $\\widehat{R}(u)$ in $\\tau$ in $O(\\log\\log N)$ time. Thus, as in the other prediction models, DecreaseKey can be implemented by deleting the element and reinserting it with the new key, which requires the same complexity as insertion, with an additional $O(\\log\\log\\bar{N})$ term. ", "page_idx": 21}, {"type": "text", "text": "We will prove that the priority queue described above yields the claim of Theorem 3.3. We assume that the keys $u_{1},\\dotsc,u_{N}$ are inserted in this order. For all $t\\in[N]$ , we denote by $\\mathcal{Q}^{t},\\mathcal{T}^{t}$ the set of keys in the skip list and the set of integer keys in the dynamic vEB tree right after the insertion of $u_{t}$ , with the keys in $\\mathcal{Q}^{t}$ or $\\mathcal{T}^{t}$ . Note that, due to eventual deletions, the sizes of $\\mathcal{Q}^{t}$ and $\\mathcal{T}^{t}$ can be smaller than $t$ . ", "page_idx": 21}, {"type": "text", "text": "Following the definition of the rank in (1), for all $i,t\\,\\in\\,[N]$ , we denote by $r(u_{i},\\mathcal{Q}^{t})$ the rank of $u_{i}$ in $\\mathcal{Q}^{t}$ , and $r(\\widehat{R}(u_{i}),\\mathcal{T}^{t})$ the rank of $\\widehat{R}(u_{i})$ in $\\mathcal{T}^{t}$ . The following lemma shows that the absolute difference betwe en the two previous qu antities for any given $i,t$ is at most twice the maximal rank prediction error. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.2. For any subset $I\\subset[N]$ , it holds for all $i\\in I$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\n|r(u_{i},\\{u_{j}\\}_{j\\in I})-r(\\widehat{R}(u_{i}),\\{\\widehat{R}(u_{j})\\}_{j\\in I})|\\leq2\\operatorname*{max}_{j\\in[N]}\\eta^{\\Delta}(u_{j})\\ .\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Let ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta_{*}=\\operatorname*{max}_{I\\subset[N]}\\left(\\operatorname*{max}_{i\\in I}|r(u_{i},\\{u_{j}\\}_{j\\in I})-r(\\widehat{R}(u_{i}),\\{\\widehat{R}(u_{j})\\}_{j\\in I})|\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and let $I\\subset[N]$ for which this maximum is reached. We assume without loss of generality that $I=[m]$ . For all $s\\in[N]$ , let ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{Q}}^{s}=\\{u_{j}\\}_{j\\in[s]},\\quad\\tilde{\\mathcal{T}}^{s}=\\{\\widehat{R}(u_{j})\\}_{j\\in[s]},\\quad\\Delta^{s}=\\operatorname*{max}_{i\\in[s]}|r(u_{i},\\tilde{\\mathcal{Q}}^{s})-r(\\widehat{R}(u_{i}),\\tilde{\\mathcal{T}}^{s})|\\ .\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To simplify the expressions, we denote by $r_{i}^{s}=r(u_{i},\\Tilde{\\mathcal{Q}}^{s})$ and $\\widehat{r}_{i}^{s}=r(\\widehat{R}(u_{i}),\\widetilde{\\mathcal{T}}^{s})$ for all $(s,i)\\in$ $[N]^{2}$ . We will prove that $\\Delta^{s}\\,=\\,\\Delta_{*}$ for all $s\\,\\in\\,\\{m,\\dots,N\\}$ .   By defi nition of $\\Delta_{*}$ , it holds that $\\Delta_{*}\\geq\\Delta^{s}$ for all $s$ , it remains to prove the other inequality. It is true for $s=m$ by definition of $I$ and $m$ . Now let $s\\in\\{m,\\ldots,N-1\\}$ and assume that $\\Delta^{s}=\\Delta_{*}$ , i.e. there exists $i\\leq s$ such that $|r_{i}^{s}-\\widehat{r}_{i}^{s}|=\\Delta_{*}$ . Assume for example that $\\widehat{r}_{i}^{s}=r_{i}^{s}+\\Delta_{*}$ . ", "page_idx": 21}, {"type": "text", "text": "\u2022 If $u_{s+1}<u_{i}$ , then $r_{s+1}^{s+1}<r_{i}^{s+1}$ and rs+1 $r_{i}^{s+1}=r_{i}^{s}+1$ . By definition of $\\Delta_{*}$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{r}_{s+1}^{s+1}\\leq r_{s+1}^{s+1}+\\Delta_{*}\\leq r_{i}^{s+1}-1+\\Delta_{*}=r_{i}^{s}+\\Delta_{*}=\\widehat{r}_{i}^{s}\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This implies necessarily that $\\widehat{R}(u_{s+1})\\leq\\widehat{R}(u_{i})$ , and therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{r}_{i}^{s+1}=\\widehat{r}_{i}^{s}+1=r_{i}^{s}+1+\\Delta_{*}=r_{i}^{s+1}+\\Delta_{*}\\ ,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which gives that $\\Delta^{s+1}\\geq|\\widehat{r}_{i}^{s+1}-r_{i}^{s+1}|=\\Delta_{*}$ . ", "page_idx": 22}, {"type": "text", "text": "\u2022 If $u_{s+1}>u_{i}$ , then $r_{i}^{s+1}=r_{i}^{s}$ and $\\widehat{r}_{i}^{s+1}\\geq\\widehat{r}_{i}^{s}$ , thus $\\Delta^{s+1}\\geq\\widehat{r}_{i}^{s+1}-r_{i}^{s+1}\\geq\\widehat{r}_{i}^{s}-r_{i}^{s}=\\Delta_{*}.$ ", "page_idx": 22}, {"type": "text", "text": "\u2022 If $u_{s+1}=u_{i}$ then $r_{s+1}^{s+1}=r_{i}^{s+1}=r_{i}^{s}+1$ . On the other hand, if $\\widehat{R}(u_{s+1})\\leq\\widehat{R}(u_{i})$ then ris+1=ris + 1, otherwiser ss++11 \u2265ris + 1. In both cases, it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta^{s+1}\\geq\\operatorname*{max}(\\widehat{r}_{i}^{s+1}-r_{i}^{s+1}\\,,\\,\\widehat{r}_{s+1}^{s+1}-r_{s+1}^{s+1})}\\\\ {\\geq(\\widehat{r}_{i}^{s}+1)-(r_{i}^{s}+1)=\\Delta_{*}\\ .\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The same proof can be used for the case where $r_{i}^{s}\\,=\\,\\widehat{r}_{i}^{s}+\\Delta_{*}$ . Therefore, we have for all $s\\ \\in$ $\\{m,\\ldots,N\\}$ that $\\Delta_{*}\\,=\\,\\Delta^{s}$ . In particular, for $s\\,=\\,N$ , observing that $r(u_{i},\\tilde{\\mathcal{Q}}^{N})\\,=\\,R(u_{i})$ for all $i\\in[N]$ , we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Delta_{*}=\\operatorname*{max}_{i\\in[N]}|R(u_{i})-r(\\widehat{R}(u_{i}),\\tilde{\\mathcal{T}}^{N})|\\ .\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "fLoeltl ouws idnegn tohtea tby $\\eta_{\\mathrm{max}}^{\\Delta}=\\mathrm{max}_{j\\in[N]}\\,\\eta^{\\Delta}(u_{j})$ the maximum rank prediction error. We will prove in the ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall i\\in[N]:\\quad|R(u_{i})-r(\\widehat{R}(u_{i}),\\widetilde{\\mathcal{T}}^{N})|\\leq2\\eta_{\\operatorname*{max}}^{\\Delta}\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With the assumption that the keys $\\{u_{i}\\}_{i\\in[N]}$ are pairwise distinct, the ranks $(R(u_{i}))_{i\\in[N]}$ form a permutation of $[N]$ . ", "page_idx": 22}, {"type": "text", "text": "Given that $|R(u_{k})-\\widehat{R}(u_{k})|\\leq\\eta_{\\operatorname*{max}}^{\\Delta}$ for all $k\\in[N]$ , it holds for any $i,j\\in[N]$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{R}(u_{j})\\leq\\widehat{R}(u_{i})\\implies R(u_{j})-\\eta_{\\operatorname*{max}}^{\\Delta}\\leq R(u_{i})+\\eta_{\\operatorname*{max}}^{\\Delta}}\\\\ {\\implies R(u_{j})\\leq R(u_{i})+2\\eta_{\\operatorname*{max}}^{\\Delta}\\ ,\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "hence, given that $\\begin{array}{r}{\\mathcal{T}^{N}=\\{\\widehat{R}(u_{j})\\}_{j\\in[N]}}\\end{array}$ and by definition (1) of the rank $r(\\widehat{R}(u_{i}),\\mathcal{T}^{N})$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r(\\widehat{R}(u_{i}),\\mathcal{T}^{N})=\\#\\{j\\in[N]:\\widehat{R}(u_{j})\\leq\\widehat{R}(u_{i})\\}}\\\\ &{\\qquad\\qquad\\leq\\#\\{j\\in[N]:R(u_{j})\\leq R(u_{i})+2\\eta_{\\operatorname*{max}}^{\\Delta}\\}}\\\\ &{\\qquad\\qquad=\\#\\{k\\in[N]:k\\leq R(u_{i})+2\\eta_{\\operatorname*{max}}^{\\Delta}\\}}\\\\ &{\\qquad\\qquad=\\operatorname*{min}(N\\,,\\,R(u_{i})+2\\eta_{\\operatorname*{max}}^{\\Delta})}\\\\ &{\\qquad\\qquad\\leq R(u_{i})+2\\eta_{\\operatorname*{max}}^{\\Delta}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where Equation 9 holds because $(R(u_{i}))_{i\\in[N]}$ is a permutation of $[N]$ . Similarly, we have for all $i,j\\in[N]$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(u_{j})\\leq R(u_{i})-2\\eta_{\\operatorname*{max}}^{\\Delta}\\implies R(u_{j})+\\eta_{\\operatorname*{max}}^{\\Delta}\\leq R(u_{i})-\\eta_{\\operatorname*{max}}^{\\Delta}}\\\\ &{\\qquad\\qquad\\implies\\widehat{R}(u_{j})\\leq\\widehat{R}(u_{i})~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and it follows for all $i\\in[N]$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r(\\widehat{R}(u_{i}),\\mathcal{T}^{N})=\\#\\{j\\in[N]:\\widehat{R}(u_{j})\\leq\\widehat{R}(u_{i})\\}}\\\\ &{\\qquad\\qquad\\geq\\#\\{j\\in[N]:R(u_{j})\\leq R(u_{i})-2\\eta_{\\operatorname*{max}}^{\\Delta}\\}}\\\\ &{\\qquad\\qquad=\\#\\{k\\in[N]:k\\leq R(u_{i})-2\\eta_{\\operatorname*{max}}^{\\Delta}\\}}\\\\ &{\\qquad\\qquad=\\operatorname*{max}(0\\,,\\,R(u_{i})-2\\eta_{\\operatorname*{max}}^{\\Delta})}\\\\ &{\\qquad\\qquad=R(u_{i})-2\\eta_{\\operatorname*{max}}^{\\Delta}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From (10) and (11), we deduce that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall i\\in[N]:\\quad|R(u_{i})-r(\\widehat{R}(u_{i}),\\mathcal{T}^{N})|\\leq2\\eta_{\\operatorname*{max}}^{\\Delta}\\;,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining this with (8) and (7) yields the wanted result. ", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. When a new key $u_{i}$ is to be inserted, it is first inserted in the dynamic vEB tree $\\mathcal{T}^{i}$ with integer key $\\widehat{R}(u_{i})$ , and its predecessor $\\hat{w}$ in $\\mathcal{T}^{i}$ is retrieved. These first operations require $O(\\log\\log N)$ time. The position of $u_{i}$ in $\\mathcal{Q}^{i}$ is then obtained via an exponential search starting from $\\hat{w}$ , which requires $O(\\log|r(u_{i},\\mathcal{Q}^{i})-r(\\hat{w},\\mathcal{Q}^{i})|)$ expected time by Theorem 3.1. Finally, inserting $u$ in the found position takes expected $O(1)$ time. ", "page_idx": 23}, {"type": "text", "text": "For any newly inserted element, by accounting for the potential future deletion time via ExtractMin at the moment of insertion, we can ensure that all ExtractMin operations require constant amortized time. This approach results in only an additional $O(\\log\\log N)$ time for insertions. ", "page_idx": 23}, {"type": "text", "text": "Therefore, to prove Theorem 3.3, we only need to show that $\\log|r(u_{i},\\mathcal{Q}^{i})\\;-\\;r(\\hat{w},\\mathcal{Q}^{i})|\\;=$ $O(\\log\\operatorname*{max}_{j\\in[N]}\\eta^{\\Delta}(u_{i}))$ . Since $\\hat{w}$ is the predecessor of $u_{i}$ in $\\mathcal{T}^{i}$ , it holds that $r(\\widehat{R}(u_{i}),\\mathcal{T}^{i})\\in$ $\\{r(\\widehat{R}(\\hat{w}),\\mathcal{T}^{i}),r(\\widehat{R}(\\hat{w},\\mathcal{T}^{i})\\,+\\,1\\}$ , the first case occurs if $\\widehat{R}(u_{i})\\ =\\ \\widehat{R}(\\widehat{w})$ , and the second if $\\widehat{R}(u_{i})>\\widehat{R}(\\widehat{w})$ . Using this observation and Lemma C.2, it follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|r(u_{i},Q^{i})-r(\\hat{w},Q^{i})|}\\\\ &{\\ \\leq|r(u_{i},Q^{i})-r(\\widehat{R}(u_{i}),\\mathcal{T}^{i})|+|r(\\widehat{R}(u_{i}),\\mathcal{T}^{i})-r(\\widehat{R}(\\hat{w}),\\mathcal{T}^{i})|+|r(\\hat{w},Q^{i})-r(\\widehat{R}(\\hat{w}),\\mathcal{T}^{i})|}\\\\ &{\\ \\leq4\\underset{j\\in[N]}{\\operatorname*{max}}\\eta^{\\Delta}(u_{j})+1\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and it follows that $\\log|r(u_{i},\\mathcal{Q}^{i})-r(\\hat{w},\\mathcal{Q}^{i})|\\,=\\,O(\\log\\operatorname*{max}_{j\\in[N]}\\eta^{\\Delta}(u_{i}))$ , which concludes the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "D Lower bounds ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "A priority queue can be used for sorting a sequence $A=(a_{i})_{i\\in[n]}\\in\\mathcal{U}^{n}$ , by first inserting all the elements in the priority queue, then repeatedly extracting the minimum until the priority queue is empty. In settings with dirty comparisons or positional predictions, the number of comparisons required by this sorting algorithm is constrained by the impossibility result demonstrated in Theorem 1.5 of Bai and Coester [2023]. We use this impossibility result to prove the lower bounds stated in Theorem 3.4. ", "page_idx": 23}, {"type": "text", "text": "D.1 Impossibility result for sorting with predictions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We begin in this section by summarizing the setting and the impossibility result demonstrated in Theorem 1.5 of Bai and Coester [2023] for sorting with predictions. ", "page_idx": 23}, {"type": "text", "text": "Positional predictions In the positional prediction model, the objective is to sort a sequence $A=(a_{i})_{i\\in[n]}$ , given a prediction $\\widehat{R}_{i}\\in[n]$ of $R_{i}=r(a_{i},A)$ for all $i\\in[n]$ . This model differs from our rank prediction model in that the sequence $A$ and the predictions $\\widehat{R}=(\\widehat{R}_{i})_{i\\in[n]}$ are given offline to the algorithm. Two different error measures are considered in Bai and Coester [2023], but we restrict ourselves to the displacement error $\\eta_{i}^{\\Delta}=|r(a_{i},A)-\\widehat{R}_{i}|$ , which is the same as our rank prediction error 4. In all the following, consider that $\\widehat{R}$ is a fixed permutation of $[n]$ , i.e. the predicted ranks are pairwise distinct. For all $\\xi\\geq1$ , consider the following set of instances ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{cand}(\\hat{R},n\\xi)=\\{A\\in\\mathcal{U}^{n}:\\sum_{i=1}^{n}\\log(\\eta_{i}^{\\Delta}+2)\\leq n\\xi\\}~.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The authors of Bai and Coester [2023] prove that, if $1\\leq\\xi\\leq O(\\log n)$ , then no algorithm can sort every instance from cand $(\\hat{R},n\\xi)$ with $o(n\\xi)$ comparisons. However, in their proof, they demonstrate a stronger result: no algorithm can sort every instance from $\\mathcal{T}_{n}(\\widehat{R},\\xi)$ with $o(n\\xi)$ comparisons, where ${\\mathcal{T}}_{n}({\\widehat{R}},\\xi)$ is the subset of cand $(\\hat{R},n\\xi)$ defined by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{n}(\\widehat{R},\\xi)=\\{A\\in\\mathcal{U}^{n}:\\operatorname*{max}_{i\\in[n]}\\log(\\eta_{i}^{\\Delta}+2)\\leq\\xi\\}\\ .\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Dirty comparisons in the dirty comparison model, the authors prove an analogous result by a reduction to the positional prediction model. More precisely, any permutation $\\widehat{R}$ on $[n]$ defines a unique dirty order $\\widehat{\\mathbf{\\xi}}$ on $A$ given by $a_{i}\\;\\widehat{<}\\;a_{j}\\;\\;\\Longleftrightarrow\\;\\;\\widehat{R}_{i}<\\;\\widehat{R}_{i}$ , and $\\begin{array}{r}{\\operatorname*{max}_{i\\in[n]}\\eta_{i}\\ \\leq\\ \\operatorname*{max}_{i\\in[n]}\\eta_{i}^{\\Delta}}\\end{array}$ , where $\\eta_{i}=\\eta(a_{i},A)=\\#\\{j\\in[n]:(a_{i}\\stackrel{\\frown}{<}a_{j})\\neq(a_{i}<a_{j})\\}$ . We deduce that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{T}_{n}(\\widehat{R},\\xi)\\subset\\{A\\in\\mathcal{U}^{n}:\\operatorname*{max}_{i\\in[n]}\\log(\\eta_{i}+2)\\leq\\xi\\}\\ .\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, given the dirty order $\\hat{<}$ , there is no algorithm that can sort every instance with $\\mathrm{max}_{i\\in[n]}\\,\\mathrm{log}(\\eta_{i}+2)\\leq\\xi$ in $o(n\\xi)$ time. ", "page_idx": 24}, {"type": "text", "text": "In the following, we use these lower bounds on sorting to prove our Theorem 3.4. ", "page_idx": 24}, {"type": "text", "text": "D.2 Pointer prediction model ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For any permutation $\\pi$ of $[n]$ and $i\\in[n]$ , we denote by $A_{i}^{\\pi}=\\{a_{\\pi(j)}\\}_{j\\in[i]}$ . This first elementary Lemma uses ideas from the proof of Theorem 1.3 in Bai and Coester [2023]. ", "page_idx": 24}, {"type": "text", "text": "Lemma D.1. $A$ permutation $\\pi$ of $[n]$ satisfying $\\widehat{R}_{\\pi(1)}\\leq...\\leq\\widehat{R}_{\\pi(n)}$ can be constructed in $O(n)$ time, and it holds for all $i\\in\\{2,\\ldots,n\\}$ that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|r(a_{\\pi(i)},A_{i-1}^{\\pi})-r(a_{\\pi(i-1)},A_{i-1}^{\\pi})|\\le\\eta_{\\pi(i-1)}^{\\Delta}+\\eta_{\\pi(i)}^{\\Delta}+\\widehat{R}_{\\pi(i)}-\\widehat{R}_{\\pi(i-1)}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The elements of $A$ can be sorted in non-decreasing order of their predicted ranks $\\widehat{R}_{i}$ within $O(n)$ time using a bucket sort. Hence, we obtain the permutation $\\pi$ . For all $i\\;\\in\\;\\{2,\\ldots,n\\}$ , $|r(a_{\\pi(i)},A_{i-1}^{\\pi})\\stackrel{.}{-}r(a_{\\pi(i-1)},A_{i-1}^{\\pi})|$ is the number of elements in $A_{i-1}^{\\pi}$ whose ranks are between those of $a_{\\pi(i)}$ and $a_{\\pi}(i\\!-\\!1)$ . This is at most the number of elements in $A$ whose ranks are between those of $a_{\\pi(i)}$ and $a_{\\pi}(i\\!-\\!1)$ , which is $|R_{\\pi(i)}-R_{\\pi(i-1)}|$ . We deduce that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|r(a_{\\pi(i)},A_{i}^{\\pi})-r(a_{\\pi(i-1)},A_{i}^{\\pi})|\\le|R_{\\pi(i)}-R_{\\pi(i-1)}|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le|R_{\\pi(i)}-\\widehat{R}_{\\pi(i)}|+|R_{\\pi(i-1)}-\\widehat{R}_{\\pi(i-1)}|+|\\widehat{R}_{\\pi(i)}-\\widehat{R}_{\\pi(i-1)}|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\eta_{\\pi(i-1)}^{\\Delta}+\\eta_{\\pi(i)}^{\\Delta}+\\widehat{R}_{\\pi(i)}-\\widehat{R}_{\\pi(i-1)}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used the triangle inequality and that $\\widehat{R}_{\\pi(i)}\\geq\\widehat{R}_{\\pi(i-1)}$ . ", "page_idx": 24}, {"type": "text", "text": "We move now to the proof of our lower bound in the pointer prediction model, by reducing the problem of sorting with positional predictions to the design of a learning-augmented priority queue with pointer predictions. ", "page_idx": 24}, {"type": "text", "text": "D.2.1 Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. Assume that there exists an implementation of a priority queue $\\mathcal{Q}$ augmented with pointer predictions, supporting ExtractMin with $O(1)$ comparisons and the insertion of any new key $u$ with $o(\\log\\vec{\\eta}(u,\\mathcal{Q}))$ comparisons. This means that, regardless of the history of operations made on $\\mathcal{Q}$ , the number of comparisons used by ExtractMin is at most a constant $C$ , and for any $\\xi\\geq1$ , inserting a key $u$ such that $\\log(\\vec{\\eta}(u,\\mathcal{Q})+2)\\leq\\xi$ requires at most $\\varepsilon(\\xi)\\xi$ comparisons, where $\\varepsilon(\\cdot)$ is a positive function satisfying $\\begin{array}{r}{\\operatorname*{lim}_{\\xi\\to\\infty}\\varepsilon(\\xi)\\,=\\,0}\\end{array}$ . We will show that the existence of such a priority queue contradicts the impossibility result of sorting with positional predictions. ", "page_idx": 24}, {"type": "text", "text": "Let $1\\leq\\xi\\leq O(\\log n)$ , $\\widehat{R}$ a fixed permutation of $[n]$ , and $A$ an arbitrary instance from the set $\\mathcal{T}_{n}(\\widehat{R},\\xi)$ defined in (12). Let $\\pi$ be the permutation satisfying the property of Lemma D.1, and conside r the sorting algorithm which inserts the elements of $A$ in $\\mathcal{Q}$ in the order given by $\\pi$ , then extracts the minimum repeatedly until $\\mathcal{Q}$ is emptied. Let us denote by $\\mathcal{Q}^{i}$ the state of the $\\mathcal{Q}$ after $i$ insertions. Upon the insertion of $a_{\\pi(i)}$ , the algorithm uses ${\\widehat{\\operatorname{Pred}}}(a_{\\pi(i)},\\mathcal{Q}^{i-1})=a_{\\pi(i-1)}$ as a pointer prediction. By (3), the error of this prediction is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\vec{\\eta}(a_{\\pi(i)},\\mathcal{Q}^{i-1})=|r(a_{\\pi(i)},A_{i-1}^{\\pi})-r(a_{\\pi(i-1)},A_{i-1}^{\\pi})|\\ ,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $A_{i-1}^{\\pi}=\\{a_{\\pi(j)}\\}_{j<i}$ , and by Lemma D.1, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\vec{\\eta}(a_{\\pi(i)},\\mathcal{Q}^{i-1})\\leq\\eta_{\\pi(i-1)}^{\\Delta}+\\eta_{\\pi(i)}^{\\Delta}+\\widehat R_{\\pi(i)}-\\widehat R_{\\pi(i-1)}\\ .\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$\\widehat{R}$ is a permutation of $[n]$ , hence $\\widehat{R}_{\\pi(i)}=\\widehat{R}_{\\pi(i-1)}+1$ by definition of $\\pi$ . Moreover, $A\\in{\\mathcal{Z}}_{n}(\\xi)$ , thus $\\operatorname*{max}(\\log(\\eta_{\\pi(i-1)}^{\\Delta}+2),\\log(\\eta_{\\pi(i)}^{\\Delta}+2))\\leq\\xi$ , and it follows that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log(\\vec{\\eta}(a_{\\pi(i)},\\mathcal{Q}^{i-1})+2)\\leq\\log(\\eta_{\\pi(i-1)}^{\\Delta}+\\eta_{\\pi(i)}^{\\Delta}+3)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\log(\\eta_{\\pi(i-1)}^{\\Delta}+2)+\\log(\\eta_{\\pi(i)}^{\\Delta}+2)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\xi\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second inequality is a consequence of $\\log(\\alpha+\\beta)\\leq\\log(\\alpha)+\\log(\\beta)$ for all $\\alpha,\\beta\\ge2$ . ", "page_idx": 25}, {"type": "text", "text": "Therefore, all the insertions into $\\mathcal{Q}$ require at most $(2\\xi\\cdot\\varepsilon(2\\xi))$ comparisons, and the total number of comparisons $T$ used to sort $A$ is at most ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle T\\leq{n}\\left(2{\\xi}\\cdot{\\varepsilon}(2{\\xi})+C\\right)}\\\\ {\\displaystyle\\quad={n}{\\xi}\\left(2\\cdot{\\varepsilon}(2{\\xi})+\\frac{C}{{\\xi}}\\right)}\\\\ {\\displaystyle\\quad={o}({n}{\\xi})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This means that any instance in $\\mathcal{Z}_{n}(\\xi)$ can be sorted using $o(n\\xi)$ comparisons, which contradicts the lower bound on sorting algorithms augmented with positional predictions. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "D.2.2 Rank prediction and dirty comparisons model ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In the rank prediction model, the total number of inserted keys is $N=n$ . Denote by $\\eta_{i}=\\eta(a_{i},A)$ for all $i\\,\\in\\,[n]$ . If there is a data structure $\\mathcal{Q}$ not satisfying the lower bound of Theorem 3.4 for positional predictions, then we can use it for sorting $A$ . Let $1\\leq\\xi\\leq O(\\log n)$ . Similarly to the proof for pointer predictions, ifR  is a permutation of $[n]$ , using $\\mathcal{Q}$ for sorting any instance $A\\in{\\mathcal{Z}}_{n}(\\xi)$ requires at most $T=o(n\\xi)$ comparisons, which contradicts the lower bound on learning-augmented sorting algorithms. ", "page_idx": 25}, {"type": "text", "text": "The same arguments, combined with (13), give the result also for the dirty comparison model. ", "page_idx": 25}, {"type": "text", "text": "E Applications ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "E.1 Sorting ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In the case where the algorithm is given offline access to the sequence $A=(a_{1},\\ldots,a_{n})$ to sort and to the rank predictions $(\\widehat{R}(a_{i}))_{i\\in[n]}$ , we argue that pointer predictions can be used to sort the sequence within a $\\begin{array}{r}{O(\\sum_{i\\in[n]}\\log(\\eta_{i}^{\\Delta}+2))}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "With offline rank predictions, by Lemma D.1, the algorithm can construct a permutation $\\pi$ of $[n]$ satisfying $\\widehat{R}(a_{\\pi(1)})\\leq\\ldots\\leq\\widehat{R}(a_{\\pi(n)})$ , and we showed in the proof of Theorem 3.4, in (14), that inserting the elements of $A$ into the priority queue in the order given by $\\pi$ , then taking each inserted element $a_{\\pi(i)}$ as pointer prediction for the following one $a_{\\pi}(i\\!+\\!1)$ , yields a pointer prediction error of ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\vec{\\eta}(a_{\\pi(i)},\\mathcal{Q}^{i-1})\\leq\\eta_{\\pi(i-1)}^{\\Delta}+\\eta_{\\pi(i)}^{\\Delta}+\\widehat R_{\\pi(i)}-\\widehat R_{\\pi(i-1)}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for all $i\\in\\{2,\\ldots,n\\}$ . By Theorem 3.1, the runtime for inserting $a_{\\pi(i)}$ using this pointer prediction is $O(\\log\\vec{\\eta}(a_{\\pi(i)},\\mathcal{Q}^{i-1}))$ . The total time for inserting all the elements into the priority queue is ", "page_idx": 25}, {"type": "text", "text": "therefore at most proportional to ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=2}^{N}\\log(\\eta(a_{\\pi(i)},Q^{t-1})+2)\\le\\displaystyle\\sum_{i=2}^{N}\\log(\\eta_{\\pi(i-1)}^{\\Delta}+\\eta_{\\pi(i)}^{\\Delta}+\\hat{R}_{\\pi(i-1)}\\cdot\\hat{R}_{\\pi(i-1)}+2)}\\\\ &{\\le\\displaystyle\\sum_{i=2}^{N}\\log((\\eta_{\\pi(i-1)}^{\\Delta}+2)+(\\eta_{\\pi(i)}^{\\Delta}+2)+(\\hat{R}_{\\pi(i)}-\\hat{R}_{\\pi(i-1)}+2))}\\\\ &{\\le\\displaystyle\\sum_{i=2}^{N}\\left(\\log(\\eta_{\\pi(i-1)}^{\\Delta}+2)+\\log(\\eta_{\\pi(i)}^{\\Delta}+2)+\\log(\\hat{R}_{\\pi(i-1)}-\\hat{R}_{\\pi(i-1)}+2)\\right)}\\\\ &{\\le2\\displaystyle\\sum_{i=1}^{N}\\log(\\eta_{\\pi(i)}^{\\Delta}+2)+\\displaystyle\\sum_{i=2}^{N}(\\hat{R}_{\\pi(i)}-\\hat{R}_{\\pi(i-1)})+n}\\\\ &{\\le2\\displaystyle\\sum_{i=1}^{N}\\log(\\eta_{\\pi(i)}^{\\Delta}+2)+\\hat{R}_{\\pi(i)}}\\\\ &{=\\displaystyle\\sum_{i=1}^{N}\\log(\\eta_{\\pi(i)}^{\\Delta}+2)+\\hat{R}_{\\pi(i)}}\\\\ &{=O\\left(\\displaystyle\\sum_{i\\in[n]}\\log(\\eta_{i}^{\\Delta}+2)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The third inequality results from inequation $\\log(\\alpha\\!+\\!\\beta\\!+\\!\\gamma)\\le\\log\\alpha\\!+\\!\\log\\beta\\!+\\!\\log\\gamma$ for all $\\alpha,\\beta,\\gamma\\geq2$ , and the subsequent inequality follows from $\\log(\\alpha+1)\\leq\\alpha$ for all $\\alpha\\geq0$ . Finally, to complete the sorting algorithm, the minimum is repeatedly extracted from the priority queue until is it empty, and each ExtractMin only takes $O(1)$ time in the skip list. ", "page_idx": 26}, {"type": "text", "text": "E.2 Dijkstra\u2019s algorithms ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We give in this section more details on the complexity of Dijkstra\u2019s algorithm using our priority queue augmented with rank predictions. ", "page_idx": 26}, {"type": "text", "text": "Consider any priority queue implementation, having a time complexity $T_{\\mathrm{{Insert}}}$ for insertion, $T_{\\mathrm{ExtractMin}}$ for extracting the minimum, and TDecreaseKey for decreasing the key of an element. There are two possible implementations of Dijkstra\u2019s algorithm with priority queues. The first one only uses the operations Insert and ExtractMin, and the maximum number of inserted elements is $m+1$ , which yields a complexity of $O(m T_{\\mathrm{Insert}}+m T_{\\mathrm{ExtractMin}})$ . The second implementation, using also the DecreaseKey operation, yields a complexity of $O(n(T_{\\mathrm{Insert}}+T_{\\mathrm{ExtractMin}})+m T_{\\mathrm{DecreaseKey}})$ . ", "page_idx": 26}, {"type": "text", "text": "Recalling that the number of edges is at most $n^{2}$ , using a binary heap in any of both implementations gives a total runtime of $O(m\\,\\mathrm{{iog}}\\,n)$ . On the other hand, using a Fibonacci heap in the second implementation yields a runtime of $O(n\\log n+m)$ . ", "page_idx": 26}, {"type": "text", "text": "In the rank predictions model, we consider that the priority queue only uses Insert and ExtractMin operations, hence we use the first implementation. Denoting by $\\{d_{i}\\}_{i\\in[m]}$ the $m$ distinct keys that are inserted, these keys are only revealed online to the priority queue. If they are accompanied by predictions $(\\widehat{R}(d_{i}))_{i}$ of their ranks, then each insertion requires $O(\\log\\log m+\\log(\\operatorname*{max}_{i\\in[m+1]}|R(d_{i})-$ $\\widehat{R}(d_{i})|+2)\\rangle$ ) amortized time and $O(\\log\\operatorname*{max}_{i\\in[m+1]}|R(d_{i})-\\widehat{R}(d_{i})|)$ comparisons, while extractions only require a constant amortized time each. The total runtime is therefore ", "page_idx": 26}, {"type": "equation", "text": "$$\nO\\big(m\\log\\log n+m\\log\\operatorname*{max}_{i\\in[m+1]}|R(d_{i})-\\widehat{R}(d_{i})|\\big)\\ ,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and the total number of comparisons is $O(m\\log(\\operatorname*{max}_{i\\in[m+1]}|R(d_{i})-\\widehat{R}(d_{i})|)).$ . ", "page_idx": 26}, {"type": "text", "text": "F Additional experiments ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "F.1 Sorting ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The problem of sorting with similar prediction models have been studied in Bai and Coester [2023], hence we numerically compare sorting using our learning-augmented priority queue (LAPQ) with their sorting algorithms. To run their algorithms, we used the code provided by Bai and Coester ", "page_idx": 26}, {"type": "text", "text": "[2023] in their paper. We give here additional experimental results for the class and the decay setting, for smaller values of $n$ . ", "page_idx": 27}, {"type": "image", "img_path": "1ATLLgvURu/tmp/07bb2a1837ddca2b99ef12c92cfff03f8f6163bd8b8aba547960efc23dd24b2f.jpg", "img_caption": ["Figure 6: Sorting with rank predictions in the class setting, for $n\\in\\{1000,10000,100000\\}$ . "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "1ATLLgvURu/tmp/b17e878dd0952c1a2c3efef449829b83421e1c15e1e570527e6f9b92e7d1e9bd.jpg", "img_caption": ["Figure 7: Sorting with rank predictions in the decay setting, for $n\\in\\{1000,10000,100000\\}$ . "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "F.2 Dijkstra\u2019s algorithm ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Considering the same experimental setting presented in Section 5, Figures 8 and 9 show the obtained results for the cities of Brussels, Paris, New York, and London, which have. The numbers of nodes $n$ and edges $m$ in each city graph are indicated in the figures. ", "page_idx": 27}, {"type": "image", "img_path": "1ATLLgvURu/tmp/816b1ea4d8d5208ab092120aeec8b4a5137937b18f9c5e261f7471e840a67b79.jpg", "img_caption": ["Figure 8: Dijkstra\u2019s algorithm on city maps with class predictions "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "1ATLLgvURu/tmp/9b701202d8c4336ddeac3ab50a70be8799010eb4f5cf97bf2a535c3063e3b542.jpg", "img_caption": ["Figure 9: Dijkstra\u2019s algorithm on city maps with decay predictions "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Simulations on Poisson Voronoi Tesselations We also evaluate the performance of Dijkstra\u2019s algorithm with our LAPQ on synthetic random graphs. For this, we use Poisson Voronoi Tessellations (PVT), which are a commonly used random graph model for street systems Gloaguen et al. [2006], Gloaguen and Cali [2018], Benomar et al. [2022a,b]. ", "page_idx": 28}, {"type": "text", "text": "PVTs are random graphs created by sampling an integer $N$ from a Poisson distribution with parameter $n\\geq1$ . Subsequently, $N$ points, termed \"seeds,\" are uniformly chosen at random within a two-dimensional region $I$ , typically $[0,1]^{2}$ . A Voronoi diagram is then generated based on these seeds. ", "page_idx": 28}, {"type": "text", "text": "This process results in a planar graph where edges represent the boundaries between the cells of the Voronoi diagram, and the nodes are their intersections. For $I=[0,1]^{2}$ , the expected number of nodes in this construction is $n$ . ", "page_idx": 28}, {"type": "image", "img_path": "1ATLLgvURu/tmp/9f807caa402d89553eaa8495c9d626414a24cf6c83ca913385084189c405c4a2.jpg", "img_caption": ["Figure 10: PVT with $n=100$ "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 10 provides a visualization of a PVT with $n=100$ . ", "page_idx": 28}, {"type": "text", "text": "We present the results in the class and the decay settings respectively in Figures 11 and 12. Similar to previous experiments with Dijkstra on city maps, these figures illustrate how the number of comparisons decreases when the LAPQ is augmented with node rank predictions or with the corresponding dirty comparator. We compare them with the number of comparisons induced by using a binary or Fibonacci heap, as well as with the number of comparisons of the LAPQ augmented with key rank predictions. ", "page_idx": 28}, {"type": "image", "img_path": "1ATLLgvURu/tmp/7a99effd21737076519dea19757f252effbe99303e7868e087f225adcfb3d24d.jpg", "img_caption": ["Figure 11: Dijkstra\u2019s algorithm on Poisson Voronoi Tesselation with class predictions "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "1ATLLgvURu/tmp/0a286d2f42a940e5a97ba9725a708bac5f929f824f32b237a91659f590036093.jpg", "img_caption": ["Figure 12: Dijkstra\u2019s algorithm on Poisson Voronoi Tesselation with decay predictions "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "The same observations regarding the performance improvement can be made, as in the previous experiments with city maps. However, in PVT tessellations, the performance of the LAPQ with key rank predictions surpasses even that of perfect node rank predictions. This is due to the PVTs having a more uniform structure across space. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The abstract indicates the scope of the paper: learning-augmented algorithms, and describes the particular problem we consider ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the difficulties we faced for designing a learning-augmented binary heap. For learning-augmented skip-lists, we discuss the limitations and possible improvements in the rank prediction model. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: for each theoretical result, all the needed assumptions are stated. The full proofs are provided in the appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: we give full details on our data structure implementation, and all the details (instances, libraries) needed for reproducing the results are precised in the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The full implementation of the learning-augmented priority queue is given in the supplementary material. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The setting is fully described in the experiments section. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: standard deviations are reported in all the figures. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our figures show the number of comparisons used in each experiment. The computer resources are not relevant. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper respects the Code of Ethics or NeurIPS ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper presents foundational/theoretical work in the field of learningaugmented algorithms. We do not feel that any potential societal consequences of our work must be specifically highlighted. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: learning-augmented algorithms, in general, pose no such risks. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We partially use existing open source code, which we clearly cite. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not release any new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not include such experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not include such experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]