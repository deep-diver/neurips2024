[{"type": "text", "text": "Derivative-enhanced Deep Operator Network ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuan Qiu, Nolan Bridges, Peng Chen Georgia Institute of Technology, Atlanta, GA 30332 {yuan.qiu, bridges, pchen402}@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The deep operator networks (DeepONet), a class of neural operators that learn mappings between function spaces, have recently been developed as surrogate models for parametric partial differential equations (PDEs). In this work we propose a derivative-enhanced deep operator network (DE-DeepONet), which leverages derivative information to enhance the solution prediction accuracy and provides a more accurate approximation of solution-to-parameter derivatives, especially when training data are limited. DE-DeepONet explicitly incorporates linear dimension reduction of high dimensional parameter input into DeepONet to reduce training cost and adds derivative loss in the loss function to reduce the number of required parameter-solution pairs. We further demonstrate that the use of derivative loss can be extended to enhance other neural operators, such as the Fourier neural operator (FNO). Numerical experiments validate the effectiveness of our approach. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Using neural networks to approximate the maps between functions spaces governed by parametric PDEs can be very beneficial in solving many-query problems, typically arising from Bayesian inference, optimization under uncertainty, and Bayesian optimal experimental design. Indeed, once pre-trained on a dataset, neural networks are extremely fast to evaluate given unseen inputs, compared to traditional numerical methods like the finite element method. Recently various neural operators are proposed to enhance the learning capacity, with two prominent examples deep operator network (DeepONet) [1] and Fourier neural operator (FNO) [2], which are shown to be inclusive of each other in their more general settings [3, 4], see also their variants and other related neural operator architectures in [5, 6, 7, 8, 9, 10]. Though these work demonstrate to be successful in approximating the output function, they do not necessarily provide accurate approximation of the derivative of the output with respect to the input, which are often needed for many downstream tasks such as PDEconstrained optimization problems for control, inference, and experimental design [11, 12, 13, 14]. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we propose to enhance the performance of DeepONet through derivative-based dimension reduction for the function input inspired by [15, 16, 17, 18] and the incorporation of derivative information in the training to learn both the output and its derivative with respect to the input inspired by [19, 20]. These two derivative-enhanced approaches can significantly improve DeepONet\u2019s approximation accuracy for the output function and its directional derivative with respect to the input function, especially when the training samples are limited. We provide details on the computation of derivative labels of the solution of PDEs in a general form as well as the derivative-based dimension reduction to largely reduce the computational cost. We demonstrate the effectiveness of our proposed method (DE-DeepONet) compared to three other neural operators, including DeepONet, FNO, and derivative-informed neural operator (DINO) [20], in terms of both test errors and computational cost. In addition, we apply derivative learning to train the FNO and also compare its performance with other methods. The code for data generation, model training and inference, as well as configurations to reproduce the results in this paper can be found at https://github.com/qy849/DE-DeepONet. ", "page_idx": 0}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This section presents the problem setup, high-fidelity approximation using finite element for finite dimensional discretization, and the DeepONet architecture in learning the solution operator. ", "page_idx": 1}, {"type": "text", "text": "2.1 Problem setup ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $\\Omega\\subset\\mathbb{R}^{d}$ denote an open and bounded domain with boundary $\\partial\\Omega\\subset\\mathbb{R}^{d-1}$ , where the dimension $d=1,2,3$ . We consider a PDE of the general form defined in $\\Omega$ as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(m,u)=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "prescribed with proper boundary conditions. Here $m\\in V^{\\mathrm{in}}$ is an input parameter function defined in a separable Banach space $V^{\\mathrm{in}}$ with probability measure $\\nu$ and $u\\in V^{\\mathrm{out}}$ is the output as the solution of the PDE defined in a separable Banach space $V^{\\mathrm{out}}$ . Our goal is to construct a parametric model $\\hat{u}(m;\\theta)$ to approximate the solution operator that maps the parameter $m$ to the solution $u$ . ", "page_idx": 1}, {"type": "text", "text": "Once constructed, the parametric model $\\hat{u}(m;\\theta)$ should be much more computationally efficient to evaluate compared to solving the PDE with high fidelity approximation. ", "page_idx": 1}, {"type": "text", "text": "2.2 High fidelity approximation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "For the high fidelity approximation of the solution, we consider using a finite element method [21] in this work. We partition the domain $\\Omega$ into a finite set of subregions, called cells or elements. Collectively, these cells form a mesh of the domain $\\Omega$ . Let $h$ represent the diameter of the largest cell. We denote $V_{h}^{\\mathrm{in}}$ indexed by $h$ as the finite element space for the approximation of the input space $V^{\\mathrm{in}}$ with Lagrange basis $\\{\\phi_{1}^{\\mathrm{in}},\\cdot\\cdot\\cdot\\,,\\phi_{N_{h}^{\\mathrm{in}}}^{\\mathrm{in}}\\}$ of dimension $N_{h}^{\\mathrm{in}}$ such that $\\phi_{i}^{\\mathrm{in}}(x^{(j)})\\,=\\,\\delta_{i j}$ at the finite eflineitmee enlte nmoednet $\\boldsymbol{x}^{(j)}$ c, e wfiotrh $\\delta_{i j}$ abpepinrog xtihme atKiroonn oefc kthere  dsoelltuat ifounn sctpiaocne. ilwairtlhy , bawsei sd $V_{h}^{\\mathrm{out}}$ $V^{\\mathrm{out}}$ $\\{\\phi_{1}^{\\mathrm{out}},\\cdot\\cdot\\cdot\\cdot,\\phi_{N_{h}^{\\mathrm{out}}}^{\\mathrm{out}}\\}$ of dimension $N_{h}^{\\mathrm{out}}$ . Note that for the approximation to be high fidelity, $N_{h}^{\\mathrm{in}}$ and $N_{h}^{\\mathrm{out}}$ are often very large. To this end, we can write the high fidelity approximation of the input and output functions as ", "page_idx": 1}, {"type": "equation", "text": "$$\nm_{h}(\\boldsymbol{x})=\\sum_{i=1}^{N_{h}^{\\mathrm{in}}}m_{i}\\phi_{i}^{\\mathrm{in}}(\\boldsymbol{x})\\;\\;\\mathrm{and}\\;\\;u_{h}(\\boldsymbol{x})=\\sum_{i=1}^{N_{h}^{\\mathrm{out}}}u_{i}\\phi_{i}^{\\mathrm{out}}(\\boldsymbol{x}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with coefficient vectors $\\pmb{m}=(\\b{m}_{1},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\b{m}_{N_{h}^{\\mathrm{in}}})^{T}\\in\\mathbb{R}^{N_{h}^{\\mathrm{in}}}$ and $\\pmb{u}=(u_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}u_{N_{h}^{\\mathrm{out}}})^{T}\\in\\mathbb{R}^{N_{h}^{\\mathrm{out}}}$ , whose entries are the nodal values of $m$ and $u$ at the corresponding nodes. ", "page_idx": 1}, {"type": "text", "text": "2.3 DeepONet ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We briefly review the DeepONet architecture [1] with a focus on learning the solution operator of the PDE in Equation (1). To predict the evaluation of solution function $u$ at any point $x\\in\\Omega\\cup\\partial\\Omega$ for any given input function $m$ , [1] design a network architecture that comprises two separate neural networks: a trunk net $t(\\cdot\\,\\,;\\theta_{t})$ , which takes the coordinate values of the point $x$ at which we want to evaluate the solution function as inputs, and a branch net $b(\\cdot\\cdot;\\theta_{b})$ , which takes the vector $\\mathbf{\\nabla}m$ encoding the parameter function $m$ as inputs. In [1], the vector $\\mathbf{\\nabla}m$ is the function evaluations at a finite set of fixed points {x(j)}jN =h1 $\\{\\boldsymbol{x}^{(j)}\\}_{j=1}^{N_{h}^{\\mathrm{in}}}\\subseteq\\Omega\\cup\\partial\\Omega$ , that is, $m=(m(x^{(1)}),\\cdot\\cdot\\cdot\\,,m(x^{(N_{h}^{\\mathrm{in}})}))^{T}$ , which corresponds to coefficient vector in the finite element approximation with Lagrange basis at the same nodes. If the solution function is scalar-valued, then both neural networks output vectors of the same dimensions. The prediction is obtained by taking the standard inner product between these two vectors and (optionally) adding a real-valued bias parameter, i.e, ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{u}(\\pmb{m};\\pmb{\\theta})(x)=\\langle b(\\pmb{m};\\pmb{\\theta}_{b}),t(x;\\pmb{\\theta}_{t})\\rangle+\\theta_{\\mathrm{bias}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with $\\theta~=~(\\theta_{b},\\theta_{t},\\theta_{\\mathrm{bias}})$ . If the solution $u$ is vector-valued of $N_{u}$ components, i.e., $u\\ =$ $(u^{(1)},\\dots,u^{(N_{u})})$ , as in our experiments, we can use one of the four approaches in [3] to construct the DeepONet. Specifically, for each solution component, we use the same outputs of branch net with dimension $N_{b}$ and different corresponding groups of outputs of trunk net to compute the inner product. More precisely, the solution $u^{(i)}$ of component $i=1,\\dots,N_{u}$ , is approximated by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{u}^{(i)}(\\pmb{m};\\pmb{\\theta})(x)=\\langle b(\\pmb{m};\\pmb{\\theta}_{b}),t^{(i)}(x;\\pmb{\\theta}_{t})\\rangle+\\theta_{\\mathrm{bias}}^{(i)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $t^{(i)}(x;\\theta_{t})=t(x;\\theta_{t})[(i-1)N_{b}+1:i N_{b}]$ , the vector slice of $t(x;\\theta_{t})$ with indices ranging from $(i-1)N_{b}+1$ to $i N_{b}$ , $i=1,\\dots,N_{u}$ . ", "page_idx": 2}, {"type": "text", "text": "For this construction, the outputs of the branch net can be interpreted as the coefficients of the basis learned through the trunk net. By partitioning the outputs of the trunk net into different groups, we essentially partition the basis functions used for predicting different components of the solution. The DeepONet is trained using dataset $\\bar{\\boldsymbol{D}}=\\{(\\bar{m^{(i)}},u^{(i)})\\}_{i=1}^{N_{\\mathcal{D}}}$ with $N_{D}$ samples, where $m^{(i)}$ are random samples independently drawn from $\\nu$ and $u^{(i)}$ are the solution of the PDE (with slight abuse of notation from the vector-valued solution) at $m^{(i)}$ , $i=1,\\dots,N_{D}$ . ", "page_idx": 2}, {"type": "text", "text": "3 DE-DeepONet ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The DeepONet uses the input parameter and output solution pairs as the labels for the model training. The approximation for the derivative of the solution with respect to the parameter (and the coordinate) are not necessarily accurate. However, in many downstream tasks such as Bayesian inference and experimental design, the derivatives are required. We consider incorporating the the Fr\u00e9chet derivative $d\\bar{u}(m;\\cdot)$ for the supervised training of the DeepONet, which we call derivative-enhanced DeepONet (DE-DeepONet). By doing this we hope the optimization process can improve the neural network\u2019s ability to predict the derivative of the output function with respect to the input function. Let $\\theta$ denote the trainable parameters in the DE-DeepONet. We propose to minimize the loss function ", "page_idx": 2}, {"type": "equation", "text": "$$\nL(\\theta)=\\lambda_{1}\\mathbb{E}_{m\\sim\\nu}\\|u(m)-\\hat{u}(m)\\|_{L^{2}(\\Omega)}^{2}+\\lambda_{2}\\mathbb{E}_{m\\sim\\nu}\\|d u(m;\\cdot)-d\\hat{u}(m;\\cdot)\\|_{\\mathrm{HS}}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the $L^{2}(\\Omega)$ norm of a square integrable function $f$ is defined as $\\begin{array}{r l}{\\left\\lVert f\\right\\rVert_{L^{2}(\\Omega)}^{2}}&{{}=}\\end{array}$ $\\textstyle(\\int_{\\Omega}\\|f(x)\\|_{2}^{2}\\;\\mathrm{d}x)^{1/2}$ , the Hilbert\u2013Schmidt norm of an operator $T:H\\rightarrow H$ that acts on a Hilbert space $H$ is defined as $\\begin{array}{r}{\\|T\\|_{\\mathrm{HS}}^{2}=\\sum_{i\\in I}\\|T e_{i}\\|_{H}^{2}}\\end{array}$ , where $\\{e_{i}\\}_{i\\in I}$ is an orthonormal basis of $H$ . Here, $\\lambda_{1},\\lambda_{2}>0$ are hyperparameters  that balance different loss terms. ", "page_idx": 2}, {"type": "text", "text": "The main challenge of minimizing the loss function (3) in practice is that with high fidelity approximation of the functions $m$ and $u$ using high dimensional vectors $\\mathbf{\\nabla}m$ and $\\textbf{\\em u}$ , the term $\\lVert\\hat{d u}(m;\\cdot)-d\\hat{u}(m;\\cdot)\\rVert_{\\mathrm{HS}}^{2}$ (approximately) becomes $\\|\\nabla_{m}\\pmb{u}-\\nabla_{m}\\hat{\\pmb u}\\|_{F}^{2}$ , where the Frobenius norm of a matrix $M\\in\\mathbb{R}^{m\\times n}$ is defined as $\\begin{array}{r}{\\|\\dot{M}\\|_{F}^{2}=\\sum_{i=1}^{m}\\sum_{j=1}^{n}M_{i j}^{2}}\\end{array}$ . It is a critical challenge to both compute and store the Jacobian matrix at each sample as well as to load and use it for training since it is often very large with size $N_{h}^{\\mathrm{out}}\\times N_{h}^{\\mathrm{in}}$ , with $N_{h}^{\\mathrm{in}},N_{h}^{\\mathrm{out}}\\gg1$ . ", "page_idx": 2}, {"type": "text", "text": "To tackle this challenge, we employ dimension reduction for the high dimensional input vector $\\mathbf{\\nabla}m$ . The reduced representation of $\\mathbf{\\nabla}m$ is given by projecting $\\mathbf{\\nabla}m$ into a low dimensional linear space spanned by a basis $\\psi_{1}^{\\mathrm{in}},\\dots,\\psi_{r}^{\\mathrm{in}}\\in\\mathbb{R}^{N_{h}^{\\mathrm{in}}}$ , with $r\\ll N_{h}^{\\mathrm{in}}$ , that is, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{m}}=(\\langle\\pmb{\\psi}_{1}^{\\mathrm{in}},\\pmb{m}\\rangle,\\pmb{\\ldots},\\langle\\pmb{\\psi}_{r}^{\\mathrm{in}},\\pmb{m}\\rangle)^{T}\\in\\mathbb{R}^{r}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "To better leverage the information of both the input probability measure $\\nu$ and the map $u:m\\rightarrow u(m)$ , we consider the basis generated by active subspace method (ASM) using derivatives [22]. ASM identifies directions in the input space that significantly affect the output variance, or in which the output is most sensitive. See Section 3.3 for the detailed construction. In this case, the term $\\mathbb{E}_{m\\sim\\nu}\\|d u(m;\\cdot)-$ du\u02c6(m; \u00b7)\u22252HS can be approximated byN11N2 iN=11 jN=21 \u2225\u2207m u(m(i))(x(j)) \u2212\u2207 mu\u02c6(m(i))(x(j))\u222522 with a small amount of functions sampled from input probability measure $\\nu$ and points in the domain $\\Omega$ . Note that $\\nabla_{\\tilde{m}}\\hat{u}(m^{(i)})(x^{(j)})$ is vector of size $r$ , which is computationally feasible. For comparison, we also condu ct experiments if the basis is the most commonly used Karhunen\u2013Lo\u00e8ve Expansion (KLE) basis. ", "page_idx": 2}, {"type": "text", "text": "We next formally introduces our DE-DeepONet, which uses dimension reduction for the input of the branch net and incorporates its output-input directional derivative labels as additional soft constraints into the loss function. ", "page_idx": 2}, {"type": "text", "text": "3.1 Model architecture ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We incorporate dimension reduction into DeepONet to construct a parametric model for approximating the solution operator. Specifically, if the solution is scalar-valued (the vector-valued case can be constructed similar to (2)), the prediction is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{u}(\\pmb{m};\\pmb{\\theta})(x)=\\langle b(\\widetilde{\\pmb{m}};\\pmb{\\theta}_{b}),t(x;\\pmb{\\theta}_{t})\\rangle+\\theta_{\\mathrm{bias}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta\\,=\\,(\\theta_{b},\\theta_{t},\\theta_{\\mathrm{bias}})$ are the parameters to be learned. The branch net $b(\\cdot;\\theta_{b})$ and trunk net $t(\\cdot;\\theta_{t})$ can be chosen as an MLP, ResNet, etc. Note that the branch net takes a small vector of the projected parameter as input. We also apply the Fourier feature embeddings [5], defined as $\\gamma(x)=[\\cos(B x),\\sin(B x)]$ , to the trunk net, where each entry in $B\\in\\mathbb{R}^{m\\times d}$ is sampled from a Gaussian distribution ${\\mathcal{N}}(0,\\sigma^{2})$ and $m\\in\\mathbb{N}^{+},\\sigma\\in\\mathbb{R}^{+}$ are hyper-parameters. ", "page_idx": 3}, {"type": "text", "text": "3.2 Loss function ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In practical training of the DE-DeepONet, we formulate the loss function as follows ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}(\\boldsymbol{\\theta})=\\frac{\\lambda_{1}}{N_{\\mathscr{D}}}\\sum_{i=1}^{N_{\\mathscr{D}}}\\mathrm{err}(\\{(\\hat{u}(\\pmb{m}^{(i)};\\boldsymbol{\\theta})(\\boldsymbol{x}^{(j)}),\\boldsymbol{u}(\\boldsymbol{m}^{(i)})(\\boldsymbol{x}^{(j)}))\\}_{j=1}^{N_{x}})}}\\\\ {{\\displaystyle\\qquad+\\,\\frac{\\lambda_{2}}{N_{\\mathscr{D}}}\\sum_{i=1}^{N_{\\mathscr{D}}}\\mathrm{err}(\\{(\\nabla_{m}\\hat{u}(\\pmb{m}^{(i)};\\boldsymbol{\\theta})(\\boldsymbol{x}^{(j)})\\Psi^{\\mathrm{in}},\\Phi^{\\mathrm{out}}(\\boldsymbol{x}^{(j)})(\\nabla_{m}\\pmb{u}(\\boldsymbol{m}^{(i)})\\Psi^{\\mathrm{in}})\\}_{j=1}^{N_{x}}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\{x^{(j)}\\}_{j=1}^{N_{x}}$ are the nodes of the mesh, $\\Psi^{\\mathrm{in}}=(\\psi_{1}^{\\mathrm{in}}|\\cdot\\cdot\\cdot|\\psi_{r}^{\\mathrm{in}})$ is the matrix collecting the nodal values of the reduced basis of the input function space, and $\\Phi^{\\mathrm{out}}(x)=(\\phi_{1}^{\\mathrm{out}}(x),\\dots,\\phi_{N_{h}^{\\mathrm{out}}}^{\\mathrm{out}}(x))$ is the vector-valued function consisting of the finite element basis functions of the output function space. The $\\mathrm{err}(\\{(\\pmb{a}^{(i)},\\pmb{b}^{(i)})\\}_{i=1}^{n})$ denotes the relative error between any two groups of vectors $\\pmb{a}^{(i)}$ and $\\boldsymbol{b}^{(i)}$ , computed as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{err}(\\{(a^{(i)},b^{(i)})\\}_{i=1}^{n})=\\frac{(\\sum_{i=1}^{n}\\|\\pmb{a}^{(i)}-\\pmb{b}^{(i)}\\|_{2}^{2})^{1/2}}{\\varepsilon+(\\sum_{i=1}^{n}\\|\\pmb{b}^{(i)}\\|_{2}^{2})^{1/2}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\varepsilon>0$ is some small positive number to prevent the fraction dividing by zero. ", "page_idx": 3}, {"type": "text", "text": "In the following, we explain how to compute different loss terms in Equation (5) ", "page_idx": 3}, {"type": "text", "text": "\u2022 The first term is for matching the prediction of the parametric model $\\hat{u}(m^{(i)};\\theta)$ evaluated at any set of points $\\{\\boldsymbol{x}^{(j)}\\}_{j=1}^{N_{x}}\\subseteq\\Omega\\cup\\partial\\Omega$ with the high fidelity solution $u(m^{(i)})$ evaluated at the same points. The prediction $\\hat{u}(\\pmb{m}^{(i)};\\pmb{\\theta})(\\pmb{x}^{(j)})$ is straightforward to compute using Equation (4). This involves passing the reduced branch inputs $\\widetilde{m}^{\\left(i\\right)}$ and the coordinates of point $x^{(j)}$ into the branch net and trunk net, respectively. Th e  label $u(m^{(i)})(x^{(j)})$ is obtained using finite element method solvers.   \n\u2022 The second term is for learning the directional derivative of the evaluation $u(x^{(j)})$ with respect to the input function $m$ , in the direction of the reduced basis $\\psi_{1}^{\\mathrm{in}},\\dots,\\psi_{r}^{\\mathrm{in}}$ . It can be shown that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{m}\\widehat{u}(\\pmb{m}^{(i)};\\theta)(x^{(j)})\\Psi^{\\mathrm{in}}=\\nabla_{\\widetilde{m}}\\widehat{u}(\\pmb{m}^{(i)};\\theta)(x^{(j)}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thus, the derivative of the outputs of the parametric model can be computed as the partial derivatives of the output with respect to the input of the branch net via automatic differentiation. On the other hand, the derivative labels ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Phi^{\\mathrm{out}}(x^{(j)})(\\nabla_{m}u(m^{(i)})\\Psi^{\\mathrm{in}})=(d u(m^{(i)};\\psi_{1}^{\\mathrm{in}})(x^{(j)}),\\dots,d u(m^{(i)};\\psi_{r}^{\\mathrm{in}})(x^{(j)}))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "are obtained by first computing the Gateaux derivatives $d u(m^{(i)};\\psi_{1}^{\\mathrm{in}}),\\dots,d u(m^{(i)};\\psi_{r}^{\\mathrm{in}})$ and then evaluating them at $x^{(j)}$ . See Appendix B.3 for details about the computation of the G\u00e2teaux derivative $d u(m;\\psi)$ . \u2022 We initialize the loss weights $\\lambda_{1}=\\lambda_{2}=1$ and choose a loss balancing algorithm called the self-adaptive learning rate annealing algorithm [23] to update them at a certain frequency. This ensures that the gradient of each loss term have similar magnitudes, thereby enabling the neural network to learn all these labels simultaneously. ", "page_idx": 3}, {"type": "text", "text": "Remark. For the training, the computational cost of the second term of the loss function (and its gradients) largely depends on the number of points used in each iteration. To reduce the computational and especially memory cost, we can use a subset of points, $N_{x}^{b a t c h}=\\alpha N_{x}$ , where $\\alpha$ is small number between 0 and 1 (e.g., $\\alpha=0.1$ in our experiments), in a batch of functions, though their locations could vary among different batches in one epoch. We find that this approach has little impact on the prediction accuracy of the model when $N_{x}$ is large enough. ", "page_idx": 3}, {"type": "text", "text": "3.3 Dimension reduction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Throughout the paper, we assume that the parameter functions $m$ are independently drawn from some Gaussian random field [24]. In particular, we consider the case where the covariance function is the Whittle-Mat\u00e9rn covariance function, that is, $m\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(\\bar{m},\\mathcal{C})$ , where $\\bar{m}$ is the (deterministic) mean function and $\\mathcal{C}=(\\delta I-\\gamma\\Delta)^{-2}$ ( $I$ is the identity and $\\Delta$ is the Laplacian) is an operator such that the square root of its inverse, $\\mathcal{C}^{-\\frac{1}{2}}$ , maps random function $(m-{\\bar{m}})$ to Gaussian white noise with unit variance [25]. The parameters $\\delta,\\gamma\\in\\mathbb{R}^{+}$ jointly control the marginal variance and correlation length. ", "page_idx": 4}, {"type": "text", "text": "We consider two linear projection bases for dimension reduction of the parameter function. ", "page_idx": 4}, {"type": "text", "text": "Karhunen\u2013Lo\u00e8ve Expansion (KLE) basis. The KLE basis is optimal in the sense that the mean-square error resulting from a finite representation of the random field $m$ is minimized [26]. It consists of eigenfunctions determined by the covariance function of the random field. Specifically, an eigenfunction $\\psi$ of the covariance operator $\\mathcal{C}=(\\delta I-\\gamma\\Delta)^{-2}$ satisfies the differential equation ", "page_idx": 4}, {"type": "equation", "text": "$$\nc\\psi=\\lambda\\psi.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When solved using the finite element method, Equation (6) is equivalent to the following linear system (See Appendix A.2 for the derivation) ", "page_idx": 4}, {"type": "equation", "text": "$$\nM^{\\mathrm{in}}(A^{\\mathrm{in}})^{-1}M^{\\mathrm{in}}(A^{\\mathrm{in}})^{-1}M^{\\mathrm{in}}\\psi=\\lambda M^{\\mathrm{in}}\\psi,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the $(i,j)$ -entries of matices $A^{\\mathrm{in}}$ and $M^{\\mathrm{in}}$ are given by ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{i j}^{\\mathrm{in}}=\\delta\\langle\\phi_{j}^{\\mathrm{in}},\\phi_{i}^{\\mathrm{in}}\\rangle+\\gamma\\langle\\nabla\\phi_{j}^{\\mathrm{in}},\\nabla\\phi_{i}^{\\mathrm{in}}\\rangle,\\quad M_{i j}^{\\mathrm{in}}=\\langle\\phi_{j}^{\\mathrm{in}},\\phi_{i}^{\\mathrm{in}}\\rangle.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, we recall that $V_{h}^{\\mathrm{in}}$ is the finite element function space of input function, $\\phi_{i}^{\\mathrm{in}},i=1,\\dots,N_{h}^{\\mathrm{in}}$ for the finite element basis of $V_{h}^{\\mathrm{in}}$ , and $\\langle\\cdot,\\cdot\\rangle$ for the $L^{2}(\\Omega)$ -inner product. We select the first $r$ (typically $r\\ll N_{h}^{\\mathrm{in}})$ eigenfunctions $\\psi_{1}^{\\mathrm{in}},\\ldots,\\psi_{r}^{\\mathrm{in}}$ corresponding to the $r$ largest eigenvalues for the dimension reduction. Let $\\Psi^{\\mathrm{in}}\\,=\\,(\\psi_{1}^{\\mathrm{in}}|\\,\\cdot\\,\\cdot\\,\\cdot|\\psi_{r}^{\\mathrm{in}})$ denote the corresponding nodal values of these eigenfunctions. Since the eigenvectors $\\psi_{i}^{\\mathrm{in}}$ are $M^{\\mathrm{in}}$ -orthogonal (or equivalently, eigenfunctions $\\psi_{i}^{\\mathrm{in}}$ are $L^{2}(\\Omega)$ -orthogonal), the reduced representation of $\\mathbf{\\nabla}m$ can be computed as $\\widetilde{\\pmb{m}}=(\\Psi^{\\mathrm{in}})^{T}M^{\\mathrm{in}}\\pmb{m}$ , that is, the coefficients of the low rank approximation of $m$ in the linear subs p ace spanned by the eigenfunctions $\\psi_{1}^{\\mathrm{in}},\\dots,\\psi_{r}^{\\mathrm{in}}$ . ", "page_idx": 4}, {"type": "text", "text": "Active Subspace Method (ASM) basis. The active subspace method is a gradient-based dimension reduction method that looks for directions in the input space contributing most significantly to the output variability [27]. In contrast to the KLE basis, the ASM basis is more computationally expensive. However, since the ASM basis captures sensitivity information in the input-output map rather than solely the variability of the input space, it typically achieves higher accuracy in predicting the output than KLE basis. We consider the case where the output is a multidimensional vector [22], representing the nodal values of the output function $u$ . The ASM basis $\\psi_{i}$ , $i=1,\\hdots,r$ , are the eigenfunctions corresponding to the $r$ largest eigenvalues of the generalized eigenvalue problem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}\\boldsymbol{\\psi}=\\lambda\\mathcal{C}^{-1}\\boldsymbol{\\psi},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the action of operator $\\mathcal{H}$ on function $\\psi$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}\\psi=\\mathbb{E}_{m\\sim\\nu(m)}[d^{*}u(m;d u(m;\\psi))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $d u(m;\\psi)$ is the G\u00e2teaux derivative of $u$ at $m\\in V_{h}^{\\mathrm{in}}$ in the direction of $\\psi\\in V_{h}^{\\mathrm{in}}$ , defined as $\\begin{array}{r}{\\operatorname*{lim}_{\\varepsilon\\to0}(u(m+\\varepsilon\\psi)-u(m))/\\varepsilon}\\end{array}$ , and $d^{*}u(m;\\cdot)$ is the adjoint of the operator $d u(m;\\cdot)$ . When solved using finite element method, Equation (8) is equivalent to the following linear system (See Appendix A.3 for the derivation) ", "page_idx": 4}, {"type": "equation", "text": "$$\nH\\psi=\\lambda C^{-1}\\psi,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the action of matrix $H$ on vector $\\psi$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\nH\\psi=\\mathbb{E}_{m\\sim\\nu(m)}[(\\nabla_{m}\\pmb{u})^{T}M^{\\mathrm{{out}}}(\\nabla_{m}\\pmb{u})\\psi],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the action of matrix $C^{-1}$ on vector $\\psi$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C^{-1}\\psi=A^{\\mathrm{in}}(M^{\\mathrm{in}})^{-1}A^{\\mathrm{in}}\\psi.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "wHehreen, $M^{\\mathrm{out}}$ ndge nEoqtueas ttihoen  (m1a0s)s,  mwaet roibx toaif nt hites  oluetftp uhta fnudn scitidoen t hsrpoauceg,h i .ceo., $M_{i j}^{\\mathrm{out}}=\\langle\\phi_{j}^{\\mathrm{out}},\\phi_{i}^{\\mathrm{out}}\\rangle$ . In practice, ", "page_idx": 5}, {"type": "equation", "text": "$$\n(\\langle\\mathcal{H}\\psi,\\phi_{1}^{\\mathrm{in}}\\rangle,\\dots,\\langle\\mathcal{H}\\psi,\\phi_{N_{h}^{\\mathrm{in}}}^{\\mathrm{in}}\\rangle)^{T}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and its right hand side through the matrix-vector multiplication in Equation (12) (See Appendix A.3 for details). Similar to the KLE case, let $\\Psi^{\\mathrm{in}}$ denote the nodal values of $r$ dominant eigenfunctions. Since the eigenvectors $\\psi_{i}^{\\mathrm{in}}$ are $C^{-1}$ -orthogonal, the reduced representation of $\\mathbf{\\nabla}m$ can be computed as $\\widetilde{\\pmb{m}}=(\\Psi^{\\mathrm{in}})^{T}C^{-1}\\pmb{m}$ . We use a scalable double pass randomized algorithm [28] implemented in hIPPYlib to solve the generalized eigenproblems Equation (7) and Equation (10). ", "page_idx": 5}, {"type": "text", "text": "To this end, we present the computation of the derivative label and the action of $\\mathcal{H}$ as follows. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Suppose the PDE in the general form of (1) is well-posed with a unique solution map from the input function $m\\,\\in\\,V^{i n}$ to the output function $u\\,\\in\\,V^{o u t}$ with dual $(V^{o u t})^{\\prime}$ . Suppose the PDE operator $\\mathcal{R}:V^{i n}\\times V^{o u t}\\to(V^{o u t})^{\\prime}$ is differentiable with derivatives $\\partial_{m}\\mathcal{R}:V^{i n}\\to(V^{o u t})^{\\prime}$ and $\\partial_{u}\\mathcal{R}:\\bar{V}^{o u t}\\to(V^{o u t})^{\\prime}$ , and in addition $\\partial_{u}\\mathcal{R}$ is invertible with invertible adjoint $(\\partial_{u}\\mathcal{R})^{*}$ . Then the directional derivative $p=d u(m;\\psi)$ for any function $\\psi\\in V^{i n}$ , and an auxillary variable $q$ such that $d^{*}u(m;p)=-(\\partial_{m}\\mathcal{R})^{*}q$ can be obtained as the solution of the linearized PDEs ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\partial_{u}\\mathcal{R})p+(\\partial_{m}\\mathcal{R})\\psi=0,}\\\\ {(\\partial_{u}\\mathcal{R})^{*}q=p.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof sketch. We perturb $m$ with $\\varepsilon\\psi$ for any small $\\varepsilon>0$ and obtain $\\mathcal{R}(m+\\varepsilon\\psi,u(m+\\varepsilon\\psi))=0$ . Using Taylor expansion to expand it to the first order and letting $\\varepsilon$ approach 0, we obtain Equation (13), where $p\\;=\\;d u(m;\\psi)$ . Next we compute $d^{*}u(m;p)$ . By Equation (13), we have $d u(m;\\psi)\\;=\\;$ $-(\\partial_{u}\\mathcal{R})^{-1}(\\partial_{m}\\mathcal{R})\\psi$ . Thus, $d^{*}u(m;\\psi)\\,=\\,-(\\partial_{m}{\\mathcal R})^{*}(\\partial_{u}{\\mathcal R})^{-*}p$ . Then we can first solve for $q=$ $(\\partial_{u}\\mathcal{R})^{-*}p$ and then compute $-(\\partial_{m}\\mathcal{R})^{*}q$ . See Appendix A.1 for a full proof. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present experiment results to demonstrate the derivative-enhanced accuracy and cost of our method on two test problems of nonlinear vector-valued PDEs in comparison with DeepONet, FNO, and DINO. Details about the data generation and training can be found in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "4.1 Input probability measure ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In all test cases, we assume that the input functions $m^{(i)}$ are i.i.d. samples drawn from a Gaussian random field with mean function $\\bar{m}$ and covariance operator $(\\delta I-\\gamma\\Delta)^{-\\alpha}$ . We take $\\alpha\\,=\\,2$ in two space dimensions so the covariance operator is of trace class [29]. It is worth noting that the parameters $\\gamma$ and $\\delta$ jointly control the marginal variance and correlation length, for which we take values to have large variation of the input samples that lead to large variation of the output PDE solutions. In such cases, especially when the mapping from the input to output is highly nonlinear as in our test examples, a vanilla neural network approximations tend to result in relatively large errors, particularly with a limited number of training data. To generate samples from this Gaussian random field, we use a scalable (in mesh resolution) sampling algorithm implemented in hIPPYlib [28]. ", "page_idx": 5}, {"type": "text", "text": "4.2 Governing equations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We consider two nonlinear PDE examples, including a nonlinear vector-valued hyperelasticity equation with one state (displacement), and a nonlinear vector-valued Navier\u2013Stokes equations with multiple states (velocity and pressure). For the hyperelasticity equation, we consider an experimental scenario where a square of hyperelasticity material is secured along its left edge while a fixed upwardright force is applied to its right edge [30]. Our goal is to learn the map between (the logarithm of) the material\u2019s Young\u2019s modulus and its displacement. We also consider the Navier\u2013Stokes equations that describes viscous, incompressible creeping flow. In particular, we consider the lid-driven cavity case where a square cavity consisting of three rigid walls with no-slip conditions and a lid moving with a tangential unit velocity. We consider that the uncertainty comes from the viscosity term and aim to predict the velocity field. See Appendix B.1 for details. ", "page_idx": 5}, {"type": "text", "text": "4.3 Evaluation metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We use the following three metrics to evaluate the performance of different methods. For the approximations of the solution $u(m)$ , we compute the relative error in the $L^{2}(\\Omega)$ norm and $H^{1}(\\Omega)$ norm on the test dataset $\\mathcal{D}_{\\mathrm{test}}=\\dot{\\{(m^{(i)},u^{(i)})\\}}_{i=1}^{N_{\\mathrm{test}}}$ , that is, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{N_{\\mathrm{test}}}\\sum_{m^{(i)}\\in\\mathcal{D}_{\\mathrm{test}}}\\frac{\\|\\hat{u}(m^{(i)};\\theta)-u(m^{(i)})\\|_{L^{2}(\\Omega)}}{\\|u(m^{(i)})\\|_{L^{2}(\\Omega)}},\\quad\\frac{1}{N_{\\mathrm{test}}}\\sum_{m^{(i)}\\in\\mathcal{D}_{\\mathrm{test}}}\\frac{\\|\\hat{u}(m^{(i)};\\theta)-u(m^{(i)})\\|_{H^{1}(\\Omega)}}{\\|u(m^{(i)})\\|_{H^{1}(\\Omega)}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|u\\|_{L^{2}(\\Omega)}=\\int_{\\Omega}\\|u(x)\\|_{2}^{2}\\,\\mathrm{d}x=u^{T}M^{\\mathrm{out}}u,\\quad\\|u\\|_{H^{1}(\\Omega)}=\\big(\\|u\\|_{L^{2}(\\Omega)}^{2}+\\|\\nabla u\\|_{L^{2}(\\Omega)}^{2}\\big)^{1/2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For the approximation of the Jacobian $d u(m;\\cdot)$ , we compute the relative error (for the discrete Jacobian) in the Frobenius norm on $\\ensuremath{\\mathcal{D}}_{\\mathrm{test}}$ along random directions $\\omega=\\{\\omega_{i}\\}_{i=1}^{N_{\\mathrm{dir}}}$ , that is, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{|\\mathcal{D}_{\\mathrm{test}}|}\\sum_{m^{(i)}\\in\\mathcal{D}_{\\mathrm{test}}}\\frac{\\lVert d\\hat{u}(m^{(i)};\\omega)-d u(m^{(i)};\\omega)\\rVert_{F}}{\\lVert d u(m^{(i)};\\omega)\\rVert_{F}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\omega_{i}$ are samples drawn from the same distribution as $m$ . ", "page_idx": 6}, {"type": "text", "text": "4.4 Main results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare the prediction errors measured in the above three evaluation metrics and computational cost in data generation and neural network training for four neural operator architectures, including DeepONet [1], FNO [2], DINO [20], and our DE-DeepONet. We also add experiments to demonstrate the performance of the FNO trained with the derivative loss (DE-FNO) and the DeepONet trained with input dimension reduction but without the derivative loss (DeepONet (ASM) or DeepONet (KLE)). For FNO, we use additional position embedding [4] that improves its approximation accuracy in our test cases. For DINO, we use ASM basis (v.s. KLE basis) as the input reduced basis and POD basis (v.s. output ASM basis [20]) as the output reduced basis, which gives the best approximation accuracy. For the input reduced basis of DE-DeepONet, we also test and present the results for both KLE basis and ASM basis. ", "page_idx": 6}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/a8bd41f59d79f81b53356037d0ee597972249ac03f37eae7e8523e4e12fb909a.jpg", "img_caption": ["Figure 1: Mean relative errors ( $\\pm$ standard deviation) over 5 random seeds of neural network training for a varying number of training samples for the [top: hyperelasticity; bottom: Navier\u2013Stokes] equation using different methods. Relative errors in the $\\mathbf{\\bar{\\rho}}L^{2}(\\bar{\\Omega})$ norm (left) and $H^{1}(\\Omega)$ norm (middle) for the prediction of $\\boldsymbol{u}=(u_{1},u_{2})$ . Right: Relative error in the Frobenius (Fro) norm for the prediction of $d u(\\bar{m};\\omega)=(d u_{1}(m;\\dot{\\omega}),d u_{2}(m;\\stackrel{\\cdot}{\\omega}))$ . "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/6d4790a2db89579b376202e7286cb23ab8f60c3c4a07c4e16915f4754c68e0a6.jpg", "img_caption": ["Figure 2: Mean relative errors ( $\\pm$ standard deviation) over 5 random seeds versus model training time for the Navier\u2013Stokes equations when the number of training samples is [top: 16; bottom: 256]. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Test errors. In Figure 1, we show a comparison of different methods for the hyperelasticity and Navier\u2013Stokes problem in predicting the solution (in $L^{2}$ norm, left, and $H^{1}$ norm, middle) and its derivative (in Frobenius norm, right) with respect to parameter in the direction of $N_{\\mathrm{dir}}=128$ test sDaEm-pDleees $\\{\\omega_{i}\\}_{i=1}^{N_{\\mathrm{dir}}}$ . mFpirasrte, d wtoe  tchaen  voabnsilelrav eD esiegpnOifNiceat nfto ri malpl rtohrveeem menett riocf st ihne  aallp pcraosexsi mofa ttiroainn iancgc usraamcpyl eosf. Second, we can see that FNO leads to larger approximation errors than DINO and DE-DeepONet for all metrics except when number of training samples is 1024. We believe that the reason why FNO performs better than DE-DeepONet and DINO when training samples are large enough is mainly due to the use of input dimensionality reduction in DE-DeepONet and DINO (where the linear reduction error cannot be eliminated by increasing training samples) whereas in FNO we use full inputs. We also see that DE-FNO performs the best among all models when the training samples are sufficient (256 or 1024), although in the compensation of much longer training time shown in Figure 2. Third, we can see that the approximation accuracy of DINO is similar to DE-DeepONet (ASM) but requires much longer inference time as shown in Table 5 (see Section 5 and Appendix B.3.2 for reasons). In DINO, the output reduced basis dimension is set to be smaller than or equal to the number of training samples as the output POD basis are computed from these samples, i.e., 16 for 16 samples and 64 for $\\geq64$ samples. Increasing the output dimension beyond 64 does not lead to smaller errors in our test. Finally, we can observe that DE-DeepONet using ASM basis leads to smaller errors than using KLE basis, especially for the Navier-Stokes problem. ", "page_idx": 7}, {"type": "table", "img_path": "WAiqLGfqX6/tmp/e7c91d178c9ea0dda67fd2695cb523aa8ed53bbaaee548e52c0ccaf36e3a4186.jpg", "table_caption": ["Table 1: Output reconstruction error with 16 input reduced bases "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Moreover, we present the output reconstruction error due to the input dimension reduction using KLE or ASM basis in Table 1. The errors provide the lower bound of the relative errors in $L^{2}(\\Omega)$ norm of DINO and DE-DeepONet. We can see that using the ASM basis results in a lower output reconstruction error than the KLE basis (more significant difference observed in the more nonlinear Navier\u2013Stokes equations). See Appendix B.6 for the decay of the reconstruction error with increasing number of reduced basis. ", "page_idx": 7}, {"type": "text", "text": "In addition, we provide full visualizations of the ground truth and prediction of both solution and derivatives in Appendix B.7. Visually, DE-DeepONet (ASM) consistently provides the best estimate (in terms of pattern and smoothness similarity with the ground truth) for both the solution and its derivative when the training data is limited (16 or 64 samples). ", "page_idx": 7}, {"type": "text", "text": "Data generation computational cost. We use MPI and the finite element library FEniCS [31] to distribute the computational load of offline data generation to 64 processes for the PDE models considered in this work. See Table 2 for the wall clock time in generating the samples of Gaussian random fields (GRFs), solving the PDEs, computing the $r=16$ reduced basis functions (KLE or ASM) corresponding to the 16 largest eigenvalues, generating the derivative labels, respectively. In Table 3, We also provide the total wall clock time of data generation of DE-DeepONet (ASM) (we only includes the major parts \u2013 computing high fidelity solution, ASM basis and dm labels [16 directions]) when $N_{\\mathrm{train}}=16,64,256,1024$ using 16 CPU processors. ", "page_idx": 8}, {"type": "table", "img_path": "WAiqLGfqX6/tmp/62d5c90c6fe5f7aa4ce6a4532bf5e611e533becbec5987941c61527dd75f6fc0.jpg", "table_caption": ["Table 2: Wall clock time (in seconds) for data generation on $2\\times\\mathrm{AMD}$ EPYC 7543 32-Core Processors "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "WAiqLGfqX6/tmp/e756465b525e52a6c810faaec4c8086afc6a46c6d3a983984a935b0e00a26036.jpg", "table_caption": ["Table 3: Wall clock time (in seconds) for Table 4: Wall clock time (seconds/iteration with batch size 8) data generation with different number of for training on a single NVIDIA RTX A6000 GPU training samples using 16 CPU processors "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "WAiqLGfqX6/tmp/c85b437aff18d2551521928be54f08b4b22d4abe948d803b9dfd4c934b09b721.jpg", "table_caption": ["Table 5: Total wall clock time (in seconds) for each model inferring on 500 test samples of both the solution and dm in 128 random directions, using a single GPU and a single CPU (except where specified) "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "1 The inference time of DINO is dominated by the time required to compute evaluations of all finite element basis functions at the grid points using FEniCS (which may not be the most efficient, see Appendix B.3.2). Even though these grid points overlap with parts of the finite element nodes\u2014allowing us to skip evaluations by extracting the relevant nodes\u2014for a fairer comparison with DE-DeepONet (in terms of its ability to evaluate at any arbitrary point), we assume they are arbitrary points requiring explicit evaluation. ", "page_idx": 8}, {"type": "text", "text": "Model training computational cost. We present comparisons of the wall clock time of each optimization iteration (with batch size 8) of different methods in Table 4 and convergence plot (error versus training time) in Figure 2 and the figures in Appendix B.5. We find that incorporating derivative loss leads to longer training time as expected. However, when the training data are limited, the increased computation cost is compensated for a significant reduction of errors. We note that there are potential ways to further reduce the training cost, e.g., by training the model with additional derivative loss only during the later stage of training, or by using fewer points for computing the derivative losses in each iteration. Additionally, thanks to the dimension reduction of the input, we can define a relatively small neural network and thus are able to efficiently compute the derivatives using automatic differentiation. ", "page_idx": 8}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our work is related to Sobolev training for neural networks [32], which was found to be effective in their application to model distillation/compression and meta-optimization. In the domain of surrogate models for parametric partial differential equations, our work is more closely related to derivativeinformed neural operator (DINO) [20] which is based on a derivative-informed projected neural network (DIPNet) [17], and presents an extension to enhance the performance of the DeepONet. Compared to DINO, although the DeepONet architecture (and its formulation of dm loss) requires longer training time, it offers the following advantages: (1) Potentially shorter inference time. The additional trunk net (which receives spatial coordinates) allows us to quickly query the sensitivity of output function at any point when input function is perturbed in any direction. While DINO can only provide the derivative of the output coefficients respect to the input coefficients (we call reduced dm), in order to compute the sensitivity at a batch of points, we need to post process the reduced dm by querying the finite element basis on these points and computing large matrix multiplications; (2) Greater flexibility and potential for improvements. Although both DeepONet and DINO approximate solution by a linear combination of a small set of functions, these functions together in DeepONet is essentially the trunk net, which is \"optimized\" via model training, whereas in DINO, they are POD or derivative-informed basis precomputed on training samples. When using DINO, if we encounter a case where the training samples not enough to accurately compute the output basis, the large approximation error between the linear subspace and solution manifold will greatly restrict the model prediction accuracy (see Figure 2 when $N_{\\mathrm{train}}=16)$ ). And the reduced dm labels only supports linear reduction of output. However, it is possible that we can further improve DeepONet by, e.g., adding physical losses (to enhance generalization performance) and Fourier feature embeddings (to learn high-frequency components more effectively) on the trunk net [5] and replacing the inner product of the outputs of two networks by more flexible operations [33, 9] (to enhance expressive power). The dm loss formulation of our work is broadly suitable any network architecture that has multiple subnetworks, where at least one of them receives high-dimensional inputs. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we proposed a new neural operator\u2013Derivative-enhanced Deep Operator Network (DE-DeepONet) to address the limited accuracy of DeepONet in both function and derivative approximations. Specifically, DE-DeepONet employs a derivative-informed reduced representation of input function and incorporates additional loss into the loss function for the supervised learning of the derivative of the output with respect to the inputs of the branch net. Our experiments for nonlinear PDE problems with high variations in both input and output functions demonstrate that adding this loss term to the loss function greatly enhances the accuracy of both function and derivative approximations, especially when the training data are limited. We also demonstrate that the use of derivative loss can be extended to enhance other neural operators, such as the Fourier neural operator. ", "page_idx": 9}, {"type": "text", "text": "We presented matrix-free computation of the derivative label and the derivative-informed dimension reduction for a general form of PDE problems by using randomized algorithms and linearized PDE solves. Thanks to this scalable approach, the computational cost in generating the derivative label data is shown to be only marginally higher than generating the input-output function pairs for the test problems, especially for the more complex Navier\u2013Stokes equations which require more iterations in the nonlinear solves than the hyperelasticity equation. ", "page_idx": 9}, {"type": "text", "text": "Limitations: We require the derivative information in the training and dimension reduction using ASM, which may not be available if the explicit form of the PDE is unknown or if the simulation only provides input-output pairs from some legacy code. Another limitation is that dimension reduction of the input function plays a key role in scalable data generation and training, which may not be feasible or accurate for intrinsically very high-dimensional problems such as high frequency wave equations. Such problems are also very challenging and remain unsolved by other methods to our knowledge. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank Jinwoo Go, Dingcheng Luo and Lianghao Cao for insightful discussions and helpful feedback. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis, \u201cLearning nonlinear operators via Deeponet based on the universal approximation theorem of operators,\u201d Nature machine intelligence, vol. 3, no. 3, pp. 218\u2013229, 2021.   \n[2] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar, \u201cFourier neural operator for parametric partial differential equations,\u201d arXiv preprint arXiv:2010.08895, 2020.   \n[3] L. Lu, X. Meng, S. Cai, Z. Mao, S. Goswami, Z. Zhang, and G. E. Karniadakis, \u201cA comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data,\u201d Computer Methods in Applied Mechanics and Engineering, vol. 393, p. 114778, 2022.   \n[4] N. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, and A. Anandkumar, \u201cNeural operator: Learning maps between function spaces with applications to PDEs,\u201d Journal of Machine Learning Research, vol. 24, no. 89, pp. 1\u201397, 2023.   \n[5] S. Wang, H. Wang, and P. Perdikaris, \u201cLearning the solution operator of parametric partial differential equations with physics-informed deeponets,\u201d Science advances, vol. 7, no. 40, p. eabi8605, 2021.   \n[6] S. Goswami, M. Yin, Y. Yu, and G. E. Karniadakis, \u201cA physics-informed variational deeponet for predicting crack path in quasi-brittle materials,\u201d Computer Methods in Applied Mechanics and Engineering, vol. 391, p. 114587, 2022.   \n[7] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, A. Stuart, K. Bhattacharya, and A. Anandkumar, \u201cMultipole graph neural operator for parametric partial differential equations,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 6755\u20136766, 2020.   \n[8] B. Raoni\u00b4c, R. Molinaro, T. Rohner, S. Mishra, and E. de Bezenac, \u201cConvolutional Neural Operators,\u201d arXiv preprint arXiv:2302.01178, 2023.   \n[9] Z. Hao, Z. Wang, H. Su, C. Ying, Y. Dong, S. Liu, Z. Cheng, J. Song, and J. Zhu, \u201cGnot: A general neural operator transformer for operator learning,\u201d in International Conference on Machine Learning, pp. 12556\u201312569, PMLR, 2023.   \n[10] A. Tran, A. Mathews, L. Xie, and C. S. Ong, \u201cFactorized fourier neural operators,\u201d arXiv preprint arXiv:2111.13802, 2021.   \n[11] D. Luo, T. O\u2019Leary-Roseberry, P. Chen, and O. Ghattas, \u201cEfficient PDE-constrained optimization under high-dimensional uncertainty using derivative-informed neural operators,\u201d arXiv preprint arXiv:2305.20053, 2023.   \n[12] L. Cao, T. O\u2019Leary-Roseberry, and O. Ghattas, \u201cEfficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators,\u201d arXiv preprint arXiv:2403.08220, 2024.   \n[13] J. Go and P. Chen, \u201cAccelerating Bayesian optimal experimental design with derivative-informed neural operators,\u201d arXiv preprint arXiv:2312.14810, 2023.   \n[14] J. Go and P. Chen, \u201cSequential infinite-dimensional Bayesian optimal experimental design with derivative-informed latent attention neural operator,\u201d arXiv preprint arXiv:2409.09141, 2024.   \n[15] P. G. Constantine, E. Dow, and Q. Wang, \u201cActive subspace methods in theory and practice: applications to kriging surfaces,\u201d SIAM Journal on Scientific Computing, vol. 36, no. 4, pp. A1500\u2013A1524, 2014.   \n[16] P. Chen and O. Ghattas, \u201cProjected Stein variational gradient descent,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 1947\u20131958, 2020.   \n[17] T. O\u2019Leary-Roseberry, U. Villa, P. Chen, and O. Ghattas, \u201cDerivative-informed projected neural networks for high-dimensional parametric maps governed by PDEs,\u201d Computer Methods in Applied Mechanics and Engineering, vol. 388, p. 114199, 2022.   \n[18] O. Zahm, T. Cui, K. Law, A. Spantini, and Y. Marzouk, \u201cCertified dimension reduction in nonlinear Bayesian inverse problems,\u201d Mathematics of Computation, vol. 91, no. 336, pp. 1789\u2013 1835, 2022.   \n[19] H. Son, J. W. Jang, W. J. Han, and H. J. Hwang, \u201cSobolev training for the neural network solutions of PDEs,\u201d 2020.   \n[20] T. O\u2019Leary-Roseberry, P. Chen, U. Villa, and O. Ghattas, \u201cDerivative-informed neural operator: an efficient framework for high-dimensional parametric derivative learning,\u201d Journal of Computational Physics, vol. 496, p. 112555, 2024.   \n[21] J. T. Oden and J. N. Reddy, An introduction to the mathematical theory of finite elements. Courier Corporation, 2012.   \n[22] O. Zahm, P. G. Constantine, C. Prieur, and Y. M. Marzouk, \u201cGradient-based dimension reduction of multivariate vector-valued functions,\u201d SIAM Journal on Scientific Computing, vol. 42, no. 1, pp. A534\u2013A558, 2020.   \n[23] S. Wang, S. Sankaran, H. Wang, and P. Perdikaris, \u201cAn expert\u2019s guide to training physicsinformed neural networks,\u201d arXiv preprint arXiv:2308.08468, 2023.   \n[24] R. J. Adler and J. E. Taylor, Random fields and geometry. Springer Science & Business Media, 2009.   \n[25] F. Lindgren, H. Rue, and J. Lindstr\u00f6m, \u201cAn explicit link between gaussian fields and Gaussian markov random fields: the stochastic partial differential equation approach,\u201d Journal of the Royal Statistical Society Series B: Statistical Methodology, vol. 73, no. 4, pp. 423\u2013498, 2011.   \n[26] R. G. Ghanem and P. D. Spanos, Stochastic finite elements: a spectral approach. Courier Corporation, 2003.   \n[27] P. G. Constantine, Active subspaces: Emerging ideas for dimension reduction in parameter studies. SIAM, 2015.   \n[28] U. Villa, N. Petra, and O. Ghattas, \u201cHIPPYlib: an extensible software framework for large-scale inverse problems governed by PDEs: part I: deterministic inversion and linearized Bayesian inference,\u201d ACM Transactions on Mathematical Software (TOMS), vol. 47, no. 2, pp. 1\u201334, 2021.   \n[29] A. M. Stuart, \u201cInverse problems: a Bayesian perspective,\u201d Acta numerica, vol. 19, pp. 451\u2013559, 2010.   \n[30] L. Cao, T. O\u2019Leary-Roseberry, P. K. Jha, J. T. Oden, and O. Ghattas, \u201cResidual-based error correction for neural operator accelerated infinite-dimensional bayesian inverse problems,\u201d Journal of Computational Physics, vol. 486, p. 112104, 2023.   \n[31] A. Logg, K.-A. Mardal, and G. Wells, Automated solution of differential equations by the finite element method: The FEniCS book, vol. 84. Springer Science & Business Media, 2012.   \n[32] W. M. Czarnecki, S. Osindero, M. Jaderberg, G. Swirszcz, and R. Pascanu, \u201cSobolev training for neural networks,\u201d Advances in neural information processing systems, vol. 30, 2017.   \n[33] S. Pan, S. L. Brunton, and J. N. Kutz, \u201cNeural implicit flow: a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data,\u201d Journal of Machine Learning Research, vol. 24, no. 41, pp. 1\u201360, 2023.   \n[34] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d arXiv preprint arXiv:1711.05101, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We first show how to compute $p$ given the PDE residual $\\mathscr{R}(m,u)=0$ . Since $u$ is uniquely determined by $m$ , we can write $u=u(m)$ so that $\\mathcal{R}(m,u(m))\\equiv0$ holds for any $m\\in V^{\\mathrm{in}}$ . Thus, for any $\\varepsilon>0$ and $\\psi\\in V^{\\mathrm{in}}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{R}(m+\\varepsilon\\psi,u(m+\\varepsilon\\psi))=0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Using the Taylor expansion we obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\n(\\partial_{m}\\mathcal{R}(m,u(m)))\\varepsilon\\psi+(\\partial_{u}\\mathcal{R}(m,u(m)))\\delta u\\approx0,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\delta u=u(m+\\varepsilon\\psi)-u(m)$ . Dividing both sides of Equation (15) by $\\varepsilon$ and letting $\\varepsilon$ approach 0 yields ", "page_idx": 12}, {"type": "equation", "text": "$$\n(\\partial_{m}\\mathcal{R}(m,u(m)))\\psi+(\\partial_{u}\\mathcal{R}(m,u(m)))d u(m;\\psi)=0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For ease of notation, we write ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\partial_{m}\\mathcal{R}=\\partial_{m}\\mathcal{R}(m,u(m)),\\quad\\partial_{u}\\mathcal{R}=\\partial_{u}\\mathcal{R}(m,u(m)).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then $p$ is the solution to the linear PDE ", "page_idx": 12}, {"type": "equation", "text": "$$\n(\\partial_{m}\\mathcal{R})\\psi+(\\partial_{u}\\mathcal{R})p=0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We solve Equation (16) via its weak form ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\langle(\\partial_{m}\\mathcal{R})\\psi,v\\rangle+\\langle(\\partial_{u}\\mathcal{R})p,v\\rangle=0,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $v$ is a test function in $V^{\\mathrm{out}}$ ", "page_idx": 12}, {"type": "text", "text": "Next we show how to compute $d^{*}u(m;p)$ . By Equation (16), we have $\\begin{array}{r l}{d u(m;\\psi)}&{{}=}\\end{array}$ $-(\\partial_{u}\\mathcal{R})^{-1}(\\partial_{m}\\mathcal{R})\\psi$ . Thus, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{w:=d^{\\ast}u(m;p)=(-(\\partial_{u}\\mathcal{R})^{-1}(\\partial_{m}\\mathcal{R}))^{\\ast}p}}\\\\ {{=-(\\partial_{m}\\mathcal{R})^{\\ast}(\\partial_{u}\\mathcal{R})^{-\\ast}p.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Let $q:=(\\partial_{u}\\mathcal{R})^{-*}p$ . We can solve for $q$ via the weak form ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\langle(\\partial_{u}\\mathcal{R})^{*}q,v\\rangle=\\langle p,v\\rangle,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "or equivalently, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\langle q,(\\partial_{u}\\mathcal{R})v\\rangle=\\langle p,v\\rangle,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $v$ is a test function in $V^{\\mathrm{out}}$ . ", "page_idx": 12}, {"type": "text", "text": "By Equation (18), we have $w=-(\\partial_{m}\\mathcal{R})^{*}q$ . For any test function $v\\in V^{\\mathrm{in}}$ , it holds that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle w,v\\rangle=\\langle-(\\partial_{m}\\mathcal{R})^{*}q,v\\rangle}\\\\ {=\\langle q,-(\\partial_{m}\\mathcal{R})v\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that we do not need to solve for $w$ explicitly; we only compute $\\langle w,v\\rangle$ with $v$ as the finite element basis functions $\\phi_{1}^{\\mathrm{in}},...,\\phi_{N_{h}^{\\mathrm{in}}}^{\\mathrm{in}}$ . The cost of computing the right hand side of Equation (20) arises from evaluating the directional derivative and its inner product with the finite element basis functions. ", "page_idx": 12}, {"type": "text", "text": "Remark. By Equation (9), we use $N_{g r a d}$ samples $\\{(m^{(i)},u^{(i)})\\}_{i=1}^{N_{g r a d}}$ to compute the Monte Carlo estimate of the action of operator on any function $\\psi\\in V^{i n}$ , that is, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{H}\\psi\\approx\\frac{1}{N_{g r a d}}\\sum_{i=1}^{N_{g r a d}}d^{*}u(m^{(i)};d u(m^{(i)};\\psi)).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Remark. When using the double pass randomized algorithm to obtain the first $r$ eigenpairs in Equation (10), we need to compute the action of $\\mathcal{H}$ on $2(r+s)$ random functions in $V^{i n}$ (e.g., their nodal values are sampled from the standard Gaussian distribution), where $s\\in\\mathbb{N}^{+}$ is typically $a$ small oversampling parameter, often chosen between 5 and 20. To speed up the computation, we first compute the LU factorization of the matrices resulting from the discretization of the linear PDEs in Equation (17) and Equation (19). Then the action of $d^{*}u(m^{(i)};d u(m^{(i)};\\cdot))$ on these random functions can be efficiently computed via the forward and backward substitution. Furthermore, the computational time can be significantly reduced by parallelizing the computation of the average value in Equation (21) across multiple processors. ", "page_idx": 12}, {"type": "text", "text": "A.2 Proof of the equivalence of Equation (6) and Equation (7) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "By the definition of $\\mathcal{C}=(\\delta I-\\gamma\\Delta)^{-2}$ , Equation (6) is equivalent to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\psi=\\lambda(\\delta I-\\gamma\\Delta)^{2}\\psi.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We first compute $(\\delta I-\\gamma\\Delta)\\psi$ . To do this, let $p\\,=\\,(\\delta I\\,-\\,\\gamma\\Delta)\\psi$ and multiply both sides of this equation by a test function $v$ and integrate ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\Omega}p(x)v(x)\\;\\mathrm{d}x=\\int_{\\Omega}(\\delta I-\\gamma\\Delta)\\psi(x)v(x)\\;\\mathrm{d}x}}\\\\ &{\\qquad\\qquad=\\delta\\int_{\\Omega}\\psi(x)v(x)\\;\\mathrm{d}x+\\gamma\\int_{\\Omega}\\langle\\nabla\\psi(x),\\nabla v(x)\\rangle\\;\\mathrm{d}x,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where, in the second equality, we use integration by parts and the assumption that the test function vanishes on the boundary. Then we substitute $v$ with all of the finite element basis functions $\\phi_{1}^{\\mathrm{in}},...,\\phi_{N_{h}^{\\mathrm{in}}}^{\\mathrm{in}}$ and collect the corresponding linear equations for the nodal values of $p$ ", "page_idx": 13}, {"type": "equation", "text": "$$\nA^{\\mathrm{in}}\\psi=M^{\\mathrm{in}}p,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the $(i,j)$ -entries of $A^{\\mathrm{in}}$ and $M^{\\mathrm{in}}$ are given by ", "page_idx": 13}, {"type": "equation", "text": "$$\nA_{i j}^{\\mathrm{in}}=\\delta\\langle\\phi_{j}^{\\mathrm{in}},\\phi_{i}^{\\mathrm{in}}\\rangle+\\gamma\\langle\\nabla\\phi_{j}^{\\mathrm{in}},\\nabla\\phi_{i}^{\\mathrm{in}}\\rangle,\\quad M_{i j}^{\\mathrm{in}}=\\langle\\phi_{j}^{\\mathrm{in}},\\phi_{i}^{\\mathrm{in}}\\rangle.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By Equation (22), function $p$ satisfies $\\psi=\\lambda(\\delta I-\\gamma\\Delta)p$ . In the same manner we can see that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda A^{\\mathrm{in}}p=M^{\\mathrm{in}}\\psi.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that both matrices $M^{\\mathrm{in}}$ and $A^{\\mathrm{in}}$ are symmetric positive definite and thus nonsingular. Combining Equation (24) with Equation (25) yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda A^{\\mathrm{in}}(M^{\\mathrm{in}})^{-1}A^{\\mathrm{in}}\\psi=M^{\\mathrm{in}}\\psi,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "or equivalently, ", "page_idx": 13}, {"type": "equation", "text": "$$\nM^{\\mathrm{in}}(A^{\\mathrm{in}})^{-1}M^{\\mathrm{in}}(A^{\\mathrm{in}})^{-1}M^{\\mathrm{in}}\\psi=\\lambda M^{\\mathrm{in}}\\psi.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.3 Proof of the equivalence of Equation (8) and Equation (10) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "By Equation (8), for any test function $v\\in V_{h}^{\\sin}$ , it holds that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\langle\\mathcal{H}\\psi,v\\rangle=\\langle\\lambda\\mathcal{C}^{-1}\\psi,v\\rangle.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In particular, for any $m\\sim\\nu(m)$ , as we let $v$ go through all of the finite element basis functions $\\phi_{i}^{\\mathrm{in}^{\\star}}\\in V_{h}^{\\mathrm{in}}$ , we can show that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c}{\\langle d^{*}u(m;d u(m;\\psi)),\\phi_{1}^{\\mathrm{in}}\\rangle}\\\\ {\\vdots}\\\\ {\\langle d^{*}u(m;d u(m;\\psi)),\\phi_{N_{h}^{\\mathrm{in}}}^{\\mathrm{in}}\\rangle}\\end{array}\\right)=(\\nabla_{m}u)^{T}M^{\\mathrm{out}}(\\nabla_{m}u)\\psi.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Indeed, by the definition of Gateaux derivative, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{d u(m;\\psi)=\\operatorname*{lim}_{\\varepsilon\\to0}\\frac{u(m+\\varepsilon\\psi)-u(m)}{\\varepsilon}}}\\\\ &{=\\operatorname*{lim}_{\\varepsilon\\to0}\\frac{N_{\\mathrm{sh}}^{\\varkappa}}{i}\\frac{u_{i}(m+\\varepsilon\\psi)-u_{i}(m)}{\\varepsilon}\\phi_{i}^{\\mathrm{sut}}(x)}\\\\ &{=\\operatorname*{lim}_{\\varepsilon\\to0}\\sum_{i=1}^{N_{\\mathrm{sh}}^{\\varkappa}}\\frac{u_{i}(m_{1}+\\varepsilon\\psi_{1},\\cdots,m_{N_{h}}\\div\\varepsilon\\psi_{N_{h}^{\\varkappa}})-u_{i}(m_{1},\\cdots,m_{N_{h}^{\\varkappa}})}{\\varepsilon}\\phi_{i}^{\\mathrm{sut}}(x)}\\\\ &{=\\displaystyle\\sum_{\\varepsilon\\to0}^{N_{\\mathrm{sh}}^{\\varkappa}}(\\frac{\\partial u_{i}}{\\partial m_{1}}\\psi_{1}+\\cdots+\\frac{\\partial u_{i}}{\\partial m_{N_{h}^{\\varkappa}}}\\psi_{N_{h}^{\\varkappa}})\\phi_{i}^{\\mathrm{sut}}(x)}\\\\ &{=\\delta^{\\mathrm{out}}(x)(\\nabla_{m}u)\\psi,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\phi^{\\mathrm{out}}(x)\\,=\\,(\\phi_{1}^{\\mathrm{out}}(x),\\cdot\\,\\cdot\\,.\\,,\\phi_{N_{h}^{\\mathrm{out}}}(x))$ are the finite element basis functions of output function space $V_{h}^{\\mathrm{out}}$ . ", "page_idx": 14}, {"type": "text", "text": "Then for any test function $v\\in V_{h}^{\\sin}$ , it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle d^{*}u(m;p),v\\rangle=\\langle p,d u(m;v)\\rangle\\quad(\\mathrm{by~the~definition~of~adjoint~operator})}\\\\ &{\\qquad\\qquad\\qquad=\\langle\\phi^{\\mathrm{out}}(x)(\\nabla_{m}u)\\psi,\\phi^{\\mathrm{out}}(x)(\\nabla_{m}u)v\\rangle\\quad(\\mathrm{by~Equation~}(30))}\\\\ &{\\qquad\\qquad\\qquad=v^{T}(\\nabla_{m}u)^{T}M^{\\mathrm{out}}(\\nabla_{m}u)\\psi,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where M out is the mass matrix of output function space V hou t, i.e., M iojut = \u27e8\u03d5jou t, \u03d5iou t\u27e9for 1 \u2264i, j \u2264 $N_{h}^{\\mathrm{out}}$ . Note that if we replace $v$ by the $i$ -th finite element basis functions $\\phi_{i}^{\\mathrm{in}}$ , then $\\pmb{v}$ becomes the standard unit vector $e_{i}\\in\\mathbb{R}^{N_{h}^{\\mathrm{in}}}$ (with the $k$ -th entry one and all others zero). Thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle d^{*}u(m;p),\\phi_{i}^{\\mathrm{in}}\\rangle=e_{i}^{T}(\\nabla_{m}u)^{T}M^{\\mathrm{out}}(\\nabla_{m}u)\\psi,\\quad1\\leq i\\leq N_{h}^{\\mathrm{in}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Concatenating all the above equations yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c}{\\langle d^{*}u(m;p),\\phi_{1}^{\\mathrm{in}}\\rangle}\\\\ {\\vdots}\\\\ {\\langle d^{*}u(m;p),\\phi_{N_{h}^{\\mathrm{in}}}^{\\mathrm{in}}\\rangle}\\end{array}\\right)=(\\nabla_{m}\\pmb{u})^{T}M^{\\mathrm{out}}(\\nabla_{m}\\pmb{u})\\psi.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, we prove that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c}{{\\langle\\lambda\\mathcal{C}^{-1}\\psi,\\phi_{1}^{\\mathrm{in}}\\rangle}}\\\\ {{\\vdots}}\\\\ {{\\langle\\lambda\\mathcal{C}^{-1}\\psi,\\phi_{N_{h}^{\\mathrm{in}}}^{\\mathrm{in}}\\rangle}}\\end{array}\\right)=\\lambda A^{\\mathrm{in}}(M^{\\mathrm{in}})^{-1}A^{\\mathrm{in}}\\psi.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Indeed, if we let $w=\\lambda\\mathcal{C}^{-1}\\psi$ , then similar to the argument in Appendix A.2, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda A^{\\mathrm{in}}(M^{\\mathrm{in}})^{-1}A^{\\mathrm{in}}\\psi=M^{\\mathrm{in}}{\\pmb w},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle w,\\phi_{i}^{\\mathrm{in}}\\rangle=e_{i}^{T}M^{\\mathrm{in}}{\\pmb w},\\quad1\\leq i\\leq N_{h}^{\\mathrm{in}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c}{{\\langle w,\\phi_{1}^{\\mathrm{in}}\\rangle}}\\\\ {{\\vdots}}\\\\ {{\\langle w,\\phi_{N_{h}^{\\mathrm{in}}}^{\\mathrm{in}}\\rangle}}\\end{array}\\right)=I M^{\\mathrm{in}}{\\pmb w}=\\lambda A^{\\mathrm{in}}(M^{\\mathrm{in}})^{-1}A^{\\mathrm{in}}\\psi.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining (28), (29) and (31) yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{m\\sim\\nu(m)}[(\\nabla_{m}u)^{T}M^{\\mathrm{out}}(\\nabla_{m}u)\\psi]=\\lambda A^{\\mathrm{in}}(M^{\\mathrm{in}})^{-1}A^{\\mathrm{in}}\\psi.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Experimental details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Governing equations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Hyperelasticity equation. We follow the problem setup in [30]. Write $X$ (instead of $x$ ) for a material point in the domain $\\Omega$ and $u\\,=\\,u(\\dot{\\boldsymbol{X}})\\,:\\,\\mathbb{R}^{2}\\,\\rightarrow\\,\\dot{\\mathbb{R}}^{2}$ for the displacement of the material point. Under the influence of internal and/or external forces, the material point is mapped to a spatial point $x=x(X)=X+u(X):\\mathbb{R}^{2}\\to\\mathbb{R}^{2}$ . Let $F=\\nabla_{X}x=I+\\nabla_{X}\\boldsymbol{\\dot{u}}:\\mathbb{R}^{2}\\rightarrow\\dot{\\mathbb{R}}^{2\\times2}$ denote the deformation gradient. For a hyperelasticity material, the internal forces can be derived from a strain energy density ", "page_idx": 14}, {"type": "equation", "text": "$$\nW(X,C)=\\frac{\\mu(X)}{2}(\\mathrm{tr}(C)-3)+\\frac{\\lambda(X)}{2}(\\ln(J))^{2}-\\mu(X)\\ln(J).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, $C=F^{T}F$ is the right Cauchy-Green strain tensor, $\\operatorname{tr}(C)$ is the trace of matrix $C,\\,J$ is the determinant of matrix $F$ , and $\\mu(X),\\dot{\\lambda}(X):\\mathbb{R}^{2}\\to\\mathbb{R}$ are the Lam\u00e9 parameters which we assume to be related to Young\u2019s modulus of elasticity $E(X):\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ and Poisson ratio $\\nu\\in\\mathbb{R}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu(X)=\\frac{E(X)}{2(1+\\nu)},\\quad\\lambda(X)=\\frac{\\nu E(X)}{(1+\\nu)(1-2\\nu)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We assume the randomness comes from the Young\u2019s modulus $E(X)=e^{m(X)}+1.$ . Let $\\begin{array}{r}{S=2\\frac{\\partial W}{\\partial C}}\\end{array}$ denote the second Piola-Kirchhoff stress tensor. We consider the case where the left boundary of the material is fixed, and the right boundary is subjected to stretching by an external force $t=t(X):\\mathbb{R}^{2}\\to\\mathbb{R}^{2}$ . The strong form of the steady state PDE can be written as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\nabla_{X}\\cdot(F S)=0,\\qquad}&{X\\in\\Omega,}\\\\ {u=0,\\qquad}&{X\\in\\Gamma_{\\mathrm{left}},}\\\\ {F S\\cdot n=0,\\qquad}&{X\\in\\Gamma_{\\mathrm{top}}\\cup\\Gamma_{\\mathrm{bottom}},}\\\\ {F S\\cdot n=t,\\qquad}&{X\\in\\Gamma_{\\mathrm{right}},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\Gamma_{\\mathrm{left}},\\Gamma_{\\mathrm{right}},\\Gamma_{\\mathrm{top}}$ and $\\Gamma_{\\mathrm{bottom}}$ denote the left, right, top, and bottom boundary of the material domain $\\Omega$ , respectively, and $n$ is the unit outward normal vector on the boundary. Our goal is to learn the operator that maps the parameter $m$ to the displacement $u$ . For demonstration, we choose $\\bar{m}=0$ , $\\delta=\\bar{0}.4$ , $\\gamma=0.04$ , Poisson ratio $\\nu=0.4$ , and the external force ", "page_idx": 15}, {"type": "equation", "text": "$$\nt(X)=\\left(0.06\\exp(-0.25|X_{2}-0.5|^{2}),0.03(1+0.1X_{2})\\right)^{T}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In practice, we solve the PDE by first formulating the energyW in the weak form ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widetilde W=\\int_{\\Omega}W\\mathrm{d}X-\\int_{\\Gamma_{\\mathrm{right}}}\\langle t,u\\rangle\\mathrm{~d}s\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and then solving for $u$ that satisfies the stationary condition, that is, the equation ", "page_idx": 15}, {"type": "equation", "text": "$$\nd\\widetilde{W}(u;v)=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "holds for any test function $v$ in the state space. See Figure 3 for the visualization for one parametersolution pair of the hyperelasticity equation. ", "page_idx": 15}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/1b0b665ab016e758d830206cfb08b0aedd4102f3effba0d8eeca9b7ea1ddf966.jpg", "img_caption": ["Figure 3: Visualization of one parameter-solution pair of hyperelasticity equation. The color of the output indicates the magnitude of the displacement $u$ (which maps from domain $\\Omega$ to $\\mathbb{R}^{2}$ ) instead of its componentwise function $u_{1}$ or $u_{2}$ . The skewed square shows locations of any domain point after deformation $X\\rightarrow x$ . See Figures 13 and 14 for $u_{1}$ and $u_{2}$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Navier\u2013Stokes equations. Let $u=u(x)\\in\\mathbb{R}^{2}$ and $p=p(x)\\in\\mathbb{R}$ denote the velocity and pressure at point $x\\in\\Omega=\\bar{(0,1)}^{2}$ . The strong form of the PDE can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{-\\nabla\\cdot e^{m}\\nabla u+(u\\cdot\\nabla)u+\\nabla p=0,\\quad}&{x\\in\\Omega,}\\\\ {\\nabla\\cdot u=0,\\quad}&{x\\in\\Omega,}\\\\ {u=(1,0)^{T},\\quad}&{x\\in\\Gamma_{\\mathrm{top}},}\\\\ {u=(0,0)^{T},\\quad}&{x\\in\\Gamma\\setminus\\Gamma_{\\mathrm{top}},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Gamma_{\\mathrm{top}}$ and $\\Gamma$ denote the left and whole boundary of the cavity domain $\\Omega$ , respectively. Here, we assume that the randomness arises from the viscosity term $e^{m}$ . Our goal is learn the operator that maps the parameter $m$ to the velocity $u$ . For demonstration, we choose $\\bar{m}=6.908\\;(e^{\\bar{m}^{2}}\\approx10^{3},$ thus the viscosity term dominates), $\\delta=0.6$ , and $\\gamma=0.06$ . See Figure 4 for the visualization for one parameter-solution pair of the Navier\u2013Stokes equations. ", "page_idx": 16}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/9001b21560b23a6ed2df9a650bf7e5f5a73e3b2db8fd9feb3095038087a551ed.jpg", "img_caption": ["Figure 4: Visualization of one parameter-solution pair of Navier\u2013Stokes equations. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 Data generation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For all PDEs in this work, we use the class dolfin.UnitSquareMesh to create a triangular mesh of the 2D unit square with 64 cells in horizontal direction and 64 cells in vertical direction. For the Darcy flow equation and hyperelasticity equation, we set the direction of the diagonals as \u2019right\u2019, while for the Navier\u2013Stokes equation, we set the direction of the diagonals as \u2019crossed\u2019. See Figure 5 for a visualization of the unit square mesh with a 10 by 10 resolution. ", "page_idx": 16}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/2b7cb7d37c420d080b857def09bd2a29971f113bc908021bf190147fe11b9ed5.jpg", "img_caption": ["Figure 5: Visualization of the 10 by 10 unit square mesh. Left: diagona $\\r=$ \u2018right\u2019; Right: diagonal $\\mathbf{\\chi}=$ \u2018crossed\u2019 "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "We use the class dolfin.FunctionSpace or dolfin.VectorFunctionSpace to create the finite element function space of input function and output function. For the finite element basis functions, we consider the Continuous Galerkin (CG) family (or the standard Lagrange family) with degree 1 or 2. See Table 6 for details. ", "page_idx": 17}, {"type": "table", "img_path": "WAiqLGfqX6/tmp/da244e67ee04839a14d33d2262f2bb5b7990bf1a9aa843e7c1f15aeaa3fc718c.jpg", "table_caption": ["Table 6: Configurations of data generation for different datasets "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "We generate $N_{\\mathrm{train}}=1500$ and $N_{\\mathrm{test}}=500$ input-output pairs $(m^{(i)},u^{(i)})$ for training and testing, respectively. We compute first $r\\,=\\,16$ KLE basis and ASM basis using double pass randomized algorithm with an oversampling parameter $s$ of 10. In the computation of ASM basis, we use $N_{\\mathrm{grad}}=16$ samples for the Monte Carlo estimate of the action of operator $\\mathcal{H}$ in Equation (21). In our method, we formulate the different labels into arrays with the shape as follows ", "page_idx": 17}, {"type": "text", "text": "\u2022 Evaluation labels: $\\left.N_{\\mathrm{train}},\\,N_{x},\\,N_{u}\\right)$ \u2022 Derivative $m$ labels: $(N_{\\mathrm{train}},\\,N_{x},\\,r,\\,N_{u})$ ", "page_idx": 17}, {"type": "text", "text": "Recall that $N_{\\mathrm{train}}$ is the number of functions used for training, $N_{x}$ is the number of nodes of the mesh, $N_{u}$ is the dimension of output function, $r$ is the number of reduced basis, and $d$ is the dimension of the domain. ", "page_idx": 17}, {"type": "text", "text": "B.3 Computation of derivative labels and outputs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.3.1 Derivative labels $p:=d u(m;\\psi)$ as ground truth ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Since the PDE residual $\\mathscr{R}(m,u(m))\\equiv0$ holds for any $m\\in V_{h}^{\\mathrm{in}}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(m+\\varepsilon\\psi,u(m+\\varepsilon\\psi))=0,\\quad\\forall\\varepsilon>0,\\psi\\in V_{h}^{\\mathrm{in}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By the Taylor expansion and $\\mathcal{R}(m,u(m))=0$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\partial_{m}\\mathcal{R}(m,u(m)))\\varepsilon\\psi+(\\partial_{u}\\mathcal{R}(m,u(m)))\\delta u\\approx0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\delta u=u(m+\\varepsilon\\psi)-u(m)$ . Dividing both sides of Eq. (32) by $\\varepsilon$ and letting $\\varepsilon$ approach 0 yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\partial_{m}\\mathcal{R})\\psi+(\\partial_{u}\\mathcal{R})p=0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $p:=d u(m;\\psi)$ . We solve Eq. (33) for $p$ via its weak form ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle(\\partial_{m}\\mathcal{R})\\psi,v\\rangle+\\langle(\\partial_{u}\\mathcal{R})p,v\\rangle=0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $v$ is a test function in $V_{h}^{\\mathrm{out}}$ ", "page_idx": 17}, {"type": "text", "text": "Example. Consider the (nonlinear) diffusion-reaction equation ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\begin{array}{r l}{-\\nabla\\cdot(e^{m}\\nabla u)+u^{3}=1,\\quad x\\in\\Omega,}\\\\ {u(x)=0,\\quad x\\in\\partial\\Omega.}\\end{array}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\mathcal{R}=-\\nabla\\cdot(e^{m}\\nabla u)+u^{3}-1}\\\\ &{\\bullet\\ (\\partial_{m}\\mathcal{R})\\psi=-\\nabla\\cdot(e^{m}\\psi\\nabla u)}\\\\ &{\\bullet\\ (\\partial_{u}\\mathcal{R})p=-\\nabla\\cdot(e^{m}\\nabla p)+3u^{2}p}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, $p$ satisfies the linear PDE ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\langle e^{m}\\psi\\nabla u,\\nabla v\\right\\rangle+\\left\\langle e^{m}\\nabla p,\\nabla v\\right\\rangle+\\left\\langle3u^{2}p,v\\right\\rangle=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using FEniCS, we can easily derive G\u00e2teaux derivative via automatic symbolic differentiation instead of by hand. In this case, the Python code for representing the weak form of the residual $\\langle\\mathcal{R},v\\rangle$ and G\u00e2teaux derivatives $\\langle(\\partial_{m}\\mathcal{R})\\psi,v\\rangle$ and $\\langle(\\partial_{u}\\mathcal{R})p,v\\rangle$ can be written as ", "page_idx": 17}, {"type": "text", "text": "import dolfin as dl ", "page_idx": 17}, {"type": "text", "text": "\u2022 ${\\tt R}{=}$ (dl.inner(dl.exp(m)\\*dl.grad(u), dl.grad(v))\\*dl.dx   \n+( $\\cdot{\\tt u}**3$ -dl.Constant(1.0))\\*v\\*dl.dx)   \n\u2022 dl.derivative(R,m,psi)   \n\u2022 dl.derivative $(\\mathtt{R},\\mathtt{u},\\mathtt{p})$ ", "page_idx": 18}, {"type": "text", "text": "B.3.2 Derivative outputs of neural networks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "DE-DeepONet. For notation simplicity, we illustrate the case where the input reduced basis is ASM basis and the output function is real-valued. The output of the model is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{u}(\\pmb{m};\\theta)(x)=\\langle b((\\Psi^{\\mathrm{in}})^{T}C^{-1}\\pmb{m};\\theta_{b}),t(x;\\theta_{t})\\rangle+\\theta_{\\mathrm{bias}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Psi^{\\mathrm{in}}=\\left(\\psi_{1}^{\\mathrm{in}}|\\cdot\\cdot\\cdot|\\psi_{r_{\\mathrm{in}}}^{\\mathrm{in}}\\right)\\,\\in\\,\\mathbb{R}^{N_{h}^{\\mathrm{in}}\\times r_{\\mathrm{in}}}$ are the nodal values of input (ASM) reduced basis functions, $C^{-1}$ is the inverse of the covariance matrix of the Gaussian random field $\\nu$ where parameter $m$ is sampled from (Recall that the ASM basis are orthonormal in the inner product with weight matrix $C^{-1}$ .) Thus, by the chain rule of derivative, for any test direction $\\psi_{\\mathrm{test}}$ , one has ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{m}\\hat{u}(m;\\theta)(x)\\psi_{\\mathrm{test}}=\\nabla_{\\Tilde{m}}\\langle b(\\Tilde{m}),t(x;\\theta_{t})\\rangle(\\Psi^{\\mathrm{in}})^{T}C^{-1}\\psi_{\\mathrm{test}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\widetilde{m}=(\\Psi^{\\mathrm{in}})^{T}C^{-1}m$ . The Jacobian matrix $\\nabla_{\\tilde{m}}\\langle b(\\tilde{m}),t(x;\\theta_{t})\\rangle$ can be efficiently computed using,  e. g., torch.func.jacrev, and further parallel ized   with torch.vmap. If $\\psi_{\\mathrm{test}}$ is in $\\Psi^{\\mathrm{in}}$ during model training, we can see that $(\\Psi^{\\mathrm{in}})^{T}C^{-1}\\dot{\\psi_{\\mathrm{test}}}$ becomes a unit vector which frees us the need to compute it, otherwise in the model inference stage when $\\psi_{\\mathrm{test}}$ is (the nodal values of) a random function sampled from $\\nu$ , we compute $T=(\\Psi^{\\mathrm{in}})^{\\check{T}}C^{-1}$ and then $T\\psi_{\\mathrm{rest}}$ . ", "page_idx": 18}, {"type": "text", "text": "DINO. The output of the model is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{u}(\\pmb{m};\\pmb{\\theta})(x)=\\Phi^{\\mathrm{out}}(x)\\Psi^{\\mathrm{out}}f_{\\theta}((\\Psi^{\\mathrm{in}})^{T}C^{-1}\\pmb{m}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Phi^{\\mathrm{out}}(x)=(\\phi_{1}^{\\mathrm{out}}(x),\\cdot\\cdot\\cdot,\\phi_{N_{h}^{\\mathrm{out}}}^{\\mathrm{out}}(x))\\in\\mathbb{R}^{1\\times N_{h}^{\\mathrm{out}}}$ denotes the output finite element basis functions evaluated on point $x\\in\\Omega$ , $\\Psi^{\\mathrm{out}}=(\\psi_{1}^{\\mathrm{out}}|\\cdot\\cdot\\cdot\\cdot|\\psi_{r_{\\mathrm{out}}}^{\\mathrm{out}})\\in\\mathbb{R}^{N_{h}^{\\mathrm{out}}\\times r_{\\mathrm{out}}}$ are the nodal values of output (POD) reduced basis functions, similarly for $\\Psi^{\\mathrm{in}}$ denoting the input (ASM) reduced basis, and $C^{-1}$ is the inverse of the covariance matrix of the Gaussian random field where parameter $m$ is sampled from. Thus, by the chain rule of derivative, for any test direction $\\psi_{\\mathrm{test}}$ , one has ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{m}\\hat{u}(m;\\theta)(x)\\psi_{\\mathrm{test}}=\\Phi^{\\mathrm{out}}(x)\\Psi^{\\mathrm{out}}\\nabla_{\\widetilde{m}}f_{\\theta}(\\widetilde{m})(\\Psi^{\\mathrm{in}})^{T}C^{-1}\\psi_{\\mathrm{test}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\widetilde{m}=(\\Psi^{\\mathrm{in}})^{T}C^{-1}m$ . For fast evaluations given different $x,\\,m$ , and $\\psi_{\\mathrm{test}}$ , we first compute $T_{\\mathrm{left}}\\,=\\,\\Phi^{\\mathrm{out}}(\\textit{x})\\Psi^{\\mathrm{out}}$ for all points $x$ that need to be evaluated and $T_{\\mathrm{right}}\\,=\\,(\\Psi^{\\mathrm{in}})^{T}C^{-1}$ . Next, we compute $J=\\nabla_{\\widetilde{m}}f_{\\theta}(\\widetilde{m})$ using, e.g., torch.func.jacrev, and finally compute $T_{\\mathrm{left}}J T_{\\mathrm{right}}\\psi_{\\mathrm{test}}$ . ", "page_idx": 18}, {"type": "text", "text": "DeepONet & FNO. Both models receive the full high dimensional parameter $\\mathbf{\\nabla}m$ , so we compute the directional derivative $\\nabla_{m}\\hat{u}(\\pmb{m};\\pmb{\\theta})\\psi_{\\mathrm{test}}$ using the Jacobian-vector product torch. $\\mathtt{j v p}$ instead of the full Jacobian in DE-DeepONet and DINO. For the coordinates $x$ as additional inputs, we pad zeros to the tensor $\\psi_{\\mathrm{test}}$ to match the dimension of the input tensor $(m,\\{x^{(j)}\\}_{j=1}^{N_{x}})$ . ", "page_idx": 18}, {"type": "text", "text": "B.4 Training details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the DeepONet, we parameterize the branch net using a CNN and the trunk net using a ResNet. For the FNO, we use the package neuraloperator. For the DINO, we use 16 ASM basis functions for the input dimension reduction and 64 POD basis functions for the output dimension reduction, and parameterize the neural network using a ResNet. For the DE-DeepONet, both the branch net and trunk net are parameterized using ResNets. We train each model for 32768 iterations (with the same batch size 8) using an AdamW optimizer [34] and a StepLR learning rate scheduler (We disable learning rate scheduler for DE-DeepONet). See Tables 7 to 10 for details. When the loss function comprises two terms, we apply the self-adaptive learning rate annealing algorithm from [23], with an update frequency of 100 and a moving average parameter of 0.9, to automatically adjust the loss weights $\\{\\lambda_{i}\\}_{i=1}^{2}$ in Equation (5). Additionally, we standardize the inputs and labels before training. ", "page_idx": 18}, {"type": "table", "img_path": "WAiqLGfqX6/tmp/37c7e090841029666e6b65210fe8ec2ca01dd818cae2126d9891bf79a6a58eed.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "WAiqLGfqX6/tmp/3b157911b756707dc1f9884ddfc9499d69d8be4a49c6bc63da60fc5a31e9eaa5.jpg", "table_caption": ["Table 8: Training details for FNO & DE-FNO "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "WAiqLGfqX6/tmp/493b171dc0fed9a572f2053972250c01f7b578163c7c491d6d68aefb6822ac78.jpg", "table_caption": ["Table 9: Training details for DINO "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "WAiqLGfqX6/tmp/cddce3e2542cd7ab2c555e98b4ed63f390bb70f5c2e749b66ced298c98acfb38.jpg", "table_caption": ["Table 10: Training details for DE-DeepONet "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "WAiqLGfqX6/tmp/73284e8a35314853c648f6b9174e893f0423089571a12de5f0975482dfed3b76.jpg", "table_caption": ["Table 11: Number of trainable parameters in each model "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.5 Convergence plot ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Based on the training time (seconds/iteration) of each model in Table 4, we obtain the convergence plots when the number of training samples is either limited $(N_{\\mathrm{train}}=16,64)$ or sufficient $\\ N_{\\mathrm{train}}=$ 256, 1024) in Figures 6 to 9 for the hyperelasticity equation, and in Figures 2, 10 and 11 for the Navier\u2013Stokes equations. ", "page_idx": 20}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/aab5238ef91fac36ea01ed597820a83146081fdbb4fbe62777622b57c8b8c181.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 6: Mean relative errors ( $\\pm$ standard deviation) over 5 trials versus model training time for the hyperelasticity equations when the number of training samples is 16. ", "page_idx": 20}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/80b59d79229626a658f65b7109c552b8157ff5ff73281f72fc5aa95a822c9c6c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 7: Mean relative errors ( $\\pm$ standard deviation) over 5 trials versus model training time for the hyperelasticity equations when the number of training samples is 64. ", "page_idx": 21}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/6f3034c4d2e4f3682f99eda2685689c50bfbdf4d1444436527ef809b07c1c7cc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 8: Mean relative errors ( $\\pm$ standard deviation) over 5 trials versus model training time for the hyperelasticity equations when the number of training samples is 256. ", "page_idx": 21}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/d9fe57e80160f1f44d1eb137a5315e745a943571a4800d38b9bd8e4a0cc0975b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 9: Mean relative errors ( $\\pm$ standard deviation) over 5 trials versus model training time for the hyperelasticity equations when the number of training samples is 1024. ", "page_idx": 21}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/231bc47c84a81e8096dbf67b4d80ef70a3f821e16e77b240f530fc63491900bf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 10: Mean relative errors $\\pm$ standard deviation) over 5 trials versus model training time for the Navier\u2013Stokes equations when the number of training samples is 64. ", "page_idx": 21}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/55b9a88edd079b1f669dcc741f3ad691b184e435fa955fcf0386e642bc5b6726.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 11: Mean relative errors $\\pm$ standard deviation) over 5 trials versus model training time for the Navier\u2013Stokes equations when the number of training samples is 1024. ", "page_idx": 22}, {"type": "text", "text": "B.6 Output reconstruction error ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To measure the error induced by the projection, we define the output reconstruction error as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{i=1}^{N}\\frac{||u(P_{r}m^{(i)})-u(m^{(i)})||_{L^{2}}}{||u(m^{(i)})||_{L^{2}}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $P_{r}$ is the rank $r$ linear projector. We provide the plots of the output reconstruction error vs number of reduced basis $r$ using KLE basis and ASM basis in Figure 12. We can see that using the ASM basis results in a lower output reconstruction error than the KLE basis. ", "page_idx": 22}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/57c66da5723ac2f5b72454c0a58b82f464cabe16b6aafede377eadb37697895c.jpg", "img_caption": ["Figure 12: Output reconstruction error using KLE and ASM basis. Left: Hyperelasticity; Right: Navier\u2013Stokes "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "B.7 Visualization of the ground truth and prediction ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We present the comparisons of the ground truth of solution $u$ , model prediction $\\hat{u}$ using different methods, and absolute value of their difference $|u(x)-\\hat{u}(x)|$ in Figures 13 and 14 for the hyperelasticity equation, and Figures 17 and 18 for the Navier\u2013Stokes equations. In addition, we present the comparisons of the ground truth of the derivative of $u$ with respect to $m$ in the direction of $\\omega_{1}$ , denoted as $d u(m;\\omega_{1})$ , model prediction $d{\\hat{u}}(m;\\psi_{1}^{\\mathrm{in}})$ using different methods, and absolute value of their difference $|d u(m;\\omega_{1})-d\\hat{u}(m;\\omega_{1})|$ in Figures 15 and 16 for the hyperelasticity equation, and Figures 19 and 20 for the Navier\u2013Stokes equations. ", "page_idx": 22}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/b7892107b48d62983393ca9d713ea3f9eacc4b44ae7d36602494956bc4d7d17d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 13: Hyperelasticity. Comparison of the predictions of $u_{1}$ with (16, 64, 256, 1024) training samples using different methods. Top row $->$ bottom row: DeepONet, FNO, DE-FNO (Random), DINO, DE-DeepONet (KLE), DE-DeepONet (ASM). ", "page_idx": 23}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/93c2f1ae78e41be340c8f4d1d02ff4895c519b168201a4a4ecdd9ae1d6da87ad.jpg", "img_caption": ["Figure 14: Hyperelasticity. Comparison of the predictions of $u_{2}$ with (16, 64, 256, 1024) training samples using different methods. Top row $->$ bottom row: DeepONet, FNO, DE-FNO (Random), DINO, DE-DeepONet (KLE), DE-DeepONet (ASM). "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/5ca76fd229584150df574cb956fd99f11e155565cc9ac27b3313d8213cee31c6.jpg", "img_caption": ["Figure 15: Hyperelasticity. Comparison of the predictions of directional derivative $d u_{1}(m;\\omega_{1})$ with (16, 64, 256, 1024) training samples using different methods. Top row $->$ bottom row: DeepONet, FNO, DE-FNO (Random), DINO, DE-DeepONet (KLE), DE-DeepONet (ASM). "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/238d7379ef98da5c0271888c44491e38a4e57494cbcdbe26630e868b95aff2cc.jpg", "img_caption": ["Figure 16: Hyperelasticity. Comparison of the predictions of directional derivative $d u_{2}(m;\\omega_{1})$ with (16, 64, 256, 1024) training samples using different methods.Top row $->$ bottom row: DeepONet, FNO, DE-FNO (Random), DINO, DE-DeepONet (KLE), DE-DeepONet (ASM). "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/7df2e3d0b13f4ebe285662ed564a12554f65580f43a7140f51520537d2838021.jpg", "img_caption": ["Figure 17: Navier\u2013Stokes. Comparison of the predictions of velocity- $\\mathbf{\\nabla}\\cdot\\mathbf{X}$ with (16, 64, 256, 1024) training samples using different methods. Top row $->$ bottom row: DeepONet, FNO, DE-FNO (Random), DINO, DE-DeepONet (KLE), DE-DeepONet (ASM). "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/b5cbcb378f87ca92f7c2ad06f755ed8f205902d6816bb5b554925066b18de4e9.jpg", "img_caption": ["Figure 18: Navier\u2013Stokes. Comparison of the predictions of velocity-y with (16, 64, 256, 1024) training samples using different methods. Top row $->$ bottom row: DeepONet, FNO, DE-FNO (Random), DINO, DE-DeepONet (KLE), DE-DeepONet (ASM). "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/1465f20c0c98159112b3d9722736e9e6bd8cf9185f473f804ef856e1c70af898.jpg", "img_caption": ["Figure 19: Navier\u2013Stokes. Comparison of the predictions of directional derivative $d u_{1}(m;\\omega_{1})$ with (16, 64, 256, 1024) training samples using different methods. Top row $->$ bottom row: DeepONet, FNO, DE-FNO (Random), DINO, DE-DeepONet (KLE), DE-DeepONet (ASM). "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "WAiqLGfqX6/tmp/5478ceef1de86953403fba744a5db3a2ea58bd5beb45cd4f5e298ab151b9d000.jpg", "img_caption": ["Figure 20: Navier\u2013Stokes. Comparison of the predictions of directional derivative $d u_{2}(m;\\omega_{1})$ with (16, 64, 256, 1024) training samples using different methods. Top row $->$ bottom row: DeepONet, FNO, DE-FNO (Random), DINO, DE-DeepONet (KLE), DE-DeepONet (ASM). "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We include our paper\u2019s contributions in the abstract and the second paragraph of introduction and its scope in the first and third paragraphs of introduction. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We discuss the limitations in the last section. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide one theorem with clearly stated assumptions and proof. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We discuss all details (including data generation and model training) that help reproduce the experiments in the appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide the code necessary to generate the data and train and test the different models considered in the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We discuss the experimental details in the Appendix B. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We run five trails with different random seed for each method and report the mean value and standard deviation of the corresponding relative error. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide the machine information and time of execution in the Tables 4 and 6. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We have reviewed the code of ethics and checked that the research follows them. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our work lies in the field of scientific machine learning. We do not notice any societal impact for now. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: We think our research poses no such risk. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We cite the corresponding papers and respect the licenses when using the packages FEniCS, hIPPYlib, and neuraloperator to conduct experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not release new assets at the moment. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our research is in the field of the scientific machine learning and does not involve crowdsourcing or human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our research is in the field of the scientific machine learning and does not involve crowdsourcing or human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]