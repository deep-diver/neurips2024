{"importance": "This paper is important because it **significantly improves the accuracy of Deep Operator Networks (DeepONets)**, a popular method for solving many-query problems involving partial differential equations (PDEs).  The enhanced accuracy, particularly beneficial with limited data, **opens new avenues in various applications**, including Bayesian inference, optimization under uncertainty, and experimental design. This work is especially relevant to the growing field of physics-informed machine learning. The new method, DE-DeepONet, is also shown to be easily adapted to other neural operators, thus greatly increasing the usefulness and applicability of this approach.", "summary": "Derivative-enhanced DeepONets boost PDE solution accuracy and derivative approximation, particularly valuable with limited training data.", "takeaways": ["Derivative-enhanced DeepONets (DE-DeepONets) improve solution accuracy and derivative approximation of PDEs.", "DE-DeepONets are computationally efficient thanks to dimension reduction techniques, making them suitable for many-query problems.", "The derivative loss function in DE-DeepONets can be extended to other neural operators like FNOs."], "tldr": "Deep Operator Networks (DeepONets) are effective surrogate models for parametric partial differential equations (PDEs). However, they often struggle to accurately approximate solution derivatives, especially when training data is scarce. This paper introduces Derivative-enhanced DeepONets (DE-DeepONets) to address this limitation.  The key issue is that existing neural operators do not provide accurate estimates of the derivative of the output with respect to the input, which is necessary for many downstream tasks.\nDE-DeepONets improve upon DeepONets by incorporating derivative information into the training process and using a dimension reduction technique to reduce computational cost and improve training efficiency. Numerical experiments demonstrate that DE-DeepONets outperform existing methods in terms of both accuracy and computational efficiency, particularly when the amount of training data is limited.  Furthermore, the approach can easily be generalized to other neural operators such as the Fourier Neural Operator (FNO).", "affiliation": "Georgia Institute of Technology", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "WAiqLGfqX6/podcast.wav"}