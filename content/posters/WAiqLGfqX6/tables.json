[{"figure_path": "WAiqLGfqX6/tables/tables_7_1.jpg", "caption": "Table 1: Output reconstruction error with 16 input reduced bases", "description": "This table shows the relative L2 error for output reconstruction using KLE and ASM basis with 16 input reduced bases for both hyperelasticity and Navier-Stokes datasets.  It demonstrates the impact of the chosen dimension reduction technique on the accuracy of the output approximation.  The lower error values indicate a better approximation using the selected dimension reduction method. ", "section": "4.4 Main results"}, {"figure_path": "WAiqLGfqX6/tables/tables_8_1.jpg", "caption": "Table 2: Wall clock time (in seconds) for data generation on 2 \u00d7 AMD EPYC 7543 32-Core Processors", "description": "This table shows the time spent on different stages of data generation for the hyperelasticity and Navier-Stokes equations.  The data generation process includes generating Gaussian random fields (GRFs), solving the PDEs using 64 processors, calculating the KLE and ASM bases using 1 and 16 processors, respectively, and generating derivative labels using 64 processors. The number of samples used is N<sub>all</sub> = 2000, and the number of reduced basis functions is r = 16.", "section": "B.2 Data generation"}, {"figure_path": "WAiqLGfqX6/tables/tables_8_2.jpg", "caption": "Table 5: Total wall clock time (in seconds) for each model inferring on 500 test samples of both the solution and dm in 128 random directions, using a single GPU and a single CPU (except where specified)", "description": "This table presents the inference time (in seconds) for five different models (DeepONet, FNO, DE-FNO, DINO, and DE-DeepONet) when evaluating 500 test samples.  The inference involves calculating both the solution and its derivative (dm) in 128 random directions.  The time is measured using a single GPU and a single CPU, except for DINO where it uses both a GPU and a CPU, and a separate measurement is provided using 16 CPUs for DE-DeepONet. This demonstrates the relative computational efficiency of different methods for solving PDEs.", "section": "4.4 Main results"}, {"figure_path": "WAiqLGfqX6/tables/tables_8_3.jpg", "caption": "Table 5: Total wall clock time (in seconds) for each model inferring on 500 test samples of both the solution and dm in 128 random directions, using a single GPU and a single CPU (except where specified)", "description": "This table compares the inference time of different models (DeepONet, FNO, DE-FNO, DINO, and DE-DeepONet) on two datasets (Hyperelasticity and Navier-Stokes). The inference time is measured for 500 test samples and directional derivatives in 128 random directions.  The use of a single GPU and a single CPU is consistent, except for DINO which uses 1 GPU and 1 CPU or 16 CPUs depending on the approach, and the numerical solver which uses only 16 CPUs. The table highlights the computational efficiency of different models in the context of evaluating the solutions and derivatives.", "section": "4.4 Main results"}, {"figure_path": "WAiqLGfqX6/tables/tables_17_1.jpg", "caption": "Table 6: Configurations of data generation for different datasets", "description": "This table details the configurations used for generating the datasets employed in the paper's experiments.  It specifies the mesh resolution (64x64), the type of finite element used (CG), the polynomial degree of the finite element (deg=1 or deg=2), the number of training and testing samples (Ntrain, Ntest), the number of reduced basis functions (r), and the number of samples used for computing the ASM basis (Ngrad).  Separate configurations are given for the hyperelasticity and Navier-Stokes datasets, reflecting differences in problem formulation and solution space dimensions.", "section": "B.2 Data generation"}, {"figure_path": "WAiqLGfqX6/tables/tables_19_1.jpg", "caption": "Table 7: Training details for DeepONet", "description": "This table details the configurations used for training the DeepONet model on two datasets: Hyperelasticity and Navier-Stokes.  It specifies the architecture of both the branch net and trunk net (using CNN and ResNet respectively), including the number of hidden layers, output dimensions, hidden dimensions, activation functions (ReLU), and initialization method (Kaiming Uniform).  The table also provides hyperparameters for the AdamW optimizer (learning rate and weight decay), StepLR scheduler (gamma and step size), number of Fourier features, and the scale of Fourier feature embeddings (\u03c3).  The specific values of these parameters vary slightly between the Hyperelasticity and Navier-Stokes datasets.", "section": "B.4 Training details"}, {"figure_path": "WAiqLGfqX6/tables/tables_19_2.jpg", "caption": "Table 8: Training details for FNO & DE-FNO", "description": "This table details the hyperparameters used for training the Fourier Neural Operator (FNO) and the derivative-enhanced FNO (DE-FNO) models.  The hyperparameters include the number of modes, input and output channels, hidden channels, number of layers, lifting and projection channel ratios, activation function, AdamW optimizer learning rate and weight decay, and StepLR scheduler parameters.  Separate configurations are provided for the hyperelasticity and Navier-Stokes datasets.", "section": "B.4 Training details"}, {"figure_path": "WAiqLGfqX6/tables/tables_19_3.jpg", "caption": "Table 9: Training details for DINO", "description": "This table presents the configuration of the neural network architecture (ResNet), initialization method (Kaiming Normal), optimizer (AdamW) with learning rate and weight decay, learning rate scheduler (StepLR) with gamma and step size for the hyperelasticity and Navier-Stokes datasets in the DINO model.  The number of hidden layers, output dimension, and hidden dimension are also specified for both datasets.", "section": "B Experimental details"}, {"figure_path": "WAiqLGfqX6/tables/tables_20_1.jpg", "caption": "Table 10: Training details for DE-DeepONet", "description": "This table details the specific hyperparameters and settings used to train the DE-DeepONet model for both the hyperelasticity and Navier-Stokes datasets.  It specifies the architecture of the branch and trunk networks (both ResNet), the number of hidden layers, output dimensions, hidden dimensions, activation functions (ELU or ReLU), initialization method (Kaiming Uniform), AdamW optimizer parameters (learning rate and weight decay), whether the learning rate scheduler is disabled, the number of Fourier features used, the scale of the Fourier features (\u03c3), and the batch size (Nbatch) used during training.", "section": "B.4 Training details"}, {"figure_path": "WAiqLGfqX6/tables/tables_20_2.jpg", "caption": "Table 11: Number of trainable parameters in each model", "description": "This table shows the number of trainable parameters for four different neural operator architectures (DeepONet, FNO, DINO, and DE-DeepONet) applied to two different datasets (Hyperelasticity and Navier-Stokes).  It highlights the relative model complexity and provides context for the computational cost comparisons discussed in the paper. Note that DE-DeepONet uses significantly fewer parameters than FNO, suggesting a potential advantage in terms of computational efficiency.", "section": "4.4 Main results"}]