{"references": [{"fullname_first_author": "Agarwal", "paper_title": "Voql: Towards optimal regret in model-free rl with nonlinear function approximation", "publication_date": "2022-12-06", "reason": "This paper provides a near-optimal algorithm for reinforcement learning with general function approximation, which the current paper builds upon and improves."}, {"fullname_first_author": "Jin", "paper_title": "Provably efficient reinforcement learning with linear function approximation", "publication_date": "2020-00-00", "reason": "This paper is foundational for linear function approximation in reinforcement learning, providing theoretical guarantees that are extended in the current paper to the general setting."}, {"fullname_first_author": "He", "paper_title": "Nearly minimax optimal reinforcement learning for linear Markov decision processes", "publication_date": "2022-12-06", "reason": "This paper offers a near-optimal algorithm for linear Markov decision processes, which is relevant to the current paper's focus on sample efficiency."}, {"fullname_first_author": "Russo", "paper_title": "Eluder dimension and the sample complexity of optimistic exploration", "publication_date": "2013-00-00", "reason": "This paper introduces the concept of eluder dimension, a key complexity measure used in the current paper's theoretical analysis."}, {"fullname_first_author": "Wang", "paper_title": "Provably efficient q-learning with low switching cost", "publication_date": "2019-00-00", "reason": "This paper addresses the important issue of deployment efficiency in reinforcement learning, a focus that is also addressed by the current paper."}]}