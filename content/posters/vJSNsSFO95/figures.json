[{"figure_path": "vJSNsSFO95/figures/figures_1_1.jpg", "caption": "Figure 1: Analysis of Inherent Ambiguity in SAM. (a): Feeding SAM with slightly different prompts from multiple experts for a single image can significantly alter the segmentation output. (b)(c): We evaluate SAM using canonical box prompts and various perturbed versions, measuring the mean and variance of segmentation IoU on LIDC. Perturbations involve shifting the box five pixels in different directions and employing various granular outputs within SAM. Results highlight SAM's sensitivity to prompt variations and granularity.", "description": "This figure analyzes the inherent ambiguity in the Segment Anything Model (SAM).  Part (a) demonstrates how slightly different prompts from multiple experts lead to significantly different segmentation outputs for the same image. Parts (b) and (c) show quantitative results of an experiment on the LIDC dataset.  They measure the Intersection over Union (IoU)  for various perturbed versions of canonical box prompts.  Perturbations include shifts of the prompt box by 5 pixels in different directions and variations in the output granularity of SAM. The results highlight SAM's high sensitivity to both prompt variations and variations in object granularity.", "section": "1 Introduction"}, {"figure_path": "vJSNsSFO95/figures/figures_3_1.jpg", "caption": "Figure 2: A-SAM Training Pipeline. We probabilistically model the prompt and object-level ambiguity by jointly probabilities the SAM embeddings with PGN and IGN, respectively.", "description": "This figure illustrates the training pipeline of the A-SAM framework.  It shows how the framework probabilistically models both prompt and object-level ambiguity.  The process involves using a prompt encoder and image encoder to generate embeddings which are then input to a Prompt Generation Network (PGN) and an Image Generation Network (IGN).  These networks model the latent distribution of prompts and object granularities, respectively.  The output of PGN and IGN, along with the SAM embeddings, are used to generate mask outputs which are then compared to the ground truth using a cross-entropy loss.  The entire process is optimized using a variational autoencoder approach. Two posterior networks, one for prompts and one for images, are used during training to guide the learning process and ensure the generated distributions are aligned with the ground truth. The model's ability to generate diverse and reasonable segmentations results from this probabilistic modeling of ambiguity.", "section": "3 Method"}, {"figure_path": "vJSNsSFO95/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative comparison with prompted segmentation models adapted for ambiguous segmentation. Examples include three ground-truth expert labels and sampled segmentation masks.", "description": "This figure presents a qualitative comparison of the proposed A-SAM model with other prompted segmentation methods (SegGPT, SEEM, and SAM) for ambiguous segmentation tasks.  It shows the segmentation results for several examples, including the ground truth segmentation labels provided by multiple experts, highlighting how A-SAM handles ambiguities more effectively by generating a range of plausible segmentations instead of a single, potentially inaccurate one. The results from the other methods are shown for comparison, demonstrating limitations in dealing with ambiguous object boundaries and prompt variations.", "section": "4.2 Comparison to Prompted Segmentation Models"}, {"figure_path": "vJSNsSFO95/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative comparison with efforts specially designed for ambiguous segmentation. Examples include four ground-truth expert labels and sampled segmentation masks.", "description": "This figure compares the results of different ambiguous image segmentation methods, including PixelSeg, Mose, and the proposed A-SAM method, with the ground truth.  It visually demonstrates the ability of A-SAM to generate multiple plausible segmentations for ambiguous objects, showing a greater variety and accuracy compared to other methods.", "section": "4.3 Comparison to Conventional Ambiguous Segmentation Models"}, {"figure_path": "vJSNsSFO95/figures/figures_8_1.jpg", "caption": "Figure 5: Robustness analysis of our A-SAM framework over the SAM baseline against prompt perturbation.", "description": "This figure shows the robustness of the proposed A-SAM framework and the baseline SAM model against different prompt perturbations.  The x-axis represents the type of perturbation applied to the prompts (Shift, Scale, Shift&Scale, Shift+, Scale+, Shift+&Scale+), while the y-axis shows the Intersection over Union (IoU) metric. The blue line represents the performance of the SAM model, while the red line represents the performance of the A-SAM model. The dashed lines represent the performance of both models under standard prompts, which serves as an upper bound for comparison. The results demonstrate that the A-SAM model is more robust to prompt variations than the SAM model, maintaining a higher IoU even under severe perturbations.", "section": "4.4 Further Empirical Results"}, {"figure_path": "vJSNsSFO95/figures/figures_15_1.jpg", "caption": "Figure 3: Qualitative comparison with prompted segmentation models adapted for ambiguous segmentation. Examples include three ground-truth expert labels and sampled segmentation masks.", "description": "This figure shows a qualitative comparison of the proposed A-SAM model with other prompted segmentation models on ambiguous segmentation tasks. It displays the results of three different models (SegGPT, SEEM, and SAM) compared with the ground truth annotations. Each model's outputs are shown alongside the corresponding ground truth for three different ambiguous examples. This helps to visually demonstrate the strengths and limitations of each model in handling ambiguous segmentation scenarios.", "section": "4 Experiment"}, {"figure_path": "vJSNsSFO95/figures/figures_16_1.jpg", "caption": "Figure 3: Qualitative comparison with prompted segmentation models adapted for ambiguous segmentation. Examples include three ground-truth expert labels and sampled segmentation masks.", "description": "This figure shows a qualitative comparison of different segmentation models on ambiguous segmentation tasks.  It includes three ground truth expert labels for comparison and showcases sampled segmentation masks generated by each model (SAM w/ Box Shift, SEEM w/ Mask Shift, and the proposed A-SAM method). The visual comparison highlights the differences in accuracy and diversity of results between the models.", "section": "4.2 Comparison to Prompted Segmentation Models"}]