[{"type": "text", "text": "Latent Feature Mining with Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Predictive modeling often faces challenges due to limited data availability and   \n2 quality, especially in domains where collected features are weakly correlated   \n3 with outcomes and where additional data collection is constrained by ethical   \n4 or practical difficulties. Traditional machine learning (ML) models struggle to   \n5 incorporate unobserved yet critical factors. We propose a framework that leverages   \n6 large language models (LLMs) to augment observed features with latent features,   \n7 enhancing the predictive power of ML models in downstream tasks. Our novel   \n8 approach transforms the latent feature mining task to a text-to-text propositional   \n9 reasoning task. We validate our framework with a case study in the criminal justice   \n10 system, a domain characterized by limited and ethically challenging data collection.   \n11 Our results show that inferred latent features align well with ground truth labels and   \n12 significantly enhance the downstream classifier. Our framework is generalizable   \n13 across various domains with minimal domain-specific customization, ensuring easy   \n14 transfer to other areas facing similar challenges in data availability. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 In numerous application domains, predicting individual outcomes and optimizing resource planning   \n17 are critical but often limited by gaps in data availability and quality. Despite the popular belief that   \n18 we operate in a \u201clarge data regime,\u201d many decisions, especially those impacting human lives, have to   \n19 be made based on small amounts of data with limited features, such as in criminal justice, healthcare,   \n20 and social services (Lu et al., 2021; Yuan et al., 2023). This poses both technical limitations and   \n21 ethical concerns. Traditional ML models, while powerful, are limited by the availability of collected   \n22 (observed) data features. This limitation is especially prominent when it comes to incorporating   \n23 unstructured data or inferring nuanced relationships between observed features and the outcomes. In   \n24 this paper, we explore how domain-informed language models can help identify latent (unobserved)   \n25 features and improve prediction accuracy for downstream tasks.   \n26 We illustrate our motivation with an example from the criminal justice setting. Predicting an   \n27 individual\u2019s in-program revocation probability (chance of committing a new crime during probation)   \n28 is critical for determining their eligibility for incarceration-diversion programs and for planning   \n29 resources like staffing ratios (Rotter and Barber-Rioja, 2015; Li et al., 2024). Typically, the data   \n30 collected includes only a limited set of features, e.g., basic demographic and criminal history   \n31 information. Crucial factors such as socio-economic status, community support availability, or   \n32 psychological proflies, which significantly influence outcomes, are often missing from these datasets.   \n33 Collecting such sensitive information can be invasive and raises ethical concerns. Additionally, the   \n34 process of gathering these data can be logistically challenging and resource-intensive. Human case   \n35 managers in these settings often have the advantage of drawing on their professional experience   \n36 and human intuition to infer these critical but unrecorded details from observed data. In contrast,   \n37 traditional ML models cannot reason beyond the explicit data provided, leading to predictions based   \n38 on incomplete information. This limitation not only undermines the accuracy of the models but also   \n39 poses concerns regarding the fairness of decisions derived from such data. Moreover, ML models are   \n40 not designed to handle unstructured data like case notes, which may contain contextual insights to   \n41 improve prediction accuracy.   \n42 Recent advancements in large language models (LLMs) offer a promising avenue to bridge these data   \n43 gaps (Brown et al., 2020; Ouyang et al., 2022; Achiam et al., 2023). LLMs are capable of processing   \n44 and generating information in a way that mimics human reasoning, allowing for the inference of latent   \n45 features that are not directly observable but are critical for accurate predictions and decision-making.   \n46 They can also analyze both structured and unstructured data to offer a holistic view of the underlying   \n47 factors influencing individual outcomes.   \nOur proposed framework leverages LLMs to augment observed features collected in given datasets   \n49 with latent features, enhancing the predictive power of ML models for downstream tasks such as   \n50 classifications. Unlike conventional data augmentation approaches to increase the sample size, we   \n51 train LLMs to infer underlying socio-economic conditions, treatment needs, and other critical but   \n52 often unrecorded characteristics from collected features. This augments the feature space $X$ to   \n53 improve predictions. Additionally, our framework enables generating more complete and realistic   \n54 synthetic data points via learned correlations between observed and unobserved features for simulation   \n55 and counterfactual policy analysis. We summarize our main contributions as follows.   \n56 1. We introduce a novel approach to formulate latent feature mining as text-to-text propositional   \n57 logical reasoning. This approach effectively infers latent features from observed features, offering   \n58 significantly improved accuracy and interpretability compared to alternative approaches.   \n59 2. We develop a four-step framework to implement our approach, which is generalizable with   \n60 minimal domain-specific customization and has remarkably low human-annotated training data   \n61 requirements. This framework expands data utility by enhancing downstream predictions without   \n62 additional invasive or forbidden data collection.   \n63 3. We empirically validate our framework in the criminal justice setting to address weak observed   \n64 features and unbalanced datasets. Designed as a plug-and-play solution, we demonstrate our   \n65 framework\u2019s adaptability through two different prediction tasks, making it valuable for various   \n66 applications with similar challenges. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "67 2 Background and Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "68 Data Augmentation and Latent Feature Extraction. Data augmentation is a technique commonly   \n69 used in AI (Van Dyk and Meng, 2001). Generative models, such as Generative Adversarial Networks   \n70 (GANs) and Variational Autoencoders (VAEs), learn data patterns and generate synthetic data to   \n71 augment training sample size (Goodfellow et al., 2014; Kingma and Welling, 2013). Unlike these   \n72 approaches, our framework leverages LLMs to augment the features of different individuals. Trained   \n73 on crowd-sourced data rich in human behavior and societal context, LLMs have the potential to   \n74 enhance feature spaces for social computing and operations improvement.   \n75 Latent features are hidden characteristics in a dataset that are not directly observed but can be   \n76 inferred from available data. Incorporating meaningful latent features can enhance the performance   \n77 of downstream applications (Zhai and Peng, 2016; Jiang et al., 2023). Two common approaches to   \n78 infer latent features are human annotation and machine learning models. Human annotation, while   \n79 reliable, is often expensive and time-consuming. It requires significant effort and resources, making   \n80 it impractical for large-scale tasks. Machine learning methods like Expectation-Maximization (EM)   \n81 and VAEs offer alternative techniques to infer latent features from observed data. EM algorithms   \n82 estimate latent variable assignments and update model parameters to maximize data likelihood, but   \n83 their results can be hard to interpret and require strong parametric assumptions. Similarly, VAEs use   \n84 probabilistic approaches to describe data distribution with latent variables, but the learned mappings   \n85 can also be difficult to interpret.   \n86 Synthetic Data for Training. Fine-tuning is a promising approach for LLMs to reduce hallucinations   \n87 and align outputs with real-world data and human preferences (Tonmoy et al., 2024; Qiao et al.,   \n88 2022; Hu et al., 2021). Synthetic data has proven to be an effective, low-cost alternative to real data   \n89 to improve the LLMs\u2019 reasoning performance across various domains (Liu et al., 2024). Studies   \n90 by (Zelikman et al., 2022), (Wang et al., 2022) demonstrate that synthetic data improves model   \n91 generalization and robustness. Our approach also uses synthetic data to augment training during   \n92 fine-tuning. Unlike existing work that directly mimics observed features, we are one of the first   \n93 to formulate the generation of synthetic latent features as a reasoning task. Our approach employs   \n94 few-shot prompting to create synthetic data that infers these latent features, followed by fine-tuning   \n95 to enhance model accuracy and reduce hallucinations. This technique falls under the self-instruction   \n96 paradigm, where models iteratively learn from augmented data.   \n97 Note that we distinguish between augmenting the feature space and augmenting training data. Our   \n98 primary goal is to augment the feature space by inferring and adding latent features to the observed   \n99 data to improve downstream predictions. As part of the steps in our framework to achieve this   \n100 goal, we augment training data for LLM fine-tuning with synthetic samples to improve the model\u2019s   \n101 reasoning capabilities.   \n102 Incarceration-diversion Programs and Data Description. This work conducts case studies on   \n103 incarceration-diversion programs, which aim to support individuals who have committed minor   \n104 offenses by providing community-based services to improve societal reintegration and reduce re  \n105 cidivism. Eligible individuals were diverted from traditional incarceration to such programs after   \n106 risk assessment and screening. Case managers determined specific program requirements, such as   \n107 substance use treatment and cognitive-behavioral therapy. There are four types of program outcomes:   \n108 Completed (successfully completed the program), Revoked (committed new crimes while in the   \n109 program), Not Completed (unable to finish for various reasons), and Other (unrecorded reasons).   \n110 We obtained de-identified data from our community partner for a state-wide incarceration-diversion   \n111 program in Illinois. The consolidated dataset includes records of adult participants admitted to the   \n112 program. The collected data features include timestamps such as the arrival and termination dates to   \n113 the program, program outcomes, and individual features such as the race, gender, education, county,   \n114 marriage status, housing, risk assessment scores, prior crime history, and sources of referral (e.g.,   \n115 from probation officer or from the court). See Appendix F for summary statistics. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "116 3 The Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "117 In this section we formally describe our problem setting that leverages latent features to enhance   \n118 downstream tasks. The downstream task we focus on is a multi-class classification problem, but the   \n119 framework can easily extend to other downstream prediction tasks such as regression problems.   \n120 In a standard multi-class classification problem setting, suppose we have a dataset $\\textit{D}=$   \n121 $(x_{1},y_{1}),(x_{2},y_{2}),\\dotsc,(x_{n},y_{n})$ , where $x_{i}$ is a $d$ -dimensional vector representing the input features   \n122 $X\\ \\in\\ X$ and $y_{i}\\ \\in\\ \\mathcal{V}\\,=\\,\\{1,2,...\\,,C\\}$ denotes the corresponding class label $Y$ for individual   \n123 $i=1,\\dots,n$ . The goal is to learn a classifier $f:\\mathcal X\\to\\mathcal Y$ that accurately predicts the class labels.   \n124 Consider the following scenarios in which $f$ struggles to capture the relationship between $X$ and $Y$ :   \n125 1. The size of the training dataset is small relative to the complexity of the classification task or the   \n126 dimensionality of the feature space;   \n127 2. When the input features $X$ are weakly correlated with class labels $Y$ , the input features may not   \n128 provide discriminating information to accurately predict the corresponding class labels.   \n129 To address these challenges, we could use additional informative features to enhance the classifier\u2019s   \n130 ability to capture the relationship between $X$ and $Y$ . Latent features can serve such a purpose. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Definition of Latent Features. ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Latent features, denoted as $Z$ , represent underlying attributes that are not directly observed within the dataset but are correlated with both the observed features $X$ and the class labels $Y$ . We use a function $g$ with $Z=g(X)$ to denote the correlations between the latent features and the observed features $X$ . As shown in figure 3, latent features $Z$ are correlated with $X$ and $Y$ . One can learn the latent features from the original features $X$ and augment the features $f(\\mathbf{X},\\mathbf{Z})$ to learn the classifier $Y$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underbrace{\\widehat{\\mathbf{x}}}_{t(x)\\setminus\\displaystyle\\sum_{i=1}^{g(x)},t(z)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "131 In typical ML settings, latent features primarily reduce the dimensionality of the feature space.   \n132 Beyond this, latent features can capture discriminative information not explicitly present in the original   \n133 features. Our approach focuses on this latter benefit, extracting informative latent representations   \n134 to help classifiers better differentiate between classes. Essentially, $Z$ acts as ensemble features   \n135 derived from the original features $X$ , capturing complex patterns that individual features might miss,   \n136 especially when $X$ is weakly correlated with the outcome $Y$ .   \n137 While this approach seems beneficial intuitively, it is important to note that adding more features is   \n138 not always helpful if the extracted features are not meaningful and introduce noise. In the following   \n139 lemma, we show in a simple logistic regression setting that while adding features can reduce in-sample   \n140 loss, it does not always reduce out-of-sample loss if the added features are not informative. We use   \n141 the log-loss (the cross-entropy loss) of the logistics regression for binary outcome $Y\\in\\{0,1\\}$ . We   \n142 denote the optimal coefficients that minimize the in-sample log-loss function as $\\beta^{*}$ for the original   \n143 features and $\\tilde{\\beta}^{*}$ for the augmented features.   \n144 Lemma 1. The in-sample log-loss always follows $\\underline{{\\ell}}^{i n}(\\tilde{D},\\tilde{\\beta}^{*})\\,\\leq\\,\\underline{{\\ell}}^{i n}(D,\\beta^{*})$ . When the added   \n145 features are non-informative, there exist instances such that the out-of-sample log-loss $\\mathcal L^{o u t}(\\tilde{D},\\tilde{\\beta}^{*})>$   \n146 $\\mathcal{L}^{o u t}(D,\\beta^{*})$ .   \n147 The results in the lemma can be generalized to multi-class labels. Since augmenting the feature space   \n148 is not necessarily beneficial unless the added features are meaningful, a major part of our case study   \n149 is to empirically test whether the extracted features from our framework indeed improve downstream   \n150 prediction. If the added features significantly enhance downstream prediction accuracy, this provides   \n151 strong evidence that the inferred latent features are meaningful. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "152 4 Latent Feature Mining with LLMs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "153 To overcome the limitations of existing approaches, we propose a new approach to efficiently and   \n154 accurately extract latent features and augment observed features to enhance the prediction accuracy.   \n155 At a high level, our approach transform the latent feature mining as a text-to-text propositional   \n156 reasoning task, i.e., infer the relationship $Z=g(X)$ through logical reasoning with natural language.   \n157 Following the framework established in previous work (Zhang et al., 2022), we denote the predicates   \n158 related to the observed features as $P_{1},P_{2},\\dots,P_{m}$ . Consider a propositional theory $S$ that contains   \n159 rules that connect $P$ \u2019s to the latent feature $Z$ . We say $Z$ can be deduced from $S$ if the logic implication   \n160 $(P_{1}\\land P_{2}\\land\\dotsc\\land P_{m})\\rightarrow Z$ is covered in $S$ . For potentially complicated logical connections between   \n161 $P$ \u2019s and $Z$ , we also introduce intermediate predicates $O$ \u2019s and formulate a logical chain (a sequence   \n162 of logical implications) that connects $X$ to the latent features $Z$ as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nX\\rightarrow(P_{1}\\land P_{2}\\land\\dotsc\\land P_{m})\\rightarrow(O_{1}\\land O_{2}\\land\\dotsc\\land O_{\\ell})\\rightarrow Z.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "163 Our approach formulates this logical chain as a multi-stage Chain of Thoughts (CoT) prompt template,   \n164 and then guide LLMs to infer $Z$ from $X$ using the prompt template. Specifically, we first extract   \n165 predicates $P$ \u2019s from $X$ . Then we infer intermediate predicates with a rule $(P_{1}\\land P_{2}\\land\\dotsc\\land P_{m})\\rightarrow O_{l}$   \n166 for $l=1,\\ldots,\\ell-1$ , and forward the intermediate predicates into the next stage to infer ${\\cal O}_{l+1}$ . Finally,   \n167 we infer latent features with $\\left(O_{1}\\land O_{2}\\land\\dotsc\\land O_{\\ell}\\right)\\rightarrow Z$ . With the formulated multi-stage CoT prompt   \n168 template, we generate synthetic data to fine-tune LLMs to enhance the logical reasoning ability of   \n169 LLMs in self-instruct fashion (Wang et al., 2022), and ensure that the generate text is aligned with   \n170 each step of our desired \u201cchain of reasoning\u201d format.   \n171 We use a hypothetical example from our case study setting to illustrate the formulation of the logic   \n172 chain. The blue (leftmost) box in Figure 1 shows the observed feature $X$ for one individual. Examples   \n173 for the predicates $P$ \u2019s formulated from $X$ could be:   \n74 $P_{1}$ :\u201cthe client has part-time job\u201d, $P_{2}$ : \" the client hasn\u2019t complete high school\",   \n75 $P_{3}$ :\u201cthe client is single\u201d, $P_{4}$ : \"the client has drug issue\", $P_{5}$ :\" the client lives in   \n76 high crime area\", $P_{6}$ : \" the client is assessed with high risk\" ...   \n177 To infer the latent feature $Z-\\mathrm{in}$ this example, the required programs to attend during probation \u2013 we   \n178 go through a multi-stage reasoning to infer the intermediate predicates $O$ \u2019s; see the white (middle)   \n179 boxes in Figure 1. One example logic that connects $P$ \u2019s to $O$ \u2019s could be:   \n180 $P_{1}=$ \"The client has unstable employment\"   \n181 $P_{2}=$ \"The highest education level of client is less than 10th grade\"   \n182 $O_{1}=\\,^{\\prime\\prime}$ The client has low socioeconomic status\"   \n183 If $(P_{1}\\land P_{2}\\land O_{1})\\in S$ , then $O_{1}$ is True.   \n184 Finally, with $P$ \u2019s and $O$ \u2019s, we can connect $X$ with $Z$ though the logic chains. One example of the   \n185 logical chain is as follows:   \n186 \u201cThe client is grappling with unstable employment and a relatively low educational   \n187 level, factors that likely contribute to a low socioeconomic status. Additionally,   \n188 being single, struggling with drug issues, and residing in a high-crime area further   \n189 exacerbate the lack of positive social support. Given these circumstances, education   \n190 could serve as a valuable intervention. Community service can be particularly   \n191 beneficial for someone who is single and may lack a broad support network.   \n192 Substance abuse treatment is crucial for individuals from lower socioeconomic   \n193 backgrounds to aid in recovery from substance abuse. Hence we can choose   \n194 education, substance abuse treatment, community service for this client.\u201d   \n195 Here, \u201cunstable employment and a relatively low educational level\u201d and \u201cbeing single, struggling   \n196 with drug issues, and residing in a high-crime area\u201d are $P$ \u2019s extracted from the features $X$ , while   \n197 \u201ca low socioeconomic status\u201d and \u201clack of positive social support\u201d are $O$ \u2019s. Finally, the rationales   \n198 \u201ceducation could serve as a valuable intervention . . . recovery from substance abuse. Hence we   \n199 can choose education, substance abuse treatment, community service for this client connect the   \n200 intermediate predicates to the latent variables $Z$ (program requirements) we want to infer, i.e.,   \n201 $Z_{1}\\!=$ \u2018education\u2019, $Z_{2}{=}$ \u2018substance abuse treatment\u2019, $Z_{3}{=}^{\\prime}$ \u2018community service\u2019. ", "page_idx": 3}, {"type": "image", "img_path": "Mdd3f8Cui8/tmp/3b445a423884457e88be49a31cad12e49a4c43072f4842a2a0a30462b74e1f94.jpg", "img_caption": ["Figure 1: Example of latent feature mining through chain of reasoning "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "Mdd3f8Cui8/tmp/618aac541fef5522bcc0f6458e328f788c1eaf82effb655386b75cd9619e7f3a.jpg", "img_caption": ["Figure 2: Overview of latent feature inference framework. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "202 Figure 2 illustrates the full process of of our proposed framework with four steps. ", "page_idx": 4}, {"type": "text", "text": "203 (1) Formulate baseline rationales: The first step is to formulate baseline rationales, whic serve as   \n204 guidelines for LLMs to infer latent features from observed ones. This involves two sub-steps:   \n205 \u2013 The first sub-step is to develop some baseline rationales, i.e., identify observed features potentially   \n206 correlated with latent features and formulate their relationships \u2013 the logic chain that connects $X$ to $Z$ .   \n207 Sources to help formulate these baseline rationales include established correlations (e.g., risk score   \n208 formulas), human input, and external information like socio-economic status in the neighborhood.   \n209 \u2013 In the second sub-step, we craft prompts with interactive alignment. This is a critical component   \n210 to establish correct reasoning steps for prompts used in Step 2 to generate synthetic rationales. We   \n211 involve human who are experienced in the domain to provide a prompt template for LLMs to generate   \n212 rationales aligned with the baseline rationales, then test the prompt template on a few examples   \n213 using zero-shot. If the LLM fails to certain example, we provide the ground truth back to the LLM,   \n214 allowing it to revise the prompt template (Miao et al., 2023). This process iteratively refines the   \n215 template until LLMs consistently generate the desired output for all selected examples.   \n216 (2) Enlarge data with synthetic rationales for fine-tuning: We generate synthetic training data in   \n217 self-instruct fashion (Wang et al., 2022). With a handful of examples of the baseline rationales as a   \n218 reference, we then guide the LLMs via in-context learning to generate similar rationales to enlarge   \n219 the training data samples. To ensure the quality and diversity of the generated dataset, we introduce   \n220 human-in-the-loop interventions to filter out low-quality or invalid data based on heuristics. We   \n221 also leverage automatic evaluation metrics for quality control, e.g., removing data that lack essential   \n222 keywords.   \n223 (3) Fine-tuning LLMs: To enhance the reasoning capabilities of the LLMs and better align their   \n224 outputs in specific domains, we employ a fine-tuning process which utilizes the processed dataset   \n225 from the previous step (Qiao et al., 2022). Fine-tuning not only boosts the accuracy and reliability of   \n226 the LLMs, but also significantly improves their ability to reason with complex inputs, and reducing   \n227 hallucination (Tonmoy et al., 2024).   \n228 (4) Latent feature inference: The fine-tuned model is able to mirror the nuanced decision-making   \n229 process of human experts. We use the fine-tuned model to identify latent features and feed them into   \n230 downstream prediction tasks.   \n231 Regarding the generalizability of our framework, Steps 2-4 rely primarily on the mechanics of   \n232 LLMs, which naturally have a high degree of adaptability across different domains. Step 1, which   \n233 involves the identification and formulation of baseline domain-specific rationales, requires more   \n234 expert knowledge. To assist with Step 1, our interactive-alignment strategy can help craft effective   \n235 prompts by allowing iterative refinement based on feedback, reducing the burden on domain experts. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "236 5 Experiments Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "237 In this section, we demonstrate the efficacy of our proposed framework on a unique dataset from   \n238 a state-wide incarceration diversion program as described in Section 2. We design two sets of   \n239 experiments to empirically investigate: (1) Can our approach accurately imitate the human thinking   \n240 process to infer latent features? (2) Is our approach more effective than alternative techniques to infer   \n241 latent features? (3) Does our approach enhance the performance of downstream prediction tasks?   \n242 In the first experiment, we treat the risk level of individuals as a latent feature, despite it being collected   \n243 in the dataset. This experiment examines whether the latent features $\\hat{Z}$ inferred by LLMs match well   \n244 with the actual features $Z$ . In the second experiment, we assume that the program requirements are   \n245 latent features, which lack ground truth labels for most individuals (only a few dozen individuals   \n246 have the program requirements recorded in the data). We first have LLMs deduce these requirements,   \n247 then add them to the downstream prediction task of program outcomes $Y\\sim f(X,{\\hat{Z}})$ and evaluate   \n248 whether the prediction accuracy is improved, i.e., the inferred features are indeed beneficial and not   \n249 detrimental (recall the results in Lemma 1). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "250 5.1 Risk Level Prediction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "251 Task Description. In this task we treat an observed feature\u2014Risk Level\u2014as the latent feature to   \n252 infer. The task is a multi-classification problem to learn $Z\\sim g(X)$ among four labels for the latent   \n253 variable $Z\\in\\{m o d e r a t e,h i g h,v e r y\\_\\bar{h}i g h\\}$ based on each client\u2019s profile $X$ .   \n254 Implementation Details. We implement our proposed framework as follows. All prompt templates   \n255 are attached to Appendix section C.   \n256 - Step 0. Profile writing: In this pre-processing step, we translate structured profile data $X$ into   \n257 text that can be better handled by LLMs, i.e., formulating predicates $P$ \u2019s from the features $X$ . To   \n258 enrich the profile with important in formations that could potential benefits the following steps,   \n259 we formulate the intermediate predicates $O$ \u2019s, where we prompt LLMs to extract and summarize   \n260 underlying information such as background, socio-economic status, and challenges in two or three   \n261 sentences. We then merge these sentences into the client\u2019s proflie. We use zero-shot prompting with   \n262 GPT-4 for this step.   \n263 - Step 1. Formulating rationales: Using human input, established risk score calculations (Corrections),   \n264 and the code book with risk calculation details provided by our community partner, we summarize a   \n265 general rule for inferring risk levels from the predicates, i.e., establishing the logic chains from $P$ \u2019s   \n266 and $O$ \u2019s to $Z$ . We then sample 40 client features from the dataset and manually formulate 40 baseline   \n267 rationales that logically connect features to corresponding risk levels and that are aligned with the   \n268 high-level general rule. To avoid the primacy effect of LLMs, we rate risk scores from 0 to 10 to add   \n269 variability in the labels, categorized as follows: 0-4 (moderate risk), 4-7.5 (high risk), and 7.5-10   \n270 (very high risk).   \n271 - Step 2. Enlarge fine-tuning data: With the 40 baseline rationales, we generate additional synthetic   \n272 rationales. We sample client features and corresponding ground truth risk scores from the dataset,   \n273 using one of the 40 rationales as an example, to prompt LLMs to produce similar narratives with CoT   \n274 prompts. In total we got 3000 rationales for the training data.   \n275 - Step 3. Fine-tune LLMs: Our framework is designed to be plug-and-play, allowing the synthetic   \n276 data generated in the previous step to be used across different language models. We fine-tune two   \n277 pre-trained language models for cross-validation purposes: GPT-3.5 and Llama2-13b(OpenAI, 2021).   \n278 We use OpenAI API to fine-tune GPT-3.5-turbo-0125 (Touvron et al., 2023; OpenAI). We fine-tune   \n279 Llama2-13b-chat using LoRA (Hu et al., 2021).   \n280 - Step 4. Inference with LLMs: We prompt fine-tuned LLMs to infer risk level ${\\hat{Z}}_{i}$ from features $X_{i}$   \n281 for each client $i$ in the test data and evaluate the out-of-sample accuracy by comparing the inferred   \n282 latent variable (risk level) $\\hat{Z_{i}}$ with the ground truth label $Z_{i}$ .   \n283 Evaluation. We choose ML classifiers (e.g., Neural Networks or Gradient Boosting Trees) as the   \n284 baseline to infer ${\\hat{Z}}_{i}$ from features $X_{i}$ . We compare the prediction performance of ${\\hat{Z}}_{i}$ inferred from   \n285 our approach with that from ML models using out-of-sample accuracy and F1 score. Additionally,   \n286 we evaluate the quality of generated text with an automatic evaluation metric. In the pre-processing   \nstep, we assess the keyword coverage rate in the generated profile assuming each feature value is   \n288 a keyword. For synthetic rationales, we use YAKE, a pretrained keyword extractor (Campos et al.,   \n289 2020), to identify keywords. We then evaluate the keyword coverage rate with a rule-based detector   \n290 to determine how many logical information points are covered. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "291 5.2 Outcome Prediction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "292 Task Description. In this task, we treat the program requirements (e.g., substance treatment,   \n293 counseling) for each client as the latent features $Z$ and use them to augment the original feature $X$ for   \n294 outcome prediction, which is a multi-classification problem to learn $Y\\sim f(X,Z)$ among four labels   \n295 for the outcome $Y\\in\\{C o m p l e t e d,R e v o k e d,N o t C o m p l e t e d,O t h e r\\}$ . The raw dataset does not   \n296 record the program requirements except for a very few clients; thus, the latent feature $Z$ in this task   \n297 is truly unobservable (in contrast to the one used in the first task). Available program requirement   \n298 options for this task are attached to the appendix section D.   \n299 Implementation Details. Steps 0 and 2-4 remain almost the same as in the risk-level prediction   \n300 task. Step 1 requires a slight adjustment (as discussed in Section 4, this step is the main part in   \n301 our framework that requires customization). Here, we formulate 40 baseline rationales in step 1 to   \n302 deduce clients\u2019 program requirements from their features. We leverage multi-stage prompting strategy   \n303 (Qiao et al., 2022) to break down the task into three sub-tasks: (1) identify the main challenges   \n304 from the client\u2019s proflie, (2) rank these challenges by priority, (3) match the challenges with suitable   \n305 requirements. Particularly, the third task is our main goal, with the first two serving as steps to   \n306 streamline the process and simplified the task.   \n307 Evaluation. We train an ML classifier to predict outcomes with and without the inferred latent   \n308 features, i.e., $\\hat{Y_{i}}\\;\\sim\\;f(X_{i},\\hat{Z}_{i})$ versus $\\hat{Y}_{i}\\;\\sim\\;f(X_{i})$ . We evaluate the out-of-sample accuracy by   \n309 comparing the predicted outcome ${\\hat{Y}}_{i}$ with the true label $Y_{i}$ in the test data. This comparison allows us   \n310 to assess whether incorporating the latent features enhances the classifier\u2019s performance. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "311 6 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "312 In this section, we demonstrate experiments results for two case studies we designed and additional   \n313 results for sensitivity analyses. ", "page_idx": 6}, {"type": "text", "text": "314 6.1 Risk Level Prediction Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "315 As mentioned in Section 5.1, we infer risk level on the client\u2019s profile. We compare our approach\u2019s   \n316 performance to baseline ML model\u2019s performance using the accuracy score and F1 score. Before   \n317 showing this performance comparison, we first show results on the generated text quality.   \n318 Generated Text Quality. For profile writing in Step 0, we treat each individual feature in $X_{i}$ as a   \n319 keyword to cover, and measure the keyword coverage rate. The generated profiles demonstrated an   \n320 average keyword coverage rate of $98\\%$ , indicating that they effectively capture the most important   \n321 information from the original data. For the generated synthetic rationales in Step 2, we treat terms   \n322 such as age, gender, employment, and education as critical keywords and assess their coverage rate.   \n323 The fine-tuned GPT-3.5 and Llama2-13b-chat both achieved a keyword coverage rate of $100\\%$ . This   \n324 indicates that the generated content adheres strictly to the guidelines established in the training data,   \n325 ensuring that all necessary information is accurately represented.   \n326 Latent Variable Inference Performance. As shown in Figure 3(a), our approach achieves the   \n327 highest overall accuracy. In particular, the fine-tuned GPT-3.5 achieves an accuracy that is $20\\%$   \n328 higher than other baseline ML approaches. The reason that ML models struggle to predict well   \n329 is due to the fact that there is no strong correlation between the observed features and the targets   \n330 (risk level); see the correlation plot in Appendix F. In contrast, our approach demonstrates superior   \n331 performance, since it more effectively handles datasets with subtle or non-obvious relationships   \n332 between the observed and target variables. This result shows that our approach is able to make   \n333 accurate inference of latent features and outperforms traditional ML approaches.   \n334 Table 3(b) details the prediction performance by class, showing F1 scores for each class using ML   \n335 models and our approach. Notably, all ML models struggle with the \u2018Very High Risk\u2019 category   \n336 \u2013 this category is often misclassified as \u2018High Risk\u2019 due to similar feature distributions of these   \n337 two categories and unbalanced data (only 371 training points for \u2018Very High Risk\u2019). In contrast,   \n338 our approach significantly improves the prediction performance for this category, highlighting its   \n339 effectiveness for unbalanced datasets. This improvement is likely because our LLM-based approach   \n340 has intermediate steps (proflie writing to obtain the socio-economic status and other contextual factors   \n341 in step 0 and connecting these factors with the latent variables in step 1), which help capturing the   \n342 subtle distinctions between \u2018High Risk\u2019 and \u2018Very High Risk\u2019 that are not explicitly recorded. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "Mdd3f8Cui8/tmp/b1ed658249d6f5a13b8a0d897674054a5130d4b4f263c27ebe570a48c1cf0e58.jpg", "img_caption": ["Figure 3: Risk level prediction results: (a) Model accuracy; (b) F1 scores per-category. LR - logistic regression; MLP - Neural Networks; RF- random forest; GBT - Gradient Boosting Trees. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "343 6.2 Outcome Prediction Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "344 As mentioned in Section 5.2, we infer program requirements as additional latent features and use   \n345 them for the downstream outcome prediction task. We compare the performance of the downstream   \n346 classifiers that trained with and without the latent features. Note that in the first task (risk-level   \n347 inferrence), GPT3.5 demonstrated better performance than llama2-13b. Thus, we focused on fine  \n348 tuning GPT-3.5 when using our approach for this task.   \n349 As illustrated in Table 4(a), incorporating latent features significantly improves the performance   \n350 of the downstream classifiers. Specifically, the addition of latent features increases the ROC AUC   \n351 score of Logistic Regression from 0.70 to 0.89 and from 0.84 to 0.92 for the Gradient Boosting Tree.   \n352 Furthermore, the feature importance in Figure 4(b) shows that the inferred features \u2013 \u2018requirement_1\u2019,   \n353 \u2018requirement $\\mathbf{\\mathcal{Z}^{\\prime}}$ , and \u2018requirement_ $\\mathit{\\Omega}_{3},$ \u2013 are among the top-ranked features. This implies the significant   \n354 relevance of these features on the downstream classification task. Hence, we can conclude that our   \n355 approach has the capability of enhancing the downstream classifier\u2019s accuracy with inferred   \n356 latent features. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "Mdd3f8Cui8/tmp/f1af0f57e1ce4e38e4e28a67a630c2a4de5f716edd28dda768836d4cd3ad54f2.jpg", "img_caption": ["(a) Model Performance "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Mdd3f8Cui8/tmp/d0168822bf4e7618866f0db1a40592aa42442fb89b2d83758ad23446c1aeb20b.jpg", "img_caption": ["(b) Feature Importance Plot "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Outcome prediction results: (a) Model performance with/without the inferred latent features (program requirements); (b) feature importance plot. LR - logistic regression; MLP - Neural Networks; GBT - Gradient Boosting Trees. ", "page_idx": 8}, {"type": "text", "text": "357 6.3 Sensitivity Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "358 In our sensitivity analysis, we further investigate the following three questions: (1) How sensitive is   \n359 our approach to the quality of human guidelines? (2) How important is fine-tuning in our framework?   \n360 For the first question, perhaps not surprisingly, our approach is sensitive to human guidelines,   \n361 specifically the baseline rationales and prompt templates formulated in Step 1. We have conducted   \n362 an ablation study to determine the optimal level of details required in the prompts. As shown in   \n363 Figure 9 in Appendix D, the best performance was achieved with the most reasoning steps and a   \n364 sentence length of two per step. In other words, increasing the number of reasoning steps allows   \n365 us to decompose the task into simpler components and enhances the performance of LLMs. More   \n366 importantly, while human guidelines are important, the interactive self-revise alignment strategy   \n367 can significantly help during the sub-step of Step 1 (prompt crafting). By providing ground truth and   \n368 encouraging self-reflection, GPT-4 can revise the prompt template to include crucial details, ensuring   \n369 a more accurate evaluation.   \n370 The answer to the second question is that fine-tuning is necessary. We have conducted another   \n371 ablation study, where we repeated the risk-level prediction task with zero-shot, one-shot, and three  \n372 shot prompting to compare with our fine-tuned model. In zero-shot, we provided only the task   \n373 description. In one-shot and three-shot, we included randomly selected human-verified examples.   \n374 Accuracy rankings from lowest to highest were: three-shot $(40\\%)$ , zero-shot $(55\\%)$ , one-shot $(60\\%)$ ,   \n375 and the fine-tuned model $(75\\%)$ ; see Table 9 in Appendix D. The three-shot\u2019s poor performance   \n376 may be due to information loss from long inputs. Zero-shot responses are highly variable and not   \n377 well-suited for downstream tasks. Although one-shot showed improvement, the fine-tuned model   \n378 significantly outperformed all others. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "379 7 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "380 This study presents a framework that leverages the capabilities of LLMs to enhance the prediction   \n381 accuracy in downstream tasks without necessitating invasive data collection methods. Our approach   \n382 reduces the need for collecting extensive personal data, thus mitigating privacy concerns. This aligns   \n383 with ethical data usage standards, especially in sensitive domains. Note that we do not explicitly   \n384 address bias in the data or LLM reasoning processes in this paper. We excluded the \u2018race\u2019 feature in   \n385 our case study and found alignment in risk level distributions across genders, implying no additional   \n386 bias introduced by our approach. However, existing biases in LLMs could be perpetuated if not   \n387 monitored and adjusted. Addressing these biases is beyond this paper\u2019s scope and is left for future   \n388 research as a critical area.   \n389 This framework has vast potential applications, particularly in areas with limited data and ethical   \n390 constraints. For example, in healthcare, our framework can help predict readmission or post-discharge   \n391 mortality by inferring unrecorded social determinants of health. For low-volume niche product rec  \n392 ommendations, our framework can synthesize customer preference data to enhance recommendation   \n393 systems without extensive user tracking. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "394 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "395 Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,   \n396 Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.   \n397 arXiv preprint arXiv:2303.08774, 2023.   \n398 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,   \n399 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are   \n400 few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n401 Ricardo Campos, V\u00edtor Mangaravite, Arian Pasquali, Al\u00edpio Jorge, C\u00e9lia Nunes, and Adam Jatowt.   \n402 Yake! keyword extraction from single documents using multiple local features. Information   \n403 Sciences, 509:257\u2013289, 2020.   \n404 South Dakota Department Of Corrections. Lsi-r assessment and case planning.   \n405 https://doc.sd.gov/documents/about/policies/LSI-R%20Assessment%20and%   \n406 20Case%20Planning.pdf. [Accessed 19-05-2024].   \n407 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,   \n408 Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information   \n409 processing systems, 27, 2014.   \n410 Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,   \n411 and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint   \n412 arXiv:2106.09685, 2021.   \n413 Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen, Qing Ping, Son Dinh Tran, Yi Xu, Belinda Zeng,   \n414 and Trishul Chilimbi. Understanding and constructing latent modality structures in multi-modal   \n415 representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and   \n416 Pattern Recognition, pages 7661\u20137671, 2023.   \n417 Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint   \n418 arXiv:1312.6114, 2013.   \n419 Bingxuan Li, Antonio Castellanos, Pengyi Shi, and Amy Ward. Combining machine learning and   \n420 queueing theory for data-driven incarceration-diversion program management. In Proceedings of   \n421 the Thirty-Sixth Annual Conference on Innovative Applications of Artificial Intelligence. AAAI,   \n422 2024.   \n423 Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi   \n424 Peng, Diyi Yang, Denny Zhou, et al. Best practices and lessons learned on synthetic data for   \n425 language models. arXiv preprint arXiv:2404.07503, 2024.   \n426 Qiuhao Lu, Dejing Dou, and Thien Huu Nguyen. Textual data augmentation for patient outcomes   \n427 prediction. In 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),   \n428 pages 2817\u20132821, 2021. doi: 10.1109/BIBM52615.2021.9669861.   \n429 Ning Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own   \n430 step-by-step reasoning. arXiv preprint arXiv:2308.00436, 2023.   \n431 OpenAI. Fine-tuning. https://platform.openai.com/docs/guides/fine-tuning. Ac  \n432 cessed: 2024-05-22.   \n433 OpenAI. Gpt-3.5. https://platform.openai.com/docs/models/gpt-3.5, 2021. Accessed:   \n434 2024-05-22.   \n435 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong   \n436 Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow   \n437 instructions with human feedback. Advances in neural information processing systems, 35:27730\u2013   \n438 27744, 2022.   \n439 Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei   \n440 Huang, and Huajun Chen. Reasoning with language model prompting: A survey. arXiv preprint   \n441 arXiv:2212.09597, 2022.   \n442 Merrill Rotter and Virginia Barber-Rioja. Diversion programs and alternatives to incarceration.   \n443 Oxford University Press, May 2015. doi: 10.1093/med/9780199360574.003.0021. URL http:   \n444 //dx.doi.org/10.1093/med/9780199360574.003.0021.   \n445 SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das.   \n446 A comprehensive survey of hallucination mitigation techniques in large language models. arXiv   \n447 preprint arXiv:2401.01313, 2024.   \n448 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay   \n449 Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation   \n450 and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n451 David A Van Dyk and Xiao-Li Meng. The art of data augmentation. Journal of Computational and   \n452 Graphical Statistics, 10(1):1\u201350, 2001.   \n453 Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and   \n454 Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions.   \n455 arXiv preprint arXiv:2212.10560, 2022.   \n456 Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. Llm for patient-trial matching: Privacy  \n457 aware data augmentation towards better performance and generalizability. In American Medical   \n458 Informatics Association (AMIA) Annual Symposium, 2023.   \n459 Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with   \n460 reasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022.   \n461 Chenyu Zhai and Jing Peng. Mining latent features from reviews and ratings for item recommendation.   \n462 In 2016 International Conference on Computational Science and Computational Intelligence   \n463 (CSCI), pages 1119\u20131125, 2016. doi: 10.1109/CSCI.2016.0213.   \n464 Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck. On the   \n465 paradox of learning to reason from data. arXiv preprint arXiv:2205.11502, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "466 Appendix ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "467 A Proof of Lemma 1 ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "468 We use the log-loss, defined as ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\mathcal{L}(D,\\beta)=-\\frac{1}{n}\\sum_{i=1}^{n}\\left[y_{i}\\log(p_{i})+(1-y_{i})\\log(1-p_{i})\\right]\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "469 for given data $D=\\{(x_{i},y_{i})\\}_{i=1}^{n}$ and $p_{i}=1/\\big(1+e^{-(\\beta_{0}+\\beta_{1}x_{i})}\\big)$ . When using the augmented feature   \n470 $\\tilde{x}_{i}=(x_{i},z_{i})$ , we denote the data as $\\tilde{D}=\\{\\left((x_{i},z_{i}),y_{i}\\right)\\}_{i=1}^{n}$ . ", "page_idx": 10}, {"type": "text", "text": "471 For the first part of the lemma, we note that the in-sample log-loss for the original features follows ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{in}}(D,\\beta)=-\\frac{1}{n}\\sum_{i=1}^{n}\\left[y_{i}\\log(p_{i})+(1-y_{i})\\log(1-p_{i})\\right],\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "472 and the in-sample log-loss for the augmented features follows ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{in}}(\\tilde{D},\\beta)=-\\frac{1}{n}\\sum_{i=1}^{n}\\left[y_{i}\\log(\\tilde{p}_{i})+(1-y_{i})\\log(1-\\tilde{p}_{i})\\right],\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "473 where $p_{i}=1/\\big(1+e^{-(\\beta_{0}+\\beta_{1}x_{i})}\\big)$ and $\\tilde{p}_{i}=1/\\bigl(1+e^{-(\\beta_{0}+\\beta_{1}x_{i}+\\beta_{2}z_{i})}\\bigr)$ . ", "page_idx": 10}, {"type": "text", "text": "474 We denote the optimal coefficients that minimize the log-loss in (3) as $\\beta^{*}\\,=\\,(\\beta_{0}^{*},\\beta_{1}^{*})$ , and the   \n475 coefficients that minimize the log-loss in (4) as $\\tilde{\\beta}^{*}=(\\tilde{\\beta}_{0}^{*},\\tilde{\\beta}_{1}^{*},\\tilde{\\beta}_{2}^{*})$ . Note that $\\check{\\beta}=(\\beta_{0}^{*},\\beta_{1}^{*},0)$ is a   \n476 feasible solution for the log-loss in (4). Therefore, using the optimization property, we have ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal L^{\\mathrm{in}}(\\tilde{D},\\tilde{\\beta}^{*})\\leq\\mathcal L^{\\mathrm{in}}(\\tilde{D},\\breve{\\beta})=\\mathcal L^{\\mathrm{in}}(D,\\beta^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "477 which completes the first part of the lemma. ", "page_idx": 11}, {"type": "text", "text": "478 For the second part of the lemma, we first assume that for the given data $D$ , $\\mathcal{L}^{\\mathrm{in}}(\\tilde{D},\\tilde{\\beta}^{*})\\;=$   \n479 $\\mathcal{L}^{\\mathrm{in}}(D,\\beta^{*})-\\epsilon/\\bar{n}$ where $\\epsilon\\,\\geq\\,0$ from the first part of the lemma. We now construct an instance   \n480 with an out-of-sample dataset $D^{\\prime}$ that contains $n+1$ samples, where $D^{\\prime}$ consists of (i) the $n$ data   \n481 points that exactly match with $D$ (or $\\tilde{D}$ ) for the first $n$ samples, and (ii) one additional sample   \n482 $(x_{i+1},y_{i+1})$ (or $((x_{i+1},z_{i+1}),y_{i+1})$ when using the augmented features). Without loss of generality,   \n483 assume that $y_{i+1}=1$ . Then we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{out}}(D^{\\prime},\\beta^{*})=\\frac{1}{n+1}\\left(n\\mathcal{L}^{\\mathrm{in}}(D,\\beta^{*})-\\log(p_{i+1})\\right))\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "484 and ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{out}}(\\tilde{D}^{\\prime},\\tilde{\\beta}^{*})=\\frac{1}{n+1}\\left(n\\mathcal{L}^{\\mathrm{in}}(\\tilde{D},\\tilde{\\beta}^{*})-\\log(\\tilde{p}_{i+1})\\right)\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "485 When the added features $Z$ \u2019s are non-informative, we consider the scenarios that they are noise and   \n486 the additional term $\\tilde{\\beta}_{2}^{*}Z$ also contributes noise to the predictions. In other words, the coefficients ${\\tilde{\\beta}}^{*}$   \n487 do not generalize well to the test data. Therefore, there exists an instance where the realization of $Z$ ,   \n488 $z_{i+1}$ deviates from the predicted probability significantly, such that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\tilde{p}_{i+1}<p_{i+1}/\\exp(\\epsilon)\\leq p_{i+1}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "489 Note that this instance exists since the noise terms do not correspond to any actual pattern in the test   \n490 data, causing incorrect predictions, and in our construction, a smaller predicted probability would be   \n491 less accurate as the label $y_{i+1}=1$ . Therefore, ", "page_idx": 11}, {"type": "equation", "text": "$$\n-\\log(\\tilde{p}_{i+1})>-\\log(p_{i+1})+\\epsilon,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "492 and ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{L}^{\\mathrm{out}}(\\tilde{D}^{\\prime},\\tilde{\\beta}^{*})}&{=}&{\\displaystyle\\frac{1}{n+1}\\left(n\\mathcal{L}^{\\mathrm{in}}(D,\\beta^{*})-\\epsilon-\\log(\\tilde{p}_{i+1})\\right))}\\\\ &{>}&{\\displaystyle\\frac{1}{n+1}\\left(n\\mathcal{L}^{\\mathrm{in}}(D,\\beta^{*})-\\log(p_{i+1})\\right))=\\mathcal{L}^{\\mathrm{out}}(D^{\\prime},\\beta^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "493 B Compute Resources ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "494 For all experiments, we split data into training and testing dataset with ratio of 8:2. ", "page_idx": 11}, {"type": "text", "text": "495 For experiment 1 (risk level prediction), we finetune LLaMA2-13b-chat on 2 X NVIDIA RTX A6000   \n496 for 4 hours with LoRA. And we finetuned three times for different subtasks. We use OpenAI offical   \n497 API to finetune GPT3.5 model, which requires no GPUs. Each finetune job takes about 2 hours. We   \n498 repeat 3 times for different sub tasks. Additionally, we also run Machine Learning baseline model on   \n499 CPU (Intel i7). We run grid search for each classifier.   \n500 For experiment 2 (outcome prediction), we use OpenAI offical API to finetune GPT3.5 model, which   \n501 requires no GPUs. Each finetune job takes about 2 hours. We repeat 6 times for different sub   \n502 tasks.Additionally, we also run Machine Learning baseline model on CPU (Intel i7). We run grid   \n503 search for each classifier. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "504 All other experiments (e.g. sensitive experiment) are conducted on ChatGPT, which requires no GPU. ", "page_idx": 11}, {"type": "text", "text": "505 C Prompt template ", "text_level": 1, "page_idx": 11}, {"type": "image", "img_path": "Mdd3f8Cui8/tmp/0d6e64bd5c80aafa9c8f2180f556b954bfdb7b442b28cd1e6f717d283b32e2f4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "Mdd3f8Cui8/tmp/a0936d2e40dd52f69406f300768bd6a93a830de042d0aa4b9ec89990e171eac1.jpg", "table_caption": ["Figure 5: Profile writing prompt ", "Figure 6: Risk Level Prediction: Prompt template and response CoT template "], "table_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "Mdd3f8Cui8/tmp/9e1494e261c66d35b2c6beb1bd4adcb07756653e4dd94e2caf1d366109ac1a0e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 7: Requirement selection: Multi-stage Prompt template ", "page_idx": 12}, {"type": "text", "text": "To select the top 3 programs that would be most beneficial for the client, let\u2019s analyze each available options:   \n1. Thinking for a Change (It aims to transform criminogenic thinking patterns with designed cognitive-behavioral curriculum. Recommend for clients assessed at relatively high risk level):   \n2. Employment (It aims to help client develop employability. Recommend this for clients with unstable employment status):   \n3. Education (It aims to engage clients in educational programs. Recommend clients without a high school diploma or GED):   \n4. Positive Peer Mentoring (It offers positive role models and fosters a supportive network, which can deter criminal associations. Recommend this for clients residing in high-crime areas):   \n5. Community Service (It aids in building a sense of responsibility and community connection. Recommend for clients with property offense or drug-related offenses):_   \n6. Mental Health Treatment (It addresses underlying mental health issues that may contribute to criminal behavior. Recommend for clients with a history of substance abuse or unstable living conditions):   \n7. Anger Management (It focuses on teaching effective emotion and reaction management techniques. Recommend for clients who exhibit aggressive behaviors or have property-related offenses):_   \n8. Substance Abuse Treatment (It aims to help clients overcome substance dependencies. Recommend for clients with histories of drug-related offenses or primary drug use):_ 9. Domestic Violence Counseling (It aims to address and modify violent behavior patterns. Recommend for clients involved in violent incidents):_   \n10. Sex Offender Counseling (It focuses on behavior modification and preventing recidivism. Recommend for clients with sex-related offenses):_   \nConclusion: ", "page_idx": 13}, {"type": "text", "text": "Figure 8: Requirement selection: Response CoT template ", "page_idx": 13}, {"type": "table", "img_path": "Mdd3f8Cui8/tmp/e5c96683265f8b64c1f4441fc371df08b88782645e3bb99dd3a8359b4207e5f0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "(a) Risk level prediction results across different setting ", "page_idx": 14}, {"type": "image", "img_path": "Mdd3f8Cui8/tmp/9984181037fe97e2fab919dc681aeefd0586501a2d0306af04c1454591135837.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 9: Ablation study results: (a) Experiments on risk level prediction task using GPT4 with different prompting setting. (b) Experiments using GPT4 with different prompting setting different prompting strategies. ", "page_idx": 14}, {"type": "text", "text": "507 E Program Requirements ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "Mdd3f8Cui8/tmp/dc330c927f4b6347a1edcb58e384671f14d7488f41038553729081852fe94ec4.jpg", "table_caption": ["Table 1: Available Programs "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "508 F Data Description ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "Mdd3f8Cui8/tmp/50ac5725449d7d8d361f65d66206692259765ee9f9d97b660bb018930c933f41.jpg", "table_caption": ["Table 2: Categorical Covariates Summary Statistics (N/A or Other Categories are Omitted). "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "Mdd3f8Cui8/tmp/796a4d0eb21823c19eae6ff73415913e5cd5633b1a2d50c8070ce528acab45ed.jpg", "img_caption": ["Figure 10: Correlation Matrix of features "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "509 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "510 1. Claims   \n511 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n512 paper\u2019s contributions and scope?   \n513 Answer: [Yes]   \n514 Justification: Our abstract and introduction accurately reflect the paper\u2019s contribution and   \n515 scope.   \n516 Guidelines:   \n517 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n518 made in the paper.   \n519 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n520 contributions made in the paper and important assumptions and limitations. A No or   \n521 NA answer to this question will not be perceived well by the reviewers.   \n522 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n523 much the results can be expected to generalize to other settings.   \n524 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n525 are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer:[Yes] ", "page_idx": 17}, {"type": "text", "text": "30 Guidelines:   \n31 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n32 the paper has limitations, but those are not discussed in the paper.   \n33 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n34 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n35 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n36 model well-specification, asymptotic approximations only holding locally). The authors   \n37 should reflect on how these assumptions might be violated in practice and what the   \n38 implications would be.   \n39 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n40 only tested on a few datasets or with a few runs. In general, empirical results often   \n41 depend on implicit assumptions, which should be articulated.   \n42 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n43 For example, a facial recognition algorithm may perform poorly when image resolution   \n44 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n45 used reliably to provide closed captions for online lectures because it fails to handle   \n46 technical jargon.   \n47 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n48 and how they scale with dataset size.   \n49 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n50 address problems of privacy and fairness.   \n51 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n52 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n53 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n54 judgment and recognize that individual actions in favor of transparency play an impor  \n55 tant role in developing norms that preserve the integrity of the community. Reviewers   \n56 will be specifically instructed to not penalize honesty concerning limitations.   \n557 3. Theory Assumptions and Proofs   \n558 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n559 a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have theoretical result in section 3, and we have more detailed proof in Appendix section A. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "574 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "75 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n76 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n77 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide all information needed to reproduce the main experimental results in section 5. We have provided all implementation detail for reproduction. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "613 5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "14 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n15 tions to faithfully reproduce the main experimental results, as described in supplemental   \n16 material?   \n17 Answer: [No]   \n18 Justification: Access to the data and code is restricted under the terms of the non-disclosure   \n19 agreement signed with our data-providing partner. The code includes sensitive details perti  \n620 nent to the data, such as specific information embedded within the prompts. Consequently,   \n621 we are unable to share the code at this time.   \n622 Guidelines:   \n623 \u2022 The answer NA means that paper does not include experiments requiring code.   \n624 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n25 public/guides/CodeSubmissionPolicy) for more details.   \n626 \u2022 While we encourage the release of code and data, we understand that this might not be   \n27 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n628 including code, unless this is central to the contribution (e.g., for a new open-source   \n29 benchmark).   \n630 \u2022 The instructions should contain the exact command and environment needed to run to   \n631 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n632 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n633 \u2022 The authors should provide instructions on data access and preparation, including how   \n634 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n635 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n636 proposed method and baselines. If only a subset of experiments are reproducible, they   \n637 should state which ones are omitted from the script and why.   \n638 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n639 versions (if applicable).   \n640 \u2022 Providing as much information as possible in supplemental material (appended to the   \n641 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We clarify all experiment setting in section 5. We also provide more training details on Appendix section C. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "655 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "656 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n657 information about the statistical significance of the experiments?   \n666 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n667 example, train/test split, initialization, random drawing of some parameter, or overall   \n668 run with given experimental conditions).   \n669 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n670 call to a library function, bootstrap, etc.)   \n671 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n672 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n673 of the mean.   \n674 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n675 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n676 of Normality of errors is not verified.   \n677 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n678 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n679 error rates).   \n680 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n681 they were calculated and reference the corresponding figures or tables in the text.   \n682 8. Experiments Compute Resources   \n683 Question: For each experiment, does the paper provide sufficient information on the com  \n684 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n685 the experiments?   \n686 Answer: [Yes]   \n687 Justification: We have a brief introduction of experiments compute resources in section 5.   \n688 We have more detailed information in Appendix section C.   \n689 Guidelines:   \n690 \u2022 The answer NA means that the paper does not include experiments.   \n691 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n692 or cloud provider, including relevant memory and storage.   \n693 \u2022 The paper should provide the amount of compute required for each of the individual   \n694 experimental runs as well as estimate the total compute.   \n695 \u2022 The paper should disclose whether the full research project required more compute   \n696 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n697 didn\u2019t make it into the paper).   \n698 9. Code Of Ethics   \n699 Question: Does the research conducted in the paper conform, in every respect, with the   \n700 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n701 Answer: [Yes]   \n702 Justification: We reviewed the NeurIPS Code of Ethics. The research conducted in the paper   \n703 conform, in every respect, with the NeurIPS Code of Ethics.   \n704 Guidelines:   \n705 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n706 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n707 deviation from the Code of Ethics.   \n708 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n709 eration due to laws or regulations in their jurisdiction).   \n710 10. Broader Impacts   \n711 Question: Does the paper discuss both potential positive societal impacts and negative   \n712 societal impacts of the work performed?   \n713 Answer: [Yes]   \n714 Justification: We discuss the potential societal impacts in the section 1 Introduction, and   \n715 section7 Discussion.   \n716 Guidelines:   \n717 \u2022 The answer NA means that there is no societal impact of the work performed.   \n718 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n719 impact or why the paper does not address societal impact.   \n720 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n721 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n722 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n723 groups), privacy considerations, and security considerations.   \n724 \u2022 The conference expects that many papers will be foundational research and not tied   \n725 to particular applications, let alone deployments. However, if there is a direct path to   \n726 any negative applications, the authors should point it out. For example, it is legitimate   \n727 to point out that an improvement in the quality of generative models could be used to   \n728 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n729 that a generic algorithm for optimizing neural networks could enable people to train   \n730 models that generate Deepfakes faster.   \n731 \u2022 The authors should consider possible harms that could arise when the technology is   \n732 being used as intended and functioning correctly, harms that could arise when the   \n733 technology is being used as intended but gives incorrect results, and harms following   \n734 from (intentional or unintentional) misuse of the technology.   \n735 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n736 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n737 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n738 feedback over time, improving the efficiency and accessibility of ML).   \n739 11. Safeguards   \n740 Question: Does the paper describe safeguards that have been put in place for responsible   \n741 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n742 image generators, or scraped datasets)?   \n743 Answer: [Yes]   \n744 Justification: We promote human-in-the loop verification and emphasized on domain exper  \n745 tise. Moreover,we are not gonna make the dataset public - the framework is genralizable but   \n746 we caution users to be aware of bias and use human-in the loop verification.   \n747 Guidelines:   \n748 \u2022 The answer NA means that the paper poses no such risks.   \n749 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n750 necessary safeguards to allow for controlled use of the model, for example by requiring   \n751 that users adhere to usage guidelines or restrictions to access the model or implementing   \n752 safety filters.   \n753 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n754 should describe how they avoided releasing unsafe images.   \n755 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n756 not require this, but we encourage authors to take this into account and make a best   \n757 faith effort.   \n758 12. Licenses for existing assets   \n759 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n760 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n761 properly respected?   \n762 Answer: [NA]   \n763 Justification: Our paper does not use existing assets.   \n764 Guidelines:   \n765 \u2022 The answer NA means that the paper does not use existing assets.   \n766 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n767 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n768 URL.   \n769 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n770 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n771 service of that source should be provided.   \n772 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n773 package should be provided. For popular datasets, paperswithcode.com/datasets   \n774 has curated licenses for some datasets. Their licensing guide can help determine the   \n775 license of a dataset.   \n776 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n777 the derived asset (if it has changed) should be provided.   \n778 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n779 the asset\u2019s creators.   \n780 13. New Assets   \n781 Question: Are new assets introduced in the paper well documented and is the documentation   \n782 provided alongside the assets?   \n783 Answer: [NA]   \n784 Justification: Our paper does not release new assets.   \n785 Guidelines:   \n786 \u2022 The answer NA means that the paper does not release new assets.   \n787 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n788 submissions via structured templates. This includes details about training, license,   \n789 limitations, etc.   \n790 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n791 asset is used.   \n792 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n793 create an anonymized URL or include an anonymized zip file.   \n794 14. Crowdsourcing and Research with Human Subjects   \n795 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n796 include the full text of instructions given to participants and screenshots, if applicable, as   \n797 well as details about compensation (if any)?   \n798 Answer: [NA]   \n799 Justification: Our paper does not involve crowdsourcing nor research with human subjects.   \n800 Guidelines:   \n801 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n802 human subjects.   \n803 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n804 tion of the paper involves human subjects, then as much detail as possible should be   \n805 included in the main paper.   \n806 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n807 or other labor should be paid at least the minimum wage in the country of the data   \n808 collector.   \n809 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n810 Subjects   \n811 Question: Does the paper describe potential risks incurred by study participants, whether   \n812 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n813 approvals (or an equivalent approval/review based on the requirements of your country or   \n814 institution) were obtained?   \n815 Answer: [Yes]   \n816 Justification: We received IRB approval from University of Chicago. The study title is   \n817 Data-driven Evaluation of Alternative Sentencing Allocation.   \n818 Guidelines:   \n819 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n820 human subjects. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]