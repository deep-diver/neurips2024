[{"figure_path": "G0yxFmP87g/tables/tables_6_1.jpg", "caption": "Table 1: Compare with baseline methods under varying remaining ratios on LLaMA2 7B.", "description": "This table compares the performance of AmoebaLLM against several baseline methods (LLM-Pruner, FLAP, Shortened LLaMA) on the LLaMA2 7B model.  The comparison is done across multiple evaluation metrics (MMLU, Average, BoolQ, PIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, OBQA) and varying remaining ratios (80%, 65%, 50%) representing different levels of model compression.  AmoebaLLM shows improvement on most metrics when compared to baselines at all compression ratios. The table also includes AmoebaLLM+ which represents individually fine-tuned versions of the subnets for comparison.", "section": "4.2 Benchmark with SOTA LLM Compression Methods"}, {"figure_path": "G0yxFmP87g/tables/tables_6_2.jpg", "caption": "Table 2: Compare with baseline methods under varying remaining ratios on Vicuna 7B v1.5.", "description": "This table compares the performance of AmoebaLLM with three baseline methods (LLM-Pruner, FLAP, Shortened LLaMA) on the Vicuna 7B v1.5 model across various downstream tasks.  The comparison is done under different remaining ratios (80%, 65%, and 50%), representing different model sizes.  The performance metrics include MMLU (Massive Multitask Language Understanding), Average (average score across all tasks), BoolQ (boolean question answering), PIQA (physical interaction question answering), HellaSwag (commonsense reasoning), Winogrande (commonsense reasoning), ARC-e (AI2 reasoning challenge - easy), ARC-c (AI2 reasoning challenge - challenge), and OBQA (openbook question answering).  AmoebaLLM is shown to outperform the baselines in various scenarios. The AmoebaLLM\u2020 rows show the results after individual fine-tuning for each subnet configuration, highlighting the effectiveness of the one-for-all approach.", "section": "4.2 Benchmark with SOTA LLM Compression Methods"}, {"figure_path": "G0yxFmP87g/tables/tables_7_1.jpg", "caption": "Table 3: Ablation Study on the effectiveness of the DP-based depth shrinking on LLaMA2 7B.", "description": "This table presents an ablation study on the effectiveness of the dynamic programming (DP)-based depth shrinking method used in AmoebaLLM.  It compares the performance of AmoebaLLM's DP-based approach against two other methods: Unreasonable [33] and ShortenLLaMA [9]. The results are shown for different numbers of remaining layers (from 24 to 16) and are measured using two metrics: perplexity (PPL) on the Wikitext2 dataset and accuracy on the MMLU dataset. This comparison aims to demonstrate the advantage of AmoebaLLM's knowledge-preserving subnet selection strategy in maintaining model performance while reducing the depth.", "section": "4.3 Ablation Study: Effectiveness of Each Component"}, {"figure_path": "G0yxFmP87g/tables/tables_7_2.jpg", "caption": "Table 4: Ablation Study on different components in our AmoebaLLM on LLaMA2 7B.", "description": "This table presents the ablation study of different components of AmoebaLLM on LLaMA2 7B. It compares the performance of different configurations of AmoebaLLM against the baseline (per-subnet fine-tuning) across three different depth settings: 32, 24, and 20.  The configurations tested include removing SMOL adapter, removing SMOL and replacing with standard LoRA, removing the loss-magnitude balancing scheme.  The results (Wikitext2 perplexity and MMLU accuracy) are shown for each configuration and depth, demonstrating the individual contributions of each AmoebaLLM component to the overall performance.", "section": "4.3 Ablation Study: Effectiveness of Each Component"}, {"figure_path": "G0yxFmP87g/tables/tables_8_1.jpg", "caption": "Table 5: Ablation Study on the selection of calibration datasets on LLaMA2 7B.", "description": "This table presents an ablation study on the choice of calibration datasets for the DP-based depth shrinking method used in the AmoebaLLM framework. It shows the results of using three different calibration datasets (BookCorpus, Wikitext2, and MMLU) for training LLMs with varying numbers of layers (32, 24, and 20).  The results are shown both before fine-tuning and after fine-tuning.  The goal is to determine which calibration dataset is most effective for maintaining both accuracy and language modeling capabilities after compressing the LLM.", "section": "4.4 Ablation Study: The Selection of Calibration Datasets"}]