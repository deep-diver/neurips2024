[{"type": "text", "text": "MAC Advice for Facility Location Mechanism Design ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zohar Barak Anupam Gupta School of Computer Science New York University and Google Research Tel Aviv University anupam.g@nyu.edu zoharbarak@mail.tau.ac.il ", "page_idx": 0}, {"type": "text", "text": "Inbal Talgam-Cohen   \nSchool of Computer Science Tel Aviv University   \ninbaltalgam@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Algorithms with predictions are gaining traction across various domains, as a way to surpass traditional worst-case bounds through (machine-learned) advice. We study the canonical problem of $k$ -facility location mechanism design, where the $n$ agents are strategic and might misreport their locations. We receive a prediction for each agent\u2019s location, and these predictions are crucially allowed to be only \u201cmostly\u201d and \u201capproximately\u201d correct (MAC for short): a $\\delta$ -fraction of the predicted locations are allowed to be arbitrarily incorrect, and the remainder of the predictions are required to be correct up to an $\\varepsilon$ -error. Moreover, we make no assumption on the independence of the errors. Can such \u201cflawed\u201d predictions allow us to beat the current best bounds for strategyproof facility location? ", "page_idx": 0}, {"type": "text", "text": "We show how natural robustness of the 1-median (also known as the geometric median) of a set of points leads to an algorithm for single-facility location with MAC predictions. We extend our results to a natural \u201cbalanced\u201d variant of the $k$ -facility case, and show that without balancedness, robustness completely breaks down even for $k=2$ facilities on a line. As our main result, for this \u201cunbalanced\u201d setting we devise a truthful random mechanism, which outperforms the best known mechanism (with no predictions) by Lu et al. [2010]. En route, we introduce the problem of \u201csecond\u201d facility location, in which the first facility location is already fixed. Our robustness findings may be of independent interest, as quantitative versions of classic breakdown-point results in robust statistics. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Algorithms with predictions is a popular field of study in recent years within the paradigm of beyond the worst case analysis of algorithms \u2014 see Mitzenmacher and Vassilvitskii (2022) for a comprehensive survey. Motivated by developments in machine learning, this area assumes that algorithms can access predictions regarding the input or solution, thus leveraging predictability in typical computational problems. Recently, Agrawal et al. (2022) and $\\mathrm{Xu}$ and Lu (2022) proposed to study predictions in the context of mechanism design, where they have the potential to transform existing designs by adding information about agents\u2019 private knowledge (see Balkanski et al. (2023a)). ", "page_idx": 0}, {"type": "text", "text": "Our focus in this work is on the canonical problem of facility location. The (utilitarian) $k$ -facility location problem is as follows: Consider a multi-set of $n$ points $X\\subset\\mathbb{R}^{d}$ and $k$ facility locations; the cost of point $x\\in X$ is the minimum distance between $x$ and any of the $k$ locations. The goal is to compute $k$ locations that minimize the utilitarian cost, which is the total cost of points in $X$ . In the context of mechanism design, the points $X$ are the privately-known true locations of strategic agents, and the mechanism receives location reports (see Chan et al. (2021) for a recent survey). Facility location mechanism design with predictions has been studied by Agrawal et al. (2022); Xu and Lu (2022).1 We diverge from previous work by assuming a prediction for each point \u2014 that is, the advice $X^{\\prime}$ consists of a prediction $\\ensuremath{\\boldsymbol{{x}}}_{i}^{\\prime}$ for every $x_{i}\\in X$ . Importantly, our measure of prediction error allows for very large errors (outliers) for a $\\delta$ -fraction of points (hence we call it the \u201cmostly approximately correct\u201d model). Our work shows how to get improved bounds despite these large errors, and can thus be of interest beyond facility location. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Standard worst-case prediction error. In the majority of the literature, the goal of (algorithm or mechanism) design with predictions centers around two seemingly-confilcting properties: robustness to erroneous predictions, and consistency with predictions that are non-erroneous. A fairly common way of achieving both consistency and robustness is interpolating between the approaches of completely following the predictions, and applying the best worst-case algorithm while disregarding the predictions. An ideal design is one that gracefully transitions \u2014 as the prediction error increases \u2014 from optimal performance when the prediction is correct, to the best-known worst-case performance (Lykouris and Vassilvitskii, 2021; Purohit et al., 2018; Agrawal et al., 2022). By definition, achieving such graceful degradation hinges on how the prediction error is measured. ", "page_idx": 1}, {"type": "text", "text": "The most common measure of prediction error is arguably the distance between the predicted and actual values. If $X\\,=\\,\\{{x_{1}},\\ldots,{x_{n}}\\}$ contains the actual values and $X^{\\prime}\\,=\\,\\{x_{1}^{\\prime},\\cdot\\cdot\\cdot,x_{n}^{\\prime}\\}$ is the prediction, the error is defined as $\\dot{\\eta}\\,\\stackrel{.}{=}\\,\\ell_{p}(X-X^{\\prime})$ , where $\\ell_{p}$ represents either the $\\ell_{1}$ norm $\\begin{array}{r}{(\\ell_{1}(\\dot{t})=\\sum_{i}|t_{i}|)}\\end{array}$ or the $\\ell_{\\infty}$ norm ${\\mathcal{E}}_{\\infty}(t)=\\operatorname*{mix}_{i}|t_{i}|)$ . This is a \u201cworst case\u201d measure of prediction error, since a single deviation in one of the $n$ entries can significantly inflate the error. Prediction errors measured in this way appear in the context of ski rental Purohit et al. (2018), online caching Lykouris and Vassilvitskii (2021); Rohatgi (2020), the secretary problem Antoniadis et al. (2020), single-item auctions $\\mathrm{Xu}$ and Lu (2022), and many other problems. In what follows, we address this type of prediction error as the \u201cworst case\u201d error model. ", "page_idx": 1}, {"type": "text", "text": "Motivation for an alternative measure. We demonstrate the need for an alternative measure of prediction error combined with new algorithms through the following $k$ -facility location instance, in which a single error causes the optimal solution for prediction $X^{\\prime}$ to perform badly for the actual dataset $X$ .2 ", "page_idx": 1}, {"type": "text", "text": "Example 1 (Sensitivity of 2-facility location on a line). Let $k=2$ , and assume the n agents\u2019 actual locations $X$ are divided evenly between 0 and $^{\\,l}$ on the real line. Let $X^{\\prime}$ be the prediction, with all coordinates predicted accurately except for a single coordinate $i$ , which is predicted erroneously to be located at $x_{i}^{\\prime}=M\\gg n$ . While the optimal solution for $X$ is to place the $k=2$ facilities at 0 and 1 for a total cost of zero, in the optimal solution for $X^{\\prime}$ one of the facilities moves to $M$ . This solution performs badly for $X$ \u2014 the approximation ratio compared to the optimal solution is unbounded. ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{P r e d i c t i o n s:}\\mathsf{R e a l}}\\\\ &{\\mathsf{n/2}\\qquad\\mathsf{n/2-1}\\qquad\\mathsf{1}\\qquad\\mathsf{1}\\qquad\\mathsf{n/2}\\qquad\\mathsf{n/2}}\\\\ &{\\mathsf{L}\\qquad\\mathsf{L}\\qquad\\qquad\\mathsf{L}\\qquad\\vdots\\qquad\\mathsf{L}\\qquad\\mathsf{L}}\\\\ &{\\mathsf{0}\\qquad\\mathsf{1}\\qquad\\qquad\\mathsf{M}\\qquad\\mathsf{1}\\qquad\\mathsf{0}\\qquad\\mathsf{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustration of Example 1; the predictions $X^{\\prime}$ are on the left side and the real points $X$ are on the right side. The triangles are the optimal facility locations for the points. A single bad prediction translates into an unbounded \u201cworst case\u201d error and approximation ratio. ", "page_idx": 1}, {"type": "text", "text": "In many ML contexts, it is very possible that a small fraction of points have a large \u201cworst case\u201d prediction error \u2014 even excellent predictors may err on a certain fraction of the dataset. This motivates alternative measures of prediction accuracy. Concretely, we ask: ", "page_idx": 1}, {"type": "text", "text": "Question 1. For 2-facility location on a line (as in Example 1), can we design a mechanism that uses agent reports to get a good solution despite a large \u201cworst case\u201d prediction error? ", "page_idx": 1}, {"type": "text", "text": "We answer this question positively, proving that, using agent reports, it is possible to design resilient mechanisms against adversarial \u201cworst case\u201d errors. The main take-away is that predictions can still be valuable despite high \u201cworst case\u201d prediction error, if the fraction of such grossly-erroneous points is small. In other words, a high \u201cworst case\u201d prediction error may not always justify \u201cthrowing out\u201d the predictions and settling for the best worst-case guarantee (although it is currently standard in design with predictions). Our new error model and results thus suggest a new pathway for effectively utilizing predictive advice in strategic settings, despite significant uncertainties in the predictions. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Overview of Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Our Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce MAC predictions where MAC stands for Mostly Approximately Correct. This formulation captures the essential features of some closely related previous models, e.g., the model (Azar et al., 2022) formulated for online metric problems, and the PMAC model of (Balcan and Harvey, 2018) formulated in the context of learning submodular functions. A vector of predictions $X^{\\prime}$ is $\\mathbf{MAC}(\\varepsilon,\\delta)$ with respect to dataset $X$ if at least a $(1-\\delta)$ -fraction (i.e., most) of the predictions are approximately correct up to an additive $\\varepsilon$ -error. Intuitively, if we do not think it is likely that most of our predictions will be close to being correct \u2014we would typically refrain from relying on them in decision-making processes. Throughout, $\\delta\\in[0,0.5)$ is assumed to be small (but can still be a constant). Crucially, when a prediction is not approximately correct (i.e., it belongs to the $\\delta$ -fraction of incorrect predictions), the error is allowed to be arbitrarily large. Also, the $\\delta\\!\\cdot$ -fraction of wrong predictions is allowed to be arbitrary among all possible subsets of that size (there is no assumption of independence). We show that despite their adversarial nature, and the fact that the prediction error is unbounded, such predictions can be used, in combination with strategic reports, to produce a solution with better cost compared to the best no-prediction solution. ", "page_idx": 2}, {"type": "text", "text": "The MAC prediction model is suitable for capturing errors from a variety of sources including MLgenerated predictions, data that accommodates changes over time, data corrupted by a malicious user, and expert advice (see Appendix B for more details). The accuracy parameters $\\varepsilon,\\delta$ of the MAC model can also be viewed as confidence parameters. The use of confidence parameters is prevalent in the algorithms with predictions literature (see, e.g., Agrawal et al. (2022)). Arguably, $\\varepsilon,\\delta$ are highly explainable compared to most confidence measures in the literature, and it is also quite natural to estimate them from data. ", "page_idx": 2}, {"type": "text", "text": "2.2 Our Results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We design both deterministic and randomized strategyproof anonymous mechanisms for facility location problems, with MAC predictions of the $n$ agent locations. A preliminary simplifying observation is that in the context of facility location, it suffices to consider mostly-correct (i.e., $\\mathbf{MAC}(0,\\delta))$ predictions, since handling a nonzero approximation parameter $\\varepsilon$ follows directly (see Appendix C). ", "page_idx": 2}, {"type": "text", "text": "Randomized mechanism design. As our main technical result, for 2-facility location on a line we design a randomized mechanism that guarantees an expected approximation ratio of $3.6+O(\\delta)$ (see Algorithm 4 and Theorem 9). For sufficiently small (but still constant) $\\delta$ , this improves upon the best-known no-prediction expected guarantee of 4 by Lu et al. (2010). (The mechanism of Lu et al. (2010) works for any metric space.) This result provides a positive answer to Question 1, justifying the take-away that predictions with high \u201cworst case\u201d error are useful. We give a brief description of the mechanism in Section 2.3. Note that 2-facility location on a line was also studied by Procaccia and Tennenholtz (2013), and they provided a deterministic mechanism with an $(n-2)$ approximation ratio, which is tight (Lu et al., 2010; Fotakis and Tzamos, 2014). ", "page_idx": 2}, {"type": "text", "text": "Deterministic mechanism design. For single-facility l\u221aocation in $\\mathbb{R}^{d}$ , we design a mechanism that guarantees an approximation ratio of $\\begin{array}{r}{\\operatorname*{min}\\lbrace1+\\frac{4\\delta}{1-2\\delta},\\sqrt{d}\\rbrace}\\end{array}$ (see Algorithm 1 and Theor\u221aem 5). For sufficiently small (but still constant) $\\delta$ , this improves upon the no-prediction guarantee of $\\sqrt{d}$ by Meir (2019), which is tight for $d=2$ . For $\\beta$ -balanced $k$ -facility location with constant $k$ (in any metric space), which is a natural extension of facility location where at least a $\\beta$ -fraction of the $n$ points must be assigned to each of the $k$ facilities (for cost-sharing or load-balancing reasons), we design a simple strategyproof mechanism which guarantees an approximation ratio of $\\begin{array}{r}{\\bar{c}+O(\\frac{\\delta}{\\beta})}\\end{array}$ , where $c$ is a constant (see Algorithm 7 and Theorem 13). For a sufficiently small (but still constant) $\\delta$ , this greatly improves upon the best-known no-prediction guarantee of $n/2-1$ by Aziz et al. (2020) (we remark that Aziz et al. (2020) study a variant called capacitated, to which our result applies).3 ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "While we have not optimized the constants, our conceptual contribution is in showing that even if the worst case error of the predictions is unbounded, it can still provide useful information for improving the performance of facility location mechanisms when coupled with strategic reports. In other words, the MAC model allows achieving robustness to outliers while still beating the no-prediction performance. Our single-facility result demonstrates how the accuracy parameter of the model, $\\delta$ , can serve as a trust parameter which has natural explainability. Another interesting feature of our results is that utilizing MAC predictions seems to suggest a richer family of mechanisms than standard interpolations of \u201cno-predictions\u201d and \u201ccomplete trust\u201d in the predictions. ", "page_idx": 3}, {"type": "table", "img_path": "LPbqZszt8Y/tmp/a599a69d60f59b83feda41555b454042086d0a0e5431262b8af4e577f3a6f032.jpg", "table_caption": ["Below we give a summary of the results in tables. "], "table_footnote": ["Table 1: Deterministic Mechanism Design. "], "page_idx": 3}, {"type": "text", "text": "\\*The linear appoximation ratio is of Aziz et al. (2020) for the capacitated facility location variant where there is an additional constraint of a maximum cluster size. The $\\beta$ -balanced variant of the problem was not studied without predictions, but is comparable to the capacitated variant, since a minimum cluster size (as in the balanced variant) implies a maximum cluster size (and for $k=2$ , a maximum cluster size implies a minimum cluster size as well). ", "page_idx": 3}, {"type": "table", "img_path": "LPbqZszt8Y/tmp/1afdcf302db011e7000edf77e1be9590d3d6787b98b3a153d640d1090fc408b4.jpg", "table_caption": ["Table 2: Random Mechanism Design "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "2.3 Our Techniques and Roadmap ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We break the problem into two natural conceptual steps. First, we establish what can be done by just using the predictions \u2013 this requires us to show some stability properties of algorithms for $\\mathbf{k}\\cdot$ -medians. For some problems, such as the single facility location, we show that using only the predictions is enough to get a good approximation ratio. For other problems, such as 2-facility location, it is not enough, and thus we need to include the input in the computation. For this purpose, we show how to choose the first facility by just using the predictions, and a way to choose the second facility by taking the first facility and the input into account. Then, we show how the interplay between these parts allows us to get a better approximation. Let us now give a few more details for these steps. ", "page_idx": 3}, {"type": "text", "text": "Robust statistics and the robustness of the geometric median and $k$ -medians. We first develop quantitative versions of classic results in robust statistics on breakdown points, which may be of possible independent interest. Consider a location estimator, a function that gets $n$ points in a metric space and returns $k$ locations. Its breakdown point is the proportion $\\delta$ of adversarially-modified points it can handle before returning an arbitrarily far result from the original estimator. It is well-known that the mean has a breakdown point of $\\delta={^1}/n$ (consider changing one point to an arbitrarily large value) and that the median is robust with a breakdown point of $\\delta={}^{1}\\!/2$ . However, the notion of breakdown point is qualitative, and does not reveal the relation between the fraction $\\delta$ of modified points, and how much the estimator changes. In Section 5 we flil this gap by defining measures of robustness (distance robustness (Definition 9) and approximation robustness (Definition 10)). Next, we show robustness properties of the median and its generalizations, showing that the cost of the geometric median behaves smoothly as the fraction of modified points increases. These smooth robustness results prove crucial to obtaining improved mechanisms for facility locations using predictions. Moreover, the robustness guarantees have interesting interpretations in the context of robust statistics and robust clustering in machine learning, and might also be of independent interest as explained in Appendix D. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Second facility location. In Section 6.2.1 we introduce the second facility location problem. It is often the case that facilities are created incrementally and not all at once. Hence, it is natural to ask how can we design a mechanism for choosing the location of a second facility (say a second hospital) given an existing first one. We show that the random proportional choice (second step) used in the proportional mechanism by Lu et al. (2010) achieves a (tight) approximation ratio of 3 for the line metric, improving over the factor of 4 in the general case for the second facility location problem. ", "page_idx": 4}, {"type": "text", "text": "The Robust Half Technique: Beyond interpolation. We generate half of the solution (the first facility location) from the predictions, and use the result together with the input (agent reports) to generate the other half of the solution (the second facility location). To generate the first half of the solution, we define and analyze a robust location estimator we name BIG-CLUSTER-CENTER (which turns out to be a non-trivial task); Hence the name: \"Robust Half\". To generate the second half of the solution from the input (given the first half) we define and analyze the second facility location on the line problem. Finally, to combine the results we consider two cases: the input either induces a balanced clustering or an unbalanced clustering. We utilize the robustness results (both for balanced $\\mathrm{k\\cdot}$ -medians and for Big-Cluster-Center) to show that in both cases we get a good approximation ratio. Our mechanism differs from many existing designs which interpolate between the two approaches of trusting the predictions and using the no-predictions algorithm. ", "page_idx": 4}, {"type": "text", "text": "3 Related Work ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Facility location mechanism design with predictions. In the \u201cworst case\u201d error model, Agrawal et al. (2022) show a mechanism for single-facility location on the real plane $\\mathbb{R}^{2}$ . Their prediction consists of a single value representing advice for where to locate the facilit\u221ay. Given a trust parameter $c\\in[0,1]$ , the\u221air mechanism guarantees an approximation ratio of at most $\\textstyle{\\frac{\\sqrt{2c^{2}+2}}{1+c}}$ if the prediction is perfect, and $\\textstyle{\\frac{\\sqrt{2c^{2}+2}}{1-c}}$ in the worst case. In theory, we could reduce the MAC 1m+ocdel to the one studied by Agrawal et al. (2022), by setting their single prediction to be the geometric median of the MAC predictions. Then $c$ can be determined according to $\\delta$ . However, knowing $\\delta$ and the $\\delta$ -robustness of 1-Median, we can directly choose either the no-predictions mechanism or the predictions\u2019 1-Median, rather than interpolating between these two solutions. ", "page_idx": 4}, {"type": "text", "text": "Mechanism design for the 2-facility location on the line with predictions is studied by $\\mathrm{Xu}$ and Lu (2022), where the predictions are for the locations of the agents. While they do not define an error parameter, their deterministic mechanism is in line with the \u201cworst case\u201d prediction error model: it achieves an approximation ratio of $\\begin{array}{l}{{\\frac{n}{2}}}\\end{array}$ if the predictions are perfect, and $2n$ if they are arbitrarily bad. ", "page_idx": 4}, {"type": "text", "text": "Two-facility location on a line mechanism design without predictions. In the setting of nopredictions, the best strategyproof deterministic anonymous mechanism achieves a tight approximation ratio of $n-2$ Procaccia and Tennenholtz (2013); Fotakis and Tzamos (2014). In comparison, in our MAC model under the additional assumption that the number of agents assigned to each facility in the optimal solution is of minimum size $\\Omega(\\delta n)$ , the deterministic mechanism in Algorithm 7 achieves a constant approximation ratio. Under no further assumption, our randomized mechanism in Algorithm 4 achieves a constant expected approximation ratio for small $\\delta$ . ", "page_idx": 4}, {"type": "text", "text": "Additional related work is deferred to Appendix A. ", "page_idx": 4}, {"type": "text", "text": "4 Formal Definitions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Let $(V,d)$ be a metric space and $k,n\\in\\mathbb{N}$ . The input to the facility location problem is a multi-set $X=$ $\\{x_{1},\\ldots,x_{n}\\}\\in V^{n}$ . We are also given a prediction, which is another multi-set $X^{\\prime}=\\{x_{1}^{\\prime},\\ldots,x_{n}^{\\prime}\\}\\in$ $V^{n}$ . The distance between a point $u$ and a multi-set $W$ is $\\begin{array}{r}{d(u,W)\\ =\\ \\operatorname*{min}_{w\\in\\dot{W}}\\bar{d}(u,w)}\\end{array}$ . The Hausdorff distance between multi-sets $U,W\\in V^{n}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{H}(U,W):=\\operatorname*{max}\\{\\operatorname*{max}_{u\\in U}d(u,W),\\operatorname*{max}_{w\\in W}d(w,U)\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "that is, the maximum distance between a point in one multi-set and the other multi-set. ", "page_idx": 4}, {"type": "text", "text": "Definition 1. Fix $\\varepsilon\\ge0$ . For $x,x^{\\prime}\\in V$ , we say $x^{\\prime}$ is an $\\varepsilon$ -correct prediction of $x$ if $d(x,x^{\\prime})\\leq\\varepsilon$ , otherwise we say it is $\\varepsilon$ -incorrect. ", "page_idx": 5}, {"type": "text", "text": "Definition 2. The $(\\varepsilon,r)$ -neighborhood of multi-set $X\\in V^{n}$ is defined to be all predictions $X^{\\prime}\\in V^{n}$ such that $|\\{i\\in[n]\\mid x_{i}^{\\prime}$ is an $\\varepsilon$ -incorrect prediction of $x_{i}\\}|\\leq r$ . ", "page_idx": 5}, {"type": "text", "text": "The $(0,r)$ -neighborhood of $X$ is often just referred to as the $r$ -neighborhood of $X$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 3 (Mostly Approximately Correct (MAC) Predictions). Fix $\\varepsilon\\ge0$ , $\\delta\\in[0,0.5)$ . The point set $X^{\\prime}$ is an $(\\varepsilon,\\delta)$ -MAC prediction for $X$ if $X^{\\prime}$ belongs to the $(\\varepsilon,\\delta|X|)$ -neighborhood of $X$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 4 (Location Estimator). For $n,k\\in\\mathbb{N},$ , a function $f(X):V^{n}\\to V^{k}$ is called a location estimator. ", "page_idx": 5}, {"type": "text", "text": "Some common examples of location estimators with $k=1$ for points on the real line are the minimum, maximum, mean, and median; for general metric spaces, $k$ -means and $k$ -medians are well-studied examples ( $k$ -means is similar to $k$ -medians with squared distances). For $k=1$ and $V=\\mathbb{R}^{d}$ , the $k$ -medians solution is also called the GEOMETRIC-MEDIAN which is a generalization of the median to higher dimensions. A different such median generalization is the COORDINATE-WISE-MEDIAN: ", "page_idx": 5}, {"type": "text", "text": "Definition 5. For a multi-set of $n$ points $X~\\subseteq~\\mathbb{R}^{d}$ : COORDINATE-WISE-MEDIAN $\\mathbf{\\Sigma}(X)\\ :=$ $(l_{1},\\ldots,l_{d})$ where where $l_{j}$ is the median of the multi-set of the $j$ \u2019th coordinates of $X$ for all $j\\in[d]$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 6 ( $\\mathrm{\\bfk}$ -medians cost function). Given multi-sets $X\\,\\in\\,V^{n},F\\,\\in\\,V^{k}$ , the $k$ -medians cost function is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{med}_{k}(X,F):=\\sum_{x\\in X}d(x,F).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the context of the facility location problem, this function is also known as the social cost function or the utilitarian goal function. ", "page_idx": 5}, {"type": "text", "text": "The $k$ -medians problem takes as input a multi-set $X\\in V^{n}$ and outputs a multi-set $F$ that minimizes this cost function. The minimizer $\\dot{F}^{*}\\in V^{k}$ the problem is called the $\\mathrm{k\\Omega}$ -Medians solution. k-Medians can be viewed as a location estimator of $X$ that optimizes the $k$ -medians objective function: ", "page_idx": 5}, {"type": "text", "text": "Definition 7 (Center-Induced Partitions). A collection of centers $F=\\{f_{1},\\dots,f_{k}\\}\\subseteq V^{k}$ induces a partition $\\mathcal{C}=\\{C_{1},\\ldots,C_{k}\\}$ of the dataset $X$ if $C_{i}\\subseteq\\{x\\in X\\mid d(x,f_{i})\\leq d(x,f_{j})\\,\\forall j\\}$ . This partition is called the clustering of $X$ induced by $F$ . This partition (or clustering) is $\\beta$ -balanced $i f$ $\\left\\bar{|{C_{i}}|}\\geq\\beta|{X}|\\right.$ for all $i\\in[k]$ . ", "page_idx": 5}, {"type": "text", "text": "Note that a collection $\\mathcal{C}$ need not be unique, since points in $X$ can \u201cchoose\u201d between any of their closest centers in $F$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 8 ( $\\beta$ -balanced $k$ -median). The $\\beta$ -balanced $k$ -medians problem takes a dataset $X\\in V^{n}$ as input, and outputs centers $F\\in V^{k}$ along with an induced $\\beta$ -balanced partition $\\mathcal{C}$ of $X$ ; the goal is to minimize the cost function $\\begin{array}{r}{\\mathrm{med}_{k}(X,\\bar{F})=\\sum_{x\\in X}d(x,F)}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "This problem is similar to the lower-bounded $k$ -median problem where we want to output ${\\textbf{\\textit{F}}}=$ $\\{f_{1},\\dot{\\cdot}\\cdot\\cdot,f_{k}\\}\\,\\in\\,V^{k}$ as well as a partition $\\{C_{1},\\ldots,C_{k}\\}$ of $X$ such that $\\left|C_{i}\\right|\\,\\geq\\,\\beta|X|$ , in order to minimize $\\sum_{x\\in C_{i.}}d(x,f_{i})$ (see Han et al. (2020), Wu et al. (2022)). A significant difference between that defini tion and ours is that we require the partition to be induced by $F$ (in the sense of Definition 7), which is not required by lower-bounded $k$ -median. ", "page_idx": 5}, {"type": "text", "text": "5 $\\delta$ -Robustness of Location Estimators ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section we define notions of robustness with respect to changing a $\\delta$ -fraction of the points (hence referred to as $\\delta$ -robustness) and analyze the $\\delta$ -robustness of 1-Median and $\\mathrm{k\\cdot}$ -Medians. These results will be very useful for the mechanism design for facility location with predictions. We consider two robustness notions for location estimators: The first notion measures the movement in the solution (as measured by the Hausdorff distance), and the second measures the change in an objective function applied to the solution. ", "page_idx": 5}, {"type": "text", "text": "Definition 9 (Distance Robustness). Let $\\rho\\geq0$ , $\\delta\\in[0,0.5)$ . For location estimators $f,{\\widehat{f}}:V^{n}\\to V^{k}$ , we say that $\\widehat{f}$ is $(\\rho,\\delta)$ -distance-robust with respect to $f$ if for any $X\\in V^{n}$ and any $X^{\\prime}$ in the $\\delta|X|$ - neighborho od of $X$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nd_{H}\\Big(f(X),\\widehat{f}(X^{\\prime})\\Big)\\leq\\rho.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the next definition, cost function $F$ is evaluated for the same dataset $X\\in V^{n}$ with respect to two different solutions $f(X)$ and ${\\widehat{f}}(X^{\\prime})$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 10 (Approximation Robustness). Let $\\gamma\\geq1$ , $\\delta\\in[0,0.5)$ and let $F:V^{n}\\times V^{k}\\rightarrow\\mathbb{R}_{\\geq0}$ be a cost measure. For location estimators $f,{\\widehat{f}}:V^{n}\\to V^{k}$ , we say that $\\widehat{f}\\operatorname{\\Pi}i s\\,a\\,(\\gamma,\\delta)$ -approximation-robust solution for cost function $F$ with respect  to $f$ if for any $X\\in V^{n}$ and  any $X^{\\prime}$ in the $\\delta|X|$ -neighborhood of $X$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\nF(X,{\\widehat{f}}(X^{\\prime}))\\leq\\gamma\\cdot F(X,f(X)).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$I f\\,\\widehat{f}\\,=\\,f$ , we say that $f$ is a $(\\gamma,\\delta)$ -approximation-robust solution for $F$ . If $E q$ . (2) only holds for da tasets $X\\in{\\mathcal{Y}}$ for some ${\\mathcal{Y}}\\subseteq V^{n}$ , we say that $f$ is a $(\\gamma,\\delta)$ -approximation-robust solution for $F$ restricted to instances $\\mathcal{Y}$ . ", "page_idx": 6}, {"type": "text", "text": "We give the the following $\\delta$ -robustness results. For the 1-Median: ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. For $\\delta\\ <\\ {^1}/{2},$ , the 1-Median location estimator is \u00b7 me|d1(|X), \u03b4 -distancerobust, where ${\\mathrm{med}}_{1}(X)$ is the optimal cost of the geometric median, that is: ${\\mathrm{med}}_{1}(X)\\ =$ $\\begin{array}{r}{\\sum_{x_{i}\\in X}d\\bigl(x_{i},\\mathbf{GEOMETRIC-MEDIAN}(X)\\bigr)}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 3. For $\\delta<{^1}\\!/\\!2$ , the 1-Median estimator is $\\begin{array}{r}{(1+\\frac{4\\delta}{1-2\\delta},\\delta)}\\end{array}$ -approximation-robust for $\\mathrm{\\med_{1}}$ . ", "page_idx": 6}, {"type": "text", "text": "Recall Example 1 from Section 1. In this example, no choice of $k\\,=\\,2$ centers simultaneously achieves a good (bounded) approximation for the 2-medians problem on both $X$ and $X^{\\prime}$ . Given this negative example, we turn to a balanced version of $k$ -medians for which we are able to give a robustness guarantee. The following theorem shows that computing the best \u201cslightly less balanced\u201d solution on the predictions has a cost (on the original dataset $X$ ) that is close to the optimal one. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. Let $b>2k+2$ . Consider the algorithm $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ that computes the optimal $(b-1)\\delta$ -balanced $k$ -medians solution on its input. For any instance $X\\in V^{n}$ of the $b\\delta$ -balanced $k$ -medians problem with optimal solution $G$ , let $X^{\\prime}\\in V^{n}$ belong to the $\\delta$ -neighborhood of $X$ . The algorithm $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , when given $X^{\\prime}$ , returns a solution $H$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\nd_{H}(G,H)\\leq{\\frac{2k}{\\delta|X|\\cdot(b-2-2k)}}\\cdot\\sum_{x\\in X}d(x,G).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover, the $k$ -medians cost $\\begin{array}{r}{\\operatorname{med}_{k}(X,H)\\leq(1+\\frac{4k}{b-2-2k})\\operatorname{med}_{k}(X,G),}\\end{array}$ , and $H$ induces a $(b-2)\\delta$ balanced partition of . ", "page_idx": 6}, {"type": "text", "text": "For a fixed value of $k$ , this theorem gives a solution $H$ that is $(b{-}2)\\delta$ -balanced and also $(1\\!+\\!O(^{1}\\!/\\!b),\\delta)$ - approximation-robust with respect to the best $b\\delta$ -balanced solution. ", "page_idx": 6}, {"type": "text", "text": "Missing proofs are provided in Appendix E. The intuition behind the proofs in this section is that since most points $(1-\\delta)$ are shared between the actual instances and the predictions, any cost-minimizing solution for the predictions will be close (in terms of Hausdorff distance) to the cost-minimizing solution for the real points, thus resulting in a similar cost as implied by Lemma 10 in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "6 Mechanism Design for Facility Location with MAC Predictions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "6.1 Warmup: Deterministic Mechanism Design for Facility Location ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section we show how to design deterministic mechanisms for facility location problems that utilize the MAC predictions to get better approximation guarantees. We do so by utilizing the $\\delta$ -robustness results from Section 5. In the mechanism design setting, there are $n$ agents with (true) locations $X=\\{x_{i}\\mid i\\in[n]\\}\\in\\mathbb{R}^{d}$ , where $d$ is the dimension of the facility location problem. The mechanism has access to location reports by the strategic agents, and also to $X^{\\prime}={\\bf{\\dot{\\{}}}}x_{1}^{\\prime},\\dots,x_{n}^{\\prime}\\}$ , which are the $\\mathbf{MAC}(\\varepsilon,\\delta)$ predictions of the true locations $X$ . The mechanism only has access to the reported locations $(\\tilde{X})$ and the predictions $(X^{\\prime})$ . For any strategyproof mechanism, the reported locations will be the same as real ones $\\!\\!\\!\\!X={\\tilde{X}}$ ) as illustrated in Fig. 2. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bullet~\\mathrm{{\\Large~Real}~}~~~~~}&{{}\\stackrel{\\circ}{\\mathrm{{\\Large~e~}}}_{\\bullet}~~\\bullet~\\cdots~\\cdots}\\\\ {\\circ~\\mathrm{{\\Large~Reported}~}~~}&{{}\\bullet~~\\bullet~\\bullet~~\\cdots~}\\\\ {\\bullet~\\mathrm{{\\Large~Predicted}~}~~~}&{{}\\stackrel{\\circ}{\\mathrm{{\\Large~9~}}}_{\\bullet}~~\\bullet~\\cdots~}\\\\ {\\bullet~~}&{{}\\bullet~~\\mathrm{{\\Large~\\sigma}}_{\\circ}}\\end{array}\\bullet~~\\cdots~\\circ\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Figure 2: There are three sets of points: The real locations $X$ (black), the reported locations $\\tilde{X}$ (yellow), and the predicted locations $X^{\\prime}(\\mathrm{blue})$ . The algorithm can only access ${\\tilde{X}},X^{\\prime}$ . For any strategyproof mechanism, the yellow and black points overlap. ", "page_idx": 7}, {"type": "text", "text": "As explained in Appendix C, for the sake of the analysis we assume that $\\varepsilon=0$ . All proofs can be found at Appendix F. ", "page_idx": 7}, {"type": "text", "text": "For the single facility location problem $\\left[k=1\\right]$ ): In this setting we assume\u221a $\\delta$ (or an upper bound value) is known and thus we can simply check in advance if $\\begin{array}{r}{1+\\frac{4\\delta}{1-2\\delta}\\leq\\sqrt{d}}\\end{array}$ or not, and use the algorithm that guarantees us the best approximation ratio. ", "page_idx": 7}, {"type": "table", "img_path": "LPbqZszt8Y/tmp/384297a7dda4b239a77a6fe2c0d7305e84b8c33ad33b4112d17c91cf0f4e3d3a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "See Appendix F.2 for a clarification on the computation of the geometric median. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5. Algorithm 1 is strategyproof and gets an approximation ratio of min $\\textstyle\\left(1+{\\frac{4\\delta}{1-2\\delta}},{\\sqrt{d}}\\right)$ ", "page_idx": 7}, {"type": "text", "text": "Supporting the weaker setting of high probability (only w.p. $\\mathbf{1}-o(^{1}/n))$ MAC predictions: In Appendix F.3 we show how to achieve the same approximation ratio as in Theorem 5. ", "page_idx": 7}, {"type": "text", "text": "For the $k$ facility location problem: In a similar way to the single facility, we can also get a deterministic mechanism for the $k$ facility location problem with predictions, under some balancedness condition (see Appendix G). The balancedness condition roughly translates to a minimum cluster size in the optimal solution. The resulting approximation ratio is bounded by $1+{\\frac{4k}{b-2-2k}}$ where $b$ is a constant that depends on the minimum cluster size and $\\delta$ . ", "page_idx": 7}, {"type": "text", "text": "6.2 Randomized Mechanism Design for 2-Facility Location on a Line ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now consider the 2-facility-location problem, where the metric is given by the real line $\\mathbb{R}$ . Formally, our metric is over the point set $V=\\mathbb{R}$ , with $d(x,y)=|x-y|$ . There are $n$ agents with locations on the real line $X=\\{x_{1},\\ldots,x_{n}\\}$ , and the cost of a solution $\\dot{G}\\in V^{2}$ is $\\overline{{\\mathrm{med}_{k}(X,G)}}=$ $\\textstyle\\sum_{x_{i}\\in X}d(x_{i},G)$ . In the mechanism-design setting, the agent locations $X$ are unknown, but we are given (a) MAC predictions $X^{\\prime}$ for these locations, as well as (b) the agents\u2019 reports for their locations. Unlike in the $\\beta$ -balanced problem variation, we do not assume the balancedness of the optimal solution, but handle the unconstrained case where the optimal solution might induce either a balanced or an unbalanced clustering of $X$ . In this section, we present a strategy-proof radnom mechanism with a small (expected) approximation ratio. ", "page_idx": 7}, {"type": "text", "text": "Before doing so, we solve two other problems. After we solve those in Sections 6.2.1 and 6.2.2, we use the solutions in our mechanism for the 2-facility-location on a line problem in Section 6.2.3. ", "page_idx": 7}, {"type": "text", "text": "6.2.1 The Second Facility Location Problem ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "First, we define another independent mechanism design problem (the second facility location) for which we show a randomized mechanism. We later use to solve the 2-facility-location problem. ", "page_idx": 7}, {"type": "text", "text": "Definition 11 (Second Facility Location). Given a metric space $(V,d)$ , a dataset $X\\,\\in\\,V^{n}$ , and a single facility $h_{S}\\,\\in\\,V$ , the goal is to find another facility $h_{T}$ to minimize the 2-medians cost $\\mathrm{med_{2}}(X,\\{h_{S},h_{T}\\})$ . ", "page_idx": 7}, {"type": "text", "text": "We consider the following SECOND-PROPORTIONAL-MECHANISM, which is the second step of the random mechanism for the 2 facility location on the line problem proposed by Lu et al. (2010). ", "page_idx": 8}, {"type": "table", "img_path": "LPbqZszt8Y/tmp/f3aa1385de07c6920b5cdc049cc488480e170902f67a52cdb5bec82d9fcbc32d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Lemma 6. SECOND-PROPORTIONAL-MECHANISM is strategyproof for any metric space. ", "page_idx": 8}, {"type": "text", "text": "Theorem 7 (Second Facility Results). Let $V=\\mathbb{R}$ . Fix any dataset $X$ and first facility $\\{g_{S}\\}$ . For any second facility $g_{T}^{*}\\in V$ ; let $(S,T)$ be a partition of the dataset induced by $F^{*}=\\{g_{S},g_{T}^{*}\\}$ . The expected cost of SECOND-PROPORTIONAL-MECHANISM given $X$ and $g_{S}$ is: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{med}_{2}(X,\\{g_{S},g_{T}\\})]\\leq2\\,\\mathrm{med}_{1}\\big(S,\\{g_{S}\\}\\big)+3\\,\\mathrm{med}_{1}\\big(T,\\{g_{T}^{*}\\}\\big),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $g_{T}$ is the second facility location chosen by SECOND-PROPORTIONAL-MECHANISM. ", "page_idx": 8}, {"type": "text", "text": "We defer the proofs of Lemma 6 and Theorem 7 to Appendix I. See Remark 29 for a tight example. ", "page_idx": 8}, {"type": "text", "text": "The algorithm prioritizes farther locations (belonging to agents who pay more if their location is not chosen) over agents that are near the first facility. The intuition for the strategyproofness is that if the agent deviates to a different location, it might be chosen with a higher probability, but the agent will also pay more. The largest technical difference in the analysis from Lu et al. (2010) is our introducing of a more delicate analysis for the line metric. We stress, however, that the better approximation ratio of 3 (rather than the ratio of 4 of Lu et al. (2010)) is due to the different nature of the problem (rather then the more delicate analysis) as both algorithms are tight for the line metric. One intuitive explanation for why the similar approach works better for the second facility problem is that the first facility is fixed and so we do not have to pay the cost of \"guessing it\" as done in Lu et al. (2010). ", "page_idx": 8}, {"type": "text", "text": "6.2.2 The $\\delta$ -Robustness of the Big Cluster Center ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our randomized mechanism uses the predictions to estimate one facility, and randomly chooses the second facility. For this purpose, we come up with an algorithm to \u201cestimate\u201d the first facility location. We formally define what it means to \u201cestimate\u201d one of the two facilities, and then we quantify the distance and approximation robustness of the estimator. This is the last piece of the puzzle that we need for our randomized mechanism. The idea here is that even though Example 1 shows we cannot simply compute the optimal solution, most of the predictions are correct and contain useful information: We can still get a good estimation for one of the two centers of the optimal solution. In this section, we show an algorithm (location estimator), BIG-CLUSTER-CENTER, that \u201cestimates\u201d of the center of the bigger4 cluster center out of the two clusters induced by 2-Medians. ", "page_idx": 8}, {"type": "text", "text": "Definition 12 (BIG-CLUSTER-COST). For any $X\\,=\\,\\{x_{1},\\ldots,x_{n}\\}\\,\\in\\,V^{n}$ , let $F$ be its optimal 2-median, and let $\\mathcal{C}=\\mathcal{C}(X,F)$ be the induced clustering. Let BIG-CLUSTER $(X)$ be the cluster in clustering $\\mathcal{C}$ with at least $|X|/_{2}$ points (if both clusters have the same size BIG-CLUSTER $(X)$ can be the one containing $x_{1}$ ). For $t\\in V$ , define ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\exists\\mathrm{IG-CLUSTER-COST}(X,t):=\\mathrm{med}_{1}\\big(\\mathrm{BIG-CLUSTER}(X),t\\big).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Definition 13 (Big Cluster Location Problem). Given n points $X\\in V^{n}$ , the objective is to find a point $t\\in V$ to minimize the cost function BIG-CLUSTER- $\\mathrm{CoST}(X,t)$ . ", "page_idx": 8}, {"type": "text", "text": "We specifically focus on the line metric, and only consider $X\\in\\mathbb{R}^{n}$ instances where the clustering induced by 2-Medians $(X)$ are $b\\delta$ -unbalanced for some (some constants) $b>1$ , $\\delta\\in[0,{^1}/2)$ . ", "page_idx": 8}, {"type": "table", "img_path": "LPbqZszt8Y/tmp/1425b6a416c464747dc139910a07f240da861b029c5d47867a7512ba360f976a.jpg", "table_caption": [], "table_footnote": ["4For any clustering induced by any 2 points in space, there is always one bigger cluster (that is, the cluster that contains at least half the points). "], "page_idx": 8}, {"type": "text", "text": "Theorem 8. Let $\\mathcal{Y}$ be the collection of all datasets in R for which the optimal 2-medians induces no $b\\delta$ -balanced clusterings. Then for a small enough (constant) \u03b4, Algorithm BIG-CLUSTER-CENTER is $(1.8+O(\\delta),\\delta)$ -approximation-robust for the cost function BIG-CLUSTER-COST restricted to instances in $\\mathcal{Y}$ . ", "page_idx": 9}, {"type": "text", "text": "The intuition for the proof of Theorem 8 is that one cluster has a big portion of the points, and most of them also appear in the predictions. Thus, in the cost minimizing solution computed on the predictions, there must be a big center \u201cnear\u201d these points (since any solution far away from these points would result in a higher cost). The analysis is close to tight (see Remark 25 in Appendix H). ", "page_idx": 9}, {"type": "text", "text": "6.2.3 The 2-Facility Location Algorithm ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our randomized algorithm below first uses the BIG-CLUSTER-CENTER procedure on the predictions to approximate the \u201cbig\u201d cluster center, and then uses the SECOND-PROPORTIONAL-MECHANISM to approximate the second cluster center using only the agents\u2019 reports. (We set $b\\in\\mathbb{R}_{>1}$ such that the approximation robustness in Theorem 4 equals 1.2.) The proof is at Appendix J. ", "page_idx": 9}, {"type": "table", "img_path": "LPbqZszt8Y/tmp/13f083ee812d2bb653618c202d36ab4f426862f582420b3e29cd222caafa6b20.jpg", "table_caption": ["Theorem 9. For a small enough (but still constant) $\\delta$ : Algorithm $^{4}$ is strategyproof and has an expected approximation ratio of $3.6+O(\\delta)$ . "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "The proof uses most of the tools we came up with so far, and it considers two main scenarios: when clustering is $b\\delta$ -balanced and when it is unbalanced. In both cases, by leveraging the $\\delta$ -robustness of 1-median, 2-medians, big-cluster-center and the (randomized) strategic placement of the second facility, the mechanism ensures that the solution approaches the optimal cost. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We explore strategyproof facility location within our introduced MAC predictions error model. Our model integrates the trust level into the error model, handles outliers, and leads to new mechanism designs (that do not seem to be an interpolation between completely trusting the predictions and resorting to a \"no-predictions\" method). To analyze our designs, we utilize distance and approximation robustness notions established in Section 5. ", "page_idx": 9}, {"type": "text", "text": "An immediate question arising from our results is whether there is a deterministic mechanism with MAC predictions for 2-facility location on a line that yields a constant approximation. Another avenue for future research is improving the approximation ratio by randomized mechanisms for this problem. One promising strategy entails refining the selection process for the first facility in Algorithm 4, which can lead to a superior approximation ratio. Our current mechanisms treat the multi-sets of predictions and reported values as independent entities; an intriguing direction for further investigation is to devise mechanisms that capitalize on the matching between predictions and agent-reported values, potentially yielding a more accurate approximation ratio. Lastly, exploring the application of the MAC model to problems beyond mechanism design for facility location may yield insights into the efficacy of such predictions in diverse contexts. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We are grateful to Batya Berzack for comments on an earlier draft of the paper, and to Haim Kaplan, Michal Feldman, Amos Fiat and Yossi Azar for discussions about a previous version of the model. We would like to thank the NeurIPS 2024 anonymous reviewers for their comments on this paper. This work is funded by the European Union (ERC grant No. 101077862, project: ALGOCONTRACT, PI: Inbal Talgam-Cohen), a Google Research Scholar award, the Israel Science Foundation (ISF grant No. 3331/24), the Binational Science Foundation (BSF grant No. 2021680), and the National Science Foundation (NSF grant Nos. CCF-1955785 and CCF-2006953). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Gagan Aggarwal, Rina Panigrahy, Tom\u00e1s Feder, Dilys Thomas, Krishnaram Kenthapadi, Samir Khuller, and An Zhu. 2010. Achieving anonymity via clustering. ACM Transactions on Algorithms (TALG) 6, 3 (2010), 1\u201319.   \nAkanksha Agrawal, Tanmay Inamdar, Saket Saurabh, and Jie Xue. 2023. Clustering what matters: Optimal approximation for clustering with outliers. Journal of Artificial Intelligence Research 78 (2023), 143\u2013166.   \nPriyank Agrawal, Eric Balkanski, Vasilis Gkatzelis, Tingting Ou, and Xizhi Tan. 2022. Learningaugmented mechanism design: Leveraging predictions for facility location. In Proceedings of the 23rd ACM Conference on Economics and Computation. 497\u2013528.   \nMatteo Almanza, Flavio Chierichetti, Silvio Lattanzi, Alessandro Panconesi, and Giuseppe Re. 2021. Online facility location with multiple advice. Advances in Neural Information Processing Systems 34 (2021), 4661\u20134673.   \nIdan Amir, Idan Attias, Tomer Koren, Yishay Mansour, and Roi Livni. 2020. Prediction with corrupted expert advice. Advances in Neural Information Processing Systems 33 (2020), 14315\u201314325.   \nKeerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. 2021. A regression approach to learning-augmented online algorithms. Advances in Neural Information Processing Systems 34 (2021), 30504\u201330517.   \nKeerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. 2022. Online algorithms with multiple predictions. In International Conference on Machine Learning. PMLR, 582\u2013598.   \nKeerti Anand, Rong Ge, and Debmalya Panigrahi. 2020. Customizing ML predictions for online algorithms. In International Conference on Machine Learning. PMLR, 303\u2013313.   \nAntonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. 2020. Secretary and online matching problems with machine learned advice. Advances in Neural Information Processing Systems 33 (2020), 7933\u20137944.   \nYossi Azar, Debmalya Panigrahi, and Noam Touitou. 2022. Online graph algorithms with predictions. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). SIAM, 35\u201366.   \nYossi Azar, Debmalya Panigrahi, and Noam Touitou. 2023. Discrete-Smoothness in Online Algorithms with Predictions. In Thirty-seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id $\\equiv$ DDmH3H78iJ   \nHaris Aziz, Hau Chan, Barton Lee, Bo Li, and Toby Walsh. 2020. Facility location problem with capacity constraints: Algorithmic and mechanism design perspectives. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 1806\u20131813.   \nMaria-Florina Balcan, Siddharth Prasad, and Tuomas Sandholm. 2023. Bicriteria Multidimensional Mechanism Design with Side Information. CoRR abs/2302.14234 (2023). https://doi.org/ 10.48550/ARXIV.2302.14234 arXiv:2302.14234   \nMaria-Florina Balcan and Nicholas JA Harvey. 2018. Submodular functions: Learnability, structure, and optimization. SIAM J. Comput. 47, 3 (2018), 703\u2013754.   \nEric Balkanski, Vasilis Gkatzelis, and Xizhi Tan. 2023a. Mechanism Design with Predictions: An Annotated Reading List. SIGecom Exchanges 21, 1 (2023), 54\u201357.   \nEric Balkanski, Vasilis Gkatzelis, and Xizhi Tan. 2023b. Strategyproof Scheduling with Predictions. In 14th Innovations in Theoretical Computer Science Conference, ITCS 2023, January 10-13, 2023, MIT, Cambridge, Massachusetts, USA (LIPIcs, Vol. 251), Yael Tauman Kalai (Ed.). Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 11:1\u201311:22. https://doi.org/10.4230/LIPICS. ITCS.2023.11   \nEric Balkanski, Vasilis Gkatzelis, Xizhi Tan, and Cherlin Zhu. 2023c. Online Mechanism Design with Predictions. CoRR abs/2310.02879 (2023). https://doi.org/10.48550/ARXIV.2310.02879 arXiv:2310.02879   \nAmir Beck and Shoham Sabach. 2015. Weiszfeld\u2019s method: Old and new results. Journal of Optimization Theory and Applications 164 (2015), 1\u201340.   \nBen Berger, Michal Feldman, Vasilis Gkatzelis, and Xizhi Tan. 2023. Optimal Metric Distortion with Predictions. CoRR abs/2307.07495 (2023). https://doi.org/10.48550/ARXIV.2307.07495 arXiv:2307.07495   \nGiulia Bernardini, Alexander Lindermayr, Alberto Marchetti-Spaccamela, Nicole Megow, Leen Stougie, and Michelle Sweering. 2022. A universal error measure for input predictions applied to online graph problems. Advances in Neural Information Processing Systems 35 (2022), 3178\u2013 3190.   \nAnup Bhattacharya, Dishant Goyal, and Ragesh Jaiswal. 2020. Hardness of Approximation of Euclidean $k$ -Median. arXiv preprint arXiv:2011.04221 (2020).   \nHau Chan, Aris Filos-Ratsikas, Bo Li, Minming Li, and Chenhao Wang. 2021. Mechanism Design for Facility Location Problems: A Survey. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, Zhi-Hua Zhou (Ed.). International Joint Conferences on Artificial Intelligence Organization, 4356\u20134365. https://doi.org/10.24963/ijcai.2021/ 596 Survey Track.   \nMoses Charikar, Samir Khuller, David M Mount, and Giri Narasimhan. 2001. Algorithms for facility location problems with outliers. In SODA, Vol. 1. Citeseer, 642\u2013651.   \nMichael B Cohen, Yin Tat Lee, Gary Miller, Jakub Pachocki, and Aaron Sidford. 2016. Geometric median in nearly linear time. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing. 9\u201321.   \nIlias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Ali Vakilian, and Nikos Zarifis. 2021. Learning online algorithms with distributional advice. In International Conference on Machine Learning. PMLR, 2687\u20132696.   \nPaul D\u00fctting, Silvio Lattanzi, Renato Paes Leme, and Sergei Vassilvitskii. 2021. Secretaries with advice. In Proceedings of the 22nd ACM Conference on Economics and Computation. 409\u2013429.   \nUlrich Eckhardt. 1980. Weber\u2019s problem and Weiszfeld\u2019s algorithm in general spaces. Mathematical Programming 18 (1980), 186\u2013196.   \nYuval Emek, Yuval Gil, Maciej Pacut, and Stefan Schmid. 2023. Online Algorithms with Randomly Infused Advice. arXiv preprint arXiv:2302.05366 (2023).   \nDimitris Fotakis, Evangelia Gergatsouli, Themis Gouleakis, and Nikolas Patris. 2021. Learning augmented online facility location. arXiv preprint arXiv:2107.08277 (2021).   \nDimitris Fotakis and Christos Tzamos. 2014. On the power of deterministic mechanisms for facility location games. ACM Transactions on Economics and Computation (TEAC) 2, 4 (2014), 1\u201337.   \nVasilis Gkatzelis, Kostas Kollias, Alkmini Sgouritsa, and Xizhi Tan. 2022. Improved Price of Anarchy via Predictions. In EC \u201922: The 23rd ACM Conference on Economics and Computation. 529\u2013557.   \nSumit Goel and Wade Hann-Caruthers. 2023. Optimality of the coordinate-wise median mechanism for strategyproof facility location in two dimensions. Social Choice and Welfare 61, 1 (2023), 11\u201334.   \nAnupam Gupta, Debmalya Panigrahi, Bernardo Subercaseaux, and Kevin Sun. 2022. Augmenting Online Algorithms with epsilon Accurate Predictions. Advances in neural information processing systems (2022).   \nLu Han, Chunlin Hao, Chenchen Wu, and Zhenning Zhang. 2020. Approximation algorithms for the lower-bounded $\\boldsymbol{\\mathrm{k}}$ -median and its generalizations. In International Computing and Combinatorics Conference. Springer, 627\u2013639.   \nGabriel Istrate and Cosmin Bonchis. 2022. Mechanism Design With Predictions for Obnoxious Facility Location. CoRR abs/2212.09521 (2022). https://doi.org/10.48550/ARXIV.2212. 09521 arXiv:2212.09521   \nShaofeng H-C Jiang, Erzhi Liu, You Lyu, Zhihao Gavin Tang, and Yubo Zhang. 2021a. Online facility location with predictions. arXiv preprint arXiv:2110.08840 (2021).   \nZhihao Jiang, Pinyan Lu, Zhihao Gavin Tang, and Yuhao Zhang. 2021b. Online selection problems against constrained adversary. In International Conference on Machine Learning. PMLR, 5002\u2013 5012.   \nMisha Khodak, Maria-Florina F Balcan, Ameet Talwalkar, and Sergei Vassilvitskii. 2022. Learning predictions for algorithms with predictions. Advances in Neural Information Processing Systems 35 (2022), 3542\u20133555.   \nAkshay Krishnamurthy, Thodoris Lykouris, and Chara Podimata. 2020. Corrupted Multidimensional Binary Search: Learning in the Presence of Irrational Agents. CoRR abs/2002.11650 (2020). arXiv:2002.11650 https://arxiv.org/abs/2002.11650   \nRavishankar Krishnaswamy, Shi Li, and Sai Sandeep. 2018. Constant approximation for $\\mathbf{k}$ -median and k-means with outliers via iterative rounding. In Proceedings of the 50th annual ACM SIGACT symposium on theory of computing. 646\u2013659.   \nKevin A. Lai, Anup B. Rao, and Santosh Vempala. 2016. Agnostic Estimation of Mean and Covariance. arXiv:1604.06968 [cs.DS]   \nThomas Lavastida, Benjamin Moseley, R Ravi, and Chenyang Xu. 2020. Learnable and instancerobust predictions for online matching, flows and load balancing. arXiv preprint arXiv:2011.11743 (2020).   \nHendrik P Lopuhaa and Peter J Rousseeuw. 1991. Breakdown points of affine equivariant estimators of multivariate location and covariance matrices. The Annals of Statistics (1991), 229\u2013248.   \nPinyan Lu, Xiaorui Sun, Yajun Wang, and Zeyuan Allen Zhu. 2010. Asymptotically optimal strategyproof mechanisms for two-facility games. In Proceedings of the 11th ACM conference on Electronic commerce. 315\u2013324.   \nPinyan Lu, Yajun Wang, and Yuan Zhou. 2009. Tighter bounds for facility games. In Internet and Network Economics: 5th International Workshop, WINE 2009, Rome, Italy, December 14-18, 2009. Proceedings 5. Springer, 137\u2013148.   \nThodoris Lykouris, Vahab S. Mirrokni, and Renato Paes Leme. 2018. Stochastic bandits robust to adversarial corruptions. CoRR abs/1803.09353 (2018). arXiv:1803.09353 http://arxiv.org/ abs/1803.09353   \nThodoris Lykouris and Sergei Vassilvitskii. 2021. Competitive caching with machine learned advice. Journal of the ACM (JACM) 68, 4 (2021), 1\u201325.   \nNimrod Megiddo and Kenneth J Supowit. 1984. On the complexity of some common geometric location problems. SIAM journal on computing 13, 1 (1984), 182\u2013196.   \nReshef Meir. 2019. Strategyproof facility location for three agents on a circle. In International symposium on algorithmic game theory. Springer, 18\u201333.   \nMichael Mitzenmacher and Sergei Vassilvitskii. 2022. Algorithms with predictions. Commun. ACM 65, 7 (2022), 33\u201335. https://doi.org/10.1145/3528087   \nAriel D Procaccia and Moshe Tennenholtz. 2013. Approximate mechanism design without money. ACM Transactions on Economics and Computation (TEAC) 1, 4 (2013), 1\u201326.   \nManish Purohit, Zoya Svitkina, and Ravi Kumar. 2018. Improving online algorithms via ML predictions. Advances in Neural Information Processing Systems 31 (2018).   \nDhruv Rohatgi. 2020. Near-optimal bounds for online caching with machine learned advice. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms. SIAM, 1834\u20131845.   \nBo Sun, Jerry Huang, Nicolas Christianson, Mohammad Hajiesmaili, and Adam Wierman. 2023. Online Algorithms with Uncertainty-Quantified Predictions. arXiv preprint arXiv:2310.11558 (2023).   \nXiaoliang Wu, Feng Shi, Yutian Guo, Zhen Zhang, Junyu Huang, and Jianxin Wang. 2022. An approximation algorithm for lower-bounded k-median with constant factor. Science China Information Sciences 65, 4 (2022), 140601.   \nChenyang Xu and Pinyan Lu. 2022. Mechanism Design with Predictions. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, Lud De Raedt (Ed.). International Joint Conferences on Artificial Intelligence Organization, 571\u2013577. https: //doi.org/10.24963/ijcai.2022/81 Main Track.   \nChenyang Xu and Benjamin Moseley. 2022. Learning-augmented algorithms for online steiner tree. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 8744\u20138752.   \nEmmanouil Zampetakis and Fred Zhang. 2023. Bayesian Strategy-Proof Facility Location via Robust Estimation. In International Conference on Artificial Intelligence and Statistics. PMLR, 4196\u20134208. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Organization of the Appendices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Appendix A contains more related work. ", "page_idx": 14}, {"type": "text", "text": "Appendix B contains more examples for sources of MAC predictions.   \nAppendix C contains the explanation for the effect of $\\varepsilon>0$ .   \nAppendix D contains an interpretation of the established robust statistics $\\delta$ -robustness results. Appendix E contains the proofs for Section 5.   \nAppendix F and Appendix G contain proofs and supplementary material for Section 6.1.   \nAppendix H, Appendix I and Appendix J contain the missing proofs for section Section 6.2 (for the analysis of Algorithm 4 - \"Robust Half\" mechanism). Namely: \u2022 Appendix I contains the proof for Algorithm 2 properties: Lemma 6, Theorem 7.   \n\u2022 Appendix H contains the proof of Theorem 8.   \n\u2022 Appendix J contains the proof of Theorem 9. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Additional Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Models of prediction accuracy. Azar et al. (2022) study online graph algorithms with a notion of error they call metric error with outliers, applied to problems where the predictions are locations in a metric space. The error parameters of this model are $D$ and $\\Delta_{}$ \u2014roughly, $D$ is the minimum cost matching between the predictions and the actual locations, where $\\Delta$ outliers may be excluded from the matching. These parameters are similar to the $(\\varepsilon,\\delta)$ parameters of the MAC model. They then give a general framework for solving online graph problems in their model. Interestingly, their matching cost $D$ captures a notion of average error, whereas we focus on individual error; while our results can be extended to fit within their model, it would be interesting to investigate a version of MAC where $\\varepsilon$ captures some notion of average error. ", "page_idx": 15}, {"type": "text", "text": "Gkatzelis et al. (2022) generalize the outlier model of Azar et al. (2022) and apply it to improve the price of anarchy of cost-sharing mechanisms. Xu and Moseley (2022) study online Steiner tree problems with predictions where the error parameter is the number of erroneous predictions. Bernardini et al. (2022) introduce a model named the \u201ccover error model\u201d that generalizes the model of Azar et al. (2022) to achieve more precision for online problems. ", "page_idx": 15}, {"type": "text", "text": "Gupta et al. (2022) study predictions that are incorrect independently with probability $\\delta$ . Their work focuses on online algorithms rather than mechanism design. Our model is related to their model since independent errors imply MAC predictions w.h.p. by Chernoff. The MAC model is more general in the sense that it does not assume independence of erroneous predictions. Emek et al. (2023) also consider a similar model to the one of Gupta et al. (2022), assuming access to predictions that are correct with some probability or contain random bits otherwise. D\u00fctting et al. (2021) consider a signaling scheme advice model for the secretary, which for binary ML-advice roughly translates into measures of accuracy of the signals (named recall and specificity). ", "page_idx": 15}, {"type": "text", "text": "Jiang et al. (2021b) consider a prediction guaranteed to be inside a prediction interval. While their error model is closer to the \u201cworst case\u201d error model, their motivation is confidence intervals from statistics, where it is reasonable to assume that a confidence interval is achieved with probability $1-\\delta$ for some $\\delta\\in[0,1]$ . Sun et al. (2023) consider an approach for \u201cuncertainty quantified\u201d prediction. One class of predictions they focus on, the \u201cprobabilistic interval prediction\u201d, can also be seen as a a confidence interval, as in the motivation of Jiang et al. (2021b). Like our MAC model, they \u201cmodel in\u201d the amount of trust in the advice. Sun et al. (2023) focus on settings where the algorithm receives a single prediction (such as the ski rental problem). They use online learning for tuning the trust parameters, and introduce a regret minimization analysis for learning the trust parameter online (over many instances of the same problem). The MAC model addresses problems that necessitate the quantification of uncertainty for multiple predictions simultaneously within the same input instance, such as in the $k$ -facility location problem. ", "page_idx": 15}, {"type": "text", "text": "Mechanism design for facility location problems without predictions. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Mechanism design for the single facility location problem. In the single facility location in $\\mathbb{R}^{d}$ problem, the task is to return a single facility minimizing the social cost function. For the case of nopredictio\u221ans, Meir (2019) shows a deterministic strategyproof mechanism that gets an approximation ratio of $\\sqrt{d}$ which is optimal for $d=2$ (Goel and Hann-Caruthers (2023)). The optimal deterministic strat\u221aegyproof mechanism is computing the COORDINATE-WISE-MEDIAN of the points. In short, the $\\dot{\\sqrt{d}}$ ratio follows from the fact that COORDINATE-WISE-MEDIAN is the optimal solution is the optimal solution w.r.t the $l1$ norm, which is \u221aat most $s q r t(d)$ times l2 norm. The goal of introducing predictions is to get something better than $\\sqrt{d}$ in this context. ", "page_idx": 15}, {"type": "text", "text": "Mechanism design for the two facility location on the line. In this variant the goal is to return two facilities to minimize the social cost, where all points lie on the real line $\\mathbb{R}$ . We want to find a strategyproof mechanism that given the agent reported locations returns the two locations for facilities in $\\mathbb{R}$ , s.t. the social cost is minimal. A deterministic $n-2$ approximation ratio mechanism (called the \"Two Extremes\" mechanism) was given by Procaccia and Tennenholtz (2013), and a lower bound of $n/2-1$ was given by Lu et al. (2010) which was later improved to (the tight) lower bound of $n-2$ by Fotakis and Tzamos (2014). A randomized strategyproof mechanism with an expected approximation ratio of 4 was given by Lu et al. (2010) (which also works for any metric space), while the currently best known lower bound for random mechanisms (Lu et al. (2009)) is 1.045. ", "page_idx": 15}, {"type": "text", "text": "$k$ -facility location. For $k\\,>\\,2$ , Fotakis and Tzamos (2014) show that there is no deterministic anonymous strategyproof mechanism with a bounded approximation ratio for $k$ -facility location on the line for the general case (not just balanced) for any $k\\geq3$ , even for simple instances with $k+1$ agents. Moreover, they show that there do not exist any deterministic strategyproof mechanisms with a bounded approximation ratio for 2-Facility Location on more general metric spaces, which is true even for simple instances with 3 agents located in a star. ", "page_idx": 16}, {"type": "text", "text": "Mechanism design for the capacitated facility location is a variant of the problem studied by Aziz et al. (2020) where each facility has a maximum capacity, limiting the number of points that can be assigned to it. For the utilitarian cost function, they have shown a $n/2-1$ approximation ratio. ", "page_idx": 16}, {"type": "text", "text": "Robust $k$ medians and facility location. The robustness of the (offilne non-strategic) $k$ medians and $k$ facility location has been studied under different variations. As Example 1 demonstrates, it is not possible to get any bounded approximation ratio for the optimal solution. The approach of Charikar et al. (2001) is to look at different variants of the problems with less restrictive objectives. In one variant they consider, the problem is to place facilities so as to minimize the service cost to any subset of facilities of size at least $p$ for some parameter $p$ . Another variant they consider allows denial of service for some of the clients with the additional cost of some penalty for each such denied client. For work in these kind of models see Krishnaswamy et al. (2018); Agrawal et al. (2023). ", "page_idx": 16}, {"type": "text", "text": "Other models of advice/predictions. In the recent literature of algorithms with predictions there are existing other models for algorithms with predictions which are different from the models we already discussed. We now mention some of them. ", "page_idx": 16}, {"type": "text", "text": "Some other ML advice is to focus on values that are learned via classical PAC learnability and focus on the learnability and analysis of sample complexity bounds. The assumption is that there is some distribution generating the input for the algorithm. Then the approach is to learn the distribution in terms of classic supervised learning. This kind of modeling is detailed in Anand et al. (2020), Lavastida et al. (2020), Anand et al. (2021). ", "page_idx": 16}, {"type": "text", "text": "Diakonikolas et al. (2021) assumes that the advice is not given deterministically, but that they can access the distributions of the inputs and sample from these: they consider access to the instances of interest, and the goal is to learn a competitive algorithm given access to i.i.d. samples. They provide sample complexity bounds for the underlying learning tasks. ", "page_idx": 16}, {"type": "text", "text": "Online variants of the facility location problem with different notions of prediction were studied e.g. by Almanza et al. (2021); Fotakis et al. (2021); Jiang et al. (2021a); Azar et al. (2022); Gupta et al. (2022); Azar et al. (2023); Anand et al. (2022). The online setting of the problem differs significantly from our mechanism design setting: the real points arrive in sequence where each time an irrevocable decision must be made by the online algorithm (unlike the mechanism design setting where all the input is given to the mechanism at once, but is reported by strategic agents). ", "page_idx": 16}, {"type": "text", "text": "Learning the trust parameter. In the context of online algorithms with prediction, Khodak et al. (2022), show that the confidence parameter can be estimated under certain conditions via online learning. Sun et al. (2023) also have online learning analysis to estimate their different type of confidence parameter. ", "page_idx": 16}, {"type": "text", "text": "Other algorithmic game theory with prediction work Istrate and Bonchis (2022) study mechanism design with predictions for obnoxious facility location where agents prefer to be as far as possible from the facilities. Research in algorithmic game theory incorporating predictions, beyond the previously discussed work, has been explored by Balkanski et al. (2023b,c); Berger et al. (2023); Balcan et al. (2023). ", "page_idx": 16}, {"type": "text", "text": "Models outside of algorithms with predictions literature. One way to view the MAC model is to view the predictions as data with with corruptions. Designing algorithms to try and handle the corruptions was studied before. Multi-Arm-Bandit with corruptions is such a setting (Lykouris et al. (2018); Krishnamurthy et al. (2020); Amir et al. (2020)). ", "page_idx": 16}, {"type": "text", "text": "Zampetakis and Zhang (2023) propose the use of robust statistical estimators for strategyproof mechanism design in a Bayesian setting. They show how to use a location estimator which is robust to corrupting $\\delta$ fraction of the data drawn from some known distribution, to get a strategyproof mechanism for the same location problem. This is conceptually very similar to our result for the single facility case. Since there is no robust estimator for the two-facility problem, their approach cannot apply without making Bayesian assumptions; nonetheless, in our work we show how to use predictions and agent reports to get improvements on the worst-case approximation guarantees. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "B MAC Prediction Sources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The MAC prediction model is suitable for capturing errors from a variety of sources, mainly: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Machine learning. In practice, useful ML models produce predictions which are mostly accurate, and the accuracy parameters $\\epsilon,\\delta$ can be estimated through testing. For example, we often have a predictor with the guarantee that each prediction (independently) will be $\\varepsilon$ -correct with probability at least $1-\\delta$ . Thus, through concentration bounds such as Chernoff, with high probability the number of predictions which are $\\varepsilon$ -incorrect is at most $\\approx\\delta n$ . This was the motivation in Balcan and Harvey (2018). Gupta et al. (2022) also consider independent predictions, each correct w.p. $\\delta$ . Note that unlike this example, the MAC model does not require that the predictor\u2019s errors are independent (and in that sense is a generalization, having a weaker assumption). ", "page_idx": 17}, {"type": "text", "text": "\u2022 Data with corruptions. In a setting in which the algorithm\u2019s input itself might contain errors, we can treat the corrupted input as a prediction of the true input and apply the MAC model. Corruptions can be a result of adversarial changes to the algorithm\u2019s input, e.g., in a malicious attempt to affect the output \u2014 in which case the adversary may change only a $\\delta$ -fraction of the input to avoid getting caught. Corruptions can also capture outliers in the data, e.g., as a result of rare but arbitrary measurement errors. This was the motivation in Azar et al. (2022). As another example, the input data may have been collected at a certain point in time, while the ground truth continues to evolve \u2014 in the context of facility location, a population census may form the prediction, with errors occurring since a small fraction of the population has relocated since being surveyed.   \n\u2022 Experts advice. The data can also originate from expert advice, where experts are usually correct, but can be significantly inaccurate when they are wrong. ", "page_idx": 17}, {"type": "text", "text": "C Supporting Small $\\varepsilon>0$ Values ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Throughout the analysis we assume that $\\varepsilon=0$ , meaning each prediction is either wrong, or exactly correct. As we predict values in space (such as real values) it is hard to expect exact precision in the predictions. Thus, we can weaken this assumption by allowing a prediction to be \u201ccorrect\u201d if it is $\\varepsilon$ -accurate for some small $\\varepsilon>0$ value. Consider, for example, the scenario where we want to locate a hospital close to where people live where the locations are the house addresses. An \u201capproximately correct\u201d prediction that equals to the location of the house next door to the actual house of a person would be a reasonable prediction but would result in a small $\\varepsilon>0$ . The prediction (and the cost) is almost the same, but it is not exactly right. ", "page_idx": 17}, {"type": "text", "text": "Consider, for example, the case of the single facility location problem. The effect of $\\varepsilon$ on the approximation ratio is bounded by an additive term of at most $\\varepsilon n$ , simply because each of the $n$ points \u201ccontributes\u201d another additive $\\varepsilon$ term. If $\\varepsilon n$ is small in comparison to $O P T$ , or equivalently, if $\\varepsilon$ is small compared to $O P T/n$ then the effect on the approximation ratio will be small; also, it simply means that every agent pays at most $\\varepsilon$ more on average. On the other hand, if $\\varepsilon$ is big in comparison to ${}^{O P T}/n$ , then we have no chance to compute a good solution. Indeed, ${}^{O P T}/n$ is the average deviation of the points from their true geometric median, which can be viewed as a notion of the variance of the input; this is a measure of the \u201cscale\u201d of the cost of the problem, and a noise level above this value swamps the signal in the data. ", "page_idx": 17}, {"type": "text", "text": "We drop the $\\varepsilon$ from the calculations to avoid \u201cdragging\u201d the additive $\\varepsilon$ -dependent term along each result. ", "page_idx": 17}, {"type": "text", "text": "D Robust Statistics and Clustering Results Interpretation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Section 2.3 we\u2019ve mentioned how the $\\delta$ -robustness notion generalizes the breakdown point notion from robust statistics. The existing literature does have a quantitative approach, but mostly for specific distributions or for distributions under some constraints, and mostly for mean and covariance robust estimation. For example, Lai et al. (2016) shows th\u221aat the distance between the mean of a Gaussian distribution in $\\mathbb{R}^{d}$ from the geometric median is $\\Omega(\\delta\\sqrt{d})$ . In section Section 5 we give the definitions of $\\delta$ -robustness, to consider the robust estimation of general location estimators (such as the mean, median and their generalizations) for data chosen adversarially (thus ftiting any distribution, with no further assumption). Then, we use these to study the robustness of the median and its generalizations to adversarial corruptions. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "In Appendix E.1, we show that 1-Median (also known as the geometric median) has a distance robustness of $O_{\\delta}(1)\\cdot\\mathrm{MAD}(X)$ where $O_{\\delta}(1)$ is a small $\\delta$ -dependent constant, and $M A D$ stands for Mean Average Deviation, a known notion of variance defined as $\\mathrm{{MAD}}(X)\\ :=$ $\\begin{array}{r}{\\frac{1}{n}\\sum_{x_{j}\\in X}d(x_{i},\\underset{\\circ\\,\\dots}{\\mathrm{GEOMETRIC-MEDIAN}}(X))}\\end{array}$ . An interpretation of our result is that given a small $\\delta$ , the robustness of the geometric median depends on the MAD of the points, and the quantification of this relation is given by Theorem 2 and Corollary 3. When we calculate the (geometric) median we often want a single point representing the dataset. The geometric median represents the dataset in the sense that it is the point closest to the dataset points. Intuitively, for data with high MAD (or variance), there is no single point that represents it in a good way. Consider the examples in Fig. 3: ", "page_idx": 18}, {"type": "image", "img_path": "LPbqZszt8Y/tmp/13816ac4b1dfe7073919f1cf673b4b7d9744b6fc5e5f83cbc95e6aea20f567a0.jpg", "img_caption": ["Figure 3: Illustration of instances with different MAD; In the LHS example, MAD is very high unlike RHS where $\\mathrm{MAD}=0$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "On the left example (high MAD example) half the points at 0 and half the points at 1: no single point can represent the data in a good way \u2014 any point returned would misrepresent half the data. In the second example (low MAD example) all points are co-located at 0. In this case $\\mathrm{MAD}=0$ and so the geometric median (at 0) represents the dataset in the best possible way \u2014 it equals to every point in the dataset. So the interpretation of the results here is that the geometric median is robust in the sense that it still returns a good representation of the dataset: a point that gets a representation of the dataset, which is almost as good as the real \"best\" representation of the dataset. ", "page_idx": 18}, {"type": "text", "text": "In Appendix E.2, we show that for $k>1$ there is no hope for robustness of $k$ -medians. We thus turn to $\\beta$ -balanced $k$ -medians, which are $k$ medians that induce clusters of size at least $\\beta n$ , where $n$ is the total number of points (see Definition 7). Clustering with the additional constraint of minimum cluster size has applications such as data privacy (Aggarwal et al. (2010)), or clustering with a minimum/maximum cluster size (capacitated variation). The robustness of $\\beta$ -balanced $k$ -medians implies that balanced clustering algorithms can be robust to adversarial corruptions, outliers or missing values. ", "page_idx": 18}, {"type": "text", "text": "In our context, we show the usefulness of these $\\delta$ -robustness results for all of the mechanisms for the MAC predictions settings. ", "page_idx": 18}, {"type": "text", "text": "E $\\delta$ -Robustness of the Median and Its Generalizations: Proofs and More Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "First, we show a lemma connecting the distance robustness to the approximation robustness notions, in the context of $\\mathrm{k\\Omega}$ -Medians. ", "page_idx": 18}, {"type": "text", "text": "Lemma 10. Consider location estimators $f,{\\widehat{f}}:V^{n}\\to V^{k}$ that satisfy the following two properties: 1. For any $X\\in V^{n}$ and $X^{\\prime}$ in the $\\delta|X|$ -neighborhood of $X$ , $\\mathrm{med}_{k}(X^{\\prime},{\\widehat{f}}(X^{\\prime}))\\leq\\mathrm{med}_{k}(X^{\\prime},f(X));$ 2. $\\widehat{f}\\,i s\\,(\\rho,\\delta)$ -distance-robust with respect to $f$ . ", "page_idx": 18}, {"type": "text", "text": "Then $\\widehat{f}$ is a med2k\u03b4(|XX,|f\u03c1(X)), \u03b4 -approximation-robust solution for F = medk with respect to f. ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $n=|X|$ , let $A:=\\{i\\in[n]\\mid x_{i}=x_{i}^{\\prime}\\}$ be the indices where $X,X^{\\prime}$ are the same, and $B:=[n]\\setminus A$ be the remaining indices. Define $G:=f(X)$ and $H:={\\widehat{f}}(X^{\\prime})$ . Using this notation we can write ${\\mathrm{med}}_{k}(X,{\\widehat{f}}(X^{\\prime}))$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i}d(x_{i},H)=\\sum_{i}d(x_{i}^{\\prime},H)+\\sum_{i\\in B}(d(x_{i},H)-d(x_{i}^{\\prime},H))}\\\\ &{\\qquad\\qquad\\leq\\sum_{i}d(x_{i}^{\\prime},G)+\\displaystyle\\sum_{i\\in B}(d(x_{i},H)-d(x_{i}^{\\prime},H))}\\\\ &{\\displaystyle=\\sum_{i}d(x_{i},G)+\\sum_{i\\in B}(d(x_{i}^{\\prime},G)-d(x_{i},G))+\\sum_{i\\in B}(d(x_{i},H)-d(x_{i}^{\\prime},H))}\\\\ &{\\displaystyle=\\mathrm{med}_{k}(X,f(X))+\\sum_{i\\in B}(d(x_{i}^{\\prime},G)-d(x_{i}^{\\prime},H))+\\sum_{i\\in B}(d(x_{i},H)-d(x_{i},G)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (4) uses the first assumption of the theorem. ", "page_idx": 19}, {"type": "text", "text": "We claim that each of the differences in Eq. (5) is at most $d_{H}(G,H)$ . Indeed, for any $x\\in V$ , let its closest points in $G$ and $H$ be $g$ and $h$ respectively. Then $\\dot{d}(x,g)\\d j-d(x,h)\\,\\leq\\,d\\dot{(g,h)}$ by the triangle inequality, and this is at most the Hausdorff distance; the case of $d(x,h)-d(x,g)$ is identical. Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{med}_{k}(X,{\\widehat{f}}(X^{\\prime}))\\leq\\mathrm{med}_{k}(X,f(X))+2|B|\\cdot d_{H}(G,H).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, using the definition of $(\\rho,\\delta)$ -distance-robustness implies that $d_{H}(G,H)$ is at most $\\rho$ . The fact that $|B|\\leq\\delta|X|$ completes the proof. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Now, we move onto proving the robustness of 1-Median (which is GEOMETRIC-MEDIAN for a Eucledian space). ", "page_idx": 19}, {"type": "text", "text": "E.1 The $\\delta$ -Robustness of 1-Median ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we quantify the distance robustness (and hence approximation robustness, by Lemma 10) of the 1-median location estimator. We show that changing any $\\delta$ -fraction of a dataset $X$ where $\\delta<{^1\\!/2}$ can move the 1-Median by only $\\frac{2}{1-2\\delta}$ times the average cost of a point in the optimal solution of $X$ . Hence, this changes the total 1-Median cost by only a $1+O\\big(\\frac{\\delta}{1\\!-\\!2\\delta}\\big)$ factor. ", "page_idx": 19}, {"type": "text", "text": "This is captured by Theorem 2, Corollary 3. ", "page_idx": 19}, {"type": "text", "text": "E.1.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Let $X\\,=\\,\\{x_{1},\\ldots,x_{n}\\},X^{\\prime}\\,=\\,\\{x_{1}^{\\prime},\\ldots,x_{n}^{\\prime}\\}\\,\\in{\\cal V}^{n}$ where $X^{\\prime}$ is in the $\\delta|X|$ -neighborhood of $X$ . Let $n\\,=\\,|X|$ , let $A\\,:=\\,\\{i\\,\\in\\,[n]\\;\\;|\\;\\,x_{i}\\,=\\,x_{i}^{\\prime}\\}$ be the indices where $X,X^{\\prime}$ are the same, and $B:=\\;[n]\\setminus\\dot{A}$ be the remaining indices. Let $\\begin{array}{r}{\\overbar{m}\\;:=\\;\\arg\\operatorname*{min}_{v\\in V}\\mathrm{med}_{1}(X,\\{v\\})}\\end{array}$ and $m^{\\prime}:=$ ar $;\\mathrm{min}_{v\\in V}\\,\\mathrm{med}_{1}(X^{\\prime},\\{v\\})$ . It follows that $\\begin{array}{r}{\\sum_{i\\in[n]}d(x_{i}^{\\prime},m^{\\prime})\\leq\\sum_{i\\in[n]}d(x_{i}^{\\prime},\\dot{m})}\\end{array}$ . Using the triangle inequality and the fact that $x_{i}^{\\prime}=x_{i}$ for all $i\\in A$ , we get that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\sum_{i\\in A}(d(m^{\\prime},m)-d(m,x_{i}))+\\sum_{i\\in B}(d(x_{i}^{\\prime},m)-d(m^{\\prime},m))\\leq\\sum_{i\\in A}d(x_{i},m)+\\sum_{i\\in B}d(x_{i}^{\\prime},m)}\\\\ &{}&{\\implies d(m^{\\prime},m)\\cdot(|A|-|B|)\\leq2\\displaystyle\\sum_{i\\in A}d(x_{i},m)\\leq2\\displaystyle\\sum_{i\\in[n]}d(x_{i},m)=2\\bmod(X).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $|B|\\leq\\delta n$ and $|A|+|B|=n$ , we get $\\begin{array}{r}{d(m^{\\prime},m)\\leq\\frac{2}{(1-2\\delta)n}\\,\\mathrm{med}_{1}(X).}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "The quantity me|dX1(|X)is called the mean absolute deviation and can be viewed as a notion of variance. Hence, Theorem 2 essentially says that since $\\delta\\,<\\,{^1/2}$ , the 1-Median only changes by $O_{\\delta}(1)$ (a constant depending on $\\delta$ ) times the mean absolute deviation. ", "page_idx": 19}, {"type": "text", "text": "To get Corollary 3 we simply substituting the distance-robustness parameter of \u03c1 =1\u221222\u03b4me|dX1(|X into Lemma 10. ", "page_idx": 19}, {"type": "text", "text": "Corollary 3 says that if $X^{\\prime}$ is obtained by perturbing a $\\delta$ -fraction of the points in $X$ , then the geometric median of $X^{\\prime}$ is a $(1+\\frac{4\\delta}{1-2\\delta})$ -approximately optimal solution for the original point set $X$ (with respect to the 1-Median cost objective). This result can be viewed as a quantitative version of a classical result of Lopuhaa and Rousseeuw (1991), which shows that the breakdown point for the geometric median is $^1\\!/\\!2$ . This gives us an understanding not only of the fraction of perturbations required to \u201cbreak\u201d the estimator (i.e., make it arbitrarily far from the real location), but also of the deterioration in the estimation before breaking it. The quality of estimation as a function of $\\delta$ is captured by our result of $\\rho\\,=\\,{\\frac{2}{1\\!-\\!2\\delta}}\\,{\\frac{\\mathrm{med}_{1}(X)}{|X|}}$ , showing that a smaller $\\delta$ corresponds to better estimation. ", "page_idx": 20}, {"type": "text", "text": "We conclude the discussion of $\\delta$ -robustness of the 1-Median with two remarks: ", "page_idx": 20}, {"type": "text", "text": "1. Results similar to Theorem 2 hold even for the case where $X^{\\prime}$ is obtained by adding or removing points; in fact, the approximation factors can be improved slightly (the proof is almost the same).   \n2. The quantitative bounds are tight; consider the example where $X$ comprises of $(0.5\\!-\\!\\delta)n\\!+\\!1$ points at 0, and $(0.5+\\delta)n-1$ points at 1 and $X^{\\prime}$ is obtained from $X$ by moving $\\delta n$ points from 1 to 0. A calculation shows that in this case the approximation robustness of the geometric median is (1 +1\u22124\u03b42\u03b4 $\\textstyle(1+{\\frac{4\\delta}{1-2\\delta}}-\\Theta({\\frac{1}{n}}),\\delta)$ . ", "page_idx": 20}, {"type": "text", "text": "E.2 The $\\delta$ -Robustness of $\\beta$ -Balanced $k$ -Medians ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 4: ", "page_idx": 20}, {"type": "text", "text": "Proof. The claimed algorithm $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is simple: compute the optimal $(b-1)\\delta$ -balanced $k$ -medians solution on $X^{\\prime}$ . Since $X$ admits a $b\\delta$ -balanced solution $G=\\{g_{1},\\ldots,g_{k}\\}$ , and $X^{\\prime}$ differs from $X$ in only $\\delta|X|$ points, this solution $G$ is $(b-1)\\delta$ -balanced for $X^{\\prime}$ . Now let $H=\\{h_{1},\\ldots,h_{k}\\}$ be a $(b-1)\\delta$ -balanced $k$ -medians solution of least cost for $X^{\\prime}$ computed by $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ : this means ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{x^{\\prime}\\in X^{\\prime}}d(x^{\\prime},H)\\leq\\sum_{x^{\\prime}\\in X^{\\prime}}d(x^{\\prime},G).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\mathcal{C}_{H}^{\\prime}=\\{C_{1}^{\\prime},\\ldots,C_{k}^{\\prime}\\}$ be the balanced partition of $X^{\\prime}$ induced by $H$ . Similarly, since $X$ is in the $\\delta$ -neighborhood of $X^{\\prime}$ , the centers in $H$ induce a $(b-2)\\delta$ -balanced partition of the original dataset $X$ ; call this $\\mathcal{C}_{H}=\\{C_{1},\\ldots,C_{k}\\}$ . Finally, let $\\mathcal{C}_{G}=\\{C_{1}^{*},\\ldots,C_{k}^{*}\\}$ be the $b\\delta$ -balanced partition of $X$ induced by $G$ . Recall that $n=|X|$ ; define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{O P T:=\\mathrm{med}_{k}(X,G)=\\displaystyle\\sum_{x\\in X}d(x,G)=\\sum_{i=1}^{k}\\sum_{x\\in C_{i}^{*}}d(x,g_{i})\\quad\\mathrm{and}}}\\\\ {{A L G:=\\mathrm{med}_{k}(X,H)=\\displaystyle\\sum_{x\\in X}d(x,H)=\\sum_{j=1}^{k}\\sum_{x\\in C_{j}}d(x,h_{j}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now want to prove that $\\operatorname*{max}_{i}d(g_{i},H)$ and $\\operatorname*{max}_{j}d(h_{j},G)$ are both small. To show the former, fix any $i$ and let $C_{i}^{*}$ be the cluster corresponding to $g_{i}$ . Since the clustering is $b\\delta$ -balanced, $|C_{i}^{*}|\\geq b\\delta n$ . By averaging, there exists some cluster $C_{j}\\in\\mathcal{C}_{H}$ such that $|C_{i}^{*}\\cap C_{j}|\\geq b\\delta n/k$ ; choose this $j$ , and consider the corresponding center $h_{j}\\in H$ . Now ", "page_idx": 20}, {"type": "equation", "text": "$$\nd(g_{i},H)\\leq d(g_{i},h_{j})\\stackrel{(\\star)}{\\leq}\\frac{1}{|C_{i}^{*}\\cap C_{j}|}\\sum_{x\\in C_{i}^{*}\\cap C_{j}}\\left[d(x,g_{i})+d(x,h_{j})\\right]\\leq\\frac{A L G+O P T}{b\\delta n/k},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(\\star)$ uses the triangle inequality. A similar argument shows: ", "page_idx": 20}, {"type": "equation", "text": "$$\nd(h_{j},G)\\leq\\frac{A L G+O P T}{(b-2)\\delta n/k}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence the Hausdorff distance between the two solutions is $\\begin{array}{r}{d_{H}(G,H)\\ \\leq\\ \\frac{A L G+O P T}{(b-2)\\delta n/k}}\\end{array}$ Using Lemma 10 with this bound on the Hausdorff distance, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\nA L G\\leq O P T+2\\delta n\\cdot d_{H}(G,H)\\leq O P T+2k\\cdot{\\frac{A L G+O P T}{(b-2)}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "F Proofs and Additional Analysis for Section 6.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "F.1 Proof of Theorem 5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. By Corollary 3, computing the 1-Median on $X^{\\prime}$ results in a $1+{\\frac{4\\delta}{1-2\\delta}}$ -approximation ratio: this is because the metric space is $\\mathbb{R}^{d}$ , and hence the 1-Median is in fact the geometric median (which is also known as the spatial median, $L_{1}$ median or Fermat point). Since the geometric median computation depends solely on the predictions it is strategyproof. Thus, by the the strategyproofness of COORDINATE-WISE-MEDIAN, and by the fact that the decision of which mechanism to use does not depend on the reported locations, Algorithm 1 is strategyproof. Due to the $\\sqrt{d}$ approximation ratio of COORDINATE-WISE-MEDIAN (Meir (2019)), we get a $\\begin{array}{r l}{\\operatorname*{min}\\biggr(1+\\frac{4\\delta}{1-2\\delta},\\sqrt{d}\\biggr)}\\end{array}$ approximation ratio for Algorithm 1. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "F.2 Remark on Computing the Geometric Median ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For general dimensions $d$ and number of points $n$ , there is no known formula or algorithm to find the geometric median exactly. However, since the problem is a convex optimization problem, there are optimization algorithms that find solutions with arbitrary precision efficiently. By using such an optimization algorithm we get an additional factor of $\\varepsilon^{\\prime}>0$ where $\\varepsilon^{\\prime}$ is arbitrarily small. For more information on efficiently computing the geometric median, see Eckhardt (1980), Beck and Sabach (2015) Cohen et al. (2016). ", "page_idx": 21}, {"type": "text", "text": "F.3 Combined Strategy for High Probability $\\mathbf{MAC}(\\varepsilon,\\delta)$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section we show how we can use both the predictions and the reports of the agents to gain good performance even in the case of $\\mathbf{MAC}(\\varepsilon,\\delta)$ only in high probability. ", "page_idx": 21}, {"type": "text", "text": "The $\\mathbf{MAC}(\\varepsilon,\\delta)$ model requires that at most a $\\delta$ fraction of the points have error more than $\\varepsilon$ ; one can extend this to the setting where this requirement holds only with high probability. For instance, consider the setting where the number of prediction errors can be larger than a $\\delta$ fraction with probability at most $\\bar{o}(^{1}\\!/n)$ . In this case the computed geometric median might be arbitrarily far away with this small probability, and hence the expected approximation ratio for our mechanism would be unbounded. To avoid this problem, we can use the agent reports. Intuitively the agents have the incentive to make sure that the returned facility location is not infinitely far away from them. ", "page_idx": 21}, {"type": "text", "text": "One way to do this is by using the MIN-BOUNDING-BOX mechanism, introduced by (Agrawal et al., 2022, Mechanism 2) for the egalitarian cost function for $d=2$ ; The mechanism naturally extends to any $d\\geq1$ (Algorithm 6). The mechanism calculates the minimum bounding box $B$ containing all the input points; then, given a single prediction $o$ , it returns the point ${\\hat{O}}\\in B$ closest to $o$ . We show in Appendix F.4 that the approximation ratio of the generalized MIN-BOUNDING-BOX mechanism for the utilitarian cost function ${\\mathrm{(med}}_{1})$ is $O(n)$ . ", "page_idx": 21}, {"type": "text", "text": "Therefore, by composing MIN-BOUNDING-BOX with Algorithm 1 we get a strategyproof mechanism which has an approximation ratio of $\\begin{array}{r}{m i n\\Big(1+\\frac{4\\delta}{1-2\\delta},\\sqrt{d}\\Big)}\\end{array}$ with high probability, and has an $O(n)$ approximation ratio with probability $O\\big(1/n\\big)$ ; this gives an expected approximation ratio of at most $\\begin{array}{r}{m\\dot{i}n\\Big(1+\\frac{4\\delta}{1-2\\delta},\\sqrt{d}\\Big)+o\\dot{(1)}}\\end{array}$ . The form of the resulting mechanism is simple: ", "page_idx": 21}, {"type": "table", "img_path": "LPbqZszt8Y/tmp/eed0a5ee4835ecc0561329bdda3e44a53f19f0cfa30fa660a00a6af70dabc0a7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "F.4 Generalized Minimum Bounding Box Mechanism ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we properly define the generalized MIN-BOUNDING-BOX mechanism and prove that for points in $\\mathbb{R}^{d}$ (for any $d\\geq1$ ) it has a tight approximation ratio of $O(n)$ for the $m e d_{1}$ cost function. ", "page_idx": 22}, {"type": "text", "text": "For any $\\boldsymbol{y}\\in\\mathbb{R}^{d}$ let $[y]_{j}$ denote the $j^{:}$ \u2019th coordinate of $y$ . ", "page_idx": 22}, {"type": "table", "img_path": "LPbqZszt8Y/tmp/017584b76d046865bef9a7c365014cca6f7b99f856b7154cf5360ede441c74f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Algorithm 6 simply computes, for each coordinate $j$ , the closest point to $o_{j}$ that is inside the minimum closed interval that contains all of the $j'$ th coordinates of the points of $X$ (which is what MINMAXP does, see Mechanism 1 of Agrawal et al. (2022)). Let us now prove an approximation ratio for the $\\mathrm{med_{1}}$ cost. ", "page_idx": 22}, {"type": "text", "text": "Theorem 11. Algorithm $^{\\sc6}$ is strategyproof and has a tight $O(n)$ approximation ratio for the $\\mathrm{med_{1}}$ cost function. ", "page_idx": 22}, {"type": "text", "text": "Proof. Let us assume, w.l.o.g, that the geometric median $g$ is at $g\\,=\\,0$ . Let $B$ be the minimum bounding box of $X$ . Formally: ", "page_idx": 22}, {"type": "text", "text": "$\\begin{array}{r}{B\\,=\\,\\left\\{y\\in\\mathbb{R}^{d}\\mid\\forall j\\in[d]:y_{j}\\in\\left[\\operatorname*{min}_{i\\in[n]}\\left[x_{i}\\right]_{j},\\operatorname*{max}_{i\\in[n]}\\left[x_{i}\\right]_{j}\\right]\\right\\}}\\end{array}$ . Let $h$ be the point returned by Algorithm 6 for $X\\subset\\mathbb{R}^{d}$ , $o\\in\\mathbb{R}^{d}$ (thus $h$ must be inside of $B$ ). Let $a_{j}$ be the side length of the box $B$ in each coordinate. As usual, we denote $O P T$ to be the cost of the optimal solution and $A L G$ the cost of the algorithm. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle{O P T^{2}=\\left(\\sum_{i\\in[n]}\\|x_{i}\\|\\right)^{2}\\geq\\sum_{i\\in[n]}\\sum_{j\\in d}([x_{i}]_{j})^{2}=\\sum_{j\\in[d]}\\left(\\sum_{i\\in[n]}([x_{i}]_{j})^{2}\\right)\\overset{(\\star)}{\\geq}\\sum_{j\\in[d]}\\frac{a_{j}^{2}}{4}\\implies}}}\\\\ {{\\displaystyle{O P T\\geq\\frac{1}{2}\\sqrt{\\sum_{j}a_{j}^{2}}\\stackrel{(\\star\\star)}{\\geq}\\frac{1}{2}\\sqrt{\\sum_{j}h_{j}^{2}}=\\frac{1}{2}\\|h\\|.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$(\\star)$ is due to the fact that for any $j\\in[d]$ there is some $x_{i}$ s.t. $\\begin{array}{r}{|[x_{i}]_{j}|\\geq\\frac{a_{j}}{2}}\\end{array}$ since otherwise we would get that the $j^{:}$ \u2019th side length is smaller than $a_{j}$ . ", "page_idx": 22}, {"type": "text", "text": "$({\\bf\\star}{\\bf\\star})$ explanation: It is a known property of the geometric median that it lies inside the convex hull of the points. Thus, $g$ lies inside $B$ (since the convex hull of the points lies inside the bounding box of the points). So by the fact that $h\\in B$ we get that $|h_{j}|=|h_{j}-0|=|h_{j}-g_{j}|\\leq|a_{j}|$ . ", "page_idx": 22}, {"type": "text", "text": "Finally: ", "page_idx": 22}, {"type": "equation", "text": "$$\nA L G=\\sum_{i\\in[n]}\\lVert x_{i}-h\\rVert\\stackrel{t r i a n g l e\\,i n e q u a l i t y}{\\leq}\\sum_{i\\in[n]}\\lVert x_{i}\\rVert+n\\lVert h\\rVert\\stackrel{E q.\\,(6)}{\\leq}O P T+2n\\,O P T=O(n)\\,O P T.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We deduce that Algorithm 6 has an approximation ratio of at most $O(n)$ ", "page_idx": 22}, {"type": "text", "text": "To see that the result is tight, consider the instance where $X$ is the multi-set of $n$ points where $n-1$ points are at $a\\,=\\,(-1,\\bar{\\,\\cdot\\,}\\,.\\,.\\,,-1)\\,\\in\\,\\mathbb{R}^{d}$ and one point is at $b=(1,\\dots,1)\\in\\mathbb{R}^{\\bar{d}}$ . Let $o=b$ . The optimal solution puts the facility at $a$ and has a cost of $O P T=d(a,b)=2{\\sqrt{d}}$ . Alg\u221aorithm 6 returns $b$ and therefore the cost of the algorithm is $A L G=(n-1)\\cdot d(a,b)=(n-1)\\cdot2{\\sqrt{d}}.$ . ", "page_idx": 22}, {"type": "text", "text": "The strategyproofness of the mechanism is similar to the one proof given by Agrawal et al. (2022) for the egalitarian cost function. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "One could wonder why use the minimum bounding box rather than the convex hull of the points. After all, the convex hull is always contained in the minimum bounding box, and always contains the geometric median. Unfortunately, such a mechanism is not strategyproof: ", "page_idx": 23}, {"type": "text", "text": "Remark 12. The mechanism obtained by replacing the minimum bounding box in Algorithm $^{\\sc6}$ with the convex hull is not a strategyproof. The example showing this is for $d=2$ : Given $X=$ $\\{(-0.5,0),(0.5,0),(0,1)\\}$ , a computation shows that the agent at $(0,1)$ can lower its cost by reporting $(1/2,1)$ . ", "page_idx": 23}, {"type": "text", "text": "We can further generalize the mechanism for $k$ facilities by simply using Algorithm 6 for each facility independently. ", "page_idx": 23}, {"type": "text", "text": "G A Deterministic Mechanism for Balanced $k$ -Facility Location in General Metric Spaces ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For $\\beta\\in[0,1]$ , the $\\beta$ -balanced $k$ -facility location problem considers $n$ agents with locations $X=$ $\\{x_{1},\\ldots,x_{n}\\}\\,\\subseteq\\,V$ . Each agent reports a location to the mechanism, and the goal is to return a multi-set $H=(h_{1},\\cdot\\cdot\\cdot,h_{k})$ containing $k$ points from $V$ that minimizes ${\\mathrm{med}}_{k}(X,H)$ , such that the clustering induced by $H$ is $\\beta$ -balanced (according to Definition 7). ", "page_idx": 23}, {"type": "text", "text": "For a small enough $\\delta$ , by choosing $b:=\\beta/\\delta$ the balancedness condition translates into a minimum cluster size of $b\\delta$ . Hence, we can utilize the $\\delta$ -robustness results of $(b-1)\\delta$ -BALANCED $k$ -MEDIAN (Theorem 4) to immediately obtain the deterministic mechanism in Algorithm 7 with the following guarantee: ", "page_idx": 23}, {"type": "text", "text": "Theorem 13. Algorithm 7 is a deterministic strategyproof mechanism with a constant ( $k$ -dependent) approximation ratio of at most ", "page_idx": 23}, {"type": "equation", "text": "$$\n1+{\\frac{4k}{b-2-2k}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "table", "img_path": "LPbqZszt8Y/tmp/c05a48fd8b7130d59747695ccf16b75e7614b766fbe95dc222b017da64d90bd8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "We do not specify how $(b-1)\\delta$ -BALANCED $k$ -MEDIAN is implemented. If all points lie on the real line, then there exists an $O(n^{k})$ -time algorithm for it (computing the minimum cost of all $\\binom{n}{k}$ options). ", "page_idx": 23}, {"type": "text", "text": "In Euclidean spaces of dimensions greater than one, the $k$ -medians problem (and its balanced or lower bounded variant) is classified as NP-hard Megiddo and Supowit (1984); Bhattacharya et al. (2020) necessitating the use of approximation algorithms. Any approximation algorithm with approximation ratio $c$ can be applied, incurring an additional multiplicative cost factor of $c$ . This approach allows for practical solutions within the constraints of computational complexity, ensuring that we can still achieve near-optimal placements of facilities even in high-dimensional contexts. The effect of $\\varepsilon$ values and handling the case of high probability MAC predictions is similar to the single facility location results. ", "page_idx": 23}, {"type": "text", "text": "H Full Proof of BIG-CLUSTER-CENTER $\\delta$ -Approximation Robustness ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section we show that Algorithm 3 indeed has \"good\" approximation-robustness for unbalanced clusters by providing the full proof for Theorem 8. ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $G=(g_{L},g_{R})$ be the 2 \u2212median solution, where $L=\\{i\\mid d(x_{i},g_{L})\\leq d(x_{i},g_{R})\\}$ and $R=[n]\\setminus L$ . W.l.o.g $g_{L}\\leq g_{R}$ . ", "page_idx": 23}, {"type": "text", "text": "We have a slight abuse of notation throughout for the sake of not introducing more variables, referring to $L,R$ as multisets of points and sometimes as the sets of indices of the points in $X$ . ", "page_idx": 23}, {"type": "text", "text": "We assume that the clusters are $b\\delta$ -unbalanced. That is, at least one of the clusters $L,R$ are of size less than $b\\delta n$ . W.l.o.g $|R|<b\\delta n$ and so $|L|\\geq(1-b\\delta)n$ . ", "page_idx": 23}, {"type": "text", "text": "We also assume that $\\delta$ is small enough s.t. $b\\delta<\\textstyle{\\frac{1}{4}}$ and so $|L|>\\frac{3n}{4}$ ", "page_idx": 24}, {"type": "text", "text": "Let $H\\,=\\,(h_{L},h_{R})\\,=\\,(b\\,-\\,1)\\delta$ -Balanced-2-Medians $(X^{\\prime})$ (as in the first line of Algorithm 3).   \nW.l.o.g $h_{L}\\leq h_{R}$ . ", "page_idx": 24}, {"type": "text", "text": "Let $L^{\\prime},R^{\\prime}$ be the multi-set of the elements closest to $h_{L}$ and $h_{R}$ the remaining. So $L^{\\prime}\\,=\\,\\{i\\,\\mid$ $d(x_{i}^{\\prime},h_{L})\\leq d(x_{i}^{\\prime},h_{R})\\}$ and $R^{\\prime}=[n]\\setminus L^{\\prime}$ . ", "page_idx": 24}, {"type": "text", "text": "Let $A:=\\{i\\in[n]\\mid x_{i}=x_{i}^{\\prime}\\}$ the shared points between $X,X^{\\prime}$ (the \"correct\" predictions), and $B:=[n]\\setminus A$ the remaining. ", "page_idx": 24}, {"type": "text", "text": "Let $\\begin{array}{r}{m=\\frac{g_{L}+g_{R}}{2}}\\end{array}$ be the middle point between $g_{L}$ and $g_{R}$ . Let $\\begin{array}{r}{m^{\\prime}=\\frac{h_{L}+h_{R}}{2}}\\end{array}$ be the middle point between $h_{L}$ and $h_{R}$ . By the definition of $m$ : All points of $L$ lie to the left of $m$ and all points of $R$ lie to the right of $m$ . Similarly, all points of $L^{\\prime}$ lie to the left of $m^{\\prime}$ and all points of $R^{\\prime}$ lie to the right of $m^{\\prime}$ . ", "page_idx": 24}, {"type": "text", "text": "We begin by proving the following simple lemma: ", "page_idx": 24}, {"type": "text", "text": "Lemma 14. Let $S\\subseteq\\mathbb{R}^{l}$ (for some $l\\in\\mathbb{N},$ ) be a multi-set of n points. For any $\\beta\\in[0,1]$ , $S\\subseteq V$ , let $M=(m_{1},\\ldots,m_{k}):=\\beta-b a l a n c e d-k-m e d i a n(S),$ , s.t. $S_{1},\\ldots,S_{k}$ are the disjoint clusters induced by $m_{1},\\ldots,m_{k}$ of $S$ . Then for all $i\\in[k]$ : $m_{i}$ is the 1 \u2212median of $S_{i}$ ", "page_idx": 24}, {"type": "text", "text": "Proof. Consider the implementation of $\\beta-b a l a n c e d-k-m e d i a r$ which is obtained by taking the cost-minimizing balanced partition into $k$ \u03b2 \u2212balanced clusters. That is, the implementation considers all of the partitions of $S$ into $k$ \u03b2 \u2212balanced clusters and then computes the location that minimizes the center of each cluster $S_{i}$ . For each such center $S_{i}$ , the minimizing location is (by definition) $1-m e d i a n(S_{i})$ . Thus, the implementation always returns points that are the 1 \u2212median of their clusters. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "We divide the proof into 4 cases as follows (also illustrated in Fig. 4): ", "page_idx": 24}, {"type": "text", "text": "Case $1:h_{L}\\leq g_{L}$ and $h_{R}\\geq g_{R}$ .  \nCase $2:h_{L}\\geq g_{L}$ and $h_{R}\\leq g_{R}$ .  \nCase $3:h_{L}\\ge g_{L}$ and $h_{R}\\geq g_{R}$ .  \nCase $4:h_{L}\\leq g_{L}$ and $h_{R}\\leq g_{R}$ .", "page_idx": 24}, {"type": "text", "text": "Cases 1 to 3 are similar and have short proofs. The hard case is Case (4). ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Predictions~}\\mathbf{X}^{\\mathrm{p}}}\\\\ &{\\mathrm{~-~---~---~}-\\underbrace{-\\frac{h_{L}^{\\mathbf{e}}}{2}-\\mathbf{------~}-\\mathbf{\\ensuremath{\\frac{h_{R}^{\\mathbf{e}}}{\\hbar}}---~}}_{\\mathbf{Q}_{L}^{\\mathbf{e}}}\\;\\;\\Bigg|\\begin{array}{l}{\\mathrm{Predictions~}\\mathbf{X}^{\\mathrm{,~\\,~}}}\\\\ {\\mathrm{~---~---~}-\\underbrace{-\\mathbf{\\mu}_{L}^{h_{L}^{\\mathbf{e}}}------~}_{\\mathbf{Q}_{L}^{\\mathbf{e}}}------}\\\\ {\\mathrm{Real~}\\mathbf{X}}\\end{array}}_{\\mathbf{Case\\,}\\mathbf{\\ensuremath{\\frac{g}{\\hbar}}}}-\\underbrace{-h_{L}^{\\mathbf{e}}}_{\\mathbf{Case\\,}\\mathbf{\\ensuremath{\\frac{g}{\\hbar}}}}-\\underbrace{-h_{R}^{\\mathbf{e}}}_{\\mathbf{Case\\,}\\mathbf{\\ensuremath{\\frac{g}{\\hbar}}}}}\\\\ &{\\mathrm{Predictions~}\\mathbf{X}^{\\mathrm{,~\\,~}}}\\\\ &{\\mathrm{~-~---~}-\\underbrace{-\\mathbf{\\mu}_{L}^{h_{L}^{\\mathbf{e}}}-\\mathbf{\\mu}_{R}^{-}---~}_{\\mathbf{\\mu}_{L}^{g}}}\\\\ &{\\mathrm{Real~}\\mathbf{X}}\\end{array}\\Bigg|\\begin{array}{l}{\\mathrm{Predictions~}\\mathbf{X}^{\\mathrm{,~\\,~}}}\\\\ {\\mathrm{Real~}\\mathbf{X}}\\end{array}\\Bigg|\\begin{array}{l}{\\mathrm{Predictions~}\\mathbf{X}^{\\mathrm{,~\\,~}}}\\\\ {\\mathrm{Real~}\\mathbf{X}}\\end{array}\\Bigg|\\begin{array}{l}{\\mathrm{h}_{L}^{\\mathbf{e}}}\\\\ {\\mathrm{~---~}-\\mathbf{\\mu}_{L}^{-}-\\mathbf{\\mu}_{R}^{h_{L}^{\\mathbf{e}}---~}}\\\\ {\\mathrm{Real~}\\mathbf{X}}\\end{array}-\\underbrace{-h_{R}^{\\mathbf{e}}}_{\\mathbf{Q}_{L}^{\\mathbf{e}}}-\\mathbf{\\mu}_{R}^{-}}\\\\ &{\\mathrm{Case\\,}\\mathbf{\\ensuremath{\\frac{g}{\\hbar}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Figure 4: Illustration of the 4 cases. On the top of each case drawing we have the \"estimated\"/\"predicted\" locations $H$ , computed on $X^{\\prime}$ . On the bottom we have the \"real\" locations, $G$ , computed on $X$ . ", "page_idx": 24}, {"type": "text", "text": "Case $1\\colon h_{L}\\leq g_{L}$ and $h_{R}\\geq g_{R}$ . In this case we can image the two centers moving away from one another. That is, $g_{L}$ \"moves to the left\" to $h_{L}$ , and $g_{R}$ \"moves to the right\" to $h_{R}$ . ", "page_idx": 24}, {"type": "text", "text": "If $m^{\\prime}\\geq m$ then $L^{\\prime}$ contains all of the points in $L\\cap A$ . and $\\vert L\\cap A\\vert\\geq\\vert L\\vert-\\delta n\\geq n-(b+1)\\delta n$ . Thus, for a small enough $\\delta\\colon n-(b+1)\\delta n\\geq{\\frac{n}{2}}$ and thus the algorithm returns $h_{L}$ . From Corollary 3 we get an approximation-robustness of $1+O(b\\delta)=1+O(\\delta)$ (since $L^{\\prime}$ is obtained from $L$ by change or add of at most $(b+1)\\delta$ elements). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Otherwise, $m^{\\prime}\\leq m$ . The number of elements to the right of $g_{R}$ in $R$ is $\\frac{|R|}{2}$ since it is the median of $R$ (by Lemma 14). ", "page_idx": 25}, {"type": "text", "text": "Since $h_{R}\\geq g_{R}$ then the number of elements to the right of $h_{R}$ in $R^{\\prime}$ is at most $\\begin{array}{r}{\\frac{|R|}{2}+|B|\\leq\\frac{|R|}{2}+\\delta n}\\end{array}$ . $h_{R}$ is the median of $R^{\\prime}$ (by Lemma 14) and thus $\\begin{array}{r}{|R^{\\prime}|\\leq2(\\frac{|R|}{2}+\\delta n)=|R|+2\\delta n\\leq(b+2)\\delta n}\\end{array}$ . Thus again by Corollary 3 we get an approximation-robustness of $1+O(\\delta)$ . ", "page_idx": 25}, {"type": "text", "text": "Case 2: $h_{L}\\,\\geq\\,g_{L}$ and $h_{R}\\leq g_{R}$ . In this case we can image the two centers moving towards one another. That is, $g_{L}$ \"moves to the right\" to $h_{L}$ , and $g_{R}$ \"moves to the left\" to $h_{R}$ . ", "page_idx": 25}, {"type": "text", "text": "If $m^{\\prime}\\geq m$ then $L^{\\prime}$ contains all of $L\\cap A$ plus at most $(b+1)\\delta n$ points and thus just like the previous case we get (from the 1-median approximation robustness) a approximation-robustness of $1+O(\\delta)$ . Otherwise $m^{\\prime}\\,<\\,m$ . By Lemma 14 we know that $h_{L}$ is the median of $L^{\\prime}$ and that $g_{L}$ is the median of $L$ . Since $h_{L}\\;\\geq\\;g_{L}$ , there are at least $\\frac{\\d|L|}{2}\\,-\\,\\delta n$ points in $L^{\\prime}$ to the left of $h_{L}$ . Thus: $\\begin{array}{r}{|L^{\\prime}|\\geq2(\\frac{|L|}{2}-\\delta n)\\geq|L|-2\\delta n\\geq(1-(b+2)\\delta)n}\\end{array}$ , and we can utilize Corollary 3 again to get the desired. ", "page_idx": 25}, {"type": "text", "text": "Case 3 $:h_{L}\\ge g_{L}$ and $h_{R}\\geq g_{R}$ . In this case we can image both centers \"moving to the right\". ", "page_idx": 25}, {"type": "text", "text": "In this case, $\\begin{array}{r}{m^{\\prime}=\\frac{h_{L}+h_{R}}{2}\\geq\\frac{g_{L}+g_{R}}{2}=m}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "Just like in the previous case: $L^{\\prime}$ contains all of $L\\cap A$ plus at most $(b+1)\\delta n$ points and thus (from 1-median approximation robustness) a approximation-robustness of $1+O(\\delta)$ . ", "page_idx": 25}, {"type": "text", "text": "Case 4: $h_{L}\\leq g_{L}$ and $h_{R}\\leq g_{R}$ . In this case we can image both centers \"moving to the left\". ", "page_idx": 25}, {"type": "text", "text": "By the definition of m\u2032: m\u2032 = hL+2hR \u2264 $\\begin{array}{r}{m^{\\prime}\\colon m^{\\prime}=\\frac{h_{L}+h_{R}}{2}\\leq\\frac{g_{L}+g_{R}}{2}=m.}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Until now we had an approximation-robustness result of $1+O(\\delta)$ . This case is the most difficult case, as for this one we will get a $1.8+O(\\delta)$ approximation result. Since we also have a lower bound of $\\approx1.667+\\Omega(\\delta)$ we get that this is indeed \"strictly\" the hard case. ", "page_idx": 25}, {"type": "text", "text": "If $h_{R}\\geq m$ then the number of elements in $X$ to its right is at most $|R|$ , and thus the number of elements to its right on $X^{\\prime}$ is at most $|R|+\\delta n$ . Since $h_{R}$ median of $R^{\\prime}$ it must be the case that $|R^{\\prime}|\\leq2(|R|+\\delta\\bar{n})\\leq2(b+1)\\delta n=\\dot{O}(\\delta)\\;n.$ . Thus $|L^{\\prime}|=n-|R^{\\prime}|\\geq(1-O(\\delta))n$ and therefore: (a) The algorithm returns $h_{L}$ (since $\\left|L^{\\prime}\\right|\\geq{\\frac{n}{2}}$ ) and (b) From the 1 \u2212median robustness (Corollary 3) we get the required $1+O(\\delta)$ approximation-robustness. ", "page_idx": 25}, {"type": "text", "text": "Thus, let us assume that $h_{R}<m$ . ", "page_idx": 25}, {"type": "text", "text": "We first handle the case where $m^{\\prime}\\geq g_{L}$ , and then we will show a reduction from the case $m^{\\prime}>g_{L}$ to this one. ", "page_idx": 25}, {"type": "text", "text": "H.1 Sub-case: $m^{\\prime}\\geq g_{L}$ ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Since $m^{\\prime}$ is the middle point between $h_{L}$ and $h_{R}$ we get that: $m^{\\prime}-h_{L}=h_{R}-m^{\\prime}$ but $m^{\\prime}-h_{L}=$ $m^{\\prime}-g_{L}+g_{L}-h_{L}\\geq g_{L}-h_{L}$ and so: $g_{L}-h_{L}\\leq h_{R}-m^{\\prime}$ . But $h_{R}-m^{\\prime}\\leq h_{R}-g_{L}$ and therefore: ", "page_idx": 25}, {"type": "equation", "text": "$$\ng_{L}-h_{L}\\leq h_{R}-g_{L}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We introduce the following notations (also see Fig. 5): First, we denote the left and right parts of $L^{\\prime},R^{\\prime},L,R$ : ", "page_idx": 25}, {"type": "text", "text": "\u2022 Le $\\mathsf{t}\\,L^{\\prime}\\phantom{\\dagger}_{l}=\\big\\{i\\in L^{\\prime}\\;|\\;x_{i}^{\\prime}\\le h_{L}\\big\\},L^{\\prime}\\phantom{\\dagger}_{r}=L^{\\prime}\\setminus L^{\\prime}_{l}.$ \u2022 Let $R^{\\prime}\\mathnormal_{l}=\\{i\\in R^{\\prime}\\ |\\ x_{i}^{\\prime}\\leq h_{R}\\},R^{\\prime}\\mathnormal_{r}=R^{\\prime}\\setminus R^{\\prime}\\mathnormal_{l}.$ \u2022 Let $L_{l}=\\{i\\in L\\mid x_{i}\\leq g_{L}\\},L_{r}=L\\mid L_{l}.$ \u2022 Let $R_{l}=\\{i\\in R\\mid x_{i}\\leq g_{R}\\},R_{r}=R\\setminus R_{l}.$ . ", "page_idx": 25}, {"type": "text", "text": "Next, we denote the partition of $L$ into 4 disjoint multi-sets $S,T,U,V$ by the 3 points $h_{L},m^{\\prime},h_{R}$ . So $S=\\{i\\in L\\mid x_{i}\\stackrel{.}{\\leq}h_{L}\\}.$ , $T=\\{i\\in L\\mid i\\not\\in S,x_{i}\\leq m^{\\prime}\\}$ , $U=\\{i\\in L\\mid i\\notin S\\cup T,x_{i}\\le h_{R}\\}$ , $V=\\{i\\in L\\mid i\\not\\in S\\cup T\\cup U\\}$ . ", "page_idx": 26}, {"type": "text", "text": "Let $\\begin{array}{r}{m^{\\prime\\prime}=\\,\\frac{g_{L}+h_{R}}{2}}\\end{array}$ be the middle point between $g_{L}$ and $h_{R}$ . W.l.o.g we assume that $m^{\\prime\\prime}\\,>\\,m^{\\prime}$ (otherwise the proof is similar). We define $U_{l},U_{r}$ to be the as follows: $U_{l}=\\{i\\in U\\mid x_{i}\\leq m^{\\prime\\prime}\\}$ , $U_{r}=U\\setminus U_{l}$ . ", "page_idx": 26}, {"type": "text", "text": "Finally, we introduce the notion of $\\alpha$ -approximately-equal: ${\\approx}^{\\alpha}$ : ", "page_idx": 26}, {"type": "text", "text": "Definition 14. For any $a,b,\\alpha\\in\\mathbb{R}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\na\\approx^{\\alpha}b\\iff b-\\alpha\\leq a\\leq b+\\alpha\n$$", "text_format": "latex", "page_idx": 26}, {"type": "image", "img_path": "LPbqZszt8Y/tmp/638b2ed74f2bfdb13660b430b12c55e3b454fb27cb7d26ace30ef06453ca421a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 5: Illustration of case (4) where $m^{\\prime}\\geq g_{L}$ . On the top we have $h_{L},h_{R}$ , computed on the \"predicted\" locations $X^{\\prime}$ . On the bottom we have the $g_{L},g_{R}$ , the 2-Medians of the \"real\" locations $X$ . $L^{\\prime}$ and $R^{\\prime}$ are the disjoint partitions of $X^{\\prime}$ into two multi-sets of points: those closer to $h_{L}$ and those closer to $h_{R}$ (respectively). Similarly $L$ and $R$ are the disjoint multi-sets of $g_{L},g_{R}.\\ L^{\\prime}\\imath,L^{\\prime}_{r}$ are the disjoint partitions of $L^{\\prime}$ into two multi-sets: all points to the left of $h_{L}$ and all points to the right of $h_{R}$ . In a similar manner $R^{\\prime},L,R$ are partitioned into their left and right parts $(R^{\\prime}\\iota,R^{\\prime}r,L\\iota,L_{r},R\\iota,R_{r})$ . We can also see $S,T,U,V$ which is the disjoint partition of $L$ determined by the points $h_{L},m^{\\prime},h_{R}$ . Finally, we have $U_{l}$ and $U_{r}$ which are the left and right parts of $U$ . ", "page_idx": 26}, {"type": "text", "text": "For every $M\\subseteq X$ we define $O P T_{M}$ to be the cost that the optimal solution $(G)$ pays for the multi-set of points $M$ . That is: $O P T_{M}:=\\mathrm{med}_{2}(M,G)$ (we sometimes use this notation where $M\\subseteq[n]$ in which case $O P T_{M}$ is a slight abuse of notation for $O P T_{\\{x_{i}|i\\in M\\}}$ ). ", "page_idx": 27}, {"type": "text", "text": "Let $\\beta,\\gamma\\in[0,1]$ be the ratio between $|S|,|V|$ and $|L|$ . That is: $\\beta:=|S|/|L|$ and $\\gamma:=|V|/|L|$ . ", "page_idx": 27}, {"type": "text", "text": "Claim 15. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{med}_{1}(L,h_{L})=O P T_{S}-O P T_{T\\cap L_{l}}+O P T_{L_{r}}+(g_{L}-h_{L})(|L|(1-2\\beta)).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{med}_{1}(L,h_{L})=\\displaystyle\\sum_{i\\in L}|x_{i}-h_{L}|=\\displaystyle\\sum_{i\\in S}h_{L}-x_{i}+\\displaystyle\\sum_{i\\in T\\backslash U\\cup U\\cup V}x_{i}-h_{L}}&\\\\ {\\overset{(\\star)}{=}\\displaystyle\\sum_{i\\in S}g_{L}-x_{i}-|S|(g_{L}-h_{L})-\\displaystyle\\sum_{i\\in T\\cap L_{l}}g_{L}-x_{i}+|T\\cap L_{l}|\\big(g_{L}-h_{L}\\big)}&\\\\ {+\\displaystyle\\sum_{i\\in(T\\cap L_{r})\\cup U\\cup V}x_{i}-g_{L}+\\big(|T\\cap L_{r}|+|U|+|V|\\big)\\big(g_{L}-h_{L}\\big)}&\\\\ {\\overset{(\\star)}{=}O P T_{S}-O P T_{T\\cap L_{l}}+O P T_{L_{r}}+(g_{L}-h_{L})\\big(|L|-2|S|\\big)}&\\\\ {=O P T_{S}-O P T_{T\\cap L_{l}}+O P T_{L_{r}}+(g_{L}-h_{L})\\big(|L|(1-2\\beta)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$(\\star)$ - triangle inequality ", "page_idx": 27}, {"type": "text", "text": "If the algorithm returns $h_{L}$ then we want to show that $\\mathrm{med}_{1}(L,h_{L})\\leq(1.8+O(\\delta))\\,\\mathrm{med}_{1}(L,g_{L})$ . From Claim 15 we get the following equivalent condition: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{med}_{1}(L,h_{L})\\leq(1.8+O(\\delta))O P T_{L}\\iff}\\\\ &{O P T_{S}-O P T_{T\\cap L_{l}}+O P T_{L_{r}}+(g_{L}-h_{L})(|L|(1-2\\beta))\\leq(1.8+O(\\delta))O P T_{L}\\iff}\\\\ &{(g_{L}-h_{L})(|L|(1-2\\beta))\\leq0.8O P T_{S}+2.8O P T_{T\\cap L_{l}}+0.8O P T_{L_{r}}+O(\\delta)O P T_{L}\\iff}\\\\ &{(g_{L}-h_{L})\\leq\\frac{0.8O P T_{S}+2.8O P T_{T\\cap L_{l}}+0.8O P T_{L_{r}}+O(\\delta)O P T_{L}}{|L|(1-2\\beta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, all we have to do is find the above bound for $g_{L}-h_{L}$ . Indeed we will show that this is the case. We start by proving a few claims that will help us bound $g_{L}-h_{L}$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. Since $h_{L}$ is the median of $L^{\\prime}$ , and the points in $S\\cup T$ differ from those in $L^{\\prime}$ by at most $O(\\delta n)$ , we get that $|T|\\approx^{O(\\delta n)}\\beta|L|$ . Similarly, since $h_{R}$ is the median of $R^{\\prime}$ and since $|R|=n-|L|$ : $|U|\\approx^{O(\\delta n)}\\left(\\gamma-1\\right)|L|+n$ . By using the fact that $|L|=|S|+|T|+|U|+|V|$ we get: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|L|\\approx\\!\\!\\!\\slash(\\delta n)\\ \\beta|L|+\\beta|L|+(\\gamma-1)|L|+n+\\gamma|L|\\implies}\\\\ &{2(\\beta+\\gamma)|L|\\approx\\!\\!\\!\\slash(\\delta n)\\ 2|L|-n\\implies}\\\\ &{(\\beta+\\gamma)|L|\\approx\\!\\!\\!\\slash(\\delta n)\\ |L|-\\frac{n}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Claim 17. ", "page_idx": 27}, {"type": "equation", "text": "$$\n(g_{L}-h_{L})|S|\\leq O P T_{S}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. ", "page_idx": 27}, {"type": "equation", "text": "$$\nO P T_{S}=\\sum_{i\\in S}g_{L}-x_{i}\\overset{\\forall x_{i}\\in S:\\,x_{i}\\le h_{L}}{\\ge}\\sum_{i\\in S}g_{L}-h_{L}=|S|(g_{L}-h_{L}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n(h_{R}-g_{L})|U_{r}|\\leq2O P T_{U_{r}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. ", "page_idx": 28}, {"type": "equation", "text": "$$\nO P T_{U_{r}}=\\sum_{i\\in U_{r}}x_{i}-g_{L}\\stackrel{\\forall x_{i}\\in U_{r}:\\,x_{i}\\geq m^{\\prime\\prime}}{\\geq}\\sum_{i\\in U_{r}}m^{\\prime\\prime}-g_{L}=|U_{r}|(m^{\\prime\\prime}-g_{L})=\\frac{|U_{r}|}{2}(h_{R}-g_{L}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last equality is due to the fact that $m^{\\prime\\prime}$ is the middle point between $g_{L}$ and $h_{R}$ . ", "page_idx": 28}, {"type": "text", "text": "Thus we get: ", "page_idx": 28}, {"type": "equation", "text": "$$\n(h_{R}-g_{L})|U_{r}|\\leq2O P T_{U_{r}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Claim 19. ", "page_idx": 28}, {"type": "equation", "text": "$$\n(h_{R}-g_{L})|V|\\leq O P T_{V}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. ", "page_idx": 28}, {"type": "equation", "text": "$$\nO P T_{V}=\\sum_{i\\in V}x_{i}-g_{L}\\overset{\\forall x_{i}\\in V;\\,x_{i}\\geq h_{R}}{\\geq}\\sum_{i\\in S}h_{R}-g_{L}=|V|(h_{R}-g_{L}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Consider the clustering induced by $H^{\\prime}=(g_{L},h_{R})$ . If this clustering is unbalanced then Algorithm 3 returns $h_{L}$ since $L_{l}\\cap A$ is contained in the left cluster and since $\\begin{array}{r}{|L_{l}\\cap\\bar{A}|\\geq\\frac{L}{2}\\!-\\!\\delta n\\geq\\frac{n}{2}\\!-\\!O(\\delta\\bar{n})}\\end{array}$ it must be that the right cluster is the smaller one. And so the left cluster has at least $n\\!-\\!(b\\!-\\!1)\\tilde{\\partial}n=n\\!-\\!O(\\delta n)$ elements and thus it is obtained from $L$ by modifying or dropping at mots $O(\\delta n)$ elements and thus the algorithm returns $h_{L}$ and like before we get a $(1\\bar{+{O}(\\delta)},\\bar{\\delta})$ approximation-robustness. ", "page_idx": 28}, {"type": "text", "text": "Otherwise, the clustering induced by $H^{\\prime}$ is balanced, and we get the following claim: ", "page_idx": 28}, {"type": "equation", "text": "$(h_{R}-g_{L})(|U_{l}|-O(\\delta n))\\leq2(O P T_{T\\cap L_{l}}+O P T_{U_{l}}).$ ", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. For convenience, for any multi-set of the real points $M\\subseteq X$ we denote ${\\hat{M}}\\subseteq X^{\\prime}$ to be the multi-set of the estimated points $X^{\\prime}$ that correspond to the same partition of the real line as $M$ . So we define the following multi-sets: ${\\hat{L}}_{l}=\\{x_{i}^{\\prime}\\in X^{\\prime}\\mid x_{i}^{\\prime}\\leq g_{L}\\}$ , $\\overset{\\vartriangle}{\\hat{L_{r}}}=\\{x_{i}^{\\prime}\\in X^{\\prime}\\setminus\\hat{L_{l}}\\mid x_{i}^{\\prime}\\leq m\\}$ , $\\hat{S}\\,=\\,L^{\\prime}\\iota$ , $\\hat{T}\\,=\\,L^{\\prime}{}_{r}$ , $\\hat{U}\\,=\\,R^{\\prime}\\iota$ $R^{\\prime}\\!_{l},\\,\\hat{V}\\,=\\,\\{i\\,\\in\\,R^{\\prime}\\,\\mid\\,x_{i}^{\\prime}\\,\\in\\,[h_{R},m]\\}$ , $\\hat{U_{l}}\\,=\\,\\{x_{i}^{\\prime}\\,\\in\\,R^{\\prime}{_l}\\ |\\ x_{i}^{\\prime}\\,\\le\\,m^{\\prime\\prime}\\}$ , $\\hat{U_{r}}=\\{x_{i}^{\\prime}\\in R^{\\prime}\\iota\\ |\\ x_{i}^{\\prime}>m^{\\prime\\prime}\\}$ . The reason we use this notation is that we have an inequality in terms of the points in $X^{\\prime}$ and we want to later move on to the inequality in terms of the points in $X$ . This notation will help us see the connection between the points in $X^{\\prime}$ and the points in $X$ . Each such $M,{\\hat{M}}$ contain the same points up to at most $\\delta n$ points. ", "page_idx": 28}, {"type": "text", "text": "By definition of $H$ we know that $\\operatorname{med}_{2}(X^{\\prime},H)\\leq\\operatorname{med}_{2}(X^{\\prime},T)$ for any $T\\,=\\,(t_{1},t_{2})\\,\\in\\,\\mathbb{R}^{2}$ that induces a $(b-1)\\delta$ -balanced clustering of $X^{\\prime}$ . Therefore, $\\operatorname{med}_{2}(X^{\\prime},H)\\leq\\mathrm{med}_{2}(X^{\\prime},(g_{L},h_{R}))$ . ", "page_idx": 28}, {"type": "text", "text": "By the definition of $\\mathrm{med_{2}}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{i}^{\\prime}\\in L^{\\prime}_{l}}h_{L}-x_{i}^{\\prime}+\\sum_{x_{i}^{\\prime}\\in L^{\\prime}_{r}}x_{i}^{\\prime}-h_{L}+\\displaystyle\\sum_{x_{i}^{\\prime}\\in R^{\\prime}_{l}}h_{R}-x_{i}^{\\prime}+\\displaystyle\\sum_{x_{i}^{\\prime}\\in R_{r}^{\\prime}}x_{i}^{\\prime}-h_{R}\\leq}\\\\ &{\\displaystyle\\sum_{x_{i}^{\\prime}\\in\\hat{S}\\cup(\\hat{T}\\cap\\hat{L}_{l})}g_{L}-x_{i}^{\\prime}+\\displaystyle\\sum_{x_{i}^{\\prime}\\in(\\hat{T}\\cap\\hat{L}_{r})\\cup\\hat{U}_{l}}x_{i}^{\\prime}-g_{L}+\\displaystyle\\sum_{x_{i}^{\\prime}\\in\\hat{U}_{r}}h_{R}-x_{i}^{\\prime}+\\displaystyle\\sum_{x_{i}^{\\prime}\\in R_{r}^{\\prime}}x_{i}^{\\prime}-h_{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Which implies: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{i}^{\\prime}\\in L_{h_{l}}}h_{L}-x_{i}^{\\prime}+\\displaystyle\\sum_{x_{i}^{\\prime}\\in L^{\\prime}_{r}}x_{i}^{\\prime}-h_{L}+\\displaystyle\\sum_{x_{i}^{\\prime}\\in R^{\\prime}_{l}}h_{R}-x_{i}^{\\prime}}\\\\ &{\\le\\displaystyle\\sum_{x_{i}^{\\prime}\\in\\hat{S}\\cup(\\hat{T}\\cap\\hat{L}_{l})}g_{L}-x_{i}^{\\prime}+\\displaystyle\\sum_{x_{i}^{\\prime}\\in(\\hat{T}\\cap\\hat{L}_{r})\\cup\\hat{U}_{l}}x_{i}^{\\prime}-g_{L}+\\displaystyle\\sum_{x_{i}^{\\prime}\\in\\hat{U}_{r}}h_{R}-x_{i}^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let us observe LHS: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau_{i}^{\\prime}\\in L_{h_{l}}}h_{L}-x_{i}^{\\prime}+\\sum_{x_{i}^{\\prime}\\in L^{\\prime}}x_{i}^{\\prime}-h_{L}+\\sum_{x_{i}^{\\prime}\\in R_{l}}h_{R}-x_{i}^{\\prime}}\\\\ &{=|L_{i}^{\\prime}|(h_{L}-g_{L})+\\sum_{x_{i}^{\\prime}\\in L_{h_{l}}}g_{L}-x_{i}^{\\prime}+\\sum_{x_{i}^{\\prime}\\in L^{\\prime}}x_{i}^{\\prime}-g_{L}+|L_{r}^{\\prime}|(g_{L}-h_{L})+|R_{i}^{\\prime}|(h_{R}-g_{L})+\\sum_{x_{i}^{\\prime}\\in R_{l}}z_{i}^{\\prime}-g_{L}-x_{i}^{\\prime}|(h_{R}-g_{L})|^{2}}\\\\ &{\\displaystyle\\sum_{x_{i}^{\\prime}\\in L_{i}^{\\prime}}\\sum_{x_{i}^{\\prime}\\in L_{h_{l}}}g_{L}-x_{i}^{\\prime}+\\sum_{x_{i}^{\\prime}\\in L^{\\prime}}g_{L}-|R_{i}^{\\prime}|(h_{R}-g_{L})+\\sum_{x_{i}^{\\prime}\\in R_{l}}g_{L}-x_{i}^{\\prime}}\\\\ &{=\\displaystyle\\sum_{x_{i}^{\\prime}\\in S}g_{L}-x_{i}^{\\prime}+\\sum_{x_{i}^{\\prime}}x_{i}^{\\prime}+|\\tilde{U}|(h_{R}-g_{L})+\\sum_{x_{i}^{\\prime}\\in R}g_{L}-x_{i}^{\\prime}}\\\\ &{=\\displaystyle\\sum_{x_{i}^{\\prime}\\in S}g_{L}-x_{i}^{\\prime}+\\sum_{x_{i}^{\\prime}\\in T}x_{i}^{\\prime}-g_{L}+|\\tilde{U}|(h_{R}-g_{L})+\\sum_{x_{i}^{\\prime}\\in R}g_{L}}\\\\ &{=\\displaystyle\\sum_{x_{i}^{\\prime}\\in S}g_{L}-x_{i}^{\\prime}+\\sum_{x_{i}^{\\prime}\\in T}x_{i}^{\\prime}-g_{L}+\\sum_{x_{i}^{\\prime}\\in T}x_{i}^{\\prime}-g_{L}+|\\tilde{U}| \n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "So by plugging this in (12) we get: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x^{\\prime}\\in S}g_{L^{-}}\\alpha_{x^{\\prime}}^{\\prime}+\\sum_{x^{\\prime}\\in T}\\alpha_{x^{\\prime}}^{\\prime}-g_{L}+\\sum_{x^{\\prime}\\in T}x_{x^{\\prime}}^{\\prime}-g_{L}+|\\mathcal{O}|(h_{R}-g_{L})+\\sum_{x^{\\prime}\\in T}g_{L}-x_{x^{\\prime}}^{\\prime}}\\\\ &{\\le\\displaystyle\\sum_{x^{\\prime}\\in S}\\displaystyle\\sum_{(f^{\\prime}\\cap L_{1})}g_{L}-x_{i}^{\\prime}+\\sum_{x^{\\prime}\\in(f^{\\prime}\\cap L_{1})\\cup\\atop x^{\\prime}\\in(f^{\\prime}\\setminus C)}x_{i}^{\\prime}-g_{L}+\\displaystyle\\sum_{x^{\\prime}\\in Q_{r}}h_{R}-x_{i^{\\prime}}^{\\prime}}\\\\ &{\\implies}\\\\ &{\\displaystyle\\widehat{V}|(h_{R}-g_{L})+\\sum_{x^{\\prime}\\in\\mathcal{O}}g_{L}-x_{i}^{\\prime}\\leq2\\sum_{x^{\\prime}\\in T}g_{L}-x_{i}^{\\prime}+\\sum_{x^{\\prime}\\in\\mathcal{O}_{r}}x_{i}^{\\prime}-g_{L}+\\sum_{x^{\\prime}\\in\\mathcal{O}_{r}}h_{R}-x_{i}^{\\prime}}\\\\ &{=2\\displaystyle\\sum_{x^{\\prime}\\in\\mathcal{O}_{r}}g_{L}-x_{i^{\\prime}}^{\\prime}+\\sum_{x^{\\prime}\\in\\mathcal{O}_{r}}x_{i}^{\\prime}-g_{L}+|\\mathcal{O}_{r}|(h_{R}-g_{L})+\\displaystyle\\sum_{x^{\\prime}\\in\\mathcal{O}_{r}}g_{L}-x_{i^{\\prime}}^{\\prime}\\implies}\\\\ &{\\qquad\\qquad x_{i}^{\\prime}\\in\\mathcal{O}_{r}}\\\\ &{\\displaystyle\\widehat{V}_{i}|(h_{R}-g_{L})\\leq2\\sum_{x^{\\prime}\\in T}g_{L}-x_{i}^{\\prime}+2\\sum_{x^{\\prime}\\in\\mathcal{O}_{r}}x_{i}^{\\prime}-g_{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By the definition of $\\hat{T},\\hat{L_{l}};\\hat{T}\\cap\\hat{L_{l}}$ and $T\\cap L_{l}$ differ by at most $\\delta n$ elements, and for any $x_{i}^{\\prime}\\in\\hat{T}\\cap\\hat{L_{l}}$ : $x_{i}^{\\prime}\\geq h_{L}$ . Similarly $\\hat{U_{l}}$ and $U_{l}$ differ by at most $\\delta n$ elements and for any $x_{i}^{\\prime}\\in\\hat{U}_{l}$ : $x_{i}^{\\prime}\\leq m^{\\prime\\prime}$ . ", "page_idx": 29}, {"type": "text", "text": "By plugging this in the above (13) we get: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|U_{l}|-\\delta n)(h_{R}-g_{L})\\le|\\hat{U_{l}}|(h_{R}-g_{L})}\\\\ &{\\phantom{\\le\\;}\\le2\\sum_{x_{i}\\in T\\cap L_{l}}g_{L}-x_{i}+2\\delta n(g_{L}-h_{L})+2\\sum_{x_{i}\\in U_{l}}x_{i}-g_{L}+2\\delta n(m^{\\prime\\prime}-g_{L}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Which implies: ", "page_idx": 29}, {"type": "equation", "text": "$$\n(h_{R}-g_{L})|U_{l}|\\le2(O P T_{T\\cap L_{l}}+O P T_{U_{l}})+\\delta n(2(g_{L}-h_{L})+2(m^{\\prime\\prime}-g_{L})+(h_{R}-g_{L})).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $m^{\\prime\\prime}-g_{L}=(h_{R}\\!-\\!g_{L})\\big/2$ (by the definition of $m^{\\prime\\prime}$ ) and from Eq. (7) we get: ", "page_idx": 29}, {"type": "equation", "text": "$$\n(h_{R}-g_{L})|U_{l}|\\leq2(O P T_{T\\cap L_{l}}+O P T_{U_{l}})+O(\\delta n)(h_{R}-g_{L}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n(h_{R}-g_{L})(|U_{l}|-O(\\delta n))\\leq2(O P T_{T\\cap L_{l}}+O P T_{U_{l}}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now we are ready to bound $g_{L}-h_{L}$ . ", "page_idx": 29}, {"type": "text", "text": "Lemma 21. If the algorithm returns $h_{L}$ then: ", "page_idx": 30}, {"type": "equation", "text": "$$\n(g_{L}-h_{L})\\le\\frac{0.8O P T_{S}+2.8O P T_{T\\cap L_{l}}+0.8O P T_{L_{r}}+O(\\delta)O P T_{L}}{|L|(1-2\\beta)}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. By Claim 17, Claim 18, Claim 20, Claim 19 and the fact that $g_{L}-h_{L}\\leq h_{R}-g_{L}$ we get the following four inequalities: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad(g_{L}-h_{L})\\vert U_{r}\\vert\\le2O P T_{U_{r}},}\\\\ &{\\bigl(g_{L}-h_{L}\\bigr)(\\vert U_{l}\\vert-O(\\delta n))\\le2(O P T_{T\\cap L_{l}}+O P T_{U_{l}}),}\\\\ &{\\qquad\\qquad\\quad(g_{L}-h_{L})2\\vert V\\vert\\le2O P T_{V},}\\\\ &{\\qquad\\qquad\\quad(g_{L}-h_{L})2\\vert S\\vert\\le2O P T_{S},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By summing the above inequalities: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(g_{L}-h_{L})(2|S|+|U|+2|V|-O(\\delta n))\\le2(O P T_{S}+O P T_{T\\cap L_{l}}+O P T_{U}+O P T_{V})\\implies}\\\\ &{g_{L}-h_{L}\\le\\displaystyle\\frac{(O P T_{S}+O P T_{T\\cap L_{l}}+O P T_{U}+O P T_{V})}{(1-2\\beta)|L|}\\cdot\\displaystyle\\frac{2(1-2\\beta)|L|}{2|S|+|U|+2|V|-O(\\delta n)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $|L^{\\prime}\\iota|=|L^{\\prime}{}_{r}|$ we get that $|T|\\approx^{\\delta n}|S|=\\beta|L|$ and so ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{2|S|+|U|+2|V|-O(\\delta n)}&{}\\\\ &{\\geq|S|+|T|+|U|+2|V|-O(\\delta n)}\\\\ &{=(|S|+|T|+|U|+|V|)+(|S|+|V|)-|S|-O(\\delta n)}\\\\ &{\\overset{\\mathrm{(*)}}{=}|L|+(\\beta+\\gamma)|L|-\\beta|L|-O(\\delta n)}\\\\ &{\\overset{C l a i m~16}{\\geq}|L|+|L|-\\frac{n}{2}-\\beta|L|-O(\\delta n)}\\\\ &{=2|L|-\\beta|L|-\\frac{n}{2}-O(\\delta n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Where $(\\star)$ is due to: $|L|=|S|+|T|+|U|+|V|,|V|=\\gamma|L|,|S|=\\beta|L|.$ ", "page_idx": 30}, {"type": "text", "text": "By plugging this inequality into Eq. (15): ", "page_idx": 30}, {"type": "equation", "text": "$$\ng_{L}-h_{L}\\le\\frac{(O P T_{S}+O P T_{T\\cap L_{l}}+O P T_{U}+O P T_{V})}{(1-2\\beta)|L|}\\cdot\\bigg(\\frac{2(1-2\\beta)|L|}{2|L|-\\beta|L|-\\frac{n}{2}-O(\\delta n)}\\bigg).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We will bound the second term in RHS. First, we know that $|L|\\approx^{O(\\delta n)}n$ and thus: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{2(1-2\\beta)|L|}{2|L|-\\beta|L|-\\frac{n}{2}-O(\\delta n)}\\le\\frac{2(1-2\\beta)(n-O(\\delta n))}{\\frac{3n}{2}-\\beta n-O(\\delta n)}\\le\\frac{2(1-2\\beta)}{\\frac{3}{2}-\\beta}(1+O(\\delta))\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We denote: $\\begin{array}{r}{f(\\beta):=\\frac{2(1-2\\beta)}{\\frac{3}{2}-\\beta}}\\end{array}$ ", "page_idx": 30}, {"type": "text", "text": "Since the algorithm returns $h_{L}$ we deduce that $\\left|L^{\\prime}\\right|\\geq{\\frac{n}{2}}$ . Thus, $\\begin{array}{r}{|L^{\\prime}\\iota|,|L^{\\prime}{_r}|\\ge\\frac{n}{4}}\\end{array}$ . But $S$ and $L^{\\prime}l$ differ by at most $\\delta n$ elements, and so $\\begin{array}{r}{|S|\\ge\\frac{n}{4}-\\delta n}\\end{array}$ . Since $|\\overline{{S}}|=\\beta|L|$ and $|L|\\leq n$ we get: $\\begin{array}{r}{\\beta n\\ge\\frac{n}{4}-\\delta n}\\end{array}$ which implies $\\beta\\geq\\frac{1}{4}-\\delta$ . ", "page_idx": 30}, {"type": "text", "text": "Also obviously $\\beta\\leq0.5$ since $h_{L}\\leq g_{L}$ by the definition of $S$ . ", "page_idx": 30}, {"type": "text", "text": "So $f:[\\textstyle{\\frac{1}{4}}-\\delta,\\frac{1}{2}]\\to\\mathbb{R}$ is a well defined function. We find its maximum: ", "page_idx": 30}, {"type": "equation", "text": "$$\nf^{\\prime}(\\beta)=2\\frac{-2(\\frac{3}{2}-\\beta)-(1-2\\beta)(-1)}{(\\frac{3}{2}-\\beta)^{2}}=2\\frac{-3+2\\beta+1-2\\beta}{(\\frac{3}{2}-\\beta)^{2}}=\\frac{-4}{(\\frac{3}{2}-\\beta)^{2}}<0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "So $f$ gets its maximum at the left boundary where $\\begin{array}{r}{\\beta=\\frac{1}{4}-\\delta}\\end{array}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\nf(\\frac{1}{4}-\\delta)=2\\frac{\\frac{1}{2}+2\\delta}{\\frac{5}{4}+\\delta}=\\frac{4}{5}(1+O(\\delta)).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By plugging the above into Eq. (16) we get: ", "page_idx": 31}, {"type": "equation", "text": "$$\ng_{L}-h_{L}\\leq\\frac{4}{5}\\frac{(O P T_{S}+O P T_{T\\cap L_{l}}+O P T_{U}+O P T_{V}+O(\\delta)O P T_{L})}{(1-2\\beta)|L|}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "What we have shown is that if the algorithm returns $h_{L}$ we are done. If the algorithm returns $h_{R}$ we can similarly show that in this case $\\mathrm{med}_{1}(L,h_{R})\\leq(1.8+\\Theta(\\delta))\\,\\mathrm{med}_{1}(L,g_{L})$ . ", "page_idx": 31}, {"type": "text", "text": "H.1.1 Handling the Case where the Algorithm Returns $h_{R}$ ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this case the cost $\\mathrm{med}_{1}(L,h_{R})$ will be: ", "page_idx": 31}, {"type": "text", "text": "Claim 22. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{ned}_{1}(L,h_{R})=O P T_{S}+O P T_{T\\cap L_{l}}-O P T_{T\\cap L_{r}}-O P T_{U}+O P T_{V}+(|L|(1-2\\gamma))(h_{R}-g_{L}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{ned}_{1}(L,h_{R})=\\displaystyle\\sum_{i\\in S\\cup T\\cup U}h_{R}-x_{i}+\\displaystyle\\sum_{i\\in V}x_{i}-h_{R}}\\\\ &{\\qquad=\\displaystyle\\sum_{i\\in S\\cup T\\cup U}\\left(h_{R}-g_{L}\\right)+\\left(g_{L}-x_{i}\\right)+\\displaystyle\\sum_{i\\in V}(x_{i}-g_{L})-\\left(h_{R}-g_{L}\\right)}\\\\ &{\\qquad=O P T S+O P T_{T\\cap L_{I}}-O P T_{T\\cap L_{r}}-O P T_{U}+O P T_{V}+(|S|+|T|+|U|-|V|)(h_{R}-g_{L})}\\\\ &{\\qquad=\\displaystyle\\sum_{|S|+|T|+|\\underline{{U}}|+|V|=|L|}O P T_{S}+O P T_{T\\cap L_{I}}-O P T_{T\\cap L_{r}}-O P T_{U}+O P T_{V}+O P T_{V}+(|L|-2|V|)}\\\\ &{\\qquad=O P T_{S}+O P T_{T\\cap L_{I}}-O P T_{T\\cap L_{r}}-O P T_{U}+O P T_{V}+(|L|(1-2\\gamma))(h_{R}-g_{L}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To get $\\mathrm{med}_{1}(L,h_{R})\\leq(1.8\\mathrm{+}O(\\delta))\\,\\mathrm{med}_{1}(L,g_{L})$ we need to show (by the above claim) the following lemma: ", "page_idx": 31}, {"type": "text", "text": "Lemma 23. If the algorithm returns $h_{R}$ then: ", "page_idx": 31}, {"type": "equation", "text": "$$\n(h_{R}-g_{L})\\leq\\frac{0.8(O P T_{S}+O P T_{T\\cap L_{l}}+O P T_{V})+2.8(O P T_{T\\cap L_{r}}+O P T_{U})+O(\\delta)O P T_{L}}{(1-2\\gamma)|L|}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. We begin by showing the following claim: ", "page_idx": 31}, {"type": "text", "text": "Claim 24. ", "page_idx": 31}, {"type": "equation", "text": "$$\n(h_{R}-g_{L})|S|\\leq(h_{R}-g_{L})|S|\\leq O P T_{S}+\\frac{|S|}{|U|}O P T_{U}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. $h_{R}-g_{L}=m^{\\prime}-h_{L}=(m^{\\prime}-g_{L})+(g_{L}-h_{L})$ and so $g_{L}-h_{L}=\\left(h_{R}-g_{L}\\right)-\\left(m^{\\prime}-g_{L}\\right)$ . Together with Claim 17 we get: ", "page_idx": 31}, {"type": "equation", "text": "$$\n(h_{R}-g_{L})|S|\\leq O P T_{S}+|S|(m^{\\prime}-g_{L}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Also, $\\begin{array}{r}{O P T_{U}=\\sum_{i\\in U}x_{i}-g_{L}\\geq\\sum_{i\\in U}m^{\\prime}-g_{L}=|U|(m^{\\prime}-g_{L})}\\end{array}$ which implies ", "page_idx": 31}, {"type": "equation", "text": "$$\nm^{\\prime}-g_{L}\\leq\\frac{O P T_{U}}{|U|}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By Eq. (17) and Eq. (18) we get the desired inequality. ", "page_idx": 31}, {"type": "text", "text": "From Claim 24, Claim 18, Claim 20, Claim 19: ", "page_idx": 32}, {"type": "text", "text": "we get the following four inequalities: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{(h_{R}-g_{L})2|S|\\leq2O P T_{S}+2\\displaystyle\\frac{|S|}{|U|}O P T_{U},}}\\\\ {{(h_{R}-g_{L})|U_{r}|\\leq2O P T_{U_{r}},}}\\\\ {{(h_{R}-g_{L})(|U_{l}|-O(\\delta n))\\leq2(O P T_{T\\cap L_{l}}+O P T_{U_{l}}),}}\\\\ {{(h_{R}-g_{L})2|V|\\leq2O P T_{V},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By summing these inequalities: ", "page_idx": 32}, {"type": "equation", "text": "$$\nh_{R}-g_{L})(2|S|+|U|+2|V|-O(\\delta n))\\leq\\frac{O P T_{S}+O P T_{U}+O P T_{T\\cap L_{l}}+O P T_{V}+\\frac{2|S|}{|U|}O P T_{U}}{(1-2\\gamma)|L|}\\cdot2|U|+2|S|.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since the algorithm returns $h_{R}$ it must be the case that $\\left|R^{\\prime}\\right|\\geq\\frac{n}{2}$ and so $|R^{\\prime}\\iota|\\ge\\textstyle{\\frac{n}{4}}$ which means that $\\begin{array}{r}{|U|\\ge\\frac{n}{4}-\\delta n}\\end{array}$ . This also means that $|L^{\\prime}|\\leq\\frac{n}{2}$ and thus similarly $\\begin{array}{r}{|S|\\le\\frac{n}{4}+\\delta n}\\end{array}$ . So together: ", "page_idx": 32}, {"type": "equation", "text": "$$\n2{\\frac{|S|}{|U|}}\\leq{\\frac{\\frac{n}{2}+2\\delta n}{{\\frac{n}{4}}-\\delta n}}=2+O(\\delta).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By plugging this back into Eq. (19) we get: ", "page_idx": 32}, {"type": "equation", "text": "$$\nh_{R}-g_{L})(2|S|+|U|+2|V|-O(\\delta n))\\leq\\frac{O P T_{S}+(3+O(\\delta))O P T_{U}+O P T_{T\\cap L_{l}}+O P T_{V}}{(1-2\\gamma)|L|}\\cdot2(1-\\delta).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $|L^{\\prime}\\iota|=|L^{\\prime}{}_{r}|$ we get that $|T|\\approx^{\\delta n}|S|$ and so ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2|S|+|U|+2|V|-O(\\delta n)}\\\\ &{\\phantom{2}\\geq|S|+|T|+|U|+2|V|-O(\\delta n)}\\\\ &{\\phantom{2}=(|S|+|T|+|U|+|V|)+|V|-O(\\delta n)}\\\\ &{\\phantom{2}\\stackrel{(\\star)}{=}(1+\\gamma)|L|-O(\\delta n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Where $(\\star)$ is due to: $|L|=|S|+|T|+|U|+|V|,|V|=\\gamma|L|$ ", "page_idx": 32}, {"type": "text", "text": "Let us use this fact in Eq. (20) and get: ", "page_idx": 32}, {"type": "equation", "text": "$$\nh_{R}-g_{L}\\leq\\frac{O P T_{S}+(3+O(\\delta))O P T_{U}+O P T_{T\\cap L_{l}}+O P T_{V}}{(1-2\\gamma)|L|}\\cdot\\bigg(2\\frac{(1-2\\gamma)|L|}{(1+\\gamma)|L|-O(\\delta n)}\\bigg).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The second term on RHS is (up to $1+O(\\delta)$ factor): $\\begin{array}{r}{h(\\gamma):=2\\frac{1-2\\gamma}{1+\\gamma}}\\end{array}$ . Let us find a bound f or $h$ ", "page_idx": 32}, {"type": "text", "text": "Since $\\left|R^{\\prime}\\right|\\geq\\frac{n}{2}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n|V|+|R|\\approx=^{O(\\delta n)}|R^{\\prime}{}_{r}|\\geq{\\frac{n}{4}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and since $\\begin{array}{r}{|R|\\leq b\\delta n;\\,\\gamma|L|=|V|\\geq\\frac{n}{4}-b\\delta n}\\end{array}$ . We deduce: ", "page_idx": 32}, {"type": "text", "text": "$\\begin{array}{r}{\\gamma\\geq\\frac{1}{4}-O(\\delta)}\\end{array}$ . The nominator is decreasing in $\\gamma$ and the denominator is increasing in $\\gamma$ and thus the maximum is received on the left end point: $\\begin{array}{r}{\\gamma=\\frac{1}{4}-O(\\delta)}\\end{array}$ . The maximum value of $h$ is thus: $\\textstyle2{\\frac{{\\frac{1}{2}}-O(\\delta)}{{\\frac{5}{4}}-O(\\delta)}}={\\frac{4}{5}}+O(\\delta)$ . ", "page_idx": 32}, {"type": "text", "text": "So we found a bound for the RHS of Eq. (21) and therefore: ", "page_idx": 32}, {"type": "equation", "text": "$$\nh_{R}-g_{L}\\leq\\frac{\\frac{4}{5}O P T_{S}+\\frac{12}{5}O P T_{U}+\\frac{4}{5}O P T_{T\\cap L_{l}}+\\frac{4}{5}O P T_{V}+O(\\delta)O P T_{L}}{(1-2\\gamma)|L|}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "H.2 Sub-case: $m^{\\prime}<g_{L}$ . ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "To handle this sub-case we use a reduction to the previous sub-case and show that we pay another multiplicative factor of at most $1+O(\\delta)$ . We show a bound of $\\mathrm{med}_{1}(h_{L},L)$ in case the algorithm returns $h_{L}$ . In the case the algorithm returns $h_{R}$ a bound for $\\mathrm{med}_{1}(h_{R},L)$ is achieved similarly. ", "page_idx": 33}, {"type": "text", "text": "Let $\\tilde{g_{L}}=m^{\\prime}$ . For any multi-set of points $M\\subseteq L$ let $\\begin{array}{r}{\\widetilde{O P T_{M}}=\\sum_{i\\in M}|x_{i}-\\tilde{g_{L}}|.}\\end{array}$ . ", "page_idx": 33}, {"type": "image", "img_path": "LPbqZszt8Y/tmp/9e8892d66b9d55a1d1c43a52f83c246f174788902c5ffb30f8f600c53775013e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 6: Illustration of case (4) where $m^{\\prime}<g_{L}$ . On the top we have the \"estimated\" locations $h_{L},h_{R}$ , computed on $X^{\\prime}$ . On the bottom we have the \"real\" locations, $g_{L},g_{R}$ , computed on $X$ . $L^{\\prime},R^{\\prime},L^{\\prime}\\iota,L^{\\prime}{}_{r},R^{\\prime}\\iota,R^{\\prime}{}_{r},S,T,U,V,U_{l},U_{r}$ are all the same as before. The difference is that $m^{\\prime}$ is now to the left of $g_{L}$ . ", "page_idx": 33}, {"type": "text", "text": "By the exact same analysis of the case where $m^{\\prime}\\geq g_{L}$ , now we have that $m^{\\prime}\\geq\\tilde{g_{L}}$ and thus, just like before, g\u02dcL \u2212hL \u226445 $\\begin{array}{r}{\\tilde{g_{L}}-h_{L}\\le\\frac{4}{5}\\overleftarrow{\\frac{\\partial\\widehat{P T_{S}}+\\widehat{O P T_{T}}+\\widehat{O P T_{L_{r}}}}{(1-2\\beta)|L|}}}\\end{array}$   \nBut $\\widetilde{O P T_{S}}+\\widetilde{O P T_{T}}=O P T_{S}+O P T_{T}-(g_{L}-m^{\\prime})(|S|+|T|)$ and $\\widetilde{O P T_{L_{r}}}=O P T_{L_{r}}+(g_{L}-$ $m^{\\prime})|L_{r}|$ . ", "page_idx": 34}, {"type": "text", "text": "So together we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{g_{L}}-h_{L}\\leq\\displaystyle\\frac{4}{5}\\frac{O P T_{S}+O P T_{T}-(g_{L}-m^{\\prime})(|S|+|T|)+O P T_{L_{r}}+(g_{L}-m^{\\prime})(|U|+|V|)}{(1-2\\beta)|L|}}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{4}{5}\\frac{(O P T_{S}+O P T_{T}+O P T_{L_{r}}+(g_{L}-m^{\\prime})(-|S|-|T|+|U|+|V|))}{(1-2\\beta)|L|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since $\\left|L^{\\prime}\\right|\\geq\\frac{n}{2}$ it must be the case that $\\begin{array}{r}{|S|+|T|\\ge\\frac{n}{2}-\\delta n}\\end{array}$ since otherwise there would be strictly less than $\\begin{array}{l}{{\\frac{n}{2}}}\\end{array}$ elements in $|L^{\\prime}|$ . For this reason also $\\begin{array}{r}{|U|\\bar{+}|V|\\leq\\frac{n}{2}}\\end{array}$ ", "page_idx": 34}, {"type": "text", "text": "$\\begin{array}{r}{{\\mathrm{So}}-|S|-|T|+|U|+|V|\\leq\\delta n}\\end{array}$ . By plugging this in Eq. (22) we get: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tilde{g_{L}}-h_{L}\\leq\\frac{4}{5}\\frac{(O P T_{S}+O P T_{T}+O P T_{L_{r}}+(g_{L}-m^{\\prime})(\\delta n)}{(1-2\\beta)|L|}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that $\\tilde{g_{L}}-h_{L}=\\tilde{g_{L}}-g_{L}+g_{L}-h_{L}=g_{L}-h_{L}-\\left(g_{L}-\\tilde{g_{L}}\\right)=(g_{L}-h_{L})-(g_{L}-m^{\\prime}).$ By plugging this in the above we get: ", "page_idx": 34}, {"type": "equation", "text": "$$\ng_{L}-h_{L}\\leq\\frac{4}{5}\\frac{\\left(O P T_{S}+O P T_{T}+O P T_{L_{r}}+\\frac{10}{4}(g_{L}-m^{\\prime})\\delta n\\right)}{(1-2\\beta)|L|}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O P T_{T}=\\sum_{i\\in T}g_{L}-x_{i}\\geq\\sum_{i\\in T}g_{L}-m^{\\prime}\\operatorname{so}g_{L}-m^{\\prime}\\leq\\frac{O P T_{T}}{|T|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{g_{L}}-h_{L}\\leq\\frac{4}{5}\\frac{\\left(O P T_{S}+O P T_{T}+O P T_{L_{r}}+2.5O P T_{T}\\frac{\\delta n}{|T|}\\right)}{(1-2\\beta)|L|}}\\\\ &{\\qquad\\qquad\\leq\\frac{4}{5}\\frac{\\left(O P T_{S}+O P T_{T}+O P T_{L_{r}}+2O P T_{T}\\frac{\\delta n}{n/2-\\delta n}\\right)}{(1-2\\beta)|L|}}\\\\ &{\\qquad\\qquad=\\frac{4}{5}\\frac{\\left(O P T_{S}+(1+\\Theta(\\delta))O P T_{T}+O P T_{L_{r}}\\right)}{(1-2\\beta)|L|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and so we get the desired bound (due to the equivalent condition stated in Eq. (8). ", "page_idx": 34}, {"type": "text", "text": "We conclude with a final remark: ", "page_idx": 34}, {"type": "text", "text": "Remark 25. Algorithm 3 does not have $(c,\\delta)$ approximation robustness for any $c\\,<\\,\\textstyle{\\frac{5}{3}}\\approx1.667$ . This lower bound is obtained by observing the following instance: Let $\\varepsilon>0$ be a small enough value, $X\\in\\mathbb{R}^{n}$ where $n/2$ points are at $x=0$ , $n/4$ points are at $x=-0.5-\\varepsilon,\\,r$ $n/4$ points at $x=-1$ , and one point at $M\\gg n$ . Now let $X^{\\prime}$ be obtained from $X$ by moving the point at $x=M$ to $x=-1$ . Calculation shows that the above instance leads to a $\\frac{5}{3}$ approximation ratio. ", "page_idx": 34}, {"type": "text", "text": "I Second Facility Location: SECOND-PROPORTIONAL-MECHANISM Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section we show the strategyproofness and the expected approximation ratio of Algorithm 2. ", "page_idx": 34}, {"type": "text", "text": "Proof of Lemma 6. We follow the proof of (Lu et al., 2010, Thm 4.1). In the proof they show that given that the first facility is at $x_{k}$ for any $k\\in[n]$ , then for any $X^{\\prime}=<x_{i}^{\\prime},\\bar{X}_{-i}>$ that is gained from $X$ by agent $i$ deviating from $x_{i}$ to $\\boldsymbol{x}_{i}^{\\prime}$ , the expected cost of agent $i$ only increases by deviating to $\\boldsymbol{x}_{i}^{\\prime}$ . The proof does not depend at all on the location of the first facility. Since the first facility is entirely independent of the agent reported values, then the above implies that the choice of the second facility is also strategy proof. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "Proof of Theorem 7. The proof is similar in structure to the one given by Lu et al. (2010) for the Proportional-Mechanism; the main difference is a careful (and tight) analysis of Lemma 27 for the real line which leads to an approximation ratio of at most 3 rather than 4. ", "page_idx": 35}, {"type": "text", "text": "For any $M\\subseteq X$ let $O P T_{M}=\\mathrm{med}_{2}(M,G)$ . So $,O P T_{S}=\\mathrm{med}_{1}(S,g_{S}),O P T_{T}=\\mathrm{med}_{1}(T,g_{T})$ and thus $O P T=\\mathrm{med}_{2}(X,G)=O P T_{S}+O P T_{T}$ . ", "page_idx": 35}, {"type": "text", "text": "Let $\\boldsymbol{H}=(g_{S},h)$ be the algorithms solution for the problem, and $A L G={\\mathrm{med}}_{2}(X,H)$ . We denote $A L G_{S},A L G_{T}$ to be $\\mathrm{med}_{2}^{*}(S,H),\\mathrm{med}_{2}(T,H)$ respectively, so that $A L G=A L G_{S}+A L G_{T}$ and therefore $\\mathbb{E}[A L G]=\\mathbb{E}[A L G_{S}]+\\mathbb{E}[A L G_{T}]$ . ", "page_idx": 35}, {"type": "text", "text": "Since $A L G_{S}\\leq\\mathrm{med}_{1}(S,g_{S})=O P T_{S}$ , we get that $\\mathbb{E}[A L G_{S}]\\leq O P T_{S}$ , and we only need to find a bound for $\\mathbb{E}[A L G_{T}]$ . ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathbb{E}[A L G_{T}]=\\displaystyle\\sum_{i\\in[n]}\\mathbb{E}[A L G_{T}\\mid h=x_{i}]P r(h=x_{i})}}\\\\ {{=\\displaystyle\\sum_{i\\in T}\\mathbb{E}[A L G_{T}\\mid h=x_{i}]P r(h=x_{i})+\\sum_{i\\in T}\\mathbb{E}[A L G_{T}\\mid h=x_{i}]P r(h=x_{i})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "In the summation above the first term is the expected cost due to choosing the second facility to be a location in $S$ and the second term is the expected cost due to choosing the second facility to be a location in $T$ . We will bound each one individually. ", "page_idx": 35}, {"type": "text", "text": "For all i \u2208 [n] let ai = d(xi, gS), pi = $\\begin{array}{r}{p_{i}\\,=\\,\\frac{a_{i}}{\\sum_{j\\,\\in\\,[n]\\,a_{j}}}}\\end{array}$ and $b_{i}\\,=\\,d(x_{i},g_{T})$ . So $\\textstyle\\sum_{i\\in S}a_{i}\\,=\\,O P T_{S}$ , $\\begin{array}{r}{\\sum_{i\\in T}b_{i}=O P T_{T}}\\end{array}$ and $\\textstyle\\sum_{i\\in[n]}p_{i}=1$ . For each $i,j\\in[n]$ let $d_{i,j}=d(x_{i},x_{j})$ . ", "page_idx": 35}, {"type": "text", "text": "Lemma 26. $\\begin{array}{r}{\\sum_{i\\in S}\\mathbb{E}[A L G_{T}\\mid h=x_{i}]P r(h=x_{i})\\le O P T_{S}}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "Proof. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in S}\\mathbb{E}[A L G_{T}\\ |\\ h=x_{i}]P r(h=x_{i})=\\sum_{i\\in S}\\left(\\sum_{t\\in T}\\operatorname*{min}\\{a_{t},d_{i,t}\\}\\right)\\cdot p_{i}}\\\\ &{\\stackrel{(\\star)}{\\le}\\displaystyle\\sum_{i\\in S}\\left(\\sum_{t\\in T}a_{t}\\right)\\cdot p_{i}=\\sum_{i\\in S}\\left(\\frac{\\sum_{t\\in T}a_{t}}{\\sum_{t\\in T}a_{t}+\\sum_{s\\in S}a_{s}}\\right)\\cdot a_{i}\\le\\sum_{i\\in S}a_{i}=O P T_{S}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Where inequality $(\\star)$ is gained by ignoring the second facility. ", "page_idx": 35}, {"type": "equation", "text": "$\\begin{array}{r}{\\sum_{i\\in T}\\mathbb{E}[A L G_{T}\\mid h=x_{i}]P r(h=x_{i})\\le3O P T_{T}}\\end{array}$ ", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in T}\\mathbb{E}[A L G_{T}\\mid h=x_{i}]P r(h=x_{i})=\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}\\operatorname*{min}\\{a_{t},d(x_{t},x_{i})\\}\\right)\\cdot p_{i}\\right]}\\\\ &{\\displaystyle^{t r i a n g l e~i n e q u a l i t y}\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}\\operatorname*{min}\\{a_{t},b_{t}+b_{i}\\}\\right)\\cdot p_{i}\\right]=\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}b_{t}+\\sum_{t\\in T}\\operatorname*{min}\\{a_{t}-b_{t},b_{i}\\}\\right)\\cdot p_{i}\\right]}\\\\ &{\\displaystyle=\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}b_{t}\\right)\\cdot p_{i}\\right]+\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}\\operatorname*{min}\\{a_{t}-b_{t},b_{i}\\}\\right)\\cdot p_{i}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{=\\displaystyle\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}b_{t}\\right)\\cdot p_{i}\\right]+\\displaystyle\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}\\operatorname*{min}\\{a_{t}-b_{t},b_{i}\\}\\right)\\cdot\\frac{b_{i}}{\\sum_{j\\in[n]}a_{j}}\\right]}\\\\ {+\\displaystyle\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}\\operatorname*{min}\\{a_{t}-b_{t},b_{i}\\}\\right)\\cdot\\frac{a_{i}-b_{i}}{\\sum_{j\\in[n]}a_{j}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We bound each of these 3 terms individually by $O P T_{T}$ , and thus get the desired bound. The first term bound: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}b_{t}\\right)\\cdot p_{i}\\right]=\\left(\\sum_{t\\in T}b_{t}\\right)\\sum_{i\\in T}p_{i}\\leq O P T_{T}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The second term bound: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}\\operatorname*{min}\\{a_{t}-b_{t},b_{i}\\}\\right)\\cdot\\frac{b_{i}}{\\sum_{j\\in[n]}a_{j}}\\right]\\leq\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}a_{t}-b_{t}\\right)\\cdot\\frac{b_{i}}{\\sum_{j\\in[n]}a_{j}}\\right]}\\\\ &{\\le\\displaystyle\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}a_{t}\\right)\\cdot\\frac{b_{i}}{\\sum_{j\\in[n]}a_{j}}\\right]=\\sum_{i\\in T}b_{i}=O P T_{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The third term bound: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{\\epsilon T}\\left[\\left(\\sum_{t\\in T}\\operatorname*{min}\\{a_{t}-b_{t},b_{i}\\}\\right)\\cdot\\frac{a_{i}-b_{i}}{\\sum_{j\\in[n]}a_{j}}\\right]\\leq\\sum_{i\\in T}\\left[\\left(\\sum_{t\\in T}b_{i}\\right)\\cdot\\frac{a_{i}-b_{i}}{\\sum_{j\\in[n]}a_{j}}\\right]=\\sum_{i\\in T}\\left[|T|b_{i}\\cdot\\frac{a_{i}-b_{i}}{\\sum_{j\\in[n]}a_{j}}\\right]=0.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We finish the third term bound by showing the following claim ", "page_idx": 36}, {"type": "text", "text": "Claim 28. ", "text_level": 1, "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{i\\in T}\\left[|T|b_{i}\\cdot{\\frac{a_{i}-b_{i}}{\\sum_{j\\in[n]}a_{j}}}\\right]\\leq O P T_{T}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Let $T_{a}$ be the part of $T$ closer to $g_{S}$ and $T_{b}$ be the other part of $T$ . ", "page_idx": 36}, {"type": "text", "text": "So $T_{a}=\\{i\\mid i\\in T,d(x_{i},g_{S})\\leq d(x_{i},g_{T})\\}$ , and $\\begin{array}{r}{T_{b}=T\\setminus T_{a}}\\end{array}$ . Since the points are all on the line metric: ", "page_idx": 36}, {"type": "text", "text": "For all $i\\in T_{a}\\colon a_{i}=d(g_{S},x_{i})=d(g_{S},g_{T})-b_{i}=D-b_{i}.$ For all $i\\in T_{b}$ : $a_{i}=d(g_{S},x_{i})=d(g_{S},g_{T})+b_{i}=D+b_{i}.$ . ", "page_idx": 36}, {"type": "text", "text": "Let us use these facts in Eq. (28) to get: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in T}\\left[|T|b_{i}\\cdot\\frac{a_{i}-b_{i}}{\\sum_{j\\in[n]}a_{j}}\\right]=\\displaystyle\\sum_{i\\in T_{a}}\\left[|T|b_{i}\\cdot\\frac{D-b_{i}-b_{i}}{\\sum_{j\\in[n]}a_{j}}\\right]+\\displaystyle\\sum_{i\\in T_{b}}\\left[|T|b_{i}\\cdot\\frac{D+b_{i}-b_{i}}{\\sum_{j\\in[n]}a_{j}}\\right]}\\\\ &{=\\displaystyle\\frac{|T|D}{\\sum_{j\\in[n]}a_{j}}\\left(\\sum_{i\\in T}b_{i}\\right)-\\frac{2|T|\\sum_{i\\in T_{a}}b_{i}^{\\,2}}{\\sum_{j\\in[n]}a_{j}}=\\frac{|T|D\\cdot O P T_{T}-2|T|\\sum_{i\\in T_{a}}b_{i}^{\\,2}}{\\sum_{j\\in[n]}a_{j}}}\\\\ &{\\overset{(\\star)}{\\leq}\\frac{|T|D\\cdot O P T_{T}-2(O P T_{T_{a}})^{2}}{\\sum_{j\\in[n]}a_{j}}\\overset{(\\star\\star)}{\\leq}\\frac{|T|D-2\\frac{(O P T_{T_{a}})^{2}}{O P T_{T}}}{|T|D+O P T_{T_{b}}-O P T_{T_{a}}}\\cdot O P T_{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "$(\\star)$ is due to $Q M-A M$ inequality since ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{i\\in T_{a}}{b_{i}}^{2}\\geq\\frac{\\left(\\sum_{i\\in T_{a}}b_{i}\\right)^{2}}{\\left|T_{a}\\right|}\\stackrel{|T_{a}|\\leq|T|}{\\geq}(O P T_{T_{a}})^{2}\\frac{1}{|T|}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "$({\\bf\\star}{\\bf\\star})$ explanation: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{j\\in[n]}a_{j}=\\sum_{j\\in S}a_{j}+\\sum_{j\\in T_{a}}a_{j}+\\sum_{j\\in T_{b}}a_{j}=O P T_{S}+\\sum_{j\\in T_{a}}D-b_{j}+\\sum_{j\\in T_{b}}D+b_{j}}}\\\\ {{\\displaystyle=O P T_{S}+D|T|+O P T_{T_{b}}-O P T_{T_{a}}\\geq|T|D+O P T_{T_{b}}-O P T_{T_{a}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We now show that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{|T|D-2\\frac{(O P T_{T_{a}})^{2}}{O P T_{T}}}{|T|D+O P T_{T_{b}}-O P T_{T_{a}}}\\le1.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Indeed, if $O P T_{T_{b}}\\ge O P T_{T_{a}}$ then $|T|D+O P T_{T_{b}}-O P T_{T_{a}}\\geq|T|D$ and thus: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{|T|D-2\\frac{(O P T_{T_{a}})^{2}}{O P T_{T}}}{|T|D+O P T_{T_{b}}-O P T_{T_{a}}}\\leq\\frac{|T|D-2\\frac{(O P T_{T_{a}})^{2}}{O P T_{T}}}{|T|D}\\leq\\frac{|T|D}{|T|D}=1.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Otherwise $O P T_{T_{a}}-O P T_{T_{b}}>0$ and thus: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2P T_{T_{a}}>O P T_{T_{b}}\\implies}\\\\ &{2P T_{T}=O P T_{T_{a}}+O P T_{T_{b}}<2O P T_{T_{a}}\\implies}\\\\ &{O P T_{T_{a}}-O P T_{T_{b}})O P T_{T}\\le2(O P T_{T_{a}}-O P T_{T_{b}})O P T_{T_{a}}=2(O P T_{T_{a}})^{2}-2O P T_{T_{b}}O P T_{T_{a}}\\le2(O P T_{T_{a}})^{2}}\\\\ &{2P T_{T_{a}}-O P T_{T_{b}}\\le\\frac{2(O P T_{T_{a}})^{2}}{O P T_{T}}\\implies\\frac{|T|D-2\\frac{(O P T_{T_{a}})^{2}}{O P T_{T}}}{|T|D+O P T_{T_{b}}-O P T_{T_{a}}}\\le\\frac{|T|D+O P T_{T_{b}}-O P T_{T_{a}}}{|T|D+O P T_{T_{b}}-O P T_{T_{a}}}=\\frac{1}{2}(D\\backslash O P T_{T_{a}})^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "So from Eq. (29): $\\begin{array}{r}{\\sum_{i\\in T}\\left[|T|b_{i}\\cdot\\frac{a_{i}-b_{i}}{\\sum_{j\\in[n]}a_{j}}\\right]\\leq O P T_{T}.}\\end{array}$ ", "page_idx": 37}, {"type": "equation", "text": "$\\begin{array}{r}{\\sum_{i\\in T}\\mathbb{E}[A L G_{T}\\ |\\ h=x_{i}]P r(h=x_{i})\\le O P T_{T}+O P T_{T}+O P T_{T}=3O P T_{T}}\\end{array}$ ", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "From plugging Lemma 26, Lemma 27 into Eq. (26) we get: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}[A L G]\\leq2O P T_{S}+3O P T_{T}\\leq3O P T.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Remark 29. The upper bound of 3 in the analysis of Theorem 7 is tight. ", "page_idx": 37}, {"type": "text", "text": "To show tightness, consider an instance with $n$ points on the real line: $n/2$ points at $x=0,\\,n/2\\,-$ 1 points at $x\\,=\\,1$ , and a single point at $x\\ =\\ 2$ . Assume the given first facility is at 0. The optimal solution puts the second facility at 1 and pays the cost of 1. On the other hand $\\mathbb{E}[A L G]=$ $\\begin{array}{r}{\\frac{1}{\\frac{n}{2}+1}\\Big(\\frac{n-2}{2}+2\\frac{n-2}{2}\\Big)=3\\frac{n/2-1}{n/2+1}\\approx3.}\\end{array}$ . ", "page_idx": 37}, {"type": "text", "text": "J Proof of Theorem 9 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Proof. The strategyproofness is due to the fact that the first facility is chosen based only using the predictions, and the second facility choice is strategyproof due to Lemma 6. ", "page_idx": 37}, {"type": "text", "text": "Let $\\boldsymbol{G}=\\left(g_{L},g_{R}\\right)$ be the optimal cluster centers for $X$ , and let $L,R$ be the respective corresponding clusters. We consider two cases: when this optimal clustering is $b\\delta$ -balanced, and when it is not; in both cases we show that the algorithm achieves low expected cost. Let $H=(h_{1},h_{2})$ be the solution returned by Algorithm 4. ", "page_idx": 37}, {"type": "text", "text": "The first case is when $(L,R)$ is a $b\\delta$ -balanced clustering of $X$ . In this case we claim that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\mathrm{med}}_{2}(X,H)]\\leq(3.6+O(\\delta))\\,{\\mathrm{med}}_{2}(X,G).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Indeed, let $T=(t_{L},t_{R})$ be the two centers of the $(b-1)\\delta$ -balanced 2-medians algorithm on $X^{\\prime}$ . Since $G$ induces a $b\\delta$ -balanced clustering, Theorem 4 and our choice of $b$ ensures that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname{med}_{2}(X,T)\\leq1.2\\operatorname{med}_{2}(X,G).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Since Algorithm 3 returns one of the centers of the $(b-1)\\delta$ -balanced 2-medians algorithm on $X^{\\prime}$ as $h_{1}$ , let us assume w.l.o.g. that BIG-CLUSTER-CENTER returns $h_{1}\\,=\\,t_{L}$ . Let $(X_{L},X_{R})$ and $(X_{L}^{\\prime},X_{R}^{\\prime})$ be the clusterings of $X$ and $X^{\\prime}$ induced by $T=\\{t_{L},t_{R}\\}$ . From Theorem 7: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E[\\mathrm{med}_{2}(X,H)]_{(\\frac{\\omega}{c})}\\mathbb{E}[\\mathrm{med}_{2}(X,\\{t_{L},h_{2}\\})]\\leq2\\,\\mathrm{med}_{1}(X_{L},t_{L})+3\\,\\mathrm{med}_{1}(X_{R},m e d i a n(X_{R}))}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\,\\mathrm{med}_{1}(X_{L},t_{L})+3(1+O(\\delta))\\,\\mathrm{med}_{1}(X_{R},t_{R})}\\\\ &{\\qquad\\qquad\\qquad\\leq(3+O(\\delta))\\,\\mathrm{med}_{2}(X,T)\\leq(3.6+O(\\delta))\\,\\mathrm{med}_{2}(X,G),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the inequality $(\\star)$ uses the fact that $X_{R}$ and $X_{R}^{\\prime}$ differ on at most $\\delta|X|$ points, and hence we can sue $(1+O(\\delta),\\delta)$ approximation-robustness of 1-Median from Corollary 3. ", "page_idx": 38}, {"type": "text", "text": "Now for the other case: suppose $(L,R)$ is a $b\\delta$ -unbalanced clustering. Theorem 8 implies that either $\\mathrm{med}_{1}(L,h_{1})\\leq(1.8+O(\\bar{\\delta}))\\,\\mathrm{med}_{1}(L,g_{L})$ or $\\mathrm{med}_{1}(R,h_{1})\\leq(1.8\\,\\bar{+}\\,O(\\delta))\\,\\mathrm{med}_{1}(\\bar{R},g_{R})$ . W.l.o.g., consider the first option. Then from Theorem 7: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E[\\mathrm{med}_{2}(X,H)]\\leq2\\,\\mathrm{med}_{1}(L,h_{1})+3\\,\\mathrm{med}_{1}(R,g_{R})}\\\\ &{\\qquad\\qquad\\qquad\\leq2((1.8+O(\\delta))\\,\\mathrm{med}_{1}(L,g_{L}))+3\\,\\mathrm{med}_{1}(R,g_{R})\\leq(3.6+O(\\delta))O P T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Combining the two cases completes the proof. ", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: See Section 7, where we list possible directions to improve what we have done in this work, and directions for future work. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: It is a theoretic paper discussing beyond worst case analysis of algorithms with predictions. ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}]