[{"figure_path": "Mrs9a1XQAp/tables/tables_7_1.jpg", "caption": "Table 1: Extraction Performance Carlini (C), Canales-Martinez (CM) versus Ours on layer 2 of random models. Since extraction times vary significantly between layers in different models, we perform comparison of layer by layer extraction time and not whole model extraction time. We compare layer 2 because layer 1 and 3 are more straightforward to extract. A 10-5x2-1 model, following Carlini et al. [1], represents input layer of size 10, two hidden layers of size 5 and output layer of size 1. The numbers highlighted in red capture the gist of the performance improvement and the numbers in blue are our best performances.", "description": "This table compares the performance of three different methods for extracting model parameters: Carlini's method, Canales-Martinez's method, and the authors' improved method. It shows the extraction time for each method on layer 2 of several random models with varying sizes. The results demonstrate the significant speedup achieved by the authors' method in terms of both signature and sign extraction.", "section": "4.2 Performance"}, {"figure_path": "Mrs9a1XQAp/tables/tables_8_1.jpg", "caption": "Table 2: Extraction Performance across models, training seeds and extractions seeds. The two different training seeds used are denoted as s1 and s2. The measurements were all taken over four extraction seeds. All signature extraction times are without the precision improvement function, since for MNIST models this takes up to 33 times longer than the actual signature extraction time and we have shown that this can be skipped or handled while already proceeding with further extraction processes. Extractions of deeper layers of MNIST784-16x8-1 did not lead to a full extraction after 36 hours with 6/16 and 0/16 extracted for layers 4 and 8. The most interesting contrasting results for discussion are highlighted pairwise in colours. In green one can see how layer 2 extraction for the same number of neurons can vary with model depth. In blue one can see the variance of extracting two models trained similarly but on different randomness. In red one can see how deeper layers become increasingly hard to extract.", "description": "This table presents a comprehensive performance evaluation of the model extraction process across various models, training seeds, and extraction seeds.  It highlights the impact of different factors, such as model architecture (layer depth and size), randomness in training and extraction, on the efficiency and success of the extraction.  The table contrasts extraction times and query numbers for different layer configurations, showing variations in performance based on these factors. Key performance aspects are shown, including the mean and variance of signature and sign extraction times and queries.", "section": "5 Discussion"}, {"figure_path": "Mrs9a1XQAp/tables/tables_18_1.jpg", "caption": "Table 1: Extraction Performance Carlini (C), Canales-Martinez (CM) versus Ours on layer 2 of random models. Since extraction times vary significantly between layers in different models, we perform comparison of layer by layer extraction time and not whole model extraction time. We compare layer 2 because layer 1 and 3 are more straightforward to extract. A 10-5x2-1 model, following Carlini et al. [1], represents input layer of size 10, two hidden layers of size 5 and output layer of size 1. The numbers highlighted in red capture the gist of the performance improvement and the numbers in blue are our best performances.", "description": "This table compares the performance of three different model extraction methods: the original method by Carlini et al. [1], the improved method by Canales-Martinez et al. [2], and the authors' new method.  The comparison focuses on layer 2 of random models with varying sizes. Extraction time for signature, sign (unified), and sign (original) are shown and compared. Highlighted values denote significant improvements. The table also describes the model structure using a notation that specifies the input and output layer sizes, and the number of hidden layers and their sizes.", "section": "4.2 Performance"}]