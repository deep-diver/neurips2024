[{"type": "text", "text": "Learning to Understand: Identifying Interactions via the M\u00a8obius Transform ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Justin Singh Kang Yigit Efe Erginbas Landon Butler UC Berkeley UC Berkeley UC Berkeley justin kang@berkeley.edu erginbas@berkeley.edu landonb@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Ramtin Pedarsani Kannan Ramchandran UC Santa Barbara UC Berkeley ramtin@ece.ucsb.edu kannanr@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "One of the key challenges in machine learning is to find interpretable representations of learned functions. The M\u00a8obius transform is essential for this purpose, as its coefficients correspond to unique importance scores for sets of input variables. This transform is closely related to widely used game-theoretic notions of importance like the Shapley and Bhanzaf value, but it also captures crucial higher-order interactions. Although computing the M\u00a8obius Transform of a function with $n$ inputs involves $2^{n}$ coefficients, it becomes tractable when the function is sparse and of low degree as we show is the case for many real-world functions. Under these conditions, the complexity of the transform computation is significantly reduced. When there are $K$ non-zero coefficients, our algorithm recovers the Mo\u00a8bius transform in $O(K n)$ samples and $O(K n^{2})$ time asymptotically under certain assumptions, the first non-adaptive algorithm to do so. We also uncover a surprising connection between group testing and the M\u00a8obius transform. For functions where all interactions involve at most $t$ inputs, we use group testing results to compute the M\u00a8obius transform with $O(K t\\log n)$ sample complexity and $O(K\\,\\mathrm{poly}(\\bar{n}))$ time. A robust version of this algorithm withstands noise and maintains this complexity. This marks the first $n$ sub-linear query complexity, noise-tolerant algorithm for the Mo\u00a8bius transform. In several examples, we observe that representations generated via sparse Mo\u00a8bius transform are up to twice as faithful to the original function, as compared to Shapley and Banzhaf values, while using the same number of terms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As machine learning models become increasingly complex, our ability to interpret them has not kept pace. A natural question to ask is: What is the most fundamental interpretable representation of the functions we learn? The Shapley value [1], a concept from cooperative game theory, has become a popular way to interpret model predictions [2] by assigning importance scores to individual inputs such as features, data samples or tokens. This value represents the weighted average marginal contribution of an input, quantifying the change in the function\u2019s output when that input is included. Recent research has expanded the scope of interpretability to encompass sets of inputs [3, 4], capturing the collective influence of input combinations and their synergies on model predictions. Central to this advancement is the Mo\u00a8bius Transform [5], a mathematical transformation that projects functions onto a fundamental interpretable basis known in game theory as the unanimity function basis. ", "page_idx": 0}, {"type": "text", "text": "The M\u00a8obius transform has a more powerful and nuanced explanation capability than the Shapley value. Consider a sentiment analysis model (BERT [6] fine-tuned on the IMDB dataset [7]) explained using both Shapley values and the M\u00a8obius transform as depicted in Fig. 1. The model\u2019s objective is to classify the sentiment of the review as positive or negative. The M\u00a8obius transform assigns a score to all word subsets within a sentence. For instance, in the sentence \u201cHer acting never fails to impress\u201d each subset of words is evaluated\u2014positive interactions receive positive scores, and negative interactions, negative scores. Summing these scores yields the overall sentiment $+0.98$ . This granular analysis reveals the model\u2019s understanding of linguistic constructs like double negatives, as seen in the interaction between never and fails, and the inherent positivity of words like impress. When the word never is masked, interactions involving never are excluded, shifting the sentiment negatively to $-0.96$ . ", "page_idx": 0}, {"type": "image", "img_path": "glGeXu1zG4/tmp/b949ddba6f58ef18241a114c3848665a516d17bb60a1e03ee7b532fb507ac188.jpg", "img_caption": ["Figure 1: The movie review \u201cHer acting never fails to impress\u201d is passed into a BERT language model fine-tuned to do sentiment analysis [8]. Presented are $\\bar{1}^{s t}$ , $2^{n d}$ and $3^{r d}$ order M\u00a8obius coefficients, with positive interactions in green and negative in red computed via (1). The coefficients explain how groups of words influence BERT\u2019s perception of sentiment. For instance, while never and fails have strong negative sentiments individually, when combined, they impose a profound positive sentiment. In the second row, the word never is deleted, resulting in a large change in sentiment. In contrast, the Shapley values of each word $\\mathrm{SV}(\\cdot)$ , presented at the bottom of the figure, are less informative. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This level of detail is not readily available with the Shapley value, which assigns scores to individual words without considering their interplay. The value of the Mo\u00a8bius transform is apparent, but given its complex structure, is it possible to compute efficiently? ", "page_idx": 1}, {"type": "text", "text": "In general, to compute a Mo\u00a8bius transform over $n$ features requires $2^{n}$ inferences (masking over all $2^{n}$ subsets of features), as well as $n2^{n}$ time using a divide-and-conquer approach similar to that of the Fast Fourier Transform (FFT) algorithm. GPT-4 currently supports in the range of 8000 words-per-prompt, and context length will continue to grow with new architectures [9]. Running inference $2^{8\\bar{0}00}$ times is not even close to possible, and even if you could, $2^{8000}$ coefficients are hardly interpretable! In Fig. 1 we see that many coefficients are insignificant. This is typical. The solution to the computational problem is to just focus on computing the largest M\u00a8obius interactions and ignore the small ones. Is this possible in a systematic way? Yes\u2014assuming that only $K$ Mo\u00a8bius coefficients (which $K$ values are significant is unknown) are non-zero, our algorithm enables us to intelligently query the model to significantly reduce the number of samples of that are required to $O(K n)$ with $\\bar{O}(\\bar{K}n^{2})$ time. We also explore the regime where the non-zero interactions occur between at most $t$ inputs, with $t\\ll n$ , showing that only ${\\bar{O}}(K t\\log(n))$ samples are required in $O(K\\,\\mathrm{poly}(n))$ time. We also have a robust algorithm that allows for some noise in the sampling process, effectively relaxing the constraint that the insignificant coefficients are exactly zero while maintaining the same complexities. ", "page_idx": 1}, {"type": "text", "text": "Defining the M\u00a8obius Transform We define a value function for a model with $n$ inputs across subsets $\\bar{S}\\subseteq[n]$ denoted as $f(S)$ . The construction of this function varies based on the model: in Fig. 1, words not in $S$ might be masked or omitted. In other cases, we might take a conditional expectation over words not in $S$ . To facilitate later discussion on group testing, we express the function as $f:\\mathbb{Z}_{2}^{n}\\to\\mathbb{R}$ , where $f(S)=f(\\mathbf{m})$ with $S=\\{i:m_{i}=1\\}$ . The relationship between $f:\\mathbb{Z}_{2}^{n}\\to\\mathbb{R}$ and its Mo\u00a8bius transform $F:\\mathbb{Z}_{2}^{n}\\to\\mathbb{R}$ is characterized by the forward and inverse transforms: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{Inverse:}\\quad f(\\mathbf{m})=\\sum_{\\mathbf{k}\\leq\\mathbf{m}}F(\\mathbf{k}),\\qquad\\mathrm{Forward:}\\quad F(\\mathbf{k})=\\sum_{\\mathbf{m}\\leq\\mathbf{k}}(-1)^{\\mathbf{1}^{\\mathrm{T}}(\\mathbf{k}-\\mathbf{m})}f(\\mathbf{m}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{k}\\leq\\mathbf{m}$ means that $k_{i}\\leq m_{i}\\;\\forall i$ . This transform acts as a bridge, connecting various importance metrics, which can be expressed as projections onto a subset of the Mo\u00a8bius basis. The Shapley value $\\mathrm{SV}(i)$ and Banzhaf value $\\mathrm{BZ}(i)$ for feature $i$ is elegantly represented within this framework: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{SV}(i)=\\sum_{\\mathbf{k}:k_{i}=1}{\\frac{1}{|\\mathbf{k}|}}F(\\mathbf{k}),\\qquad\\mathrm{BZ}(i)=\\sum_{\\mathbf{k}:k_{i}=1}{\\frac{1}{2^{|\\mathbf{k}|-1}}}F(\\mathbf{k}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "These relationships are foundational to the definition of the Shapley and Bhanzaf values themselves. The left equality appears as Eq. 10 in Lloyd Shapley\u2019s original report from 1952 [1] where the concept of Shapley value was first introduced and is central to his derivation of a closed-form expression. ", "page_idx": 2}, {"type": "text", "text": "1.1 Related Works and Applications ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This work is inspired by the literature on sparse Fourier transforms, which began with [10, 11, 12].   \nThe sparse Boolean Fourier (Hadamard) transform [13, 14] is most relevant. ", "page_idx": 2}, {"type": "text", "text": "Group Testing This manuscript establishes a strong connection between the interaction identification problem and group testing [15]. Group testing was first described by Dorfman [16], who noted that when testing soldiers for syphilis, pooling blood samples from many soldiers, and testing the pooled blood samples reduced the total number of tests needed. [17] is the first work to exploit group testing in a feature selection/importance problem, using a group testing matrix in their algorithm. [18] also mentions group testing in relation to Shapley values. ", "page_idx": 2}, {"type": "text", "text": "M\u00a8obius Transform M\u00a8obius transforms [5] have been studied in the pseudo-Boolean (set) function literature, and dates back to at least [19]. [20] develops a framework for computing sparse transforms of pseudo-Boolean functions. They do not directly consider the M\u00a8obius transform as we define it, but one can apply their algorithm to compute a $K$ sparse transform in $O(n K)$ adaptive samples and $O(K^{2}n)$ time. In the sparse and noiseless setting, our algorithm improves on this by being fully nonadaptive and having lower time complexity in most non-trivial settings. [20] does not consider the important low degree setting and does not consider robustness to noise (approximate sparsity), which are critical aspects of this work. In [21], the authors show that a classifier satisfying certain properties can be well represented by a sparse and low degree Mo\u00a8bius transform. ", "page_idx": 2}, {"type": "text", "text": "Explainability [2] proposes model explanation via pseudo-Boolean functions approximated by Shapley values, effectively utilizing only first-order Mo\u00a8bius coefficients. Constructing these functions, [22, 23, 24, 25] especially for generative models with complex outputs [26, 27, 28], is an ongoing research area. [3] presents the Taylor-Shapley interaction index (STII), scoring interactions up to size $t$ . For sets smaller than $t$ , STII are exactly M\u00a8obius coefficients. [4] introduces the Faithful Shapley Interaction index (FSI), which computes scores via projection onto up to $t^{t h}$ order Mo\u00a8bius coefficients. [29] develops methods for computing FSI, STII, and other interaction indices. The relationship between the Mo\u00a8bius transform, FSI, STII, Shapley value, and Banzhaf value is detailed in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Data Valuation In data valuation [18] the goal is to assign an importance score to data, either to determine a fair price [30] or to curate a more efficient dataset [31]. A feature of this problem is the high cost of getting a sample since we need to determine the accuracy of our model when trained on different subsets of data, making sample complexity of critical importance. [32, 33] try to approximate this by looking at the accuracy of partially trained models, though this introduces sampling noise. ", "page_idx": 2}, {"type": "text", "text": "1.2 Main Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our algorithm and proofs are deeply interdisciplinary, and the contributions of this paper are theoretical. We use modern ideas spanning across signal processing, algebra, coding and information theory, and group testing to address the important problem of interpretability at the forefront of machine learning. The main contributions of this manuscript are: ", "page_idx": 2}, {"type": "text", "text": "\u2022 For a function with $K$ non-zero M\u00a8obius coefficients chosen uniformly at random, the Sparse M\u00a8obius Transform (SMT) algorithm exactly recovers the transform $F$ in $O(K n)$ samples and $O(K n^{2})$ time in the limit as $n\\to\\infty$ with $K$ growing at most as $2^{n\\delta}$ with $\\delta\\leq{\\frac{1}{3}}$ . ", "page_idx": 2}, {"type": "image", "img_path": "glGeXu1zG4/tmp/61d62c1c69f99822ef4bbd1296357c3990a4c4ba56128189c9c231bae106cd6d.jpg", "img_caption": ["Figure 2: These plots are strong indicators that sparsity and low degree assumptions are worthy of consideration. We consider three different learning tasks. The left-most plot shows results from an XGBoost [34] model used for breast cancer diagnosis. The middle plot shows results from wordlevel sentiment analysis task using a BERT model [8] like in Fig. 1. The right-most plot shows results from a multiple choice question and answer task also using a BERT model [35]. Error bars represent standard deviation over 10 different instances. Details for each setting are in Appendix B. In all cases, the number of features $n\\approx20$ , for which it is possible to perform the full Mo\u00a8bius transform. On the top row, we plot achievable faithfulness $R^{2}$ as a function of sparsity. We observe that in all cases, faithfulness approaching 1 requires only a few thousand Mo\u00a8bius coefficients, motivating our sparsity assumption. The bottom row of plots considers achievable faithfulness vs. degree, i.e., what $\\dot{R}^{2}$ can be achieved using only Mo\u00a8bius coefficients $\\hat{F}$ up to a given degree. Here we observe that in nearly all cases, low degree coefficients suffice to get quite small $R^{2}$ , motivating our low degree assumption. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "\u2022 We develop a formal connection with group testing and present a variant of SMT that works when all non-zero interactions are low order. If the maximum order of interaction is $t=\\Theta(n^{\\alpha})$ where $\\alpha<0.409$ then we can compute the Mo\u00a8bius transform in $O(K t\\log(n))$ samples in $O({\\bar{K}}\\,\\mathrm{poly}(n))$ time with error going to zero as $n\\to\\infty$ with growing $K$ . \u2022 Using robust group testing, we develop an algorithm that, under certain assumptions, computes the Mo\u00a8bius transform in ${\\bar{O}}(K t\\log(n))$ samples, with vanishing error as $n\\to\\infty$ with growing $K$ . ", "page_idx": 3}, {"type": "text", "text": "In addition to our asymptotic analysis, we provide synthetic and real-world experiments that verify that our algorithm performs well even in the finite $n$ regime. Furthermore, our results are non-adaptive meaning that all samples can be computed in parallel. Code has been made publicly available 1. ", "page_idx": 3}, {"type": "text", "text": "Notation Lowercase boldface $\\mathbf{x}$ and uppercase boldface $\\mathbf{X}$ denote vectors and matrices respectively. $\\mathbf x\\geq\\mathbf y$ means that $x_{i}\\geq y_{i}\\;\\forall i$ . Multiplication is always standard real field multiplication, but addition between two elements in $\\mathbb{Z}_{2}$ should be interpreted as a logical OR \u2228. We define subtraction, of $\\mathbf x-\\mathbf y$ for $\\mathbf x\\geq\\mathbf y$ by standard real field subtraction. $\\bar{\\bf x}$ corresponds to bit-wise negation for Boolean $\\mathbf{x}$ , and x $\\odot$ y represents an element-wise multiplication. ", "page_idx": 3}, {"type": "text", "text": "2 Understanding Assumptions: Sparsity and Low Degree ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Computing the forward transform (1) typically requires sampling all $2^{n}$ input combinations, an infeasible task, even for modest $n$ . For an arbitrary $f$ , one cannot do any better. In fact, the same is true of the Shapley value, yet, computational tools like SHAP [2] exist because practical functions of interest are not arbitrary. To help understand this, we define faithfulness for an explanation model $\\hat{f}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nR^{2}=1-\\|{\\hat{f}}-f\\|^{2}/\\left\\|f\\right\\|^{2},{\\mathrm{where}}\\left\\|f\\right\\|^{2}=\\sum_{\\mathbf{m}\\in\\mathbb{Z}_{2}^{n}}f(\\mathbf{m})^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that this corresponds to the standard definition of $R^{2}$ in statistics when $f$ is zero-mean, and we generally define $f$ such that this is the case. A good explanation model should have a high $R^{2}$ , a succinct representation, and most importantly, be easily computed. For the M\u00a8obius transform, we aim to learn coefficients ${\\hat{F}}(\\mathbf{k})$ efficiently and construct $\\hat{f}$ using the inverse transform (1). With no restrictions on ${\\hat{F}}(\\mathbf{k})$ we can achieve $R^{2}\\,=\\,1$ , but this fails to meet our simplicity criterion. Fortunately, many real-world functions are sparse\u2014only a few ${\\hat{F}}(\\mathbf{k})$ coefficients need to be nonzero to yield $R^{2}\\approx1$ . Fig. 2 considers three machine learning models for breast cancer diagnosis, sentiment analysis, and question answering respectively. In all three cases, we find that we only need a small number of Mo\u00a8bius coefficients to achieve $R^{2}\\approx1$ . Furthermore, real-world functions are low degree, such that those small number of non-zero coefficients satisfy $|\\mathbf{k}|\\leq t$ for some small $t$ . This results in a much more compact representation and as we shall see, also enables efficient computation. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Fig. 2 validates our assumption for the deep-learning models mentioned above. Prior research [36, 21], have presented empirical and theoretical evidence that sparsity and low degree properties are common in well-trained models. Further investigation of the spectral properties of explanation functions could be a promising avenue for future research. Our formal statements of assumptions are given below: ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.1. ( $K$ Uniform Interactions) $f:\\mathbb{Z}_{2}^{n}\\mapsto\\mathbb{R}$ has a M\u00a8obius transform of the following form: $\\mathbf{k}_{1},\\ldots,\\mathbf{k}_{K}$ are sampled uniformly at random from $\\mathbb{Z}_{2}^{n}$ , and have $F(\\mathbf{k}_{i})\\neq0$ , $\\forall i\\in[K]$ , but $F(\\mathbf{k})=0$ for all other $\\mathbf{k}\\in\\mathbb{Z}_{2}^{n}$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.2. ( $\\kappa\\,t$ -Degree Interactions) $f:\\mathbb{Z}_{2}^{n}\\mapsto\\mathbb{R}$ has a M\u00a8obius transform of the following form: $\\mathbf{k}_{1},\\ldots,\\mathbf{k}_{K}$ are sampled uniformly from $\\{\\bar{\\mathbf{k}^{\\mathsf{\\bar{\\Delta}}}}\\colon|\\mathbf{k}|\\leq t,\\mathbf{k}\\in\\mathbb{Z}_{2}^{n}\\}$ , and have $F(\\mathbf{k}_{i})\\neq0$ , $\\forall i\\in$ $[K]$ , but $F(\\mathbf{k})=0$ other $\\mathbf{k}\\in\\mathbb{Z}_{2}^{n}$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption Limitations By assuming that the non-zero coefficients are uncorrelated and uniformly distributed, we aim to understand the fundamental difficulty in learning a sparse M\u00a8obius transform. Correlation between non-zero coefficients means identifying one coefficient would tell us information about the locations of the others, which can be further exploited. The existence of a scheme that works well under the uniform setting suggests that it is possible to solve the problem where correlations between interactions exist. We also consider exact sparsity in our assumptions. In practice, these \u201czero\u201d coefficients may instead have some small magnitude. We investigate this in Section 4. ", "page_idx": 4}, {"type": "text", "text": "3 Algorithm Overview ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Subsampling and Aliasing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "First we perform functional subsampling: For some $b<n$ we construct $u$ according to ", "page_idx": 4}, {"type": "equation", "text": "$$\nu(\\ell)=f(\\mathbf{m}_{\\ell}),\\;\\;\\ell\\in\\mathbb{Z}_{2}^{b},\\;\\;\\mathbf{m}_{\\ell}\\in\\mathbb{Z}_{2}^{n},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we have the freedom to choose $\\mathbf{m}_{\\ell}$ . Critically, the M\u00a8obius transform of $u$ , denoted $U$ , is related to $F$ via the well-known signal processing phenomenon of aliasing: ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Sparse Mo\u00a8bius Transform (SMT) 1: Input: $\\mathbf{H}_{c}\\in\\mathbb{Z}_{2}^{b\\times n}$ for $c=1,\\ldots,C$   \n2: ${\\bf D}_{c}\\in\\mathbb{Z}_{2}^{P\\times n}$ for $c=1,\\ldots,C$ 3: $\\hat{F}({\\bf k})\\gets0\\,\\forall{\\bf k}$ ; $\\kappa\\gets\\emptyset$ ; 4: for $c=1$ to $C$ do 5: for $p=1$ to $P$ do 6: $\\begin{array}{r l}&{\\dot{u_{c,p}}(\\ell)\\gets f\\left(\\overline{{\\mathbf{H}_{c}^{\\mathrm{T}}\\overline{{\\ell}}+\\mathbf{d}_{c,p}}}\\right),\\forall\\ell\\in\\mathbb{Z}_{2}^{b}}\\\\ &{U_{c,p}\\gets\\mathrm{FastMobius}\\left(u_{c,p}\\right)}\\end{array}$ 7: 8: end for 9: end for   \n10: $S=\\{(c,\\mathbf{j},\\mathbf{k},v):\\operatorname{Detect}\\left(\\mathbf{U}_{c}(\\mathbf{j})\\right)=\\mathcal{H}_{S}(\\mathbf{k},v)\\}$   \n11: while $\\vert{\\cal S}\\vert>0$ do   \n12: for $(c,\\mathbf{j},\\mathbf{k},v)\\in S$ with $\\mathbf{k}\\in\\mathcal{K}$ do   \n13: $\\hat{F}(\\mathbf{k})\\gets v;\\kappa\\gets\\kappa\\cup\\{\\mathbf{k}\\}$   \n14: for $c^{\\prime}=1$ to $C$ do   \n15: $\\begin{array}{r l}&{\\mathrm{res}\\leftarrow\\mathbf{\\bar{U}}_{c^{\\prime}}(\\mathbf{{H}}_{c^{\\prime}}\\mathbf{k})-\\hat{F}(\\mathbf{k})(\\mathbf{1}-\\mathbf{D}_{c^{\\prime}}\\mathbf{k})}\\\\ &{\\mathbf{U}_{c^{\\prime}}(\\mathbf{H}_{c^{\\prime}}\\mathbf{k})\\leftarrow\\mathrm{res}}\\end{array}$   \n16:   \n17: end for   \n18: end for   \n19: Update S : Re-run Detect (\u00b7)   \n20: end while   \n21: Output: $\\hat{F}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nU(\\mathbf{j})=\\sum_{\\mathbf{k}\\in\\mathcal{A}(\\mathbf{j})}F(\\mathbf{k}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{\\nabla}A(\\mathbf{j})$ corresponds to an aliasing set determined by $\\mathbf{m}_{\\ell}$ . Fig. 3 shows this subsampling procedure on a \u201csparsified\u201d version of our sentiment analysis example using different $\\mathbf{m}_{\\ell}$ . Our goal is to choose $\\mathbf{m}_{\\ell}$ such that the non-zero values of $F(\\mathbf{k})$ are uniformly spread across the aliasing sets, since that makes them easier to recover. If only a single $\\mathbf{k}$ with non-zero $F(\\mathbf{k})$ ends up in an aliasing set $\\boldsymbol{A}(\\mathbf{j})$ , we call it a singleton. In Fig. 3, our ", "page_idx": 4}, {"type": "text", "text": "first subsampling generated two singletons, while our second one generated only one. Maximizing the number of singletons is one of our goals since we can ultimately use those singletons to construct the M\u00a8obius transform. In this work, we have determined two different subsampling procedures that perform well under our two assumptions: ", "page_idx": 4}, {"type": "image", "img_path": "glGeXu1zG4/tmp/e2705b74843ea58f93f2c4b1b76adfd41b259789f281f9bbeaa4e299b076af50.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: This figure considers a \u201csparsified\u201d version of the M\u00a8obius coefficients depicted in Fig 1, keeping only the largest 4 depicted. Two different sampling choices are shown, as well as the resulting aliasing sets. In the first aliasing set, there is one zeroton, two singletons, and one multiton. In the second aliasing set, there are two zerotons, one singleton, and one multiton. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.1. Choose $\\mathbf{m}_{\\ell}=\\overline{{\\mathbf{H}^{T}\\overline{{\\ell}}}},$ , which results in $\\mathcal{A}(\\mathbf{j})=\\left\\{\\mathbf{k}:\\mathbf{H}\\mathbf{k}=\\mathbf{j}\\right\\}$ . H is chosen as follows: 1. Under Assumption 2.1, we choose $\\mathbf{H}=[\\mathbf{I}_{b\\times b}\\mathbf{0}_{b,n-b}],$ , or any column permutation of this matrix. 2. Under Assumption 2.2 with $t=\\Theta(n^{\\alpha})$ for some $\\alpha\\le0.409$ , there exists a matrix $\\mathbf{H}$ chosen from b rows of a near constant column weight group testing matrix. ", "page_idx": 5}, {"type": "text", "text": "With the given $\\mathbf{H}$ , non-zero indices are mapped to the $2^{b}$ sampling sets $\\mathcal{A}(\\mathbf{j})$ independently and uniformly at random asymptotically, thus maximizing the number of singletons when $b=\\Theta(\\log(K))$ . ", "page_idx": 5}, {"type": "text", "text": "In both cases the matrix $\\mathbf{H}$ can be viewed as a part of a group testing matrix. This is explicit under Assumption 2.2, but under Assumption 2.1, $\\mathbf{H}$ can be viewed as part of an individual testing matrix, where elements are tested one-by-one. These matrices are optimal in an information-theoretic sense because they achieve an optimal rate asymptotically in their respective setting. We can think of rate as the amount of information about $\\mathbf{k}$ what we get from the matrix product $\\mathbf{Hk}$ (see [15] for a formal definition). For example, if the matrix Hk was always the same for all $\\mathbf{k}$ , then we would get little information about $\\mathbf{k}$ from $\\mathbf{Hk}$ , and we would say H has a low rate. Conversely, if $\\mathbf{H}\\mathbf{k}$ is different for different $\\mathbf{k}$ then the product $\\mathbf{Hk}$ provides more information about k. Information theoretically, maximizing the rate of $\\mathbf{H}$ can be thought of as maximizing the entropy [37] of Hk. It is this connection between rate and the randomness of the product Hk that enables us to prove Lemma 3.1. ", "page_idx": 5}, {"type": "text", "text": "A detailed discussion of Lemma 3.1 is in Appendix C.2, with an enhanced version for independence across multiple $\\mathbf{H}$ , as is required for our overall result. The proof of this lemma touches many areas of mathematics, including the theory of monoids, information theory, and optimal group testing. ", "page_idx": 5}, {"type": "text", "text": "3.2 Singleton Detection and Identification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Singletons are useful, but we cannot immediately use them to recover $F(\\mathbf{k})$ . We first need to know that a given $U(\\mathbf{j})$ is a singleton. Secondly, we need to identify the value of $\\mathbf{k}$ that singleton corresponds to. Section 4 discusses both tasks. For now, we discuss the rest of the algorithm assuming that we can accomplish both tasks. ", "page_idx": 5}, {"type": "text", "text": "3.3 Message Passing to Resolve Collisions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Since we don\u2019t know the non-zero indices beforehand, collisions between multiple non-zero indices in the same aliasing set are inevitable. These are called multitons. One approach to deal with these multitons is to repeat the procedure over again: ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{c}(\\ell)=f(\\mathbf{m}_{c,\\ell})\\iff U_{c}(\\mathbf{j})=\\sum_{\\mathbf{k}\\in A_{c}(\\mathbf{j})}F(\\mathbf{k}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "glGeXu1zG4/tmp/7e306d6c4dc03e7548b4715ae629592fcd16d8baa80f31ce8835c681390a2c36.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: Depiction of our peeling message passing algorithm for the samples in Fig. 3. The singleton in $U_{2}(01)$ is subtracted (peeled) so we can resolve $F(\\mathbf{k}_{2})$ from $U_{1}(11)$ . ", "page_idx": 5}, {"type": "text", "text": "$c=1,\\ldots,C$ . Each time, we get different aliasing sets $\\mathbf{\\nabla}A_{c}(\\mathbf{j})$ resulting in different singletons, and thus find different $\\mathbf{k}$ with non-zero $F(\\mathbf{k})$ . While this approach works, a better approach is to combine this idea with message passing to use known non-zero indices and values $(\\mathbf{k},\\bar{F}(\\mathbf{k}))$ to resolve these multitons and turn them into singletons. The type of message passing algorithm we use is called graph peeling. The aliasing structure can be represented as a bipartite graph. Each $U_{c}(\\mathbf{j})$ is a check node, and each non-zero coefficient $F(\\mathbf{k})$ is a variable node. The variable node $F(\\mathbf{k})$ is connected to the check node $U_{c}(\\mathbf{j})$ if $\\mathbf{H}_{c}\\mathbf{k}=\\mathbf{j}.$ . Fig. 4 constructs this bipartite graph for the aliasing in Fig. 3. Note that $U_{1}(11)=F(\\mathbf{k}_{2})+F(\\mathbf{k}_{4})$ is a multiton; however, in the other sub-samping group $U_{2}(01)=$ $F(\\mathbf{k}_{4})$ is a singleton. Once we resolve $U_{2}(01)$ , we can simply subtract $F(\\mathbf{k}_{4})$ from $U_{1}(11)$ , allowing us to create a new singleton, and extract $F(\\mathbf{k}_{2})$ . The remaining values of $F$ both appear as singletons in the first sampling group, so we can resolve all 4 non-zero interactions $F$ with only 8 (7 unique) samples. Peeling algorithms were popularized in information and coding theory for decoding fountain codes [38] and have since been applied to variety of applications in communications [39] and signal processing [13, 40]. They can be analyzed using density evolution theory [41], which we also use as part of our proof. ", "page_idx": 6}, {"type": "text", "text": "4 Singleton Detection and Identification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We have discussed how to subsample efficiently to maximize singletons and how to use message passing to recover as many coefficients as possible. Now we discuss (1) how to identify singletons and (2) how to determine the $\\mathbf{k}^{*}$ corresponding to the singleton. The following result is key: ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.1. Consider $\\mathbf{H}\\in\\mathbb{Z}_{2}^{b\\times n}$ , and $f:\\mathbb{Z}_{2}^{n}\\mapsto\\mathbb{R},$ , and some $\\mathbf{d}\\in\\mathbb{Z}_{2}^{n}$ . If $U$ is the Mo\u00a8bius transform of $u$ , and is the M\u00a8obius transform of $f$ we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\nu(\\ell)=f\\left(\\overline{{\\mathbf{H}^{T}\\overline{{\\ell}}+\\mathbf{d}}}\\right)\\,\\Longleftrightarrow\\,U(\\mathbf{j})=\\sum_{\\mathbf{k}\\leq\\overline{{\\mathbf{d}}}\\ \\mathbf{s}.\\mathbf{t}.\\ \\mathbf{H}\\mathbf{k}=\\mathbf{j}}F(\\mathbf{k}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof can be found in Appendix C.4. The form of (6) allows us to shrink the aliasing set in a controlled way. Define $\\mathbf{d}_{c,0}:=\\mathbf{0}_{n}$ , and ${\\bf D}_{c}\\in\\mathbb{Z}_{2}^{P\\times n}$ for some $P>0$ . The $i^{\\mathrm{th}}$ row of $\\mathbf{D}_{c}$ is denoted $\\mathbf{d}_{c,p}$ , $p=1\\ldots,P$ . Using these vectors, we construct $C(P+1)$ different subsampled functions $u_{c,p}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nu_{c,p}(\\ell)=f\\left(\\overline{{\\mathbf{H}_{c}^{\\mathrm{T}}\\overline{{\\ell}}+\\mathbf{d}_{c,p}}}\\right),\\,\\forall\\ell\\in\\mathbb{Z}_{2}^{b}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We compute the Mo\u00a8bius transform of each $u_{c,p}$ denoted by $U_{c,p}$ and construct a vector-valued function $\\mathbf{U}_{c}(\\mathbf{j}):=[U_{c,0}(\\mathbf{j}),\\dotsc,U_{c,P}(\\mathbf{j})]^{\\mathrm{T}}$ . The goal of singleton detection is to identify when $\\mathbf{U}_{c}(\\mathbf{j})$ reduces to a single term, and for what value $\\mathbf{k}$ that term corresponds to. To do so, we define the Type $(\\cdot)$ : ", "page_idx": 6}, {"type": "text", "text": "1. Type $(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{Z}$ denotes a zeroton, for which there does not exist $F(\\mathbf{k})\\neq0$ s.t. $\\mathbf{H}\\mathbf{k}=\\mathbf{j}$ .   \n2. Type $(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{S}(\\mathbf{k},F(\\mathbf{k}))$ denotes a singleton with only one $\\mathbf{k}$ with $F(\\mathbf{k})\\neq0$ s.t. $\\mathbf{H}\\mathbf{k}=\\mathbf{j}$ .   \n3. Type $(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{M}$ denotes a multiton with more than one $\\mathbf{k}$ with $F(\\mathbf{k})\\neq0$ s.t. $\\mathbf{H}\\mathbf{k}=\\mathbf{j}$ . ", "page_idx": 6}, {"type": "text", "text": "To describe our type estimation rule, we define the following ratios between elements of $\\mathbf{U}_{c}(\\mathbf{j})$ ", "page_idx": 6}, {"type": "equation", "text": "$$\ny_{c,p}:=1-\\frac{U_{c,p}(\\mathbf{j})}{U_{c,0}(\\mathbf{j})},\\;\\;p=1,\\ldots,P,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and construct the vector $\\mathbf y_{c}:=[y_{c,1},\\dotsc,y_{c,P}]^{\\mathrm T}$ . Then, our estimate for the type is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Detect}\\left(\\mathbf{U}_{c}(\\mathbf{j})\\right):=\\left\\{\\begin{array}{l l}{\\mathcal{H}_{Z},}&{\\mathbf{U}_{c}(\\mathbf{j})=\\mathbf{0}}\\\\ {\\mathcal{H}_{M},}&{\\mathbf{y}_{c}\\notin\\{0,1\\}^{P}}\\\\ {\\mathcal{H}_{S}(\\mathbf{k},F(\\mathbf{k})),}&{\\mathbf{y}_{c}\\in\\{0,1\\}^{P}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By considering the definition of $\\mathbf{U}_{c}$ it is possible to show that if Typ $?\\left(\\mathbf{U}_{c}(\\mathbf{j})\\right)=\\mathcal{H}_{S}(\\mathbf{k}^{*},F(\\mathbf{k}^{*}))$ , then $\\mathbf{y}_{c}=\\mathbf{D}_{c}\\mathbf{k}^{*}$ . When we have a singleton taking $\\mathbf{D}_{c}=\\mathbf{I}$ , and thus $P=n$ suffices to recover $\\mathbf{k}^{*}$ So long as the non-zero coefficients, $F(\\mathbf{k})$ are not chosen in an adversarial way, this choice of $\\mathbf{D}_{c}$ also ensures that Detect $(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathrm{Type}\\left(\\mathbf{U}_{c}(\\mathbf{j})\\right)$ . For the purposes of our formal proof, we will assume that non-zero $F(\\mathbf{k})$ are drawn from an absolutely continuous joint distribution. We can\u2019t do better if we don\u2019t have any extra information about $\\mathbf{k}^{*}$ , but we can if we know $|\\mathbf{k}^{*}|\\leq t$ as we show below. Going back to our example in Fig. 3, with $\\mathbf{D}_{c}=\\mathbf{I}$ we use a total of $8\\times7=56$ samples as opposed to $2^{6}\\ =64$ . While this improvement is modest at this scale, for larger problems the improvement is dramatic. ", "page_idx": 6}, {"type": "text", "text": "Singleton Identification in the Low Degree Setting Let\u2019s say we want to determine the singleton from $U_{1}(10)$ in Fig. 3, and we know $|\\mathbf{k}^{*}|\\leq1$ . By exploiting group testing, it is possible to design $\\mathbf{D}_{c}$ with fewer rows, and thus fewer overall measurements: ", "page_idx": 7}, {"type": "table", "img_path": "glGeXu1zG4/tmp/e9eced9316dfa698aa904cb6f1ab7eb72cc2879b3ce889127b491a9d6cfaa937.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: We use group testing [16] to identify the singleton $\\mathbf{k}_{1}$ , which corresponds to the first-order term \u201cnever\u201d. By designing the masking patterns with the help of group testing, we can efficiently recover interactions with ${\\bar{O}}(t\\log(n))$ extra measurements. ", "page_idx": 7}, {"type": "text", "text": "The matrix product $\\mathbf{D}_{c}\\mathbf{k}^{*}$ has a different output for each $|\\mathbf{k}|\\leq1$ . In this case, the result y corresponds to the binary index of the location of the 1. It requires $P=3$ , rather than the $P=6$ for $\\mathbf{D}_{c}=\\mathbf{I}$ . If all non-zero $F(\\mathbf{k})$ had satisfied $|\\mathbf{k}|\\leq1$ , we could use this matrix for our example in Fig. 3. However, we only have $|\\mathbf{k}|\\leq3$ for non-zero $F(\\mathbf{k})$ in this example, so $\\mathbf{D}_{c}$ as in (5) does not suffice. In the case of general $|\\mathbf{k}|\\leq t$ , [42] says that for any scaling of $t$ with $n$ , there exists a group testing design $\\mathbf{D}_{c}$ with $P=O(t\\log(n))$ that can recover $\\mathbf{k}^{*}$ in the limit as $n\\to\\infty$ with vanishing error in $\\mathrm{poly}(n)$ time. If we also assume that $F(\\mathbf{k})$ are Detect $(\\mathbf{U}_{c}(\\mathbf{j}))$ has vanishing error (see Appendix C.7.2). ", "page_idx": 7}, {"type": "text", "text": "Extension to Noisy Setting We now relax the assumption that most of the coefficients are exactly zero. To do this, we assume each subsampled Mo\u00a8bius coefficient is corrupted by noise: ", "page_idx": 7}, {"type": "equation", "text": "$$\nU_{c,p}(\\mathbf{j})=\\sum_{\\mathbf{k}\\leq\\overline{{\\mathbf{d}}}_{p}\\ \\mathrm{~s.t.\\,}\\mathbf{H}_{c}\\mathbf{k}=\\mathbf{j}}F(\\mathbf{k})+Z_{c,p}(\\mathbf{j}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $Z_{c,p}(\\mathbf{j})\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma^{2})$ . There are two main changes that must be made compared to the noiseless case. First, we must place an assumption on the magnitude of non-zero coefficients $|F(\\mathbf{k}_{i})|$ , such that the signal-to-noise ratio (SNR) remains fixed. Secondly, the matrix $\\mathbf{D}_{c}$ must be modified. It now consists of two parts: $\\mathbf{D}_{c}=[\\mathbf{D}_{c}^{1};\\mathbf{D}_{c}^{2}]$ . We design $\\mathbf{D}_{c}^{2}\\in\\mathbb{Z}_{2}^{P_{2}\\times n}$ as a standard noise robust Bernoulli group testing matrix with $P_{2}=O(t\\log(n))$ tests, which suffices for singleton identification under any fixed SNR [43]. However, unlike the noiseless case, the samples from the rows of $\\mathbf{D}_{c}^{2}$ are not enough to ensure a vanishing error for singleton detection in the Detect $(\\cdot)$ procedure. To solve this, we design $\\mathbf{D}_{c}^{1}\\in\\mathbb{Z}_{2}^{P_{1}\\times n}$ as a Bernoulli group testing matrix with a different parameter. In Appendix C.7.4, we show this modified version of Detect $(\\cdot)$ has vanishing error if $P_{1}=O(t\\log(n))$ . ", "page_idx": 7}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Now that we have discussed all components of the algorithm, we present our theoretical guarantees: ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1. (Recovery with $K$ Uniform Interactions) Let $f$ satisfy Assumption 2.1 for some $K=$ $O(2^{n\\delta})$ with $\\delta\\leq{\\frac{1}{3}}$ and let the non-zero coefficients of $F$ be drawn from an absolutely continuous distribution. For $\\{\\mathbf{H}_{c}\\}_{c=1}^{C}$ chosen as in Lemma C.3 with $b\\,=\\,O(\\log(K))$ , $C\\,=\\,3$ and $\\mathbf{D}_{c}=\\mathbf{I}$ Algorithm $^{\\,l}$ exactly computes the transform $F$ in $O(K n)$ samples and $O(K n^{2})$ time complexity with probability at least $1-O(1/K)$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2. (Noise-Robust Recovery with $K$ $t$ -Degree Interactions) Let $f$ satisfy Assumption 2.2 for $K=O(\\mathrm{poly}(n))$ and $t=\\Theta(n^{\\alpha})$ with $\\alpha\\leq0.409.$ . Assume either: ", "page_idx": 7}, {"type": "text", "text": "1. The non-zero coefficients of $F$ are drawn from an arbitrary continuous distribution, or   \n2. $U_{c,p}$ is corrupted by noise as in (10) and let non-zero coefficients satisfy $|F(\\mathbf{k})|=\\rho$ . ", "page_idx": 7}, {"type": "text", "text": "Then, for $\\{\\mathbf{H}_{c}\\}_{c=1}^{C}$ chosen as in Lemma C.4 with $b=O(\\log(K))$ , $C=3$ , and $\\mathbf{D}_{c}$ chosen as a suitable group testing matrix, Algorithm $^{\\,l}$ exactly computes the transform $F$ in $O(K t\\log(n))$ samples and $O(K\\,\\mathrm{poly}(n))$ time complexity with probability at least $1-O(1/K)$ in both the noiseless case $(I)$ and noisy case (2). ", "page_idx": 7}, {"type": "image", "img_path": "glGeXu1zG4/tmp/7bfc294d79360d5a601589716e0254bb8f26ac41ec7e61ec9a4a7ff9444fb6f3.jpg", "img_caption": ["Figure 6: (a) Perfect reconstruction against $n$ and sample complexity under Assumption 2.1. Holding $C=3$ , we scale $b$ to increase the sample complexity. We observe that the number of samples required to achieve perfect reconstruction is scaling linearly in $n$ as predicted. Results are plotted across 5 runs for each choice of $b$ and $n$ . (b) Plot of the noise-robust version of our algorithm. For various values of $t$ , we set $n=500$ and $K=500$ , using a group testing matrix with $P=1000$ . We plot the performance of our algorithm against SNR, measured in terms of the $R^{2}$ . Error bands represent the standard deviation over 10 runs. (c) Runtime comparison of SMT, SHAP-IQ [29], and $t=5$ order FSI via LASSO [4]. All are computing the Mo\u00a8bius transform in the setting where all non-zero interactions are order $t$ , $K=10$ . SMT easily outperforms both, while the other methods quickly become intractable. Error bands represent standard deviation over 10 runs. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "glGeXu1zG4/tmp/174e4c17cf2bff5509479c606c95575eb179c731d049ce0b89bdbbc2021e9e60.jpg", "img_caption": ["Figure 7: Since our ultimate goal is compact, meaningful and computable representations, we compare representations generated from SMT (Algorithm 1) with other popular explanation models. We plot $R^{2}$ (faithfulness) vs. the number of terms used in the representation (sparsity). For Shapley and Banzhaf values, to generate an $r$ -sparse representation, we use the top $r$ magnitude values. For SMT and Faith-Banzhaf, we do a slightly more sophisticated refinement procedure. Faith-Banzhaf is included because it is the first-order metric that maximizes $R^{2}$ . As observed in the breast cancer and sentiment analysis tasks, SMT can achieve better $R^{2}$ than other approaches by utilizing higher-order interactions. In the sentence-level multiple choice dataset, we observe less of a difference, since in those cases the entire answer to a question is usually contained in a single sentence, thus higher-order interactions provide little advantage. Error bands represent the standard deviation over 10 instances. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The proof of Theorem 5.1 and 5.2 is provided in Appendix C.5. It combines results from all of the parts of the algorithm we have discussed: aliasing, singleton detection and graph peeling. The requirement $|F({\\bf k})|=\\rho$ is only due to limitations of group testing theory. In practice, we observe that a lower bound on the magnitude suffices. In addition to our theoretical results, we also conduct numerical experiments on synthetic and real word models, which are discussed below. ", "page_idx": 8}, {"type": "text", "text": "5.1 Synthetic Simulations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We tested SMT\u2019s efficacy on functions satisfying Assumption 2.1 and 2.2, setting non-zero $F(\\mathbf{k})$ uniformly in $[-1,1]$ . SMT is implemented as in Algorithm 1, with group testing decoding via linear programming (see Appendix F.2). Fig. 6a is a phase transition plot that shows the percent of cases where SMT achieves $\\bar{R}^{2}=1$ with fixed $K=100$ at different sample complexities and values of $n$ . We vastly outperform the naive approach: when $n=1000$ , we get perfect reconstruction with only $10^{-294}$ percent of total samples! Furthermore, the number of samples necessary to achieve perfect reconstruction scales linearly in $n$ as predicted. Fig. 6b assesses SMT under noise for various values of $t$ , plotting $R^{2}$ against SNR with $K=500$ , $n=500$ , and $P=1000$ . Fig. 6c plots the runtime for SMT and competing methods. Test functions $f$ have $K=10$ non-zero M\u00a8obius coefficients at locations that satisfy $|\\mathbf{k}|=5$ (restricted to equality due to limitations in the SHAP-IQ code at the time of running). We compare against SHAP-IQ [29] configured to compute $5^{\\mathrm{th}}$ order FSI, as well as the method of [4] which computes $5^{\\mathrm{th}}$ order FSI via LASSO. As shown in Appendix A, the $t^{\\mathrm{th}}$ order FSI are exactly the $t^{\\mathrm{th}}$ order Mo\u00a8bius coefficients for our chosen $f$ . This figure exemplifies that SMT is the sole feasible method for identifying interactions on the scale of $n\\ge100$ . Additional simulations and discussion can be found in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.2 Real-World Models ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our objective is a computable, faithful, and compact representation of real-world machine-learned functions. Fig. 7 addresses this goal head-on, by plotting $R^{2}$ against the number of terms used in the representation (sparsity) for SMT and other popular model explanation approaches. We consider three different tasks: The first is an XGboost model for breast cancer diagnosis, and the other two are transformer-based BERT models for the tasks of sentiment analysis and multiple choice question answering respectively. Appendix B discusses the setup in great detail. For Shapley and Banzhaf values, to generate an $r$ -sparse representation, we use the top $r$ magnitude values. For SMT and FaithBanzhaf, we do a slightly more sophisticated refinement procedure using LASSO [44], described in the Appendix. We observe that for the breast cancer and sentiment analysis tasks, SMT can generate representations that, with the same number of terms, achieve a much higher $R^{2}$ . This is done by identifying interactions between inputs that are important to the model output. Interestingly, in the case of the multiple choice model, there is less of a difference between the Faith-Banzhaf Indices and the SMT representations. This is likely because in the corresponding dataset, answers to the questions are usually contained in single sentences, making interactions less important. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Identifying interactions between inputs is an important open research question in machine learning, with applications to explainability, data valuation, and many other problems. We approached this problem by studying the Mo\u00a8bius transform, which is a representation over the fundamental interaction basis. We introduced several new tools to the problem of identifying interactions. The use of ideas from sparse signal processing and group testing has allowed SMT to operate in regimes where all other methods fail due to computational burden. Our theoretical results guarantee asymptotic exact reconstruction and are complemented by numerical simulations that show SMT performs well with finite parameters and also under a noisy model. ", "page_idx": 9}, {"type": "text", "text": "Limitations Our assumption of independently sampled interactions was made for informationtheoretic hardness and may not hold in some settings where correlated interactions exist. For instance, in the sentiment problem in Fig. 1, words with strong $2^{\\mathrm{nd}}$ order interactions are likely to appear together in important $3^{\\mathrm{rd}}$ order interactions. In such settings, correlation is exploitable, so a more specific algorithm can likely exploit this correlation and eliminate this assumption. Another limitation is that we have focused on taking a sparse M\u00a8obius transform in this work. In practice, we may be more interested in taking a sparse projection onto a subset of low-order terms. For unitary transforms, projection can be achieved by truncation, however, with non-orthogonal transforms like the Mo\u00a8bius Transform, projection is not straightforward. This is an important distinction, because there can be functions which are well-approximated by a sparse low degree Mo\u00a8bius projection, but do not have a sparse transform. ", "page_idx": 9}, {"type": "text", "text": "Future Work Applying SMT to real-world tasks like understanding protein language models [45], LLM chatbots [46] or diffusion models [47] would be insightful. Working with large and complicated models will likely require further improvements to robustness\u2014both in terms of dealing with noise from small but non-zero interactions and dealing with potential correlations between interactions. Some interesting ideas in this direction could be using more standard statistical ideas like in [29] or considering concepts from adaptive group testing. Finally, due to the connection with the Shapley or Banzhaf values (2), methods for computing the M\u00a8obius transform can also be used for computing these values by first computing the Mo\u00a8bius transform and then using (2). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] L. S. Shapley, A Value for N-Person Games. Santa Monica, CA: RAND Corporation, 1952.   \n[2] S. M. Lundberg and S.-I. Lee, \u201cA unified approach to interpreting model predictions,\u201d in Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS\u201917. Red Hook, NY, USA: Curran Associates Inc., 2017, p. 4768\u20134777. [3] M. Sundararajan, K. Dhamdhere, and A. Agarwal, \u201cThe Shapley Taylor interaction index,\u201d in International Conference on Machine Learning, Jul 2020, pp. 9259\u20139268. [Online]. Available: https://proceedings.mlr.press/v119/sundararajan20a.html [4] C.-P. Tsai, C.-K. Yeh, and P. Ravikumar, \u201cFaith-shap: The faithful Shapley interaction index,\u201d Journal of Machine Learning Research, vol. 24, no. 94, pp. 1\u201342, 2023.   \n[5] M. Grabisch, J.-L. Marichal, and M. Roubens, \u201cEquivalent Representations of Set Functions,\u201d Mathematics of Operations Research, vol. 25, no. 2, pp. 157\u2013178, May 2000. [Online]. Available: https://pubsonline.informs.org/doi/10.1287/moor.25.2.157.12225   \n[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional transformers for language understanding,\u201d in North American Chapter of the Association for Computational Linguistics, 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID: 52967399   \n[7] J. Lee, \u201cIMDB Finetuned BERT-base-uncased,\u201d 2023, accessed Jan 2024. [Online]. Available: https://huggingface.co/JiaqiLee/imdb-finetuned-bert-base-uncased   \n[8] J. M. Pe\u00b4rez, J. C. Giudici, and F. Luque, \u201cpysentimiento: A python toolkit for sentiment analysis and SocialNLP tasks,\u201d 2021.   \n[9] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Re\u00b4, \u201cFlashattention: Fast and memory-efficient exact attention with IO-awareness,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 16 344\u201316 359, 2022.   \n[10] H. Hassanieh, P. Indyk, D. Katabi, and E. Price, \u201cSimple and practical algorithm for sparse Fourier transform,\u201d in SIAM Symposium on Discrete Algorithms (SODA), 2012, pp. 1183\u20131194. [Online]. Available: https://epubs.siam.org/doi/abs/10.1137/1.9781611973099.93   \n[11] P. Stobbe and A. Krause, \u201cLearning Fourier sparse set functions,\u201d in International Conference on Artificial Intelligence and Statistics (AISTATS), ser. Proceedings of Machine Learning Research, La Palma, Canary Islands, Apr 2012, pp. 1125\u20131133. [Online]. Available: https://proceedings.mlr.press/v22/stobbe12.html   \n[12] S. Pawar and K. Ramchandran, \u201cComputing a $k$ -sparse $n$ -length Discrete Fourier Transform using at most $4k$ samples and $O(k l o g k)$ complexity,\u201d in IEEE International Symposium on Information Theory (ISIT), 2013, pp. 464\u2013468.   \n[13] X. Li, J. K. Bradley, S. Pawar, and K. Ramchandran, \u201cThe SPRIGHT algorithm for robust sparse Hadamard Transforms,\u201d in IEEE International Symposium on Information Theory (ISIT), 2014, pp. 1857\u20131861.   \n[14] A. Amrollahi, A. Zandieh, M. Kapralov, and A. Krause, \u201cEfficiently learning fourier sparse set functions,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019.   \n[15] M. Aldridge, O. Johnson, and J. Scarlett, \u201cGroup testing: An information theory perspective,\u201d Foundations and Trends\u00ae in Communications and Information Theory, vol. 15, no. 3\u20134, p. 196\u2013392, 2019. [Online]. Available: http://dx.doi.org/10.1561/0100000099   \n[16] R. Dorfman, \u201cThe detection of defective members of large populations,\u201d The Annals of mathematical statistics, vol. 14, no. 4, pp. 436\u2013440, 1943.   \n[17] Y. Zhou, U. Porwal, C. Zhang, H. Q. Ngo, X. Nguyen, C. R\u00b4e, and V. Govindaraju, \u201cParallel feature selection inspired by group testing,\u201d in Advances in Neural Information Processing Systems, vol. 27, 2014.   \n[18] R. Jia, D. Dao, B. Wang, F. A. Hubis, N. Hynes, N. M. G\u00a8urel, B. Li, C. Zhang, D. Song, and C. J. Spanos, \u201cTowards efficient data valuation based on the shapley value,\u201d in International Conference on Artificial Intelligence and Statistics (AISTATS), vol. 89, 16\u201318 Apr 2019, pp. 1167\u20131176. [Online]. Available: https://proceedings.mlr.press/v89/jia19a.html   \n[19] J. C. Harsanyi, \u201cA bargaining model for the cooperative $n$ -person game,\u201d Ph.D. dissertation, Department of Economics, Stanford University, Stanford, CA, USA, 1958.   \n[20] C. Wendler, A. Amrollahi, B. Seifert, A. Krause, and M. P\u00a8uschel, \u201cLearning set functions that are sparse in non-orthogonal Fourier bases,\u201d in AAAI Conference on Artificial Intelligence, vol. 35, 2021, pp. 10 283\u201310 292.   \n[21] Q. Ren, J. Gao, W. Shen, and Q. Zhang, \u201cWhere we have arrived in proving the emergence of sparse interaction primitives in DNNs,\u201d in International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id $\\equiv$ 3pWSL8My6B   \n[22] K. Aas, M. Jullum, and A. L\u00f8land, \u201cExplaining individual predictions when features are dependent: More accurate approximations to Shapley values,\u201d Artificial Intelligence, vol. 298, p. 103502, 2021.   \n[23] H. Chen, J. D. Janizek, S. Lundberg, and S.-I. Lee, \u201cTrue to the model or true to the data?\u201d 2020. [Online]. Available: https://arxiv.org/pdf/2006.16234   \n[24] D. Janzing, L. Minorics, and P. Bl\u00a8obaum, \u201cFeature relevance quantification in explainable AI: A causal problem,\u201d in International Conference on Artificial Intelligence and Statistics, 2020, pp. 2907\u20132916.   \n[25] J. Ren, Z. Zhou, Q. Chen, and Q. Zhang, \u201cCan we faithfully represent absence states to compute shapley values on a DNN?\u201d in International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id $\\equiv$ YV8tP7bW6Kt   \n[26] J. Enouen, H. Nakhost, S. Ebrahimi, S. O. Arik, Y. Liu, and T. Pfister, \u201cTextGenSHAP: Scalable post-hoc explanations in text generation with long documents,\u201d 2023. [Online]. Available: https://arxiv.org/pdf/2312.01279   \n[27] V. Miglani, A. Yang, A. Markosyan, D. Garcia-Olano, and N. Kokhlikyan, \u201cUsing Captum to explain generative language models,\u201d in Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), Singapore, Dec. 2023, pp. 165\u2013173. [Online]. Available: https://aclanthology.org/2023.nlposs-1.19   \n[28] L. M. Paes, D. Wei, H. J. Do, H. Strobelt, R. Luss, A. Dhurandhar, M. Nagireddy, K. N. Ramamurthy, P. Sattigeri, W. Geyer et al., \u201cMulti-level explanations for generative language models,\u201d 2024. [Online]. Available: https://arxiv.org/pdf/2403.14459   \n[29] F. Fumagalli, M. Muschalik, P. Kolpaczki, E. H\u00a8ullermeier, and B. E. Hammer, \u201cSHAP-IQ: Unified approximation of any-order shapley interactions,\u201d in Conference on Neural Information Processing Systems, 2023. [Online]. Available: https://openreview.net/forum?id=IEMLNF4gK4   \n[30] J. S. Kang, R. Pedarsani, and K. Ramchandran, \u201cThe fair value of data under heterogeneous privacy constraints in federated learning,\u201d Transactions on Machine Learning Research, 2024. [Online]. Available: https://openreview.net/forum?id=ynG5Ak7n7Q   \n[31] J. T. Wang and R. Jia, \u201cData Banzhaf: A robust data valuation framework for machine learning,\u201d in International Conference on Artificial Intelligence and Statistics, vol. 206, 25\u201327 Apr 2023, pp. 6388\u20136421. [Online]. Available: https://proceedings.mlr.press/v206/wang23e.html   \n[32] A. Ghorbani and J. Zou, \u201cData Shapley: Equitable valuation of data for machine learning,\u201d in International Conference on Machine Learning, vol. 97, 09\u201315 Jun 2019, pp. 2242\u20132251. [Online]. Available: https://proceedings.mlr.press/v97/ghorbani19c.html   \n[33] A. Ghorbani, M. Kim, and J. Zou, \u201cA distributional framework for data valuation,\u201d in International Conference on Machine Learning, vol. 119, 13\u201318 Jul 2020, pp. 3535\u20133544. [Online]. Available: https://proceedings.mlr.press/v119/ghorbani20a.html   \n[34] T. Chen and C. Guestrin, \u201cXGBoost: A scalable tree boosting system,\u201d in ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, 2016, pp. 785\u2013794. [Online]. Available: http://doi.acm.org/10.1145/2939672.2939785   \n[35] A. Barbosa, \u201cRoberta large fine tuned on race,\u201d 2021. [Online]. Available: https://huggingface. co/LIAMF-USP/roberta-large-finetuned-race/tree/main   \n[36] Q. Ren, Y. Xu, J. Zhang, Y. Xin, D. Liu, and Q. Zhang, \u201cTowards the dynamics of a DNN learning symbolic interactions,\u201d 2024. [Online]. Available: https://arxiv.org/pdf/2407.19198   \n[37] T. M. Cover, Elements of Information Theory. John Wiley & Sons, 1999.   \n[38] M. Luby, \u201cLT codes,\u201d in IEEE Symposium on Foundations of Computer Science, 2002, pp. 271\u2013 280.   \n[39] K. R. Narayanan and H. D. Pfister, \u201cIterative collision resolution for slotted aloha: An optimal uncoordinated transmission policy,\u201d in International Symposium on Turbo Codes and Iterative Information Processing (ISTC), 2012, pp. 136\u2013139.   \n[40] Y. E. Erginbas, J. Kang, A. Aghazadeh, and K. Ramchandran, \u201cEfficiently computing sparse fourier transforms of q-ary functions,\u201d in IEEE International Symposium on Information Theory (ISIT), 2023, pp. 513\u2013518.   \n[41] S.-Y. Chung, T. Richardson, and R. Urbanke, \u201cAnalysis of sum-product decoding of low-density parity-check codes using a gaussian approximation,\u201d IEEE Transactions on Information Theory, vol. 47, no. 2, pp. 657\u2013670, 2001.   \n[42] W. H. Bay, J. Scarlett, and E. Price, \u201cOptimal non-adaptive probabilistic group testing in general sparsity regimes,\u201d Information and Inference: A Journal of the IMA, vol. 11, no. 3, pp. 1037\u20131053, 02 2022. [Online]. Available: https://doi.org/10.1093/imaiai/iaab020   \n[43] J. Scarlett and O. Johnson, \u201cNoisy non-adaptive group testing: A (near-)definite defectives approach,\u201d IEEE Transactions on Information Theory, vol. 66, no. 6, pp. 3775\u20133797, 2020.   \n[44] R. Tibshirani, \u201cRegression shrinkage and selection via the lasso,\u201d Journal of the Royal Statistical Society Series B: Statistical Methodology, vol. 58, no. 1, pp. 267\u2013288, 1996.   \n[45] Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, R. Verkuil, O. Kabeli, Y. Shmueli, A. dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido, and A. Rives, \u201cEvolutionary-scale prediction of atomic-level protein structure with a language model,\u201d Science, vol. 379, no. 6637, pp. 1123\u20131130, 2023. [Online]. Available: https://www.science.org/doi/abs/10.1126/science. ade2574   \n[46] OpenAI, \u201cGPT-4 technical report,\u201d 2023.   \n[47] D. Kingma, T. Salimans, B. Poole, and J. Ho, \u201cVariational diffusion models,\u201d in Advances in Neural Information Processing Systems, vol. 34, 2021, pp. 21 696\u201321 707.   \n[48] P. L. Hammer and R. Holzman, \u201cApproximations of pseudo-boolean functions; applications to game theory,\u201d Zeitschrift f\u00a8ur Operations Research, vol. 36, no. 1, pp. 3\u201321, 1992. [Online]. Available: https://doi.org/10.1007/BF01541028   \n[49] W. Wolberg, O. Mangasarian, N. Street, and W. Street, \u201cBreast Cancer Wisconsin (Diagnostic),\u201d UCI Machine Learning Repository, 1995, DOI: https://doi.org/10.24432/C5DW2B.   \n[50] D. Q. Nguyen, T. Vu, and A. T. Nguyen, \u201cBERTweet: A pre-trained language model for English Tweets,\u201d in Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020, pp. 9\u201314.   \n[51] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, \u201cLearning word vectors for sentiment analysis,\u201d in Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Portland, Oregon, USA: Association for Computational Linguistics, June 2011, pp. 142\u2013150. [Online]. Available: http://www.aclweb.org/anthology/ P11-1015   \n[52] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, \u201cRACE: Large-scale ReAding comprehension dataset from examinations,\u201d in Conference on Empirical Methods in Natural Language Processing, Copenhagen, Denmark, Sep. 2017, pp. 785\u2013794. [Online]. Available: https: //aclanthology.org/D17-1082   \n[53] A. Coja-Oghlan, O. Gebhard, M. Hahn-Klimroth, and P. Loick, \u201cInformation-theoretic and algorithmic thresholds for group testing,\u201d IEEE Transactions on Information Theory, vol. 66, no. 12, pp. 7911\u20137928, 2020.   \n[54] C. L. Chan, S. Jaggi, V. Saligrama, and S. Agnihotri, \u201cNon-adaptive group testing: Explicit bounds and novel algorithms,\u201d IEEE Transactions on Information Theory, vol. 60, no. 5, pp. 3019\u20133035, 2014. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Relationship between M\u00a8obius Transform and Other Importance Metrics ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We begin with some notation. We define the Mo\u00a8bius basis function (which are all possible products of inputs) as: ", "page_idx": 13}, {"type": "equation", "text": "$$\nb_{\\mathbf{k}}(\\mathbf{m}):=\\prod_{i:k_{i}=1}m_{i}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now we define the following sub-spaces of pseudo-Boolean function in terms of the linear span of Mo\u00a8bius basis functions: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}_{t}:=\\operatorname{span}\\{b_{\\mathbf{k}}(\\mathbf{m}):|\\mathbf{k}|\\leq t\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now we define the projection operator $\\operatorname{Proj}_{\\mu}(f,\\mathcal{D})$ , as the projection of the function $f$ onto the function space $\\mathcal{D}$ with respect to the measure $\\mu$ . If $g({\\bf m})=\\mathrm{Proj}_{\\mu}(f,\\mathcal{D})$ , we write its decomposition as $\\begin{array}{r}{g(\\mathbf{m})=\\sum_{\\mathbf{k}\\in\\mathbb{Z}_{2}^{n}}c(f,\\mathcal{D},\\mu,\\mathbf{k})b_{\\mathbf{k}}(\\mathbf{m})}\\end{array}$ . Note that linear independence implies the uniqueness of this representat ion. ", "page_idx": 13}, {"type": "text", "text": "Shapley Value The Shapley values $\\mathrm{SV}(i)$ [1] of the inputs $m_{i},i=1,\\ldots,n$ with respect to the function $f$ are [48]: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{SV}(i)=c(f,\\mathcal{M}_{1},\\sigma,\\mathbf{e}_{i})=\\sum_{\\mathbf{k}:k_{i}=1}\\frac{1}{|\\mathbf{k}|}F(\\mathbf{k}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\sigma$ is the Shapley kernel. $\\mathrm{SV}(i)=F(\\mathbf{e}_{i})$ when $f$ is a linear function. ", "page_idx": 13}, {"type": "text", "text": "Banzhaf Index The Banzhaf index $\\mathrm{BZ}(i)$ of the inputs $m_{i},i=1,\\ldots,n$ with respect to the function $f$ are [48]: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{BZ}(i)=c(f,\\mathcal{M}_{1},\\mu,\\mathbf{e}_{i})=\\sum_{\\mathbf{k}:k_{i}=1}\\frac{1}{2^{|\\mathbf{k}|-1}}F(\\mathbf{k}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mu$ is the uniform measure. $\\mathrm{BZ}(i)=F(\\mathbf{e}_{i})$ when $f$ is a linear function. ", "page_idx": 13}, {"type": "text", "text": "Faith Shapley Interaction Index The $t^{\\mathrm{th}}$ order Faith Shapley interaction index $\\operatorname{SV}_{t}(\\mathbf{k})$ for $|\\mathbf{k}|\\leq t$ [4] is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{SV}_{t}(\\mathbf{k})=c(f,\\mathcal{M}_{t},\\sigma,\\mathbf{k})=F(\\mathbf{k})+(-1)^{t-|\\mathbf{k}|}\\frac{|\\mathbf{k}|}{t+|\\mathbf{k}|}{\\binom{t}{|\\mathbf{k}|}}\\sum_{\\stackrel{\\mathbf{p}>\\mathbf{k}}{|\\mathbf{p}|>t}}F(\\mathbf{p}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\sigma$ is the Shapley kernel. $\\operatorname{SV}_{t}(\\mathbf{k})=F(\\mathbf{k})$ when $f$ is a $t^{\\mathrm{th}}$ order function, i.e., $F(\\mathbf{k})=0$ when $|\\mathbf{k}|>t$ . ", "page_idx": 13}, {"type": "text", "text": "Faith Banzhaf Interaction Index The $t^{\\mathrm{th}}$ order Faith Shapley interaction index $\\mathrm{BZ}_{t}({\\bf k})$ for $|\\mathbf{k}|\\leq t$ [4] is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{BZ}_{t}(\\mathbf{k})=c(f,\\mathcal{M}_{t},\\mu,\\mathbf{k})=F(\\mathbf{k})+(-1)^{t-|\\mathbf{k}|}\\sum_{\\mathbf{p}>\\mathbf{k}}\\frac{1}{2^{|\\mathbf{p}|-|\\mathbf{k}|}}\\binom{|\\mathbf{p}|-|\\mathbf{k}|-1}{t-|\\mathbf{k}|}F(\\mathbf{p}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mu$ is the uniform measure. $\\mathrm{BZ}_{t}(i)=F(\\mathbf{k})$ when $f$ is a $t^{\\mathrm{th}}$ order function, i.e., $F(\\mathbf{k})=0$ when $|\\mathbf{k}|>t$ . ", "page_idx": 13}, {"type": "text", "text": "Shapley-Taylor Interaction Index The $t^{t h}$ order Shapley-Taylor Interaction Index [3] $\\mathrm{STII}_{t}(\\mathbf{k})$ is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{STII}_{t}({\\bf k})=\\left\\{{\\cal F}({\\bf k})\\begin{array}{l l}{{\\vert{\\bf k}\\vert<t}}\\\\ {{c(f-f^{t-1},\\mathcal{M}_{t}-\\mathcal{M}_{t-1},\\sigma,{\\bf k})}}&{{\\vert{\\bf k}\\vert=t,}}\\end{array}}\\right.\\;\\;{\\cal f}^{t-1}({\\bf m})=\\sum_{{\\bf k}\\leq{\\bf m}}{\\cal F}({\\bf k}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\sigma$ is the Shapley kernel. Explicitly, it can be shown that: ", "page_idx": 13}, {"type": "equation", "text": "$$\nc(f-f^{t-1},\\mathcal{M}_{t}-\\mathcal{M}_{t-1},\\sigma,\\mathbf{k})=\\sum_{\\mathbf{k}\\leq\\mathbf{k}^{\\prime}}{\\binom{|\\mathbf{k}^{\\prime}|}{t}}^{-1}F(\\mathbf{k}^{\\prime})\\;\\mathrm{for}\\;\\left|\\mathbf{k}\\right|=t.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As a consequence of the above, we have $\\mathrm{STII}_{t}(\\mathbf{k})=\\mathrm{SV}_{t}(\\mathbf{k})=F(\\mathbf{k})$ when $f$ is a $t^{t h}$ order function, i.e., $F(\\mathbf{k})=0$ for $|\\mathbf{k}|>t$ . ", "page_idx": 13}, {"type": "text", "text": "B Experiment Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $f$ be the real-world function we wish to explain. In subsections B.1, B.2, and B.3, we describe how we formed these functions for the tasks of breast cancer diagnosis, sentiment analysis, and multiple choice answering respectively. For our experiments, we plot the $R^{2}$ (faithfulness) for a variety of explanation models $\\hat{f}$ , measured through: ", "page_idx": 14}, {"type": "equation", "text": "$$\nR^{2}=1-\\frac{\\left\\|\\displaystyle\\hat{f}-f\\right\\|_{2}^{2}}{\\left\\|f-\\overline{{f}}\\right\\|_{2}^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we use the notation $\\begin{array}{r}{\\|f\\|_{2}^{2}=\\sum_{\\mathbf{m}\\in\\mathbb{Z}_{2}^{n}}f(\\mathbf{m})^{2}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "In Figure 2, we consider settings where $n\\approx20$ , such that we can run optimization procedures to find faithful approximations that are sparse and low degree. ", "page_idx": 14}, {"type": "text", "text": "Achievable Low Degree: To find the best approximation $\\hat{f}$ of up to degree $t$ , we solve the following quadratic programming problem: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\hat{f},\\alpha}{\\operatorname*{min}}\\left\\|\\hat{f}-f\\right\\|_{2}^{2}}\\\\ &{\\mathrm{~s.t.~}\\ \\ \\hat{f}(\\mathbf{m})=\\underset{\\mathbf{k}\\leq\\mathbf{m},\\|\\mathbf{k}\\|\\leq t}{\\sum}\\alpha_{\\mathbf{k}},\\forall\\mathbf{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Achievable Sparsity: On the other hand, we cannot efficiently find the optimal faithful $K$ -sparse approximation due to the problem\u2019s combinatorial nature. Instead, informed by the strong faithfulness of low degree approximations, we employ the following heuristic to obtain some sparse approximation. ", "page_idx": 14}, {"type": "text", "text": "Let $S_{K}\\subseteq\\mathbb{Z}_{2}^{n}$ be a set containing the first $K$ coordinates with the lowest degree, where ties are randomly broken. With this set, we solve the following quadratic programming problem: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\hat{f},\\alpha}{\\operatorname*{min}}\\left\\|\\hat{f}-f\\right\\|_{2}^{2}}&{}\\\\ {\\mathrm{~s.t.~}\\ \\ \\hat{f}(\\mathbf{m})=\\underset{\\mathbf{k}\\leq\\mathbf{m},\\mathbf{k}\\in S_{K}}{\\sum}\\alpha_{\\mathbf{k}},\\forall\\mathbf{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In Figure 7, we consider the four explanation models described below. ", "page_idx": 14}, {"type": "text", "text": "Shapley Values: We approximate Shapley values by iterating through permutations of the inputs [2]. For an efficient implementation of the algorithm, we use the SHAP Python package [2]. To measure the faithfulness captured by Shapley values at some sparsity level $r$ , we consider approximations that only include the top- $^r$ Shapley values by magnitude. ", "page_idx": 14}, {"type": "text", "text": "Banzhaf Values: We approximate Banzhaf values using the Maximum Sample Reuse Monte Carlo procedure described in [31]. To measure the faithfulness captured by Banzhaf values at some sparsity level $r$ , we consider approximations that only include the top- $^r$ Banzhaf values by magnitude. ", "page_idx": 14}, {"type": "text", "text": "Faith-Banzhaf Indices: We calculate Faith-Banzhaf indices using the regression formulation described in [4]. To measure the faithfulness captured by sparse approximations of Faith-Banzhaf indices, we modify the regression problem by adding an $\\ell_{1}$ penalty on the values of the Faith-Banzhaf indices. We vary the penalty coefficient to obtain different levels of sparsity. ", "page_idx": 14}, {"type": "text", "text": "SMT: We run SMT (Algorithm 1) to obtain a sparse Mo\u00a8bius representation $\\hat{F}$ with support $\\operatorname{supp}(\\hat{F})$ . Then, we fine-tune the values of the coefficients by solving the following regression problem over a uniformly sampled set of points $\\mathcal{D}\\subseteq\\mathbb{Z}_{2}^{n}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\hat{f},\\alpha}\\ \\sum_{\\mathbf{m}\\in\\mathcal{D}}\\left(\\hat{f}(\\mathbf{m})-f(\\mathbf{m})\\right)^{2}}\\\\ &{\\mathrm{s.t.}\\quad\\hat{f}(\\mathbf{m})=\\displaystyle\\sum_{\\mathbf{k}\\leq\\mathbf{m},\\mathbf{k}\\in\\mathrm{supp}(\\hat{F})}\\alpha_{\\mathbf{k}},\\forall\\mathbf{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To measure the faithfulness captured by sparse approximations, we modify the regression problem by adding an $\\ell_{1}$ penalty on the values of the Mo\u00a8bius coefficients. Then, we vary the penalty coefficient $\\lambda$ to obtain different levels of sparsity: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\hat{f},\\alpha}\\,\\sum_{\\mathbf{m}\\in\\mathcal{D}}(\\hat{f}(\\mathbf{m})-f(\\mathbf{m}))^{2}+\\lambda\\sum_{\\mathbf{k}\\in\\mathrm{supp}(\\hat{F})}\\vert\\alpha_{\\mathbf{k}}\\vert}\\\\ &{\\displaystyle\\mathrm{s.t.}\\quad\\hat{f}(\\mathbf{m})=\\sum_{\\mathbf{k}\\le\\mathbf{m},\\mathbf{k}\\in\\mathrm{supp}(\\hat{F})}\\alpha_{\\mathbf{k}},\\forall\\mathbf{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.1 XGBoost for Breast Cancer Diagnosis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We train an XGBoost model for classification using the Wisconsin Breast Cancer dataset [49]. This dataset contains the mean, standard deviation, and largest value of ten measurements, resulting in thirty features. For Figure 2, we use only the mean and standard deviation, resulting in twenty features. For Figure 2, we use the first ten data points in the training set and for Figure 7, we present the aggregated results over the first twenty. ", "page_idx": 15}, {"type": "text", "text": "To explain the XGBoost model $h$ (the probability associated with a positive classification) on each data point $x\\,\\in\\,{\\mathcal{X}}$ , we use an interventional expected value formulation: we freeze some of the features and take an expectation over all data points by infilling the remaining features. Formally, ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\mathbf{m})=\\mathbb{E}\\left[h(X)|\\mathrm{do}(X_{\\mathbf{m}}=x_{\\mathbf{m}})\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we use the notation $x_{\\mathbf{m}}=\\{x_{i}:m_{i}=1\\}$ . ", "page_idx": 15}, {"type": "text", "text": "B.2 BERT for Sentiment Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We employ the sentiment analysis model from [8], which is built upon BERTweet [50], a RoBERTa model trained on English tweets. We take movie reviews from the IMDb Movie Reviews dataset [51]. For a particular review, we define its function as a mapping from maskings of words (using the [UNK] token) to the model\u2019s logit value associated to the correct sentiment classification. ", "page_idx": 15}, {"type": "text", "text": "For Figure 2, we use the first ten sentences in the dataset with 17, 18, or 19 words, where words separated through spaces in the review. Below, we include the reviews and their low degree and sparse approximations calculated with equations 20 and 22 respectively. ", "page_idx": 15}, {"type": "image", "img_path": "glGeXu1zG4/tmp/bf6a0c7a6b7d73dd4cba0a31d9eb8719872ff63f9a91222513d9e9eed38c2127.jpg", "img_caption": ["In Figure 7, we take a random sampling of reviews, with number of words spanning from 17 to 38. The reviews we used, alongside their word counts and sentiment, are included below: "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.3 BERT for Multiple Choice ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For multiple choice answering, we use a RoBERTa model [35] fine-tuned on RACE [52]: a largescale reading comprehension dataset. This dataset contains over 28,000 passages, each containing ", "page_idx": 15}, {"type": "image", "img_path": "glGeXu1zG4/tmp/7485010a3924eb22966795a93c24e183a647f11f69fa09ba2f64abf9af571343.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "corresponding multiple-choice questions. For our experiments, we found the first ten passages with 15 sentences, and took their first multiple-choice question. ", "page_idx": 16}, {"type": "text", "text": "To construct the function, we consider sentence-level maskings of the passages using the [PAD] token. We pass the masked passage, alongside the multiple choice question into the RoBERTa model, and measure the logit value of the question\u2019s correct answer. ", "page_idx": 16}, {"type": "table", "img_path": "glGeXu1zG4/tmp/d27a826a24a11685a926bb8321e3ce3ba783a8f285de4a556d76a7062b55223d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Missing Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Boolean Arithmetic ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 1 the addition and multiplication table for arithmetic between $x,y\\in\\mathbb{Z}_{2}$ . We also note that $\\mathbb{Z}_{2}$ is typically used to refer to the integer ring modulo 2. The arithmetic we are describing here is actually that of a monoid. Since the audience for this paper is people interested in machine learning, we continue to use $\\mathbb{Z}_{2}$ since it is commonly used to simply refer to the set $\\{0,1\\}$ . ", "page_idx": 17}, {"type": "table", "img_path": "glGeXu1zG4/tmp/6298dd0f1c150fc6f9eb9980064732b1f3b3f30aa8087155308b9faaec040e29.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 1: Addition, Multiplication and Subtraction table for Boolean arithmetic in this paper. Subtraction is for $y-x$ . ", "page_idx": 17}, {"type": "text", "text": "C.2 Discussion of Aliasing of the Mo\u00a8bius Transform ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "When a function has many small or zero Mobius coefficients (interactions), our goal is to subsample (4) in such a way that the aliasing causes the non-zero coefficients to end up in different aliasing sets (5) (as opposed to all of them being aliased together, making them more difficult to reconstruct). Lemma C.1 is a key tool that we will use in this work to design subsampling patterns that result in good aliasing patterns. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.1. Consider $\\mathbf{H}\\in\\mathbb{Z}_{2}^{b\\times n}$ , $b<n$ and $f:\\mathbb{Z}_{2}^{n}\\mapsto\\mathbb{R}$ . Let ", "page_idx": 17}, {"type": "equation", "text": "$$\nu(\\ell)=f\\left(\\overline{{\\mathbf{H}^{T}\\overline{{\\ell}}}}\\right),\\,\\forall\\ell\\in\\mathbb{Z}_{2}^{b}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If $U$ is the Mobius transform of $u$ , and $F$ is the Mobius transform of $f$ we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\nU(\\mathbf{j})=\\sum_{\\mathbf{H}\\mathbf{k}=\\mathbf{j}}F(\\mathbf{k}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This lemma is a powerful tool, allowing us to control the aliasing sets through the matrix $\\mathbf{H}$ . The proof can be found in Appendix C.3, and is straightforward, given the relationship between $u$ and ", "page_idx": 17}, {"type": "text", "text": "$f$ . Understanding why we choose this relationship, however, is more complicated. Underlying this choice is the algebraic theory of monoids and abstract algebra. ", "page_idx": 18}, {"type": "text", "text": "As we have mentioned, our ultimate goal is to design $\\mathbf{H}$ to sufficiently \u201cspread out\u201d the non-zero indices among the aliasing sets. Below, we define a simple and useful construction for $\\mathbf{H}$ . ", "page_idx": 18}, {"type": "text", "text": "Definition C.2. Consider $\\{i_{1},\\ldots,i_{b}\\}=I\\subset[n]$ , with $|I|=b$ , and $\\mathbf{H}\\in\\mathbb{Z}_{2}^{b\\times n}$ . Let $\\mathbf{h}_{i}$ correspond to the $i^{\\mathrm{th}}$ row of $\\mathbf{H}$ , given by $\\mathbf{h}_{i}=\\mathbf{e}_{i_{j}}$ , the length $n$ unit vector in coordinate $i_{j}$ . Then if we subsample according to (23) we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\nU(\\mathbf{j})=\\sum_{\\mathbf{k}:\\:k_{i}=j_{i}}\\!\\!\\!\\!\\sum_{\\forall i\\in I}\\!\\!\\!\\!F(\\mathbf{k}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which happens to result in aliasing sets $A(\\mathbf{j})=\\{\\mathbf{k}:k_{i}=j_{i}\\;\\forall i\\in I\\}$ all of equal size $2^{b}$ . The above choice $\\mathbf{H}$ actually induces a rather simple sampling procedure when we follow (23). For instance if $I=[b]$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\nu(\\ell)=f\\left(\\left[\\overline{{\\ell}};\\mathbf{1}_{n-b}\\right]\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In other words, in this case, we construct samples by freezing $n-b$ of the inputs to 1 and then varying the remaining $b$ inputs across all the $2^{b}$ possible options. In the case where the non-zero Mobius interactions are chosen uniformly at random, this construction does a good job at spacing them out across the various aliasing sets. The following result formalizes this. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.3. (Uniform interactions) Let $\\mathbf{k}_{1},\\ldots,\\mathbf{k}_{K}$ be sampled uniformly at random from $\\mathbb{Z}_{2}^{n}$ , where $F(\\mathbf{k}_{i})\\neq0$ , $\\forall i\\in[K]$ , but $F(\\mathbf{k})=0$ for all other $\\mathbf{k}\\in\\mathbb{Z}_{2}^{n}$ . Construct disjoint sets $I_{c}\\subset[n]$ for $c=1,\\ldots,C$ , and the corresponding matrix $\\mathbf{H}_{c}$ according to Definition C.2. Let $\\mathbf{\\nabla}A_{c}(\\mathbf{j})$ correspond to the aliasing sets after sampling with respect to matrix $\\mathbf{H}_{c}$ . Now define: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\textbf{j}s u c h\\;t h a t\\;\\mathbf{k}_{i}\\in\\mathcal{A}_{c}(\\mathbf{j}):=\\mathbf{j}_{i}^{c}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then if $b=O(\\log(K))$ , $K=O(2^{n/C})$ , in the limit as $n\\to\\infty$ with $C=O(1),\\mathbf{j}_{i}^{c}$ are mutually independent and uniformly distributed over $\\mathbb{Z}_{2}^{b}$ . ", "page_idx": 18}, {"type": "text", "text": "The proof is given in Appendix C.6.1, and follows directly from the form of the aliasing sets $\\mathbf{\\nabla}A_{c}(\\mathbf{j})$ . Corollary C.3 means that using $\\mathbf{H}$ as constructed in Definition C.2 ensures that we all $\\mathbf{k}$ with $F(\\mathbf{k})\\neq$ 0 are uniformly distributed over the aliasing sets, which maximizes the number of singletons. This result, however, hinges on the fact that the non-zero coefficients are uniformly distributed. We are also interested in the case where the non-zero coefficients are all low degree. In order to induce a uniform distribution in this case, we need to exploit a group testing matrix. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.4. (low degree interactions) Let $\\mathbf{k}_{1},\\ldots,\\mathbf{k}_{K}$ be sampled uniformly at random from $\\{\\mathbf{k}:$ $|\\mathbf{k}|\\leq t,\\mathbf{k}\\in\\mathbb{Z}_{2}^{n}\\Bigr\\}$ , where $F(\\mathbf{k}_{i})\\neq0$ , $\\forall i\\in[K]$ , but $F(\\mathbf{k})=0$ for all other $\\mathbf{k}\\in\\mathbb{Z}_{2}^{n}$ . By constructing $C$ matrices $\\mathbf{H}_{c}$ $\\mathbf{I}_{c},c=1,\\dots,C$ from rows of a near constant column weight group testing matrix, and sampling as in (23), $i f t=\\Theta(n^{\\alpha})$ for $\\alpha<0.409$ , and $b=O(\\log(K))$ , $\\bar{K^{\\stackrel{*}{}}}=\\bar{O}(n^{t})$ , in the limit as $n\\to\\infty$ , $\\mathbf{j}_{i}^{c}$ as defined in (27) are mutually independent and uniformly distributed over $\\mathbb{Z}_{2}^{b}$ . ", "page_idx": 18}, {"type": "text", "text": "The proof is given in Appendix C.6.2. It relies on an information theoretic argument, exploiting a result from optimal group testing [53]. ", "page_idx": 18}, {"type": "text", "text": "C.3 Proof of Lemma C.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Taking the Mobius transform of $u$ gives us: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{U(\\mathbf{k})}&{=\\underset{\\varepsilon\\leq t}{\\sum}(-1)^{\\varepsilon^{\\top}(1-\\varepsilon-\\varepsilon)}u(\\ell)}\\\\ &{=\\underset{\\varepsilon\\leq t}{\\sum}(-1)^{\\varepsilon^{\\top}(1-\\varepsilon-\\varepsilon)}f\\left(\\underset{\\varepsilon\\leq t}{\\sum}\\frac{\\mathbb{1}_{i}}{\\mathbb{1}_{i}}\\right)}\\\\ &{=\\underset{\\varepsilon\\leq k}{\\sum}(-1)^{\\varepsilon^{\\top}(1-\\varepsilon-\\varepsilon)}\\underset{\\varepsilon\\leq0}{\\sum}\\underset{\\varepsilon_{k}\\leq t}{\\sum}F(\\mathbf{r})}\\\\ &{=\\underset{\\varepsilon\\leq t}{\\sum}(-1)^{\\varepsilon^{\\top}(1-\\varepsilon-\\varepsilon)}\\underset{\\varepsilon\\leq t}{\\sum}(\\ell\\leq\\mathbf{k})\\underset{\\varepsilon\\leq t}{\\sum}F(\\mathbf{r})\\left\\{\\underset{\\varepsilon\\leq t}{\\leq}(\\Omega)\\frac{\\mathbb{1}_{i}}{\\mathbb{1}_{i}}\\right\\}}\\\\ &{=\\underset{\\varepsilon\\leq t}{\\sum}F(\\mathbf{r})\\left(\\underset{\\varepsilon\\leq t}{\\sum}(-1)^{\\varepsilon^{\\top}(1-\\varepsilon-\\varepsilon)}\\{\\ell\\leq\\mathbf{k}\\}\\left\\{r\\leq\\underset{\\varepsilon,\\tau_{0}=0}{\\sum}\\frac{\\mathbb{1}_{i}}{\\mathbb{1}_{i}}\\right\\}\\right)}\\\\ &{=\\underset{\\varepsilon\\leq t}{\\sum}F(\\mathbf{r})F(\\mathbf{r})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now let\u2019s just focus on the term in the parenthesis for now, which we have called $I({\\bf r})$ . ", "page_idx": 19}, {"type": "text", "text": "Case 1: $\\mathbf{H}\\mathbf{r}=\\mathbf{k}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{I(\\mathbf r)}&{=}&{\\displaystyle\\sum_{\\ell\\leq\\mathbf k}(-1)^{\\mathbb1^{T}(\\mathbf k-\\ell)}\\mathbb1\\left\\{\\mathbf r\\leq\\bigodot_{i:\\ell_{i}=0}\\overline{{\\mathbf h}}_{i}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "First note that under this condition, $\\ell=\\mathbf{k}\\implies\\mathbf{r}\\leq\\odot_{i:\\ell_{i}=0}\\overline{{\\mathbf{h}}}_{i}$ . To see this, note that $k_{j}=0\\implies$ ${\\bf r}\\le\\overline{{\\bf h}}_{j}$ . Since this holds for all $j$ such that $k_{j}=0$ , we have the previously mentioned implication. Conversely, if $\\ell_{j}<k_{j}$ (this means $\\ell_{j}=0\\,\\mathsf{A N D}\\,k_{j}=1)$ ) for some $j$ , then $\\mathbf{r}$ and $\\mathbf{h}_{j}$ must overlap. Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{1}\\left\\{\\mathbf{r}\\leq\\overline{{\\mathbf{h}}}_{j}\\right\\}=0\\implies\\mathbb{1}\\left\\{\\mathbf{r}\\leq\\bigodot_{i:\\ell_{i}=0}\\overline{{\\mathbf{h}}}_{i}\\right\\}=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can split $I(\\mathbf{r})$ into two parts, the part where $\\ell=\\mathbf{k}$ and the part where $\\ell<\\mathbf{k}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r c l l}{I(\\mathbf{r})}&{=}&{1\\left\\{\\mathbf{r}\\leq\\displaystyle\\sum_{i:\\mathbf{k}_{i}=0}{\\overline{{\\mathbf{h}}}}_{i}\\right\\}+\\displaystyle\\sum_{\\ell<\\mathbf{k}}(-1)^{\\mathbf{I}^{T}(\\mathbf{k}-\\ell)}\\mathbb{1}\\left\\{\\mathbf{r}\\leq\\displaystyle\\sum_{i:\\ell_{i}=0}{\\overline{{\\mathbf{h}}}}_{i}\\right\\}}&{\\left(\\mathbf{Hr}=\\mathbf{k}\\right)}\\\\ &{=}&{1+\\displaystyle\\sum_{\\ell<\\mathbf{k}}0}\\\\ &{=}&{1}\\end{array}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Case 2: $\\mathbf{H}\\mathbf{r}\\neq\\mathbf{k}$ Let $\\mathbf{H}\\mathbf{r}=\\mathbf{k}^{\\prime}\\neq\\mathbf{k}$ . This case itself will be broken into two parts. First let\u2019s say there is some $j$ such that $k_{j}\\,=\\,0$ and $k_{j}^{\\prime}\\,=\\,1$ . Since $k_{j}^{\\prime}\\,=\\,1$ we know that $\\bar{\\mathbb{1}}\\left\\{\\mathbf{r}\\leq\\overline{{\\mathbf{h}}}_{j}\\right\\}=0$ . Furthermore, since $\\forall\\ell\\in\\{\\ell:\\ell\\leq\\mathbf{k}\\}$ we have $\\ell_{j}=0$ . Then by a similar argument to our previous one, we have 1 $\\left\\{{\\bf r}\\leq\\odot_{i:\\ell_{i}=0}\\overline{{{\\bf h}}}_{i}\\right\\}=0\\,\\forall\\ell\\leq{\\bf k}$ . It follows immediately that $I(\\mathbf{r})=0$ in this case. ", "page_idx": 19}, {"type": "text", "text": "Finally, we have the case where $\\mathbf{k}^{\\prime}<\\mathbf{k}$ . First, if there is a coordinate $j$ such that $0=\\ell_{j}<k_{j}^{\\prime}=1$ , we know that 1 $\\left\\{\\mathbf{r}\\leq\\overline{{\\mathbf{h}}}_{j}\\right\\}=0$ so we have 1 $\\left\\{\\mathbf{r}\\leq\\bigodot_{i:\\ell_{i}=0}\\overline{{\\mathbf{h}}}_{i}\\right\\}=0\\;\\forall\\ell$ s.t. $\\exists j,\\ell_{j}<k_{j}^{\\prime}$ . The only $\\ell$ that remain are those such that ${\\bf k}^{\\prime}\\le\\ell\\le{\\bf k}$ . It is easy to see that this is a sufficient condition for $\\begin{array}{r}{\\mathbb{1}\\left\\{\\mathbf{r}\\leq\\bigcirc_{i:\\ell_{i}=0}\\overline{{\\mathbf{h}}}_{i}\\right\\}=1}\\end{array}$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{I(\\mathbf{r})}&{=}&{\\displaystyle\\sum_{\\ell\\leq\\mathbf{k}}(-1)^{\\mathbb{I}^{T}(\\mathbf{k}-\\ell)}\\mathbb{1}\\left\\{\\mathbf{r}\\leq\\bigoplus_{i:\\ell_{i}=0}\\overline{{\\mathbf{h}}}_{i}\\right\\}}\\\\ &{=}&{\\displaystyle\\sum_{\\mathbf{k}^{\\prime}\\leq\\ell\\leq\\mathbf{k}}(-1)^{\\mathbb{I}^{T}(\\mathbf{k}-\\ell)}}\\\\ &{=}&{0}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Where the final sum is zero because exactly half of the $\\ell$ have even and odd parity respectively. Thus, the subsampling pattern becomes: ", "page_idx": 20}, {"type": "equation", "text": "$$\nU(\\mathbf{k})=\\sum_{\\mathbf{H}\\mathbf{r}=\\mathbf{k}}F(\\mathbf{r}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C.4 Proof of Section 4 ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{U(\\mathbf{k})}&{=\\underset{\\underset{k\\in\\mathbb{Z}}{\\sum}}{\\sum}(-1)^{\\tau^{\\mathrm{l}}(\\tau-\\delta)}\\pi(\\ell)}\\\\ &{=\\underset{k\\in\\mathbb{Z}}{\\sum}(-1)^{\\tau^{\\mathrm{l}}(\\tau-\\delta)}f\\left(\\left(\\underset{(k)=0}{\\overset{\\mathbb{E}}{\\prod}}\\frac{\\mathbf{1}_{k}}{\\varepsilon}\\right)\\odot\\vec{\\mathbf{a}}\\right)}\\\\ &{=\\underset{k\\in\\mathbb{Z}}{\\sum}(-1)^{\\tau^{\\mathrm{l}}(\\tau-\\delta)}\\underset{(k)\\in\\mathbb{Z}}{\\sum}\\begin{array}{r l}{F(\\tau)\\left\\{\\mathbf{1}\\in\\widehat{\\mathbf{Z}}\\right\\}}\\\\ &{=\\left.\\underset{k\\in\\mathbb{Z}}{\\sum}(-1)^{\\tau^{\\mathrm{l}}(\\tau-\\delta)}\\underset{(k)=0}{\\sum}\\mathbb{E}\\pi(1)}\\\\ &{=\\underset{\\tau\\in\\mathbb{Z}}{\\sum}(-1)^{\\tau^{\\mathrm{l}}(\\tau-\\delta)}\\mathbb{E}\\pi(1)\\underset{(k)=0}{\\sum}\\mathbb{E}\\pi(1)\\underset{(k)=0}{\\sum}\\mathbb{E}\\pi\\left\\{\\mathbf{1}\\!\\!\\!\\begin{array}{l}{\\mathbf{1}_{k}}\\\\ &{=\\widehat{\\mathbf{a}}}\\\\ &{\\underset{(k)=0}{\\sum}\\mathbb{E}\\pi(1)\\left\\{\\mathbf{1}\\in\\widehat{\\mathbf{Z}}\\right\\}}\\end{array}\\right\\}}\\\\ &{=\\underset{k\\in\\mathbb{Z}}{\\sum}f(\\tau)\\left\\{\\mathbf{1}\\in\\widehat{\\mathbf{Z}}\\right\\}\\left(\\underset{(k)=0}{\\sum}\\mathbb{E}^{\\tau}(\\tau)\\mathbb{I}(\\xi\\leq1)\\left\\{\\underset{(k)=0}{\\leq}\\xi\\sum_{i=0}^{\\infty}\\mathbb{E}\\right\\}\\right)}\\\\ &{=\\underset{k\\in\\mathbb{Z}}{\\sum}f(\\tau)\\left\\{\\mathbf{1}\\in\\widehat{\\mathbf{Z}}\\right\\}\\left(\\tau\\right)}\\\\ &{=\\underset{k\\in\\mathbb{Z}}{\\sum}K(\\tau)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C.5 Proof of Main Theorems ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem 5.1. (Recovery with $K$ Uniform Interactions) Let $f$ satisfy Assumption 2.1 for some $K=$ $O(2^{n\\delta})$ with $\\delta\\leq{\\frac{1}{3}}$ and let the non-zero coefficients of $F$ be drawn from an absolutely continuous distribution. For $\\{\\mathbf{H}_{c}\\}_{c=1}^{C}$ chosen as in Lemma C.3 with $b\\,=\\,O(\\log(K))$ , $C\\,=\\,3$ and $\\mathbf{D}_{c}=\\mathbf{I}_{\\mathrm{~\\!~}}$ Algorithm $^{\\,l}$ exactly computes the transform $F$ in $O(K n)$ samples and $O(K n^{2})$ time complexity with probability at least $1-\\operatorname{\\bar{O}}(1/K)$ . ", "page_idx": 20}, {"type": "text", "text": "Theorem 5.2. (Noise-Robust Recovery with $K$ $t$ -Degree Interactions) Let $f$ satisfy Assumption 2.2 for $K=O(\\mathrm{poly}(n))$ and $t=\\Theta(n^{\\alpha})$ with $\\alpha\\leq0.409.$ . Assume either: ", "page_idx": 20}, {"type": "text", "text": "1. The non-zero coefficients of $F$ are drawn from an arbitrary continuous distribution, or   \n2. $U_{c,p}$ is corrupted by noise as in (10) and let non-zero coefficients satisfy $|F({\\bf k})|=\\rho.$ . ", "page_idx": 20}, {"type": "text", "text": "Then, for $\\{\\mathbf{H}_{c}\\}_{c=1}^{C}$ chosen as in Lemma C.4 with $b=O(\\log(K))$ , $C=3$ , and $\\mathbf{D}_{c}$ chosen as a suitable group testing matrix, Algorithm 1 exactly computes the transform $F$ in $O(K t\\log(n))$ samples and $O(\\bar{K}\\,\\mathrm{po}\\bar{\\mathrm{ly}}(n))$ time complexity with probability at least $1-O(1/K)$ in both the noiseless case $(I)$ and noisy case (2). ", "page_idx": 20}, {"type": "text", "text": "Proof. The first step for proving both Theorem 5.1 and Theorem 5.2 is to show that Algorithm 1 can successfully recover all Mobius coefficients with probability $1-O(1/K)$ under the assumption that we have access to a Detect $(\\mathbf{U}_{c}(\\mathbf{j}))$ function that can output the type Type $(\\mathbf{U}_{c}(\\mathbf{j}))$ for any aliasing set $\\mathbf{U}_{c}(\\mathbf{j})$ . Under this assumption, we use density evolution proof techniques to obtain Theorem C.5 and conclude both theorems. ", "page_idx": 20}, {"type": "text", "text": "Then, to remove this assumption, we need to show that we can process each aliasing set $\\mathbf{U}_{c}(\\mathbf{j})$ correctly, meaning that each bin is correctly identified as a zeroton, singleton, or multiton. Define $\\mathcal{E}$ as the error event where the detector makes a mistake in $O(K)$ peeling iterations. If the error probability satisfies $\\operatorname*{Pr}({\\mathcal{E}})\\leq O(1/K)$ , the probability of failure of the algorithm satisfies ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{F}=\\operatorname*{Pr}\\left(\\widehat{F}\\neq F|\\mathcal{E}^{\\mathrm{c}}\\right)\\operatorname*{Pr}(\\mathcal{E}^{\\mathrm{c}})+\\operatorname*{Pr}\\left(\\widehat{F}\\neq F|\\mathcal{E}\\right)\\operatorname*{Pr}(\\mathcal{E})}\\\\ &{\\quad\\leq\\operatorname*{Pr}\\left(\\widehat{F}\\neq F|\\mathcal{E}^{\\mathrm{c}}\\right)+\\operatorname*{Pr}(\\mathcal{E})}\\\\ &{\\quad=O(1/K).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the following, we describe how we achieve $\\operatorname*{Pr}({\\mathcal{E}})\\leq O(1/K)$ under different scenarios. ", "page_idx": 21}, {"type": "text", "text": "In the case of uniformly distributed interactions without noise, singleton identification and detection can be performed without error as described in Section C.7.1. In the case of interactions with low degree and without noise, singleton identification and detection can be performed with vanishing error as described in Section C.7.2. Lastly, we can perform noisy singleton identification and detection with vanishing error for low degree interactions as described in Section C.7.2. ", "page_idx": 21}, {"type": "text", "text": "C.6 Density Evolution Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The density evolution proof is generally separated into two parts. ", "page_idx": 21}, {"type": "text", "text": "\u2022 We show that with high probability, nearly all of the variable nodes will be resolved. \u2022 We show that with high probability, the graph is a good expander, which ensures that if only a small number are unresolved, the remaining variable nodes will be resolved. ", "page_idx": 21}, {"type": "text", "text": "Whether the decoding succeeds or fails depends entirely on the graph (or rather distribution over graphs) that is induced by the algorithm. The graph ensemble is parameterized as $\\mathcal{G}\\left(\\mathcal{D},\\{\\mathbf{M}_{c}\\}_{c\\in\\left[C\\right]}\\right)$ . $\\mathcal{D}$ is the support distribution. The set of non-zero Mobius coefficients $\\{\\mathbf{r}:\\mathcal{M}[f](\\mathbf{r})\\neq0\\}\\sim\\mathcal{D}$ is drawn from this distribution. In [13], using the arguments above it is shown that if the following conditions hold, the peeling message passing successfully resolves all variable nodes: ", "page_idx": 21}, {"type": "text", "text": "1. In the limit as $n\\to\\infty$ asymptotic check-node degree distribution from an edge perspective converges to that of independent an identically distributed Poisson distribution (shifted by 1). 2. The variable nodes have a constant degree $C\\geq3$ (This is needed for the expander property). 3. The number of check nodes $b$ in each of the $C$ sampling group is such that $2^{b}=O(K)$ . ", "page_idx": 21}, {"type": "text", "text": "Theorem C.5 ([13]). If the above three conditions hold, the peeling decoder recovers all Mobius coefficients with probability $1-O(1/K)$ . ", "page_idx": 21}, {"type": "text", "text": "In the following section, we show that for suitable choice of sampling matrix, these conditions are satisfied, both in the case of uniformly distributed and low degree Mobius coefficients. ", "page_idx": 21}, {"type": "text", "text": "C.6.1 Uniform Distribution ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In order to satisfy the conditions for the case of a uniform distribution of we use the matrix in Corollary C.3. We select $C=3$ different $I_{1},I_{2},I_{3}$ such that $I_{i}\\cap I_{j}=\\emptyset\\ \\ \\forall i\\neq j\\in\\{1,2,3\\}$ . Note that this satisfies condition (2) above. Furthermore, we let $k$ scale as $O(2^{n\\delta})$ . In order to satisfy condition (3), we must have $\\begin{array}{r}{\\delta<\\frac{1}{3}}\\end{array}$ , since each $I_{i}$ can consist of at most $\\begin{array}{l}{{\\frac{1}{3}}}\\end{array}$ of all the coordinates. ", "page_idx": 21}, {"type": "text", "text": "We now introduce some notation. Let $\\mathbf{g}_{j}(\\cdot)$ represent the hash function, that maps a frequency $\\mathbf{r}$ to a check node index $\\mathbf{k}$ in each subsampling group $j=1,\\dots,C$ , i.e., $\\mathbf{g}_{j}(\\mathbf{r})=\\mathbf{H}_{j}\\mathbf{r}$ . Per our assumption, we have $K$ non-zero variable notes $\\mathbf{r}^{(1)},\\ldots,\\mathbf{r}^{(K)}$ chosen uniformly at random. Technically, we are sampling without replacement, however, since $\\frac{K}{2^{n}}\\,\\rightarrow\\,0$ , the probability of selecting a previously selected $\\mathbf{r}^{(i)}$ vanishes. Going forward in this subsection, we will assume that each $\\mathbf{r}_{i}$ is sampled with replacement for a more brief solution. A more careful analysis that deals with sampling with replacement before taking limits yields an identical result. ", "page_idx": 21}, {"type": "text", "text": "First, let\u2019s consider the marginal distribution of $\\mathbf{g}_{j}(\\mathbf{r}^{(i)})$ for some arbitrary $j\\in[C]$ and $i\\,\\in\\,[K]$ . Assuming sampling with replacement, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\mathbf{g}_{j}(\\mathbf{r}^{(i)})=\\mathbf{k}\\right)=\\mathrm{Pr}\\left(\\mathbf{r}_{I_{j}}^{(i)}=\\mathbf{k}\\right)=\\prod_{m\\in I_{j}}\\mathrm{Pr}\\left(r_{m}^{(i)}=k_{m}\\right)=\\frac{1}{2^{b}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, we have established that the our approach induces a uniform marginal distribution over the $2^{b}$ check nodes. Next, we consider the independence of our bins. By assuming sampling with replacement, we can immediately factor our probability mass function. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\bigcap_{i,j}\\mathbf{g}_{j}(\\mathbf{r}^{(i)})=\\mathbf{k}^{(i,j)}\\right)=\\prod_{i}\\operatorname*{Pr}\\left(\\bigcap_{j}\\mathbf{g}_{j}(\\mathbf{r}^{(i)})=\\mathbf{k}^{(i,j)}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Furthermore, since we carefully chose the $I_{i}$ such that they are pairwise disjoint, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\mathrm{Pr}\\left(\\bigcap_{j}\\mathbf{g}_{j}(\\mathbf{r}^{(i)})=\\mathbf{k}^{(i,j)}\\right)=\\mathrm{Pr}\\left(\\bigcap_{j}\\mathbf{r}_{I_{j}}^{(i)}=\\mathbf{k}^{(i,j)}\\right)=\\prod_{j}\\mathrm{Pr}\\left(\\mathbf{r}_{I_{j}}^{(i)}=\\mathbf{k}^{(i,j)}\\right)=\\hfill}\\\\ {\\displaystyle\\prod_{j}\\mathrm{Pr}\\left(\\mathbf{g}_{j}(\\mathbf{r}^{(i)})=\\mathbf{k}^{(i,j)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "establishing independence. Let\u2019s define an inverse load factor $\\begin{array}{r}{\\eta=\\frac{2^{b}}{K}}\\end{array}$ . From a edge perspective, sampling with replacement with independent uniformly distributed gives us: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho_{j}=j\\eta{\\binom{K}{j}}\\left(\\frac{1}{2^{b}}\\right)^{j}\\left(1-\\frac{1}{2^{b}}\\right)^{k-j},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For fixed $\\eta$ , asymptotically as $K\\rightarrow\\infty$ this converges to: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho_{j}\\rightarrow\\frac{(1/\\eta)^{j-1}e^{-1/\\eta}}{(j-1)!}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.6.2 Low Degree Distribution ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For this proof, we take an entirely different approach to the uniform case. We instead exploit the results of Theorem F.1, which is about asymptotically exactly optimal group testing, and then make an information-theoretic argument. Let $\\mathbf{X}^{n}$ be a group testing matrix (constructed either by an i.i.d. Bernoulli design or a constant column weight design using the parameters required for the given $n$ ). We don\u2019t explicitly write the dependence of $\\mathbf{X}^{n}$ on $t$ , since by invoking Theorem F.1, we assume some implicit relationship where $t=\\Theta(n^{\\theta})$ for $\\theta$ satisfying the theorem conditions. Now consider some ${\\bf r}_{n}$ chosen uniformly at random from the $\\binom{n}{t}$ weight $t$ binary vectors. Note that in this work we actually use what is known as the \u201ci.i.d prior\u201d as opposed to the \u201ccombinatorial prior\u201d that we have just defined. As noted in [15], these are actually equivalent, so we can arbitrarily choose to work with one, and the result holds for the other as well. We define: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{Y}^{n}=\\mathbf{X}^{n}\\mathbf{r}^{n}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Furthermore, we define the decoding function $\\operatorname{Dec}_{n}(\\cdot)$ , which represents the deterministic procedure that successfully recovers $\\mathbf{r}$ with vanishing error probability. We have the following bounds on the entropy of $\\mathbf{Y}_{n}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\begin{array}{r c l}{H(\\mathbf{Y}^{n})}&{=}&{H(Y_{1}^{n})+H(Y_{2}^{n}\\mid Y_{1}^{n})+\\cdot\\cdot\\cdot+H(Y_{T}^{n}\\mid Y_{1}^{n},\\cdot\\cdot\\cdot,Y_{T-1}^{n})}\\\\ &{\\leq}&{T,}\\end{array}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we have used the fact that binary random variables have a maximum entropy of 1. Furthermore, by the properties of entropy we also have $H(\\mathbf{Y}^{n})\\geq H(\\operatorname{Dec}(\\mathbf{Y}^{n},\\mathbf{X}^{n})\\mid\\mathbf{X}^{n})$ . Dividing through by $T$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{H(\\operatorname{Dec}(\\mathbf{Y}^{n},\\mathbf{X}^{n})\\mid\\mathbf{X}^{n}))}{T}\\leq\\frac{H(\\mathbf{Y}^{n})}{T}\\leq1.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $\\mathrm{Dec}_{n}(\\mathbf{Y}^{n},\\mathbf{X}^{n}))=\\mathbf{r}^{n}+\\mathrm{err}_{n}(\\mathbf{Y}^{n},\\mathbf{X}^{n})$ . It is known (see [15]) that $\\operatorname*{Pr}(\\operatorname{err}_{n}(\\mathbf{Y}^{n},\\mathbf{X}^{n})\\neq0)=$ $O(\\mathrm{poly}(T)e^{-T})$ . Thus, we can bound the left-hand side as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\frac{H(\\mathrm{Dec}(\\mathbf{Y}^{n},\\mathbf{X}^{n})\\mid\\mathbf{X}^{n})}{T}}&{=}&{\\frac{H(\\mathbf{r}^{n}+\\mathrm{err}_{n}(\\mathbf{Y}^{n},\\mathbf{X}^{n})\\mid\\mathbf{X}^{n})}{T}}\\\\ &{\\ge}&{\\frac{H(\\mathbf{r}^{n})-H(\\mathrm{err}_{n}(\\mathbf{Y}^{n},\\mathbf{X}^{n})\\mid\\mathbf{X}^{n})}{T}}\\\\ &{\\ge}&{\\frac{H(\\mathbf{r}^{n})-H(\\mathrm{err}_{n}(\\mathbf{Y}^{n},\\mathbf{X}^{n}))}{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Where in (45) we have used the bound $H(A+B)\\geq H(A)-H(B)$ and the fact that $\\mathbf{X}^{n}$ and $\\mathbf{r}^{n}$ are independent, and in (46) we have used the fact that conditioning only decreases entropy. By the continuity of entropy and Theorem F.1, we have that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\frac{H(\\mathbf{r}^{n})-H(\\mathrm{err}_{n}(\\mathbf{Y}^{n},\\mathbf{X}^{n}))}{T}=\\operatorname*{lim}_{n\\to\\infty}\\frac{\\log\\binom{n}{t}-\\operatorname*{lim}_{n\\to\\infty}\\frac{H(\\mathrm{err}_{n}(\\mathbf{Y}^{n},\\mathbf{X}^{n}))}{T}=1-0=1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This establishes that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{T(n)}\\sum_{i=1}^{T(n)}H\\left(Y_{i}^{n}\\mid\\mathbf{Y}_{1:(i-1)}^{n}\\right)=1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Unfortunately, this does not immediately imply that all of the summands have a limit of 1, however, it does mean that the fraction of total summands that are less than one goes to zero (it grows as $o(T(n)))$ . Let $G\\subset\\mathbb{N}$ correspond to the set containing all the indicies $i$ of the summands that are equal to 1. By using the fact that conditioning only reduces entropy, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}H\\left(Y_{i}^{n}\\mid\\mathbf{Y}_{S_{i}}^{n}\\right)=1,\\;\\;S_{i}=\\{j<i,j\\in G\\},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we define the countable sequence of random variables: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{Y}_{i}=\\operatorname*{lim}_{n\\to\\infty}Y_{i}^{n},\\;i\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By continuity of entropy, and the above limit and definition, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\nH\\left(\\Bar{Y}_{i}\\mid\\bar{\\mathbf{Y}}_{S_{i}}\\right)=1,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Noting that conditioning only decreases entropy, we immediately have that $\\begin{array}{r}{\\bar{Y}_{i}\\sim\\mathrm{Bern}(\\frac{1}{2})}\\end{array}$ . Now consider some arbitrary finite set $\\cal S^{*}\\subset\\cal G$ . We will now prove that $\\{\\bar{Y}_{i},i~\\in~S^{*}\\}$ is mutually independent. ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $i_{1}<i_{2}<...<i_{|S^{*}|}$ be an ordered indexing of the elements of $S^{*}$ . Furthermore, let $Q_{j}=\\{i_{q}\\mid1\\leq q\\leq j\\}$ . Assume the set $\\{\\bar{Y}_{i},i\\in Q_{j}\\}$ is mutually independent, and use the notation ${\\bf Y}_{Q_{j}}$ to represent a vector containing all of these entries. We have: ", "page_idx": 23}, {"type": "equation", "text": "$$\nH(Y_{i_{j+1}},\\mathbf{Y}_{Q_{j}})=H(\\mathbf{Y}_{Q_{j}})+H(Y_{i_{j+1}}\\mid\\mathbf{Y}_{Q_{j}}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "However, by using the fact that conditioning only decreases entropy we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n1=H(Y_{i_{j+1}}\\mid\\mathbf{Y}_{S_{j+1}})\\leq H(Y_{i_{j+1}}\\mid\\mathbf{Y}_{Q_{j}})\\leq H(Y_{i_{j+1}})\\leq1,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\nH(Y_{i_{j+1}}\\mid\\mathbf{Y}_{Q_{j}})=H(Y_{i_{j+1}})=1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This leads to the following chain of implications: ", "page_idx": 23}, {"type": "equation", "text": "$$\nH(Y_{i_{j+1}},\\mathbf{Y}_{Q_{j}})=H(\\mathbf{Y}_{Q_{j}})+H(Y_{i_{j+1}})\\iff Y_{i_{j+1}}\\perp\\mathbf{Y}_{Q_{j}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From this, and the initial inductive assumption, we can conclude that $\\{\\bar{Y}_{i},i\\in Q_{j+1}\\}$ is mutually independent. The base case of $j=1$ follows from the fact that a set containing just one single random variable is mutually independent. Since $Q_{|S^{*}|}=S^{*}$ the proof is complete. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Now let $L(n)\\,=\\,|G\\cap[n]|$ we know $L\\,=\\,\\Theta(T(n))$ , which follows from the stronger result that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\frac{L(n)}{T(n)}=1}\\end{array}$ = 1. Take b \u2264L(Cn) By leveraging the above results, we can select our subsampling matrices $\\{\\mathbf{H}_{i}\\}_{i=1}^{C}$ from suitable rows of $\\mathbf{X}_{n}$ . Let $S_{1}^{(n)},...\\,,S_{C}^{(n)}\\subset G\\cap[n],\\,\\left|S_{i}^{(n)}\\right|$ $\\left|S_{i}^{(n)}\\right|=b$ and $S_{i}^{(n)}\\cap$ Sj(n)= \u2205. Then take ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{H}_{i}(n)=\\mathbf{X}_{S_{i},:}^{n}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Due to the independence result proved above, the asymptotic degree distribution is: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\rho_{j}\\rightarrow\\frac{(1/\\eta)^{j-1}e^{-1/\\eta}}{(j-1)!}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C.7 Singleton Detection and Identification ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we prove the main results about singleton detection and identification. For the noiseless case, we introduce another assumption: ", "page_idx": 24}, {"type": "text", "text": "Assumption C.6. (No Cancellation) Suppose the non-zero values $F(\\mathbf{k}_{1}),\\ldots,F(\\mathbf{k}_{K})$ are sampled from a joint distribution $\\mathbb{P}$ that satisfies the following condition: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i\\in S}F(\\mathbf{k}_{i})\\neq0,;\\forall S\\subseteq[K],;S\\neq\\varnothing.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This assumption is quite mild, and there are many classes of $\\mathbb{P}$ that satisfy this assumption. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.7. Any absolutely continuous $\\mathbb{P}$ satisfies Assumption C.6 a.s.. ", "page_idx": 24}, {"type": "text", "text": "Proof. For simplicity let $\\mathbf{F}$ represent a $K$ dimensional random vector containing $F(\\mathbf{k}_{i})$ at index $i$ Let the set $\\begin{array}{r}{\\mathcal{R}(\\bar{S},\\alpha)\\stackrel{{}}{=}\\{\\mathbf{F}:\\sum_{i\\in S}F(\\mathbf{k}_{i})=\\alpha\\}}\\end{array}$ . Since $\\mathbb{P}$ is absolutely continuous, a density $p$ exists such that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathbf{F}\\in\\mathcal{R}(S,\\alpha))=\\int_{\\mathcal{R}(s,\\alpha)}d p.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "However, $\\mathrm{dim}(\\mathcal{R}(S,\\alpha))=K-\\left|S\\right|$ . Thus for $S\\neq\\emptyset$ , $\\mathcal{R}(S,\\alpha)$ has Lebesgue measure zero, thus ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\sum_{i\\in S}F(\\mathbf{k}_{i})=0\\right)=\\mathbb{P}(\\mathbf{F}\\in\\mathcal{R}(S,0))=\\int_{\\mathcal{R}(S,0)}d p=0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "C.7.1 Uniform Interactions Singleton Identification and Detection without Noise ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For singleton identification, we observe that in the case of a singleton, $\\mathbf{y}_{c}=\\mathbf{k}^{*}$ , thus, in the case of a singleton ${\\bf k}^{*}$ can be recovered. We still need to show that the Detect function correctly identifies the type. We separate the proof into three parts: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Prove~Detect}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{Z}\\implies\\mathrm{Type}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{Z}.}\\\\ &{\\mathrm{Prove~Detect}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{S}\\implies\\mathrm{Type}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{S}.}\\\\ &{\\mathrm{Prove~Detect}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{M}\\implies\\mathrm{Type}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Consider the subsampling group $\\mathbf{U}_{c}(\\mathbf{j})$ for some fixed $c,\\mathbf{j}$ . We first consider the case where $\\left\\vert\\mathbf{k}\\right\\vert$ is not restricted, and denote the set of non-zero indices $\\mathbf{k}_{1},\\ldots,\\mathbf{k}_{K}$ as $\\mathcal{N}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof of (1) Let Detect $(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{Z}$ , and for contradiction\u2019s sake assume $\\mathrm{Type}(\\mathbf{U}_{c}(\\mathbf{j}))\\neq\\mathcal{H}_{Z}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Detect}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{Z}\\implies\\sum_{\\mathbf{k}\\leq\\bar{\\mathbf{d}}_{p}}\\sum_{\\mathbf{\\mu}_{\\mathrm{s.t.}}\\mathbf{H}_{c}\\mathbf{k}=\\mathbf{j}}F(\\mathbf{k})=0\\,\\forall p,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\mathrm{Type}(\\mathbf{U}_{c}(\\mathbf{j}))\\neq\\mathcal{H}_{Z}$ , we have that $\\mathcal{N}\\cap\\left\\{\\mathbf{k}:\\mathbf{H}_{c}\\mathbf{k}=\\mathbf{j}\\right\\}\\neq\\emptyset$ . Thus, if we consider the above implication for the case of $p=0$ and noting that $\\mathbf{d}_{0}=\\mathbf{0}$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Detect}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{Z}\\implies\\sum_{\\mathcal{N}\\cap\\{\\mathbf{k}:\\mathbf{H}_{c}\\mathbf{k}=\\mathbf{j}\\}}F(\\mathbf{k})=0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "But considering the no cancellation condition, this is impossible, thus proving (1). ", "page_idx": 24}, {"type": "text", "text": "Proof of (2) Note that the converse of (1) is true (the proof is immediate). Thus, proving $\\mathrm{Detect}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{S}\\implies\\neg(\\mathrm{Type}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{M})$ is the same as proving (2). We will again use the method of contradiction, and assume Detect $;(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{S}$ and $\\mathrm{Type}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{M}$ . ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Detect}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{S}\\implies U_{c,p}(\\mathbf{j})\\in\\{0,U_{c,0}(\\mathbf{j})\\}\\:\\forall p>1.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that by our assumption, $U_{c,0}(\\mathbf{j})\\neq0$ . By our assumption that $\\mathrm{Type}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{M}$ , we must have $|N\\cap\\left\\{\\mathbf{k}:\\mathbf{H}_{c}\\mathbf{k}=\\mathbf{\\bar{j}}\\right\\}\\neq\\varnothing|\\ge2$ . Choose $\\mathbf{k}_{1},\\mathbf{k}_{2}\\in\\mathcal{N}\\cap\\left\\{\\mathbf{k}:\\mathbf{H}_{c}\\mathbf{k}=\\mathbf{j}\\right\\}$ . Given our choice of ${\\bf d}_{p}$ ", "page_idx": 24}, {"type": "text", "text": "$\\exists p^{*}>1$ s.t. only \\*one\\* of $\\mathbf{k}_{1}$ or $\\mathbf{k}_{2}\\leq\\bar{\\mathbf{d}}_{p^{*}}$ . Without loss of generality we will assume $\\mathbf{k}_{2}\\leq\\bar{\\mathbf{d}}_{p^{*}}$ and $\\mathbf{k}_{1}\\nleq\\bar{\\mathbf{d}}_{p^{*}}$ . Now, define the following sets: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{I}_{0}}&{=}&{\\mathcal{N}\\cap\\left\\{\\mathbf{k}:\\mathbf{H}_{c}\\mathbf{k}=\\mathbf{j}\\right\\}}\\\\ {\\mathcal{I}_{p^{*}}}&{=}&{\\mathcal{N}\\cap\\left\\{\\mathbf{k}:\\mathbf{H}_{c}\\mathbf{k}=\\mathbf{j}\\;\\mathbf{k}\\leq\\bar{\\mathbf{d}}_{p^{*}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We know ${\\bf k}_{2}\\in\\mathcal{I}_{p^{*}}$ and $\\mathbf{k_{1}}\\in\\mathcal{I}_{0}\\setminus\\mathcal{I}_{p^{*}}$ , thus $|\\mathcal{I}_{p^{*}}|\\geq1$ and $|\\mathcal{I}_{0}\\setminus\\mathcal{I}_{p^{*}}|\\geq1$ . With this, we can show that the implication above cannot be satisfied. ", "page_idx": 25}, {"type": "text", "text": "Case 1: $U_{c,p^{*}}(\\mathbf{j})=0$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\nU_{c,p^{*}}(\\mathbf{j})=0\\implies\\sum_{\\mathbf{k}\\in\\mathcal{I}_{p}^{*}}F(\\mathbf{k})=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\mathcal{I}_{p}^{*}$ is not empty, from our distributional assumption, the above sum cannot be 0. ", "page_idx": 25}, {"type": "text", "text": "Case 2: $U_{c,p^{*}}=U_{c,0}(\\mathbf{j})$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\nU_{c,0}(\\mathbf{j})-U_{c,p^{*}}(\\mathbf{j})=0\\implies\\sum_{\\mathbf{k}\\in\\mathcal{I}_{0}\\backslash\\mathcal{I}_{p}^{*}}F(\\mathbf{k})=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\mathcal{I}_{0}\\setminus\\mathcal{I}_{p}^{*}$ is not empty, from our distributional assumption, the above sum cannot be 0. This implies that $\\dot{\\mathrm{Type}}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{M}$ must be false, thus proving (2). ", "page_idx": 25}, {"type": "text", "text": "Proof of (3): Since we have a converse for (1), a converse for (2) suffices to prove (3). The converse follows below: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Type}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{S}\\implies\\exists\\mathbf{k}^{*}\\mathrm{~s.t.~}U_{c,p}\\in\\{0,F(\\mathbf{k}^{*})\\},\\ \\forall p.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $F(\\mathbf{k}^{*})\\,\\neq\\,0$ , we have $U_{c,0}(\\mathbf{j})\\neq\\,0$ and all entries of $\\mathbf{U}_{c}(\\mathbf{j})$ are either $F(\\mathbf{k}^{*})$ or 0. Thus, Detect $(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{S}$ . ", "page_idx": 25}, {"type": "text", "text": "C.7.2 Low Degree Singleton Identification and Detection without Noise ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Here we include a sketch of the argument for the noiseless part of Theorem 2, which focuses on the case $|\\mathbf{k}|\\leq t$ (the rest of Theorem 2 is unchanged). ", "page_idx": 25}, {"type": "text", "text": "(1) Pro $\\operatorname{ve}\\ D\\mathrm{etect}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{Z}\\implies\\mathrm{Type}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathcal{H}_{Z}.$ $(2+3)$ Prove $\\mathrm{Pr}\\left(\\mathrm{Detect}(\\mathbf{U}_{c}(\\mathbf{j}))=\\mathrm{Type}(\\mathbf{U}_{c}(\\mathbf{j}))\\right)\\to1.$ ", "page_idx": 25}, {"type": "text", "text": "Proof of (1) The proof is identical to above. ", "page_idx": 25}, {"type": "text", "text": "Proof of $_{(2+3)}$ The proof is the same, with one notable exception. In the low degree case, $p^{*}$ may not always exist. In this case, we can simply rely on the result of [54]. Since $\\mathrm{Pr}(\\hat{\\mathbf{k}}\\neq\\mathbf{k}^{*})\\leq n^{-\\beta}$ , we correctly recover each ${\\bf k}^{*}$ in the limit, meaning we must have $\\mathrm{Pr}(\\mathbf{D}_{c}\\mathbf{k}_{1}\\neq\\mathbf{D}_{c}\\mathbf{k}_{2})\\rightarrow1$ (this is equivalent to the existence of $p^{*}$ ). Thus, by the same argument this implies that Detect $(\\mathbf{U}_{c}(\\mathbf{j}))$ ) has vanishing error in the limit. The complete argument requires us to union bound over all of the singleton identifications success, but since $T$ is linear in $\\beta$ , so long as $K=\\mathrm{poly}(n)$ , constant $\\beta$ suffices for vanishing error. ", "page_idx": 25}, {"type": "text", "text": "C.7.3 Singleton Identification in i.i.d. Spectral Noise ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we discuss how to ensure that we can detect the true non-zero index $\\mathbf{r}^{*}$ from the $\\mathbb{Z}_{2}^{P_{1}\\overset{.}{\\times}n}$ .d  Asas minp ltehse,  nuonidseelre tshs ec ia.si.ed, . wne oiwsaen ta stsou cmhpotoisoen .t hiWs em faitrrsitx  dtios cbues sa  tghreo udpe ltaeyst imnga trmixa tirtisxe. lfF, $\\textbf{D}\\in$ purposes of theory, we will choose such that each element is drawn i.i.d. as a $\\textstyle\\operatorname{Bern}\\left({\\frac{\\nu}{t}}\\right)$ for some $\\nu=\\Theta(1)$ . We denote the $i^{t h}$ row of $\\mathbf{D}$ as ${\\bf d}_{i}$ . Each group test is derived from one of the delayed samples. Under the i.i.d. spectral noise model, this means each sample has the form: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{U_{i}(\\mathbf{k})}}&{{=}}&{{\\displaystyle\\sum_{\\mathbf{H}\\mathbf{r}=\\mathbf{k}}F(\\mathbf{r})+Z_{i}(\\mathbf{k})}}\\\\ {{}}&{{}}&{{\\displaystyle\\mathbf{r}\\!\\leq\\!\\overline{{\\mathbf{d}}}_{i}}}\\\\ {{}}&{{=}}&{{F(\\mathbf{r}^{*})\\mathbb{1}\\left\\{\\mathbf{r}^{*}\\leq\\bar{\\mathbf{d}}_{i}\\right\\}+Z_{i}(\\mathbf{k}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $Z_{i}(\\mathbf{k})\\sim\\mathcal{N}\\left(0,\\sigma^{2}\\right)$ . Essentially, we can view this as a hypothesis testing problem, where we have one sample $X$ , and hypothesis and the alternative are: ", "page_idx": 26}, {"type": "equation", "text": "$$\nH_{0}:X=Z\\ \\ \\ H_{1}:X=F(\\mathbf{r^{*}})+Z,\\ \\ Z\\sim{\\mathcal{N}}(0,\\sigma^{2})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Furthermore, lets say the magnitude of $|F[\\mathbf{k}]|=\\rho$ is known. We construct a threshold test: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\varphi(X)=\\mathbb{1}\\left\\{|X|>\\gamma\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With such a test, we can compute the cross-over probabilities: ", "page_idx": 26}, {"type": "equation", "text": "$$\np_{01}=\\operatorname*{Pr}_{H_{0}}\\left(|X|>\\gamma\\right)=2Q(\\gamma/\\sigma),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\np_{10}=\\operatorname*{Pr}_{H_{1}}\\big(|X|<\\gamma\\big)=\\Phi\\big((\\gamma-\\rho)/\\sigma\\big)-\\Phi\\big((-\\gamma-\\rho)/\\sigma\\big).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the sake of simplicity, we will make the choice to choose $\\gamma$ such that $p_{10}=p_{01}$ . In that case, we can numerically solve for the cross-over probability which is fixed for a given signal-to-noise ratio. ", "page_idx": 26}, {"type": "image", "img_path": "glGeXu1zG4/tmp/afae96de9a2c95533c9f12b9b52a1e4b1a57f1b777541262961777707cb4d081.jpg", "img_caption": ["Figure 14: Symmetric cross-over probability induced by hypothesis testing problem for noisy singleton identification/detection. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Now we can use [54], to prove our desired result. let $q$ be the resulting cross-over probability for a given $\\rho/\\sigma$ . The probability that all of our Singleton identifications succeed can computed by a union bound on $P_{e}=\\mathrm{Pr}\\left(\\hat{\\mathbf{k}}\\neq\\mathbf{k}\\right)\\leq n^{-\\beta}$ . If $K=\\mathrm{poly}(n)$ , a constant $\\beta$ suffices us to drive the union bound to zero. ", "page_idx": 26}, {"type": "text", "text": "Lemma C.8. For any fixed SNR, taking $\\mathbf{D}_{c}$ such that each element is $\\textstyle\\operatorname{Bern}\\left({\\frac{\\nu}{t}}\\right)$ , and $t=\\Theta(n^{\\alpha})$ for $\\alpha\\in(0,1)$ , taking $P_{1}=O(t\\log(n))$ suffices to ensure that we can achieve error of bin identification failure with probability of error $O(1/K^{3})$ . ", "page_idx": 26}, {"type": "text", "text": "C.7.4 Singleton Detection in i.i.d. Spectral Noise ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We note that the general flow of this proof follows [40], but there are several fundamental differences that make this proof overall quite different. We define $\\mathcal{E}_{b}$ as the error event where a bin $\\mathbf{k}$ is decoded wrongly, and then using a union bound over different bins and different iterations, the probability of the algorithm making a mistake in bin identification satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}({\\mathcal{E}})\\leq(\\#\\operatorname{of\\,iterations})\\times(\\#\\operatorname{of\\,bins})\\times\\operatorname*{Pr}({\\mathcal{E}}_{b})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The number of bins is at most $\\eta K$ for some constant $\\eta$ and the number of iterations is at most $C K$ (at least one edge is peeled off at each iteration in the worst case). Hence, $\\mathrm{Pr}({\\mathcal{E}})\\leq\\eta C K^{2}\\mathrm{Pr}({\\mathcal{E}}_{b})$ . In order to satisfy $\\operatorname*{Pr}({\\mathcal{E}})\\leq O(1/K)$ , we need to show that $\\mathrm{Pr}({\\mathcal{E}}_{b})\\le{O}(1/K^{3})$ . ", "page_idx": 26}, {"type": "text", "text": "We already showed in Lemma C.8 that we can achieve singleton identification under noise with vanishing error $O(1/K^{3})$ with a delay matrix $\\mathbf{D}\\in\\mathbb{Z}_{2}^{P_{1}\\times n}$ . ", "page_idx": 26}, {"type": "text", "text": "To achieve type detection, we construct another pair of delay matrices $\\mathbf{D}^{1}\\in\\mathbb{Z}_{2}^{P_{2}\\times n}$ and $\\mathbf{D}^{2}\\in\\mathbb{Z}_{2}^{P_{2}\\times n}$ . We will choose $\\mathbf{D}^{1}$ and $\\mathbf{D}^{2}$ such that each element is drawn i.i.d. as a $\\mathrm{Bern}\\left((1/2)^{1/t}\\right)$ . We denote ", "page_idx": 26}, {"type": "text", "text": "the $i^{t h}$ row of $\\mathbf{D}^{1}$ as ${\\bf d}_{i}^{1}$ and denote the $i^{t h}$ row of $\\mathbf{D}^{2}$ as $\\mathbf{d}_{i}^{2}$ . Then, with these delay matrices, we can obtain observations of the form ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{i}^{1}(\\mathbf{k})=\\displaystyle\\sum_{\\mathbf{H}\\mathbf{r}=\\mathbf{k}}F(\\mathbf{r})+Z_{i}(\\mathbf{k})}\\\\ &{U_{i}^{2}(\\mathbf{k})=\\displaystyle\\sum_{\\mathbf{H}\\mathbf{r}=\\mathbf{k}}F(\\mathbf{r})+Z_{i}(\\mathbf{k}).}\\\\ &{\\qquad\\qquad\\qquad\\mathbf{r}\\leq\\overline{{\\mathbf{q}}}_{i}^{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that we can represent these observations as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{U}^{1}=\\mathbf{S}^{1}\\alpha+\\mathbf{W}^{1}}\\\\ {\\mathbf{U}^{2}=\\mathbf{S}^{2}\\alpha+\\mathbf{W}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with $\\mathbf{W}^{1},\\mathbf{W}^{2}\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I})$ , a $_{\\alpha}$ vector with entries $F(\\mathbf{r})$ for coefficients in the set and binary signature matrices $\\mathbf{S}^{1}$ , $\\mathbf{S}^{2}$ with entries indicating the subsets of coefficients included in each sum. ", "page_idx": 27}, {"type": "text", "text": "Then, we subtract these observations to obtain a single observation $\\mathbf{U}=\\mathbf{U}^{1}-\\mathbf{U}^{2}$ which can be written as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{U}=\\mathbf{S}\\alpha+\\mathbf{W}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with $\\mathbf{W}\\sim{\\mathcal{N}}(0,2\\sigma^{2}\\mathbf{I})\\,$ and $\\mathbf{S}=\\mathbf{S}^{1}-\\mathbf{S}^{2}$ . This construction allows us to show that the columns of S are sufficiently incoherent and hence we can correctly perform identification. ", "page_idx": 27}, {"type": "text", "text": "Lemma C.9. For any fixed SNR, taking $\\mathbf{D}_{c}^{1}$ and $\\mathbf{D}_{c}^{2}$ such that each element is Bern $\\left((1/2)^{1/t}\\right)$ , and $t=\\Theta(n^{\\alpha})$ for $\\alpha\\in(0,1/2)$ and taking $P_{2}\\,=\\,O(t\\log(n))$ suffices to ensure that the probability $\\operatorname{Pr}({\\mathcal{E}}_{b})$ for an arbitrary bin can be upper bounded as $\\operatorname*{Pr}(\\mathcal{E}_{b})\\le{O}(1/K^{3})$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. In the following, we prove that $\\operatorname*{Pr}({\\mathcal{E}}_{b})\\le{O}(1/K^{3})$ holds using the observation model. We consider separate cases where the bin in consideration is fixed as a zeroton, singleton, or multiton. ", "page_idx": 27}, {"type": "text", "text": "The error probability $\\operatorname{Pr}({\\mathcal{E}}_{b})$ for an arbitrary bin can be upper bounded as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{Pr}(\\mathscr{E}_{b})\\leq\\sum_{\\mathcal{F}\\in\\{\\mathcal{H}_{Z},\\mathcal{H}_{M}\\}}\\mathrm{Pr}(\\mathcal{F}\\gets\\mathcal{H}_{S}(\\mathbf{r},F(\\mathbf{r})))}}\\\\ &{}&{+\\sum_{\\mathcal{F}\\in\\{\\mathcal{H}_{Z},\\mathcal{H}_{M}\\}}\\mathrm{Pr}(\\mathcal{H}_{S}(\\widehat{\\mathbf{r}},\\widehat{F}(\\mathbf{r}))\\gets\\mathcal{F})}\\\\ &{}&{+\\mathrm{Pr}(\\mathcal{H}_{S}(\\widehat{\\mathbf{r}},\\widehat{F}(\\mathbf{r}))\\gets\\mathcal{H}_{S}(\\mathbf{r},F(\\mathbf{r})))}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "above, each of these events should be read as: ", "page_idx": 27}, {"type": "text", "text": "1. $\\{\\mathcal{F}\\gets\\mathcal{H}_{S}(\\mathbf{r},F(\\mathbf{r}))\\}$ : missed verification in which the singleton verification fails when the ground truth is in fact a singleton.   \n2. $\\{\\mathcal{H}_{S}(\\widehat{\\mathbf{r}},\\widehat{F}(\\mathbf{r}))\\gets\\mathcal{F}\\}$ : false verification in which the singleton verification is passed when the gr ou n d truth is not a singleton.   \n3. $\\{\\mathcal{H}_{S}(\\widehat{\\mathbf{r}},\\widehat{F}(\\mathbf{r}))\\xleftarrow{}\\mathcal{H}_{S}(\\mathbf{r},F(\\mathbf{r}))\\}$ : crossed verification in which a singleton with a wrong index- va l ue pair passes the singleton verification when the ground truth is another singleton pair. ", "page_idx": 27}, {"type": "text", "text": "We can upper-bound each of these error terms using Propositions C.10, C.11, and C.12. Note that all upper-bound terms decay exponentially with $P_{2}$ except for the term $\\operatorname*{Pr}(\\widehat{\\mathbf{r}}\\neq\\mathbf{r})\\le O(1/K^{3})$ . ", "page_idx": 27}, {"type": "text", "text": "We use Theorem F.3 to show that we can achieve $\\mathrm{Pr}(\\widehat{\\mathbf{r}}\\neq\\mathbf{r})\\,\\leq\\,O(1/K^{3})$ if we choose $P_{1}\\,=$ $O(t\\log n)$ . Since all other error probabilities decay exp onentially with $P_{2}$ , it is clear that if $P_{2}$ is chosen as $P_{2}=O(t\\log n)$ , the error probability can be bounded as $\\operatorname*{Pr}({\\mathcal{E}}_{b})\\le{O}(1/K^{3})$ . ", "page_idx": 27}, {"type": "text", "text": "Proposition C.10 (False Verification Rate). For $\\begin{array}{r}{0<\\gamma<\\frac{\\eta}{4}\\mathrm{SNR},}\\end{array}$ the false verification rate for each bin hypothesis satisfies: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(\\mathcal{H}_{S}(\\widehat{\\mathbf{r}},\\widehat{F}(\\widehat{\\mathbf{r}}))\\leftarrow\\mathcal{H}_{Z})\\leq e^{-\\frac{P_{2}}{2}(\\sqrt{1+2\\gamma}-1)^{2}},}\\\\ &{\\operatorname*{Pr}(\\mathcal{H}_{S}(\\widehat{\\mathbf{r}},\\widehat{F}(\\widehat{\\mathbf{r}}))\\leftarrow\\mathcal{H}_{M})\\leq e^{-\\frac{P_{2}\\gamma^{2}}{4(1+4\\gamma)}}+K^{2}e^{-\\epsilon\\left(1-\\frac{4\\gamma\\nu^{2}}{\\rho^{2}}\\right)^{2}P_{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $P_{2}$ is the number of t he ra ndom offsets. ", "page_idx": 28}, {"type": "text", "text": "Proof. The probability of detecting a zeroton as a singleton can be upper-bounded by the probability of a zeroton failing the zeroton verification. This means ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(\\mathcal{H}_{S}(\\widehat{\\mathbf{r}},\\widehat{F}(\\widehat{\\mathbf{r}}))\\leftarrow\\mathcal{H}_{Z})\\leq\\operatorname*{Pr}\\left(\\frac{1}{P_{2}}\\|\\mathbf{W}\\|^{2}\\geq(1+\\gamma)\\nu^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq e^{-\\frac{P_{2}}{4}(\\sqrt{1+2\\gamma}-1)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "by noting that $\\mathbf{W}\\sim\\mathcal{N}(0,\\nu^{2}\\mathbf{I})$ and applying Lemma C.13. ", "page_idx": 28}, {"type": "text", "text": "On the other hand, given some multiton observation $\\mathbf{U}=\\mathbf{S}\\alpha+\\mathbf{W}$ , the probability of detecting it as a singleton with index-value pair $(\\widehat{\\mathbf{r}},\\widehat{F}(\\widehat{\\mathbf{r}}))$ can be written as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(\\mathcal{H}_{S}(\\widehat{\\mathbf{r}},\\widehat{F}(\\widehat{\\mathbf{r}}))\\gets\\mathcal{H}_{M})=\\operatorname*{Pr}\\left(\\frac{1}{P_{2}}\\left\\|\\mathbf{U}-\\widehat{F}(\\widehat{\\mathbf{r}})\\mathbf{s}_{\\widehat{\\mathbf{r}}}\\right\\|^{2}\\leq(1+\\gamma)\\nu^{2}\\right)=}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\operatorname*{Pr}\\left(\\frac{1}{P_{2}}\\left\\|\\mathbf{g}+\\mathbf{v}\\right\\|^{2}\\leq(1+\\gamma)\\nu^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathbf{g}:=\\mathbf{S}(\\alpha-\\widehat{F}(\\widehat{\\mathbf{r}})\\mathbf{e}_{\\widehat{\\mathbf{r}}})$ and $\\mathbf{v}:=\\mathbf{W}$ . Then, we can upper bound this probability as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left({\\frac{1}{P_{2}}}\\left\\|\\mathbf{g}+\\mathbf{v}\\right\\|^{2}\\leq(1+\\gamma)\\nu^{2}{\\bigg|}{\\frac{\\|\\mathbf{g}\\|^{2}}{P_{2}}}\\geq2\\gamma\\nu^{2}\\right)+\\operatorname*{Pr}\\left({\\frac{\\|\\mathbf{g}\\|^{2}}{P_{2}}}\\leq2\\gamma\\nu^{2}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To upper bound the first term, we use Lemma C.13. Note that the first term is conditioned on the event $\\|\\mathbf{g}\\|^{2}/P_{2}\\,\\geq\\,2\\gamma\\nu^{2}$ , thus the normalized non-centrality parameter satisfies $\\theta_{0}\\;\\geq\\;2\\gamma$ . As a result, we can use Lemma C.13 by letting $\\tau_{2}=(1+\\gamma)\\nu^{2}$ . Then, the first term is upper bounded by $\\exp\\{-(P_{2}\\gamma^{2})/(4(1+4\\gamma))\\}$ . To analyze the second term, we let $\\beta=\\alpha-\\widehat F(\\widehat{\\mathbf{r}})\\mathbf{e}_{\\widehat{\\mathbf{r}}}$ and write $\\mathbf{g}=\\mathbf{S}\\beta$ . Denoting its support as $\\mathcal{L}:=\\operatorname{supp}(\\beta)$ , we can further write $\\mathbf{S}\\beta=\\mathbf{S}_{\\mathcal{L}}\\beta_{\\mathcal{L}}$ w her e $\\mathbf{S}_{\\mathcal{L}}$ is the sub-matrix of $\\mathbf{S}$ consisting of the columns in $\\mathcal{L}$ and $\\beta_{\\mathcal{L}}$ is the sub-vector consisting of the elements in $\\mathcal{L}$ . Then, we consider two scenarios: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The multiton size is a constant, i.e., $|{\\mathcal{L}}|=L=O(1)$ . In this case, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\lambda_{\\mathrm{min}}(\\mathbf{S}_{\\mathcal{L}}^{\\top}\\mathbf{S}_{\\mathcal{L}})\\|\\beta_{\\mathcal{L}}\\|^{2}\\leq\\|\\mathbf{S}_{\\mathcal{L}}\\beta_{\\mathcal{L}}\\|^{2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Using $\\|\\beta_{\\mathcal{L}}\\|^{2}\\geq L\\rho^{2}$ , the probability can be bounded as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\frac{\\|\\mathbf{g}\\|^{2}}{P_{2}}\\leq2\\gamma\\nu^{2}\\right)\\leq\\operatorname*{Pr}\\left(\\lambda_{\\operatorname*{min}}\\left(\\frac{1}{P_{2}}\\mathbf{S}_{\\mathcal{L}}^{\\top}\\mathbf{S}_{\\mathcal{L}}\\right)\\leq\\frac{2\\gamma\\nu^{2}}{L\\rho^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "On the other hand, using Lemma C.14 with the selection $\\beta=1/2$ and $\\begin{array}{r}{\\eta=\\frac{1}{1+2L}(\\frac{1}{2}-\\frac{2\\gamma\\nu^{2}}{L\\rho^{2}})}\\end{array}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left({\\frac{\\|\\mathbf{g}\\|^{2}}{P_{2}}}\\leq2\\gamma\\nu^{2}\\right)\\leq2L^{2}e^{-{\\frac{P_{2}}{2(1+2L)^{2}}}\\left({\\frac{1}{2}}-{\\frac{2\\gamma\\nu^{2}}{L\\rho^{2}}}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which holds as long as $\\begin{array}{r}{\\gamma<L\\rho^{2}/(4\\nu^{2})=\\frac{L\\eta}{4}\\mathrm{SNR}}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "\u2022 The multiton size grows asymptotically with respect to $K$ , i.e., $|{\\mathcal{L}}|=L=\\omega(1)$ . As a result, the vector of random variables $\\mathbf{g}=\\mathbf{S}_{\\mathcal{L}}\\beta_{\\mathcal{L}}$ becomes asymptotically Gaussian due to the central limit theorem with zero mean and a covariance ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{g}\\mathbf{g}^{\\mathrm{H}}]=\\frac{1}{2}L\\rho^{2}\\mathbf{I}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, by Lemma C.13, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left({\\frac{\\|\\mathbf{g}\\|^{2}}{P_{2}}}\\leq2\\gamma\\nu^{2}\\right)\\leq e^{-{\\frac{P_{2}}{2}}\\left(1-{\\frac{\\gamma\\nu^{2}}{L\\rho^{2}}}\\right)}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which holds as long as $\\gamma<L\\rho^{2}/\\nu^{2}=L\\eta\\mathrm{SNR}$ . ", "page_idx": 29}, {"type": "text", "text": "By combining the results from both cases, there exists some absolute constant $\\epsilon>0$ such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\frac{\\|\\mathbf{g}\\|^{2}}{P_{2}}\\leq2\\gamma\\nu^{2}\\right)\\leq K^{2}e^{-\\epsilon\\left(1-\\frac{4\\gamma\\nu^{2}}{\\rho^{2}}\\right)^{2}P_{2}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "as long as $\\begin{array}{r}{\\gamma<\\rho^{2}/(4\\nu^{2})=\\frac{\\eta}{4}\\mathrm{SNR}}\\end{array}$ . ", "page_idx": 29}, {"type": "text", "text": "Proposition C.11 (Missed Verification Rate). For $\\begin{array}{r}{0<\\gamma<\\frac{\\eta}{2}\\mathrm{SNR},}\\end{array}$ , the missed verification rate for each bin hypothesis satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(\\mathcal{H}_{Z}\\gets\\mathcal{H}_{S}(\\mathbf{r},F[\\mathbf{r}]))\\leq e^{-\\frac{P_{2}}{4}\\frac{(\\rho^{2}/\\nu^{2}-\\gamma)^{2}}{1+2\\rho^{2}/\\nu^{2}}}}\\\\ &{\\operatorname*{Pr}(\\mathcal{H}_{M}\\gets\\mathcal{H}_{S}(\\mathbf{r},F[\\mathbf{r}]))\\leq e^{-\\frac{P_{2}}{4}(\\sqrt{1+2\\gamma}-1)^{2}}+2e^{-\\frac{\\rho^{2}}{2\\nu^{2}}P_{2}}+2\\mathrm{Pr}(\\widehat{\\mathbf{r}}\\neq\\mathbf{r})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $P_{2}$ is the number of the random offsets. ", "page_idx": 29}, {"type": "text", "text": "Proof. The probability of detecting a singleton as a zeroton can be upper bounded by the probability of a singleton passing the zeroton verification. Hence, by noting that $\\mathbf{\\dot{W}}\\sim\\mathcal{N}(0,\\nu^{2}\\mathbf{I})$ and applying Lemma C.13, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}(\\mathcal{H}_{Z}\\gets\\mathcal{H}_{S}(\\mathbf{r},F[\\mathbf{r}]))}\\\\ &{\\quad\\leq\\mathrm{Pr}\\left(\\frac{1}{P_{2}}\\|F[\\mathbf{r}]\\mathbf{s_{r}}+\\mathbf{W}\\|^{2}\\leq(1+\\gamma)\\nu^{2}\\right)}\\\\ &{\\quad\\leq e^{-\\frac{P_{2}}{4}\\frac{(\\rho^{2}/\\nu^{2}-\\gamma)^{2}}{1+2\\rho^{2}/\\nu^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which holds as long as $\\gamma<\\rho^{2}/\\nu^{2}=\\eta\\mathrm{SNR}$ . ", "page_idx": 29}, {"type": "text", "text": "On the other hand, the probability of detecting a singleton as a multiton can be written as the probability of failing the singleton verification step for some index-value pair $(\\widehat{\\mathbf{r}},\\widehat{F}[\\widehat{\\mathbf{r}}])$ . Hence, we can write ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(\\mathcal{H}_{M}\\leftarrow\\mathcal{H}_{S}(\\mathbf{r},F[\\mathbf{r}]))=\\operatorname*{Pr}\\Big(\\frac{1}{P_{2}}\\left\\|\\mathbf{U}-\\widehat{F}[\\widehat{\\mathbf{r}}]\\mathbf{s}_{\\widehat{k}}\\right\\|^{2}\\geq(1+\\gamma)\\nu^{2}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\operatorname*{Pr}\\Big(\\frac{1}{P_{2}}\\left\\|\\mathbf{U}-\\widehat{F}[\\widehat{\\mathbf{r}}]\\mathbf{s}_{\\widehat{k}}\\right\\|^{2}\\geq(1+\\gamma)\\nu^{2}\\Big|\\widehat{F}[\\widehat{\\mathbf{r}}]=F[\\mathbf{r}]\\wedge\\widehat{\\mathbf{r}}=\\mathbf{r}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\operatorname*{Pr}(\\widehat{F}[\\widehat{\\mathbf{r}}]\\neq F[\\mathbf{r}]\\vee\\widehat{\\mathbf{r}}\\neq\\mathbf{r}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, using Lemma C.13, the first term is upper-bounded as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\bigg(\\frac{1}{P_{2}}\\left\\|\\mathbf{U}-\\widehat{F}[\\widehat{\\mathbf{r}}]\\mathbf{s}_{\\widehat{k}}\\right\\|^{2}\\bigg)\\geq(1+\\gamma)\\nu^{2}\\bigg|\\widehat{F}[\\widehat{\\mathbf{r}}]=F[\\mathbf{r}]\\wedge\\widehat{\\mathbf{r}}=\\mathbf{r}\\bigg)\\leq\\operatorname*{Pr}\\!\\bigg(\\frac{1}{P_{2}}\\|\\mathbf{W}\\|^{2}\\geq(1+\\gamma)\\nu^{2}\\bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq e^{-\\frac{P_{2}}{4}(\\sqrt{1+2\\gamma}-1)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "On the other hand, the second term can be bounded as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}(\\widehat{F}[\\widehat{\\mathbf{r}}]\\neq F[\\mathbf{r}]\\lor\\widehat{\\mathbf{r}}\\neq\\mathbf{r})\\leq\\mathrm{Pr}(\\widehat{F}[\\widehat{\\mathbf{r}}]\\neq F[\\mathbf{r}])+\\mathrm{Pr}(\\widehat{\\mathbf{r}}\\neq\\mathbf{r})}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{Pr}(\\widehat{F}[\\widehat{\\mathbf{r}}]\\neq F[\\mathbf{r}]|\\widehat{\\mathbf{r}}\\neq\\mathbf{r})\\mathrm{Pr}(\\widehat{\\mathbf{r}}\\neq\\mathbf{r})}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\mathrm{Pr}(\\widehat{F}[\\widehat{\\mathbf{r}}]\\neq F[\\mathbf{r}]|\\widehat{\\mathbf{r}}=\\mathbf{r})\\mathrm{Pr}(\\widehat{\\mathbf{r}}=\\mathbf{r})}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\mathrm{Pr}(\\widehat{\\mathbf{r}}\\neq\\mathbf{r})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathrm{Pr}(\\widehat{F}[\\widehat{\\mathbf{r}}]\\neq F[\\mathbf{r}]|\\widehat{\\mathbf{r}}=\\mathbf{r})+2\\mathrm{Pr}(\\widehat{\\mathbf{r}}\\neq\\mathbf{r})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The first term is the error probability of a BPSK signal with amplitude $\\rho$ , and it can be bounded as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(\\widehat{F}[\\widehat{\\mathbf{r}}]\\neq F[\\mathbf{r}]|\\widehat{\\mathbf{r}}=\\mathbf{r})\\leq2e^{-\\frac{\\rho^{2}}{2\\nu^{2}}P_{2}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proposition C.12 (Crossed Verification Rate). For $\\begin{array}{r}{0<\\gamma<\\frac{\\eta}{2}\\mathrm{SNR}}\\end{array}$ , the crossed verification rate for each bin hypothesis satisfies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}(\\mathcal{H}_{S}(\\widehat{\\mathbf{r}},\\widehat{F}[\\widehat{\\mathbf{r}}])\\gets\\mathcal{H}_{S}(\\mathbf{r},F[\\mathbf{r}]))\\leq e^{-\\frac{P_{2}\\gamma^{2}}{4(1+4\\gamma)}}+K e^{-\\epsilon\\left(1-\\frac{4\\gamma\\nu^{2}}{\\rho^{2}}\\right)^{2}P_{2}}+K^{2}e^{-\\epsilon\\left(1-\\frac{4\\gamma\\nu^{2}}{\\rho^{2}}\\right)^{2}P_{2}^{2}/t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $P_{2}$ is the number of the random offsets. ", "page_idx": 30}, {"type": "text", "text": "Proof. This error event can only occur if a singleton with index-value pair $(\\mathbf{r},F[\\mathbf{r}])$ passes the singleton verification step for some index-value pair $(\\widehat{\\mathbf{r}},\\widehat{F}[\\widehat{\\mathbf{r}}])$ such that $\\mathbf{r}\\neq\\widehat{\\mathbf{r}}$ . Hence, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\big(\\mathcal{H}_{S}(\\widehat{\\mathbf{r}},\\widehat{F}[\\widehat{\\mathbf{r}}])\\leftarrow\\mathcal{H}_{S}(\\mathbf{r},F[\\mathbf{r}])\\big)}\\\\ &{\\leq\\operatorname*{Pr}\\bigg(\\frac{1}{P_{2}}\\|F[\\mathbf{r}]\\mathbf{s}_{\\mathbf{r}}-\\widehat{F}[\\widehat{\\mathbf{r}}]\\mathbf{s}_{\\widehat{\\mathbf{r}}}+\\mathbf{W}\\|^{2}\\leq(1+\\gamma)\\nu^{2}\\bigg)}\\\\ &{=\\operatorname*{Pr}\\bigg(\\frac{1}{P_{2}}\\|\\mathbf{S}\\beta+\\mathbf{W}\\|^{2}\\leq(1+\\gamma)\\nu^{2}\\bigg)}\\\\ &{=\\operatorname*{Pr}\\bigg(\\frac{1}{P_{2}}\\|\\mathbf{S}\\beta+\\mathbf{W}\\|^{2}\\leq(1+\\gamma)\\nu^{2}\\bigg|\\|\\mathbf{S}\\beta\\|^{2}\\geq2\\gamma\\nu^{2}\\bigg)}\\\\ &{\\qquad+\\operatorname*{Pr}\\big(\\|\\mathbf{S}\\beta\\|^{2}\\leq2\\gamma\\nu^{2}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\beta$ is a 2-sparse vector with non-zero entries from $\\{\\rho,-\\rho\\}$ . Using Lemma C.13, the first term is upper-bounded as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\frac{1}{P_{2}}\\|\\mathbf{S}\\boldsymbol{\\beta}+\\mathbf{W}\\|^{2}\\leq(1+\\gamma)\\nu^{2}\\bigg|\\|\\mathbf{S}\\boldsymbol{\\beta}\\|^{2}\\geq2\\gamma\\nu^{2}\\right)\\leq e^{-\\frac{P_{2}\\gamma^{2}}{4(1+4\\gamma)}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma C.14, the second term is upper bounded as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\left(\\|\\mathbf{S}\\boldsymbol{\\beta}\\|^{2}\\leq2\\gamma\\nu^{2}\\right)\\leq8e^{-\\frac{P_{2}}{50}\\left(\\frac{1}{2}-\\frac{\\gamma\\nu^{2}}{L\\rho^{2}}\\right)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which holds as long as $\\begin{array}{r}{\\gamma<\\rho^{2}/(2\\nu^{2})=\\frac{\\eta}{2}\\mathrm{SNR}}\\end{array}$ . ", "page_idx": 30}, {"type": "text", "text": "Lemma C.13 (Non-central Tail Bounds (Lemma 11 in [13])). Given any $\\mathbf{g}\\in\\mathbb{R}^{P}$ and a Gaussian vector $\\mathbf{v}\\sim\\mathcal{N}(0,\\nu^{2}\\mathbf{I})$ , the following tail bounds hold: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{Pr}\\left(\\frac{1}{P}\\|\\mathbf{g}+\\mathbf{v}\\|^{2}\\geq\\tau_{1}\\right)\\leq e^{-\\frac{P}{4}(\\sqrt{2\\tau_{1}/\\nu^{2}-1}-\\sqrt{1+2\\theta_{0}})^{2}}}\\\\ {\\displaystyle\\mathrm{Pr}\\left(\\frac{1}{P}\\|\\mathbf{g}+\\mathbf{v}\\|^{2}\\leq\\tau_{2}\\right)\\leq e^{-\\frac{P}{4}\\frac{\\left(1+\\theta_{0}-\\tau_{2}/\\nu^{2}\\right)^{2}}{1+2\\theta_{0}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for any $\\tau_{1}$ and $\\tau_{2}$ that satisfy $\\tau_{1}\\geq\\nu^{2}(1+\\theta_{0})\\geq\\tau_{2}$ where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\theta_{0}:=\\frac{\\|\\mathbf{g}\\|^{2}}{P\\nu^{2}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "is the normalized non-centrality parameter. ", "page_idx": 30}, {"type": "text", "text": "Lemma C.14. Suppose $\\beta=\\Theta(1)$ , $\\eta=\\Omega(1)$ , and $t=\\Theta(n^{\\alpha})$ for some $\\alpha\\in(0,1/2)$ . Then, there exists some $n_{0}$ such that for all $n\\geq n_{0}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\lambda_{\\operatorname*{min}}\\left(\\frac{1}{P_{2}}{\\bf S}_{\\mathcal{L}}^{\\top}{\\bf S}_{\\mathcal{L}}\\right)\\leq2\\beta(1-\\beta)-(2L+1)\\eta\\right)\\leq2L^{2}\\exp\\left(-\\frac{\\eta^{2}}{2}P_{2}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. For any $\\mathbf{r}$ sampled uniformly from vectors up to degree $t$ , the probability that it will have degree $0\\leq k\\leq t$ can be written as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left|\\mathbf{r}\\right|=k\\right)=\\frac{\\binom{n}{k}}{\\sum_{k=1}^{t}{\\binom{n}{k}}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We know that the entries of $\\mathbf{s_{r}}$ are given as $(\\mathbf{s}_{\\mathbf{r}}^{1})_{i}=\\mathbb{1}\\left\\{\\mathbf{r}\\le\\bar{\\mathbf{d}}_{i}^{1}\\right\\}$ and $(\\mathbf{s}_{\\mathbf{r}}^{2})_{i}=\\mathbb{1}\\left\\{\\mathbf{r}\\le\\bar{\\mathbf{d}}_{i}^{2}\\right\\}$ . Therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}\\left((\\mathbf{s}_{\\mathbf{r}}^{1})_{i}=1\\right)=\\mathrm{Pr}\\left(d_{i j}^{1}=0,\\forall j\\in\\mathrm{supp}(\\mathbf{r})\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{k=1}^{t}\\mathrm{Pr}\\left(d_{i j}^{1}=0,\\forall j\\in\\mathrm{supp}(\\mathbf{r})||\\mathbf{r}|=k\\right)\\mathrm{Pr}\\left(|\\mathbf{r}|=k\\right)}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{\\sum_{k=1}^{t}\\binom{n}{k}\\beta^{k/t}}{\\sum_{k=1}^{t}\\binom{n}{k}}.}\\\\ &{\\qquad=:g(t,n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "With $\\beta=\\Theta(1)$ and $t=\\Theta(n^{\\alpha})$ for $\\alpha\\in(0,1/2)$ , we can show that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}g(t,n)=\\beta}\\end{array}$ . Therefore, there exists some $n_{0}$ such that $|\\operatorname*{Pr}\\left((\\mathbf{s}_{\\mathbf{r}}^{1})_{i}=1\\right)-\\beta|\\leq\\eta$ for all $n\\geq n_{0}$ . For the rest of the proof, let $g=\\mathrm{Pr}\\left((\\mathbf{s}_{\\mathbf{r}}^{1})_{i}=1\\right)$ and assume $|g-\\beta|\\leq\\eta$ . ", "page_idx": 31}, {"type": "text", "text": "Then, recalling $(\\mathbf{s_{r}})_{i}=(\\mathbf{s_{r}^{1}})_{i}-(\\mathbf{s_{r}^{2}})_{i}$ , the distribution for each entry of $\\mathbf{s_{r}}$ can be written as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left((\\mathbf{s_{r}})_{i}=1\\right)=\\mathrm{Pr}\\left((\\mathbf{s_{r}})_{i}=-1\\right)=g(1-g).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence, using Hoeffding\u2019s inequality, we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\frac{1}{P_{2}}{\\mathbf{s}_{\\mathbf{r}}^{\\top}}{\\mathbf{s}_{\\mathbf{r}}}\\leq2\\beta(1-\\beta)-\\eta\\right)\\leq\\operatorname*{Pr}\\left(\\frac{1}{P_{2}}{\\mathbf{s}_{\\mathbf{r}}^{\\top}}{\\mathbf{s}_{\\mathbf{r}}}\\leq2g(1-g)-\\eta\\right)\\leq\\exp\\left(-\\frac{\\eta^{2}}{2}P_{2}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Furthermore, the conditional probability of another vector $\\mathbf{m}\\neq\\mathbf{r}$ being included in test $i$ is given by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{Pr}\\left((\\mathbf{s_{m}^{1}})_{i}=1|(\\mathbf{s_{r}^{1}})_{i}=1,|\\mathbf{r}|=k\\right)=\\operatorname*{Pr}\\left(d_{i j}=0,\\forall j\\in\\mathrm{supp}(\\mathbf{m})\\setminus\\mathrm{supp}(\\mathbf{r})||\\mathbf{r}|=k\\right)}\\\\ {\\displaystyle=\\sum_{\\ell=0}^{t}\\left(\\beta^{1/t}\\right)^{\\ell}\\left(1-\\frac{k}{n}\\right)^{\\ell}\\left(\\frac{k}{n}\\right)^{t-\\ell}}\\\\ {\\displaystyle=\\left(\\frac{k}{n}+\\left(1-\\frac{k}{n}\\right)\\beta^{1/t}\\right)^{t}}\\\\ {\\displaystyle=:f(t,n,k).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "With $\\beta=\\Theta(1)$ and $t=\\Theta(n^{\\alpha})$ for $\\alpha\\in(0,1)$ , for any $k\\leq t$ , we can show that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}f(t,n,k)=}\\end{array}$ $\\beta$ . Therefore, there exists some $n_{0}$ such that $|\\operatorname*{Pr}\\left((\\mathbf{s}_{\\mathbf{m}}^{1})_{i}=1|(\\mathbf{s}_{\\mathbf{r}}^{1})_{i}=1\\right)-\\beta|\\leq\\eta$ for all $n\\geq n_{0}$ . For the rest of the proof, let $f=\\mathrm{Pr}\\left((\\mathbf{s}_{\\mathbf{m}}^{1})_{i}=1|(\\mathbf{s}_{\\mathbf{r}}^{1})_{i}=1\\right)$ and assume $|f-\\beta|\\leq\\eta$ . ", "page_idx": 31}, {"type": "text", "text": "On the other hand, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\operatorname*{Pr}\\left((\\mathbf{s_{m}})_{i}(\\mathbf{s_{r}})_{i}=1\\right)=2f g\\left[1-g-(1-f)g\\right]}\\\\ &{\\operatorname*{Pr}\\left((\\mathbf{s_{m}})_{i}(\\mathbf{s_{r}})_{i}=-1\\right)=2\\left[(1-f)g\\right]^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "As a result, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[(\\mathbf{s_{m}})_{i}(\\mathbf{s_{r}})_{i}]=2g(f-g).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathbb{E}[(\\mathbf{s}_{\\mathbf{m}})_{i}(\\mathbf{s}_{\\mathbf{r}})_{i}]=0}\\end{array}$ , there exists some $n_{0}$ such that $-\\eta\\leq\\mathbb{E}[(\\mathbf{s}_{\\mathbf{m}})_{i}(\\mathbf{s}_{\\mathbf{r}})_{i}]\\leq\\eta$ for all $n\\geq n_{0}$ . For the rest of the proof assume $-\\eta\\leq\\mathbb{E}[(\\mathbf{s}_{\\mathbf{m}})_{i}(\\mathbf{s}_{\\mathbf{r}})_{i}]\\leq\\eta$ . As a result, we can write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\frac{1}{P_{2}}|\\mathbf{s}_{\\mathbf{r}}^{\\top}\\mathbf{s}_{\\mathbf{m}}|\\geq2\\eta\\right)\\leq\\mathrm{Pr}\\left(|\\mathbf{s}_{\\mathbf{r}}^{\\top}\\mathbf{s}_{\\mathbf{m}}-P_{2}\\mathbb{E}[(\\mathbf{s}_{\\mathbf{m}})_{i}(\\mathbf{s}_{\\mathbf{r}})_{i}]|\\geq P_{2}\\eta\\right)\\leq\\exp\\left(-\\frac{\\eta^{2}}{2}P_{2}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By Gershgorin Circle Theorem, the minimum eigenvalue of $\\begin{array}{r}{\\frac{1}{P_{2}}{\\bf S}_{\\cal L}^{\\top}{\\bf S}_{\\cal L}}\\end{array}$ is lower bounded as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\lambda_{\\mathrm{min}}\\left(\\frac{1}{P_{2}}\\mathbf{S}_{C}^{\\top}\\mathbf{S}_{C}\\right)\\geq\\frac{1}{P_{2}}\\operatorname*{min}_{\\mathbf{r}\\in\\mathcal{L}}\\left(|\\mathbf{s}_{\\mathbf{r}}^{\\top}\\mathbf{s}_{\\mathbf{r}}|-\\sum_{\\mathbf{m}\\in\\mathcal{L}}|\\mathbf{s}_{\\mathbf{r}}^{\\top}\\mathbf{s}_{\\mathbf{m}}|\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lastly, we apply a union bound over all $(\\mathbf{r},\\mathbf{m})$ pairs to obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\lambda_{\\operatorname*{min}}\\left(\\frac{1}{P_{2}}{\\bf S}_{\\mathcal{L}}^{\\top}{\\bf S}_{\\mathcal{L}}\\right)\\leq2\\beta(1-\\beta)-(2L+1)\\eta\\right)\\leq2L^{2}\\exp\\left(-\\frac{\\eta^{2}}{2}P_{2}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "D Worst-Case Time Complexity ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we discuss the computational complexity of Algorithm 1, which is broken down into the following parts: ", "page_idx": 32}, {"type": "text", "text": "Computing Samples Computing samples for one sapling matrix requires computing the row-span of $\\mathbf{H}_{c}$ , which can be computed in $\\overline{{n2^{b}}}$ operations. Then for each sample, we must take the bit-wise and with each row of the delay matrix, so the total complexity is: $C n\\dot{2}^{b}P$ . ", "page_idx": 32}, {"type": "text", "text": "Taking Small Mobius Transform Computing the Mobius transform for each of the $C P$ subsampled functions is $C P b2^{b}$ . ", "page_idx": 32}, {"type": "text", "text": "Singleton Detection To detect each singleton requires computing y. This requires $P$ divisions for each of the $C2^{b}$ bins, for a total of $C P2^{b}$ operations. ", "page_idx": 32}, {"type": "text", "text": "Singleton Identification To identify each singleton requires different complexity for our different assumptions. ", "page_idx": 32}, {"type": "text", "text": "1. In the case of uniformly distributed interactions, singleton detection is $O(1)$ , since $\\mathbf{y}=\\mathbf{k}^{*}$ immediately, so doing this for each singleton makes the total complexity $C K$ . 2. In the noiseless low degree case decoding $\\mathbf{k}^{*}$ from $\\mathbf{y}$ is $\\mathrm{poly}(n)$ , so for each singleton the complexity is $C K\\,\\mathrm{poly}(n)$ ", "page_idx": 32}, {"type": "text", "text": "Message Passing In the worst case, we peel exactly one singleton per iteration, resulting in $C K$ subtractions (the above singleton identification bounds already take into account the need to re-do singleton identification). ", "page_idx": 32}, {"type": "text", "text": "Thus in the case of uniformly distributed and low degree interactions respectively, the complexity is: ", "page_idx": 32}, {"type": "text", "text": "Uniform distributed noiseless time complexity ${\\begin{array}{r l}{=}&{{\\cal O}(C P n2^{b}+C P b2^{b}+C K)}\\\\ {=}&{{\\cal O}(C P n K)}\\\\ {=}&{{\\cal O}(n^{2}K).}\\end{array}}$ Low degree (noisy) time complexity $\\begin{array}{r l}{=}&{{}O(C P n2^{b}+C P b2^{b}+C K\\,\\mathrm{poly}(n))}\\\\ {=}&{{}O(C P\\,\\mathrm{poly}(n)K)}\\\\ {=}&{{}O(\\mathrm{poly}(n)K).}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "E Additional Simulations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we present some additional simulations that did not fti in the body of the manuscript. Fig 15 and 16. Plot the runtime of SMT vs. $n$ under both of our assumptions. In both cases we observe excellent scaling with $n$ . We note that our low degree setting has a higher fixed cost since we ", "page_idx": 32}, {"type": "text", "text": "are using linear programming to solve our group testing problem and the solver appears to have some non-trivial fixed time cost. ", "page_idx": 33}, {"type": "text", "text": "Fig. 17 plots the perfect reconstruction percentage against $n$ and sample complexity. We also observe a phase transition, however, the phase threshold appears very insensitive to $n$ , as expected, since our sample complexity requirement is growing like $\\log(n)$ , and we are already plotting on a log scale. ", "page_idx": 33}, {"type": "image", "img_path": "glGeXu1zG4/tmp/0851c0cd229437156be4ff574700cf75984df6daf0592a47e6a89348a026d952.jpg", "img_caption": ["Figure 15: Time complexity of SMT under Assumption 2.1. The parameter $K$ is fixed and we plot the runtime v.s. $n$ . our algorithm remains possible to run for $n=1000$ where other competitors fail. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "glGeXu1zG4/tmp/93f5afd495cc0d8f62502f9274ecdb468d50138c33feb7f1166f4898d712ef24.jpg", "img_caption": ["Figure 16: Time complexity of SMT under assumption 2.2. The parameters $K$ and $t$ are fixed and we plot the runtime v.s. $n$ . Our theory says we have a $\\mathrm{poly}(n)$ complexity. In practice, for reasonable $n$ our algorithm is running quickly. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "glGeXu1zG4/tmp/ed0a051b366ea91fc1a44b804a5e64684a0b75064aeb32d1fa067605bb34bfc6.jpg", "img_caption": ["Figure 17: Perfect reconstruction percentage plotted against sample complexity and $n$ under Assumption 2.2. Holding $C=3$ , we scale $b$ to increase the sample complexity. We observe that the number of samples required to achieve perfect reconstruction is scaling linearly is very insensitive to $n$ as predicted. We also include $N=2^{n}$ on the bottom axis, which is the total number of interactions. In this regime we do not appear to consistently maintain zero error. This could be due to the fact that the asymptotic behaviour of group testing might not yet be fuly realized in the regime with $n\\leq1000$ . "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "F Group Testing ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "F.1 Group Testing Achievability Results From Literature ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Theorem F.1 (Part of Theorem 4.1 and 4.2 in [15]). Asymptotic Rate 1 Noiseless Group Testing: Consider a noiseless group testing problem with $t=\\Theta(\\dot{n}^{\\theta})$ defects out of n elements. We define the rate of a group testing procedure as: ", "page_idx": 33}, {"type": "equation", "text": "$$\nR:=\\frac{\\log\\binom{n}{t}}{T}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $T$ is the number of tests performed by the group testing procedure. For an i.i.d. Bernoulli design matrix, for $\\theta\\in[0,1/3]$ , in the limit as $n\\to\\infty$ , a rate $R_{B E R N}^{*}=1$ is achievable with vanishing error. Furthermore, for the constant column-weight design matrix, for $\\theta\\in[0,0.409]$ a rate $R_{C C W}^{*}=1$ is achievable with vanishing error. ", "page_idx": 33}, {"type": "text", "text": "Theorem F.2 ([42, 54]). Noiseless Group Testing: Consider the noiseless non-adaptive group testing setup with $t=|\\mathbf{k}|$ defects out of n items, with $t$ scaling arbitrarily in n. Let \u02c6k be the output of a group testing decoder and let $T^{*}=\\Theta\\left(\\operatorname*{min}\\left\\{t\\log(n),n\\right\\}\\right)$ . Then there exists a strategy using $T\\leq(1+\\epsilon)T^{*}$ such that in the limit as $n\\to\\infty$ we have: ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\hat{\\mathbf{k}}\\neq\\mathbf{k}\\right)\\rightarrow0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Furthermore, there is $a\\,\\mathrm{poly}(n)$ algorithm for computing $\\hat{\\mathbf{k}}$ . From $I54J,$ for $t=o(n)$ we can achieve: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\hat{\\mathbf{k}}\\neq\\mathbf{k}\\right)\\leq n^{-\\delta}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "with number of tests $T=O((1+\\delta)t\\log(n))$ . ", "page_idx": 34}, {"type": "text", "text": "Note that the above error rate is not a state-of-art result, but suffices in this case for our proof, and is very convenient in its form. ", "page_idx": 34}, {"type": "text", "text": "Theorem F.3 ([43]). Noisy Group Testing Under General Binary Noise: Consider the general binary noisy group testing setup with crossover probabilities $p_{10}$ and $p_{01}$ . We use i.i.d Bernoulli testing with parameter $\\nu>0$ . There are a total of $|\\mathbf{k}|=t=\\hat{\\Theta}(n^{\\theta})$ defects, where $\\theta\\in(0,1)$ . Let $T^{*}=\\operatorname*{max}\\Big\\{T_{1}^{(D)},T_{1}^{(N D)},T_{2}^{(D)},T_{2}^{(N D)}\\Big\\},$ , where we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{T_{1}^{(D)}}}&{{=}}&{{\\displaystyle\\frac{1}{\\nu p_{10}D_{(}\\alpha/p_{10})}t\\log(t),}}\\\\ {{{T_{1}^{(N D)}}}}&{{=}}&{{\\displaystyle\\frac{1}{\\nu w D(\\alpha/w)}t\\log(n),}}\\\\ {{{T_{2}^{(D)}}}}&{{=}}&{{\\displaystyle\\frac{1}{\\nu e^{-\\nu}(1-p_{10})D(\\beta/p_{10})}t\\log(t),}}\\\\ {{{T_{2}^{(N D)}}}}&{{=}}&{{\\displaystyle\\frac{1}{\\nu p_{01}D(\\beta/p_{01})}t\\log(n).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $D(x)=x\\log(x)-x+1,$ , and $w=(1-p_{01})e^{-\\nu}+p_{10}(1-e^{-\\nu})$ . For any $\\alpha\\in\\left(p_{10},1-p_{01}\\right)$ , $\\beta\\in\\left(p_{01},1-p_{10}\\right)$ , there exist some number of tests $T<(1+\\epsilon)T^{*}$ where the Noisy $_{D D}$ algorithm produces $\\hat{\\mathbf{k}}$ such that in the limit as $n\\to\\infty$ we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left({\\hat{\\mathbf{k}}}\\neq\\mathbf{k}\\right)\\rightarrow0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The above result is state-of-art for noisy group testing and could be of interest generally for proving the type of results we have here, however, but for simplicity, we state a similar more compact result that suffices for our proofs in this paper. ", "page_idx": 34}, {"type": "text", "text": "Theorem F.4 ([54]). Let $|\\mathbf{k}|=t=o(n)$ , and consider an i.i.d. Bernoulli design group testing matrix. Further consider the binary symmetric noise model with crossover probability $q$ . If we construct $\\hat{\\mathbf{k}}$ via the noisy column matching algorithm, we achieve: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\hat{\\mathbf{k}}\\neq\\mathbf{k}\\right)\\leq n^{-\\beta},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "with number of tests ", "page_idx": 34}, {"type": "equation", "text": "$$\nT=\\frac{16(1+\\sqrt{\\gamma})^{2}(1+\\beta)\\ln(2)}{1-e^{-2}(1-2q)^{2}}t\\log(n).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\gamma$ is a constant that depends on $\\beta$ . ", "page_idx": 34}, {"type": "text", "text": "F.2 Group Testing Implementation ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We implement group testing via linear programming. As noted in [15], linear programming generally outperforms most other group testing algorithms in both the noisy and noiseless case. We use the ", "page_idx": 34}, {"type": "text", "text": "following linear program, to implement group testing. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\mathbf{k},\\xi}}&{~\\displaystyle\\sum_{i=1}^{n}k_{i}+\\lambda\\displaystyle\\sum_{p=1}^{P}\\xi_{j}}\\\\ {\\mathrm{s.t.}}&{k_{i}\\geq0}\\\\ &{\\xi_{p}\\geq0}\\\\ &{\\xi_{p}\\leq1\\quad p\\mathrm{~s.t.~}y_{p}=1}\\\\ &{\\mathbf{d}_{p}^{\\mathrm{TR}}=\\xi_{p}\\quad p\\mathrm{~s.t.~}y_{p}=0}\\\\ &{\\mathbf{d}_{p}^{\\mathrm{TR}}+\\xi_{p}\\geq1\\quad p\\mathrm{~s.t.~}y_{p}=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "G Impact Statement ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Rigorous tools for understanding models can potentially profoundly increase trust in deep learning systems. If we can understand and reason for ourselves why a model is making a decision, we can put greater trust into those decisions. Furthermore, if we understand why a model is doing something that we believe is incorrect, we can better steer it towards doing what we believe is correct. This \u201csteering\u201d of model behavior is sometimes described as alignment, and is a critical task for addressing things like incorrect or misleading information generated by a model, or for address any undesirable biases. In terms of concerns, it is important to not misinterpret or over-interpret the interaction indices that come out of SMT. It could be the case that looking over some selection of interactions doesn\u2019t reveal the full picture, and leads one down an incorrect line of reasoning. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 36}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 36}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 36}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 36}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \u201d[Yes] \u201d is generally preferable to \u201d[No] \u201d, it is perfectly acceptable to answer \u201d[No] \u201d provided a proper justification is given (e.g., \u201derror bars are not reported because it would be too computationally expensive\u201d or \u201dwe were unable to find the license for the dataset we used\u201d). In general, answering \u201d[No] \u201d or \u201d[NA] \u201d is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: ", "page_idx": 36}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\u201d, \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The abstract and introduction explicitly state the main theoretical contributions of this work. The main contributions section highlight this, and specifically call out the theoretical nature of this work. The introduction contains information about the practical applications and motivations. A detailed descriptions of assumptions is left to Section 2, to avoid an overly technical introduction, but it is made clear that there are limiting assumptions ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: There is a limitations section, and limitations are mentioned periodically when relevant. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: The proofs are included in the Appendix, and are complete and correct. Section 3 and 4 are effectively a proof sketch explaining how and why the algorithm works. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The algorithm is exactly stated in the paper, and implementation details (specifically related to group testing) are included in the appendix. A reader would be able to exactly reproduce our code with the given information. We plan to release the code with the camera ready version. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: Code is included. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The algorithms are relatively simple and complicated implementation details are in the appendix. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Error bars are included in the main paper. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Experiments are not run at very large scale and runtime numbers are included only for relative comparison. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: Ethical concerns are satisfied. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: Though this work is primarily theoretical, there is a clear application in mind, and thus is discussed in appendix section G. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: The details of the code are included in the main body of the paper. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]