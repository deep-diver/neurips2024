[{"figure_path": "w6vbfSC1y0/tables/tables_7_1.jpg", "caption": "Table 1: Comparison results on ImageNet-1k OOD benchmarks. All methods are trained on the same backbone CLIP-ViT-B/16. Bold numbers are superior results. \u2191 indicates larger values are better, and \u2193 indicates smaller values are better. Results marked with \u2020 are taken from [Wang et al., 2023] and [Miyai et al., 2024b]. The prompt tuning based methods are run under multiple trials with reporting the mean and standard deviation of the performance.", "description": "This table presents a comparison of various OOD detection methods on the ImageNet-1k dataset.  It shows the performance (FPR95 and AUROC) of zero-shot methods, post-hoc methods, and prompt-tuning based methods.  The table highlights the superior performance of certain methods (in bold) and indicates that prompt tuning methods generally perform better using few-shot data.  Results marked with \u2020 indicate values taken from cited papers.", "section": "4.2 Main results"}, {"figure_path": "w6vbfSC1y0/tables/tables_7_2.jpg", "caption": "Table 1: Comparison results on ImageNet-1k OOD benchmarks. All methods are trained on the same backbone CLIP-ViT-B/16. Bold numbers are superior results. \u2191 indicates larger values are better, and \u2193 indicates smaller values are better. Results marked with \u2020 are taken from [Wang et al., 2023] and [Miyai et al., 2024b]. The prompt tuning based methods are run under multiple trials with reporting the mean and standard deviation of the performance.", "description": "This table presents a comparison of various methods for out-of-distribution (OOD) detection on the ImageNet-1k dataset.  It compares zero-shot methods (MCM, GL-MCM), post-hoc methods (MSP, ODIN, Energy, ReAct, MaxLogit), and prompt-tuning based methods (CoOp, LoCoOp, IDLike, NegPrompt, LSN, SCT). The results are reported in terms of False Positive Rate at 95% True Positive Rate (FPR95) and Area Under the ROC Curve (AUROC).  The table shows the performance of each method under 1-shot and 16-shot training scenarios.", "section": "4.2 Main results"}, {"figure_path": "w6vbfSC1y0/tables/tables_8_1.jpg", "caption": "Table 3: OOD detection performance comparison with LoCoOp on hard OOD detection tasks. Bold numbers represents superior results.", "description": "This table presents a comparison of the OOD detection performance between the proposed SCT method and the baseline LoCoOp method on four different hard OOD detection tasks.  Each task involves a different combination of ImageNet subsets as ID and OOD datasets. The results are shown in terms of FPR95 and AUROC, with lower FPR95 and higher AUROC indicating better performance.  The bold numbers highlight where SCT outperforms LoCoOp.", "section": "4.3 Ablation study"}, {"figure_path": "w6vbfSC1y0/tables/tables_8_2.jpg", "caption": "Table 1: Comparison results on ImageNet-1k OOD benchmarks. All methods are trained on the same backbone CLIP-ViT-B/16. Bold numbers are superior results. \u2191 indicates larger values are better, and \u2193 indicates smaller values are better. Results marked with \u2020 are taken from [Wang et al., 2023] and [Miyai et al., 2024b]. The prompt tuning based methods are run under multiple trials with reporting the mean and standard deviation of the performance.", "description": "This table compares the performance of various OOD detection methods on the ImageNet-1k benchmark.  It includes zero-shot methods, CLIP-based post-hoc methods, and prompt-tuning based methods.  The results are presented in terms of FPR95 (False Positive Rate at 95% True Positive Rate) and AUROC (Area Under the Receiver Operating Characteristic curve), with lower FPR95 and higher AUROC indicating better performance. The table shows results for 1-shot and 16-shot settings for prompt-tuning based methods and averages across multiple trials.", "section": "4.2 Main results"}, {"figure_path": "w6vbfSC1y0/tables/tables_16_1.jpg", "caption": "Table 1: Comparison results on ImageNet-1k OOD benchmarks. All methods are trained on the same backbone CLIP-ViT-B/16. Bold numbers are superior results. \u2191 indicates larger values are better, and \u2193 indicates smaller values are better. Results marked with \u2020 are taken from [Wang et al., 2023] and [Miyai et al., 2024b]. The prompt tuning based methods are run under multiple trials with reporting the mean and standard deviation of the performance.", "description": "This table presents a comparison of various OOD detection methods on the ImageNet-1k benchmark.  It includes zero-shot methods, post-hoc methods, and prompt tuning based methods.  The table shows the FPR95 and AUROC metrics for each method across four different OOD datasets (iNaturalist, SUN, Places365, Textures), along with an average performance.  Results for prompt tuning methods are averaged over multiple trials, with standard deviations reported.", "section": "4.2 Main results"}, {"figure_path": "w6vbfSC1y0/tables/tables_16_2.jpg", "caption": "Table 6: ID classification performance of SCT and all the considered baselines. All methods are trained on the same backbone CLIP-ViT-B/16.", "description": "This table presents the in-distribution (ID) classification accuracy for various OOD detection methods, including the proposed SCT (Self-Calibrated Tuning) and several baselines (zero-shot methods, CLIP-based post-hoc methods, and prompt tuning based methods).  The results are shown for both 1-shot and 16-shot training scenarios, demonstrating the impact of the number of training samples on the ID classification accuracy, while also allowing comparison of the proposed method's performance with other OOD detection techniques.", "section": "4.2 Main results"}, {"figure_path": "w6vbfSC1y0/tables/tables_17_1.jpg", "caption": "Table 1: Comparison results on ImageNet-1k OOD benchmarks. All methods are trained on the same backbone CLIP-ViT-B/16. Bold numbers are superior results. \u2191 indicates larger values are better, and \u2193 indicates smaller values are better. Results marked with \u2020 are taken from [Wang et al., 2023] and [Miyai et al., 2024b]. The prompt tuning based methods are run under multiple trials with reporting the mean and standard deviation of the performance.", "description": "This table presents a comparison of various OOD detection methods on the ImageNet-1k dataset.  It includes zero-shot methods, CLIP-based post-hoc methods, and prompt tuning-based methods.  The results are presented for AUROC and FPR95 metrics, showing the performance of each method across multiple OOD datasets.  The table also distinguishes between 1-shot and 16-shot settings for the prompt-tuning methods, highlighting the impact of the number of training samples on performance.", "section": "4.2 Main results"}, {"figure_path": "w6vbfSC1y0/tables/tables_17_2.jpg", "caption": "Table 1: Comparison results on ImageNet-1k OOD benchmarks. All methods are trained on the same backbone CLIP-ViT-B/16. Bold numbers are superior results. \u2191 indicates larger values are better, and \u2193 indicates smaller values are better. Results marked with \u2020 are taken from [Wang et al., 2023] and [Miyai et al., 2024b]. The prompt tuning based methods are run under multiple trials with reporting the mean and standard deviation of the performance.", "description": "This table compares the performance of various OOD detection methods on the ImageNet-1k dataset.  It includes zero-shot methods, CLIP-based post-hoc methods, and prompt tuning-based methods. The results are shown for multiple OOD datasets and are evaluated using AUROC and FPR95 metrics.  Note that the prompt tuning methods utilize few-shot learning, and their results are averaged over multiple trials.", "section": "4.2 Main results"}, {"figure_path": "w6vbfSC1y0/tables/tables_18_1.jpg", "caption": "Table 1: Comparison results on ImageNet-1k OOD benchmarks. All methods are trained on the same backbone CLIP-ViT-B/16. Bold numbers are superior results. \u2191 indicates larger values are better, and \u2193 indicates smaller values are better. Results marked with \u2020 are taken from [Wang et al., 2023] and [Miyai et al., 2024b]. The prompt tuning based methods are run under multiple trials with reporting the mean and standard deviation of the performance.", "description": "This table presents a comparison of various methods for out-of-distribution (OOD) detection on the ImageNet-1k dataset.  It shows the performance (FPR95 and AUROC) of different methods, categorized into zero-shot methods, CLIP-based post-hoc methods, and prompt tuning-based methods. The results are broken down by OOD dataset (iNaturalist, SUN, Places365, Textures) and shot number (1-shot and 16-shot) for prompt-tuning methods, providing a comprehensive comparison of different approaches to OOD detection.", "section": "4.2 Main results"}, {"figure_path": "w6vbfSC1y0/tables/tables_18_2.jpg", "caption": "Table 10: Experiments on CIFAR benchmark with 16-shot data.", "description": "This table presents the results of experiments conducted on CIFAR-10 and CIFAR-100 datasets using 16-shot training data.  It compares the performance of three OOD detection methods: MCM, LoCoOp, and the proposed SCT (Self-Calibrated Tuning).  The metrics used for evaluation are FPR95 (False Positive Rate at 95% True Positive Rate), AUROC (Area Under the Receiver Operating Characteristic Curve), and ID-ACC (In-distribution accuracy). The table showcases the effectiveness of SCT in achieving better OOD detection performance compared to the baseline methods while maintaining comparable in-distribution accuracy.", "section": "4 Experiment"}, {"figure_path": "w6vbfSC1y0/tables/tables_18_3.jpg", "caption": "Table 1: Comparison results on ImageNet-1k OOD benchmarks. All methods are trained on the same backbone CLIP-ViT-B/16. Bold numbers are superior results. \u2191 indicates larger values are better, and \u2193 indicates smaller values are better. Results marked with \u2020 are taken from [Wang et al., 2023] and [Miyai et al., 2024b]. The prompt tuning based methods are run under multiple trials with reporting the mean and standard deviation of the performance.", "description": "This table compares the performance of various OOD detection methods on the ImageNet-1k dataset.  It includes zero-shot methods (MCM, GL-MCM), post-hoc methods (MSP, ODIN, Energy, ReAct, MaxLogit), and prompt-tuning based methods (CoOp, LoCoOp, IDLike, NegPrompt, LSN, SCT). The results are reported in terms of FPR95 and AUROC for multiple OOD datasets (iNaturalist, SUN, Places365, Textures) with both 1-shot and 16-shot settings, showing the effectiveness of the SCT method.", "section": "4.2 Main results"}, {"figure_path": "w6vbfSC1y0/tables/tables_20_1.jpg", "caption": "Table 12: Experiments on compatibility of Neg-Label and SCT on 16-shot data.", "description": "This table presents the results of experiments conducted to evaluate the compatibility of the proposed Self-Calibrated Tuning (SCT) method with the NegLabel method.  The experiment setup uses 16-shot data. The table shows the performance of NegLabel alone and NegLabel combined with SCT, using several metrics (FPR95, AUROC) across different OOD datasets (iNaturalist, SUN, Places365, Textures).  The results demonstrate how combining SCT with other state-of-the-art OOD detection methods can enhance their overall performance.", "section": "4.2 Main results"}]