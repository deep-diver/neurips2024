[{"figure_path": "h024LpF3bZ/figures/figures_4_1.jpg", "caption": "Figure 2: We show (a) the distribution of sample counts across different difficulty levels, and the average number of tokens per sample. We also exhibit (b) the distribution of time cost for human players and the number of tokens for each reference answer (200 samples in total) in difficulty levels.", "description": "This figure presents two subfigures visualizing data related to the difficulty levels in the SPLAT benchmark.  Subfigure (a) shows a bar chart illustrating the number of samples per difficulty level (1-9), overlaid with line charts showing the average number of tokens used in questions and answers for each difficulty level. Subfigure (b) displays a scatter plot correlating the time taken by human players to solve a puzzle with the number of tokens in the corresponding reference answer.  This scatter plot helps illustrate the relationship between puzzle complexity (as measured by solution length and time to solve) and difficulty level.", "section": "3.4 Statistics on SPLAT Benchmark"}, {"figure_path": "h024LpF3bZ/figures/figures_6_1.jpg", "caption": "Figure 2: We show (a) the distribution of sample counts across different difficulty levels, and the average number of tokens per sample. We also exhibit (b) the distribution of time cost for human players and the number of tokens for each reference answer (200 samples in total) in difficulty levels.", "description": "This figure shows two subfigures. Subfigure (a) presents a bar chart illustrating the distribution of situation puzzles across three difficulty levels (Easy, Medium, Hard), indicating the number of samples in each level and the average number of tokens in questions and answers for each level. Subfigure (b) displays a scatter plot showing the relationship between the time taken by human players to solve the puzzles and the length (number of tokens) of the reference answers for 200 samples across all difficulty levels.", "section": "3.4 Statistics on SPLAT Benchmark"}, {"figure_path": "h024LpF3bZ/figures/figures_8_1.jpg", "caption": "Figure 4: Performance of various LLMs on RiddleSense (dev set). Llama3 (8B & 70B) and GPT-4 are in the zero-shot setting, while others are trained on the training set of RiddleSense and CSQA [32]. '*' means models with our auxiliary reasoning prompts.", "description": "This figure shows the performance comparison of several LLMs on the RiddleSense benchmark.  It highlights the impact of using auxiliary reasoning prompts derived from the SPLAT benchmark.  Llama3 (8B & 70B) and GPT-4 are evaluated in a zero-shot setting, while other models are fine-tuned on RiddleSense and CSQA datasets. The '*' indicates models that incorporated auxiliary reasoning prompts from the SPLAT benchmark, demonstrating improved performance compared to their zero-shot counterparts.", "section": "5.3 Eliciting Lateral Thinking of LLMs"}]