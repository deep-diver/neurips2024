[{"figure_path": "h024LpF3bZ/tables/tables_3_1.jpg", "caption": "Table 1: Comparison between our SPLAT and related benchmarks, including both vertical (Ver.) and lateral (Lat.) thinking. For a comprehensive comparison, we evaluate the benchmarks across four dimensions: the target task, the fashion of answers, the evaluation paradigm, and the statistics of samples. 'Ref. A' indicates whether the question or dialogue comes with a reference answer. 'Open-E.' denotes whether the response is open-ended. \u2018Ref.-G.' and 'Mod.-B.' refer to whether the result evaluation is reference-guided or model-based, respectively. \u2018Assessment' denotes the specific evaluation pattern used to assess the quality of the predicted results. \u2018Avg. Q' and 'Avg. A' are the average number of tokens in questions (or input dialogues) and reference answers, respectively.", "description": "This table compares the SPLAT benchmark with other related benchmarks focusing on both vertical and lateral thinking.  It analyzes key characteristics like the type of task, answer format (open-ended or not), evaluation method (reference-guided, model-based, or other), and assessment strategies.  Statistical information such as the number of samples, average question length, and average answer length are also provided for each benchmark.", "section": "3 Related Benchmarks"}, {"figure_path": "h024LpF3bZ/tables/tables_7_1.jpg", "caption": "Table 2: Final answer agreement among three types of judges on SPLAT benchmark.", "description": "This table presents the level of agreement between three different judge models (WizardLM-2, Llama3-70B, and human judges) on the final answer accuracy of the SPLAT benchmark's puzzles. The agreement is measured separately for three difficulty levels: Easy, Medium, and Hard.  Each entry in the table shows the percentage agreement between the specified judge model and the human judgments.", "section": "5.1 Agreement Evaluation"}, {"figure_path": "h024LpF3bZ/tables/tables_7_2.jpg", "caption": "Table 3: Reasoning process agreement among three types of judges on SPLAT benchmark.", "description": "This table presents the agreement rate between three different types of judges (WizardLM-2, Llama3-70B, and human evaluators) on the reasoning process of solving situation puzzles within the SPLAT benchmark. The agreement is measured across three difficulty levels (easy, medium, and hard).  A higher percentage indicates a stronger agreement between the judge and human evaluations on the reasoning process.", "section": "5.1 Agreement Evaluation"}, {"figure_path": "h024LpF3bZ/tables/tables_8_1.jpg", "caption": "Table 4: Performance of different LLMs on the proposed SPLAT benchmark.", "description": "This table presents the performance of various Large Language Models (LLMs) on the Situation Puzzles for LAteral Thinking benchmark (SPLAT).  The performance is evaluated across three difficulty levels (Easy, Medium, Hard) and overall.  Three metrics are reported for each LLM and difficulty level: Accuracy (Acc, %), representing the percentage of correctly solved puzzles; Average Round (Rnd, \u2193), indicating the average number of interaction rounds needed to solve a puzzle (lower is better); and Overall (O/A, \u2191), a combined metric that considers both accuracy and the number of rounds (higher is better). The table allows for a comparison of LLMs' lateral thinking capabilities across difficulty levels.", "section": "5.2 Performance of LLMs on SPLAT Benchmark"}, {"figure_path": "h024LpF3bZ/tables/tables_9_1.jpg", "caption": "Table 4: Performance of different LLMs on the proposed SPLAT benchmark.", "description": "This table presents the performance of various Large Language Models (LLMs) on the Situation Puzzles for Lateral Thinking benchmark (SPLAT).  It shows the accuracy (Acc), average number of rounds (Rnd) needed to solve the puzzles, and an overall performance score (O/A) for each LLM across three difficulty levels (Easy, Medium, Hard) of the SPLAT benchmark.  The results highlight the varying capabilities of different LLMs in solving situation puzzles, demonstrating the challenges and opportunities in developing LLMs with stronger lateral thinking abilities.", "section": "5.2 Performance of LLMs on SPLAT Benchmark"}, {"figure_path": "h024LpF3bZ/tables/tables_9_2.jpg", "caption": "Table 6: Impact of our data and reasoning processes. Models with \u2018\u2020\u2019 mean using our data only while with \u2018*\u2019 mean using both data and reasoning processes. \u2018RS\u2019 refers to the results on RiddleSense (Dev). \u2018BT (S.)\u2019 and \u2018BT (W.)\u2019 are the overall results on Brain Teaser (Sentence) and Brain-Teaser (Word), respectively.", "description": "This table presents the results of applying the data and reasoning processes from the SPLAT benchmark to other lateral thinking benchmarks, RiddleSense and Brain Teaser.  It compares the performance of Llama3 8B and 70B models under three conditions: \n1. Original (no data or reasoning processes applied)\n2. Using only the SPLAT data\n3. Using both the SPLAT data and reasoning processes.\nThe results show that incorporating both the data and reasoning processes improves performance across all benchmarks and models.", "section": "5.3 Eliciting Lateral Thinking of LLMs"}]