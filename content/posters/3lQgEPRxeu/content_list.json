[{"type": "text", "text": "Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qinbo Bai Washim Uddin Mondal Vaneet Aggarwal Purdue University Indian Institute of Technology Kanpur Purdue University West Lafayette, IN 47906 Kanpur, UP, India 208016 West Lafayette, IN 47906 bai113@purdue.edu wmondal@iitk.ac.in vaneet@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDPs). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual-based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, our proposed algorithm achieves $\\tilde{\\mathcal{O}}(T^{4/5})$ objective regret and $\\tilde{\\mathcal{O}}(T^{4/5})$ constraint violation bounds. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The framework of Reinforcement Learning (RL) is concerned with a class of problems where an agent learns to yield the maximum cumulative reward in an unknown environment via repeated interaction. RL finds applications in diverse areas, such as wireless communication, transportation, and epidemic control [1, 2, 3]. RL problems are mainly categorized into three setups: episodic, infinite horizon discounted reward, and infinite horizon average reward. Among them, the infinite horizon average reward setup is particularly significant for real-world applications. It aligns with most of the practical scenarios and captures their long-term goals. Some applications in real life require the learning procedure to respect the boundaries of certain constraints. In an epidemic control setup, for example, vaccination policies must take the supply shortage (budget constraint) into account. Such restrictive decision-making routines are described by constrained Markov Decision Processes (CMDP) [4, 5, 6]. Existing papers on CMDPs utilize either a tabular or a linear MDP structure. This work provides the first algorithm for an infinite horizon average reward CMDP with general parametrization and proves its sub-linear regret and constraint violation bounds. ", "page_idx": 0}, {"type": "text", "text": "There are two primary ways to solve a CMDP problem in the infinite horizon average reward setting. The first one, known as the model-based approach, involves constructing estimates of the transition probabilities of the underlying CMDP, which are subsequently utilized to derive policies [6, 7, 5]. The caveat of this approach is the large memory requirement to store the estimated parameters, which effectively curtails its applicability to CMDPs with large state spaces. The alternative strategy, known as the model-free approach, either directly estimates the policy function or maintains an estimate of the $Q$ function, which is subsequently used for policy generation [8]. Model-free algorithms typically demand lower memory and computational resources than their model-based counterparts. Although the CMDP has been solved in a model-free manner in the tabular [8] and linear [9] setups, its exploration with the general parameterization is still open and is the goal of this paper. ", "page_idx": 0}, {"type": "text", "text": "General parameterization indexes the policies by finite-dimensional parameters (e.g., weights of neural networks) to accommodate large state spaces. The learning is manifested by updating these parameters using policy gradient (PG)-type algorithms. Note that PG algorithms are primarily studied in discounted reward setups. For example, [10] characterizes the sample complexities of the PG and the Natural PG (NPG) algorithms with softmax and direct parameterization. Similar results for general parameterization are obtained by [11, 12]. The regret analysis of a PG algorithm with the general parameterization has been recently performed for an infinite horizon average reward MDP without constraints [13]. Similar regret and constraint violation analysis for the average reward CMDP is still missing in the literature. In this paper, we bridge this gap. ", "page_idx": 0}, {"type": "table", "img_path": "3lQgEPRxeu/tmp/1175dcf55ecd4a8bd849db882af052986fc60e46d7a6c5ee15e57b15a7988c90.jpg", "table_caption": [], "table_footnote": ["Table 1: This table summarizes the different model-based and mode-free state-of-the-art algorithms available in the literature for average reward CMDPs. We note that our proposed algorithm is the first to analyze the regret and constraint violation for average reward CMDP with general parametrization. Here, the parameter $d$ refersto the dimension of the feature map for linear MDPs. "], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Challenges and Contribution: We propose a PG-based algorithm with general parameterized policies for the average reward CMDP and establish its sublinear regret and constraint violation bounds. In particular, assuming the underlying CMDP to be ergodic, we demonstrate that our PG algorithm achieves an average optimality rate of $\\tilde{\\mathcal{O}}(T^{-\\frac{1}{5}})$ and average constraint violation rate of $\\tilde{\\mathcal{O}}(T^{-{\\frac{1}{5}}})$ . Invoking this convergence result, we establish that our algorithm achieves regret and constraint violation bounds of $\\tilde{\\mathcal{O}}(T^{{\\frac{4}{5}}})$ . Apart from providing the first sublinear regret guarantee for the average reward CMDP with general parameterization, our work also improves the state-of-the-art regret guarantee, $\\tilde{\\mathcal{O}}(T^{5/6})$ in the model-free tabular setup [8]. ", "page_idx": 1}, {"type": "text", "text": "Despite the availability of sample complexity analysis of PG algorithms with constraints in the discounted reward setup [14, 4] and PG algorithms without constraint in average reward setup [13], obtaining sublinear regret and constraint violation bounds for their average reward counterpart is challenging. ", "page_idx": 1}, {"type": "text", "text": "\u00b7[14, 4] solely needs an estimate of the value function $V$ while we additionally need the estimate of the gain function, $J$   \n\u00b7[14, 4] assume access to a simulator to generate unbiased value estimates. In contrast, our algorithm uses a sample trajectory of length $H$ to estimate the values and gains and does not assume the availability of a simulator.   \n\u00b7The first-order convergence analysis (Lemma 6) differs from that in [13]. Note that both of these papers use an ascent-like inequality. In [13], this bounds the term $J(\\theta_{k+1})\\,-\\,J(\\theta_{k})$ .The final result is obtained by calculating a sum over $k$ which cancels the intermediate terms and leaves us with $J(\\theta_{K})-J(\\theta_{1}\\overset{\\cdot}{)}$ . We would like to emphasize that the cancellation of the intermediate terms is crucial to establishing the result. However, a similar effort in our case only leads to a bound of $J_{\\mathrm{L}}(\\theta_{k+1},\\lambda_{k})-J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})$ . Note that directly performing a sum over this difference does not lead to the cancellation of intermediate terms. We had to take a different route and apply the bounds of the Lagrange multipliers and the estimate of the constraint function to achieve that goal.   \n\u00b7After solving the problems mentioned above, we prove $\\tilde{\\mathcal{O}}(T^{-\\frac{1}{5}})$ convergence rate of the Lagrange function. Unfortunately, the strong duality property, which is central to proving convergence results of CMDPs for tabular and softmax policies, does not hold under the general parameterization. As a result, the convergence result for the dual problem does not automatically translate to that for the primal problem, which is a main difference from [13]. We overcome this barrier by introducing a novel constraint violation analysis and a series of intermediate results (Lemma 16-18) that help disentangle the regret and constraint violation rates from the Lagrange convergence. It is important to mention that although the techniques applied are inspired by the [14], those techniques cannot be directly adopted for average reward MDPs. This is primarily because the estimate $\\bar{J}_{c}(\\bar{\\theta}_{k})$ is biased in the average case. To the best of our knowledge, constraint violation analysis with a biased estimate of the cost value is not available in the literature and is performed for the first time in our paper. ", "page_idx": 1}, {"type": "text", "text": "\u00b7Due to the presence of the Lagrange multiplier, the convergence analysis of a CMDP is much more convoluted than its unconstrained counterpart. The learning rate of the Lagrange update, $\\beta$ ,turns out to be pivotal in determining the growth rate of regret and constraint violation. Low values of $\\beta$ push the regret down while simultaneously increasing the constraint violation. Finding the optimal value Oof $\\beta$ that judiciously balances these two competing goals is one of the cornerstones of our analysis. ", "page_idx": 2}, {"type": "text", "text": "Related work for unconstrained average reward RL: In the absence of constraints, both modelbased and model-free tabular setups have been widely studied for infinite horizon average reward MDPs. For example, the model-based algorithms proposed by [15, 16] achieve the optimal regret bound of $\\tilde{\\mathcal{O}}(\\sqrt{T})$ . Similarly, the model-free algorithm proposed by [17] for tabular MDP results in $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret. Regret analysis for average reward MDP with general parametrization has been recently studied in [13], where a regret bound of $\\tilde{\\mathcal{O}}(T^{3/4})$ is derived. ", "page_idx": 2}, {"type": "text", "text": "Related work for constrained RL: The constrained reinforcement learning problem has been extensively studied both for infinite horizon discounted reward and episodic MDPs. For example, discounted reward CMDPs have been recently studied in the tabular setup [18], with both softmax [14, 19], and general policy parameterization [14, 19, 4, 12]. Moreover, [20, 21, 22] investigated episodic CMDPs in the tabular setting. ", "page_idx": 2}, {"type": "text", "text": "Recently, the infinite horizon average reward CMDPs have been investigated in model-based setups [5, 6, 7], tabular model-free setting [8] and linear CMDP setting [9]. For model-based CMDP setup, [6] proposed a model-based online mirror descent algorithm in the ergodic setting which achieves $\\tilde{\\mathcal{O}}(\\sqrt{T})$ for regret and violation at the same time. [7] proposed algorithms based on the posterior sampling and the optimism principle that achieve $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret with zero constraint violations in the ergodic setting. However, the above model-based algorithms cannot be extended to large state space. In the tabular model-free setup, the algorithm proposed by [8] achieves a regret of $\\tilde{\\mathcal{O}}(T^{\\bar{5}/6})$ withzero constraint violations. Finally, in the linear CMDP setting, [9] achieves $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret bound with zero constraint violation. Note that the linear CMDP setting assumes that the transition probability has a certain linear structure with a known feature map which is not realistic. Table 1 summarizes all relevant works. Unfortunately, none of these papers study the infinite horizon average reward CMDPs with general parametrization which is the main focus of our article. ", "page_idx": 2}, {"type": "text", "text": "Additionally, for the weakly communicating setting, [6] proposed a model-based algorithm achieving $\\tilde{\\mathcal{O}}(T^{2/3})$ for both regret and violation in tabular case. [9] further extends such result to linear MDP setting with $\\tilde{\\mathcal{O}}(T^{3/4})$ regret and violation. In general, it is difficult to propose a model-free algorithm with provable guarantees for Constrained MDPs (CMDPs) without considering the ergodic model. [6] pointed out several extra challenges in Weakly communicating MDP compared to the ergodic case. For example, there is no uniform bound for the span of the value function for all stationary policies. It is also unclear how to estimate a policy's bias function accurately without the estimated model, which is an important step for estimating the policy gradient. ", "page_idx": 2}, {"type": "text", "text": "2Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This paper analyzes an infinite-horizon average reward constrained Markov Decision Process (CMDP) denoted as $\\mathcal{M}\\,=\\,(S,\\mathcal{A},r,c,P,\\rho)$ where $\\boldsymbol{S}$ denotes the state space, $\\boldsymbol{\\mathcal{A}}$ is the action space of size $A,\\,r:S\\times A\\to[0,1]$ is the reward function, $c:S\\times A\\to[-\\bar{1},1]$ is the constraint cost function, $P:S\\times A\\to\\Delta^{|S|}$ is the state transition function where $\\Delta^{\\vert S\\vert}$ denotes a probability simplex with dimension $|{\\cal S}|$ , and $\\rho\\in\\Delta^{|S|}$ is the initial distribution of states. A policy $\\pi\\in\\Pi:S\\to\\Delta^{A}$ maps the current state to an action distribution. The average reward and cost of a policy, $\\pi$ , is, ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ_{g,\\rho}^{\\pi}\\triangleq\\operatorname*{lim}_{T\\rightarrow\\infty}\\frac{1}{T}\\mathbf{E}\\bigg[\\sum_{t=0}^{T-1}g(s_{t},a_{t})\\bigg|s_{0}\\sim\\rho,\\pi\\bigg]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\textit{g}=\\textit{r},\\textit{c}$ for average reward and cost respectively. The expectation is calculated over the distribution of all sampled trajectories $\\{(s_{t},a_{t})\\}_{t=0}^{\\infty}$ Where $\\mathbf{\\Psi}_{t}\\sim\\bar{\\pi}(s_{t}),\\,s_{t+1}\\sim P(\\cdot|s_{t},a_{t}),\\forall t\\in$ $\\{0,1,\\cdots\\}$ . For notational convenience, we shall drop the dependence on $\\rho$ whenever there is no confusion. Our goal is to maximize the average reward function while ensuring that the average cost is above a given threshold. Without loss of generality, we can mathematically write this problem as, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi\\in\\Pi}\\;J_{r}^{\\pi}\\;\\;\\mathrm{s.t.}\\;J_{c}^{\\pi}\\ge0\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, the above problem becomes difficult to handle when the underlying state space, $\\boldsymbol{S}$ is large. Therefore, we consider a class of parametrized policies, $\\{\\pi_{\\theta}|\\theta\\in\\Theta\\}$ whose elements are indexed by a d-dimensional parameter, $\\theta\\in\\ensuremath{\\mathbb{R}}^{\\mathrm{d}}$ where $\\mathrm{d}\\ll|\\boldsymbol{S}||\\boldsymbol{A}|$ . Thus, the original problem in Eq (2) can be reformulated as the following parameterized problem. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta\\in\\Theta}\\;J_{r}^{\\pi_{\\theta}}\\;\\mathrm{~s.t.~}J_{c}^{\\pi_{\\theta}}\\ge0\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We denote $J_{g}^{\\pi_{\\theta}}=J_{g}(\\theta)$ \uff0c $g\\in\\{r,c\\}$ for notational convenience. Let, $P^{\\pi_{\\theta}}:{\\cal S}\\rightarrow\\Delta^{|S|}$ be a transition function induced by $\\pi_{\\theta}$ and defined as, $\\begin{array}{r}{P^{\\pi_{\\theta}}(s,s^{\\prime})=\\sum_{a\\in\\mathcal{A}}P(s^{\\prime}|s,a)\\pi_{\\theta}(a|s),\\forall s,s^{\\prime}}\\end{array}$ If $\\mathcal{M}$ is such that for every policy $\\pi$ , the function, $P^{\\pi}$ is irreducible and aperiodic, then $\\mathcal{M}$ is called ergodic. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. The CMDP $\\mathcal{M}$ is ergodic. ", "page_idx": 3}, {"type": "text", "text": "Ergodicity is a common assumption in the literature [23, 24]. If $\\mathcal{M}$ is ergodic, then $\\forall\\theta$ , there exists a unique stationary distribution, $\\bar{d}^{\\pi_{\\theta}}\\in\\Delta^{|S|}$ given as follows. ", "page_idx": 3}, {"type": "equation", "text": "$$\nd^{\\pi_{\\theta}}(s)=\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\sum_{t=0}^{T-1}\\operatorname*{Pr}(s_{t}=s|s_{0}\\sim\\rho,\\pi_{\\theta})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Ergodicity implies that $d^{\\pi_{\\theta}}$ is independent of the initial distribution, $\\rho$ and obeys $P^{\\pi_{\\theta}}d^{\\pi_{\\theta}}=d^{\\pi_{\\theta}}$ Hence, the average reward and cost functions can be expressed as, ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ_{g}(\\theta)=\\mathbf{E}_{s\\sim d^{\\pi_{\\theta}},a\\sim\\pi_{\\theta}(s)}[g(s,a)]=(d^{\\pi_{\\theta}})^{T}g^{\\pi_{\\theta}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{g^{\\pi_{\\theta}}(s)\\triangleq\\sum_{a\\in\\mathcal{A}}g(s,a)\\pi_{\\theta}(a|s)}\\end{array}$ \uff0c $g\\in\\{r,c\\}$ Note that the functions $J_{g}(\\theta),g\\in\\{r,c\\}$ are also independent of the inital distribution, $\\rho$ Furthermore, $\\forall\\theta$ ,there exist a function $Q_{g}^{\\pi_{\\theta}}:S\\times\\mathcal{A}\\rightarrow\\mathbb{R}$ such that the following Bellman equation is satisfied $\\forall(s,a)\\in S\\times A$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ_{g}^{\\pi_{\\theta}}(s,a)=g(s,a)-J_{g}(\\theta)+{\\bf E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[V_{g}^{\\pi_{\\theta}}(s^{\\prime})\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g\\in\\{r,c\\}$ and $V_{g}^{\\pi_{\\theta}}:\\mathcal{S}\\rightarrow\\mathbb{R}$ is given as $\\begin{array}{r}{V_{g}^{\\pi_{\\theta}}(s)=\\sum_{a\\in\\mathcal{A}}\\pi_{\\theta}(a|s)Q_{g}^{\\pi_{\\theta}}(s,a)}\\end{array}$ \uff0c $\\forall s\\in S$ Note that if $Q_{g}^{\\pi_{\\theta}}$ satisfies (6),then itis also satisfedby $Q_{g}^{\\pi_{\\theta}}+c$ for any arbitrary, $c$ To uniquely define the value functions, we assume that $\\begin{array}{r}{\\sum_{s\\in S}d^{\\pi_{\\theta}}(s)V_{g}^{\\pi_{\\bar{\\theta}}}(s)=0}\\end{array}$ . In this case, $V_{g}^{\\pi_{\\theta}}(s)$ is given by, ", "page_idx": 3}, {"type": "equation", "text": "$$\nV_{g}^{\\pi_{\\theta}}(s)=\\sum_{t=0}^{\\infty}\\sum_{s^{\\prime}\\in S}\\left[(P^{\\pi_{\\theta}})^{t}(s,s^{\\prime})-d^{\\pi_{\\theta}}(s^{\\prime})\\right]g^{\\pi_{\\theta}}(s^{\\prime})=\\sum_{t=0}^{\\infty}\\mathbf{E}\\left[\\left\\{g(s_{t},a_{t})-J_{g}(\\theta)\\right\\}\\big|s_{0}=s\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the expectation is computed over all $\\pi_{\\theta}$ -induced trajectories. In a similar way, $\\forall(s,a)$ , one can uniquely define $Q_{g}^{\\pi_{\\theta}}(s,a)$ \uff0c $\\bar{\\boldsymbol{g}^{\\in}}\\left\\{\\boldsymbol{r},\\boldsymbol{c}\\right\\}$ as follows. ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ_{g}^{\\pi_{\\theta}}(s,a)=\\sum_{t=0}^{\\infty}\\mathbf{E}\\left[\\left\\{g(s_{t},a_{t})-J_{g}(\\theta)\\right\\}\\middle|s_{0}=s,a_{0}=a\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, the advantage function $A_{g}^{\\pi_{\\theta}}:S\\times\\mathcal{A}\\rightarrow\\mathbb{R}$ is defined such that $A_{g}^{\\pi_{\\theta}}(s,a)\\triangleq Q_{g}^{\\pi_{\\theta}}(s,a)-$ $V_{g}^{\\pi_{\\theta}}(s),\\,\\forall(s,a),\\,\\forall g\\;\\in\\;\\{r,c\\}$ . Assumption 1 also implies the existence of a finite mixing time. Specifically, for an ergodic MDP, $\\mathcal{M}$ , the mixing time is defined as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 1. The mixing time, $t_{\\mathrm{mix}}^{\\theta}$ , of the CMDP $\\mathcal{M}$ for a parameterized policy, $\\pi_{\\theta}$ , is defined as, $\\begin{array}{r}{t_{\\mathrm{mix}}^{\\theta}\\triangleq\\operatorname*{min}\\left\\{t\\geq1\\big|\\|(P^{\\pi_{\\theta}})^{t}(s,\\cdot)-d^{\\pi_{\\theta}}\\|\\leq\\frac{1}{4},\\forall s\\right\\}}\\end{array}$ The overll mixing time is $t_{\\mathrm{mix}}\\triangleq\\operatorname*{sup}_{\\theta\\in\\Theta}t_{\\mathrm{mix}}^{\\theta}$ In this paper, $t_{\\mathrm{mix}}$ is finite due to ergodicity. ", "page_idx": 3}, {"type": "text", "text": "Mixing time characterizes how fast a CMDP converges to its stationary state distribution, $d^{\\pi_{\\theta}}$ ,under a given policy, $\\pi_{\\theta}$ . We also define the hitting time as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 2. The hitting time of an ergodic CMDP $\\mathcal{M}$ with respect to a policy, $\\pi_{\\theta}$ , is defined as $t_{\\mathrm{hit}}^{\\theta}\\triangleq\\operatorname*{max}_{s\\in S}[d^{\\pi_{\\theta}}(s)]^{-1}$ The overll hitin time is deined s $\\textstyle t_{\\mathrm{hit}}\\triangleq\\operatorname*{sup}_{\\theta\\in\\Theta}t_{\\mathrm{hit}}^{\\theta}$ . In thispaper, $t_{\\mathrm{hit}}$ is finite due to ergodicity as well. ", "page_idx": 3}, {"type": "text", "text": "Define $\\pi^{*}$ as the optimal solution to the unparameterized problem (2). For a given CMDP $\\mathcal{M}$ ,and a time horizon $T$ , the regret and constraint violation of any algorithm A is defined as follows. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{T}(\\mathbb{A},\\mathcal{M})\\triangleq\\sum_{t=0}^{T-1}\\left(J_{r}^{\\pi^{*}}-r(s_{t},a_{t})\\right),\\ \\mathrm{Vio}_{T}(\\mathbb{A},\\mathcal{M})\\triangleq-\\sum_{t=0}^{T-1}c(s_{t},a_{t})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the algorithm, A, executes the actions, $\\{a_{t}\\},t\\in\\{0,1,\\cdots\\}$ based on the trajectory observed up to time, $t$ , and the state, $s_{t+1}$ is decided according to the state transition function, $P$ . For simplicity, we shall denote the regret and constraint violation as $\\mathrm{Reg}_{T}$ and $\\mathrm{Vio}_{T}$ respectively. Our goal is to design an algorithm A that achieves low regret and constraint violation bounds. ", "page_idx": 4}, {"type": "text", "text": "3 Proposed Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We solve (3) via a primal-dual algorithm based on the following problem. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta\\in\\Theta}\\operatorname*{min}_{\\lambda\\geq0}\\;J_{\\mathrm{L}}(\\theta,\\lambda),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $J_{\\mathrm{L}}(\\theta,\\lambda)\\triangleq J_{r}(\\theta)+\\lambda J_{c}(\\theta)$ . The function, $J_{\\mathrm{L}}(\\cdot,\\cdot)$ , is called the Lagrange function and $\\lambda$ the Lagrange multiplier. Our algorithm updates the pair $(\\theta,\\lambda)$ following the policy gradient iteration as shown below $\\forall k\\in\\{1,\\cdots\\,,K\\}$ with an initial point $(\\theta_{1},\\lambda_{1})$ \uff0c $\\lambda_{1}=0$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{k+1}=\\theta_{k}+\\alpha\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k}),\\;\\lambda_{k+1}=\\mathcal{P}_{[0,\\frac{2}{\\delta}]}[\\lambda_{k}-\\beta J_{c}(\\theta_{k})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha$ and $\\beta$ are learning parameters and $\\delta$ is the Slater parameter introduced in the following assumption. Finally, for any set, $\\Lambda,\\mathcal{P}_{\\Lambda}[\\cdot]$ denotes projection onto $\\Lambda$ . The assumption stated below ensures that we have at least one feasible interior point solution to (2). ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Slater condition). There exists a $\\delta\\in(0,1)$ and $\\bar{\\theta}\\in\\Theta$ such that $J_{c}(\\bar{\\theta})\\ge\\delta$ ", "page_idx": 4}, {"type": "text", "text": "Note that in (11), the dual update is projected onto the set $[0,{\\frac{2}{\\delta}}]$ because the optimal dual variable for the parameterized problem is bounded in Lemma 16. The gradient of $J_{\\mathrm{L}}(\\cdot,\\lambda)$ can be computed by invoking a variant of the well-known policy gradient theorem [25]. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. The gradient of $J_{\\mathrm{L}}(\\cdot,\\lambda)$ is computed as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta,\\lambda)=\\mathbf{E}_{s\\sim d^{\\pi_{\\theta}},a\\sim\\pi_{\\theta}(s)}\\big[A_{\\mathrm{L},\\lambda}^{\\pi_{\\theta}}(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\big]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Primal-Dual Parameterized Policy Gradient ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1: Input: Episode length $H$ , learning rates $\\alpha,\\beta$ , initial parameters $\\theta_{1},\\lambda_{1}$ , initial state $s_{0}\\sim\\rho(\\cdot)$ \uff0c   \n2: $K=T/H$   \n3: for $k\\in\\{1,\\cdots,K\\}$ do   \n4: $\\mathcal{T}_{k}\\gets\\phi$   \n5: for $t\\in\\{(k-1)H,\\cdots,k H-1\\}$ do   \n6: Execute $a_{t}\\sim\\pi_{\\theta_{k}}(\\cdot|s_{t})$   \n7: Observe $r(s_{t},a_{t})$ \uff0c $c(s_{t},a_{t})$ and $s_{t+1}$   \n8: $\\mathcal{T}_{k}\\gets\\mathcal{T}_{k}\\cup\\{(s_{t},a_{t})\\}$   \n9: end for   \n10: for $t\\in\\{(k-1)H,\\cdots,k H-1\\}$ do   \n11: Obtain $\\hat{A}_{\\mathrm{L},\\lambda_{k}}^{\\pi_{\\theta_{k}}}(s_{t},a_{t})$ via Algorithm 2 and $\\mathcal{T}_{k}$   \n12: end for   \n13: Compute $\\omega_{k}$ using (15)   \n14: Update the parameters:   \n$\\begin{array}{r l}&{\\theta_{k+1}=\\theta_{k}+\\alpha\\omega_{k},}\\\\ &{\\lambda_{k+1}=\\mathcal{P}_{[0,\\frac{2}{\\delta}]}\\left[\\lambda_{k}-\\beta\\hat{J}_{c}(\\theta_{k})\\right]}\\\\ &{\\mathrm{~where~}\\hat{J}_{c}(\\theta_{k})=\\displaystyle\\frac{1}{H-N}\\sum_{t=(k-1)H+N}^{k H-1}c(s_{t},a_{t})}\\end{array}$ (12) ", "page_idx": 4}, {"type": "text", "text": "15: end for ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "where $\\forall(s,a),\\,A_{\\mathrm{L},\\lambda}^{\\pi_{\\theta}}(s,a)\\triangleq A_{r}^{\\pi_{\\theta}}(s,a)+\\lambda A_{c}^{\\pi_{\\theta}}(s,a),$ and $\\{A_{g}^{\\pi_{\\theta}}\\}_{g\\in\\{r,c\\}}$ are the advantage functions corresponding to reward and cost. In typical RL scenarios, learners do not have access to the state transition function, $P$ and thereby to the functions $d^{\\pi_{\\theta}}$ and $A_{\\mathrm{L},\\lambda}^{\\pi_{\\theta}}$ . This makes computing the exact gradient a difficult task. In Algorithm 1, we demonstrate how one can still obtain good estimates of the gradient using sampled trajectories. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 runs $K$ epochs, each of duration $H=16t_{\\mathrm{hit}}t_{\\mathrm{mix}}T^{\\xi}(\\log T)^{2}$ where $\\xi\\in(0,1)$ defines a constant whose value is specified later. Clearly, $K=T/H$ . Note that the learner is assumed to know the horizon length, $T$ . This can be relaxed utilizing the well-known doubling trick [26]. Additionally, it is assumed that the algorithm is aware of the mixing time and the hitting time. This assumption is common in the literature [13, 17]. The first step in obtaining a gradient estimate is estimating the advantage value for a given pair $(s,a)$ . This can be accomplished via Algorithm 2. At the $k$ th epoch, a $\\pi_{\\theta_{k}}$ -induced rajectory, $\\boldsymbol{\\mathcal{T}_{k}}=\\{(s_{t},a_{t})\\}_{t=(k-1)H}^{k H-1}$ is obtained and subsequently passed to Algorithm 2 that searches for subtrajectories within it that start with a given state $s$ , are of length $N=4t_{\\mathrm{mix}}(\\log T)$ , and are at least $N$ distance apart from each other. Assume that there are $M$ such subtrajectories. Let the total reward and cost of the $i$ th subtrajectory be $\\{r_{i},c_{i}\\}$ respectively and $\\tau_{i}$ be its starting time. The value function estimates for the $k$ th epoch are ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{Q}_{g}^{\\pi_{\\theta_{k}}}(s,a)=\\frac{1}{\\pi_{\\theta_{k}}(a|s)}\\left[\\frac{1}{M}\\sum_{i=1}^{M}g_{i}1(a_{\\tau_{i}}=a)\\right],\\ \\hat{V}_{g}^{\\pi_{\\theta_{k}}}(s)=\\frac{1}{M}\\sum_{i=1}^{M}g_{i},\\ \\forall g\\in\\{r,c\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This leads to the following advantage estimator. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{A}_{\\mathrm{L},\\lambda_{k}}^{\\pi_{\\theta_{k}}}(s,a)=\\hat{A}_{r}^{\\pi_{\\theta_{k}}}(s,a)+\\lambda_{k}\\hat{A}_{c}^{\\pi_{\\theta_{k}}}(s,a),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{A}_{g}^{\\pi_{\\theta_{k}}}(s,a)=\\hat{Q}_{g}^{\\pi_{\\theta_{k}}}(s,a)-\\hat{V}_{g}^{\\pi_{\\theta_{k}}}(s)$ \uff0c $g\\in\\{r,c\\}$ .Finally, the gradient estimator is, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\omega_{k}\\triangleq\\hat{\\nabla}_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})=\\frac{1}{H}\\sum_{t=t_{k}}^{t_{k+1}-1}\\hat{A}_{\\mathrm{L},\\lambda_{k}}^{\\pi_{\\theta_{k}}}(s_{t},a_{t})\\nabla_{\\theta}\\log\\pi_{\\theta_{k}}(a_{t}|s_{t})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $t_{k}=(k-1)H$ is the starting time of the $k$ th epoch. The parameters are updated following (12). To update the Lagrange multiplier, we need an estimation of $J_{c}(\\theta_{k})$ , which is obtained as the average cost of the $k$ th epoch. It should be noted that we remove the first $N$ samples from the $k$ th epoch because we require the state distribution emanating from the remaining samples to be close enough to the stationary distribution $d^{\\pi_{\\theta_{k}}}$ , which is the key to make $\\hat{J}_{c}(\\theta_{k})$ close to $J_{c}(\\theta_{k})$ . The following lemma demonstrates that $\\hat{A}_{\\mathrm{L},\\lambda_{k}}^{\\pi_{\\theta_{k}}}(s,a)$ is a good estimator of $A_{\\mathrm{L},\\lambda_{k}}^{\\pi_{\\theta_{k}}}(s,a)$ ", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 Advantage Estimation   \n1: Input: Trajectory $\\left({{s_{t}}_{1}},{{a}_{t}}_{1},...,{{s}_{t}}_{2},{{a}_{t}}_{2}\\right)$ state $s$ action $a$ , Lagrange multiplier $\\lambda$ , and parameter   \n\u03b8   \n2: Initialize: $M\\gets0$ $\\tau\\gets t_{1}$   \n3: Define: $N=4t_{\\mathrm{mix}}\\log_{2}T$   \n4: while $\\tau\\leq t_{2}-N$ do   \n5:if $s_{\\tau}=s$ then   \n6: $\\begin{array}{r l}&{\\dot{M}\\gets M+1,\\ \\tau_{M}\\gets\\tau}\\\\ &{g_{M}\\gets\\sum_{t=\\tau}^{\\tau+N-1}g(s_{t},a_{t}),\\ \\forall g\\in\\{r,c\\}}\\\\ &{\\tau\\gets\\tau+2N.}\\end{array}$   \n78   \n9: else   \n10: $\\tau\\gets\\tau+1$   \n11: end if   \n12: end while   \n13: if $M>0$ then   \n14:Compute $\\hat{Q}_{g}(s,a),\\hat{V}_{g}(s)$ via (13), $\\forall g\\in\\{r,c\\}$   \n15: else   \n16: $\\hat{V}_{g}(s)=0$ $:0,\\hat{Q}_{g}(s,a)=0,\\;\\;\\forall g\\in\\{r,c\\}$   \n17: end if   \n18: return $(\\hat{Q}_{r}(s,a)-\\hat{V}_{r}(s))+\\lambda(\\hat{Q}_{c}(s,a)-\\hat{V}_{c}(s))$ ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. The following inequality holds $\\forall k$ $\\forall(s,a)$ and sufficiently large $T$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left(\\hat{A}_{\\mathrm{L},\\lambda_{k}}^{\\pi_{\\theta_{k}}}(s,a)-A_{\\mathrm{L},\\lambda_{k}}^{\\pi_{\\theta_{k}}}(s,a)\\right)^{2}\\right]\\leq\\mathcal{O}\\left(\\frac{t_{\\mathrm{hit}}N^{3}\\log T}{\\delta^{2}H\\pi_{\\theta_{k}}(a|s)}\\right)=\\mathcal{O}\\left(\\frac{t_{\\mathrm{mix}}^{2}(\\log T)^{2}}{\\delta^{2}T^{5}\\pi_{\\theta_{k}}(a|s)}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 2 shows that the $L_{2}$ error of our proposed advantage estimator can be bounded above as $\\tilde{\\mathcal{O}}(T^{-\\xi})$ . We later utilize the above result to prove the goodness of the gradient estimator. It is to be clarified that our Algorithm 2 is inspired by Algorithm 2 of [17]. However, while the authors of [17] choose $H=\\tilde{\\mathcal{O}}(1)$ ,we adapt $H=\\bar{\\mathcal{O}}(T^{\\xi})$ . This subtle change is important in proving a sublinear regret for general parametrization. ", "page_idx": 6}, {"type": "text", "text": "4  Global Convergence Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section frst shows that the sequence $\\{\\theta_{k},\\lambda_{k}\\}_{k=1}^{K}$ producedby Algorithm1is such that their associated Lagrange sequence $\\{J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\}_{k=1}^{\\infty}$ converges globally. By expanding the Lagrange function,we thenexhibit convergenceo each ofiscomponents $\\{J_{g}(\\theta_{k},\\lambda_{k})\\}_{k=1}^{K}$ $g\\in\\{r,c\\}$ This is later used for regret and constraint violation analysis. Before delving into the details, we would like to state a few necessary assumptions. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3. The score function (stated below) is $G$ -Lipschitz and $B$ -smooth. Specifically, $\\forall\\theta,\\theta_{1},\\theta_{2}\\in\\Theta$ , and $\\forall(s,a)$ , the following inequalities hold. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\|\\le G,\\,\\,\\|\\nabla_{\\theta}\\log\\pi_{\\theta_{1}}(a|s)-\\nabla_{\\theta}\\log\\pi_{\\theta_{2}}(a|s)\\|\\le B\\|\\theta_{1}-\\theta_{2}\\|\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 1. The Lipschitz and smoothness properties of the score function are commonly assumed for policy gradient analyses [27, 28, 29]. These assumptions hold for simple parameterization classes such as Gaussian policies. ", "page_idx": 6}, {"type": "text", "text": "Note that by combining Assumption 3 with Lemma 2 and using the gradient estimator as given in (15), one can deduce the following result. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3. The following inequality holds k provided that assumptions $^{\\,l}$ and $^3$ aretrue. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left\\Vert\\omega_{k}-\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\right\\Vert^{2}\\right]\\leq\\tilde{\\mathcal{O}}\\left(\\delta^{-2}A G^{2}t_{\\mathrm{mix}}^{2}T^{-\\xi}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Lemma 3 claims that the gradient estimation error can be bounded as $\\tilde{\\mathcal{O}}(T^{-\\xi})$ . We will use this result later to prove the global convergence of our algorithm. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4. Let the transferred compatible function approximation error be defined as follows. ", "page_idx": 6}, {"type": "equation", "text": "$$\nL_{d^{\\pi^{*}},\\pi^{*}}(\\omega_{\\theta,\\lambda}^{*},\\theta,\\lambda)=\\mathbf{E}_{s\\sim d^{\\pi^{*}}}\\mathbf{E}_{a\\sim\\pi^{*}(s)}\\Bigg[\\Bigg(\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\cdot\\boldsymbol{\\omega}_{\\theta,\\lambda}^{*}-A_{\\mathrm{L},\\lambda}^{\\pi_{\\theta}}(s,a)\\Bigg)^{2}\\Bigg]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\pi^{*}$ is the optimal solution of unparameterized problem in (2) and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\omega_{\\theta,\\lambda}^{*}=\\arg\\operatorname*{min}_{\\omega\\in\\mathbb{R}^{\\mathrm{d}}}\\,\\mathbf{E}_{s\\sim d^{\\pi_{\\theta}}}\\mathbf{E}_{a\\sim\\pi_{\\theta}(s)}\\bigg[\\bigg(\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\cdot\\omega-A_{\\mathrm{L},\\lambda}^{\\pi_{\\theta}}(s,a)\\bigg)^{2}\\bigg]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We assume that $\\begin{array}{r}{L_{d^{\\pi^{*}},\\pi^{*}}(\\omega_{\\theta,\\lambda}^{*},\\theta,\\lambda)\\le\\epsilon_{\\mathrm{bias}},\\lambda\\in[0,\\frac{2}{\\delta}]}\\end{array}$ and $\\theta\\in\\Theta$ where $\\epsilon_{\\mathrm{bias}}$ is a positive constant. Remark 2. The transferred compatible function approximation error quantifies the expressivity of the parameterized policy class. We can show that $\\epsilon_{\\mathrm{bias}}=0$ for softmax parameterization [10] and linear MDPs [30]. If the policy class is restricted, i.e., it does not contain all stochastic policies, $\\epsilon_{\\mathrm{bias}}$ turns out to be strictly positive. However, if the policy class is parameterized by a rich neural network, then $\\epsilon_{\\mathrm{bias}}$ can be assumed to be negligibly small [31]. Such assumptions are common [29, 10]. Remark 3. Note that $\\omega_{\\theta,\\lambda}^{*}$ defined in (19) can be written as, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\omega_{\\theta,\\lambda}^{*}=F_{\\rho}(\\theta)^{\\dagger}\\mathbf{E}_{s\\sim d_{\\rho}^{\\pi_{\\theta}}}\\mathbf{E}_{a\\sim\\pi_{\\theta}(s)}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)A_{\\mathrm{L},\\lambda}^{\\pi_{\\theta}}(s,a)\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\dagger$ is the Moore-Penrose pseudoinverse and $F_{\\rho}(\\theta)$ is the Fisher information matrix defined as, ", "page_idx": 6}, {"type": "equation", "text": "$$\nF_{\\rho}(\\theta)=\\mathbf{E}_{s\\sim d_{\\rho}^{\\pi_{\\theta}}}\\mathbf{E}_{a\\sim\\pi_{\\theta}(\\cdot|s)}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)(\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s))^{T}]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Assumption 5. There exists a constant $\\mu_{F}\\,>\\,0$ such that $F_{\\rho}(\\theta)-\\mu_{F}I_{\\mathrm{d}}$ is positive semidefinite where $I_{\\mathrm{d}}$ is an identity matrix of dimension, d. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5 is also called Fisher-non-degenerate policy assumption and is quite common in the literature [29, 32, 33] in the policy gradient analysis. [29][Assumption 2.1] provided a detailed discussion on the requirement of policy class to satisfy the assumption 5. Moreover, [34] describes a class of policies that obeys assumptions $3-5$ simultaneously. The Lagrange difference lemma stated below is important in establishing global convergence. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4. With a slight abuse of notation, let $J_{\\mathrm{L}}(\\pi,\\lambda)=J_{r}^{\\pi}+\\lambda J_{c}^{\\pi}$ For any two policies $\\pi,\\,\\pi^{\\prime}$ the followingresult holds $\\forall\\lambda>0$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\boldsymbol{J}_{\\mathrm{L}}(\\pi,\\lambda)-\\boldsymbol{J}_{\\mathrm{L}}(\\pi^{\\prime},\\lambda)=\\mathbf{E}_{s\\sim d^{\\pi}}\\mathbf{E}_{a\\sim\\pi(s)}\\left[\\boldsymbol{A}_{\\mathrm{L},\\lambda}^{\\pi^{\\prime}}(s,a)\\right]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We now present a general framework for the convergence analysis of Algorithm 1. ", "page_idx": 7}, {"type": "text", "text": "Lemma5. If the oliey parameters, $\\{\\theta_{k},\\lambda_{k}\\}_{k=1}^{K}$ are updated via 12 and assumpions 3 4,and 5 hold, then we have the following inequality for any ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}\\bigg(J_{\\mathrm{L}}(\\pi^{*},\\lambda_{k})-J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\bigg)\\leq\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\displaystyle\\frac{G}{K}\\sum_{k}^{K}\\mathbf{E}\\|(\\omega_{k}-\\omega_{k}^{*})\\|+\\displaystyle\\frac{B\\alpha}{2K}\\sum_{k=1}^{K}\\mathbf{E}\\|\\omega_{k}\\|^{2}}&{}\\\\ {\\displaystyle+\\,\\displaystyle\\frac{1}{\\alpha K}\\mathbf{E}_{s\\sim d^{\\pi^{*}}}[K L(\\pi^{*}(\\cdot|s)\\|\\pi_{\\theta_{1}}(\\cdot|s))]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\omega_{k}^{*}:=\\omega_{\\theta_{k},\\lambda_{k}}^{*},\\,\\omega_{\\theta_{k},\\lambda_{k}}^{*}$ is defined in (19), and $\\pi^{*}$ isthe optimal solution tothe problem 2). ", "page_idx": 7}, {"type": "text", "text": "Lemma 5 proves that the optimality error of the Lagrange sequence can be bounded by the average first-order and second-order norms of the intermediate gradients. Note the presence of $\\epsilon_{\\mathrm{bias}}$ in the result. If the policy class is severely restricted, the optimality bound loses its importance. Consider the expectation of the second term in (20). Note that, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\slash}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}\\big\\|\\omega_{k}-\\omega_{k}^{*}\\|\\Big)^{2}\\leq\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}\\bigg[\\|\\omega_{k}-\\omega_{k}^{*}\\|^{2}\\bigg]=\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}\\bigg[\\|\\omega_{k}-F_{\\rho}(\\theta_{k})^{\\dagger}\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}\\bigg]}\\\\ {\\displaystyle\\leq\\displaystyle\\frac{2}{K}\\displaystyle\\sum_{k=1}^{K}\\Bigg\\{\\mathbf{E}\\bigg[\\|\\omega_{k}-\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}\\bigg]+\\mathbf{E}\\bigg[\\|\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-F_{\\rho}(\\theta_{k})^{\\dagger}\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}\\bigg]\\Bigg\\}}\\\\ {\\displaystyle\\overset{(a)}{\\leq}\\displaystyle\\frac{2}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}\\bigg[\\|\\omega_{k}-\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}\\bigg]+\\displaystyle\\frac{2}{K}\\displaystyle\\sum_{k=1}^{K}\\bigg(1+\\displaystyle\\frac{1}{\\mu_{F}^{2}}\\bigg)\\,\\mathbf{E}\\bigg[\\|\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $(a)$ follows from Assumption 5. The expectation of the third term in (20) can be bounded as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\bigg[\\|\\omega_{k}\\|^{2}\\bigg]\\leq\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\left[\\|\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}\\right]+\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\bigg[\\|\\omega_{k}-\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}\\bigg]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In both (4) and (20), $\\mathbf{E}\\big\\|\\omega_{k}-\\nabla_{\\theta}J_{\\mathrm{L}}\\big(\\theta_{k},\\lambda_{k}\\big)\\big\\|^{2}$ is bounded above by Lemma 3. To bound the term, $\\mathbf{E}\\|\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}$ , the following lemma is applied. ", "page_idx": 7}, {"type": "text", "text": "Lemma 6. Let $J_{g}(\\cdot)$ be $L$ -smooth, $\\forall g\\in\\{r,c\\}$ and $\\begin{array}{r}{\\alpha=\\frac{1}{4L(1+\\frac{2}{\\delta})}}\\end{array}$ Then the folowing holds. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}\\|\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}\\,\\le\\frac{288L}{\\delta^{2}K}+\\frac{3}{K}\\sum_{k=1}^{K}\\|\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-\\omega_{k}\\|^{2}+\\beta\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note the presence of $\\beta$ in (20). To ensure convergence, $\\beta$ must be a function of $T$ . Invoking Lemma 3, we get the following relation under the same set of assumptions and the choice of parameters as in Lemma6. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\|\\nabla_{\\theta}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}\\leq\\tilde{\\mathcal{O}}\\left(\\frac{A G^{2}t_{\\mathrm{mix}}^{2}}{\\delta^{2}T^{\\xi}}\\right)+\\tilde{\\mathcal{O}}\\left(\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta^{2}T^{1-\\xi}}\\right)+\\beta\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Applying Lemma 3 and (21) in (20), we arrive at, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\bigg[\\|\\omega_{k}\\|^{2}\\bigg]\\leq\\tilde{\\mathcal{O}}\\left(\\frac{A G^{2}t_{\\mathrm{mix}}^{2}}{\\delta^{2}T^{\\xi}}+\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta^{2}T^{1-\\xi}}\\right)+\\beta\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Similarly, using (4), we deduce the following. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\|\\omega_{k}-\\omega_{k}^{*}\\|\\leq\\left(1+\\frac{1}{\\mu_{F}}\\right)\\sqrt{\\beta}+\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(\\frac{\\sqrt{A}G t_{\\mathrm{mix}}}{\\delta T^{\\xi/2}}+\\frac{\\sqrt{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}}{\\delta T^{(1-\\xi)/2}}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Inequalities (22) and (23) lead to the following global convergence of the Lagrange function. ", "page_idx": 7}, {"type": "text", "text": "Lemma 7. Let $\\{\\theta_{k}\\}_{k=1}^{K}$ beasdescribedinLemma 5.Ifassutions $I{-}5$ hold, $\\{J_{g}(\\cdot)\\}_{g\\in\\{r,c\\}}$ are L-smooth functions, \u03b1 = 4L(i+3) $\\begin{array}{r}{K=\\frac{T}{H}}\\end{array}$ ,and $H=16t_{\\mathrm{mix}}t_{\\mathrm{hit}}T^{\\xi}(\\log_{2}T)^{2}$ ,then ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\bigg(J_{\\mathrm{L}}(\\pi^{*},\\lambda_{k})-J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\bigg)\\leq G\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(\\sqrt{\\beta}+\\frac{\\sqrt{A}G t_{\\mathrm{mix}}}{\\delta T^{\\xi/2}}+\\frac{\\sqrt{L t_{\\mathrm{mix}}t_{\\mathrm{Mit}}}}{\\delta T^{(1-\\xi)/2}}\\right)}\\\\ &{+\\displaystyle\\frac{B}{L}\\tilde{\\mathcal{O}}\\left(\\frac{A G^{2}t_{\\mathrm{mix}}^{2}}{\\delta^{2}T^{\\xi}}+\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{Mit}}}{\\delta^{2}T^{1-\\xi}}+\\beta\\right)+\\tilde{\\mathcal{O}}\\bigg(\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}\\mathbf{E}_{s\\times d^{\\pi^{*}}}\\left[K L(\\pi^{*}(\\cdot|s)\\|\\pi_{\\theta_{1}}(\\cdot|s))\\right]}{T^{1-\\xi}\\delta}\\bigg)+\\sqrt{\\epsilon_{\\mathrm{bias}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Lemma 7 establishes that the average difference between $J_{\\mathrm{L}}(\\pi^{*},\\lambda_{k})$ and $J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})$ is $\\tilde{\\mathcal{O}}(\\sqrt{\\beta}+$ $T^{-\\xi/2}+T^{-(1-\\xi)/2})$ . Expanding the function, $J_{\\mathrm{L}}$ , and utiliing the update rule of the Lagrange multiplier, we achieve the global convergence for the objective and the constraint in Theorem 1 (stated below). In its proof, Lemma 18 (stated in the appendix) serves as an important tool in disentangling the convergence rates of regret and constraint violation. Interestingly, Lemma 18 is built upon the strong duality property of the unparameterized optimization (2) and has no apparent direct connection with the parameterized setup. ", "page_idx": 8}, {"type": "text", "text": "Theorem 1. Consider the same parameters as in Lemma 7 and set $\\beta=T^{-2/5}$ $\\xi=2/5$ Wehave, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\bigg(J_{r}^{\\pi^{*}}-J_{r}(\\theta_{k})\\bigg)\\leq\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\frac{\\sqrt{A}G^{2}t_{\\mathrm{mix}}}{\\delta}\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(T^{-1/5}\\right)}\\\\ &{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\bigg(-J_{c}(\\theta_{k})\\bigg)\\leq\\delta\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\tilde{\\mathcal{O}}\\left(\\frac{t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta T^{1/5}}\\right)+\\sqrt{A}G^{2}t_{\\mathrm{mix}}\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(T^{-1/5}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\pi^{*}$ is a solution to (2). In the above bounds, we write only the dominating terms of $T$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 1 establishes $\\tilde{\\mathcal{O}}(T^{-1/5})$ convergence rates for both the objective and the constraint violation. ", "page_idx": 8}, {"type": "text", "text": "5   Regret and Violation Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we use the convergence result of the previous section to bound the expected regret and constraint violation of Algorithm 1. Note that the regret and constraint violation decompose as, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{Reg}_{T}=\\sum_{t=0}^{T-1}\\left(J_{r}^{\\pi^{*}}-r(s_{t},a_{t})\\right)=H\\sum_{k=1}^{K}\\left(J_{r}^{\\pi^{*}}-J(\\theta_{k})\\right)+\\displaystyle\\sum_{k=1}^{K}\\sum_{t\\in\\mathbb{Z}_{k}}\\left(J(\\theta_{k})-r(s_{t},a_{t})\\right)}\\\\ {\\displaystyle\\mathrm{Vio}_{T}=\\sum_{t=0}^{T-1}\\left(-c(s_{t},a_{t})\\right)=H\\sum_{k=1}^{K}(-J_{c}(\\theta_{k}))+\\displaystyle\\sum_{k=1}^{K}\\sum_{t\\in\\mathbb{Z}_{k}}\\left(J_{c}(\\theta_{k})-c(s_{t},a_{t})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where ${\\mathcal{T}}_{k}\\triangleq\\{(k-1)H,\\cdot\\cdot\\cdot\\,,k H-1\\}$ . Observe that the expectation of the first terms in regret and violation can be bounded by Theorem 1. The expectation of the second term in regret and violation can be expanded as follows, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\left[\\displaystyle\\sum_{k=1}^{K}\\sum_{t\\in T_{k}}\\left(J_{g}(\\theta_{k})-g(s_{t},a_{t})\\right)\\right]\\stackrel{(a)}{=}\\mathbf{E}\\left[\\displaystyle\\sum_{k=1}^{K}\\sum_{t\\in T_{k}}\\mathbf{E}_{s^{\\prime}\\sim P\\left(\\cdot\\vert s_{t},a_{t}\\right)}[V_{g}^{\\pi_{\\theta_{k}}}(s^{\\prime})]-Q_{g}^{\\pi_{\\theta_{k}}}(s_{t},a_{t})\\right]}\\\\ &{\\stackrel{(b)}{=}\\mathbf{E}\\left[\\displaystyle\\sum_{k=1}^{K}\\sum_{t\\in T_{k}}V_{g}^{\\pi_{\\theta_{k}}}(s_{t+1})-V_{g}^{\\pi_{\\theta_{k}}}(s_{t})\\right]=\\mathbf{E}\\left[\\displaystyle\\sum_{k=1}^{K}V_{g}^{\\pi_{\\theta_{k}}}(s_{k H})-V_{g}^{\\pi_{\\theta_{k}}}(s_{(k-1)H})\\right]}\\\\ &{=\\mathbf{E}\\left[\\displaystyle\\sum_{k=1}^{K-1}V_{g}^{\\pi_{\\theta_{k+1}}}(s_{k H})-V_{g}^{\\pi_{\\theta_{k}}}(s_{k H})\\right]+\\mathbf{E}\\left[V_{g}^{\\pi_{\\theta_{K}}}(s_{T})-V_{g}^{\\pi_{\\theta_{0}}}(s_{0})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $g\\in\\{r,c\\}$ . Equality $(a)$ uses the Bellman equation and $(b)$ follows from the definition of $Q_{g}$ The first term in the last line of Eq. (24) can be upper bounded by Lemma 8 (stated below). On the other hand, the second term can be upper bounded as $\\mathcal{O}(t_{\\mathrm{mix}})$ using Lemma 9. ", "page_idx": 8}, {"type": "text", "text": "Lemma 8. If assumptions $^{\\,l}$ and 3 hold, then for $\\begin{array}{r}{K=\\frac{T}{H}}\\end{array}$ where $H=16t_{\\mathrm{mix}}t_{\\mathrm{hit}}T^{\\frac{2}{5}}(\\log_{2}T)^{2}$ the following inequalities hold $\\forall k$ $\\forall(s,a)$ and sufficiently large $T$ ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle(a)\\left|\\pi_{\\theta_{k+1}}(a|s)-\\pi_{\\theta_{k}}(a|s)\\right|\\leq G\\lVert\\theta_{k+1}-\\theta_{k}\\rVert}\\\\ &{\\displaystyle(b)\\sum_{k=1}^{K}\\mathbf{E}|J_{g}(\\theta_{k+1})-J_{g}(\\theta_{k})|\\leq\\tilde{\\mathcal{O}}\\left(\\frac{\\alpha A G}{\\delta t_{\\mathrm{hit}}}\\left[\\left(\\sqrt{A}G t_{\\mathrm{mix}}+\\delta\\right)T^{\\frac{2}{5}}+\\sqrt{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}T^{\\frac{3}{10}}\\right]\\right)}\\\\ &{\\displaystyle(c)\\sum_{k=1}^{K}\\mathbf{E}|V_{g}^{\\pi_{\\theta_{k+1}}}(s_{k})-V_{g}^{\\pi_{\\theta_{k}}}(s_{k})|\\leq\\tilde{\\mathcal{O}}\\left(\\frac{\\alpha A G t_{\\mathrm{mix}}}{\\delta t_{\\mathrm{hit}}}\\left[\\left(\\sqrt{A}G t_{\\mathrm{mix}}+\\delta\\right)T^{\\frac{2}{5}}+\\sqrt{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}T^{\\frac{3}{10}}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $g\\in\\{r,c\\}$ and $\\{s_{k}\\}_{k=1}^{K}$ is an arbitrary sequence of states. ", "page_idx": 9}, {"type": "text", "text": "Lemma 8 states that the obtained policy parameters are such that the average consecutive difference in thesequence $\\{J_{g}(\\theta_{k})\\}_{k=1}^{K}$ $g\\in\\bar{\\{r,c\\}}$ deeases withtime horizon, $T$ We would iketoemphasize that Lemma 8 works for both reward and constraint functions. Hence, we can prove our regret guarantee and constraint violation as shown below. ", "page_idx": 9}, {"type": "text", "text": "Theorem 2. If assumptions $I{-}5$ hold, $J_{g}(\\cdot)$ 's are $L$ smooth, $\\forall g\\in\\{r,c\\}$ and $T$ are suffciently large, then ourproposedAlgorithm $^{\\,l}$ achieves the following expected regret and constraint violation bounds with learning rates 4L(1+) and \u03b2 =T-2/5 ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\left[\\mathrm{Reg}_{T}\\right]\\leq T\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\tilde{\\mathcal{O}}(T^{4/5})+\\mathcal{O}(t_{\\mathrm{mix}})}\\\\ &{\\mathbf{E}\\left[\\mathrm{Vio}_{T}\\right]\\leq T\\delta\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\tilde{\\mathcal{O}}(T^{4/5})+\\mathcal{O}(t_{\\mathrm{mix}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The detailed expressions of these bounds are provided in the Appendix. Here, we keep only those terms that emphasize the order of $T$ . Note that our result outperforms the state-of-the-art model-free tabular result in average-reward CMDP [8]. However, our regret bound is worse than that achievable in average reward unconstrained MDP with general parameterization [13]. Interestingly, the gap between the convergence results of constrained and unconstrained setups is a common observation across the literature. For example, in the tabular model-free average reward MDP, the state-of-the-art regret bound for unconstrained setup, $\\tilde{\\mathcal{O}}(T^{1/2})$ [17], is better than that in the constrained setup, $\\tilde{\\mathcal{O}}(T^{5/6})$ [8]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper establishes the first sublinear regret and constraint violation bounds in the average reward CMDP setup with general parametrization (and do not assume the underlying constrained Markov Decision Process (CMDP) to be tabular or linear). We show that our proposed algorithm achieves $\\tilde{\\mathcal{O}}(T^{4/5})$ regret and constraint violation bounds where $T$ is the time horizon. Note that the state of the art in unconstrained counterpart is $\\tilde{\\mathcal{O}}(T^{3/4})$ Closing this gap by designing more efficient algorithms is an open question in the average reward CMDP literature with the general parametrization. Moreover, our current algorithm requires the knowledge of mixing time. Relaxing such assumptions is another important future direction in realistic settings. For further discussions on future work directions, the readers are referred to [35]. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported in part by the National Science Foundation under grant CCF-2149588 and Cisco, Inc. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Liu, C., N. Geng, et al. Cmix: Deep multi-agent reinforcement learning with peak and average constraints. In Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference,ECML PKDD 2021,Bilbao,Spain,September 13-17,2021,Proceedings, Part I 21, pages 157-173. Springer, 2021. ", "page_idx": 9}, {"type": "text", "text": "[2]  Al-Abbasi, A. O., A. Ghosh, V. Aggarwal. Deeppool: Distributed model-free algorithm for ridesharing using deep reinforcement learning. IEEE Transactions on Intelligent Transportation Systems, 20(12):4714-4727, 2019. [3] Ling, L., W. U. Mondal, S. V. Ukkusuri. Cooperating graph neural networks with deep reinforcement learning for vacine prioritization. arXiv preprint arXiv:2305.05163, 2023.   \n[4] Bai, Q., A. S. Bedi, V. Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via conservative natural policy gradient primal-dual algorithm. In Proceedings of the AAA1 Conference on Artificial Intelligence, pages 6737-6744. 2023. [5]  Agarwal, M., Q. Bai, V. Aggarwal. Concave utility reinforcement learning with zero-constraint violations. Transactions on Machine Learning Research, 2022.   \n[6] Chen, L., R. Jain, H. Luo. Learning infnite-horizon average-reward markov decision process with constraints. In International Conference on Machine Learning, pages 3246-3270.PMLR, 2022. [7]  Agarwal, M., Q. Bai, V. Aggarwal. Regret guarantees for model-based reinforcement learning with long-term average constraints. In Uncertainty in Artifcial Intelligence, pages 22-31. PMLR, 2022.   \n[8]  Wei, H., X. Liu, L. Ying. A provably-efcient model-free algorithm for infnite-horizon averagereward constrained markov decision processes. In Proceedings of the AAAl Conference on Artijfcial Intelligence, pages 3868-3876. 2022. [9] Ghosh, A., X. Zhou, N. Shroff. Achieving sub-linear regret in infinite horizon average reward constrained mdp with linear function approximation. In The Eleventh International Conference on Learning Representations. 2023.   \n[10] Agarwal, A., S. M. Kakade, J. D. Lee, G. Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. The Journal of Machine Learning Research, 22(1):4431-4506, 2021.   \n[11] Mondal, W. U., V. Aggarwal. Improved sample complexity analysis of natural policy gradient algorithm with general parameterization for infinite horizon discounted reward markov decision processes. In International Conference on Artificial Intelligence and Statistics (AISTATS). 2024.   \n[12] . Sample-effcient constrained reinforcement learning with general parameterization. arXiv preprint arXiv:2405.10624, 2024.   \n[13]  Bai, Q., W. U. Mondal, V. Aggarwal. Regret analysis of policy gradient algorithm for infinite horizon average reward markov decision processes. In Proceedings of the AAAl Conference on Artificial Intelligence. 2024.   \n[14] Ding, D., K. Zhang, T. Basar, M. Jovanovic. Natural policy gradient primal-dual method for constrained markov decision processes. Advances in Neural Information Processing Systems, 33:8378-8390, 2020.   \n[15]  Agrawal, S., R. Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. Advances in Neural Information Processing Systems, 30, 2017.   \n[16]  Auer, P, T. Jksch, R. Ortner. Near-optimal regret bounds for reinforcement learning. Advances in neural information processing systems, 21,2008.   \n[17] Wei, C.-Y., M. J. Jahromi, H. Luo, H. Sharma, R. Jain. Model-free reinforcement learning in infinite-horizon average-reward markov decision processes. In International conference on machine learning, pages 10170-10180. PMLR, 2020.   \n[18] Bai, Q., A. S. Bedi, M. Agarwal, A. Koppel, V. Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach. In Proceedings of the AAAl Conference on Artificial Intelligence, pages 3682-3689. 2022.   \n[19] Xu, T., Y. Liang, G. Lan. Crpo: A new approach for safe reinforcement learning with convergence guarantee. In International Conference on Machine Learning, pages 11480-11491. PMLR, 2021.   \n[20] Efroni, Y. S. Mannor, M. Pirotta. Exploration-exploitation in constrained mdps. arXiv preprint arXiv:2003.02189, 2020.   \n[21] Qiu, S., X. Wei, Z. Yang, J. Ye, Z. Wang. Upper confidence primal-dual reinforcement learning for cmdp with adversarial loss. Advances in Neural Information Processing Systems, 33:15277- 15287, 2020.   \n[22] Germano, J., F. E. Stradi, G. Genalti, M. Castiglioni, A. Marchesi, N. Gatti. A best-ofboth-worlds algorithm for constrained mdps with long-term constraints. arXiv preprint arXiv:2304.14326, 2023.   \n[23] Pesquerel, F., O.-A. Maillard. Imed-rl: Regret optimal learning of ergodic markov decision processes. In NeurIPS 2022-Thirty-sixth Conference on Neural Information Processing Systems. 2022.   \n[24] Gong, H, M. Wang. A duality approach for regret minimization in average-award ergodic markov decision processes. In Learning for Dynamics and Control, pages 862-883. PMLR, 2020.   \n[25]  Sutton, R. S., D. McAllester, S. Singh, Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999.   \n[26]  Lattimore, T., C. Szepesvari. Bandit algorithms. Cambridge University Press, 2020.   \n[27] Agarwal, A., S. M. Kakade, J. D. Lee, G. Mahajan. Optimality and approximation with policy gradient methods in markov decision processes. In Conference on Learning Theory, pages 64-66. PMLR, 2020.   \n[28] Zhang, J., C. Ni, C. Szepesvari, M. Wang. On the convergence and sample efficiency of variance-reduced policy gradient method. Advances in Neural Information Processing Systems, 34:2228-2240, 2021.   \n[29] Liu, Y, K. Zhang, T. Basar, W. Yin. An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods. Advances in Neural Information Processing Systems, 33:7624-7636, 2020.   \n[30] Jin, C., Z. Yang, Z. Wang, M. I. Jordan. Provably efficient reinforcement learning with linear function approximation. In J. Abernethy, S. Agarwal, eds., Proceedings of Thirty Third Conference on Learning Theory, vol. 125 of Proceedings of Machine Learning Research, pages 2137-2143. PMLR, 2020.   \n[31] Wang, L., Q. Cai, Z. Yang, Z. Wang. Neural policy gradient methods: Global optimality and rates of convergence. In International Conference on Learning Representations. 2019.   \n[32] Yuan, R., R. M. Gower, A. Lazaric. A general sample complexity analysis of vanilla policy gradient. In International Conference on Artificial Intelligence and Statistics, pages 3332-3380. PMLR, 2022.   \n[33] Fatkhullin, I., A. Barakat, A. Kireeva, N. He. Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies. In International Conference on Machine Learning, pages 9827-9869. PMLR, 2023.   \n[34]  Mondal, W. U., V. Aggarwal, S. V. Ukkusuri. Mean-field control based approximation of multi-agent reinforcement learning in presence of a non-decomposable shared global state. Transactions on Machine Learning Research, 2023.   \n[35]  Aggarwal, V., W. U. Mondal, Q. Bai. Constrained reinforcement learning with average reward objective: Model-based and model-free algorithms. Found. Trends Optim., 6(4):193-298, 2024.   \n[36]  Dorfman, R., K. Y. Levy. Adapting to mixing time in stochastic optimization with markovian data. In International Conference on Machine Learning, pages 5429-5446. PMLR, 2022.   \n[37] Ding, D., K. Zhang, J. Duan, T. Basar, M. R. Jovanovic. Convergence and sample complexity of natural policy gradient primal-dual methods for constrained mdps. arXiv preprint arXiv:2206.02346, 2022.   \n[38]  Bai, Q., V. Aggarwal, A. Gattami. Provably sample-effcient model-free algorithm for mdps with peak constraints. Journal of Machine Learning Research, 24(60):1-25, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs for Lemmas in Section 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Since the first step of the proof works in the same way for functions $J_{r}$ and $J_{c}$ , we use the generic notations $J_{g},V_{g},Q_{g}$ where $g=r,c$ and derive the following. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle7_{\\theta}V_{g}^{\\pi_{\\theta_{k}}}(s)=\\nabla_{\\theta}\\bigg(\\sum_{a}\\pi_{\\theta}(a|s)Q_{g}^{\\pi_{\\theta}}(s,a)\\bigg)}\\\\ &{=\\displaystyle\\sum_{a}\\bigg(\\nabla_{\\theta}\\pi_{\\theta}(a|s)\\bigg)Q_{g}^{\\pi_{\\theta}}(s,a)+\\sum_{a}\\pi_{\\theta}(a|s)\\nabla_{\\theta}Q_{g}^{\\pi_{\\theta}}(s,a)}\\\\ &{\\displaystyle\\overset{(a)}{=}\\sum_{a}\\pi_{\\theta}(a|s)\\bigg(\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\bigg)Q_{g}^{\\pi_{\\theta}}(s,a)+\\sum_{a}\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\bigg(g(s,a)-J_{g}(\\theta)+\\sum_{s^{\\prime}}P(s^{\\prime}|s,a)V_{g}^{\\pi_{\\theta}}(s,a)\\bigg)}\\\\ &{=\\displaystyle\\sum_{a}\\pi_{\\theta}(a|s)\\bigg(\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\bigg)Q_{g}^{\\pi_{\\theta}}(s,a)+\\sum_{a}\\pi_{\\theta}(a|s)\\bigg(\\sum_{s^{\\prime}}P(s^{\\prime}|s,a)\\nabla_{\\theta}V_{g}^{\\pi_{\\theta}}(s^{\\prime})\\bigg)-\\nabla_{\\theta}J_{g}(\\theta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the step (a) is a consequence of $\\begin{array}{r}{\\nabla_{\\theta}\\log\\pi_{\\theta}=\\frac{\\nabla\\pi_{\\theta}}{\\pi_{\\theta}}}\\end{array}$ and the Bellman equation. Multiplying both sides by $d^{\\pi_{\\theta}}(s)$ , taking a sum over $s\\in S$ , and rearranging the terms, we obtain the following. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\nabla_{\\theta}J_{\\theta}(\\theta)=\\sum_{s}d^{n_{\\theta}}(s)\\nabla_{\\theta}J_{\\theta}(\\theta)}\\\\ &{=\\displaystyle\\sum_{s}d^{n_{\\theta}}(s)\\sum_{a}\\pi_{\\theta}(a|s)\\Bigg(\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\Bigg)Q_{\\theta}^{n_{\\theta}}(s,a)+\\sum_{s}d^{n_{\\theta}}(s)\\sum_{a}\\pi_{\\theta}(a|s)\\Bigg(\\sum_{s^{\\prime}}P(s^{\\prime}|s,a)\\nabla_{\\theta}V_{\\theta}^{n_{\\theta}}(s)}\\\\ &{-\\displaystyle\\sum_{s}d^{n_{\\theta}}(s)\\nabla_{\\theta}V_{\\theta}^{n_{\\theta}}(s)}\\\\ &{=\\displaystyle\\mathbf{E}_{s\\sim d^{n_{\\theta}},a\\sim s\\pi_{\\theta}(c,1)s}\\Bigg[Q_{g}^{n_{\\theta}}(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\Bigg]+\\sum_{s}d^{n_{\\theta}}(s)\\sum_{s^{\\prime}}P^{n_{\\theta}}(s^{\\prime}|s)\\nabla_{\\theta}V_{g}^{n_{\\theta}}(s^{\\prime})-\\sum_{s}d^{n_{\\theta}}(s)\\nabla_{\\theta}V_{g}^{n_{\\theta}}(s^{\\prime})}\\\\ &{\\stackrel{(a)}{=}\\mathbf{E}_{s\\sim d^{n_{\\theta}},a\\sim s\\pi_{\\theta}(c,1)s}\\Bigg[Q_{g}^{n_{\\theta}}(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\Bigg]+\\sum_{s^{\\prime}}d^{n_{\\theta}}(s^{\\prime})\\nabla_{\\theta}V_{g}^{n_{\\theta}}(s^{\\prime})-\\sum_{s}d^{n_{\\theta}}(s)\\nabla_{\\theta}V_{g}^{n_{\\theta}}(s)}\\\\ &{=\\mathbf{E}_{s\\sim d^{n_{\\theta}},a\\sim s\\pi_{\\theta}(c,1)s}\\Bigg[Q_{g}^{n_{\\theta}}(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $(a)$ uses the fact that $d^{\\pi_{\\theta}}$ is a stationary distribution. Note that, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}_{s\\sim d^{\\pi_{\\theta}},a\\sim\\pi_{\\theta}(\\cdot\\vert s)}\\bigg[V_{g}^{\\pi_{\\theta}}(s)\\nabla\\log\\pi_{\\theta}(a\\vert s)\\bigg]}\\\\ &{\\ =\\mathbf{E}_{s\\sim d^{\\pi_{\\theta}}}\\left[\\displaystyle\\sum_{a\\in A}V_{g}^{\\pi_{\\theta}}(s)\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)\\right]}\\\\ &{\\ =\\mathbf{E}_{s\\sim d^{\\pi_{\\theta}}}\\bigg[V_{g}^{\\pi_{\\theta}}(s)\\nabla_{\\theta}\\left(\\displaystyle\\sum_{a\\in A}\\pi_{\\theta}(a\\vert s)\\right)\\bigg]=\\mathbf{E}_{s\\sim d^{\\pi_{\\theta}}}\\bigg[V_{g}^{\\pi_{\\theta}}(s)\\nabla_{\\theta}(1)\\bigg]=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We can, therefore, replace the function $Q_{g}^{\\pi_{\\theta}}$ in the policy gradient with the advantage function $A_{g}^{\\pi_{\\theta}}(s,a)=Q_{g}^{\\pi_{\\theta}}(s,a)-V_{g}^{\\pi_{\\theta}}(s),\\forall(s,a)\\in\\bar{\\cal S}\\times{\\cal A}$ Thus, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}J_{g}(\\theta)=\\mathbf{E}_{s\\sim d^{\\pi_{\\theta}},a\\sim\\pi_{\\theta}(\\cdot\\vert s)}\\bigg[A_{g}^{\\pi_{\\theta}}(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(a\\vert s)\\bigg]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The proof is completed using the definitions of $J_{\\mathrm{L},\\lambda}$ and $A_{\\mathrm{L},\\lambda}$ ", "page_idx": 12}, {"type": "text", "text": "A.2Proof of Lemma 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof. The proof is similar to the proof of [17, Lemma 6]. Consider the $k$ th epoch and assume that $\\pi_{\\theta_{k}}$ is denoted as $\\pi$ for notational convenience. Let, $M$ be the number of disjoint sub-trajectories of ", "page_idx": 12}, {"type": "text", "text": "length $N$ that start with the state $s$ and are at least $N$ distance apart (found by Algorithm 2). Let, $g_{k,i}$ be the sum of rewards or constraint ( $g=r,c$ accordingly) observed in the ith sub-trajectory and $\\tau_{i}$ denote its starting time. The advantage function estimate is, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{A}_{g}^{\\pi}(s,a)=\\left\\{\\begin{array}{l l}{\\displaystyle{\\frac{1}{\\pi(a|s)}}\\left[\\frac{1}{M}\\sum_{i=1}^{M}g_{k,i}1(a_{\\tau_{i}}=a)\\right]-\\frac{1}{M}\\sum_{i=1}^{M}g_{k,i}}&{\\mathrm{~if~}M>0}\\\\ {0}&{\\mathrm{~if~}M=0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note the following, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left.\\operatorname*{lim}_{i}\\left\\lvert s_{t_{i}}=s,a_{\\tau_{i}}=a\\right\\rvert\\right]=g(s,a)+\\mathbb{E}\\left[\\sum_{i=1}^{n+N}g(s,a_{t_{i}})\\right\\rvert\\mathbf{s}_{t_{i}}=s,a_{\\tau_{i}}=a\\right]}\\\\ &{=g(s,a)+\\sum_{i}P(s^{\\prime}|s,a)\\mathbb{E}\\left[\\sum_{j=\\sigma_{i}+1}^{n+N}g(s,a_{j})\\right\\rvert s_{t_{i}+1}=s^{\\prime}\\right]}\\\\ &{=g(s,a)+\\sum_{i}P(s^{\\prime}|s,a)\\left[\\sum_{j=0}^{N-1}(P^{\\sigma})^{j}(s^{\\prime},{\\cdot})\\right]^{T}g^{\\prime}}\\\\ &{=g(s,a)+\\sum_{i}P(s^{\\prime}|s,a)\\left[\\sum_{j=0}^{N-1}(P^{\\sigma})^{j}(s^{\\prime},{\\cdot})\\right]^{T}g^{\\prime\\prime}}\\\\ &{=g(s,a)+\\sum_{i}P(s^{\\prime}|s,a)\\left[\\sum_{j=0}^{N-1}(P^{\\sigma})^{j}(s^{\\prime},{\\cdot})-a^{\\prime\\prime}\\right]^{T}g^{\\prime\\prime}+N(d^{\\prime\\prime})^{T}g^{\\prime\\prime}}\\\\ &{\\overset{(a)}{=}g(s,a)+\\sum_{i}P(s^{\\prime}|s,a)\\left[\\sum_{j=0}^{N}(P^{\\sigma})^{j}(s^{\\prime},{\\cdot})-a^{\\prime\\prime}\\right]^{T}g^{\\prime\\prime}+N{\\cal J}_{g}^{\\prime\\prime}-\\sum_{i}P(s^{\\prime}|s,a)\\left[\\sum_{j=N}^{N}(P^{\\sigma})^{j}(s^{\\prime},{\\cdot})\\right]^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\overset{(b)}{=}g(s,a)+\\sum_{s^{\\prime}}P(s^{\\prime}|s,a)V_{g}^{\\pi}(s^{\\prime})+N J_{g}^{\\pi}-\\mathbb{E}_{T}^{\\pi}(s,a)\\overset{(c)}{=}Q_{g}^{\\pi}(s,a)+(N+1)J_{g}^{\\pi}-\\mathbb{E}_{T}^{\\pi}(s,a)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(a)$ follows from the definition of $J_{g}^{\\pi}$ as given in (5), $(b)$ is an application of the definition of $V_{g}^{\\pi}$ given in (7), and $(c)$ follows from the Bellman equation. Define the following quantity. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\delta^{\\pi}(s,T)\\triangleq\\sum_{t=N}^{\\infty}\\|(P^{\\pi})^{t}(s,\\cdot)-d^{\\pi}\\|_{1}\\ \\mathrm{~where~}N=4t_{\\mathrm{mix}}(\\log_{2}T)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using Lemma 10, we get $\\begin{array}{r}{\\delta^{\\pi}(s,T)\\le\\frac{1}{T^{3}}}\\end{array}$ which implies, $\\begin{array}{r}{|\\mathrm{E}_{T}^{\\pi}(s,a)|\\leq\\frac{1}{T^{3}}}\\end{array}$ . Observe that, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\left[\\left(\\frac{1}{\\pi(a|s)}g_{k,i}1(a_{\\tau_{i}}=a)-g_{k,i}\\right)\\bigg|s_{\\tau_{i}}=s\\right]}\\\\ &{=\\mathbf{E}\\left[g_{k,i}\\Big|s_{\\tau_{i}}=s,a_{\\tau_{i}}=a\\right]-\\displaystyle\\sum_{a^{\\prime}}\\pi(a^{\\prime}|s)\\mathbf{E}\\left[g_{k,i}\\Big|s_{\\tau_{i}}=s,a_{\\tau_{i}}=a^{\\prime}\\right]}\\\\ &{=Q_{g}^{\\pi}(s,a)+(N+1)J_{g}^{\\pi}-\\mathrm{E}_{T}^{\\pi}(s,a)-\\displaystyle\\sum_{a^{\\prime}}\\pi(a^{\\prime}|s)[Q^{\\pi}(s,a)+(N+1)J_{g}^{\\pi}-\\mathrm{E}_{T}^{\\pi}(s,a)]}\\\\ &{=Q_{g}^{\\pi}(s,a)-V_{g}^{\\pi}(s)-\\left[\\mathrm{E}_{T}(s,a)-\\displaystyle\\sum_{a^{\\prime}}\\pi(a^{\\prime}|s)\\mathrm{E}_{T}^{\\pi}(s,a^{\\prime})\\right]}\\\\ &{=A^{\\pi}(s,a)-\\Delta\\pi_{g}^{\\pi}(s,a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\begin{array}{r}{\\Delta_{T}^{\\pi}(s,a)\\,\\triangleq\\,\\operatorname{E}_{T}(s,a)\\,-\\,\\sum_{a^{\\prime}}\\pi(a^{\\prime}|s)\\operatorname{E}_{T}^{\\pi}(s,a^{\\prime})}\\end{array}$ . Using the bound on $\\mathrm{E}_{T}^{\\pi}(s,a)$ , we derive, $\\begin{array}{r}{|\\Delta_{T}^{\\pi}(s,a)|\\leq\\frac{2}{T^{3}}}\\end{array}$ , which implies, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left|\\mathbf{E}\\left[\\left(\\frac{1}{\\pi(a|s)}g_{k,i}1(a_{\\tau_{i}}=a)-g_{k,i}\\right)\\bigg|s_{\\tau_{i}}=s\\right]-A_{g}^{\\pi}(s,a)\\right|\\leq|\\Delta_{T}^{\\pi}(s,a)|\\leq\\frac{2}{T^{3}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that (35) cannot be directly used to bound the bias of $\\hat{A}_{g}^{\\pi}(s,a)$ . This is because the random variable $M$ is correlated with the variables $\\{g_{k,i}\\}_{i=1}^{M}$ TdecratthmmaMP the state distribution resets to the stationary distribution, $d^{\\pi}$ after exactly $N$ time steps since the completion of a sub-trajectory. In other words, if a sub-trajectory starts at $\\tau_{i}$ , and ends at $\\tau_{i}+N$ then the system \u2018rests\u2019 for additional $N$ steps before rejuvenating with the state distribution, $d^{\\pi}$ at $\\tau_{i}+2N$ . Clearly, the wait time between the reset after the $(i-1)\\mathfrak{t h}$ sub-trajectory and the start of the ith sub-trajectory is, $w_{i}=\\tau_{i}-(\\tau_{i-1}+2N),i\\mathrm{~}$ $i>1$ . Let $w_{1}$ be the difference between the start time of the $k$ th epoch and the start time of the first sub-trajectory. Note that, ", "page_idx": 14}, {"type": "text", "text": "$(a)\\ w_{1}$ only depends on the initial state, $s_{(k-1)H}$ ana tne inaucea ransiuon Iuncuon, F\", ", "page_idx": 14}, {"type": "text", "text": "$(b)\\ w_{i}$ ,where $i>1$ , depends on the stationary distribution, $d^{\\pi}$ , and the induced transition function, $P^{\\pi}$ \uff0c   \n$(c)\\,M$ only depends on $\\{w_{1},w_{2},\\cdot\\cdot\\cdot\\}$ as other segments of the epoch have fixed length, $2N$ Clearly, in this imaginary CMDP, the sequence, $\\{w_{1},w_{2},\\cdot\\cdot\\cdot\\}$ , and hence, $M$ is independent of $\\{g_{k,1},g_{k,2},\\cdot\\cdot\\cdot\\}$ .Let, $\\mathbf{E}^{\\prime}$ denote the expectation operation and $\\mathrm{Pr^{\\prime}}$ denote the probability of events in this imaginary system. Define the following. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta_{i}\\triangleq\\frac{g_{k,i}1(a_{\\tau_{i}}=a)}{\\pi(a|s)}-g_{k,i}-A_{g}^{\\pi}(s,a)+\\Delta_{T}^{\\pi}(s,a)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\Delta_{T}^{\\pi}(s,a)$ is given in (34). Note that we have suppressed the dependence on $T,s,a$ ,and $\\pi$ while defining $\\Delta_{i}$ to remove clutter. Using (34), one can write ${\\bf E}^{\\prime}\\left[\\Delta_{i}(\\bar{s_{\\it1}}\\,a)|\\{w_{i}\\}\\right]=0$ Moreover, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{r}\\left[\\left(\\bar{A}_{y}^{n}(s,a)-A_{y}^{n}(s,a)\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}^{r}\\left[\\left(\\bar{A}_{y}^{\\top}(s,a)-A_{y}^{n}(s,a)\\right)^{2}\\bigg|{\\boldsymbol M}>\\mathsf{p}\\right]\\times\\mathbb{P}^{r}({\\boldsymbol M}>0)+\\left(A_{y}^{\\top}(s,a)\\right)^{2}\\times\\mathbb{P}^{r}({\\boldsymbol M}=0)}\\\\ &{=\\mathbb{E}^{r}\\left[\\left(\\frac{1}{M}\\frac{M}{\\sum_{i=1}^{M}\\Delta_{i}-\\Delta_{i}^{T}(s,a)}\\right)^{2}\\bigg|{\\boldsymbol M}>0\\right]\\times\\mathbb{P}^{r}({\\boldsymbol M}>0)+\\left(A_{y}^{\\top}(s,a)\\right)^{2}\\times\\mathbb{P}^{r}({\\boldsymbol M}=0)}\\\\ &{\\le2\\mathbb{E}_{\\{\\boldsymbol u,\\eta\\}}^{r}\\left[\\mathbb{E}^{r}\\left[\\left(\\frac{1}{M}\\frac{M}{\\sum_{i=1}^{M}\\Delta_{i}}\\right)^{2}\\bigg|\\{\\boldsymbol w_{1}\\}\\bigg\\}\\right]\\bigg|{\\boldsymbol w_{1}\\le{\\boldsymbol H}-{\\boldsymbol N}}\\right]\\times\\mathbb{P}^{r}({\\boldsymbol w_{1}\\le{\\boldsymbol H}-{\\boldsymbol N}})}\\\\ &{\\ +2\\left(\\Delta_{\\mathbf{T}}^{r}(s,a)\\right)^{2}+\\left(A_{y}^{\\top}(s,a)\\right)^{2}\\times\\mathbb{P}^{r}({\\boldsymbol M}=0)}\\\\ &{\\overset{(a)}{\\le}2\\mathbb{E}_{\\{\\boldsymbol u,\\eta\\}}^{r}\\left[\\frac{1}{M^{2}}\\frac{M}{\\sum_{i=1}^{M}\\mathbb{E}\\left[\\Delta_{i}\\right]}\\right]\\bigg|{\\boldsymbol w_{1}\\le{\\boldsymbol H}-{\\boldsymbol N}}\\right]\\times\\mathbb{P}^{r}({\\boldsymbol w_{1}\\le{\\boldsymbol H}-{\\boldsymbol N}})}\\\\ &{\\ +\\frac{8}{\\mathbb{P}^{r}}+\\left(A_{y}^{\\top}(s,a)\\right)^{2}\\times\\mathbb{P}^{r}({\\boldsymbol M}=0)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $(a)$ uses the bound $\\begin{array}{r}{|\\Delta_{T}^{\\pi}(s,a)|\\leq\\frac{2}{T^{3}}}\\end{array}$ derived in (35), and the fact that $\\{\\Delta_{i}\\}$ are zero mean independent random variables conditioned on $\\{w_{i}\\}$ . Note that $|g_{k,i}|\\leq N$ almost surely, $|A_{g}^{\\pi}(s,a)|\\leq$ $\\mathcal{O}(t_{\\mathrm{mix}})$ via Lemma 9, and $\\begin{array}{r}{|\\Delta_{T}^{\\pi}(s,a)|\\leq\\frac{2}{T^{3}}}\\end{array}$ as shown in (35). Combining, we get, ${\\bf E}^{\\prime}[|\\Delta_{i}|^{2}|\\{w_{i}\\}]\\le$ ${\\mathcal{O}}(N^{2}/\\pi(a|s))$ (see the definition of $\\Delta_{i}$ in (36)). Invoking this bound into (37), we get the following result. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}^{\\prime}\\left[\\left(\\hat{A}_{g}^{\\pi}(s,a)-A_{g}^{\\pi}(s,a)\\right)^{2}\\right]\\le2\\mathbf{E}^{\\prime}\\left[\\cfrac{1}{M}\\Big|w_{1}\\le H-N\\right]\\mathcal{O}\\left(\\frac{N^{2}}{\\pi(a|s)}\\right)+\\frac{8}{T^{6}}}\\\\ &{\\quad+\\mathcal{O}(t_{\\mathrm{mix}}^{2})\\times\\operatorname*{Pr}^{\\prime}(w_{1}>H-N)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that, one can use Lemma 11 to bound the following violation probability. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Gamma\\mathrm{r}^{\\prime}(w_{1}>H-N)\\le\\left(1-\\frac{3d^{\\pi}(s)}{4}\\right)^{4t_{\\mathrm{hif}}T^{\\xi}(\\log T)-1}\\stackrel{(a)}{\\le}\\left(1-\\frac{3d^{\\pi}(s)}{4}\\right)^{\\frac{4}{d^{\\pi}(s)}(\\log T)}\\le\\frac{1}{T^{3}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Where $(a)$ isaconsequnceof the factthat $\\begin{array}{r}{4t_{\\mathrm{hit}}T^{\\xi}(\\log_{2}T)-1\\geq\\frac{4}{d^{\\pi}(s)}\\log_{2}T}\\end{array}$ for suficientlylarge $T$ . Finally, note that, if $M<M_{0}$ , where $M_{0}$ is defined as, ", "page_idx": 15}, {"type": "equation", "text": "$$\nM_{0}\\triangleq{\\frac{H-N}{2N+{\\frac{4N\\log T}{d^{\\pi}(s)}}}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then there exists at least one $w_{i}$ that exceeds $4N\\log_{2}T/d^{\\pi}(s)$ which can happen with the following maximum probability according to Lemma 11. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Pr}^{\\prime}\\left(M<M_{0}\\right)\\leq\\left(1-\\frac{3d^{\\pi}(s)}{4}\\right)^{\\frac{4\\log T}{d^{\\pi(s)}}}\\leq\\frac{1}{T^{3}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The above probability bound can be used to obtain the following result, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}^{\\prime}\\left[\\frac{1}{M}\\middle|M>0\\right]=\\frac{\\sum_{m=1}^{\\infty}\\frac{1}{m}\\mathbf{Pr}^{\\prime}(M=m)}{\\mathbf{Pr}^{\\prime}(M>0)}\\leq\\frac{1\\times\\mathbf{Pr}^{\\prime}(M\\leq M_{0})+\\frac{1}{M_{0}}\\mathbf{Pr}^{\\prime}(M>M_{0})}{\\mathbf{Pr}^{\\prime}(M>0)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\frac{4N\\log T}{d^{\\pi}(s)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\frac{1}{T^{3}}+\\frac{2N+\\frac{4N\\log T}{M-N}}{M-N}}{1-\\frac{1}{T^{3}}}\\leq\\mathcal{O}\\left(\\frac{N\\log T}{H d^{\\pi}(s)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Injecting (39) and (42) into (38), we finally obtain the following. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}^{\\prime}\\left[\\left(\\hat{A}_{g}^{\\pi}(s,a)-A_{g}^{\\pi}(s,a)\\right)^{2}\\right]\\le\\mathcal{O}\\left(\\frac{N^{3}\\log T}{H d^{\\pi}(s)\\pi(a|s)}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathcal{O}\\left(\\frac{N^{3}t_{\\mathrm{hit}}\\log T}{H\\pi(a|s)}\\right)=\\mathcal{O}\\left(\\frac{t_{\\mathrm{mix}}^{2}(\\log T)^{2}}{T^{5}\\pi(a|s)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Eq. (43) demonstrates that our desired inequality is obeyed in the imaginary system. We now need a mechanism to translate this result to our actual CMDP. Note that $(\\hat{A}_{g}^{\\pi}(s,a)-A_{g}^{\\pi}(s,a))^{2}=f(X)$ where $X=(M,\\tau_{1},\\mathcal{T}_{1},\\cdot\\cdot\\cdot\\,,\\tau_{M},\\mathcal{T}_{M})$ and $\\mathcal{T}_{i}=(a_{\\tau_{i}},s_{\\tau_{i}+1},a_{\\tau_{i}+1},\\cdot\\cdot\\cdot\\cdot,s_{\\tau_{i}+N},\\bar{a_{\\tau_{i}+N}})$ We have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{\\mathbf{E}[f(X)]}{\\mathbf{E}^{\\prime}[f(X)]}}={\\frac{\\sum_{X}f(X)\\mathrm{Pr}(X)}{\\sum_{X}f(X)\\mathrm{Pr}^{\\prime}(X)}}\\leq\\operatorname*{max}_{X}{\\frac{\\mathrm{Pr}(X)}{\\mathrm{Pr}^{\\prime}(X)}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The last inequality uses the non-negativity of $f(\\cdot)$ . Observe that, for a fixed sequence, $X$ , we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(X)=\\operatorname*{Pr}(\\tau_{1})\\times\\operatorname*{Pr}(\\mathcal{T}_{1}|\\tau_{1})\\times\\operatorname*{Pr}(\\tau_{2}|\\tau_{1},\\mathcal{T}_{1})\\times\\operatorname*{Pr}(\\mathcal{T}_{2}|\\tau_{2})\\times\\cdots}\\\\ &{\\quad\\quad\\quad\\times\\operatorname*{Pr}(\\tau_{M}|\\tau_{M-1},\\mathcal{T}_{M-1})\\times\\operatorname*{Pr}(\\mathcal{T}_{M}|\\tau_{M})\\times\\operatorname*{Pr}(s_{t}\\ne s,\\forall t\\in[\\tau_{M}+2N,k H-N]|\\tau_{M},\\mathcal{T}_{M}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{\\sfPr}^{\\prime}(X)=\\operatorname*{Pr}(\\tau_{1})\\times\\operatorname*{Pr}({\\mathcal T}_{1}|\\tau_{1})\\times\\operatorname*{Pr}^{\\prime}(\\tau_{2}|\\tau_{1},{\\mathcal T}_{1})\\times\\operatorname*{Pr}({\\mathcal T}_{2}|\\tau_{2})\\times\\cdots}\\\\ &{\\quad\\quad\\quad\\times\\operatorname*{Pr}^{\\prime}(\\tau_{M}|\\tau_{M-1},{\\mathcal T}_{M-1})\\times\\operatorname*{Pr}({\\mathcal T}_{M}|\\tau_{M})\\times\\operatorname*{Pr}(s_{t}\\ne s,\\forall t\\in[\\tau_{M}+2N,k H-N]|\\tau_{M},{\\mathcal T}_{M}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The difference between $\\operatorname{Pr}(X)$ and $\\operatorname{Pr}^{\\prime}(X)$ arises because $\\operatorname*{Pr}(\\tau_{i+1}|\\tau_{i},\\mathcal{T}_{i})\\neq\\operatorname*{Pr}^{\\prime}(\\tau_{i+1}|\\tau_{i},\\mathcal{T}_{i}),\\forall$ $\\forall i\\in$ $\\{1,\\cdot\\cdot\\cdot,M-1\\}$ . Note that the ratio of these two terms can be bounded as follows, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{Pr}(\\tau_{i+1}|\\tau_{i},\\mathcal{T}_{i})}{\\mathrm{Pr}^{\\prime}(\\tau_{i+1}|\\tau_{i},\\mathcal{T}_{i})}}\\\\ &{=\\frac{\\sum_{s^{\\prime}\\neq s}\\mathrm{Pr}\\left(s_{\\tau_{i}+2N}=s^{\\prime}|\\tau_{i},\\mathcal{T}_{i}\\right)\\times\\mathrm{Pr}\\left(s_{t}\\neq s,\\forall t\\in[\\tau_{i}+2N,\\tau_{i+1}-1],s_{\\tau_{i+1}}=s|s_{\\tau_{i}+2N}=s^{\\prime}\\right)}{\\sum_{s^{\\prime}\\neq s}\\mathrm{Pr}^{\\prime}\\left(s_{\\tau_{i}+2N}=s^{\\prime}|\\tau_{i},\\mathcal{T}_{i}\\right)\\times\\mathrm{Pr}\\left(s_{t}\\neq s,\\forall t\\in[\\tau_{i}+2N,\\tau_{i+1}-1],s_{\\tau_{i+1}}=s|s_{\\tau_{i}+2N}=s^{\\prime}\\right)}}\\\\ &{\\leq\\underset{s^{\\prime}}{\\operatorname*{max}}\\frac{\\mathrm{Pr}\\left(s_{\\tau_{i}+2N}=s^{\\prime}|\\tau_{i},\\mathcal{T}_{i}\\right)}{\\mathrm{Pr}^{\\prime}\\left(s_{\\tau_{i}+2N}=s^{\\prime}|\\tau_{i},\\mathcal{T}_{i}\\right)}}\\\\ &{=\\underset{s^{\\prime}}{\\operatorname*{max}}1+\\frac{\\mathrm{Pr}\\left(s_{\\tau_{i}+2N}=s^{\\prime}|\\tau_{i},\\mathcal{T}_{i}\\right)}{d^{\\pi}(s^{\\prime})}\\leq\\underset{s^{\\prime}}{\\operatorname*{max}}1+\\frac{1}{T^{3}d^{\\pi}(s^{\\prime})}\\leq1+\\frac{t_{\\mathrm{hit}}}{T^{3}}\\leq1+\\frac{1}{T^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $(a)$ is a consequence of Lemma 10. We have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{\\operatorname*{Pr}(X)}{\\operatorname*{Pr}^{\\prime}(X)}}\\leq\\left(1+{\\frac{1}{T^{2}}}\\right)^{M}\\leq e^{\\frac{M}{T^{2}}}\\leq e^{\\frac{1}{T}}\\leq{\\mathcal{O}}\\left(1+{\\frac{1}{T}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(a)$ uses the fact that $M\\leq T$ . Combining (44) and (48), we get, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\left[\\left(\\hat{A}_{g}^{\\pi}(s,a)-A_{g}^{\\pi}(s,a)\\right)^{2}\\right]\\le\\mathcal{O}\\left(1+\\displaystyle\\frac{1}{T}\\right)\\mathbf{E}^{\\prime}\\left[\\left(\\hat{A}_{g}^{\\pi}(s,a)-A_{g}^{\\pi}(s,a)\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{(a)}{\\le}\\mathcal{O}\\left(\\displaystyle\\frac{t_{\\mathrm{mix}}^{2}(\\log T)^{2}}{T^{\\xi}\\pi(a|s)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(a)$ follows from (43). Using the definition of $A_{\\mathrm{L},\\lambda}$ ,we get, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\left[\\left(\\hat{A}_{\\mathrm{L},\\lambda}^{\\pi}(s,a)-A_{\\mathrm{L},\\lambda}^{\\pi}(s,a)\\right)^{2}\\right]}\\\\ &{=\\mathbf{E}\\left[\\left((\\hat{A}_{r}^{\\pi}(s,a)-A_{r}^{\\pi}(s,a))+\\lambda(\\hat{A}_{c}^{\\pi}(s,a)-A_{c}^{\\pi}(s,a))\\right)^{2}\\right]}\\\\ &{\\le2\\mathbf{E}\\left[\\left(\\hat{A}_{r}^{\\pi}(s,a)-A_{r}^{\\pi}(s,a)\\right)^{2}\\right]+2\\lambda^{2}\\mathbf{E}\\left[\\left(\\hat{A}_{c}^{\\pi}(s,a)-A_{c}^{\\pi}(s,a)\\right)^{2}\\right]\\le\\mathcal{O}\\left(\\frac{t_{\\operatorname*{mix}}^{2}(\\log T)^{2}}{\\delta^{2}T^{5}\\pi(a|s)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 16}, {"type": "text", "text": "B  Proofs for the Section of Global Convergence Analysis B.1 Proof of Lemma 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Recall from Eq. (15) that, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\omega_{k}=\\frac{1}{H}\\sum_{t=t_{k}}^{t_{k+1}-1}\\hat{A}_{\\mathrm{L},\\lambda}^{\\pi_{\\theta_{k}}}(s_{t},a_{t})\\nabla_{\\theta}\\log\\pi_{\\theta_{k}}\\big(a_{t}|s_{t}\\big),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Define the following quantity, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{\\omega}_{k}=\\frac{1}{H}\\sum_{t=t_{k}}^{t_{k+1}-1}A_{\\mathrm{L},\\lambda}^{\\pi_{\\theta_{k}}}(s_{t},a_{t})\\nabla_{\\theta}\\log\\pi_{\\theta_{k}}(a_{t}|s_{t})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $t_{k}=(k-1)H$ is the starting time of the $k$ th epoch. Note that the true gradient is given by, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}J_{\\mathrm{L},\\lambda}(\\theta_{k})=\\mathbf{E}_{s\\sim d^{\\pi_{\\theta_{k}}},\\,a\\sim\\pi_{\\theta_{k}}(\\cdot|s)}\\left[A_{\\mathrm{L},\\lambda}^{\\pi_{\\theta_{k}}}(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using Assumption 3, Lemma 9, and $\\textstyle\\lambda\\in[0,{\\frac{2}{\\delta}}]$ , one can exhibit that $|A_{\\mathrm{L},\\lambda}^{\\pi_{\\theta_{k}}}(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)|\\leq$ $\\begin{array}{r}{\\mathcal{O}(\\frac{t_{\\mathrm{mix}}G}{\\delta}),\\,\\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}\\end{array}$ which implies $\\begin{array}{r l}{\\vert\\nabla_{\\theta}J_{\\mathrm{L},\\lambda}(\\theta_{k})\\vert\\,\\le\\,\\mathcal{O}(\\frac{t_{\\mathrm{mix}}G}{\\delta})}\\end{array}$ . Applying Lemma 14, one, therefore, arrives at ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left\\Vert\\bar{\\omega}_{k}-\\nabla_{\\theta}J_{L,\\lambda}(\\theta_{k})\\right\\Vert^{2}\\right]\\leq\\mathcal{O}\\left(\\frac{1}{\\delta^{2}}G^{2}t_{\\mathrm{mix}}^{2}\\log T\\right)\\times\\mathcal{O}\\left(\\frac{t_{\\mathrm{mix}}\\log T}{H}\\right)=\\mathcal{O}\\left(\\frac{G^{2}t_{\\mathrm{mix}}^{2}}{\\delta^{2}t_{\\mathrm{hif}}T^{\\xi}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, the difference, $\\mathbf{E}\\|\\omega_{k}-\\bar{\\omega}_{k}\\|^{2}$ can be bounded as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbf{E}\\left[\\left\\|\\frac{1}{H}\\sum_{t=t_{k}}^{t_{k+1}-1}\\hat{A}_{\\mathrm{L},\\lambda}^{\\pi_{\\theta_{k}}}(s_{t},a_{t})\\nabla_{\\theta}\\log\\pi_{\\theta_{k}}(a_{t}|s_{t})-\\frac{1}{H}\\sum_{t=t_{k}}^{t_{k+1}-1}\\hat{A}_{\\mathrm{L},\\lambda}^{\\pi_{\\theta_{k}}}(s_{t},a_{t})\\nabla_{\\theta}\\log\\pi_{\\theta_{k}}(a_{t}|s_{t})\\right\\|^{2}\\right]}\\\\ &{\\stackrel{(a)}{\\le}\\frac{G^{2}}{H}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{E}\\left[\\left(\\hat{A}_{\\mathrm{L},\\lambda}^{\\pi_{\\theta_{k}}}(s_{t},a_{t})-A_{\\mathrm{L},\\lambda}^{\\pi_{\\theta_{k}}}(s_{t},a_{t})\\right)^{2}\\right]}\\\\ &{\\le\\frac{G^{2}}{H}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{E}\\left[\\sum_{a}\\pi_{\\theta_{k}}(a|s_{t})\\mathbf{E}\\left[\\left(\\hat{A}_{\\mathrm{L},\\lambda}^{\\pi_{\\theta_{k}}}(s_{t},a)-A_{\\mathrm{L},\\lambda}^{\\pi_{\\theta_{k}}}(s_{t},a)\\right)^{2}\\bigg|s_{t}\\right]\\right]\\stackrel{(b)}{\\le}\\mathcal{O}\\left(\\frac{A G^{2}t_{\\operatorname*{mix}}^{2}(\\log T)^{2}}{\\delta^{2}T^{\\xi}}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(a)$ follows from Assumption 3 and Jensen's inequality whereas $(b)$ follows from Lemma 2. Combining, (53) and (54), we conclude the result. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.2Proof of Lemma 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Using the Lemma 12, it is obvious to see that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{g}^{\\pi}-J_{g}^{\\pi^{\\prime}}=\\displaystyle\\sum_{s}\\sum_{a}d^{\\pi}(s)(\\pi(a|s)-\\pi^{\\prime}(a|s))Q_{g}^{\\pi^{\\prime}}(s,a)}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{s}\\sum_{a}d^{\\pi}(s)\\pi(a|s)Q_{g}^{\\pi^{\\prime}}(s,a)-\\displaystyle\\sum_{s}d^{\\pi}(s)V_{g}^{\\pi^{\\prime}}(s)}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{s}\\sum_{a}d^{\\pi}(s)\\pi(a|s)Q_{g}^{\\pi^{\\prime}}(s,a)-\\displaystyle\\sum_{s}\\sum_{a}d^{\\pi}(s)\\pi(a|s)V_{g}^{\\pi^{\\prime}}(s)}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{s}\\sum_{a}d^{\\pi}(s)\\pi(a|s)[Q_{g}^{\\pi^{\\prime}}(s,a)-V_{g}^{\\pi^{\\prime}}(s)]=\\mathbf{E}_{s\\sim d^{\\pi}}\\mathbf{E}_{a\\sim\\pi(\\cdot|s)}\\left[A_{g}^{\\pi^{\\prime}}(s,a)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We conclude the lemma using the definition of $J_{\\mathrm{L},\\lambda}$ and $A_{\\mathrm{L},\\lambda}$ ", "page_idx": 17}, {"type": "text", "text": "B.3Proof of Lemma 5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. We start with the definition of KL divergence. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\mathbf{u}_{t},-\\mathbf{u}_{t},-\\mathbf{u}_{t},-\\mathbf{u}_{t},\\tau}^{*}[\\langle X_{t}|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})}\\\\ &{\\mathbf{b}_{t}\\mathbf{b}_{t}\\mathcal{a}_{t},\\tau_{-\\delta_{t}-u_{t},\\tau}^{*}[\\langle X_{t}|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})}\\\\ &{\\mathrm{i}\\delta_{t}\\int_{\\mathbb{R}_{t}\\to u_{t},\\tau}^{*}\\mathrm{d}\\tau_{t}\\langle u_{t}|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|^{2}}\\\\ &{\\delta(u_{t},-u_{t},\\tau_{-\\delta_{t}-u_{t},\\tau}^{*}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})}\\\\ &{-\\delta(u_{t},-u_{t},\\tau_{-\\delta_{t}-u_{t},\\tau}^{*}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})|\\mathcal{H}_{B_{\\lambda}}(u_{t})}\\\\ &{-\\delta(u_{t},\\tau_{-\\delta_{t}-u_{t},\\tau}^{*}(u_{t})|\\mathcal{H}_\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the step (a) holds by Assumption 3 and step (b) holds by Lemma 4. Step (c) uses the convexity of the function $f(x)=x^{\\natural}$ . Finally, step (d) comes from the Assumption 4. Rearranging items, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{J_{\\mathrm{L}}(\\pi^{*},\\lambda_{k})-J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\le\\sqrt{\\epsilon_{\\mathrm{bias}}}+G\\|(\\omega_{k}-\\omega_{k}^{*})\\|+\\displaystyle\\frac{B\\alpha}{2}\\|\\omega_{k}\\|^{2}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad+\\,\\frac{1}{\\alpha}\\mathbf{E}_{s\\sim d^{\\pi^{*}}}[K L(\\pi^{*}(\\cdot|s)\\|\\pi_{\\theta_{k}}(\\cdot|s))-K L(\\pi^{*}(\\cdot|s)\\|\\pi_{\\theta_{k+1}}(\\cdot|s))]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Summing from $k=1$ to $K$ , using the non-negativity of $\\mathrm{KL}$ divergence and dividing the resulting expression by $K$ , we get the desired result. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.4 Proof of Lemma 6 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof.Bythe $L$ -smooth property of the objective function and constraint function, we know that $J_{\\mathrm{L}}(\\cdot,\\lambda)$ isa $L(1+\\lambda)$ -smooth function. Thus, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{\\mathrm{L}}(\\theta_{k+1},\\lambda_{k})\\geq J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})+\\langle\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k}),\\theta_{k+1}-\\theta_{k}\\rangle-\\frac{L(1+\\lambda_{k})}{2}\\|\\theta_{k+1}-\\theta_{k}\\|^{2}}\\\\ &{\\overset{(a)}{=}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})+\\alpha\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})^{T}\\omega_{k}-\\frac{L(1+\\lambda_{k})\\alpha^{2}}{2}\\|\\omega_{k}\\|^{2}}\\\\ &{=J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})+\\alpha\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}-\\alpha\\langle\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-\\omega_{k},\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\rangle}\\\\ &{\\quad-\\frac{L(1+\\lambda_{k})\\alpha^{2}}{2}\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-\\omega_{k}-\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}}\\\\ &{\\overset{(b)}{\\geq}J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})+\\alpha\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}-\\frac{\\alpha}{2}\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-\\omega_{k}\\|^{2}-\\frac{\\alpha}{2}\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}}\\\\ &{\\quad-L(1+\\lambda_{k})\\alpha^{2}\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-\\omega_{k}\\|^{2}-L(1+\\lambda_{k})\\alpha^{2}\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}}\\\\ &{=J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})+\\left(\\frac{\\alpha}{2}-L(1+\\lambda_{k})\\alpha^{2}\\right)\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}-\\left(\\\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where step (a) follows from the fact that $\\theta_{k+1}\\,=\\,\\theta_{k}\\,+\\,\\alpha\\omega_{k}$ and inequality (b) holds due to the Cauchy-Schwarz inequality. Now, adding $J_{\\mathrm{L}}(\\theta_{k+1},\\lambda_{k+1})$ on both sides, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{f}_{\\mathrm{L}}(\\theta_{k+1},\\lambda_{k+1})\\ge J_{\\mathrm{L}}(\\theta_{k+1},\\lambda_{k+1})-J_{\\mathrm{L}}(\\theta_{k+1},\\lambda_{k})+J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})+\\left(\\frac{\\alpha}{2}-L(1+\\lambda_{k})\\alpha^{2}\\right)\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})}\\\\ &{\\quad\\quad\\quad\\quad\\quad-\\left(\\frac{\\alpha}{2}+L(1+\\lambda_{k})\\alpha^{2}\\right)\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-\\omega_{k}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\frac{(\\alpha)}{=}\\left(\\lambda_{k+1}-\\lambda_{k}\\right)J_{\\mathrm{c}}(\\theta_{k+1})+J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})+\\left(\\frac{\\alpha}{2}-L(1+\\lambda_{k})\\alpha^{2}\\right)\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad-\\left(\\frac{\\alpha}{2}+L(1+\\lambda_{k})\\alpha^{2}\\right)\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-\\omega_{k}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\frac{(b)}{\\ge}-\\beta+J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})+\\left(\\frac{\\alpha}{2}-L(1+\\lambda_{k})\\alpha^{2}\\right)\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad-\\left(\\frac{\\alpha}{2}+L(1+\\lambda_{k})\\alpha^{2}\\right)\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-\\omega_{k}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (a) holds by the definition of $J_{\\mathrm{L}}(\\theta,\\lambda)$ and step (b) is true because $|J_{c}(\\theta)|\\;\\leq\\;1,\\forall\\theta$ and $|\\lambda_{k+1}-\\lambda_{k}|\\le\\beta|\\hat{J}_{c}(\\theta_{k})|\\le\\beta$ where the last inequality uses the fact that $|\\hat{J}_{c}(\\theta_{k})|\\leq1$ . Summing over $k\\in\\{1,\\cdots\\,,K\\}$ , we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{k=1}^{K}\\left[J_{\\mathrm{L}}(\\theta_{k+1},\\lambda_{k+1})-J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\right]\\geq-\\beta K+\\displaystyle\\sum_{k=1}^{K}\\left(\\frac{\\alpha}{2}-L(1+\\lambda_{k})\\alpha^{2}\\right)\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}}\\\\ {-\\displaystyle\\sum_{k=1}^{K}\\left(\\frac{\\alpha}{2}+L(1+\\lambda_{k})\\alpha^{2}\\right)\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-\\omega_{k}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which leads to the following. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{J_{\\mathrm{L}}(\\theta_{K+1},\\lambda_{K+1})-J_{\\mathrm{L}}(\\theta_{1},\\lambda_{1})\\ge-\\beta K+\\displaystyle\\sum_{k=1}^{K}\\left(\\frac{\\alpha}{2}-L(1+\\lambda_{k})\\alpha^{2}\\right)\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}}\\\\ &{}&{\\displaystyle-\\sum_{k=1}^{K}\\left(\\frac{\\alpha}{2}+L(1+\\lambda_{k})\\alpha^{2}\\right)\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-\\omega_{k}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Rearranging the terms and using $\\begin{array}{r}{0\\le\\lambda_{k}\\le\\frac{2}{\\delta}}\\end{array}$ due to the dual update, we arrive at the following. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}\\leq\\frac{J_{\\mathrm{L}}(\\theta_{K+1},\\lambda_{K+1})-J_{\\mathrm{L}}(\\theta_{1},\\lambda_{1})+\\beta K+(\\frac{\\alpha}{2}+L(1+\\frac{2}{\\delta})\\alpha^{2})\\sum_{k=1}^{K}\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}}{\\frac{\\alpha}{2}-L(1+\\frac{2}{\\delta})\\alpha^{2}}\\|\\nabla J_{\\mathrm{L}}(\\theta_{1},\\lambda_{1})\\|^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Choosing 4L(1+\\*) and dividing both sides by K, we conclude the result. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{K}\\sum_{k=1}^{K}\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}\\leq\\frac{16L(1+\\frac{2}{\\delta})}{K}\\left[J_{\\mathrm{L}}(\\theta_{K+1},\\lambda_{K+1})-J_{\\mathrm{L}}(\\theta_{1},\\lambda_{1})\\right]}}\\\\ &{\\quad+\\displaystyle\\frac{3}{K}\\sum_{k=1}^{K}\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-\\omega_{k}\\|^{2}+\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall that $\\begin{array}{r}{|J_{\\mathrm{L}}(\\theta,\\lambda)|\\le1+\\lambda\\le1+\\frac{2}{\\delta}\\le\\frac{3}{\\delta},\\forall\\theta\\in\\Theta,\\forall\\lambda\\ge0}\\end{array}$ Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})\\|^{2}\\leq\\frac{288L}{\\delta^{2}K}+\\frac{3}{K}\\sum_{k=1}^{K}\\|\\nabla J_{\\mathrm{L}}(\\theta_{k},\\lambda_{k})-\\omega_{k}\\|^{2}+\\beta\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This completes the proof. ", "page_idx": 19}, {"type": "text", "text": "B.5 Proof of Theorem 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.5.1 Rate of Convergence of the Objective ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall the definition of $J_{\\mathrm{L}}(\\theta,\\lambda)=J_{r}(\\theta)+\\lambda J_{c}(\\theta)$ . Using Lemma 7, we arrive at the following. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\bigg(J_{r}^{\\pi^{*}}-J_{r}(\\theta_{k})\\bigg)\\le G\\left(1+\\frac{1}{\\mu_{F}}\\right)\\Tilde{\\mathcal{O}}\\left(\\sqrt{\\beta}+\\frac{\\sqrt{A}G t_{\\mathrm{mix}}}{\\delta T^{\\xi/2}}+\\frac{\\sqrt{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}}{\\delta T^{(1-\\xi)/2}}\\right)}\\\\ &{\\displaystyle+\\frac{B}{L}\\Tilde{\\mathcal{O}}\\left(\\frac{A G^{2}t_{\\mathrm{mix}}^{2}}{\\delta^{2}T^{\\xi}}+\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta^{2}T^{1-\\xi}}+\\beta\\right)+\\Tilde{\\mathcal{O}}\\bigg(\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}\\mathbf{E}_{s\\sim d^{\\pi^{*}}}[K L(\\pi^{*}(\\cdot|s)\\|\\pi_{\\theta_{1}}(\\cdot|s))]}{T^{1-\\xi}\\delta}\\bigg)}\\\\ &{\\displaystyle-\\,\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\bigg[\\lambda_{k}\\bigg(J_{c}^{\\pi^{*}}-J_{c}(\\theta_{k})\\bigg)\\bigg]+\\sqrt{\\epsilon_{\\mathrm{bias}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, we need to find a bound for the last term in the above equation. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{I}\\otimes\\mathbb{E}\\left(\\left(\\left(\\lambda_{K+1}\\right)^{2}\\right)^{2}\\right.}\\\\ &{\\left.\\stackrel{(a)}{\\le}\\frac{\\sqrt{6}}{16}\\left(\\left((\\lambda_{K+1})^{2}-(\\lambda_{K})^{2}\\right)\\right)}\\\\ &{\\quad\\stackrel{(b)}{=}\\frac{\\sqrt{6}}{16}\\left(\\left(\\rho_{12}\\left|\\lambda_{K}-\\beta_{2}\\right|,(\\lambda_{K})\\right)^{2}-(\\lambda_{K})^{2}\\right)}\\\\ &{\\le\\frac{\\sqrt{6}}{16}\\left(\\left(\\lambda_{K}-\\beta_{2}\\right)^{2}-(\\lambda_{K})^{2}\\right)}\\\\ &{=-2\\tilde{\\sigma}\\sum_{k=1}^{N}\\lambda_{j}(\\rho_{k})+\\beta_{k}^{2}\\frac{\\sqrt{6}}{16}\\left(\\alpha_{k}^{2}\\right)^{2}}\\\\ &{\\stackrel{(c)}{=}\\frac{\\sqrt{6}}{16}\\sum_{k=1}^{N}(\\lambda_{K})^{2}-\\tilde{\\sigma}_{k}(\\rho_{k})+\\beta_{k}^{2}\\frac{\\sqrt{6}}{16}(\\rho_{k})^{2}}\\\\ &{\\le2\\tilde{\\sigma}\\sum_{k=1}^{N}\\lambda_{(K})^{2}-\\tilde{\\sigma}_{k}(\\rho_{k})+2\\beta_{k}^{2}\\sum_{l}^{N}\\tilde{\\sigma}_{l}(\\rho_{l})^{2}}\\\\ &{\\le2\\tilde{\\sigma}\\sum_{k=1}^{N}\\lambda_{(k)}(r_{k}-\\beta_{l}(\\rho_{k})+2\\beta_{k}^{2}\\sum_{l}^{N}\\tilde{\\sigma}_{l}(\\rho_{l})^{2}}\\\\ &{=2\\tilde{\\sigma}\\sum_{k=1}^{N}(\\lambda_{k}-\\beta_{l}(\\rho_{k})+2\\beta_{k}^{2}\\sum_{l}^{N}\\lambda_{l}(\\rho_{l},\\theta_{l})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (a) uses $\\lambda_{1}=0$ and inequality (b) holds because $\\theta^{*}$ is a feasible solution to the constrained optimization problem. Rearranging items and taking the expectation, we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}\\Big[\\lambda_{k}(J_{\\mathrm{c}}^{\\pi^{*}}-J_{\\mathrm{c}}(\\theta_{k}))\\Big]\\leq\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}\\Big[\\lambda_{k}(J_{\\mathrm{c}}(\\theta_{k})-\\hat{J}_{\\mathrm{c}}(\\theta_{k}))\\Big]+\\displaystyle\\frac{\\beta}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}[\\hat{J}_{\\mathrm{c}}(\\theta_{k})]^{2}}\\\\ {\\overset{(a)}{\\leq}\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}\\Big[\\lambda_{k}\\left(J_{\\mathrm{c}}(\\theta_{k})-\\hat{J}_{\\mathrm{c}}(\\theta_{k})\\right)\\Big]+\\beta}\\\\ {\\overset{(b)}{=}\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}\\Big[\\lambda_{k}\\left(J_{\\mathrm{c}}(\\theta_{k})-\\mathbf{E}\\left[\\hat{J}_{\\mathrm{c}}(\\theta_{k})\\big|\\theta_{k}\\right]\\right)\\Big]+\\beta}\\\\ {\\leq\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}\\Big[\\lambda_{k}\\left|J_{\\mathrm{c}}(\\theta_{k})-\\mathbf{E}\\left[\\hat{J}_{\\mathrm{c}}(\\theta_{k})\\big|\\theta_{k}\\right]\\right|\\Big]+\\beta\\stackrel{(c)}{\\leq}\\displaystyle\\frac{2}{\\delta T^{2}}+\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (a) results from $|\\hat{J}_{c,\\rho}(\\theta)|^{2}\\,\\leq\\,1,\\,\\forall\\theta\\,\\in\\,\\Theta$ and (b) uses the fact that $\\hat{J}_{c,\\rho}(\\theta_{k})$ and $\\lambda_{k}$ are conditionally independent given $\\theta_{k}$ . Finally, (c) is a consequence of Lemma 13. Combining (67) with (65), we deduce, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\bigg(J_{r}^{\\pi^{*}}-J_{r}(\\theta_{k})\\bigg)}\\\\ &{\\le\\sqrt{\\epsilon_{\\mathrm{bias}}}+G\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(\\sqrt{\\beta}+\\frac{\\sqrt{A}G t_{\\mathrm{mix}}}{\\delta T^{\\xi/2}}+\\frac{\\sqrt{L t_{\\mathrm{mix}}}t_{\\mathrm{hit}}}{\\delta T^{(1-\\xi)/2}}\\right)+\\mathcal{O}\\left(\\frac{1}{\\delta T^{2}}+\\beta\\right)}\\\\ &{+\\displaystyle\\frac{B}{L}\\tilde{\\mathcal{O}}\\left(\\frac{A G^{2}t_{\\mathrm{mix}}^{2}}{\\delta^{2}T^{\\xi}}+\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta^{2}T^{1-\\xi}}+\\beta\\right)+\\tilde{\\mathcal{O}}\\left(\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}\\mathbf{E}_{s\\sim d^{\\pi^{*}}}\\left[K L\\left(\\pi^{*}\\langle\\cdot|s\\rangle\\right|\\right]\\pi_{\\theta_{1}}\\left(\\cdot|s\\rangle\\right)\\right]}{T^{1-\\xi}\\delta}\\right)}\\\\ &{\\leq\\sqrt{\\epsilon_{\\mathrm{bias}}}+G\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(\\sqrt{\\beta}+\\frac{\\sqrt{A}G t_{\\mathrm{mix}}}{\\delta T^{\\xi/2}}+\\frac{\\sqrt{L t_{\\mathrm{mix}}}t_{\\mathrm{hit}}}{\\delta T^{(1-\\xi)/2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The last inequality presents only the dominant terms of $\\beta$ and $T$ ", "page_idx": 20}, {"type": "text", "text": "B.5.2 Rate of Constraint Violation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Since $\\{\\lambda_{k}\\}_{k=1}^{K}$ are derived by applying the dual update in Algorithm 1, we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi\\left|\\lambda_{k+1}-\\frac{2}{\\delta}\\right|^{2}\\overset{(a)}{\\leq}\\mathbb{E}\\left|\\lambda_{k}-\\beta\\hat{f}_{\\varepsilon}(\\theta_{k})-\\frac{2}{\\delta}\\right|^{2}}\\\\ &{=\\mathbb{E}\\left|\\lambda_{k}-\\frac{2}{\\delta}\\right|^{2}-2\\beta\\mathbb{E}\\left[\\hat{f}_{\\varepsilon}(\\theta_{k})\\left(\\lambda_{k}-\\frac{2}{\\delta}\\right)\\right]+\\beta^{2}\\mathbb{E}\\left[\\hat{f}_{\\varepsilon}^{2}(\\theta_{k})\\right]}\\\\ &{\\overset{(b)}{\\leq}\\mathbb{E}\\left|\\lambda_{k}-\\frac{2}{\\delta}\\right|^{2}-2\\beta\\mathbb{E}\\left[f_{\\varepsilon}(\\theta_{k})\\left(\\lambda_{k}-\\frac{2}{\\delta}\\right)\\right]-2\\beta\\mathbb{E}\\left[\\left(\\hat{f}_{\\varepsilon}(\\theta_{k})-J_{\\varepsilon}(\\theta_{k})\\right)\\left(\\lambda_{k}-\\frac{2}{\\delta}\\right)\\right]+\\beta^{2}}\\\\ &{\\overset{(c)}{=}\\mathbb{E}\\left|\\lambda_{k}-\\frac{2}{\\delta}\\right|^{2}-2\\beta\\mathbb{E}\\left[f_{\\varepsilon}(\\theta_{k})\\left(\\lambda_{k}-\\frac{2}{\\delta}\\right)\\right]-2\\beta\\mathbb{E}\\left[\\left(\\mathbb{E}\\left[\\hat{f}_{\\varepsilon}(\\theta_{k})\\big|\\theta_{k}\\right]-J_{\\varepsilon}(\\theta_{k})\\right)\\left(\\lambda_{k}-\\frac{2}{\\delta}\\right)\\right]+\\beta^{2}}\\\\ &{\\leq\\mathbb{E}\\left|\\lambda_{k}-\\frac{2}{\\delta}\\right|^{2}-2\\beta\\mathbb{E}\\left[J_{\\varepsilon}(\\theta_{k})\\left(\\lambda_{k}-\\frac{2}{\\delta}\\right)\\right]+2\\beta\\mathbb{E}\\left[\\bigg|\\mathbb{E}\\left[\\hat{f}_{\\varepsilon}(\\theta_{k})\\big|\\theta_{k}\\right]-J_{\\varepsilon}(\\theta_{k})\\bigg|\\lambda_{k}-\\frac{2}{\\delta}\\bigg|\\right]+\\beta^{2}}\\\\ &{\\overset{(d)}{\\leq}\\mathbb{E}\\left|\\lambda_{k}-\\frac{2}{\\delta}\\right|^{2}-2\\beta\\mathbb{E}\\left[J_{\\varepsilon}(\\theta_{k})\\left(\\lambda_{k}-\\frac{2}{\\delta}\\right)\\right]+4\\beta^{2}}\\\\ &{\\overset{(c)}{ \n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(a)$ is due to the non-expansiveness of the projection $\\mathcal{P}_{[0,\\frac{2}{\\delta}]}$ and $(b)$ holds because $\\hat{J}_{c}(\\theta)\\in[0,1]$ $\\forall\\theta\\in\\Theta$ according to its definition in Algorithm 1. Finally, $(c)$ is a consequence of the fact that $\\hat{J}_{c}(\\theta_{k})$ ", "page_idx": 20}, {"type": "text", "text": "and $\\lambda_{k}$ are conditionally independent given $\\theta_{k}$ whereas $(d)$ applies $|\\lambda_{k}-\\textstyle\\frac{2}{\\delta}|\\leq\\frac{2}{\\delta}$ and Lemma 13. Averaging (69) over $k\\in\\{1,\\ldots,K\\}$ , we get, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\left[J_{c}(\\theta_{k})\\left(\\lambda_{k}-\\frac{2}{\\delta}\\right)\\right]\\leq\\frac{\\left|\\lambda_{1}-\\frac{2}{\\delta}\\right|^{2}-\\left|\\lambda_{K+1}-\\frac{2}{\\delta}\\right|^{2}}{2\\beta K}+\\frac{2}{\\delta T^{2}}+\\frac{\\beta}{2}\\stackrel{(a)}{\\leq}\\frac{2}{\\delta^{2}\\beta K}+\\frac{2}{\\delta T^{2}}+\\frac{\\beta}{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (a) uses $\\lambda_{1}=0$ . Note that $\\lambda_{k}J_{c}^{\\pi^{*}}\\geq0,\\forall k$ . Adding the above inequality to (65) at both sides we, therefore, have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\bigg[J_{r}^{\\pi^{*}}-\\cfrac{1}{K}\\displaystyle\\sum_{k=1}^{K}J_{r}(\\theta_{k})\\bigg]+\\frac{2}{\\delta}\\mathbf{E}\\bigg[\\cfrac{1}{K}\\sum_{k=1}^{K}-J_{c}(\\theta_{k})\\bigg]\\leq\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\frac{2}{\\delta^{2}\\beta K}+\\frac{2}{T^{2}\\delta}+\\frac{\\beta}{2}}\\\\ &{\\,\\,+G\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(\\sqrt{\\beta}+\\frac{\\sqrt{A}G t_{\\mathrm{mix}}}{\\delta T^{5/2}}+\\frac{\\sqrt{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}}{\\delta T^{(1-\\xi)/2}}\\right)+\\frac{B}{L}\\tilde{\\mathcal{O}}\\left(\\frac{A G^{2}t_{\\mathrm{mix}}^{2}}{\\delta^{2}T^{\\xi}}+\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta^{2}T^{1-\\xi}}+\\beta\\right)}\\\\ &{\\,\\,+\\tilde{\\mathcal{O}}\\bigg(\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}\\mathbf{E}_{s\\sim d^{\\pi^{*}}}[K L\\left(\\pi^{*}(\\cdot|s)\\right)]\\pi_{\\theta_{1}}(\\cdot|s))]\\bigg)}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since the functions $\\{J_{g}(\\theta_{k})\\},k\\in\\{0,\\cdots,K-1\\},g\\in\\{r,c\\}$ are linear in occupancy measure, there exists a policy $\\bar{\\pi}$ such that the following holds $\\forall g\\in\\{r,c\\}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}J_{g}(\\theta_{k})=J_{g}^{\\bar{\\pi}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Injecting the above relation to (71), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\bigg[J_{r}^{\\pi^{*}}-J_{r}^{\\bar{\\pi}}\\bigg]+\\frac{2}{\\delta}\\mathbf{E}\\bigg[-J_{c}^{\\bar{\\pi}}\\bigg]\\leq\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\frac{2}{\\delta^{2}\\beta K}+\\frac{2}{T^{2}\\delta}+\\frac{\\beta}{2}}\\\\ &{\\,\\,+G\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(\\sqrt{\\beta}+\\frac{\\sqrt{A}G t_{\\mathrm{mix}}}{\\delta T^{5/2}}+\\frac{\\sqrt{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}}{\\delta T^{(1-\\xi)/2}}\\right)}\\\\ &{\\,\\,+\\frac{B}{L}\\tilde{\\mathcal{O}}\\left(\\frac{A G^{2}t_{\\mathrm{mix}}^{2}}{\\delta^{2}T^{\\xi}}+\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta^{2}T^{1-\\xi}}+\\beta\\right)+\\tilde{\\mathcal{O}}\\bigg(\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}\\mathbf{E}_{s\\sim d^{\\pi^{*}}}\\left[K L\\left(\\pi^{*}(\\cdot|s)\\right|\\pi_{\\theta_{1}}(\\cdot|s)\\right)\\right]}{T^{1-\\xi}\\delta}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Lemma 18, we arrive at, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\Bigg[-J_{c}^{\\pi}\\Bigg]}\\\\ &{\\le\\delta\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\frac{2}{\\delta\\beta K}+\\frac{2}{T^{2}}+\\frac{\\delta\\beta}{2}+G\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(\\delta\\sqrt{\\beta}+\\frac{\\sqrt{A}G t_{\\mathrm{mix}}}{T^{\\xi/2}}+\\frac{\\sqrt{L t_{\\mathrm{mix}}}t_{\\mathrm{hit}}}{T^{(1-\\xi)/2}}\\right)}\\\\ &{+\\frac{B}{L}\\tilde{\\mathcal{O}}\\left(\\frac{A G^{2}t_{\\mathrm{mix}}^{2}}{\\delta T^{\\xi}}+\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta T^{1-\\xi}}+\\delta\\beta\\right)+\\tilde{\\mathcal{O}}\\left(\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}\\mathbf{E}_{s\\sim d^{\\pi}}\\left[K L\\left(\\pi^{*}(\\cdot|s)\\right|\\pi_{\\theta_{1}}\\left(\\cdot|s\\right)\\right)\\right]}{T^{1-\\xi}}\\right)}\\\\ &{\\le\\delta\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\tilde{\\mathcal{O}}\\left(\\frac{2t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta\\beta T^{1-\\xi}}\\right)+G\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(\\delta\\sqrt{\\beta}+\\frac{\\sqrt{A}G t_{\\mathrm{mix}}}{T^{\\xi/2}}+\\frac{\\sqrt{L t_{\\mathrm{mix}}}t_{\\mathrm{hit}}}{T^{(1-\\xi)/2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The last inequality presents only the dominant terms of $\\beta$ and $T$ ", "page_idx": 21}, {"type": "text", "text": "B.5.3  Optimal Choice of $\\beta$ and $\\xi$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "If we choose $\\beta=T^{-\\eta}$ for some $\\eta\\in(0,1)$ , then following (68) and (74), we can write, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\bigg(J_{r}^{\\pi^{*}}-J_{r}(\\theta_{k})\\bigg)\\le\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\tilde{\\mathcal{O}}\\left(T^{-\\eta/2}+T^{-\\xi/2}+T^{-(1-\\xi)/2}\\right),}\\\\ &{\\qquad\\mathbf{E}\\left[\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}-J_{c}(\\theta_{k})\\right]\\le\\delta\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\tilde{\\mathcal{O}}\\left(T^{-(1-\\xi-\\eta)}+T^{-\\eta/2}+T^{-\\xi/2}+T^{-(1-\\xi)/2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Clearly, the optimal values of $\\eta$ and $\\xi$ can be obtained by solving the following optimization. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{(\\eta,\\xi)\\in(0,1)^{2}}\\operatorname*{min}\\left\\{1-\\xi-\\eta,\\frac{\\eta}{2},\\frac{\\xi}{2},\\frac{1-\\xi}{2}\\right\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "One can easily verify that $(\\xi,\\eta)=(2/5,2/5)$ is the solution of the above optimization. Therefore, the convergence rate of the objective function can be written as follows. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{E}\\bigg(J_{r}^{\\pi^{*}}-J_{r}(\\theta_{k})\\bigg)}\\\\ &{\\le\\sqrt{\\epsilon_{\\mathrm{bias}}}+G\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(\\frac{1}{T^{1/5}}+\\frac{\\sqrt{A}G t_{\\mathrm{mix}}}{\\delta T^{1/5}}+\\frac{\\sqrt{L t_{\\mathrm{mix}}}t_{\\mathrm{bit}}}{\\delta T^{3/10}}\\right)+\\mathcal{O}\\left(\\frac{1}{\\delta T^{2}}+\\frac{1}{T^{2/5}}\\right)}\\\\ &{+\\displaystyle\\frac{B}{L}\\tilde{\\mathcal{O}}\\left(\\frac{\\delta^{2}+A G^{2}t_{\\mathrm{mix}}^{2}}{\\delta^{2}T^{2/5}}+\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta^{2}T^{3/5}}\\right)+\\tilde{\\mathcal{O}}\\left(\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}\\mathbf{E}_{s\\sim d^{\\pi^{*}}}\\left[K L\\left(\\pi^{*}(\\cdot|s)\\right|\\pi_{\\theta_{1}}(\\cdot|s)\\right)\\right]}{T^{3/5}\\delta}\\right)}\\\\ &{\\leq\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\frac{\\sqrt{A G^{2}t_{\\mathrm{mix}}}}{\\delta}\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(T^{-1/5}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The last expression only considers the dominant terms of $T$ . Similarly, the constraint violation rate can be computed as, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbb{E}\\bigg[\\frac{1}{K}\\sum_{k=1}^{K}-J_{c}(\\theta_{k})\\bigg]}\\\\ &{\\leq\\delta\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\tilde{\\mathcal{O}}\\left(\\frac{t_{\\mathrm{mix}}f_{\\mathrm{hit}}}{\\delta T^{1/5}}+\\frac{1}{T^{2}}+\\frac{\\delta}{T^{2/5}}\\right)+G\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(\\frac{\\delta+\\sqrt{A}G t_{\\mathrm{mix}}}{T^{1/5}}+\\frac{\\sqrt{L t_{\\mathrm{mix}}}t_{\\mathrm{hit}}}{T^{3/10}}\\right)}\\\\ &{+\\frac{B}{L}\\tilde{\\mathcal{O}}\\left(\\frac{\\delta^{2}+A G^{2}t_{\\mathrm{mix}}^{2}}{\\delta T^{2/5}}+\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta T^{3/5}}\\right)+\\tilde{\\mathcal{O}}\\bigg(\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}\\mathbf{E}_{s\\sim d^{\\pi}}\\left[K L(\\pi^{*}(\\cdot|s)\\|\\pi_{\\theta_{1}}(\\cdot|s))\\right]}{T^{3/5}}\\bigg)}\\\\ &{\\leq\\delta\\sqrt{\\epsilon_{\\mathrm{bias}}}+\\tilde{\\mathcal{O}}\\left(\\frac{t_{\\mathrm{mix}}f_{\\mathrm{hit}}}{\\delta T^{1/5}}\\right)+\\sqrt{A}G^{2}t_{\\mathrm{mix}}\\left(1+\\frac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(T^{-1/5}\\right)}&{\\ldots}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last expression contains only the dominant terms of $T$ . This concludes the theorem. ", "page_idx": 22}, {"type": "text", "text": "C Proofs for the Regret and Violation Analysis ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.1 Proof of Lemma 8 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. Using Taylor's expansion, we can write the following $\\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A},\\forall k$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\pi_{\\theta_{k+1}}(a|s)-\\pi_{\\theta_{k}}(a|s)|=\\left|(\\theta_{k+1}-\\theta_{k})^{T}\\nabla_{\\theta}\\pi_{\\bar{\\theta}}(a|s)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\pi_{\\bar{\\theta}_{k}}(a|s)\\left|(\\theta_{k+1}-\\theta_{k})^{T}\\nabla_{\\theta}\\log\\pi_{\\bar{\\theta}_{k}}(a|s)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\pi_{\\bar{\\theta}_{k}}(a|s)\\|\\theta_{k+1}-\\theta_{k}\\|\\|\\nabla_{\\theta}\\log\\pi_{\\bar{\\theta}_{k}}(a|s)\\|\\overset{(a)}{\\leq}G\\|\\theta_{k+1}-\\theta_{k}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\bar{\\theta}_{k}$ is some convex combination' of $\\theta_{k}$ and $\\theta_{k+1}$ and $(a)$ results from Assumption 3. This concludes the first statement. Applying (80) and Lemma 12, we obtain the following for $g\\in\\{r,c\\}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{K}\\mathbb{E}\\left\\{\\sum_{j\\in(\\ell_{k-1})}^{L_{\\ell}(\\ell_{k+1})-2\\ell_{j}(\\theta_{i})}\\right\\}=\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\left\\{\\sum_{\\alpha_{k}=\\ell^{\\prime}+1(s)}^{L_{\\ell}(\\ell_{k+1})}(s(\\alpha_{k+1}(a)s)-\\alpha_{k+1}(a)s)Q_{\\ell}^{\\alpha_{k}}(s,a)\\right\\}}\\\\ &{\\displaystyle\\leq\\sum_{i=1}^{K}\\mathbb{E}\\left[\\sum_{\\alpha_{k}=\\ell^{\\prime}+1(s)}^{L_{\\ell}(\\ell_{k+1})}(s|\\alpha_{k+1}(a)s-\\alpha_{k}(a)s|)\\Big|Q_{\\ell}^{\\alpha_{k}}(s,a)\\Big]\\right.}\\\\ &{\\displaystyle\\leq\\left.\\mathcal{Q}\\sum_{i=1}^{K}\\mathbb{E}\\left[\\sum_{\\alpha_{k}=\\ell^{\\prime}+1(s)}^{L_{\\ell}(\\ell_{k+1})-\\alpha_{k}}|\\mathcal{Q}_{\\ell}^{\\alpha_{k}}(s,a)|\\right]\\right.}\\\\ &{\\displaystyle\\overset{(i)}{\\leq}G\\sum_{k=1}^{K}\\mathbb{E}\\left[\\sum_{\\alpha_{k}=\\ell^{\\prime}+1(s)}^{L_{\\ell}(\\ell_{k+1})}|\\alpha_{k+1}|\\cdot\\mathcal{Q}_{\\ell_{k+1}}\\right]=6A G\\alpha_{k+1}\\sum_{k=1}^{K}\\mathbb{E}\\|\\alpha_{k}\\|}\\\\ &{\\displaystyle\\overset{(i)}{\\leq}G A G\\alpha_{k+1}\\sqrt{K}\\left(\\sum_{i=1}^{K}\\mathbb{E}\\|\\alpha_{k+1}\\|^{2}\\right)^{k}}\\\\ &{\\displaystyle\\overset{(i)}{\\leq}\\mathcal{Q}\\left(\\alpha_{k}^{\\ell}\\Delta\\alpha_{k}^{\\ell}\\left(\\sqrt{A G_{\\ell_{k+1}}(s)}\\right)^{\\ell}+1\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Inequality $(a)$ uses Lemma 9 and the update rule $\\theta_{k+1}=\\theta_{k}+\\alpha\\omega_{k}$ Step $(b)$ holds by the Cauchy inequality and Jensen inequality whereas $(c)$ can be derived using (22) and substituting $K=T/H$ This establishes the second statement. Next, recall from (5) that for any policy $\\pi_{\\theta}$ \uff0c $g^{\\pi_{\\theta}}(s)\\ \\triangleq$ $\\textstyle\\sum_{a}\\pi_{\\theta}(a|s)g(s,a)$ . Note that, for any policy parameter $\\theta$ , and any state $s\\in S$ , the following holds. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\chi_{g}^{\\pi_{\\theta}}(s)=\\sum_{t=0}^{\\infty}\\left\\langle(P^{\\pi_{\\theta}})^{t}(s,\\cdot)-d^{\\pi_{\\theta}},g^{\\pi_{\\theta}}\\right\\rangle=\\sum_{t=0}^{N-1}\\left\\langle(P^{\\pi_{\\theta}})^{t}(s,\\cdot),g^{\\pi_{\\theta}}\\right\\rangle-N J(\\theta)+\\sum_{t=N}^{\\infty}\\left\\langle(P^{\\pi_{\\theta}})^{t}(s,\\cdot)-c^{\\pi_{\\theta}}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Define the following quantity. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\delta^{\\pi_{\\theta}}(s,T)\\triangleq\\sum_{t=N}^{\\infty}\\|(P^{\\pi_{\\theta}})^{t}(s,\\cdot)-d^{\\pi_{\\theta}}\\|_{1}\\ {\\mathrm{~where~}}N=4t_{\\operatorname*{mix}}(\\log_{2}T)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 10 states that for sufficiently large $T$ ,we have $\\begin{array}{r}{\\delta^{\\pi_{\\theta}}(s,T)\\le\\frac{1}{T^{3}}}\\end{array}$ for any policy $\\pi_{\\theta}$ and state $s$ Combining this result with the fact that the $g^{\\pi_{\\theta}}$ function is absolutely bounded in $[0,1]$ , we obtain, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{k=1}^{K}\\mathbb{E}\\big|V_{g}^{\\pi_{k+1}}(s_{k})-V_{g}^{\\pi_{k}}(s_{k})\\big|}}\\\\ &{\\le\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\bigg|\\sum_{t=0}^{N-1}\\big\\langle(P^{\\pi_{k+1}})^{t}(s_{k},\\cdot)-(P^{\\pi_{k}})^{t}(s_{k},\\cdot),g^{\\pi_{k+1}}\\big\\rangle\\bigg|+\\sum_{k=1}^{K}\\mathbb{E}\\bigg|\\sum_{t=0}^{N-1}\\big\\langle(P^{\\pi_{k}})^{t}(s_{k},\\cdot),g^{\\pi_{k+1}}\\big\\rangle}\\\\ &{+N\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\big|I_{j}(\\theta_{k+1})-J_{g}(\\theta_{k})\\big|+\\frac{2K}{T^{3}}}\\\\ &{\\overset{(a)}{\\le}\\displaystyle\\sum_{k=1}^{K}\\sum_{t=0}^{N-1}\\mathbb{E}\\big\\|(P^{\\pi_{k+1}})^{t}-(P^{\\pi_{k}})^{t})g^{\\pi_{k+1}}\\big\\|_{\\infty}+\\displaystyle\\sum_{k=1}^{K}\\sum_{t=0}^{N-1}\\mathbb{E}\\big\\|g^{\\pi_{k+1}}-g^{\\pi_{k}}\\big\\|_{\\infty}}\\\\ &{+\\tilde{\\mathcal{O}}\\left(\\frac{\\alpha A G_{\\operatorname*{min}}}{\\delta\\|\\tau_{1}\\|_{\\infty}}\\left[\\Big(\\sqrt{A}G_{\\operatorname*{min}}+\\delta\\Big)T^{2}+\\sqrt{A}t_{\\operatorname*{min}}\\mathrm{Ein}T^{\\frac{3}{2}}\\Big)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $(a)$ follows from (81) and substituting $N=4t_{\\operatorname*{mix}}(\\log_{2}T)$ . For the first term, note that, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\big(\\big(P^{\\pi_{\\theta_{k+1}}}\\big)^{t}-\\big(P^{\\pi_{\\theta_{k}}}\\big)^{t}\\big)g^{\\pi_{\\theta_{k+1}}}\\|_{\\infty}}\\\\ &{\\leq\\|P^{\\pi_{\\theta_{k+1}}}\\big(\\big(P^{\\pi_{\\theta_{k+1}}}\\big)^{t-1}-\\big(P^{\\pi_{\\theta_{k}}}\\big)^{t-1}\\big)g^{\\pi_{\\theta_{k+1}}}\\|_{\\infty}+\\|\\big(P^{\\pi_{\\theta_{k+1}}}-P^{\\pi_{\\theta_{k}}}\\big)\\big(P^{\\pi_{\\theta_{k}}}\\big)^{t-1}g^{\\pi_{\\theta_{k+1}}}\\|_{\\infty}}\\\\ &{\\overset{(a)}{\\leq}\\|\\big(\\big(P^{\\pi_{\\theta_{k+1}}}\\big)^{t-1}-\\big(P^{\\pi_{\\theta_{k}}}\\big)^{t-1}\\big)g^{\\pi_{\\theta_{k+1}}}\\|_{\\infty}+\\underset{s}{\\operatorname*{max}}\\|P^{\\pi_{\\theta_{k+1}}}\\big(s,\\cdot\\big)-P^{\\pi_{\\theta_{k}}}\\big(s,\\cdot\\big)\\|_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Inequality $(a)$ holds since every row of $P^{\\pi_{\\theta_{k}}}$ sums to 1 and $\\|(P^{\\pi_{\\theta_{k}}})^{t-1}g^{\\pi_{\\theta_{k+1}}}\\|_{\\infty}\\leq1$ Moreover, invoking (80), and the parameter update rule $\\theta_{k+1}=\\theta_{k}+\\alpha\\omega_{k}$ ,we get, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{max}_{s}\\|P^{\\pi_{\\theta_{k+1}}}(s,\\cdot)-P^{\\pi_{\\theta_{k}}}(s,\\cdot)\\|_{1}=\\operatorname*{max}_{s}\\left|\\displaystyle\\sum_{s^{\\prime}}\\sum_{a}(\\pi_{\\theta_{k+1}}(a|s)-\\pi_{\\theta_{k}}(a|s))P(s^{\\prime}|s,a)\\right|}\\\\ {\\displaystyle\\leq G\\|\\theta_{k+1}-\\theta_{k}\\|\\operatorname*{max}_{s}\\left|\\displaystyle\\sum_{s^{\\prime}}\\sum_{a}P(s^{\\prime}|s,a)\\right|}\\\\ {\\displaystyle\\leq\\alpha A G\\|\\omega_{k}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Plugging the above result into (85) and using a recursive argument, we get, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|((P^{\\pi_{\\theta_{k+1}}})^{t}-(P^{\\pi_{\\theta_{k}}})^{t})g^{\\pi_{\\theta_{k+1}}}\\|_{\\infty}\\leq\\displaystyle\\sum_{t^{\\prime}=1}^{t}\\operatorname*{max}_{s}\\|P^{\\pi_{\\theta_{k+1}}}(s,\\cdot)-P^{\\pi_{\\theta_{k}}}(s,\\cdot)\\|_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{t^{\\prime}=1}^{t}\\alpha A G\\|\\omega_{k}\\|\\leq\\alpha t A G\\|\\omega_{k}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{t=0}^{N-1}\\mathbf{E}\\big\\lVert\\big((P^{\\pi_{k+1}})^{t}-(P^{\\pi_{k}})^{t}\\big)g^{\\pi_{k+1}}\\big\\rVert_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{k=1}^{K}\\sum_{t=0}^{N-1}\\alpha t A G\\big\\lVert\\omega_{k}\\big\\rVert}\\\\ &{\\qquad\\qquad\\leq\\mathcal{O}(\\alpha A G N^{2})\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}\\big\\lVert\\omega_{k}\\big\\rVert}\\\\ &{\\qquad\\qquad\\leq\\mathcal{O}(\\alpha A G N^{2}\\sqrt{K})\\left(\\displaystyle\\sum_{k=1}^{K}\\mathbf{E}\\big\\lVert\\omega_{k}\\big\\rVert^{2}\\right)^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\stackrel{(a)}{=}\\mathcal{O}\\left(\\frac{\\alpha A G\\Gamma_{m k}}{\\delta t_{k}}\\left[\\Big(\\sqrt{A}G_{m k}+\\delta\\Big)T^{\\frac{1}{2}}+\\sqrt{L t_{m k}t_{m k}}T^{\\frac{3}{10}}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $(a)$ follows from (22). Moreover, notice that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{t=0}^{N-1}\\mathbb{E}\\|g^{\\pi_{\\theta_{k+1}}}-g^{\\pi_{\\theta_{k}}}\\|_{\\infty}\\leq\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{t=0}^{N-1}\\mathbb{E}\\left[\\operatorname*{max}_{s}\\left|\\sum_{a}(\\pi_{\\theta_{k+1}}(a|s)-\\pi_{\\theta_{k}}(a|s))g(s,a)\\right|\\right]}\\\\ {\\displaystyle\\overset{(a)}{\\leq}\\alpha A G N\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\|\\omega_{k}\\|}\\\\ {\\displaystyle\\leq\\alpha A G N\\sqrt{K}\\left(\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\|\\omega_{k}\\|^{2}\\right)^{\\frac{1}{2}}}\\\\ {\\displaystyle\\overset{(b)}{\\leq}\\tilde{\\mathcal{O}}\\left(\\frac{\\alpha A G}{\\delta t_{\\mathrm{hit}}}\\left[\\left(\\sqrt{A}G t_{\\mathrm{mix}}+\\delta\\right)T^{\\frac{2}{5}}+\\sqrt{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}T^{\\frac{3}{10}}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $(a)$ follows from (80) and the update rule $\\theta_{k+1}=\\theta_{k}+\\alpha\\omega_{k}$ whereas $(b)$ is a consequence of (22). Combining (84), (86), and (87), we establish the third statement. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "C.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. Recall the decomposition of the regret in section 5 and take the expectation. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\exists[\\mathrm{Reg}_{T}]=\\sum_{t=0}^{T-1}\\left(J_{r}^{\\pi^{*}}-r(s_{t},a_{t})\\right)=H\\sum_{k=1}^{K}\\left(J_{r}^{\\pi^{*}}-J_{r}(\\theta_{k})\\right)+\\sum_{k=1}^{K}\\sum_{t\\in\\mathcal{T}_{k}}\\left(J_{r}(\\theta_{k})-r(s_{t},a_{t})\\right)}\\\\ &{\\displaystyle=H\\sum_{k=1}^{K}\\left(J_{r}^{\\pi^{*}}-J_{r}(\\theta_{k})\\right)+\\mathbf{E}\\left[\\sum_{k=1}^{K-1}V_{r}^{\\pi_{\\theta_{k+1}}}(s_{k H})-V_{r}^{\\pi_{\\theta_{k}}}(s_{k H})\\right]+\\mathbf{E}\\left[V_{r}^{\\pi_{\\theta_{K}}}(s_{T})-V_{r}^{\\pi_{\\theta_{0}}}(s_{0})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using the result in (78), Lemma 8 and Lemma 9, we get, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi[\\mathrm{Reg}_{T}]\\leq T\\sqrt{\\epsilon_{\\mathrm{bias}}}+G\\left(1+\\cfrac{1}{\\mu_{F}}\\right)\\tilde{\\mathcal{O}}\\left(T^{\\frac{4}{5}}+\\frac{\\sqrt{A}G t_{\\mathrm{mix}}}{\\delta}T^{\\frac{4}{5}}+\\frac{\\sqrt{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}}{\\delta}T^{\\frac{7}{10}}\\right)+\\mathcal{O}\\left(\\frac{1}{T}+T^{\\frac{3}{5}}\\right)}\\\\ &{+\\cfrac{B}{L}\\tilde{\\mathcal{O}}\\left(\\frac{\\delta^{2}+A G^{2}t_{\\mathrm{mix}}^{2}}{\\delta^{2}}T^{\\frac{3}{5}}+\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta^{2}}T^{\\frac{2}{5}}\\right)+\\tilde{\\mathcal{O}}\\left(\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}\\mathbf{E}_{s\\sim d^{\\ast}}\\left[K L(\\pi^{*}(\\cdot|s)\\|\\pi_{\\theta_{1}}(\\cdot|s))\\right]}{\\delta}T^{\\frac{2}{5}}\\right)}\\\\ &{+\\tilde{\\mathcal{O}}\\left(\\frac{\\alpha A G t_{\\mathrm{mix}}}{\\delta t_{\\mathrm{hit}}}\\left[\\left(\\sqrt{A}G t_{\\mathrm{mix}}+\\delta\\right)T^{\\frac{2}{5}}+\\sqrt{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}T^{\\frac{3}{10}}\\right]\\right)+\\mathcal{O}(t_{\\mathrm{mix}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarly, for the constraint violation, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Im[\\mathrm{Vior}]=\\sum_{t=0}^{T-1}\\left(-c(s_{t},a_{t})\\right)=H\\sum_{k=1}^{K}-J_{c}(\\theta_{k})+\\displaystyle\\sum_{k=1}^{K}\\sum_{t\\in\\cal T_{k}}\\left(J_{c}(\\theta_{k})-c(s_{t},a_{t})\\right)}\\\\ &{\\displaystyle\\qquad=-H\\sum_{k=1}^{K}J_{c}(\\theta_{k})+{\\bf E}\\left[\\sum_{k=1}^{K-1}V_{c}^{\\pi_{\\theta_{k+1}}}(s_{k H})-V_{c}^{\\pi_{\\theta_{k}}}(s_{k H})\\right]+{\\bf E}\\left[V_{c}^{\\pi_{\\theta_{K}}}(s_{T})-V_{c}^{\\pi_{\\theta_{0}}}(s_{0})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using the result in (79), Lemma 8 and Lemma 9, we get, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}[\\mathrm{Vior}]\\le T\\delta\\sqrt{\\epsilon_{\\mathrm{bias}}}+G\\left(1+\\frac{1}{\\mu_{\\mathrm{F}}}\\right)\\tilde{\\mathcal{O}}\\left(\\left[\\delta+\\sqrt{A}G t_{\\mathrm{mix}}\\right]T^{\\frac{4}{5}}+\\sqrt{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}T^{\\frac{7}{10}}\\right)}\\\\ &{\\quad\\quad\\quad+\\mathcal{O}\\left(\\frac{t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta}T^{\\frac{4}{5}}+\\frac{1}{\\delta T}+\\delta T^{\\frac{3}{5}}\\right)+\\frac{B}{L}\\tilde{\\mathcal{O}}\\left(\\frac{\\delta^{2}+A G^{2}t_{\\mathrm{mix}}^{2}}{\\delta}T^{\\frac{3}{5}}+\\frac{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}{\\delta}T^{\\frac{2}{5}}\\right)}\\\\ &{\\quad\\quad\\quad+\\tilde{\\mathcal{O}}\\Bigg(L t_{\\mathrm{mix}}t_{\\mathrm{hit}}\\mathbf{E}_{s\\sim d^{\\ast}}[K L(\\pi^{*}(\\cdot|s)|\\pi_{\\theta_{1}}(\\cdot|s))]T^{\\frac{2}{5}}\\Bigg)}\\\\ &{\\quad\\quad\\quad+\\tilde{\\mathcal{O}}\\left(\\frac{\\alpha A G t_{\\mathrm{mix}}}{\\delta t_{\\mathrm{hit}}}\\left[\\left(\\sqrt{A}G t_{\\mathrm{mix}}+\\delta\\right)T^{\\frac{2}{5}}+\\sqrt{L t_{\\mathrm{mix}}t_{\\mathrm{hit}}}T^{\\frac{3}{10}}\\right]\\right)+\\mathcal{O}(t_{\\mathrm{mix}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This concludes the theorem. ", "page_idx": 25}, {"type": "text", "text": "D   Some Auxiliary Lemmas for the Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma 9. $I I7,$ Lemma 141 For any ergodic MDP with mixing time $t_{\\mathrm{mix}}$ the following holds $\\forall(s,a)\\in S\\times A,$ anypolicy $\\pi$ and $\\forall g\\in\\{r,c\\}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n(a)\\vert V_{g}^{\\pi}(s)\\vert\\le5t_{\\mathrm{mix}},\\enspace(b)\\vert Q_{g}^{\\pi}(s,a)\\vert\\le6t_{\\mathrm{mix}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 10. $I I7,$ Corollary $I3.2J\\,L e t\\,\\delta^{\\pi}(\\cdot,T)$ be defined as written below for an arbitrary policy $\\pi$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\delta^{\\pi}(s,T)\\triangleq\\sum_{t=N}^{\\infty}\\|(P^{\\pi})^{t}(s,\\cdot)-d^{\\pi}\\|_{1},\\ \\forall s\\in\\mathcal{S}\\ w h e r e\\ N=4t_{\\operatorname*{mix}}(\\log_{2}T)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$I f t_{\\mathrm{mix}}<T/4_{\\cdot}$ we have the following inequality $\\forall s\\in S$ $\\begin{array}{r}{\\delta^{\\pi}(s,T)\\le\\frac{1}{T^{3}}}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Lemma 11. [17, Lemma 16] Let ${\\mathcal{Z}}=\\{t_{1}+1,t_{1}+2,\\cdots\\,,t_{2}\\}$ be a certain period of an epoch $k$ of Algorithm 2 with length $N$ . Then for any $s$ the probability that the algorithm never visits s in $\\mathcal{Z}$ is upper bounded by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left(1-{\\frac{3d^{\\pi_{\\theta_{k}}}(s)}{4}}\\right)^{\\lfloor{\\frac{\\lfloor{\\mathcal{T}}\\rfloor}{N}}\\rfloor}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 12. $I I7,$ Lemma 151 The difference of the values of the function $J_{g}$ $g\\in\\{r,c\\}$ atpolicies $\\pi$ and $\\pi^{\\prime}$ ,is ", "page_idx": 26}, {"type": "equation", "text": "$$\nJ_{g}^{\\pi}-J_{g}^{\\pi^{\\prime}}=\\sum_{s}\\sum_{a}d^{\\pi}(s)(\\pi(a|s)-\\pi^{\\prime}(a|s))Q_{g}^{\\pi^{\\prime}}(s,a)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 13. [6, Lemma $7J$ The term $\\hat{J}_{c}(\\theta)$ for any $\\theta\\in\\Theta$ is a good estimator of $J_{c}(\\theta)$ which means ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\mathbf{E}[\\hat{J}_{c}(\\theta)]-J_{c}(\\theta)\\right|\\leq\\frac{1}{T^{2}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 14. [36, Lemma A.6] Let $\\theta\\ \\in\\ \\Theta$ be a policy parameter. Fix a trajectory $z=$ $\\{\\left({{s_{t}},{a_{t}},{r_{t}},{s_{t+1}}}\\right)\\}_{t\\in\\mathbb{N}}$ generated by following the policy $\\pi_{\\theta}$ starting from some initial state $s_{0}\\sim\\rho$ Let, $\\nabla L(\\theta)$ be the gradient that we wish to estimate over $z$ and $l(\\theta,\\cdot)$ is a function such that $\\mathbf{E}_{z\\sim d^{\\pi_{\\theta}},\\pi_{\\theta}}\\bar{l}(\\theta,z)=\\nabla L(\\theta)$ .Assume that $\\lVert l(\\theta,z)\\rVert,\\lVert\\nabla L(\\theta)\\rVert\\leq G_{L},\\,\\forall\\dot{\\theta}$ $\\forall\\theta\\in\\Theta$ $\\forall z\\in S\\times A\\times\\mathbb{R}\\times S$ Define $\\begin{array}{r}{l^{Q}=\\frac{1}{Q}\\sum_{i=1}^{Q}l(\\theta,z_{i})}\\end{array}$ If $P=2t_{\\mathrm{mix}}\\log T$ then the following holds as long as $Q\\leq T$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left\\|l^{Q}-\\nabla L(\\theta)\\right\\|^{2}\\right]\\leq\\mathcal{O}\\left(G_{L}^{2}\\log\\left(P Q\\right)\\frac{P}{Q}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 15 (Strong duality). $I37$ Lemma $3J$ For convenience, we rewrite the unparameterized problem (2). ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{max}_{\\pi\\in\\Pi}\\;J_{r}^{\\pi}}\\\\ {s.t.\\;J_{c}^{\\pi}\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Define $\\pi^{*}$ as the optimal solution to the above problem. Define the associated dual function as ", "page_idx": 26}, {"type": "equation", "text": "$$\nJ_{D}^{\\lambda}\\triangleq\\operatorname*{max}_{\\pi\\in\\Pi}J_{r}^{\\pi}+\\lambda J_{c}^{\\pi}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "anddenote $\\lambda^{*}=\\arg\\operatorname*{min}_{\\lambda\\geq0}J_{D}^{\\lambda}$ . We have the following strong duality property for the unparameterized problem whenever Assumption 2 holds. ", "page_idx": 26}, {"type": "equation", "text": "$$\nJ_{r}^{\\pi^{*}}=J_{D}^{\\lambda^{*}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Although the strong duality holds for the unparameterized problem, the same is not true for parameterizedclass $\\{\\pi_{\\theta}|\\bar{\\theta\\in\\Theta}\\}$ . To formalize this statement, define the dual function associated with the parameterized problem as follows. ", "page_idx": 26}, {"type": "equation", "text": "$$\nJ_{D,\\Theta}^{\\lambda}\\triangleq\\operatorname*{max}_{\\theta\\in\\Theta}J_{r}(\\theta)+\\lambda J_{c}(\\theta)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and denote $\\lambda_{\\Theta}^{*}=\\arg\\operatorname*{min}_{\\lambda\\geq0}J_{D,\\Theta}^{\\lambda}$ . The lack of srong duality states that, in general, $J_{D,\\Theta}^{\\lambda_{\\Theta}^{*}}\\neq J_{r}(\\theta^{*})$ where $\\theta^{*}$ is a solution of the parameterized constrained optimization (3). However, the parameter $\\lambda_{\\Theta}^{*}$ as we demonstrate below, must obey some restrictions. ", "page_idx": 26}, {"type": "text", "text": "Lemma 16. Under Assumption 2, the optimal dual variable for the parameterized problem is bounded as ", "page_idx": 26}, {"type": "equation", "text": "$$\n0\\leq\\lambda_{\\Theta}^{*}\\leq\\frac{J_{r}^{\\pi^{*}}-J_{r}(\\bar{\\theta})}{\\delta}\\leq\\frac{1}{\\delta}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. The proof follows the approach in [37, Lemma 3], but is revised to the general parameterization setup. Let $\\Lambda_{a}\\,\\triangleq\\,\\{\\lambda\\,\\geq\\,0\\,|\\,J_{D,\\Theta}^{\\lambda}\\,\\leq\\,a\\}$ be a sublevel set of the dual function for $a\\in\\mathbb{R}$ . If $\\Lambda_{a}$ is non-empty, then for any $\\lambda\\in\\Lambda_{a}$ \uff0c ", "page_idx": 26}, {"type": "equation", "text": "$$\na\\geq J_{D,\\Theta}^{\\lambda}\\geq J_{r}(\\bar{\\theta})+\\lambda J_{c}(\\bar{\\theta})\\geq J_{r}(\\bar{\\theta})+\\lambda\\delta\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\bar{\\theta}$ is a Slater pointin Assumption 2. Thus, $\\lambda\\,\\leq\\,(a\\,-\\,J_{r}({\\bar{\\theta}}))/\\delta$ .If we take $a\\,=\\,J_{D,\\Theta}^{\\lambda_{\\Theta}^{*}}\\,\\leq$ $J_{D,\\Theta}^{\\lambda^{*}}\\leq J_{D}^{\\lambda^{*}}=J_{r}^{\\pi^{*}}$ , then we have $\\lambda_{\\Theta}^{*}\\in\\Lambda_{a}$ , which proves the Lemma. The last inequality holds since $J_{r}^{\\pi}\\in[0,1]$ for anypolicy, $\\pi$ \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Since the above inequality holds for arbitrary $\\Theta$ , we also have, $0\\ \\leq\\ \\lambda^{*}\\ \\leq\\ \\frac{1}{\\delta}$ .Define $\\boldsymbol{v}(\\tau)\\;\\triangleq$ $\\operatorname*{max}_{\\pi\\in\\Pi}\\{J_{r}^{\\pi}|J_{c}^{\\pi}\\geq\\tau\\}$ . Using the strong duality property of the unparameterized problem (97), we establish the following property of the function, $\\boldsymbol{v}(\\cdot)$ ", "page_idx": 27}, {"type": "text", "text": "Lemma 17. Assume that the Assumption 2 holds, we have for any $\\tau\\in\\mathbb{R}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nv(0)-\\tau\\lambda^{*}\\geq v(\\tau)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. By the definition of $v(\\tau)$ ,wehave $v(0)\\,=\\,J_{r}^{\\pi^{*}}$ . With a slight abuse of notation, denote $J_{\\mathrm{L}}(\\pi,\\lambda)=J_{r}^{\\pi}+\\lambda J_{c}^{\\pi}$ . By the strong duality stated in Lemma 15, we have the following for any $\\pi\\in\\Pi$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nJ_{\\mathrm{L}}(\\pi,\\lambda^{*})\\le\\operatorname*{max}_{\\pi\\in\\Pi}J_{\\mathrm{L}}(\\pi,\\lambda^{*})\\stackrel{D e f}{=}J_{D}^{\\lambda^{*}}\\stackrel{(99)}{=}J_{r}^{\\pi^{*}}=v(0)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, for any $\\pi\\in\\left\\{\\pi\\in\\Pi\\,|\\,J_{c}^{\\pi}\\geq\\tau\\right\\}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v(0)-\\tau\\lambda^{*}\\geq J_{\\mathrm{L}}(\\pi,\\lambda^{*})-\\tau\\lambda^{*}}\\\\ &{\\qquad\\qquad\\qquad=J_{r}^{\\pi}+\\lambda^{*}(J_{c}^{\\pi}-\\tau)\\geq J_{r}^{\\pi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Maximizing the right-hand side of this inequality over ${\\{\\pi\\in\\Pi|{\\cal J}_{c}^{\\pi}\\geq\\tau\\}}$ yields ", "page_idx": 27}, {"type": "equation", "text": "$$\nv(0)-\\tau\\lambda^{*}\\geq v(\\tau)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This completes the proof of the lemma. ", "page_idx": 27}, {"type": "text", "text": "We note that a similar result was shown in [38, Lemma 15]. However, the setup of the stated paper is different from that of ours. Specifically, [38] considers a tabular setup with peak constraints. Note that Lemma 17 has no direct connection with the parameterized setup since its proof uses strong duality and the function, $\\boldsymbol{v}(\\cdot)$ , is defined via a constrained optimization over the entire policy set, $\\Pi$ , rather than the parameterized policy set. Interestingly, however, the relationship between $v(\\tau)$ and $v(0)$ leads to the lemma stated below which turns out to be pivotal in establishing regret and constraint violation bounds in the parameterized setup. ", "page_idx": 27}, {"type": "text", "text": "Lemma 18. Let Assumption 2 hold. For any constant $C\\geq2\\lambda^{*}$ if there exists a $\\pi\\in\\Pi$ and $\\zeta>0$ suchthat $J_{r}^{\\pi^{*}}-J_{r}^{\\pi}+C[-J_{c}^{\\pi}]\\leq\\zeta$ then ", "page_idx": 27}, {"type": "equation", "text": "$$\n-J_{c}^{\\pi}\\leq2\\zeta/C\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Let $\\tau=J_{c}^{\\pi}$ . Using the definition of $v(\\tau)$ , one can write, ", "page_idx": 27}, {"type": "equation", "text": "$$\nJ_{r}^{\\pi}\\leq v(\\tau)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining Eq. (106) and (108), we obtain the following. ", "page_idx": 27}, {"type": "equation", "text": "$$\nJ_{r}^{\\pi}-J_{r}^{\\pi^{*}}\\le v(\\tau)-v(0)\\le-\\tau\\lambda^{*}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The condition in the Lemma leads to, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(C-\\lambda^{*})(-\\tau)=\\tau\\lambda^{*}+C(-\\tau)\\leq J_{r}^{\\pi^{*}}-J_{r}^{\\pi}+C[-J_{c}^{\\pi}]\\leq\\zeta}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, we have, ", "page_idx": 27}, {"type": "equation", "text": "$$\n-\\tau\\leq\\frac{\\zeta}{C-\\lambda^{*}}\\leq\\frac{2\\zeta}{C}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which completes the proof. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The contribution and challenges are clearly described at the end of the introduction. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We add all the assumptions in the work, list the gap with lower bound in Table 1, and give future work direction. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All the assumptions are clearly stated with a remark to discuss. All the proof are given in the appendix in details. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: the paper does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: the paper does not include experiments. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: the paper does not include experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: the paper does not include experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: the paper does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: the research conducted in the paper satisfies the NeurIPs Code of Ethics Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: there is no societal impact of the work performed. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: the paper poses no such risks ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: the paper does not use existing assets ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 32}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: the paper does not use release new assets ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]