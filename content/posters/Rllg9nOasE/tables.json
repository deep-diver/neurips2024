[{"figure_path": "Rllg9nOasE/tables/tables_20_1.jpg", "caption": "Table 1: Pretrained Transformer-based Encoder Models. All models have 12 layers.", "description": "This table lists the names of pretrained transformer-based encoder models used in the paper, along with the modality (video or speech) they were pretrained on.  It shows that some models were trained using both video and audio, while others were trained using only one modality.", "section": "4.2 Unimodal Models"}]