[{"figure_path": "Rllg9nOasE/figures/figures_1_1.jpg", "caption": "Figure 1: (A) Overview of our proposed Multi-modal Brain Encoding Pipeline. (B) Residual Analysis.", "description": "This figure illustrates the methodology used in the paper. Panel A shows the Multi-modal Brain Encoding Pipeline. A multi-modal naturalistic stimulus (movie video with audio) is processed by video and audio encoders. The encoders' output is fed to different types of models: unimodal video model, unimodal speech model, cross-modality model, and jointly pretrained model. The results from each model are then used with ridge regression to predict actual brain activations from fMRI. Panel B shows the residual analysis, where the residuals from the cross-modality model and unimodal video model are used with ridge regression to predict actual brain activations.", "section": "Methodology"}, {"figure_path": "Rllg9nOasE/figures/figures_6_1.jpg", "caption": "Figure 2: Average normalized brain alignment for both multi-modal and individual modality features across whole brain, language, and visual regions. Error bars indicate the standard error of the mean across participants. * indicates cases where multi-modal embeddings are significantly better than unimodal video models (VM), i.e., p\u2264 0.05. A, indicates cases where multi-modal embeddings are significantly better than unimodal speech models (SM), i.e., p\u2264 0.05.", "description": "This figure compares the brain alignment performance (measured by Pearson correlation) of different model types: multi-modal (cross-modal and jointly pretrained), and unimodal (video and speech) models.  The results are shown for the whole brain and separately for language and visual brain regions.  Error bars represent the standard error of the mean across participants. Statistical significance is indicated by asterisks (*) for comparisons against unimodal video models and by the symbol (A) for comparisons against unimodal speech models, using p-values \u2264 0.05.", "section": "6 Results"}, {"figure_path": "Rllg9nOasE/figures/figures_7_1.jpg", "caption": "Figure 2: Average normalized brain alignment for both multi-modal and individual modality features across whole brain, language, and visual regions. Error bars indicate the standard error of the mean across participants. * indicates cases where multi-modal embeddings are significantly better than unimodal video models (VM), i.e., p\u2264 0.05. A, indicates cases where multi-modal embeddings are significantly better than unimodal speech models (SM), i.e., p\u2264 0.05.", "description": "This figure displays the average normalized brain alignment scores for multi-modal and unimodal models across three brain regions: whole brain, language regions, and visual regions.  The results demonstrate that multi-modal models generally outperform unimodal models in terms of brain alignment, particularly when compared to unimodal speech models.  Statistical significance is indicated with asterisks, highlighting instances where multi-modal models significantly surpass unimodal video or speech models in predictive accuracy. Error bars show the standard error of the mean across participants.", "section": "6 Results"}, {"figure_path": "Rllg9nOasE/figures/figures_7_2.jpg", "caption": "Figure 9: Residual analysis for ATL, PTL, IFG, MFG, IFGOrb, PCC and dmPFC regions: Average normalized brain alignment was computed across participants before and after removal of video and audio embeddings from both jointly pretrained and cross-modality models. Error bars indicate the standard error of the mean across participants.", "description": "This figure displays the results of a residual analysis performed to determine the contribution of video and audio modalities to brain alignment in multi-modal models.  The analysis compared brain alignment before and after removing either video or audio features from the model representations.  The results are shown for several language regions (ATL, PTL, IFG, MFG, IFGOrb, PCC, and dmPFC). The error bars represent the standard error across participants.", "section": "6.3 How is the brain alignment of multi-modal features affected by the elimination of a particular modality?"}, {"figure_path": "Rllg9nOasE/figures/figures_8_1.jpg", "caption": "Figure 5: Percentage decrease of brain alignment after removal of (left) Unimodal VM embeddings from IB-Concat (middle) Unimodal VM embeddings from jointly pretrained TVLT, and (right) Unimodal SM embeddings from TVLT Joint. Colorbar indicates the percentage of decrease where red denotes higher and white denotes zero.", "description": "This figure shows a qualitative analysis of the impact of removing unimodal features from multi-modal models on brain alignment. Three sub-figures are presented, each depicting the percentage decrease in brain alignment after removing either video features (Unimodal VM) from IB Concat and TVLT Joint models, or audio features (Unimodal SM) from the TVLT Joint model. Redder colors indicate a greater percentage decrease in alignment, while white indicates no change.  This visualization helps in understanding the contribution of each modality to the overall brain alignment in multi-modal models.", "section": "6.3 How is the brain alignment of multi-modal features affected by the elimination of a particular modality?"}, {"figure_path": "Rllg9nOasE/figures/figures_19_1.jpg", "caption": "Figure 6: Cross-subject prediction accuracy: (top) across whole brain, (bottom) across language, visual and auditory regions.", "description": "This figure displays the cross-subject prediction accuracy of a model in three different training settings: using only \"The Bourne Supremacy\" movie, using only \"The Wolf of Wall Street\" movie, and using both movies for training.  The top panel shows the average Pearson correlation across all brain voxels for these three settings. The bottom panel breaks this down further, showing the average Pearson correlation for several language, visual, and auditory regions of interest (ROIs) in the brain.  Error bars in both panels represent the standard error of the mean across participants. This data helps determine the generalizability and robustness of the brain encoding model across different datasets and brain regions.", "section": "Estimating dataset cross-subject prediction accuracy"}, {"figure_path": "Rllg9nOasE/figures/figures_20_1.jpg", "caption": "Figure 2: Average normalized brain alignment for both multi-modal and individual modality features across whole brain, language, and visual regions. Error bars indicate the standard error of the mean across participants. * indicates cases where multi-modal embeddings are significantly better than unimodal video models (VM), i.e., p\u2264 0.05. A, indicates cases where multi-modal embeddings are significantly better than unimodal speech models (SM), i.e., p\u2264 0.05.", "description": "This figure compares the average normalized brain alignment scores achieved by different encoding models. The models include multi-modal models (ImageBind and TVLT), unimodal video models (ViT-B, VideoMAE, ViViT), and unimodal speech models (Wav2Vec2.0, AST). The comparison is performed across the whole brain, language regions, and visual regions. The error bars represent the standard error of the mean across participants. Statistically significant differences (p\u2264 0.05) between multi-modal and unimodal models are indicated by asterisks (* for video, ^ for speech).", "section": "Results"}, {"figure_path": "Rllg9nOasE/figures/figures_22_1.jpg", "caption": "Figure 2: Average normalized brain alignment for both multi-modal and individual modality features across whole brain, language, and visual regions. Error bars indicate the standard error of the mean across participants. * indicates cases where multi-modal embeddings are significantly better than unimodal video models (VM), i.e., p\u2264 0.05. A, indicates cases where multi-modal embeddings are significantly better than unimodal speech models (SM), i.e., p\u2264 0.05.", "description": "This figure displays a comparison of the average normalized brain alignment scores for multi-modal and unimodal models across different brain regions. The normalized brain alignment score reflects how well the model's predictions correlate with actual brain activity recordings. Higher scores indicate better alignment. The figure shows that multi-modal models generally outperform unimodal models in predicting brain activity. The error bars represent the standard error of the mean across participants, and asterisks denote statistically significant differences compared to unimodal video (VM) and speech (SM) models.", "section": "6 Results"}, {"figure_path": "Rllg9nOasE/figures/figures_23_1.jpg", "caption": "Figure 2: Average normalized brain alignment for both multi-modal and individual modality features across whole brain, language, and visual regions. Error bars indicate the standard error of the mean across participants. * indicates cases where multi-modal embeddings are significantly better than unimodal video models (VM), i.e., p\u2264 0.05. A, indicates cases where multi-modal embeddings are significantly better than unimodal speech models (SM), i.e., p\u2264 0.05.", "description": "This figure compares the brain alignment performance of multi-modal models (cross-modal and jointly pretrained) against unimodal models (video and speech). It shows the average normalized brain alignment across the whole brain, language regions, and visual regions. Statistical significance is indicated by asterisks, showing cases where multi-modal embeddings outperform unimodal models.", "section": "6 Results"}, {"figure_path": "Rllg9nOasE/figures/figures_24_1.jpg", "caption": "Figure 9: Residual analysis for ATL, PTL, IFG, MFG, IFGOrb, PCC and dmPFC regions: Average normalized brain alignment was computed across participants before and after removal of video and audio embeddings from both jointly pretrained and cross-modality models. Error bars indicate the standard error of the mean across participants.", "description": "This figure shows the results of a residual analysis, where video and audio features were removed from both cross-modal and jointly pretrained models, separately and combined. The resulting brain alignment is compared to the original multi-modal models and unimodal models for various brain regions associated with language processing.  The analysis aims to determine the contribution of each modality to the overall brain alignment.", "section": "6.3 How is the brain alignment of multi-modal features affected by the elimination of a particular modality?"}, {"figure_path": "Rllg9nOasE/figures/figures_25_1.jpg", "caption": "Figure 11: Normalized brain alignment across layers for multi-modal model (TVLT joint embeddings) and unimodal video and speech models.", "description": "This figure displays the average normalized brain alignment across different layers of three types of models: unimodal video models, unimodal speech models, and the TVLT joint embeddings (a multi-modal model).  The x-axis represents the layer depth, ranging from 1 to 12.  The y-axis shows the average normalized brain alignment for each layer.  The purpose is to compare the layer-wise performance of the different model types to understand how the brain alignment changes as the models process information through their layers.", "section": "6 Results"}]