[{"type": "text", "text": "Multi-modal brain encoding models for multi-modal stimuli ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Despite participants engaging in single modality stimuli, such as watching images   \n2 or silent videos, recent work has demonstrated that multi-modal Transformer   \n3 models can predict visual brain activity impressively well, even with incongruent   \n4 modality representations. This raises the question of how accurately these multi  \n5 modal models can predict brain activity when participants are engaged in multi  \n6 modal stimuli. As these models grow increasingly popular, their use in studying   \n7 neural activity provides insights into how our brains respond to such multi-modal   \n8 naturalistic stimuli, i.e., where it separates and integrates information from different   \n9 sensory modalities. We investigate this question by using multiple unimodal   \n10 and two types of multi-modal models\u2014cross-modal and jointly pretrained\u2014to   \n11 determine which type of models is more relevant to fMRI brain activity when   \n12 participants were engaged in watching movies (videos with audio). We observe that   \n13 both types of multi-modal models show improved alignment in several language   \n14 and visual regions. This study also helps in identifying which brain regions   \n15 process unimodal versus multi-modal information. We further investigate the   \n16 impact of removal of unimodal features from multi-modal representations and   \n17 find that there is additional information beyond the unimodal embeddings that   \n18 is processed in the visual and language regions. Based on this investigation, we   \n19 find that while for cross-modal models, their brain alignment is partially attributed   \n20 to the video modality; for jointly pretrained models, it is partially attributed to   \n21 both the video and audio modalities. The inability of individual modalities in   \n22 explaining the brain alignment effectiveness of multi-modal models suggests that   \n23 multi-modal models capture additional information processed by all brain regions.   \n24 This serves as a strong motivation for the neuro-science community to investigate   \n25 the interpretability of these models for deepening our understanding of multi-modal   \n26 information processing in brain. ", "page_idx": 0}, {"type": "text", "text": "27 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "28 The study of brain encoding aims at predicting the neural brain activity recordings from an input   \n29 stimulus representation. Recent brain encoding studies use neural models as a powerful approach to   \n30 better understand the information processing in the brain in response to naturalistic stimuli (Oota   \n31 et al., 2023a). Current encoding models are trained and tested on brain responses captured from   \n32 participants who are engaged in a single stimulus modality, using stimulus representations extracted   \n33 from AI systems that are pretrained on single modality, such as language (Wehbe et al., 2014; Jain &   \n34 Huth, 2018; Toneva & Wehbe, 2019; Caucheteux & King, 2020; Schrimpf et al., 2021; Toneva et al.,   \n35 2022; Aw & Toneva, 2023), vision (Yamins et al., 2014; Eickenberg et al., 2017; Schrimpf et al.,   \n36 2018; Wang et al., 2019) or speech (Millet et al., 2022; Vaidya et al., 2022; Tuckute et al., 2023). In   \n37 this paper, we build encoding models where participants are engaged with multi-modal stimuli (e.g.,   \n38 watching movies that also include audio). We explore multi-modal stimulus representations extracted   \n39 using Transformer (Vaswani et al., 2017) based multi-modal models. Our analysis focuses on their   \n40 alignment with both uni- and multi-modal brain regions.   \n41 There is a growing evidence that the human brain\u2019s ability for multi-modal processing is underpinned   \n42 by synchronized cortical representations of identical concepts across various sensory modalities (Gau  \n43 thier et al., 2003; Bracci & Op de Beeck, 2023). Reflecting similar principles, the recent advances in   \n44 AI systems have led to the development of multi-modal models (like CLIP (Radford et al., 2021),   \n45 ImageBind (Girdhar et al., 2023), and TVLT (Tang et al., 2022)) using massive interleaved image-text   \n46 data, speech-text data or video-audio-text data to represent multi-modal input. This recent progress in   \n47 AI has stimulated advancements in brain encoding models (Doerig et al., 2022; Oota et al., 2022;   \n48 Popham et al., 2021; Wang et al., 2022; Tang et al., 2024; Nakagi et al., 2024) that learn effectively   \n49 from multiple input modalities, despite participants being engaged with single stimulus modality   \n50 during experiments, e.g., watching natural scene images, or silent movie clips. However, these studies   \n51 have experimented with subjects engaged with single-modality stimulus, leaving the full potential of   \n52 these models in true multi-modal scenarios still unclear.   \n53 Using brain recordings of participants watching several popular movies included with audio (St  \n54 Laurent et al., 2023), we investigate several research questions. First, we investigate the effectiveness   \n55 of multi-modal stimulus representations obtained using multi-modal models versus unimodal models   \n56 for brain encoding. Multi-modal models are of two broad types: (i) cross-modal pretrained models,   \n57 where first individual modality encoders are trained and then cross-modal alignment is performed, and   \n58 (ii) jointly pretrained models, which involve combining data from multiple modalities and training a   \n59 single joint encoder. Hence, we also investigate which of the two types (cross-modal versus joint) are   \n60 better for encoding. In this work, we focus on one cross-modal (ImageBind), one jointly pretrained   \n61 (TVLT), three video and two speech models. Additionally, we explore which modality representations   \n62 are more brain relevant, and identify which brain regions process uni- and multi-modal information.   \n63 Overall, this research utilizes various modality representations to develop encoding models based on   \n64 fMRI responses within a multi-modal model framework (see Fig. 1 for workflow).   \n65 Using our multi-modal brain encoding approach, we examine several insights. First, we use previous   \n66 neuroscience findings that have identified brain regions involved in visual, language and auditory   \n67 processing, and investigate how well our model aligns with these regions when both the model and a   \n68 human participant watch the same multi-modal video stimuli. Second, we expect that multi-modal   \n69 models which can learn cross-modal and joint embeddings across modalities in a brain-relevant   \n70 way would significantly align with these regions. However, alignment with these brain regions   \n71 doesn\u2019t necessarily mean that the model is effectively learning from multiple modalities, as unimodal   \n72 models for vision or language or audio have also been shown to significantly align with these brain   \n73 regions (Wehbe et al., 2014; Toneva et al., 2022; Schrimpf et al., 2021; Millet et al., 2022; Vaidya   \n74 et al., 2022). To check the second aspect, we investigate this question via a direct approach, closely   \n75 related to previous studies (Toneva et al., 2022; Oota et al., 2023b,c). For each modality, we analyze   \n76 how the alignment between brain recordings and multi-modal model representations is affected by   \n77 the elimination of information related to that particular modality from the model representation.   \n78 Our analysis of multi-modal brain alignment leads to several key conclusions: (1) Both cross-modal   \n79 and jointly pretrained models demonstrate significantly improved brain alignment with language   \n80 regions (AG, PCC, PTL, and IFG) and visual regions (EVC and MT) when analyzed against unimodal   \n81 video data. In contrast, compared to unimodal speech-based models, all multi-modal embeddings   \n82 show significantly better brain alignment, except in the OV (object visual processing) region. This   \n83 highlights the ability of multi-modal models to capture additional information\u2014either through   \n84 knowledge transfer or integration between modalities\u2014which is crucial for multi-modal brain   \n85 alignment. (2) Using our residual approach, we find that the improved brain alignment in cross  \n86 modal models can be partially attributed to the removal of video features alone, rather than auditory   \n87 features. On the other hand, the improved brain alignment in jointly pretrained models can be partially   \n88 attributed to the removal of both video and auditory features.   \n89 Overall, we make the following contributions in this paper. (1) To the best of our knowledge, this   \n90 study is the first to leverage both cross-modal and jointly pretrained multi-modal models to perform   \n91 brain alignment while subjects are engaged with multi-modal naturalistic stimuli. (2) We evaluate the   \n92 performance of several unimodal Transformer models (three video and two audio) and measure their   \n93 brain alignment. (3) Additionally, we remove unimodal features from multi-modal representations   \n94 to explore the impact on brain alignment before and after their removal. We will release code upon   \n95 publication of this paper. ", "page_idx": 0}, {"type": "image", "img_path": "Rllg9nOasE/tmp/83e01185eda1544b7da29e0e6f35839c687405903a190332f96a1445a7363b92.jpg", "img_caption": ["Figure 1: (A) Overview of our proposed Multi-modal Brain Encoding Pipeline. (B) Residual Analysis. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "96 2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "97 Multi-modal models. Pretrained Transformer-based models have been found to be very effective in   \n98 various tasks related to language (Devlin et al., 2019; Radford et al., 2019), speech (Baevski et al.,   \n99 2020), and images (Dosovitskiy et al., 2020). To learn associations between pairs of modalities,   \n100 Transformer models have been pretrained on multiple modalities, showing excellent results in multi  \n101 modal tasks like visual question answering and visual common-sense reasoning. These multi-modal   \n102 models are pretrained in two different ways: (i) cross-modal models that integrate information   \n103 from multiple modalities and learn a joint encoder, such as VisualBERT (Li et al., 2019) and   \n104 ImageBind (Girdhar et al., 2023), and (ii) jointly pretrained models like LXMERT (Tan & Bansal,   \n105 2019), CLIP (Radford et al., 2021), ViLBERT (Lu et al., 2019), and TVLT (Tang et al., 2022) which   \n106 fuse individual modality encoders at different stages, transferring knowledge from one modality   \n107 to another. In this work, we investigate how the representations extracted from cross-modal and   \n108 jointly-pretrained Transformer models align with human brain recordings when participants engage   \n109 with multi-modal stimuli.   \n110 Brain Encoding using Multi-modal Models. Since human brain perceives the environment using   \n111 information from multiple modalities (Gauthier et al., 2003), examining the alignment between   \n112 language and visual representations in the brain by training encoding models on fMRI responses,   \n113 while extracting joint representations from multi-modal models, can offer insights into the relation  \n114 ship between the two modalities. For instance, it has been shown that multi-modal models like   \n115 CLIP (Radford et al., 2021) better predict neural responses in the high-level visual cortex as compared   \n116 to previous vision-only models (Doerig et al., 2022; Wang et al., 2022). Additionally, Tang et al.   \n117 (2024) demonstrate the use of multi-modal models in a cross-modal experiment to assess how well   \n118 the language encoding models can predict movie-fMRI responses and how well the vision encoding   \n119 models can predict narrative story-fMRI. Nakagi et al. (2024) analyzed fMRI related to video content   \n120 viewing and found distinct brain regions associated with different semantic levels, highlighting the   \n121 significance of modeling various levels of semantic content simultaneously. However, these studies   \n122 have experimented with subjects engaged with single-modality stimulus, leaving the full potential of   \n123 these models in true multi-modal scenarios still unclear. Recently, Dong & Toneva (2023) interpreted   \n124 the effectiveness of pretrained versus finetuned multi-modal video transformer using video+text   \n125 stimuli-based brain activity. However, they did not perform any cross-modal vs jointly-pretrained   \n126 model analysis or analysis of multi-modal versus unimodal models, leaving it unclear which type   \n127 of multi-modal models perform best for brain activity prediction. Further, unlike them, we study   \n128 video+audio stimuli, and perform comprehensive residual analysis. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "129 3 Dataset Curation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "130 Brain Imaging Dataset. We experiment with a multi-modal naturalistic fMRI dataset, Movie10 (St  \n131 Laurent et al., 2023) obtained from the Courtois NeuroMod databank. This dataset was collected   \n132 while six human subjects passively watched four different movies: The Bourne supremacy $(\\sim\\!I O O$   \n133 mins), The wolf of wall street $\\sim\\!170$ mins), Hidden figures ( $\\sim\\!120$ mins) and Life $r{\\sim}50$ mins). Among   \n134 these, Hidden figures and Life are repeated twice, with the repeats used for testing and the remaining   \n135 movies for training. In this work, we use Life movie for testing where we average the two repetitions   \n136 to reduce noise in brain data. This dataset is one of the largest publicly available multi-modal fMRI   \n137 dataset in terms of number of samples per participant. It includes 4024 TRs (Time Repetitions) for   \n138 The Bourne supremacy, 6898 TRs for The wolf of wall street used in train and 2028 TRs for Life in   \n139 test. The fMRI data is collected every 1.49 seconds $\\left(=1\\mathrm{TR}\\right)$ ).   \n140 The dataset is already preprocessed and projected onto the surface space (\u201cfsaverage6\u201d). We use the   \n141 multi-modal parcellation of the human cerebral cortex based on the Glasser Atlas (which consists   \n142 of 180 regions of interest in each hemisphere) to report the ROI (region of interest) analysis for the   \n143 brain maps (Glasser et al., 2016). This includes four visual processing regions (early visual (EV),   \n144 object-related areas (LO), face-related areas (OFA) and scene-related areas (PPA)), one early auditory   \n145 area (AC), and eight language-relevant regions, encompassing broader language regions: angular   \n146 gyrus (AG), anterior temporal lobe (ATL), posterior temporal lobe (PTL), inferior frontal gyrus (IFG),   \n147 inferior frontal gyrus orbital (IFGOrb), middle frontal gyrus (MFG), posterior cingulate cortex (PCC)   \n148 and dorsal medium prefrontal cortex (dmPFC), based on the Fedorenko lab\u2019s language parcels (Milton   \n149 et al., 2021; Desai et al., 2022). We list the detailed sub-ROIs of these ROIs in Appendix B.   \n150 Estimating dataset cross-subject prediction accuracy. To account for the intrinsic noise in   \n151 biological measurements, we adapt Schrimpf et al. (2021)\u2019s method to estimate the cross-subject   \n152 prediction accuracy for a model\u2019s performance for the Movie10 fMRI datasets. By subsampling   \n153 fMRI datasets from 6 participants, we generate all possible combinations of $s$ participants $S\\in[2,6]\\rangle$ )   \n154 for watching movies, and use a voxel-wise encoding model (see Sec. 5) to predict one participant\u2019s   \n155 response from others. Note that the estimated cross-subject prediction accuracy is based on the   \n156 assumption of a perfect model, which might differ from real-world scenarios, yet offers valuable   \n157 insights into model\u2019s performance. We estimate cross-subject prediction accuracy in three settings:   \n158 (i) training with The Bourne supremacy and testing with Life data, (ii) training with The wolf of wall   \n159 street and testing with Life data, and (iii) training with both The Bourne supremacy and The wolf   \n160 of wall street and testing with Life data. We present the average cross-subject prediction accuracy   \n161 across voxels for the Movie10 fMRI dataset and across the three settings in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "162 4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "163 4.1 Multi-modal models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "164 To analyse how human brain process information while engaged in multi-modal stimuli, we use recent   \n165 popular deep learning models to explore multiple modalities information and build the encoding   \n166 models in two different ways: \u201ccross-modality pretraining\" and \u201cjoint pretraining\".   \n167 Cross-modality Pretrained Multi-modal Models. Cross-modality representations involve transfer  \n168 ring information or learning from one modality to another. For example, in a cross-modal learning   \n169 scenario, text descriptions can be used to improve the accuracy of image/video recognition tasks.   \n170 This approach is often used in scenarios where one modality might have limited data or less direct   \n171 relevance but can be informed by another modality.   \n172 Recently, a cross-modal model called ImageBind (IB) (Girdhar et al., 2023) has shown immense   \n173 promise in binding data from six modalities at once, without the need for explicit supervision.   \n174 ImageBind model uses separate encoders for each individual modality and learns a single shared   \n175 representation space by leveraging multiple types of image-paired data. ImageBind consists of 12   \n176 layers and outputs a 1024 dimensional representation for each modality.   \n177 Jointly Pretrained Multi-modal Models. Jointly pretrained multi-modal model representations,   \n178 on the other hand, involve combining data from multiple modalities to build a more comprehensive   \n179 joint understanding to improve decision-making processes. The system processes these diverse inputs   \n180 concurrently to make more informed and robust decisions.   \n181 TVLT (Zellers et al., 2022) is an end-to-end Text-less Vision-Language multi-modal Transformer   \n182 model for learning joint representations of video and speech from YouTube videos. This joint encoder   \n183 model consists of a 12-layer encoder (hidden size 768) and uses masked autoencoding objective for   \n184 both videos and speech. Given the video-speech pairs, the TVLT model provides 768 dimensional   \n185 representations for each modality across 12 layers.   \n186 Extraction of multi-modal features. To extract video and audio embedding representations from   \n187 multi-modal models for the brain encoding task, we input video and audio pairs at each TR and   \n188 obtain the aligned embeddings for the two modalities. Here, we first segment the input video and   \n189 audio into clips corresponding to 1.49 seconds, which matches the fMRI image rate. For both the   \n190 models, ImageBind and TVLT, we use the pretrained Transformer weights. ImageBind generates   \n191 an embedding for each modality (IB video and IB audio) in an aligned space. We concatenate these   \n192 embeddings to create what we refer to as IB concat embeddings. On the other hand, TVLT provides   \n193 a joint embedding across all modalities at each layer. Only for the last layer, TVLT provides an   \n194 embedding for each modality. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "195 4.2 Unimodal Models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "196 To investigate the effectiveness of multi-modal representations in comparison to representations for   \n197 individual modalities, we use the following methods to obtain embeddings for individual modalities.   \n198 Video-based models. To extract representations of the video stimulus, we use three popular pretrained   \n199 Transformer video-based models from Huggingface (Wolf et al., 2020): (1) Vision Transformer Base   \n200 (ViT-B) (Dosovitskiy et al., 2020), (2) Video Masked Autoencoders (VideoMAE) (Tong et al., 2022)   \n201 and (3) Video Vision Transformer (ViViT) (Arnab et al., 2021). Details of each model are reported in   \n202 Table 1 in Appendix.   \n203 Speech-based models. Similar to video-based models, we use two popular pretrained Transformer   \n204 speech-based models from Huggingface: (1) Wav2Vec2.0 (Baevski et al., 2020) and (2) AST (Baade   \n205 et al., 2022). Details of each model are reported in Table 1 in Appendix.   \n206 Extraction of video features. ViT-B (Dosovitskiy et al., 2020), the underlying video encoder model   \n207 for ImageBind is used for extracting representations for all frames in each TR for every video. To   \n208 extract embedding at each TR, we average all frame embeddings and obtain the corresponding video   \n209 representation. For VideoMAE and ViViT, we directly obtain the video embeddings for each TR. All   \n210 3 models provide 768 dimensional representations and all of them are 12-layer Transformer encoders.   \n211 Extraction of speech features. To explore whether speech models incorporate linguistic information,   \n212 we extract representations beyond 1.49 secs, i.e., we considered context window of 16 secs with   \n213 stride of 100 msecs and considered the last token as the representative for each context window. The   \n214 pretrained speech-based models output token representations at different layers. Both Wav2Vec2.0   \n215 and AST models provide 768 dimensional representations and all of them are 12-layer Transformer   \n216 encoders. Finally, we align these representations with the fMRI data acquisition rate by downsampling   \n217 the stimulus features with a 3-lobed Lanczos filter, thus producing chunk-embeddings for each TR. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "218 5 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "219 Encoding Model. We train bootstrap ridge regression based voxel-wise encoding models (Deniz   \n220 et al., 2019) to predict the fMRI brain activity associated with the stimulus representations obtained   \n221 from the individual modalities (speech and video) and multi-modal embeddings from cross-modal and   \n222 jointly pretrained multi-modal models. For each subject, we account for the delay in the hemodynamic   \n223 response by modeling hemodynamic response function using a finite response filter (FIR) per voxel   \n224 with 5 temporal delays (TRs) corresponding to ${\\sim}7.5$ seconds (Huth et al., 2022). Formally, at each   \n225 time step $t$ , we encode the stimuli as $X_{t}\\in\\overline{{\\mathbb{R}}}^{D}$ and brain region voxels $Y_{t}\\in\\mathbb{R}^{V}$ , where $D$ denotes   \n226 the dimension of the concatenation of delayed 5 TRs, and $V$ denotes the number of voxels. Overall,   \n227 with $N$ such TRs, we obtain $N$ training examples.   \n228 Train-test Setup. We build encoding models in three settings: (i) We used all data samples from   \n229 10 training sessions of the The Bourne supremacy movie for training and tested generalization on   \n230 samples from the test sessions (5 sessions) of the Life movie. (ii) We used data from 17 training   \n231 sessions of the The wolf of wall street movie for training, with the Life movie used for testing. (iii)   \n232 We combined data from the The Bourne supremacy and The wolf of wall street movies for training,   \n233 and tested on the Life movie.   \n234 Removal of a single modality features from multi-modal representations. To remove features   \n235 for a particular modality $m$ from multi-modal model representations, we rely on a simple method   \n236 proposed previously by Toneva et al. (2022) and Oota et al. (2023b), in which the linear contribution   \n237 of the features to the multi-modal model activations is removed via ridge regression. Specifically, for   \n238 this ridge regression the feature vector corresponding to modality $m$ is considered as input and the   \n239 multi-modal representations are the target. We compute the residuals by subtracting the predicted   \n240 multi-modal feature representations from the actual multi-modal features resulting in the (linear)   \n241 removal of feature vector for modality $m$ from the pretrained multi-modal embeddings. Because   \n242 the brain prediction method is also a linear function, this linear removal limits the contribution of   \n243 features for modality $m$ to the eventual brain alignment. See Fig. 1(B).   \n244 Evaluation Metrics. We evaluate our models using Pearson Correlation (PC) which is a standard   \n245 metric for evaluating brain alignment (Jain & Huth, 2018; Schrimpf et al., 2021; Goldstein et al.,   \n246 2022). Let TR be the number of time repetitions in the test set. Let $Y=\\{Y_{i}\\}_{i=1}^{T R}$ and $\\hat{Y}=\\{\\hat{Y}_{i}\\}_{i=1}^{T R}$   \n247 denote the actual and predicted value vectors for a single voxel. Thus, $Y$ and $\\hat{Y}\\,\\in\\mathbb{R}^{T R}$ . We use   \n248 Pearson Correlation (PC) which is computed as corr $(Y,{\\hat{Y}})$ where corr is the correlation function.   \n249 The final measure of a model\u2019s performance is obtained by calculating Pearson\u2019s correlation between   \n250 the model\u2019s predictions and neural recordings. This correlation is then divided by the estimated   \n251 cross-subject prediction accuracy and averaged across voxels, regions, and participants, resulting in   \n252 a standardized measure of performance referred to as normalized brain alignment. For calculating   \n253 normalized alignment, we select the voxels whose cross-subject prediction accuracy is $\\ge0.05$ .   \n254 Implementation Details for Reproducibility. All experiments were conducted on a machine with   \n255 1 NVIDIA GeForce-GTX GPU with 16GB GPU RAM. We used bootstrap ridge-regression with   \n256 the following parameters: MSE loss function; L2-decay $(\\lambda)$ varied from $10^{\\bar{1}}$ to $\\bar{10}^{3}$ ; the best $\\lambda$ was   \n257 chosen by tuning on validation data that comprised a randomly chosen $10\\%$ subset from the train set   \n258 used only for hyper-parameter tuning.   \n259 Statistical Significance. To determine if normalized predictivity scores significantly higher than   \n260 chance, we run a permutation test using blocks of 10 contiguous fMRI TRs (considering the slowness   \n261 of hemodynamic response) rather than individual TRs. By permuting predictions 5000 times, we   \n262 create an empirical distribution for chance performance, from which we estimate the p-value of   \n263 the actual performance. To estimate the statistical significance of performance differences, such   \n264 as between the model\u2019s predictions and chance or residual predictions and chance, we utilized the   \n265 Wilcoxon signed-rank test (Conover, 1999), applying it to the mean normalized predictivity for the   \n266 participants. In all cases, we denote significant differences $\\mathrm{\\underline{{p}}\\leq0.05})$ with $\\mathbf{a}*\\mathbf{or}\\wedge$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "267 6 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "268 6.1 How effective are multi-modal representations obtained from multi-modal models? ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "269 In Fig. 2, we present the average normalized brain alignment scores for both multi-modal and   \n270 individual modality features. Specifically, we show the normalized brain alignment for cross-modality   \n271 (ImageBind), jointly pretrained multi-modal (TVLT), and the average from individual video and   \n272 speech models. The results are shown for whole brain, and also for average across language and   \n273 visual ROIs. Results for individual ROIs are in Fig. 3.   \n274 Baseline comparison. To compare the brain predictivity of multi-modal and unimodal models against   \n275 baseline performance, we employ randomly generated vector embeddings to predict brain activity as   \n276 baseline. We observe that the brain alignment from a random vector is significantly lower than that   \n277 of both multi-modal and unimodal models across the whole brain and language-visual processing   \n278 regions. This shows that the representations from these multi-modal models are significant enough   \n279 for learning non-trivial alignment with the fMRI recordings of multi-modal stimuli.   \n280 Cross-modal vs. Jointly pretrained multi-modal models vs. Unimodal Models. Fig. 2(left)   \n281 displays results for whole brain analysis, where the IB Concat bar plot corresponds to results for   \n282 representations from a cross-modal model, while TVLT Joint bar plot corresponds to results for   \n283 representations from a jointly pretrained multi-modal model. From Fig. 2(left), we make the following   \n284 observations: (i) At the whole brain level, the Wilcoxon signed-rank test shows that the differences in   \n285 embeddings from the IB Concat and TVLT models are not statistically significant. (ii) The multi  \n286 modal embeddings show improved brain alignment compared to unimodal models. Specifically,   \n287 cross-modal embeddings are significantly better than both unimodal video and speech models, while   \n288 jointly pretrained embeddings are significantly better than speech models. This implies that cross  \n289 modal embeddings contain additional information beyond the two modalities, while embeddings   \n290 from a jointly pretrained model do not provide extra information beyond unimodal visual information   \n291 but do contain additional information beyond unimodal speech.   \n292 We also present average results across language and visual regions in Figs. 2 (middle), and 2(right),   \n293 respectively. The Wilcoxon signed-rank test shows that the differences in embeddings from the IB   \n294 Concat and TVLT models are not statistically significant when averaged across language and visual   \n295 regions. Similar to whole brain performance, in the language regions, cross-modal embeddings   \n296 are significantly better than both unimodal video and speech models, while jointly pretrained em  \n297 beddings are significantly better than unimodal speech models. In contrast, for the visual regions,   \n298 the normalized brain alignment of cross-modal and jointly pretrained embeddings is similar to the   \n299 performance of unimodal video models. This implies that when we average across visual regions,   \n300 there is no additional information beyond unimodal video features. However, when compared to   \n301 unimodal speech features, both multi-modal embeddings show significant improvement.   \n302 Since we didn\u2019t observe any significant difference at the whole brain level and when averaged across   \n303 language and visual regions, between cross-modal and jointly pretrained multi-modal models, we   \n304 attempt to seek if there any any differences when we pay a closer look at the individual ROIs. We   \n305 present results for language and visual regions such as Angular gyrus (AG), the posterior temporal   \n306 lobe (PTL), and the inferior frontal gyrus (IFG) in Fig. 3. Additionally, we cover visual regions   \n307 like early visual cortex (EVC), scene visual areas (SV) and middle temporal gyrus (MT), as well   \n308 as early auditory cortex (AC). In this figure, we also report the average normalized brain alignment   \n309 of each modality obtained from multi-modal models. Unlike the whole brain analysis, we observe   \n310 some differences between cross-modal and jointly pretrained models in several language and visual   \n311 ROIs. Results for other ROIs are in Fig. 7 in Appendix. Our observations are as follows: (i) Cross  \n312 modal IB Concat embeddings are significantly better than TVLT Joint embeddings in semantic   \n313 regions such as AG and PCC, as well as the multi-modal processing region MT. (ii) Conversely,   \n314 TVLT Joint embeddings are significantly better than IB Concat embeddings in dmPFC regions.   \n315 While considering both joint and each modality embeddings from multi-modal models, we make the   \n316 following observations from Fig. 3: (1) Cross-modal IB video embeddings exhibit improved brain   \n317 alignment compared to unimodal video in the AG and MT regions with the exceptions of the PTL   \n318 and AC regions. But this is not the case for IB audio vs unimodal audio. This suggests that video   \n319 modality information is more relevant and beneficial in the brain for IB Concat embeddings from   \n320 cross-modality models. (2) TVLT video embeddings show improved brain alignment in the AG, PTL,   \n321 PCC, dmPFC and EVC regions, with other regions displaying similar normalized brain alignment   \n322 unimodal video embeddings. (3) Consistent with the cross-modality models, in jointly pretrained   \n323 TVLT models, TVLT video embeddings significantly outperform TVLT audio embeddings, except in   \n324 PTL region. These observations indicate that video information is advantageous for both cross-modal   \n325 and jointly pretrained models, whereas audio embeddings mainly benefit the PTL region. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "Rllg9nOasE/tmp/c6a9f113ef4e3f05ce1fb18fa010359aa5c339f02e45fa32abf39342e1d7c3c1.jpg", "img_caption": ["Figure 2: Average normalized brain alignment for both multi-modal and individual modality features across whole brain, language, and visual regions. Error bars indicate the standard error of the mean across participants. $^*$ indicates cases where multi-modal embeddings are significantly better than unimodal video models (VM), i.e., ${\\tt p}\\mathrm{\\le}\\;0.05.\\mathrm{~\\textperthousand~}$ , indicates cases where multi-modal embeddings are significantly better than unimodal speech models (SM), i.e., ${\\tt p}\\mathrm{\\leq}\\;0.05$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "326 6.2 Which brain regions process uni- and multi-modal information? ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "327 From Fig. 3, we observe that multi-modal video embeddings exhibit improved brain alignment not   \n328 only in the whole brain but also in various language, visual and multi-modal regions. For instance,   \n329 the cross-modal IB Concat embeddings demonstrate superior brain alignment compared to unimodal   \n330 video-based models in areas such as the AG, PTL, IFG, and PCC. Moreover, TVLT-joint embeddings   \n331 show notable enhancements in the AG, PTL, IFG, PCC, dmPFC and EVC regions. In contrast,   \n332 compared to unimodal speech-based models, all multi-modal embeddings display significantly better   \n333 brain alignment, except the OV (object visual processing) region. Overall, this observation suggests   \n334 that integrating multiple modalities leads to transferring information from one modality to another,   \n335 resulting in improved brain predictability. Based on these, it can be inferred that these multi-modal   \n336 models can indeed learn multi-modal linkages that are relevant to the brain.   \n337 When subjects engage with multi-modality stimuli, we observe that multi-modal embeddings show   \n338 improvements in semantic regions such as the AG, PCC and dmPFC, and syntactic regions such as   \n339 the PTL and IFG. Overall, we find that multi-modal information is processed in only a few regions.   \n340 Furthermore, several regions, including the SV (scene visual area), EVC (early visual cortex), ATL   \n341 (anterior temporal lobe), IFGOrb, MFG, and dmPFC, exhibit similar brain alignment with both   \n342 unimodal and multi-modal embeddings. ", "page_idx": 6}, {"type": "image", "img_path": "Rllg9nOasE/tmp/adb093c0441ea3a4a57081c9fd7e3c8ef6517c1309cfa5394db9f4aa58c2853f.jpg", "img_caption": ["Figure 3: Average normalized brain alignment for video and audio modalities from multi-modal and individual modality features across whole brain and several ROIs of language (AG, PTL and IFG), visual (EVC, SV and MT) and auditory cortex (AC). Error bars indicate the standard error of the mean across participants. $^*$ indicates cases where multi-modal embeddings are significantly better than unimodal video models (VM), i.e., $\\mathsf{p}{\\leq}\\,0.05$ . $\\wedge$ indicates cases where multi-modal embeddings are significantly better than unimodal speech models (SM), i.e., $\\mathsf{p}\\mathrm{\\leq}\\;0.05.3$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "343 6.3 How is the brain alignment of multi-modal features affected by the elimination of a 344 particular modality? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "345 To understand the contribution of each modality to the multi-modal brain alignment for multi-modal   \n346 naturalistic stimulus, we perform residual analyses by removing the unimodality features from   \n347 multi-modal joint representations as well as multi-modal video or audio representations from joint   \n348 representations and measure the differences in brain alignment before and after removal modality  \n349 specific features. Fig. 4 displays the normalized brain alignment for language (AG) and visual regions   \n350 (MT). We note a decrease in brain alignment for both the AG and MT regions following the removal   \n351 of video embeddings from cross-modality models, whereas the removal of audio embeddings does   \n352 not affect the brain alignment. On the other hand, for jointly pretrained models, removal of both   \n353 video and audio embeddings partially impacts the brain alignment. We observe similar findings for   \n354 language ROIs such as PTL, MFG, ATL, PCC and visual regions EVC, OV and FV, as shown in   \n355 Figs. 9 and 10 in Appendix. These results suggest that there is additional information beyond the   \n356 unimodal embeddings considered in this study that is processed in the visual and language regions.   \n357 Qualitative analysis. We compute the percentage decrease in alignment for each voxel following the   \n358 removal of unimodal video embeddings from the IB Concat (cross-modality) and the TVLT Joint   \n359 (jointly pretrained model), with projections onto the brain surface averaged across participants, as   \n360 depicted in Fig. 5. The colorbar shows the percentage decrease in brain alignment, where red voxels   \n361 indicate a higher percentage decrease and white voxels indicate areas where unimodal video features   \n362 do not contribute any shared information within the multi-modal context. We observe that removal of   \n363 unimodal video features leads to a significant drop $(40{-}50\\%)$ in performance in the visual regions for   \n364 IB Concat, and in language regions (PTL & MFG) for TVLT Joint. ", "page_idx": 7}, {"type": "image", "img_path": "Rllg9nOasE/tmp/de4ef330ea7e59c2f465de129ff4a141087f95717f3d3f9e7f02236191b47741.jpg", "img_caption": ["Figure 4: Residual analysis: Average normalized brain alignment was computed across participants before and after removal of video and audio embeddings from both jointly pretrained and crossmodality models. Error bars indicate the standard error of the mean across participants. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "Rllg9nOasE/tmp/5bb3962ed272323f9094ddd6f2b3a3ab223b0dd7a77bee9cc92d3ff9ac41227e.jpg", "img_caption": ["Figure 5: Percentage decrease of brain alignment after removal of (left) Unimodal VM embeddings from IB-Concat (middle) Unimodal VM embeddings from jointly pretrained TVLT, and (right) Unimodal SM embeddings from TVLT Joint. Colorbar indicates the percentage of decrease where red denotes higher and white denotes zero. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "365 7 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "366 Using multi-modal model representations, including both cross-modal and jointly pretrained types,   \n367 we evaluated how these representations can predict fMRI brain activity when participants are   \n368 engaged in multi-modal naturalistic stimuli. Further, we compared both multi-modal and unimodal   \n369 representations and observed their alignment with both unimodal and multi-modal brain regions.   \n370 This is achieved by removing information related to unimodal stimulus features (audio and video)   \n371 and observing how this perturbation affects the alignment with fMRI brain recordings acquired while   \n372 participants are engaged in watching multi-modal naturalistic movies.   \n373 Our analysis of multi-modal brain alignment yields several important conclusions: (1) The improved   \n374 brain alignment of the multi-modal models over unimodal models, across several language, visual, and   \n375 auditory regions is only partially attributable to the video and audio stimulus features presented to the   \n376 model. A deeper understanding of these models is required to shed light on the underlying information   \n377 processing of both unimodal and multi-modal information. (2) Cross-modal representations have   \n378 significantly improved brain alignment in language regions such as AG, PCC and PTL. This variance   \n379 can be partially attributed to the removal of video features alone, rather than auditory features. (3)   \n380 Video embeddings from multi-modal models exhibit higher brain alignment than audio embeddings,   \n381 except in the PTL and AC regions. This suggests that audio-based models may encode weaker brain  \n382 relevant semantics. (4) Both cross-modal and jointly pretrained models demonstrate significantly   \n383 improved brain alignment with language regions (AG, PCC, PTL and IFG) compared to visual regions   \n384 when analyzed against unimodal video data. In contrast, when compared to unimodal audio-based   \n385 models, all multi-modal embeddings display significantly better brain alignment, with the exception   \n386 of the OV region. This underscores the capability of multi-modal models to capture additional   \n387 information\u2014either through knowledge transfer or integration between modalities\u2014crucial for   \n388 multi-modal brain alignment.   \n389 Limitations. The low alignment scores clearly show that despite the increasing popularity of multi  \n390 modal models in tackling complex tasks such as visual question answering, we are still far from   \n391 developing a model that fully encapsulates the complete information processing steps involved   \n392 in handling multi-modal naturalistic information in the brain. In the future, by fine-tuning these   \n393 multi-modal models on specific tasks such as generating captions for videos, we can better leverage   \n394 their alignment strengths. This approach will allow us to explore task-level brain alignment of three   \n395 modalities\u2014video, audio, and text\u2014more effectively. Further, multi-modal large language models   \n396 (MLLMs) (Zhang et al., 2023; Ataallah et al., 2024; Wu et al., 2023) that align visual features from   \n397 video frames into the LLM embedding space via a trainable linear projection layer, offer promise for   \n398 enhanced multi-modal capabilities. We would further extend this work by comparing the region-wise   \n399 brain alignment performance of these multi-modal LLM models with existing approaches. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "400 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "401 Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u02c7ci\u00b4c, and Cordelia Schmid.   \n402 Vivit: A video vision transformer. In Proceedings of the IEEE/CVF international conference on   \n403 computer vision, pp. 6836\u20136846, 2021.   \n404 Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and   \n405 Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding with   \n406 interleaved visual-textual tokens. arXiv preprint arXiv:2404.03413, 2024.   \n407 Khai Loong Aw and Mariya Toneva. Training language models to summarize narratives improves   \n408 brain alignment. In The Eleventh International Conference on Learning Representations, 2023.   \n409 Alan Baade, Puyuan Peng, and David Harwath. Mae-ast: Masked autoencoding audio spectrogram   \n410 transformer. Interspeech 2022, 2022.   \n411 Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework   \n412 for self-supervised learning of speech representations. Advances in neural information processing   \n413 systems, 2020.   \n414 Cordell M Baker, Joshua D Burks, Robert G Briggs, Andrew K Conner, Chad A Glenn, Kathleen N   \n415 Taylor, Goksel Sali, Tressie M McCoy, James D Battiste, Daniel L O\u2019Donoghue, et al. A connec  \n416 tomic atlas of the human cerebrum\u2014chapter 7: the lateral parietal lobe. Operative Neurosurgery,   \n417 15(suppl_1):S295\u2013S349, 2018.   \n418 Stefania Bracci and Hans P Op de Beeck. Understanding human object vision: a picture is worth a   \n419 thousand representations. Annual review of psychology, 74:113\u2013135, 2023.   \n420 Charlotte Caucheteux and Jean-R\u00e9mi King. Language processing in brains and deep neural networks:   \n421 computational convergence and its limits. BioRxiv, 2020.   \n422 William Jay Conover. Practical nonparametric statistics, volume 350. john wiley & sons, 1999.   \n423 Fatma Deniz, Anwar O Nunez-Elizalde, Alexander G Huth, and Jack L Gallant. The representation   \n424 of semantic information across human cerebral cortex during listening versus reading is invariant   \n425 to stimulus modality. Journal of Neuroscience, 2019.   \n426 Rutvik Desai, Usha Tadimeti, and Nicholas Riccardi. Proper and common names in the semantic   \n427 system, 2022.   \n428 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep   \n429 bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of   \n430 the North American Chapter of the Association for Computational Linguistics: Human Language   \n431 Technologies, Volume 1 (Long and Short Papers), 2019.   \n432 Adrien Doerig, Tim C Kietzmann, Emily Allen, Yihan Wu, Thomas Naselaris, Kendrick Kay,   \n433 and Ian Charest. Semantic scene descriptions as an objective of human vision. arXiv preprint   \n434 arXiv:2209.11737, 2022.   \n435 Dota Tianai Dong and Mariya Toneva. Vision-language integration in multimodal video transformers   \n436 (partially) aligns with the brain. arXiv preprint arXiv:2311.07766, 2023.   \n437 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas   \n438 Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image   \n439 is worth 16x16 words: Transformers for image recognition at scale. In International Conference   \n440 on Learning Representations, 2020.   \n441 Michael Eickenberg, Alexandre Gramfort, Ga\u00ebl Varoquaux, and Bertrand Thirion. Seeing it all:   \n442 Convolutional network layers map the function of the human visual system. NeuroImage, 152:   \n443 184\u2013194, 2017.   \n444 Isabel Gauthier, Thomas W James, Kim M Curby, and Michael J Tarr. The influence of conceptual   \n445 knowledge on visual discrimination. Cognitive Neuropsychology, 20(3-6):507\u2013523, 2003.   \n446 Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand   \n447 Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the   \n448 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15180\u201315190, 2023.   \n449 Matthew F Glasser, Timothy S Coalson, Emma C Robinson, Carl D Hacker, John Harwell, Essa   \n450 Yacoub, Kamil Ugurbil, Jesper Andersson, Christian F Beckmann, Mark Jenkinson, et al. A   \n451 multi-modal parcellation of human cerebral cortex. Nature, 536(7615):171\u2013178, 2016.   \n452 Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A   \n453 Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, et al. Shared computational principles for   \n454 language processing in humans and deep language models. Nature neuroscience, 25(3):369\u2013380,   \n455 2022.   \n456 Alexander G Huth, Shinji Nishimoto, An T Vu, and T Dupre La Tour. Gallant lab natural short clips   \n457 3t fmri data. 50 GiB, 2022.   \n458 Shailee Jain and Alexander G Huth. Incorporating context into language encoding models for fmri.   \n459 In NIPS, pp. 6629\u20136638, 2018.   \n460 Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple   \n461 and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.   \n462 Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: pretraining task-agnostic visiolinguistic   \n463 representations for vision-and-language tasks. In Proceedings of the 33rd International Conference   \n464 on Neural Information Processing Systems, pp. 13\u201323, 2019.   \n465 Juliette Millet, Charlotte Caucheteux, Yves Boubenec, Alexandre Gramfort, Ewan Dunbar, Christophe   \n466 Pallier, Jean-Remi King, et al. Toward a realistic model of speech processing in the brain with   \n467 self-supervised learning. Advances in Neural Information Processing Systems, 35:33428\u201333443,   \n468 2022.   \n469 Camille K Milton, Vukshitha Dhanaraj, Isabella M Young, Hugh M Taylor, Peter J Nicholas, Robert G   \n470 Briggs, Michael Y Bai, Rannulu D Fonseka, Jorge Hormovas, Yueh-Hsin Lin, et al. Parcellation  \n471 based anatomic model of the semantic network. Brain and behavior, 11(4):e02065, 2021.   \n472 Yuko Nakagi, Takuya Matsuyama, Naoko Koide-Majima, Hiroto Yamaguchi, Rieko Kubo, Shinji   \n473 Nishimoto, and Yu Takagi. The brain tells a story: Unveiling distinct representations of semantic   \n474 content in speech, objects, and stories in the human brain with large language models. bioRxiv, pp.   \n475 2024\u201302, 2024.   \n476 Subba Reddy Oota, Jashn Arora, Vijay Rowtula, Manish Gupta, and Raju S Bapi. Visio-linguistic   \n477 brain encoding. In COLING, pp. 116\u2013133, 2022.   \n478 Subba Reddy Oota, Manish Gupta, Raju S Bapi, Gael Jobard, Fr\u00e9d\u00e9ric Alexandre, and Xavier Hinaut.   \n479 Deep neural networks and brain alignment: Brain encoding and decoding (survey). arXiv preprint   \n480 arXiv:2307.10246, 2023a.   \n481 Subba Reddy Oota, Manish Gupta, and Mariya Toneva. Joint processing of linguistic properties in   \n482 brains and language models. NeurIPS, 2023b.   \n483 Subba Reddy Oota, Agarwal Veeral, Marreddy Mounika, Gupta Manish, and Raju Surampudi Bapi.   \n484 Speech taskonomy: Which speech tasks are the most predictive of fmri brain activity? In 24th   \n485 INTERSPEECH Conference, 2023c.   \n486 Sara F Popham, Alexander G Huth, Natalia Y Bilenko, Fatma Deniz, James S Gao, Anwar O Nunez  \n487 Elizalde, and Jack L Gallant. Visual and linguistic semantic representations are aligned at the   \n488 border of human visual cortex. Nature neuroscience, 24(11):1628\u20131636, 2021.   \n489 Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language   \n490 models are unsupervised multitask learners. OpenAI, 2019.   \n491 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,   \n492 Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual   \n493 models from natural language supervision. Image, 2:T2, 2021.   \n494 Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J Majaj, Rishi Rajalingham, Elias B Issa, Kohitij   \n495 Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, et al. Brain-score: Which artificial   \n496 neural network for object recognition is most brain-like? BioRxiv, pp. 407007, 2018.   \n497 Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kan  \n498 wisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural architecture of language:   \n499 Integrative modeling converges on predictive processing. Proceedings of the National Academy of   \n500 Sciences, 2021.   \n501 Marie St-Laurent, Basile Pinsard, Oliver Contier, Katja Seeliger, Valentina Borghesani, Julie Boyle,   \n502 Pierre Bellec, and Martin Hebart. cneuromod-things: a large-scale fmri dataset for task-and   \n503 data-driven assessment of object representation and visual memory recognition in the human brain.   \n504 Journal of Vision, 23(9):5424\u20135424, 2023.   \n505 Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transform  \n506 ers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing   \n507 and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),   \n508 pp. 5100\u20135111, 2019.   \n509 Jerry Tang, Meng Du, Vy Vo, Vasudev Lal, and Alexander Huth. Brain encoding models based on   \n510 multimodal transformers can transfer across language and vision. Advances in Neural Information   \n511 Processing Systems, 36, 2024.   \n512 Zineng Tang, Jaemin Cho, Yixin Nie, and Mohit Bansal. Tvlt: Textless vision-language transformer.   \n513 Advances in Neural Information Processing Systems, 35:9617\u20139632, 2022.   \n514 Mariya Toneva and Leila Wehbe. Interpreting and improving natural-language processing (in   \n515 machines) with natural language-processing (in the brain). Advances in Neural Information   \n516 Processing Systems, 32, 2019.   \n517 Mariya Toneva, Tom M Mitchell, and Leila Wehbe. Combining computational controls with natural   \n518 text reveals aspects of meaning composition. Nature Computational Science, 2(11):745\u2013757, 2022.   \n519 Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data  \n520 efficient learners for self-supervised video pre-training. Advances in neural information processing   \n521 systems, 35:10078\u201310093, 2022.   \n522 Greta Tuckute, Jenelle Feather, Dana Boebinger, and Josh H McDermott. Many but not all deep   \n523 neural network audio models capture brain responses and exhibit correspondence between model   \n524 stages and brain regions. Plos Biology, 21(12):e3002366, 2023.   \n525 Aditya Vaidya, Shailee Jain, and Alexander Huth. Self-supervised models of audio effectively   \n526 explain human cortical responses to speech. In International Conference on Machine Learning, pp.   \n527 21927\u201321944. PMLR, 2022.   \n528 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz   \n529 Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing   \n530 systems, 30, 2017.   \n531 Aria Wang, Michael Tarr, and Leila Wehbe. Neural taskonomy: Inferring the similarity of task-derived   \n532 representations from brain activity. NeurIPS, 32:15501\u201315511, 2019.   \n533 Aria Y Wang, Kendrick Kay, Thomas Naselaris, Michael J Tarr, and Leila Wehbe. Natural language   \n534 supervision with a large and diverse dataset builds better models of human high-level visual cortex.   \n535 BioRxiv, pp. 2022\u201309, 2022.   \n536 Leila Wehbe, Brian Murphy, Partha Talukdar, Alona Fyshe, Aaditya Ramdas, and Tom Mitchell.   \n537 Simultaneously uncovering the patterns of brain regions involved in different story reading subpro  \n538 cesses. PloS one, 11, 2014.   \n539 Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,   \n540 Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art   \n541 natural language processing. In Proceedings of the 2020 conference on empirical methods in   \n542 natural language processing: system demonstrations, pp. 38\u201345, 2020.   \n543 Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal   \n544 llm. arXiv preprint arXiv:2309.05519, 2023.   \n545 Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J   \n546 DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual   \n547 cortex. PNAS, 111(23):8619\u20138624, 2014.   \n548 Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya   \n549 Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge   \n550 through vision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer   \n551 Vision and Pattern Recognition, pp. 16375\u201316387, 2022.   \n552 Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language   \n553 model for video understanding. arXiv preprint arXiv:2306.02858, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "554 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: We have ensured that the main claims made in the abstract and introduction ", "page_idx": 13}, {"type": "text", "text": "are directly correlating to the research findings and the methods we have employed.   \nGuidelines: \u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "571 2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The paper discusses the main limitations of the work performed by the authors in the discussion section. ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "606 Answer: [NA]   \n607 Justification: Our paper does not require any explicit theorems and proofs.   \n608 Guidelines:   \n609 \u2022 The answer NA means that the paper does not include theoretical results.   \n10 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n611 referenced.   \n12 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n13 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n14 they appear in the supplemental material, the authors are encouraged to provide a short   \n15 proof sketch to provide intuition.   \n16 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n17 by formal proofs provided in appendix or supplemental material.   \n18 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "619 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "620 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n621 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n622 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The paper has delineated all the information related to the experimental setup in the experimental setup section. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "658 5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "659 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n660 tions to faithfully reproduce the main experimental results, as described in supplemental   \n661 material?   \n662 Answer: [NA]   \n663 Justification: We will release the code upon acceptance. The dataset is publicly available   \n664 through a licence.   \n665 Guidelines:   \n666 \u2022 The answer NA means that paper does not include experiments requiring code.   \n667 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n668 public/guides/CodeSubmissionPolicy) for more details.   \n669 \u2022 While we encourage the release of code and data, we understand that this might not be   \n670 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n671 including code, unless this is central to the contribution (e.g., for a new open-source   \n672 benchmark).   \n673 \u2022 The instructions should contain the exact command and environment needed to run to   \n674 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n675 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n676 \u2022 The authors should provide instructions on data access and preparation, including how   \n677 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n678 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n679 proposed method and baselines. If only a subset of experiments are reproducible, they   \n680 should state which ones are omitted from the script and why.   \n681 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n682 versions (if applicable).   \n683 \u2022 Providing as much information as possible in supplemental material (appended to the   \n684 paper) is recommended, but including URLs to data and code is permitted.   \n685 6. Experimental Setting/Details   \n686 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n687 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n688 results?   \n689 Answer: [Yes]   \n690 Justification: We provide all the training and test details in the experimental setup.   \n691 Guidelines:   \n692 \u2022 The answer NA means that the paper does not include experiments.   \n693 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n694 that is necessary to appreciate the results and make sense of them.   \n695 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n696 material.   \n697 7. Experiment Statistical Significance   \n698 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n699 information about the statistical significance of the experiments?   \n700 Answer: [Yes]   \n701 Justification: We conducted our experiments multiple times across 6 participants and took   \n702 the average results. We also include error bars in the plots.   \n703 Guidelines:   \n704 \u2022 The answer NA means that the paper does not include experiments.   \n705 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n706 dence intervals, or statistical significance tests, at least for the experiments that support   \n707 the main claims of the paper.   \n708 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n709 example, train/test split, initialization, random drawing of some parameter, or overall   \n710 run with given experimental conditions).   \n711 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n712 call to a library function, bootstrap, etc.)   \n713 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n714 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n715 of the mean.   \n716 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n717 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n718 of Normality of errors is not verified.   \n719 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n720 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n721 error rates).   \n722 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n723 they were calculated and reference the corresponding figures or tables in the text.   \n724 8. Experiments Compute Resources   \n725 Question: For each experiment, does the paper provide sufficient information on the com  \n726 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n727 the experiments?   \n728 Answer: [Yes]   \n729 Justification: We have included the specifications of the hardware and software environments   \n730 to ensure the reproducibility of our results.   \n731 Guidelines:   \n732 \u2022 The answer NA means that the paper does not include experiments.   \n733 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n734 or cloud provider, including relevant memory and storage.   \n735 \u2022 The paper should provide the amount of compute required for each of the individual   \n736 experimental runs as well as estimate the total compute.   \n737 \u2022 The paper should disclose whether the full research project required more compute   \n738 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n739 didn\u2019t make it into the paper).   \n740 9. Code Of Ethics   \n741 Question: Does the research conducted in the paper conform, in every respect, with the   \n742 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n743 Answer: [Yes]   \n744 Justification: The research conducted in this paper fully conforms with the NeurIPS Code of   \n745 Ethics in every respect.   \n746 Guidelines:   \n747 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n748 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n749 deviation from the Code of Ethics.   \n750 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n751 eration due to laws or regulations in their jurisdiction).   \n752 10. Broader Impacts   \n753 Question: Does the paper discuss both potential positive societal impacts and negative   \n754 societal impacts of the work performed?   \n755 Answer: [Yes]   \n756 Justification: The paper explores how the advancements and applications of our findings   \n757 could benefit society in terms of computational neuroscience research by specifically inves  \n758 tigating the effectiveness of the current state of the art multimodal models in encoding brain   \n759 activity.   \n760 Guidelines:   \n761 \u2022 The answer NA means that there is no societal impact of the work performed.   \n762 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n763 impact or why the paper does not address societal impact.   \n764 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n765 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n766 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n767 groups), privacy considerations, and security considerations.   \n768 \u2022 The conference expects that many papers will be foundational research and not tied   \n769 to particular applications, let alone deployments. However, if there is a direct path to   \n770 any negative applications, the authors should point it out. For example, it is legitimate   \n771 to point out that an improvement in the quality of generative models could be used to   \n772 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n773 that a generic algorithm for optimizing neural networks could enable people to train   \n774 models that generate Deepfakes faster.   \n775 \u2022 The authors should consider possible harms that could arise when the technology is   \n776 being used as intended and functioning correctly, harms that could arise when the   \n777 technology is being used as intended but gives incorrect results, and harms following   \n778 from (intentional or unintentional) misuse of the technology.   \n779 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n780 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n781 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n782 feedback over time, improving the efficiency and accessibility of ML).   \n783 11. Safeguards   \n784 Question: Does the paper describe safeguards that have been put in place for responsible   \n785 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n786 image generators, or scraped datasets)?   \n787 Answer: [NA]   \n788 Justification: Our research does not pose any risks for misuse.   \n789 Guidelines:   \n790 \u2022 The answer NA means that the paper poses no such risks.   \n791 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n792 necessary safeguards to allow for controlled use of the model, for example by requiring   \n793 that users adhere to usage guidelines or restrictions to access the model or implementing   \n794 safety filters.   \n795 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n796 should describe how they avoided releasing unsafe images.   \n797 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n798 not require this, but we encourage authors to take this into account and make a best   \n799 faith effort.   \n800 12. Licenses for existing assets   \n801 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n802 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n803 properly respected?   \n804 Answer: [Yes]   \n805 Justification: We have explicitly cited the datasets, code and models used.   \n806 Guidelines:   \n807 \u2022 The answer NA means that the paper does not use existing assets.   \n808 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n809 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n810 URL.   \n811 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n812 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n813 service of that source should be provided.   \n814 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n815 package should be provided. For popular datasets, paperswithcode.com/datasets   \n816 has curated licenses for some datasets. Their licensing guide can help determine the   \n817 license of a dataset.   \n818 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n819 the derived asset (if it has changed) should be provided.   \n820 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n821 the asset\u2019s creators.   \n822 13. New Assets   \n823 Question: Are new assets introduced in the paper well documented and is the documentation   \n824 provided alongside the assets?   \n825 Answer: [NA]   \n826 Justification: We will try to opensource the code and provide complete documentation for   \n827 our assets upon acceptance.   \n828 Guidelines:   \n829 \u2022 The answer NA means that the paper does not release new assets.   \n830 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n831 submissions via structured templates. This includes details about training, license,   \n832 limitations, etc.   \n833 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n834 asset is used.   \n835 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n836 create an anonymized URL or include an anonymized zip file.   \n837 14. Crowdsourcing and Research with Human Subjects   \n838 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n839 include the full text of instructions given to participants and screenshots, if applicable, as   \n840 well as details about compensation (if any)?   \n841 Answer: [NA]   \n842 Justification: We use publicly available fMRI dataset and do not collect any new data.   \n843 Guidelines:   \n844 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n845 human subjects.   \n846 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n847 tion of the paper involves human subjects, then as much detail as possible should be   \n848 included in the main paper.   \n849 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n850 or other labor should be paid at least the minimum wage in the country of the data   \n851 collector.   \n852 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n853 Subjects   \n854 Question: Does the paper describe potential risks incurred by study participants, whether   \n855 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n856 approvals (or an equivalent approval/review based on the requirements of your country or   \n857 institution) were obtained?   \n858 Answer: [NA]   \n859 Justification: We use publicly available fMRI dataset and do not collect any new data.   \n860 Guidelines:   \n861 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n862 human subjects.   \n863 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n864 may be required for any human subjects research. If you obtained IRB approval, you   \n865 should clearly state this in the paper. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}, {"type": "text", "text": "871 A Cross-subject prediction accuracy ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "872 We estimate cross-subject prediction accuracy in three settings: (i) training with The Bourne   \n873 supremacy and testing with Life data, (ii) training with The wolf of wall street and testing with   \n874 Life data, and (iii) training with both The Bourne supremacy and The wolf of wall street and testing   \n875 with Life data. We present the average cross-subject prediction accuracy across voxels for the   \n876 Movie10 fMRI dataset and across the three settings in Fig. 6. ", "page_idx": 19}, {"type": "image", "img_path": "Rllg9nOasE/tmp/8a20e62bcff24eb3c401a357c9f2e0f682d895ea4e8287c609087611225228d5.jpg", "img_caption": ["", "Figure 6: Cross-subject prediction accuracy: (top) across whole brain, (bottom) across language, visual and auditory regions. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "877 B Detailed sub-ROIs of language, visual and auditory regions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "878 The data covers seven brain regions of interest (ROIs) in the human brain with the following sub  \n879 divisions: (i) early visual (EV: V1, V2, V3, V3B, and V4); (ii) object-related areas (LO1 and LO2);   \n880 (iii) face-related areas (OFA), (iv) scene-related areas (PPA), (v) middle temporal (MT: MT, MST,   \n881 LO3, FST and V3CD), (vi) late language regions, encompassing broader language regions: angular   \n882 gyrus (AG: PFm, PGs, PGi, TPOJ2, TPOJ3), lateral temporal cortex (LTC: STSda, STSva, STGa,   \n883 TE1a, TE2a, TGv, TGd, A5, STSdp, STSvp, PSL, STV, TPOJ1), inferior frontal gyrus (IFG: 44, 45,   \n884 IFJa, IFSp) and middle frontal gyrus (MFG: 55b) (Baker et al., 2018; Milton et al., 2021; Desai et al.,   \n885 2022). ", "page_idx": 19}, {"type": "text", "text": "Table 1: Pretrained Transformer-based Encoder Models. All models have 12 layers. ", "page_idx": 20}, {"type": "table", "img_path": "Rllg9nOasE/tmp/a0e1005969d0b6f0960b8cf752191da183ab9c8967a58efe82a937be9ea0f3e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "887 D Effectiveness of multi-modal vs unimodal representations for various brain 888 regions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "889 We now present the results for per unimodal video model and per speech model in Fig. 8. Similar to   \n890 the average results of unimodal video and speech models, we observe that multi-modal models exhibit   \n891 better normalized brain alignment than individual unimodal video and speech models across language   \n892 and visual regions. Among unimodal speech models, the AST model shows better normalized brain   \n893 alignment than the Wav2vec2.0 model. Among unimodal video models, each unimodal video model   \n894 displays notably consistent performance across regions. ", "page_idx": 20}, {"type": "image", "img_path": "Rllg9nOasE/tmp/b867af63d8839fc153e5c535ec579b0ff55c99f4238b916a8e02927a9d7b0507.jpg", "img_caption": ["Figure 7: Average normalized brain alignment for per video and audio modalities from multi-modal and individual modality features across whole brain and several ROIs of language (ATL, IFGOrb, MFG, PCC, dmPFC) and visual (OV, FV). Error bars indicate the standard error of the mean across participants. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "895 E How is the brain alignment of multi-modal features affected by the 896 elimination of a particular modality? ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "897 To understand the contribution of each modality to the multi-modal brain alignment for multi-modal   \n898 naturalistic stimulus, we perform residual analyses by removing the unimodality features from   \n899 multi-modal joint representations as well as multi-modal video or audio representations from joint   \n900 representations and measure the differences in brain alignment before and after removal modality  \n901 specific features. Figs. 9 and 10 display the normalized brain alignment for language ROIs such as   \n902 PTL, MFG, ATL, PCC and visual regions EVC, OV and FV. We note a decrease in brain alignment   \n903 for these regions following the removal of video embeddings from cross-modality models, whereas   \n904 the removal of audio embeddings does not affect the brain alignment. On the other hand, for jointly   \n905 pretrained models, removal of both video and audio embeddings partially impacts the brain alignment. ", "page_idx": 20}, {"type": "text", "text": "906 F Layerwise brain alignment ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "907 We now plot the layer-wise normalized brain alignment for the Unimodal models and TVLT joint   \n908 model, as shown in Fig. 11. Observation from Fig. 11 indicates a consistent drop in performance from   \n909 early to lower layers, specifically for both TVLT joint and unimodal video models. The key finding   \n910 here is that our results that TVLT joint embeddings showcase improved brain alignment across all the   \n911 layers compared to unimodal video and speech embeddings. ", "page_idx": 21}, {"type": "image", "img_path": "Rllg9nOasE/tmp/48cf0bf5c9f62f4af8a0112e3147b3f9dc7956c9c8d2aa3ddb7ed421c9c59d95.jpg", "img_caption": ["Figure 8: Average normalized brain alignment for video and audio modalities from multi-modal and individual modality features across whole brain and several ROIs of language (ATL, ATL, PTL, IFG, PCC, dmPFC) and visual (EVC, MT). Error bars indicate the standard error of the mean across participants. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Rllg9nOasE/tmp/16a81b52db0645c54a838ae0f8c29d03e17b970b288bf21dbd4692ad40633ccc.jpg", "img_caption": ["Figure 9: Residual analysis for ATL, PTL, IFG, MFG, IFGOrb, PCC and dmPFC regions: Average normalized brain alignment was computed across participants before and after removal of video and audio embeddings from both jointly pretrained and cross-modality models. Error bars indicate the standard error of the mean across participants. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "Rllg9nOasE/tmp/d588f7fab2ad9f5aa05024f32141f8202d7b42d7bc5ae20160f995d8c6655422.jpg", "img_caption": ["Figure 10: Residual analysis for EVC, OV, SV, FV and AC regions: Average normalized brain alignment was computed across participants before and after removal of video and audio embeddings from both jointly pretrained and cross-modality models. Error bars indicate the standard error of the mean across participants. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "Rllg9nOasE/tmp/136300ffaea42cc55c4a47b89c7928f45ae2db0cd923e6e3ea59faa89e14bc80.jpg", "img_caption": ["Figure 11: Normalized brain alignment across layers for multi-modal model (TVLT joint embeddings) and unimodal video and speech models. "], "img_footnote": [], "page_idx": 25}]