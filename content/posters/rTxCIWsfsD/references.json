{"references": [{"fullname_first_author": "Scott Fujimoto", "paper_title": "Off-policy deep reinforcement learning without exploration", "publication_date": "2019-00-00", "reason": "This paper is foundational to offline reinforcement learning, introducing a crucial method for learning effective policies from fixed datasets without the need for exploration in the environment."}, {"fullname_first_author": "Rafael Figueiredo Prudencio", "paper_title": "A survey on offline reinforcement learning: Taxonomy, review, and open problems", "publication_date": "2022-00-00", "reason": "This survey provides a comprehensive overview of offline reinforcement learning, categorizing existing methods and identifying key challenges and open research areas in the field."}, {"fullname_first_author": "Xuezhou Zhang", "paper_title": "Corruption-robust offline reinforcement learning", "publication_date": "2022-00-00", "reason": "This paper directly addresses the challenge of robustness in offline RL under data corruption, a crucial problem for real-world applications of offline RL."}, {"fullname_first_author": "Rui Yang", "paper_title": "Towards robust offline reinforcement learning under diverse data corruption", "publication_date": "2023-00-00", "reason": "This paper is highly relevant as it directly addresses the limitations of existing corruption-robust offline RL methods by proposing a new approach, TRACER, that outperforms previous state-of-the-art methods."}, {"fullname_first_author": "Marc G. Bellemare", "paper_title": "A distributional perspective on reinforcement learning", "publication_date": "2017-00-00", "reason": "This paper introduces a novel distributional approach to reinforcement learning, which is important because it provides a more nuanced way of representing the uncertainty in reinforcement learning which helps improve robustness and performance."}]}