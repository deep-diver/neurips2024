[{"figure_path": "teVxVdy8R2/figures/figures_0_1.jpg", "caption": "Figure 1: Multi-task performance comparisons in two domains.", "description": "This figure compares the success rates of different methods across multiple robotic manipulation tasks.  The methods include Diffusion Policy, SuSIE, RT-1, RT-2*, GR-1, and PAD (the authors' method). The tasks are categorized into two domains: simulated Metaworld tasks (all tasks) and real-world Panda robot tasks (seen and unseen tasks). PAD shows consistently high success rates across both domains, especially on unseen tasks, highlighting its strong generalization capabilities.", "section": "1 Introduction"}, {"figure_path": "teVxVdy8R2/figures/figures_1_1.jpg", "caption": "Figure 2: Diffusion models have achieved impressive success in visual generation tasks (a) and visual-motor control tasks (b). Image prediction and robot action are actually highly correlated since they share the same underlying physical dynamics. The PAD framework predicts the future and generates actions in a joint denoising process.", "description": "This figure illustrates the core idea behind the PAD framework. It shows how diffusion models are used in image generation (a) and visual policy learning (b), highlighting the shared underlying physical dynamics. PAD integrates both image prediction and action generation into a unified joint denoising process (c), leveraging the correlation between image prediction and robot actions for improved performance.", "section": "3 PAD: Prediction with Action via Joint Denoising Process"}, {"figure_path": "teVxVdy8R2/figures/figures_3_1.jpg", "caption": "Figure 3: Visualization of the PAD framework. Current observations in different modalities are first encoded into latent and concatenated with white noise channel-wise. These noised latent are then tokenized into tokens and perform a joint denoising process to predict the images and robot actions simultaneously. PAD can flexibly accommodate extra or missing modal inputs through a masked-attention mechanism", "description": "This figure visualizes the PAD (Prediction with Action Diffuser) framework.  It shows how current observations (RGB image, robot pose, and other modalities) are encoded into a latent space, combined with noise, tokenized, and processed through multiple Diffusion Transformer blocks to jointly denoise and predict both future images and robot actions.  The architecture highlights the framework's ability to handle missing modalities using a masked multi-head attention mechanism.", "section": "3.2 Model Architectures"}, {"figure_path": "teVxVdy8R2/figures/figures_4_1.jpg", "caption": "Figure 4: We learn a single vision-language conditioned policy to solve all tasks in each domain with limited demonstrations, co-training with the bridge video data. In simulated MetaWorld, we learn a policy to tackle all 50 tasks. In real-world panda manipulations, we split objects into seen objects and unseen new objects to test the generalization ability of our policy.", "description": "This figure shows three different experimental setups used to evaluate the proposed PAD (Prediction with Action Diffuser) model.  The first shows Bridge video data used for pretraining, demonstrating the model's ability to learn from large-scale unlabeled video data. The second shows the Metaworld benchmark, which contains 50 robotic manipulation tasks for testing the model's ability to generalize to diverse tasks and environments. The third shows the real-world robot manipulation setting with a Panda robot arm where the tasks are divided into seen and unseen tasks to evaluate generalization performance on unseen situations and objects. The figure highlights the model's capability to handle both simulated and real-world scenarios, showcasing its learning potential from diverse data sources and its generalization to unseen tasks.", "section": "4 Experiments"}, {"figure_path": "teVxVdy8R2/figures/figures_6_1.jpg", "caption": "Figure 5: Generalization test under 3 levels of difficulties. The yellow bounding box suggests the target position. Our proposed PAD shows the strongest generalization abilities in unseen tasks.", "description": "This figure demonstrates the generalization capabilities of the proposed PAD (Prediction with Action Diffuser) framework on unseen tasks.  It shows the results of tests performed at three difficulty levels: easy, medium, and hard. Each level presents a different level of complexity in terms of the number and types of objects to manipulate. The yellow bounding boxes highlight the target objects the robot needs to interact with. The results indicate that PAD outperforms other methods in its ability to generalize to tasks it hasn't encountered during training.", "section": "4.3 Generalization Analysis"}, {"figure_path": "teVxVdy8R2/figures/figures_7_1.jpg", "caption": "Figure 6: Comparisons on predicted images between PAD and GR-1. PAD generates more precise images than GR-1 which may potentially lead to more accurate control actions. Zoom in for better comparisons.", "description": "This figure compares the predicted future images generated by PAD and GR-1, alongside the ground truth future images.  The comparison highlights PAD's superior ability to generate precise future images, suggesting its potential to contribute to improved accuracy in robot control action predictions. The enhanced image precision from PAD is visually evident when zooming in on the images.", "section": "4.2 Main Results"}, {"figure_path": "teVxVdy8R2/figures/figures_7_2.jpg", "caption": "Figure 7: Predictions on bridge datasets. PAD predicts futures align with instructions but also keeps uncertainty. In the first image, PAD imagines \u201ca yellow pear\u201d instead of the ground truth \u201cbanana\u201d; in the second image, PAD imagines scenes faster than the ground truth.", "description": "This figure shows two examples of PAD's predictions on the Bridge dataset, a large-scale internet video dataset.  Each example shows three image frames: the current observation, the ground truth future, and PAD\u2019s predicted future. The caption highlights that while PAD\u2019s predictions generally align with the instruction given (e.g., \"Put corn in bowl sink\"), there is some uncertainty. In the first example, PAD incorrectly predicts a yellow pear instead of a banana. In the second, PAD predicts the action faster than in the ground truth.", "section": "Quality of the Generated Images"}, {"figure_path": "teVxVdy8R2/figures/figures_8_1.jpg", "caption": "Figure 8: We observe that co-training with an internet video dataset leads to better image generation qualities, which may potentially lead to better robot action predictions.", "description": "This figure shows a comparison of image predictions from three different training methods: using only robotic data, using robotic data and internet video data, and a ground truth. The left-hand side shows a robotic arm moving blocks, with the right-hand side showing a robotic arm manipulating various objects. In both examples, the co-training method with videos produces better quality image predictions than training only on robotic data, showcasing the benefits of incorporating diverse datasets in visual policy learning.", "section": "4.4 Ablation Studies"}, {"figure_path": "teVxVdy8R2/figures/figures_8_2.jpg", "caption": "Figure 9: PAD can flexibly train with additional modality, and simultaneously predict all the futures through joint denoising process.", "description": "This figure demonstrates PAD's ability to incorporate additional modalities beyond RGB images and robot poses.  Specifically, it shows the results when depth images are included as a condition for prediction. The left side shows the ground truth conditions (RGB and Depth images) and the right side shows the corresponding predictions generated by PAD for future time steps.  The figure highlights that PAD can effectively integrate multiple input modalities to accurately predict both future RGB images and depth maps, indicating a robust and versatile approach to visual policy learning.", "section": "3.3 Model Architectures"}, {"figure_path": "teVxVdy8R2/figures/figures_9_1.jpg", "caption": "Figure 10: Correlation between Transformer Gflops and policy success rate.", "description": "This figure shows the correlation between the computational cost (measured in Transformer GFLOPS) and the success rate of the learned policy in the Metaworld benchmark.  It visually demonstrates that higher computational resources generally lead to better performance.", "section": "4.5 Scaling Analysis"}, {"figure_path": "teVxVdy8R2/figures/figures_15_1.jpg", "caption": "Figure 4: We learn a single vision-language conditioned policy to solve all tasks in each domain with limited demonstrations, co-training with the bridge video data. In simulated MetaWorld, we learn a policy to tackle all 50 tasks. In real-world panda manipulations, we split objects into seen objects and unseen new objects to test the generalization ability of our policy.", "description": "This figure shows the experimental setup of the PAD model. The left side shows the bridge video data used for pre-training. The middle shows the simulated MetaWorld environment with 50 tasks. The right side shows the real-world robot manipulation experiment with both seen and unseen tasks. The figure illustrates the PAD model's ability to learn a single policy that can generalize to unseen tasks in both simulated and real-world environments.", "section": "4 Experiments"}, {"figure_path": "teVxVdy8R2/figures/figures_15_2.jpg", "caption": "Figure 5: Generalization test under 3 levels of difficulties. The yellow bounding box suggests the target position. Our proposed PAD shows the strongest generalization abilities in unseen tasks.", "description": "This figure shows the results of a generalization test conducted to evaluate the ability of the PAD model to handle unseen tasks.  The test was performed across three difficulty levels, each presenting increasing challenges (easy, medium, hard). The yellow bounding boxes indicate the target positions for the robot manipulation tasks. The success rate of PAD is compared to several baseline methods across these difficulty levels, demonstrating PAD's superior performance in generalizing to novel, unseen tasks.", "section": "4.3 Generalization Analysis"}]