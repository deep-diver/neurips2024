[{"figure_path": "sKCKPr8cRL/figures/figures_1_1.jpg", "caption": "Figure 1: The relationship between non-vocabulary parameters Nnv and the corresponding optimal vocabulary parameters Nopt follows a power law, where Nopt should be scaled slower than Nnv as y < 1. Empirical results align with predictions of our proposed approaches, with larger circles indicating higher loss values. Here V refers to the vocabulary size i.e. the number of distinct tokens.", "description": "This figure illustrates the relationship between the number of non-vocabulary parameters (Nnv) in a language model and the corresponding optimal number of vocabulary parameters (Nopt).  The data suggests a power-law relationship, where Nopt scales slower than Nnv (y < 1).  The plot shows empirical results from model training, which generally align with the predictions made using three different approaches proposed in the paper. Larger circles represent higher loss values during training.", "section": "Analysis: Why the optimal vocabulary size is bounded by compute"}, {"figure_path": "sKCKPr8cRL/figures/figures_2_1.jpg", "caption": "Figure 2: Vocabulary parameters of popular LLMs and predicted optimal vocabulary parameters at a compute-optimal number of training tokens. Most current LLMs have suboptimal vocabulary parameters due to vocabulary sizes, which are smaller than the predicted optimal values. Among the current models, StarCoder2-3B, OLMo-7B, InternLM2-20B, and Gemma2-27B have vocabulary sizes that come closest to the optimal allocation for their respective model sizes.", "description": "This figure compares the vocabulary parameters used in several popular LLMs against the optimal vocabulary parameters predicted by the authors' proposed methods.  It shows that most existing LLMs significantly underutilize their vocabulary size compared to what is predicted to be optimal given their computational budget and model size. Only a few models come close to the predicted optimal allocation. The x-axis represents the non-vocabulary parameters, and the y-axis represents the optimal vocabulary parameters.", "section": "2 Preliminary"}, {"figure_path": "sKCKPr8cRL/figures/figures_4_1.jpg", "caption": "Figure 3: Left: FLOPs curve with various vocabulary sizes, assuming all configurations achieve a fixed loss. There exists an optimal vocabulary size that minimizes FLOPs. Right: Loss curves with various vocabulary sizes given different FLOP budgets. For each budget there exists an optimal vocabulary size that minimizes loss. As the FLOP budget increases this optimal vocabulary size increases (shifts to the right).", "description": "The left panel shows that for a fixed loss, there is an optimal vocabulary size that minimizes the FLOPs.  The right panel shows that for various FLOPs budgets, there is an optimal vocabulary size that minimizes loss, and that this optimal size increases with increasing FLOPs budget.", "section": "Analysis: Why the optimal vocabulary size is bounded by compute"}, {"figure_path": "sKCKPr8cRL/figures/figures_5_1.jpg", "caption": "Figure 4: Training curves of the experiments used in Approach 1 (\u00a74.1) and Approach 3 (\u00a74.3). We train models with the non-vocabulary parameters fixed and vocabulary sizes varying from 4K to 96K.", "description": "This figure displays training curves for models with varying vocabulary sizes (4K to 96K) but fixed non-vocabulary parameters, showing the relationship between FLOPs and the normalized loss (Lu).  The data points represent the compute-optimal allocation of resources for different FLOPs budgets. This data is used in Approach 1 to fit power laws and in Approach 3 to parametrically fit the loss function.", "section": "4.1 Approach 1: Estimating power laws via IsoFLOPs"}, {"figure_path": "sKCKPr8cRL/figures/figures_5_2.jpg", "caption": "Figure 5: Fitting results of the Approach 1. Blue stars denote the selected data points where the combination (Nnv, Nv, H) reaches the lowest loss given various FLOPs budgets. We find power law fits with respect to the optimal non-vocabulary parameters, vocabulary parameters, and the number of training characters, respectively.", "description": "This figure shows the results of fitting power laws to the relationship between FLOPs and the optimal allocation of non-vocabulary parameters, vocabulary parameters, and training data.  The blue stars represent the data points where the lowest loss was achieved for each FLOPs budget.  Three separate power laws are fitted to show the scaling relationships for each of these parameters.", "section": "4.1 Approach 1: Estimating power laws via IsoFLOPs"}, {"figure_path": "sKCKPr8cRL/figures/figures_7_1.jpg", "caption": "Figure 2: Vocabulary parameters of popular LLMs and predicted optimal vocabulary parameters at a compute-optimal number of training tokens. Most current LLMs have suboptimal vocabulary parameters due to vocabulary sizes, which are smaller than the predicted optimal values. Among the current models, StarCoder2-3B, OLMo-7B, InternLM2-20B, and Gemma2-27B have vocabulary sizes that come closest to the optimal allocation for their respective model sizes.", "description": "This figure compares the vocabulary parameters used in several popular large language models (LLMs) with the predicted optimal vocabulary parameters based on the compute-optimal number of training tokens.  The authors' research suggests that most current LLMs underutilize their vocabulary size, resulting in suboptimal performance.  The chart highlights the discrepancy between the actual vocabulary sizes and the authors' predicted optimal sizes, indicating a potential area for improvement in LLM design and training.", "section": "2 Preliminary"}, {"figure_path": "sKCKPr8cRL/figures/figures_9_1.jpg", "caption": "Figure 7: Left: The heatmap illustrates how the best vocabulary size among all choices of vocabularies shifts with the training data. The non-vocabulary parameter is fixed (Nnv = 302M). Each cell in the heatmap represents the loss given a certain FLOPs budget for a fair evaluation, with the color intensity indicating the loss value. The black line with markers denotes the best vocabulary size for each FLOPs budget, which basically increases as the number of training data increases. Right: The number of training tokens are slightly varying for different vocabulary sizes given a certain FLOPs budget. To keep FLOPs consistent, models with larger vocabulary sizes are trained on fewer tokens.", "description": "The figure shows how the optimal vocabulary size changes depending on the amount of training data used and the FLOPs budget.  The left panel is a heatmap showing the loss for different vocabulary sizes and FLOPs budgets, with different markers indicating different data regimes.  The right panel shows the number of training tokens for different vocabulary sizes under a fixed FLOPs budget. In short, more data and compute allow for a larger optimal vocabulary size.", "section": "Analysis: Why the optimal vocabulary size is bounded by compute"}, {"figure_path": "sKCKPr8cRL/figures/figures_17_1.jpg", "caption": "Figure 8: Left: The curve of the derivative of FLOPs with respect to vocabulary size V. The curve of \u2202FLOPs/\u2202V increases as V increases, and the FLOPs reach a minima at the solution of \u2202FLOPs/\u2202V = 0. Middle: The curve of FLOPs with respect to vocabulary size V, where V reaches its optimal point V*. Right: The curve of training characters with a given FLOPs budget. Take Nnv = 302M and H = 43B as an example. The FLOPs budget is decided by the Nnv, H and the predicted V.", "description": "This figure shows three plots illustrating the relationship between the derivative of FLOPs, FLOPs, and training characters with respect to vocabulary size (V). The left plot shows that the derivative of FLOPs increases with increasing vocabulary size, reaching a minimum at the optimal vocabulary size. The middle plot shows that FLOPs decrease with increasing vocabulary size, reaching a minimum at the optimal vocabulary size. The right plot shows that the number of training characters increase with increasing vocabulary size, reaching a maximum at the optimal vocabulary size.  These plots together illustrate that there is an optimal vocabulary size for a given FLOPs budget, where performance is maximized.", "section": "A.1 The derivation of FLOPs w.r.t the vocabulary size for the Approach 2"}, {"figure_path": "sKCKPr8cRL/figures/figures_18_1.jpg", "caption": "Figure 9: The SVD plots of the learned word embedding for V=4K (left), V=16K (middle) and V=64K (right) for a model with Nnv = 85M. Different colors represent different log frequencies.", "description": "This figure visualizes the distribution of word embeddings learned by models with different vocabulary sizes (4K, 16K, and 64K) using singular value decomposition (SVD). The color intensity represents the log frequency of words.  The figure shows that smaller vocabularies lead to a more dispersed distribution of embeddings, while larger vocabularies show more clustering, especially for low-frequency words, indicating insufficient training for those embeddings.  This suggests an optimal vocabulary size exists that balances adequate representation of words with sufficient training for all embeddings.", "section": "A.3 More visualizations for the analyses: Why the optimal vocabulary size is bounded by compute"}, {"figure_path": "sKCKPr8cRL/figures/figures_19_1.jpg", "caption": "Figure 3: Left: FLOPs curve with various vocabulary sizes, assuming all configurations achieve a fixed loss. There exists an optimal vocabulary size that minimizes FLOPs. Right: Loss curves with various vocabulary sizes given different FLOP budgets. For each budget there exists an optimal vocabulary size that minimizes loss. As the FLOP budget increases this optimal vocabulary size increases (shifts to the right).", "description": "The figure shows two plots related to the optimal vocabulary size for language models. The left plot shows that there is an optimal vocabulary size that minimizes the FLOPs (floating point operations) for a fixed loss. The right plot shows that there is an optimal vocabulary size that minimizes the loss for each FLOPs budget, and that this optimal vocabulary size increases as the FLOPs budget increases.", "section": "Analysis: Why the optimal vocabulary size is bounded by compute"}, {"figure_path": "sKCKPr8cRL/figures/figures_19_2.jpg", "caption": "Figure 11: Correlation between the unigram-normalized loss \nLu and BPC, where p and e denote the Pearson correlation\ncoefficient and the root mean square error of the linear fit, respectively.", "description": "This figure shows the strong positive correlation between the unigram-normalized loss (Lu) and bits per character (BPC).  The unigram-normalized loss is a vocabulary-insensitive metric used to compare language models with different vocabulary sizes fairly.  BPC measures the average number of bits needed to represent each character in the text corpus. The high correlation (p=0.9888) and low error (e=0.0683) indicate that these two metrics are highly related, demonstrating the effectiveness of Lu as a metric for comparing language models with different vocabulary sizes.", "section": "A.5 The Vocabulary-insensitive Metric: Lu and BPC"}, {"figure_path": "sKCKPr8cRL/figures/figures_22_1.jpg", "caption": "Figure 3: Left: FLOPs curve with various vocabulary sizes, assuming all configurations achieve a fixed loss. There exists an optimal vocabulary size that minimizes FLOPs. Right: Loss curves with various vocabulary sizes given different FLOP budgets. For each budget there exists an optimal vocabulary size that minimizes loss. As the FLOP budget increases this optimal vocabulary size increases (shifts to the right).", "description": "The figure shows two plots. The left plot shows the relationship between FLOPs and vocabulary size when the loss is fixed. There is an optimal vocabulary size that minimizes the FLOPs required. The right plot shows the relationship between loss and vocabulary size for different FLOPs budgets. For each budget, there is an optimal vocabulary size that minimizes the loss. As the budget increases, the optimal vocabulary size also increases.", "section": "Analysis: Why the optimal vocabulary size is bounded by compute"}, {"figure_path": "sKCKPr8cRL/figures/figures_23_1.jpg", "caption": "Figure 13: Empirical examination of the fairness of our unigram-normalized loss, Lu. Dots correspond to trained models with varying vocabulary size. We plot their losses (y-axis) and performance on 7 downstream tasks (x-axis): WG [56], PIQA [9], OBQA [42], Hellaswag [83], BoolQ [16], ARC-E [17] and ARC-C [17]. The straight line reflects the results of the regression fit with the shade indicating the confidence interval.", "description": "This figure empirically validates the fairness of the unigram-normalized loss (Lu) used in the paper to compare models with different vocabulary sizes.  It shows a strong negative correlation between the unigram-normalized loss and the average normalized accuracy across seven downstream tasks.  This confirms that Lu is a suitable metric for comparing models with varying vocabulary sizes, as it avoids the bias introduced by the raw language modeling loss, which increases with vocabulary size.", "section": "A.10 Experimental verification on the fairness of the unigram-normalized language modeling loss"}, {"figure_path": "sKCKPr8cRL/figures/figures_23_2.jpg", "caption": "Figure 13: Empirical examination of the fairness of our unigram-normalized loss, Lu. Dots correspond to trained models with varying vocabulary size. We plot their losses (y-axis) and performance on 7 downstream tasks (x-axis): WG [56], PIQA [9], OBQA [42], Hellaswag [83], BoolQ [16], ARC-E [17] and ARC-C [17]. The straight line reflects the results of the regression fit with the shade indicating the confidence interval.", "description": "This figure empirically validates the use of the unigram-normalized loss (Lu) by showing the relationship between the commonly used language modeling loss and the unigram-normalized loss with the performance on seven downstream tasks. It demonstrates that Lu is a fair metric to compare language models with different vocabulary sizes as it shows a negative correlation with downstream performance.", "section": "A.10 Experimental verification on the fairness of the unigram-normalized language modeling loss"}, {"figure_path": "sKCKPr8cRL/figures/figures_24_1.jpg", "caption": "Figure 1: The relationship between non-vocabulary parameters Nnv and the corresponding optimal vocabulary parameters Nopt follows a power law, where Nopt should be scaled slower than Nnv as y < 1. Empirical results align with predictions of our proposed approaches, with larger circles indicating higher loss values. Here V refers to the vocabulary size i.e. the number of distinct tokens.", "description": "This figure shows the relationship between the number of non-vocabulary parameters (Nnv) in a language model and the corresponding optimal number of vocabulary parameters (Nopt).  The relationship follows a power law, meaning Nopt scales slower than Nnv (y < 1). The plot includes empirical results from model training, which generally align with the predictions from three different methods proposed in the paper for estimating optimal vocabulary size. Larger circles represent higher loss values, indicating that deviations from the optimal relationship lead to worse model performance.  The figure demonstrates the importance of carefully scaling the vocabulary size relative to other model parameters.", "section": "4.1 Approach 1: Estimating power laws via IsoFLOPs"}]