[{"type": "text", "text": "KrwEmd: Revising the Imperfect Recall Abstraction from Forgetting Everything ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 A recent research has shown that an extreme interpretation of imperfect recall   \n2 abstraction \u2013 completely forgetting all past information \u2013 has led to excessive ab  \n3 straction issues. Currently, there are no hand abstraction algorithms that effectively   \n4 integrate historical information. This paper aims to develop the first such algorithm.   \n5 Initially, we introduce the KRWI abstraction for Texas Hold\u2019em-style games, which   \n6 categorizes hands based on K-recall winrate features that incorporate historical   \n7 information. Statistical results indicate that, in terms of the number of distinct   \n8 infosets identified, KRWI significantly outperforms POI, an abstraction that identi  \n9 fies the most abstracted infosets that forget all historical information. Following   \n10 this, we introduce the KrwEmd algorithm, the first hand abstraction algorithm to   \n11 effectively use historical information by combining K-recall win rate features and   \n12 earth mover\u2019s distance for hand classification. Experimental studies conducted   \n13 in the Numeral211 Hold\u2019em environment show that under identical abstracted   \n14 infoset sizes, KrwEmd not only surpasses POI but also outperforms state-of-the-art   \n15 hand abstraction algorithms such as Ehs and PaEmd. These findings suggest that   \n16 incorporating historical information can significantly enhance the performance of   \n17 hand abstraction algorithms, positioning KrwEmd as a promising approach for   \n18 advancing strategic computation in large-scale adversarial games. ", "page_idx": 0}, {"type": "text", "text": "19 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "20 Imperfect recall abstraction has proven to be very important for solving large-scale computational   \n21 games, significantly reducing computational complexity. Recently, AI using imperfect recall abstrac  \n22 tion has developed better-than-human strategies for Texas Hold\u2019em testbed\u2014even when using limited   \n23 computational resources [23, 7, 8].   \n24 The task of hand abstraction in Texas Hold\u2019em aims to re  \n25 duce computational overhead by applying the same strategy   \n26 to similar hands. In an imperfect recall setting [29, 20], the   \n27 hand abstraction in the later phase does not strict depend   \n28 on the results of the hand abstraction in the earlier phase.   \n29 However, the term imperfect recall is often interpreted in   \n30 an extreme manner in practice. Researchers typically un  \n31 derstand it as completely forgetting all past information\u2014in   \n32 other words, considering only future information\u2014and de  \n33 sign abstraction algorithms based on this understanding   \n34 [16, 17, 19, 15, 14]. There are two major factors that mainly   \n35 affect the results of abstraction for each phase: the number   \n36 of clustering centers (i.e. centroids), which can be set man  \n37 ually, and the number of distinct features that are used to categorize hands at each phase. Recent   \n38 research [12] has found that constructing hand features solely based on future information can lead   \n39 to excessive abstraction. For example, as shown in the Figure 1, two hands: A and B constructed   \n40 with only future information can have the same hand features. As the game progresses, the rate of   \n41 feature repetition among different hands gradually increases, while the distribution of distinct hand   \n42 features assumes a spindle-shaped pattern. Additionally, constructing hand features with historical   \n43 information in addition to the future may differentiate two hands sharing the same future information   \n44 and hence makes more features available for clustering as well as enhances the performance of hand   \n45 abstraction.   \n46 However, there still remain two unsolved issues. First, Fu et al. [12] have introduced a K-recall   \n47 outcome feature, which incorporates historical information. This feature can only identify if elements   \n48 are identical or not, but it lacks the capability to discern the extent of differences between features.   \n49 Therefore, it is difficult to adjust the number of clusters appropriately, which makes it challenging to   \n50 construct an effective hand abstraction algorithm that integrates historical information. Second, due to   \n51 the inability to modify the number of clusters, Fu et al. [12] only compared the performance between   \n52 the maximum clusters cases of integration of historical information (KROI) and no integration   \n53 at all (POI). In this condition, although KROI significantly outperforms POI, the comparison is   \n54 inconclusive because KROI recognizes more abstracted infosets than POI. Thus, it does not prove   \n55 that the performance of abstraction algorithms that integrate historical information is necessarily   \n56 superior under the condition of having the same number of abstracted infosets.   \n57 This paper introduces a framework for constructing hand features based on winrates, with the K  \n58 recall winrate feature being the most crucial one. Based on this, we developed the K-recall winrate   \n59 isomorphism (KRWI), an abstraction that integrates historical information. Across the same game   \n60 phases, KRWI identifies slightly fewer hand features than KROI but significantly more than POI.   \n61 Importantly, the K-recall winrate feature is capable of discerning the extent of differences between   \n62 features. Therefore, by combining the earth mover\u2019s distance with the K-recall winrate feature, we   \n63 developed the first hand abstraction algorithm that integrates historical information, named KrwEmd,   \n64 and designed an efficient computational method. We validated our approach in the Numeral211 game   \n65 environment, where KrwEmd demonstrated superior performance to POI under the same infosets   \n66 conditions. Additionally, in clustering settings, KrwEmd also outperformed the Ehs and PaEmd   \n67 algorithms, with PaEmd being the current state-of-the-art hand abstraction algorithm. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "1URMG6B3WW/tmp/d296f26bbc2fab971193ad2647082e8a3f251b835dc12a1229fd1eda626d4a74.jpg", "img_caption": ["Figure 1: In a 4-phase game hand abstraction task, the current goal is to classify hands A and B. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "68 2 Background and Notation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "69 Generally, Texas Hold\u2019em-style poker games are modeled as imperfect information games. However,   \n70 for the task of hand abstraction, games with ordered signals [18, 12] offer a better theoretical tool.   \n71 The game with ordered signals is a subclass of imperfect information games in that they further   \n72 subdivide the nodes (also called histories, states, or trajectories) in imperfect information games into   \n73 mutually independent signals and public nodes. This allows for each aspect to be studied in isolation.   \n74 Under this framework, the hand abstraction task in Texas Hold\u2019em-style games is modeled as signal   \n75 abstraction.   \n76 In a game with ordered signals $\\tilde{\\Gamma}\\,=\\,\\Bigl\\langle\\tilde{\\mathcal{N}},\\tilde{H},\\tilde{Z},\\tilde{\\rho},\\tilde{A},\\tilde{\\chi},\\tilde{\\tau},\\gamma,\\Theta,\\varsigma,O,\\omega,\\succeq,\\tilde{u}\\Bigr\\rangle$ , there is a set of   \n77 players $\\tilde{\\mathcal{N}}=\\mathcal{N}\\cup\\{c,p u b\\}$ , which includes not only the main participants $\\mathcal{N}=\\{1,\\dots,N\\}$ but   \n78 also a special nature player $c$ who controls the randomness and an observer player pub who can   \n79 see everything but doesn\u2019t take any actions. The game progresses through a series of public nodes   \n80 $\\tilde{X}=\\tilde{H}\\cup\\tilde{Z}$ . Some of these public nodes are terminal public nodes $\\tilde{Z}$ where the game ends and   \n81 outcomes are determined, while the others are non-terminal public nodes $\\tilde{H}$ . Among the non-terminal   \n82 public nodes, some are where players make decisions within the action space $\\tilde{A}$ , and the remaining   \n83 are chance public nodes where the nature player reveals signals, with the special action Reveal   \n84 within A\u02dc.   \n85 At every non-terminal public node, $\\tilde{\\rho}:\\tilde{H}\\mapsto\\mathcal{N}c$ (i.e., ${\\mathcal{N}}\\cup\\{c\\})$ specifies which player is responsible   \n86 for making an action, and $\\tilde{\\chi}:\\tilde{H}\\mapsto2^{\\tilde{A}}$ confines the possible actions they can take. When the nature   \n87 player makes a move, it reveals signals $\\theta\\in\\Theta$ that carry information relevant to the game. These   \n88 signals are then observed by all players except $c$ , $O(\\theta)\\,=\\,(O_{1}(\\theta),\\ldots,O_{N}(\\theta),O_{p u b}\\bar{(}\\theta))$ , though   \n89 what they can see might differ.   \n90 The progression from one public node to another is clearly defined $\\tilde{\\tau}:\\tilde{H}\\times\\tilde{A}\\mapsto\\tilde{X}$ , ensuring that   \n91 the game\u2019s structure is sequential and predictable. Similarly, the signals are revealed according to a   \n92 probability distribution $\\bar{\\varsigma^{\\,\\cdot}}\\,\\Theta\\mapsto\\Delta(\\Theta\\bar{)}$ , which specifies the likelihood of the next signal given the   \n93 current one. We use $\\tilde{h}\\subseteq\\tilde{h}^{\\prime}$ to indicate that $\\tilde{h}$ is a predecessor of $\\widetilde{h}^{\\prime}$ , and $\\theta\\subseteq\\theta^{\\prime}$ to indicate that $\\theta$ is a   \n94 predecessor of $\\theta^{\\prime}$ . Each phase of the game is the number of times nature player has revealed signals,   \n95 denoted by $\\gamma:\\tilde{X}\\mapsto\\bar{\\mathbb{N}}^{+}$ . $\\mathfrak{r}=\\{\\gamma(\\tilde{x})\\ |\\ \\tilde{x}\\in\\tilde{X}\\}$ represents the phases that a game with ordered   \n96 signals may go through. Since the root is a chance public node, we have $\\operatorname*{min}\\mathfrak{r}=1$ .   \n97 At the end of the game, players receive their payoffs based on the signals and the terminal public   \n98 node, represented by $\\boldsymbol{\\tilde{u}}\\,=\\,(\\tilde{u}_{1},\\dots,\\tilde{u}_{N})$ , where $\\tilde{u}_{i}:\\,\\Theta\\times\\tilde{Z}\\,\\mapsto\\,\\mathbb{R}$ . Additionally, each player\u2019s   \n99 survival status is determined at these terminal public nodes, denoted by $\\boldsymbol{\\omega}=(\\omega_{1},\\dots,\\omega_{N})$ , where   \n100 $\\omega_{i}:\\tilde{Z}\\mapsto\\{t r u e,f a l s e\\}$ . The signals possess a partial order within their subset, terminal signals   \n101 $\\Tilde{\\Theta}$ , indicated by $\\succeq\\colon\\tilde{\\Theta}\\times\\mathcal{N}\\times\\mathcal{N}\\mapsto\\{t r u e,f a l s e\\}$ . It is required that for any terminal signal $\\bar{\\theta^{\\mathrm{~}}}\\in\\bar{\\Theta}$   \n102 and terminal public nodes $\\tilde{z}\\,\\in\\,\\{\\tilde{z}^{\\prime}\\,\\in\\,\\tilde{Z}\\,\\mid\\,\\omega_{i}(\\tilde{z}^{\\prime})\\,=\\,\\omega_{j}(\\tilde{z}^{\\prime})\\,=\\,t r u e\\}$ , i $\\mathbf{f}\\succeq\\left(\\theta,i,j\\right)=t r u e$ , then   \n103 $\\tilde{u}_{i}(\\theta,\\tilde{z})\\geq\\tilde{u}_{j}\\bar{(\\theta,\\tilde{z})}$ .   \n104 Players make decisions based on their observations of signals and the current non-terminal public   \n105 node. A player may have the same observation for different signals, forming a signal infoset for   \n106 signals they cannot distinguish. For a player $i\\in\\mathcal{N}$ , the signal infoset for a signal $\\theta$ is denoted as   \n107 $\\bar{\\vartheta_{i}(\\theta)}=\\{\\bar{\\theta}^{\\prime}\\in\\Theta\\ |\\ O_{i}(\\theta\\bar{)}=O_{i}(\\theta^{\\prime})\\ \\bar{\\wedge}\\ O_{p u b}(\\theta)=O_{p u b}(\\bar{\\theta^{\\prime}})\\}$ . Specifically, for the nature player,   \n108 $\\vartheta_{c}(\\theta)=\\{\\theta^{\\prime}\\in\\Theta\\ |\\ O_{p u b}(\\theta^{\\prime})=O_{p u b}(\\theta)\\}$ . We abuse the notation $\\vartheta\\in\\Theta_{i}$ to represent a signal infoset,   \n109 where for any player $i\\in\\mathcal{N}$ , $\\Theta_{i}$ is a partition of $\\Theta$ , representing the collection of player $i$ \u2019s signal   \n110 infosets. $\\Theta_{i}^{(1)},\\dots,\\Theta_{i}^{(|\\mathfrak{r}|)}$ are the collections of player $i$ \u2019s signal infosets for each phase, and they   \n111 form a partition of $\\theta_{i}$ . In games with ordered signals, the signals describe all private information.   \n112 The signal infoset, combined with public nodes, can be transformed into the infoset of an imperfect   \n113 information game. Fu et al. [12] detailed this transformation process.   \n114 The game with ordered signals model allows us to study the issue of signal abstraction independently.   \n115 For this purpose, we introduce a signal (infoset) abstraction proflie, $\\boldsymbol{\\alpha}=(\\alpha_{1},.,\\alpha_{N})$ , where for each   \n116 player $i\\in\\mathcal{N}$ , $\\alpha_{i}$ is a partition of $\\Theta$ called the signal (infoset) abstraction. Any $\\hat{\\vartheta}\\,\\in\\,\\alpha_{i}$ then is   \n117 said to be an abstracted signal infoset for player $i$ , and it can be further divided into several signal   \n118 infosets within $\\Theta_{i}$ . These finer signal infosets collectively form a partition of $\\hat{\\vartheta}$ . In general, two signal   \n119 abstractions cannot be directly compared in terms of performance, but in a few specific cases there   \n120 does exist a special relationship between them, which is called refinement. Consider two abstractions   \n121 $\\alpha_{i}$ and $\\beta_{i}$ . If $\\forall\\hat{\\vartheta}\\in\\beta_{i}$ , there exists one or more abstracted signal infosets in $\\alpha_{i}$ such that the union   \n122 of these forms a partition of $\\hat{\\vartheta}$ , then we said that $\\alpha_{i}$ refines $\\beta_{i}$ , symbolically $\\alpha_{i}\\supseteq\\beta_{i}$ . The signal   \n123 abstracted game $\\tilde{\\Gamma}^{\\alpha}$ was derived by substituting $\\theta_{i}$ with $\\alpha_{i}$ across all $\\tilde{x}\\in\\tilde{X}$ .   \n124 Perfect/imperfect recall originally describes a property of imperfect information games, indicating   \n125 that players do not need to remember all the information they have observed throughout the game.   \n126 Since games with ordered signals are a subset of imperfect information games, we derived the concept   \n127 of signal perfect/imperfect recall from them. A player $i$ in a game $\\tilde{\\Gamma}$ is said to have signal perfect   \n128 recall if, for any $\\theta_{1}^{\\prime},\\theta_{2}^{\\prime}\\in\\vartheta^{\\prime}$ , any predecessor $\\theta_{1}$ of $\\theta_{1}^{\\prime}$ has a corresponding predecessor $\\theta_{2}$ of $\\theta_{2}^{\\prime}$ such   \n129 that $\\theta_{2}\\in\\mathcal{V}(\\theta_{1})$ . If all players have signal perfect recall, the game \u0393\u02dc is said to have signal perfect   \n130 recall. For a game $\\tilde{\\Gamma}$ with signal perfect recall, if $\\alpha_{i}$ is the signal abstraction of player $i\\in\\mathcal{N}$ , let   \n131 $(\\alpha_{i},\\Theta_{-i})$ denote the signal abstraction profile where player $i$ adopts the signal abstraction $\\alpha_{i}$ while   \n132 other players do not do abstraction. If $\\tilde{\\Gamma}^{(\\alpha_{i},\\Theta_{-i})}$ retains signal perfect recall, then $\\alpha_{i}$ is considered a   \n133 signal abstraction with perfect recall; otherwise, it is an signal abstraction with imperfect recall.   \n134 In games with ordered signals, the strategy $\\pi_{i}$ for player $i$ maps from a non-terminal public node   \n135 and a signal infoset to a probability distribution over actions, with the strategy profile denoted as   \n136 $\\pi=(\\pi_{1},\\ldots,\\pi_{N})$ . When all players adopt the strategy proflie $\\pi$ , the expected sum of future rewards,   \n137 also known as expected value, for player $i$ at public node $\\tilde{x}$ and signal $\\theta$ is denoted as $v_{i}^{\\pi}(\\theta,\\tilde{x})$ ,   \n138 and the expected value for the entire game is denoted as $v_{i}(\\pi)$ . A Nash equilibrium is a strategy   \n139 profile where no player can obtain a higher expected value by changing their strategy. Formally,   \n140 $\\pi^{*}$ is a Nash equilibrium if for every player $i$ , $\\bar{v_{i}}(\\pi^{*})=\\operatorname*{max}_{\\pi_{i}}\\bar{v_{i}(\\pi_{i},\\pi_{-i}^{*})}$ , where $\\pi_{-i}$ denotes the   \n141 strategies of all players except $i$ . In two-player zero-sum scenarios, the exploitability of $\\pi$ is denoted ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "143 3 Related Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "144 Our research focuses on hand abstraction techniques in AI systems for Texas Hold\u2019em-style games   \n145 (i.e. the signal abstraction in games with ordered signals), building on the initial works of Shi and   \n146 Littman [25] and Billings et al. [4]. These seminal works introduced the concept of game abstraction,   \n147 which aims to simplify games while preserving essential characteristics. The researchers started by   \n148 manually forming hand buckets as a result of their expertise with game-playing strategy. The first   \n149 automated hand abstraction was that of Gilpin and Sandholm [16]. Later, a model of games with   \n150 ordered signals was given for Texas Hold\u2019em by Gilpin and Sandholm [18]; lossless isomorphism   \n151 (LI) was developed with signal rotation. Despite the elegance of LI, its low compression rates hinder   \n52 its application in large-scale games, whereas lossy abstraction shows potential for such application.   \n153 An expectation-based clustering method was proposed by Gilpin and Sandholm [17] in their work,   \n154 and a histogram-based clustering method was introduced by Gilpin et al. [19]. The former is known   \n155 as Ehs, while the latter is referred to as the potential-aware method. Subsequent studies by Gilpin   \n156 and Sandholm [15] and Johanson et al. [20] compared Ehs and potential-aware methods, concluding   \n157 that the latter holds an advantage in large-scale games. Johanson et al. [20] also introduced the   \n158 use of earth mover\u2019s distance1(EMD) in potential-aware methods. Ganzfried and Sandholm [14]   \n159 introduced a more efficient approximation algorithm for earth mover\u2019s distance in potential-aware   \n160 methods (PaEmd). Brown et al. [9] further applied PaEmd to distributed environments for solving   \n161 large-scale imperfect-information games. This paradigm has found success in Texas Hold\u2019em AI   \n162 systems and is considered state-of-the-art in hand abstraction. Very recently, Fu et al. [12] proposed   \n163 several novel tools, such as abstraction resolution and common refinement. They introduced two   \n164 signal abstraction: one is the potential outcome isomorphism (POI), which identifies the maximum   \n165 number of abstracted signal infosets considering future information only; The other is the K-recall   \n166 outcome isomorphism (KROI), which identifies the maximum number of abstracted signal infosets   \n167 considering historical information. They emphasized that current imperfect recall signal abstraction   \n168 algorithms, which consider only future information, are prone to excessive abstraction. However,   \n169 they did not provide practical signal abstraction algorithms.   \n170 Other abstraction techniques for decision-making problems include action abstraction [13, 6, 21] and   \n171 general imperfect recall abstraction [10, 11] in extensive-form games, as well as state abstraction and   \n172 action abstraction in reinforcement learning [1, 2]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "173 4 Winrate Isomorphism ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "174 The first contribution of this paper is an isomorphism framework of winrate-based features, including   \n175 the potential winrate isomorphism (PWI) and the k-recall winrate Isomorphism (KRWI). Compared   \n176 with outcome-based features, winrate-based features offer a streamlined approach, focusing exclu  \n177 sively on the distribution of loss, draw, and win outcomes of signals emanating from a signal infoset   \n178 (and its predecessors) as it evolves towards the terminal signals. Winrate-based features are numerical   \n179 vectors of consistent length. In this section, an identical Winrate-based feature uniquely determines   \n180 an abstracted signal infoset. It is worth noting that the similarity of Winrate-based features reflects   \n181 the similarity among signal infosets, allowing for clustering based on these features (see Section 5).   \n182 Both PWI and KRWI share the similar isomorphism construction process for player $i$ in phase $r$ , as   \n183 illustrated in algorithm 1. The difference lies only in the construction operator for the winrate-based   \n184 features, FEATURE, used in lines 5 and 12. The isomorphism construction process starts by iterating   \n185 through all signal infosets of $\\Theta_{i}^{(r)}$ and collecting their features. Next, these features are deduplicated   \n186 and stored in lexicographical order within set $\\mathcal{C}_{i}^{(r)}$ , which is implemented as a vector data structure.   \n187 Within $\\mathcal{C}_{i}^{(r)}$ , the index of a feature serves as an identifier for an abstracted signal infoset. Then,   \n88 by utilizing a hash table $\\mathcal{C}\\mathcal{Z}_{i}^{(r)}$ , we can identify an abstracted signal infoset\u2019s identifier based on   \n189 its feature. In the final step, we traverse $\\Theta_{i}^{(r)}$ again, associating the identifier of a signal infoset   \n190 with the identifier of its corresponding abstracted signal infoset, and this relationship is recorded in   \n191 $\\mathcal{D}_{i}^{(r)}$ , an isomorphism map. The function $I n d e x_{i}(r,\\cdot)$ is a domain-specific mapping that assigns a   \n192 unique identifier to each signal infoset at phase $r$ , within the numeric range of 0 to $|\\Theta_{i}^{(r)}|-1$ . In ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Require:   \n$r=1,\\ldots,R$ . Phases.   \n$\\Theta_{i}^{(r)}$ . Signal infoset space for player $i$ .   \n$I n d e x_{i}(r,\\cdot):\\Theta_{i}^{(r)}\\mapsto\\mathbb{N}.$ Signal infoset index function for player $i$ .   \n1: procedure ISOMORPHISMCONSTRUCTOR(r, $\\Theta_{i}^{(r)}$ , FEATURE(\u00b7))   \n2: Initialize $\\mathcal{C}_{i}^{(r)}$ vector as empty.   \n3: Initialize $\\mathcal{D}_{i}^{(r)}$ array arbitrarily with length $|\\Theta_{i}^{(r)}|$ .   \n4: for $\\vartheta\\in\\Theta_{i}^{(r)}$ do   \n5: $f e a t u r e\\gets\\mathrm{FEATURE}(\\vartheta)$ .   \n6: Append feature to $\\mathcal{C}_{i}^{(r)}$ .   \n7: end for   \n8: Eliminate duplicates from $\\mathcal{C}_{i}^{(r)}$ .   \n9: Sort the elements of $\\mathcal{C}_{i}^{(r)}$ in lexicographical order.   \n10: Construct hash table $\\mathcal{C}\\mathcal{Z}_{i}^{(r)}$ from $\\mathcal{C}_{i}^{(r)}$ . Store the index lexid and value feature of $\\mathcal{C}_{i}^{(r)}$ in $\\mathcal{C}\\mathcal{Z}_{i}^{(r)}$ as key-value pairs $(f e a t u r e,l e x i d)$ .   \n11: for $\\vartheta\\in\\Theta_{i}^{(r)}$ do   \n12: $f e a t u\\bar{r}e\\gets\\mathrm{FEATURE}(\\vartheta),\\,i d x\\gets I n d e x_{i}(r,\\vartheta)$ .   \n13: Update $\\mathcal{D}_{i}^{(r)}[i d x]$ with $\\mathcal{C T}_{i}^{(r)}[f e a t u r e]$ .   \n14: end for   \n15: return $(\\mathcal{C}_{i}^{(r)},\\mathcal{D}_{i}^{(r)})$ .   \n16: end procedure ", "page_idx": 4}, {"type": "text", "text": "193 Texas Hold\u2019em-style games, one optional approach for implementing this function is through lossless   \n194 isomorphism [18, 27]. ", "page_idx": 4}, {"type": "text", "text": "195 4.1 Potential Winrate Isomorphism ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "196 Potential winrate isomorphism (PWI) is a signal abstraction that classify signal infosets based on its   \n197 potential winrate features. These features focus on the distribution of a player\u2019s winrate over terminal   \n198 signals after passing through a given signal infoset, without considering the history of how the player   \n199 reached the signal infoset. Specifically, for player $i$ in phase $r$ , the potential winrate feature associated   \n200 with $\\vartheta\\in\\Theta_{i}^{(r)}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\np f_{i}^{(r)}(\\vartheta)=(p f_{i}^{(r),0}(\\vartheta),p f_{i}^{(r),1}(\\vartheta),\\ldots,p f_{i}^{(r),N}(\\vartheta)),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "201 where ", "page_idx": 4}, {"type": "text", "text": "02 $p f_{i}^{(r),0}(\\vartheta)$ denotes the probability that player $i$ ranks lower than least one other player in   \n03 the terminal signals, after passing through .   \n04 \u2022 $p f_{i}^{(r),l}(\\vartheta)$ , for $l>0$ , denotes the probability that player $i$ ranks no lower than any other   \n05 player and ranks higher than exactly $l-1$ other players in the terminal signals, after passing   \n06 through $\\vartheta$ .   \n207 In the terminal phase, the winrate feature is calculated by directly statisticing the game outcomes for   \n208 players in the given signal infoset. Moreover, in the non-terminal phases, we use a recursive approach   \n209 to simplify the computation of the winrate feature, thereby avoiding the need to enumerate every   \n210 signal infoset down to the terminal phase. The recursive formula is ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\np f_{i}^{(r),l}(\\vartheta)=\\sum_{\\vartheta^{(r+1)}\\in\\Theta_{i}^{(r+1)}}p f_{i}^{(r+1),l}(\\vartheta^{(r+1)})P r\\{\\vartheta^{(r+1)}|\\vartheta\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "1URMG6B3WW/tmp/ec04c137009425b256762f55ae0f3910a662149c4a1830a1fdb4e2e70fba68d3.jpg", "table_caption": [], "table_footnote": ["Table 1: The number of abstracted signal infosets identified by KRWI, and KROI in each phase and k of HUNL&HUNLE, with W/O indicating the ratio identified by PWI and POI. "], "page_idx": 5}, {"type": "text", "text": "211 The PWI algorithm is derived from the   \n212 POI algorithm [12], and the details of   \n213 the PWI algorithm are elaborated in Ap  \n214 pendix A.1. Both algorithms use the po  \n215 tential winrate feature to distinguish be  \n216 tween different abstracted signal infosets   \n217 in the terminal phase. However, unlike   \n218 POI, PWI also uses the potential winrate   \n219 feature in non-terminal phases to identify   \n220 different abstracted signal infoset classes,   \n221 while POI relies on the potential outcome ", "page_idx": 5}, {"type": "table", "img_path": "1URMG6B3WW/tmp/b731aefca2d47f9898c65fdbe70271a84fc3faf649962fd4e189e96f9f3d6edd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: The number of abstracted signal infosets identified by LI, PWI, and POI in each phase of HUNL&HUNLE, with W/O indicating the ratio identified by PWI and POI. ", "page_idx": 5}, {"type": "text", "text": "222 feature (which captures the distribution of the abstracted signal infoset class for future signal infoset).   \n223 In non-terminal phases, the potential winrate feature is a simplified version of the potential outcome   \n224 feature. Unsurprisingly, PWI also results in excessive abstraction similar to POI. As shown in Table   \n25 2, in heads-up limit hold\u2019em (HULHE) and heads-up no-limit hold\u2019em (HUNL), the number of   \n226 abstracted signal infosets identifiable by lossless isomorphism increases with each phase, indicating   \n27 that the game becomes increasingly complex. However, the number of abstracted signal infosets   \n228 identifiable by PWI and POI first increases and then decreases, showing a spindle-shaped pattern.   \n229 And we observed that when only future information is considered, winrate-based features may lead   \n230 to greater information loss compared to outcome-based features. For instance, in the River phase, the   \n231 number of abstracted signal infosets identified by PWI is only $79.16\\%$ of that identified by POI. ", "page_idx": 5}, {"type": "text", "text": "232 4.2 K-Recall Winrate Isomorphism ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "233 As Fu et al. [12] mentioned, supplementing historical information can enhance the ability of signal   \n234 abstraction to identify abstracted signal infosets. Inspired by KROI\u2019s construction approach, we   \n235 developed the k-recall winrate isomorphism (KRWI). The key difference is that instead of using   \n236 $\\mathbf{k}\\cdot$ -recall outcome features to distinguish between different signal infosets, KRWI utilizes $\\mathbf{k}$ -recall   \n237 winrate features.   \n238 In a game with signal perfect recall, all signals within the signal infoset $\\vartheta$ have their predecessors at   \n239 phase $r^{\\prime}$ , which belong to the identical signal infoset $\\vartheta^{\\prime}$ . For player $i$ at phase $r$ , the signal infoset   \n240 $\\vartheta\\in\\Theta_{i}^{(r)}$ has a $k$ -recall winrate feature $(k<r)$ ) represented as a numerical array with a dimension of   \n241 $(k+1)(N+1)$ : ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nr f_{i}^{(r,k)}(\\vartheta)=(p f_{i}^{(r)}(\\vartheta);p f_{i}^{(r-1)}(\\vartheta);\\cdot\\cdot\\cdot;p f_{i}^{(r-k)}(\\vartheta))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "242 When $r^{\\prime}$ is less than $r$ , $p f_{i}^{(r^{\\prime})}(\\vartheta)$ denotes the potential winrate feature for the predecessor signal   \n243 infoset $\\vartheta^{\\prime}$ of $\\vartheta$ at phase $r^{\\prime}$ . Since we have stored all the potential winrate features of $\\vartheta\\in\\Theta_{i}^{(r)}$ through   \n244 PCi(r ), PDi(r)and assigned them unique identifiers in Algorithm A1. To save storage space and   \n245 facilitate retrieval, what we actually store is ", "page_idx": 5}, {"type": "equation", "text": "$$\nr f i_{i}^{(r,k)}(\\vartheta)=(\\mathcal{P}\\mathcal{D}_{i}^{(r)}[\\vartheta],\\mathcal{P}\\mathcal{D}_{i}^{(r-1)}[\\vartheta],\\ldots,\\mathcal{P}\\mathcal{D}_{i}^{(r-k)}[\\vartheta))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "246 $\\mathcal{P}\\mathcal{D}_{i}^{(r^{\\prime})}[\\vartheta]$ is the identifier for the potential winrate feature of the predecessor $\\vartheta^{\\prime}$ of $\\vartheta$ in the $r^{\\prime}$ phase,   \n247 $r^{\\prime}\\le r$ . For algorithm details, please refer to Appendix A.2.   \n248 Just as the potential winrate feature is a simplified version of the potential outcome feature, the   \n249 $\\mathbf{k}$ -recall winrate feature is a simplified version of the $\\mathbf{k}$ -recall outcome feature. Table 1 shows the   \n250 number of signal infosets that KRWI and KROI can identify and their ratio in HUNL&HULHE. We   \n251 were pleasantly surprised to find that while the ratio of PWI to POI resolution can drop below $80\\%$ ,   \n252 when $k$ is set to its maximum value, i.e. $r-1$ , the ratio of KRWI to KROI resolution can reach nearly   \n253 $90\\%$ at a minimum, with most of the information preserved. Also, we can easily observe that the   \n254 number of abstracted signal infosets identified by KRWI is much higher than that identified by POI. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "255 5 K-Recall Winrate Abstraction with Earth Mover\u2019s Distance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "256 Fu et al. [12] introduced potential and k-recall outcome features, referred to as outcome-based features,   \n257 to distinguish different abstracted signal infosets. In the previous section, we developed potential and   \n258 k-recall winrate features, termed winrate-based features, for the same purpose. In these two methods,   \n259 Each unique feature corresponds to a single abstracted signal infoset. Intuitively, we can infer that   \n260 feature similarity might reflect the similarity among abstracted signal infosets, enabling further   \n261 abstraction and compression for application in large-scale games. However, assessing similarity with   \n262 outcome-based features is challenging because the identification code indicates only the category,   \n263 without reflecting the degree of similarity. In contrast, winrate-based features represent winrates,   \n264 which are inherently comparable, allowing for an easy definition of distances between them.   \n265 For the signal information sets $\\vartheta,\\vartheta^{\\prime}$ of player $i$ at phase $r$ , we can define the distance of their $\\mathbf{k}$ -recall   \n266 winrate feature as ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nd(r f_{i}^{(r,k)}(\\vartheta),r f_{i}^{(r,k)}(\\vartheta^{\\prime}))=\\sum_{j=0}^{k}w_{j}\\cdot\\mathrm{Emd}(p f_{i}^{(r-j)}(\\vartheta),p f_{i}^{(r-j)}(\\vartheta^{\\prime}))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "267 Among Equation (5), Emd is the operator used to calculate the earth mover\u2019s distance (EMD) [24].   \n268 The EMD calculates the distance between two histograms using optimal transport theory. Since it   \n269 requires solving linear programming equations, the computational complexity of the EMD is sensitive   \n270 to the dimensionality of the histograms, and approximate algorithms are usually used for larger-scale   \n271 problems. However, the dimensionality of winrate-based features is small, with a dimension of 3 in a   \n272 two-player scenario, so we attempt to use a fast algorithm for accurately computing the EMD [5].   \n273 $w_{0},\\ldots,w_{k}$ are hyperparameters used to control the importance of EMD at each phase $r,\\ldots,r-k$   \n274 We use the KMeans $^{++}$ algorithm [3], combined with the distance of their k-recall winrate feature, to   \n275 cluster the abstracted signal infosets of KRWI. We named this algorithm KrwEmd.   \n276 Although calculating EMD on small-dimensional histograms is already very fast, clustering ac  \n277 tual Texas Hold\u2019em still faces a significant computation. For example, for the River phase of   \n278 HUNL&HULHE, the clustering input size of the KRWI abstracted signal infoset is approximately   \n279 $5.8\\times10^{8}$ . When we set the number of centroids to 20000, a single Kmean $\\mathbf{S++}$ iteration takes about   \n280 19000 core hours on a computer with a 2.40GHz clock frequency, which is a significant time cost.   \n281 Therefore, we need to find ways to reduce this time cost. We have developed an accelerated algorithm,   \n282 please refer to Appendix A.3 for details. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "283 6 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "284 We conducted experiments on the Nu  \n285 meral211 Hold\u2019em [12] testbed. Nu  \n286 meral211 is a two-player three-phase   \n287 Taxes Hold\u2019em-style game with more   \n288 complex hand systems than the Leduc   \n289 Hold\u2019em [26] and Rhode Island Hold\u2019em   \n290 [25] test environments, making it suitable   \n291 for studying hand abstraction issues. De  \n292 tailed rules are included in Appendix B.   \n293 Table 3 shows the number of abstracted   \n294 signal infosets recognized by KRWI and   \n295 KROI, along with lossless isomorphism,   \n296 in Numeral211 Hold\u2019em.   \n297 Let $\\alpha\\,=\\,(\\alpha_{1},\\alpha_{2})$ be the signal abstraction we would like to assess. We will test the strength of   \n298 the signal abstraction by measuring exploitability of the approximate equilibrium derived using the   \n299 CSMCCFR algorithm [30, 22] in different abstracted signal infoset scales. We gauge the performance   \n300 over exploitability. For doing that, we consider both symmetric and asymmetric abstraction scenarios.   \n301 In this symmetric abstraction setting, we measure the exploitability of approximate equilibrium   \n302 that is yielded when both the players in the game employ signal abstraction in the original game.   \n303 However, it may lead to the abstraction pathology [28]. To avoid such problems, we illustrate the   \n304 theoretical performance of the signal abstraction under evaluation through asymmetric abstraction.   \n330056 oTbhtea ian iamnda ,u rileisbpre imv eilny .t hFei nsialglny,a l waeb sctornacctaet dt hgea tmweos $\\tilde{\\Gamma}^{(\\alpha_{1},\\Theta_{2})}$ taon dg $\\tilde{\\Gamma}^{(\\Theta_{1},\\alpha_{2})}$ da ntod   \n$\\pi^{*,1}$ $\\pi^{*,2}$ $\\pi^{\\prime}=(\\pi_{1}^{*,1},\\pi_{2}^{*,2})$ 2 )   \n307 check the exploitability of $\\pi^{\\prime}$ ", "page_idx": 6}, {"type": "table", "img_path": "1URMG6B3WW/tmp/0549a49a1926af501e04cee84eee8d804e964b4b888fdc0a514f735abb64c630.jpg", "table_caption": [], "table_footnote": ["Figure 3: The number of abstracted signal infosets identified by LI, PWI, and POI in each phase of HUNL&HUNLE, with W/O indicating the ratio identified by PWI and POI. "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "308 7 Experiment ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "1URMG6B3WW/tmp/e61a0e61419f8c73d63aa762ef6bdb069efef311aeac5f5c1b662162866d0131.jpg", "img_caption": ["Figure 4: Full abstraction setting experiment, trained for $5.5\\times10^{10}$ iterations. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "309 Firstly, we provide an evaluation of the performance of KRWI (2-RWI) compared with KROI (2-ROI)   \n310 and POI (0-ROI) approaches and lossless isomorphism. We keep the most abstracted signal infosets   \n311 identified under the full abstraction setting. Note that POI is the common refinement of existing   \n312 signal abstraction algorithms that only consider future information. And, since previous works cannot   \n313 control the number of abstracted infoset, they cannot justify their performance in that considering   \n314 historical information in signal abstraction was better than that in signal abstraction with the same   \n315 number of abstracted infoset. To investigate this issue, we included KrwEmd and set the clustering   \n316 scale to be consistent with POI. Note here, that 2-RWI and 2-ROI share the same capability of infoset   \n317 recognition in Preflop and Flop, while POI is only a little bit worse than 2-RWI and 2-ROI in Flop.   \n318 Thus, we can directly allow clustering of KrwEmd abstraction use the abstracted signal infosets   \n319 identified by POI in Preflop and Flop, and only perform clustering in River. Here, we design four   \n320 sets of hyper-parameters: $(w_{0},w_{1},w_{2})$ , i.e., exponentially decreasing: $(16,4,1)$ , linearly decreasing:   \n321 $(7,5,3)$ , constant: $(1,1,1)$ , and increasing: $(3,5,7)$ in the importance of historical information. We   \n22 only show the result of best- and worst-performing parameters (to make the figure neat). The full   \n323 figures appear in the Appendix C. Figure 4a shows the result of symmetric abstraction, while Figure   \n24 4b shows the result of asymmetric abstraction. We observed that both symmetric and asymmetric   \n325 abstractions maintained consistent abstraction performance without abstraction pathologies. As   \n326 expected, overfitting was observed in the symmetric abstraction scenario while in the asymmetric   \n327 scenario overfitting was significant only for POI. The performance difference between 2-RWI and   \n328 2-ROI is small, which means that under the full abstraction setting, using simple winrate-based   \n329 features instead of complex outcome-based features can achieve nearly the same performance. Even   \n330 with the worst parameter configuration (increasing importance), KrwEmd with the same number of   \n331 abstracted signal inforsets as POI still outperforms POI.   \n332 Next, we compared the performance of KrwEmd with the currently applied signal abstraction   \n333 algorithms Ehs and PaEmd. It should be noted that POI is the common refinement both for Ehs and   \n334 PaEmd, meaning that the maximum number of abstracted signal infosets they can recognize will not   \n335 exceed that of POI. Thus, we set a compression rate that is 10 times lower than that of POI, while not   \n336 performing abstraction for Preflop. The final number of abstracted infosets is set to (100, 225, 396).   \n337 To exclude the influence of random events on performance, we generated 3 sets of abstractions   \n338 for Ehs and PaEmd each. KrwEmd used hyperparameters $\\left(w_{3,0},w_{3,1},w_{3,2};w_{2,0},w_{2,1}\\right)$ in Flop and   \n339 River, which are exponentially decreasing $(16,4,1;4,1)$ , linearly decreasing $(7,5,3;5,3)$ , constant   \n340 $(1,1,1;1,1)$ , and increasing $(3,5,7;5,7)$ in the importance of historical information. Additionally,   \n341 since PaEmd uses approximate EMD calculations, its approximate distance is asymmetric, making it   \n342 difficult for the algorithm to converge. We truncated after 1000 iterations on a single core, with an   \n343 average cost of 1427.7s, while Ehs and KrwEmd both achieved convergent clustering results, requiring   \n344 an average of 12.3 and 96.7 iterations, with average time costs of 11.2s and 341.4s, respectively.   \n345 Figure 5a shows the results of symmetric abstraction experiments, while Figure 5b shows the results of   \n346 asymmetric abstraction experiments. We observed that both symmetric and asymmetric abstractions   \n347 maintained consistent abstraction performance, similar to the full abstraction scenario, without   \n348 significant abstraction pathologies. The experimental results show that KrwEmd\u2019s performance is   \n349 far superior to that of Ehs and PaEmd under all parameter settings. Our experiments also confirmed   \n350 that, despite PaEmd\u2019s convergence issues, it is indeed a more effective abstraction algorithm than   \n351 Ehs. Additionally, we further validated that the importance of historical information decreases   \n352 progressively from bottom to top, although this time the best-performing parameter was exponentially   \n353 decreasing rather than linearly decreasing as in the previous experiment.   \n354 These two experiments validate that considering historical information is indeed more effective than   \n355 considering future information only in signal abstraction even in imperfect recall setting. ", "page_idx": 7}, {"type": "image", "img_path": "1URMG6B3WW/tmp/bb2d701cb7b9abd011262b30767b8d76b41de4764bc37012511b8088b29c708a.jpg", "img_caption": ["Figure 5: Performance comparison of KrwEmd versus other imperfect recall signal abstraction algorithms considering only future information, trained for $3.7\\times10^{10}$ iterations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "356 8 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "357 This research introduces the first imperfect recall signal abstraction algorithm that considers historical   \n358 information. This algorithm has the ability to adjust the scale of the abstracted signal infosets. Based   \n359 on this, we fully verified that the imperfect recall signal abstraction and abstraction algorithms   \n360 considering historical information is superior to that only considering future information. Therefore,   \n361 the KrwEmd algorithm has replaced the PaEmd algorithm and become the SOTA in this field. Based   \n362 on the KrwEmd algorithm, we are expected to build a stronger Texas Hold\u2019em AI. ", "page_idx": 8}, {"type": "text", "text": "363 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "364 [1] David Abel. A theory of state abstraction for reinforcement learning. In AAAI conference on   \n365 artificial intelligence, volume 33, pages 9876\u20139877, 2019.   \n366 [2] David Abel, Nate Umbanhowar, Khimya Khetarpal, Dilip Arumugam, Doina Precup, and   \n367 Michael Littman. Value preserving state-action abstractions. In International Conference on   \n368 Artificial Intelligence and Statistics (AISTATS), pages 1639\u20131650, 2020.   \n369 [3] David Arthur and Sergei Vassilvitskii. k-means $^{++}$ the advantages of careful seeding. In   \n370 ACM-SIAM symposium on Discrete algorithms (SODA), pages 1027\u20131035, 2007.   \n371 [4] D Billings, N Burch, A Davidson, R Holte, J Schaeffer, T Schauenberg, and D Szafron.   \n372 Approximating game-theoretic optimal strategies for full-scale poker. In International Joint   \n373 Conference on Artificial Intelligence (IJCAI), volume 3, pages 661\u2013668, 2003.   \n374 [5] Nicolas Bonneel, Michiel van de Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement   \n375 Interpolation Using Lagrangian Mass Transport. ACM Transactions on Graphics (SIGGRAPH   \n376 ASIA 2011), 30(6), 2011.   \n377 [6] Noam Brown and Tuomas Sandholm. Regret transfer and parameter optimization. In AAAI   \n378 Conference on Artificial Intelligence, volume 28, 2014.   \n379 [7] Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus   \n380 beats top professionals. Science, 359(6374):418\u2013424, 2018.   \n381 [8] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365   \n382 (6456):885\u2013890, 2019.   \n383 [9] Noam Brown, Sam Ganzfried, and Tuomas Sandholm. Hierarchical abstraction, distributed   \n384 equilibrium computation, and post-processing, with application to a champion no-limit texas   \n385 hold\u2019em agent. In International Conference on Autonomous Agents and Multiagent Systems   \n386 (AAMAS), pages 7\u201315, 2015.   \n387 [10] Ji\u02c7r\u00ed \u02c7Cerm\u00e1k, Branislav Bo\u0161ansky, and Viliam Lisy. An algorithm for constructing and solving   \n388 imperfect recall abstractions of large extensive-form games. In International Joint Conference   \n389 on Artificial Intelligence (IJCAI), pages 936\u2013942, 2017.   \n390 [11] Ji\u02c7r\u00ed \u02c7Cerm\u00e1k, Viliam Lis\\`y, and Branislav Bo\u0161ansk\\`y. Automated construction of bounded-loss   \n391 imperfect-recall abstractions in extensive-form games. Artificial Intelligence, 282:103248,   \n392 2020.   \n393 [12] Yanchang Fu, Junge Zhang, Dongdong Bai, Lingyun Zhao, Jialu Song, and Kaiqi Huang.   \n394 Expanding the resolution boundary of outcome-based imperfect-recall abstraction in games   \n395 with ordered signals. arXiv preprint arXiv:2403.11486, 2024.   \n396 [13] Sam Ganzfried and Tuomas Sandholm. Action translation in extensive-form games with large   \n397 action spaces: axioms, paradoxes, and the pseudo-harmonic mapping. In International Joint   \n398 Conference on Artificial Intelligence (IJCAI), pages 120\u2013128, 2013.   \n399 [14] Sam Ganzfried and Tuomas Sandholm. Potential-aware imperfect-recall abstraction with earth   \n400 mover\u2019s distance in imperfect-information games. In AAAI Conference on Artificial Intelligence,   \n401 volume 28, 2014.   \n402 [15] Andrew Gilpin and Thomas Sandholm. Expectation-based versus potential-aware automated   \n403 abstraction in imperfect information games: An experimental comparison using poker. In   \n404 National Conference on Artificial Intelligence (NCAI), volume 3, pages 1454\u20131457, 2008.   \n405 [16] Andrew Gilpin and Tuomas Sandholm. A competitive texas hold\u2019em poker player via auto  \n406 mated abstraction and real-time equilibrium computation. In National Conference on Artificial   \n407 Intelligence (NCAI), volume 21, page 1007. Menlo Park, CA; Cambridge, MA; London; AAAI   \n408 Press; MIT Press; 1999, 2006.   \n409 [17] Andrew Gilpin and Tuomas Sandholm. Better automated abstraction techniques for imperfect   \n410 information games, with application to texas hold\u2019em poker. In International Joint Conference   \n411 on Artificial Intelligence (IJCAI), pages 1\u20138, 2007.   \n412 [18] Andrew Gilpin and Tuomas Sandholm. Lossless abstraction of imperfect information games.   \n413 Journal of the ACM (JACM), 54(5):25\u2013es, 2007.   \n414 [19] Andrew Gilpin, Tuomas Sandholm, and Troels Bjerre S\u00f8rensen. Potential-aware automated   \n415 abstraction of sequential games, and holistic equilibrium analysis of texas hold\u2019em poker. In   \n416 National Conference on Artificial Intelligence (NCAI), volume 22, page 50. Menlo Park, CA;   \n417 Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2007.   \n418 [20] Michael Johanson, Neil Burch, Richard Valenzano, and Michael Bowling. Evaluating state  \n419 space abstractions in extensive-form games. In International Conference on Autonomous Agents   \n420 and Multiagent Systems (AAMAS), pages 271\u2013278, 2013.   \n421 [21] Christian Kroer and Tuomas Sandholm. Discretization of continuous action spaces in extensive  \n422 form games. In International Conference on Autonomous Agents and Multiagent Systems   \n423 (AAMAS), pages 47\u201356, 2015.   \n424 [22] Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte carlo sampling   \n425 for regret minimization in extensive games. International Conference on Neural Information   \n426 Processing Systems (NeurIPS), 22, 2009.   \n427 [23] Matej Moravc\u02c7\u00edk, Martin Schmid, Neil Burch, Viliam Lisy\\`, Dustin Morrill, Nolan Bard, Trevor   \n428 Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level   \n429 artificial intelligence in heads-up no-limit poker. Science, 356(6337):508\u2013513, 2017.   \n430 [24] Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover\u2019s distance as a metric for   \n431 image retrieval. International journal of computer vision, 40:99\u2013121, 2000.   \n432 [25] Jiefu Shi and Michael L Littman. Abstraction methods for game theoretic poker. In Computers   \n433 and Games: Second International Conference, CG 2000 Hamamatsu, Japan, October 26\u201328,   \n434 2000 Revised Papers 2, pages 333\u2013345. Springer, 2001.   \n435 [26] Finnegan Southey, Michael Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse   \n436 Billings, and Chris Rayner. Bayes\u2019 bluff: opponent modelling in poker. In Proceedings of the   \n437 Twenty-First Conference on Uncertainty in Artificial Intelligence, pages 550\u2013558, 2005.   \n438 [27] Kevin Waugh. A fast and optimal hand isomorphism algorithm. In AAAI Workshop on Computer   \n439 Poker and Incomplete Information, 2013.   \n440 [28] Kevin Waugh, David Schnizlein, Michael Bowling, and Duane Szafron. Abstraction pathologies   \n441 in extensive games. In International Conference on Autonomous Agents and Multiagent Systems   \n442 (AAMAS), volume 2, pages 781\u2013788, 2009.   \n443 [29] Kevin Waugh, Martin Zinkevich, Michael Johanson, Morgan Kan, David Schnizlein, and   \n444 Michael Bowling. A practical use of imperfect recall. In Symposium on Abstraction, Reformu  \n445 lation and Approximation (SARA), 01 2009.   \n446 [30] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret min  \n447 imization in games with incomplete information. In International Conference on Neural   \n448 Information Processing Systems (NeurIPS), pages 1729\u20131736, 2007. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Algorithm A1 Potential Winrate Isomorphism ", "page_idx": 11}, {"type": "text", "text": "Require: $r=1,\\ldots,R$ . Phases. $\\textstyle\\theta_{i}=\\bigcup_{r=1}^{R}\\theta_{i}^{(r)}$ . Signal infoset space for player $i$ . $I n d e x_{i}(r,\\cdot):\\Theta_{i}^{(r)}\\mapsto\\mathbb{N}.$ Signal infoset index function for player $i$ . 1: procedure POTENTIALWINRATEISOMORPHISM $(\\Theta_{i})$ 2: for $r=R$ to 1 do 3: if $r==R$ then 4: FEATUREFUNC $\\leftarrow$ POTENTIALWINRATEFEATURELASTPHASE(\u00b7). 5: else 6: FEATUREFUNC $\\leftarrow$ POTENTIALWINRATEFEATURE( $\\cdot,r,\\mathcal{P C}_{i}^{(r+1)},\\mathcal{P D}_{i}^{(r+1)})$ . 7: end if 8: (PCi(r ), PDi(r )) \u2190ISOMORPHISMCONSTRUCTOR(r, $\\Theta_{i}^{(r)}$ , FEATUREFUNC). 9: end for   \n10: return $(\\mathcal{P C}_{i}^{(1)},\\mathcal{P D}_{i}^{(1)}),\\ldots,(\\mathcal{P C}_{i}^{(R)},\\mathcal{P D}_{i}^{(R)}).$   \n11: end procedure   \n12: procedure POTENTIALWINRATESFEATURELASTPHASE $(\\vartheta)$   \n13: return pf i(R $p f_{i}^{(R)}(\\vartheta)$ $\\triangleright$ compute according Equation (1)   \n14: end procedure   \n15: procedure POTENTIALWINRATEFEATUR $\\boldsymbol{:}(\\vartheta,r,\\mathcal{P C}_{i}^{(r+1)},\\mathcal{P D}_{i}^{(r+1)})$   \n16: $f e a t u r e_{\\vartheta}\\gets$ zero array with length $N+1$   \n17: for \u03d1\u2032 \u2208\u0398(r+1), such that $\\exists\\theta^{\\prime}\\in\\vartheta^{\\prime},\\exists\\theta\\in\\vartheta;$ $\\varsigma(\\theta^{\\prime}|\\theta)>0$ do   \n18: $i d x\\gets I n d e x_{i}(r+1,\\vartheta^{\\prime})$ $,a b s\\gets\\mathcal{P}\\mathcal{D}_{i}^{(r+1)}[i d x],\\,f e a t u r e_{\\vartheta^{\\prime}}\\gets\\mathcal{P}\\mathcal{C}_{i}^{(r+1)}[a b s].$   \n19: for $j=0$ to $N$ do   \n20: $f e a t u r e_{\\vartheta}[j]\\gets f e a t u r e_{\\vartheta}[j]+f e a t u r e_{\\vartheta^{\\prime}}[j]P r\\{\\vartheta^{\\prime}|\\vartheta\\}$   \n21: end for   \n22: end for   \n23: end procedure ", "page_idx": 11}, {"type": "text", "text": "449 A Algorithm Details ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "450 A.1 Potential Winrate Isomorphism ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "451 Algorithm A1 describes the computation process for potential winrate isomorphism. This algorithm   \n452 operates in reverse, starting from the game\u2019s final phase $R$ . ", "page_idx": 11}, {"type": "text", "text": "453 A.2 K-Recall Winrate Isomorphism ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "454 Algorithm A2 constructs the $\\mathbf{k}$ -recall winrate isomorphism using the $\\mathbf{k}$ -recall winrate feature. This   \n455 process requires the prior construction of the potential winrate isomorphism map $\\mathcal{P}\\mathcal{D}_{i}^{(r)}$ using   \n456 Algorithm A1. ", "page_idx": 11}, {"type": "text", "text": "457 A.3 Accelerating Distance Computing for K-Recall Winrate Features ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "458 According to Equation (5), we note that the distance calculation between a $\\boldsymbol{\\mathrm{k}}$ -recall winrate isomor  \n459 phism class and a centroid\u2019s k-recall winrate feature can be decomposed into $_{\\mathrm{k+1}}$ pairs of potential   \n460 winrate feature EMD calculations. The potential winrate feature of the hand remains unchanged,   \n461 while only the potential winrate feature of the centroid changes. Decomposing the calculation into the   \n462 EMDs of potential winrate features involves significantly fewer computations than directly calculating   \n463 the EMD of two $\\mathrm{k}$ -recall winrate features. Specifically, for the River phase of HUNL&HULHE, we   \n464 have the compression ratio as $\\begin{array}{r}{\\frac{169+1028325+1850624+2\\tilde{0}687}{529890863}=\\frac{2899805}{529890863}=0.0054725.}\\end{array}$ .   \n465 Algorithm A3 describes how we accelerate the batch EMD computation between a centroid and all   \n466 KRWI classes\u2019 $\\boldsymbol{\\mathrm{k}}$ -recall winrate features. It should be noted that the K-recall winrate feature involved   \n467 in the calculation of the centroid in the algorithm is in the form of Equation (3), while the K-recall   \n468 winrate feature in $\\mathcal{R}\\mathcal{C}^{(r,k)}$ is in the form of Equation (4). This method reduced the computational ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Algorithm A2 K-Recall Winrate Isomorphism ", "page_idx": 12}, {"type": "text", "text": "Require:   \n$r=1,\\ldots,R$ . Phases.   \n$\\Theta_{i}^{(r)}$ . Signal infoset space for player $i$ .   \n$I n d e x_{i}(r,\\cdot):\\Theta_{i}^{(r)}\\mapsto\\mathbb{N}.$ Signal infoset index function for player $i$ .   \nPDi(r): N  \u2192N. Potential winrate isomporphism map.   \n1: procedure KRECALLWINRATEISOMORPHISM $(\\Theta_{i},k)$   \n2: for $r=1$ to $R$ do   \n3: $k^{\\prime}\\gets\\mathbf{M}\\mathbf{IN}(r-1,k)$ .   \n4: FEATUREFUNC \u2190KRECALLWINRATEFEATURE $(\\cdot,\\,r,\\,k^{\\prime})$ .   \n5: $(\\mathcal{R C}_{i}^{(r,k^{\\prime})},\\mathcal{R D}_{i}^{(r,k^{\\prime})})\\leftarrow$ ISOMORPHISMCONSTRUCTOR $(r,\\theta_{i}^{(r)}$ , FEATUREFUNC). 67:: erentdu fron $(\\mathcal{R C}_{i}^{(1,0)},\\mathcal{R D}_{i}^{(1,0)}),\\ldots,(\\mathcal{R C}_{i}^{(k+1,k)},\\mathcal{R D}_{i}^{(k+1,k)}),\\ldots,(\\mathcal{R C}_{i}^{(R,k)},\\mathcal{R D}_{i}^{(R,k)}).$ 8: end procedure   \n9: procedure KRECALLWINRATESFEATURE $(\\vartheta,r,k)$   \n10: initial a empty vector feature.   \n11: for $s=r$ to $r-k$ do   \n12: $\\vartheta^{\\prime}\\leftarrow$ the predecessor signal infoset of $\\vartheta$ in the $s$ phase for player $i$ .   \n13: $i d x\\gets I n d e x_{i}(s,\\vartheta^{\\prime})$ , $i\\bar{b}s\\leftarrow\\mathcal{P}\\mathcal{D}_{i}^{(s)}[i d x]$ .   \n14: Append feature with abs.   \n15: end for   \n16: return feature   \n17: end procedure ", "page_idx": 12}, {"type": "text", "text": "469 cost of EMD from 19000 core hours to approximately 104 core hours, which is significantly lower   \n470 than the time cost of summarizing the distance for each KRWI class, which is about 524 core hours   \n471 and is an unavoidable $O(1)$ cost.   \n472 The distance batch calculation for each centroid can be processed independently and distributed   \n473 across tens of multi-core computer (e.g. 96-core computers), with each computer responsible for   \n474 calculating the features of some centroids in one iteration, which are then aggregated. Using this   \n475 technique, we can reduce an iteration to a few hours, which is acceptable for Texas Hold\u2019em AI   \n476 training. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "477 B Numerall211 Hold\u2019em Rules ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "478 Numeral211 Hold\u2019em is played according to the following rule: ", "page_idx": 12}, {"type": "text", "text": "479 1. Ante: Each player antes 5 chip into the pot at the start of the hand.   \n480 2. Hole Card: Both players are dealt one private card face down, known as the hole card.   \n481 3. Deck: The deck consists of a standard poker deck, excluding the Jokers, Kings, Queens,   \n482 and Jacks, resulting in a total of 40 cards. There are four suits: spades $(\\spadesuit)$ , hearts $(\\heartsuit)$ , clubs   \n483 $(\\clubsuit)$ , and diamonds $(\\diamond)$ , each containing ten cards numbered 2 through 9, and including the   \n484 ten (T) and ace (A).   \n485 4. First Betting Phase: Following the distribution of hole cards, a phase of betting occurs.   \n486 Players can choose to check or bet, with the bet size set at 10 chips.   \n487 5. Flop: After the initial betting phase, a single community card, termed the flop, is revealed   \n488 from the deck.   \n489 6. Second Betting Phase: Another phase of betting takes place after the flop, with the bet size   \n490 increasing to 20 chips.   \n491 7. Turn: After the Second betting phase, another community card, termed the turn, is revealed   \n492 from the deck.   \n493 8. Third Betting Phase: Another phase of betting takes place after the turn, with the bet size   \n494 still set at 20 chips. ", "page_idx": 12}, {"type": "text", "text": "Require: $r=1,\\ldots,R$ . Phases. $\\mathcal{R C}_{i}^{(r,k)}:\\mathbb{N}\\mapsto\\mathbb{N}^{k+1}.$ K-recall winrate feature set. $\\mathcal{P C}_{i}^{(r)}:\\mathbb{N}\\mapsto[0,1]^{N+1}$ . Potential winrate feature set. $\\mathcal{P}\\mathcal{D}_{i}^{(r)}:\\mathbb{N}\\mapsto\\mathbb{N}$ . Potential winrate isomporphism map. $r c=(p c^{(r)},\\cdot\\cdot\\cdot,p c^{(r-k)})$ . K-recall winrate feature of the input centroid.   \nEnsure: Distances of all k-recall winrate feature with centroid.   \n1: procedure DISTANCEBA $\\mathrm{TCH}(w_{0},\\ldots,w_{k},r c,r,k)$ Initial phase $s$ empty earth mover\u2019s distance vector $E m d D i s^{(s)}$ for $s=r,\\ldots,r-k$ . Initial empty output distance vector $D i s$ .   \n2: 3: for for pf in PCi(s)do $t=0$ to $k$ do   \n4: Append $\\dot{E}m d D i s^{(r-t)}$ with $\\operatorname{Emd}(p f,r c[t])$   \n5: end for   \n6: end for   \n7: for $r f i$ in $\\mathcal{R C}_{i}^{(r,k)}$ do   \n8: $d i s\\gets0$ .   \n190:: for $d i s\\gets d i s+w_{t}*E m d D i s^{(r-t)}[\\mathcal{P}\\mathcal{D}_{i}^{(r-t)}[r f i[t]]].$ $t=0$ $k$   \n11: end for   \n12: Append Dis with dis.   \n13: end for return Dis.   \n14: end procedure ", "page_idx": 13}, {"type": "text", "text": "9. Showdown: If neither player folds, a showdown occurs. Players reveal their cards, aiming to form the best possible hand. The player with the highest-ranked hand wins the pot. In the case of a tie, the pot is split evenly. The Table 2 show the hand ranks of Numeral211 Hold\u2019em. ", "page_idx": 13}, {"type": "text", "text": "495   \n496   \n497   \n498   \n499   \n500   \n501 ", "page_idx": 13}, {"type": "text", "text": "10. Betting Options: Throughout the game, players have options to fold, call, or raise. In each betting phase, the total sum of bets and raises is limited to a maximum of 4, with fixed bet sizes of 10 chips in the first phase and 20 chips in the last two betting phases. ", "page_idx": 13}, {"type": "text", "text": "502 C Supplementary Data for Experiment 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "503 Figure 6 show all of the result in experiment 1. ", "page_idx": 13}, {"type": "table", "img_path": "1URMG6B3WW/tmp/678a244351a42d72cf5d942a653c1b24537d4ab7fe2bd0ed08d0823fad904881.jpg", "table_caption": [], "table_footnote": [""], "page_idx": 14}, {"type": "image", "img_path": "1URMG6B3WW/tmp/2ad15347e070d6da65ff2bd3015743b385ec92cf3f0f068047923add05cc3d19.jpg", "img_caption": ["Figure 6: All data within experiment 1 "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "504 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "06 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n07 paper\u2019s contributions and scope?   \n08 Answer: [Yes]   \n09 Justification: We have clearly defined our scope and contributions in both the abstract and   \n10 introduction sections.   \n11 Guidelines:   \n12 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n13 made in the paper. ", "page_idx": 14}, {"type": "text", "text": "\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: This paper introduces a novel hand abstraction algorithm that has been experimentally validated to outperform the previous state-of-the-art (SOTA) algorithm, PaEmd, and no significant flaws have been identified thus far. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "554 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "a complete (and correct) proof?   \nAnswer: [NA]   \nJustification: This paper introduces a novel algorithm and validates its effectiveness through   \nexperiments, without involving theory or proofs.   \nGuidelines: \u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibilit ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have provided detailed information on the testbed, evaluation metrics, and experimental scenarios in Section 6. Additionally, specific experimental parameters and equipment are given in Section 7. Therefore, we have included sufficient details in the paper to reproduce the experiments. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "568   \n569   \n570   \n571   \n572   \n573   \n574   \n575   \n576   \n577   \n578   \n579   \n580   \n581   \n582   \n583   \n584   \n585   \n586   \n587   \n588   \n589   \n590   \n591   \n592   \n593   \n594   \n595   \n596   \n597   \n598   \n599   \n600   \n601   \n602   \n603   \n604   \n605   \n606   \n607   \n608   \n609   \n610   \n611   \n612   \n613   \n614   \n615   \n616   \n617   \n618   \n619   \n620   \n621 ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Justification: The experiments in this paper are time-consuming, and we utilized a large number of machines to simultaneously conduct various parts of the experiments. Currently, we do not have a ready-to-use script for one-click deployment of the experiments (the time required to run on a single computer is unacceptable). In the future, we will open-source this work and provide the code to reproduce these experiments. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: As stated in the reproducibility statement, we have provided detailed information on the testbed, evaluation metrics, and experimental scenarios in Section 6. Additionally, specific experimental parameters and equipment are given in Section 7. ", "page_idx": 17}, {"type": "text", "text": "622   \n623   \n624   \n625   \n626   \n627   \n628   \n629   \n630   \n631   \n632   \n633   \n634   \n635   \n636   \n637   \n638   \n639   \n640   \n641   \n642   \n643   \n644   \n645   \n646   \n647   \n648   \n649   \n650   \n651   \n652   \n653   \n654   \n655   \n656   \n657   \n658   \n659   \n660   \n661   \n662   \n663   \n664   \n665   \n666   \n667   \n668   \n669   \n670   \n671   \n672   \n673   \n674   \n675 ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: Due to the long experimental time and limited sample size, error bars cannot be provided. In the first experiment (Figure 4), the baseline settings adopt fixed abstraction settings and have a large performance gap, so the performance of strategies solved by CSMCCFR is stable and not easily affected by random factors. In the second experiment (Figure 5), random factors may indeed affect individual experimental data. Therefore, we sampled the control group multiple times and drew the performance range, which is far lower than the performance of our algorithm under the worst parameters, which also proves the effectiveness of the algorithm. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). ", "page_idx": 17}, {"type": "text", "text": "676 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n677 call to a library function, bootstrap, etc.)   \n678 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n679 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n680 of the mean.   \n681 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n682 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n683 of Normality of errors is not verified.   \n684 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n685 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n686 error rates).   \n687 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n688 they were calculated and reference the corresponding figures or tables in the text.   \n689 8. Experiments Compute Resources   \n690 Question: For each experiment, does the paper provide sufficient information on the com  \n691 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n692 the experiments?   \n693 Answer: [Yes]   \n694 Justification: We discuss the computational resources and time cost of the experiments in   \n695 Sections 5, 7, and Appendix A.3.   \n696 Guidelines:   \n697 \u2022 The answer NA means that the paper does not include experiments.   \n698 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n699 or cloud provider, including relevant memory and storage.   \n700 \u2022 The paper should provide the amount of compute required for each of the individual   \n701 experimental runs as well as estimate the total compute.   \n702 \u2022 The paper should disclose whether the full research project required more compute   \n703 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n704 didn\u2019t make it into the paper).   \n705 9. Code Of Ethics   \n706 Question: Does the research conducted in the paper conform, in every respect, with the   \n707 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n708 Answer: [Yes]   \n709 Justification: We carefully review the NeurIPS Code of Ethics and ensure that the research   \n710 aligns with it in all aspects.   \n711 Guidelines:   \n712 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n713 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n714 deviation from the Code of Ethics.   \n715 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n716 eration due to laws or regulations in their jurisdiction).   \n717 10. Broader Impacts   \n718 Question: Does the paper discuss both potential positive societal impacts and negative   \n719 societal impacts of the work performed?   \n720 Answer: [Yes]   \n721 Justification: We discuss the impact of this work in Section 8. As noted, this work represents   \n722 the SOTA in hand abstraction algorithms and could be used to create more powerful Texas   \n723 Hold\u2019em AI.   \n724 Guidelines:   \n725 \u2022 The answer NA means that there is no societal impact of the work performed.   \n726 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n727 impact or why the paper does not address societal impact.   \n728 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n729 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n730 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n731 groups), privacy considerations, and security considerations.   \n732 \u2022 The conference expects that many papers will be foundational research and not tied   \n733 to particular applications, let alone deployments. However, if there is a direct path to   \n734 any negative applications, the authors should point it out. For example, it is legitimate   \n735 to point out that an improvement in the quality of generative models could be used to   \n736 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n737 that a generic algorithm for optimizing neural networks could enable people to train   \n738 models that generate Deepfakes faster.   \n739 \u2022 The authors should consider possible harms that could arise when the technology is   \n740 being used as intended and functioning correctly, harms that could arise when the   \n741 technology is being used as intended but gives incorrect results, and harms following   \n742 from (intentional or unintentional) misuse of the technology.   \n743 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n744 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n745 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n746 feedback over time, improving the efficiency and accessibility of ML).   \n747 11. Safeguards   \n748 Question: Does the paper describe safeguards that have been put in place for responsible   \n749 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n750 image generators, or scraped datasets)?   \n751 Answer: [NA]   \n752 Justification: Our work focuses solely on introducing a more efficient algorithm for hand   \n753 abstraction and does not involve the release of data or models.   \n754 Guidelines:   \n755 \u2022 The answer NA means that the paper poses no such risks.   \n756 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n757 necessary safeguards to allow for controlled use of the model, for example by requiring   \n758 that users adhere to usage guidelines or restrictions to access the model or implementing   \n759 safety filters.   \n760 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n761 should describe how they avoided releasing unsafe images.   \n762 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n763 not require this, but we encourage authors to take this into account and make a best   \n764 faith effort.   \n765 12. Licenses for existing assets   \n766 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n767 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n768 properly respected?   \n769 Answer: [Yes]   \n770 Justification: This paper provides comprehensive citations for all comparative methods   \n771 involved, and all comparison experiments were re-implemented without using existing tools   \n772 or code.   \n773 Guidelines:   \n774 \u2022 The answer NA means that the paper does not use existing assets.   \n775 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n776 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n777 URL.   \n778 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n779 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n780 service of that source should be provided.   \n781 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n782 package should be provided. For popular datasets, paperswithcode.com/datasets   \n783 has curated licenses for some datasets. Their licensing guide can help determine the   \n784 license of a dataset.   \n785 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n786 the derived asset (if it has changed) should be provided.   \n787 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n788 the asset\u2019s creators.   \n789 13. New Assets   \n790 Question: Are new assets introduced in the paper well documented and is the documentation   \n791 provided alongside the assets?   \n792 Answer: [NA]   \n793 Justification: This paper does not release any new assets.   \n794 Guidelines:   \n795 \u2022 The answer NA means that the paper does not release new assets.   \n796 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n797 submissions via structured templates. This includes details about training, license,   \n798 limitations, etc.   \n799 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n800 asset is used.   \n801 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n802 create an anonymized URL or include an anonymized zip file.   \n803 14. Crowdsourcing and Research with Human Subjects   \n804 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n805 include the full text of instructions given to participants and screenshots, if applicable, as   \n806 well as details about compensation (if any)?   \n807 Answer: [NA]   \n808 Justification: This paper does not involve human subjects.   \n809 Guidelines:   \n810 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n811 human subjects.   \n812 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n813 tion of the paper involves human subjects, then as much detail as possible should be   \n814 included in the main paper.   \n815 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n816 or other labor should be paid at least the minimum wage in the country of the data   \n817 collector.   \n818 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n819 Subjects   \n820 Question: Does the paper describe potential risks incurred by study participants, whether   \n821 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n822 approvals (or an equivalent approval/review based on the requirements of your country or   \n823 institution) were obtained?   \n824 Answer: [NA]   \n825 Justification: This paper does not involve human subjects.   \n826 Guidelines:   \n827 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n828 human subjects. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]