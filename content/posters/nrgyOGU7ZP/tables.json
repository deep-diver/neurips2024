[{"figure_path": "nrgyOGU7ZP/tables/tables_7_1.jpg", "caption": "Table 1: This table presents the quality and latency of NLP and vision experiments. Overall across both the domains, SS1 gives best quality models under similar parameters and better quality per unit compute. In terms of inference latency, we see upto 1.3\u00d7 increase in throughput. Exact experimental details are present in Appendix I.", "description": "This table presents the results of NLP and vision experiments comparing the quality and latency of different models: original, SS1 (with different compression rates), Monarch (with different numbers of blocks), and LowRank (with different compression rates).  It shows that across different model sizes and tasks, SS1 generally offers better quality-efficiency tradeoffs than the compared methods, and achieves significantly faster inference throughput. Detailed experiment setups are available in the Appendix I.", "section": "5 Experiments"}, {"figure_path": "nrgyOGU7ZP/tables/tables_7_2.jpg", "caption": "Table 1: This table presents the quality and latency of NLP and vision experiments. Overall across both the domains, SS1 gives best quality models under similar parameters and better quality per unit compute. In terms of inference latency, we see upto 1.3\u00d7 increase in throughput. Exact experimental details are present in Appendix I.", "description": "This table presents the results of NLP and Vision experiments, comparing the quality and latency of models using SS1 against baselines like Monarch and LowRank.  It shows that SS1 achieves better quality-efficiency tradeoffs, and up to 1.3x faster inference throughput.", "section": "5 Experiments"}, {"figure_path": "nrgyOGU7ZP/tables/tables_7_3.jpg", "caption": "Table 2: PPL(loss) for [Left]: Applying quantization on some saved checkpoints of original and SS1 models. The effect of quantization on SS1 is similar to that on full model. [Right]: SS1 models can outperform standard models. The std-deviation is around 0.2PPL for these experiments.", "description": "This table presents the results of applying post-training quantization to both the original GPT2 and the SS1-compressed models.  The left side shows that the impact of quantization on the SS1 models is similar to its effect on the full-sized models; the quality remains largely unaffected. The right side demonstrates that SS1 models can achieve higher quality (lower perplexity) than smaller, less-parameterized versions of the original GPT2 model.", "section": "5 Experiments"}, {"figure_path": "nrgyOGU7ZP/tables/tables_8_1.jpg", "caption": "Table 3: [Left:]BERT pretrained model projected onto SS1 and finetuned on GLUE benchmark. Due to the descripancy, we report both online and our local results of full model. The SS1 model is 1.31\u00d7 faster for higher batch sizes. Details are available in I.2 [Right:] Proof of concept of applications in Llama-3-8B. We obtain 1.11\u00d7 speed up in latency by compressing selective layers, without any form of retraining or finetuning. Details of the experiment are in I.3", "description": "This table presents the results of applying Sketch Structured Transform (SST) to BERT and Llama models. The left side shows the results of finetuning a BERT model projected onto SST on the GLUE benchmark, demonstrating a 1.31x speedup in inference. The right side shows a proof-of-concept for Llama-3-8B, achieving a 1.11x speedup by compressing selective layers without finetuning.", "section": "5 Experiments"}, {"figure_path": "nrgyOGU7ZP/tables/tables_8_2.jpg", "caption": "Table 4: The time spent by 100GB sized DLRM MLperf benchmark model in various components. Specifically 70% latency is spent in Top MLP. We can reduce MLP workload by factor of 2\u00d7 by training SS1 layers without compromising quality of the model. (Statistical significance: multiple runs gives 0.8032)", "description": "This table shows the breakdown of time spent in different components of the DLRM model.  The majority (70%) of the latency is in the Top MLP. By using SS1, the MLP workload is reduced by half while maintaining the same level of accuracy. This demonstrates the performance improvement offered by SS1, specifically for CPU-bound tasks where matrix multiplications dominate the computational workload.", "section": "5.4 Improving CPU workloads e.g. DLRM MLPerf Benchmark"}, {"figure_path": "nrgyOGU7ZP/tables/tables_15_1.jpg", "caption": "Table 1: This table presents the quality and latency of NLP and vision experiments. Overall across both the domains, SS1 gives best quality models under similar parameters and better quality per unit compute. In terms of inference latency, we see upto 1.3\u00d7 increase in throughput. Exact experimental details are present in Appendix I.", "description": "This table compares the performance of SS1 against Monarch and LowRank baselines across various NLP and vision tasks.  It shows that SS1 achieves better quality-efficiency tradeoffs (quality per parameter and FLOPs) and faster inference latency (up to 1.3x speedup) compared to baselines.  The Appendix I contains more detailed experimental results.", "section": "5 Experiments"}, {"figure_path": "nrgyOGU7ZP/tables/tables_24_1.jpg", "caption": "Table 1: This table presents the quality and latency of NLP and vision experiments. Overall across both the domains, SS1 gives best quality models under similar parameters and better quality per unit compute. In terms of inference latency, we see upto 1.3\u00d7 increase in throughput. Exact experimental details are present in Appendix I.", "description": "This table shows the results of NLP and vision experiments using different models.  It compares the quality (measured by PPL for NLP and accuracy for vision) and latency of models using the proposed SS1 method against baselines (Monarch, LowRank). The table highlights that SS1 consistently achieves better quality-efficiency tradeoffs than baselines, with up to a 1.3x increase in inference throughput.  Detailed experimental settings are provided in Appendix I.", "section": "5 Experiments"}, {"figure_path": "nrgyOGU7ZP/tables/tables_26_1.jpg", "caption": "Table 8: Fine-tuning results for Bert Large", "description": "This table presents the results of fine-tuning experiments on the GLUE benchmark using the BERT-Large model and its SS1 compressed variant.  The table shows the performance on various subtasks of the GLUE benchmark, including the number of parameters used and average accuracy for both models. The SS1 model demonstrates a significant reduction in the number of parameters while maintaining a relatively high level of accuracy compared to the original BERT-Large model.", "section": "5. Experiments"}, {"figure_path": "nrgyOGU7ZP/tables/tables_26_2.jpg", "caption": "Table 9: Fine-tuning results for Bert Base\nBERT-L BERT-L \u00d7 SS1", "description": "This table presents the results of fine-tuning BERT-base and a compressed version of BERT-base (using SS1) on the GLUE benchmark.  The table shows the performance (accuracy) on several sub-tasks of GLUE, including COLA, STSB, RTE, MRPC, WNLI, QNLI, QQP, SST2, and MNLI, as well as the average accuracy across all tasks. It demonstrates that using SS1 to compress BERT-base resulted in only a minimal drop in accuracy compared to the original model.", "section": "5.2 BERT finetuning settings"}, {"figure_path": "nrgyOGU7ZP/tables/tables_26_3.jpg", "caption": "Table 1: This table presents the quality and latency of NLP and vision experiments. Overall across both the domains, SS1 gives best quality models under similar parameters and better quality per unit compute. In terms of inference latency, we see upto 1.3\u00d7 increase in throughput. Exact experimental details are present in Appendix I.", "description": "This table compares the performance of the proposed Sketch Structured Transform (SS1) method against other state-of-the-art methods (Monarch and LowRank) on various NLP and vision tasks.  It shows the number of parameters, accuracy/perplexity, and inference latency for different model sizes and compression levels for GPT and MLPMixer models. The results demonstrate that SS1 achieves better quality-efficiency trade-offs and faster inference compared to the baselines.", "section": "5 Experiments"}, {"figure_path": "nrgyOGU7ZP/tables/tables_26_4.jpg", "caption": "Table 1: This table presents the quality and latency of NLP and vision experiments. Overall across both the domains, SS1 gives best quality models under similar parameters and better quality per unit compute. In terms of inference latency, we see upto 1.3\u00d7 increase in throughput. Exact experimental details are present in Appendix I.", "description": "This table compares the performance of SS1 against other methods (Monarch, LowRank) on NLP and vision tasks.  It shows that SS1 achieves better quality-efficiency trade-offs across various model sizes and benchmarks, offering improvements in both accuracy and inference speed.", "section": "5 Experiments"}, {"figure_path": "nrgyOGU7ZP/tables/tables_26_5.jpg", "caption": "Table 1: This table presents the quality and latency of NLP and vision experiments. Overall across both the domains, SS1 gives best quality models under similar parameters and better quality per unit compute. In terms of inference latency, we see upto 1.3\u00d7 increase in throughput. Exact experimental details are present in Appendix I.", "description": "This table compares the performance of SS1 against Monarch and LowRank methods on NLP and vision tasks.  It shows that SS1 achieves better quality-efficiency tradeoffs (quality per parameter and FLOP) and faster inference latency (up to 1.3x speedup).  The table presents metrics such as perplexity, accuracy, and latency for various model sizes and configurations.", "section": "5 Experiments"}, {"figure_path": "nrgyOGU7ZP/tables/tables_26_6.jpg", "caption": "Table 12: Latency results for Bert Models", "description": "This table shows the median latency (in milliseconds) for BERT-Large and BERT-Large with SS1 compression across different batch sizes (8, 16, 32, 64, 128).  It also calculates the throughput increase achieved by using SS1 compression compared to the original BERT-Large model for each batch size. The results demonstrate the significant latency reduction and throughput improvement obtained by using SS1, especially at larger batch sizes.", "section": "5 Experiments"}, {"figure_path": "nrgyOGU7ZP/tables/tables_27_1.jpg", "caption": "Table 9: Llama Projection Results", "description": "This table presents the evaluation results of the compressed Llama model on the full MMLU and Winogrande datasets.  It shows the number of parameters (#param), accuracy on the MMLU and Winogrande benchmarks, and the speedup achieved by using SS1 compression.  The results demonstrate that SS1 compression achieves a significant reduction in model size while maintaining comparable performance, showing a 1.1x speedup.", "section": "5 Experiments"}, {"figure_path": "nrgyOGU7ZP/tables/tables_27_2.jpg", "caption": "Table 1: This table presents the quality and latency of NLP and vision experiments. Overall across both the domains, SS1 gives best quality models under similar parameters and better quality per unit compute. In terms of inference latency, we see upto 1.3\u00d7 increase in throughput. Exact experimental details are present in Appendix I.", "description": "This table compares the performance of SS1 against Monarch and LowRank across various NLP and vision tasks.  It shows that SS1 achieves better quality-efficiency tradeoffs in terms of both accuracy/perplexity and inference latency, often significantly outperforming the baselines while using fewer parameters.  Appendix I contains more detailed experimental results.", "section": "5 Experiments"}]