[{"figure_path": "MihOCXte41/tables/tables_7_1.jpg", "caption": "Table 1: The comparison with existing SOTA methods on class-conditional image generation without classifier-free guidance on ImageNet 256x256. We report the training speed (T-speed), inference speed (I-speed), and memory consumption (Mem.) of inference. The EDT* denotes the EDT without our proposed token relation-enhanced masking training strategy.", "description": "This table compares the performance of the proposed EDT model with other state-of-the-art (SOTA) methods for class-conditional image generation on the ImageNet dataset (256x256 resolution) without using classifier-free guidance.  The metrics shown include training speed, inference speed, memory usage during inference, and the Fr\u00e9chet Inception Distance (FID), a common metric to assess the quality of generated images.  A version of EDT without the proposed masking training strategy is included for comparison.", "section": "3.2 Comparison with SOTA transformer-based diffusion methods"}, {"figure_path": "MihOCXte41/tables/tables_7_2.jpg", "caption": "Table 2: The comparison with existing transformer-based models on class-conditional image generation without classifier-free guidance on ImageNet 512x512.", "description": "This table compares the performance of EDT-S, DiT-S, and MDTv2-S models on ImageNet 512x512 dataset.  The metrics used for comparison include training speed (iterations per second), GFLOPs (a measure of computational cost), FID (Frechet Inception Distance, a measure of image quality), Inception Score (IS, another measure of image quality), and sFID (a variant of FID).  Lower FID and sFID scores, and higher IS scores indicate better performance. The results show that EDT-S achieves a better balance between speed and quality, compared to the other models.", "section": "3.2 Comparison with SOTA transformer-based diffusion methods"}, {"figure_path": "MihOCXte41/tables/tables_8_1.jpg", "caption": "Table 3: Results on various models with (w) AMM and without (w/o) AMM. These models are trained for 400k iterations by default. We evaluate models using FID scores.", "description": "This table presents the FID scores for various models, comparing their performance with and without the Attention Modulation Matrix (AMM).  The models were all trained for 400,000 iterations.  Lower FID scores indicate better image generation quality.", "section": "3.3 Ablation Study"}, {"figure_path": "MihOCXte41/tables/tables_9_1.jpg", "caption": "Table 4: The ablation study of the key components of the lightweight-design and masking training strategy of EDT. The experiment is conducted on the small-size EDT model (W/o AMM).", "description": "This table presents the ablation study results focusing on the key components of the lightweight-design and masking training strategy within the Efficient Diffusion Transformer (EDT) framework.  The study uses a small-size EDT model without the Attention Modulation Matrix (AMM). It assesses the impact of token information enhancement (TIE), positional encoding supplement (PES), and two different masking training strategies (MDT and EDT) on the model's performance, measured by FID and IS scores.  Each row represents a different model configuration, indicating the presence or absence of these components with checkmarks (\u2713) and crosses (\u2717). The results showcase how these components individually and collectively affect the model's image generation quality.", "section": "3.3 Ablation Study"}, {"figure_path": "MihOCXte41/tables/tables_14_1.jpg", "caption": "Table 5: The model details of EDT across three different sizes.", "description": "This table presents the architecture details for three different sizes of the Efficient Diffusion Transformer (EDT) model: small (EDT-S), base (EDT-B), and extra-large (EDT-XL).  For each model size, the table shows the total number of parameters (Params.), the total number of blocks, the number of blocks in each of the five stages (Down-sampling phase has three stages, Up-sampling phase has two stages), the dimensions of the feature maps at each stage, and the number of attention heads used at each stage. This information provides a detailed comparison of the model's complexity and capacity across different sizes.", "section": "3.1 Implementation Details"}, {"figure_path": "MihOCXte41/tables/tables_15_1.jpg", "caption": "Table 6: FLOPs in a DiT block", "description": "This table details the computational cost (FLOPs) and the number of parameters for each operation within a DiT (Diffusion Transformer) block.  It breaks down the calculations for AdaLN (Adaptive Layer Normalization), Attention (including key, query, value, and attention operations), and FFN (Feed-Forward Network) layers.  The table provides a granular view of the computational complexity at each stage of the DiT block architecture.", "section": "A.2.1 Applicable scenarios of the conventional down-sampling module"}, {"figure_path": "MihOCXte41/tables/tables_17_1.jpg", "caption": "Table 7: Training cost of EDT, MDTv2, and DiT on ImageNet", "description": "This table compares the training costs of three different models (EDT, MDTv2, and DiT) on the ImageNet dataset.  The comparison is done for three different sizes of each model (small, base, and extra-large) and two resolutions (256x256 and 512x512).  The metrics presented are the number of epochs, the total cost (measured in number of images), the GPU days used for training, the number of GFLOPs, and the final FID (Frechet Inception Distance) score.  The table showcases EDT's efficiency by showing significantly lower training costs and faster training times (fewer GPU days) compared to DiT and MDTv2 while achieving comparable or better FID scores.", "section": "3 Experiment"}, {"figure_path": "MihOCXte41/tables/tables_18_1.jpg", "caption": "Table 8: Comparison of adding AMM into EDT-S* during training versus inference on ImageNet 256 \u00d7 256.", "description": "This table compares the FID and IS scores of EDT-S* model with three different training and inference settings: \n- Model A: No AMM is used in training or inference.\n- Model B: AMM is used in both training and inference.\n- Model C: AMM is only used during inference.\nThe results show that adding AMM only during inference significantly improves the performance of EDT-S* model. ", "section": "3.3 Ablation Study"}, {"figure_path": "MihOCXte41/tables/tables_18_2.jpg", "caption": "Table 9: Performance of EDT-S* with varying insertion points of AMM on ImageNet 256 \u00d7 256.", "description": "This table presents the results of an ablation study on the EDT-S* model, investigating the impact of different placement strategies for the Attention Modulation Matrix (AMM) on image generation performance.  It compares FID and IS scores across four model variations (A, B, C, D), each differing in where AMM is integrated (encoder, decoder, or alternately in both) during the up-sampling process. This experiment explores how the alternation of global and local attention influences the quality of generated images.", "section": "3.3 Ablation Study"}, {"figure_path": "MihOCXte41/tables/tables_19_1.jpg", "caption": "Table 10: Exploring the value of the effective radius of local attention in EDT-S* for 256 \u00d7 256 resolution.", "description": "This table presents the results of an experiment to determine the optimal value for the hyperparameter 'effective radius of local attention (R)' in the Attention Modulation Matrix (AMM) of the EDT-S* model. Different values of R were tested, and the table shows the resulting FID50K, IS, sFID, Precision, and Recall scores.  The results indicate that a value of R = \u221a(N-1)\u00b2+4 provides the best balance among these metrics.", "section": "3.3 Ablation Study"}, {"figure_path": "MihOCXte41/tables/tables_21_1.jpg", "caption": "Table 11: Mask Ratio in the first down-sampling module.", "description": "This table presents the results of experiments conducted to determine the optimal mask ratio for the first down-sampling module in the EDT model.  Different mask ratios (0.1-0.2, 0.2-0.3, 0.3-0.4, 0.4-0.5, and 0.5-0.6) were tested and evaluated based on FID50k, Inception Score (IS), sFID, Precision, and Recall. The results suggest that a mask ratio of 0.3-0.4 yields the best performance, with the lowest FID50k and sFID scores and high IS, Precision, and Recall.", "section": "3.3 Ablation Study"}, {"figure_path": "MihOCXte41/tables/tables_21_2.jpg", "caption": "Table 12: Mask Ratio in the second down-sampling module. (Based on the 0.4 ~ 0.5 mask ratio in the first down-sampling module)", "description": "This table shows the results of experiments to determine the optimal mask ratio for the second down-sampling module in the EDT model.  The experiments were conducted using different mask ratios (0.1~0.2, 0.2~0.3, 0.3~0.4, 0.4~0.5), and the results are evaluated based on FID50K, IS, sFID, Precision, and Recall. The mask ratio of the first down-sampling module was fixed at 0.4 ~ 0.5 based on previous experiments.", "section": "3.3 Ablation Study"}, {"figure_path": "MihOCXte41/tables/tables_21_3.jpg", "caption": "Table 13: The comparison with existing SOTA methods on class-conditional image generation with classifier-free guidance on ImageNet 256x256 (CFG=2 in EDT; according to DiT and MDTv2, their optimal CFG settings are 1.5 and 3.8, respectively).", "description": "This table compares the performance of EDT against other state-of-the-art (SOTA) methods for class-conditional image generation using classifier-free guidance.  It shows that EDT achieves a good balance between training cost, inference speed (GFLOPs), and image generation performance (FID). The comparison includes various model sizes and training iteration counts.  The classifier-free guidance setting is noted for each model.", "section": "3.2 Comparison with SOTA transformer-based diffusion methods"}, {"figure_path": "MihOCXte41/tables/tables_22_1.jpg", "caption": "Table 14: FID of EDTs* under different iterations on Imagenet 256 \u00d7 256.", "description": "This table shows the FID (Frechet Inception Distance) scores for three different sizes of the EDT model (EDT-S*, EDT-B*, EDT-XL*) with and without the Attention Modulation Matrix (AMM) at different training iterations (50k, 100k, ..., 400k).  It demonstrates how the FID score improves with the addition of AMM as the training progresses for all three model sizes.", "section": "3.3 Ablation Study"}, {"figure_path": "MihOCXte41/tables/tables_22_2.jpg", "caption": "Table 1: The comparison with existing SOTA methods on class-conditional image generation without classifier-free guidance on ImageNet 256x256. We report the training speed (T-speed), inference speed (I-speed), and memory consumption (Mem.) of inference. The EDT* denotes the EDT without our proposed token relation-enhanced masking training strategy.", "description": "This table compares the performance of the proposed EDT model with several state-of-the-art (SOTA) models on ImageNet 256x256 dataset for class-conditional image generation without classifier-free guidance.  Key metrics include FID (lower is better), training speed, inference speed, and memory usage.  The table highlights EDT's superior performance and efficiency compared to existing methods, particularly showing significant speed improvements in both training and inference.", "section": "3.2 Comparison with SOTA transformer-based diffusion methods"}]