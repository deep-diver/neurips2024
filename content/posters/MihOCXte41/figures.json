[{"figure_path": "MihOCXte41/figures/figures_0_1.jpg", "caption": "Figure 1: Illustration of the alternation process of local and global attention during sketching.", "description": "This figure illustrates the process of sketching a tree in a landscape. The process starts with a global view of the scene and a rough outline of the tree. Then, the focus shifts to local details, such as the branches and leaves of the tree. After refining the local details, the focus shifts back to the global view, where the tree is evaluated in the context of the whole scene. This process repeats until the sketch is complete. The figure is intended to illustrate how human-like sketching incorporates both local and global attention, which inspired the Attention Modulation Matrix (AMM) in the EDT framework.", "section": "1 Introduction"}, {"figure_path": "MihOCXte41/figures/figures_3_1.jpg", "caption": "Figure 2: The architecture of lightweight-design diffusion transformer.", "description": "This figure illustrates the architecture of the lightweight-design diffusion transformer.  It shows the process of the model, starting with a latent image which is then noised.  The model then uses a series of EDT (Efficient Diffusion Transformer) stages, alternating between downsampling and upsampling. Downsampling is described as an encoding process, while upsampling is a decoding process. The downsampling phases use masking training, and the upsampling phase leverages an Attention Modulation Matrix.  Long skip connections are employed between the stages. Finally, the model output is a denoised latent that is then decoded back into an image using a VAE (Variational Autoencoder).", "section": "2.2 Lightweight-design diffusion transformer"}, {"figure_path": "MihOCXte41/figures/figures_3_2.jpg", "caption": "Figure 3: The design of down-sampling, long skip connection and up-sampling modules.", "description": "This figure shows the detailed architecture of the down-sampling, long skip connection, and up-sampling modules in EDT.  The down-sampling module reduces the number of tokens while enhancing key features using AdaLN and positional encoding. The long skip connection module concatenates information from earlier stages. The up-sampling module increases the number of tokens and incorporates positional encoding.  These modules are designed to balance computational efficiency and information preservation in the EDT framework.", "section": "2.2 Lightweight-design diffusion transformer"}, {"figure_path": "MihOCXte41/figures/figures_4_1.jpg", "caption": "Figure 4: The position of Attention Modulation Matrix (local attention) in an EDT stage in the up-sampling phase.", "description": "This figure shows how the Attention Modulation Matrix (AMM) is integrated into an EDT stage during the up-sampling phase.  It illustrates the alternation between global and local attention, mimicking the human-like sketching process.  Specifically, it depicts an EDT stage with AMM integrated into the self-attention module of some EDT blocks, while others remain without AMM.  This alternation allows for a coarse-to-fine refinement of image details, starting with a general framework from global attention and then refining local details with local attention provided by the AMM.", "section": "2.3 Integrating local attention into the up-sampling phase of EDT"}, {"figure_path": "MihOCXte41/figures/figures_6_1.jpg", "caption": "Figure 5: Token relation-enhanced masking training strategy. MDT is fed the remained tokens after token masking into models. EDT is fed full tokens into shallow EDT blocks, and the operation of token masking is performed in down-sampling modules.", "description": "This figure compares two masking training strategies: MDT and EDT.  MDT masks tokens randomly at the beginning of the training process, which may lead to loss of token information and an unbalanced focus on reconstructing masked regions. The EDT strategy, however, feeds full tokens into the shallow EDT blocks before introducing the masking operation in the down-sampling modules. This approach allows the model to learn relationships between tokens before masking, thereby enhancing the training process and reducing information loss.  The masking operation itself is postponed to the down-sampling phase, making the training process more balanced.", "section": "2.4 Token relation-enhanced masking training strategy"}, {"figure_path": "MihOCXte41/figures/figures_8_1.jpg", "caption": "Figure 6: EDT-XL with AMM achieves more realistic visual effects. Area A: There are some blue stains on the panda's arm. Area B: An unreasonable gray area. Area C: Black smoke in the red fog. Area D: Unrealistic eyes of the fox. Area E: Fish with an odd shape. The parrot image generated by EDT-XL without AMM is realistic. And the parrot image generated by EDT-XL with AMM remains equally realistic. The add of AMM does not negatively affect the original quality.", "description": "This figure shows a comparison of images generated by EDT-XL with and without the Attention Modulation Matrix (AMM). The images on the left are generated without AMM, while those on the right are generated with AMM.  The red boxes highlight areas where the images generated without AMM show unrealistic or unnatural visual details. The images generated with AMM show improved realism in those areas, highlighting the effectiveness of AMM in improving image quality without negatively impacting overall realism.", "section": "3.3.1 Attention Modulation Matrix"}, {"figure_path": "MihOCXte41/figures/figures_15_1.jpg", "caption": "Figure 2: The architecture of lightweight-design diffusion transformer.", "description": "This figure illustrates the architecture of the lightweight diffusion transformer used in the Efficient Diffusion Transformer (EDT) framework.  The model employs a down-sampling phase (encoding) with three EDT stages, progressively compressing tokens. This is followed by an up-sampling phase (decoding) with two EDT stages, gradually reconstructing tokens. These five stages are interconnected via down-sampling, up-sampling, and long skip connection modules. Each EDT stage consists of multiple consecutive transformer blocks. The figure visually explains the process and flow of information within the EDT model architecture, highlighting the key components and their roles in achieving efficient image synthesis.", "section": "2.2 Lightweight-design diffusion transformer"}, {"figure_path": "MihOCXte41/figures/figures_17_1.jpg", "caption": "Figure 8: The process of modulating the attention score matrix and the changes in tensor shape.", "description": "The figure illustrates the step-by-step process of how the Attention Modulation Matrix (AMM) modulates the attention score matrix.  It begins with the calculation of Euclidean distances between all pairs of tokens, resulting in a token distance matrix.  A modulation matrix is then generated based on these distances, and this matrix is expanded to match the dimensions of the attention score matrix. Finally, a Hadamard product is performed between the expanded modulation matrix and the attention score matrix, resulting in a modulated attention score matrix. The figure clearly shows the shapes and dimensions of the tensors at each stage of the process.", "section": "2.3.2 Attention modulation matrix"}, {"figure_path": "MihOCXte41/figures/figures_19_1.jpg", "caption": "Figure 9: Different arrangement forms of AMM in EDT and their corresponding FID scores.", "description": "This figure shows four different ways of integrating the Attention Modulation Matrix (AMM) into the Efficient Diffusion Transformer (EDT) architecture and their resulting FID (Frechet Inception Distance) scores.  The variations involve placing AMM in different positions within the encoder and decoder blocks of the EDT. The FID scores, a measure of image generation quality, demonstrate that the arrangement of AMM significantly impacts performance.", "section": "3.3 Ablation Study"}, {"figure_path": "MihOCXte41/figures/figures_20_1.jpg", "caption": "Figure 10: Comparing the loss changes of different masking training strategies.", "description": "This figure compares the loss curves for the masking training strategies of MDT and EDT. The top two subfigures show the loss changes for MDT across two different training stages (300k-305k and 300k-400k iterations).  It highlights a conflict between the masked input loss (red line) and full input loss (green line) for MDT. When one loss decreases, the other increases, indicating conflicting training objectives. The bottom two subfigures show the corresponding loss changes for EDT. The loss curves are more synchronized, with both loss values decreasing during the training stages, demonstrating a more consistent and harmonious training process compared to MDT. ", "section": "A.4.1 Analysis of the loss of EDT and MDT"}, {"figure_path": "MihOCXte41/figures/figures_22_1.jpg", "caption": "Figure 6: EDT-XL with AMM achieves more realistic visual effects. Area A: There are some blue stains on the panda's arm. Area B: An unreasonable gray area. Area C: Black smoke in the red fog. Area D: Unrealistic eyes of the fox. Area E: Fish with an odd shape. The parrot image generated by EDT-XL without AMM is realistic. And the parrot image generated by EDT-XL with AMM remains equally realistic. The add of AMM does not negatively affect the original quality.", "description": "This figure compares image generation results of EDT-XL model with and without Attention Modulation Matrix (AMM).  It highlights how AMM improves the realism of generated images by addressing artifacts like unnatural colors, shapes, or missing details, while maintaining the overall quality of realistic images. The red boxes pinpoint specific areas where AMM made improvements.", "section": "3.3.1 Attention Modulation Matrix"}, {"figure_path": "MihOCXte41/figures/figures_23_1.jpg", "caption": "Figure 2: The architecture of lightweight-design diffusion transformer.", "description": "This figure shows the architecture of the lightweight diffusion transformer. The model includes three EDT stages in the down-sampling phase, viewed as an encoding process where tokens are progressively compressed, and two EDT stages in the up-sampling phase, viewed as a decoding process where tokens are gradually reconstructed. These five EDT stages are interconnected through down-sampling, up-sampling, and long skip connection modules.", "section": "2.2 Lightweight-design diffusion transformer"}]