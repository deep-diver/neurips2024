[{"type": "text", "text": "Effective Exploration Based on the Structural Information Principles ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Xianghua Zeng1, Hao Peng1, Angsheng Li1,2   \n1 State Key Laboratory of Software Development Environment, Beihang University, Beijing, China 2 Zhongguancun Laboratory, Beijing, China   \n{zengxianghua, penghao, angsheng}@buaa.edu.cn, liangsheng@gmail.zgclab.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Traditional information theory provides a valuable foundation for Reinforcement Learning (RL), particularly through representation learning and entropy maximization for agent exploration. However, existing methods primarily concentrate on modeling the uncertainty associated with RL\u2019s random variables, neglecting the inherent structure within the state and action spaces. In this paper, we propose a novel Structural Information principles-based Effective Exploration framework, namely SI2E. Structural mutual information between two variables is defined to address the single-variable limitation in structural information, and an innovative embedding principle is presented to capture dynamics-relevant state-action representations. The SI2E analyzes value differences in the agent\u2019s policy between state-action pairs and minimizes structural entropy to derive the hierarchical state-action structure, referred to as the encoding tree. Under this tree structure, value-conditional structural entropy is defined and maximized to design an intrinsic reward mechanism that avoids redundant transitions and promotes enhanced coverage in the state-action space. Theoretical connections are established between SI2E and classical information-theoretic methodologies, highlighting our framework\u2019s rationality and advantage. Comprehensive evaluations in the MiniGrid, MetaWorld, and DeepMind Control Suite benchmarks demonstrate that SI2E significantly outperforms state-of-the-art exploration baselines regarding final performance and sample efficiency, with maximum improvements of $\\bar{37}.63\\bar{\\%}$ and $\\bar{6}0.25\\%$ , respectively. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement Learning (RL) has emerged as a pivotal technique for addressing sequential decisionmaking problems, including game intelligence [Vinyals et al., 2019, Badia et al., 2020], robotic control [Andrychowicz et al., 2017, Liu and Abbeel, 2021], and autonomous driving [Prathiba et al., 2021, P\u00e9rez-Gil et al., 2022]. In the realm of RL, striking a balance between exploration and exploitation is crucial for optimizing agent policies and mitigating the risk of suboptimal outcomes, especially in scenarios characterized by high dimensions and sparse rewards [Zhang et al., 2021b]. ", "page_idx": 0}, {"type": "text", "text": "Recently, advancements in information-theoretic approaches have shown promise for exploration in self-supervised settings. The maximum entropy framework over the action space [Haarnoja et al., 2017] has led to the development of robust algorithms such as Soft Q-learning [Nachum et al., 2017], SAC [Haarnoja et al., 2018], and MPO [Abdolmaleki et al., 2018]. Additionally, various objectives focused on maximizing state entropy are utilized to ensure comprehensive state coverage [Hazan et al., 2019, Islam et al., 2019]. To facilitate the exploration of complex state-action pairs, MaxRenyi optimizes R\u00e9nyi entropy across the state-action space [Zhang et al., 2021a]. However, a prevalent issue with entropy maximization strategies is their tendency to bias exploration towards low-value states, making them vulnerable to imbalanced state-value distributions in supervised settings. To mitigate this, value-conditional state entropy is introduced to compute intrinsic rewards based on the estimated values of visited states [Kim et al., 2023]. Due to their instability in noisy and high-dimensional environments, a Dynamic Bottleneck (DB) [Bai et al., 2021] is developed based on the Information Bottleneck (IB) principle [Tishby et al., 2000], thereby obtaining dynamics-relevant representations of state-action pairs. Despite their successes, existing information-theoretic exploration methods have a critical limitation: they often overlook the inherent structure within state and action spaces. This oversight necessitates new approaches to enhance exploration effectiveness. ", "page_idx": 0}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/a80b9e8449518268c99a0f7b3f196280d0df650ad233828b884fa68eeefc873d.jpg", "img_caption": ["Figure 1: By incorporating the inherent state-action structure, we simplify the original six-state Markov Decision Process (MDP) with four actions to a five-state MDP with two actions, effectively reducing the size of state-action space from $24(6\\times4)$ to $10(5\\times2)$ . Here, $s_{2}^{\\prime}$ and $a_{0}^{\\prime}$ represent vertex communities $\\{s_{2},s_{5}\\}$ and $\\left\\{\\bar{a}_{0},a_{1}\\right\\}$ , respectively. In this scenario, a policy maximizing stateaction Shannon entropy would encompass all possible transitions (blue color). In contrast, a policy maximizing structural entropy would selectively focus on crucial transitions (red color), avoiding redundant transitions between $s_{2}$ and $s_{5}$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Figure 1 illustrates a simple six-state Markov Decision Process (MDP) with four actions. The different densities of the blue and red lines represent different actions, as indicated in the legend, leading to state transitions aimed at optimizing the return to the initial state $s_{0}$ . Solid lines specifically denote actions $a_{0}$ and $a_{1}$ . The transitions between states $s_{2}$ and $s_{5}$ are deemed redundant as they do not facilitate the primary objective of efficiently returning to $s_{0}$ . Therefore, the state-action pairs $(s_{2},a_{3})$ and $(s_{5},a_{3})$ have lower policy values. A policy maximizing state-action Shannon entropy would encompass all possible transitions (blue color). In contrast, a policy incorporating the inherent state-action structure will divide these redundant state-action pairs into a vertex sub-community and minimize the entropy of this sub-community to avoid visiting it unnecessarily. Simultaneously, it maximizes state-action entropy, resulting in maximal coverage for transitions (red color) that are more likely to contribute to the desired outcome in the simplified five-state MDP. ", "page_idx": 1}, {"type": "text", "text": "Departing from traditional information theory applied to random variables, structural information [Li and Pan, 2016] has been devised to quantify dynamic uncertainty within complex graphs under a hierarchical partitioning structure known as an \u201cencoding tree\". Structural entropy is conceptualized as the minimum number of bits required to encode a vertex accessible through a single-step random walk and is minimized to optimize the encoding tree. However, this definition is limited to singlevariable graphs and cannot capture the structural relationship between two variables. While the underlying graph can encompass multiple variables, current structural information principles are limited in treating these variables as a single joint variable, measuring only its structural entropy. This limitation prevents the effective quantification of structural similarity between variables, inherently imposing a single-variable constraint. Prior research on reinforcement learning using structural information principles [Zeng et al., 2023b,c] has focused on independently modeling state or action variables without simultaneously considering state-action representations. ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose SI2E, a novel and unified framework grounded in structural information principles for effective exploration within high-dimensional and sparse-reward environments. Initially, we embed state-action pairs into a low-dimensional space and present an innovative representation learning principle to capture dynamics-relevant information and compress dynamics-irrelevant information. Then, we strategically increase the state-action pairs\u2019 structural mutual information with subsequent states while decreasing it with current states. We analyze value differences among state-action representations to form a complete graph and minimize its structural entropy to derive the optimal encoding tree, thereby unveiling the hierarchical community structure of state-action pairs. By leveraging this identified structure, we design an intrinsic reward mechanism tailored to avoid redundant transitions and enhance maximal coverage in exploring the state-action space. Furthermore, we establish theoretical connections between our framework and classical information-theoretic methodologies, highlighting the rationality and advantage of SI2E. Our thorough evaluations across diverse and challenging tasks in the MiniGrid, MetaWorld, and DeepMind Control Suite benchmarks have consistently shown SI2E\u2019s superiority, with significant improvements in final performance and sample efficiency, surpassing state-of-the-art exploration baselines. For further research, the source code is available at 1. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 A novel framework based on structural information principles, SI2E, is proposed for effective exploration in high-dimensional RL environments with sparse rewards.   \n\u2022 An innovative principle of structural mutual information is introduced to overcome the singlevariable constraint inherent in existing structural information and to enhance the acquisition of dynamics-relevant representations for state-action pairs.   \n\u2022 A unique intrinsic reward mechanism that maximizes the value-conditional structural entropy is designed to avoid redundant transitions and promote enhanced coverage in the state-action space.   \n\u2022 Our experiments on various challenging tasks demonstrate that SI2E significantly improves final performance and sample efficiency by up to $37.63\\%$ and $60.25\\%$ , respectively, compared to state-of-the-art baselines. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formalize the definitions of fundamental concepts. The descriptions of primary notations are summarized in Appendix A.1 for ease of reference. ", "page_idx": 2}, {"type": "text", "text": "2.1 Traditional Information Principles ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider the random variable pair $Z\\,=\\,(X,Y)$ with a joint distribution probability denoted by $p(x,y)\\,\\in\\,(0,1)$ . The marginal probabilities, $p(x)$ and $p(y)$ , are defined as $\\begin{array}{r}{p(x)\\,\\stackrel{!}{=}\\,\\sum_{y}p(x,\\dot{y})}\\end{array}$ and $\\begin{array}{r}{p(y)\\,=\\,\\sum_{x}p(x,y)}\\end{array}$ , respectively. The joint Shannon entropy [Shannon, 1953] of $X$ and $Y$ is $\\begin{array}{r}{H(X,Y)\\,=\\,\\stackrel{\\_}{-}\\sum_{(x,y)}\\left[p(x,y)\\cdot\\log p(x,y)\\right]}\\end{array}$ , which quantifies the total uncertainty in $Z$ . Conversely, the marginal entropies $\\begin{array}{r}{H(X)=-\\sum_{x}\\left[p(x)\\cdot\\log p(x)\\right]}\\end{array}$ and $\\begin{array}{r}{H(Y)=-\\sum_{y}\\left[p(y)\\cdot\\log p(y)\\right]}\\end{array}$ characterize the uncertainty in $X$ and $Y$ individually. The mutual information $I(X;Y)\\;=\\;$ $\\begin{array}{r}{\\sum_{x,y}\\left[p(x,y)\\cdot\\log\\frac{p(x,y)}{p(x)p(y)}\\right]}\\end{array}$ quantifies the shared uncertainty between $X$ and $Y$ . It satisfies the following relationship: $I(\\bar{X Y_{}Y_{}})=H(X)+H(Y)-H(X,Y).$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Reinforcement Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Within the context of RL, the sequential decision-making problem is formalized as a Markov Decision Process (MDP) [Bellman, 1957]. The MDP is characterized by a tuple $(\\mathcal{O},\\mathcal{A},\\mathcal{P},\\mathcal{R}^{e},\\gamma)$ , where $\\scriptscriptstyle\\mathcal{O}$ denotes the observation space, $\\boldsymbol{\\mathcal{A}}$ the action space, $\\mathcal{P}$ the environmental transition function, $\\mathcal{R}^{e}$ the extrinsic reward function, and $\\gamma\\in[0,1)$ the discount factor. At each discrete timestep $t$ , the agent selects an action $a_{t}\\,\\in\\,A$ upon observing $o_{t}\\,\\in\\,\\mathcal{O}$ . This leads to a transition to a new observation $\\sigma_{t+1}\\,\\sim\\,\\mathcal{P}(o_{t},a_{t})$ and a reward $r_{t}^{e}\\,\\in\\,\\mathbb{R}$ . The policy network $\\pi$ is optimized to maximize the cumulative long-term expected discounted reward. ", "page_idx": 2}, {"type": "text", "text": "Maximum State Entropy Exploration. In environments with sparse rewards, agents are encouraged to explore the state space extensively, which can be incentivized by maximizing the Shannon entropy $H(S)$ of state variable $S$ . When the prior distribution $p(s)$ is not available, the non-parametric $k$ -nearest neighbors ( $k$ -NN) entropy estimator [Singh et al., 2003] is employed. For a given set of $n$ independent and identically distributed samples from a $d_{x}$ -dimensional space $\\{x_{i}\\}_{i=0}^{n-\\bar{1}}$ , the entropy of variable $X$ is estimated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{H}_{K L}(X)=\\frac{d_{x}}{n}\\sum_{i=0}^{n-1}\\log d(x_{i})+C,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $d(x_{i})$ is twice the distance from $x_{i}$ to its $k$ -th nearest neighbor, and $C$ is a constant term. ", "page_idx": 3}, {"type": "text", "text": "Information Bottleneck Principle. In the supervised learning paradigm, representation learning aims to transform an input source $X$ into a representation $Z$ , targeted towards an output source $Y$ . The Information Bottleneck (IB) principle [Tishby et al., 2000] refines this process by maximizing the mutual information $I(Z;Y)$ between $Z$ and $Y$ , capturing the relevant features of $Y$ within $Z$ . Concurrently, the IB principle imposes a complexity constraint by minimizing the mutual information $I(Z;X)$ between $Z$ and $X$ , effectively discarding irrelevant features. To balance these objectives, the IB principle utilizes a Lagrangian multiplier, facilitating a balanced trade-off between the richness of the representation and its complexity. ", "page_idx": 3}, {"type": "text", "text": "2.3 Structural Information Principles ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The encoding tree $T$ of an undirected and weighted graph $G=(V,E)$ is characterized as a rooted tree with the following properties: 1) Each tree node $\\alpha$ in $T$ corresponds to a subset of graph vertices $V_{\\alpha}\\subseteq V$ . 2) The subset $V_{\\lambda}$ of tree root $\\lambda$ encompasses all vertices in $V$ . 3) Each subset $V_{\\nu}$ of a leaf node $\\nu$ in $T$ only contains a single vertex $v$ , thus $V_{\\nu}=\\{v\\}$ . 4) For each non-leaf node $\\alpha$ , the number of its children is assumed as $l_{\\alpha}$ , with the $i$ -th child specified as $\\alpha_{i}$ . The collection of subsets $V_{\\alpha_{1}},\\ldots,V_{\\alpha_{l_{\\alpha}}}$ constitutes a sub-partition of $V_{\\alpha}$ . ", "page_idx": 3}, {"type": "text", "text": "Given an encoding tree $T$ whose height is at most $K$ , the $K$ -dimensional structural entropy of graph $G$ is defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nH^{T}(G)=-\\sum_{\\alpha\\in T,\\alpha\\neq\\lambda}\\left[{\\frac{g_{\\alpha}}{\\operatorname{vol}(G)}}\\cdot\\log{\\frac{\\operatorname{vol}(\\alpha)}{\\operatorname{vol}(\\alpha^{-})}}\\right],\\;\\;\\;H^{K}(G)=\\operatorname*{min}_{T}H^{T}(G),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g_{\\alpha}$ is the weighted sum of all edges connecting vertices within the subset $V_{\\alpha}$ to vertices outside the subset $V_{\\alpha}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Structural Mutual Information ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we address the single-variable constraint prevalent in existing structural information principles and introduce the concept of structural mutual information for subsequent state-action representation learning within our SI2E framework. ", "page_idx": 3}, {"type": "text", "text": "Given the random variable pair $(X,Y)$ with $|X|=|Y|=n$ , we construct an undirected bipartite graph $G_{x y}$ to represent the joint distribution of $X$ and $Y$ . In $G_{x y}$ , each vertex $x\\in X$ connects to each vertex $y\\in Y$ via weighted edges, where the weight of each edge equals the joint probability $p(x,y)$ . Notably, no edges connect vertices within the same set, $X$ or $Y$ , and the total sum of the edge weights is 1, $\\begin{array}{r}{\\sum_{x,y}\\bar{p}(x,y)=1}\\end{array}$ . Each single-step random walk in $G_{x y}$ accesses either a vertex from $X$ or $Y$ . The structural entropy of variable $X$ in $G_{x y}$ is defined as the number of bits required to encode all accessible vertices in the set $X$ . It is calculated using the following formula: ", "page_idx": 3}, {"type": "equation", "text": "$$\nH^{S I}(X)=-\\sum_{x\\in X}\\bigg[\\frac{p(x)}{\\mathrm{vol}(G_{x y})}\\cdot\\log\\frac{p(x)}{\\mathrm{vol}(G_{x y})}\\bigg]=-\\sum_{x\\in X}\\bigg[\\frac{p(x)}{2}\\cdot\\log\\frac{p(x)}{2}\\bigg],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the sum of all vertex degrees is twice the total sum of edge weights, resulting in $\\mathrm{vol}(G_{x y})=2$ ", "page_idx": 3}, {"type": "text", "text": "The structural entropy $H^{S I}(Y)$ is defined similarly. We restrict the partitioning structure of $G_{x y}$ to 2-layer approximate binary trees, denoted as $\\mathcal{T}^{2}$ , to calculate the required bits to encode accessible vertices in $X$ or $Y$ , defined as the joint structural entropy. This tree structure mandates that each intermediate node (neither root nor leaf) has precisely two children. We begin by initializing a one-layer encoding tree, $T_{x y}^{0}$ , designating each non-root node $\\alpha$ \u2019s parent as the root $\\lambda$ , with $\\alpha^{-}=\\lambda$ By applying the stretch operator from the HCSE algorithm [Pan et al., 2021], we pursue an iterative and greedy optimization of $T_{x y}^{0}$ , further detailed in Appendix A.2. The optimal encoding tree, $T_{x y}^{*}$ for $G_{x y}$ and the joint entropy under $T_{x y}^{*}$ are achieved through: ", "page_idx": 3}, {"type": "equation", "text": "$$\nT_{x y}^{*}=\\arg\\operatorname*{min}_{T\\in{\\mathcal{T}}^{2}}H^{T}(G_{x y}),\\;\\;\\;H^{T_{x y}^{*}}(X,Y)=H^{T_{x y}^{*}}(G_{x y}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Utilizing 2-layer approximate binary trees as the structural framework ensures computational tractability and more complex structures will increase the cost of increased computational complexity, which can be prohibitive for practical applications. ", "page_idx": 3}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/b24f634625daee336f3228d460fd77edaa201e1e728728f539efb63d6a0e2023.jpg", "img_caption": ["Figure 2: The SI2E\u2019s overview architecture, including state-action representation learning and maximum structural entropy exploration. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We derive the following proposition regarding $\\mathcal{T}^{2}$ , with the detailed proof provided in Appendix B.1. Proposition 3.1. Consider an undirected graph $G=(V,E)$ with vertices $v_{i}$ and $v_{j}$ in $V$ . If the edge $(v_{i},v_{j})$ is absent from $E$ , then in the 2-layer approximate binary optimal encoding tree $T^{*}\\,\\in\\,\\tau^{2}$ , there does not exist any non-root node $\\alpha$ such that both $v_{i}$ and $v_{j}$ are included in its subset $V_{\\alpha}$ . ", "page_idx": 4}, {"type": "text", "text": "Each intermediate node $\\alpha\\in T_{x y}^{*}$ corresponds to a subset comprising exactly one $x$ vertex and one $y$ vertex, thus establishing a one-to-one matching structure between variables $X$ and $Y$ . The $i$ -th intermediate node in $T_{x y}^{*}$ , ordered from left to right, is denoted as $\\alpha_{i}$ . Within this subset, the $x$ and $y$ vertices are labeled as $x_{i}$ and $y_{i}$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "To define structural mutual information accurately, it is essential to consider the joint entropy of two variables under various partition structures. We introduce an $l$ -transformation applied to $T_{x y}^{*}$ to systematically traverse all potential one-to-one matching of these variables, providing a comprehensive measure of their structural similarity. Given an integer parameter $l~>~0$ , this transformation generates a new 2-layer approximate binary tree, T xly, representing an alternative one-to-one matching structure. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.2. For each intermediate node $\\alpha_{i}$ in $T_{x y}^{l}$ , the $x$ and $y$ vertices in $T_{\\alpha_{i}}^{l}$ are specified as $T_{\\alpha_{i}}^{l}=\\{x_{i^{\\prime}},y_{i}\\}$ , where $i^{\\prime}=(i+l)$ mod $n$ . ", "page_idx": 4}, {"type": "text", "text": "The resulting tree $T_{x y}^{l}$ is equivalent to the optimal tree $T_{x y}^{*}$ when $l=0$ . We provide an example in Appendix A.3 with $n^{\"}=4$ for an intuitive understanding of the described process. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.3. Leveraging the relationship $I(X;Y)=H(X)+H(Y)-\\underline{{H}}(X,Y)$ in traditional information theory, we formally define the structural mutual information, $I^{S I}(\\dot{X};Y)$ , as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nI^{S I}(X;Y)=\\sum_{l=0}^{n-1}\\left[H^{S I}(X)+H^{S I}(Y)-H^{T_{x y}^{l}}(X,Y)\\right]=\\sum_{i,j}\\left[p(x_{i},y_{j})\\cdot\\log\\frac{2}{p(x_{i})+p(y_{j})}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The detailed derivation is provided in Appendix C.1. Structural mutual information quantifies the average difference between the required encoding bits of accessible vertices of a single variable and the joint variable. The following theorem outlines the connection between $I^{S I}(X;\\breve{Y})$ and $I(X;Y)$ , with a detailed proof in Appendix B.2. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4. For a tuning parameter $0\\leq\\epsilon\\leq1$ , its holds for the structural mutual information $I^{S I}(X;Y)$ , traditional mutual information $I(X;Y)$ , and joint Shannon entropy $H(X,Y)$ that: ", "page_idx": 4}, {"type": "equation", "text": "$$\nI(X;Y)\\leq I^{S I}(X;Y)\\leq I(X;Y)+(1-\\epsilon)\\cdot H(X,Y).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4 The Proposed SI2E Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we describe the detailed designs of the proposed SI2E framework, which captures dynamic-relevant state-action representations through structural mutual information (see Section ", "page_idx": 4}, {"type": "text", "text": "4.1) and enhances state-action coverage conditioned by the agent\u2019s policy by maximizing structural entropy (see Section 4.2). The overall architecture of our framework is illustrated in Figure 2. ", "page_idx": 5}, {"type": "text", "text": "4.1 State-action Representation Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To effectively learn dynamics-relevant state-action representations, we present an innovative embedding principle that maximizes the structural mutual information with subsequent states and minimizes it with current states. ", "page_idx": 5}, {"type": "text", "text": "Structural Mutual Information Principle. In this phase, the input variables at timestep $t$ encompass the current observation $O_{t}$ and the action $A_{t}$ , with the target being the subsequent observation $O_{t+1}$ . We denote the encoding of observations $O_{t}$ and $O_{t+1}$ as states $S_{t}$ and $S_{t+1}$ , respectively. We aim to generate a latent representation $Z_{t}$ for the tuple $(S_{t},A_{t})$ , which preserves information relevant to $S_{t+1}$ while compressing information pertinent to $S_{t}$ . This embedding process mentioned above is detailed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS_{t}=f_{s}(O_{t}),\\quad S_{t+1}=f_{s}(O_{t+1}),\\quad Z_{t}=f_{z}(S_{t},A_{t}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f_{s}$ and $f_{z}$ are the respective encoders for states and state-action pairs (step I. a in Figure 2). For the state-action embeddings $Z_{t}$ , we construct two undirected bipartite graphs, $G_{z s}$ and $G_{z s^{\\prime}}$ , as shown in step I. b of Figure 2. These graphs represent the joint distributions of $Z_{t}$ with the current states $S_{t}$ and subsequent states $S_{t+1}$ . In step I. c of Figure 2, we generate 2-layer approximate binary trees for $G_{z s}$ and $G_{z s^{\\prime}}$ and calculate the mutual information $I^{S T}(Z_{t};S_{t})$ and $I^{S I}\\dot{(Z_{t};S_{t+1})}$ using Equation 5. Building upon the Information Bottleneck (IB) [Tishby et al., 2000], we present an embedding principle that aims to minimize $I^{S I}(Z_{t};S_{t})$ while maximizing $I^{S I}(Z_{t};S_{t+1}\\overline{{{}}})$ , as demonstrated in step I. d of Figure 2. When the joint distribution between variables $Z_{t}$ and $S_{t+1}$ shows a one-to-one correspondence-meaning for each $z_{t}\\in Z_{t}$ value, there is a unique $s_{t+1}\\in S_{t+1}$ corresponding to it, and vice versa-their mutual information takes its maximum value. We introduce a theorem to elucidate the equivalence between $I^{S I}(Z_{t};S_{t+1})$ and $I(Z_{t};S_{t+1})$ under this condition. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. For a joint distribution of variables $X$ and $Y$ that shows a one-to-one correspondence, $I^{S I}(X;Y)$ equals $I(X;Y)$ . ", "page_idx": 5}, {"type": "text", "text": "A detailed proof is provided in Appendix B.3. When $Z_{t}$ and $S_{t}$ are mutually independent, the mutual information $I(Z_{t};S_{t})$ attains its minimum value. Our $I^{S I}(Z_{t};S_{t})$ goes beyond this, incorporating the joint entropy $H(Z_{t},S_{t})$ according to Theorem 3.4. This integration effectively eliminates the irrelevant information embedded in the representation variable $Z_{t}$ , a significant step in our research. Consequently, structural mutual information can be considered a reasonable and desirable learning objective for acquiring dynamics-relevant state-action representations. ", "page_idx": 5}, {"type": "text", "text": "Representation Learning Objective. Due to the computational challenges of directly minimizing $I^{S I}(Z_{t};S_{t})$ , we formulate a variational upper bound $I(\\bar{Z}_{t};S_{t})+H(Z_{t}|\\bar{S_{t}})+H(S_{t})$ (see Appendix C.2). Noting that the term $H(S_{t})$ is extraneous to our model, we equate the minimization of $I^{S I}(Z_{t};S_{t})$ to the minimization of $I(Z_{t};S_{t})$ and $H(Z_{t}|S_{t})$ . ", "page_idx": 5}, {"type": "text", "text": "By employing a feasible decoder to approximate the marginal distribution of $Z_{t}$ , we derive an upper bound of $I(Z_{t};S_{t})$ (See Appendix C.3) as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nI(Z_{t};S_{t})\\leq\\sum\\left[p(z_{t},s_{t})\\cdot D_{K L}(p(z_{t}|s_{t})||q_{m}(z_{t}))\\right]\\triangleq L_{u p}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To concurrently decrease the conditional entropy $H(Z_{t}|S_{t})$ , we introduce a predictive objective (See Appendix C.4) through a tractable decoder $q_{z|s}$ for the conditional probability $p(z_{t}|s_{t})$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nH(Z_{t}|S_{t})\\le\\sum\\left[p(z_{t},s_{t})\\cdot\\log\\frac{1}{q_{z|s}(z_{t}|s_{t})}\\right]\\triangleq L_{z|s},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L_{z|s}$ represents the log-likelihood of $Z_{t}$ given $S_{t}$ . ", "page_idx": 5}, {"type": "text", "text": "To efficiently optimize $I^{S I}(Z_{t};S_{t+1})$ , we maximize its lower bound, $I(Z_{t};S_{t+1})$ , as detailed in Theorem 3.4. By utilizing an alternative decoder $q_{s\\mid z}$ for the conditional probability $p(s_{t+1}|z_{t})$ , we obtain a lower bound of $I(Z_{t};S_{t+1})$ (See Appendix C.5) as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nI(Z_{t};S_{t+1})\\ge\\sum\\left[p(z_{t},s_{t+1})\\cdot\\log q_{s|z}(s_{t+1}|z_{t})\\right]\\triangleq L_{s|z},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L_{s|z}$ denotes the log-likelihood of $S_{t+1}$ conditioned on $Z_{t}$ . ", "page_idx": 5}, {"type": "text", "text": "Within our SI2E framework, the definitive loss for representation learning is a combination of the above bounds, ${\\cal L}\\,=\\,{\\cal L}_{u p}+{\\cal L}_{z|s}+\\eta\\cdot{\\cal L}_{s|z}$ , where $\\eta$ is a Lagrange multiplier used to maintain equilibrium among the specified terms. ", "page_idx": 6}, {"type": "text", "text": "4.2 Maximum Structural Entropy Exploration ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We have designed a unique intrinsic reward mechanism to address the challenge of imbalance exploration towards low-value states in traditional entropy strategies, as discussed by [Kim et al., 2023]. Specifically, we generate a hierarchical state-action structure based on the agent\u2019s policy and define value-conditional structural entropy as an intrinsic reward for effective exploration. ", "page_idx": 6}, {"type": "text", "text": "Hierarchical State-action Structure. Derived from the history of agent-environment interactions, we extract state-action pairs (step II. a in Figure 2) to form a complete graph $\\boldsymbol{G}_{s a}$ (step II. b in Figure 2) that encapsulates the value relationships caused by the agent\u2019s policy. Within this graph, any two vertices $v_{i}$ and $v_{j}$ is connected by an undirected edge whose weight $w_{i j}$ is determined as: $w_{i j}=||\\pi(s_{t}^{i},a_{t}^{i})-\\pi(s_{t}^{j},a_{t}^{j})||_{2}$ . The state-action pairs $(s_{t}^{i},a_{t}^{i})$ and $(s_{t}^{j},a_{t}^{j})$ are associated with vertices $v_{i}$ and $v_{j}$ , respectively. We minimize the 2-dimensional structural entropy of this graph $G_{s a}$ to generate its 2-layer optimal encoding tree, denoted as $T_{s a}^{*}$ (step II. c in Figure 2). This tree $T_{s a}^{*}$ delineates a hierarchical community structure among the state-action vertices, with the root node corresponding to a community encompassing all vertices. Each intermediate node in $T_{s a}^{*}$ corresponds to a sub-community, including vertices that share similar $\\pi$ values. ", "page_idx": 6}, {"type": "text", "text": "Value-conditional Structural Entropy. To measure the extent of the policy\u2019s coverage across the state-action space, we construct an additional distribution graph $\\boldsymbol{G}_{s a}^{\\prime}$ (step II. d in Figure 2). The graph $\\boldsymbol{G}_{s a}^{'}$ shares the same vertex set as $G_{s a}$ . The following proposition confirms the existence of such a graph, with a detailed proof provided in Appendix B.4. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.2. Given positive visitation probabilities $p(s_{t}^{0},a_{t}^{0}),\\dots,p(s_{t}^{n-1},a_{t}^{n-1})$ for all stateaction pairs, there exists a weighted, undirected, and connected graph $\\boldsymbol{G}_{s a}^{'}$ , where each vertex\u2019s degree $d_{i}$ equals its visitation probability $p(s_{t}^{i},a_{t}^{i})$ . ", "page_idx": 6}, {"type": "text", "text": "In the graph $G_{s a}^{\\prime}$ , the set of all state-action vertices is denoted as $V_{0}$ , and the set of all stateaction sub-communities is denoted as $V_{1}$ . The Shannon entropies associated with the distribution of visitation probabilities for these sets are represented as $H(V_{0})$ and $H(V_{1})$ , respectively, where $H(V_{0})=H(S_{t},A_{t})$ . Within the 2-layer state-action community represented by $T_{s a}^{*}$ , we define the structural entropy of $G_{s a}^{\\prime}$ using Equation 2, denoted as $H^{T_{s a}^{*}}(G_{s a}^{\\prime})$ (step II. e in Figure 2). The following theorem delineates the relationship between the value-conditional entropy $H^{T_{s a}^{*}}(G_{s a}^{\\prime})$ with the state-action Shannon entropy $H(S_{t},A_{t})$ . A detailed proof is provided in Appendix B.5. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3. For a tuning parameter $0\\leq\\zeta\\leq1$ , it holds for the structural entropy $H^{T_{s a}^{*}}(G_{s a}^{\\prime})$ and the Shannon entropy $H(S_{t},A_{t})$ that: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\zeta\\cdot H(S_{t},A_{t})\\leq H(V_{0})-H(V_{1})\\leq H^{T_{s a}^{*}}(G_{s a}^{\\prime})\\leq H(S_{t},A_{t}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $H(V_{0})-H(V_{1})$ is a variational lower bound of $H^{T_{s a}^{*}}(G_{s a}^{\\prime})$ . On the one hand, the term $H(V_{0})$ ensures maximal coverage of the entire state-action space, analogous to the traditional Shannon entropy. On the other hand, the term $H(V_{1})$ mitigates uniform coverage among state-action subcommunities with diverse $\\pi$ values, thus addressing the challenge of imbalance exploration. By identifying the hierarchical state-action structure caused by the agent\u2019s policy, the SI2E achieves enhanced maximum coverage exploration, thereby guaranteeing its exploration advantage. ", "page_idx": 6}, {"type": "text", "text": "Estimation and Intrinsic Reward. Considering the impracticality of directly acquiring visitation probabilities, we employ the $k$ -NN entropy estimator in Equation 1 to estimate the lower bound: ", "page_idx": 6}, {"type": "equation", "text": "$$\nH(V_{0})-H(V_{1})\\approx\\frac{d_{z}}{n_{0}}\\cdot\\sum_{i=0}^{n_{0}-1}\\log d(v_{i}^{0})-\\frac{d_{z}}{n_{1}}\\cdot\\sum_{i=0}^{n_{1}-1}\\log d(v_{i}^{1})+C,\\ \\ v_{i}^{0}\\in V_{0},v_{i}^{1}\\in V_{1},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $d_{z}$ is the dimension of state-action embedding, $n_{0}$ and $n_{1}$ are the vertex numbers in $V_{0}$ and $V_{1}$ , and $d(v)$ is twice the distance from vertex $v$ to its $k$ -th nearest neighbor. By ignoring the constant term in Equation 12, we define the intrinsic reward $\\boldsymbol{r}_{t}^{i}$ and train RL agents to address the target task using a combined reward $r_{t}=r_{t}^{e}+\\beta\\cdot r_{t}^{i}$ (step II. f in Figure 2), where $\\beta$ is a positive hyperparameter that modulates the trade-off between exploration and exploitation. The pseudocode, complexity analysis, and limitations of our framework are provided in Appendix A. ", "page_idx": 6}, {"type": "table", "img_path": "Bjh4mcYs20/tmp/781bc22a4ef5a1f54f227c68c746d3ef8cd5a5144a71719ffacafecd4d7a31c8.jpg", "table_caption": ["Table 1: Summary of success rates and required steps to achieve target rewards in MiniGrid and MetaWorld tasks: \u201caverage value $\\pm$ standard deviation\" and \u201caverage improvement\". Bold: the best performance, underline: the second performance. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present a comprehensive suite of comparative experiments on MiniGrid [ChevalierBoisvert et al., 2018], MetaWorld [Yu et al., 2020], and the DeepMind Control Suite (DMControl) [Tunyasuvunakool et al., 2020] to evaluate the effectiveness of SI2E in terms of both final performance and sample efficiency. Consistent with previous work [Zeng et al., 2023c], we measure the required steps to attain specified rewards (0.9 times SI2E\u2019s convergence reward) as a benchmark for assessing sample efficiency. For the SI2E implementation, we employ a randomly initialized encoder optimized to minimize the combined loss $L$ . All experiments are conducted with 10 different random seeds, and the learning curves are delineated in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "5.1 MiniGrid Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Initially, we assess our framework on navigation tasks using the MiniGrid benchmark, which includes goal-reaching tasks in sparse-reward environments. This setting is partially observable: the agent receives a $7\\times7\\times3$ embedding of the immediate surrounding grid rather than the entire grid. For comparative purposes, we employ the A2C agent [Mnih et al., 2016] with Shannon entropy (SE) [Seo et al., 2021] and value-based state entropy (VCSE) [Kim et al., 2023] as our baselines. Table 1 (upper) displays the average values and standard deviations of success rates and required steps for various navigation tasks. The tasks encompass navigation with obstacles (SimpleCrossingS9N1), long-horizon navigation (RedBlueDoors, DoorKey, and Unlock), and long-horizon navigation with obstacles (KeyCorridorS3R1). The SI2E consistently exhibits enhanced final performance and sample efficiency across tasks, with an average success rate increase of $4.92\\%$ , from $89.97\\%$ to $94.40\\%$ , and an average decrease in required steps of $38.10\\%$ , from $635.65K$ to $393.47K$ . In the RedBlueDoors task, where baseline performances are inadequate, our SI2E significantly improves the success rate from $79.82\\%$ to $85.8\\bar{0}\\%$ and reduces the required steps from $1161.90K$ to $461.90K$ . ", "page_idx": 7}, {"type": "text", "text": "5.2 MetaWorld Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We further evaluate the SI2E framework on visual manipulation tasks from the MetaWorld benchmark, which presents exploration challenges due to its large state space. We select the model-free DrQv2 algorithm as the underlying RL methodology. Adhering to the setup of [Seo et al., 2023], we employ the same camera configuration and normalize the reward with a scale of 1. We summarize the success rates and required steps for all exploration methods across six MetaWorld tasks in Table 1 (lower). ", "page_idx": 7}, {"type": "text", "text": "Table 2: Summary of average episode rewards for control tasks in DMControl, encompassing two cartpole tasks characterized by sparse rewards: \u201caverage value $\\pm$ standard deviation\" and \u201caverage improvement\" (absolute value $(\\%)$ ). Bold: the best performance, underline: the second performance. ", "page_idx": 8}, {"type": "table", "img_path": "Bjh4mcYs20/tmp/1b6e6a1d17f13322e39c6793e36748f0ce3eaeeaeeb6e16c4843418d3ab21f6d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Our SI2E framework enables the $\\mathrm{DrQv}2$ agent to solve all tasks with an average success rate of 97.87 after an average of $64.79K$ environmental steps, significantly outperforming other baselines. Specifically, in the Door Open task, all baselines struggle to achieve a meaningful success rate with a satisfactory number of environmental steps. This result demonstrates the SI2E\u2019s effectiveness in improving and accelerating agent exploration in challenging tasks with expansive state-action spaces. ", "page_idx": 8}, {"type": "text", "text": "5.3 DMControl Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Subsequently, we evaluate our framework across various continuous control tasks within the DMControl suite. As the foundational agent, we choose the same DrQv2 algorithm, which operates on pixel-based observations. We incorporate a state-action exploration baseline, MADE [Zhang et al., 2021b], for a more comprehensive comparison. We evaluate all exploration methods across six continuous control tasks, documenting the episode rewards in Table 2. Observations reveal that SI2E remarkably increases the mean episode reward in each DMControl task. Specifically, in the Cartpole Swingup task characterized by sparse rewards, our framework boosts the average reward from 707.76 to 795.09, resulting in a $12.34\\%$ improvement in the final performance. Moreover, we compare the sample efficiency of SI2E and the best-performing baseline in Appendix E.3. ", "page_idx": 8}, {"type": "text", "text": "These results not only demonstrate the effectiveness of SI2E in acquiring dynamics-relevant representations for state-action pairs but also highlight its potential to motivate agents to explore the state-action space. To better understand the rationality and advantage of the SI2E framework, we provide visualization experiments in Appendix E.4. ", "page_idx": 8}, {"type": "text", "text": "5.4 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To further investigate the impact of two critical components within the SI2E framework, embedding principle (Section 4.1) and intrinsic reward mechanism (4.2), we perform ablation studies on MetaWorld and DMControl tasks, focusing on two distinct variants: (i) SI2E-DB, which utilizes the DB bottleneck [Bai et al., 2021] for learning state-action representations, and (ii) SI2E-VCSE, employing the state-of-the-art VCSE approach [Kim et al., 2023] for calculating intrinsic rewards. As depicted in Figure 3, SI2E surpasses all variants regarding final performance and sample efficiency. This outcome underscores the essential role of these critical components in conferring SI2E\u2019s superior capabilities. Additional ablation studies for the parameters $\\beta$ and $n$ are available in Appendix E.5. ", "page_idx": 8}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/03096d8df1c9b82de13bab38c0a365bc464ccbcb72b0a2c6e9d3732be3cb1e8f.jpg", "img_caption": ["Figure 3: Learning curves across MetaWorld and DMControl tasks for ablation studies. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "6.1 Maximum Entropy Exploration ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Maximum entropy exploration has evolved from focusing initially on unsupervised methods to incorporating task rewards in more advanced supervised models. In the unsupervised paradigm, agents autonomously acquire behaviors by using state entropy as an intrinsic reward for exploration [Liu and Abbeel, 2021, Mutti et al., 2022, Yang and Spaan, 2023]. In contrast, in the supervised paradigm, agents aim to maximize state entropy in conjunction with task rewards [Seo et al., 2021, Yuan et al., 2022]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "However, these methods face challenges due to imbalances in the distributions of states with differing policy values. To address this issue, a value-based approach [Kim et al., 2023] has been proposed, which integrates value estimates into the entropy calculation to ensure balanced exploration. Nevertheless, the effectiveness of this approach heavily depends on the partitioning structure of states according to policy values, requiring prior knowledge about downstream tasks. ", "page_idx": 9}, {"type": "text", "text": "In this work, we leverage structural information principles to derive the hierarchical state-action structure in an unsupervised manner. We further define the value-conditional structural entropy as an intrinsic reward to achieve more effective agent exploration. Compared to current maximum entropy explorations, SI2E introduces an additional sub-community entropy and minimizes this entropy to motivate the agent to explore specific sub-communities with high policy values. This approach helps avoid redundant explorations within low-value sub-communities and achieves enhanced maximum coverage exploration. Our method allows for balanced exploration without requiring prior knowledge of downstream tasks, effectively addressing the limitations of previous approaches. ", "page_idx": 9}, {"type": "text", "text": "6.2 Representation Learning ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Novelty Search [Tao et al., 2020] and Curiosity Bottleneck [Kim et al., 2019b] leverage the Information Bottleneck principle for effective representation learning. Additionally, the EMI method [Kim et al., 2019a] maximizes mutual information in both forward and inverse dynamics to develop desirable representations. However, these methods are limited by the lack of an explicit mechanism to address the white noise issue in the state space. To overcome this challenge, the Dynamic Bottleneck model [Bai et al., 2021] is introduced for robust exploration in complex environments. ", "page_idx": 9}, {"type": "text", "text": "Our work defines structural mutual information to measure the structural similarity between two variables for the first time. Additionally, we present an innovative embedding principle that incorporates the entropy of the representation variable. This approach more effectively eliminates irrelevant information than the traditional information bottleneck principle. ", "page_idx": 9}, {"type": "text", "text": "6.3 Structural Information Principles ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Since the introduction of structural information principles [Li and Pan, 2016], these principles have significantly transformed the analysis of network complexities, employing metrics such as structural entropy and partitioning trees. This innovative approach has not only deepened the understanding of network dynamics\u2014even in the context of multi-relational graphs [Cao et al., 2024a]\u2014but has also led to a wide array of applications across different domains. The application of structural information principles has extended to various fields, including graph learning [Wu et al., 2022], skin segmentation [Zeng et al., 2023a], and the analysis of social networks [Peng et al., Zeng et al., 2024, Cao et al., 2024b]. In the domain of reinforcement learning, these principles have been instrumental in defining hierarchical action and state abstractions through encoding trees [Zeng et al., 2023b,c], marking a significant advancement in robust decision-making frameworks. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose SI2E, a novel exploration framework based on structural information principles. This framework defines structural mutual information to effectively capture state-action representations relevant to environmental dynamics. It maximizes the value-conditional structural entropy to enhance coverage across the state-action space. We have established theoretical connections between SI2E and traditional information-theoretic methodologies, underscoring the framework\u2019s rationality and advantages. Through extensive and comparative evaluations, SI2E significantly improves final performance and sample efficiency over state-of-the-art exploration methods. Our future work includes expanding the height of encoding trees and the range of experimental environments. Our goal is for SI2E to remain a robust and adaptable tool in reinforcement learning, particularly suited to high-dimensional and sparse-reward contexts. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The corresponding authors are Hao Peng and Angsheng Li. This work is supported by the National Key R&D Program of China through grant 2021YFB1714800, NSFC through grants 61932002, 62322202, and 62432006, Beijing Natural Science Foundation through grant 4222030, Local Science and Technology Development Fund of Hebei Province Guided by the Central Government of China through grant 246Z0102G, Hebei Natural Science Foundation through grant F2024210008, and Guangdong Basic and Applied Basic Research Foundation through grant 2023B1515120020. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. In International Conference on Learning Representations, 2018. ", "page_idx": 10}, {"type": "text", "text": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017. ", "page_idx": 10}, {"type": "text", "text": "Adri\u00e0 Puigdom\u00e8nech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. In International conference on machine learning, pages 507\u2013517. PMLR, 2020. ", "page_idx": 10}, {"type": "text", "text": "Chenjia Bai, Lingxiao Wang, Lei Han, Animesh Garg, Jianye Hao, Peng Liu, and Zhaoran Wang. Dynamic bottleneck for robust self-supervised exploration. Advances in Neural Information Processing Systems, 34:17007\u201317020, 2021. ", "page_idx": 10}, {"type": "text", "text": "Richard Bellman. A markovian decision process. Journal of Mathematics and Mechanics, pages 679\u2013684, 1957. ", "page_idx": 10}, {"type": "text", "text": "Yuwei Cao, Hao Peng, Angsheng Li, Chenyu You, Zhifeng Hao, and Philip S. Yu. Multi-relational structural entropy. In The 40th Conference on Uncertainty in Artificial Intelligence, pages 4289\u2013 4298, 2024a. ", "page_idx": 10}, {"type": "text", "text": "Yuwei Cao, Hao Peng, Zhengtao Yu, and Philip S. Yu. Hierarchical and incremental structural entropy minimization for unsupervised social event detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 8255\u20138264, 2024b. ", "page_idx": 10}, {"type": "text", "text": "Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openai gym. 2018. ", "page_idx": 10}, {"type": "text", "text": "Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International conference on machine learning, pages 1352\u20131361. PMLR, 2017. ", "page_idx": 10}, {"type": "text", "text": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018. ", "page_idx": 10}, {"type": "text", "text": "Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum entropy exploration. In International Conference on Machine Learning, pages 2681\u20132691. PMLR, 2019. ", "page_idx": 10}, {"type": "text", "text": "Riashat Islam, Raihan Seraj, Pierre-Luc Bacon, and Doina Precup. Entropy regularization with discounted future state distribution in policy gradient methods. ArXiv, 2019. ", "page_idx": 10}, {"type": "text", "text": "Daejin Jo, Sungwoong Kim, Daniel Nam, Taehwan Kwon, Seungeun Rho, Jongmin Kim, and Donghoon Lee. Leco: Learnable episodic count for task-specific intrinsic reward. Advances in Neural Information Processing Systems, 35:30432\u201330445, 2022. ", "page_idx": 10}, {"type": "text", "text": "Hyoungseok Kim, Jaekyeom Kim, Yeonwoo Jeong, Sergey Levine, and Hyun Oh Song. Emi: Exploration with mutual information. In International Conference on Machine Learning, pages 3360\u20133369. PMLR, 2019a.   \nYoungjin Kim, Wontae Nam, Hyunwoo Kim, Ji-Hoon Kim, and Gunhee Kim. Curiosity-bottleneck: Exploration by distilling task-specific novelty. In International conference on machine learning, pages 3379\u20133388. PMLR, 2019b.   \nAngsheng Li and Yicheng Pan. Structural information and dynamical complexity of networks. IEEE Transactions on Information Theory, 62:3290\u20133339, 2016.   \nHao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. Advances in Neural Information Processing Systems, 34:18459\u201318473, 2021.   \nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\u20131937. PMLR, 2016.   \nMirco Mutti, Riccardo De Santi, and Marcello Restelli. The importance of non-markovianity in maximum state entropy exploration. In International Conference on Machine Learning, pages 16223\u201316239. PMLR, 2022.   \nOfir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. Advances in neural information processing systems, 30, 2017.   \nYicheng Pan, Feng Zheng, and Bingchen Fan. An information-theoretic perspective of hierarchical clustering. arXiv preprint arXiv:2108.06036, 2021.   \nDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pages 2778\u20132787. PMLR, 2017.   \nHao Peng, Jingyun Zhang, Xiang Huang, Zhifeng Hao, Angsheng Li, Zhengtao Yu, and Philip S Yu. Unsupervised social bot detection via structural information theory. ACM Transactions on Information Systems, pages 1\u201342.   \n\u00d3scar P\u00e9rez-Gil, Rafael Barea, Elena L\u00f3pez-Guill\u00e9n, Luis M Bergasa, Carlos Gomez-Huelamo, Rodrigo Guti\u00e9rrez, and Alejandro Diaz-Diaz. Deep reinforcement learning based control for autonomous vehicles in carla. Multimedia Tools and Applications, 81(3):3553\u20133576, 2022.   \nSahaya Beni Prathiba, Gunasekaran Raja, Kapal Dev, Neeraj Kumar, and Mohsen Guizani. A hybrid deep reinforcement learning for autonomous vehicles smart-platooning. IEEE Transactions on Vehicular Technology, 70(12):13340\u201313350, 2021.   \nYounggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State entropy maximization with random encoders for efficient exploration. In International Conference on Machine Learning, pages 9443\u20139454. PMLR, 2021.   \nYounggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel. Masked world models for visual control. In Conference on Robot Learning, pages 1332\u20131344. PMLR, 2023.   \nClaude Shannon. The lattice theory of information. Transactions of the IRE professional Group on Information Theory, 1(1):105\u2013107, 1953.   \nHarshinder Singh, Neeraj Misra, Vladimir Hnizdo, Adam Fedorowicz, and Eugene Demchuk. Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23 (3-4):301\u2013321, 2003.   \nRuo Yu Tao, Vincent Fran\u00e7ois-Lavet, and Joelle Pineau. Novelty search in representational space for sample efficient exploration. Advances in Neural Information Processing Systems, 33:8114\u20138126, 2020.   \nNaftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000.   \nSaran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm_control: Software and tasks for continuous control. Software Impacts, 6:100022, 2020.   \nOriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, and etc. Alphastar: Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019.   \nShanchuan Wan, Yujin Tang, Yingtao Tian, and Tomoyuki Kaneko. Deir: efficient and robust exploration through discriminative-model-based episodic intrinsic rewards. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 4289\u20134298, 2023.   \nJunran Wu, Xueyuan Chen, Ke Xu, and Shangzhe Li. Structural entropy guided graph hierarchical pooling. In ICML, pages 24017\u201324030. PMLR, 2022.   \nQisong Yang and Matthijs TJ Spaan. Cem: Constrained entropy maximization for task-agnostic safe exploration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10798\u201310806, 2023.   \nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. In International Conference on Learning Representations, 2021.   \nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 1094\u20131100. PMLR, 2020.   \nMingqi Yuan, Man-On Pun, and Dong Wang. R\u00e9nyi state entropy maximization for exploration acceleration in reinforcement learning. IEEE Transactions on Artificial Intelligence, 2022.   \nGuangjie Zeng, Hao Peng, Angsheng Li, Zhiwei Liu, Chunyang Liu, Philip $\\textit{S}\\mathrm{Y}\\mathbf{u}$ , and Lifang He. Unsupervised skin lesion segmentation via structural entropy minimization on multi-scale superpixel graphs. In IEEE ICDM, pages 768\u2013777, 2023a.   \nXianghua Zeng, Hao Peng, and Angsheng Li. Effective and stable role-based multi-agent collaboration by structural information principles. Proceedings of the AAAI Conference on Artificial Intelligence, (10):11772\u201311780, Jun. 2023b.   \nXianghua Zeng, Hao Peng, Angsheng Li, Chunyang Liu, Lifang He, and Philip S Yu. Hierarchical state abstraction based on structural information principles. In IJCAI, pages 4549\u20134557, 2023c.   \nXianghua Zeng, Hao Peng, and Angsheng Li. Adversarial socialbots modeling based on structural information principles. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 392\u2013400, 2024.   \nChuheng Zhang, Yuanying Cai, Longbo Huang, and Jian Li. Exploration by maximizing r\u00e9nyi entropy for reward-free rl framework. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 10859\u201310867, 2021a.   \nTianjun Zhang, Paria Rashidinejad, Jiantao Jiao, Yuandong Tian, Joseph E Gonzalez, and Stuart Russell. Made: Exploration via maximizing deviation from explored regions. Advances in Neural Information Processing Systems, 34:9663\u20139680, 2021b.   \nRuijie Zheng, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, Huazhe Xu, Hal Daum\u00e9 III, and Furong Huang. Taco: Temporal latent action-driven contrastive loss for visual reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Framework Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Notations ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "Bjh4mcYs20/tmp/7e4b3fee83a42d44f95cbf5e4a59ae1316cdaa3f44b90e883e0176db33313e8b.jpg", "table_caption": ["Table 3: Glossary of Notations. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Tree Optimization on $\\mathcal{T}^{2}$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this subsection, we have provided additional explanations and illustrative examples for the encoding tree optimization on $\\mathcal{T}^{2}$ . As shown in Figure 4, the stretch operator is executed over sibling nodes $\\alpha_{i}$ and $\\alpha_{j}$ that share the same parent node, $\\lambda$ . The detailed steps of this operation are as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\alpha^{\\prime}}^{-}=\\lambda,\\;\\;\\;{\\alpha_{i}}^{-}={\\alpha^{\\prime}},\\;\\;\\;{\\alpha_{j}}^{-}={\\alpha^{\\prime}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\alpha^{\\prime}$ is the added tree node via the stretch operation. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{8^{a_{i}}}\\textcircled{=}\\frac{1}{8}\\frac{\\frac{\\cdots}{8^{t r e t c h}}}{8^{a_{i}}}\\times\\frac{2}{\\alpha_{i}^{\\prime}8^{a_{j}}}\\textcircled{=}\\frac{1}{8^{a_{i}}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Figure 4: Stretch operation on sibling nodes within the encoding tree. The corresponding variation in structural entropy, $\\Delta H$ , due to the stretch operation is calculated as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H^{T}(G;\\alpha_{i})=-\\frac{g_{\\alpha_{i}}}{\\mathrm{vol}(G)}\\cdot\\log\\frac{\\mathrm{vol}(\\alpha_{i})}{\\mathrm{vol}(G)},\\quad H^{T}(G;\\alpha_{i})=-\\frac{g_{\\alpha_{i}}}{\\mathrm{vol}(G)}\\cdot\\log\\frac{\\mathrm{vol}(\\alpha_{j})}{\\mathrm{vol}(G)},}\\\\ &{H^{T^{\\prime}}(G;\\alpha_{i})=-\\frac{g_{\\alpha_{i}}}{\\mathrm{vol}(G)}\\cdot\\log\\frac{\\mathrm{vol}(\\alpha_{i})}{\\mathrm{vol}(G)},\\quad H^{T^{\\prime}}(G;\\alpha_{j})=-\\frac{g_{\\alpha_{j}}}{\\mathrm{vol}(G)}\\cdot\\log\\frac{\\mathrm{vol}(\\alpha_{j})}{\\mathrm{vol}(\\alpha^{\\prime})},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad H^{T^{\\prime}}(G;\\alpha^{\\prime})=-\\frac{g_{\\alpha^{\\prime}}}{\\mathrm{vol}(G)}\\cdot\\log\\frac{\\mathrm{vol}(\\alpha^{\\prime})}{\\mathrm{vol}(G)},}\\\\ &{\\Delta H=H^{T}(G;\\alpha_{i})+H^{T}(G;\\alpha_{j})-H^{T^{\\prime}}(G;\\alpha_{i})-H^{T^{\\prime}}(G;\\alpha_{j})-H^{T^{\\prime}}(G;\\alpha^{\\prime})}\\\\ &{\\qquad=(H^{T}(G;\\alpha_{i})-H^{T^{\\prime}}(G;\\alpha_{i}))+(H^{T}(G;\\alpha_{j})-H^{T^{\\prime}}(G;\\alpha_{j}))-H^{T^{\\prime}}(G;\\alpha_{j}))}\\\\ &{\\qquad=-\\frac{g_{\\alpha_{i}}}{\\mathrm{vol}(G)}\\cdot\\log\\frac{\\mathrm{vol}(\\alpha^{\\prime})}{\\mathrm{vol}(G)}-\\frac{g_{\\alpha_{j}}}{\\mathrm{vol}(G)}\\cdot\\log\\frac{\\mathrm{vol}(\\alpha^{\\prime})}{\\mathrm{vol}(G)}+\\frac{g_{\\alpha^{\\prime}}}{\\mathrm{vol}(G)}\\cdot\\log\\frac{\\mathrm{vol}(\\alpha^{\\prime})}{\\mathrm{vol}(G)}}\\\\ &{=-\\frac{g_{\\alpha_{i}}+g_{\\alpha_{j}}-g_{\\alpha^{\\prime}}}{\\mathrm{vol}(G)}\\cdot\\log\\frac \n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As shown in Algorithm 1, the HCSE algorithm iteratively and greedily selects the pair of sibling nodes that cause the maximum entropy variation, $\\Delta H$ to execute one stretch optimization. ", "page_idx": 14}, {"type": "table", "img_path": "Bjh4mcYs20/tmp/3eb21530481a746ff2cb2b50050db8e17f892c55a37a72b8a1d1a95db8b90bb3.jpg", "table_caption": ["Algorithm 1 The Encoding Tree Optimization on $\\mathcal{T}^{2}$ "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.3 Intuitive Example of Optimal Encoding Tree ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/5df4b6d169394872051163f14dca60bad79f83accd140838d0a8ff67c5b99471.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 5: Illustration from a joint distribution to 2-layer approximate binary trees: a) Bipartite distribution graph, b) Optimal encoding tree, c) Resulting encoding tree via a 1-transformation. ", "page_idx": 14}, {"type": "table", "img_path": "Bjh4mcYs20/tmp/9570e913ed32799094776b7060077b85d3617438ec33dc70fb6ccb332031a59f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.5 Complexity Analysis of SI2E ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Within the SI2E framework, we analyze the time complexities of critical components independent of the underlying RL algorithm. During the state-action representation phase, the construction of bipartite graphs takes $O(n^{2})$ time complexity, the generation of 2-layer approximate binary trees requires $O(n\\cdot\\log^{2}n)$ time complexity, and the calculation of mutual information involves a time complexity of $O(n^{2})$ . During the effective exploration phase, the generation of hierarchical community structure incurs a $O(n\\cdot\\log^{2}n)$ complexity, the construction of the distribution graph leads to a complexity of $O(n^{2})$ , and value-conditional structural entropy is calculated with $O(n)$ time complexity. ", "page_idx": 15}, {"type": "text", "text": "A.6 Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our work, which is a result of thorough research, aims to address the limitations of information theory methods and structural information theory research in reinforcement learning. Therefore, we have selected the state-of-the-art information theory exploration method as the baseline in our evaluation. Despite the current limitations in height due to complexity and cost issues, the encoding tree structure in the SI2E framework holds immense potential. In our future research, we are optimistic about expanding its height further and conducting more research on the advantages and restrictions brought by this expansion. ", "page_idx": 15}, {"type": "text", "text": "B Theorem Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. For any two vertices $v_{i}\\in V$ and $v_{j}\\in V$ without any edge connecting them, their corresponding tree nodes are denoted as $\\alpha_{i}$ and $\\alpha_{j}$ . These nodes\u2019 parents are initially assigned as the root node $\\lambda$ . Before executing one stretch operation on vertices $\\alpha_{i}$ and $\\alpha_{j}$ , their structural entropies are calculated as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nH(G;\\alpha_{i})=-\\frac{d_{i}}{\\mathrm{vol}(G)}\\cdot\\log\\frac{d_{i}}{\\mathrm{vol}(G)},\\ \\ H(G;\\alpha_{j})=-\\frac{d_{j}}{\\mathrm{vol}(G)}\\cdot\\log\\frac{d_{j}}{\\mathrm{vol}(G)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $d_{i}$ and $d_{j}$ are the degrees of vertices $v_{i}$ and $v_{j}$ . Post-stretch operation, their structural entropies are given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\nH(G;\\alpha_{i})=-\\frac{d_{i}}{\\mathrm{vol}(G)}\\cdot\\log\\frac{d_{i}}{\\mathrm{vol}(\\alpha^{\\prime})},\\ \\ H(G;\\alpha_{j})=-\\frac{d_{j}}{\\mathrm{vol}(G)}\\cdot\\log\\frac{d_{j}}{\\mathrm{vol}(\\alpha^{\\prime})},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\alpha^{\\prime}$ are their new common parent node. The absence of an edge between $v_{i}$ and $v_{j}$ ensures that: ", "page_idx": 16}, {"type": "equation", "text": "$$\ng_{\\alpha^{\\prime}}=d_{i}+d_{j},\\quad\\mathrm{vol}(\\alpha^{\\prime})=d_{i}+d_{j}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The structural entropy of $\\alpha^{\\prime}$ can be determined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\nH(G;\\alpha^{\\prime})=-{\\frac{g_{\\alpha^{\\prime}}}{\\operatorname{vol}(G)}}\\cdot\\log{\\frac{\\operatorname{vol}(\\alpha^{\\prime})}{\\operatorname{vol}(G)}}=-{\\frac{d_{i}+d_{j}}{\\operatorname{vol}(G)}}\\cdot\\log{\\frac{d_{i}+d_{j}}{\\operatorname{vol}(G)}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The entropy reduction $\\Delta H$ , consequent to the stretch operation on vertices $v_{i}$ and $v_{j}$ , is calculated as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\mathrm{3}}H=\\left[-{\\frac{d_{i}}{\\operatorname{vol}(G)}}\\cdot\\log{\\frac{d_{i}}{\\operatorname{vol}(G)}}-{\\frac{d_{j}}{\\operatorname{vol}(G)}}\\cdot\\log{\\frac{d_{j}}{\\operatorname{vol}(G)}}\\right]}\\\\ &{\\qquad-\\left[-{\\frac{d_{i}}{\\operatorname{vol}(G)}}\\cdot\\log{\\frac{d_{i}}{d_{i}+d_{j}}}-{\\frac{d_{j}}{\\operatorname{vol}(G)}}\\cdot\\log{\\frac{d_{j}}{d_{i}+d_{j}}}-{\\frac{d_{i}+d_{j}}{\\operatorname{vol}(G)}}\\cdot\\log{\\frac{d_{i}+d_{j}}{\\operatorname{vol}(G)}}\\right]}\\\\ &{\\qquad=\\left[-{\\frac{d_{i}}{\\operatorname{vol}(G)}}\\cdot\\log{\\frac{d_{i}}{\\operatorname{vol}(G)}}-{\\frac{d_{j}}{\\operatorname{vol}(G)}}\\cdot\\log{\\frac{d_{j}}{\\operatorname{vol}(G)}}\\right]-\\left[-{\\frac{d_{i}}{\\operatorname{vol}(G)}}\\cdot\\log{\\frac{d_{i}}{\\operatorname{vol}(G)}}-{\\frac{d_{j}}{\\operatorname{vol}(G)}}\\cdot\\log{\\frac{d_{i}}{\\operatorname{vol}(G)}}\\right]}\\end{array}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given the zero reduction in entropy, as per lines 5 and 6 of the optimization algorithm for $\\mathcal{T}^{2}$ (See Appendix A.2), the stretch operation involving $v_{i}$ and $v_{j}$ is omitted from the optimization process. ", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. The difference between the mutual information $I^{S I}(X;Y)$ and $I(X;Y)$ is expressed as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{r S I}(X;Y)-I(X;Y)=\\sum_{i,j}\\left[p(x_{i},y_{j})\\cdot\\log\\frac{2}{p(x_{i})+p(y_{j})}\\right]-\\sum_{i,j}\\left[p(x_{i},y_{j})\\cdot\\log\\frac{p(x_{i},y_{j})}{p(x_{i})\\cdot p(y_{j})}\\right]}\\quad}&{}\\\\ &{=\\sum_{i,j}\\left[p(x_{i},y_{j})\\cdot\\log\\left[\\frac{2}{p(x_{i})+p(y_{j})}\\cdot\\frac{p(x_{i})\\cdot p(y_{j})}{p(x_{i},y_{j})}\\right]\\right]}\\\\ &{=\\sum_{i,j}\\left[p(x_{i},y_{j})\\cdot\\log\\left[\\frac{1}{p(x_{i},y_{j})}\\cdot\\frac{2\\cdot p(x_{i})\\cdot p(y_{j})}{p(x_{i})+p(y_{j})}\\right]\\right]}\\\\ &{=\\sum_{i,j}\\left[p(x_{i},y_{j})\\cdot\\log\\left[\\frac{1}{p(x_{i},y_{j})}\\cdot\\frac{2}{p(x_{i})}\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given the conditions $p(x_{i},y_{j})\\leq p(x_{i})\\leq1$ and $p(x_{i},y_{j})\\leq p(y_{j})\\leq1$ , the following inequalities are satisfied: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{p(x_{i})}+\\frac{1}{p(y_{j})}\\leq\\frac{2}{p(x_{i},y_{j})},\\:\\:\\:\\:\\frac{1}{p(x_{i},y_{j})}\\cdot\\frac{2}{\\frac{1}{p(x_{i})}+\\frac{1}{p(y_{j})}}\\geq1,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n0\\leq\\log_{p(x_{i},y_{j})}p(x_{i})\\leq1,\\;\\;\\;0\\leq\\log_{p(x_{i},y_{j})}p(y_{j})\\leq1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By defining $\\epsilon_{i}=\\log_{p(x_{i},y_{j})}p(x_{i})$ and $\\epsilon_{j}=\\log_{p(x_{i},y_{j})}p(y_{j})$ , we obtain the following results: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{I^{S I}(X;Y)-I(X;Y)=\\displaystyle\\sum_{i,j}\\left[p(x_{i},y_{j})\\cdot\\log\\left[\\frac{1}{p(x_{i},y_{j})}\\cdot\\frac{2}{\\left[\\frac{1}{p(x_{i},y_{j})}\\right]^{\\epsilon_{i}}+\\left[\\frac{1}{p(x_{i},y_{j})}\\right]^{\\epsilon_{j}}}\\right]\\right]}\\\\ &{\\leq\\left(1-\\operatorname*{min}(\\epsilon_{i},\\epsilon_{j})\\right)\\cdot\\displaystyle\\sum_{i,j}\\left[p(x_{i},y_{j})\\cdot\\log\\frac{1}{p(x_{i},y_{j})}\\right]}\\\\ &{=\\left(1-\\operatorname*{min}(\\epsilon_{i},\\epsilon_{j})\\right)\\cdot H(X,Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ 0\\leq I^{S I}(X;Y)-I(X;Y)\\leq(1-\\operatorname*{min}(\\epsilon_{i},\\epsilon_{j}))\\cdot H(X,Y),}\\\\ &{I(X;Y)\\leq I^{S I}(X;Y)\\leq I(X;Y)+(1-\\operatorname*{min}(\\epsilon_{i},\\epsilon_{j}))\\cdot H(X,Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.3 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. For a one-to-one correspondence of variables $X$ and $Y$ , the joint probability of a tuple $(x_{i},y_{j})$ is as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\np(x_{i},y_{j})={\\binom{p(x_{i})=p(y_{j})\\quad{\\mathrm{if~}}i=j,}{0}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The calculation for $I^{S I}(X;Y)$ is carried out in the following manner: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{I^{S I}(X;Y)=\\displaystyle\\sum_{l=0}^{n}\\left[H^{S I}(X)+H^{S I}(Y)-H^{T_{x y}^{l}}(X,Y)\\right]}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}}\\\\ \\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, the calculation for $I(X;Y)$ proceeds as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{I(X;Y)=\\sum_{i,j}\\Bigg[p(x_{i},y_{j})\\cdot\\log\\frac{p(x_{i},y_{j})}{p(x_{i})\\cdot p(y_{j})}\\Bigg]}}\\\\ &{=\\displaystyle\\sum_{i}\\Bigg[p(x_{i})\\cdot\\log\\frac{1}{p(x_{i})}\\Bigg]}\\\\ &{=H(X).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, $I^{S I}(X;Y)$ equals $I(X;Y)$ when the joint distribution between $X$ and $Y$ shows a one-to-one correspondence. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.4 Proof of Proposition 4.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Now, we employ mathematical induction to demonstrate the existence of the graph $\\boldsymbol{G}_{s a}^{'}$ . Base Case $(n=2)$ ): Suppose the degree distribution of two vertices is given by $(p_{0},p_{1})$ with $p_{0}\\leq p_{1}$ . We construct the graph $\\boldsymbol{G}_{s a}^{\\prime}$ as follows:   \n\u2022 Create an edge with weight $p_{0}$ between $v_{0}$ and $v_{1}$ .   \n\u2022 Add a self-connected edge at vertex $v_{1}$ with weight $p_{1}-p_{0}$ .   \nInductive Step $(n=k)$ ): Assume that, the graph $\\boldsymbol{G}_{s a}^{\\prime}$ with $k$ vertices exists and satisfies Proposition 4.2.   \nInductive Case $(n=k+1)$ : Consider the addition of a new vertex $v_{k}$ to construct $\\boldsymbol{G}_{s a}^{\\prime}$ with $k+1$ ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "vertices using the following steps: ", "page_idx": 18}, {"type": "text", "text": "$\\bullet$ Start with a subgraph that includes the first $k$ vertices, yielding a distribution of $(p_{0}^{\\prime},\\dots,p_{k-1}^{\\prime})$ with  ik=\u221201 p\u2032i = 1.   \n\u2022 Modify the weight of all edges connected to each vertex vi (0 \u2264i \u2264k) by a factor (1\u2212pp\u2032n)pi. \u2022 For each vertex $v_{i}$ , create an edge with weight $p_{i}p_{n}$ connecting it to the new vertex $v_{k}$ .   \n\u2022 Add a self-connected edge at vertex $v_{k}$ with weight $p_{k}^{2}$ .   \nThese modifications ensure that the degree distribution of the graph remains consistent with the addition of the new vertex, thus completing the inductive step and proving the existence of $\\boldsymbol{G}_{s a}^{\\prime}$ for any $n$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.5 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the tree $T_{s a}^{*}$ , we denote the $i$ -th intermediate node as $\\alpha_{i}$ and its $j$ -th child node as $\\alpha_{i j}$ . The single state-action vertex in the corresponding subset of $\\alpha_{i j}$ is assumed as $(s_{i j},a_{i j})$ . In the graph $G_{s a}^{\\prime}$ , the degree of any state-action vertex $(s_{i j},a_{i j})$ is equated to its visitation probability, thereby: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{vol}(G_{s a}^{\\prime})=\\sum_{i,j}p(s_{i j},a_{i j})=1,\\quad\\operatorname{vol}(\\alpha_{i j})=g_{\\alpha_{i j}}=p(s_{i j},a_{i j}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The 2-dimensional value-conditional structural entropy $H^{T_{s a}^{*}}(G_{s a}^{\\prime})$ is calculated through the following expressions: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T^{T_{\\alpha}^{*}}(G_{s\\alpha}^{\\prime})=-\\displaystyle\\sum_{i}\\Bigg[g_{\\alpha_{i}}\\cdot\\log\\mathrm{vol}(\\alpha_{i})+\\displaystyle\\sum_{j}\\bigg[g_{\\alpha_{i j}}\\cdot\\log\\frac{\\mathrm{vol}(\\alpha_{i j})}{\\mathrm{vol}(\\alpha_{i})}\\bigg]\\Bigg]}\\\\ &{\\phantom{T^{T_{\\alpha}^{*}}}=-\\displaystyle\\sum_{i}\\Bigg[g_{\\alpha_{i}}\\cdot\\log\\mathrm{vol}(\\alpha_{i})+\\displaystyle\\sum_{j}\\bigg[p(s_{i j},a_{i j})\\cdot\\log\\frac{p(s_{i j},a_{i j})}{\\mathrm{vol}(\\alpha_{i})}\\bigg]\\Bigg]}\\\\ &{\\phantom{T^{T_{\\alpha}^{*}}}=\\displaystyle\\sum_{i,j}\\Bigg[p(s_{i j},a_{i j})\\cdot\\log\\frac{1}{p(s_{i j},a_{i j})}\\Bigg]-\\displaystyle\\sum_{i}\\Bigg[\\mathrm{vol}(\\alpha_{i})\\cdot\\log\\frac{1}{\\mathrm{vol}(\\alpha_{i})}\\Bigg]+\\sum_{i}\\bigg[g_{\\alpha_{i}}\\cdot\\log\\frac{1}{\\mathrm{vol}(\\alpha_{i})}}\\\\ &{\\phantom{T^{T_{\\alpha{i}}}}=H(V_{0})-H(V_{1})+\\displaystyle\\sum_{i}\\bigg[g_{\\alpha_{i}}\\cdot\\log\\frac{1}{\\mathrm{vol}(\\alpha_{i})}\\bigg]}\\\\ &{\\phantom{T^{T_{\\alpha{i}}}}\\geq H(V_{0})-H(V_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Given that $p(s_{i j},a_{i j})\\cdot\\mathrm{vol}(\\alpha_{i})\\leq p(s_{i j},a_{i j})\\leq\\mathrm{vol}(\\alpha_{i})$ , this inequalities hold that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n0\\leq\\log_{p(s_{i j},a_{i j})}\\frac{p(s_{i j},a_{i j})}{\\mathrm{vol}(\\alpha_{i})}\\leq1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By defining \u03b6ij = logp(sij,aij $\\begin{array}{r}{\\zeta_{i j}=\\log_{p(s_{i j},a_{i j})}\\frac{p(s_{i j},a_{i j})}{\\mathrm{vol}(\\alpha_{i})}}\\end{array}$ )p(vsoilj(,\u03b1aii)j ), we obtain that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{vol}(\\alpha_{i})\\cdot\\log\\frac{1}{\\mathrm{vol}(\\alpha_{i})}=\\displaystyle\\sum_{j}p(s_{i j},a_{i j})\\cdot\\frac{1}{\\mathrm{vol}(\\alpha_{i})}}&{}\\\\ {=\\displaystyle\\sum_{j}p(s_{i j},a_{i j})\\cdot\\log\\frac{1}{\\left[p(s_{i j},a_{i j})\\right]^{1-\\zeta_{i j}}}}&{}\\\\ {=\\displaystyle\\sum_{j}\\left[(1-\\zeta_{i j})\\cdot p(s_{i j},a_{i j})\\cdot\\log\\frac{1}{p(s_{i j},a_{i j})}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Selecting the minimal $\\zeta$ -value as $\\zeta^{*}$ allow us to reformulate Equation 33 as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{H(V_{1})=\\sum_{i}\\bigg[(\\mathrm{vol}(\\alpha_{i}))\\cdot\\log\\frac{1}{\\mathrm{vol}(\\alpha_{i})}\\bigg]\\le(1-\\zeta^{*})\\cdot H(S_{t},A_{t}),}&\\\\ &{\\qquad\\qquad\\qquad\\qquad H(V_{0})=H(S_{t},A_{t}),}&\\\\ &{\\zeta^{*}\\cdot H(S_{t},A_{t})\\le H(V_{0})-H(V_{1})\\le H^{T_{s a}^{*}}(G_{s a}^{\\prime})\\le H(S_{t},A_{t}).}&\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C Detailed Derivations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Derivation of $I^{S I}(X;Y)$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For each intermediate node $\\alpha_{i}\\in T_{x y}^{*}$ with vertex subset $\\{x_{i},y_{i}\\}$ , the entropy sum of this node and its children is calculated as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\,\\frac{g_{\\alpha_{i}}}{\\operatorname{vol}(G_{x y})}\\cdot\\log\\frac{\\operatorname{vol}(\\alpha_{i})}{\\operatorname{vol}(G_{x y})}-\\frac{p(x_{i})}{\\operatorname{vol}(G_{x y})}\\cdot\\log\\frac{p(x_{i})}{\\operatorname{vol}(\\alpha_{i})}-\\frac{p(y_{i})}{\\operatorname{vol}(G_{x y})}\\cdot\\log\\frac{p(y_{i})}{\\operatorname{vol}(\\alpha_{i})}}\\\\ &{=-\\,\\frac{g_{\\alpha_{i}}}{2}\\cdot\\log\\frac{\\operatorname{vol}(\\alpha_{i})}{2}-\\frac{p(x_{i})}{2}\\cdot\\log\\frac{p(x_{i})}{\\operatorname{vol}(\\alpha_{i})}-\\frac{p(y_{i})}{2}\\cdot\\log\\frac{p(y_{i})}{\\operatorname{vol}(\\alpha_{i})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The $\\begin{array}{r}{H^{S I}(X)+H^{S I}(Y)-H^{T_{x y}^{*}}(X,Y)}\\end{array}$ term in Equation 5 is calculated as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i}\\left[-{\\frac{p(x_{i})}{2}}\\cdot\\log{\\frac{p(x_{i})}{2}}-{\\frac{p(y_{i})}{2}}\\cdot\\log{\\frac{p(y_{i})}{2}}\\right]-\\sum_{i}\\left[-{\\frac{g_{\\alpha_{i}}}{2}}\\cdot\\log{\\frac{\\operatorname{vol}(\\alpha_{i})}{2}}-{\\frac{p(x_{i})}{2}}\\cdot\\log{\\frac{p(x_{i})}{\\operatorname{vol}(\\alpha_{i})}}-{\\frac{p(y_{i})}{2}}\\cdot\\log{\\frac{p(x_{i})}{2}}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proposition 3.1 ensures that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{vol}(\\alpha_{i})=p(x_{i})+p(y_{i}),\\ \\ \\ g_{\\alpha_{i}}=p(x_{i})+p(y_{i})-2p(x_{i},y_{i}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Consequently, we can reformulate Equation 40 as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i}\\left[\\frac{p(x_{i})}{2}\\cdot\\log\\frac{2}{p(x_{i})+p(y_{i})}+\\frac{p(y_{i})}{2}\\cdot\\log\\frac{2}{p(x_{i})+p(y_{i})}-\\frac{p(x_{i})+p(y_{i})-2\\cdot p(x_{i},y_{i})}{2}\\cdot\\log\\frac{1}{p(x_{i})}\\right]}\\\\ &{\\displaystyle=\\sum_{i}\\left[p(x_{i},y_{i})\\cdot\\log\\frac{2}{p(x_{i})+p(y_{i})}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For any integer $l>0$ , the $\\begin{array}{r}{H^{S I}(X)+H^{S I}(Y)-H^{T_{x y}^{l}}(X,Y)}\\end{array}$ term in Equation 5 is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i}\\left[p(x_{i^{\\prime}},y_{i})\\cdot\\log\\frac{2}{p(x_{i^{\\prime}})+p(y_{i})}\\right],\\;\\;\\;i^{\\prime}=(i+l)\\bmod n.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Consequently, ", "page_idx": 19}, {"type": "equation", "text": "$$\nI^{S I}(X;Y)=\\sum_{l=0}^{n}\\left[H^{S I}(X)+H^{S I}(Y)-H^{T_{x y}^{l}}(X,Y)\\right]=\\sum_{i,j}\\left[p(x_{i},y_{j})\\cdot\\log\\frac{2}{p(x_{i})+p(y_{j})}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C.2 Upper Bound of $I^{S I}(Z_{t};S_{t})$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem 3.4 assures the following inequality: ", "page_idx": 19}, {"type": "equation", "text": "$$\nI^{S I}(X;Y)\\leq I(X;Y)+(1-\\epsilon)\\cdot H(X,Y),~~0\\leq\\epsilon\\leq1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given the relationship between joint entropy and conditional entropy in traditional information theory, this inequality can be reformulated as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I^{S I}(X;Y)\\leq I(X;Y)+(1-\\epsilon)\\cdot H(X,Y)}\\\\ &{\\qquad\\qquad=I(X;Y)+(1-\\epsilon)\\cdot H(X|Y)+(1-\\epsilon)\\cdot H(Y)}\\\\ &{\\qquad\\qquad\\leq I(X;Y)+H(X|Y)+H(Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C.3 Upper Bound of $I(Z_{t};S_{t})$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Through the non-negativity of KL-divergence, the following upper bound of $I(Z_{t};S_{t})$ holds that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{I(Z_{t};S_{t})=\\sum\\Bigg[p(z_{t},s_{t})\\cdot\\log\\frac{p(z_{t}|s_{t})}{p(z_{t})}\\Bigg]}\\quad}&{}\\\\ &{=\\sum\\Bigg[p(z_{t},s_{t})\\cdot\\log\\frac{p(z_{t}|s_{t})}{q_{m}(z_{t})}\\Bigg]-D_{K L}(p||q_{m})}\\\\ &{\\leq\\sum\\big[p(z_{t},s_{t})\\cdot D_{K L}(p(z_{t}|s_{t})||q_{m}(z_{t}))\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C.4 Upper Bound of $H(Z_{t}|S_{t})$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Through the non-negativity of KL-divergence, the following upper bound of $H(Z_{t}|S_{t})$ holds that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H(Z_{t}|S_{t})=\\displaystyle\\sum\\left[p(z_{t},s_{t})\\cdot\\log\\frac{1}{p(z_{t}|s_{t})}\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum\\left[p(z_{t},s_{t})\\cdot\\log\\frac{1}{q_{z|s}(z_{t}|s_{t})}\\right]-D_{K L}(p||q_{z|s})}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum\\left[p(z_{t},s_{t})\\cdot\\log\\frac{1}{q_{z|s}(z_{t}|s_{t})}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C.5 Lower Bound of $I(Z_{t};S_{t+1})$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Leveraging the non-negative Shannon entropy and KL-divergence, we obtain the lower bound of $I(Z_{t};S_{t+1})$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I(Z_{t};S_{t+1})=\\displaystyle\\sum\\left[p(z_{t},s_{t+1})\\cdot\\log\\frac{p(s_{t+1}|z_{t})}{p(s_{t+1})}\\right]}\\\\ &{=\\displaystyle\\sum\\left[p(z_{t},s_{t+1})\\cdot\\log q_{s|z}(s_{t+1}|z_{t})\\right]+H(S_{t+1})+D_{K L}(p||q_{s|z})}\\\\ &{\\geq\\displaystyle\\sum\\left[p(z_{t},s_{t+1})\\cdot\\log q_{s|z}(s_{t+1}|z_{t})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the experiments conducted for this work, we utilize a single NVIDIA RTX A1000 GPU and eight Intel Core i9 CPU cores clocked at $3.00\\mathrm{GHz}$ for each training run. The total number of environmental steps was set to $3000K/1000K$ for the MiniGrid benchmark, $200K/100K$ for the MetaWorld benchmark, and $250K$ for the DeepMind Control Suite (DMControl). ", "page_idx": 21}, {"type": "text", "text": "D.1 Implementation Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "A2C implementation details. In our implementation of the A2C algorithm, we utilize the official RE3 implementation2, adhering to the pre-established hyperparameters set, except where explicitly noted. State representations in baselines are derived using a fixed encoder that is randomly initialized, and intrinsic rewards are normalized based on the standard deviation computed from sample data, consistent with the original methodology. However, this normalization process is omitted in the intrinsic reward calculations for VCSE and SI2E implementations. Across all exploration methods, we maintain fixed scale parameters $\\beta=0.005$ and $k=5$ , in line with the original framework. The comprehensive hyperparameters for the A2C algorithm are detailed in Table 4. ", "page_idx": 21}, {"type": "table", "img_path": "Bjh4mcYs20/tmp/671507f9f5145a21f07a474d44b78c5180df5f8f3a9eb8b991cb2e0d12abac91.jpg", "table_caption": ["Table 4: Hyperparameters for the A2C algorithm on the MiniGrid benchmark. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "DrQv2 implementation details. For the $\\mathrm{DrQv}2$ algorithm, we employ its official implementation3 [Yarats et al., 2021], maintaining the original hyperparameter settings unless specified otherwise. A fixed noise level of 0.2 and $k=12$ are used for all exploration methods, including SE, VCSE, MADE, and SI2E. In the calculation of intrinsic rewards in baselines, we train the Intrinsic Curiosity Module [Pathak et al., 2017] using representations from the visual encoder to measure vertex distance in the estimation of value-conditional structural entropy. Specific hyperparameters in the $\\mathrm{DrQv}2$ are summarized in Table 5. ", "page_idx": 21}, {"type": "table", "img_path": "Bjh4mcYs20/tmp/e45d793f455e57604bfb3ed5e591af4caf8db4267f68baf1a3bc3d982073360f.jpg", "table_caption": ["Table 5: Hyperparameters for the $\\mathrm{DrQv}2$ algorithm on the DeepMind Control Suite. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.2 Environment Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "MiniGrid Experiments. In our MiniGrid benchmark experiments, we encompass six navigation tasks, including RedBlueDoors, SimpleCorssingS9N1, KeyCorridor, DoorKey-6x6, DoorKey-8x8, and Unlock, with visual representations provided in Figure 6. Notably, all tasks are employed in their original forms without any modifications. ", "page_idx": 22}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/0ecf9a05d3603e944405a38ddb84893da5f9cdac40299c50fd7f64744c64cf20.jpg", "img_caption": ["Figure 6: Examples of navigation tasks used in our MiniGrid experiments include: (a) RedBlueDoors, (b) SimpleCorssingS9N1, (c) KeyCorridor, (d) DoorKey-6x6, (e) DoorKey-8x8, (f) Unlock. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "MetaWorld Experiments. In our evaluation using the MetaWorld benchmark, we conduct experiments on six manipulation tasks: Door Open, Drawer Open, Faucet Open, Window Open, Button Press, and Faucet Close. These tasks are visualized in Figure 7. ", "page_idx": 22}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/f5e82ed2db3f27c5d7847dbfc99eaa6e4a4920c0e03344b4ff3f9bcb827764c8.jpg", "img_caption": ["Figure 7: Examples of manipulation tasks used in our MetaWorld experiments include: (a) Door Open, (b) Drawer Open, (c) Faucet Open, (d) Window Open, (e) Button Press, (f) Faucet Close. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "DMControl Experiments. Our research in DMControl suite focues on six continuous control tasks, specifically Hopper Stand, Cheetah Run, Quadruped Walk, Pendulum Swingup, Cartpole Balance Sparse, and Cartpole Swingup Sparse. And visualizations of thes tasks are provided in Figure 8. ", "page_idx": 22}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/251e516298632ec009afe1ac45bc2518ee226de2fed0096f9ade6d3edeb9b4c5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 8: Examples of control tasks used in our DeepMind Control Suite experiments include: (a) Hopper Stand, (b) Cheetah Run, (c) Quadruped Walk, (d) Pendulum Swingup, (e) Cartpole Balance Sparse, (f) Cartpole Swingup Sparse. ", "page_idx": 22}, {"type": "text", "text": "E Additional Experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1 Experiments on MiniGrid Benchmark ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Figure 9 illustrates the learning curves for the A2C algorithm integrated with our SI2E framework, as well as with other exploration baselines, SE and VCSE. The corresponding variants are labeled as A2C, $\\mathrm{A2C+SE}$ , $\\mathrm{A2C+VCSE}$ , and $\\mathrm{A2C+SI2E}$ . These results demonstrate that SI2E consistently outperforms other baselines across various navigation tasks. In terms of final performance, $\\mathrm{A2C+SI2E}$ achieves an average success rate of $94.40\\%$ across all navigation tasks, significantly outperforming the second-best $\\mathrm{A2C+VCSE}$ , which has an average success rate of $89.78\\%$ . Regarding sample efficiency, SI2E converges in fewer than $50\\%$ of the total environmental steps in almost all tasks, except for DoorKey-8x8. This indicates SI2E\u2019s effectiveness in enhancing the agent\u2019s exploration of the state-action space, surpassing the baseline methods. ", "page_idx": 23}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/fcaece669754b2f02683b439ee2f395994f19f63042d625aed41154276b87140.jpg", "img_caption": ["Figure 9: Learning curves for six navigation tasks in MiniGrid, measured in terms of success rate. The solid lines represent the interquartile mean, while the shaded regions indicate the standard deviation, both calculated across 10 runs. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "To further substantiate our results on the Minigrid environment, we have introduced Leco[Jo et al., 2022] and DEIR[Wan et al., 2023], two additional state-of-the-art baselines representing advanced mechanisms for episodic intrinsic rewards. These results in Table 6 demonstrate that our method consistently maintains a performance advantage in terms of effectiveness and efficiency, even when compared to advanced episodic intrinsic reward mechanisms. ", "page_idx": 23}, {"type": "table", "img_path": "Bjh4mcYs20/tmp/d3e8d27e6c73980574f15a5b2bd8cf267b84d7353f4e49ef85aa71b76997318b.jpg", "table_caption": ["Table 6: Comparative results between SI2E and advanced episodic intrinsic reward mechanisms in MiniGrid benchmark: : \u201caverage value $\\pm$ standard deviation\" and \u201caverage improvement\" "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "E.2 Experiments on MetaWorld Benchmark ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Figure 10 summarizes the learning curves for the $\\mathrm{DrQv}2$ algorithm with different exploration methods, including SI2E (ours), SE, and VCSE. These variants are denoted as $\\mathrm{DrQv}2$ , $\\mathrm{DrOv}2{+}\\mathrm{SE}$ , $\\scriptstyle\\mathrm{DrOv}2+\\mathrm{VCSE}$ , and $\\mathrm{Dr}\\mathrm{Qv}2{+}\\mathrm{SI}\\mathrm{2}\\mathrm{E}$ , respectively. As shown in Figure 10, SI2E consistently and significantly achieves higher success rates with fewer environmental steps across all manipulation tasks. On average, SI2E improves the final performance by $10.21\\%$ and sample efficiency by $\\mathrm{45.06\\%}$ . ", "page_idx": 24}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/09897ae201c1aaddca4598582960b8d8c38ad4dfec5c702dcc1c6c567ff478ae.jpg", "img_caption": ["Figure 10: Learning curves for six manipulation tasks in MetaWorld, measured in terms of success rate. The solid lines represent the interquartile mean, while the shaded regions indicate the standard deviation, both calculated across 10 runs. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "To provide additional validation, we conducted further experiments in the MetaWorld environment using the advanced TACO [Zheng et al., 2024] as the underlying agent. The new experimental results, as documented in Table 7, demonstrate the robustness and superior performance of our method with different underlying agents. ", "page_idx": 24}, {"type": "text", "text": "Table 7: Summary of success rates and required steps for the TACO agent in MetaWorld tasks: \u201caverage value $\\pm$ standard deviation\" and \u201caverage improvement\". Bold: the best performance, underline: the second performance. ", "page_idx": 24}, {"type": "table", "img_path": "Bjh4mcYs20/tmp/06fe340da3d357188ff8b1f055fb82b5a79534b8c481cc75c7d85fe9ea21a931.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.3 Experiments on DMControl Suite ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For each task, we benchmark the convergence reward of the best-performing baseline as the target and track the environmental steps required by both SI2E and this baseline to reach the target. As illustrated in Figure 11, SISA demonstrates an average improvement of $35.60\\%$ in sample efficiency. This improvement is reflected in a reduction in the required environmental steps, decreasing from $31.83\\%$ to $20.5\\%$ of the total steps required to achieve the reward target. ", "page_idx": 25}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/f3dc181b3254ef5f46d283a184c9ac272794926ae2519fa8879f4a4f0626da2d.jpg", "img_caption": ["Figure 11: Comparison of sample efficiency between SI2E and the best-performing baseline in DMControl, focusing on the required environmental steps to reach the reward target, expressed as a proportion of the total $250K$ steps. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 12 shows the learning curves for the $\\mathrm{DrQv}2$ algorithm when integrated with our SI2E framework and other exploration baselines, SE, VCSE, and MADE. The variants are identified as DrQv2, $\\mathrm{DrOv}2{+}\\mathrm{SE}$ , $\\scriptstyle\\mathrm{DrOv}2+\\mathrm{VCSE}$ , $\\scriptstyle\\mathrm{DrOv}2+\\mathrm{MADE}$ , and $\\mathrm{Dr}\\mathrm{Qv}2{+}\\mathrm{SI}\\mathrm{2}\\mathrm{E}$ . These results reveal that SI2E exploration significantly improves the sample efficiency of DrQv2 in both sparse reward and dense reward tasks, outperforming all other baselines. Particular in sparse reward tasks (Cartpole Balance and Cartpole Swingup), our framework successfully accelerates training and achieves higher episode rewards. This suggests that SI2E avoids exploring states that may not contribute to task resolution, thereby enhancing performance with a $5.17\\%$ improvement in final performance and a $15.28\\%$ increase in sample efficiency. ", "page_idx": 25}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/85c7ed7ff0100ab3d90732c367f016f6adc32c78a740e41ddab407f9f331f594.jpg", "img_caption": ["Figure 12: Learning curves for six continuous control tasks from DMControl Suite, measured in terms of episode reward. The solid lines represent the interquartile mean, while the shaded regions indicate the standard deviation, both calculated across 10 runs. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "E.4 Visualization Experiment. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To understand the learned representation through our structural mutual information principle (Section 4.1), we use t-SNE to project the original observation $O$ and state-action embedding $Z$ into 2- dimensional vectors. Figure 13a visualizes these 2-dimensional vectors and measures the Euclidean distance between temporally consecutive vectors. Our presented principle aligns all movements on the same curve and reduces their distance by an average of 36.41, demonstrating its effectiveness for dynamics-relevant state-action embedding. ", "page_idx": 26}, {"type": "text", "text": "To analyze the beneftis of our intrinsic reward mechanism (Section 4.2), we keep the map configuration constant in the SimpleCrossing task and display the exploration paths of various methods in Figure 13b. Our SI2E, in comparison to the SE and VCSE baselines, prevents biased exploration towards low-value areas and effectively explores high-value crucial areas, like the crossing point, with fewer environmental steps, ensuring its performance superiority. ", "page_idx": 26}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/53278d05bfcbe9af2c5ac4cdaca9789da039e2af31ea48ea62a221f9d3512bdf.jpg", "img_caption": ["Figure 13: Visualization results of SI2E under DMControl and Minigrid tasks. (a) Visualization of representation learning based on structural mutual information principle. (b) Visualization of agent exploration through maximizing the value-conditional structural entropy. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Moreover, we have provided the visualizations of state coverage of our SI2E framework and other baselines (SE and VCSE) in the continuous Cartpole Balance task in Figure 14. Compared with the SE and VCSE baselines, our SI2E framework achieves a more efficient exploration strategy by minimizing occurrences in extreme positions and angles, resulting in a higher density in key areas. ", "page_idx": 26}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/dd05fe3829a5abd015724675d9fdb150e197cf33f52fa521dee7af5f1e3a3b9a.jpg", "img_caption": ["Figure 14: Visualization of agent exploration in the CartPole Balance task. Heat maps illustrate the final state densities for cart position and pole angle of the learned policies: $(\\mathrm{b})\\mathrm{DrQv}2\\mathrm{+SE}$ , (c) $\\scriptstyle\\mathrm{DrOv}2+\\mathrm{VCSE}$ , and (d) $\\mathrm{Dr}\\mathrm{Qv}2{+}\\mathrm{SI}\\mathrm{2}\\mathrm{E}$ . "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "E.5 Ablation Studies ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To investigate the influence of parameters $\\beta$ and $n$ on our framework\u2019s performance, we incrementally adjust these parameters across two distinct tasks: DMControl tasks Hopper Stand and Pendulum Swingup. We meticulously document the resulting learning curves to assess the outcomes. Figure 15a illustrates that an increase in parameter $\\beta$ consistently enhances performance across both tasks, substantiating the effectiveness of our exploration method. Conversely, Figure 15b demonstrates that variations in batch size $n$ yield comparable performance outcomes, particularly notable in Pendulum Swingup task, thereby confirming the SI2E\u2019s stability amidst variations in batch size. ", "page_idx": 27}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/5ff129d9e78c6598ecbdf1986c899864d5f73735a6717efdc917ba838b79bc27.jpg", "img_caption": ["(a) Effect of scale parameter $\\beta$ "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "Bjh4mcYs20/tmp/33bdadf837c65634ad634f4a633275ad5f67c2990f08175a8a2c98b66ccb1780.jpg", "img_caption": ["(b) Effect of batch size n "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 15: Learning curves of SI2E with varied $\\beta$ and $n$ values on Hopper Stand and Pendulum Swingup tasks. (a) shows the effect of the scale parameter $\\beta$ on episode reward. (b) shows the effect of the batch size $n$ on episode reward. The solid line represents the interquartile mean across 10 runs. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have clearly outlined the contributions and scope of our paper in the abstract and introduction. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We have outlined the limitations of our paper in the conclusion and the details are provided in our appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: For each theoretical result, we have included the complete set of assumptions and a correct proof in our appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The source codes are accessible via an anonymous link in the introduction, and specific experimental setups are provided in the appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The main experimental results can be replicated using the source codes accessible via an anonymous link provided in the introduction. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We have provided detailed experimental settings (hyperparameter and optimizer) in our evaluation and appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: In our evaluation section, we have conducted multiple trials with different random seeds, and the charts reflect the average performance and standard deviation. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have provided sufficient information on the computer resources in our appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our paper strictly maintains anonymity. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our work is focused on fundamental research in reinforcement learning and is not specifically tied to any particular applications. Furthermore, there is no direct path to negative applications. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We appropriately cited the original papers for our experimental datasets, including MiniGrid, MetaWorld, and the DeepMind Control Suite. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]