[{"figure_path": "njvPjG0BfK/tables/tables_7_1.jpg", "caption": "Table 1: Evaluation of generated datasets on LEC data. Hardness level (%): percentage of runtime of generated dataset relative to original dataset, closer to 100% is better. Speed (s): average time cost to generate one instance, lower is better. Maximum Mean Discrepancy (MMD): distance between distributions of generated and original datasets, lower is better.", "description": "This table compares four different methods for generating synthetic SAT problem instances based on their hardness, generation speed, and similarity to real-world instances.  Hardness is measured as the percentage of runtime compared to original instances; a higher percentage indicates harder instances. Speed represents the average time to generate a single instance. Finally, similarity is quantified using the Maximum Mean Discrepancy (MMD), measuring the difference in distribution between generated and original instances; a lower MMD signifies greater similarity.", "section": "6.2.1 Question 1: Is the method able to generate hard instances?"}, {"figure_path": "njvPjG0BfK/tables/tables_9_1.jpg", "caption": "Table 1: Evaluation of generated datasets on LEC data. Hardness level (%): percentage of runtime of generated dataset relative to original dataset, closer to 100% is better. Speed (s): average time cost to generate one instance, lower is better. Maximum Mean Discrepancy (MMD): distance between distributions of generated and original datasets, lower is better.", "description": "This table compares the performance of different SAT instance generation methods on a dataset of LEC (Logic Equivalence Checking) instances.  It evaluates three key aspects: \n1. **Hardness**: The percentage of the original dataset's runtime achieved by the generated instances (closer to 100% indicates better preservation of hardness). \n2. **Speed**: The average time taken to generate a single instance (lower is better). \n3. **Similarity**:  The Maximum Mean Discrepancy (MMD) between the distributions of generated and original instances (lower values indicate higher similarity). The table shows that HardCore achieves a good balance between hardness and speed compared to other methods.", "section": "6 Experiments and Results"}, {"figure_path": "njvPjG0BfK/tables/tables_11_1.jpg", "caption": "Table 1: Evaluation of generated datasets on LEC data. Hardness level (%): percentage of runtime of generated dataset relative to original dataset, closer to 100% is better. Speed (s): average time cost to generate one instance, lower is better. Maximum Mean Discrepancy (MMD): distance between distributions of generated and original datasets, lower is better.", "description": "This table presents a comparison of different SAT instance generation methods on their ability to generate hard instances similar to real-world examples, as measured by runtime and distribution similarity.  It shows the hardness level (percentage of original runtime), generation speed (seconds per instance), and Maximum Mean Discrepancy (MMD) for each method, indicating their performance in creating realistic and challenging instances for machine learning.", "section": "6 Experiments and Results"}, {"figure_path": "njvPjG0BfK/tables/tables_12_1.jpg", "caption": "Table 1: Evaluation of generated datasets on LEC data. Hardness level (%): percentage of runtime of generated dataset relative to original dataset, closer to 100% is better. Speed (s): average time cost to generate one instance, lower is better. Maximum Mean Discrepancy (MMD): distance between distributions of generated and original datasets, lower is better.", "description": "This table presents a comparison of different SAT generation methods on a specific dataset (LEC).  The comparison focuses on three key metrics: Hardness (percentage of original runtime), Speed (time to generate a single instance), and Similarity (measured by Maximum Mean Discrepancy, or MMD, between the generated and original data distributions). Lower MMD indicates greater similarity. The table helps assess the efficiency and quality of each generation method in terms of producing instances that are both challenging (hard) and similar to real-world problems.", "section": "6.2 Experiments and Results"}, {"figure_path": "njvPjG0BfK/tables/tables_13_1.jpg", "caption": "Table 1: Evaluation of generated datasets on LEC data. Hardness level (%): percentage of runtime of generated dataset relative to original dataset, closer to 100% is better. Speed (s): average time cost to generate one instance, lower is better. Maximum Mean Discrepancy (MMD): distance between distributions of generated and original datasets, lower is better.", "description": "This table presents a comparison of different SAT generation methods on LEC data.  It evaluates three key metrics: Hardness (the percentage of runtime of generated problems relative to original problems), Speed (the average time needed to generate a single instance), and Similarity (measured by Maximum Mean Discrepancy, a lower value indicating greater similarity between the generated and original dataset distributions).", "section": "6.2.1 Question 1: Is the method able to generate hard instances?"}, {"figure_path": "njvPjG0BfK/tables/tables_14_1.jpg", "caption": "Table 2: MAE of Runtime Prediction averaged across 7 solvers and 15 trials. Asterisks are placed at the best result which passes the Wilcoxon pairwise ranking test against the second-best for p < 0.05. For a boxplot visualization showing each trials result, see Appendix Figure 7", "description": "This table presents the Mean Absolute Error (MAE) of runtime prediction for different dataset sizes, averaged across seven solvers and fifteen trials.  The data shows the MAE for runtime prediction models trained on both original data and data augmented with the HardCore method. The Wilcoxon signed-rank test was used to determine statistical significance (p<0.05) between HardCore augmented data and the original data, with asterisks marking statistically significant improvements. A supplementary figure provides a box plot visualization of the results.", "section": "6.2.4 Question 4: Can we successfully augment training data with the method's generated data for machine learning?"}, {"figure_path": "njvPjG0BfK/tables/tables_15_1.jpg", "caption": "Table 6: Data Augmentation experiment: MAE of Runtime Prediction averaged across 7 solvers and 15 trials. We train a runtime prediction model according to the experimental setting in 6.4.2 of the paper. Columns in the table indicate the number of original problems used in the training set (we generate 4 times per original problem in the training set). Results in the \u201cHardCore\u201d row are MAE for a runtime prediction model trained on HardCore-augmented data, whereas \u201cOriginal\u201d indicates un-augmented performance.", "description": "This table presents the Mean Absolute Error (MAE) of runtime prediction for different training data sizes using the HardCore method and the original data.  It shows that using HardCore-augmented data leads to lower MAE compared to using only the original data.", "section": "6.2.4 Question 4: Can we successfully augment training data with the method's generated data for machine learning?"}]