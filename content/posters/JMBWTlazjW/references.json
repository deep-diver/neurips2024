{"references": [{"fullname_first_author": "L. Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-02", "reason": "This paper is foundational for learning from preference feedback, introducing a widely used approach and influencing many subsequent works."}, {"fullname_first_author": "R. Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-05-18", "reason": "This paper presents DPO, a popular and efficient algorithm for learning from preference feedback, offering a computationally less intensive alternative to PPO."}, {"fullname_first_author": "H. Ivison", "paper_title": "Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2", "publication_date": "2023-MM-DD", "reason": "This paper introduces T\u00dcLU 2, the model on which the current study is based, providing essential context and demonstrating the state-of-the-art for this model."}, {"fullname_first_author": "G. Cui", "paper_title": "Ultrafeedback: Boosting language models with high-quality feedback", "publication_date": "2023-10-01", "reason": "This study explores the impact of high-quality preference data, finding that its quality significantly affects the performance of models, and its analysis is vital for the current work."}, {"fullname_first_author": "J. Wei", "paper_title": "Chain of thought prompting elicits reasoning in large language models", "publication_date": "2022-01-11", "reason": "This paper introduces Chain of Thought prompting, a technique shown to improve reasoning capabilities in LLMs, a key aspect investigated and enhanced in the current research."}]}