[{"figure_path": "JMBWTlazjW/figures/figures_1_1.jpg", "caption": "Figure 1: Performance improvements resulted by changing different components in the preference training of T\u00dcLU. Left: Accuracy on GSM [9], for testing math capabilities. Right: Overall performance, aggregated over the 11 benchmarks described in \u00a72.2.", "description": "This figure shows the impact of different factors on the performance of a language model trained using preference feedback.  The left panel shows accuracy on the GSM (Grade School Math) benchmark, focusing on mathematical reasoning. The right panel displays the overall performance across 11 different benchmarks. Each bar represents a model trained with a specific combination of factors, such as the quality of preference data, learning algorithm (DPO or PPO), and reward model. The figure highlights the importance of each of these factors in achieving good performance. Notably, PPO generally outperforms DPO, and high-quality preference data leads to the most significant improvement.", "section": "3 Exploring Learning from Preference Feedback"}, {"figure_path": "JMBWTlazjW/figures/figures_2_1.jpg", "caption": "Figure 2: The core aspects of learning from preference feedback. For DPO (solid line), preference data is directly used to train a policy model. For PPO (dashed line), preference data is used to train a reward model, which is then used to score model-generated responses during PPO training.", "description": "This figure illustrates the core components of two prominent preference-based learning algorithms: Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO).  DPO directly trains a policy model using preference data, represented by prompts, responses, and rankings. In contrast, PPO uses a two-stage approach. First, it trains a reward model on the preference data. Then, this reward model scores the responses generated by the policy model during training. These scores guide the policy model's training, resulting in improved response quality based on the learned preferences.", "section": "2 Setup"}, {"figure_path": "JMBWTlazjW/figures/figures_7_1.jpg", "caption": "Figure 3: Policy training prompt math evaluation: Performance of models trained on 20K prompts from varying sources using PPO and evaluated on GSM. Training with larger RMs trained on more data benefits more from in-domain prompts (i.e., prompts directly from the GSM train set), while weaker RMs struggle to generalize beyond their training prompts.", "description": "This figure shows the results of an experiment where the authors trained models using different sets of prompts in the PPO algorithm, and evaluated their performance on the GSM math task.  They varied the size and training data of the reward model, and the source of the policy training prompts (UltraFeedback, prompts specifically mined for math, or prompts directly from the GSM training set). The results indicate that larger reward models trained on more data perform significantly better when using prompts matched to the test setting (GSM training prompts), demonstrating the importance of tailoring prompts to the specific task.", "section": "3.4 Policy Training Prompts"}, {"figure_path": "JMBWTlazjW/figures/figures_22_1.jpg", "caption": "Figure 4: Performance of models trained with 13B and 70B UltraF. RMs with varying KL coefficient values. (a) GSM Accuracy, (b) AlpacaEval 2 winrate, (c) Overall performance across entire evaluation suite. Overall best performance occurs at different KL coefficient values for different reward models. However, AlpacaEval 2 performance grows with reduced KL coefficient values.", "description": "This figure shows the effect of different KL penalty coefficients (\u03b2) on the performance of models trained with 13B and 70B UltraFeedback Reward Models (RMs).  It presents three subfigures:\n\n(a) GSM Accuracy:  Illustrates the accuracy on the GSM (GSM8k) benchmark as \u03b2 varies.\n(b) AlpacaEval 2 Winrate: Shows the winrate on the AlpacaEval 2 benchmark as \u03b2 varies.\n(c) Overall Average: Displays the overall average performance across all evaluation metrics.\n\nThe results indicate that the optimal \u03b2 value depends on the RM size. The 70B RM shows more robustness to changes in \u03b2 compared to the 13B RM.  Interestingly, AlpacaEval 2 performance increases as \u03b2 decreases, suggesting a trade-off between performance on different benchmarks when tuning \u03b2.", "section": "I KL Penalty Coefficient Exploration"}, {"figure_path": "JMBWTlazjW/figures/figures_24_1.jpg", "caption": "Figure 6: Performance of all evaluations over PPO training steps when training using the 70B UltraFeedback RM and UltraFeedback prompts for 3 epochs. Grey dashed lines indicate epoch boundaries.", "description": "This figure displays the performance of eleven different evaluation metrics across various training steps during the PPO training process.  The training utilizes the 70B UltraFeedback Reward Model and UltraFeedback prompts, extending over three epochs.  Each metric's performance trajectory is plotted, revealing the changes over time. The grey dashed lines demarcate epoch boundaries for easier visualization and understanding of the training progress.", "section": "L Model Performance over Training"}, {"figure_path": "JMBWTlazjW/figures/figures_24_2.jpg", "caption": "Figure 7: Performance on (left) AlpacaEval 2, (middle) IFEval, (right) GSM8k over PPO training steps when training using the 70B UltraFeedback RM and UltraFeedback prompts for 3 epochs. Grey dashed lines indicate epoch boundaries.", "description": "This figure displays the performance change over training steps for three different evaluation metrics: AlpacaEval 2, IFEval, and GSM8k.  The training utilized the 70B UltraFeedback Reward Model and UltraFeedback prompts over three epochs. Dashed lines in the graph mark the boundaries between each epoch.", "section": "Model Performance over Training"}]