[{"figure_path": "JMBWTlazjW/tables/tables_4_1.jpg", "caption": "Table 1: Preference data: Performance of T\u00dcLU 2 13B models trained on various preference datasets using DPO. Blue indicates improvements over the SFT baseline, orange degradations. Overall, synthetic data works best. DPO training improves truthfulness and instruction-following most, with limited to no improvements in factuality and reasoning.", "description": "This table presents the results of training a 13B parameter T\u00dcLU language model using Direct Preference Optimization (DPO) on fourteen different preference datasets.  The table shows the model's performance across several evaluation categories (Factuality, Reasoning, Coding, Truthfulness, Safety, and Instruction Following).  The performance is compared against a baseline model trained with supervised fine-tuning (SFT).  The color-coding highlights improvements (blue) and degradations (orange) compared to the baseline. The overall findings indicate that synthetic datasets tend to yield the best results, and DPO training primarily improves truthfulness and instruction-following capabilities.", "section": "3.1 Preference Data"}, {"figure_path": "JMBWTlazjW/tables/tables_4_2.jpg", "caption": "Table 2: DPO vs PPO: Average performance of 13B models trained using DPO and PPO across different datasets, along with the performance difference between DPO and PPO (\u0394). Blue indicates improvements over the SFT baseline, orange degradations. All datasets are downsampled to 60,908 examples (except ChatArena, which is made up of 20,465 responses). PPO outperforms DPO by an average of 0.7 points, where most improvements are in reasoning, coding, and chat capabilities.", "description": "This table compares the performance of DPO and PPO on several datasets.  It shows the average performance across various evaluation categories (factuality, reasoning, coding, etc.) for both algorithms, highlighting the difference in performance between them.  The table emphasizes that PPO generally outperforms DPO, particularly in reasoning, coding, and conversational abilities.", "section": "3.2 Preference Learning Algorithm: DPO vs. PPO"}, {"figure_path": "JMBWTlazjW/tables/tables_5_1.jpg", "caption": "Table 3: Reward model evaluation: (a) Direct evaluation: reward models when directly evaluated using RewardBench [26] and Best-of-N (left two columns); for BoN, we report both raw average performance, and the difference in performance over the base SFT model in brackets. (b) Downstream evaluation: models trained using PPO and the given reward model (right three columns). We report GSM and AlpacaEval 2 performance along with average performance across the entire evaluation suite defined in \u00a72.2. Directly comparing reward models indicate increasing scale and data improves reward models, but these only minimally impact downstream performance.", "description": "This table presents the results of evaluating reward models both directly and indirectly through downstream PPO training. Direct evaluation uses RewardBench and Best-of-N metrics to assess the quality of the reward models. Downstream evaluation measures the impact of using different reward models on the performance of PPO-trained policy models across various benchmarks, including GSM and AlpacaEval2.", "section": "3.3 Reward Models"}, {"figure_path": "JMBWTlazjW/tables/tables_7_1.jpg", "caption": "Table 4: Policy training prompt overall evaluation: Performance of PPO policy models trained with the given reward models on 60K prompts from either UltraFeedback or the remixed prompt set that adds additional unlabeled math and coding-related prompts. Using the remixed prompt set does not improve performance, either on specific evaluations (math, code) or in terms of overall performance.", "description": "This table shows the results of an experiment evaluating the effect of different policy training prompts on the performance of models trained using PPO with different reward models.  Two sets of prompts were used: UltraFeedback prompts and a remixed set of prompts that included additional math and coding-related prompts. The table reports the average performance across multiple evaluation metrics. The findings suggest that using the remixed prompt set did not significantly improve model performance.", "section": "3.4 Policy Training Prompts"}, {"figure_path": "JMBWTlazjW/tables/tables_8_1.jpg", "caption": "Table 5: Putting together a recipe for preference-based learning: Performance of our best-performing models along with popular open models based on Llama 2 13B. 'MP' refers to using the mixed prompt set described in \u00a74. 'L3' stands for experiments using Llama 3 as a base model. Using PPO with a large reward model performs best overall.", "description": "This table presents the performance comparison of different language models, including several popular open-source models and the authors' best-performing models.  The models are evaluated across multiple benchmarks measuring various capabilities like factuality, reasoning, coding, truthfulness, safety, and instruction following. The table highlights the superior performance of models trained using the Proximal Policy Optimization (PPO) algorithm with a large reward model, especially when combined with a mixed set of prompts tailored for specific tasks.", "section": "A Recipe for Learning from Preferences"}, {"figure_path": "JMBWTlazjW/tables/tables_15_1.jpg", "caption": "Table 6: Size of the subsplits of our RM mixture data.", "description": "This table shows the number of samples used for each of the six datasets in the reward model (RM) mixture data.  The datasets were subsampled to create a balanced mixture for training the reward model, except for HelpSteer and PRM800k, which retained their original sizes. The total number of samples in the RM mixture data is 259,851.", "section": "3.3 Reward Models"}, {"figure_path": "JMBWTlazjW/tables/tables_17_1.jpg", "caption": "Table 1: Preference data: Performance of T\u00dcLU 2 13B models trained on various preference datasets using DPO. Blue indicates improvements over the SFT baseline, orange degradations. Overall, synthetic data works best. DPO training improves truthfulness and instruction-following most, with limited to no improvements in factuality and reasoning.", "description": "This table presents the results of training the T\u00dcLU 2 13B language model using the DPO (Direct Preference Optimization) algorithm on fourteen different preference datasets.  The table shows the performance of the resulting models across several evaluation metrics, including factuality, reasoning, coding, truthfulness, safety, and instruction following.  The color-coding highlights datasets where the model performance improved (blue) or decreased (orange) compared to a standard Supervised Fine-Tuning (SFT) baseline.  The overall findings suggest that synthetic preference datasets generally lead to better model performance compared to real-world datasets.", "section": "3.1 Preference Data"}, {"figure_path": "JMBWTlazjW/tables/tables_18_1.jpg", "caption": "Table 7: Performance of varied sub splits of UltraFeedback, considering both samples using generations from models of varying strength (weak, middle, strong model gen.) strong and only considering samples using prompts from single datasets (e.g., FalseQA, Flan V2). Sampling from different-quality models makes relatively little difference, while prompt choice matters. Additionally, different source datasets provide improvements in different evaluations.", "description": "This table shows the results of using different subsets of the UltraFeedback dataset.  It explores the impact of model quality (weak, middle, strong) on performance when generating responses, as well as the performance of using prompts from different datasets.  The findings suggest that model quality has a relatively small impact compared to prompt source, and different sources improve performance on different evaluation metrics.", "section": "3.1 Preference Data"}, {"figure_path": "JMBWTlazjW/tables/tables_20_1.jpg", "caption": "Table 1: Preference data: Performance of T\u00dcLU 2 13B models trained on various preference datasets using DPO. Blue indicates improvements over the SFT baseline, orange degradations. Overall, synthetic data works best. DPO training improves truthfulness and instruction-following most, with limited to no improvements in factuality and reasoning.", "description": "This table presents the performance of models trained using Direct Preference Optimization (DPO) on various preference datasets. The performance is measured on several downstream tasks related to language model capabilities.  The results show that synthetic datasets generally lead to better performance than human-annotated or web-scraped data.  The use of DPO improves instruction following and truthfulness more than other aspects (factuality and reasoning).", "section": "3.1 Preference Data"}, {"figure_path": "JMBWTlazjW/tables/tables_20_2.jpg", "caption": "Table 10: Variations in implementation of the PPO algorithm. Compared with serveral open-source PPO implementations: Quark [33], Rainier [30], Crystal [31], FG-RLHF [58], and AlpacaFarm [14].", "description": "This table compares different implementations of the Proximal Policy Optimization (PPO) algorithm, highlighting key differences in their approaches.  It contrasts our implementation with those from several other open-source projects (Quark, Rainier/Crystal, FG-RLHF, AlpacaFarm) by detailing the presence or absence of specific techniques.  These techniques include initializing the value model from the reward model, using the EOS trick for truncated completions, reward normalization, reward whitening, advantage whitening, the use of an adaptive KL controller, KL clamping, and employing multiple rollouts per prompt.", "section": "3.3 Reward Models"}, {"figure_path": "JMBWTlazjW/tables/tables_21_1.jpg", "caption": "Table 11: Performance of PPO under bigger prompt batch size. The first row uses the same experiment setup as the PPO model trained on UltraFeedback (FG), as in Table 2. Hyperparameter notations are same as Table 9: B = prompt batch size, b = minibatch size for forward-backward passes, r = # rollouts per prompt, g = gradient accumulation. Number of training examples, gradient update batch size, and total number of gradient updates are relative to the first row. We keep the total number of gradient updates fixed, train all models for 1 epoch. Increasing the prompt batch size can speed up PPO training at some performance cost, and most performance loss can be recovered by increasing r and g (which effectively increases the gradient update batch size).", "description": "This table compares the performance of the Proximal Policy Optimization (PPO) algorithm under different prompt batch sizes. It shows that increasing the batch size can reduce training time, but at the cost of some performance degradation. However, this performance loss can be mitigated by increasing the number of rollouts and gradient accumulation steps.", "section": "3.4 Policy Training Prompts"}, {"figure_path": "JMBWTlazjW/tables/tables_21_2.jpg", "caption": "Table 2: DPO vs PPO: Average performance of 13B models trained using DPO and PPO across different datasets, along with the performance difference between DPO and PPO (\u0394). Blue indicates improvements over the SFT baseline, orange degradations. All datasets are downsampled to 60,908 examples (except ChatArena, which is made up of 20,465 responses). PPO outperforms DPO by an average of 0.7 points, where most improvements are in reasoning, coding, and chat capabilities.", "description": "This table compares the performance of two preference-based learning algorithms, DPO and PPO, across multiple datasets.  It shows the average performance scores for several categories (Factuality, Reasoning, Coding, Truthfulness, Safety, Instruction Following) and highlights the difference in performance between DPO and PPO for each dataset. The results indicate that PPO generally outperforms DPO, particularly in reasoning, coding, and chat-related tasks.", "section": "3.2 Preference Learning Algorithm: DPO vs. PPO"}, {"figure_path": "JMBWTlazjW/tables/tables_21_3.jpg", "caption": "Table 13: Full results from RewardBench evaluation for results shown in Table 3. Score is a weighted average of subsets with prior sets given weight 0.5 and all other sets given weight 1, following Lambert et al. [26].", "description": "This table presents the detailed results of evaluating reward models using the RewardBench dataset.  Reward models of varying sizes (13B and 70B) trained on different datasets (UltraFeedback and a mixture of datasets) are assessed. The \"Score\" column reflects the overall performance across various sub-categories of RewardBench (Chat, Chat Hard, Safety, Reasoning, Prior Sets), with prior sets weighted differently than other categories as per Lambert et al. [26].  This provides a more granular view of the reward model performance compared to the summary in Table 3.", "section": "3.3 Reward Models"}]