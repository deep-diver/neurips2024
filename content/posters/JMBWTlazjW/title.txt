Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback