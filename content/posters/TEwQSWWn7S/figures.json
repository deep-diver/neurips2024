[{"figure_path": "TEwQSWWn7S/figures/figures_2_1.jpg", "caption": "Figure 1: Clustered datasets. In A-C we show the 4 different clustered data sets that we will consider in this paper, projected onto their first two PCA components. In A we show the data of the MNIST 01 dataset (both projected and some instances), which contains only the 0-1 images of the complete MNIST dataset. In B, we show the Mickey dataset, an artificial dataset whose PCA forms a \u201cMickey\u201d face shape. In C, we show data from the Human Genome Dataset (HGD), which contains binary vectors each corresponding to a human individual and whose sites correspond to selected genes. A value of 1 at a particular position means that a mutation was observed there compared to an individual reference sequence. Details of these data sets can be found in the SI. In D-F we show the samples we generate with the low-rank RBMs that are used as initial point of a standard training.", "description": "This figure shows four different clustered datasets projected onto their first two principal components using PCA.  The datasets include MNIST 0/1 (digits 0 and 1 from the MNIST dataset), Mickey (an artificial dataset), Human Genome Dataset (HGD), and low-rank RBM generation of each dataset. Each plot displays the dataset's distribution and the samples generated by the low-rank RBM used as starting points for standard RBM training.  The figure demonstrates the complexity of the datasets and the ability of the low-rank RBMs to capture their key features.", "section": "1 Introduction"}, {"figure_path": "TEwQSWWn7S/figures/figures_5_1.jpg", "caption": "Figure 2: We compare the equilibrium samples generated by RBMs trained on the Mickey, MNIST01, and HGD datasets using three different training schemes: Jarzynski (JarRBM), PCD, and PCD initialized on low-rank RBMs (used to generate the samples in Fig. 1\u2013D-F). To assess the fitting of the modes, we show a density plot of the projections of the data in the first two principal directions of each dataset. We compare these results with the density plot of the original datasets in the first column.", "description": "This figure compares equilibrium samples generated by Restricted Boltzmann Machines (RBMs) trained on three different datasets (Mickey, MNIST01, and HGD) using three different training methods: Jarzynski, Persistent Contrastive Divergence (PCD), and PCD initialized with low-rank RBMs. Density plots of the data projected onto the first two principal components are shown for each dataset and training method, allowing for a visual comparison of how well each method captures the modes of the data.", "section": "4 The low-rank RBM pretrained"}, {"figure_path": "TEwQSWWn7S/figures/figures_6_1.jpg", "caption": "Figure 3: We compare the samples generated by the 3 RBMs (JarRBM, PCD, pretrain+PCD) trained with MNIST01 data. In A, we show the histograms of the generated data projected on the first, second and third principal directions with those of the dataset. We see that only the pretrain+PCD correctly balances the different modes. In B we show 10 images generated by each machine. In C, we compare the log-likelihood of each model's dataset as a function of training time. The dark and full curves were obtained using the PTT algorithm discussed in section 5, and the lighter and dashed curves using the AIS method [42].", "description": "This figure compares the results obtained by three different training methods (Jarzynski, PCD, and pre-train+PCD) on the MNIST01 dataset.  Panel A shows histograms of generated data projected onto the first three principal components, demonstrating that only pre-train+PCD accurately represents the data's modes. Panel B displays sample images generated by each method. Panel C plots log-likelihood versus training time, highlighting the superior performance of pre-train+PCD.  The PTT sampling method is shown to be superior to AIS.", "section": "3 The Restricted Boltzmann Machine"}, {"figure_path": "TEwQSWWn7S/figures/figures_7_1.jpg", "caption": "Figure 4: Comparison between PTT and classical Gibbs sampling for the MNIST01 dataset (A and B, respectively) and the human genome dataset (C and D, respectively). In A and C, we show the trajectory of two independent chains (red and orange) projected onto the PCA along the sampling process of the pretraining+PCD model for 104 MCMC steps. The black contour represents the density profile of the dataset and the position of the chains is plotted every 10 steps. In B and D we show the average number of jumps from one cluster to another as a function of the MCMC steps performed. The average is calculated over a population of 100 chains. In D, we show the average jump time between clusters along the first (solid line) and second (dashed line) principal components of the data.", "description": "This figure compares the performance of Parallel Trajectory Tempering (PTT) and standard Gibbs sampling in exploring the equilibrium distribution of Restricted Boltzmann Machines (RBMs) trained on MNIST01 and Human Genome datasets.  Panels A and C visualize the sampling trajectories projected onto the first two principal components, illustrating PTT's ability to efficiently traverse between distinct clusters compared to Gibbs sampling. Panels B and D quantitatively show the average number of jumps between clusters over time, demonstrating PTT's superior mixing speed.", "section": "5 Standard Gibbs sampling vs. Parallel Trajectory Tempering (PTT)"}, {"figure_path": "TEwQSWWn7S/figures/figures_8_1.jpg", "caption": "Figure 5: We compare the quality of the RBMs trained with the human genome data (HGD). In A, we show the log-likelihood as a function of the training epochs for the 3 training procedures. Solid lines correspond to AIS-PTT and dashed lines to AIS. The JarRBM falls down because the training breaks eventually. In B and C we compare privacy and overfitting based on the AATs indicator.", "description": "This figure compares the performance of three different RBM training methods (PCD, Jarzynski, and pre-train+PCD) on the Human Genome Dataset (HGD). Panel A shows the log-likelihood, using both AIS-PTT and AIS methods, indicating that pre-train+PCD leads to higher log-likelihood compared to other methods. Panel B illustrates the privacy loss, which is lower for pre-train+PCD, suggesting better privacy preservation.  Finally, Panel C presents the AATruth and AAsyn values, demonstrating that pre-train+PCD achieves a better balance between the real and synthetic data compared to the other methods, meaning better generalization and lower risk of overfitting.", "section": "6 Overfitting and privacy loss as quality indicators"}, {"figure_path": "TEwQSWWn7S/figures/figures_14_1.jpg", "caption": "Figure 6: Scheme of PTT. We Initialize the chains of the models by starting from a configuration x<sup>(0)</sup> and passing it through the machines along the training trajectory, each time performing k mcmc steps. For pre-train+PCD, x<sup>(0)</sup> is a sampling from the RCM, otherwise it is a uniform random initialization. The sampling consists of alternating one mcmc step for each model with a swap attempt between adjacent machines. For pre-train+PCD, at each step we sample a new independent configuration for RBM<sub>0</sub> using the RCM.", "description": "This figure illustrates the Parallel Trajectory Tempering (PTT) algorithm.  The algorithm initializes multiple Markov chains (represented by different colors) using a low-rank Restricted Boltzmann Machine (RBM) or random initialization. It then iteratively performs a Gibbs sampling step within each model, followed by a swap attempt between neighboring models along the training trajectory (models saved at different stages of training).  The acceptance probability for the swap is determined by the energy difference between the models.  For the pre-train+PCD method, a new independent sample is drawn from the RCM at each step, while for JarRBM and PCD methods, a standard Gibbs sampling step is performed. This process helps the chains efficiently explore the model's equilibrium distribution by crossing only second-order phase transitions during training.", "section": "5 Standard Gibbs sampling vs. Parallel Trajectory Tempering (PTT)"}, {"figure_path": "TEwQSWWn7S/figures/figures_16_1.jpg", "caption": "Figure 4: Comparison between PTT and classical Gibbs sampling for the MNIST01 dataset (A and B, respectively) and the human genome dataset (C and D, respectively). In A and C, we show the trajectory of two independent chains (red and orange) projected onto the PCA along the sampling process of the pretraining+PCD model for 104 MCMC steps. The black contour represents the density profile of the dataset and the position of the chains is plotted every 10 steps. In B and D we show the average number of jumps from one cluster to another as a function of the MCMC steps performed. The average is calculated over a population of 100 chains. In D, we show the average jump time between clusters along the first (solid line) and second (dashed line) principal components of the data.", "description": "This figure compares the Parallel Trajectory Tempering (PTT) and Gibbs sampling methods for two datasets: MNIST01 and Human Genome.  The top row (A and B) shows the results for MNIST01, illustrating the trajectories of two chains in the PCA space (A) and a comparison of the average number of jumps between clusters over time (B).  The bottom row (C and D) displays the equivalent for the Human Genome dataset. The plots reveal that PTT significantly accelerates the mixing and jump rate between data clusters, allowing for faster convergence to equilibrium during sampling.", "section": "5 Standard Gibbs sampling vs. Parallel Trajectory Tempering (PTT)"}, {"figure_path": "TEwQSWWn7S/figures/figures_16_2.jpg", "caption": "Figure 4: Comparison between PTT and classical Gibbs sampling for the MNIST01 dataset (A and B, respectively) and the human genome dataset (C and D, respectively). In A and C, we show the trajectory of two independent chains (red and orange) projected onto the PCA along the sampling process of the pretraining+PCD model for 104 MCMC steps. The black contour represents the density profile of the dataset and the position of the chains is plotted every 10 steps. In B and D we show the average number of jumps from one cluster to another as a function of the MCMC steps performed. The average is calculated over a population of 100 chains. In D, we show the average jump time between clusters along the first (solid line) and second (dashed line) principal components of the data.", "description": "This figure compares the performance of Parallel Trajectory Tempering (PTT) and Gibbs sampling for two different datasets: MNIST01 and Human Genome.  It demonstrates that PTT significantly improves the mixing time (the ability to explore different modes of the data), especially in highly structured datasets.  Subplots A and C show the trajectories of Markov chains in the PCA space highlighting the increased exploration of PTT compared to Gibbs sampling. Subplots B and D quantitatively show the average number of jumps between clusters over time for each method across multiple independent runs.", "section": "5 Standard Gibbs sampling vs. Parallel Trajectory Tempering (PTT)"}]