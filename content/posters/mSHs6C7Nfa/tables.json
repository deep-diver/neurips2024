[{"figure_path": "mSHs6C7Nfa/tables/tables_2_1.jpg", "caption": "Table 1: Effects of the improved training techniques. The baseline (config A) is the 2-rectified flow with the uniform timestep distribution and the squared l2 metric [Liu et al., 2022]. Config B is the improved baseline with EDM initialization (Sec. 4.3) and increased batch size (128 \u2192 512 on CIFAR-10). FID (the lower the better) is computed using 50,000 synthetic samples and the entire training set. We train the models for 800,000 iterations on CIFAR-10 and 1,000,000 iterations on AFHQ and FFHQ and report the best FID for each setting.", "description": "This table presents the results of an ablation study comparing different training techniques for 2-rectified flows. It shows how various configurations impact the Fr\u00e9chet Inception Distance (FID) score, a metric for evaluating the quality of generated images. The configurations involve changes to the timestep distribution, loss function, model initialization, and incorporation of real data.", "section": "4 Improved Training Techniques for Reflow"}, {"figure_path": "mSHs6C7Nfa/tables/tables_5_1.jpg", "caption": "Table 1: Effects of the improved training techniques. The baseline (config A) is the 2-rectified flow with the uniform timestep distribution and the squared l2 metric [Liu et al., 2022]. Config B is the improved baseline with EDM initialization (Sec. 4.3) and increased batch size (128 \u2192 512 on CIFAR-10). FID (the lower the better) is computed using 50,000 synthetic samples and the entire training set. We train the models for 800,000 iterations on CIFAR-10 and 1,000,000 iterations on AFHQ and FFHQ and report the best FID for each setting.", "description": "This table presents the results of an ablation study evaluating different training techniques for 2-rectified flows on three datasets: CIFAR-10, AFHQ 64x64, and FFHQ 64x64. It shows how FID (Frechet Inception Distance), a metric for image quality, changes with different training techniques, including different loss functions and timestep distributions, both for one and two NFE (number of function evaluations).  The baseline uses the squared l2 distance and a uniform timestep distribution. The improved configurations show the FID improvements sequentially as each technique is applied.", "section": "4 Improved Training Techniques for Reflow"}, {"figure_path": "mSHs6C7Nfa/tables/tables_7_1.jpg", "caption": "Table 3: Unconditional generation on CIFAR-10. Table 4: Class-conditional generation on ImageNet 64 \u00d7 64.", "description": "This table presents a comparison of unconditional image generation results on CIFAR-10 and class-conditional image generation results on ImageNet 64x64.  It compares various methods, including diffusion models, distilled diffusion models, GANs, consistency models, and rectified flows, across different numbers of function evaluations (NFEs), evaluating performance using FID (Frechet Inception Distance) and, where applicable, Inception Score (IS), Precision, and Recall.", "section": "5.1 Unconditional and class-conditional image generation"}, {"figure_path": "mSHs6C7Nfa/tables/tables_8_1.jpg", "caption": "Table 6: Comparison of the number of forward passes. Reflow uses 395M forward passes for generating pairs and 1, 433.6M for training.", "description": "This table compares the computational cost of Reflow with two other distillation methods, CD and CT, in terms of the number of forward passes required during training.  It shows that while Reflow initially requires generating synthetic data-noise pairs, the overall number of forward passes for Reflow is lower than CD and slightly lower than CT, suggesting Reflow could be computationally efficient despite this initial step.", "section": "5.2 Reflow can be computationally more efficient than other distillation methods"}, {"figure_path": "mSHs6C7Nfa/tables/tables_17_1.jpg", "caption": "Table 1: Effects of the improved training techniques. The baseline (config A) is the 2-rectified flow with the uniform timestep distribution and the squared l2 metric [Liu et al., 2022]. Config B is the improved baseline with EDM initialization (Sec. 4.3) and increased batch size (128 \u2192 512 on CIFAR-10). FID (the lower the better) is computed using 50,000 synthetic samples and the entire training set. We train the models for 800,000 iterations on CIFAR-10 and 1,000,000 iterations on AFHQ and FFHQ and report the best FID for each setting.", "description": "This table presents the effects of various training techniques on the performance of 2-rectified flow.  It compares a baseline configuration with several improved variations, showing the impact of different timestep distributions, loss functions, and initializations. The results are measured by FID scores across three datasets: CIFAR-10, AFHQ 64x64, and FFHQ 64x64.", "section": "4 Improved Training Techniques for Reflow"}]