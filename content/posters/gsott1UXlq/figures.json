[{"figure_path": "gsott1UXlq/figures/figures_2_1.jpg", "caption": "Figure 1: Architecture of GATSM.", "description": "This figure shows the architecture of the Generalized Additive Time Series Model (GATSM). It consists of two main modules: 1) Time-Sharing Neural Basis Model (NBM) for learning feature representations, and 2) Masked Multi-Head Attention (MHA) for learning temporal patterns. The Time-Sharing NBM uses shared basis functions across all time steps and features to reduce the number of learnable parameters. The Masked MHA takes the output of the Time-Sharing NBM, adds positional encoding, and learns temporal patterns using a masked self-attention mechanism. Finally, the output of the Masked MHA is combined with the transformed features from the Time-Sharing NBM to produce the final prediction.", "section": "4 Our Method: Generalized Additive Time Series Model"}, {"figure_path": "gsott1UXlq/figures/figures_6_1.jpg", "caption": "Figure 1: Architecture of GATSM.", "description": "The figure shows the architecture of the Generalized Additive Time Series Model (GATSM). It consists of two main modules: 1) Time-Sharing Neural Basis Model (NBM) for learning feature representations, which shares weights across all time steps to reduce the number of learnable parameters; and 2) Masked Multi-Head Attention (MHA) for learning temporal patterns, which uses positional encoding to capture temporal relationships effectively. The NBM transforms input features into feature representations, and the MHA integrates these representations across different time steps to generate final predictions. This architecture allows GATSM to efficiently capture temporal patterns, handle dynamic-length time series, and maintain transparency.", "section": "4 Our Method: Generalized Additive Time Series Model"}, {"figure_path": "gsott1UXlq/figures/figures_7_1.jpg", "caption": "Figure 1: Architecture of GATSM.", "description": "This figure presents the architecture of the Generalized Additive Time Series Model (GATSM).  It shows the two main modules: 1) independent feature networks (Time-Sharing NBM) which learn feature representations for each input feature across all time steps, and 2) a transparent temporal module (masked multi-head attention) which learns temporal patterns from the feature representations.  The figure illustrates how the feature networks use shared weights across time steps to handle dynamic-length time series effectively. The masked multi-head attention processes the combined feature representations to capture dependencies between different time steps. Finally, a linear combination of the transformed features and the attention outputs produces the model's prediction.", "section": "4 Our Method: Generalized Additive Time Series Model"}, {"figure_path": "gsott1UXlq/figures/figures_7_2.jpg", "caption": "Figure 1: Architecture of GATSM.", "description": "The figure shows the architecture of the Generalized Additive Time Series Model (GATSM). It consists of two main modules: 1) Time-Sharing Neural Basis Model (NBM) for learning feature representations and 2) Masked Multi-Head Attention (MHA) for learning temporal patterns.  The Time-Sharing NBM uses shared basis functions across all time steps and features to reduce the number of parameters, and the MHA captures temporal dependencies between time steps using positional encoding and a masking mechanism to prevent information leakage from the future. The final predictions are generated by integrating the feature representations from NBM and the temporal information from MHA.", "section": "4 Our Method: Generalized Additive Time Series Model"}, {"figure_path": "gsott1UXlq/figures/figures_20_1.jpg", "caption": "Figure 1: Architecture of GATSM.", "description": "The figure shows the architecture of the Generalized Additive Time Series Model (GATSM).  It consists of two main modules: 1) Time-Sharing Neural Basis Model (NBM) for learning feature representations, and 2) Masked Multi-Head Attention (MHA) for learning temporal patterns. The NBM uses shared basis functions across all time steps and features to reduce the number of parameters, while the MHA captures temporal dependencies between time steps. The model outputs final predictions by combining the feature representations and temporal information.", "section": "4 Our Method: Generalized Additive Time Series Model"}, {"figure_path": "gsott1UXlq/figures/figures_22_1.jpg", "caption": "Figure 1: Architecture of GATSM.", "description": "The figure shows the architecture of the Generalized Additive Time Series Model (GATSM). It consists of two main modules: 1) feature networks, which are time-sharing neural basis models (NBM) that learn feature representations by sharing weights across all time steps; and 2) a temporal module, which is a masked multi-head attention mechanism that learns temporal patterns independently. The feature representations from the NBM and the temporal information from the masked MHA are integrated to generate final predictions.  The diagram illustrates the flow of data through these modules, highlighting the key components and their interactions.", "section": "4 Our Method: Generalized Additive Time Series Model"}, {"figure_path": "gsott1UXlq/figures/figures_22_2.jpg", "caption": "Figure 1: Architecture of GATSM.", "description": "The figure shows the architecture of the Generalized Additive Time Series Model (GATSM). It consists of two main modules: 1) Time-Sharing Neural Basis Model (NBM) for learning feature representations, and 2) Masked Multi-Head Attention (MHA) for learning temporal patterns. The Time-Sharing NBM uses shared basis functions across all time steps to reduce the number of parameters, and the MHA captures temporal dependencies between time steps using an attention mechanism.  The final predictions are generated by combining the feature representations from NBM with the temporal information from MHA.", "section": "4 Our Method: Generalized Additive Time Series Model"}, {"figure_path": "gsott1UXlq/figures/figures_22_3.jpg", "caption": "Figure 1: Architecture of GATSM.", "description": "The figure illustrates the architecture of the Generalized Additive Time Series Model (GATSM). It consists of two main modules: 1) feature networks (Time-Sharing NBM) that learn feature representations by using shared weights across all time steps, and 2) a temporal module (Masked MHA) that learns temporal patterns across different time steps using the feature representations. The feature networks employ a multi-layer perceptron (MLP) to transform input features, and the temporal module uses a masked multi-head attention mechanism to capture temporal dependencies. The final predictions are generated by integrating the feature representations with temporal information from the temporal module.  The figure provides a visual representation of data flow within each module and how they combine for prediction.", "section": "4 Our Method: Generalized Additive Time Series Model"}, {"figure_path": "gsott1UXlq/figures/figures_22_4.jpg", "caption": "Figure 1: Architecture of GATSM.", "description": "This figure shows the architecture of the Generalized Additive Time Series Model (GATSM).  It consists of two main modules: 1) Feature Networks (Time-Sharing NBM):  This module uses shared neural basis models to learn feature representations across all time steps, efficiently handling dynamic time series. The output is passed to the 2) Temporal Module (Masked MHA): This module utilizes masked multi-head attention to capture temporal patterns and relationships between different time steps. The combination of these modules produces the final predictions. The figure details the flow of data through each component, highlighting the weight sharing and masked attention mechanisms.", "section": "4 Our Method: Generalized Additive Time Series Model"}]