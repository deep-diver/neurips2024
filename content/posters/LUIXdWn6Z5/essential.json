{"importance": "This paper is **crucial** for researchers in reinforcement learning and optimal control because it **unifies** existing frameworks, offers **novel risk-sensitive RL algorithms**, and **bridges the gap** between control theory and Bayesian inference.  The findings **advance risk-sensitive control**, offering **robust and efficient** methods for solving complex problems.", "summary": "Risk-sensitive control is recast as inference using R\u00e9nyi divergence, yielding new algorithms and revealing equivalences between seemingly disparate methods.", "takeaways": ["Risk-sensitive control as inference (RCaI) is proposed, unifying existing frameworks and showing equivalences between risk-sensitive control, MaxEnt control, and linearly solvable control.", "Novel risk-sensitive reinforcement learning methods, policy gradient and soft actor-critic are derived based on RCaI, demonstrating equivalence to risk-neutral methods as risk sensitivity vanishes.", "The duality between exponential integrals and R\u00e9nyi entropy is established, providing another risk-sensitive generalization of MaxEnt control with a similar optimal policy structure."], "tldr": "Optimal control often assumes full knowledge of system dynamics, while reinforcement learning tackles uncertainty.  Control as inference (CaI) connects optimal control and Bayesian inference, offering solutions for complex RL problems.  However, CaI's reliance on the Kullback-Leibler (KL) divergence limits its ability to address risk-sensitive problems where policy robustness is crucial. \nThis paper introduces **risk-sensitive control as inference (RCaI)** which extends CaI using R\u00e9nyi divergence, a generalized measure of information difference.  RCaI is shown to be equivalent to log-probability regularized risk-sensitive control and offers a unifying framework. The authors derive **risk-sensitive RL algorithms** (policy gradient and soft actor-critic) based on RCaI.  As risk sensitivity vanishes, these algorithms revert to their risk-neutral counterparts. The analysis also provides another risk-sensitive generalization of MaxEnt control using R\u00e9nyi entropy, demonstrating that both approaches yield similar optimal policies despite different derivations. ", "affiliation": "University of Tokyo", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "LUIXdWn6Z5/podcast.wav"}