{"references": [{"fullname_first_author": "On\u00e9simo Hern\u00e1ndez-Lerma", "paper_title": "Discrete-time Markov Control Processes: Basic Optimality Criteria", "publication_date": "1996", "reason": "This paper provides the foundational mathematical framework for optimal control theory, which is essential for understanding the theoretical underpinnings of the risk-sensitive control methods discussed in the main paper."}, {"fullname_first_author": "Richard S. Sutton", "paper_title": "Reinforcement Learning: An Introduction", "publication_date": "2018", "reason": "This is a foundational textbook in reinforcement learning, providing the background for the reinforcement learning algorithms used in the risk-sensitive control framework."}, {"fullname_first_author": "Sergey Levine", "paper_title": "Reinforcement learning and control as probabilistic inference: Tutorial and review", "publication_date": "2018", "reason": "This review paper establishes the connection between optimal control and probabilistic inference, which is crucial for understanding the control-as-inference (CaI) framework extended in the main paper."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018", "reason": "This paper introduces the soft actor-critic algorithm, a key method in reinforcement learning that is extended to the risk-sensitive setting in the main paper."}, {"fullname_first_author": "Yingzhen Li", "paper_title": "R\u00e9nyi divergence variational inference", "publication_date": "2016", "reason": "This paper introduces R\u00e9nyi divergence variational inference, a crucial theoretical tool used in the main paper to extend the control-as-inference framework to risk-sensitive settings."}]}