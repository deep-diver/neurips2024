{"references": [{"fullname_first_author": "Arthur Jacot", "paper_title": "Neural Tangent Kernel: Convergence and generalization in neural networks", "publication_date": "2018-12-01", "reason": "This paper introduced the Neural Tangent Kernel, a key concept for understanding the behavior of neural networks in the infinite-width limit, which is heavily used in the analysis presented in this paper."}, {"fullname_first_author": "Song Mei", "paper_title": "A mean field view of the landscape of two-layer neural networks", "publication_date": "2018-01-01", "reason": "This paper provided a mean-field analysis of the loss landscape of two-layer neural networks, which is foundational for understanding the dynamics of deep learning models, as referenced several times in the present study."}, {"fullname_first_author": "L\u00e9na\u00efc Chizat", "paper_title": "On lazy training in differentiable programming", "publication_date": "2019-12-01", "reason": "This paper introduced the concept of \"lazy training,\" explaining why deep neural networks may not learn hierarchical features as initially hypothesized, a key concept related to the current paper's analysis of feature learning."}, {"fullname_first_author": "Blake Bordelon", "paper_title": "Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit", "publication_date": "2023-09-16", "reason": "This recent work explored depthwise hyperparameter scaling in residual networks and is directly compared to the present paper's findings on hyperparameter scaling."}, {"fullname_first_author": "Samy Jelassi", "paper_title": "Depth dependence of \u00b5-P learning rates in ReLU MLPs", "publication_date": "2023-05-07", "reason": "This work is directly relevant to the current work, focusing specifically on the depth dependence of learning rates and the behavior of ReLU MLPs, providing a key theoretical foundation for the current paper's results."}]}