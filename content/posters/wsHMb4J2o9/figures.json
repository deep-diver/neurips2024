[{"figure_path": "wsHMb4J2o9/figures/figures_6_1.jpg", "caption": "Figure 1: Backward-Feature Angle (BFA) \u03b8\u2082 observed at initialization in MLPs (\u03b2 = 1) and ResNets (width m = 200), for a few random realizations. (a) for all architectures, BFA \u03b8\u2082 varies in the first few layers and then stabilizes. (b) BFA at output layer \u03b8L\u22121 is asymptotically independent of depth, with a non-trivial angle only when \u03b2\u221d 1/\u221aL (same color scheme as (a)). (c) for a branch scale \u03b2 = c/\u221aL, the factor c directly determines asymptotic output BFA \u03b8L\u22121 (averaged over 5 draws).", "description": "This figure shows the Backward-Feature Angle (BFA) at initialization for MLPs and ResNets with varying architectures. Panel (a) demonstrates that the BFA changes in the initial layers before stabilizing. Panel (b) shows that the BFA at the output layer is asymptotically independent of depth, but only when the branch scale is proportional to 1/\u221aL. Lastly, panel (c) illustrates that the asymptotic output BFA is directly determined by the branch scale.", "section": "5.1 BFA for single input MLPs and ResNets at initialization"}, {"figure_path": "wsHMb4J2o9/figures/figures_6_2.jpg", "caption": "Figure 1: Backward-Feature Angle (BFA) \u03b8\u2082 observed at initialization in MLPs (\u03b2 = 1) and ResNets (width m = 200), for a few random realizations. (a) for all architectures, BFA \u03b8\u2082 varies in the first few layers and then stabilizes. (b) BFA at output layer \u03b8L\u22121 is asymptotically independent of depth, with a non-trivial angle only when \u03b2\u221d 1/\u221aL (same color scheme as (a)). (c) for a branch scale \u03b2 = c/\u221aL, the factor c directly determines asymptotic output BFA \u03b8L\u22121 (averaged over 5 draws).", "description": "This figure visualizes the Backward-Feature Angle (BFA) at initialization for both MLPs and ResNets.  Panel (a) shows that the BFA changes in the initial layers before stabilizing. Panel (b) demonstrates that the BFA at the output layer is independent of depth, but only if \u03b2 is proportional to 1/\u221aL. Panel (c) shows the relationship between the output BFA and branch scale.", "section": "5.1 BFA for single input MLPs and ResNets at initialization"}, {"figure_path": "wsHMb4J2o9/figures/figures_6_3.jpg", "caption": "Figure 1: Backward-Feature Angle (BFA) \u03b8\u2113 observed at initialization in MLPs (\u03b2 = 1) and ResNets (width m = 200), for a few random realizations. (a) for all architectures, BFA \u03b8\u2113 varies in the first few layers and then stabilizes. (b) BFA at output layer \u03b8L\u22121 is asymptotically independent of depth, with a non-trivial angle only when \u03b2\u221d 1/\u221aL (same color scheme as (a)). (c) for a branch scale \u03b2 = c/\u221aL, the factor c directly determines asymptotic output BFA \u03b8L\u22121 (averaged over 5 draws).", "description": "This figure shows the Backward-Feature Angle (BFA) at initialization for Multilayer Perceptrons (MLPs) and Residual Networks (ResNets).  Panel (a) demonstrates that the BFA varies initially across layers but stabilizes. Panel (b) illustrates that the output layer's BFA (\u03b8L\u22121) is largely independent of depth, exhibiting a non-trivial value only when the branch scale (\u03b2) is proportional to 1/\u221aL.  Finally, panel (c) reveals a direct relationship between the asymptotic output BFA and the normalized branch scale (\u221aL\u22c5\u03b2).", "section": "5.1 BFA for single input MLPs and ResNets at initialization"}, {"figure_path": "wsHMb4J2o9/figures/figures_8_1.jpg", "caption": "Figure 2: Sensitivities SL\u22121 (see Eq. (4)) of the last layer of activations (gL\u22121 in the MLP and fL\u22121 in the ResNet) computed via the formula ||\u03b4fL\u22121||rms/|SL| where \u03b4 denotes the change after one GD step (master learning rate dt = 0.01, d = 10, n = 1 input sample on the unit sphere and k = 1). (a) ReLU MLP of width m = 400. From our theory we have for NTK, S = \u0398(1/\u221am) (close to 0 and constant with depth); for MF+\u00b5P S = \u0398(\u221aL) and for the FSC S = \u0398(1) (b) ReLU ResNet of width m = 400: for both choices of branch scale, the sensitivities is stable around a nonzero value.", "description": "This figure compares the sensitivities of the output feature for three different hyperparameter scalings (NTK, MF+\u00b5P, and FSC) across various depths of ReLU MLPs and ReLU ResNets.  The sensitivity, represented by S, measures the proportionality between the loss decay and the speed of feature updates. The plots show that the NTK scaling has consistently low sensitivity across depths, whereas MF+\u00b5P scaling shows increasing sensitivity with depth. Notably, the FSC scaling maintains stable and relatively constant sensitivity across different depths for both MLPs and ResNets, indicating its potential advantages for controlling training dynamics.", "section": "5.2 Characterizing HP scalings for MLPs"}, {"figure_path": "wsHMb4J2o9/figures/figures_8_2.jpg", "caption": "Figure 2: Sensitivities SL-1 (see Eq. (4)) of the last layer of activations (GL-1 in the MLP and fL-1 in the ResNet) computed via the formula ||8fL\u22121||rms/|SL| where 8 denotes the change after one GD step (master learning rate dt = 0.01, d = 10, n = 1 input sample on the unit sphere and k = 1). (a) ReLU MLP of width m = 400. From our theory we have for NTK, S = \u0398(1/\u221am) (close to 0 and constant with depth); for MF+\u00b5P S = \u0398(\u221aL) and for the FSC S = \u0398(1) (b) ReLU ResNet of width m = 400: for both choices of branch scale, the sensitivities is stable around a nonzero value.", "description": "The figure shows the sensitivity of the last layer of activations for ReLU MLP and ReLU ResNet. The sensitivity is computed using the formula ||\u03b4fL\u22121||rms/|SL|, where \u03b4 represents the change after one gradient descent step.  For the ReLU MLP, the sensitivity is close to 0 for NTK, increases with depth for MF+\u00b5P and stays stable around 1 for FSC. For the ReLU ResNet, the sensitivity is stable for both branch scales \u03b2=1/\u221aL and \u03b2=1/L. This illustrates how different hyperparameter scalings affect the training dynamics of neural networks.", "section": "5.2 Characterizing HP scalings for MLPS"}]