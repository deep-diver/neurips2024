[{"figure_path": "VikufBLOW1/tables/tables_5_1.jpg", "caption": "Table 1: Comparison with the state of the art on OVEN entity and query test splits. We report the harmonic mean (HM) of the seen and unseen sets (top-1 accuracy) before and after finetuning on OVEN training seen categories (\u201c+ seen finetune\u201d). We indicate model architectures and their total number of parameters (\u201c# par.\u201d) in billions as well as the training dataset details.", "description": "This table compares the performance of different models on the Open-domain Visual Entity recognitioN (OVEN) benchmark.  It shows the harmonic mean (HM) of top-1 accuracy for both seen and unseen entities, before and after fine-tuning on the OVEN training set.  The table also provides information on model architectures, the number of parameters, and the training datasets used for each model.", "section": "4 Experiments"}, {"figure_path": "VikufBLOW1/tables/tables_5_2.jpg", "caption": "Table 2: Zero-shot transfer of generative models to finegrained image classification. We report top-1 accuracies. All models are run by us and are based on the same architecture.", "description": "This table presents the results of a zero-shot transfer learning experiment on fine-grained image classification datasets.  The authors used several pre-trained generative models (GiT-Large) trained on different datasets (WebLI-100M, Entity-WebLI, REW-47M) and evaluated their performance on five different fine-grained datasets (Flowers, Sun397, Food, Aircraft, Sports100).  The table shows the top-1 accuracy for each model and dataset combination, highlighting the impact of different training data on transfer learning performance. The key takeaway is that using the automatically curated dataset (REW-47M) created by their proposed method leads to superior zero-shot transfer performance compared to models trained on existing datasets.", "section": "4 Experiments"}, {"figure_path": "VikufBLOW1/tables/tables_6_1.jpg", "caption": "Table 3: Visual matching. We report top-1 accuracies of visual matching with CLIP or DINOv2 ViT-L/14 visual backbones. We compare two types of annotations for the visual matching memory database: either the candidate entities or our multimodal LLM corrected entities. We report the absolute improvements of using the latter compared to the former between parentheses as well as the relative improvement averaged across the six datasets in the last column.", "description": "This table presents the results of visual matching experiments using two different visual backbones (CLIP-L/14 and DINOv2-L/14).  The experiments compare the performance of visual matching when using two types of annotations for the memory database: candidate entities (from the original dataset) and multimodal LLM-corrected entities (refined using the model's approach).  The table shows top-1 accuracy for each dataset, along with the absolute and relative improvements achieved by using the LLM-corrected entities.", "section": "Visual matching"}, {"figure_path": "VikufBLOW1/tables/tables_7_1.jpg", "caption": "Table 4: Importance of entity verification and correction. We report HM top-1 accuracy on OVEN validation entity split. To isolate the effect of the entity target source, we train only with entity targets (i.e. only with loss Lentity). We do not perform seen finetuning and evaluate the models directly after pretraining. All models are trained with the same pretraining images and use the same architecture.", "description": "This table shows the impact of using a multimodal LLM for entity verification and correction on the performance of a visual entity recognition model.  Four different approaches are compared, varying whether the LLM is used for direct entity prediction, or for correction of initial entity candidates, and whether the LLM has access to additional context (Wikipedia and original caption). The results demonstrate that using the LLM for correction, especially with access to additional context, significantly improves accuracy.", "section": "Analysis and ablation study"}, {"figure_path": "VikufBLOW1/tables/tables_8_1.jpg", "caption": "Table 5: Ablation study: (left): Generating rationales and QA pairs with multimodal LLM. (right): Multi-task training with generated rationales and QAs. We validate the robustness to the base image-caption dataset used by performing the latter ablation with both WebLI [9] and LAION [46]. We report HM top-1 accuracy on OVEN validation splits directly after REW training.", "description": "This table presents the results of an ablation study to investigate the impact of using a multimodal LLM to generate rationales and question-answer pairs for web-scale visual entity recognition. The left side shows the impact of different metadata provided to the LLM during rationale and QA generation on the OVEN validation set performance. The right side demonstrates the robustness of multi-task training with the generated data by comparing results using two different base image-caption datasets, WebLI and LAION, showing consistent improvements when including rationales and QAs in the training.", "section": "4.3 Analysis and ablation study"}, {"figure_path": "VikufBLOW1/tables/tables_9_1.jpg", "caption": "Table 6: Results with open source PaliGemma and Gemma models. We report HM top-1 accuracy on OVEN validation entity split directly after REW training. We use a 5M subset for all the pretraining datasets.", "description": "This table compares the performance of models trained on different datasets using two open-source LLMs, PaliGemma and Gemma, against the state-of-the-art results.  It showcases the impact of using the LLM-refined datasets (REW)  on model performance in the entity and query split of the OVEN benchmark.", "section": "4.4 Results with open source models"}, {"figure_path": "VikufBLOW1/tables/tables_13_1.jpg", "caption": "Table 7: Retrieval-enhanced contrastive training (RECO). We compare three types of annotations for memory database: original caption, candidate entities or our multimodal LLM corrected entities. We report the absolute improvements over the CLIP baseline between parentheses as well as the relative improvement averaged across the datasets in the last column. For reference, we also include the results from Iscen et al. [18] but note that they use a memory base 20\u00d7 bigger (WebLI-1B).", "description": "This table compares the performance of Retrieval-enhanced contrastive training (RECO) using three different types of annotations for the memory database: original captions, candidate entities from Entity-WebLI, and corrected entities from the proposed REW dataset.  The table shows top-1 accuracy on six fine-grained image classification datasets (Cars, CUB, ImNet, Flowers, Places, Dogs).  Absolute and relative improvements over a CLIP-L/14 baseline are reported, along with a comparison to previous RECO results using a much larger memory database.", "section": "4.3 Analysis and ablation study"}, {"figure_path": "VikufBLOW1/tables/tables_15_1.jpg", "caption": "Table 9: Statistical significance. We report the harmonic mean (HM) of the seen and unseen splits (top-1 accuracy) on OVEN training seen categories for 5 different seeds. We use the small version of REW for this experiment. We report the mean and the standard deviation in the last row.", "description": "This table shows the statistical significance of the experiments by running the model training five times with different random seeds.  The table reports the harmonic mean (HM) of top-1 accuracy on the OVEN training seen categories for both the entity and query splits, showing the mean and standard deviation of the results.", "section": "A.3.2 Statistical significance of the experiments"}]