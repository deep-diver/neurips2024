[{"figure_path": "VUuOsBrqaw/tables/tables_4_1.jpg", "caption": "Table 1: Comparative Analysis of Model Components in PCA, Contrastive Pre-training Models (CL), and the proposed FUG", "description": "This table compares the components of PCA, contrastive learning models, and the proposed FUG method.  It highlights the presence or absence of three key components: Dim(\u00b7) (a dimension-specific encoder), Fea(\u00b7) (an encoder of relationships between dimensions), positive samples (X+), and negative samples (X-).  The table shows that PCA only uses Dim(\u00b7) and Fea(\u00b7), while traditional contrastive learning uses Fea(\u00b7), X+, and X-. The FUG method, however, includes all four components, aiming for a more comprehensive and adaptable approach.", "section": "3 Graph Contrastive Pretrained Models from the PCA perspective"}, {"figure_path": "VUuOsBrqaw/tables/tables_6_1.jpg", "caption": "Table 2: Cross-domain Node Classification. The performances of OFA and GraphControl are reported from [17, 18]. FUG-C (Cora) means FUG-C (Cora) means FUG-C model pre-trained on Cora dataset.", "description": "This table presents the results of cross-domain node classification experiments.  It compares the performance of the proposed FUG model (trained on different datasets) against state-of-the-art models (OFA and GraphControl). The results demonstrate FUG's ability to adapt to datasets with different node feature shapes and its competitive performance compared to models that either reshape features or ignore node attributes.", "section": "5 Experiments"}, {"figure_path": "VUuOsBrqaw/tables/tables_7_1.jpg", "caption": "Table 3: Intra-domain Model Re-building Node Classification. Data without standard deviation are reported from previous works [5, 42, 32]. Aside from GCN, the best results are highlighted in bold, and the second-best results are underlined. OOM indicates Out-Of-Memory on an RTX 3090 GPU.", "description": "This table presents the results of node classification experiments conducted within the same dataset (intra-domain).  It compares the performance of various methods, including FUG, against state-of-the-art self-supervised and supervised methods on several benchmark datasets. The table highlights the best and second-best performing methods for each dataset, indicating FUG's competitiveness in this setting.  'OOM' signifies that the model ran out of memory on the specified hardware.", "section": "5.3 Evaluating on In-domain Learning"}, {"figure_path": "VUuOsBrqaw/tables/tables_7_2.jpg", "caption": "Table 4: Ablation study of the loss functions of FUG.", "description": "This table presents the ablation study results for the FUG model, evaluating the impact of removing each loss function (LDE, LRT-FUG-, LRT-FUG+) individually and comparing the results to the full FUG model.  The results are shown in terms of node classification accuracy for several datasets (Cora, CiteSeer, PubMed, Photo, Computers, CS, Physics).  The study demonstrates the contribution of each loss function to the overall performance of the FUG model.", "section": "5 Experiments"}, {"figure_path": "VUuOsBrqaw/tables/tables_8_1.jpg", "caption": "Table 5: Comparison of computational cost", "description": "This table compares the computational cost (time and VRAM) of four different methods: GRACE, BGRL, GBT, and FUG, on two datasets: CS and Physics.  It shows that FUG has a lower computational cost than GRACE, particularly on the larger Physics dataset where GRACE runs out of memory (OOM).  The table highlights FUG\u2019s efficiency in terms of resource utilization.", "section": "5.1 Experiment Settings"}, {"figure_path": "VUuOsBrqaw/tables/tables_18_1.jpg", "caption": "Table 2: Cross-domain Node Classification. The performances of OFA and GraphControl are reported from [17, 18]. FUG-C (Cora) means FUG-C (Cora) means FUG-C model pre-trained on Cora dataset.", "description": "This table presents the results of cross-domain node classification experiments.  It compares the performance of the proposed FUG model (trained on various datasets) against state-of-the-art models like OFA and GraphControl. The results highlight FUG's ability to adapt to datasets with different node feature shapes, showcasing its transferability across domains.", "section": "5 Experiments"}, {"figure_path": "VUuOsBrqaw/tables/tables_19_1.jpg", "caption": "Table 2: Cross-domain Node Classification. The performances of OFA and GraphControl are reported from [17, 18]. FUG-C (Cora) means FUG-C (Cora) means FUG-C model pre-trained on Cora dataset.", "description": "This table presents the results of cross-domain node classification experiments.  It compares the performance of the proposed Feature-Universal Graph (FUG) model against state-of-the-art methods (OFA and GraphControl) on several datasets. The FUG model is pre-trained on one dataset and then tested on others, demonstrating its ability to transfer knowledge across domains.  The results show FUG's performance is competitive with models that have the advantage of being trained and tested on the same datasets.", "section": "5 Experiments"}, {"figure_path": "VUuOsBrqaw/tables/tables_20_1.jpg", "caption": "Table 7: Dataset statistics", "description": "This table presents the statistics of seven graph datasets used in the paper's experiments.  Each dataset is described by the number of nodes, edges, features per node, and number of classes.", "section": "E.1 Datasets"}, {"figure_path": "VUuOsBrqaw/tables/tables_21_1.jpg", "caption": "Table 8: Hyper-parameters of FUG-C in cross-domain learning", "description": "This table presents the hyperparameters used for the FUG-C model during cross-domain learning experiments.  It shows the number of training epochs, the size of the hidden units in the model, and the weighting factors (lambda values) applied to the three loss functions (LRT-FUG+, LRT-FUG-, and LDE) within the FUG model. These hyperparameters were kept constant across different datasets during the cross-domain learning experiments to test the model's adaptability.", "section": "5.1 Experiment Settings"}, {"figure_path": "VUuOsBrqaw/tables/tables_22_1.jpg", "caption": "Table 9: Hyper-parameters of FUG in in-domain learning", "description": "This table shows the hyper-parameters used for the FUG model during in-domain learning experiments on different datasets.  The parameters include the number of training epochs, the number of hidden units in the model, and the weights assigned to each of the three loss functions used in the FUG training process (LDE, LRT-FUG+, and LRT-FUG-). The number of hop for topological propagation during embedding is also listed.", "section": "5.1 Experiment Settings"}, {"figure_path": "VUuOsBrqaw/tables/tables_22_2.jpg", "caption": "Table 2: Cross-domain Node Classification. The performances of OFA and GraphControl are reported from [17, 18]. FUG-C (Cora) means FUG-C (Cora) means FUG-C model pre-trained on Cora dataset.", "description": "This table presents the results of cross-domain node classification experiments.  It compares the performance of the proposed FUG model (pre-trained on various datasets) against state-of-the-art methods, OFA and GraphControl, demonstrating FUG's ability to adapt to datasets with different node feature shapes.", "section": "5 Experiments"}]