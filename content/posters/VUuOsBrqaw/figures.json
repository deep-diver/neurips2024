[{"figure_path": "VUuOsBrqaw/figures/figures_1_1.jpg", "caption": "Figure 2: The overview of the proposed FUG model.Given the node features X and structure A of a graph, X is first processed through a learnable dimensional encoding component, DE(\u00b7), to generate a basis transformation matrix T, which is used to embed features into a unified shape H = XT \u2208 Rn\u00d7k. H and A are then input into the graph encoder GE(\u00b7) to produce representations Z. We set three losses, LDE, LRT-FUG+, and LRT-FUG-, which collaboratively train GE(\u00b7) and DE(\u00b7).", "description": "This figure illustrates the FUG model's architecture and workflow.  The model takes node features (X) and graph structure (A) as input. A dimensional encoder (DE) processes the features, producing a basis transformation matrix (T) to unify feature shapes. The transformed features (H) and the graph structure are then fed into a graph encoder (GE), generating final representations (Z). Three loss functions\u2014LDE, LRT-FUG+, and LRT-FUG\u2212\u2014are used for training, ensuring dimensional encoding consistency and optimizing positive and negative sample relations.", "section": "4 Feature-Universal Graph Pre-training Model"}, {"figure_path": "VUuOsBrqaw/figures/figures_5_1.jpg", "caption": "Figure 2: The overview of the proposed FUG model.Given the node features X and structure A of a graph, X is first processed through a learnable dimensional encoding component, DE(\u00b7), to generate a basis transformation matrix T, which is used to embed features into a unified shape H = XT \u2208 Rn\u00d7k. H and A are then input into the graph encoder GE(\u00b7) to produce representations Z. We set three losses, LDE, LRT-FUG+, and LRT-FUG-, which collaboratively train GE(\u00b7) and DE(\u00b7).", "description": "This figure illustrates the Feature-Universal Graph (FUG) model's architecture and training process.  It consists of two main components: a dimensional encoder (DE) and a graph encoder (GE). The DE processes node features (X) to produce a basis transformation matrix (T) for unifying feature shapes across different datasets, resulting in a unified feature representation (H). The GE then takes H and the graph structure (A) as input and produces the final graph representation (Z).  The training involves three losses: LDE to enforce global uniformity in the learned transformations, LRT-FUG+ to optimize relationships between positive node pairs, and LRT-FUG- to optimize the global uniformity of the graph representations.  The figure visually depicts these steps and the effects of each loss.", "section": "4 Feature-Universal Graph Pre-training Model"}, {"figure_path": "VUuOsBrqaw/figures/figures_8_1.jpg", "caption": "Figure 3: Hyper-parameter analysis of FUG.", "description": "This figure shows the impact of different hyperparameters on the performance of the FUG model.  The left panel displays how varying the sample size affects accuracy across multiple datasets. The right panel illustrates the effect of changing the base transformation vector size on the accuracy of the model on different datasets.  The shaded areas around the lines represent confidence intervals, showing the variability in model performance.", "section": "5 Experiments"}, {"figure_path": "VUuOsBrqaw/figures/figures_23_1.jpg", "caption": "Figure 2: The overview of the proposed FUG model.Given the node features X and structure A of a graph, X is first processed through a learnable dimensional encoding component, DE(\u00b7), to generate a basis transformation matrix T, which is used to embed features into a unified shape H = XT \u2208 Rn\u00d7k. H and A are then input into the graph encoder GE(\u00b7) to produce representations Z. We set three losses, LDE, LRT-FUG+, and LRT-FUG-, which collaboratively train GE(\u00b7) and DE(\u00b7).", "description": "This figure shows the architecture of the Feature-Universal Graph contrastive pre-training (FUG) model. The model consists of three main components: a learnable dimensional encoding component (DE), a graph encoder (GE), and a relative relationship optimization task (LRT-FUG). The DE component learns a basis transformation matrix T that converts node features into a unified shape. The GE component encodes both the transformed node features and the graph structure to produce graph representations. The LRT-FUG loss function guides the model to learn relative relationships between nodes, improving performance and generalization.  Three losses, LDE, LRT-FUG+, and LRT-FUG-, are used to collaboratively train the DE and GE components.", "section": "4 Feature-Universal Graph Pre-training Model"}]