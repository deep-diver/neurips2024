[{"figure_path": "LQR22jM5l3/figures/figures_6_1.jpg", "caption": "Figure 2: Accuracy across different values of the MMD hyper-parameter for models trained on balanced data and evaluated on their respective training distribution (dashed) and P\u00ba (solid line) averaged across replicates. We consider anti-causal tasks: (left) purely spurious case, (middle) when another confounder V is present, and (right) the entangled dataset. Worst group performance on P\u00ba is displayed in red. Markers display individual replicates.", "description": "This figure displays the accuracy of models trained on balanced data for three different anti-causal tasks, each with varying levels of spurious correlation and confounding variables. The x-axis represents the strength of maximum mean discrepancy (MMD) regularization, and the y-axis shows accuracy.  The left panel shows a purely spurious case, where data balancing succeeds even with regularization. The middle panel demonstrates a case where another confounder V negatively impacts the model's performance after balancing. The right panel displays an entangled dataset where the model fails to generalize well. Red lines in each panel indicate worst group accuracy on the test distribution P\u2070, emphasizing the importance of considering the impact of data balancing on the model's fairness and robustness.", "section": "3 Can we predict when data balancing fails?"}, {"figure_path": "LQR22jM5l3/figures/figures_7_1.jpg", "caption": "Figure 3: Accuracy across different values of the confounder strength (i.e., different P' \u2208 P), for each value of MMD regularization considered (displayed by the color gradient). (a) Models trained on Pt. (b) Models trained on Q. Results are averaged across seeds for clarity. Notice the different y-scales. (c) Displays the mean and standard deviation across seeds for MMD=16.", "description": "This figure shows the accuracy of models trained on two different distributions (Pt and Q) with varying levels of Maximum Mean Discrepancy (MMD) regularization, tested across different levels of confounding strength in the data. It illustrates how data balancing and regularization techniques interact to influence model performance and robustness in the presence of spurious correlations.", "section": "5 Impact of data balancing on the CBN"}, {"figure_path": "LQR22jM5l3/figures/figures_8_1.jpg", "caption": "Figure 2: Accuracy across different values of the MMD hyper-parameter for models trained on balanced data and evaluated on their respective training distribution (dashed) and P\u00ba (solid line) averaged across replicates. We consider anti-causal tasks: (left) purely spurious case, (middle) when another confounder V is present, and (right) the entangled dataset. Worst group performance on P\u00ba is displayed in red. Markers display individual replicates.", "description": "This figure displays the accuracy of models trained on balanced data for three different anti-causal tasks, evaluated on both their training distribution and a distribution (P\u00ba) without the undesired dependency.  The x-axis represents different values of the Maximum Mean Discrepancy (MMD) regularization hyperparameter. Each subfigure shows a different scenario: (left) purely spurious correlation; (middle) with an additional confounder; and (right) entangled signals. The red line indicates the worst group accuracy on P\u00ba, highlighting the impact of balancing on fairness/robustness under different levels of regularization.", "section": "Can we predict when data balancing fails?"}, {"figure_path": "LQR22jM5l3/figures/figures_16_1.jpg", "caption": "Figure 5: Proportions of Y = 0, 1 (grey bars) and Z = 0, 1 (purple bars) before (left) and after (right) balancing the data on Y.", "description": "This figure shows the proportions of Y and Z before and after balancing the data on Y. It illustrates two scenarios: 'same direction', where the biases of Y and Z are in the same direction, and 'reverse direction', where the biases are in opposite directions. The figure demonstrates how data balancing on Y affects the marginal distribution of Z, potentially increasing bias in certain cases.", "section": "A Failure modes of data balancing"}, {"figure_path": "LQR22jM5l3/figures/figures_17_1.jpg", "caption": "Figure 6: Distribution Pt(Z = 1) before (blue) and after (orange) balancing the data according to Y, for different values of the binarization threshold of Z which translates into different correlation coefficients between Y and Z. Left: similar direction of under-representation. Right: opposite direction.", "description": "This figure shows the results of a simulation to illustrate how data balancing affects the marginal distribution of a binary sensitive attribute Z when the labels Y are balanced. The x-axis represents the correlation between Y and Z, while the y-axis shows the proportion of Z=1 in the dataset. Blue dots represent the original distribution, while orange dots show the distribution after balancing Y. The dashed line indicates a uniform distribution of Z (i.e., P(Z=1)=0.5). The left panel shows the case where Y and Z have similar biases, while the right panel shows the case with opposite biases. The results confirm that balancing Y can exacerbate the bias in Z if Y and Z have opposite biases.", "section": "A Failure modes of data balancing"}, {"figure_path": "LQR22jM5l3/figures/figures_26_1.jpg", "caption": "Figure 9: Pearson correlation between each attribute and Y (left), or Z (right) in a sample of the original data (teal), compared to a balanced sample (blue) of the training data.", "description": "This figure displays the correlation between each attribute in the CelebA dataset and the target variable Y (helpfulness of review) and the sensitive attribute Z (gender), before and after data balancing.  The teal bars represent the correlations in the original data, and the blue bars show the correlations after the data has been balanced. Comparing these correlations provides insight into how balancing the data affects the relationship between attributes and both the target and sensitive attribute.", "section": "E.2.2 Distinguishing between failure modes"}]