[{"figure_path": "NLqdudgBfy/figures/figures_3_1.jpg", "caption": "Figure 1: Comparison between different transformations for E-SSL on CIFAR-10 with ResNet-18. Note that different pretraining tasks may have different classes (e.g., 4 for rotation and 2 for horizontal flip). The baseline is a random initialized encoder with 34% test accuracy under linear probing.", "description": "This figure compares the training loss and test linear accuracy of different transformations used in E-SSL on CIFAR-10 using ResNet-18.  It shows the training loss curves for seven different transformations (horizontal flip, grayscale, four-fold rotation, vertical flip, jigsaw, four-fold blur, and color inversion) and a baseline of a randomly initialized encoder. The test linear accuracy curves for the same transformations and baseline are also plotted to show the performance of the learned representations on a downstream task (linear classification).  The results indicate that the choice of transformation significantly impacts the performance of E-SSL.", "section": "4 A Theory of Equivariant SSL"}, {"figure_path": "NLqdudgBfy/figures/figures_4_1.jpg", "caption": "Figure 2: The causal diagram of equivariant self-supervised learning. The observed variables are in grey. C: class; S: style; A: intrinsic equivariance variable; X: raw input; A: augmentation; X: augmented input; Z: representation.", "description": "This figure shows a causal diagram representing the data generation process in equivariant self-supervised learning.  The nodes represent variables, with shaded nodes representing observed variables and unshaded nodes representing latent variables. The arrows indicate causal relationships.  The variables are: C (class), S (style), \u0100 (intrinsic equivariance), X (raw input), A (augmentation), X (augmented input), and Z (representation).  The diagram illustrates how the class, style, and intrinsic equivariance influence the raw input, which is then augmented to produce the final input used to generate a representation.  This helps illustrate the 'explaining away' effect discussed in the paper, showing the relationships between class, augmentation, and representation.", "section": "3 The Challenges of Understanding Equivariant SSL"}, {"figure_path": "NLqdudgBfy/figures/figures_5_1.jpg", "caption": "Figure 1: Comparison between different transformations for E-SSL on CIFAR-10 with ResNet-18. Note that different pretraining tasks may have different classes (e.g., 4 for rotation and 2 for horizontal flip). The baseline is a random initialized encoder with 34% test accuracy under linear probing.", "description": "This figure compares the training loss and test accuracy of different data augmentation methods for equivariant self-supervised learning (E-SSL) on the CIFAR-10 dataset using a ResNet-18 model.  It shows the training loss curves for various transformations like horizontal flip, grayscale, four-fold rotation, vertical flip, jigsaw, four-fold blur, and color inversion.  The test accuracy, measured using linear probing, is also displayed for each transformation.  A baseline using a randomly initialized encoder is included for comparison. The results highlight the significant performance differences obtained with different augmentation types, suggesting that simply achieving high equivariance isn't sufficient to guarantee high downstream task performance.  Some transformations result in very low accuracy, despite having a low training loss.", "section": "4 A Theory of Equivariant SSL"}, {"figure_path": "NLqdudgBfy/figures/figures_19_1.jpg", "caption": "Figure 4: The model of this experiment. X: raw input; Z: representation; R: rotation prediction; C: class prediction. For rotation prediction, unless specified, the gradient flowing from the classifier to the encoder is detached.", "description": "This figure shows the architecture used in the experiment to study how class information affects equivariant pretraining tasks.  The input X is first encoded into a representation Z. This representation is then passed through two separate branches: one for rotation prediction (R), and one for class prediction (C).  The gradient from the classifier to the encoder is detached for the rotation prediction task, unless explicitly specified. This setup allows researchers to isolate and analyze the impact of class information on the equivariance learning process. ", "section": "B.2 Experiment Details of How Class Information Affects Equivariant Pretraining Tasks"}]