[{"figure_path": "NLqdudgBfy/tables/tables_6_1.jpg", "caption": "Table 2: Training rotation prediction accuracy and test linear classification accuracy under different base augmentations (CIFAR-10, ResNet18).", "description": "This table presents the results of an experiment comparing the performance of different augmentation strategies in a self-supervised learning setting.  Specifically, it shows the training accuracy for rotation prediction and the test accuracy for linear classification after using different augmentations (None, Crop+flip, SimCLR) with ResNet-18 on the CIFAR-10 dataset.  The goal is to determine how different levels of augmentation impact the performance of the model.", "section": "5.3 Model Equivariance"}, {"figure_path": "NLqdudgBfy/tables/tables_8_1.jpg", "caption": "Table 2: Training rotation prediction accuracy and test linear classification accuracy under different base augmentations (CIFAR-10, ResNet18).", "description": "This table presents the results of experiments comparing the performance of ResNet18 and its equivariant counterpart (EqResNet18) on CIFAR-10 dataset using three different augmentation strategies: no augmentation, crop and flip, and SimCLR.  For each augmentation strategy, the table displays the training accuracy of the rotation prediction task, the test accuracy achieved via linear probing for classification, and the gain in classification accuracy obtained by using the equivariant model over the non-equivariant model. The table showcases the benefits of using equivariant models, particularly under more aggressive augmentation schemes like SimCLR.", "section": "5.3 Model Equivariance"}, {"figure_path": "NLqdudgBfy/tables/tables_9_1.jpg", "caption": "Table 3: Comparison of augmentation-aware and truly equivariant methods (CIFAR-10, ResNet18).", "description": "This table compares the performance of two different loss functions for training an equivariant neural network on the CIFAR-10 dataset using a ResNet-18 architecture.  The 'CE loss' row shows results using cross-entropy loss for predicting the rotation angle of augmented images.  The 'CARE loss' row presents results when using the CARE (Contrastive Augmentation-Aware Rotation Equivariant) loss, which enforces strict equivariance. The table highlights that the CARE loss leads to improved performance in both the training rotation accuracy and the downstream classification accuracy after linear probing.", "section": "5.4 Strict Equivariant Objectives"}, {"figure_path": "NLqdudgBfy/tables/tables_16_1.jpg", "caption": "Table 2: Training rotation prediction accuracy and test linear classification accuracy under different base augmentations (CIFAR-10, ResNet18).", "description": "This table presents a comparison of the training rotation prediction accuracy and test linear classification accuracy achieved using ResNet18 and an equivariant ResNet18 (EqResNet18) model on the CIFAR-10 dataset under different base augmentations. The augmentations include no augmentation, a combination of random cropping and flipping, and SimCLR augmentations. The table shows that the equivariant model consistently outperforms the standard ResNet18 model in terms of both training and test accuracy, demonstrating the benefits of incorporating equivariance into the model architecture.", "section": "5.3 Model Equivariance"}, {"figure_path": "NLqdudgBfy/tables/tables_19_1.jpg", "caption": "Table 4: Training rotation prediction accuracy and test linear classification accuracy under different base augmentations (CIFAR-100, ResNet18).", "description": "This table presents the results of an experiment comparing the performance of standard ResNet18 and equivariant ResNet18 models on the CIFAR-100 dataset.  The models were pre-trained using rotation prediction, and their performance is evaluated using both the training rotation accuracy and the test classification accuracy after linear probing. The results are shown for three different augmentation strategies: no augmentation, crop and flip, and SimCLR augmentations. The \"Gain\" column shows the improvement in test classification accuracy achieved by using the equivariant model compared to the standard model.  This demonstrates the benefit of incorporating model equivariance for improving the quality of learned representations and the downstream classification performance.", "section": "5.3 Model Equivariance"}]