{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-00-00", "reason": "This paper is foundational for Vision Transformers (ViTs), introducing the core architecture used by the current paper and many subsequent ViT-based works."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-00-00", "reason": "This paper significantly improved training efficiency and performance of ViTs, addressing a key limitation of early ViT models and influencing the development of subsequent models including the one in the current paper."}, {"fullname_first_author": "Wenhai Wang", "paper_title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions", "publication_date": "2021-00-00", "reason": "This paper introduced a novel ViT architecture, PVT, which effectively handles dense prediction tasks (like semantic segmentation) and is directly relevant to the current paper's exploration of high-resolution ViTs."}, {"fullname_first_author": "Lucas Beyer", "paper_title": "FlexiViT: One model for all patch sizes", "publication_date": "2023-00-00", "reason": "This paper directly tackles the problem of variable input resolution in ViTs, proposing a key approach that is similar to, yet different from, the approach in the current paper."}, {"fullname_first_author": "Rui Tian", "paper_title": "ResFormer: Scaling Vits with multi-resolution training", "publication_date": "2023-00-00", "reason": "This paper proposes another prominent method that addresses variable resolution by leveraging a multi-resolution training strategy, providing a comparison point for the current paper's proposed approach."}]}