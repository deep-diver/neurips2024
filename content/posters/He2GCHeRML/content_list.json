[{"type": "text", "text": "Empowering Active Learning for 3D Molecular Graphs with Geometric Graph Isomorphism ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ronast Subedi\u2217 Lu Wei\u2217 Wenhan Gao\u2217 Florida State University Stony Brook University Stony Brook University rs22ce@fsu.edu lu.wei.1@stonybrook.edu wenhan.gao@stonybrook.edu ", "page_idx": 0}, {"type": "text", "text": "Shayok Chakraborty\u2020 Florida State University shayok@cs.fsu.edu ", "page_idx": 0}, {"type": "text", "text": "Yi Liu\u2020 Stony Brook University yi.liu.4@stonybrook.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Molecular learning is pivotal in many real-world applications, such as drug discovery. Supervised learning requires heavy human annotation, which is particularly challenging for molecular data; e.g., the commonly used density functional theory (DFT) is highly computationally expensive. Active learning (AL) automatically queries labels for the most informative samples, thereby remarkably alleviating the annotation hurdle. In this paper, we present a principled AL paradigm for molecular learning, where we treat molecules as 3D molecular graphs. Specifically, we propose a new diversity sampling method to eliminate mutual redundancy built on distributions of 3D geometries. We first propose a set of new 3D graph isometries for 3D graph isomorphism analysis. Our method is provably at least as expressive as the Geometric Weisfeiler-Lehman (GWL) test. The moments of the distributions of the associated geometries are then extracted for efficient diversity computing. To ensure our AL paradigm selects samples with maximal uncertainties, we carefully design a Bayesian geometric graph neural network to compute uncertainties specifically for 3D molecular graphs. We pose active sampling as a quadratic programming (QP) problem using the proposed components. Experimental results demonstrate the effectiveness of our AL paradigm, as well as the proposed diversity and uncertainty methods. The code is publicly available at https://github.com/sronast/al_3dgraph. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Molecular representation learning is essential for various real-world applications, such as molecular design, drug discovery, material design, etc.. In recent studies, molecules have been formulated as 3D graphs, based on the evidence that 3D spatial information is crucial to determine the properties of molecules [Liu et al., 2019, Townshend et al., 2019, Axelrod and Gomez-Bombarelli, 2020]. Generally, in a 3D graph, atoms are represented as nodes, each associated with Cartesian coordinates in 3D space. A predefined cut-off distance can be used as a threshold to determine if there is an edge between two nodes in the 3D graph. With the advance of deep learning, 3D graph neural networks (GNNs) have been developed to learn from 3D molecular graph data [Thomas et al., 2018, Sch\u00fctt et al., 2017, Satorras et al., 2021, Gasteiger et al., 2020c, Liu et al., 2021, 2022, Wang et al., 2022, Liao and Smidt, 2022, Zhou et al., 2022, Yan et al., 2022, Wang et al., 2023, Lin et al., 2023, Zhang et al., 2023]. These models are data-hungry and necessitate a large amount of annotated training data to attain good performance. However, annotation usually consumes excessive manpower, which is particularly challenging for molecules, e.g., the commonly used density functional theory (DFT) for molecular energy computing [Hohenberg and Kohn, 1964] is very expensive, inducing a complexity of $O(n_{e}^{3})$ , where $n_{e}$ is the number of electrons. As a concrete example, DFT can be hundreds of thousands of times slower than a reasonably good GNN for inference [Gilmer et al., 2017]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Active Learning $(A L)$ algorithms automatically identify the salient and exemplar samples from large amounts of unlabeled data [Settles, 2009, Ren et al., 2021]. This tremendously reduces the human annotation effort, as only the few samples identified by the algorithm need to be labeled manually. Further, since the deep network gets trained on the representative samples from the underlying data population, it typically depicts better generalization capability than a passive learner, where the training data are selected at random. Deep AL has been used with remarkable success in various applications, such as computer vision [Yoo and Kweon, 2019, Sinha et al., 2019], natural language processing [Zhang et al., 2022], medical diagnosis [Blanch et al., 2017], chemistry [Smith et al., 2018], and anomaly detection [Pimentel et al., 2020] among others. There are a few AL applications for 3D GNNs [Smith et al., 2021, van der Oord et al., 2023]; however, these works do not specifically account for 3D geometric information. The 3D geometry of molecules is crucial for determining molecular properties, but it introduces unique challenges in designing effective AL schemes. Currently, a principled AL algorithm for 3D molecular graphs is still lacking. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a principled AL paradigm for 3D molecular graphs. We formulate a criterion based on uncertainty and diversity, which ensures that the queried molecules are those where the graph learning model has maximal uncertainty about the labels, and that are also mutually diverse to avoid duplicate sample queries. In particular, diversity computing for 3D graphs is challenging and the difficulties are twofold. Firstly, the AL pipeline requires computing the difference between any two 3D molecular graphs, which could have different planar (2D) molecules (entangling different atom numbers, etc.), in most cases. Secondly, the 3D shape (geometry) of a 3D graph should be captured completely for expressive geometric representations and accurate diversity computation. ", "page_idx": 1}, {"type": "text", "text": "To tackle these challenges, we propose a novel diversity sampling method for 3D molecular graphs based on distributions of important 3D geometries. We propose a set of new 3D graph isometries for geometric modeling, which produces geometric representations that are at least as powerful as the Geometric Weisfeiler-Leman (GWL) test [Joshi et al., 2023] in distinguishing 3D graph geometries. This indicates our approach sets an upper bound on the expressive power of any existing 3D GNN models. Hence, the geometries derived from our geometric modeling method (e.g., reference distances, triangles) can be used for accurate diversity computing. To compare any two 3D molecules (with different planar graphs), the moments of the distributions of the derived geometries are extracted for final diversity computing of 3D graphs. In addition, to ensure our AL paradigm selects samples with maximal uncertainties, we carefully design a Bayesian geometric GNN specifically for 3D graph uncertainty computing. Our method is shown to be effective and efficient based on a set of ground approximations. With our novel components, we pose the sample selection as a quadratic programming (QP) problem and implement a fast QP solver to identify exemplar molecules to be annotated. Our method is easy to implement and can be applied in conjunction with any 3D GNN architecture. ", "page_idx": 1}, {"type": "text", "text": "Overall, our proposed AL paradigm incorporates both diversity and uncertainty for 3D molecular graphs. The diversity component, driven by proposed geometric isometries, captures diverse chemical properties from geometries. The uncertainty component leverages chemical contexts, such as atom types, as node features, enhancing the model\u2019s ability to identify and learn from uncertain chemical interactions. By considering both, our method represents a powerful AL paradigm for 3D molecular graphs. We conduct extensive experiments, and the results demonstrate the effectiveness of the proposed diversity and uncertainty methods as well as the overall AL paradigm. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized below. (i) We propose a principled AL paradigm to alleviate the annotation hurdle of 3D molecular graphs. We employ diversity and uncertainty measures to select the most informative subset for AL. (ii) We introduce a novel diversity component for 3D molecular graphs. Investigating geometric graph isomorphism, we introduce a model-agnostic geometric modeling method, which is provably at least as expressive as the GWL test. Our method can significantly enhance the accuracy of diversity computing for 3D molecular graphs. (iii) Our proposed graph isometries set the theoretical upper bound to the expressive power of all existing 3D GNNs, and thus can serve as the new gold standard to test the expressiveness of various 3D ", "page_idx": 1}, {"type": "text", "text": "GNNs. (iv) Rooted in Bayesian inference, we develop an effective and efficient pipeline to compute uncertainties for 3D molecular graphs. (v) Our framework significantly outperforms mainstream AL baselines, achieving remarkable efficiency owing to the cheap complexity of $O(N^{2})$ as well as the implementation of a fast QP solver. ", "page_idx": 2}, {"type": "text", "text": "2 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Diversity Computing for 3D Molecular Graphs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In molecular AL tasks, diversity sampling is important for eliminating redundancy, thereby wisely leveraging the annotation budget. The model\u2019s capability of capturing the 3D shape diversity among molecules is crucial for informed sampling. A particular challenge is that a diversity measure for two 3D molecules with different planar graphs is indispensable. Methods for diversity measures for 3D molecules with the same planar graph have been developed [Kumar and Zhang, 2018, Kearnes et al., 2016, Gfeller et al., 2013], but a diversity method for two 3D molecules with different planar graphs (entailing different atoms, etc) is demanding. Inspired by the USR method [Ballester and Richards, 2007], we propose a novel solution to achieve the goal from the distribution perspective. Generally, we develop a set of new isometries for expressive representations of 3D molecular graphs, after which the distributions of geometries associated with the isometries are obtained for diversity computing. ", "page_idx": 2}, {"type": "text", "text": "2.1.1 Isometries of 3D Molecular Graphs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As the first step, we introduce a set of new isometries as a basis, aiming at expressive representations of 3D graphs. As we focus on 3D geometry of molecules in this section, for simplicity, we use 3D point clouds to illustrate our ideas. Let $A\\,=\\,\\{a_{1},a_{2},...,a_{n}\\}$ and $B=\\{f(a_{1}),f(a_{2}),...,f(a_{n})\\}$ be two sets representing 3D point clouds. Here, each $a_{i}$ in $A$ is associated with a positional vector $a_{i}=(x_{a_{i}},y_{a_{i}},z_{a_{i}})$ in 3D space. $f$ denotes a bijective mapping between $A$ and $B$ . Then, similarly, each point $f(a_{i})$ in $B$ is associated with a positional vector $\\begin{array}{r}{f(a_{i})=(x_{f(a_{i})},y_{f(a_{i})},z_{f(a_{i})})}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Two 3D point clouds, $A$ and $B$ , are said to be $E(3)$ -isomorphic, if there exists $\\gamma\\in E(3)$ such that $A=\\gamma B$ . We further choose or compute a consistent reference point (e.g., centroid) for each point cloud, denoted as $r_{1}$ and $r_{2}$ , respectively. Without loss of generality, we use $a_{\\mathrm{far}}$ to denote the farthest point from the reference point in point cloud $A$ . Below, we will define three levels of isometries, each of which fulfills an isometric mapping between $A$ and $B$ . To satisfy any isometry, there needs to exist a bijective function $f:A\\rightarrow B$ , such that $h_{f(a)}=h_{a}$ for any node $a\\in A$ . Here, $\\boldsymbol{h}_{f(a)}$ and $h_{a}$ denote the node feature vectors for $f(a)$ and $a$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Reference Distance Isometry: If there exists a collection of global group elements $\\gamma_{i}\\in E(3)$ , such that $(r_{2},f(a_{i}))=(\\gamma_{i}r_{1},\\gamma_{i}a_{i})$ for each point $a_{i}\\in A$ , $A$ is reference distance isometric to $B$ . ", "page_idx": 2}, {"type": "text", "text": "Reference distance isometry involves the Euclidean distance between any atom in the molecule and the predefined reference point. ", "page_idx": 2}, {"type": "text", "text": "Triangular Isometry: If there exists a collection of global group elements $\\gamma_{i}\\in$ $E(3)$ , such that $\\bar{(r_{2},f\\,(\\bar{a}_{\\mathrm{far}})\\,,\\,f(a_{i}))}\\;=$ $\\left(\\gamma_{i}r_{1},\\gamma_{i}a_{\\mathrm{far}},\\gamma_{i}a_{i}\\right)$ for each point $a_{i}\\in A$ , $A$ is triangular isometric to $B$ . ", "page_idx": 2}, {"type": "text", "text": "With reference point $r$ , we define the reference vector $\\pmb{v}_{0}$ as $r$ pointing to the farthest point $a_{\\mathrm{far}}$ in a 3D molecule. Based on ref", "page_idx": 2}, {"type": "image", "img_path": "He2GCHeRML/tmp/eafa1c30607b5f36b99a57031f5cefad79b9565bb0c50e819a2f3a3d34c1825f.jpg", "img_caption": ["Figure 1: The illustrations of encoding the molecular triangular and cross-angular isometries "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "erence distance isometry, triangular isometry further involves the angle between $\\pmb{v}_{0}$ and other vectors pointing from $r$ to any other point in the molecule, computed as $\\begin{array}{r}{\\theta_{k}=\\cos^{-1}\\left(\\frac{\\boldsymbol{v}_{0}\\cdot\\boldsymbol{v}_{k}}{\\|\\boldsymbol{v}_{0}\\|\\|\\boldsymbol{v}_{k}\\|}\\right)}\\end{array}$ \u2225vv00\u2225\u2225vvkk\u2225 , where vk denotes vectors originating from $r$ and directed towards $k^{\\mathrm{th}}$ atoms in the molecule. The process is illustrated in part A of Fig. 1. For a molecule with $N$ nodes, we compute $N-1$ angles. Essentially, such angles provide insights into the spatial arrangement of atoms with respect to the pre-assigned reference vector. ", "page_idx": 2}, {"type": "text", "text": "Cross-angle Isometry: If there exists a collection of global group elements $\\gamma_{i j}\\in E(3)$ , such that $\\left(r_{2},f\\left(a_{j}\\right),f(a_{i})\\right)=\\bar{(\\gamma_{i j}r_{1},\\gamma_{i j}a_{j},\\gamma_{i j}a_{i})},\\forall a_{i},a_{j}\\in\\bar{A^{}}\\left(i\\neq j\\right),A^{}$ is cross-angle isometric to $B$ . Beyond the angles in triangular isometry as well as based on reference distance isometry, crossangular isometry further considers angles formed by any two atoms in the molecule with respect to the reference vector as above. Specifically, for every pair of atoms $i$ and $j$ , a vector $\\pmb{v}_{i j}$ is formed from $i$ to $j$ . With the reference vector $\\pmb{v}_{0}$ , the cross angle is computed as $\\begin{array}{r}{\\alpha_{i j}=\\cos^{-1}\\left(\\frac{\\pmb{v}_{0}\\cdot\\pmb{v}_{i j}}{\\|\\pmb{v}_{0}\\|\\|\\pmb{v}_{i j}\\|}\\right)}\\end{array}$ This approach, as depicted in part $\\mathbf{B}$ of Fig. 1, essentially reflects cross-angle information globally. For a molecule with $N$ nodes, we compute $N(N-1)/2$ cross angles with the complexity of $O(N^{2})$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Next, we propose Theorem 1 to indicate the relationship between these three isometries as below. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. If $A$ and $B$ are triangular isometric, then $A$ and $B$ are reference distance isometric; If $A$ and $B$ are crossangle isometric, then $A$ and $B$ are triangular isometric. ", "page_idx": 3}, {"type": "image", "img_path": "He2GCHeRML/tmp/6b3d48a3c5b1532ce2522993b2ea764fdb35d0afb2114131331a963f90063de0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: $A$ and $B$ are triangular isometric but not ", "page_idx": 3}, {"type": "text", "text": "cross-angular isometric. The angles $\\angle b r_{1}a_{f a r}$ , $\\angle c r_{1}a_{f a r}$ , The proof of Theorem 1 can be found and $\\angle d r_{1}a_{f a r}$ in structure $A$ are equal to the angles in Appendix A.1. Generally, we define $\\angle f(b)r_{2}f(\\overset{\\cdot}{a}_{f a r})$ , $\\angle f(c)r_{2}f(a_{f a r})$ , and $\\angle f(d)r_{2}f(a_{f a r})$ three levels of isometries for graph iso- in structure $B$ , respectively. However, the cross angle morphism. Reference distance isometry $\\angle d r_{1}c$ is not equal to the cross angle $\\angle f(d)r_{2}f(c)$ . ensures that the Euclidean distance between each point and a predefined reference point is consistent in two different point clouds. Triangular isometry further manifests the spatial arrangement of atoms referring to the pre-assigned pivot. Built on triangular isometry, cross-angular isometry then reflects the pair-wise global information. An illustrative example for triangular isometry and cross-angular isometry is also given in Fig. 2. Clearly, cross-angular isometry represents the strictest isometry among the three. In the following Sec. 2.1.2, we show that a designed geometric representation based on cross-angular isometry can exhibit great expressive power. ", "page_idx": 3}, {"type": "text", "text": "2.1.2 Expressive Power of Our Geometric Representations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we aim to formally elucidate the expressive power of a geometric representation (GR) based on our developed isometries in Sec. 2.1.1. Naturally, we formulate $G R_{\\mathrm{ours}}$ as a set containing all reference distances, triangles, and cross angles in a 3D graph. ", "page_idx": 3}, {"type": "text", "text": "We explore the Geometric Weisfeiler-Leman (GWL) test [Joshi et al., 2023], and then leverage GWL to illustrate the expressiveness power of our model. GWL test is an extension of the classic WL Test, enhancing its capabilities by incorporating both the topological structure of the graph and the geometric attributes of its vertices. Such an integration allows the GWL test especially apt for evaluating all 3D graph representation methods. Similar to the regular WL test, GWL test imposes an upper bound to the expressive power of 3D GNNs, i.e., if GWL test fails to distinguish two 3D graphs, then all existing 3D GNNs would also fail. See details of the GWL test in Appendix A.2. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. $G R_{o u r s}$ is at least as expressive as the GWL test. In other words, $G R_{o u r s}$ suffices to distinguish any non-isomorphic molecular structures that are distinguishable by any 3D GNN. ", "page_idx": 3}, {"type": "text", "text": "The proof of Proposition 1 can be found in Appendix A.1. In conclusion, the molecular geometric representation $G R_{\\mathrm{ours}}$ developed in this work has the greater expressive power than the GWL test, which indicates our diversity sampling method is accurate enough to capture the 3D shape diversity among different molecules. Notably, as mentioned before, GWL test sets the upper bound to the expresiveness of any existing 3D GNNs. Apparently, our geometric representation $G R_{o u r s}$ is provably at least as powerful as any existing 3D GNN for learning geometric features. Essentially, the three isometries associated with $G R_{\\mathrm{ours}}$ define expressiveness at different levels. For example, as only considering distance information, a well pretrained SchNet is upper bounded by reference distance isometry (but not triangular isometry or cross-angular isometry); as a more powerful model than SchNet, a well pretrained DimeNet is upper bounded by triangular isometry (but not crossangular isometry). Additionally, learning accurate geometric representations requires a perfectly pretrained 3D GNN model, which is hard to guarantee in practice. Our isomorphy study provides a deterministic and model-agnostic diversity component for 3D graphs, avoiding the need of a \u2018perfectly\u2019 pretrained 3D GNN model, as well as achieving a theoretically guaranteed upper bound of the expressiveness of all existing 3D GNN models. ", "page_idx": 3}, {"type": "text", "text": "2.1.3 Final Distributional Representations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Based on the isomorphy study in Sec. 2.1.1, we obtain our geometric representation method $G R_{\\mathrm{ours}}$ and prove $G R_{\\mathrm{ours}}$ possesses greater expressive power than any existing 3D GNN models in Sec. ", "page_idx": 3}, {"type": "text", "text": "2.1.2. In this section, we aim to extract the distributions of the entangled three geometries in $G R_{o u r s},$ , including reference distances, triangles, and cross angles, for diversity computing. Fortunately, we have the theorem [Hall, 1983] implying that the sequence of translated moments can be used to determine the original distribution. Following the USR work [Ballester and Richards, 2007], for each of the three aforementioned geometries, we also use four reference points to reflect the \u201ctranslated\u201d geometries; those are, the centroid (denoted as ctd) computed by the mean position of all the atoms in the 3D molecule, the point closest to the centroid (denoted as cst), the point farthest from the centroid (denoted as fct), and the point farthest from fct (denoted as ftf). For each reference point, we use a set of moments, including mean, variance, skewness, and kurtosis, which describe a distribution from different angles, e.g., skewness indicates the asymmetry and kurtosis describes the tailedness of a distribution. Detailed formulae for these moments can be found in the Appendix A.3. Notably, we compute these translated moments for all three entangled geometries as above. Eventually, we obtain summarized representations of distributions over geometries of 3D graphs, capturing essential characteristics of a molecule\u2019s shape. ", "page_idx": 4}, {"type": "text", "text": "We use cross angles as an example to describe the final distributional vector. For a molecule with $N$ $\\left[\\alpha_{i j}^{\\mathrm{ref}}\\right]_{i\\neq j,0<i,j<N}^{N(N-1)/2}$ (e.g., ctd). After applying statistical moments as an approximation, we can obtain a 4-dimensional vector M ca $\\overrightarrow{M_{\\mathrm{ref}}^{\\mathrm{ca}}}=[m_{\\mathrm{ref}}^{\\mathrm{ca}},v_{\\mathrm{ref}}^{\\mathrm{ca}},s_{\\mathrm{ref}}^{\\mathrm{ca}},k_{\\mathrm{ref}}^{\\mathrm{ca}}]$ , where the four elements denote the mean, variance, skewness, and kurtosis for this reference point, respectively. We perform a similar process for all four reference points mentioned above. By doing this, we can obtain four 4-dimensional vectors including $\\overrightarrow{M_{\\mathrm{ctd}}^{\\mathrm{cd}}}$ $\\dot{\\overline{{M_{\\mathrm{cst}}^{\\mathrm{c}\\mathrm{4}}}}},\\,\\overrightarrow{M_{\\mathrm{fct}}^{\\mathrm{ca}}}$ , and $\\overrightarrow{M_{\\mathrm{ftf}}^{\\mathrm{ca}}}$ , which are then concatenated together, resulting in the final 16-dimensional vector to represent the distribution of cross angles. We repeat the similar process for reference distances and triangles, and then all three corresponding 16-dimensional vectors are further concatenated as a 48-dimensional distributional vector to represent the geometric information of the input molecule. The 48-dimensional distributional vectors are then used to compute the diversity matrix. For any two molecules $n_{1}$ and $n_{2}$ in the dataset with $N$ molecules, we perform the inner product on their distributional vectors to achieve the similarity, and then use $1-$ similarity to obtain the final value $D_{n_{1}n_{2}}$ as the diversity measure between them. Finally, a matrix $D\\,\\in\\,\\dot{\\mathfrak{R}}^{N\\times N}$ is obtained, which contains the diversity between every pair of molecules. ", "page_idx": 4}, {"type": "text", "text": "Comparing Our Method to Traditional Structural Descriptors. Our method generates a 48- dimensional vector that encodes the geometric structure of a molecule. This representation is both equivariant to roto-translations and invariant to atomic permutations as the statistical quantities remain unchanged under such transformations. In contrast, Smooth Overlap of Atomic Positions (SOAP) [Bart\u00f3k et al., 2013, De et al., 2016, J\u00e4ger et al., 2018] generates atom-wise vectors that capture local atomic environments by employing spherical harmonics and radial basis functions. While SOAP is also equivariant to roto-translations, it is not invariant to atomic permutations. On the other hand, Atomic Cluster Expansion (ACE) [Drautz, 2019] uses a systematic expansion to describe interactions of varying orders (e.g., two-body, three-body interactions). However, ACE is less of a traditional descriptor compared to our method and SOAP; it is designed to provide a complete and systematic representation of atomic interactions by focusing on higher-order expansions (e.g., two-body, threebody interactions). This makes ACE more comprehensive in capturing the physical interactions within a system, but less suited for producing a fixed-dimensional, flexible descriptor. Unlike our method and SOAP, which generate more compact and adaptable descriptors, ACE emphasizes thorough expansions, making it less ideal for tasks requiring flexible, low-dimensional representations that can adapt easily to the active learning scheme. An empirical comparison between our method and the approach that uses SOAP will be provided, highlighting the effectiveness of our method in capturing molecular geometries. ", "page_idx": 4}, {"type": "text", "text": "2.2 Uncertainty Computing for 3D Molecular Graphs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Sec. 2.1, we develop an effective method for diversity computing among different 3D molecular graphs. In addition to selecting diverse molecules, it is important to select molecules where the model has maximal prediction uncertainty about the labels, so as to append maximal information to the model. Uncertainty qualification is well-studied in planar graph analysis [Hirschfeld et al., 2020], but an effective paradigm for 3D molecular graphs is currently lacking. Additionally, existing methods, such as Bayesian neural networks (BNNs) [Lampinen and Vehtari, 2001, Titterington, 2004, Goan and Fookes, 2020] and deep model ensemble methods [Lakshminarayanan et al., 2017, Huang et al., 2017], are excessively computationally expensive, limiting their capacity in 3D graph analyses. In a concurrent work [Thaler et al., 2024] on active learning for partial charge prediction of metal-organic frameworks, a dropout Monte Carlo scheme has been proposed to lessen these issues. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In this work, we develop an effective and efficient method, known as Bayesian geometric graph neural network (BGGNN), that takes a 3D graph as input and produces the demanding properties as well as uncertainty values, e.g., mean and variance. Formally, a 3D graph is represented as $\\mathbf{G}=(V,E,P)$ , where $V$ denotes the set of vertices (atoms), $E$ denotes the set of edges (bonds), and $P$ denotes the set of Cartesian coordinates for all atoms. A 3D molecular graph is associated with a set of properties, denoted as O. Recently, researchers have developed 3D GNNs, such as SchNet [Sch\u00fctt et al., 2017], DimeNet [Gasteiger et al., 2020b], SphereNet [Liu et al., 2022], and GemNet [Gasteiger et al., 2021], for 3D graph representation learning. The likelihood of a 3D GNN can be represented as $p_{3\\mathrm{DGNN}}(\\mathbf{O}\\mid\\mathbf{G},\\mathbf{w})$ , where 3DGNN indicates any existing 3D GNN and w denotes the set of parameters of the used 3D GNN. We also use $p_{3\\mathrm{DGNN}}(\\mathbf{w})$ to represent the prior distribution for the parameters. Assume we collect a new input and output pair, denoted as $\\mathbf{g}^{*}$ and $\\mathbf{o}^{*}$ . Then based on the conventional Bayesian theorem, Bayesian inference for this new output $\\mathbf{o}^{*}$ is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\mathrm{3DGNN}}\\left(\\mathbf{o}^{*}\\mid\\mathbf{g}^{*},\\mathbf{G},\\mathbf{O}\\right)=\\int_{\\mathbb{R}^{n}}p_{\\mathrm{3DGNN}}\\left(\\mathbf{o}^{*}\\mid\\mathbf{g}^{*},\\mathbf{w}\\right)p_{\\mathrm{3DGNN}}(\\mathbf{w}\\mid\\mathbf{\\bar{G}},\\mathbf{O})d\\mathbf{w},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbb{R}^{n}$ is the whole space of $n$ parameters in 3DGNN. It\u2019s infeasible to perform the above integration on $\\mathbb{R}^{n}$ due to prohibitive computational cost. To tackle this, the variational inference method is introduced to approximate $p_{3\\mathrm{DGNN}}(\\mathbf{O}\\mid\\mathbf{G},\\mathbf{w})$ with the parameterized $q_{\\theta}(\\mathbf{w})$ through minimizing the Kullback-Leibler (KL) divergence between these two distributions. After applying Bayesian theorem once more, the minimization objective becomes ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{VI}}(\\theta)=-\\int_{\\mathbb{R}^{n}}q_{\\theta}({\\bf w})\\log p_{\\mathrm{3DGNN}}(\\mathbf{O}\\mid{\\mathbf{G}},{\\mathbf{w}})d{\\mathbf{w}}+\\mathrm{KL}\\left(q_{\\theta}({\\mathbf{w}})\\|p_{\\mathrm{3DGNN}}({\\mathbf{w}})\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To completely avoid the integration over the whole parameter space, the MC-dropout method [Gal and Ghahramani, 2016, Srivastava et al., 2014] is further used in our BGGNN. Specifically, it employes the Monte-Carlo estimator [Gal et al., 2016, Gal and Ghahramani, 2016] to approximate the integration by performing summation over the sampled models. In practice, researchers implement an MC-dropout network by using dropout as the network\u2019s regularization[Gal and Ghahramani, 2016]. Following this, we propose to insert dropout layers after the linear layers in our used 3DGNN as an effective yet efficient estimation of Bayesian inference. ", "page_idx": 5}, {"type": "text", "text": "Now as we have obtained the variational predictive distribution of a new output with $q_{\\theta}(\\mathbf{w})$ , we can easily compute the predictive mean and variance of this distribution. For the molecular property prediction tasks, after we sample $N$ outputs from the same input, the heteroscedastic predictive uncertainty is then given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\stackrel{\\mathrm{glven\\,}\\,\\mathrm{oy}}{\\widehat{\\sigma^{2}}}(\\mathbf{o}^{*}\\mid\\mathbf{g}^{*})=\\frac{1}{N}\\sum_{n=1}^{N}\\left(\\hat{\\mathbf{o}}_{n}^{*}\\right)^{2}-\\left(\\frac{1}{N}\\sum_{n=1}^{N}\\hat{\\mathbf{o}}_{n}^{*}\\right)^{2}+\\frac{1}{N}\\sum_{n=1}^{N}\\hat{\\sigma}_{n}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\mathbf{o}}_{n}^{*}$ is the $n^{t h}$ sampled output and ${\\widehat{\\sigma}}_{n}^{2}$ is the variance that is the same among all the data samples. By doing this, we can obtain an uncertainty value (variance) for each molecule. Additionally, built on a 3D GNN, our BGGNN can faithfully produce a set of molecular properties $\\mathbf{o}$ . ", "page_idx": 5}, {"type": "text", "text": "Practically, any of the existing 3D GNN can be used as the backbone network for property prediction and uncertainty computing. In this study, we employ SphereNet [Liu et al., 2022] as our 3DGNN, owing to its great power in incorporating 3D geometric information. We apply dropout layers onto the linear layers of SphereNet for Bayesian inference in our BGGNN. To allow more accurate AL selections, we particularly employ the concrete dropout with a learnable dropout rate [Gal et al., 2017] in our BGGNN. Overall, our method is shown to be an effective and efficient paradigm for 3D graph uncertainty computing, as further empirically demonstrated in Sec. 4. ", "page_idx": 5}, {"type": "text", "text": "2.3 Active Sampling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A schematic diagram of our active sampling framework is depicted in Fig. 6 and described in A.4 in Appendix. Specifically, in Sec. 2.1, we obtain the matrix $D\\in\\Re^{N\\times N^{*}}$ containing the mutual diversity between every pair of unlabeled molecules, where $N$ is the number of unlabeled molecules. In Sec. 2.2, we employ our designed BGGNN to achieve the vector $r\\in\\Re^{N\\times1}$ quantifying the prediction uncertainty score of each unlabeled molecule. In the AL setting, our objective is to select a batch of $k$ unlabeled molecules ( $k$ is a pre-defined query batch size) with high prediction uncertainty and high mutual diversity among them. Let $z\\in\\{0,\\dot{1}\\}^{N\\times1}$ be a binary vector with $N$ entries which denotes whether the unlabeled molecule $x_{i}$ will be included in the batch $z_{i}=1$ ) or not $(z_{i}=0)$ ). The molecule selection can thus be posed as the following optimization problem as in Eq. (4), where $\\lambda$ is a weight parameter governing the relative importance of the two terms. This is a standard quadratic programming (QP) problem; we relax the integer constraints into continuous constraints and solve the problem using an off-the-shelf QP solver. In this work, we employ the widely used Operator Splitting Quadratic Program (OSQP) [Stellato et al., 2020] to solve the QP problem in Eq. (4). We then apply a greedy approach to project the continuous solution back to the binary space, where the $k$ highest entries of the continuous solution vector are set to 1 and the remaining to 0. Such an approach is commonly used to convert continuous solutions obtained from a QP solver to binary solutions in AL [Chattopadhyay et al., 2013, Wang and Ye, 2013]. To accelerate the optimization, we implement a solution to execute the problem in the GPU (instead of the CPU) using the parallel implementation of the alternating direction method of multipliers, as detailed in Schubiger et al. [2020]. Notably, the predictions in the main tasks (e.g., molecular properties) are produced by our BGGNN built on SphereNet as in Sec. 2.2. ", "page_idx": 5}, {"type": "table", "img_path": "He2GCHeRML/tmp/5bb78e50fc7662494b8c3aec8f4c8866bb03577ef7a346ae8f0efb974cb88705.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3 Related Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "3.1 Active Learning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "AL is a well-researched problem in the machine learning community [Settles, 2009]. There exist two commonly used strategies for AL sampling. Uncertainty based sampling queries unlabeled samples with the highest prediction uncertainties for annotation. Diversity/representativeness based sampling aims to select the subset that can well represent the entire data distribution. A full review of the two AL sampling methods is provided in Appendix A.5. ", "page_idx": 6}, {"type": "text", "text": "3.2 Molecular Shape Similarity ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Molecular shape similarity plays a pivotal role in drug discovery and virtual screening of compounds [Kumar and Zhang, 2018, Murgueitio et al., 2012, Shang et al., 2017]. Methods predominantly fall into several categories [Kumar and Zhang, 2018], including descriptor-based methods [Schreyer and Blundell, 2012, Cannon et al., 2008, Li et al., 2016, Armstrong et al., 2009, Zhou et al., 2010], atom-centered Gaussian-based methods [Haque and Pande, 2010, de Lima and Nascimento, 2013, Yan et al., 2013], surface-based methods [Hofbauer et al., 2004, Mavridis et al., 2007, Cai et al., 2012, Karaboga et al., 2013, Venkatraman et al., 2009, Sael et al., 2008], etc. Descriptor-based methods are notably represented by the Ultrafast Shape Recognition (USR) algorithm [Ballester and Richards, 2007], which uses statistical moments of the distance distribution to characterize molecular shapes. Gaussian overlay-based methods, with ROCS [Rush et al., 2005, Hawkins et al., 2007] being the most commonly used one, evaluate the maximum volume overlap between two molecules. Surface-based methods typically employ shape signatures [Zauhar et al., 2013] or shape histograms to delineate molecular surfaces for shape similarity assessment. Despite the progress, a principled and theoretically ground similarity method for 3D molecular graphs is currently lacking. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Implementation Details: We use two mainstream 3D GNNs SphereNet [Liu et al., 2022] and DimeNet $^{++}$ [Gasteiger et al., 2020a] as the backbone models of our BGGNN. We directly use the optimal network configurations from the original papers for both backbone models. We train the network for 200 epochs, unless otherwise specified. We use the Adam Optimizer with an initial learning rate $5\\times10^{-4}$ and scale it by a factor of 0.5 every 15 epochs. ", "page_idx": 6}, {"type": "text", "text": "Data and Active Learning Setup: We first perform experiments on the QM9 benchmark dataset. Since SphereNet is more stable and incorporates more 3D information, we conduct experiments on mu, alpha, homo, and lumo for SphereNet, and mu and lumo for DimeNet++. These properties have continuous values, making the prediction problem a regression task. We randomly divide the training set of 110, 000 molecules into three splits of size 25, 000 each. From each split, we randomly select 5, 000 molecules as the initial labeled set and the remaining 20, 000 molecules as the unlabeled set. In each AL iteration, we query $1,500$ molecules from the unlabeled set, which are labeled and appended to the labeled set. The model\u2019s performance is evaluated on a held-out validation set containing 10, 000 molecules. We save the best-performing model on the validation set and report its performance on the test set containing 10, 831 molecules. The process is repeated for 7 AL iterations, which is taken as the stopping criterion. The final results are averaged over the three splits to rule out the effects of randomness. $\\lambda$ in Eq. 4 is taken as 1. The Mean Absolute Error (MAE) is used as the evaluation metric. In addition, to study the generalizability of our framework to more geometric data, we also conduct experiments to predict atomic forces for Aspirin in MD17 using our framework. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Comparison Baselines: We use four classic AL methods as baselines: Random Sampling, Coreset [Sener and Savarese, 2018], Learning Loss [Yoo and Kweon, 2019], and Evidential Uncertainty [Beluch et al., 2018, Amini et al., 2020]. Random Sampling is the default comparison baseline in AL research. Coreset and Learning Loss are two extensively used deep active learning algorithms for regression applications. Evidential Uncertainty is also a commonly used technique to quantify uncertainty for molecular property prediction and was hence included as a comparison baseline. Note some existing studies [Kulichenko et al., 2023, Gusev et al., 2023, Craig and Garc\u00eda-Melchor, 2021] have applied AL to molecule research and chemistry. However, these works focus on 2D molecules without considering 3D geometry, which is the focus of our work. Additionally, the techniques used in existing studies can arguably fall into the aforementioned AL categories. Hence, we think comparing with these classic AL methods is sufficient to demonstrate the superiority of our pipeline. ", "page_idx": 7}, {"type": "text", "text": "4.2 Active Learning Performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The active learning performance with SphereNet is depicted in Fig. 3. In each graph, the $x$ -axis denotes the iteration number and the $y$ -axis denotes the MAE on the test set. Our analysis revealed that Evidential Uncertainty depicted the worst performance and furnished significantly high error values for all four properties, which obscured the difference in performance among the other methods in the plots. For better interpretation and understanding, we exclude the Evidential Uncertainty method from the plots here and present the results with this baseline in Sec. A.6 ", "page_idx": 7}, {"type": "image", "img_path": "He2GCHeRML/tmp/570192b5e09e403cf000ccfe41ccaa8570b176f9a237b4c933d7cbecf3063b55.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "of the Appendix. The other base- Figure 3: Active learning performance results with SphereNet. line methods depict more or less The graphs show the mean (averaged over 3 runs) and the errorsimilar performance, with Coreset bars for all the methods. We plot the MAE values from the first marginally outperforming the other iteration onwards, to focus on the comparative performance of baselines. Our method comprehen- the methods after they start selecting samples using AL. Best sively outperforms all the baselines. viewed in color. ", "page_idx": 7}, {"type": "text", "text": "At any given AL iteration, it consistently attains a lower MAE compared to all the baselines. ", "page_idx": 7}, {"type": "text", "text": "We also conducted statistical tests of significance using paired t-test to assess whether the improvement in performance achieved by our method is statistically significant. For this purpose, we compared the average MAE achieved by our method against each of the baselines individually. The results are reported in Table 1; each entry in the table denotes the p-value of the paired t-test between our method ", "page_idx": 7}, {"type": "table", "img_path": "He2GCHeRML/tmp/ca9f2f7e6e4dac05d8edf71a58debd32d1e9fe50d6892623c6b51d0fa54e0311.jpg", "table_caption": ["Table 1: The table shows the p-values obtained using paired t-test between the results our method against each of the baselines for all the properties studied. Here, L. Loss refers to Learning Loss. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "and the corresponding baseline (denoted in the columns) for the property studied (denoted in the rows). From the table, we note that the improvement in performance achieved by our method is statistically significant $\\it{[p<0.05)}$ compared to all the baselines, consistently for all the four properties studied. These results unanimously corroborate the promise and potential of the proposed active sampling method to tremendously reduce the annotation cost in inducing a robust 3D graph neural network for molecular property prediction. ", "page_idx": 7}, {"type": "text", "text": "In addition, to study the robustness of our framework to the underlying network architecture and generalizability to the underlying geometric graph data, we have the following results: 1. To study the robustness of our framework to the underlying network architecture, results on the mu and lumo properties of the QM9 dataset using DimeNet $^{++}$ [Gasteiger et al., 2020a] as the backbone model are presented in Section A.7 of the Appendix due to space constraints. The results depict a similar pattern as Figure 3, with the proposed method consistently outperforming all the baselines for both the properties. A paired t-test, presented in Table 3 revealed that the performance improvement achieved by our framework is statistically significant. 2. To study the generalizability of our framework to the underlying geometric graph data, results on predicting atomic forces for Aspirin molecules in the MD17 benchmark dataset [Chmiela et al., 2017b] using our framework are depicted in Section A.8 of the Appendix and further corroborate the potential of our framework. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "He2GCHeRML/tmp/7e8d706cceef73c347bd3dc1877bd85805fd966c869628d856064202d334eea9.jpg", "img_caption": ["Figure 4: Study of query budget on the active learning performance. The graphs show the mean (averaged over 3 runs) and the errorbars for all the methods. The results with budget 1500 are the same as the those presented in Figure 3 and are included here for comparison. Best viewed in color. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Study of Query Budget ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The goal of this experiment is to study the effect of query budget (batch size) on the AL performance. The results on the mu property with SphereNet for query budgets $1,000,1,500$ and $2,000$ are depicted in Fig. 4. Since Evidential Uncertainty ", "page_idx": 8}, {"type": "image", "img_path": "He2GCHeRML/tmp/7525ed7af6ed7a752da6d96605bbb3438d5f42b53d11a7f8a285eb085d88fddc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "depicted much worse perfor- Figure 5: Ablation study results on the mu and lumo properties with mance than all the methods, it SphereNet. Best viewed in color. ", "page_idx": 8}, {"type": "text", "text": "was excluded from this comparison. Our framework once again outperforms all the baselines consistently for all the query budgets. As before, we conducted a paired t-test and the results are presented in Appendix A.9. From the p-values, we conclude that the error values furnished by our method are statistically significantly better $p<0.05)$ than all the baselines, consistently for all the query budgets. These results are particularly significant from a practical standpoint as the available query budget in a real-world application is dependent on time, resources, and other constraints. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct ablation studies to examine the power of our diversity computing method, as it is our primary contribution in this research. We perform experiments on the mu and lumo properties with SphereNet from two aspects. Firstly, we compare our framework with only the diversity term in Eq. 4 against Coreset, the state-of-the-art diversity-based AL technique. The results are reported in Fig. 5, from which we note that the diversity component of our framework consistently furnishes much lower MAE values than Coreset over all the AL iterations, for both properties. Secondly, we also conducted experiments where we compared the performance of our overall framework (using both uncertainty and diversity) against the baseline where only the uncertainty term in Eq. (4) was used for active sampling. The results revealed that removing the diversity term adversely affected the performance of our framework. A paired t-test revealed that the improvement in performance achieved by our diversity component is statistically significant $\\langle p<0.05)$ for both these properties $p=0.0001$ for mu and $p=0.04$ for lumo). These results show the effectiveness of the proposed diversity metric for AL framework to train a 3D GNN for molecular property prediction. Additionally, we examine the individual impact of diversity and uncertainty components in Appendix A.10. We also compare our proposed diversity component with the SOAP-based diversity, and test our method against BatchBALD [Kirsch et al., 2019], a greedy clustering-based Bayesian uncertainty approach in Appendix A.10. ", "page_idx": 8}, {"type": "text", "text": "4.5 Computation Time Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this experiment, we analyze the computation time of all the methods studied in this paper. The average time taken to query a batch of unlabeled samples and update the SphereNet model (one active learning iteration) are shown in Table 2. For fair comparison, all the methods were run on the same NVIDIA RTX A4500 20GB GPU. ", "page_idx": 9}, {"type": "text", "text": "The computation time of our Table 2: Average (\u00b1 std) time (minutes) taken by each method for framework is much less than sample selection and training the SphereNet model (one iteration of hich needs to solve AL). Here, L. Loss refers to Learning Loss. ", "page_idx": 9}, {"type": "text", "text": "a mixed integer programming (MIP) problem. The other three methods have similar ", "page_idx": 9}, {"type": "table", "img_path": "He2GCHeRML/tmp/2f3d6ac443b7c27f1f4a9758889dac1aa4b2b88d1f8e4c91fa064e3c966fefbd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "computation time, as they don\u2019t involve iterative algorithms. Our method takes only slightly more time than them, owing to the implementation of a faster QP solver as mentioned in Sec. 2.3, as well as our vectorized implementation to enable the use of GPUs to perform diversity matrix computation. The performance studies in Sec. 4.2 show that our framework is much more accurate than these baselines, and the ablation studies in Sec. 4.4 indicate both the diversity and uncertainty components are necessary to form a QP problem. Given the large margin of performance improvement, we think the efficiency of our method is acceptable. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion, Limitations, Future Work, and Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present a principled active learning framework with the goal of reducing the annotation cost for learning from 3D molecules represented as 3D graphs. The sample selection is posed as a QP problem, which selects samples with high mutual diversity and high uncertainty. Novel diversity and uncertainty components are proposed for 3D graphs, with strong empirical results presented. ", "page_idx": 9}, {"type": "text", "text": "We present a model-agnostic diversity component for 3D graphs, and our method is provably at least as powerful as any existing 3D GNN for learning geometric information. Even though our method can set the upper bound of the accuracy of diversity sampling for 3D molecules, it remains unexplored if such an advantage can be incorporated into 3D GNN models for diversity sampling. For example, molecular similarity might be incorporated into 3D GNNs to achieve comparable AL performance. Moreover, our experimental studies focus on small molecules in this work. ", "page_idx": 9}, {"type": "text", "text": "As part of future work, we plan to apply our methods to problems where much more accurate but expensive annotation is required, such as computing molecular systems\u2019 ground states using the Schr\u00f6dinger equation. DFT calculations are widely used but still involve approximations, as Schr\u00f6dinger equation is prohibitively expensive and its use is limited in very small molecules. Our AL pipeline is anticipated to unleash greater potential in such extreme-scale applications. Additionally, given AL needs several interactions with each requiring the model is well-trained, we test our methods on the commonly used but medium-scale QM9 and MD17 datasets in this work. Even though we think the empirical studies are sufficient to support our theory, we still plan to test the scalability of our methods on large-scale molecule datasets, such as OC20 [Chanussot et al., 2021], in the future. ", "page_idx": 9}, {"type": "text", "text": "This work facilitates a new avenue in graph analysis by effective and efficient representation of 3D geometric information, thereby dramatically advancing graph learning and mining. Our methods can reduce the annotation cost for molecular data and also have the potential in a broad set of scientific data types, such as materials and proteins, facilitating various disciplines including basic biology, material science, and quantum chemistry. This work is anticipated to have strong impacts on drug discovery and material design by enabling low-cost representation learning. Any positive and negative societal impact associated with those applications and domains can be applied to our methods. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work of Y. Liu has used the computational equipment supported by the US Army Research Office under the award W911NF-20-10159. Y. Liu and W. Gao would like to thank Xiaolin Li at Stony Brook University for providing this computational equipment. The work of S. Chakraborty is partially supported by the National Science Foundation under the award IIS-2143424 (NSF CAREER Award). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential regression. Advances in Neural Information Processing Systems, 33:14927\u201314937, 2020.   \nM Stuart Armstrong, Garrett M Morris, Paul W Finn, Raman Sharma, and W Graham Richards. Molecular similarity including chirality. Journal of Molecular Graphics and Modelling, 28(4): 368\u2013370, 2009.   \nJordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. In International Conference on Learning Representations (ICLR), 2020.   \nSimon Axelrod and Rafael Gomez-Bombarelli. Geom: Energy-annotated molecular conformations for property prediction and molecular generation. arXiv preprint arXiv:2006.05531, 2020.   \nPedro J Ballester and W Graham Richards. Ultrafast shape recognition to search compound databases for similar molecular shapes. Journal of computational chemistry, 28(10):1711\u20131723, 2007.   \nAlbert P. Bart\u00f3k, Risi Kondor, and G\u00e1bor Cs\u00e1nyi. On representing chemical environments. Phys. Rev. B, 87:184115, May 2013. doi: 10.1103/PhysRevB.87.184115. URL https://link.aps.org/ doi/10.1103/PhysRevB.87.184115.   \nWilliam H Beluch, Tim Genewein, Andreas N\u00fcrnberger, and Jan M K\u00f6hler. The power of ensembles for active learning in image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9368\u20139377, 2018.   \nMarc Gorriz Blanch, Xavier Giro I Nieto, Axel Carlier, and Emmanuel Faure. Cost-effective active learning for melanoma segmentation. In 31st Conference on Machine Learning for Health: Workshop at NIPS 2017 (ML4H 2017), pages 1\u20135, 2017.   \nFelix Buchert, Nassir Navab, and Seong Tae Kim. Toward label-efficient neural network training: Diversity-based sampling in semi-supervised active learning. IEEE Access, 11:5193\u20135205, 2023.   \nChaoqian Cai, Jiayu Gong, Xiaofeng Liu, Hualiang Jiang, Daqi Gao, and Honglin Li. A novel, customizable and optimizable parameter method using spherical harmonics for molecular shape similarity comparisons. Journal of molecular modeling, 18:1597\u20131610, 2012.   \nEdward O Cannon, Florian Nigsch, and John BO Mitchell. A novel hybrid ultrafast shape descriptor method for use in virtual screening. Chemistry Central Journal, 2:1\u20139, 2008.   \nShayok Chakraborty, Vineeth Balasubramanian, Qian Sun, Sethuraman Panchanathan, and Jieping Ye. Active batch selection via convex relaxations with guaranteed solution bounds. IEEE transactions on pattern analysis and machine intelligence, 37(10):1945\u20131958, 2015.   \nLowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, et al. Open catalyst 2020 (oc20) dataset and community challenges. Acs Catalysis, 11(10):6059\u20136072, 2021.   \nRita Chattopadhyay, Wei Fan, Ian Davidson, Sethuraman Panchanathan, and Jieping Ye. Joint transfer and batch-mode active learning. In International Conference on Machine Learning (ICML), 2013.   \nStefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Sch\u00fctt, and Klaus-Robert M\u00fcller. Machine learning of accurate energy-conserving molecular force fields. Science advances, 3(5):e1603015, 2017a.   \nStefan Chmiela, Alexandre Tkatchenko, Huziel E. Sauceda, Igor Poltavsky, Kristof T. Sch\u00fctt, and Klaus-Robert M\u00fcller. Machine learning of accurate energy-conserving molecular force fields. Science Advances, 3(5):e1603015, 2017b. doi: 10.1126/sciadv.1603015. URL https://www. science.org/doi/abs/10.1126/sciadv.1603015.   \nMichael John Craig and Max Garc\u00eda-Melchor. Applying active learning to the screening of molecular oxygen evolution catalysts. Molecules, 26(21):6362, 2021.   \nSandip De, Albert P. Bart\u00f3k, G\u00e1bor Cs\u00e1nyi, and Michele Ceriotti. Comparing molecules and solids across structural and alchemical space. Phys. Chem. Chem. Phys., 18:13754\u201313769, 2016. doi: 10.1039/C6CP00415F. URL http://dx.doi.org/10.1039/C6CP00415F.   \nLuis Ant\u00f4nio C Vaz de Lima and Alessandro S Nascimento. Molshacs: a free and open source tool for ligand similarity identification based on gaussian descriptors. European journal of medicinal chemistry, 59:296\u2013303, 2013.   \nRalf Drautz. Atomic cluster expansion for accurate and transferable interatomic potentials. Phys. Rev. B, 99:014104, Jan 2019. doi: 10.1103/PhysRevB.99.014104. URL https://link.aps.org/ doi/10.1103/PhysRevB.99.014104.   \nYoav Freund, Sebastian Seung, Eli Shamir, and Naftali Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28(2-3):133\u2013168, 1997.   \nAlexander Freytag, Erik Rodner, and Joachim Denzler. Selecting influential examples: Active learning with expected model output changes. In European Conference on Computer Vision (ECCV), 2014.   \nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050\u20131059. PMLR, 2016.   \nYarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. Advances in neural information processing systems, 30, 2017.   \nYarin Gal et al. Uncertainty in deep learning, 2016.   \nJohannes Gasteiger, Shankari Giri, Johannes T Margraf, and Stephan G\u00fcnnemann. Fast and uncertainty-aware directional message passing for non-equilibrium molecules. arXiv preprint arXiv:2011.14115, 2020a.   \nJohannes Gasteiger, Janek Gro\u00df, and Stephan G\u00fcnnemann. Directional message passing for molecular graphs. arXiv preprint arXiv:2003.03123, 2020b.   \nJohannes Gasteiger, Janek Gro\u00df, and Stephan G\u00fcnnemann. Directional message passing for molecular graphs. arXiv preprint arXiv:2003.03123, 2020c.   \nJohannes Gasteiger, Florian Becker, and Stephan G\u00fcnnemann. Gemnet: Universal directional graph neural networks for molecules. Advances in Neural Information Processing Systems, 34: 6790\u20136802, 2021.   \nDavid Gfeller, Olivier Michielin, and Vincent Zoete. Shaping the interaction landscape of bioactive molecules. Bioinformatics, 29(23):3073\u20133079, 2013.   \nRan Gilad-Bachrach, Amir Navot, and Naftali Tishby. Query by committee made real. In Neural Information Processing Systems (NeurIPS), 2005.   \nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263\u20131272. PMLR, 2017.   \nEthan Goan and Clinton Fookes. Bayesian neural networks: An introduction and survey. Case Studies in Applied Bayesian Data Science: CIRM Jean-Morlet Chair, Fall 2018, pages 45\u201387, 2020.   \nYuhong Guo and Russell Greiner. Optimistic active learning using mutual information. In International Joint Conference on Artificial Intelligence (IJCAI), 2007.   \nYuhong Guo and Dale Schuurmans. Discriminative batch mode active learning. Advances in neural information processing systems, 20, 2007.   \nFilipp Gusev, Evgeny Gutkin, Maria G Kurnikova, and Olexandr Isayev. Active learning guided drug design lead optimization based on relative binding free energy modeling. Journal of Chemical Information and Modeling, 63(2):583\u2013594, 2023.   \nPeter Hall. A distribution is completely determined by its translated moments. Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und Verwandte Gebiete, 62(3):355\u2013359, 1983.   \nImran S Haque and Vijay S Pande. accelerating parallel evaluations of rocs. Journal of computational chemistry, 31(1):117\u2013132, 2010.   \nPaul CD Hawkins, A Geoffrey Skillman, and Anthony Nicholls. Comparison of shape-matching and docking as virtual screening tools. Journal of medicinal chemistry, 50(1):74\u201382, 2007.   \nHideitsu Hino. Active learning: Problem settings and recent developments. arXiv preprint arXiv:2012.04225, 2020.   \nLior Hirschfeld, Kyle Swanson, Kevin Yang, Regina Barzilay, and Connor W Coley. Uncertainty quantification using neural networks for molecular property prediction. Journal of Chemical Information and Modeling, 60(8):3770\u20133780, 2020.   \nChristian Hofbauer, Hans Lohninger, and Andr\u00e1s Asz\u00f3di. Surfcomp: a novel graph-based approach to molecular surface comparison. Journal of chemical information and computer sciences, 44(3): 837\u2013847, 2004.   \nPierre Hohenberg and Walter Kohn. Inhomogeneous electron gas. Physical review, 136(3B):B864, 1964.   \nSteven HOI, Rong JIN, Jianke ZHU, and Michael R LYU. Semi-supervised SVM batch mode active learning for image retrieval. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008.   \nSteven CH Hoi, Rong Jin, and Michael R Lyu. Large-scale text categorization by batch mode active learning. In ACM International Conference on World Wide Web, 2006.   \nGao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. arXiv preprint arXiv:1704.00109, 2017.   \nSiyu Huang, Tianyang Wang, Haoyi Xiong, Jun Huan, and Dejing Dou. Semi-supervised active learning with temporal output discrepancy. In IEEE International Conference on Computer Vision (ICCV), 2021.   \nMarc OJ J\u00e4ger, Eiaki V Morooka, Filippo Federici Canova, Lauri Himanen, and Adam S Foster. Machine learning hydrogen adsorption on nanoclusters through structural descriptors. npj Computational Materials, 4(1):37, 2018.   \nChaitanya K Joshi, Cristian Bodnar, Simon V Mathis, Taco Cohen, and Pietro Lio. On the expressive power of geometric graph neural networks. arXiv preprint arXiv:2301.09308, 2023.   \nArnaud S Karaboga, Florent Petronin, Gino Marchetti, Michel Souchet, and Bernard Maigret. Benchmarking of hpcc: a novel 3d molecular representation combining shape and pharmacophoric descriptors for efficient molecular similarity assessments. Journal of Molecular Graphics and Modelling, 41:20\u201330, 2013.   \nSteven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30: 595\u2013608, 2016.   \nAndreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. Advances in neural information processing systems, 32, 2019.   \nMaksim Kulichenko, Kipton Barros, Nicholas Lubbers, Ying Wai Li, Richard Messerly, Sergei Tretiak, Justin S Smith, and Benjamin Nebgen. Uncertainty-driven dynamics for active learning of interatomic potentials. Nature Computational Science, 3(3):230\u2013239, 2023.   \nAshutosh Kumar and Kam YJ Zhang. Advances in the development of shape similarity methods and their application in drug discovery. Frontiers in chemistry, 6:315, 2018.   \nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.   \nJouko Lampinen and Aki Vehtari. Bayesian approach for neural networks\u2014review and case studies. Neural networks, 14(3):257\u2013274, 2001.   \nHongjian Li, Kwong-S Leung, Man-H Wong, and Pedro J Ballester. Usr-vs: a web server for large-scale prospective virtual screening using ultrafast shape recognition techniques. Nucleic acids research, 44(W1):W436\u2013W441, 2016.   \nXin Li and Yuhong Guo. Adaptive active learning for image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 859\u2013866, 2013.   \nYi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic graphs. arXiv preprint arXiv:2206.11990, 2022.   \nYuchao Lin, Keqiang Yan, Youzhi Luo, Yi Liu, Xiaoning Qian, and Shuiwang Ji. Efficient approximations of complete interatomic potentials for crystal property prediction. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \nMeng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu, Jingtun Zhang, Yi Liu, Keqiang Yan, Haoran Liu, Cong Fu, Bora M Oztekin, Xuan Zhang, and Shuiwang Ji. DIG: A turnkey library for diving into graph deep learning research. Journal of Machine Learning Research, 22(240):1\u20139, 2021. URL http://jmlr.org/papers/v22/ 21-0343.html.   \nShengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised representation for graphs, with applications to molecules. Advances in Neural Information Processing Systems, 32:8464\u20138476, 2019.   \nYi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3D molecular graphs. In International Conference on Learning Representations, 2022.   \nLazaros Mavridis, Brian D Hudson, and David W Ritchie. Toward high throughput 3d virtual screening using spherical harmonic surface representations. Journal of chemical information and modeling, 47(5):1787\u20131796, 2007.   \nChristoph Mayer and Radu Timofte. Adversarial sampling for active learning. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2020.   \nManuela S Murgueitio, Sandra Santos-Sierra, and Gerhard Wolber. Discovery of novel tlr modulators by molecular modeling and virtual screening. Journal of Cheminformatics, 4:1\u20131, 2012.   \nTiago Pimentel, Marianne Monteiro, Adriano Veloso, and Nivio Ziviani. Deep active learning for anomaly detection. In IEEE International Joint Conference on Neural Networks (IJCNN), 2020.   \nRaghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1\u20137, 2014.   \nHiranmayi Ranganathan, Hemanth Venkateswara, Shayok Chakraborty, and Sethuraman Panchanathan. Deep active learning for image classification. In IEEE International Conference on Image Processing (ICIP), 2017.   \nPengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. ACM computing surveys (CSUR), 54(9):1\u201340, 2021.   \nThomas S Rush, J Andrew Grant, Lidia Mosyak, and Anthony Nicholls. A shape-based 3-d scaffold hopping method and its application to a bacterial protein- protein interaction. Journal of medicinal chemistry, 48(5):1489\u20131495, 2005.   \nLee Sael, David La, Bin Li, Raif Rustamov, and Daisuke Kihara. Rapid comparison of properties on protein surface. Proteins: Structure, function, and bioinformatics, 73(1):1\u201310, 2008.   \nV\u0131ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In International conference on machine learning, pages 9323\u20139332. PMLR, 2021.   \nAdrian M Schreyer and Tom Blundell. Usrcat: real-time ultrafast shape recognition with pharmacophoric constraints. Journal of cheminformatics, 4(1):1\u201312, 2012.   \nMichel Schubiger, Goran Banjac, and John Lygeros. GPU acceleration of admm for large-scale quadratic programming. Journal of Parallel and Distributed Computing, 144:55\u201367, 2020.   \nKristof Sch\u00fctt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert M\u00fcller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. Advances in neural information processing systems, 30, 2017.   \nOzan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations (ICLR), 2018.   \nBurr Settles. Active learning literature survey. 2009.   \nJinling Shang, Xi Dai, Yecheng Li, Marco Pistolozzi, and Ling Wang. Hybridsim-vs: a web server for large-scale ligand-based virtual screening using hybrid similarity recognition techniques. Bioinformatics, 33(21):3480\u20133481, 2017.   \nSamarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational adversarial active learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5972\u20135981, 2019.   \nJustin S Smith, Ben Nebgen, Nicholas Lubbers, Olexandr Isayev, and Adrian E Roitberg. Less is more: Sampling chemical space with active learning. The Journal of chemical physics, 148(24), 2018.   \nJustin S Smith, Benjamin Nebgen, Nithin Mathew, Jie Chen, Nicholas Lubbers, Leonid Burakovsky, Sergei Tretiak, Hai Ah Nam, Timothy Germann, Saryu Fensin, et al. Automated discovery of a robust interatomic potential for aluminum. Nature communications, 12(1):1257, 2021.   \nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929\u20131958, 2014.   \nB. Stellato, G. Banjac, P. Goulart, A. Bemporad, and S. Boyd. OSQP: an operator splitting solver for quadratic programs. Mathematical Programming Computation, 12(4):637\u2013672, 2020. doi: 10.1007/s12532-020-00179-2. URL https://doi.org/10.1007/s12532-020-00179-2.   \nStephan Thaler, Felix Mayr, Siby Thomas, Alessio Gagliardi, and Julija Zavadlav. Active learning graph neural networks for partial charge prediction of metal-organic frameworks via dropout monte carlo. npj Computational Materials, 10(1):86, 2024.   \nNathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018.   \nD Michael Titterington. Bayesian methods for neural networks and related models. Statistical science, pages 128\u2013139, 2004.   \nSimon Tong and Daphne Koller. Support vector machine active learning with applications to text classification. Journal of Machine Learning Research (JMLR), 2:45\u201366, 2001.   \nRaphael Townshend, Rishi Bedi, Patricia Suriana, and Ron Dror. End-to-end learning on 3d protein structure for interface prediction. Advances in Neural Information Processing Systems, 32:15642\u2013 15651, 2019.   \nCas van der Oord, Matthias Sachs, D\u00e1vid P\u00e9ter Kov\u00e1cs, Christoph Ortner, and G\u00e1bor Cs\u00e1nyi. Hyperactive learning for data-driven interatomic potentials. npj Computational Materials, 9(1): 168, 2023.   \nVishwesh Venkatraman, Padmasini Ramji Chakravarthy, and Daisuke Kihara. Application of 3d zernike descriptors to shape-based ligand similarity searching. Journal of cheminformatics, 1(1): 1\u201319, 2009.   \nDan Wang and Yi Shang. A new active labeling method for deep learning. In International Joint Conference on Neural Networks (IJCNN), 2014.   \nLimei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. ComENet: Towards complete and efficient message passing for 3D molecular graphs. In The 36th Annual Conference on Neural Information Processing Systems, pages 650\u2013664, 2022.   \nLimei Wang, Haoran Liu, Yi Liu, Jerry Kurtin, and Shuiwang Ji. Learning hierarchical protein representations via complete 3D graph networks. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=9X-hgLDLYkQ.   \nZheng Wang and Jieping Ye. Querying discriminative and representative samples for batch mode active learning. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2013.   \nJiaxi Wu, Jiaxin Chen, and Di Huang. Entropy-based active learning for object detection with progressive diversity constraint. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \nKeqiang Yan, Yi Liu, Yuchao Lin, and Shuiwang Ji. Periodic graph transformers for crystal material property prediction. In The 36th Annual Conference on Neural Information Processing Systems, pages 15066\u201315080, 2022.   \nXin Yan, Jiabo Li, Zhihong Liu, Minghao Zheng, Hu Ge, and Jun Xu. Enhancing molecular shape comparison by weighted gaussian functions. Journal of chemical information and modeling, 53(8): 1967\u20131978, 2013.   \nDonggeun Yoo and In So Kweon. Learning loss for active learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 93\u2013102, 2019.   \nRandy J Zauhar, Eleonora Gianti, and William J Welsh. Fragment-based shape signatures: a new tool for virtual screening and drug discovery. Journal of Computer-Aided Molecular Design, 27: 1009\u20131036, 2013.   \nBeichen Zhang, Liang Li, Shijie Yang, Shuhui Wang, Zheng-Jun Zha, and Qingming Huang. Staterelabeling adversarial active learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.   \nXuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler, Xiner Li, Tianfan Fu, Yucheng Wang, Haiyang Yu, YuQing Xie, Xiang Fu, Alex Strasser, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling, Hannah Lawrence, Hannes St\u00e4rk, Shurui Gui, Carl Edwards, Nicholas Gao, Adriana Ladera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar Azizzadenesheli, Ada Fang, Al\u00e1n Aspuru-Guzik, Erik Bekkers, Michael Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Li\u00f2, Rose Yu, Stephan G\u00fcnnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina Barzilay, Tommi Jaakkola, Connor W. Coley, Xiaoning Qian, Xiaofeng Qian, Tess Smidt, and Shuiwang Ji. Artificial intelligence for science in quantum, atomistic, and continuum systems. arXiv preprint arXiv:2307.08423, 2023.   \nZhisong Zhang, Emma Strubell, and Eduard Hovy. A survey of active learning for natural language processing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6166\u20136190, 2022.   \nGengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. In The Eleventh International Conference on Learning Representations, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Ting Zhou, Karine Lafleur, and Amedeo Caflisch. Complementing ultrafast shape recognition with an optical isomerism descriptor. Journal of Molecular Graphics and Modelling, 29(3):443\u2013449, 2010. ", "page_idx": 16}, {"type": "text", "text": "Jia-Jie Zhu and Jos\u00e9 Bento. Generative adversarial active learning. In arXiv:1702.07956, 2017. ", "page_idx": 16}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1 Proofs of the Theorems and Propositions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem 1. If $A$ and $B$ are triangular isometric, then $A$ and $B$ are reference distance isometric; If $A$ and $B$ are cross-angle isometric, then $A$ and $B$ are triangular isometric. ", "page_idx": 17}, {"type": "text", "text": "Proof. Suppose that $A$ and $B$ are triangular isometric, then there exists a collection of global group elements $\\gamma_{i}\\in S E(3)$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(r_{2},f\\left(a_{\\mathrm{far}}\\right),f(a_{i})\\right)=(\\gamma_{i}r_{1},\\gamma_{i}a_{\\mathrm{far}},\\gamma_{i}a_{i}),\\quad\\forall a_{i}\\in{\\cal A}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It follows immediately that for each point $a_{i}\\in A,\\left(r_{2},f(a_{i})\\right)=\\left(\\gamma_{i}r_{1},\\gamma_{i}a_{i}\\right)$ also holds. Thus, if $A$ and $B$ are triangular isometric, then $A$ and $B$ are reference distance isometric. ", "page_idx": 17}, {"type": "text", "text": "Suppose that $A$ and $B$ are cross-angular isometric, then there exists a collection of global group elements $\\gamma_{i j}\\in S E(3)$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(r_{2},f(a_{j}),f(a_{i}))=(\\gamma_{i j}r_{1},\\gamma_{i j}a_{j},\\gamma_{i j}a_{i}),\\quad\\forall a_{i},a_{j}\\in A,i\\neq j.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By fixing $a_{j}$ to be $a_{f a r}$ and the corresponding $\\gamma_{i j}$ to be $\\gamma_{i}$ , it follows immediately that for each point $\\bar{a}_{i}\\ \\in\\ A$ , there exists a collection of global group elements $\\gamma_{i}~\\in~S E(3)$ such that $\\left(r_{2},f\\left(a_{\\mathrm{far}}\\right),f(a_{i})\\right)=\\left(\\gamma_{i}r_{1},\\gamma_{i}a_{\\mathrm{far}},\\gamma_{i}a_{i}\\right)$ . Thus, if $A$ and $B$ are cross-angular isometric, then $A$ and $B$ are triangular isometric. ", "page_idx": 17}, {"type": "text", "text": "Proposition 1. $G R_{o u r s}$ is at least as expressive as the GWL test. In other words, $G R_{o u r s}$ suffices to distinguish any non-isomorphic molecular structures that are distinguishable by any 3D GNN. ", "page_idx": 17}, {"type": "text", "text": "Proof. We prove the case when the reference points $r_{1}$ and $r_{2}$ are the centroids of the point clouds $A$ and $B$ , respectively. The proof for other choices of reference points follows analogously. First, we will show that $\\zeta$ , which is the function that gives us the geometric representation $G R_{\\mathrm{ours}}$ given a point cloud, is $E(3)$ -orbit injective. ", "page_idx": 17}, {"type": "text", "text": "Without loss of generality, assume that the centroids of these two point clouds are at the origin. Otherwise, they can be fixed by a translation in $\\ensuremath{\\mathbb{T}}(3)\\cong E(3)/O(3)$ . For simplicity, we denote the point $f(a_{i})$ as $b_{i}$ , and bold symbol represents the corresponding vectors. Suppose that $G R_{\\mathrm{ours}}$ is the same for both points clouds $A$ and $B$ , that is to say, we have the following conditions: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{||a_{i}||=||b_{i}||,\\ \\forall i\\in\\mathbb{N}_{\\leq n}}\\\\ &{\\frac{\\langle a_{\\mathrm{far}},a_{i}\\rangle}{||a_{\\mathrm{far}}||\\cdot||a_{i}||}=\\frac{\\langle b_{\\mathrm{far}},b_{i}\\rangle}{||b_{\\mathrm{far}}||\\cdot||b_{i}||},\\ \\forall i\\in\\mathbb{N}_{\\leq n}}\\\\ &{\\frac{\\langle a_{\\mathrm{far}},a_{i}-a_{j}\\rangle}{||a_{\\mathrm{far}}||\\cdot||a_{i}-a_{j}||}=\\frac{\\langle b_{\\mathrm{far}},b_{i}-b_{j}\\rangle}{||b_{\\mathrm{far}}||\\cdot||b_{i}-b_{j}||},\\ \\forall i,j\\in\\mathbb{N}_{\\leq n},i\\not=j.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It follows from (5) and (6) that for any $k\\in\\mathbb{N}_{\\leq n}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle\\boldsymbol{a}_{\\mathrm{far}},\\boldsymbol{a}_{k}\\rangle=\\langle\\boldsymbol{b}_{\\mathrm{far}},\\boldsymbol{b}_{k}\\rangle.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, for all $i,j\\in\\mathbb{N}_{\\leq n},i\\neq j$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle a_{\\mathrm{far}},a_{i}-a_{j}\\rangle=\\langle b_{\\mathrm{far}},b_{i}-b_{j}\\rangle.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, it is clear from (7) that all the pair-wise distances are the same, i.e., $\\lVert a_{i}-a_{j}\\rVert=\\lVert b_{i}-b_{j}\\rVert$ for all $i,j\\in\\mathbb{N}_{\\leq n},i\\neq j.$ . Thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\pmb{{a}}_{i}-\\pmb{{a}}_{j}\\|^{2}=\\|\\pmb{{a}}_{i}\\|^{2}-2\\left\\langle\\pmb{{a}}_{i},\\pmb{{a}}_{j}\\right\\rangle+\\|\\pmb{{a}}_{j}\\|^{2}}\\\\ {\\|\\pmb{{b}}_{i}-\\pmb{{b}}_{j}\\|^{2}=\\|\\pmb{{b}}_{i}\\|^{2}-2\\left\\langle\\pmb{{b}}_{i},\\pmb{{b}}_{j}\\right\\rangle+\\|\\pmb{{b}}_{j}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It follows that $\\langle{a_{i},a_{j}}\\rangle=\\langle{b_{i},b_{j}}\\rangle$ from (5). ", "page_idx": 17}, {"type": "text", "text": "It is safe to assume that $(a_{1},a_{2},\\ldots,a_{n})$ spans $\\mathbb{E}^{3}$ , otherwise the proof is trivial when all points are co-planer. Without loss of generality, let $(a_{1},a_{2},a_{3})$ be a basis for $\\mathbb{E}^{3}$ . It is easy to see that $(b_{1},b_{2},\\overline{{{b_{3}}}})$ is also a basis for $\\mathbb{E}^{\\breve{3}}$ . ", "page_idx": 17}, {"type": "text", "text": "Let $X$ and $Y$ denote the matrices whose columns are $(a_{1},a_{2},a_{3})$ and $\\left(b_{1},b_{2},b_{3}\\right)$ , respectively. Let $G$ denote the associated Gram matrix, i.e. $G=X^{T}X=Y^{T}Y.$ , then $G$ is symmetric and positive semi-definite. Moreover, as both $X$ and $Y$ are full-rank, there exist orthogonal matrices $Q_{X},Q_{Y}$ and upper triangular matrices $R_{X},R_{Y}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{X=Q_{X}R_{X}}\\\\ {Y=Q_{Y}R_{Y}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{G=X^{\\top}X=R_{X}^{\\top}R_{X}}\\\\ {G=Y^{\\top}Y=R_{Y}^{\\top}R_{Y}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The form above follows the pattern of Cholesky decomposition. As $G$ is symmetric and positive semi-definite, the Cholesky decomposition is unique. Thus, $R_{X}\\,=\\,R_{Y}$ . Thus, $X=Q_{X}Q_{Y}^{-1}Y$ , where $Q_{X}Q_{Y}^{-1}$ is an orthogonal matrix. Thus, there exists $g\\in O(3)$ such that $g X=Y$ . If $n\\leq3$ , this completes the proof. ", "page_idx": 18}, {"type": "text", "text": "When n \u22654, for any k \u22654, ak =  i3=1 ciai, where {ci}i3=1 are uniquely determined by ci = $\\left<\\mathbf{{a}}_{k},\\mathbf{{a}}_{i}\\right>$ . Then, $g\\pmb{a}_{k}=g\\left(\\sum_{i=1}^{3}c_{i}\\pmb{a}_{i}\\right)=\\sum_{i=1}^{3}c_{i}\\left(g\\pmb{a}_{i}\\right)=\\sum_{i=1}^{3}c_{i}\\pmb{b}_{i}=\\pmb{b}_{k}.$ ", "page_idx": 18}, {"type": "text", "text": "Now, without loss of generality, we can conclude that if $\\zeta\\left(A\\right)=\\zeta\\left(B\\right)$ , then there exists $g\\in E(3)$ such that $g A=B$ . As we have an injective map, our method is naturally at least as expressive as the GWL test for $E(3)$ isomorphism. As a result, our method surpasses all existing 3D GNNs in terms of distinguishing non-isomorphic point clouds. ", "page_idx": 18}, {"type": "text", "text": "A.2 Geometric Weisfeiler-Leman (GWL) Test ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the Geometric Weisfeiler-Leman (GWL) test, consider a graph $\\mathcal{G}$ with its set of vertices represented as $\\mathscr{V}(\\mathscr{G})$ and its set of edges as $\\mathcal E(\\mathcal G)$ . A vertex in graph $\\mathcal{G}$ is denoted by $i$ , and ${\\mathcal N}_{i}$ signifies the set of vertices adjacent to $i$ . The color of vertex $i$ at iteration $t$ is given by $c_{i}^{(t)}$ , and the geometric object for vertex $i$ at iteration $t$ is represented by ${\\pmb g}_{i}^{(t)}$ . ", "page_idx": 18}, {"type": "text", "text": "The procedure for the GWL test is as follows: ", "page_idx": 18}, {"type": "text", "text": "1. Initialization: Each vertex $i$ is assigned an initial color $c_{i}^{(0)}$ and a geometric object gi(0), typically based on its local property or geometric attributes.   \n2. Iterative Aggregation: For each iteration $t\\geq1$ , the geometric object of each vertex $i$ is updated to aggregate geometric information from its $t$ -hop neighborhood, represented as ${\\pmb g}_{i}^{(t)}$ , which includes the colors and geometric objects from the previous iteration of vertex $i$ and its neighbors.   \n3. Color Update: The color of each vertex $i$ at iteration $t$ is computed by aggregating the geometric information around vertex $i$ using a $\\mathfrak{G}$ -orbit injective and $\\mathfrak{G}$ -invariant function, denoted by I-HASH, i.e., $c_{i}^{(t)}:=\\operatorname{I}^{-\\mathrm{HASH}^{(t)}}\\left(g_{i}^{(t)}\\right)$   \n4. Termination: The procedure terminates when colors do not change from the previous iteration or a predetermined maximum number of iterations is reached.   \n5. Graph Comparison: Finally, two geometric graphs $\\mathcal{G}$ and $\\mathcal{H}$ are geometrically nonisomorphic if there exists some iteration $t$ for which the sets of colors of their vertices are not equal, i.e., $\\left\\{\\left\\{c_{i}^{(t)}\\mid i\\in\\mathcal{V}(\\mathcal{G})\\right\\}\\right\\}\\neq\\left\\{\\left\\{c_{i}^{(t)}\\mid i\\in\\mathcal{V}(\\mathcal{H})\\right\\}\\right\\}$ . ", "page_idx": 18}, {"type": "text", "text": "A.3 Statistical Moments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The equations that we used for calculating four moments are as follows. ", "page_idx": 18}, {"type": "text", "text": "The mean, often referred to as the average, represents the sum of all data points divided by the number of data points and is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Mean}={\\frac{\\sum_{i=1}^{n}x_{i}}{n}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Variance measures the spread or dispersion of a dataset and is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\mathrm{Variance}}={\\frac{\\sum_{i=1}^{n}(x_{i}-\\mathbf{Mean})^{2}}{n-1}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Skewness gauges the asymmetry of a dataset\u2019s distribution. Here we sightly change its definition to be positive for convenience as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Skewness}=\\frac{\\sum_{i=1}^{n}|x_{i}-\\mathbf{M}\\mathrm{ean}|^{3}/n}{\\{\\sum_{i=1}^{n}(x_{i}-\\mathbf{M}\\mathrm{ean})^{2}/(n-1)\\}^{3/2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Kurtosis assesses the \u201ctailedness\u201d of a dataset\u2019s distribution as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Kurtosis}={\\frac{\\sum_{i=1}^{n}(x_{i}-\\mathbf{Mean})^{4}/n}{\\{\\sum_{i=1}^{n}(x_{i}-\\mathbf{Mean})^{2}/(n-1)\\}^{2}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.4 Schematic Diagram of our Framework ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A schematic diagram of our active sampling framework is depicted in Fig. 6. We are given a labeled training set $L$ , an unlabeled set $U$ and a query budget $k$ for each active learning iteration. The SphereNet model is first trained on the labeled set $L$ . In the second step, the trained model is applied on the unlabeled set to compute a prediction uncertainty of each unlabeled molecule, which is used to populate the uncertainty vector $r$ ; the diversity matrix $D$ is also computed in this step where $D(i,j)$ is the diversity between unlabeled molecules $x_{i}$ and $x_{j}$ . Next, the QP problem is solved to select $k$ unlabeled molecules for annotation. These molecules are removed from the unlabeled set $U$ and appended to the labeled set $L$ . The active sampling process is continued iteratively until some stopping criterion is satisfied (taken as 7 iterations in our work). ", "page_idx": 19}, {"type": "text", "text": "Note that, computing the diversity matrix $D$ in Step 3 needs to be executed just once for the whole process. Once we have the initial $D$ , as more and more samples are queried through AL, we keep deleting the corresponding rows and columns from $D$ to derive the updated matrix. ", "page_idx": 19}, {"type": "image", "img_path": "He2GCHeRML/tmp/81578dd4274b3c231520bf7452bb3c1f7cfda88bb65aca040d5e8de7ee22d32c.jpg", "img_caption": ["Figure 6: Schematic diagram of the proposed active learning framework. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.5 Related Work for Active Learning ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Active Learning (AL) is a well-researched problem in the machine learning community [Settles, 2009]. Uncertainty sampling is an important strategy for AL, where unlabeled samples with the highest prediction uncertainties are queried for annotation. Several techniques have been explored to compute the uncertainty, such as Shannon\u2019s entropy [Guo and Schuurmans, 2007, Li and Guo, 2013], the distance of a sample from the separating hyperplane for SVM classifiers [Tong and Koller, 2001], the disagreement among a committee of classifiers regarding the label of a sample [Freund et al., ", "page_idx": 19}, {"type": "image", "img_path": "He2GCHeRML/tmp/4b97428889d6f7fa1584e1aa21c69b513a12d5f2e09201dfb8dd83dda0d52759.jpg", "img_caption": ["Figure 7: Active learning performance results. The graphs show the mean (averaged over 3 runs) and the errorbars for all the methods. Best viewed in color. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "1997, Gilad-Bachrach et al., 2005], among others [Hoi et al., 2006, HOI et al., 2008, Guo and Greiner, 2007, Freytag et al., 2014]. With the advent of deep learning, Deep AL has attracted significant research attention [Hino, 2020, Ren et al., 2021], Entropy-based methods are developed as well [Wang and Shang, 2014, Ranganathan et al., 2017]. Yoo and Kweon [2019] cascaded a task-agnostic loss learning module that queries samples with the highest predicted loss values. Huang et al. [2021] proposed a strategy based on temporal output discrepancy. Techniques based on adversarial training have also been explored [Sinha et al., 2019, Mayer and Timofte, 2020, Zhang et al., 2020, Zhu and Bento, 2017]. Bayesian neural networks (BNNs) [Lampinen and Vehtari, 2001, Titterington, 2004, Goan and Fookes, 2020] and deep model ensemble [Lakshminarayanan et al., 2017, Huang et al., 2017] generally achieve superior performance but may induce excessive computational cost. ", "page_idx": 20}, {"type": "text", "text": "Diversity/representativeness based AL sampling has also been exploited. A core-set sampling technique proposed by ? queries a batch of samples such that a model trained on the queried subset is competitive for the remaining data samples. Diversity sampling has also been exploited in the context of Bayesian neural networks [Kirsch et al., 2019]. Buchert et al. [2023] uses diversity sampling, together with self-supervised representation learning to select an informative seed set for AL. Combinations of uncertainty/diversity/representativeness-based criteria have also been used as query functions in AL research [Chakraborty et al., 2015, Wu et al., 2022, Ash et al., 2020]. ", "page_idx": 20}, {"type": "text", "text": "A.6 Results with the Evidential Uncertainty Baseline ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The active learning performance results on the four properties studied (mu, alpha, homo, and lumo) are depicted in Fig. 7. As mentioned in Sec. 4.2, we note that Evidential Uncertainty depicts significantly high error values than the other methods, for all the four properties. ", "page_idx": 20}, {"type": "text", "text": "A.7 Performance using the DimeNet++ Backbone ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The objective of this experiment is to study the performance of our framework with DimeNet++ [Gasteiger et al., 2020a] as the backbone model of our active learning approach. We use the mu and lumo properties from the QM9 dataset in this experiment. We use the same experimental setup as detailed in Section 4.1 of the paper. The results are depicted in Figure 8. The proposed framework consistently outperforms all the baselines at each AL iteration across both the datasets. ", "page_idx": 20}, {"type": "text", "text": "The results of the statistical tests of significance are reported in Table 3. Each entry in the table denotes the p-value of the paired t-test between our method and the corresponding baseline (denoted in the columns) for the property studied (denoted in the rows). We note that the performance improvement achieved by our method is statistically significant $(p<0.05)$ compared to all the baselines for both the properties. These results corroborate the robustness of our framework to the underlying GNN backbone. ", "page_idx": 20}, {"type": "table", "img_path": "He2GCHeRML/tmp/e9ecaeb035be7f27922f6c907abdcb63155a9032ea5f135ddd7fbb40ef6afbc2.jpg", "table_caption": ["Table 3: The table shows the p-values obtained using paired t-test between the results our method against each of the baselines for the mu and lumo properties, using Dimenet $^{++}$ backbone. "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "He2GCHeRML/tmp/c9934decac66cc6ed5da79f0df06f0abb8af14885b234ddbc22550a9d5b9c478.jpg", "img_caption": ["Figure 8: Study of our framework using the DimeNet $^{++}$ backbone. The graphs show the mean (averaged over 3 runs) and the errorbars for all the methods. Best viewed in color. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "He2GCHeRML/tmp/ad0b731ea2c58364222c4d46156cfa3f558092e2490fcb889aa5a218cd4fa389.jpg", "img_caption": ["Figure 9: Study of our framework on Aspirin molecules using the SphereNet backbone. Best viewed in color. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "A.8 Generalization: Performance on the MD17 Dataset ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The research of 3D molecular learning is new, and there are only a few reliable benchmark datasets for 3D molecules (containing atom types as well as XYZ coordinates for all atoms for each molecule). To test the generalization ability of our proposed method, we study the performance of our framework on the MD17 dataset [Chmiela et al., 2017a]. QM9 consists of molecules in equilibrium, while MD17 contains several thermalized (i.e., non-equilibrium, slightly moving) molecular systems. Additionally, QM9 contains various quantum properties for molecules, like the important homo and lomo orbitals. MD17 is for dynamic system simulation, thus it contains labels for both the energy and atomic forces. In summary, we test our methods on molecule systems in both equilibrium and non-equilibrium, covering various quantum properties and molecular dynamics tasks. ", "page_idx": 21}, {"type": "text", "text": "We used 300 samples as the initial training set, 700 samples as the unlabeled set and $1,000$ test samples; we used 100 as the batch size and conducted 7 iterations of active learning. For this dataset, we train the network for 500 epochs. The results on Aspirin molecules using the SphereNet as the backbone of our GNN are depicted in Fig. 9. Our framework once again depicts promising performance and attains the lowest MAE values across all the AL iterations compared to all the baselines. These results further demonstrate the promise and potential of our method for scientific applications. ", "page_idx": 21}, {"type": "text", "text": "A.9 Statistical Tests of Significance for the Query Budget Experiment ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table 4 reports the results of the statistical tests of significance for the study of query budget (presented in Sec. 4.3). Each entry in the table denotes the p-value of the paired t-test between our method and the corresponding baseline (denoted in the columns) for the query budget (denoted in the rows) for the mu property. From the table, we note that the improvement in performance achieved by our method is statistically significant $p<0.05)$ compared to all the baselines, consistently for all the query budgets. ", "page_idx": 21}, {"type": "table", "img_path": "He2GCHeRML/tmp/bbe54838f0c10e375b41a9ab699a7373e457b47a060d3fac83d25483678bf457.jpg", "table_caption": ["Table 4: The table shows the p-values obtained using paired t-test between the results our method against each of the baselines for the mu property for query budgets 1, 000, 1, 500 and 2, 000. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "A.10 Addtional Ablation Studies ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The Individual Impact of Diversity and Uncertainty Components. In Fig. 10, we present the result on the individual impact of the diversity and uncertainty components. Our proposed method outperforms the individual use of diversity or uncertainty alone. The key to this outperformance lies in our method\u2019s dual focus on both geometric importance and chemical contexts. Moreover, it can be observed that the diversity component alone shows strong performance; it is only slightly less effective than our method because it captures the geometries of molecules, which are fundamental in distinguishing different molecules with different properties. On top of this, we also conduct statistical tests to conclude that the improvement of our method is significant compared to only diversity or only uncertainty in Table 5. ", "page_idx": 22}, {"type": "table", "img_path": "He2GCHeRML/tmp/784dbdacdacc098d4a66e7386dd37114de3a4018a7619c559ca822885244ccc9.jpg", "table_caption": ["Table 5: The table shows the p-values obtained using paired t-test between the result of our method against uncertainty only and diversity only components in ablation study for mu and lumo prediction. "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "He2GCHeRML/tmp/32564a0715e6db1c7c4ce0fcb3f9c87ab58402cea59401f11e0198c10890f7f9.jpg", "img_caption": ["Figure 10: Ablation study results studying the individual impact of uncertainty and diversity on the mu and lumo properties with SphereNet. Best viewed in color. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "A Comparison between Our Diversity Component and SOAP. We include a comparison between our method and one that uses a well-known geometric descriptor in chemistry, the SOAP descriptor [Bart\u00f3k et al., 2013]. SOAP produces descriptors that characterize local atomic environments using spherical harmonics and radial basis functions. It incorporates both geometric information and elemental (species) details. Table 6 presents the results on the QM9 dataset for two important properties, mu and lumo. These results demonstrate that our diversity component clearly outperforms SOAP. Consequently, our overall method, which combines both diversity and uncertainty, also surpasses the performance of the SOAP descriptors. The p-values in Table 7 further illustrate that our proposed method significantly improves the selection strategy compared to SOAP. This improvement can be attributed to the more localized nature of the SOAP descriptors and the inability to maintain permutation invariance. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "table", "img_path": "He2GCHeRML/tmp/b87c25847cebbabbfe4f4617385a51138cc9407f8c17efcc89de6038a1ec167c.jpg", "table_caption": ["Table 6: The table shows a comparison between our proposed descriptor and the SOAP descriptor for the properties mu and lumo, using SphereNet as the backbone. "], "table_footnote": ["Abbreviations: D means using diversity Only; B means using both uncertainty $^+$ diversity. The results from the method with superior performance are highlighted in bold. "], "page_idx": 23}, {"type": "text", "text": "Table 7: The table shows the $\\mathbf{p}$ -values obtained from a paired t-test comparing the results of our method against those of SOAP for the properties mu and lumo, using SphereNet as the backbone. ", "page_idx": 23}, {"type": "table", "img_path": "He2GCHeRML/tmp/7e532a41ec04a34b3b092d1718927b0f5ae46727c0a0fa7aa240f8ab78bef1d2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "A Comparison between our method and BatchBALD. We investigate the impact of our quadratic programming formulation compared to the greedy, clustering-based Bayesian uncertainty baseline, BatchBALD [Kirsch et al., 2019], which selects a diverse batch of samples by maximizing mutual information. Our method provides a more structured approach to uncertainty, particularly tailored for 3D molecular data. The results, presented in Fig. 11, demonstrate the effectiveness of our approach. Additionally, we conducted statistical tests, as shown in Table 8, which confirm that the improvement of our method over the baselines is statistically significant. This outperformance can be attributed to the components specifically designed for 3D molecular graphs. ", "page_idx": 23}, {"type": "table", "img_path": "He2GCHeRML/tmp/5e7c1ffccb9a805e57891a2bb7d40d2e98a19e5ca4068243fc345dc2a062a797.jpg", "table_caption": ["Table 8: The p-values obtained using paired t-test between the results our method against each of the baselines for all the properties studied. Here, L. Loss refers to Learning Loss. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "A.11 Licenses for Existing Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We list all the licenses for existing assets in Table 9. ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "He2GCHeRML/tmp/43c69f65138cfa6656aa0f7af91b40f39c2af4df9ad20c9b78325e69068e3447.jpg", "img_caption": ["Figure 11: Active learning performance results with SphereNet on QM9 Dataset. Best viewed in color. "], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "He2GCHeRML/tmp/df9414dd4efde3bd6e9fec5dc0f24316888ed00e4ffc77743e63f37c47fa1e64.jpg", "table_caption": ["Table 9: Assets, Licenses, and Descriptions "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our novelty, contributions, and scope are accurately supported theoretically and empirically. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We include a separate Limitation paragraph at the end of the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The proofs are complete without strong assumptions. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The used dataset, baseline models, and implementation details are provided in great detail in the main paper and the Appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have released the code and the link is provided in the paper. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Experimental setup details are provided in both the main paper and the Appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Most results are averaged over the three splits to rule out the effects of randomness. Both means and error bars are given. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Detailed information on the used GPUs is provided. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The research conducted conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We include a separate paragraph to discuss the potential societal impacts of our research. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risk of misuse. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The licenses for all the data and models we used are explicitly mentioned in Table 9. In certain cases where no official implementation was released, we implemented our own; thus, the license is not applicable, and we note N/A in the table. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]