{"references": [{"fullname_first_author": "Mark Abdelshiheed", "paper_title": "Leveraging deep reinforcement learning for metacognitive interventions across intelligent tutoring systems", "publication_date": "2023-00-00", "reason": "This paper is directly relevant to the main topic of the current paper, as it discusses the application of deep reinforcement learning in intelligent tutoring systems, which is one of the main application domains explored in the current paper."}, {"fullname_first_author": "Min Chi", "paper_title": "Empirically evaluating the application of reinforcement learning to the induction of effective and adaptive pedagogical strategies", "publication_date": "2011-00-00", "reason": "This paper is important because it provides a foundational understanding of how reinforcement learning can be applied to create effective and adaptive pedagogical strategies, which is a key aspect of the research presented in the current paper."}, {"fullname_first_author": "Hongseok Namkoong", "paper_title": "Off-policy policy evaluation for sequential decisions under unobserved confounding", "publication_date": "2020-00-00", "reason": "This paper is highly relevant to the current paper's focus on off-policy selection, as it deals with the challenges of evaluating policies in situations where confounding factors are present, which is a common issue in human-centric systems."}, {"fullname_first_author": "Allen Nie", "paper_title": "Data-efficient pipeline for offline reinforcement learning with limited data", "publication_date": "2022-00-00", "reason": "This paper is highly relevant because it tackles a key practical challenge in offline reinforcement learning, the limited amount of data available, offering methods that address the scarcity of data in human-centric scenarios, aligning with the current paper's focus on real-world application."}, {"fullname_first_author": "Philip Thomas", "paper_title": "Data-efficient off-policy policy evaluation for reinforcement learning", "publication_date": "2016-00-00", "reason": "This paper provides a foundational contribution to off-policy evaluation, a crucial technique for evaluating policies without requiring extensive online interaction, directly relevant to the current paper's methodology and results."}]}