[{"figure_path": "p1ft33Mu3J/figures/figures_6_1.jpg", "caption": "Figure 1: In-context learning performance for noisy linear regression problem across models with different number of layers and \u03c3max for \u03c3\u03c4 ~ U(0, \u03c3max). Each marker corresponds to a separately trained model with a given number of layers. Models with diagonal attention weights (DIAG) match those with full attention weights (FULL). Models specialized on a fixed noise (GD++) perform poorly, similar to a Ridge Regression solution with a constant noise (CONSTRR). Among the baselines, only tuned exact Ridge Regression solution (TUNEDRR) is comparable with linear transformers.", "description": "The figure displays the performance of various models on a noisy linear regression task.  Each model is evaluated across a range of maximum noise levels (\u03c3max) and different numbers of layers.  The plot shows that linear transformers (both full and diagonal attention variants) significantly outperform models designed for fixed noise levels and even a tuned ridge regression model.  The diagonal attention and full attention models achieve comparable results, while the fixed-noise model (GD++) performs poorly.", "section": "5 Power of diagonal attention matrices"}, {"figure_path": "p1ft33Mu3J/figures/figures_7_1.jpg", "caption": "Figure 1: In-context learning performance for noisy linear regression problem across models with different number of layers and \u03c3max for \u03c3\u03c4 ~ U(0, \u03c3max). Each marker corresponds to a separately trained model with a given number of layers. Models with diagonal attention weights (DIAG) match those with full attention weights (FULL). Models specialized on a fixed noise (GD++) perform poorly, similar to a Ridge Regression solution with a constant noise (CONSTRR). Among the baselines, only tuned exact Ridge Regression solution (TUNEDRR) is comparable with linear transformers.", "description": "This figure compares the performance of different models (GD++, Diag, Full, ConstRR, AdaRR, TunedRR) on a noisy linear regression task.  It shows how the adjusted evaluation loss varies with the number of layers in the model and different maximum noise levels (\u03c3max). The key takeaway is that linear transformers (Diag and Full) significantly outperform simpler baselines, especially with higher noise levels and a greater number of layers.", "section": "5 Power of diagonal attention matrices"}, {"figure_path": "p1ft33Mu3J/figures/figures_8_1.jpg", "caption": "Figure 3: In-context learning performance for noisy linear regression across models with varying number of layers for conditional noise variance \u03c3\u03c4 \u2208 {1,3} and \u03c3\u03c4 \u2208 {1,3,5}. Top: loss for models with various number of layers and per-variance profile for models with 7 layers. Bottom: Per-variance profile of the model across different numbers of layers. In-distribution noise is shaded gray.", "description": "This figure displays the in-context learning performance of various models on noisy linear regression problems with different numbers of layers and noise variance distributions.  The top section shows the overall loss for models with different layer counts and a per-variance profile for 7-layer models. The bottom section presents per-variance profiles for models with varying layer counts (2-7 layers).  The shaded gray regions indicate in-distribution noise variance. The plot helps to analyze how the model performance changes as the number of layers and the noise variance change.", "section": "Experiments"}, {"figure_path": "p1ft33Mu3J/figures/figures_9_1.jpg", "caption": "Figure 4: Weights for 4 layer linear transformer with FULL parametrization trained with categorical noise \u03c3\u03c4 \u2208 {1,3}. Top: weights for Q\u00b9 matrix, bottom: weights for P\u00b9 matrix.", "description": "This figure shows the weight matrices (Q and P) for a 4-layer linear transformer with full parametrization. The model was trained using categorical noise with values of 1 and 3.  The top half displays the Q matrix across layers 0-3, while the bottom half displays the P matrix across the same layers. Each subplot represents a layer, showing the weight matrix's values as a heatmap.  The color scale represents the magnitude of the weights, with darker colors indicating larger magnitudes. The visualization helps understand how the weights evolve across layers during the learning process within the linear transformer model for this specific noise distribution.", "section": "5 Power of diagonal attention matrices"}, {"figure_path": "p1ft33Mu3J/figures/figures_16_1.jpg", "caption": "Figure 5: Linear transformer models show a consistent decrease in error per layer when trained on data with mixed noise variance \u03c3\u03c4 ~ U(0, 5). The error bars measure variance over 5 training seeds.", "description": "The figure displays the adjusted evaluation loss for linear transformer models (GD++, Diag, Full) with varying numbers of layers (1 to 7) trained on data with mixed noise variance.  The x-axis represents the number of layers, and the y-axis shows the adjusted evaluation loss. Each line represents a different number of layers, and the shaded area around each line shows the variance over 5 training seeds. The figure demonstrates that, for all model types, the loss consistently decreases as the number of layers increases, indicating that the models learn effectively from the noisy data, with a more consistent and accurate decrease in error as the number of layers increases.", "section": "5.3 Understanding wxy: adapting step-sizes"}, {"figure_path": "p1ft33Mu3J/figures/figures_17_1.jpg", "caption": "Figure 6: Example of unadjusted loss given by directly minimizing (7). It is pretty hard to see variation between comparable methods using this loss directly.", "description": "This figure shows the unadjusted evaluation loss for different models as a function of the noise variance.  The unadjusted loss is calculated directly from the loss function, without any adjustments for the oracle loss (the best possible solution given the noise variance).  It is difficult to compare the methods using this loss because the scale of the loss is heavily influenced by the amount of noise.", "section": "6 Experiments"}, {"figure_path": "p1ft33Mu3J/figures/figures_18_1.jpg", "caption": "Figure 7: Layer by layer prediction quality for different models with \u03c3\u03c4 ~ U(0, 5). The error bars measure std over 5 training seeds.", "description": "This figure shows the performance of three different linear transformer models (GD++, Diag, and Full) across various noise levels (variance \u03c3) and different numbers of layers.  The x-axis represents the variance of the noise, and the y-axis represents the adjusted evaluation loss. Each line represents the model's performance after a specific number of layers, allowing a visualization of how model performance changes as layers are added and as noise levels vary.  The shaded regions represent the standard deviation. It reveals how the models' ability to handle noise changes with increasing depth and reveals different behaviors and convergence patterns for each model type.", "section": "More experiments"}]