[{"figure_path": "9m87e9Keq1/figures/figures_1_1.jpg", "caption": "Figure 1: Positive and negative synthetic data: Pictorial representation of positive/negative synthetic data definitions we use and how they are fed to SFT, RFT and DPO.", "description": "This figure illustrates the process of generating positive and negative synthetic data for training large language models (LLMs) on mathematical reasoning tasks.  The process begins with reasoning Q&A pairs sampled from a capable model (like GPT-4).  These pairs are used to fine-tune a base model (such as Llama-2) using Supervised Fine-Tuning (SFT), resulting in an SFT model.  This SFT model is then used to generate both positive (correct) and negative (incorrect) responses to the same questions.  The positive responses are used to further fine-tune the base model using Rejection Fine-Tuning (RFT), resulting in an RFT model. The negative responses are filtered by calculating a per-step credit (measuring the impact of each step in the solution trace). These filtered negative responses, along with the positive responses from the initial capable model, are used to fine-tune the SFT model using a per-step Direct Preference Optimization (DPO) approach, resulting in a per-step DPO model. The figure visually represents the flow of data and the different training methods used.", "section": "1 Introduction"}, {"figure_path": "9m87e9Keq1/figures/figures_4_1.jpg", "caption": "Figure 2: Positive data scaling laws: On GSM8K (a) and MATH (b), we evaluate SFT trained on Dsyn and RFT that uses SFT policy generated positives (Draft), as we scale Dsyn, observing D+ to be 2x as effective as Dsyn. In (c), we plot performance of RFT the number of correct solutions in D+ are scaled, for a fixed set of 8k/16k problems from Dsyn, observing that scaling model positives can amplify spurious correlations.", "description": "This figure shows the scaling laws of positive synthetic data for both GSM8K and MATH datasets.  It compares the performance of Supervised Fine-tuning (SFT) and Rejection Fine-tuning (RFT) as the size of the synthetic dataset increases.  Panel (a) and (b) show that RFT using self-generated positive data is twice as effective as using synthetic data from larger models. Panel (c) demonstrates that scaling model-generated positive data can lead to amplified spurious correlations, resulting in performance plateaus or even degradation.", "section": "5 Positive Data Improves Coverage, But Amplifies Spurious Correlations"}, {"figure_path": "9m87e9Keq1/figures/figures_4_2.jpg", "caption": "Figure 3: Under base LLM, D+sft has higher likelihood than Dsyn.", "description": "This figure shows the negative log-likelihood distributions of SFT data (D+sft) and RFT data (Dsyn) under the base language model (LLM).  The distribution of RFT data is shifted to the left compared to SFT data. This means the RFT data (positive responses from the 7B model) is easier to predict by the base LLM compared to the SFT data (responses from more capable models). The observation supports that the responses from the similar model are easier-to-fit than those from more capable models, resulting in reduced memorization during finetuning. ", "section": "5 Positive Data Improves Coverage, But Amplifies Spurious Correlations"}, {"figure_path": "9m87e9Keq1/figures/figures_5_1.jpg", "caption": "Figure 4: Spurious correlations in RFT data hurt performance.", "description": "This figure shows that spurious correlations in the RFT (Rejection Finetuning) data can negatively impact model performance.  The SFT (Supervised Finetuning) model trained on original synthetic data achieves relatively high test accuracy on both GSM8K and MATH datasets. However, when spurious correlations are introduced into the RFT data, the performance drops significantly, highlighting the detrimental effect of such correlations.", "section": "5 Positive Data Improves Coverage, But Amplifies Spurious Correlations"}, {"figure_path": "9m87e9Keq1/figures/figures_6_1.jpg", "caption": "Figure 5: Negative data scaling laws: We evaluate algorithms that consume negative data as we scale Dsyn, and compare them with only positive training (SFT) on Dsyn. On GSM8K (a) and MATH (b), we observe an 8\u00d7 gain from per-step DPO (Section 4) which aligns with our model of negative data that enables per-step credit assignment. In (c) we compare different negative data construction algorithms, and particularly note that na\u00efvely pairing positives and negatives [41] leads to worse performance as we scale Dsyn.", "description": "This figure demonstrates the scaling laws observed when training LLMs with negative synthetic data, showing a significant performance improvement (8x) using the per-step DPO method compared to using only positive data.  It also highlights the importance of constructing negative data appropriately, as naive methods can lead to worse performance.", "section": "6 Negative Synthetic Data Enables Per-Step Credit Assignment"}, {"figure_path": "9m87e9Keq1/figures/figures_6_2.jpg", "caption": "Figure 1: Positive and negative synthetic data: Pictorial representation of positive/negative synthetic data definitions we use and how they are fed to SFT, RFT and DPO.", "description": "This figure illustrates the process of generating and utilizing both positive and negative synthetic data for training language models.  Positive data consists of question-answer pairs with correct solutions, generated either by a capable model (e.g., GPT-4) or self-generated by the model being trained.  Negative data consists of model-generated answers deemed incorrect by a verifier, where the construction of negatives emphasizes \"critical\" steps in the reasoning process.  The figure shows how these positive and negative data points are fed into three different finetuning methods: Supervised Fine-tuning (SFT), Rejection Fine-tuning (RFT), and Direct Preference Optimization (DPO).  The arrows indicate the flow of data and the finetuning process.", "section": "Positive and negative synthetic data"}, {"figure_path": "9m87e9Keq1/figures/figures_7_1.jpg", "caption": "Figure 7: Per-step DPO improves Q-values at each step, standard DPO only improves at irrelevant steps.", "description": "This figure shows the average Q-values at each step for different negative data schemes.  The x-axis represents the step number, and the y-axis represents the average Q-value.  The lines represent different methods for incorporating negative data: SFT (supervised fine-tuning), DPO (Rafailov et al., 2023), DPO (Pal et al., 2024), and per-step DPO.  The figure demonstrates that per-step DPO improves Q-values at each step, while standard DPO only shows improvement in irrelevant steps. This highlights the advantage of per-step DPO in focusing on critical steps during training.", "section": "6.3 Why Does Credit Assignment from Negative Data Improve Model Generalization?"}, {"figure_path": "9m87e9Keq1/figures/figures_9_1.jpg", "caption": "Figure 8: Didactic analysis on star graph: In (a) we plot the SFT loss and Q-value of the critical token (adjacent node) for SFT and per-step DPO (starting from iter 60). Indicative of memorization SFT loss decreases at a slow rate, matching the slow rate of increase in the Q-value. In contrast per-step DPO loss sharply decreases during training. In (b) we notice a corresponding phase transition in the test error of per-step DPO starting from different under-trained SFT checkpoints, which does not happen for an over-trained SFT checkpoint in (c).", "description": "This figure shows the results of a didactic analysis on a star graph problem, comparing the performance of standard SFT and per-step DPO.  It demonstrates how per-step DPO addresses the memorization problem present in SFT by focusing on critical steps in the reasoning process. The plots visualize the training loss, Q-values of the critical token, and test error, highlighting the effectiveness of per-step DPO, especially when starting from under-trained models.", "section": "6.3.4 Validating Claims About Generalization: Controlled Analysis on a Didactic Problem"}, {"figure_path": "9m87e9Keq1/figures/figures_16_1.jpg", "caption": "Figure 9: RFT data with a single (self-generated) correct solution per problem outperforms SFT data (from highly-capable models) of the same size.", "description": "The figure shows that using RFT (Rejection finetuning) with self-generated data improves model performance compared to using SFT (Supervised finetuning) data from larger models.  Even with only one correct solution per problem from the RFT method, performance is better than using the same number of samples from the larger model's SFT data.  This indicates that self-generated RFT data is more sample-efficient than external SFT data for improving performance.", "section": "5 Positive Data Improves Coverage, But Amplifies Spurious Correlations"}, {"figure_path": "9m87e9Keq1/figures/figures_16_2.jpg", "caption": "Figure 10: On MATH, improving advantage estimates by computing advantages over both positive and negative traces sampled from \u03c0sft improves estimation error and final performance for per-step DPO.", "description": "This figure shows the results of an experiment on the MATH dataset comparing two variants of per-step DPO. The first uses only negative data, while the second uses both positive and negative data for computing advantage estimates. The results demonstrate that using both positive and negative data improves performance, especially as the size of the synthetic dataset increases. This is because using both types of data leads to more accurate advantage estimates, which is crucial for the effectiveness of per-step DPO.", "section": "6 Negative Synthetic Data Enables Per-Step Credit Assignment"}, {"figure_path": "9m87e9Keq1/figures/figures_17_1.jpg", "caption": "Figure 11: Advantage filtered RFT: We clone responses with high advantage steps from positive and negative responses sampled from the SFT policy. We filter all responses where the minimum advantage across all steps is in the bottom 50% percentile.", "description": "The figure shows two plots: one for the GSM8K dataset and one for the MATH dataset. Each plot displays the test error rate for RFT and per-step RFT models, as the size of the RFT dataset increases.  Advantage filtering is applied, meaning only responses with high advantage steps are included in the training data. The per-step RFT consistently outperforms standard RFT, especially as the dataset size grows, demonstrating the benefit of incorporating per-step advantage information in the training process.", "section": "6.3.1 Advantage-Weighted RL De-Emphasizes Spurious Steps and Emphasizes Critical Steps"}]