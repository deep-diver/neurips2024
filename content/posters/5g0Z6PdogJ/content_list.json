[{"type": "text", "text": "Testably Learning Polynomial Threshold Functions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lucas Slot Stefan Tiegel Department of Computer Science Department of Computer Science ETH Zurich ETH Zurich lucas.slot@inf.ethz.ch stefan.tiegel@inf.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Manuel Wiedmer   \nDepartment of Computer Science ETH Zurich   \nmanuel.wiedmer@inf.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rubinfeld & Vasilyan recently introduced the framework of testable learning as an extension of the classical agnostic model. It relaxes distributional assumptions which are difficult to verify by conditions that can be checked efficiently by a tester. The tester has to accept whenever the data truly satisfies the original assumptions, and the learner has to succeed whenever the tester accepts. We focus on the setting where the tester has to accept standard Gaussian data. There, it is known that basic concept classes such as halfspaces can be learned testably with the same time complexity as in the (distribution-specific) agnostic model. In this work, we ask whether there is a price to pay for testably learning more complex concept classes. In particular, we consider polynomial threshold functions (PTFs), which naturally generalize halfspaces. We show that PTFs of arbitrary constant degree can be testably learned up to excess eror $\\varepsilon\\,>\\,0$ in time $n^{\\mathrm{poly}(1/\\varepsilon)}$ This qualitatively matches the best known guarantees in the agnostic model. Our results build on a connection between testable learning and fooling. In particular, we show that distributions that approximately match at least poly $(1/\\varepsilon)$ moments of the standard Gaussian fool constant-degree PTFs (up to error $\\varepsilon$ ). As a secondary result, we prove that a direct approach to show testable learning (without fooling), which was successfully used for halfspaces, cannot work for PTFs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The PAC learning model of Valiant [30] has long served as a test-bed to study which learning tasks can be performed efficiently and which might be computationally difficult. One drawback of this model is that it is inherently noiseless. In order to capture noisy learning tasks, the following extension, called the agnostic model, has been introduced [19, 24]: Let $\\mathcal{F}$ be a class of boolean functions and let $\\mathcal{D}_{\\mathrm{joint}}$ be an (unknown) distribution over example-label-pairs in $\\mathcal{X}\\times\\{\\pm1\\}$ . Typically, ${\\mathcal{X}}=\\{0,1\\}^{n}$ or $\\mathbb{R}^{n}$ As input, we receive id samples from $\\mathcal{D}_{\\mathrm{joint}}$ . For a small $\\varepsilon>0$ , our task is to output a classifier $\\hat{f}$ (not necessarily in $\\mathcal{F}$ whose loss $L(\\hat{f},\\dot{{\\cal D}_{\\mathrm{joint}}}):={\\mathbb P}_{(x,z)\\sim{\\cal D}_{\\mathrm{joint}}}(\\hat{f}(x)\\neq z)$ is at most $\\mathrm{opt}+\\varepsilon$ where $\\operatorname{opt}:=\\operatorname*{inf}_{f\\in\\mathcal{F}}L(f,\\mathcal{D}_{\\mathrm{joint}})$ . The parameter opt thus indicates how \"noisy\" the instance is. We say that an algorithm agnostically learns $\\mathcal{F}$ up to error $\\varepsilon$ if it outputs such an $\\hat{f}$ . This model is appealing since it makes assumptions neither on the distribution of the input, nor on the type and amount of noise. After running an agnostic learning algorithm, we can therefore be certain that the output $\\hat{f}$ achieves error close to that of the best function in $\\mathcal{F}$ even without knowing what distribution the data came from. ", "page_idx": 0}, {"type": "text", "text": "Efficient learning and distributional assumptions. We are interested in understanding when agnostic learning can be performed effciently. Unfortunately, efficient learning is likely impossible without making assumptions on the distribution $\\mathcal{D}_{\\mathrm{joint}}$ , even for very simple function classes $\\mathcal{F}$ . For instance, consider the class $\\mathcal{F}_{\\mathrm{HS}}$ of halfspaces, i.e., boolean functions of the form $f(x)=\\mathrm{sign}(\\langle v,x\\rangle-\\theta)$ . Then, even if there exists a halfspace achieving arbitrarily small error, it is widely believed that outputting an $\\hat{f}$ that performs better than a random guess in the agnostic model takes at least super-polynomial time if no assumptions are made on $\\mathcal{D}_{\\mathrm{joint}}$ [8, 29]. To find efficient algorithms, one therefore has to make such assumptions. Typically, these take the form of assuming that the marginal $\\mathcal{D}_{\\mathcal{X}}$ of $\\mathcal{D}_{\\mathrm{joint}}$ over the examples $\\mathcal{X}$ belongs to a specific family of distributions. ", "page_idx": 1}, {"type": "text", "text": "Definition 1 (Agnostic learning with distributional assumptions). Let $\\varepsilon>0$ Alearner $\\boldsymbol{\\mathcal{A}}$ agnostically learns $\\mathcal{F}$ with respect to $\\mathcal{D}$ up to error $\\varepsilon$ if,for any distribution $\\mathcal{D}_{\\mathrm{joint}}$ on $\\mathcal{X}\\times\\{\\pm1\\}$ whosemarginal $\\mathcal{D}_{\\mathcal{X}}$ on $\\mathcal{X}$ is equal to $\\mathcal{D}$ given sufficient samplesfrom $\\mathcal{D}_{\\mathrm{joint}}$ \uff0c $i t$ outputswithhigh probability $a$ function $f:\\mathcal{X}\\to\\{\\pm1\\}$ satisfying $L(f,\\mathcal{D}_{\\mathrm{joint}})\\leq\\mathrm{opt}(\\mathcal{F},\\bar{\\mathcal{D}}_{\\mathrm{joint}})+\\varepsilon$ ", "page_idx": 1}, {"type": "text", "text": "For example, under the assumption that $\\mathcal{D}_{\\mathcal{X}}$ is standard Gaussian, we can find $\\hat{f}$ such that $L(\\hat{f},\\mathcal{D}_{\\mathrm{joint}})\\leq\\mathrm{opt}(\\mathcal{F}_{\\mathrm{HS}},\\mathcal{D}_{\\mathrm{joint}})+\\varepsilon$ in time $n^{O(1/\\varepsilon^{2})}$ [20, 11]. This runtime is likely bestpossible [12, 29, 13].1 Efficient learning is still possible under weaker assumptions on $\\mathcal{D}_{\\mathcal{X}}$ .e.g., log-concavity [20]. Regardless, we cannot know whether a learning algorithm achieves its claimed error without a guarantee that the input actually satisfies our distributional assumptions. Such guarantees are inherently difficult to obtain from a finite (small) sample. Furthermore, approaches like cross-validation (i.e., computing the empirical error of $\\hat{f}$ on a hold-out data set) fail in the noisy agnostic model, since we do not know the noise level opt. This represents a severe limitation of the agnostic learning model with distributional assumptions. ", "page_idx": 1}, {"type": "text", "text": "1.1  Testable learning. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To address this limitation, Rubinfeld & Vasilyan [28] recently introduced the following model, which they call testable learning: First, they run a tester on the input data, which attempts to verify a computationally tractable relaxation of the distributional assumptions. If the tester accepts, they then run a (standard) agnostic learning algorithm. The tester is required to accept whenever the data truly satisfies the distributional assumptions, and whenever the tester accepts, the output of the algorithm must achieve error close to opt. More formally, they define: ", "page_idx": 1}, {"type": "text", "text": "Definition 2 (Testable learning [28]). Let $\\varepsilon>0$ A tester-learner pair $(\\mathcal{T},\\mathcal{A})$ testably learns $\\mathcal{F}$ with respect to a distribution $\\mathcal{D}$ on $\\mathcal{X}$ up to error $\\varepsilon$ if, for any distribution $\\mathcal{D}_{\\mathrm{joint}}$ on $\\mathcal{X}\\times\\{\\pm1\\}$ , the following hold ", "page_idx": 1}, {"type": "text", "text": "1. (Soundness). If samples drawn from $\\mathcal{D}_{\\mathrm{joint}}$ are accepted by the tester $\\tau$ with high probability, then the learner $\\boldsymbol{\\mathcal{A}}$ must agnostically learn $\\mathcal{F}$ w.rt. $\\mathcal{D}_{\\mathrm{joint}}$ up to error $\\varepsilon$ 2. (Completeness). If the marginal of $\\mathcal{D}_{\\mathrm{joint}}$ on $\\mathcal{X}$ is equal to $\\mathcal{D}$ then the tester must accept samples drawn from $\\mathcal{D}_{\\mathrm{joint}}$ with high probability. ", "page_idx": 1}, {"type": "text", "text": "Soundness tells us that whenever a testable learning algorithm outputs a function $\\hat{f}$ ,this function achieves low error (regardless of whether $\\mathcal{D}_{\\mathrm{joint}}$ satisfies any distributional assumption). On the other hand, completeness tells us testable learners are no weaker than (distribution-specific) agnostic ones, in the sense that they achieve the same error whenever $\\mathcal{D}_{\\mathrm{joint}}$ actually satisfies our assumptions (i.e., whenever this error can in fact be guaranteed for the agnostic learner). The testable model is thus substantially stronger than the agnostic model with distributional assumptions. ", "page_idx": 1}, {"type": "text", "text": "Which function classes can be learned testably? A natural question is whether testable learning comes at an additional computational cost compared to (distribution-specific) agnostic learning. We focus on the setting where $\\mathcal{D}$ is the standard Gaussian on $\\mathcal{X}=\\mathbb{R}^{n}$ . Following [28, 15], we consider the following simple tester: Accept if and only if the empirical moments up to degree $k$ of the input distribution (approximately) match those of $\\mathcal{D}$ . This tester satisfies completeness as the empirical moments of a Gaussian concentrate well. Using this tester, Rubinfeld & Vasilyan [28] show that halfspaces can be testably learned in time $n^{\\tilde{O}(1/\\varepsilon^{4})}$ . Their runtime guarantee was improved to $n^{\\tilde{O}(1/\\varepsilon^{2})}$ in [15],(nearly) matching the best known non-tetable algorithm. This shows that there is no separation between the two models for halfspaces. On the other hand, a separation does exist for more complex function classes. Namely, for fixed accuracy $\\varepsilon>0$ , testably learning the class of indicator functions of convex sets requires at least $2^{\\Omega(n)}$ samples (and hence also time) [28], whereas agnostically learning them only takes subexponential time $2^{O({\\sqrt{n}})}$ , se [25]. The relation between agnostic and testable learning is thus non-trivial, depending strongly on the concept class considered. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.2 Our contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we continue to explore testable learning and its relation to the agnostic model. We consider the concept class of polynomial threshold functions (short PTFs). A degree- $d$ PTF is a function of the form $f(x)=\\mathrm{sign}(p(x))$ , where $p$ is a polynomial of degree at most $d$ . PTFs naturally generalize halfspaces, which correspond to the case $d=1$ . They form an expressive function class with applications throughout (theoretical) computer science, and have been studied in the context of circuit complexity [5, 26, 27, 3], and learning [18, 14]. Despite their expressiveness, PTFs can be agnostically learned in time $n^{O(d^{2}/\\varepsilon^{4})}$ [22], which is polynomial in $n$ for any fixed degree $d\\in\\mathbb{N}$ and error $\\varepsilon>0$ . They are thus significantly easier to learn in the agnostic model than convex sets. Our main result is that PTFs can be learned efficiently in the testable model as well. ", "page_idx": 2}, {"type": "text", "text": "Theorem 3 (Informal version of Theorem 19). Fix $d\\in\\mathbb{N}$ Then,for any $\\varepsilon>0,$ theconceptclass of degree-d polynomial threshold functions can betestably learned up to error $\\varepsilon$ w.r.t. the standard Gaussian in time and sample complexity $n^{\\mathrm{poly}(1/\\varepsilon)}$ ", "page_idx": 2}, {"type": "text", "text": "Theorem 3 is the first result achieving effcient testable learning for PTFs of any fixed degree $d$ (upto constanterror $\\varepsilon>0$ ). Previously, such a result was not even available for learning degree-2 PTFs with respect to the Gaussian distribution. It also sheds new light on the relation between agnostic and testable learning: there is no qualitative computational gap between the two models for the concept class of PTFs, whose complexity lies between that of halfspaces and convex sets in the agnostic model. ", "page_idx": 2}, {"type": "text", "text": "In addition to Theorem 3, we also show an impossibility result ruling out a certain natural approach to prove testable learning guarantees for PTFs. In particular, we show in Section 2.4 that an approach which has been successful for testably learning halfspaces in [28] provably cannot work for PTFs. ", "page_idx": 2}, {"type": "text", "text": "Limitations.  The dependence of the running time on the degree parameter $d$ and the error $\\varepsilon$ is (much) worse than in the agnostic model (see Theorem 19). Moreover, we do not have access to lower bounds on the complexity of testably learning PTFs which might indicate whether these dependencies are inherent to the problem, or an artifact of our analysis. The only lower bounds available apply already in the agnostic model, and show that the time complexity of agnostically (and thus also testably) learning degree- $d$ PTFs is at least $n^{\\Omega(d^{2}/\\varepsilon^{2})}$ in the SQ-model [12], and at least $n^{\\tilde{\\Omega}(d^{2-\\beta}/\\varepsilon^{2-\\beta})}$ for any $\\beta>0$ under a cryptographic hardnessassumption [29]. ", "page_idx": 2}, {"type": "text", "text": "1.3 Previous work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The two works most closely related to this paper are [28, 15]. Both rely on the following highlevel strategy. A standard result [20] shows that one can agnostically learn a concept class $\\mathcal{F}$ w.r.t. a distribution $\\mathcal{D}$ in time $n^{O(k)}$ if all elements of $\\mathcal{F}$ are well-approximated w.r.t. $\\mathcal{D}$ by degree- ${\\cdot k}$ polynomials. That is, if for all $f\\,\\in\\,{\\mathcal{F}}$ , there exists a degree- $\\cdot k$ polynomial $h$ such that $\\mathbb{E}_{X\\sim{\\mathcal{D}}}\\left[|{\\bar{h(X)}}-f(X)|\\right]\\leq\\varepsilon.$ . This result can be extended to the testable setting, but now one needs a good low-degree $L_{1}$ -approximation w.r.t. any distribution $\\mathcal{D}^{\\prime}$ accepted by the tester. Using the moment-matching tester outlined above, one thus needs to exhibit low-degree approximations to all functions in $\\mathcal{F}$ w.r.t. any distribution which approximately matches the first few moments of $\\mathcal{D}$ ", "page_idx": 2}, {"type": "text", "text": "A direct approach. In [28], the authors use a direct approach to show that if $\\mathcal{F}=\\mathcal{F}_{\\mathrm{HS}}$ is the class of halfspaces and $\\mathscr{D}=\\mathcal{N}(0,I_{n})$ , these approximators exist for $k=O(1/\\varepsilon^{4})$ , leading to an overall running time of $n^{O(1/\\varepsilon^{4})}$ for their testable learner. Their approach consists of two steps. First, they construct a low-degree approximation $q\\approx\\mathrm{sign}$ of the sign function in one dimension using standard techniques. Then, for any halfspace $f(x)=\\mathrm{sign}(\\langle v,x\\rangle-\\theta)$ ,theyset $h(x)=q(\\langle v,x\\rangle-\\theta)$ .By exploiting concentration and anti-concentration properties of the push-forward under linear functions of distributions that match the moments of a Gaussian, they show that $h$ is a good approximation of $f$ Unfortunately, this kind of approach cannot work for PTFs: We formally rule it out in Theorem 16. This is the aforementioned secondary contribution of our paper, which extends earlier impossibility results for (agnostic) learning of Bun & Steinke [6]. See Section 2.4 for details. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "An indirect approach using fooling.  In order to prove our main theorem we thus need a different approach. Gollakota, Klivans & Kothari [15] establish a connection between testable learning and the notion of fooling, which has played an important role in the study of pseudorandomness [4, 2, 9]. Its connection to learning theory had previously been observed in [23]. We say a distribution $\\mathcal{D}^{\\prime}$ fools a concept class $\\mathcal{F}$ up to error $\\varepsilon\\ >\\ 0$ with respect to $\\mathcal{D}$ if, for all $f\\ \\in\\ {\\mathcal{F}}$ , it holds that $|\\mathbb{E}_{X\\sim\\mathcal{D}}\\left[f(X)\\bar{\\right]}-\\mathbb{E}_{X\\sim\\mathcal{D^{\\prime}}}\\left[\\bar{f}(X)\\right]|\\le\\varepsilon$ . Roughly speaking, the work [15] shows that, if any distribution $\\mathcal{D}^{\\prime}$ which approximately matches the moments of $\\mathcal{D}$ up to degree $k$ fools $\\mathcal{F}$ with respect to $\\mathcal{D}$ then $\\mathcal{F}$ can be testably learned in time $n^{O(k)}$ (see Theorem 9 below). We remark that (approximately) moment-matching distributions have not been considered much in the existing literature on fooling. Rather, it has focused on distributions $\\mathcal{D}^{\\prime}$ whose marginals on any subset of $k$ variables are equal to those of $\\mathcal{D}$ , which is a stronger condition a priori. While it coincides with moment-matching in special cases (e.g., when $\\mathcal{D}$ is the uniform distribution over the hypercube), it does not when $\\bar{D}\\,\\bar{=}\\,\\mathcal{N}(0,I_{n})$ . Nevertheless, the authors of [15] show that (approximate) moment matching up to degree $k\\,=\\,\\tilde{O}(1/\\varepsilon^{2})$ fools halfspaces with respect to ${\\mathcal{N}}(0,I_{n})$ , allowing them to obtain the aforementioned result for testably learning $\\mathcal{F}_{\\mathrm{HS}}$ . In fact, they show that this continues to hold when $\\mathcal{F}$ consists of arbitrary boolean functions applied to a constant number of halfspaces. They also use existing results in the fooling literature to show that degree-2 PTFs can be testably learned under the uniform distribution over $\\{0,1\\}^{n}$ (but these do not extend to learning over $\\mathbb{R}^{n}$ w.r.t. a standard Gaussian). ", "page_idx": 3}, {"type": "text", "text": "Other previous work on testable learning. In weaker error models than the agnostic model or under less stringent requirements on the error of the learner it is known how to construct tester-learner pairs with runtime poly $(n,1/\\varepsilon)$ [16, 10]. These results have been extended to allow for the following stronger completeness condition: The tester has to accept, whenever $\\mathcal{D}_{\\mathrm{joint}}$ is an isotropic strongly log-concave distribution [17]. ", "page_idx": 3}, {"type": "text", "text": "2  Technical overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "From here, we restrict to the setting $\\mathcal{X}\\,=\\,\\mathbb{R}^{n}$ . We let $\\mathcal{D}$ be a well-behaved distribution on $\\mathbb{R}^{n}$ usually $\\smash{\\ensuremath{\\mathcal{D}_{\\!p}{}}=\\ensuremath{\\mathcal{N}_{\\!\\}}(0,I_{n})}$ is the standard Gaussian. For $x\\in\\mathbb{R}^{n}$ and a multi-index $\\alpha\\in\\mathbb{N}^{n}$ , we write $\\begin{array}{r}{x^{\\alpha}:=\\prod_{i=1}^{n}x_{i}^{\\alpha_{i}}}\\end{array}$ For $k\\in\\mathbb N$ we write $\\mathbb{N}_{k}^{n}:=\\{\\alpha\\in\\mathbb{N}^{n}$ \uff0c $\\textstyle\\sum_{i=1}^{n}\\alpha_{i}\\leq k\\}$ We say a statement holds \u2018with high probability\u2019 if it holds with probability $\\geq0.99$ The notation $O_{d}$ (resp. $\\Omega_{d},\\Theta_{d})$ hides factors that only depend on $d$ . We now define the moment-matching tester introduced above. ", "page_idx": 3}, {"type": "text", "text": "Definition 4 (Moment matching). Let $k\\in\\mathbb{N}$ and $\\eta\\geq0$ .We say a distribution $\\mathcal{D}^{\\prime}$ on $\\mathbb{R}^{n}$ approximatelymoment-matches $\\mathcal{D}$ up to degree $k$ and with slack n if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}_{X\\sim\\mathcal{D}}\\left[X^{\\alpha}\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[X^{\\alpha}\\right]|\\leq\\eta\\quad\\forall\\,\\alpha\\in\\mathbb{N}_{k}^{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 5. Let $k\\in\\mathbb{N}$ and $\\eta\\geq0$ . The approximate moment-matching tester $\\mathcal{T}_{\\mathrm{AMM}}=\\mathcal{T}_{\\mathrm{AMM}}(k,\\eta)$ for a distribution $\\mathcal{D}$ accepts the samples $\\bar{(x^{(1)},z^{(1)})},\\ldots,(x^{(m)},z^{(m)})^{\\!\\!\\!-}\\in\\mathbb{R}^{n}\\times\\{\\pm1\\}$ if, and only $i f,$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{X\\sim\\mathcal{D}}\\left[X^{\\alpha}\\right]-\\frac{1}{m}\\sum_{i=1}^{m}\\left(x^{(i)}\\right)^{\\alpha}\\right|\\leq\\eta\\quad\\forall\\,\\alpha\\in\\mathbb{N}_{k}^{n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "That is, $\\mathcal{T}_{\\mathrm{AMM}}(k,\\eta)$ accepts if and only if the moments of the empirical distribution belonging to the samples $\\{x^{(i)}\\}$ match themoments of $\\mathcal{D}$ up to degree $k$ and slack $\\eta$ . Note that $\\mathcal{T}_{\\mathrm{AMM}}(k,\\eta)$ requires timeatmost $O(m\\cdot n^{k})$ to decide whether to accept a set of $m$ samples. ", "page_idx": 3}, {"type": "text", "text": "The tester $\\mathcal{T}_{\\mathrm{AMM}}$ does not take the labels of the samples into account. In general, for testers $\\tau$ which depend only on the marginal $\\mathcal{D}$ of $\\mathcal{D}_{\\mathrm{joint}}$ on $\\mathbb{R}^{n}$ ,we say that $\\tau$ accepts a distribution $\\mathcal{D}^{\\prime}$ on $\\mathbb{R}^{n}$ if it accepts samples drawn from $\\mathcal{D}^{\\prime}$ with high probability (regardless of the labels). ", "page_idx": 3}, {"type": "text", "text": "2.2  Review of existing techniques for testable learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we review in more detail the existing techniques to establish guarantees for agnostic and testable learning discussed in Section 1.3. Our goals are twofold. First, we wish to highlight the technical difficulties that arise from proving error guarantees in the testable model versus the agnostic model. Second, we want to introduce the necessary prerequisites for our proof of Theorem 3 in Section 2.3, namely testable learning via fooling (see Theorem 9). ", "page_idx": 4}, {"type": "text", "text": "Learning and polynomial approximation.  A standard result [20] shows that one can agnostically learn any concept class that is well-approximated by low-degree polynomials in the following sense. ", "page_idx": 4}, {"type": "text", "text": "Theorem 6 ([20]). Let $k\\in\\mathbb{N}$ and $\\varepsilon>0$ . Suppose that, for any $f\\in\\mathcal F$ , there exists a polynomial $h$ of degree $k$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathcal{D}}\\left[|h(X)-f(X)|\\right]\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, $\\mathcal{F}$ canbe agnostically leaned up to error $\\varepsilon$ in time and sample complexity $n^{O(k)}/\\mathrm{poly}(\\varepsilon)$ ", "page_idx": 4}, {"type": "text", "text": "The underlying algorithm in the theorem above is polynomial regression w.r.t. the absolute loss function. In the testable setting, a similar result holds. The key difference is that one now needs good approximation w.r.t. all distributions accepted by the proposed tester. ", "page_idx": 4}, {"type": "text", "text": "Theorem 7. Let $k\\in\\mathbb{N}$ and $\\varepsilon>0$ Let $\\tau$ be a tester which accepts $\\mathcal{D}$ and which requires time and samplecomplexity $\\tau$ .Suppose that, for any $f\\in\\mathcal F$ ,and for any $\\mathcal{D}^{\\prime}$ accepted by $\\tau$ , there exists $a$ polynomial $h$ of degree $k$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[|h(X)-f(X)|\\right]\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, $\\mathcal{F}$ can be testably learned up to error $\\varepsilon$ in time and sample complexity $\\tau+n^{O(k)}/\\mathrm{poly}(\\varepsilon)$ ", "page_idx": 4}, {"type": "text", "text": "The takeaway is that, in order to devise efficient algorithms for agnostic or testable learning, it suffices to study low-degree polynomial approximations of elements of $\\mathcal{F}$ . Under the assumption that $\\mathcal{D}$ is a (standard) Gaussian, one has access to powerful techniques from Fourier analysis to show existence of good polynomial approximators w.r.t. $\\mathcal{D}$ for various concept classes. Using Theorem 6, this leads to efficient agnostic learning algorithms for a variety of concept classes w.r.t. ${\\mathcal{N}}(0,I_{n})$ [20, 25,22]. ", "page_idx": 4}, {"type": "text", "text": "Testable learning via direct approximation. In the testable setting, it is not sufficient to approximate with respect to $\\mathcal{D}$ alone, and so one cannot rely directly on any of its special structure. In [28], the authors overcome this obstacle to get testable learning guarantees for halfspaces w.r.t. $\\textstyle D={\\mathcal{N}}(0,I_{n})$ by appealing to more basic properties of the distributions $\\mathcal{D}^{\\prime}$ accepted by their tester. Their approach is roughly as follows. First, they use standard results from polynomial approximation theory to find a (univariate) polynomial $q$ which approximates the sign-function well on the interval $[-1,1]$ . For a halfspace $f(x)=\\mathrm{sign}(\\langle v,x\\rangle-\\theta)$ , they consider the approximator $h(x)=q(\\langle v,x\\rangle\\,\\bar{-}\\,\\theta)$ , which satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[|h(X)-f(X)|\\right]=\\mathbb{E}_{Y\\sim\\mathcal{D}_{v,\\theta}^{\\prime}}\\left[|q(Y)-\\mathrm{sign}(Y)|\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\mathcal{D}_{v,\\theta}^{\\prime}$ is the (shifted) projection of $\\mathcal{D}^{\\prime}$ onto the line $\\operatorname{span}(v)\\subseteq\\mathbb{R}^{n}$ . That is, $Y=\\langle v,X\\rangle-\\theta$ Then, for carefully chosen $k\\in\\mathbb N$ and $\\eta>0$ , they show that for any $\\mathcal{D}^{\\prime}$ accepted by $\\mathcal{T}_{\\mathrm{AMM}}(k,\\eta)$ , the distribution $\\mathcal{D}_{v,\\theta}^{\\prime}$ satisietacnetrationdat-conentationremanig tially that $\\mathcal{D}_{v,\\theta}^{\\prime}$ is distributed somewhat uniformly on $[-1,1]$ As $q$ approximates the sign-function on $[-1,1]$ , they may conclude that $\\mathbb{E}_{Y\\sim\\mathcal{D}_{v,\\theta}^{\\prime}}\\left[|q(Y)-\\mathrm{sign}(Y)|\\right]$ is small, and invoke Theorem 7. ", "page_idx": 4}, {"type": "text", "text": "Testable learning via fooling.  It is natural to attempt a generalization of the approach above to PTFs. Indeed, for $f(x)\\,=\\,\\mathrm{sign}(p(x))$ , one could consider the approximator $h(x)\\,=\\,q(p(x))$ However, as we show below in Section 2.4, this approach cannot work when $\\deg(p)\\geq6$ . Instead, we will rely on a more indirect technique, proposed in [15]. It connects the well-studied notion of fooling to low-degree polynomial approximation, and to testable learning. ", "page_idx": 4}, {"type": "text", "text": "Definition 8 (Fooling). Let $\\varepsilon>0$ We say a distribution $\\mathcal{D}^{\\prime}$ on $\\mathbb{R}^{n}$ fools $\\mathcal{F}w.r t.\\ \\mathcal{D}$ up to error $\\varepsilon$ $i f,$ for all $f\\in\\mathcal F$ we have $|\\mathbb{E}_{Y\\sim\\mathcal{D}}\\left[f(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right]|\\le\\varepsilon$ ", "page_idx": 4}, {"type": "text", "text": "The main result of [15] shows that fooling implies testable learning when using approximate momentmatching to test the distributional assumptions. It forms the basis of our proof of Theorem 3. ", "page_idx": 4}, {"type": "text", "text": "1. Any distribution $\\mathcal{D}^{\\prime}$ whose moments up to degree $k$ match those of $\\mathcal{D}$ with slack n fools $\\mathcal{F}$ w.r.t. $\\mathcal{D}$ up to error $\\varepsilon/2$   \n2. With high probability over m samples from $\\mathcal{D}$ the empirical distribution matches moments of degree at most $k$ with $\\mathcal{D}$ up to slack $\\eta$ ", "page_idx": 5}, {"type": "text", "text": "Then, using the moment-matching tester $\\mathcal{T}=\\mathcal{T}_{\\mathrm{AMM}}(k,\\eta),$ we can learn $\\mathcal{F}$ testably with respect to $\\mathcal{D}$ up to error $\\varepsilon$ in time and sample complexity $m+n^{O(k)}$ ", "page_idx": 5}, {"type": "text", "text": "Remark 10. When $\\textstyle D={\\mathcal{N}}(0,I_{n})$ is the standard Gaussian, then the second condition in Theorem 9 is satisfied for $m=\\Theta\\big((2k n)^{k}\\cdot\\eta^{-2}\\big)$ , see also Fact 36. ", "page_idx": 5}, {"type": "text", "text": "The primary technical argument in the proof of Theorem 9 in [15] is an equivalence between fooling and a type of low-degree polynomial approximation called sandwiching. Compared to Theorem 7, the advantage of sandwiching is that one needs to approximate $f$ only w.r.t. $\\mathcal{D}$ (rather than any distribution accepted by the tester). However, one needs to find not one, but two low degree approximators $h_{1},h_{2}$ that satisfy $h_{1}\\leq f\\leq h_{2}$ pointwise (i.e., \u00e9sandwich' $f$ ). We refer to [15] for details. ", "page_idx": 5}, {"type": "text", "text": "Fooling PTFs.  In light of Theorem 9 and Remark 10, in order to prove our main result Theorem 3, it suffices to show that distributions $\\mathcal{D}^{\\prime}$ which approximately match the moments of ${\\mathcal{N}}(0,I_{n})$ fool the concept class of PTFs. This is our primary technical contribution (see Proposition 12 below). It can be viewed as a generalization of the following result due to Kane [21]. ", "page_idx": 5}, {"type": "text", "text": "Theorem 11 (Informal version of [21, Theorem 1]). Let $\\mathcal{D}^{\\prime}$ be a $k$ -independent standardGaussian, meaning therestrictionof $\\mathcal{D}^{\\prime}$ toanysubset of $k$ variables has distribution $\\mathcal{N}(0,I_{k})$ . Then, $\\mathcal{D}^{\\prime}$ fools degree-d PTFs w.r.t. ${\\mathcal{N}}(0,I_{n})$ up to error $\\varepsilon>0$ as long as $k=k(d,\\varepsilon)$ is large enough. ", "page_idx": 5}, {"type": "text", "text": "Theorem 11 applies to a class of distributions that is (far) more restrictive than what we need. First, note that $k$ -independent Gaussians match the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ exactly,whereas we must allow $\\mathcal{D}^{\\prime}$ whose moments match only approximately. Second, even if $\\mathcal{D}^{\\prime}$ would match the moments of a Gaussian exactly up to degree $k$ , its $k$ -dimensional marginals need not be Gaussian. In fact, we have no information on its moments of high degree even if they depend on at most $k$ variables. These two distinctions cause substantial technical difficulties in our proof of Proposition 12 below. ", "page_idx": 5}, {"type": "text", "text": "2.3  Overview of the proof of Theorem 3: testably learning PTFs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As we have seen, in order to prove Theorem 3, it suffces to show that approximately momentmatching distributions fool PTFs. We obtain the following. ", "page_idx": 5}, {"type": "text", "text": "Proposition 12. Let $\\varepsilon>0$ Suppose that $\\mathcal{D}^{\\prime}$ approximately matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta_{:}$ where $k\\geq\\Omega_{d}\\big(\\varepsilon^{-4d\\cdot7^{d}}\\big)$ and $\\eta\\le n^{-\\Omega_{d}(k)}k^{-\\Omega_{d}(k)}$ Then, $\\mathcal{D}^{\\prime}$ fools the cass of degree-d PTFs w.r.t. ${\\mathcal{N}}(0,I_{n})$ up to error $\\varepsilon/2$ . That is, for any $f\\in\\mathcal{F}_{\\mathrm{PTF},d}.$ we then have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right]\\right|\\le\\varepsilon/2.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the rest of this section, we outline how to obtain Proposition 12. Full details can be found in Appendix A. Structurally, our proof is similar to the proof of Theorem 11 in [21]: First, in Section 2.3.1, we show fooling for the subclass of PTFs defined by multilinear polynomials. Then, in Section 2.3.2, we extend this result to general PTFs by relating arbitrary polynomials to multilinear polynomials in a larger number of variables. Our primary contribution is thus to show that the construction of [21] (which considers $k$ -independent Gaussians) remains valid for the larger class of distributions that approximately match the moments of a Gaussian. ", "page_idx": 5}, {"type": "text", "text": "2.3.1 Fooling multilinear PTFs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let $f(x)\\,=\\,\\mathrm{sign}(p(x))$ , where $p$ is a multilinear polynomial. Our goal is to establish (1) for $f$ under the assumptions of Proposition 12. We will follow the proof of Kane [21] for Theorem 11, which proceeds as follows. Let $\\mathcal{D}^{\\prime}$ be a $k$ -independent Gaussian. First, Kane constructs a degree- $k$ polynomial approximation $h$ of $f$ , satisfying ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[h(Y)\\right]\\approx\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right],\\quad\\mathrm{and,}}\\\\ &{\\qquad\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[h(X)\\right]\\approx\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since the moments of $\\mathcal{D}^{\\prime}$ are exactly equal to those of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ wehave $\\operatorname{\\mathbb{E}}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[h(Y)\\right]=\\operatorname{\\mathbb{E}}_{X\\sim\\mathcal{D}^{\\prime}}\\left[h(X)\\right]$ We may then conclude the fooling property for $\\mathcal{D}^{\\prime}$ (cf. (1): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}_{Y\\sim N(0,I_{n})}\\left[f(Y)\\right]\\approx\\operatorname{\\mathbb{E}}_{Y\\sim N(0,I_{n})}\\left[h(Y)\\right]=\\operatorname{\\mathbb{E}}_{X\\sim{\\mathcal{D}}^{\\prime}}\\left[h(X)\\right]\\approx\\operatorname{\\mathbb{E}}_{X\\sim{\\mathcal{D}}^{\\prime}}\\left[f(X)\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As we see below, Kane relies on a structure theorem (see Lemma 21) for multilinear polynomials to construct his low-degree approximation $h$ . We wish to extend this proof to our setting, where $\\mathcal{D}^{\\prime}$ merely matches the moments of the standard Gaussian up to degree $k$ and slack $\\eta$ .As we will see, the construction of the polynomial $h$ remains valid (although some care is required in bounding the approximation error). A more serious concern is that, for us, $\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[h(\\bar{Y})\\right]\\neq\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[h(\\bar{X})\\right]$ in general. Our main technical contribution in this section is dealing with the additional error terms that arise from this fact. ", "page_idx": 6}, {"type": "text", "text": "Constructing a low-degree approximation. We now give details on the construction of the lowdegree approximation $h$ that we use in our proof, which is the same as in [21]. The starting point of the construction is a structure theorem for multilinear polynomials $p$ (see Lemma 21 below). It tells us that $f\\,=\\,\\mathrm{sign}(p)$ can be decomposed as $f(x)=F{\\dot{(}}P(x))$ , where $F$ is again a PTF, and $P=(P_{i})$ is a vector of multilinear polynomials of degree $d$ , whose moments $\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P_{i}(Y)^{\\ell}\\right]$ are all at most $O_{d}(\\sqrt{\\ell})^{\\ell}$ . Note that these bounds are much stronger than what we would get from standard arguments (which would only yield $\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P_{i}(Y)^{\\ell}\\right]\\le O_{d}(\\sqrt{\\ell})^{d\\ell})$ . As in [21], we approximate $F$ by a smooth function $\\tilde{F}$ via mollification (see Appendix A.1.1). That is, $\\tilde{F}$ is the convolution $F*\\rho$ of $F$ with a carefully chosen smooth function $\\rho$ . Then, we set $h(x)=T(P(x))$ where $T$ is the Taylor approximation of $\\tilde{F}$ of appropriate degree (see Appendix A.1.2). Intuitively, taking the Taylor expansion yields a good approximation as the (Gaussian) moments of the $P_{i}$ are not too large, yielding (2), (3). ", "page_idx": 6}, {"type": "text", "text": "Error analysis of the approximation. Our goal is now to establish (2), (3) in our setting. Note that, since (2) is only concerned with the Gaussian distribution, there is no difference with [21]. For (3), we have to generalize the proof in [21]. For this, we first bound the probability under $\\mathcal{D}^{\\prime}$ that (at least) one of the $P_{i}$ is large, which we do using Markov's inequality. Then, we need to show a bound on the moments of $P_{i}$ under $\\mathcal{D}^{\\prime}$ (recall that the structure theorem only gives a bound on the Gaussian moments). Using bounds on the coefficients of the $P_{i}$ , we are able to do this under a mild condition on $\\eta$ (see Appendices A.1.2 and A.1.3). ", "page_idx": 6}, {"type": "text", "text": "Controlling the additional error terms. To conclude the argument, we need to show that, for our low-degree approximation $h$ , we have $\\operatorname{\\mathbb{E}}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[h(Y)\\right]\\approx\\operatorname{\\mathbb{E}}_{X\\sim\\mathcal{D}^{\\prime}}\\left[h(X)\\right]$ . Recall that in [21], these expectations were simply equal. The main issue lies in the fact that, in our setting, the moment matching is only approximate; equality would still hold if $\\mathcal{D}^{\\prime}$ matched the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ exactly. Under $\\eta$ -approximate moment matching, we could say that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[h(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[h(X)\\right]|\\leq\\eta\\cdot\\|h\\|_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\|h\\|_{1}$ is the 1-norm of the coefficients of $h$ . However, there is no way to control this norm directly. Instead, we rely on the fact that $h=T\\circ P$ and argue as follows. On the one hand, we show a bound on the coefficients in the Taylor approximation $T$ of $\\tilde{F}$ . On the other hand, we show bounds on all terms of the form $\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P(Y)^{\\alpha}\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P(X)^{\\alpha}\\right]\\right|$ . Combining these bounds yields an estimate on the difference $\\begin{array}{r}{\\big|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[h(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[h(X)\\right]\\big|,}\\end{array}$ which lets us conclude (1). ", "page_idx": 6}, {"type": "text", "text": "Going into more detail, the LHS of (4) is equal to the inner product $|\\langle t,u\\rangle|$ between the vector $t=(t_{\\alpha})$ of coefficients of $T$ and the vector $u=\\left(u_{\\alpha}\\right)$ , where $u_{\\alpha}=\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P(Y)^{\\alpha}\\right]-\\mathbb{E}_{X\\sim\\mathcal{D^{\\prime}}}\\left[P(X)^{\\alpha}\\right]$ This can be viewed as a \u2018change of basis? $x\\rightarrow P(x)$ . Then, (4) can bounded by $\\|u\\|_{\\infty}\\cdot\\|t\\|_{1}$ , where $\\|u\\|_{\\infty}\\,=\\,\\operatorname*{max}_{\\alpha}|u_{\\alpha}|$ . The coefficients $t_{\\alpha}$ of $T$ are related directly to the partial derivatives of $\\tilde{F}$ which in turn depend on the function $\\rho$ used in the mollification. After careful inspection of this function, we can bound $\\|t\\|_{1}\\le k^{O_{d}(k)}$ (see Lemma 27). Finally, for any $|\\alpha|\\leq k$ , it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}_{Y\\sim N(0,I_{n})}\\left[P(Y)^{\\alpha}\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P(X)^{\\alpha}\\right]\\right|\\leq\\eta\\cdot\\operatorname*{sup}_{i}\\left(\\|P_{i}\\|_{1}\\right)^{|\\alpha|}\\leq\\eta\\cdot n^{\\frac{\\left|\\alpha\\right|-d}{2}}\\leq\\eta\\cdot n^{O_{d}(k)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "see Fact 22. Putting things together, we get that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim N(0,I_{n})}\\left[h(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[h(X)\\right]\\right|\\leq\\|u\\|_{\\infty}\\cdot\\|t\\|_{1}\\leq\\eta\\cdot k^{O_{d}(k)}n^{O_{d}(k)}\\ll\\varepsilon/2,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "using the fact that $\\eta\\le n^{-\\Omega_{d}(k)}k^{-\\Omega_{d}(k)}$ and $k\\gg1/\\varepsilon$ for the last inequality. ", "page_idx": 6}, {"type": "text", "text": "2.3.2 Fooling arbitrary PTFs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Now, let $f(x)=\\mathrm{sign}(p(x))$ be an arbitrary PTF. As before, we want to establish (1). Following [21], the idea is to reduce this problem to the multilinear case as follows. Let $Y\\sim{\\mathcal{N}}(0,I_{n})$ and let $X$ be a random variable that matches the moments of $Y$ up to degree $k$ and with slack $\\eta$ . For $N\\in\\mathbb{N}$ to be chosen later, we construct new random variables $\\hat{X}$ and $\\hat{Y}$ , and a multilinear PTF $\\hat{f}=\\mathrm{sign}(\\hat{p})$ , all in $n\\cdot N$ variables, such that $\\hat{Y}\\sim\\mathcal{N}(0,I_{n N})$ \uff0c $\\hat{X}$ matches moments of $\\hat{Y}$ up to degree $k$ with slack $\\hat{\\eta}$ and ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y}\\left[f(Y)\\right]-\\mathbb{E}_{X}\\left[f(X)\\right]\\right|\\approx\\left|\\mathbb{E}_{\\hat{Y}}\\left[\\hat{f}(\\hat{Y})\\right]-\\mathbb{E}_{\\hat{X}}\\left[\\hat{f}(\\hat{X})\\right]\\right|.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Assuming $\\hat{\\eta}$ is not much bigger than $\\eta$ , and the approximation above is sufficiently good, we may then apply the result of Section 2.3.1 to $\\hat{f}$ to conclude (1) for $f$ . Our construction of $\\hat{X},\\hat{Y}$ and $\\hat{f}$ will be the same as in [21]. However, with respect to his proof, we face two difficulties. First, we need to control the slack parameter $\\hat{\\eta}$ in terms $\\eta$ . More seriously, Kane's proof of (5) breaks in our setting: He relies on the fact that $X$ is $k$ -independent Gaussian in his setting to bound high degree moments of $\\hat{X}$ which depend on at most $k$ variables. In our setting, we have no information on such moments at all (even if $X$ matched the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ exactly). ", "page_idx": 7}, {"type": "text", "text": "Construction of $\\hat{X}$ and $\\hat{Y}$ . For $i\\in[n]$ , let $Z^{(i)}$ be an $N$ -dimensional Gaussian random variable with mean O, variances $1-1/N$ and covariances $-1/N$ , independent from $X$ and all other $Z^{(i^{\\prime})}$ We define $\\hat{X}_{i j}:=X_{i}/\\sqrt{N}+Z_{j}^{(i)}$ and set $\\hat{X}=(\\hat{X}_{i j})$ . We define $\\hat{Y}$ analogously. This ensures that $\\hat{Y}\\sim\\mathcal{N}(0,I_{n N})$ . Furthermore, given that $X$ matches the moments of $Y$ with slack $\\eta$ , it turns out that $\\hat{X}$ matches the moments of $\\hat{Y}$ with slack $\\hat{\\eta}=(2k)^{k/2}\\cdot\\eta$ . This follows by direct computation after expanding the moments of $\\hat{X}$ in terms of those of $X$ and of the $Z^{(i)}$ , see Lemma 32. ", "page_idx": 7}, {"type": "text", "text": "Construction of the multilinear PTF. We want to construct a multilinear polynomial $\\hat{p}$ in $n N$ variables so that $p(X)\\;\\approx\\;\\hat{p}(\\hat{X})$ For $\\hat{x}~\\in~\\mathbb{R}^{n N}$ ,write $\\begin{array}{r}{\\varphi({\\hat{x}})\\;:=\\;(\\sum_{j=1}^{N}{\\hat{x}}_{i j}/\\sqrt{N})_{i\\in[n]}\\;\\in\\;\\mathbb{R}^{n}}\\end{array}$ Since $\\varphi(Z^{(i)})\\;=\\;0$ holds deterministically, $\\varphi({\\hat{X}})\\;=\\;X$ .So, if we were to set $\\hat{p}~=~p~\\circ~\\varphi$ it would satisfy $\\hat{p}(\\hat{X})=p(X)$ . However, it would clearly not be multilinear. To fix this, we write $\\textstyle p(\\varphi({\\hat{x}}))={\\dot{\\sum}}_{\\alpha}\\,{\\dot{\\lambda}}_{\\alpha}{\\hat{x}}^{\\alpha}$ and replace each non-multilinear term $\\lambda_{\\alpha}\\hat{x}^{\\alpha}$ by a multilinear one as follows: If the largest entry of $\\alpha$ is at least three, we remove the term completely. If the largest entry of $\\alpha$ is two, we replace the term by $\\lambda_{\\alpha}\\hat{x}^{\\alpha^{\\prime}}$ , where $\\alpha_{i j}^{\\prime}=1$ $\\alpha_{i j}=1$ and O otherwise. This is identical to the construction in [21]. Now, to show that $p(X)\\approx\\hat{p}(\\hat{X})$ we need to bound the effect of these modifications. It turns out that it suffices to control the following expressions in terms of $N$ ", "page_idx": 7}, {"type": "equation", "text": "$$\na_{i}:=\\left|\\sum_{j=1}^{N}\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right|,\\quad b_{i}:=\\left|\\sum_{j=1}^{N}\\left(\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right)^{2}-1\\right|,\\quad c_{i,\\ell}:=\\left|\\sum_{j=1}^{N}\\left(\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right)^{\\ell}\\right|\\quad(i\\in[n],\\,3\\leq\\ell\\leq d).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For the $b_{i}$ and $c_{i,\\ell}$ , we can do so using a slight modification of the arguments in [21]. For the $a_{i}$ \uff0c however, Kane [21] exploits the fact that in his setting, $X_{i}$ is standard Gaussian for each fixed $i\\in[n]$ meaning the $\\hat{X}_{i,j}$ are jointly standard Gaussian over $j$ . This gives him access to strong concentration bounds. To get such concentration bounds in our setting, we would need information on the moments of the $X_{i}$ up to degree roughly $\\log n$ . However, we only have access to moments up to degree $k$ which is not allowed to depend on $n$ (as our tester uses time $n^{\\Omega(k)}$ ). Instead, we use significantly weaker concentration bounds based on moments of constant degree. By imposing stronger conditions on the $b_{i}$ \uff0c $c_{i,\\ell}$ , we are able to show that the remainder of the argument in [21] still goes through in our setting, see Appendix A.2.2. Finally, for $N$ sufficiently large, this allows us to conclude (5) for $\\hat{f}=\\mathrm{sign}(\\hat{p})$ ", "page_idx": 7}, {"type": "text", "text": "2.4  Impossibility result: learning PTFs via the push-forward ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we show that the approach of [28] to prove testable learning guarantees for halfspaces w.r.t. the standard Gaussian cannot be generalized to PTFs. Namely, we show that in general, PTFs $f(x)=\\mathrm{sign}(p(x))$ With $\\deg(p)\\geq3$ cannot be approximated up to arbitrary error w.r.t. ${\\mathcal{N}}(0,I_{n})$ by a polynomial of the form $h(x)=q(p(x))$ , regardless of the degree of $q$ .2Importantly,we show that this is the case even if one makes certain typical structural assumptions on $p$ which only change the PTF $f\\,=\\,\\mathrm{sign}(p)$ on a set of negligible Gaussian volume; namely that $p$ is square-free and that $\\{p\\geq0\\}\\subseteq\\mathbb{R}^{n}$ is compact. Our main technical contribution is an extension of a well-known inapproximability result due to Bun & Steinke (Theorem 15 below) to distributions \u2018with a single heavy tail' (see Theorem 18). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Approximating the sign-function on the real line  Let $p_{\\#}\\mathcal{N}(0,I_{n})$ be the push-forward of the standard Gaussian distribution by $p$ , which is defined by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{P}_{Y\\sim p_{\\#}N(0,I_{n})}\\left[Y\\in A\\right]:=\\mathbb{P}_{X\\sim{\\mathcal{N}}(0,I_{n})}\\left[X\\in p^{-1}(A)\\right]\\quad(A\\subseteq\\mathbb{R}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that, if $h(x)=q(p(x))$ , we then have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}_{X\\sim\\mathcal{N}(0,I_{n})}\\left[\\left|h(X)-f(X)\\right|\\right]=\\operatorname{\\mathbb{E}}_{Y\\sim p_{\\#},N(0,I_{n})}\\left[\\left|q(Y)-\\operatorname{sign}(Y)\\right|\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Finding a good approximator $h\\approx f$ of the form $h=q\\circ p$ is thus equivalent to finding a (univariate) polynomial $q$ which approximates the sign-function on $\\mathbb{R}$ well under the push-forward distribution $p_{\\#}\\mathcal{N}(0,I_{n})$ . In light of this observation, we are interested in the following question: Let $\\mathcal{D}$ be a distribution on the real line. Is it possible to find for each $\\varepsilon\\ >\\ 0$ a polynomial $q$ such that $\\mathbb{E}_{Y\\sim\\mathcal{D}}\\left[|q(Y)-\\mathrm{sign}(Y)|\\right]\\leq\\varepsilon^{?}$ This question is well-understood for distributions $\\mathcal{D}$ whose density is of the form $w_{\\gamma}(x)\\,:=\\,C_{\\gamma}\\exp(-|x|^{\\gamma}),\\,\\gamma\\,>\\,0$ . Namely, when $\\gamma\\geq1$ , these distributions are log-concave, and the question can be answered in the affirmative. On the other hand, when $\\gamma<1$ they are log-superlinear, and polynomial approximation of the sign function is not possible. ", "page_idx": 8}, {"type": "text", "text": "Theorem 13 (see, e.g. [20]). Let $\\mathcal{D}$ be a log-concave distribution on $\\mathbb{R}$ Then, for any $\\varepsilon>0$ there exists a polynomial $q$ such that $\\mathbb{E}_{Y\\sim{\\mathcal{D}}}\\left[|q(Y)-\\mathrm{sign}(Y)|\\right]\\leq\\varepsilon$ ", "page_idx": 8}, {"type": "text", "text": "Definition 14. Let $\\mathcal{D}$ be a distribution on $\\mathbb{R}$ whose density function $w$ satisfies ", "page_idx": 8}, {"type": "equation", "text": "$$\nw(x)\\geq C\\cdot w_{\\gamma}(x)\\quad\\forall\\,x\\in\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for some $\\gamma<1$ and $C>0$ Then we say $\\mathcal{D}$ is log-superlinear (LSL) ", "page_idx": 8}, {"type": "text", "text": "Theorem 15 (Bun-Steinke [6]). Let $\\mathcal{D}$ be an LSL-distribution on $\\mathbb{R}$ Then there exists an $\\varepsilon>0$ such that, for any polynomial $q$ we have $\\mathbb{E}_{Y\\sim{\\mathcal{D}}}\\left[|q(Y)-\\mathrm{sign}(Y)|\\right]>\\varepsilon$ ", "page_idx": 8}, {"type": "text", "text": "When $p$ is of degree 1 (i.e., when $f=\\mathrm{sign}(p)$ defines a halfspace), the push-forward distribution $\\bar{D}=\\bar{p_{\\#}}\\bar{\\mathcal{N}}(0,\\bar{I_{n}\\big)}$ defined in (6) is itself a (shifted) Gaussian. In particular, it is log-concave and by Theorem 13 approximation of the sign-function w.r.t. $\\mathcal{D}$ is possible. On the other hand, when $p$ is of higher degree, $\\mathcal{D}$ could be an LSL-distribution, meaning approximation of the sign-function w.r.t. $\\mathcal{D}$ is not possible by Theorem 15. For instance, consider $p(\\bar{x})=x^{3}$ . The density $w$ of $p_{\\#}\\mathcal{N}(0,1)$ is given by $w(x)=C\\cdot|x|^{-2/3}\\cdot\\exp\\bigl(-|x|^{2/3}\\bigr)$ , and so $p_{\\#}\\mathcal{N}(0,1)$ is log-superlinear. ", "page_idx": 8}, {"type": "text", "text": "Choice of description. The example $p(x)=x^{3}$ is artificial: We have $\\mathrm{sign}(p(x))=\\mathrm{sign}(x)$ , and so the issue is not with the concept $f=\\mathrm{sign}(p)$ , but rather with our choice of description $p$ . In general, one can (and should) assume that $p$ is square-fre, meaning it is not of the form $p=p_{1}^{2}\\cdot p_{2}$ Indeed, note that for such a polynomial, we have $\\mathrm{sign}(p(x))\\,=\\,\\mathrm{\\bar{sign}}(p_{2}(x))$ almost everywhere. Square-freeness plays an important role in the analysis of learning algorithms for PTFs, see, e.g., [22, Appendix A]. It turns out that even if $p$ is square-free, the distribution $p_{\\#}\\mathcal{N}(0,I_{n})$ can still be log-superlinear, e.g., when $p(x)=x(x-1)(x-2)$ . Note that, for this example, $\\mathrm{sign}(p)$ describes a non-compact subset of $\\mathbb{R}$ . This is crucial to find that $p_{\\#}\\mathcal{N}(0,1)$ is LSL. Indeed, if $\\{\\bar{p}\\geq0\\}\\subseteq\\mathbb{R}$ were compact, then $p_{\\mathrm{max}}=\\operatorname*{sup}_{x\\in\\mathbb{R}}p(x)<\\infty$ and so the density $w$ of $p_{\\#}\\mathcal{N}(0,1)$ would satisfy $w(x)=0$ for all $x>p_{\\mathrm{max}}$ . In particular, $w$ would not be log-superlinear. One could therefore hope that assuming $\\{p\\ge0\\}$ is compact might fix our issues. This assumption is reasonable as $\\{p\\ge\\bar{0}\\}$ can be approximated arbitrarily well by a compact set (in terms of Gaussian volume). On the contrary, we show the following. ", "page_idx": 8}, {"type": "text", "text": "Theorem 16. There exists a square-free polynomial $p_{i}$ sothat $\\{p\\geq0\\}\\subseteq\\mathbb{R}$ is compact, but for which there exists $\\varepsilon>0$ so that, for any polynomial $q,$ $,\\,\\mathbb{E}_{X\\sim\\mathcal{N}(0,1)}\\,\\mathrm{\\bar{[}}|q(p(\\dot{X}))-\\mathrm{sign}(p(X))|\\mathrm{]}>\\varepsilon$ ", "page_idx": 8}, {"type": "text", "text": "To establish Theorem 16, we prove a ^one-sided\u2019 analog of Theorem 15 in Appendix B.1, which we believe to be of independent interest. It shows impossibility of approximating the sign-function under a class of distributions related to, but distinct from, those considered by Bun & Steinke [6]. The key difference is that the densities in our result need only to have a single heavy tail. However, this tail must be\u2018twice as heavy' $(\\gamma<1/2$ Vs. $\\gamma<1$ ). We emphasize that [6] does not cover compact PTFs. ", "page_idx": 8}, {"type": "text", "text": "Definition 17. Let $\\mathcal{D}$ be a distribution on $\\mathbb{R}$ whose density function $w$ satisfies ", "page_idx": 9}, {"type": "equation", "text": "$$\nw(x)\\geq C\\cdot w_{\\gamma}(x)\\quad\\forall\\,x\\in(-\\infty,1]\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "for some $\\gamma<1/2$ and $C>0$ Then we say $\\mathcal{D}$ is one-sided log-superlinear. ", "page_idx": 9}, {"type": "text", "text": "Theorem 18. Let $\\mathcal{D}$ be a one-sided LSL-distribution on $\\mathbb{R}$ Then there exists an $\\varepsilon>0$ such that, for any polynomial $q_{\\mathrm{r}}$ we have $\\mathbb{E}_{Y\\sim{\\mathcal{D}}}\\left[|q(Y)-\\mathrm{sign}(Y)|\\right]>\\varepsilon$ ", "page_idx": 9}, {"type": "text", "text": "Proof ofTheorem $I6.$ It suffices to find a square-free polynomial $p$ for which $\\{p\\ge0\\}$ is compact, and the push-forward distribution $p_{\\#}\\mathcal{N}(0,\\bar{1})$ is one-sided LSL. A direct computation shows that ", "page_idx": 9}, {"type": "equation", "text": "$$\np(x):=-x(x-1)(x-2)(x-3)(x-4)(x-5)\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "meets the criteria, see Appendix B.2 for details ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers for their valuable comments and suggestions. We thank Arsen Vasilyan for helpful discussions. This work is supported by funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 815464). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Pranjal Awasthi, Maria Florina Balcan, and Philip M. Long. \u201c\"The power of localization for efficiently learning linear separators with noise\". In: J. ACM 63.6 (2017), Art. 50, 27. 1SSN: 0004-5411,1557-735X.   \n[2]  Louay M. J. Bazzi. \u201cPolylogarithmic independence can fool DNF formulas\". In: SIAM J. Comput. 38.6 (2009), Pp. 2220-2272. 1SSN: 0097-5397,1095-7111.   \n[3] Richard Beigel. \u201c\"The polynomial method in circuit complexity\". In: Proceedings of the Eighth Annual Structure in Complexity Theory Conference (San Diego, CA, 1993). IEEE Comput. Soc. Press, Los Alamitos, CA, 1993, pp. 82-95. ISBN: 0-8186-4070-7.   \n[4]   Mark Braverman. \u201c'Polylogarithmic independence fools $\\mathrm{AC^{0}}$ circuits\". In: J. ACM 57.5 (2008). ISSN: 0004-5411.   \n[5]  Jehoshua Bruck and Roman Smolensky. \u201c'Polynomial threshold functions, $\\mathrm{AC^{0}}$ functions, and spectral norms\". In: SIAM J. Comput. 21.1 (1992), Pp. 33-42. 1SSN: 0097-5397.   \n[6] Mark Bun and Thomas Steinke. \u201c\"Weighted polynomial approximations: limits for learning and pseudorandomness\". In: Approximation, randomization, and combinatorial optimization. Algorithms and techniques. Vol. 40. LIPIcs. Leibniz Int. Proc. Inform. Schloss Dagstuhl. Leibniz-Zent. Inform., Wadern, 2015, pPp. 625-644. 1SBN: 978-3-939897-89-7.   \n[7] Anthony Carbery and James Wright. \u201cDistributional and $L^{q}$ norm inequalities for polynomials over convex bodies in $\\mathbb{R}^{n}$ '. In: Math. Res. Lett. 8.3 (2001), Pp. 233-248. 1SsN: 1073-2780.   \n[8]Amit Daniely. \u201c\"Complexity theoretic limitations on learning halfspaces\". In: STOC'16\u2014 Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing. ACM, New York, 2016, Pp. 105-117. 1SBN: 978-1-4503-4132-5.   \n[9]Mias Diakonikolas, Parikshit Gopalan, Ragesh Jaiswal, Rocco A. Servedio, and Emanuele Viola. \u201cBounded independence fools halfspaces\". In: SIAM J. Comput. 39.8 (2010), pp. 3441- 3462. ISSN: 0097-5397,1095-7111.   \n[10]Hias Diakonikolas, Daniel M. Kane, Vasilis Kontonis, Sihan Liu, and Nikos Zarifis. \u201cEfficient testable learning of halfspaces with adversarial label noise\". In: Advances in Neural Information Processing Systems. Vol. 36. Curran Associates, Inc., 2023, pp. 39470-39490.   \n[11]  Iias Diakonikolas, Daniel M. Kane, and Jelani Nelson. \u201cBounded independence fools degree-2 threshold functions\". In: 2010 IEEE 51st Annual Symposium on Foundations of Computer Science\u2014FOCS 2010. IEEE Computer Soc., Los Alamitos, CA, 2010, pp. 11-20. ISBN: 978-0-7695-4244-7.   \n[12]   Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, and Nikos Zarifis. \u201c\"The Optimality of Polynomial Regression for Agnostic Learning under Gaussian Marginals in the SQ Model'. In: Procedings of Thirty Fourth Conference on Learning Theory. Vol. 134. PMLR, 2021, pp. 1552-1584.   \n[13]  lias Diakonikolas, Daniel M. Kane, and Lisheng Ren. \u201cNear-Optimal Cryptographic Hardness of Agnostically Learning Halfspaces and ReLU Regression under Gaussian Marginals\". In: Proceedingsof the 40th International Conference on Machine Learning. Vol. 202. PMLR, 2023, pp. 7922-7938.   \n[14]  Mlias Diakonikolas, Ryan O'Donnell, Rocco A. Servedio, and Yi Wu. \u201cHardness results for agnostically learning low-degree polynomial threshold functions\". In: Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms. SIAM, Philadelphia, PA, 2011, pp. 1590-1606.   \n[15]   Aravind Gollakota, Adam R. Klivans, and Pravesh K. Kothari. \u201cA moment-matching approach to testable learning and a new characterization of Rademacher complexity\". In: STOC'23- Proceedings of the 55th Annual ACM Symposium on Theory of Computing. ACM, New York, 2023, Pp. 1657-1670. ISBN: 978-1-4503-9913-5.   \n[16]   Aravind Gollakota, Adam R. Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. \u201cAn Eficient Tester-Learner for Halfspaces\". In: The Twelfth International Conference on Learning Representations. 2024.   \n[17]   Aravind Gollakota, Adam R. Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. \u201cTesterlearners for halfpaces: Universal algorithms\" In: Advances in Neural Information Processing Systems. Vol. 36. Curran Associates, Inc., 2023, pp. 10145-10169.   \n[18]  Prahladh Harsha, Adam R. Klivans, and Raghu Meka. \u201cBounding the sensitivity of polynomial threshold functions\". In: Theory Comput. 10 (2014), Pp. 1-26. 1sSN: 1557-2862.   \n[19]  David Haussler. \u201cDecision-theoretic generalizations of the PAC model for neural net and other leaning applications\". In: Inform. and Comput. 100.1 (1992), pp. 78-150. 1ssN: 0890- 5401,1090-2651.   \n[20]  Adam Tauman Kalai, Adam R. Klivans, Yishay Mansour, and Rocco A. Servedio. \u201cAgnostically learning halfspaces\". In: SIAM J. Comput. 37.6 (2008), pp. 1777-1805. IsSSN: 0097-5397,1095-7111.   \n[21]  Daniel M. Kane. \u201c $^{k}$ -independent Gaussians fool polynomial threshold functions\". In: 26th Annual IEEE Conference on Computational Complexity. IEEE Computer Soc., Los Alamitos, CA, 2011, Pp. 252-261. ISBN: 978-0-7695-4411-3.   \n[22]  Daniel M. Kane. \u201c\\*The Gaussian surface area and noise sensitivity of degree- $D$ polynomial threshold functions\". In: Comput. Complexity 20.2 (2011), Pp. 389-412. IssN: 1016-3328,1420- 8954.   \n[23]  Daniel M. Kane, Adam R. Klivans, and Raghu Meka. \u201cLearning Halfspaces Under LogConcave Densities: Polynomial Approximations and Moment Matching\". In: Proceedings of the 26th Annual Conference on Learning Theory. Vol. 30. PMLR, 2013, pp. 522-545.   \n[24]  Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. \u201cToward efficient agnostic learning\". In: Proceedings of the Fifth Annual Workshop on Computational Learning Theory. Association for Computing Machinery, 1992, pp. 341-352. 1SBN: 089791497X.   \n[25]  Adam R. Klivans, Ryan O'Donnell, and Rocco A.Servedio. \u201cLearning Geometric Concepts via Gaussian Surface Area\". In: 2008 49th Annual IEEE Symposium on Foundations of Computer Science. 2008, pp. 541-550.   \n[26]  Matthias Krause and Pavel Pudlak. \u201cComputing Boolean functions by polynomials and threshold circuits\". In: Comput. Complexity 7.4 (1998), pp. 346-370. 1sSN: 1016-3328,1420-8954.   \n[27]   Ryan O'Donnell and Rocco A. Servedio. \u201cExtremal properties of polynomial threshold functions\". In: J. Comput. System Sci. 74.3 (2008), Pp. 298-312. ISSN: 0022-0000,1090-2724.   \n[28]  Ronitt Rubinfeld and Arsen Vasilyan. \u201cTesting distributional assumptions of learning algorithms\". In: STOC'23-Proceedings of the 55th Annual ACM Symposium on Theory of Computing. ACM, New York, 2023, pp. 1643-1656. ISBN: 978-1-4503-9913-5.   \n[29]   Stefan Tiegel. \u201cHardness of Agnostically Learning Halfspaces from Worst-Case Lattice Problems\". In: Proceedings of Thirty Sixth Conference on Learning Theory. Vol. 195. PMLR, 2023, Pp.3029-3064.   \n[30]  Leslie G. Valiant. \u201cA theory of the learnable\". In: Commun. ACM 27 (1984), pp. 1134-1142. ISSN: 0001-0782. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A  Testable learning of polynomial threshold functions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we give a formal proof of our main result, Theorem 3, that we restate here. ", "page_idx": 12}, {"type": "text", "text": "Theorem 19 (Formal version of Theorem 3). Let $d\\in\\mathbb{N}.$ For any $\\varepsilon>0$ the concept class of degree- $d$ polynomial threshold functions in n variables can be testably learned up to error $\\varepsilon$ w.r.t. the standard Gaussian ${\\mathcal{N}}(0,I_{n})$ in time and sample complexity ", "page_idx": 12}, {"type": "equation", "text": "$$\nn^{O_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right)}\\varepsilon^{-O_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right)},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In particular, if d is constant, the time and sample complexity is $n^{\\mathrm{poly}(1/\\varepsilon)}$ ", "page_idx": 12}, {"type": "text", "text": "Our goal to show this result is to apply Theorem 9. We first focus on proving the fooling condition in this theorem. Recall that for this we need to show that there are $k$ and $\\eta$ such that if $\\mathcal{D}^{\\prime}$ matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta$ ,thenwehave ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right]-\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]\\right|\\le\\varepsilon/2.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In order to show this, we follow the result of [21] (see Theorem 11). This paper shows this condition for any distribution $\\mathcal{D}^{\\prime}$ that is a $k$ -independent Gaussian, i.e. for which the marginal of every subset of $k$ coordinates is $\\mathcal{N}(0,I_{k})$ ", "page_idx": 12}, {"type": "text", "text": "The reason why it is enough to only focus on satisfying the fooling condition is that for any $\\eta$ wecan find $m$ large enough that the first condition of Theorem 9 is satisfied for ${\\mathcal{N}}(0,I_{n})$ (see Remark 10 and Fact 36). For $k$ we choose the same value as in [21], namely ", "page_idx": 12}, {"type": "equation", "text": "$$\nk=\\Theta_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Wefirst show how to choose $\\eta$ for the case of multilinear polynomial polynomials in Appendix A.1. In Appendix A.2, we generalize the fooling result to arbitrary PTFs. Finally, in Appendix A.3, we show how to apply Theorem 9 to get testable learning for PTFs. ", "page_idx": 12}, {"type": "text", "text": "A.1 Fooling for multlinear PTFs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Thus, let $f\\in\\mathcal{C}$ and let $p$ be the multilinear polynomial (of degree $d$ )such that $f(x)=\\mathrm{sign}(p(x))$ Note that this notation is different than the one used in [21]. There, the roles of $f$ and $F$ are interchanged.We use $f$ for the PTF throughout to be consistent with the previous work on (testable) learning. Without loss of generality, we assume that the sum of the squares of the coefficients of $p$ is 1. We can make this assumption since rescaling does not change the PTF. ", "page_idx": 12}, {"type": "text", "text": "The main result of this section is the following proposition. ", "page_idx": 12}, {"type": "text", "text": "Proposition 20 (Fooling for multilinear PTFs). Let $\\varepsilon>0$ Supposethat $\\mathcal{D}^{\\prime}$ approximately matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta_{-}$ where $k\\geq\\bar{\\Omega_{d}}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right).$ and $\\eta\\le n^{-\\Omega_{d}(k)}k^{-\\Omega_{d}(k)}$ Then, we have that, for any multilinear $f\\in\\mathcal{F}_{\\mathrm{PTF},d},$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right]-\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]\\right|\\le O_{d}(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that we only show $O_{d}(\\varepsilon)$ here instead of $\\varepsilon/2$ , which is needed to apply Theorem 9. This simplifies the notation in our proof of this proposition. We later apply this proposition to $\\varepsilon^{\\prime}=\\varepsilon/\\Omega_{d}(1)$ to conclude the fooling result we need. ", "page_idx": 12}, {"type": "text", "text": "As mentioned earlier, the general strategy to do this is based on [21]. We want to find a function $\\tilde{f}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ that approximates $f$ We will define this function in Appendix A.1.1. In Appendix A.1.2, we show that the expectation of $\\tilde{f}$ is close under $\\mathcal{D}^{\\prime}$ and ${\\mathcal{N}}(0,I_{n})$ . More precisely, in Lemma 25, we show that under the above assumption on $\\eta$ , we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[\\tilde{f}(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\tilde{f}(X)\\right]\\right|\\le O(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In Appendix A.1.3, we then show that under both $\\mathcal{D}^{\\prime}$ and ${\\mathcal{N}}(0,I_{n})$ the expectation of $f$ and $\\tilde{f}$ are close and complete the proof of Proposition 20. More precisely, from [21, Proposition 14] (restated in Lemma 28), we get that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[\\tilde{f}(Y)\\right]\\right|\\le O(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In Appendix A.1.3, we then also show that this also holds for $X\\sim\\mathcal{D}^{\\prime}$ instead of $Y\\sim{\\mathcal{N}}(0,I_{n})$ More precisely, we show in Lemma 29 that Lemmas 25 and 28 contain already enough information about the moment-matching distribution $\\mathcal{D}^{\\prime}$ to conclude Proposition 20. ", "page_idx": 12}, {"type": "text", "text": "A.1.1 Set up and definition of the function $\\tilde{f}$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we want to define the function $\\tilde{f}$ that should be thought of as a smooth approximation of thePTF $f$ . In order to define this function, we first restate the following structural theorem from [21]. ", "page_idx": 13}, {"type": "text", "text": "Lemma 21 ([21, Proposition 4]). Let $m_{1}\\leq m_{2}\\leq\\cdot\\cdot\\leq m_{d}$ be integers. Then there exists integers $n_{1},n_{2},\\ldots,n_{d},$ where $n_{i}\\;\\leq\\;O_{d}(m_{1}m_{2}\\ldots m_{i-1})$ and (non-constant homogeneous multilinear) polynomials $h_{1},\\ldots,h_{d},P_{i,j}$ $(1\\leq i\\leq d,1\\leq j\\leq n_{i})$ suchthat: ", "page_idx": 13}, {"type": "equation", "text": "$$\n^3\\!\\cdot\\,\\,p(Y)=\\sum_{i=1}^{d}h_{i}(P_{i,1}(Y),P_{i,2}(Y),\\dots,P_{i,n_{i}}(Y)).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The values of $m_{i}$ we want to choose are as in [21], ie. we let $m_{i}\\,=\\,\\Theta_{d}\\left(\\varepsilon^{-3\\cdot7^{i}d}\\right)$ . Given this structure theorem, we introduce now the following notation that we use throughout the remainder of this section, analogous to [21]. As before, we use $p\\,:\\,\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}$ to be denote the multilinear polynomial and $f:\\mathbb{R}^{n}\\,\\rightarrow\\,[-1,1]$ to be the PTF we are interested in, i.e. $f(x)\\,=\\,\\mathrm{sign}(p(x))$ Furthermore, the $P_{i,j}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ for $\\textit{i}\\in\\ [n]$ and $j~\\in~[n_{i}]$ are the polynomials_ in the structure theorem. We denote by $P_{i}:\\mathbb{R}^{n}\\to\\mathbb{R}^{n_{i}}$ for $\\textit{i}\\in\\ [n]$ the vector $(P_{i,1},\\dotsc,P_{i,n_{i}})$ and by $P:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n_{1}}\\times\\cdot\\cdot\\cdot\\times\\mathbb{R}^{n_{d}}$ the vector $(P_{1},\\hdots,P_{d})$ . Finally, we define $F:\\mathbb{R}^{n_{1}}\\times\\cdots\\times\\mathbb{R}^{n_{d}}\\rightarrow\\mathbb{R}$ as the function $\\begin{array}{r}{F(y_{1},\\ldots,y_{d})=\\sum_{i=1}^{d}h_{i}(y_{i})}\\end{array}$ , i.e. we get $f(x)=F(P(x))$ ", "page_idx": 13}, {"type": "text", "text": "Similar to Condition 1 in the above lemma, we can also get a bound on the sum of the absolute values of the coefficients of the $P_{i,j}$ . We need this later in the proof of Lemma 25. ", "page_idx": 13}, {"type": "text", "text": "Fact 22. For any $i\\in[n]$ and $j\\in[n_{i}]$ the sumof the absolute values of the coeffcients of $P_{i,j}$ is at most nd/2. ", "page_idx": 13}, {"type": "text", "text": "The idea to prove this is to bound the number of coefficients we have and use an inequality between the $1-$ and 2-norm. The detailed argument can be found in Appendix C.1. Furthermore, we have an analogous result to Item 2 for the moment-matching distributions, which is again proved in Appendix C.1. We need this result later for concluding that $\\tilde{f}$ is a good approximation under the moment matching distribution $\\mathcal{D}^{\\prime}$ ", "page_idx": 13}, {"type": "text", "text": "Fact 23. For any $i\\in[d],\\,j\\in[n_{i}]$ and $\\ell\\leq k/d$ we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{\\mathbb{E}}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P_{i,j}(X)^{\\ell}\\right]\\leq\\operatorname{\\mathbb{E}}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P_{i,j}(Y)^{\\ell}\\right]+\\eta n^{d\\ell/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As in [21], we now consider the function $\\rho_{C}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ defined in the following lemma. ", "page_idx": 13}, {"type": "text", "text": "Lemma 24 ([21, Lemma 5]). Let ", "page_idx": 13}, {"type": "equation", "text": "$$\nB(\\xi)=\\left\\{\\underset{0}{1-\\|\\xi\\|_{2}^{2}}\\right.\\;\\;\\dot{i}f\\,\\|\\xi\\|_{2}\\leq1\\;\\;\\;\\;\\;a n d\\;\\;\\;\\;\\rho_{2}(x)=\\frac{|\\hat{B}(x)|^{2}}{\\|B\\|_{L^{2}}^{2}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where where $\\hat{B}$ is the Fourier transform of $B$ Then, the function $\\rho{_C}$ defined by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\rho_{C}(x)=\\left(\\frac{C}{2}\\right)^{n}\\rho_{2}\\left(\\frac{C x}{2}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "satisfies the following conditions ", "page_idx": 13}, {"type": "text", "text": "1. $\\rho_{C}\\geq0$   \n2. $\\begin{array}{r}{\\int_{\\mathbb{R}^{n}}\\rho(x)\\,\\mathrm{d}x=1,}\\end{array}$   \n3. for any unit vector $v$ and any non-negative integer $\\ell$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{n}}|D_{v}^{\\ell}\\rho(C)(x)|\\,\\mathrm{d}x\\leq C^{\\ell},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We now define the following three functions $\\rho,\\tilde{F}$ and in particular $\\tilde{f}$ , in the same way as in [21]. We let $\\rho:\\mathbb{R}^{n_{1}}\\times\\cdot\\cdot\\cdot\\times\\mathbb{R}^{n_{d}}\\rightarrow\\mathbb{R}$ be defined as $\\rho(y_{1},\\ldots,y_{d})=\\rho_{C_{1}}(y_{1})\\cdot\\cdot\\cdot\\cdot\\cdot\\rho_{C_{d}}(y_{d})$ , where the $C_{i}$ are the same as in [1],ie. welet $C_{i}=\\Theta_{d}\\left(\\varepsilon^{-7^{i}d}\\right)$ . Using this function, we define an approximation $\\tilde{F}:\\mathbb{R}^{n_{1}}\\times\\cdots\\times\\mathbb{R}^{n_{d}}\\rightarrow\\mathbb{R}$ to $F$ as the convolution ${\\tilde{F}}=F*\\rho$ and an approximation $\\tilde{f}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ to $f$ as $\\tilde{f}(x)=\\tilde{F}(P(x))$ ", "page_idx": 14}, {"type": "text", "text": "The general strategy to prove Proposition 20 is show the following three steps, where as usual $Y\\sim\\bar{\\mathcal{N}}(0,I_{n})$ and $X\\sim\\mathcal{D}^{\\prime}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Y}\\left[f(Y)\\right]\\overset{\\mathrm{Lem.}\\,28}{\\approx}\\mathbb{E}_{Y}\\left[\\tilde{f}(Y)\\right]\\overset{\\mathrm{Lem.}\\,25}{\\approx}\\mathbb{E}_{X}\\left[\\tilde{f}(X)\\right]\\overset{\\mathrm{Lem.}\\,29}{\\approx}\\mathbb{E}_{X}\\left[f(X)\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.1.2Expectations of $\\tilde{f}$ under the Gaussian and the moment-matching distribution are close ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we want to prove the middle approximation of (8). More precisely, we show the following lemma. ", "page_idx": 14}, {"type": "text", "text": "Lemma 25. Let $\\varepsilon>0$ \uff1aSuppose that $\\mathcal{D}^{\\prime}$ approximately matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta_{:}$ where $k\\geq\\Omega_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right)$ , and $\\eta\\le n^{-\\Omega_{d}(k)}k^{-\\Omega_{d}(k)}$ Then we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[\\tilde{f}(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\tilde{f}(X)\\right]\\right|\\le O(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We want to do this similar to [21, Section 6]. For this, let $T$ be the Taylor approximation of $\\tilde{F}$ around O. We use degree $m_{i}$ for the $i$ th batch of coordinates (recall that $\\tilde{F}:\\mathbb{R}^{n_{1}}\\overset{\\sim}{\\times}\\cdots\\times\\mathbb{R}^{n_{d}}\\rightarrow\\mathbb{R})$ The strategy to show Lemma 25 is to proceed in the following three steps, where again $Y\\sim{\\mathcal{N}}(0,I_{n})$ and $X\\sim\\mathcal{D}^{\\prime}$ \uff0c ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Y}\\left[\\tilde{f}(Y)\\right]\\overset{[21]}{\\approx}\\mathbb{E}_{Y}\\left[T(P(Y))\\right]\\overset{\\mathrm{Pf.~of.}\\mathrm{Lem.~}25}{\\approx}\\mathbb{E}_{X}\\left[T(P(X))\\right]\\overset{\\mathrm{Lem.~}26}{\\approx}\\mathbb{E}_{X}\\left[\\tilde{f}(X)\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The first approximation above only involves the Gaussian $Y$ , so we get directly from [21, Proof of Proposition 8] that $\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[|\\tilde{f}(Y)-T(P(Y))|\\right]\\le O(\\varepsilon)$ We now want to extend this also to moment-matching distribution, which we do in the following lemma, i.e. show the third approximation in (9). ", "page_idx": 14}, {"type": "text", "text": "Lemma 26. Let $\\varepsilon>0$ Suppose that $\\mathcal{D}^{\\prime}$ approximately matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta,$ where $k\\geq\\Omega_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right)$ and $\\begin{array}{r}{\\eta\\leq\\frac{1}{k d}}\\end{array}$ Then, we havethat ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[|\\tilde{f}(X)-T(P(X))|\\right]\\le O(\\varepsilon)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This proof follows closely [21, Proof of Proposition 8], which is why we defer the proof to Appendix C.2. ", "page_idx": 14}, {"type": "text", "text": "Note that the condition on $\\eta$ in this lemma is also satisfied for the $\\eta$ from Lemma 25. Thus, it remains to prove ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[T(P(Y))\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[T(P(X))\\right]\\right|\\leq O(\\varepsilon)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "to complete the proof of Lemma 25. ", "page_idx": 14}, {"type": "text", "text": "Note that in contrast to [21], this quantity is not O in our case since we only have approximate moment matching. This is the main technical difficulty in our proof for the multilinear case. We need to give a different argument here and argue that this is small even if $X$ is only approximately moment matching a Gaussian and not a $k$ -independent Gaussian. We can write the Taylor expansion as follows ", "page_idx": 14}, {"type": "equation", "text": "$$\nT(x)=\\sum_{\\alpha=(\\alpha_{1},\\ldots,\\alpha_{d}):\\;|\\alpha_{i}|\\leq m_{i}}{\\frac{1}{\\alpha!}}\\partial^{\\alpha}{\\tilde{F}}(0)x^{\\alpha},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "wherethe $\\alpha_{i}$ are multi-indices in $\\mathbb{N}^{n_{i}}$ . We want to prove the following lemma about the coefficients in the Taylor expansion. ", "page_idx": 14}, {"type": "text", "text": "Lemma 27. For any multi-index $\\alpha=(\\alpha_{1},\\ldots,\\alpha_{d})\\in\\mathbb{N}^{n_{1}}\\times\\cdots\\times\\mathbb{N}^{n_{d}}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial^{\\alpha}\\tilde{F}(0)\\leq\\prod_{i=1}^{d}C_{i}^{|\\alpha_{i}|}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The proof of this lemma is in Appendix C.2. The ideas of this proof are based on [21, Proof of Lemmas 5 and 7]. We can now prove Lemma 25. ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 25. As argued above, we already know that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[|\\tilde{f}(Y)-T(P(Y))|\\right]\\leq O(\\varepsilon)\\quad\\mathrm{~and~}\\quad\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[|\\tilde{f}(X)-T(P(X))|\\right]\\leq O(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It thus remains to argue that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[T(P(Y))\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[T(P(X))\\right]\\right|\\le O(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Define the vectors $t=(t_{\\alpha})$ and $u=\\left(u_{\\alpha}\\right)$ by ", "page_idx": 15}, {"type": "equation", "text": "$$\nt_{\\alpha}:=\\frac{1}{\\alpha!}\\partial^{\\alpha}\\tilde{F}(0)\\quad\\mathrm{~and~}\\quad u_{\\alpha}:=\\mathbb{E}_{Y\\sim N(0,I_{n})}\\left[P(Y)^{\\alpha}\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P(X)^{\\alpha}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We then have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[T(P(Y))\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[T(P(X))\\right]\\right|=|\\langle t,u\\rangle|\\leq\\|t\\|_{1}\\cdot\\|u\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We now want to bound $\\|t\\|_{1}$ and $\\|u\\|_{\\infty}$ separately. For the bound on $\\|t\\|_{1}$ , note that, by Lemma 27, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n|t_{\\alpha}|=\\frac{1}{\\alpha!}\\partial^{\\alpha}\\tilde{F}(0)\\leq\\prod_{i=1}^{d}C_{i}^{|\\alpha_{i}|}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging in the defnition of $C_{i}=\\Theta_{d}\\left(\\varepsilon^{-7^{i}d}\\right)\\le{\\cal O}_{d}(k)$ and using $\\begin{array}{r}{|\\alpha|=\\sum_{i=1}^{d}|\\alpha_{i}|\\leq k}\\end{array}$ we get that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|t_{\\alpha}|\\leq k^{O_{d}(k)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since we have that $n_{i}\\leq k$ , we can conclude that the number of multi-indices $\\alpha$ With $|\\alpha_{i}|\\le k$ is at most $(d k)^{k}\\leq k^{O_{d}(k)}$ and thus, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|t\\|_{1}=\\sum_{\\alpha}|t_{\\alpha}|\\leq k^{O_{d}(k)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We now move on to bound $\\|u\\|_{\\infty}$ . We write $P_{i,j}$ as follows ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{i,j}(x)=\\sum_{\\beta}a_{i,j}^{(\\beta)}x^{\\beta}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, the $\\beta$ in the sum goes over all multi-indices with $|\\beta|$ being the degree of $P_{i,j}$ , which is at most $d$ . By Fact 22, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{\\beta}\\vert a_{i,j}^{(\\beta)}\\vert\\le n^{d/2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $|\\alpha|\\;=\\;\\ell$ be such that $|\\alpha_{i}|\\ \\leq\\ m_{i}$ (i.e. the term appears in the Taylor expansion) and let $(i_{1},j_{1}),\\dotsc,(i_{\\ell},j_{\\ell})$ be such that ", "page_idx": 15}, {"type": "equation", "text": "$$\nP(x)^{\\alpha}=P_{i_{1},j_{1}}(x)\\ldots P_{i_{\\ell},j_{\\ell}}(x).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that if some $(\\alpha_{i})_{j}>1$ , we include the corresponding factor $P_{i,j}$ multiple times. We can now expand $P(x)^{\\alpha}$ as follows ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{P(x)^{\\alpha}=\\left(\\sum_{\\beta_{1}}a_{i_{1},j_{1}}^{(\\beta_{1})}x^{\\beta_{1}}\\right)\\cdot\\cdot\\cdot\\left(\\sum_{\\beta_{\\ell}}a_{i_{\\ell},j_{\\ell}}^{(\\beta_{\\ell})}x^{\\beta_{\\ell}}\\right)}}\\\\ &{}&{=\\sum_{\\beta_{1}}\\cdot\\cdot\\cdot\\sum_{\\beta_{\\ell}}a_{i_{1},j_{1}}^{(\\beta_{1})}\\cdot\\cdot\\cdot a_{i_{\\ell},j_{\\ell}}^{(\\beta_{\\ell})}x^{\\beta_{1}+\\cdots+\\beta_{\\ell}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, note that $k\\geq d(m_{1}+\\cdot\\cdot+m_{d})$ (this condition is in addition to the conditions on $k$ in [21] but it does not change the asymptotic value of $k$ as stated in (7)). We get that the degree of the terms appearing in the sum is $|\\dot{\\beta}_{1}|\\dot{+}\\dots+|\\beta_{\\ell}|\\leq d|\\alpha|\\leq d(m_{1}+\\dots m_{d})^{\\!}\\leq k$ and thus that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\vert\\mathbb{E}_{Y\\sim N(0,I_{n})}\\left[P(Y)^{\\alpha}\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P(X)^{\\alpha}\\right]\\vert\\leq\\sum_{\\beta_{1}}\\cdot\\cdot\\sum_{\\beta_{\\ell}}\\vert a_{i_{1},j_{1}}^{(\\beta_{1})}\\vert\\cdot\\cdot\\vert a_{i_{\\ell},j_{\\ell}}^{(\\beta_{\\ell})}\\vert\\eta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, we used the triangle inequality and the fact that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[Y^{\\beta_{1}+\\cdots+\\beta_{\\ell}}\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[X^{\\beta_{1}+\\cdots+\\beta_{\\ell}}\\right]\\right|\\leq\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we can compute ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P(Y)^{\\alpha}\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P(X)^{\\alpha}\\right]|\\leq\\displaystyle\\sum_{\\beta_{1}}\\cdots\\sum_{\\beta_{\\ell}}|a_{i_{1},j_{1}}^{(\\beta_{1})}|\\cdot\\cdot\\cdot\\left|a_{i_{\\ell},j_{\\ell}}^{(\\beta_{\\ell})}|\\eta}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(\\displaystyle\\sum_{\\beta_{1}}|a_{i_{1},j_{1}}^{(\\beta_{1})}|\\right)\\ldots\\left(\\sum_{\\beta_{\\ell}}|a_{i_{\\ell},j_{\\ell}}^{(\\beta_{\\ell})}|\\right)\\eta}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left\\lVert a_{i_{1},j_{1}}\\right\\rVert_{1}\\ldots\\left\\lVert a_{i_{\\ell},j_{\\ell}}\\right\\rVert_{1}\\eta}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq n^{\\ell d/2}\\eta}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=n^{|\\alpha|d/2}\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|u\\|_{\\infty}\\leq n^{O_{d}(k)}\\eta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, we can conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[T(P(Y))\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[T(P(X))\\right]\\right|\\le\\|t\\|_{1}\\cdot\\|u\\|_{\\infty}\\le n^{O_{d}(k)}k^{O_{d}(k)}\\eta\\le O(\\varepsilon)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since we have the conditions $\\eta\\le n^{-\\Omega_{d}(k)}k^{-\\Omega_{d}(k)}$ and $k^{-1}\\leq O(\\varepsilon)$ ", "page_idx": 16}, {"type": "text", "text": "A.1.3  The functions $f$ and $\\tilde{f}$ are close in expectation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we want to complete the proof of Proposition 20 that shows ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right]-\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]\\right|\\le O_{d}(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So far, we already showed in Lemma 25 that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[\\tilde{f}(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\tilde{f}(X)\\right]\\right|\\le O(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, it remains to show that under both $X\\sim\\mathcal{D}^{\\prime}$ and $Y\\sim{\\mathcal{N}}(0,I_{n})$ , the expectation of $f$ and $\\tilde{f}$ differ by at most $O_{d}(\\varepsilon)$ . For $Y$ , we directly get the following. Note that the approximation $\\tilde{f}$ depends on $\\varepsilon$ via the numbers $m_{i}$ in the structure theorem Lemma 21 and the $C_{i}$ in the definition of $\\rho$ ", "page_idx": 16}, {"type": "text", "text": "Lemma 28 ([21, Proposition 14]). Let $\\varepsilon>0$ Then, we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[\\tilde{f}(Y)\\right]\\right|\\le O(\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The reason why we get this directly from [21] is that this lemma only concerns the Gaussian and not the moment matching distribution. Combining this with the above, we have now shown that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\tilde{f}(X)\\right]\\right|\\le O(\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To conclude Proposition 20 we want to use the following lemma. It is analogous to [21, Proof of Proposition 2] and we use the same definition of $B_{i}$ ,i.e.we define $B_{i}=\\Theta_{d}(\\sqrt{\\log(1/\\varepsilon)})$ .Weprove this lemmas in Appendix C.2. ", "page_idx": 16}, {"type": "text", "text": "Lemma 29 (analogous to [21, Proof of Proposition 2]). Let $\\varepsilon>0$ Supposethat $\\mathcal{D}^{\\prime}$ is a distribution such that thefollowingholds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bullet\\ \\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\tilde{f}(X)\\right]\\right|\\leq O(\\varepsilon),\\,a n d\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\exists i,j:|P_{i,j}(X)|>B_{i}\\right]\\le O(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right]|\\leq O_{d}(\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We are now ready to prove Proposition 20. ", "page_idx": 17}, {"type": "text", "text": "Proof of Proposition 20. Using Lemmas 25, 28 and 29, it remains to argue that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\exists i,j:|P_{i,j}(X)|>B_{i}\\right]\\le O(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This is true by Markov's inequality and looking at the $\\log(d n_{i}/\\varepsilon)=\\ell$ -th moment since this implies that (assuming without loss of generality that $\\ell$ is even) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{X\\sim\\mathcal{D}^{*}}\\left[\\left|P_{i,j}(Y)\\right|\\geq B_{i}\\right]\\leq\\frac{\\mathbb{E}_{X\\sim\\mathcal{N}^{*}}\\left[P_{i,j}(X)^{\\varepsilon}\\right]}{B_{i}^{\\varepsilon}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P_{i,j}(Y)^{\\varepsilon}\\right]+\\eta m^{d\\varepsilon/2}}{B_{i}^{\\varepsilon}}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P_{i,j}(Y)^{\\varepsilon}\\right]+1}{B_{i}^{\\varepsilon}}}\\\\ &{\\qquad\\qquad\\leq\\frac{O_{d}(\\sqrt{\\varepsilon})^{\\varepsilon}}{B_{i}^{\\varepsilon}}}\\\\ &{\\qquad\\qquad\\leq\\frac{O_{d}(\\sqrt{\\varepsilon})^{\\varepsilon}}{B_{i}^{\\varepsilon}}}\\\\ &{\\qquad\\qquad\\leq e^{-\\varepsilon}}\\\\ &{\\qquad\\qquad=\\frac{\\varepsilon}{m_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the second step we used Fact 23. Note that $\\ell=\\log(d n_{i}/\\varepsilon)\\leq k/d$ is ensured by the choice of $k$ as in [21]. In the third step we used that since $d\\ell/2\\leq k$ and thus the condition on $\\eta$ in the statement of this proposition ensures $\\begin{array}{r}{\\eta\\leq\\frac{1}{n^{d\\ell/2}}}\\end{array}$ . In the fourth step we used Lemma 21. In the fth step we used the condition $B_{i}\\geq\\Omega_{d}(\\sqrt{\\ell})$ . This is true by the choice of $B_{i}=\\Theta_{d}(\\sqrt{\\log(1/\\varepsilon)})$ and the fact that $\\begin{array}{r}{\\log(n_{i})\\leq\\sum_{j=1}^{i-1}\\log(m_{j})\\leq O_{d}(\\log(1/\\varepsilon))}\\end{array}$ In thlast ste,wethusd tden $\\ell$ Taking a union bound over $j$ and then over $i$ gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\exists i,j:|P_{i,j}(X)|>B_{i}\\right]\\le O(\\varepsilon),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which completes the proof. ", "page_idx": 17}, {"type": "text", "text": "A.2  Fooling for arbitrary PTFs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we want to prove a result similar to Proposition 20 for arbitrary PTFs and not just multilinear ones. Namely, we show Proposition 12, which we restate with a slight modification below. Namely,we onlyprove $\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[\\hat{f}(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right]\\right|\\le O_{d}(\\varepsilon)$ insteadof $\\varepsilon/2$ .The reason for this is, as for Proposition 20, this simplifies the proof and we take care of this difference when we apply Theorem 9 in Appendix A.3. ", "page_idx": 17}, {"type": "text", "text": "Proposition 30 (Restatement of Proposition 12). Let $\\varepsilon\\ >\\ 0$ \uff1aSuppose that $\\mathcal{D}^{\\prime}$ approximately matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta,$ where $k\\ \\geq\\ \\Omega_{d}\\big(\\varepsilon^{-4d\\cdot7^{d}}\\big)$ ,and $\\eta\\le n^{-\\Omega_{d}(k)}k^{-\\Omega_{d}(k)}$ Then, we have that for any $f\\in\\mathcal{F}_{\\mathrm{PTF},d}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right]\\right|\\le O_{d}(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The general strategy for this will be to, given a polynomial $p$ , find another polynomial $p_{\\delta}$ andreduce to Proposition 20. This strategy and the construction described in what follows are the same as used in [21, Lemma 15]. The following lemma is an analog of this lemma for our case. However, there is one key part of the proof of [21] that breaks in our setting, as explained in Section 2.3.2. Specifically, in [21] all restrictions to coordinates are exactly Gaussian, and in particular we have access to moments of all orders. The proof in [21] exploits this since it considers a number of moments depending on the dimension, whereas we only have access to a constant number of moments. ", "page_idx": 17}, {"type": "text", "text": "Lemma 31. Let $\\delta>0$ Suppose $X\\sim\\mathcal{D}^{\\prime}$ approximately matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta_{-}$ where $\\eta\\le\\delta^{O_{d}(1)}n^{-\\Omega_{d}(1)}k^{-k}$ and $k\\geq\\Omega_{d}(1)$ Let $Y\\sim{\\mathcal{N}}(0,I_{n})$ Then there are a polynomial $p_{\\delta}$ and random variables $\\hat{X}$ and $\\hat{Y}$ , all in more variables, such that ", "page_idx": 18}, {"type": "text", "text": "\u00b7 $\\hat{Y}$ is a Gaussian with mean O and covariance identity,   \n\u00b7 $\\hat{X}$ is approximately moment-matching $\\hat{Y}$ up to degree $k$ and slack $\\hat{\\eta}=\\eta(2k)^{k/2}$ \uff0c   \n$\\begin{array}{r l}&{\\bullet\\ \\mathbb{P}_{Y,\\hat{Y}}\\left[|p(Y)-p_{\\delta}(\\hat{Y})|>\\delta\\right]<\\delta,\\,a n}\\\\ &{\\bullet\\ \\mathbb{P}_{X,\\hat{X}}\\left[|p(X)-p_{\\delta}(\\hat{X})|>\\delta\\right]<\\delta.}\\end{array}$ d'\u2019 ", "page_idx": 18}, {"type": "text", "text": "Given this lemma, we can prove Proposition 30. Since this proof follows closely [21, Proof of Theorem 1], we defer it to Appendix C.2 ", "page_idx": 18}, {"type": "text", "text": "It remains to prove Lemma 31 and to explain how we construct $p_{\\delta}$ as well as $\\hat{X}$ and $\\hat{Y}$ . We do the latter in Appendix A.2.1. Namely, we show there how to construct the random variables $\\hat{X}$ and $\\hat{Y}$ from $X$ and $Y$ . In this section, we also make precise in how many variables the polynomial $p_{\\delta}$ is (and thus also the random variable $\\hat{X}$ and $\\hat{Y}$ ). The proof that they satisfy the condition required by Lemma 31 can be found in Appendix C.2. In Appendix A.2.2 we then state a lemma about how we want to replace a factor $X_{i}^{\\ell}$ in $p$ by a multilinear polynomial in $\\hat{X}$ (whose proof is in Appendix C.2) and use it construct $p_{\\delta}$ and prove Lemma 31. ", "page_idx": 18}, {"type": "text", "text": "A.2.1 Construction of the random variables $\\hat{X}$ and $\\hat{Y}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To show Lemma 31, we want, given a polynomial $p$ and $\\delta>0$ as well as two random variables $X$ and $Y$ , where $Y\\sim{\\mathcal{N}}(0,I_{n})$ and $X$ matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta$ ,to construct a multilinear polynomial $p_{\\delta}$ in more variables and two random variables $\\hat{X}$ and $\\hat{Y}$ such that $\\hat{Y}$ is a again Gaussian with mean O and covariance identity and $\\hat{X}$ matches the moments of $\\hat{Y}$ up to degree $k$ and slack $\\hat{\\eta}$ . The guarantee we then want to show in Lemma 31 is that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}_{Y,\\hat{Y}}\\left[|p(Y)-p_{\\delta}(\\hat{Y})|>\\delta\\right]<\\delta\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}_{X,\\hat{X}}\\left[|p(X)-p_{\\delta}(\\hat{X})|>\\delta\\right]<\\delta,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the probability is over the joint distribution of $Y$ and $\\hat{Y}$ respectively $X$ and $\\hat{X}$ ", "page_idx": 18}, {"type": "text", "text": "Let $N$ be a (large) positive integer that will be chosen later. Then the number of variables of the newpolynomial $p_{\\delta}$ is $n\\cdot N$ , i.e. we replace every variable of $p$ by $N$ variables for $p_{\\delta}$ .Wemake the following definition. For $i\\in\\{1,\\ldots,n\\}$ and $j\\in\\{1,\\ldots,N\\}$ , we define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{X}_{i,j}:=\\frac{1}{\\sqrt{N}}X_{i}+Z_{j}^{(i)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $Z^{(i)}$ is are multivariate Gaussians with mean O, variance $\\begin{array}{r}{1-\\frac{1}{N}}\\end{array}$ and covariance $-\\frac{1}{N}$ , independent for different $i$ and independent from $X$ . In particular, the choice of the covariance matrix ensures that we deterministically have $Z_{i,1}+...\\,Z_{i,N}=0$ and thus $\\begin{array}{r}{X_{i}=\\sum_{j=1}^{N}\\hat{X}_{i,j}}\\end{array}$ The construction for $\\hat{Y}$ is the same, i.e. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{Y}_{i,j}:=\\frac{1}{\\sqrt{N}}Y_{i}+Z_{j}^{\\prime(i)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $Z^{\\prime(i)}$ areagainlivariatassiaiai $\\begin{array}{r}{1-\\frac{1}{N}}\\end{array}$ and covariance $-\\,{\\frac{1}{N}}$ \uff0c independent for different $i$ and also independent from $Y$ (as well as $X$ and $\\bar{Z}$ ", "page_idx": 18}, {"type": "text", "text": "We now have the following two lemmas that relate $\\hat{X}$ to $X$ and $\\hat{Y}$ to $Y$ . The proofs of these lemmas are in Appendix C.2. ", "page_idx": 18}, {"type": "text", "text": "Lemma 32. Suppose $X$ approximately matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta$ Then, $\\hat{X}$ approximately matches the moments of $\\mathcal{N}(0,I_{n N})$ up to degree $k$ and slack $\\hat{\\eta}=(2k)^{k/2}\\eta$ ", "page_idx": 18}, {"type": "text", "text": "Lemma 33 is already proven in [21, Lemma 15], but for completeness we also make it explicit in AppendixC.2. ", "page_idx": 19}, {"type": "text", "text": "A.2.2 Proof of Lemma 31 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "After constructing the random variables $\\hat{X}$ and $\\hat{Y}$ , we now move on to construct the polynomial $p_{\\delta}$ As in [21, Proof of Lemma 15], the goal is to replace every factor $x_{i}^{\\ell}$ of $p$ by a multilinear polynomial in variables $(\\hat{x}_{i,j})_{j}$ such that $X_{i}^{\\ell}$ is close to this polynomial evaluated in $(\\hat{X}_{i,j})_{j}$ with large probability. Doing this for all factors $x_{i}^{\\ell}$ appearing in $p$ and combining the new multilinear terms, this then gives a multilinear polynomial $p_{\\delta}$ of degree $d$ . Note that the polynomial is in fact multilinear since for replacing $x_{i}$ we only use the variables $\\hat{x}_{i^{\\prime},j}$ where $i^{\\prime}=i$ ", "page_idx": 19}, {"type": "text", "text": "Note that it is enough to show ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|p(X)-p_{\\delta}(\\hat{X})|>\\delta\\right]<\\delta.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The reason for this is that, if $Y\\sim{\\mathcal{N}}(0,I_{n})$ , then $Y$ in particular matches the moments of ${\\mathcal{N}}(0,I_{n})$ (exactly) and the proof for $X$ applies and we can conclude Lemma 31, using also Lemmas 32 and 33. ", "page_idx": 19}, {"type": "text", "text": "In order to get the above, we let $\\delta^{\\prime}$ be a small positive number (depending on $\\delta,n,d)$ to be chosen later. We need the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma 34. Let $\\delta^{\\prime}>0$ Let $i\\in[n]$ and $\\ell\\in[d]$ .Assume that each of the following conditions holds with probability $\\textstyle1-{\\frac{\\delta^{\\prime}}{d}}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c c c}{{\\displaystyle\\left\\lvert\\displaystyle\\sum_{j=1}^{N}\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right\\rvert\\leq\\frac{1}{\\delta^{\\prime}}}}&{{}}&{{}}\\\\ {{\\displaystyle\\sum_{j=1}^{N}\\left(\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right)^{2}-1\\Bigg\\rvert\\leq\\delta^{\\prime d+1}}}&{{}}&{{}}\\\\ {{\\displaystyle\\left\\lvert\\displaystyle\\sum_{j=1}^{N}\\left(\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right)^{a}\\right\\rvert\\leq\\delta^{\\prime d+1}}}&{{~}}&{{\\displaystyle\\forall\\:3\\leq a\\leq d.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, there is a multilinear polynomial in $\\hat{X}_{i,j}$ that is within $O_{d}(\\delta^{\\prime})$ of $X_{i}^{\\ell}$ with probability $1-\\delta^{\\prime}$ ", "page_idx": 19}, {"type": "text", "text": "The proof of this lemma is analogous to [21, Proof of Lemma 15] and is deferred to Appendix C.2. However, in contrast to [21], there is a key difference in this lemma. The bound on the RHS of (10) is much weaker than the one used by Kane. The reason for that is that the bounds used there do not hold in our case, since we only have that $X$ (approximately) matches the moments of ${\\mathcal{N}}(0,I_{n})$ .By using stronger bounds for (11) and (12), we are able to generalize the proof from Kane to our setting. For a more detailed explanation why we need to change the bounds, we refer to Section 2.3.2. ", "page_idx": 19}, {"type": "text", "text": "We now define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\delta^{\\prime}:=\\operatorname*{min}\\left\\{\\frac{\\delta}{2d n},\\Theta_{d}\\left(\\frac{\\delta^{d+1}}{n^{3d/2}}\\right)\\right\\}=\\Theta_{d}\\left(\\frac{\\delta^{d+1}}{n^{3d/2}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Why we make this choice will become clear in the proof of Lemma 31 below. In Appendix C.2, we prove the following lemma that states that this choice of $\\delta^{\\prime}$ and the condition on $\\eta$ from Lemma 31 ensure that we can apply Lemma 34. ", "page_idx": 19}, {"type": "text", "text": "Lemma 35. Let $\\delta^{\\prime}$ as in (13).Assming $\\eta\\le\\delta^{O_{d}(1)}n^{-\\Omega_{d}(1)}k^{-k}$ there is a choice of $N$ (independent of i or $\\ell$ ) such that (10), (11) and (12) hold, each with probability $\\textstyle1-{\\frac{\\delta^{\\prime}}{d}}$ ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma $3l$ . Lemma 34 (together with Lemma 35) shows that we can replace $X_{i}^{\\ell}$ by a multilinear polynomial in $\\hat{X}_{i,j}$ that is within $O_{d}(\\delta^{\\prime})$ of $X_{i}^{\\ell}$ with probability $1-\\delta^{\\prime}$ for our choice of $\\delta^{\\prime}$ as i (13) Since $\\begin{array}{r}{\\delta^{\\prime}\\leq\\frac{\\delta}{2d n}}\\end{array}$ wal $i\\in[n]$ and $\\ell\\in[d]$ and get ", "page_idx": 19}, {"type": "text", "text": "that with probability $\\textstyle1-{\\frac{\\delta}{2}}$ , we have that for any $i\\in[n]$ and $\\ell\\in[d]$ , we have that the replacement polynomial for $X_{i}^{\\ell}$ is within $O_{d}(\\delta^{\\prime})$ of $X_{i}^{\\ell}$ ", "page_idx": 20}, {"type": "text", "text": "Furthermore, forany [n], wehave that with probability 1 -  t that $|X_{i}|\\leq3{\\frac{n}{\\delta}}$ . This is true since we match $k\\geq2$ moments (and $\\begin{array}{r}{\\eta\\le\\frac{1}{4}}\\end{array}$ ), and thus ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|X_{i}|\\geq3{\\frac{n}{\\delta}}\\right]\\leq{\\frac{\\mathbb{E}\\left[X_{i}^{2}\\right]}{\\left(3{\\frac{n}{\\delta}}\\right)^{2}}}\\leq{\\frac{(2+\\eta)\\delta^{2}}{9n^{2}}}\\leq{\\frac{\\delta}{4n}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, we can again apply the union bound to show that with probability $\\textstyle1\\,-\\,{\\frac{\\delta}{2}}$ ,wehavefor any i E [n] that |Xi \u2264 3. ", "page_idx": 20}, {"type": "text", "text": "Thus, with probability $1-\\delta$ , all the above events holds. Conditioned on that, we have that the replacement polynomial $p_{\\delta}$ is off by at most $\\left(3\\frac{n}{\\delta}\\right)^{d}O_{d}\\left(\\delta^{\\prime}\\right)$ multiplied by the sum of the coefficients of $p$ . The later is at most $n^{d/2}$ by Fact 22 applied to $p$ instead of $P_{i,j}$ . Hence, the replacement polynomial is off by at most $\\begin{array}{r}{O_{d}\\left(\\frac{n^{3d/2}}{\\delta^{d}}\\right)\\delta^{\\prime}}\\end{array}$ $\\begin{array}{r}{\\delta^{\\prime}\\leq\\frac{\\delta}{\\Omega_{d}\\left(\\frac{n^{3d/2}}{\\delta^{d}}\\right)}}\\end{array}$   \n$1-\\delta,p\\delta$ is off by at most $\\delta$ , which is what we wanted to show. ", "page_idx": 20}, {"type": "text", "text": "A.3 Proof of testable learning of PTFs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we prove Theorem 19. As already mentioned in the beginning of this section, we want to apply Theorem 9 for this. In Proposition 30, we have shown that if we have moment matching up to error $\\eta\\le n^{-\\Omega_{d}(k)}k^{-\\Omega_{d}(k)}$ , then we have the foling condition of Theorem 9. ", "page_idx": 20}, {"type": "text", "text": "Note that the fooling condition requires $\\begin{array}{r}{|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right]|\\,\\le\\,\\frac{\\varepsilon}{2}}\\end{array}$ but Proposition 30 only gives $O_{d}(\\varepsilon)$ . Thus, technically, we apply Proposition 30 for $\\begin{array}{r}{\\varepsilon^{\\prime}=\\frac{\\varepsilon}{\\Omega_{d}(1)}}\\end{array}$ D(l) . However, this does not change the asymptotic condition on $\\eta$ described above. In summary, if $\\eta$ satisfies the condition as described above, we get indeed the fooling condition as needed for Theorem 9. ", "page_idx": 20}, {"type": "text", "text": "The remaining part to prove Theorem 19 is to find an $m$ such that with high probability over $m$ samples from $\\bar{\\mathcal{N}}(0,I_{n})$ we have that the empirical distribution matches the moments up to degree $k$ with error at most $\\eta$ . Then, we get testable learning of PTFs with respect to Gaussian in time and sample complexity $m+n^{O(k)}$ by Theorem 9. ", "page_idx": 20}, {"type": "text", "text": "To get $m$ , we use the following fact, which we prove in Appendix C.1. Using this, we can then prove Theorem 19. ", "page_idx": 20}, {"type": "text", "text": "Fact 36. Given $m\\geq\\Omega((2k n)^{k}\\eta^{-2})$ samplesof ${\\mathcal{N}}(0,I_{n})$ we have that with high probability the empirical distribution matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta$ ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 19. Using Theorem 9, as noted before, by Proposition 30, we get testable learning of degree $d$ PTFs with respect to Gaussian in time and sample complexity bounded by $m+n^{O(k)}$ where $m$ is such that with high probability over $m$ samples from $\\mathcal{N}(0,I_{n})$ we have that the empirical distribution matches the moments up to degree $k$ with error at most $\\eta$ . It remains to determine $m$ . By Fact 36, we get that the choice of $\\stackrel{\\bullet}{m}=\\Theta((2k n)^{k}\\eta^{-2})$ is enough. Now, in order to apply Proposition 30, we need to choose $\\eta=n^{-\\Theta_{d}(k)}k^{-\\Theta_{d}(k)}$ . Plugging in the value $k=\\Theta_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right)$ ", "page_idx": 20}, {"type": "text", "text": "we get $\\eta=\\varepsilon^{\\Theta_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right)}n^{-\\Theta_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right)}$ and hence ", "page_idx": 20}, {"type": "equation", "text": "$$\nm=\\Theta\\left((2k n)^{k}n^{\\Theta_{d}(\\varepsilon^{-4d\\cdot7^{d}})}\\varepsilon^{-\\Theta_{d}(\\varepsilon^{-4d\\cdot7^{d}})}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, the time and sample complexity for testably learning PTFs is ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\cal O}\\left((2k n)^{k}n^{{\\cal O}_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right)}\\varepsilon^{-\\Omega_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right)}n^{{\\cal O}(k)}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Again, by plugging in the value $k=\\Theta_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right)$ , we can simplify this to get that the sample and time complexity for testably learning PTFs is ", "page_idx": 20}, {"type": "equation", "text": "$$\nn^{O_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right)}\\varepsilon^{-\\Omega_{d}\\left(\\varepsilon^{-4d\\cdot7^{d}}\\right)},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which completes the proof of this theorem. ", "page_idx": 20}, {"type": "text", "text": "B  Impossibility of approximating PTFs via the push-forward ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we provide further details on the results claimed in Section 2.4. Specifically, we prove our impossibility result Theorem 16 below in Appendix B.2. For this, we first need to establish our \u2018one-sided\u2019 analog Theorem 18 of the inapproximability result for LSL-distributions of Bun & Steinke (Theorem 15), which we do in Appendix B.1. ", "page_idx": 21}, {"type": "text", "text": "B.1 Proof of Theorem 18 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Let us begin by restating some important definitions and results from Section 2.4 for convenience. For $\\gamma>0$ , we write $w_{\\gamma}(x):=C_{\\gamma}\\cdot\\exp(-|x|^{\\gamma})$ , where $C_{\\gamma}$ is a normalizing constant which ensures $w_{\\gamma}$ is a probability density. A distribution $\\mathcal{D}$ on $\\mathbb{R}$ is called log-superlinear $(L S L)$ if its density function? satisfies $w(x)\\geq C\\cdot w_{\\gamma}(x)$ for all $x\\in\\mathbb{R}$ , for some $\\gamma\\in(0,1)$ and $C>0$ . Recall the following. ", "page_idx": 21}, {"type": "text", "text": "Theorem (Restatement of Theorem 15). Let $\\mathcal{D}$ be an LSL-distribution on $\\mathbb{R}$ .Then there exists an $\\varepsilon>0$ such that, for any polynomial $q$ we have $\\mathbb{E}_{Y\\sim{\\mathcal{D}}}\\left[|q(Y)-\\mathrm{sign}(Y)|\\right]>\\varepsilon$ ", "page_idx": 21}, {"type": "text", "text": "We defined in Section 2.4 a ^one-sided\u2019 analog of LSL-distributions as follows. ", "page_idx": 21}, {"type": "text", "text": "Definition (Restatement of Definition 17). Let $\\mathcal{D}$ be a distribution on $\\mathbb{R}$ whosedensityfunction $w$ satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\nw(x)\\geq C\\cdot w_{\\gamma}(x)\\quad\\forall\\,x\\in(-\\infty,1]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some $\\gamma<1/2$ and $C>0$ Then we say $\\mathcal{D}$ is one-sided log-superlinear ", "page_idx": 21}, {"type": "text", "text": "In this section, we prove Theorem 18, which is an analog of Theorem 15 for one-sided LSLdistributions, and the basis of our proof of Theorem 16 in Appendix B.2. ", "page_idx": 21}, {"type": "text", "text": "Theorem (Restatement of Theorem 18). Let $\\mathcal{D}$ be a one-sided LSL-distribution on $\\mathbb{R}$ Thenthere exists an $\\varepsilon>0$ such that, for any polynomial $q$ wehave $\\mathbb{E}_{Y\\sim{\\mathcal{D}}}\\left[|q(Y)-\\mathrm{sign}(Y)|\\right]>\\varepsilon$ ", "page_idx": 21}, {"type": "text", "text": "For our proof, it is useful to first recall the main ingredient of the proof of Theorem 15 in [6], which is the following inequality. ", "page_idx": 21}, {"type": "text", "text": "Proposition 37 ([6, Lemma 20]). Let q be a univariate polynomial, and let $\\gamma\\in(0,1)$ .Then,there existsaconstant $M_{\\gamma}>0,$ dependingonlyon $\\gamma,$ sothat ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathbb{R}}|q^{\\prime}(x)w_{\\gamma}(x)|\\leq M_{\\gamma}\\cdot\\int_{\\mathbb{R}}|q(x)|w_{\\gamma}(x)\\,d x.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Given Proposition 37, the intuition for the proof of Theorem 15 is that, if $q$ is a good approximation of the sign-function on $\\mathbb{R}$ , it must have very large derivative near the origin. On the other hand, $\\mathbb{E}_{Y\\sim\\mathcal{D}}\\left[\\bar{|{q}}(Y)\\right|\\right]$ is bounded from above (as $\\mathbb{E}_{Y\\sim\\mathcal{D}}\\left[|\\operatorname{sign}(Y)|\\right]=1)$ , leading to a contradiction. ", "page_idx": 21}, {"type": "text", "text": "The key technical tool in our proof of Theorem 18 is a version of Proposition 37 that applies to onesided LSL distributions. That is, a bound on the derivative of a polynomial in terms of its $L_{1}$ -normon $(-\\infty,1]$ W.r.t.theweight $w_{\\gamma}(x)=C_{\\gamma}\\cdot\\exp(-|x|^{\\gamma}),\\gamma<1/2$ We will only be able to obtain such a bound in a small neighborhood of 0 (Proposition 39 below), but this will turn out to be sufficient. We first need the following lemma, which bounds the derivative near 1. One can think of it as a one-sided Bernstein-Nikolski-type inequality (whereas Proposition 37 is a Markov-Nikolskii-type inequality). ", "page_idx": 21}, {"type": "text", "text": "Lemma 38. Let q be a univariate polynomial, and let $\\gamma<1/2$ Then, there exists a constant $M_{\\gamma}^{\\prime}>0$ depending only on $\\gamma$ sothat ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{|x-1|\\leq\\frac{1}{4}}|q^{\\prime}(x)|\\leq M_{\\gamma}^{\\prime}\\cdot\\int_{0}^{\\infty}|q(x)|w_{\\gamma}(x)\\,d x.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. By a substitution $u^{2}=x$ , and using the fact that $w_{\\gamma}(x):=C_{\\gamma}\\exp(-|x|^{\\gamma})$ is even, we find ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{0}^{\\infty}{|q(x)|w_{\\gamma}(x)\\,d x}=\\int_{0}^{\\infty}{|q(u^{2})|w_{\\gamma}(u^{2})\\cdot2u\\,d u}={\\frac{C_{\\gamma}}{C_{2\\gamma}}}\\cdot\\int_{-\\infty}^{\\infty}{|q(u^{2})u|w_{2\\gamma}(u)\\,d u}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "3Technically, density functions are defined only up to measure-zero sets. Therefore, one should read statements of the form $\\overdot{\\boldsymbol{w}}(\\boldsymbol{x})\\geq\\ldots$ for all $x\\in..\\,.^{\\,}$ as only holding a.e. throughout. ", "page_idx": 21}, {"type": "text", "text": "Now, since $2\\gamma<1$ : we may apply Proposition 37 to the polynomial $\\hat{q}(u)=q(u^{2})u$ yielding an upper bound on its derivative for all $u\\in\\mathbb R$ ; namely ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{w_{2\\gamma}(u)\\cdot\\bigg|\\frac{\\mathrm{d}}{\\mathrm{d}u}\\hat{q}(u)\\bigg|\\le M_{2\\gamma}\\cdot\\int_{-\\infty}^{\\infty}|q(u^{2})u|w_{2\\gamma}(u)\\,d u}}\\\\ &{}&{\\qquad=\\frac{M_{2\\gamma}C_{2\\gamma}}{C_{\\gamma}}\\cdot\\int_{0}^{\\infty}|q(x)|w_{\\gamma}(x)\\,d x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As $w_{2\\gamma}$ is monotonically decreasing, we have $w_{2\\gamma}(x)\\geq w_{2\\gamma}(5/4)$ for all $x\\leq5/4$ , and so we find ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{|u^{2}-1|\\leq\\frac{1}{4}}\\left|\\frac{\\mathrm{d}}{\\mathrm{d}u}\\hat{q}(u)\\right|\\leq\\hat{M}_{\\gamma}\\cdot\\int_{0}^{\\infty}|q(x)|w_{\\gamma}(x)\\,d x,\\quad\\hat{M}_{\\gamma}:=\\frac{M_{2\\gamma}C_{2\\gamma}}{C_{\\gamma}\\cdot w_{2\\gamma}(\\frac{5}{4})}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We wish to transform this bound on the derivative of $\\hat{q}$ into a bound on the derivative of $q$ . Note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}u}\\hat{q}(u)=\\frac{\\mathrm{d}}{\\mathrm{d}u}[q(u^{2})u]=2q^{\\prime}(u^{2})u^{2}+q(u^{2}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We can bound this as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{|u^{2}-1|\\leq\\frac{1}{4}}|2q^{\\prime}(u^{2})u^{2}+q(u^{2})|\\geq\\frac{3}{2}\\cdot\\operatorname*{sup}_{|u^{2}-1|\\leq\\frac{1}{4}}|q^{\\prime}(u^{2})|-\\operatorname*{sup}_{|u^{2}-1|\\leq\\frac{1}{4}}|q(u^{2})|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, switching our notation back to the variable $x=u^{2}$ , and using (14), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{3}{2}\\cdot\\operatorname*{sup}_{|x-1|\\leq\\frac{1}{4}}|q^{\\prime}(x)|\\leq\\hat{M}_{\\gamma}\\cdot\\int_{0}^{\\infty}|q(x)|w_{\\gamma}(x)\\,d x+\\operatorname*{sup}_{|x-1|\\leq\\frac{1}{4}}|q(x)|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It remains to bound the rightmost term in this inequality. Write $c_{q}>0$ for the constant (depending on $q$ )satisfying: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{|x-1|\\leq\\frac{1}{4}}|q(x)|=\\frac{c_{q}}{w_{\\gamma}(\\frac{5}{4})}\\cdot\\int_{0}^{\\infty}|q(x)|w_{\\gamma}(x)\\,d x.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$c_{q}\\leq2$ , we are done immediately by (15), as then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{3}{2}\\cdot\\operatorname*{sup}_{|x-1|\\leq\\frac{1}{4}}|q^{\\prime}(x)|\\leq\\left(\\hat{M}_{\\gamma}+\\frac{2}{w_{\\gamma}(\\frac{5}{4})}\\right)\\cdot\\int_{0}^{\\infty}|q(x)|w_{\\gamma}(x)\\,d x.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "So assume that $c_{q}>2$ . Note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{0}^{\\infty}|q(x)|w_{\\gamma}(x)\\,d x\\geq w_{\\gamma}(\\frac{5}{4})\\cdot\\int_{\\frac{3}{4}}^{\\frac{5}{4}}|q(x)|\\,d x\\geq w_{\\gamma}(\\frac{5}{4})\\cdot\\operatorname*{inf}_{|x-1|\\leq\\frac{1}{4}}|q(x)|,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and so ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{|x-1|\\leq\\frac{1}{4}}|q(x)|\\leq\\frac{1}{w_{\\gamma}(\\frac{5}{4})}\\cdot\\int_{0}^{\\infty}|q(x)|w_{\\gamma}(x)\\,d x.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Applying the mean value theorem to (16) and (17) on the interval $[\\textstyle{\\frac{3}{4}},{\\frac{5}{4}}]$ , we find that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{|x-1|\\leq\\frac{1}{4}}|q^{\\prime}(x)|\\geq2\\cdot\\frac{c_{q}-1}{w_{\\gamma}(\\frac{5}{4})}\\cdot\\int_{0}^{\\infty}|q(x)|w_{\\gamma}(x)\\,d x=\\frac{2(c_{q}-1)}{c_{q}}\\cdot\\operatorname*{sup}_{|1-x|\\leq\\frac{1}{4}}|q(x)|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As $c_{q}>2$ by assumption, this yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{|x-1|\\leq{\\frac{1}{4}}}|q^{\\prime}(x)|\\geq\\operatorname*{sup}_{|x-1|\\leq{\\frac{1}{4}}}|q(x)|,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and we may conclude from (15) that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left({\\frac{3}{2}}-1\\right)\\cdot\\operatorname*{sup}_{|x-1|\\leq{\\frac{1}{4}}}|q^{\\prime}(x)|\\leq{\\hat{M}}_{\\gamma}\\cdot\\int_{0}^{\\infty}|q(x)|w_{\\gamma}(x)\\,d x.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining the cases $c_{q}\\leq2,c_{q}>2$ finishes the proof with $\\begin{array}{r}{M_{\\gamma}^{\\prime}=\\operatorname*{max}\\lbrace\\hat{M}_{\\gamma}+\\frac{2}{w_{\\gamma}(5/4)},\\,2\\hat{M}_{\\gamma}\\rbrace}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Proposition 39. Let $\\mathcal{D}$ be a one-sided LSL distribution on $\\mathbb{R}.$ Then there exists a constant $C_{D}>0$ such that, for any polynomial $q$ wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{|x|\\leq\\frac{1}{4}}|q^{\\prime}(x)|\\leq C_{\\mathcal{D}}\\cdot\\mathbb{E}_{X\\sim\\mathcal{D}}\\left[|q(X)|\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. We wish to use Lemma 38, which gives us a bound on the derivative $q^{\\prime}(x)$ of $q$ when $x\\approx1$ To transport this bound to the origin, we consider the shifted polynomial ${\\hat{q}}(x):=q(1-x)$ . Let $w$ be the density function of $\\mathcal{D}$ . Since $\\mathcal{D}$ is a one-side LSL-distribution, there exists a constant $C>0$ and a $\\gamma\\in(0,1/2)$ such that $w(x)\\geq C\\cdot w_{\\gamma}(x)$ for all $x\\in(-\\infty,1]$ .As $w_{\\gamma}$ is even, bounded from above and below on $[0,1]$ , and $w_{\\gamma}(x-1)\\leq\\dot{w}_{\\gamma}(x)$ for $x\\leq0$ , we can find a constant $C^{\\prime}>0$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\nw(x)\\geq C^{\\prime}\\cdot w_{\\gamma}(1-x)\\quad\\forall\\,x\\in(-\\infty,1].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, we find that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{0}^{\\infty}|\\hat{q}(x)|w_{\\gamma}(x)\\,d x=\\int_{-\\infty}^{1}|\\hat{q}(1-x)|w_{\\gamma}(1-x)\\,d x}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\leq\\frac{1}{C^{\\prime}}\\cdot\\int_{-\\infty}^{1}|q(x)|w(x)\\,d x\\leq\\frac{1}{C^{\\prime}}\\cdot\\mathbb{E}_{X\\sim\\mathcal{D}}\\left[|q(X)|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As $\\gamma<1/2$ , we may apply Lemma 38 to find that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{|x-1|\\leq\\frac{1}{4}}|(\\hat{q})^{\\prime}(x)|\\leq M_{\\gamma}^{\\prime}\\cdot\\int_{0}^{\\infty}|\\hat{q}(x)|w_{\\gamma}(x)\\,d x\\leq\\frac{M_{\\gamma}^{\\prime}}{C^{\\prime}}\\cdot\\mathbb{E}_{X\\sim\\mathcal{D}}\\left[|q(X)|\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To finish the proof (with $C_{\\cal D}=M_{\\gamma}^{\\prime}/C^{\\prime})$ , it remains to note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{|x-1|\\leq{\\frac{1}{4}}}|({\\hat{q}})^{\\prime}(x)|=\\operatorname*{sup}_{|x|\\leq{\\frac{1}{4}}}|q^{\\prime}(x)|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We are now ready to prove Theorem 18. Our approach is similar to the proof of Theorem 15 in [6]. ", "page_idx": 23}, {"type": "text", "text": "ProofofTheorem $I8$ Let $\\mathcal{D}$ be a one-side LSL-distribution, and suppose that $q$ is a univariate polynomial satisfying ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathcal{D}}\\left[|\\operatorname{sign}(X)-q(X)|\\right]\\leq1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\mathbb{E}_{X\\sim\\mathcal{D}}\\left[|\\operatorname{sign}(X)|\\right]=1$ we may use the triangle inequality to find that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathcal{D}}\\left[|q(X)|\\right]\\leq1+1=2.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Proposition 39, this means that, for some constant $C_{D}>1$ depending only on $\\mathcal{D}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{|x|\\leq{\\frac{1}{4}}}|q^{\\prime}(x)|\\leq2\\cdot C_{\\mathcal{D}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Set $\\eta=(10C_{\\mathcal{D}})^{-1}$ . It follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{|x|\\leq\\eta}|q(x)-q(0)|\\leq(10C_{\\mathcal{D}})^{-1}\\cdot2C_{\\mathcal{D}}=\\frac{1}{5}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Assuming first that $q(0)\\leq0$ , this means that ", "page_idx": 23}, {"type": "equation", "text": "$$\n|q(x)-\\operatorname{sign}(x)|\\geq{\\frac{4}{5}}\\quad\\forall\\,x\\in[0,\\eta].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $w$ be the density function $\\mathcal{D}$ .As $\\mathcal{D}$ is one-sided LSL, and $\\eta<1$ , we know there is a constant $C>0$ , independent of $\\eta$ , such that $w(x)\\geq C$ for $x\\in[0,\\eta]$ . But, this implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}_{X\\sim{\\mathcal{D}}}\\left[\\left|\\operatorname{sign}(X)-q(X)\\right|\\right]\\geq C\\int_{0}^{\\eta}\\left|\\operatorname{sign}(x)-q(x)\\right|d x\\geq{\\frac{4}{5}}\\eta\\cdot C,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "giving a uniform lower bound on $\\mathbb{E}_{X\\sim\\mathcal{D}}\\left[|\\operatorname{sign}(X)-q(X)|\\right]$ for all polynomials $q$ .If, on the other hand $\\bar{q}(0)\\geq0$ , the same argument works after replacing $[0,\\eta]$ by $[-\\eta,0]$ \u53e3 ", "page_idx": 23}, {"type": "text", "text": "B.2 Proof details for Theorem 16 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we complete the proof of Theorem 16 started in Section 2.4. Recall that, in light of Theorem 18, it remained to prove the following. ", "page_idx": 24}, {"type": "text", "text": "Proposition 40. Let $p(x)\\,=\\,-x(x-1)(x-2)(x-3)(x-4)(x-5)$ .Then, $p$ is square-free, $\\{p\\geq0\\}\\subseteq\\mathbb{R}$ is compact, and $p_{\\#}\\mathcal{N}(0,1)$ is one-sided log-superlinear. ", "page_idx": 24}, {"type": "text", "text": "Proof. The first two properties follow directly. For the third, let $w$ be the density function Oof $p_{\\#}\\mathcal{N}(0,1)$ . Applying the inverse function rule to $w_{2}(x)=C_{2}\\cdot\\exp\\bigl(-|x|^{2}\\bigr)$ ,wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\nw(y)=C_{2}\\cdot\\sum_{x\\in p^{-1}(y)}\\exp\\bigl(-|x|^{2}\\bigr)\\cdot\\frac{1}{|p^{\\prime}(x)|}\\quad\\forall\\,y\\in\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We need to show that, for some $C>0$ and $\\gamma\\in(0,1/2)$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\nw(y)\\geq C\\cdot\\exp(-|y|^{\\gamma})\\quad\\forall\\,y\\in(-\\infty,1].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Qualitatively, the behavior of $w$ can be described as follows. When $y\\geq\\operatorname*{sup}_{x\\in\\mathbb{R}}p(x)\\approx17$ then $w(y)\\,=\\,0$ .When $y\\ll0$ , then $x\\,\\in\\,p^{-1}(y)$ implies $|x|\\approx|y|^{1/6}$ , and so $w(y)\\approx\\exp\\!\\left(-|y|^{1/3}\\right)$ Finally, for any $K>0$ , there is a uniform lower bound on $w(y)$ for all $y\\in[-K,1]$ . To show (18), it remains to make this description quantitative. ", "page_idx": 24}, {"type": "text", "text": "As the leading term of $p$ is $-x^{6}$ , there is a $K>0$ such that $|p^{-1}(y)|=2$ for all $y\\le-K$ and ", "page_idx": 24}, {"type": "equation", "text": "$$\nx\\in p^{-1}(y)\\implies|x|\\leq2|y|^{1/6}\\quad\\forall\\,y\\leq-K.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This means that, for a (possibly larger) constant $K^{\\prime}>0$ , and some $\\gamma\\in(1/3,1/2)$ ,wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\nw(y)\\geq2\\cdot C_{2}\\cdot\\exp\\!\\left(-4|y|^{1/3}\\right)\\cdot{\\frac{1}{|p^{\\prime}(2|y|^{1/6})|}}\\geq\\exp(-|y|^{\\gamma})\\quad\\forall y\\leq-K^{\\prime}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, there is an $M>0$ so that ", "page_idx": 24}, {"type": "equation", "text": "$$\nx\\in p^{-1}(y)\\implies|x|\\leq M\\quad\\forall\\,y\\in[-K^{\\prime},1].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It follows, as $p^{-1}(y)$ is non-empty for $y\\leq1$ , that ", "page_idx": 24}, {"type": "equation", "text": "$$\nw(y)\\geq\\operatorname*{inf}_{|x|\\leq M}\\left[C_{2}\\cdot\\exp\\bigl(-|x|^{2}\\bigr)\\cdot\\frac{1}{|p^{\\prime}(x)|}\\right]>0\\quad\\forall\\,y\\in[-K^{\\prime},1].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining (19) and (20), and choosing $C>0$ small enough, yields (18). ", "page_idx": 24}, {"type": "text", "text": "C  Technical details on the proof of testable learning ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "C.1  Facts and computations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we prove several facts that we used in the proofs in Appendix A about the moments and other properties of a Gaussian random variable $Y$ and a random variable $X$ that matches the moments of a Gaussian up to degree $k$ and slack $\\eta$ ", "page_idx": 24}, {"type": "text", "text": "First, we want to prove Fact 22, which we restate below, about the sum of the absolute values of the coefficients of the polynomials $P_{i,j}$ . Recall from Lemma 21 that the sum of the squares of the coefficients of the polynomials $P_{i,j}$ is $1$ ", "page_idx": 24}, {"type": "text", "text": "Fact (Restatement of Fact 22). For any $i\\in[n]$ and $j\\in[n_{i}]$ the sum of the absolute values of the coefficients of $P_{i,j}$ is at most $n^{d/2}$ ", "page_idx": 24}, {"type": "text", "text": "Proof. Similar to before, we write the $P_{i,j}$ as follows ", "page_idx": 24}, {"type": "equation", "text": "$$\nP_{i,j}(x)=\\sum_{\\beta}a_{i,j}^{(\\beta)}x^{\\beta}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The $\\beta$ in the sum goes over all multi-indices with $|\\beta|$ being the degree of $P_{i,j}$ , which is at most $d$ Thus, there are at most $n^{d}$ terms in the sum. By the structure theorem (Lemma 21), we know that ", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r}{\\|a_{i,j}\\|_{2}=\\sum_{\\beta}(a_{i,j}^{(\\beta)})^{2}=1}\\end{array}$ coeficients ai $n^{d}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\beta}|a_{i,j}^{(\\beta)}|=\\|a_{i,j}\\|_{1}\\leq n^{d/2}\\|a_{i,j}\\|_{2}\\leq n^{d/2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, we want to show two facts about the moments of $X$ . On the one hand, we want to show that under very mild assumptions on $\\eta$ , we can bound the moments of $X$ similar as the moments of a Gaussian $Y$ .We also prove Fact 23, which we also restate below, about the expectation of $P_{i,j}(X)$ . For the Gaussian, we get a bound by Lemma 21 and we generalize this in this fact to the moment-matching distribution. ", "page_idx": 25}, {"type": "text", "text": "Fact 41. Let $\\eta\\leq1$ and let $\\mathcal{D}^{\\prime}$ be a distribution that matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ andslack $\\eta$ Then, we have that for any multi-index $\\alpha$ with $|\\alpha|\\leq k$ we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n|{\\mathbb E}_{X\\sim{\\mathcal D^{\\prime}}}\\left[X^{\\alpha}\\right]|\\leq\\sqrt{|\\alpha|}^{|\\alpha|}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. We have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}[X^{\\alpha}]|\\leq|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[Y^{\\alpha}\\right]|+1}\\\\ &{\\qquad\\qquad\\qquad\\leq|\\mathbb{E}_{Z\\sim\\mathcal{N}(0,1)}\\left[Z^{|\\alpha|}\\right]|+1}\\\\ &{\\qquad\\qquad\\qquad\\leq1+\\left\\{0\\quad\\qquad\\qquad\\mathrm{if~|\\alpha|~is~odd}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.(|\\alpha|-1)!!\\quad\\mathrm{otherwise}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left.\\sqrt{|\\alpha|}^{|\\alpha|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which is what we wanted to proof. ", "page_idx": 25}, {"type": "text", "text": "Fact (Restatement of Fact 23). For any $i\\in[d],\\,j\\in[n_{i}]$ and $\\ell\\leq k/d$ we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{\\mathbb{E}}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P_{i,j}(X)^{\\ell}\\right]\\leq\\operatorname{\\mathbb{E}}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P_{i,j}(Y)^{\\ell}\\right]+\\eta n^{d\\ell/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Writing $P_{i,j}$ as in Fact 22 ", "page_idx": 25}, {"type": "equation", "text": "$$\nP_{i,j}(x)=\\sum_{\\beta}a_{i,j}^{(\\beta)}x^{\\beta},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "we can compute $\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P_{i,j}(X)^{\\ell}\\right]$ as follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P_{i,j}(X)^{\\ell}\\right]=\\sum_{\\beta_{1},\\ldots,\\beta_{\\ell}}a_{i,j}^{(\\beta_{1})}\\cdot\\cdot\\cdot a_{i,j}^{(\\beta_{\\ell})}\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[X^{\\beta_{1}+\\ldots\\beta_{\\ell}}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, since $X$ is $\\eta$ -approximately moment matching, we have that (note that $P_{i,j}$ has degree at most $d$ and thus any term in the sum that degree at most $d\\ell\\leq k$ and thus the moment matching applies) ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[X^{\\beta_{1}+\\ldots\\beta_{\\ell}}\\right]=\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[Y^{\\beta_{1}+\\ldots\\beta_{\\ell}}\\right]\\pm\\eta.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining this with the above, we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P_{i,j}(X)^{\\ell}\\right]\\leq\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P_{i,j}(Y)^{\\ell}\\right]+\\eta\\left(\\sum_{\\beta}\\left|a_{i,j}^{(\\beta)}\\right|\\right)^{\\ell}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Fact 22, we thus get that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P_{i,j}(X)^{\\ell}\\right]\\le\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P_{i,j}(Y)^{\\ell}\\right]+\\eta n^{d\\ell/2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "as wanted. ", "page_idx": 25}, {"type": "text", "text": "Finally, we show two facts about the Gaussian distribution. First, we want to give a bound on the moments of a Gaussian random vector that has not necessarily independent entries and for which we only know that the variances are at most 1. If $Z$ were independent, then the bound we show would follow directly by the formulas for the moments of a standard Gaussian. Second, we show Fact 36 that shows how many samples $m$ of ${\\mathcal{N}}(0,I_{n})$ we need such that the empirical moments up to degree $k$ match the actual moments of ${\\mathcal{N}}(0,I_{n})$ up to slack $\\eta$ ", "page_idx": 25}, {"type": "text", "text": "Fact 42.Let $Z$ be a (multivariate) Gaussian random variable with mean O and variances at most 1. Then,foranymulti-index $\\beta$ wehavethat ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}\\left[Z^{\\beta}\\right]\\right|\\leq|\\beta|^{|\\beta|/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We want to show that for any random variables $W_{1},\\dots,W_{\\ell}$ wehave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}\\left[\\prod_{j=1}^{\\ell}W_{j}\\right]\\right)^{\\ell}\\leq\\prod_{j=1}^{\\ell}\\mathbb{E}\\left[W_{j}^{\\ell}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To prove this, we use induction and Holder's inequality. The case $\\ell=1$ follows directly and for $\\ell\\geq2$ we can compute using Holder's inequality, since $\\begin{array}{r}{\\frac{\\overline{{\\epsilon}}}{\\frac{\\ell}{\\ell-1}}+\\frac{1}{\\ell}=1}\\end{array}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\prod_{j=1}^{\\ell}W_{j}\\right]\\le\\left(\\mathbb{E}\\left[\\prod_{j=1}^{\\ell-1}W_{j}^{\\frac{\\ell}{\\ell-1}}\\right]\\right)^{\\frac{\\ell-1}{\\ell}}\\cdot\\left(\\mathbb{E}\\left[W_{\\ell}^{\\ell}\\right]\\right)^{\\frac{1}{\\ell}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, we get, using the induction hypothesis ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}\\left[\\prod_{j=1}^{\\ell}W_{j}\\right]\\right)^{\\ell}\\leq\\left(\\mathbb{E}\\left[\\prod_{j=1}^{\\ell-1}W_{j}^{\\frac{\\ell}{\\ell-1}}\\right]\\right)^{\\ell-1}\\cdot\\mathbb{E}\\left[W_{\\ell}^{\\ell}\\right]\\leq\\prod_{j=1}^{\\ell}\\mathbb{E}\\left[W_{j}^{\\ell}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We now use this result with $\\ell=|\\beta|$ and the $W_{1},\\dots,W_{\\ell}$ being the $Z_{j}$ , where every entry $Z_{j}$ occurs $\\beta_{j}$ times. Then we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\left[Z^{\\beta}\\right]\\right|\\leq\\left(\\prod_{j}\\left(\\mathbb{E}\\left[Z_{j}^{\\ell}\\right]\\right)^{\\beta_{j}}\\right)^{\\frac{1}{\\ell}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\mathbb{E}\\left[Z_{j}\\right]=0$ and $\\mathrm{Var}\\left[Z_{j}\\right]\\leq1$ , we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[Z_{j}^{\\ell}\\right]\\leq\\ell^{\\ell/2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and thus we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\left[Z^{\\beta}\\right]\\right|\\leq\\ell^{\\ell/2}=|\\beta|^{|\\beta|/2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Fact (Restatement of Fact 36). Given $m\\geq\\Omega((2k n)^{k}\\eta^{-2})$ samples of ${\\mathcal{N}}(0,I_{n}).$ we have that with high probability the empirical distribution matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta$ ", "page_idx": 26}, {"type": "text", "text": "Proof. Given $m$ samples from ${\\mathcal{N}}(0,I_{n})$ , we want to compute the probability that for some $\\alpha$ with $|\\alpha|\\leq k$ we have that the empirical moment is close to the moment with high probability. We can compute using Chebyshev's inequality, where $c_{\\alpha}$ is the $\\alpha$ -moment of ${\\mathcal{N}}(0,I_{n})$ and $\\hat{c}_{\\alpha}$ is the empirical moment,that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\vert\\hat{c}_{\\alpha}-c_{\\alpha}\\vert>\\eta\\right]\\leq\\frac{\\mathrm{Var}\\left[\\hat{c}_{\\alpha}\\right]}{\\eta^{2}}=\\frac{1}{m}\\frac{\\mathrm{Var}\\left[Y^{\\alpha}\\right]}{\\eta^{2}}\\leq\\frac{1}{m}\\frac{\\mathrm{Var}\\left[Y_{1}^{\\vert\\alpha\\vert}\\right]}{\\eta^{2}}\\leq\\frac{1}{m}\\frac{(2\\vert\\alpha\\vert)^{\\vert\\alpha\\vert}}{\\eta^{2}}\\leq\\frac{1}{m}\\frac{(2k)^{k}}{\\eta^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To be able to use a union-bound, we need this to be smaller than $O(n^{-k})$ , i.e. we need ", "page_idx": 26}, {"type": "equation", "text": "$$\nm\\geq\\Omega\\left(\\frac{(2k n)^{k}}{\\eta^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "C.2  Remaining proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we prove several lemmas that follow closely [21]. Some of these lemmas are also proven in [21], but we include them here for completeness. We first prove Lemmas 26 and 27, which we need in order to show that the expectation of $\\tilde{f}$ is close under the moment-matching distribution and the Gaussian distribution. ", "page_idx": 26}, {"type": "text", "text": "Lemma (Restatement of Lemma 26). Let $\\varepsilon\\:>\\:0$ Suppose that $\\mathcal{D}^{\\prime}$ approximately matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta$ where $\\bar{k}\\geq\\Omega_{d}\\left(\\varepsilon^{-4d\\cdot\\bar{7}^{\\bar{d}}}\\right)$ and $\\begin{array}{r}{\\eta\\leq\\frac{1}{k d}}\\end{array}$ . Then, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[|\\tilde{f}(X)-T(P(X))|\\right]\\leq O(\\varepsilon)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As mentioned earlier, this proof follows closely [21, Proof of Proposition 8]. In particular, we also need the following lemma from [21]. ", "page_idx": 27}, {"type": "text", "text": "Lemma 43 ([21, Proposition 6]). We have that, for $x\\in\\mathbb{R}^{n}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n|T(P(x))-\\tilde{F}(P(x))|\\leq\\prod_{i=1}^{d}\\left(1+\\frac{C_{i}^{m_{i}}\\|P_{i}(x)\\|_{2}^{m_{i}}}{m_{i}!}\\right)-1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The $C_{i}$ and $m_{i}$ are again as in [21], i.e. we choose $C_{i}=\\Theta_{d}\\left(\\varepsilon^{-7^{i}d}\\right)$ and $m_{i}=\\Theta_{d}\\left(\\varepsilon^{-3\\cdot7^{i}d}\\right).$ For the proof, we furthermore need the bound on the expectation of $|P_{i,j}(X)|$ from Fact 23. ", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma 26. We have by Lemma 43 that ", "page_idx": 27}, {"type": "equation", "text": "$$\n|T(P(x))-\\tilde{F}(P(x))|\\leq\\prod_{i=1}^{d}\\left(1+\\frac{C_{i}^{m_{i}}\\|P_{i}(x)\\|_{2}^{m_{i}}}{m_{i}!}\\right)-1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As in [21, Proof of Proposition 8], the RHS of this inequality is the sum over all non-empty subsets $S\\subseteq\\{1,2,\\ldots,d\\}$ of the following term ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\prod_{i\\in S}\\left(\\frac{C_{i}^{m_{i}}\\Vert P_{i}(x)\\Vert_{2}^{m_{i}}}{m_{i}!}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Continuing exactly as in [21, Proof of Proposition 8], we want to show that any of these $2^{d}-1$ terms is at most $\\breve{O}(\\varepsilon/2^{\\bar{d}})$ . By the inequality of arithmetic and geometric means, we get that this term is at most ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{|S|}\\sum_{i\\in S}\\left(\\frac{C_{i}^{m_{i}}\\|P_{i}(x)\\|_{2}^{m_{i}}}{m_{i}!}\\right)^{|S|}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, it remains to bound $\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\|P_{i}(X)\\|_{2}^{\\ell_{i}}\\right];$ where $\\ell_{i}=m_{i}|S|$ . We do this as follows ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\|P_{i}(X)\\|_{2}^{\\ell_{i}}\\right]\\leq\\sqrt{\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\|P_{i}(X)\\|_{2}^{2\\ell_{i}}\\right]}}\\\\ &{\\qquad\\qquad\\qquad=\\sqrt{\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\left(\\displaystyle\\sum_{j=1}^{n_{i}}P_{i,j}(X)^{2}\\right)^{\\ell_{i}}\\right]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we used the Cauchy-Schwarz inequality in the first step. Continuing further by applying Jensen's inequality to the (convex) function $g(\\bar{a})=a^{\\ell_{i}}$ ,weget ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left(\\sum_{j=1}^{n_{i}}P_{i,j}(X)^{2}\\right)^{\\ell_{i}}=n_{i}^{\\ell_{i}}\\left({\\frac{1}{n_{i}}}\\sum_{j=1}^{n_{i}}P_{i,j}(X)^{2}\\right)^{\\ell_{i}}\\leq n_{i}^{\\ell_{i}}{\\frac{1}{n_{i}}}\\sum_{j=1}^{n_{i}}P_{i,j}(X)^{2\\ell_{i}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining these two, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\|P_{i}(X)\\|_{2}^{\\ell_{i}}\\right]\\le\\sqrt{n_{i}^{\\ell_{i}}\\frac{1}{n_{i}}\\sum_{j=1}^{n_{i}}\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P_{i,j}(X)^{2\\ell_{i}}\\right]}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The next step is now different to [21] since we only have an approximately moment-matching distribution. By Fact 23, we get that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{\\mathbb{E}}_{X\\sim\\mathcal{D}^{\\prime}}\\left[P_{i,j}(X)^{2\\ell_{i}}\\right]\\leq\\operatorname{\\mathbb{E}}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P_{i,j}(Y)^{2\\ell_{i}}\\right]+\\eta n^{d\\ell_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "assuming that $k\\,\\geq\\,2d^{2}m_{i}$ (this ensures that $2\\ell_{i}=2m_{i}|S|\\leq2m_{i}d$ is at most $k/d$ as required in Fact 23). This condition is slightly different to [21], but it does not change the (asymptotic) definition of $k$ as in (7). By Lemma 21, we now have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[P_{i,j}(Y)^{2\\ell_{i}}\\right]\\leq O_{d}\\left(\\sqrt{2\\ell_{i}}\\right)^{2\\ell_{i}}=O_{d}\\left(\\sqrt{\\ell_{i}}\\right)^{2\\ell_{i}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for any $j$ . Combining this with the above, we can conclude that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\|P_{i}(X)\\|_{2}^{\\ell_{i}}\\right]\\leq\\sqrt{n_{i}}^{\\ell_{i}}O_{d}\\left(\\sqrt{\\ell_{i}}\\right)^{\\ell_{i}}+\\sqrt{n_{i}^{\\ell_{i}}\\eta n^{d\\ell_{i}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq O_{d}\\left(\\sqrt{n_{i}m_{i}|S|}\\right)^{m_{i}|S|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "usingthat $\\begin{array}{r}{\\eta\\leq\\frac{1}{n^{d\\ell_{i}}}}\\end{array}$ This istrue since $\\ell_{i}=m_{i}|S|\\leq m_{i}d\\leq k$ and thus $\\begin{array}{r}{\\eta\\leq\\frac{1}{n^{d k}}}\\end{array}$ implies $\\begin{array}{r}{\\eta\\leq\\frac{1}{n^{d\\ell_{i}}}}\\end{array}$ The rest of this proof is again the same as in [21, Proof of Proposition 8]. We can compute ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\vert T(P(X))-\\tilde{F}(P(X))\\vert\\right]}\\\\ &{\\le2^{d}\\underset{0\\neq\\delta\\subseteq\\mathbb{Z}[d]}{\\operatorname*{max}}\\left\\lbrace\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\frac{1}{\\vert\\mathcal{S}\\vert}\\sum_{i\\in\\mathcal{S}}\\left(\\frac{C_{i}^{m_{i}}\\vert\\vert P_{i}(X)\\vert\\vert_{2}^{m_{i}}}{m_{i}!}\\right)^{\\vert S\\vert}\\right]\\right\\rbrace}\\\\ &{\\le2^{d}\\underset{0\\neq\\delta\\subseteq\\mathbb{Z}[d]}{\\operatorname*{max}}\\left\\lbrace\\underset{i\\in\\mathcal{S}}{\\operatorname*{max}}\\frac{C_{i}^{m_{i}\\vert\\vert S\\vert}O_{d}\\,\\left(\\sqrt{n_{i}m_{i}\\vert S\\vert}\\right)^{m_{i}\\vert S\\vert}}{m_{i}^{m_{i}\\vert S\\vert}}\\right\\rbrace}\\\\ &{\\le2^{d}\\underset{i\\in\\mathcal{I}[d],s\\in\\mathbb{Z}[d]}{\\operatorname*{max}}\\left\\lbrace O_{d}\\left(\\frac{C_{i}\\sqrt{n_{i}}}{\\sqrt{m_{i}}}\\right)^{m_{i}s}\\right\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By choice of $m_{i}$ , exactly as in [21], we can conclude that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\left|T(P(X))-\\tilde{f}(P(X))\\right|\\right]\\le2^{d}\\exp\\biggl(-\\operatorname*{min}_{i\\in[d]}m_{i}\\biggr)\\le O(\\varepsilon),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which completes the proof. ", "page_idx": 28}, {"type": "text", "text": "Lemma (Restatement of Lemma 27). For any multi-index $\\alpha=(\\alpha_{1},\\ldots,\\alpha_{d})\\in\\mathbb{N}^{n_{1}}\\times\\cdot\\cdot\\cdot\\times\\mathbb{N}^{n_{d}}$ wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\partial^{\\alpha}\\tilde{F}(0)\\leq\\prod_{i=1}^{d}C_{i}^{|\\alpha_{i}|}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. As mentioned earlier, the ideas of the following proof are based on [21, Proof of Lemmas 5 and 7]. Wehave that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|\\partial^{\\alpha}(F*\\rho)|=|(F*\\partial^{\\alpha}\\rho)(0)|\\qquad\\qquad}&{\\mathrm{(property~of~convolution)}}\\\\ {\\leq\\|F\\|_{L^{\\infty}}\\|\\partial^{\\alpha}\\rho\\|_{L^{1}}\\qquad\\qquad}&{\\qquad}\\\\ {\\leq\\|\\partial^{\\alpha}\\rho\\|_{L^{1}}\\qquad}&{\\qquad}&{(F\\,\\mathrm{maps~to}\\,[-1,1]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, it remains to bound $\\|\\partial^{\\alpha}\\rho\\|_{L^{1}}$ . Using the product structure of $\\rho$ , we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\partial^{\\alpha}\\rho\\|_{L^{1}}=\\prod_{i=1}^{n}\\|\\partial^{\\alpha_{i}}\\rho_{C_{i}}\\|_{L^{1}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "First, we compute a bound for $\\|\\partial^{\\alpha_{i}}\\rho_{2}\\|_{L^{1}}$ . We generalize this afterwards to $\\rho_{C_{i}}$ . Recall that ", "page_idx": 28}, {"type": "equation", "text": "$$\nB(\\xi)=\\left\\{{1-\\|\\xi\\|_{2}^{2}\\quad\\mathrm{if}\\;\\|\\xi\\|_{2}\\leq1}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\rho_{2}(x)=\\frac{|\\hat{B}(x)|^{2}}{||B||_{L^{2}}^{2}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\hat{B}$ is the Fourier transform of $B$ . We first note that ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\hat{B}}(x)\\cdot{\\overline{{{\\hat{B}}(x)}}}=|{\\hat{B}}(x)|^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, we can apply the product rule to get the following formula for the derivative ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial^{\\alpha_{i}}\\rho_{2}=\\frac{1}{\\|B\\|_{L^{2}}^{2}}\\displaystyle\\sum_{\\beta\\leq\\alpha_{i}}\\binom{\\alpha_{i}}{\\beta}(\\partial^{\\beta}\\hat{B})(\\partial^{\\alpha_{i}-\\beta}\\overline{{\\hat{B}}})}\\\\ {=\\frac{1}{\\|B\\|_{L^{2}}^{2}}\\displaystyle\\sum_{\\beta\\leq\\alpha_{i}}\\binom{\\alpha_{i}}{\\beta}(\\partial^{\\beta}\\hat{B})(\\overline{{\\partial^{\\alpha_{i}-\\beta}\\hat{B}}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we used that the conjugate of the derivative is the derivative of the conjugate. We thus get the following by triangle inequality ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|(\\partial^{\\alpha_{i}}\\rho_{2})(x)\\|_{L^{1}}\\leq\\frac{1}{\\|B\\|_{L^{2}}^{2}}\\sum_{\\beta\\leq\\alpha_{i}}{\\binom{\\alpha_{i}}{\\beta}}\\left|\\left|(\\partial^{\\beta}\\hat{B})(x)\\overline{{(\\partial^{\\alpha_{i}-\\beta}\\hat{B})(x)}}\\right|\\right|_{L^{1}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We now want to analyze $(\\partial^{\\beta}{\\hat{B}})(x)$ . We have ", "page_idx": 29}, {"type": "equation", "text": "$$\n(\\partial^{\\beta}\\hat{B})(x)=\\mathcal{F}(x^{\\beta}B(x)),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\mathcal{F}(\\cdot)=\\hat{\\cdot}$ stands for the Fourier transform and we used a fact about the derivative of the Fourier transform. Thus, we get furthermore that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|(\\partial^{\\alpha_{i}}\\rho_{2})(x)\\|_{L^{1}}\\leq\\displaystyle\\frac{1}{\\|B\\|_{L^{2}}^{2}}\\sum_{\\beta\\leq\\alpha_{i}}\\binom{\\alpha_{i}}{\\beta}\\left\\|\\mathcal{F}(x^{\\beta}B(x))\\overline{{\\mathcal{F}(x^{\\alpha_{i}-\\beta}B(x))}}\\right\\|_{L^{1}}}&{}\\\\ {=\\displaystyle\\frac{1}{\\|B\\|_{L^{2}}^{2}}\\sum_{\\beta\\leq\\alpha_{i}}\\binom{\\alpha_{i}}{\\beta}\\left\\|x^{\\beta}B(x)\\overline{{x^{\\alpha_{i}-\\beta}B(x)}}\\right\\|_{L^{1}}}&{}\\\\ {\\leq\\displaystyle\\frac{1}{\\|B\\|_{L^{2}}^{2}}\\sum_{\\beta\\leq\\alpha_{i}}\\binom{\\alpha_{i}}{\\beta}\\left\\|B(x)\\overline{{B(x)}}\\right\\|_{L^{1}}}&{}\\\\ {=\\displaystyle\\sum_{\\beta\\leq\\alpha_{i}}\\binom{\\alpha_{i}}{\\beta}}&{}\\\\ {=2^{|\\alpha_{i}|},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The second step uses the Parseval identity; the third step uses that $B(x)~\\ne~0$ implies that $\\|x\\|_{\\infty}\\leq\\|x\\|_{2}\\leq1$ ; the fourth step uses that $\\|B(x)\\overline{{B(x)}}\\|_{L^{1}}=\\|B(x)\\|_{L^{2}}^{2}$ ", "page_idx": 29}, {"type": "text", "text": "For an arbitrary $C_{i}$ , we can bound $\\|\\partial^{\\alpha_{i}}\\rho_{C_{i}}\\|_{L^{1}}$ as follows. Recall that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\rho_{C_{i}}(x)=\\left(\\frac{C_{i}}{2}\\right)^{n}\\rho_{2}\\left(\\frac{C_{i}}{2}x\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We can compute, using the chain rule that ", "page_idx": 29}, {"type": "equation", "text": "$$\n(\\partial^{\\alpha}\\rho_{C_{i}})(x)=\\left(\\frac{C_{i}}{2}\\right)^{n}(\\partial^{\\alpha}\\rho_{2})\\left(\\frac{C_{i}}{2}x\\right)\\left(\\frac{C_{i}}{2}\\right)^{|\\alpha_{i}|}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "To illustrate how the chain rule implies this, we can compute ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\displaystyle\\frac{\\partial}{\\partial x_{1}}\\rho_{C_{i}}\\right)(x)=\\left(\\displaystyle\\frac{C_{i}}{2}\\right)^{n}\\sum_{j=1}^{n}\\left(\\displaystyle\\frac{\\partial}{\\partial x_{j}}\\rho_{2}\\right)\\left(\\displaystyle\\frac{C_{i}}{2}x\\right)\\left(\\displaystyle\\frac{\\partial}{\\partial x_{1}}\\left[x\\mapsto\\displaystyle\\frac{C_{i}}{2}x_{j}\\right]\\right)(x)}\\\\ &{\\qquad\\qquad\\qquad=\\left(\\displaystyle\\frac{C_{i}}{2}\\right)^{n}\\sum_{j=1}^{n}\\left(\\displaystyle\\frac{\\partial}{\\partial x_{j}}\\rho_{2}\\right)\\left(\\displaystyle\\frac{C_{i}}{2}x\\right)\\cdot\\left\\{\\displaystyle\\frac{C_{i}}{0}\\quad\\mathrm{if~}j=1\\right.}\\\\ &{\\qquad\\qquad\\qquad=\\left.\\left(\\displaystyle\\frac{C_{i}}{2}\\right)^{n+1}\\left(\\displaystyle\\frac{\\partial}{\\partial x_{1}}\\rho_{2}\\right)\\left(\\displaystyle\\frac{C_{i}}{2}x\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Doing this iteratively, we get the formula above. Now, we want to compute the $L^{1}$ -norm of that function. We get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\partial^{\\alpha_{i}}\\rho_{C_{i}}\\|_{1}=\\int_{\\mathbb R^{n}}(\\partial^{\\alpha_{i}}\\rho_{C_{i}})(x)\\,\\mathrm d x}}\\\\ &{=\\left(\\frac{C_{i}}{2}\\right)^{n+|\\alpha_{i}|}\\int_{\\mathbb R^{n}}(\\partial^{\\alpha_{i}}\\rho_{2})\\left(\\frac{C_{i}}{2}x\\right)\\mathrm d x}\\\\ &{=\\left(\\frac{C_{i}}{2}\\right)^{|\\alpha_{i}|}\\int_{\\mathbb R^{n}}(\\partial^{\\alpha_{i}}\\rho_{2})(y)\\,\\mathrm d y}\\\\ &{=\\left(\\frac{C_{i}}{2}\\right)^{|\\alpha_{i}|}\\|\\partial^{\\alpha_{i}}\\rho_{2}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left(y=\\frac{C_{i}}{2}x\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Using the bound from above on $\\lVert\\partial^{\\alpha_{i}}\\rho_{2}\\rVert_{1}$ , we thus get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\lVert\\partial^{\\alpha_{i}}\\rho_{C_{i}}\\rVert_{1}\\leq C_{i}^{|\\alpha_{i}|}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining (21) and (22) completes the proof. ", "page_idx": 30}, {"type": "text", "text": "Next, we prove how we can generalize from ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\tilde{f}(X)\\right]\\right|\\le O(\\varepsilon)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "to the fooling condition we need ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right]\\right|\\leq O_{d}(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This proof is based on [21, Proof of Proposition 2]. It turns out that this part of [21] does not need that $X$ is $k$ -Gaussian but works on the following, weaker assumptions. ", "page_idx": 30}, {"type": "text", "text": "Lemma (Restatement of Lemma 29). Let $\\varepsilon>0$ Suppose that $\\mathcal{D}^{\\prime}$ is a distribution such that the followingholds ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\tilde{f}(X)\\right]\\right|\\leq O(\\varepsilon),\\,a n d}\\\\ &{\\bullet\\ \\mathbb{P}_{X\\sim\\mathcal{D}^{\\prime}}\\left[\\exists i,j:|P_{i,j}(X)|>B_{i}\\right]\\leq O(\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right]|\\le O_{d}(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In the proof of this lemma, we use the following theorem about anti-concentration of a polynomial of a Gaussian random variable. Importantly, it is enough to have this result for the Gaussian random variable $Y$ and we do not need it for the moment-matching distribution. ", "page_idx": 30}, {"type": "text", "text": "Theorem 44 (Carbery and Wright, see [21, Theorem 13] or [7, Theorem 8]). Let p be a degree $d$ polynomial.Supposethat $\\mathbb{E}_{Y\\sim\\tilde{\\mathcal{N}}(0,I_{n})}\\left[\\bar{p}(Y)^{2}\\right]=1$ Then,for $\\varepsilon>0$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[|p(Y)|<\\varepsilon\\right]\\le O_{d}(d\\varepsilon^{1/d}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma 29. Note that [21, Lemma 12 and proof of Proposition 14] and the second condition in the lemma imply that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[f(X)\\right]\\geq\\mathbb{E}\\left[\\tilde{f}(X)\\right]-O(\\varepsilon)-2\\mathbb{P}\\left[-O_{d}(\\varepsilon^{d})<p(X)<0\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[f(X)\\right]\\leq\\mathbb{E}\\left[\\tilde{f}(X)\\right]+O(\\varepsilon)+2\\mathbb{P}\\left[0<p(X)<O_{d}(\\varepsilon^{d})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Using these and the first assumption of the lemma we get that ", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f(X)\\right]\\geq\\mathbb{E}\\left[f(Y)\\right]-O(\\varepsilon)-2\\mathbb{P}\\left[-O_{d}(\\varepsilon^{d})<p(X)<0\\right]}\\\\ &{}\\\\ &{\\mathbb{E}\\left[f(X)\\right]\\leq\\mathbb{E}\\left[f(Y)\\right]+O(\\varepsilon)+2\\mathbb{P}\\left[0<p(X)<O_{d}(\\varepsilon^{d})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We have furthermore that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{sign}(p(X))\\right]+2\\mathbb{P}\\left[-O_{d}(\\varepsilon^{d})<p(X)<0\\right]=\\mathbb{E}\\left[\\mathrm{sign}(p(X)+O_{d}(\\varepsilon^{d})\\right]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{sign}(p(X))\\right]-2\\mathbb{P}\\left[0<p(X)<O_{d}(\\varepsilon^{d})\\right]=\\mathbb{E}\\left[\\mathrm{sign}(p(X)-O_{d}(\\varepsilon^{d})\\right].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The reason for this is that adding or subtracting the two probability terms can be interpreted as changing the sign for values of $X$ in $(-O_{d}(\\varepsilon^{d}),0)$ respectively $(0,\\dot{O}_{d}(\\varepsilon^{d}))$ ,which the same as shifting the polynomial. Thus, when combining this with the above we get that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname{sign}(p(X)+O_{d}(\\varepsilon^{d}))\\right]\\geq\\mathbb{E}\\left[\\operatorname{sign}(p(Y))\\right]-O(\\varepsilon)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\operatorname{sign}(p(X)-O_{d}(\\varepsilon^{d}))\\right]\\leq\\mathbb{E}\\left[\\operatorname{sign}(p(Y))\\right]+O(\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now we apply this result not to the polynomial $p$ , but to the polynomial $p\\mp O_{d}(\\varepsilon^{d})$ . This shifts the additional factor from the $X$ -side to the $Y$ -side and we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\mathrm{sign}(p(X))\\right]\\geq\\mathbb{E}\\left[\\mathrm{sign}(p(Y)-O_{d}(\\varepsilon^{d}))\\right]-O(\\varepsilon)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "as well as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\mathrm{sign}(p(X))\\right]\\leq\\mathbb{E}\\left[\\mathrm{sign}(p(Y)+O_{d}(\\varepsilon^{d}))\\right]+O(\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining these two inequalities we get that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{sign}(p(Y)-O_{d}(\\varepsilon^{d}))\\right]-O(\\varepsilon)\\leq\\mathbb{E}\\left[f(X)\\right]\\leq\\mathbb{E}\\left[\\mathrm{sign}(p(Y)+O_{d}(\\varepsilon^{d}))\\right]+O(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For $Y$ , we have that (since the inequality hold point-wise) ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\operatorname{sign}(p(Y)-O_{d}(\\varepsilon^{d}))\\right]\\leq\\mathbb{E}\\left[f(Y)\\right]\\leq\\mathbb{E}\\left[\\operatorname{sign}(p(Y)+O_{d}(\\varepsilon^{d}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now, the two function $\\mathrm{sign}(p(Y)-O_{d}(\\varepsilon^{d}))$ and $\\mathrm{sign}(p(Y)+O_{d}(\\varepsilon^{d}))$ differ by at most 2 and only when $|p(Y)|\\,\\leq\\,O_{d}(\\varepsilon^{d})$ . We now use an anti-concentration result for $Y$ (the standard Gaussian). Namely, we can use Theorem 44 to conclude that this happens with probability at most $O_{d}(\\varepsilon)$ Note that we have $\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[p(Y)^{2}\\right]=1$ since we assumed that the sum of the squares of the coefficients of $p$ , which is exactly $\\mathbb{E}_{Y\\sim\\bar{N}(0,I_{n})}\\left[p(Y)^{2}\\right]$ for multilinear $p$ , is 1. Thus, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}\\left[\\mathrm{sign}(p(Y)+O_{d}(\\varepsilon^{d}))\\right]-\\mathbb{E}\\left[\\mathrm{sign}(p(Y)-O_{d}(\\varepsilon^{d}))\\right]\\right|\\le2O_{d}(\\varepsilon)=O_{d}(\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, since both $\\mathbb{E}\\left[f(X)\\right]$ and $\\mathbb{E}\\left[f(Y)\\right]$ are between the values $\\mathbb{E}\\left[\\mathrm{sign}(p(Y)-O_{d}(\\varepsilon^{d}))\\right]$ and $\\mathbb{E}\\left[\\mathrm{sign}(p(Y)+O_{d}(\\varepsilon^{d}))\\right]$ (up to $O(\\varepsilon)$ for $\\mathbb{E}\\left[f(X)\\right])$ , we can conclude that also ", "page_idx": 31}, {"type": "equation", "text": "$$\n|\\mathbb{E}\\left[f(X)\\right]-\\mathbb{E}\\left[f(Y)\\right]|\\leq O_{d}(\\varepsilon),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "as wanted. ", "page_idx": 31}, {"type": "text", "text": "Next, we want to prove Proposition 30 that follows closely [21, Proof of Theorem 1]. This proposition shows the fooling condition for arbitrary PTFs. In the proof we need Proposition 20 about the fooling condition for multilinear PTFs and Lemma 31 that, given an arbitrary PTF $p$ , constructs a multilinear PTF $p_{\\delta}$ that is closeto $p$ ", "page_idx": 31}, {"type": "text", "text": "Proposition (Restatement of Proposition 30). Let $\\varepsilon>0$ .Suppose that $\\mathcal{D}^{\\prime}$ approximatelymatches the momentsof ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta,$ where $k\\geq\\Omega_{d}\\big(\\varepsilon^{-4d\\cdot7^{d}}\\big)$ and $\\eta\\le n^{-\\Omega_{d}(k)}k^{-\\Omega_{d}(k)}$ Then, we have that for any $f\\in\\mathcal{F}_{\\mathrm{PTF},d}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{Y\\sim\\mathcal{N}(0,I_{n})}\\left[f(Y)\\right]-\\mathbb{E}_{X\\sim\\mathcal{D}^{\\prime}}\\left[f(X)\\right]\\right|\\le O_{d}(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. As already before, we assume without loss of generality that the polynomial is normalized in the sense that the sum of the squares of the coefficients is 1 (this does not change the PTF). We set ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\delta=\\left(\\frac{\\varepsilon}{d}\\right)^{d}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We first want to show that the condition on $\\eta$ in this lemma ensures that we can apply both Lemma 31 to construct the multilinear polynomial $p_{\\delta}$ and Proposition 20 to get the fooling condition for $p_{\\delta}$ ", "page_idx": 31}, {"type": "text", "text": "The condition for Lemma 31 is $\\eta\\,\\le\\,\\delta^{O_{d}(1)}n^{-\\Omega_{d}(1)}k^{-k}$ . Note that by our choice of $k$ , we have $\\varepsilon^{O_{d}(1)}\\leq k^{-\\Omega_{d}(1)}$ . Thus, for our choice of $\\delta$ , the condition on $\\eta$ needed for Lemma 31 is satisfed by our assumption on $\\eta$ in this lemma. For Proposition 20, we need $\\begin{array}{r}{\\hat{\\eta}=(2k)^{k/2}\\eta\\le k^{-\\Omega_{d}(k)}n^{-\\Omega_{d}(k)}}\\end{array}$ which is also satisfied by our condition on $\\eta$ .Note that we need this condition for $\\hat{\\eta}$ (and not $\\eta$ ) since the new random variable $\\hat{X}$ is only moment-matching up to slack $\\hat{\\eta}$ ", "page_idx": 32}, {"type": "text", "text": "We now want to show that $|\\mathbb{P}\\left[p(X)>0\\right]-\\mathbb{P}\\left[p(Y)>0\\right]|\\le O_{d}(\\varepsilon)$ . Note that ", "page_idx": 32}, {"type": "equation", "text": "$$\n=\\mathbb{P}\\left[p(X)\\geq0\\right]-(1-\\mathbb{P}\\left[p(X)\\geq0\\right])\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$=2\\mathbb{P}\\left[p(X)\\geq0\\right]-1$ ", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and the same holds for $Y$ . Thus, $|\\mathbb{P}[p(X)\\geq0]-\\mathbb{P}\\left[p(Y)\\geq0\\right]|\\leq O_{d}(\\varepsilon)$ will be enough since then $\\begin{array}{r}{\\vert\\mathbb{E}\\left[\\mathrm{sign}(p(Y))\\right]-\\mathbb{E}\\left[\\mathrm{sign}(p(X))\\right]\\vert=2\\left\\vert\\mathbb{P}\\left[p(Y)\\ge0\\right]-\\mathbb{P}\\left[p(X)\\ge0\\right]\\right\\vert\\le O_{d}(\\varepsilon)}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "By Lemma 31, we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[{p}(X)\\ge0\\right]\\ge\\mathbb{P}\\left[p_{\\delta}(\\hat{X})\\ge\\delta\\right]-\\delta.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $p_{\\delta}$ is multilinear, we can apply Proposition 20 to $p_{\\delta}-\\delta$ to get that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[{p(X)\\geq0}\\right]\\geq\\mathbb{P}\\left[{p_{\\delta}(\\hat{Y})\\geq\\delta}\\right]-O_{d}(\\varepsilon)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "since $\\delta\\leq O(\\varepsilon)$ . Note that we can apply Proposition 20 to $\\mathbb{P}\\left[p(X)\\geq0\\right]$ instead of $\\mathbb{E}\\left[\\mathrm{sign}(p(X))\\right]$ by the relation $\\mathbb{E}[\\mathrm{sign}(p(X))]=2\\mathbb{P}\\bar{[}p(X)\\stackrel{\\cdot}{\\geq}0]-1$ from above. By Carbery-Wright (Theorem 44), we get that $\\mathbb{P}\\left[|p_{\\delta}(\\hat{Y})|\\le\\delta\\right]\\le O(\\varepsilon)$ , thus we further have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[p(X)\\geq0\\right]\\geq\\mathbb{P}\\left[p_{\\delta}(\\hat{Y})\\geq-\\delta\\right]-O_{d}(\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Applying again Lemma 31, we get that ", "page_idx": 32}, {"type": "equation", "text": "$\\mathbb{P}\\left[p(X)\\geq0\\right]\\geq\\mathbb{P}\\left[p(Y)\\geq0\\right]-O_{d}(\\varepsilon)$ ", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By an analogous calculation, we get that $\\mathbb{P}\\left[p(X)\\geq0\\right]\\leq\\mathbb{P}\\left[p(Y)\\geq0\\right]+O_{d}(\\varepsilon)$ , which completes the proof. ", "page_idx": 32}, {"type": "text", "text": "We now want to prove Lemmas 32 and 33 that show that the construction of $\\hat{X}$ respectively $\\hat{Y}$ preserve the assumptions on the distribution. The latter lemma is also proved in [21, Lemma 15], but we include it here for completeness. ", "page_idx": 32}, {"type": "text", "text": "Lemma (Restatement of Lemma 32). Suppose $X$ approximately matches themoments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta$ Then, $\\hat{X}$ approximately matches themoments of $\\mathcal{N}(0,I_{n N})$ up to degree $k$ and slack $\\hat{\\eta}=(2k)^{k/2}\\eta$ ", "page_idx": 32}, {"type": "text", "text": "Proof. For a multi-index $\\alpha$ , let $c_{\\alpha}$ be the $\\alpha$ -moment of a Gaussian. We want to show that $\\left|\\mathbb{E}\\left[\\hat{X}^{\\alpha}\\right]-c_{\\alpha}\\right|\\leq\\hat{\\eta}$ for any $\\alpha$ with $|\\alpha|\\leq k$ We can cmute the follwin, by wiing $Z_{i,j}=Z_{j}^{(i)}$ \uff0c ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\hat{X}^{\\alpha}\\right]-c_{\\alpha}\\Bigg|=\\Bigg|\\mathbb{E}\\left[\\underset{s=1}{\\prod}\\underset{j=1}{\\overset{N}{\\prod}}\\hat{X}_{i,j}^{\\alpha,*}\\right]-c_{\\alpha}\\Bigg|}\\\\ {=\\Bigg|\\mathbb{E}\\left[\\underset{l=1}{\\overset{N}{\\prod}}\\left(\\frac{1}{\\sqrt{N}}\\,X_{i}+\\mathcal{Z}_{i,j}\\right)^{\\alpha,*}\\right]-c_{\\alpha}\\Bigg|}\\\\ {=\\Bigg|\\mathbb{E}\\left[\\underset{s=1}{\\overset{N}{\\prod}}\\underset{j=1}{\\overset{N}{\\prod}}\\left(\\underset{r=0}{\\overset{N}{\\prod}}\\right)\\left(\\frac{1}{\\sqrt{N}}X_{i}^{\\alpha,*}\\right)^{r}\\frac{Z^{\\alpha,*}}{Z_{i,j}^{\\alpha,*}}\\Bigg|-c_{\\alpha}}\\\\ {=\\Bigg|\\mathbb{E}\\left[\\underset{s\\geq\\infty}{\\sum}\\left(\\hat{\\alpha}\\right)\\left(\\frac{X}{\\sqrt{N}}\\right)^{\\beta}Z^{\\alpha,*}\\right)-c_{\\alpha}\\Bigg|}\\\\ {=\\Bigg|\\underset{s\\geq\\infty}{\\sum}\\left(\\hat{\\alpha}\\right)\\mathbb{E}\\left[\\left(\\frac{X}{\\sqrt{N}}\\right)^{\\beta}\\Bigg]\\mathbb{E}\\left[Z^{\\alpha,*}\\right)-c_{\\alpha}\\Bigg|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In the second step we used the definition of $\\hat{X}$ and in the last step, we used that $Z$ and $X$ are independent. Now, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\left[\\left({\\frac{X}{\\sqrt{N}}}\\right)^{\\beta}\\right]-\\mathbb{E}\\left[\\left({\\frac{Y}{\\sqrt{N}}}\\right)^{\\beta}\\right]\\right|\\leq\\eta\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "since $X$ matches the moments of ${\\mathcal{N}}(0,I_{n})$ up to degree $k$ and slack $\\eta$ . Thus, we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\hat{X}^{\\alpha}\\right]-c_{\\alpha}\\Bigg\\vert=\\!\\!\\left\\vert\\sum_{\\beta\\leq\\alpha_{i}}\\!\\left(\\!\\beta\\right)\\!\\Bigg[\\!\\Bigg(\\frac{X}{\\sqrt{N}}\\Bigg)^{\\beta}\\Bigg]\\mathbb{E}\\left[Z^{\\alpha-\\beta}\\right]-c_{\\alpha}\\Bigg\\vert}\\\\ &{\\leq\\!\\Bigg\\vert\\!\\Bigg\\vert\\!\\sum_{\\beta\\leq\\alpha_{i}}\\!\\left(\\!\\beta\\right)\\Bigg(\\mathbb{E}\\left[\\Bigg(\\frac{X}{\\sqrt{N}}\\Bigg)^{\\beta}\\Bigg]-\\mathbb{E}\\left[\\Bigg(\\frac{Y}{\\sqrt{N}}\\Bigg)^{\\beta}\\Bigg]\\Bigg)\\mathbb{E}\\left[Z^{\\alpha-\\beta}\\right]\\Bigg\\vert}\\\\ &{\\quad\\quad+\\!\\left\\vert\\!\\Bigg\\vert\\!\\sum_{\\beta\\leq\\alpha_{i}}\\!\\left(\\!\\beta\\right)\\mathbb{E}\\Bigg[\\!\\Bigg(\\frac{Y}{\\sqrt{N}}\\Bigg)^{\\beta}\\Bigg]\\mathbb{E}\\left[Z^{\\alpha-\\beta}\\right]-c_{\\alpha}\\Bigg\\vert}\\\\ &{\\leq\\!\\eta\\sum_{\\beta\\leq\\alpha_{i}}\\!\\left(\\!\\beta\\right)\\Bigg\\vert\\mathbb{E}\\left[Z^{\\alpha-\\beta}\\right]\\Bigg\\vert+\\!\\Bigg\\vert\\!\\sum_{\\beta\\leq\\alpha_{i}}\\!\\left(\\!\\beta\\right)\\mathbb{E}\\Bigg[\\Bigg(\\frac{Y}{\\sqrt{N}}\\Bigg)^{\\beta}\\Bigg]\\mathbb{E}\\left[Z^{\\alpha-\\beta}\\right]-c_{\\alpha}}\\\\ &{=\\!\\eta\\sum_{\\beta\\leq\\alpha_{i}}\\!\\left(\\!\\beta\\right)\\Bigg\\vert\\mathbb{E}\\left[Z^{\\alpha-\\beta}\\right]\\Bigg\\vert+\\mathbb{E}\\left[Y^{\\alpha}\\right]-c_{\\alpha}\\Bigg\\vert}\\\\ &{=\\!\\eta\\!\\sum_{\\alpha\\leq\\alpha_{i}}\\!\\left(\\!\\beta\\right)\\Bigg\\vert\\mathbb{E}\\left[Z^{\\alpha-\\beta}\\right]\\Bigg\\vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The second-to-last step is the same computation from above but backwards (note that the moments of $Z^{\\prime}$ used for the construction of Y are the same as those of $Z$ used for the construction of $\\hat{X}$ )and the last step used that $Y$ is Gaussian and thus $\\mathbb{E}\\left[Y^{\\alpha}\\right]=c_{\\alpha}$ . We can furthermore compute ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\left[\\hat{X}^{\\alpha}\\right]-c_{\\alpha}\\Bigg|=\\eta\\sum_{\\beta\\leq\\alpha}\\binom{\\alpha}{\\beta}\\left|\\mathbb{E}\\left[Z^{\\alpha-\\beta}\\right]\\right|}}\\\\ &{}&{\\quad=\\eta\\sum_{\\beta\\leq\\alpha}\\binom{\\alpha}{\\beta}\\left|\\mathbb{E}\\left[Z^{\\beta}\\right]\\right|}\\\\ &{}&{\\quad=\\eta\\sum_{\\beta\\leq\\alpha}\\binom{\\alpha}{\\beta}\\prod_{i=1}^{n}\\mathbb{E}\\left[\\prod_{j=1}^{N}Z_{i,j}^{\\beta_{i,j}}\\right]\\Bigg|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the third step uses that the $Z_{i,j}$ are independent for different $i$ . We now get that, using Fact 42, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\vert\\mathbb{E}\\left[\\prod_{j=1}^{N}Z_{i,j}^{\\beta_{i,j}}\\right]\\right\\vert\\leq\\vert\\beta_{i,\\cdot}\\vert^{\\vert\\beta_{i,\\cdot}\\vert/2}\\leq\\sqrt{k}^{\\vert\\beta_{i,\\cdot}\\vert}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, continuing with the above, we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}\\left[\\hat{X}^{\\alpha}\\right]-c_{\\alpha}\\right|=\\eta\\displaystyle\\sum_{\\beta\\leq\\alpha}\\binom{\\alpha}{\\beta}\\displaystyle\\prod_{i=1}^{n}\\Bigg|\\mathbb{E}\\left[\\prod_{i=1}^{N}Z_{i,j}^{\\beta_{i,j}}\\right]\\Bigg|}\\\\ &{\\qquad\\qquad\\leq\\eta\\displaystyle\\sum_{\\beta\\leq\\alpha}\\binom{\\alpha}{\\beta}\\displaystyle\\prod_{i=1}^{n}\\sqrt{k}^{|\\beta_{i,i}|}}\\\\ &{\\qquad\\qquad=\\eta\\displaystyle\\sum_{\\beta\\leq\\alpha}\\binom{\\beta}{\\alpha}(\\sqrt{k}\\mathbf{I})^{\\beta}}\\\\ &{\\qquad\\qquad=\\eta(\\sqrt{k}+1)^{|\\alpha|}}\\\\ &{\\qquad\\qquad\\leq(2k)^{k/2}\\eta=\\bar{\\eta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\mathbb{1}$ is the all-ones vector. This completes the proof. ", "page_idx": 33}, {"type": "text", "text": "Lemma (Restatement of Lemma 33). If $Y\\sim{\\mathcal{N}}(0,I_{n})$ then $\\hat{Y}\\sim\\mathcal{N}(0,I_{n N})$ ", "page_idx": 34}, {"type": "text", "text": "Proof.Since $\\hat{Y}$ is a linear transformation of the Gaussian vector $(Y,Z^{\\prime})$ , it is jointly Gaussian and thus to show the lemma, it is enough to show that the expectation of $\\hat{Y}$ is O and the covariance matrix is the identity. ", "page_idx": 34}, {"type": "text", "text": "Writing as above $Z_{i,j}^{\\prime}=Z_{j}^{\\prime(i)}$ , we have for any $i\\in[n],j\\in[N]$ that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\hat{Y}_{i,j}\\right]=\\mathbb{E}\\left[\\frac{1}{\\sqrt{N}}Y_{i}+Z_{i_{j}}^{\\prime}\\right]=\\frac{1}{\\sqrt{N}}\\cdot0+0=0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Furthermore, for any $i\\in[n],j\\in[N]$ we have that, by independence of $Y$ and $Z^{\\prime}$ \uff0c ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname{Var}\\left[{\\hat{Y}}_{i,j}\\right]={\\frac{1}{N}}\\operatorname{Var}\\left[Y_{i}\\right]+\\operatorname{Var}\\left[Z_{i,j}^{\\prime}\\right]={\\frac{1}{N}}\\cdot1+\\left(1-{\\frac{1}{N}}\\right)=1.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For any $i\\in[n],j_{1}\\neq j_{2}\\in[N]$ we get that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Cov}\\left[\\hat{Y}_{i,j_{1}},\\hat{Y}_{i,j_{2}}\\right]=\\mathbb{E}\\left[\\left(\\frac{1}{\\sqrt{N}}Y_{i}+Z_{i,j_{1}}^{\\prime}\\right)\\left(\\frac{1}{\\sqrt{N}}Y_{i}+Z_{i,j_{2}}^{\\prime}\\right)\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}\\left[\\frac{1}{N}Y_{i}^{2}\\right]+\\mathbb{E}\\left[\\frac{1}{\\sqrt{N}}Y_{i}Z_{i,j_{1}}^{\\prime}\\right]+\\mathbb{E}\\left[\\frac{1}{\\sqrt{N}}Y_{i}Z_{i,j_{2}}^{\\prime}\\right]+\\mathbb{E}\\left[Z_{i,j_{1}}^{\\prime}Z_{i,j_{2}}^{\\prime}\\right]}\\\\ &{\\phantom{=}=\\frac{1}{N}+0+0-\\frac{1}{N}}\\\\ &{=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Finally, for any $i_{1}\\neq i_{2}\\in[n],j_{1},j_{2}\\in[N]$ we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{Cov}\\left[\\hat{Y}_{i_{1},j_{1}},\\hat{Y}_{i_{2},j_{2}}\\right]=0\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "by independence of $\\hat{Y}_{i_{1},j_{1}}=Y_{i_{1}}+Z_{i_{1},j_{1}}^{\\prime}$ and $\\hat{Y}_{i_{2},j_{2}}=Y_{i_{2}}+Z_{i_{2},j_{2}}^{\\prime}$ for $i_{1}\\neq i_{2}$ ", "page_idx": 34}, {"type": "text", "text": "Thus, as argued in the beginning, this shows that $\\hat{Y}\\sim\\mathcal N(0,I_{n N})$ and completes the proof. ", "page_idx": 34}, {"type": "text", "text": "Finally, we want to provide the details of the missing parts in the proof of Lemma 31 about the existence of the polynomial $p_{\\delta}$ . For this, it remains to prove Lemmas 34 and 35, which we restate below. First, recall our definition of $\\delta^{\\prime}$ in (13). ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\delta^{\\prime}:=\\operatorname*{min}\\left\\{\\frac{\\delta}{2d n},\\Theta_{d}\\left(\\frac{\\delta^{d+1}}{n^{3d/2}}\\right)\\right\\}=\\Theta_{d}\\left(\\frac{\\delta^{d+1}}{n^{3d/2}}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In the proof of Lemma 31, we then used the following lemma that allows to replace a factor $X_{i}^{\\ell}$ bya multilinear polynomial in the new variable $\\hat{X}$ ", "page_idx": 34}, {"type": "text", "text": "Lemma (Restatement of Lemma 34). Let $\\delta^{\\prime}>0$ Let $i\\in[n]$ and $\\ell\\in[d]$ .Assume that each of the following conditions holds with probability $\\textstyle1-{\\frac{\\delta^{\\prime}}{d}}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\lvert\\sum_{j=1}^{N}\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right\\rvert\\leq\\frac{1}{\\delta^{\\prime}}}&{{}\\quad}\\\\ {\\displaystyle\\sum_{j=1}^{N}\\left(\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right)^{2}-1\\Bigg\\rvert\\leq\\delta^{\\prime d+1}}&{{}\\quad}\\\\ {\\displaystyle\\left\\lvert\\sum_{j=1}^{N}\\left(\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right)^{a}\\right\\rvert\\leq\\delta^{\\prime d+1}}&{{}\\quad\\forall\\,3\\leq a\\leq d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "(restatement of 11) ", "page_idx": 34}, {"type": "text", "text": "(restatement of 12) ", "page_idx": 34}, {"type": "text", "text": "Then, there is a multilinear polynomial in $\\hat{X}_{i,j}$ that is within $O_{d}(\\delta^{\\prime})$ of $X_{i}^{\\ell}$ with probability $1-\\delta^{\\prime}$ ", "page_idx": 34}, {"type": "text", "text": "We prove this lemma below. Before that, we want to prove Lemma 35 that states that we can use the abovelemmaforour choice of $\\delta^{\\prime}$ ", "page_idx": 35}, {"type": "text", "text": "Lemma (Restatement of Lemma 35). Let $\\delta^{\\prime}$ as in (13). Assuming $\\eta\\le\\delta^{O_{d}(1)}n^{-\\Omega_{d}(1)}k^{-k}$ , there is $a$ choice of $N$ (independent of i or $\\ell$ ) such that (10), (11) and (12) hold, each with probability $\\textstyle1-{\\frac{\\delta^{\\prime}}{d}}$ ", "page_idx": 35}, {"type": "text", "text": "The proof of this lemma is a combination of the following lemmas. ", "page_idx": 35}, {"type": "text", "text": "Lemma 45. Assuming ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\eta\\le\\frac{\\frac{1}{\\delta^{\\prime}d}-1}{N(2k)^{k/2}},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "we have with probability $\\textstyle1-{\\frac{\\delta^{\\prime}}{d}}$ that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{N}\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right|\\leq\\frac{1}{\\delta^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Lemma 46. Assuming ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\eta\\le\\frac{\\delta^{\\prime2d+3}}{3(2k)^{k/2}}-\\frac{2}{N},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "we have with probability $\\textstyle1-{\\frac{\\delta^{\\prime}}{d}}$ that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{N}\\left(\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right)^{2}-1\\right|\\leq\\delta^{\\prime d+1}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that for the above condition on $\\eta$ to be meaningful (we need $\\eta>0$ ), we also need to ensure that ", "page_idx": 35}, {"type": "equation", "text": "$$\nN>\\frac{2d}{\\delta^{\\prime2d+3}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Lemma 47. Assuming ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\eta\\le\\frac{1}{(2k)^{k/2}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\nN\\geq\\frac{100d^{2}}{\\delta^{\\prime2d+3}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "we have for any $3\\leq a\\leq d$ with probability $\\textstyle1-{\\frac{\\delta^{\\prime}}{d}}$ that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{N}\\left(\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right)^{a}\\right|\\leq\\delta^{\\prime d+1}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using these three lemmas, we are now able to prove Lemma 35. ", "page_idx": 35}, {"type": "text", "text": "Proof of Lemma 35. It remains to argue that there is a choice of $N$ such that the condition on $\\eta$ in the lemma statement ensures that the conditions on $\\eta$ in Lemmas 45, 46 and 47 are satisfied. We argue below that the following choice of $N$ is enough, which will complete the proof, ", "page_idx": 35}, {"type": "equation", "text": "$$\nN:=\\Theta_{d}\\left(\\frac{1}{\\delta^{\\prime2d+3}}\\right)=\\Theta_{d}\\left(\\frac{n^{3d(2d+3)/2}}{\\delta^{(d+1)(2d+3)}}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For Lemma 45, we need ntaftsuga (note that $\\begin{array}{r}{\\frac{1}{\\delta^{\\prime}d}-1\\geq1}\\end{array}$ sine $\\delta^{\\prime}$ ismal)i $\\begin{array}{r}{\\eta\\le O_{d}\\left(\\frac{\\delta^{O_{d}(1)}}{n^{\\Omega_{d}(1)}(2k)^{k/2}}\\right)}\\end{array}$ whiholds sne byasmton $\\eta\\le\\delta^{O_{d}(1)}n^{-\\Omega_{d}(1)}k^{-k}$ ", "page_idx": 35}, {"type": "text", "text": "For Lemma 46, we ned $\\begin{array}{r}{\\eta\\le\\frac{\\delta^{\\prime2d+3}}{3(2k)^{k/2}}\\!\\le\\frac{2}{3(2k)^{k/2}}\\;}\\end{array}$ and $\\begin{array}{r}{N>\\frac{2d}{\\delta^{\\prime2d+3}}}\\end{array}$ 2d3 . The latter is clearly satisfied by the choice of $N$ as in (23). For the former, plugging in again the values of $N$ and $\\delta^{\\prime}$ , it is enough for $\\eta$ to satisfy $\\begin{array}{r}{\\eta\\le O_{d}\\left(\\frac{\\delta^{O_{d}(1)}}{n^{\\Omega_{d}(1)}(2k)^{k/2}}\\right)}\\end{array}$ which again holds since $\\eta\\leq\\delta^{O_{d}(1)}n^{-\\Omega_{d}(1)}k^{-k}$ ", "page_idx": 36}, {"type": "text", "text": "For Lemma 47, we need N \u2265 %3 and $\\begin{array}{r}{\\eta\\le\\frac{1}{(2k)^{k/2}}}\\end{array}$ . The former is again directly satisfied by the choice of $N$ . Furthermore, also the latter is true, again since we assume $\\eta\\leq\\delta^{O_{d}(1)}n^{-\\Omega_{d}(1)}k^{-k}$ .\u53e3 ", "page_idx": 36}, {"type": "text", "text": "Next, we want to prove Lemmas 45, 46 and 47. ", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma 45. Using Markov's inequality, we get that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\sum_{j=1}^{N}\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right|>\\frac{1}{\\delta^{\\prime}}\\right]\\leq\\frac{\\mathbb{E}\\left[\\frac{1}{N}\\left(\\sum_{j=1}^{N}\\hat{X}_{i,j}\\right)^{2}\\right]}{\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since, $\\hat{X}$ approximates the moments of $\\mathcal{N}(0,I_{n N})$ up to degree $k$ and error $\\hat{\\eta}$ , we can compute the above expectation as follows ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\sum_{j=1}^{N}\\hat{X}_{i,j}\\right)^{2}\\right]=\\sum_{|\\alpha|=2}{\\binom{2}{\\alpha}}\\mathbb{E}\\left[\\hat{X}_{i,\\cdot}^{\\alpha}\\right]\\leq\\hat{\\eta}N(N-1)+(1+\\hat{\\eta})N=\\hat{\\eta}N^{2}+N.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Here, we used that the expectation $\\mathbb{E}\\left[\\hat{X}_{i,\\cdot}^{\\alpha}\\right]$ is at most $\\hat{\\eta}$ if some $\\alpha_{j}=1$ (for such $\\alpha$ the expectation of $\\mathcal{N}(0,I_{n N})$ is 0) and at most $1+\\hat{\\eta}$ otherwise (since the expectation of $\\mathcal{N}(0,I_{n N})$ is 1 for such $\\alpha$ Thus, we get that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left\\lvert\\sum_{j=1}^{N}\\frac{\\hat{X}_{i,j}}{\\sqrt{N}}\\right\\rvert>\\frac{1}{\\delta^{\\prime}}\\right]\\le(\\hat{\\eta}N+1)\\delta^{\\prime2}\\le\\frac{\\delta^{\\prime}}{d}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "since by assumption we have that $\\begin{array}{r}{\\hat{\\eta}=(2k)^{k/2}\\eta\\le\\frac{\\frac{1}{\\delta^{\\prime}d}-1}{N}}\\end{array}$ ", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma 46. We again use Markov's inequality to get ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\frac{1}{N}\\left(\\sum_{j=1}^{N}\\hat{X}_{i,j}^{2}\\right)-1\\right|>\\delta^{\\prime d+1}\\right]\\leq\\frac{\\mathbb{E}\\left[\\left(\\frac{1}{N}\\left(\\sum_{j=1}^{N}\\hat{X}_{i,j}^{2}\\right)-1\\right)^{2}\\right]}{\\delta^{\\prime2d+2}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We are thus interested in $\\mathbb{E}\\left[\\left(\\sum_{j=1}^{N}\\left(\\hat{X}_{i,j}^{2}-1\\right)\\right)^{2}\\right]$ We can expand thsas folows ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\sum_{j=1}^{N}\\left(\\hat{X}_{i,j}^{2}-1\\right)\\right)^{2}\\right]=\\sum_{|\\alpha|=2}\\binom{2}{\\alpha}\\mathbb{E}\\left[\\left(\\hat{X}_{i,\\cdot}^{2}-1\\right)^{\\alpha}\\right].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "If some $\\alpha_{j}=1$ , then the expectation will be, for some $j_{1}\\neq j_{2}\\in[N]$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\hat{X}_{i,j_{1}}^{2}-1\\right)\\left(\\hat{X}_{i,j_{2}}^{2}-1\\right)\\right]=\\mathbb{E}\\left[\\hat{X}_{i,j_{1}}^{2}\\hat{X}_{i,j_{2}}^{2}\\right]-\\mathbb{E}\\left[\\hat{X}_{i,j_{1}}^{2}\\right]-\\mathbb{E}\\left[\\hat{X}_{i,j_{2}}^{2}\\right]+1}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq1+\\hat{\\eta}-\\left(1-\\hat{\\eta}\\right)-\\left(1-\\hat{\\eta}\\right)+1}\\\\ &{\\qquad\\qquad\\qquad\\qquad=3\\hat{\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "As for the proof of Lemma 45, this is true since the corresponding Gaussian moments are all 1. If no $\\alpha_{j}=1$ ,weget ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left(\\hat{X}_{i,j}^{2}-1\\right)^{2}\\right]=\\mathbb{E}\\left[\\hat{X}_{i,j}^{4}\\right]-2\\mathbb{E}\\left[\\hat{X}_{i,j}^{2}\\right]+1}&{}\\\\ {\\leq3+\\hat{\\eta}-2(1-\\hat{\\eta})+1}&{}\\\\ {=2+2\\hat{\\eta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "since the second and fourth moment of a Gaussian are 1 and 3 respectively. Summarized we get that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left(\\sum_{j=1}^{N}\\left({\\hat{X}}_{i,j}^{2}-1\\right)\\right)^{2}\\right]\\leq N(2+2\\hat{\\eta})+N(N-1)3\\hat{\\eta}}\\\\ {=2N-N\\hat{\\eta}+3N^{2}\\hat{\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Together with the above we get that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}\\left[\\left|\\frac{1}{N}\\left(\\sum_{j=1}^{N}\\hat{X}_{i,j}^{2}\\right)-1\\right|>\\delta^{\\prime d+1}\\right]\\le\\frac{2N-N\\hat{\\eta}+3N^{2}\\hat{\\eta}}{N^{2}\\delta^{\\prime2d+2}}}}\\\\ &{}&{\\le\\frac{\\frac{2}{N}+3\\hat{\\eta}}{\\delta^{\\prime2d+2}}}\\\\ &{}&{\\le\\frac{\\delta^{\\prime}}{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "since by assumption we have that $\\begin{array}{r}{\\hat{\\eta}=(2k)^{k/2}\\eta\\le\\frac{\\frac{\\delta^{\\prime2d+3}}{d}-\\frac{2}{N}}{3}}\\end{array}$ ", "page_idx": 37}, {"type": "text", "text": "Proof of Lemma 47. For any $3\\leq a\\leq d$ similar to before, we compute using Markov's inequality ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\frac{1}{N^{a/2}}\\sum_{j=1}^{N}\\hat{X}_{i,j}^{a}\\right|>\\delta^{\\prime d+1}\\right]\\leq\\frac{\\mathbb{E}\\left[\\left(\\frac{1}{N^{a/2}}\\sum_{j=1}^{N}\\hat{X}_{i,j}^{a}\\right)^{2}\\right]}{\\delta^{\\prime2d+2}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We thus need to bound $\\mathbb{E}\\left[\\left(\\sum_{j=1}^{N}\\hat{X}_{i,j}^{a}\\right)^{2}\\right]$ We get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\sum_{j=1}^{N}\\hat{X}_{i,j}^{a}\\right)^{2}\\right]=\\sum_{|\\alpha|=2}{\\binom{2}{\\alpha}}\\mathbb{E}\\left[\\hat{X}_{i,\\cdot}^{a\\cdot\\alpha}\\right]\\leq N^{2}(2a)^{a}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "since fo any $\\alpha$ wehave $\\mathbb{E}\\left[\\hat{X}_{i,\\cdot}^{a\\cdot\\alpha}\\right]\\leq(2a)^{a}$ by Fact 41, sine by asumption we have $\\begin{array}{r}{\\eta\\le\\frac{1}{(2k)^{k/2}}}\\end{array}$ in other words $\\hat{\\eta}\\leq1$ . Combining this with the above, we get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\frac{1}{N^{a/2}}\\sum_{j=1}^{N}\\hat{X}_{i,j}^{a}\\right|>\\delta^{\\prime d+1}\\right]\\leq\\frac{N^{2}(2a)^{a}}{N^{a}\\delta^{\\prime2d+2}}=\\frac{(2a)^{a}}{N^{a-2}\\delta^{\\prime2d+2}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We have for any $a\\geq3$ that $((2a)^{a})^{\\frac{1}{a-2}}\\leq100a$ and thus we get that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\left|\\displaystyle\\frac{1}{N^{a/2}}\\displaystyle\\sum_{j=1}^{N}\\hat{X}_{i,j}^{a}\\right|>\\delta^{\\prime d+1}\\right]\\leq\\frac{(100a)^{a-2}}{N^{a-2}\\delta^{\\prime2d+2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(\\displaystyle\\frac{100a}{N}\\right)^{a-2}\\frac{1}{\\delta^{\\prime2d+2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\left(\\displaystyle\\frac{\\delta^{\\prime2d+3}}{d}\\right)^{a-2}\\frac{1}{\\delta^{\\prime2d+2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\delta^{\\prime}}{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In the third step we used that by assumption we have that $\\begin{array}{r}{N\\ge\\frac{100d^{2}}{\\delta^{\\prime2d+3}}}\\end{array}$ and $d\\geq a$ . In the last step, we then used that a - 2 \u2265 1, together with 82d+3 d\u2264 1. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Finally, it remains to prove Lemma 34. ", "page_idx": 37}, {"type": "text", "text": "Proof of Lemma $34.$ By the assumptions, the conditions (10), (11) and (12) hold simultaneously with probability $\\begin{array}{r}{1-d\\frac{\\bar{\\delta}^{\\prime}}{d}=1-\\delta^{\\prime}}\\end{array}$ We want to construct a multilnear polynomial in $\\hat{X}_{i,j}$ such that conditioned on (10), (11) and (12) it is within $O_{d}(\\delta^{\\prime})$ of $X_{i}^{\\ell}$ . This will complete the proof. ", "page_idx": 38}, {"type": "text", "text": "The construction follows [21, Proof of Lemma 15]. First, note that by construction we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nX_{i}^{\\ell}=N^{-\\ell/2}\\left(\\sum_{j=1}^{N}\\hat{X}_{i,j}\\right)^{\\ell}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Expanding the sum and grouping the terms according together according how the power of $\\ell$ partitioned to different $\\hat{X}_{i,j}$ ,weget ", "page_idx": 38}, {"type": "equation", "text": "$$\nX_{i}^{\\ell}=\\sum_{r=1}^{\\ell}\\sum_{\\substack{1\\leq a_{1}\\leq\\ldots\\leq a_{r}}}c(a_{1},\\ldots,a_{r})\\sum_{\\substack{j_{1},\\ldots,j_{r}\\in[N]}}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "wherethe $c(a_{1},\\ldots,a_{r})$ are constants that capture how often the latter terms occur if we multiply the product out. More precisely, ", "page_idx": 38}, {"type": "equation", "text": "$$\nc(a_{1},\\ldots,a_{r})=\\binom{\\ell}{a_{1},\\ldots,a_{r}}\\prod_{t=1}^{\\ell}\\frac{1}{|\\{s:a_{s}=t\\}|}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The strategy is now as follows. We want to approximate the terms ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{j_{1},...,j_{r}\\in[N]}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "separately by multilinear polynomials. If $a_{r}=1$ , then the terms is already multilinear and there is nothing to do. Note that if $\\ell=1$ , then we only have this case, so from now on we assume $\\ell\\geq2$ .If $a_{r}=2$ , then we want to show that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{\\substack{j_{1},\\ldots,j_{r}\\in[N]}}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}-\\sum_{\\substack{j_{1},\\ldots,j_{\\hat{r}}\\in[N]}}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\Bigg|\\leq O_{d}(\\delta^{\\prime}),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where here and also later $\\hat{r}$ is the largest $s\\in[r]$ such that $a_{s}=1$ (we assume here and throughout this proof that in case no $a_{s}=1$ , i.e. we have an empty sum and an empty product, the second term on the LHS is 1; the reason for this is that, intuitively, we want to make the term multilinear by removing all powers higher than 1, which leaves 1 in case no $a_{s}=1$ ). If $a_{r}\\geq3$ , then we want to show that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left|\\sum_{j_{1},...,j_{r}\\in[N]}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}\\right|\\leq O_{d}(\\delta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Once we have shown these, the idea is to remove all terms with $a_{r}\\geq3$ and replace the terms for $a_{r}=2$ by the multilinear term on the LHS of (24). We get that $X_{i}^{\\ell}$ is within $O_{d}(\\delta^{\\prime})$ of ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{\\stackrel{1\\leq a_{1}\\leq\\ldots\\leq a_{r}}{a_{r}\\leq2}}^{\\ell}c(a_{1},\\ldots,a_{r})\\sum_{\\stackrel{j_{1},\\ldots,j_{\\hat{r}}\\in[N]}{\\mathrm{distinct}}}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which is multilinear. The $O_{d}$ here directly also covers that we need to multiply the $O_{d}(\\delta^{\\prime})$ from above with the constants $c(a_{1},\\ldots,a_{r})$ and then sum over the choices of $a_{1},\\dotsc..,a_{r}$ and over $r$ . This then completes the proof. ", "page_idx": 38}, {"type": "text", "text": "Thus, it remains to prove (24) and (25). We do this using the assumptions of the lemma and by inductionon $r$ (and technically also over $\\ell$ ;thebasecasefor $\\ell=1$ was already covered above).We also inductively show that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left|\\sum_{\\substack{j_{1},\\ldots,j_{\\hat{r}}\\in[N]}}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right|\\leq O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "This will be needed to prove (24) and (25). ", "page_idx": 39}, {"type": "text", "text": "Base case $r=1$ .If $r=1$ , then ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{j_{1},...,j_{r}\\in[N]}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}=\\sum_{j_{1}}\\left(\\frac{\\hat{X}_{i,j_{1}}}{\\sqrt{N}}\\right)^{\\ell}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In this case, we have $a_{r}=\\ell$ .If $\\ell=2$ , we need to show (24) and this follows directly from (11) (since $\\delta^{\\prime d+1}\\le{\\cal O}_{d}(\\delta^{\\prime}))$ If $\\ell\\geq3$ , then we need to show (25), which follows again directly from (12). Also note that (26) holds since $\\begin{array}{r}{1\\le O_{d}\\left(\\frac{1}{\\delta^{\\prime}}\\right)}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Induction step. We now assume that $r\\geq2$ and that we have proven the result for all values smaller than $r$ . The goal is to now show the result for $r$ .Wecompute ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{j_{1}\\ldots j_{e+\\lfloor N\\rfloor}\\in\\mathbb{N}}{\\sum_{i=1}^{e}\\sum_{e\\in\\mathbb{N}}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{e}}}{\\sqrt{N}}\\right)^{a_{s}}}}\\\\ &{=\\left(\\underset{j_{t-\\frac{j_{e+\\ell}}{\\sqrt{N}},\\ldots}}{\\sum_{i=1,\\cdots,i\\in\\mathbb{N}}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{e}}}{\\sqrt{N}}\\right)^{a_{s}}}\\right)\\left(\\underset{j_{s}=1}{\\sum_{i=1}^{N}}\\bigg(\\frac{\\hat{X}_{i,j_{e}}}{\\sqrt{N}}\\bigg)^{a_{r}}-\\underset{j_{e+\\ell_{\\lfloor3\\rfloor}\\ldots,j_{e+\\ell_{\\lfloor3\\rfloor}\\rfloor}}}{\\sum_{i\\in\\mathbb{N}}\\sum_{i=1}^{r}}\\bigg(\\frac{\\hat{X}_{i,j_{e}}}{\\sqrt{N}}\\bigg)^{a_{r}}\\right)}\\\\ &{=\\left(\\underset{j_{t-\\frac{j_{e+\\ell}}{\\sqrt{N}}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{e}}}{\\sqrt{N}}\\right)^{a_{s}}}}{\\sum_{i=1}^{r}\\sum_{i=1}^{j_{e+\\ell}}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{e}}}{\\sqrt{N}}\\right)^{a_{s}}}\\right)\\left(\\underset{j_{s}=1}{\\sum_{i=1}^{N}\\bigg(\\frac{\\hat{X}_{i,j_{e}}}{\\sqrt{N}}\\bigg)^{a_{s}}}\\right)}\\\\ &{\\quad-\\left(\\underset{j_{t-\\ell_{\\lfloor3\\rfloor}\\ldots,j_{e+\\ell}}}{\\sum_{i=1,\\cdots,i\\in\\mathbb{N}}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{e}}}{\\sqrt{N}}\\right)^{a_{s}}}\\right)\\left(\\underset{j_{s}\\in\\{i_{\\ell_{\\lfloor3\\rfloor}\\ldots,j_{e+\\ell_{\\lfloor3\\rfloor}\\rfloor}}}}{\\sum_{i\\in\\mathcal{S}_{i}}\\sum_{j_{e+\\ell_{\\lfloor3\\rfloor}\\ldots,j_{e+\\ell}}}\\left(\\frac{\\hat{X}_{i,j_{e}}}{\\sqrt{N}}\\right)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We first want to analyze the second term. Note that this term is equal to ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{r-1}\\sum_{\\substack{j_{1},\\dots,j_{r-1}\\in[N]}}\\prod_{s=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}+\\mathbb{1}_{[s=t]}a_{r}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Now, note that all terms in this sum have been considered in the induction hypothesis. Also note that $a_{t}+a_{r}\\ge3$ (since $a_{r}\\geq2$ , otherwise there is nothing to prove) and thus every term in the above sum over $t$ is at most $O_{d}(\\delta^{\\prime})$ in absolute value by (25). Thus, we also get that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left|\\left(\\sum_{j_{1},\\ldots,j_{r-1}\\in[N]}\\prod_{s=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}\\right)\\left(\\sum_{j_{r}\\in\\{j_{1},\\ldots,j_{r-1}\\}}\\left(\\frac{\\hat{X}_{i,j_{r}}}{\\sqrt{N}}\\right)^{a_{r}}\\right)\\right|\\leq O_{d}(\\delta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In the following it thus remains to analyze the first term in order to show (24) respectively (25) ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left(\\sum_{j_{1},...,j_{r-1}\\in[N]}\\prod_{s=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}\\right)\\left(\\sum_{j_{r}=1}^{N}\\left(\\frac{\\hat{X}_{i,j_{r}}}{\\sqrt{N}}\\right)^{a_{r}}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We now need to distinguish the cases $a_{r}\\geq3$ (in which case we need to show (25)) and $a_{r}=2$ (in which case we need to show (24)). ", "page_idx": 40}, {"type": "text", "text": "Case I: $a_{r}\\geq3$ , proof of (25). In this case, we have, by (12), that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\sum_{j_{r}=1}^{N}\\left(\\frac{\\hat{X}_{i,j_{r}}}{\\sqrt{N}}\\right)^{a_{r}}\\right|\\leq\\delta^{\\prime d+1}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "To analyze the term ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{j_{1},...,j_{r-1}\\in[N]}\\prod_{s=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "we want to apply the induction hypothesis. We need to again distinguish three cases, namely $a_{r-1}\\geq3$ (in which case we can use (25), $a_{r-1}=2$ (in which case we can use (24) and $a_{r-1}=1$ (in which case the term on the LHS of (24) are in fact equal). We do this in the following. ", "page_idx": 40}, {"type": "text", "text": "If $a_{r-1}\\geq3$ , then, by the induction hypothesis for (25), ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{j_{1},...,j_{r-1}\\in[N]}\\prod_{s=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}\\Bigg|\\le O_{d}(\\delta^{\\prime})\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and thus we get, using the decomposition (27) and the bound (28) on the second term, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\sum_{\\substack{j_{1},\\ldots,j_{r}\\in[N]}}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}\\right|\\leq O_{d}(\\delta^{\\prime})\\delta^{\\prime d+1}+O_{d}(\\delta^{\\prime})\\leq O_{d}(\\delta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "If $a_{r-1}=2$ , then, by the induction hypothesis for (24), we have that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{\\substack{j_{1},\\ldots,j_{r-1}\\in[N]}}\\prod_{s=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}-\\sum_{\\substack{j_{1},\\ldots,j_{\\hat{r}}\\in[N]}}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\Bigg|\\leq O_{d}(\\delta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By the induction hypothesis on (26), we get ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\sum_{\\substack{j_{1},\\ldots,j_{\\hat{r}}\\in[N]}}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right|\\leq O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Combining these two, we get ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\sum_{j_{1},...,j_{r-1}\\in[N]}\\prod_{s=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}\\right|\\leq O_{d}(\\delta^{\\prime})+O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus, we get (note that $r\\leq d,$ , using again the decomposition (27) and the bound (28) on the second term, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\sum_{\\substack{j_{1},\\ldots,j_{r}\\in[N]}}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}\\right|\\leq\\left(O_{d}(\\delta^{\\prime})+O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right)\\right)\\delta^{\\prime d+1}+O_{d}(\\delta^{\\prime})\\leq O_{d}(\\delta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "If $a_{r-1}=1$ , then $\\hat{r}=r-1$ and thus ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{j_{1},...,j_{r-1}\\in[N]}\\prod_{s=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}=\\sum_{j_{1},...,j_{\\hat{r}}\\in[N]}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "As above, we get that, by the induction hypothesis on (26), ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\sum_{\\substack{j_{1},\\ldots,j_{\\hat{r}}\\in[N]}}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right|\\leq O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Again by using the decomposition (27) and the bound (28) on the second term, we get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\sum_{\\substack{j_{1},\\ldots,j_{r}\\in[N]}}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}\\right|\\leq O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right)\\delta^{\\prime d+1}+O_{d}(\\delta^{\\prime})\\leq O_{d}(\\delta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus, for all three cases, we get that (25) still holds for $r$ ", "page_idx": 41}, {"type": "text", "text": "Case II: $a_{r}=2$ , proof of (24). In this case, we have, by (11), that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\sum_{j_{r}=1}^{N}\\left(\\frac{\\hat{X}_{i,j_{r}}}{\\sqrt{N}}\\right)^{a_{r}}-1\\right|\\leq\\delta^{\\prime d+1}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "For the term ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sum_{j_{1},...,j_{r-1}\\in[N]}\\prod_{s=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "we again need to distinguish two cases, namely $a_{r-1}=2$ and $a_{r-1}=1$ ", "page_idx": 41}, {"type": "text", "text": "If $a_{r-1}=2$ , then we have, as above, by the induction hypothesis for (24), ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sum_{\\substack{j_{1},\\ldots,j_{r-1}\\in[N]}}\\prod_{s=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}-\\sum_{\\substack{j_{1},\\ldots,j_{\\hat{r}}\\in[N]}}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\Bigg|\\leq O_{d}(\\delta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Also as above, by the induction hypothesis on (26), we get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\sum_{\\substack{j_{1},\\ldots,j_{\\hat{r}}\\in[N]}}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right|\\leq O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Combining these, we have that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(\\displaystyle\\sum_{j_{1},\\ldots,j_{d+m+1}}\\prod_{i=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{2}}}{\\sqrt{N}}\\right)^{a_{n}}\\right)\\left(\\displaystyle\\sum_{j_{1},\\ldots,1}^{N}\\left(\\frac{\\hat{X}_{i,j_{2}}}{\\sqrt{N}}\\right)^{a_{n}}\\right)-\\sum_{j_{1},\\ldots,j_{d+m+1}}\\prod_{i=1,j_{1}\\leq n}^{r}\\frac{\\hat{Y}_{i,j_{2}}}{\\sqrt{N}}\\right|}\\\\ &{\\leq\\left|\\displaystyle\\sum_{j_{1},\\ldots,j_{d+m+1}}\\prod_{w=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{2}}}{\\sqrt{N}}\\right)^{a_{n}}-\\sum_{j_{1},\\ldots,j_{d+m+1}}\\prod_{w=1}^{r}\\frac{\\hat{X}_{i,j_{2}}}{\\sqrt{N}}\\right|\\left|\\sum_{j_{2},\\ldots,1}^{\\hat{X}_{i,j_{d+m}}}\\right|\\left|\\sum_{j_{2},\\ldots,1}^{N}\\right|}\\\\ &{\\quad+\\left|\\displaystyle\\sum_{j_{1},\\ldots,j_{d+m+1}}\\prod_{s=1}^{r}\\frac{\\hat{Y}_{i,j_{2}}}{\\sqrt{N}}\\right|\\left|\\sum_{j_{1},\\ldots,1}^{N}\\left(\\frac{\\hat{X}_{i,j_{2}}}{\\sqrt{N}}\\right)^{a_{n}}-1\\right|}\\\\ &{\\leq O_{d}(s^{\\prime})(1+\\delta^{d+1})+O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right)\\delta^{d+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus, we get that, using again the decomposition (27) and the bound (28) on the second term, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\underset{j_{1},\\ldots,j_{r}\\in[N]}{\\sum}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}-\\underset{j_{1},\\ldots,j_{r}\\in[N]}{\\sum}\\underset{s=1}{\\overset{\\hat{r}}{\\prod}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right|}\\\\ &{\\leq O_{d}(\\delta^{\\prime})(1+\\delta^{\\prime d+1})+O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right)\\delta^{\\prime d+1}+O_{d}(\\delta^{\\prime})}\\\\ &{=O_{d}(\\delta^{\\prime})+O_{d}(\\delta^{\\prime})\\delta^{\\prime d+1}+O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right)\\delta^{\\prime d+1}+O_{d}(\\delta^{\\prime})}\\\\ &{\\leq O_{d}(\\delta^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "If $a_{r-1}=1$ , then we get ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{j_{1},...,j_{r-1}\\in[N]}\\prod_{s=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}=\\sum_{j_{1},...,j_{\\hat{r}}\\in[N]}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Again, by the induction hypothesis on (26), we get ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|\\sum_{\\substack{j_{1},\\ldots,j_{\\hat{r}}\\in[N]}}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right|\\leq O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "As above, we get that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(\\displaystyle\\sum_{j_{1},\\ldots,j_{r-1}\\in[N]}\\prod_{s=1}^{r-1}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}\\right)\\left(\\displaystyle\\sum_{j_{r}=1}^{N}\\left(\\frac{\\hat{X}_{i,j_{r}}}{\\sqrt{N}}\\right)^{a_{r}}\\right)-\\sum_{j_{1},\\ldots,j_{r}\\in[N]}\\prod_{s=1}^{r}\\frac{\\hat{Y}_{i,j_{s}}}{\\sqrt{N}}\\right|}\\\\ &{\\leq\\left|\\displaystyle\\sum_{j_{1},\\ldots,j_{r}\\in[N]}\\prod_{s=1}^{r}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\left|\\left|\\sum_{j_{r}=1}^{N}\\left(\\frac{\\hat{X}_{i,j_{r}}}{\\sqrt{N}}\\right)^{a_{r}}-1\\right|}\\\\ &{\\leq O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right)\\delta^{r d+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus, we can conclude that, exactly as above by the decomposition (27) and the bound (28) on the second term, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{j_{1},\\dots,j_{r}\\in[N]}\\prod_{s=1}^{r}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{a_{s}}-\\sum_{j_{1},\\dots,j_{\\hat{r}}\\in[N]}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}}\\\\ &{\\leq O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right)\\delta^{r d+1}+O_{d}(\\delta^{\\prime})}\\\\ &{\\leq O_{d}(\\delta^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Hence, for both cases, we get that (24) still holds for $r$ ", "page_idx": 42}, {"type": "text", "text": "Proof of (26). We can also analogously show (26). First note that if $\\hat{r}<r-1$ , then (26) follows directly from the induction hypothesis since the term ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{j_{1},...,j_{\\hat{r}}\\in[N]}\\prod_{s=1}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "already appeared for $r-1$ by using ", "page_idx": 43}, {"type": "equation", "text": "$$\na_{s}^{\\prime}={\\binom{a_{s}}{a_{r-1}+a_{r}}}{\\mathrm{}\\mathrm{}\\mathrm{}}\\mathrm{}\\mathrm{}\\mathclose{\\mathrm{~if~}}s<r-1\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and we can directly apply the induction hypothesis. Thus, we only need to show (26) for $\\hat{r}\\ge r-1$ and in particular $\\hat{r}\\geq1$ If ${\\hat{r}}=1$ , then by (10), the term is at most $\\begin{array}{r}{\\frac{1}{\\delta^{\\prime}}\\leq O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r}\\right)}\\end{array}$ . Otherwise, $\\hat{r}\\geq2$ and we can do a similar expansion as above to get ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{j_{1},\\ldots,j_{j}\\in\\{N\\}}{\\sum}\\overset{\\hat{\\Gamma}}{\\prod}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}}\\\\ &{=\\left(\\underset{j_{2},\\ldots,j_{j}\\in\\{N\\}}{\\sum}\\overset{\\hat{\\Gamma}}{\\prod}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)\\left(\\underset{j_{1}=1}{\\overset{N}{\\sum}}\\frac{\\hat{X}_{i,j_{1}}}{\\sqrt{N}}-\\underset{j_{1}\\in\\{j_{2},\\ldots,j_{j}\\}}{\\sum}\\frac{\\hat{X}_{i,j_{1}}}{\\sqrt{N}}\\right)}\\\\ &{=\\left(\\underset{j_{2},\\ldots,j_{j}\\in\\{N\\}}{\\sum}\\overset{\\hat{\\Gamma}}{\\prod}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)\\left(\\underset{j_{1}=1}{\\overset{N}{\\sum}}\\frac{\\hat{X}_{i,j_{1}}}{\\sqrt{N}}\\right)-\\left(\\underset{j_{2},\\ldots,j_{j}\\in\\{N\\}}{\\sum}\\overset{\\hat{\\Gamma}}{\\prod}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)\\left(\\underset{j_{1}\\in\\{j_{2},\\ldots,j_{j}\\}}{\\sum}\\frac{\\hat{X}_{i,j_{1}}}{\\sqrt{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The first term is then, by the induction hypothesis and (10), at most $\\begin{array}{r}{O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right)\\frac{1}{\\delta^{\\prime}}}\\end{array}$ in absolute value. The second term can be expanded as above and is thus equal to ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{t=2}^{\\hat{r}}\\sum_{j_{2},...,j_{\\hat{r}}\\in[N]}\\prod_{s=2}^{\\hat{r}}\\left(\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}\\right)^{1+1_{[s=t]}}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "By (24) for $r-1$ , we get that this is within $O_{d}(\\delta^{\\prime})$ of ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{t=2}^{\\hat{r}}\\sum_{\\substack{j_{2},\\dots,j_{t-1},j_{t+1}\\dots,j_{\\hat{r}}\\in[N]\\,s=2}}\\prod_{s\\neq t}^{\\hat{r}}\\frac{\\hat{X}_{i,j_{s}}}{\\sqrt{N}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "(again if $\\hat{r}=2$ , then the above term should be interpreted as 1). This term now is, by (26) for $r-2$ $\\begin{array}{r}{O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-2}\\right)}\\end{array}$ . Note that for $r=2$ (and thus $\\hat{r}=2$ we cannot apply the induction hypothesis but the term is $1$ and thus the bound still holds. ", "page_idx": 43}, {"type": "text", "text": "Combing these result, we get that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\sum_{j_{1},\\dots,j_{\\hat{r}}\\in[N]}\\prod_{s=1}^{\\hat{r}}\\frac{{\\hat{X}}_{i,j_{s}}}{\\sqrt{N}}\\right|\\leq O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-1}\\right)\\frac{1}{\\delta^{\\prime}}+O_{d}(\\delta^{\\prime})+O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r-2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq O_{d}\\left(\\left(\\frac{1}{\\delta^{\\prime}}\\right)^{r}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "This is exactly (26) for $r$ ", "page_idx": 43}, {"type": "text", "text": "By induction, we have now shown (24) and (25) for all $r$ and as argued above, this completes the proof of this lemma. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: For the main claims made in the abstract and introduction we give a proof overview in Section 2 and complete proofs in the appendices. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: We discuss the limitations of our work in the corresponding part of Section 1.2. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 44}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: For all the theoretical results, we give a proof in the main part of the paper or in the appendices. For the main results, we give a proof overview in Section 2. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 45}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: Since this paper discusses theoretical results only and does not contain experiments, this question does not apply. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 45}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: Since this paper discusses theoretical results only and does not contain experiments, this question does not apply. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips. cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: Since this paper discusses theoretical results only and does not contain experiments, this question does not apply. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: Since this paper discusses theoretical results only and does not contain experiments, this question does not apply. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 47}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: Since this paper discusses theoretical results only and does not contain experiments, this question does not apply. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The research conducted in this papers conforms with the NeurIPs Code of Ethics. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: This paper discussed general theoretical results and algorithms and has as such no immediate societal impact. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 48}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: Since this paper discusses theoretical results only and does not contain experiments, this question does not apply. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 48}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: Since this paper discusses theoretical results only and does not contain experiments, this question does not apply. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: Since this paper discusses theoretical results only and does not contain experiments, this question does not apply. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 49}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: Since this paper discusses theoretical results only and does not contain experiments, this question does not apply. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 49}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: Since this paper discusses theoretical results only and does not contain experiments, this question does not apply. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 50}]