[{"figure_path": "VZQmIoDGBG/tables/tables_6_1.jpg", "caption": "Table 1: Performance of different LLMs on our SAFEWORLD benchmark.", "description": "This table presents the performance of various Large Language Models (LLMs) on the SAFEWORLD benchmark.  The benchmark evaluates the models' ability to generate safe and helpful responses across diverse global contexts, considering cultural and legal standards.  The table shows the average scores for four metrics: Coverage, Faithfulness, Factuality, and Response Type Matching, allowing for a comparison of the models' strengths and weaknesses across these different dimensions of safety and appropriateness.  Open-source and proprietary LLMs are included for comparison.", "section": "4.4 LLM Evaluation Results"}, {"figure_path": "VZQmIoDGBG/tables/tables_8_1.jpg", "caption": "Table 2: Performance of our SAFEWORLDLM on the SAFEWORLD benchmark.", "description": "This table presents a comparison of the performance of various models on the SAFEWORLD benchmark.  It shows the average coverage, faithfulness, factuality, and response type matching scores for different models. The models compared include several proprietary LLMs (Command-R, Command-R+, GPT-4-turbo, GPT-40), variations of GPT-4-turbo prompting with different levels of guidance, and several open-source LLMs from the SAFEWORLDLM series (SAFEWORLDLM w/o Neg. Category 1, SAFEWORLDLM w/o Neg. Category 2, SAFEWORLDLM (50% Data), and SAFEWORLDLM).  The table highlights the superior performance of the SAFEWORLDLM model across all metrics compared to other models, demonstrating the effectiveness of the geo-diverse safety alignment training.", "section": "5.4 SAFEWORLDLM Evaluation Results"}, {"figure_path": "VZQmIoDGBG/tables/tables_9_1.jpg", "caption": "Table 3: Western vs. Non-Western models performance statistics.", "description": "This table presents a comparative analysis of the performance of various LLMs (Large Language Models) across Western and Non-Western countries.  The metrics used for comparison are Coverage, Faithfulness, Factuality, and Response Type Matching.  The difference (\u0394) between the performance in Western and Non-Western countries is also shown for each metric.  This helps to evaluate whether the models demonstrate equitable performance across different geographical regions, highlighting potential biases or disparities.", "section": "5.7 Western vs. Non-Western"}, {"figure_path": "VZQmIoDGBG/tables/tables_15_1.jpg", "caption": "Table 1: Performance of different LLMs on our SAFEWORLD benchmark.", "description": "This table presents the performance of various Large Language Models (LLMs) on the SAFEWORLD benchmark.  The benchmark evaluates the models' ability to generate safe and appropriate responses across diverse global contexts, considering cultural and legal factors.  The table shows average scores for three key dimensions: Coverage, Faithfulness, and Factuality, along with the accuracy of Response Type Matching.  This allows for a comparison of both open-source and proprietary LLMs, highlighting their strengths and weaknesses in handling geo-diverse safety scenarios.", "section": "4 Automatic Evaluation Framework"}, {"figure_path": "VZQmIoDGBG/tables/tables_17_1.jpg", "caption": "Table 5: SAFEWORLD detailed statistics.", "description": "This table presents a detailed breakdown of the SAFEWORLD dataset statistics after machine and human validation. It shows the original number of queries, the number of queries that passed human validation, for each query type (SPECIFICANSWER, COMPREANSWER, REFUSETOANSWER, DOANSWER) and category (Norms, Policies).  The numbers reflect the rigorous quality control process applied to ensure high data quality in the benchmark.", "section": "A.1 GEOSAFEDB Development"}, {"figure_path": "VZQmIoDGBG/tables/tables_20_1.jpg", "caption": "Table 1: Performance of different LLMs on our SAFEWORLD benchmark.", "description": "This table presents the performance of various LLMs (Large Language Models) on the SAFEWORLD benchmark.  The benchmark assesses the models' ability to generate safe and helpful responses to queries that involve diverse cultural and legal contexts. The table shows the average scores for three key evaluation metrics: Coverage (how comprehensively the model addresses the relevant norms and policies), Faithfulness (how accurately the model's response aligns with the ground truth), and Factuality (how accurate are the factual claims in the responses).  It also shows the percentage of correctly matched response types. The models are grouped into open-source LLMs, retrieval-augmented LLMs, and GPT-series LLMs for comparison.", "section": "4 Automatic Evaluation Framework"}, {"figure_path": "VZQmIoDGBG/tables/tables_20_2.jpg", "caption": "Table 1: Performance of different LLMs on our SAFEWORLD benchmark.", "description": "This table presents the performance of various Large Language Models (LLMs) on the SAFEWORLD benchmark.  The benchmark evaluates LLMs' ability to generate safe and helpful responses across diverse global contexts, considering cultural and legal guidelines. The table shows the average scores for three key evaluation metrics: Coverage, Faithfulness, and Factuality, along with the percentage of Response Type Matches.  It compares both open-source and proprietary LLMs, allowing for a broad comparison of model capabilities in handling geo-diverse safety scenarios.", "section": "4 Automatic Evaluation Framework"}, {"figure_path": "VZQmIoDGBG/tables/tables_22_1.jpg", "caption": "Table 8: Comparison of Pearson \u03c1 and Kendall \u03c4 correlation scores between our LLM-based evaluation framework and human judgments, using Llama-3-70B-Instruct and GPT-4-turbo as the underlying models for the evaluation metric.", "description": "This table presents the correlation results between the automatic evaluation metrics (Coverage, Faithfulness, and Response Type Matching) and human evaluations.  Two different LLMs, Llama-3-70B-Instruct and GPT-4-turbo, were used as the baselines for the automatic evaluation.  Pearson's \u03c1 and Kendall's \u03c4 correlation coefficients are reported to quantify the strength and direction of the association between the automatic and human judgments for each metric.", "section": "4.4 LLM Evaluation Results"}, {"figure_path": "VZQmIoDGBG/tables/tables_22_2.jpg", "caption": "Table 9: Model accuracy (%) on general NLP tasks and ratio of harmlessness responses (%) on general safety benchmarks.", "description": "This table presents the performance of different models on general NLP tasks (MMLU and HellaSwag) and general safety tasks (HH-RLHF and BeaverTails). It compares the performance of models trained with different combinations of training data, including Zephyr-7B-SFT-Full, ULTRAFEEDBACK, and SAFEWORLDALIGN, to show the impact of the SAFEWORLDALIGN training data on overall model performance.", "section": "5 Geo-Diverse Safety Alignment Training"}]