[{"type": "text", "text": "Optimal Top-Two Method for Best Arm Identification and Fluid Analysis ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Agniv Bandyopadhyay Sandeep Juneja TIFR Mumbai, India Ashoka University, India agniv.bandyopadhyay@tifr.res.in sandeep.juneja2010@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Shubhada Agrawal Georgia Institute of Technology, USA sagrawal362@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Top-2 methods have become popular in solving the best arm identification (BAI) problem. The best arm, or the arm with the largest mean amongst finitely many, is identified through an algorithm that at any sequential step independently pulls the empirical best arm, with a fixed probability $\\beta$ , and pulls the best challenger arm otherwise. The probability of incorrect selection is guaranteed to lie below a specified $\\delta\\,>\\,0$ . Information theoretic lower bounds on sample complexity are well known for BAI problem and are matched asymptotically as $\\delta\\rightarrow0$ by computationally demanding plug-in methods. The above top 2 algorithm for any $\\beta\\in\\bar{(0,1)}$ has sample complexity within a constant of the lower bound. However, determining the optimal $\\beta$ that matches the lower bound has proven difficult. In this paper, we address this and propose an optimal top-2 type algorithm. We consider a function of allocations anchored at a threshold. If it exceeds the threshold then the algorithm samples the empirical best arm. Otherwise, it samples the challenger arm. We show that the proposed algorithm is optimal as $\\delta\\rightarrow0$ . Our analysis relies on identifying a limiting fuid dynamics of allocations that satisfy a series of ordinary differential equations pasted together and that describe the asymptotic path followed by our algorithm. We rely on the implicit function theorem to show existence and uniqueness of these fluid ode's and to show that the proposed algorithm remains close to the ode solution. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic best arm identification (BAI) problem has attracted a great deal of attention in the multi armed bandit community (see [21], [10], [3] for some early references in BA1). The basic problem involves a finite number of unknown probability distributions or arms that can be sampled from independently and the aim is to identify the arm with the largest mean. We consider a popular fixed confidence version of the problem where the sampling is sequential and the aim is to minimise sample complexity while guaranteeing that the probability of selecting the wrong arm is restricted to apre-specified $\\delta>0$ . Applications are many including in healthcare and recommendation systems. ", "page_idx": 0}, {"type": "text", "text": "[11] developed asymptotically (as $\\delta\\rightarrow0$ ) tight lower bound on sample complexity of $\\delta-$ correct algorithms for these BAI problems under the assumption that arms belong to a single parameter exponential family (SPEF). This assumption reduces a probability distribution to a single parameter and allows the analysis to better focus on certain aspects of the problem structure. We retain it for similar reasons. The sample complexity lower bounds involve solving an optimization problem that also identifies optimal proportion of allocations across arms. They also propose a track-and-stop algorithm that plugs in the empirical estimates of the distribution parameters in the lower bound and tracks the resulting approximations to optimal proportions of arms to sample. Although, this plug-in algorithm was shown to asymptotically match the lower bound, it involves repeatedly solving an optimization problem and is computationally demanding. [14] consider linear Gaussian bandits, [1] consider bandits with general distributions. Both the references propose track and stop algorithms where computation is sped up through batch processing. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Substantial literature has come up on \u201ctop-2\u2019 based, alternative faster and intuitively appealing algorithms to identify the best arm (see [25], [26] for Bayesian approaches; [24], [22], [16] for frequentist approaches). The algorithms essentially proceed by identifying at each stage an empirical winner arm, that is, an arm with the largest mean, and its closest challenger. The empirical arm is pulled with probability $\\beta$ , and the challenger arm with the complimentary probability. In the frequentist setting, in [16], the challenger arm is the one with the smallest ^index function'. Heuristically, this index function measures the likelihood of the challenger arm actually being the best one. The smaller the index function, more the likelihood. Further, with high probability, the index function increases with increased allocations to the corresponding arm. As is standard (see, e.g., [11], [19]), the algorithm is terminated when the generalized log-likelihood ratio (GLLR, given in Section 3) statistic exceeds a specified threshold. These algorithms are shown to be $\\beta$ optimal in the sense that they match the lower bound on sample complexity satisfied by algorithms that pull the best arm $\\beta$ fraction of times (see [15] for non-asymptotic analysis when $\\beta=1/2$ 0. However, determining optimal $\\beta$ has been an open problem that has generated considerable activity and that we address in this paper. ", "page_idx": 1}, {"type": "text", "text": "Contributions - Algorithm: The key insight from index based top-2 algorithm is that once a sample is given to a challenger arm with the smallest index, its index function increases. The net effect is that as the algorithm progresses, the challenger arm indexes tend to come close to each other and move together. We build upon the above insight. Through the first order conditions associated with the lower bound problem, we identify a function $g$ that equals zero under optimal allocation when the underlying arm distributions are known. We propose an anchored top-2 type algorithm where when $g>0$ , the empirical winner arm is pulled and that tends to decrease $g$ When $g<0$ , our algorithm pulls a challenger arm (arm with the smallest index function), and that typically increases $g$ We observe that the indexes of challenger arms that have been pulled, tend to rise up together until they catch up with arms with higher indexes. Once challenger arms associated with all the indexes have been pulled, call this the time to stability, then, since $g$ is close to zero and indexes of all challenger arms are close together, it can be seen that the proportionate samples to the empirical winner and the remaining arms are close to the optimal proportions as per the lower bound. This continues until the GLLR statistic exceeds a threshold, roughly of order $\\log(1/\\delta)$ . The time to stability can be bounded from above by a random time with finite expectation independent of $\\delta$ , while the time from stability till the GLLR statistic hits a threshold scales with $\\log(1/\\delta)$ with a constant that matches the lower bound. ", "page_idx": 1}, {"type": "text", "text": "Fluid model: Our other key contribution is to capture the above intuitive description through constructing an idealized fuid dynamics where $g$ stays equal to zero once it touches zero and where the indexes that have been pulled, remain equal and rise together as the algorithm progresses. We further show that the resulting equations have an invertible Jacobian. Implicit function theorem (IFT) (see [20, Appendix A.6] for an introduction to IFT) then becomes an important tool in analyzing this idealized fuid system as it allows the arm allocations $[N_{a}:a\\in K)$ to be unique functions of the overall allocation $N$ . IFT further allows us to identify the ordinary differential equations satisfied by $\\begin{array}{r}{N_{a}^{\\prime}=\\frac{d N_{a}}{d N}}\\end{array}$ $N$   \nbeen pulled and are increasing together with $N$ , meet another higher index. Once all the indexes have been pulled, our ode stabilizes so that the proportions $N_{a}/N$ thereafter remain constant and equal the optimal proportions as $N$ increases. IFT further helps show that the proposed algorithm remains close to the fuid dynamics, and matches the lower bound for small $\\delta$ . For completeness, in Appendix E.2, we also identify the ode paths under fuid dynamics for $\\beta$ top-2 algorithms. A great deal of technical analysis goes into showing that the algorithm, observed after sufficiently large amount of samples so that the sample means are close to the true means, is close to the fluid process and they both converge to the same limit. ", "page_idx": 1}, {"type": "text", "text": "Other related literature: [22] also develop a top-2 type algorithm for a single parameter family of distributions. There algorithm decides between the empirical best and the challenger arm based on directional change in a certain index (related to the LB) when the underlying allocation proportions are perturbed. It is less directly connected to the first order conditions in the LB problem compared to our algorithm. Empirically, we observe that the our proposed algorithm has lower sample complexity, and is computationally substantially faster (Their algorithm can be sub-optimal. We discuss this in Appendix D.2). [7] consider an algorithm structurally similar to ours. They focus on the BAI fixed budget (FB) setting where the total number of samples are fixed and the aim is to allocate samples to minimise the probability of incorrect selection. Unlike the fixed confidence (FC) setting (the one that we consider),the FB setting requires optimizing the first argument of relative entropy functions that appear in the lower bound. In FC setting, the second argument is optimized ([7] vary the first argument). Fundamentally, this is because FB is concerned with sample allocations that control the probability of the data conducting a large deviations to arrive at an incorrect conclusion, while FC is concerned with controlling sample allocations on high probability paths and gathering enough evidence to rule out the likelihood that the observed data is a result of large deviations. Furthermore, [7] prove weaker a.s. convergence results for associated indexes although not for allocations, and since they focus on FB settings, they do not provide sample complexity bounds or probabilistic false selection guarantees. Our analysis is more nuanced and structurally detailed, and we prove that the sample complexity of the proposed algorithm is asymptotically optimal. [28] study thebest- $k$ -arm identification problem in the BAI setting with fixed confidence and bring out the structural complexities that arise in lower bound analysis when $k>1$ .For $k=1$ ,they develop an asymptotically optimal top-2 algorithm when arm distributions are restricted to be Gaussian. [27] consider related pure exploration problems using Frank-Wolfe algorithm. Their implementation involves solving a linear program at each iteration. [17], [13], [6] provide algorithms that provide finite $\\delta$ sample complexity guarantees, however they are order optimal and do not match the constant in the lower bound. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Finally, while fuid analysis is common in many settings including mean field analysis and games (e.g., [4]), stochastic approximation (e.g., [5]) and queuing theory (e.g., [8]), to the best of our knowledge little or no work exists that arrives at it through IFT. ", "page_idx": 2}, {"type": "text", "text": "Roadmap: In Section 2, we describe the problem and develop lower bound related analysis. The proposed algorithm and our main result, Theorem 3.1, demonstrating algorithm's efficacy are stated in Section 3 where we also develop the relevant IFT framework. Section 4 spells out the fuid dynamics associated with the algorithm. Key steps involved in proving Theorem 3.1 are outlined in Section 5. We describe the numerical experiments in Section 6. Detailed proof of all results are in the appendix. ", "page_idx": 2}, {"type": "text", "text": "Key limitations: The proposed algorithm extends from SPEF to bounded random variables in a straightforward manner. While we do not provide supporting analysis (this limitation is due to space constraints), our numerical results in Appendix J suggest that our algorithm improves upon existing ones even in this setting. As is standard in the bandit literature, we also assume that samples from arm distributions are independent. Further, another limitation is the assumption of stationarity of the underlying distributions. This may be true when relatively short sampling horizons are involved. ", "page_idx": 2}, {"type": "text", "text": "2  Problem description and lower bound ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Distributional assumption: As mentioned earlier, we focus on arm distributions that belong to a knownSPEF. Let ${\\mathcal{S}}\\subset\\mathbb{R}$ denote the open set of possible means of the SPEF under consideration. The details related to SPEF are reviewed in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "Fixed confidence BAI set-up: Consider an instance with $K$ unknown probability distributions or arms, denoted by the mean vector $\\pmb{\\mu}\\,=\\,(\\mu_{1},\\ldots,\\mu_{K})$ , where each $\\mu_{i}\\,\\in\\,{\\mathcal{S}}$ (we refer to each $\\mu_{i}$ interchangeably as a distribution as well as its mean in the SPEF context). As is standard in the BAI framework, we assume that there is a unique arm with the largest mean. Thus, without loss of generality $\\mu_{1}>\\operatorname*{max}_{i\\geq2}\\mu_{i}$ . One way to handle the case where 2 or more arms are tied for the largest mean is to look for an $\\epsilon$ -best arm (an arm whose mean is within $\\epsilon$ of the best arm). However, that is technically a significantly more demanding problem (see [12]). Assuming uniqueness of the best arm and focusing on the best arm identification allows us to highlight the simple fluid dynamics underlying the proposed algorithm. ", "page_idx": 2}, {"type": "text", "text": "Algorithm: Given an unknown bandit instance $\\mu$ we consider algorithms that sequentially generate samples - if $A_{N}$ denotes the arm pulled at sample $N$ , and $X_{N}$ denotes the associated reward generated independently from distribution $\\mu_{A N}$ , then $A_{N}$ is chosen sequentially and adaptively as a function of generated $(A_{n},X_{n}:n=1,2,\\dots,N-1)$ . Further, an algorithm stops at some finite random stopping time $\\tau$ and announces the best arm. $\\delta-$ correct algorithm is an algorithm that, given a $\\delta>0$ stops at time $\\tau_{\\delta}>0$ and outputs a best arm estimate $k_{\\tau_{\\delta}}$ such that $\\mathbb{P}(\\tau_{\\delta}<\\infty,\\,k_{\\tau_{\\delta}}\\neq1)\\le\\delta$ . That is, it identifies the arm with highest mean with probability at least $1-\\delta$ . Our interest is in identifying a $\\delta$ -correct algorithm that minimizes $\\mathbb{E}\\!\\left[\\tau_{\\delta}\\right]$ . To this end lower bounds on sample complexity of $\\delta$ -correct algorithms are established using, e.g., the data processing inequality (see, e.g., [18]). We see that ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{x}~\\left\\{\\mathbb{E}[N_{1}]d(\\mu_{1},x)+\\mathbb{E}[N_{a}]d(\\mu_{a},x)\\right\\}\\geq\\log(1/(2.4\\delta))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $d(\\nu,x)$ denoting the Kullback-Leibler divergence between two distributions in $\\boldsymbol{S}$ with means $\\nu$ and $\\begin{array}{r}{x^{\\star}=\\frac{\\mu_{1}\\mathbb{E}\\left[N_{1}\\right]+\\mu_{a}\\mathbb{E}\\left[N_{a}\\right]}{\\mathbb{E}\\left[N_{1}\\right]+\\mathbb{E}\\left[N_{a}\\right]}}\\end{array}$ $x$ , and the expectation is under measure Wihiswbnod $\\mathbb{P}_{\\mu}$ associated with $\\begin{array}{r}{\\mathbb{E}[\\tau_{\\delta}]\\geq T^{\\star}(\\pmb{\\mu})\\log\\frac{1}{2.4\\delta}}\\end{array}$ $\\pmb{\\mu}$ . The infimum above is solved at where $T^{\\star}(\\pmb{\\mu})$ is the reciprocal of the optimal value of a max-min problem, ", "page_idx": 3}, {"type": "equation", "text": "$$\n(T^{\\star}(\\mu))^{-1}\\;=\\;\\operatorname*{max}_{\\omega=(\\omega_{a}:a\\in[K])\\in\\Sigma_{K}}\\ \\operatorname*{min}_{a\\neq1}\\ (\\omega_{1}d(\\mu_{1},x_{1,a})+\\omega_{a}d(\\mu_{a},x_{1,a}))\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x_{1,a}=(\\omega_{1}\\mu_{1}+\\omega_{a}\\mu_{a})/(\\omega_{1}+\\omega_{a})$ and $\\Sigma_{K}$ denotes a simplex in $K$ dimension. ", "page_idx": 3}, {"type": "text", "text": "The popular plug-in track and stop algorithm involves solving the max-min problem (1) repeatedly for optimal weights with empirical distribution plugged in for $\\pmb{\\mu}$ above. The algorithm at each stage $t$ , generates the next sample from an arm so that the proportion of arms sampled closely match the resulting optimal weights while ensuring an adequate, sub-linear exploration (e.g., each arm gets at least $\\sqrt{t}$ samples at each stage $t$ ", "page_idx": 3}, {"type": "text", "text": "Propositions 2.1 and 2.2 below are crucial for our analysis. Proposition 2.1 helps in constructing the fluid dynamics in Section 4. Proposition 2.2 provides a characterization of the unique optimal allocation $\\omega^{\\star}=(\\omega_{a}^{\\star}:a\\in[K])$ which motivates our algorithm's sampling strategy. Before stating the two propositions, we need some notation. Let $B\\subseteq[K]/\\{1\\}$ , and $\\overline{{B}}\\,=\\,B\\cup\\{1\\}$ .Whenever $\\overline{{B}}^{c}\\neq\\emptyset$ , let $N_{\\overline{{B}}^{c}}=(N_{a}\\geq0:a\\in\\overline{{B}}^{c})$ denote an allocation of samples to the arms in ${\\overline{{B}}}^{c}$ and we treat this as a constant in the following discussion and also in the statement of the two propositions. We define the quantity $N_{1,1}$ depending on $N_{\\overline{{B}}^{c}}$ in the following way: 1) If $\\overline{{B}}^{c}=\\emptyset$ $\\textstyle\\sum_{a\\in{\\overline{{B}}}^{c}}N_{a}=0$ \uff0c N1,1 is zero. 2) Otherwise, N1,1 is the value o N at which aeBe dg,s1.a for the given allocation $N_{\\overline{{B}}^{c}}$ . To see existence of such $N_{1,1}$ , observe that whenever $\\overline{{B}}^{c}\\neq\\emptyset$ and $\\textstyle\\sum_{a\\in{\\overline{{B}}}^{c}}N_{a}>0$ \uff0c the function N \u2192aeB dpal, is continuous and it monotonically decreases from $\\infty$ to O as $N_{1}$ increases from O to $\\infty$ . Hence, a unique $N_{1}$ exists where this function equals 1. We define the quantity $\\begin{array}{r}{N_{\\mathrm{min}}=N_{1,1}+\\sum_{a\\in\\overline{{B}}^{c}}N_{a}}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.1. For every positive $N$ satisfying $N\\,\\geq\\,N_{\\mathrm{min}}$ there is a unique set of variables $N_{\\overline{{B}}}(N)=(N_{a}(N):a\\in\\overline{{B}})$ and $I_{B}(N)$ satisfying the following conditions ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{a\\ne1}\\frac{d(\\mu_{1},x_{1,a})}{d(\\mu_{a},x_{1,a})}~=~1,\\quad w h e r e\\quad x_{1,a}=\\frac{N_{1}(N)\\cdot\\mu_{1}+N_{a}(N)\\cdot\\mu_{a}}{N_{1}(N)+N_{a}(N)},\\quad\\sum_{a\\in[K]}N_{a}(N)=N,\\ \\ \\right\\}}\\\\ {a n d,\\quad f o r\\;e v e r y\\;a\\in B,\\quad N_{1}(N)\\cdot d(\\mu_{1},x_{1,a})+N_{a}(N)\\cdot d(\\mu_{a},x_{1,a})\\;=\\;I_{B}(N).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Furthermore, $N_{\\overline{{B}}}(\\cdot)$ and $I_{B}(\\cdot)$ are continuously differentiable w.r.t. $N$ for $N>N_{\\operatorname*{min}}$ ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.2. Upon taking $B=[K]/\\{1\\}$ and $N=1$ $N_{\\overline{{B}}}(1)$ . as defined in Proposition 2.1 is same as the unique allocation $\\omega^{\\star}$ solving the max-min problem in $(I)$ .Further, $I_{B}(1)=T^{\\star}(\\pmb{\\mu})^{-1}$ Moreover, for every $N>0$ $f B=[K]/\\{1\\}$ , the unique solution $N_{\\overline{{B}}}(N)=(N_{a}(N):a\\in[K])$ satisfies $N_{a}(N)=N\\omega_{a}^{\\star}$ ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.1 is proved by applying the Implicit function theorem (IFT). Proposition 2.2 is subsumed by [11, Theorem 5], but we prove it using a different set of tools by applying the IFT. See Appendix $\\mathrm{D}$ for the detailed arguments. ", "page_idx": 3}, {"type": "text", "text": "For two vectors $\\pmb{\\nu}=(\\nu_{a}\\,\\in\\,\\mathcal{S}\\,:\\,a\\,\\in\\,[K])$ and $N=(N_{a}\\,\\in\\,\\mathbb{R}_{\\geq0}\\,:\\,a\\,\\in\\,[K])$ define the anchor function, g(v, N) = ae[k]/(j} avaa) , where $\\hat{\\boldsymbol j}=\\arg\\operatorname*{max}_{a}$ $\\nu_{a}$ \uff0c $\\nu_{\\mathrm{max}}=\\operatorname*{max}_{a}\\nu_{a}$ , and $z_{a}=(N_{\\hat{j}}\\nu_{\\operatorname*{max}}+N_{a}\\nu_{a})/(N_{\\hat{j}}+N_{a})$ for all $a\\neq{\\hat{j}}$ ", "page_idx": 3}, {"type": "text", "text": "Remark 2.1. It follows from Proposition 2.2 that the anchor function $g(\\pmb{\\mu},\\pmb{\\omega})\\,=\\,0$ and all the indexes $\\omega_{1}d(\\mu_{1},x_{1,a})+\\omega_{a}d(\\mu_{a},x_{1,a})$ equal to each other, uniquely identify the optimal proportion $\\omega^{\\star}$ solving the max-min problem (1) (see Appendix D.1 for an easier and more insightful derivation of these conditions). The algorithm proposed in Section 3 ensures that the empirical version of the anchorfunction $g(\\cdot)$ quickly becomes close to zero and thereafter remains close to zero. Further, the indexes sequentially come close to each other and once they are close, they stay close through the remaining steps of the algorithm. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3  Anchored Top-2 (AT2) Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Notation: Some notation is needed to help state the proposed algorithm. For every arm $a\\in[K]$ and iteration $N$ $\\widetilde{N}_{a}(N)$ denotes the number of times arm $a$ has been drawn till iteration $N$ , and $\\widetilde{N}(N)=(\\widetilde{N}_{a}(N):a\\in[K])$ . Thus, $\\begin{array}{r}{N=\\sum_{a}\\widetilde{N}_{a}(N)}\\end{array}$ . Let $\\widetilde{\\pmb{\\mu}}(N)=(\\widetilde{\\mu}_{a}(N):a\\in[K])$ where $\\widetilde{\\mu}_{a}(N)$ denotes the sample mean of arm $a$ at time $N$ , i.e., $\\begin{array}{r}{\\widetilde{\\mu}_{a}(N)=\\sum_{t=1}^{N}\\mathbb{I}(A_{t}=a)\\cdot X_{t}/\\widetilde{N}_{a}(N)}\\end{array}$ and $\\begin{array}{r}{\\hat{i}_{N}=\\arg\\operatorname*{max}_{a\\in[K]}\\;\\widetilde{\\mu}_{a}(N)}\\end{array}$ , with an arbitrary tie breaking rule. ", "page_idx": 4}, {"type": "text", "text": "For every pair of arms $a,b$ , define ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{a,b}(N)\\!=\\!\\frac{\\widetilde{N}_{a}(N)\\cdot\\mu_{a}+\\widetilde{N}_{b}(N)\\cdot\\mu_{b}}{\\widetilde{N}_{a}(N)+\\widetilde{N}_{b}(N)},\\,\\quad\\mathrm{~and~}\\quad\\widetilde{x}_{a,b}(N)\\!=\\!\\frac{\\widetilde{N}_{a}(N)\\cdot\\widetilde{\\mu}_{a}(N)+\\widetilde{N}_{b}(N)\\cdot\\widetilde{\\mu}_{b}(N)}{\\widetilde{N}_{a}(N)+\\widetilde{N}_{b}(N)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let, $I_{a,b}(N)\\;=\\;\\widetilde{N}_{a}(N)\\,\\cdot\\,d(\\mu_{a},x_{a,b}(N))\\,+\\,\\widetilde{N}_{b}(N)\\,\\cdot\\,d(\\mu_{b},x_{a,b}(N))$ , and $\\mathcal{Z}_{a,b}(N)\\;=\\;\\widetilde{N}_{a}(N)$ $d\\left(\\widetilde{\\mu}_{a}(N),\\widetilde{x}_{a,b}(N)\\right)+\\widetilde{N}_{b}(N)\\cdot d\\left(\\widetilde{\\mu}_{b}(N),\\widetilde{x}_{a,b}(N)\\right)\\!.$ For $a\\neq\\hat{i}_{N}$ , we call $I_{\\hat{i}_{N},a}(N)$ , and $\\mathcal{T}_{\\hat{i}_{N},a}(N)$ \uff0c respectively, actual index (or, simply index) and empirical index of arm $a$ at iteration $N$ , and denote them using $I_{a}(N)$ , and $\\mathcal{Z}_{a}(N)$ . For notational simplicity, we hide the dependency on $N$ whenever it doesn't cause confusion. Note that $\\mathcal{Z}_{a}(N)$ is a function of $\\widetilde{N}_{{\\widehat{i}}_{N}}(N),\\;\\widetilde{N}_{a}(N),\\;\\widetilde{\\mu}_{{\\widehat{i}}_{N}}(N)$ and $\\widetilde{\\mu}_{a}(N)$ ", "page_idx": 4}, {"type": "text", "text": "Stopping Rule: As is typical in this literature, in our algorithm below, we follow a generalized log likelihood ratio (GLLR) to decide when to stop the algorithm. It is easy to check that $\\mathrm{min}_{a\\in[K]/\\{\\hat{i}\\}}\\,\\mathcal{I}_{a}(N)$ denotes the GLLR, that is log of likelihood function (LF) evaluated at maximum likelihood estimator (MLE) divided by the LF evaluated at MLE of parameters restricted to alternate set with a different best arm compared to MLE (see [11, Section 3.2] for a detailed derivation). Define stopping time $\\tau_{\\delta}\\:=\\:\\mathrm{inf}\\{N|\\:$ for all $a\\in[K]/\\{\\hat{i}\\}$ \uff0c $\\mathcal{T}_{a}(N)\\;>\\;\\beta(N,\\delta)\\}$ for an appropriate choice of threshold $\\beta(N,\\delta)$ . After stopping at $\\tau_{\\delta}$ , the algorithm outputs $\\hat{i}_{\\tau_{\\delta}}$ as the best arm. [19, Eq. 25, Section 5.1] argued that for instances in SPEF, upon choosing $\\beta(N,\\delta)\\approx\\log((K-1)/\\delta)+6\\log{(\\log(N/2)+1)}+8\\log(1+2\\log((K-1)/\\delta))$ , the GLLR based stopping rule is $\\delta$ -correct for any sampling strategy including the one we propose. In our numerical experiments, we follow [11] and choose a smaller threshold, $\\beta(N,\\delta)=\\bar{\\log}(\\bar{(}1+\\log N)/\\delta)$ ", "page_idx": 4}, {"type": "text", "text": "Description of the AT2 and Improved AT2 (IAT2) Algorithm: The AT2 algorithm takes in confidence parameter $\\delta\\:>\\:0$ and exploration parameter $\\alpha\\,\\in\\,(0,1)$ as inputs, and executes the following steps at iteration $N$ ", "page_idx": 4}, {"type": "text", "text": "1. Let VN  {a E [K]| Na(N - 1) < N\u03b1} be the set of under-explored arms.   \n2. If $\\mathcal{V}_{N}\\neq\\emptyset$ , choose $A_{N}=\\arg\\operatorname*{min}_{a\\in[K]}\\;\\widetilde{N}_{a}(N-1)$ , and go to step 5.   \n3. Else,if $g(\\widetilde{\\pmb{\\mu}}(N-1),\\widetilde{\\pmb{N}}(N-1))>0.$ choose the empirically best arm i.. $A_{N}=\\hat{i}_{N-1}$ , and go to step 5.   \n4. Else, if $g(\\widetilde{\\pmb{\\mu}}(N\\mathrm{~-~}1),\\widetilde{\\pmb{N}}(N\\mathrm{~-~}1))\\mathrm{~\\pm~\\}0,$ .. choose  the_ challenger arm i.e.. $\\begin{array}{r l}{A_{N}}&{{}=}\\end{array}$ arg $\\begin{array}{r}{\\operatorname*{min}_{a\\in[K]/\\{\\hat{i}_{N-1}\\}}\\;\\mathcal{T}_{a}(N-1)}\\end{array}$ using some arbitrary tie breaking rule, and go to step 5.   \n5. Sample $X_{N}$ from $A_{N}$ and update ${\\widetilde{\\pmb{\\mu}}}(N)$ and $\\widetilde{N}(N)$ using $X_{N},A_{N}$   \n6. If $\\begin{array}{r}{\\operatorname*{min}_{a\\in[K]/\\{\\hat{i}_{N}\\}}\\mathcal{Z}_{a}(N)>\\beta(N,\\delta)}\\end{array}$ terminate and return $\\hat{i}_{N}$ ", "page_idx": 4}, {"type": "text", "text": "Inspired from the Improved Transportation Cost Balancing (ITCB) policy for selecting the challenger arm in [16], Improved AT2 (IAT2) algorithm has the same input and follows the same strategy for exploration (step 1 and 2) and choosing the best arm (step 3) as AT2. IAT2 differs from AT2 only by its choice of the challenger arm in step 4, where IAT2 samples from the arm $A_{N}\\;=\\;$ arg $\\operatorname*{min}_{a\\in[K]/\\{\\hat{i}_{N-1}\\}}$ $(\\mathcal{T}_{a}(N-1)+\\log\\widetilde{N}_{a}(N-\\^{\\dagger}1))$ ", "page_idx": 4}, {"type": "text", "text": "Empirically, we see that typically IAT2 performs better than AT2 with respect to sample complexity.   \nIn the appendix, we provide pseudo-codes of AT2 and IAT2 in Algorithms 1 and 2, respectively. ", "page_idx": 5}, {"type": "text", "text": "3.1  Theoretical guarantees ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Proposition 3.1 below shows that the allocations made by AT2 and IAT2 algorithms converge to the optimal allocations $\\omega^{\\star}$ w.p. 1 in $\\mathbb{P}_{\\mu}$ . For every $a\\in[K]$ we define $\\widetilde{\\omega}_{a}(N)=\\widetilde{N}_{a}(N)/N$ , and use $\\tilde{\\omega}(N)=(\\widetilde{\\omega}_{a}(N):a\\in[\\bar{K}])$ to denote the algorithm's proportion at iteration $N$ ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.1 (Convergence to optimal proportions). There exists a random time $T_{s t a b l e}$ and $a$ constant $C_{1}>0$ dependingon $\\pmb{\\mu},\\alpha,$ and $K$ andindependent of $\\delta$ suchthat, $\\mathbb{E}_{\\mu}[T_{s t a b l e}]<\\infty$ and for every $N\\geq T_{s t a b l e}$ and arm $a\\in[K]$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\widetilde{\\omega}_{a}(N)-\\omega_{a}^{\\star}|\\leq C_{1}N^{\\frac{-3\\alpha}{8}},\\quad a n d\\quad|\\widetilde{\\mu}_{a}(N)-\\mu_{a}|\\leq\\epsilon(\\mu)N^{\\frac{-3\\alpha}{8}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\epsilon(\\mu)$ is a positive constant depending only on $\\pmb{\\mu}$ and defined inAppendix $B$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 (Asymptotic optimality of AT2 and IAT2). Both AT2 and IAT2 are $\\delta$ -correctover instances in $\\boldsymbol{S}$ ,and are asymptotically optimal, i.e., for both the algorithms, the corresponding stopping times satisf, $\\begin{array}{r}{\\operatorname{im}\\operatorname*{sup}_{\\delta\\to0}\\frac{E_{\\mu}[\\tau_{\\delta}]}{\\log(1/\\delta)}\\leq T^{\\star}(\\pmb{\\mu}),}\\end{array}$ and $\\begin{array}{r}{\\mathrm{im}\\,\\mathrm{sup}_{\\delta\\to0}\\,\\frac{\\tau_{\\delta}}{\\log(1/\\delta)}\\leq T^{\\star}(\\pmb{\\mu})}\\end{array}$ a.s. in $\\mathbb{P}_{\\mu}$ Moreover, we can find a constant $C>0$ depending ontheinstance $\\pmb{\\mu}$ and $\\alpha$ ,such that, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tau_{\\delta}~\\leq~\\operatorname*{max}\\{\\,T_{s t a b l e},\\;T^{\\star}(\\mu)\\cdot\\log(1/\\delta)+C\\left(\\log(1/\\delta)\\right)^{1-3\\alpha/8}\\ \\}\\quad a.s.\\ i n\\quad{\\mathbb P}_{\\mu}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof idea of Theorem 3.1: We assume Proposition 3.1 and sketch the argument by which asymptotic optimality follows from it. For $N\\geq T_{s t a b l e}$ , and $a\\in[K]$ , from Proposition 3.1, $\\widetilde{N}_{a}(N)\\approx\\omega_{a}^{\\star}N$ and $\\bar{\\mu_{a}}(N)\\approx\\mu_{a}$ . As a result, from Proposition 2.2, after $T_{s t a b l e}$ $\\mathscr{Z}_{a}(N)\\approx N I_{[K]/\\{1\\}}(1)=N T^{\\star}(\\pmb{\\mu})^{-1}$ for every $a\\neq1$ Therefore,if $\\mathrm{min}_{a\\neq1}\\,\\mathcal{T}_{a}(N)$ crosses $\\beta(N,\\delta)$ at $N=\\tau_{\\delta}$ , since $\\beta(N,\\delta)=\\log(1/\\delta)+$ $O(\\log\\log(1/\\delta)\\!+\\!\\log\\log(N))$ , we have $\\tau_{\\delta}T^{\\star}(\\pmb{\\mu})^{-1}\\approx_{\\delta\\rightarrow0}\\log(1/\\delta)\\!+\\!O(\\log\\log(1/\\delta)\\!+\\!\\log\\log(\\tau_{\\delta}))$ Which gives in $\\begin{array}{r}{\\operatorname*{sup}_{\\delta\\to0}\\frac{\\tau_{\\delta}}{\\log(1/\\delta)}\\leq T^{\\star}(\\pmb{\\mu})}\\end{array}$ a.s.in $\\mathbb{P}_{\\pmb{\\mu}}$ Detailed prorisin Appendik H. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "We outline the key steps of the proof of Proposition 3.1 for AT2 in Section 5, and the detailed proof is in Appendix G.2. Similar arguments hold for IAT2. Considerable technical effort goes in proving this proposition due to the noise in the empirical estimate ${\\widetilde{\\pmb{\\mu}}}(N)$ , resulting in noise in the anchor function and the empirical indexes. However, before presenting the proof sketch, in the next section, we first observe the algorithm's dynamics in the limiting fuid regime where this noise is zero. Several of the important proof steps for the algorithmic allocations rely on insights from the simpler fuid model. ", "page_idx": 5}, {"type": "text", "text": "4 Fluid dynamics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Motivation: The fuid dynamics idealizes our algorithm's evolution through making assumptions at each iteration $N$ that hold for the algorithm in the limit as the number of samples increase to infinity. Unlike the real setting with discrete samples, here we treat samples as a continuous object getting distributed between different arms as the sampling budget (also referred to as \u2018time') evolves. We denote the no. of samples allocated to an arm $a\\in[K]$ at some time $N>0$ using $N_{a}(N)$ , and define the tuple $N(N)=(\\bar{N}_{a}(N):a\\in[K])$ . Note that $\\begin{array}{r}{\\sum_{a\\in[K]}N_{a}(N)=N}\\end{array}$ The rate $N_{a}^{\\prime}(N)$ at which samples get allocated to arm $a$ at time $N$ depends on a continuous version of the AT2 algorithm, which we refer to as the algorithm's fuid dynamics. We define the index of arm $a\\ne1$ at time $N$ as, $I_{a}(N)=N_{1}(N)\\cdot d(\\mu_{1},x_{1,a}(N))+N_{a}(N)\\cdot d(\\mu_{a},x_{1,a}(N))$ where $\\begin{array}{r}{x_{1,a}(N)=\\frac{N_{1}(N)\\mu_{1}+N_{a}(N)\\mu_{a}}{N_{1}(N)+N_{a}(N)}}\\end{array}$ Notice that $I_{a}(N)$ defined in Section 3 is the index of arm $a$ with respect to the algorithm's allocation $\\widetilde{N}(N)$ , whereas in our current context, $I_{a}(N)$ represents the index with respect to the fluid allocations $N(N)$ ", "page_idx": 5}, {"type": "text", "text": "Description of the fuid dynamics: First we explain the fluid dynamics in words. We formally characterize the fluid allocation $N(N)$ via a system of ODEs in Theorem 4.1. Later in this section, we exploit the obtained ODEs to argue that, after starting the fuid dynamics from some time $N^{0}\\,\\stackrel{\\bullet}{>}\\,0$ , the allocations $N(N)$ reach the optimal proportions $\\omega^{\\star}\\,=\\,(\\omega_{a}^{\\star}\\,:\\,a\\,\\in\\,[K])$ by a time atmost $\\left(\\operatorname*{min}_{a\\in[K]}\\omega_{a}^{\\star}\\right)^{-1}\\cdot N^{0}$ In other words, for $N\\geq\\left(\\operatorname*{min}_{a\\in[K]}\\omega_{a}^{\\star}\\right)^{-1}\\cdot N^{0}$ we have $N_{a}(N)=$ $\\omega_{a}^{\\star}\\cdot N$ for every arm $a\\in[K]$ irrespective of the initial allocation we had at time $N^{0}$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "For notational simplicity, we hide the dependency on $N$ , whenever it doesn't cause any confusion. Recall the anchor function $g(\\cdot)$ introduced in Section 2. We use $g$ to denote $g(\\pmb{\\mu},N(N))$ . For every subset $A\\subseteq[K]/\\{1\\}$ , we use $\\overline{{A}}$ to denote $A\\cup\\{1\\}$ ", "page_idx": 6}, {"type": "text", "text": "We start the fluid dynamics at time $N^{0}>0$ with some initial allocation $N^{0}=(N_{a}^{0}\\geq0:a\\in[K])$ We assume that the vector of true means $\\pmb{\\mu}$ is known. The fuid dynamics evolves according to the following steps at a given total allocation $N\\geq N^{0}$ : 1) If $g>0$ , then $N_{1}$ increases with $N$ while other $N_{a}$ 's for $a\\ne1$ are held constant till $g=0$ $\\mathit{\\Delta}_{g}$ can be seen to be a monotonically decreasing function of $N_{1}$ ).2) If $g\\,=\\,0$ , let $B$ denote the set of minimum indexes. Thus, $I_{a}(N)$ are equal for all $a\\in B$ (the equal value is denoted by $I_{B}(N))$ and $I_{a}(N)>I_{B}(N)$ for all $a\\in\\overline{{B}}^{c}$ . Then, as $N$ increases, allocations $N_{1}$ and $(N_{a}:a\\in B)$ increase such that $g$ remains equal to zero, while the indexes in $B$ remain equal. In Proposition 2.1, we characterize and prove existence of such allocations, which the fuid dynamics will track. Later we observe that, $I_{B}$ increases atleast at a linear rate and indexes of arms in ${\\overline{{B}}}^{c}$ stay bounded from above by a constant. 3) If $g<0$ , let $B$ be the set of minimum index arms and $I_{B}$ be the index of arms in $B$ . In this situation, $(N_{a}:a\\in B)$ increase with $N$ keeping index of the arms in $B$ equal, while $N_{1}$ and $(N_{a}:a\\in\\overline{{B}}^{c})$ are unchanged. With this $g$ also increases, since $g$ is a strictly increasing function of $N_{a}$ for every $a\\,\\in\\,B$ .The dynamics in this case are simple and described in Proposition E.1 of Appendix E. 4) Once, $g=0$ and $B=\\{2,\\ldots,K\\}$ , we show that each allocation increases linearly with $N$ such that $N_{a}^{\\prime}=\\omega_{a}^{\\star}$ ", "page_idx": 6}, {"type": "text", "text": "The fuid ODEs: In Appendix E, we argue that if the fuid dynamics has $g\\neq0$ at time $N^{0}$ , then $g$ becomes zero within a finite time by following step 1 or step 3 of the description. This is easy to observe when $g\\,>\\,0$ at $N^{0}$ , because $g$ is strictly decreasing in $N_{1}$ , and $g\\rightarrow-1$ as $N_{1}\\to\\infty$ Therefore, following step 1, $g$ becomes O at some finite $N$ .We now consider the situation where $g=0$ at some $N\\stackrel{\\triangledown}{>}N^{0}$ . Setting $B$ to the set of minimum index arms, the algorithm evolves by tracking the allocation $N_{\\overline{{B}}}(N)=(N_{a}(N):a\\in\\overline{{B}})$ defined through the system (2) in Proposition 2.1. By Proposition 2.1, $\\bar{N}_{\\overline{{B}}}(N)$ is continuously differentiable w.r.t. $N$ .Applying IFT to (2), we obtain the ODEs via which the allocations and the indexes evolve and present them in Theorem 4.1. ", "page_idx": 6}, {"type": "text", "text": "Some defnitions: Let $\\begin{array}{r}{f(\\pmb{\\mu},a,N)=-\\frac{\\partial}{\\partial x}\\left(\\frac{d(\\mu_{1},x)}{d(\\mu_{a},x)}\\right)\\Big|_{x=x_{1,a}}.\\;f(\\pmb{\\mu},a,N)}\\end{array}$ $\\frac{d(\\mu_{1},x)}{d(\\mu_{a},x)}$ is strictly decreasing with $x$ for $x\\in\\left(\\mu_{a},\\mu_{1}\\right)$ ", "page_idx": 6}, {"type": "text", "text": "Let $\\varDelta_{a}\\,=\\,\\mu_{1}\\,-\\,\\mu_{a}$ and $\\begin{array}{r l r}{h_{a}(\\pmb{\\mu},N_{1},N_{a})}&{{}\\!=\\!}&{f(\\pmb{\\mu},a,\\pmb{N})\\frac{N_{1}^{2}\\varDelta_{a}}{(N_{1}+N_{a})^{2}}}\\end{array}$ .For notational simplity, we denote $h(\\pmb{\\mu},N_{1},N_{a})$ by $h_{a}$ . Further, for each $a$ we denote $d(\\mu_{1},x_{1,a})$ by $d_{1,a}$ and $d(\\mu_{a},x_{1,a})$ by $d_{a,a}$ . Recall that for given allocations $(N_{a}:a\\in[K])$ $B$ denotes a setsuchthat $N_{1}d_{1,a}+N_{a}d_{a,a}=$ $I_{B}(N)$ for all $a\\in B$ and $N_{1}d_{1,a}+N_{a}d_{a,a}>I_{B}(N)$ for all $a\\in\\overline{{B}}^{c}$ . Let $\\begin{array}{r}{h(B)=\\sum_{a\\in B}h_{a}d_{a,a}^{-1}}\\end{array}$ \uff0c $\\begin{array}{r}{h(N)=\\sum_{a\\in\\overline{{B}}^{c}}h_{a}N_{a}}\\end{array}$ and $\\begin{array}{r}{d_{B}=\\left(\\sum_{a\\in B}d_{a,a}^{-1}\\right)^{-1}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 (Fluid ODEs). If at total allocation $N\\geq N^{0}$ , we have $g=0,$ and $B$ is the set of minimum index arms, ie. $B=a r g\\operatorname*{min}_{a\\in[K]/\\{1\\}}$ $I_{a}(N)$ then the following holds true: ", "page_idx": 6}, {"type": "text", "text": "1. As $N$ increases,and till $I_{B}(N)$ increases to hit an index in ${\\overline{{B}}}^{c}$ \uff0c", "page_idx": 6}, {"type": "equation", "text": "$$\nN_{1}^{\\prime}\\!=\\!\\frac{N_{1}h(B)}{(N_{1}+\\sum_{a\\in B}N_{a})h(B)+d_{B}^{-1}h(N)},\\ \\ a n d\\ \\ N_{b}^{\\prime}\\!=\\!\\frac{N_{b}h(B)+d_{b,b}^{-1}h(N)}{(N_{1}+\\sum_{a\\in B}N_{a})h(B)+d_{B}^{-1}h(N)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "forall b E B. Itfollows that I(N) = (N+as Nh(B)+dh(N) ", "page_idx": 6}, {"type": "text", "text": "2. Furthermore, fora E B,I(N) = Nda= (N+aB Nb()+dhN) ", "page_idx": 6}, {"type": "text", "text": "3. There exists a $\\beta>0$ independent of $N$ such that $I_{B}^{\\prime}(N)>\\beta$ In addition, for $a\\in\\overline{{B}}^{c}$ \uff0c $N_{a}^{\\prime}=0$ $I_{a}(N)\\leq N_{a}^{0}d(\\mu_{a},\\mu_{1})$ , thus the index is bounded from above. Thus, $i f\\overline{{B}}^{c}\\neq\\emptyset$ $I_{B}(N)$ eventually catchesupwith another index in ${\\overline{{B}}}^{c}$ . In this way, the set $B$ grows into $\\{2,\\ldots,K\\}$ ", "page_idx": 6}, {"type": "text", "text": "Indexes once they meet must stay together: In Appendix F.1 we argue via contradiction that in our fuid dynamics, once a set of smallest indexes that are equal, increase and catch up with another index, their union then remains equal and increases together with $N$ . This argument is important as it motivates the proof in our algorithm that after sufficient amount of samples, once a sub-optimal arm is pulled, its index stays close to indexes of the other arms that have been pulled. ", "page_idx": 7}, {"type": "text", "text": "Bounding the time to reach optimal proportion: We define $N^{\\star}$ to be smallest time after $N^{0}$ at which the fluid dynamics has both $B=\\{2,\\ldots,K\\}$ and $g=0$ . Let $(N_{a}^{\\star}:a\\in[K])$ be the allocation at $N^{\\star}$ . We first argue that: there exists $i\\in[K]$ such that $N_{i}^{\\star}=N_{i}^{0}$ . We have argued before that if $g\\neq0$ at $N^{0}$ , then $g$ becomes zero by some finite time, which we call $M$ . By definition $M\\leq N^{\\star}$ Now if $B\\neq\\{2,\\ldots,K\\}$ at $M$ or $M=N^{0}$ , then after time $M$ the fluid dynamics evolve by the ODEs in (3) and $N^{\\star}$ is the time at which $B$ becomes $\\{2,\\ldots,K\\}$ , which is finite by statement 3 of Theorem 4.1. In this case $i$ is the last element to be added to $B$ . Otherwise if $\\boldsymbol{B}\\overset{\\cdot}{=}\\lbrace2,\\ldots,K\\rbrace$ at $M$ and $M>N^{0}$ , the only way this can happen is $g<0$ in $[N^{0},M)$ . In this case, $i=1$ and $M=N^{\\star}$ . Since $g=0$ and $B=\\{\\dot{2},\\ldots\\}=K\\}$ at time $N^{\\star}$ , Proposition 2.2 implies $N_{a}^{\\star}=\\omega_{a}^{\\star}N^{\\star}$ for all $a$ Combining our observations, we have $\\omega_{i}^{\\star}N^{\\star}=N_{i}^{\\star}=N_{i}^{0}\\le N^{0}$ Hence $\\begin{array}{r}{N^{\\star}\\,\\leq\\,\\frac{N^{0}}{\\omega_{i}^{\\star}}\\,\\leq\\,(\\operatorname*{min}_{a\\in[K]}\\omega_{a}^{\\star})^{-1}N^{0}}\\end{array}$ Thus $N^{\\star}$ is within a constant times of $N^{0}$ .We bound $T_{s t a b l e}$ of Proposition 5.1 using a similar argument. ", "page_idx": 7}, {"type": "text", "text": "Remark 4.1 (Incorporating the stopping rule into the fluid dynamics). At stopping time the idealized GLLR (which is the GLLR defined in Section 3 by replacing the estimated means with the true means) just exceeds $\\log(1/\\delta)$ . Since the idealized GLLR grows linearly with the allocated samples, the stopping time increases linearly with $\\log(1/\\delta)$ . Since the time for fluid dynamics to reach stability is independent of $\\delta$ , for small $\\delta$ , stability will be reached before the algorithm stops. ", "page_idx": 7}, {"type": "text", "text": "Remark 4.2 ( $\\beta$ -fluid dynamics). In Appendix E.2, we construct the fuid dynamics for the $\\beta$ -EBTCB algorithm [16] using IFT. We prove that, for every $\\beta\\in(0,1)$ , the $\\beta$ -fluid dynamics started at some time $N^{0}>0$ reach the $\\beta$ -optimal proportion (which is the solution to the max-min problem 1 with the added constraint $\\omega_{1}=\\beta$ ) by a time which is a constant times $N^{0}$ ", "page_idx": 7}, {"type": "text", "text": "5   Convergence of algorithmic allocations to the optimal proportions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now outline the proof steps for Proposition 3.1. To simplify our analysis, we analyze the AT2 algorithm after the random time $T_{0}$ defined as, ", "page_idx": 7}, {"type": "equation", "text": "$$\nT_{0}=\\operatorname*{inf}{\\Bigl\\{}N^{\\prime}\\geq1~{\\Big|}~\\forall a\\in[K]~{\\mathrm{and}}~N\\geq N^{\\prime},~|{\\tilde{\\mu}}_{a}(N)-\\mu_{a}|\\leq\\epsilon(\\mu)\\cdot N^{-3\\alpha/8}{\\Bigr\\}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "after which the estimates ${\\widetilde{\\pmb{\\mu}}}(N)$ are converging to $\\pmb{\\mu}$ . Recall that $\\alpha\\,\\in\\,(0,1)$ is the exploration parameter, and $\\epsilon(\\pmb{\\mu})>0$ is a constant depending only on $\\pmb{\\mu}$ . By the definition of $\\epsilon(\\mu)$ in Appendix $\\mathbf{B}$ , we have $\\widetilde{\\mu}_{a}(N)<\\widetilde{\\mu}_{1}(N)$ for all $a\\ne1$ and $N\\geq T_{0}$ . As a result, arm 1 becomes the empirically best arm after $T_{0}$ . In Appendix G.3, we use Chernoff's bound to prove that $\\mathbb{P}_{\\mu}(T_{0}=n\\mathbf{\\bar{+}}1)\\doteq$ $\\exp(-\\varOmega(n^{\\alpha/4}))$ , which implies $\\mathbb{E}_{\\mu}[T_{0}]<\\infty$ . In the folowing discussion, all the results mentioned are true for both AT2 and IAT2 algorithms. ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.1 shows that the allocations made by the proposed algorithm converges to the first order condition satisfied by the optimal proportion $\\omega^{\\star}=(\\omega_{a}^{\\star}:a\\in[K])$ at a rate $O(N^{-3\\alpha/8})$ ,where $\\alpha$ is the exploration parameter. ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.1. There exists a random time $T_{s t a b l e}\\geq T_{0}$ satisfying $\\mathbb{E}_{\\mu}[T_{s t a b l e}]<\\infty$ and a constant $C_{2}>0$ depending on $\\pmb{\\mu},\\alpha$ and $K$ ,and independent of the sample paths, such that, for $N\\geq T_{s t a b l e}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n|g(\\pmb{\\mu},\\widetilde{\\omega}(N))|~=~\\Big|~\\sum_{a\\neq1}\\frac{d(\\mu_{1},x_{1,a}(N))}{d(\\mu_{a},x_{1,a}(N))}-1~\\Big|~\\le~C_{2}N^{-3\\alpha/8},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a,b\\in[K]/\\{1\\}}|I_{a}(N)-I_{b}(N)|\\ \\leq\\ C_{2}N^{1-3\\alpha/8}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Before outlining the proof of Proposition 5.1, we explain how Proposition 3.1 follows from Proposition 5.1 just using the IFT. ", "page_idx": 7}, {"type": "text", "text": "Proof idea of Proposition 3.1: If our algorithm follows optimal proportions at time $N$ i.e., $\\widetilde{N}_{a}(N)=$ $\\omega_{a}^{\\star}N$ for all $a\\in[K]$ , RHS of (4) and (5) becomes zero by Proposition 2.2. Moreover, by Proposition $2.2\\;\\omega^{\\star}$ uniquely satisfies the conditions: anchor function is zero and all alternative arms have equal index. (4) and (5) imply that, $\\widetilde{\\omega}(N)$ satisfies these conditions upto a perturbation of $C_{2}N^{-3\\alpha/8}$ after $T_{s t a b l e}$ . Using the IFT, we prove that the algorithm's allocation is a Lipschitz continuous function of the perturbation when it is sufficiently small. Hence, by choosing $T_{s t a b l e}$ large enough and using Lipschitzness, we get $\\operatorname*{max}_{-}{\\mathord{\\left[{K}\\right]}}\\left|\\widetilde{\\omega}_{a}(N)-\\omega_{a}^{\\star}\\right|=O(N^{-3\\alpha/8})$ . Closeness of $\\widetilde{\\pmb{\\mu}}(\\cdot)$ to $\\pmb{\\mu}$ follows from the fact that $T_{s t a b l e}\\geq T_{0}$ \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Proof idea of Proposition 5.1: We separately outline the proofs of (4) and (5) in Proposition 5.1. In the following discussion, constants hidden in $O(\\cdot),\\varOmega(\\cdot)$ and $\\Theta(\\cdot)$ notations are independent of the sample path after time $T_{s t a b l e}$ . To simplify our analysis, we choose $T_{s t a b l e}$ such that exploration stops after $T_{s t a b l e}$ , i.e., $\\mathcal{V}_{N}=\\emptyset$ for $N\\geq T_{s t a b l e}$ (see the discussion before Definition G.1 in Appendix G.1.1 for justification). ", "page_idx": 8}, {"type": "text", "text": "Key ideas in the proof of (4): We prove (4) via induction. We prove the existence of a constant $D>0$ such that at every $N\\geq T_{s t a b l e}$ , whenever the actual anchor value $g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(\\pmb{N}))$ (we denote using $g(N))$ satisfies $|g(N)|>D N^{-3\\alpha/8}$ , our algorithm pushes $g(\\cdot)$ towards zero by $\\Theta(1/N)$ in the next iteration through steps 3 and 4. Whereas the interval $[-C_{2}N^{-3\\alpha/8},C_{2}N^{-3\\alpha/8}]$ shrinks by $O(N^{-(1+3\\alpha/8)})$ from both ends. Since $N^{-(1+3\\alpha/8)}<<N^{-\\bar{1}}$ wechoosetheconstant $C_{2}$ large enoughsuch that $g(\\cdot)$ stays in the said interval in iteration $N+1$ ", "page_idx": 8}, {"type": "text", "text": "Key ideas in the proof of (5): The following lemma forms a crucial part of the argument for proving closeness of the indexes in the non-fluid setting. ", "page_idx": 8}, {"type": "text", "text": "Lemma 5.1.There exists a random time $T_{g o o d}\\in[T_{0},T_{s t a b l e}]$ such that thealgorithmpicks allthe alternative arms in $[K]/\\{1\\}$ atleastoncebetweentheiterations $T_{g o o d}$ and $T_{s t a b l e}$ . Moreover, for $N\\geq T_{g o o d},$ ifthealgorithmpickssomearm $a\\in[K]/\\{1\\}$ at iteration $N$ thenit picksarmaagain within a next $O(N^{1-3\\alpha/8})$ iterations. ", "page_idx": 8}, {"type": "text", "text": "Proof of Lemma 5.1 (in Appendix G.1.2) is technically involved and requires proving several supplementary lemmas. Several of the key steps of this proof borrow insights from the fluid dynamics, and we outline them in Appendix F. Here we assume Lemma 5.1 and sketch the argument by which (5) follows from it for the AT2 algorithm. For any $a,b\\neq1$ and after any $N\\,\\geq\\,T_{s t a b l e}$ \uff0c $\\mathcal{T}_{a}(\\cdot)$ and $\\mathcal{T}_{b}(\\cdot)$ crosses each other before the algorithm picks both $a,b$ atleast once. We can show that, for $j\\,=\\,a,b$ and $N\\,\\geq\\,T_{s t a b l e}$ \uff0c $\\mathbb{Z}_{j}(N)$ and $I_{j}(N)$ differ by $O(N^{1-3\\alpha/8})$ .As a result, when $\\mathcal{T}_{a}(\\cdot)$ crosses $\\mathcal{T}_{b}(\\cdot)$ at $N+R$ , we have $\\vert I_{a}(N+R)\\,-\\,I_{b}(N+R)\\vert\\,=\\,{\\cal O}((N+R)^{1-3\\alpha/8})\\,=$ $O(N^{1-3\\alpha/8})$ since $R\\,=\\,{\\cal O}(N^{1-3\\alpha/8})$ . For $j\\;=\\;a,b$ the partial derivatives of $I_{j}(\\cdot)$ w.r.t. ${\\widetilde{N}}_{1}$ and $\\widetilde{N}_{j}$ are non-negative and bounded from above by $\\operatorname*{max}\\{d(\\mu_{1},\\mu_{j}),d(\\mu_{j},\\mu_{1})\\}=O(1)$ As a result, $|I_{j}(N+R)-I_{j}(N)|=O(R)=O(N^{1-3\\alpha/8})$ for $j\\,=\\,a,b$ Hence, $\\begin{array}{r}{|I_{a}(N)-I_{b}(N)|\\le}\\end{array}$ $\\begin{array}{r}{|I_{a}(N+R)-I_{b}(N+R)|+\\sum_{j=a,b}|I_{j}(N+R)-I_{j}(N)|=O(N^{1-3\\alpha/8})}\\end{array}$ \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Bounding $T_{s t a b l e}$ : In Appendix G.2, we choose $T_{g o o d}$ and $T_{s t a b l e}$ such that $T_{s t a b l e}$ is the time after $T_{g o o d}$ by which the algorithm picks all the sub-optimal arms atleast once. By Proposition 3.1, the algorithm approximately matches $\\omega^{\\star}$ after $T_{s t a b l e}$ . Using an argument similar to the one for bounding time to reach the optimal proportion in the fuid dynamics of Section 4, we can prove that $T_{s t a b l e}\\stackrel{\\widecheck{\\leq}}{\\approx}(\\omega_{\\operatorname*{min}}^{\\star})^{-1}T_{g o o d}$ a.s. in $\\mathbb{P}_{\\mu}$ where $\\begin{array}{r}{\\omega_{\\mathrm{min}}^{\\star}=\\operatorname*{min}_{a\\in[K]}\\omega_{a}^{\\star}}\\end{array}$ (Lemma G.4, Appendix G.2.1). ", "page_idx": 8}, {"type": "text", "text": "Role of forced exploration in analysis: As we observe in the numerical results in Appendix J.4, forced exploration (step 1 of our algorithm) does not increase the observed sample complexity. We emphasize that without the forced exploration, Propositions 5.1 and 3.1 continue to hold if we can show that the proposed algorithm perform sufficient exploration over the instance. That is, after a random time $T$ depending on the instance and satisfying $\\mathbb{E}[T]\\,<\\,\\infty$ , every arm has $\\widetilde{N}_{a}(N)\\,=\\,\\varOmega(\\sqrt{N})$ : As a result, upon proving suffcient exploration, asymptotic optimality will follow without the forced exploration step. ", "page_idx": 8}, {"type": "text", "text": "In Appendix J.4, we see in the numerical experiments, when there is no forced exploration, AT2's sample complexity blows up over instances where multiple sub-optimal arms have equal mean. On the other hand, IAT2 performs optimally over the same instances and its sample complexity remains unaffected even when there is no forced exploration. To understand AT2's sample complexity blow up when multiple sub-optimal arms have the same mean, consider the sample paths where the best arm observes unusually small values in the first few samples. As a result, with positive probability, AT2 confuses one of the multiple sub-optimal arms with equal mean as the best arm and stay stuck sampling between those sub-optimal arms forever. However, IAT2 avoids such situation because of the exploration of every sub-optimal arm induced by the extra logarithmic term in the index. Based on these observations, we make the following conjectures: 1) AT2 performs sufficient exploration over instances where the means of all the sub-optimal arms are distinct, and 2) IAT2 performs sufficient exploration over all instances including when some of the sub-optimal arms may have equal means. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Numerical results ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we numerically demonstrate the dynamics followed by the algorithm AT2, and also compare its performance against the $\\beta$ -EB-TCB algorithm of [16] for different values of $\\beta$ ,and TCB algorithm of [22]. We consider 4 armed Gaussian bandit with unit variance and mean vector $\\mu=[10,8,7,6.5$ J. We simulate one sample path of the AT2 without stopping rule, and plot the value of normalized indexes of the sub-optimal arms. Figure 1 demonstrates that the normalised indexes once close remain close, and hence, AT2 closely mimics the fuid path. In Figure 2, we plot the sample complexities of the (I)AT2, (I)TCB, and $\\beta$ -EB-(I)TCB, for different choices of $\\beta$ andobserve that (I)AT2 outperforms all the other algorithms. Note that we use the same forced exploration rule and stopping rule for all algorithms. In Appendix J, we demonstrate by several examples that both the AT2 and IAT2 algorithms significantly outperform the $\\beta$ -EB-TCB and $\\beta$ -EB-ITCB of [16] when $\\beta$ is chosen different from the optimal $\\beta$ . We also illustrate that the AT2 and IAT2 algorithms have average sample complexity significantly lesser than the TCB and ITCB policies of [22]. In fact, we observe numerically, that (I)TCB doesn't quite satisfy the asymptotic optimality conditions (Figure 4, Appendix J). Next, in Appendix J.4, we study the effect of choice of the forced exploration parameter $\\alpha$ on the sample complexities of AT2 and IAT2. Additionally, we conduct simulations for natural extensions of these algorithms to bandits with distributions supported in [O, 1] (Appendix J.5). ", "page_idx": 9}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/18169e6edfd38c49ecd8ddc9fbda52da0ef495467cdde85b10a7a9fa20b2bc71.jpg", "img_caption": ["Figure 1: Normalised index on 1 sample path. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/f15e55405a5ead5068188c76483b6798d48c5d3c2acdd317f9347dfd2fd488d8.jpg", "img_caption": ["Figure 2: Sample complexity comparison. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We considered the best-arm identification problem under the popular top-2 framework. In the literature, top-2 framework involves sequentially identifying the empirical best arm and the most-likely challenger arm, and sampling the empirical best with probability $\\beta$ and the other with the complimentary probability. However, optimal $\\beta$ was not known. [22] recently proposed a deterministic rule for deciding between the empirical best and the challenger arm. In this paper, we have provided a most natural first order optimality condition based rule to help decide between the two. We showed that our associated algorithm is asymptotically optimal, and empirically performs better than [22] both in sample and computational complexity. Our another key contribution was to identify the underlying limiting ordinary differential equation based fuid dynamics that our algorithm tracks. This structure also provides important insights which help prove convergence of the proposed algorithm. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments: We thank Arun Suggala and Karthikeyan Shanmugam from Google Research Bangalore for initial discussions on this project. The second and the third author initiated this work while visiting GoogleResearch inBangalore. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Shubhada Agrawal, Sandeep Juneja, and Peter Glynn. Optimal $\\delta$ -correct best-arm selection for heavy-tailed distributions. In Algorithmic Learning Theory, pages 61-110. PMLR, 2020.   \n[2] Shubhada Agrawal, Sandeep Juneja, Karthikeyan Shanmugam, and Arun Sai Suggala. Optimal best-arm identification in bandits with access to offine data. arXiv preprint arXiv:2306.09048, 2023. [3]  Jean- Yves Audibert, Sebastien Bubeck, and R\u00e9mi Munos. Best arm identification in multi-armed bandits. In Conference on Learning Theory, pages 41-53, 2010.   \n[4]  Alain Bensoussan, Jens Frehse, Phillip Yam, et al. Mean field games and mean field type control theory, volume 101. Springer, 2013.   \n[5]  Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer, 2009. [6]  Lijie Chen, Jian Li, and Mingda Qiao. Towards instance optimal bounds for best arm identification. In Conference on Learning Theory, pages 535-592. PMLR, 2017.   \n[7] Ye Chen and llya O Ryzhov. Balancing optimal large deviations in sequential selection. Management Science, 69(6):3457-3473, 2023.   \n[8] Jim G Dai. On positive harris recurrence of multiclass queueing networks: a unified approach via fluid limit models. The Annals of Applied Probability, 5(1):49-77, 1995.   \n[9]  Amir Dembo and Ofer Zeitouni. Large deviations techniques and applications, volume 38. Springer Science & Business Media, 2009.   \n[10] Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(6), 2006.   \n[11]  Aurelien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In Conference on Learning Theory, pages 998-1027. PMLR, 2016.   \n[12] Aurlien Garivier and Emilie Kaufmann. Nonasymptotic sequential tests for overlapping hypotheses applied to near-optimal arm identification in bandit models. Sequential Analysis, 40(1):61-96, 2021.   \n[13] Kevin Jamieson, Matthew Malloy, Robert Nowak, and Sebastien Bubeck. lil'ucb: An optimal exploration algorithm for multi-armed bandits. In Conference on Learning Theory, pages 423-439. PMLR, 2014.   \n[14]  Yassir Jedra and Alexandre Proutiere. Optimal best-arm identification in linear bandits. In Advances in Neural Information Processing Systems, volume 33, pages 10007-10017, 2020.   \n[15]  Marc Jourdan and Remy Degenne. Non-asymptotic analysis of a ucb-based top two algorithm. In Advances in Neural Information Processing Systems, volume 36, pages 68980-69020, 2023.   \n[16] Marc Jourdan, R\u00e9my Degenne, Dorian Baudry, Rianne de Heide, and Emilie Kaufmann. Top two algorithms revisited. Advances in Neural Information Processing Systems, 35:26791-26803, 2022.   \n[17]  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in stochastic multi-armed bandits. In ICML, volume 12, pages 655-662, 2012.   \n[18] Emilie Kaufmann. Contributions to the Optimal Solution of Several Bandit Problems. PhD thesis, Universite de Lille, 2020.   \n[19] Emilie Kaufmann and Wouter M. Koolen. Mixture martingales revisited with applications to sequential tests and confidence intervals. Journal of Machine Learning Research, 22(246):1-44, 2021.   \n[20] David G. Luenberger and Yinyu Ye. Linear and Nonlinear Programming. Springer US, 2008.   \n[21]  Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit problem. Journal of Machine Learning Research, 5(Jun):623-648, 2004.   \n[22]  Arpan Mukherjee and Ali Tajer. Best arm identification in stochastic bandits: Beyond $\\beta-$ optimality. arXiv preprint arXiv:2301.03785, 2023.   \n[23]  Yuri Nesterov et al. Lectures on convex optimization, volume 137. Springer, 2018.   \n[24]  Chao Qin, Diego Klabjan, and Daniel Russo. Improving the expected improvement algorithm. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[25]  Daniel Russo.  Simple bayesian algorithms for best arm identification.  In Conference on Learning Theory, pages 1417-1418. PMLR, 2016.   \n[26] Xuedong Shang, Rianne Heide, Pierre Menard, Emilie Kaufmann, and Michal Valko. Fixedconfidence guarantees for bayesian best-arm identification. In International Conference on Artificial Intelligence and Statistics, pages 1823-1832. PMLR, 2020.   \n[27] Po-An Wang, Ruo-Chun Tzeng, and Alexandre Proutiere. Fast pure exploration via frank-wolfe. Advances in Neural Information Processing Systems, 34:5810-5821, 2021.   \n[28] Wei You, Chao Qin, Zihao Wang, and Shuoguang Yang. Information-directed selection for top-two algorithms. In Proceedings of Thirty Sixth Conference on Learning Theory, volume 195 of Proceedings of Machine Learning Research, pages 2850-2851. PMLR, 12-15 Jul 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Outline ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Below we provide a brief outline of the topics presented in the appendices. ", "page_idx": 12}, {"type": "text", "text": "1. Algorithm (1) and (2) are, respectively, the pseudocodes of the AT2 and IAT2 algorithms introduced in Section 3.   \n2. Appendix B: We define the single parameter exponential family (SPEF) of distributions, and prove several inequalities bounding the index function, anchor function, and the derivatives of the anchor function, which are crucial for our analysis.   \n3. Appendix C: We introduce a framework using which we apply the implicit function theorem for proving several properties related to the fuid dynamics and the algorithm's allocations.   \n4. Appendix D: We prove Propositions 2.1 and 2.2 from Section 2.   \n5. Appendix E: We provide the proofs of the results mentioned in Section 4, and also construct the fluid dynamics for the $\\beta$ -EB-TCB algorithm ([16]) in Appendix E.2.   \n6. Appendix F: We provide a heuristic argument to show that if the minimum index meets with the index of some sub-optimal arm $a\\ne1$ following the ODEs in Theorem 4.1, then arm $a$ must be incorporated into the set of minimum index arms. We argue via contradiction to show that, if this is not the case then index of arm $a$ becomes strictly less than the minimum index of the arms, which implies a contradiction. Several of the key steps in the proof of Lemma 5.1 are extensions of the said argument to the non-fluid setting of the algorithm with additional terms because of the noise in the estimates.   \n7. Appendix G: \u03b2We prove Proposition 3.1, 5.1, and provide detailed proofs of all the results mentioned in Section 5.   \n8. Appendix H: We provide the detailed proof of Theorem 3.1.   \n9. Appendix I: We describe a natural extension of the proposed AT2 and IAT2 algorithms to the class of distributions with support contained in [0, 1]. We do not theoretically analyze this algorithm owing to space constraints. However we compare the proposed algorithm with existing algorithms experimentally in Appendix J.5.   \n10. Appendix J: We compare the performance of the proposed algorithms against existing algorithms through numerical experiments. We also illustrate how our algorithm follows the fuid dynamics as the no. of samples increase. ", "page_idx": 12}, {"type": "text", "text": "B  Single parameter exponential family of distributions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We consider single parameter exponential family (SPEF) of distributions of the form ", "page_idx": 12}, {"type": "equation", "text": "$$\nd\\nu_{\\theta}(x)=\\exp(\\theta x-b(\\theta))d\\rho(x)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\rho$ is a dominating measure which we assume to be degenerate, $\\theta$ lies in the interior of set $\\Theta$ defined below (denoted by $\\Theta^{o}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Theta=\\left\\{\\theta\\Bigm|\\int_{\\mathbb{R}}\\exp(\\theta x)d\\rho(x)<\\infty\\right\\},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\nb(\\theta)=\\log\\left(\\int_{\\mathbb{R}}\\exp(\\theta x)d\\rho(x)\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "is the log-moment generating function of the measure $\\rho(\\cdot)$ ", "page_idx": 12}, {"type": "text", "text": "For $\\theta,\\widetilde{\\theta}\\in\\Theta^{o}$ , the KL-divergence between the measures $\\nu_{\\theta}$ and $\\nu_{\\widetilde{\\theta}}$ .5 ", "page_idx": 12}, {"type": "equation", "text": "$$\nK L(\\nu_{\\theta},\\nu_{\\widetilde{\\theta}})=(\\theta-\\widetilde{\\theta})b^{\\prime}(\\theta)-b(\\theta)+b(\\widetilde{\\theta}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The mean under $\\nu_{\\theta}$ is given by $b^{\\prime}(\\theta)$ . Let $\\boldsymbol{S}$ be the image of the set $\\Theta^{o}$ under the mapping $b^{\\prime}(\\cdot)$ . Note that $\\boldsymbol{S}$ is an open interval. Also, since $b^{\\prime\\prime}(\\cdot)>0$ in $\\Theta^{o}$ \uff0c $b^{\\prime}(\\cdot)$ is strictly increasing in $\\Theta^{o}$ , and is a ", "page_idx": 12}, {"type": "text", "text": "Algorithm 1: Anchored Top-Two (AT2) Algorithm ", "page_idx": 13}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/f2b48c35568b8fd73318ba9f3f9599a47c9b689001bbc396f05b7be155c6d12f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Algorithm 2: Improved Anchored Top-Two (IAT2) Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/ac45c2d6099adbbdd010d078eaea7924240ea84a566d08e9271d2855ce516aa7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "bijection between $\\Theta^{o}$ and $\\boldsymbol{S}$ . This implies we can parameterize the distributions in the SPEF using their means as well. ", "page_idx": 13}, {"type": "text", "text": "Let $\\theta_{\\mu}$ be the unique $\\theta$ satisfying $b^{\\prime}(\\theta)\\,=\\,\\mu$ for some $\\mu\\,\\in\\,{\\mathcal{S}}$ . Clearly, $\\theta_{\\mu}$ is a strictly increasing function of $\\mu$ This follows since $b^{\\prime}(\\cdot)$ is strictly increasing in $\\Theta^{o}$ .Additionally, all the higher derivatives of $b(\\cdot)$ exist in the set $\\Theta^{o}$ (see Exercise 2.2.24 in [9]). ", "page_idx": 13}, {"type": "text", "text": "For $\\mu,{\\widetilde{\\mu}}\\in S$ we define $d(\\mu,\\widetilde{\\mu})$ as, ", "page_idx": 13}, {"type": "equation", "text": "$$\nd(\\mu,\\widetilde{\\mu})=K L(\\nu_{\\theta_{\\mu}},\\nu_{\\theta_{\\widetilde{\\mu}}})=(\\theta_{\\mu}-\\theta_{\\widetilde{\\mu}})\\mu-b(\\theta_{\\mu})+b(\\theta_{\\widetilde{\\mu}}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We define $\\mu_{\\mathrm{inf}}\\in\\mathbb{R}\\cup\\{-\\infty\\}$ and $\\mu_{\\mathrm{sup}}\\in\\mathbb{R}\\cup\\{+\\infty\\}$ respectively, to be the infmum and supremum of the interval $\\boldsymbol{S}$ . Then, $S=(\\mu_{\\mathrm{inf}},\\mu_{\\mathrm{sup}})$ ", "page_idx": 14}, {"type": "text", "text": "Definition B.1. For $\\pmb{\\mu}~=~(\\mu_{1},\\mu_{2},\\ldots,\\mu_{K})~\\in~\\mathcal{S}^{K}$ , define $\\begin{array}{r}{r}{r_{\\mathrm{min}}(\\pmb{\\mu})~=~\\operatorname*{min}_{i\\in[K]}\\{\\operatorname*{min}\\{\\mu_{i}~-}\\}}\\end{array}$ $\\mu_{\\mathrm{inf}},\\mu_{\\mathrm{sup}}-\\mu_{i}\\}\\footnote{h a p h a s t i c f o r m a l i s s s i m p e r a t e r s a r e a r i s t i c a l t o n s i t i c a l t o t h e o p e r a t u r e,w e h a v e t o r\\mu_{\\mathrm{inf}}.}$ ", "page_idx": 14}, {"type": "text", "text": "Since $\\boldsymbol{S}$ is an open interval, $r_{\\operatorname*{min}}(\\mu)$ is positive for every $\\pmb{\\mu}\\in S^{K}$ , and can be $\\infty$ if both $\\mu_{\\mathrm{inf}}$ and $\\mu_{\\mathrm{sup}}$ are $\\infty$ ", "page_idx": 14}, {"type": "text", "text": "Partial derivatives of $d(\\mu,\\widetilde{\\mu})$ : For every pair $\\mu,{\\widetilde{\\mu}}\\in S$ , the partial derivatives of $d(\\mu,\\widetilde{\\mu})$ with respect to the first argument is ", "page_idx": 14}, {"type": "equation", "text": "$$\nd_{1}(\\mu,\\widetilde{\\mu})\\stackrel{\\mathrm{def.}}{=}\\frac{\\partial}{\\partial\\mu}d(\\mu,\\widetilde{\\mu})=\\theta_{\\mu}-\\theta_{\\widetilde{\\mu}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and that with respect to the second argument is, ", "page_idx": 14}, {"type": "equation", "text": "$$\nd_{2}(\\mu,\\widetilde{\\mu})\\stackrel{\\mathrm{def.}}{=}\\frac{\\partial}{\\partial\\widetilde{\\mu}}d(\\mu,\\widetilde{\\mu})=\\frac{\\widetilde{\\mu}-\\mu}{b^{\\prime\\prime}(\\theta_{\\widetilde{\\mu}})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Enveloping the KL-divergence: In the following discussion, we try to bound the KL-divergence $d(\\mu,\\widetilde{\\mu})$ from both sides using the squared distance $|\\mu-\\widetilde{\\mu}|^{2}$ after imposing some restrictions on the choice of $\\mu,{\\widetilde{\\mu}}\\in S$ . For an instance $\\dot{\\mu}\\in S^{K}$ , we define the constants $\\varDelta_{\\mathrm{min}}(\\mu)$ and $\\epsilon(\\mu)$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\varDelta_{\\mathrm{min}}(\\mu)=\\operatorname*{min}_{i\\in[K]/\\{1\\}}(\\mu_{1}-\\mu_{i})\\quad\\mathrm{and}\\quad\\epsilon(\\mu)=\\operatorname*{min}\\left\\{\\frac{\\varDelta_{\\mathrm{min}}(\\mu)}{4},\\frac{r_{\\mathrm{min}}(\\mu)}{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We further define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{H}(\\pmb{\\mu})\\;=\\;\\displaystyle\\bigcup_{i\\in[K]/\\{1\\}}[\\mu_{i}-\\epsilon(\\pmb{\\mu}),\\mu_{1}+\\epsilon(\\pmb{\\mu})],}\\\\ {\\sigma_{\\operatorname*{max}}(\\pmb{\\mu})\\;=\\;\\displaystyle\\operatorname*{max}_{\\pmb{\\mu}\\in\\mathcal{H}(\\pmb{\\mu})}b^{\\prime\\prime}(\\theta_{\\mu})\\quad\\mathrm{and}\\quad\\sigma_{\\operatorname*{min}}(\\pmb{\\mu})\\;=\\;\\displaystyle\\operatorname*{min}_{\\mu\\in\\mathcal{H}(\\pmb{\\mu})}b^{\\prime\\prime}(\\theta_{\\mu}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\mathcal{H}(\\pmb{\\mu})\\,\\subset\\,\\mathcal{S}$ , all distributions with mean in $\\mathcal{H}(\\pmb{\\mu})$ have positive variance. Note that $b^{\\prime\\prime}(\\theta_{\\mu})$ represents the variance of the distribution with mean $\\mu$ . As a result, since $\\mathcal{H}(\\pmb{\\mu})$ is a compact set, both $\\sigma_{\\mathrm{max}}(\\mu)$ and $\\sigma_{\\mathrm{min}}(\\mu)$ are positive constants. ", "page_idx": 14}, {"type": "text", "text": "Hence, $b(\\cdot)$ is $\\sigma_{\\mathrm{min}}(\\mu)$ strongly convex and $b^{\\prime}(\\cdot)$ is $\\sigma_{\\mathrm{max}}(\\mu)$ -Lipschitz on the set $\\left(b^{\\prime}\\right)^{-1}\\!\\left(\\mathcal{H}(\\pmb{\\mu})\\right)$ Thus, using [23, Theorems 2.1.5 and 2.1.10], for every $\\theta_{1},\\ddot{\\theta_{2}}\\in(\\bar{b^{\\prime}})^{-1}(\\mathcal{H}(\\pmb{\\mu}))$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{|b^{\\prime}(\\theta_{1})-b^{\\prime}(\\theta_{2})|^{2}}{2\\sigma_{\\operatorname*{max}}(\\mu)}\\leq b(\\theta_{2})-b(\\theta_{1})-b^{\\prime}(\\theta_{1})\\cdot(\\theta_{2}-\\theta_{1})=d(\\nu_{\\theta_{1}},\\nu_{\\theta_{2}})\\leq\\frac{|b^{\\prime}(\\theta_{1})-b^{\\prime}(\\theta_{2})|^{2}}{2\\sigma_{\\operatorname*{min}}(\\mu)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and hence, for $\\mu,\\widetilde{\\mu}\\in\\mathcal{H}(\\mu)$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{|\\mu-\\widetilde{\\mu}|^{2}}{2\\sigma_{\\operatorname*{max}}(\\mu)}\\leq d(\\mu,\\widetilde{\\mu})\\leq\\frac{|\\mu-\\widetilde{\\mu}|^{2}}{2\\sigma_{\\operatorname*{min}}(\\mu)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Bounding the partial derivatives: We now introduce bounds on the partial derivatives $d_{1}$ and $d_{2}$ introduced earlier. Consider $\\mu,x,\\widetilde{\\mu}\\in\\mathcal{H}(\\mu)$ such that $\\mu>\\widetilde{\\mu}$ ,and $x\\in[\\widetilde{\\mu},\\mu]$ .Recall ", "page_idx": 14}, {"type": "equation", "text": "$$\nd_{1}(\\mu,x)=\\theta_{\\mu}-\\theta_{x}\\quad\\mathrm{and}\\quad d_{1}(\\widetilde{\\mu},x)=-(\\theta_{x}-\\theta_{\\widetilde{\\mu}}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\theta_{(\\cdot)}=(b^{\\prime})^{-1}(\\cdot)$ , we have, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\mu-x}{\\sigma_{\\operatorname*{max}}(\\mu)}\\leq d_{1}(\\mu,x)\\leq\\frac{\\mu-x}{\\sigma_{\\operatorname*{min}}(\\mu)},\\quad\\mathrm{and}\\quad-\\frac{x-\\widetilde{\\mu}}{\\sigma_{\\operatorname*{max}}(\\mu)}\\geq d_{1}(\\widetilde{\\mu},x)\\geq-\\frac{x-\\widetilde{\\mu}}{\\sigma_{\\operatorname*{min}}(\\mu)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, since, ", "page_idx": 14}, {"type": "equation", "text": "$$\nd_{2}(\\mu,x)=-\\frac{\\mu-x}{b^{\\prime\\prime}(\\theta_{x})}\\quad\\mathrm{and}\\quad d_{2}(\\widetilde{\\mu},x)=\\frac{x-\\widetilde{\\mu}}{b^{\\prime\\prime}(\\theta_{x})},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we have, ", "page_idx": 14}, {"type": "equation", "text": "$$\n-\\frac{\\mu-x}{\\sigma_{\\operatorname*{max}}(\\mu)}\\geq d_{2}(\\mu,x)\\geq-\\frac{\\mu-x}{\\sigma_{\\operatorname*{min}}(\\mu)},\\quad\\mathrm{and}\\quad\\frac{x-\\widetilde{\\mu}}{\\sigma_{\\operatorname*{min}}(\\mu)}\\leq d_{2}(\\widetilde{\\mu},x)\\leq\\frac{x-\\widetilde{\\mu}}{\\sigma_{\\operatorname*{min}}(\\mu)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B.1 Enveloping the anchor and index functions under noisy estimates of the rewards ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For every arm $a\\in[K]$ let $\\widetilde{\\mu}_{a}$ be an estimate of $\\mu_{a}$ satisfying $|\\widetilde{\\mu}_{a}-\\mu_{a}|\\leq\\epsilon(\\mu)$ and $\\widetilde{\\pmb{\\mu}}=(\\widetilde{\\mu}_{a})_{a\\in[K]}$ Since $\\begin{array}{r}{\\epsilon(\\mu)\\leq\\frac{\\varDelta_{\\operatorname*{min}}(\\mu)}{4}}\\end{array}$ $\\widetilde{\\pmb{\\mu}}$ is the firstarm. Also let $N_{a}$ be the no. of times arm $a$ has been pulled and $\\bar{\\boldsymbol{N}^{\\prime}}=(N_{a})_{a\\in[K]}$ ", "page_idx": 15}, {"type": "text", "text": "Enveloping the anchor function: As we introduced in Section 2, the anchor function is, ", "page_idx": 15}, {"type": "equation", "text": "$$\ng(\\widetilde{\\pmb{\\mu}},\\pmb{N})=\\sum_{a\\neq1}\\frac{d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})}{d(\\widetilde{\\mu}_{a},\\widetilde{x}_{1,a})}-1,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where MN+Ne. Note that a,1,a E H(\u03bc) forevery a E [K]/[1} Therefore, using (6), wehave, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\sigma_{\\operatorname*{min}}(\\mu)}{\\sigma_{\\operatorname*{max}}(\\mu)}\\sum_{a\\neq1}\\frac{(\\widetilde{\\mu}_{1}-\\widetilde{x}_{1,a})^{2}}{(\\widetilde{x}_{1,a}-\\widetilde{\\mu}_{a})^{2}}-1\\,\\leq\\,g(\\widetilde{\\mu},N)\\,\\leq\\,\\frac{\\sigma_{\\operatorname*{max}}(\\mu)}{\\sigma_{\\operatorname*{min}}(\\mu)}\\sum_{a\\neq1}\\frac{(\\widetilde{\\mu}_{1}-\\widetilde{x}_{1,a})^{2}}{(\\widetilde{x}_{1,a}-\\widetilde{\\mu}_{a})^{2}}-1,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Putting $\\begin{array}{r}{\\widetilde{x}_{1,a}=\\frac{N_{1}\\widetilde{\\mu}_{1}+N_{a}\\widetilde{\\mu}_{a}}{N_{1}+N_{a}}}\\end{array}$ Np+Na\u03bca , we obtain, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\sigma_{\\operatorname*{min}}(\\mu)}{\\sigma_{\\operatorname*{max}}(\\mu)}\\sum_{a\\neq1}\\frac{N_{a}^{2}}{N_{1}^{2}}-1\\ \\leq\\ g(\\widetilde{\\mu},N)\\ \\leq\\ \\frac{\\sigma_{\\operatorname*{max}}(\\mu)}{\\sigma_{\\operatorname*{min}}(\\mu)}\\sum_{a\\neq1}\\frac{N_{a}^{2}}{N_{1}^{2}}-1,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now $\\begin{array}{r}{\\widetilde{\\mu}_{1}-\\widetilde{x}_{1,a}=\\frac{N_{a}}{N_{1}+N_{a}}(\\widetilde{\\mu}_{1}-\\widetilde{\\mu}_{a})}\\end{array}$ and $\\begin{array}{r}{\\widetilde{x}_{1,a}-\\widetilde{\\mu}_{a}=\\frac{N_{1}}{N_{1}+N_{a}}(\\widetilde{\\mu}_{1}-\\widetilde{\\mu}_{a})}\\end{array}$ Using these, we can say. ", "page_idx": 15}, {"type": "equation", "text": "$$\ng(\\widetilde{\\pmb{\\mu}},\\pmb{N})\\,=\\,\\Theta\\left(\\sum_{a\\neq1}\\frac{N_{a}^{2}}{N_{1}^{2}}\\right)-1,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Wwhenever $|\\widetilde{\\mu}_{a}-\\mu_{a}|\\leq\\epsilon(\\mu)$ for all $a\\in[K]$ .The constants hidden in the $\\Theta(\\cdot)$ depends only on $\\pmb{\\mu}$ and obviously the choice of the SPEF family. ", "page_idx": 15}, {"type": "text", "text": "Enveloping the index: Following the definition of empirical index $\\mathcal{T}_{a}(\\cdot)$ in Section 3, we define the index of any alternative arm $a\\in[K]/\\{1\\}$ with respect to the estimates $\\widetilde{\\mu}=(\\widetilde{\\mu}_{a}:a\\in[K])$ and allocation $N=(N_{a}:a\\in[K])$ is, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{W}_{a}(\\tilde{\\pmb{\\mu}},\\pmb{N})\\;=\\;N_{1}d(\\tilde{\\mu}_{1},\\tilde{x}_{1,a})+N_{a}d(\\tilde{\\mu}_{a},\\tilde{x}_{1,a}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Observe that, the empirical index $\\mathcal{Z}_{a}(N)$ and index $I_{a}(N)$ introduced in Section 3 are, respectively, equivalent to the quantities $\\mathcal{W}_{a}(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))$ , and $\\mathscr{W}_{a}(\\pmb{\\mu},\\widetilde{\\pmb{N}}(\\pmb{N}))$ , where ${\\widetilde{\\pmb{\\mu}}}(N)$ is the empirical estimate and $\\widetilde{N}(N)$ is the algorithm's allocation at iteration $N$ ", "page_idx": 15}, {"type": "text", "text": "Using (6) we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2\\sigma_{\\operatorname*{min}}(\\mu)}(N_{1}(\\widetilde{\\mu}_{1}-\\widetilde{x}_{1,a})^{2}+N_{a}(\\widetilde{x}_{1,a}-\\widetilde{\\mu}_{a})^{2})\\ \\leq\\ \\mathcal{W}_{a}(\\widetilde{\\mu},N)\\ \\leq\\ \\frac{1}{2\\sigma_{\\operatorname*{min}}(\\mu)}(N_{1}(\\widetilde{\\mu}_{1}-\\widetilde{x}_{1,a})^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad+\\ N_{a}(\\widetilde{x}_{1,a}-\\widetilde{\\mu}_{a})^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Putting $\\begin{array}{r}{\\widetilde{x}_{1,a}=\\frac{N_{1}\\widetilde{\\mu}_{1}+N_{a}\\widetilde{\\mu}_{a}}{N_{1}+N_{a}}}\\end{array}$ Nli+Naa, we get, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{(\\widetilde{\\mu}_{1}-\\widetilde{\\mu}_{a})^{2}}{2\\sigma_{\\operatorname*{max}}(\\mu)}\\frac{N_{1}N_{a}}{N_{1}+N_{a}}\\,\\le\\,\\mathcal{W}_{a}(\\widetilde{\\mu},N)\\,\\le\\,\\frac{(\\widetilde{\\mu}_{1}-\\widetilde{\\mu}_{a})^{2}}{2\\sigma_{\\operatorname*{min}}(\\mu)}\\frac{N_{1}N_{a}}{N_{1}+N_{a}},\\mathrm{~which~implies,}}&{}\\\\ {\\frac{(\\mu_{1}-\\mu_{a}-2\\epsilon(\\mu))^{2}}{2\\sigma_{\\operatorname*{max}}(\\mu)}\\frac{N_{1}N_{a}}{N_{1}+N_{a}}\\,\\le\\,\\mathcal{W}_{a}(\\widetilde{\\mu},N)\\,\\le\\,\\frac{(\\mu_{1}-\\mu_{a}+2\\epsilon(\\mu))^{2}}{2\\sigma_{\\operatorname*{min}}(\\mu)}\\frac{N_{1}N_{a}}{N_{1}+N_{a}}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We define $\\begin{array}{r}{\\varDelta_{\\mathrm{max}}(\\pmb{\\mu})=\\mathrm{max}_{a\\in[K]/\\{1\\}}\\,\\varDelta_{a}}\\end{array}$ . Since $\\begin{array}{r}{\\epsilon(\\mu)\\leq\\frac{1}{4}\\varDelta_{\\operatorname*{min}}(\\mu)}\\end{array}$ , we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{{\\cal A}_{\\mathrm{min}}(\\mu)^{2}}{8\\sigma_{\\mathrm{max}}(\\mu)}\\frac{N_{1}N_{a}}{N_{1}+N_{a}}\\;\\leq\\;\\mathcal{W}_{a}({\\widetilde{\\pmb\\mu}},{\\pmb N})\\;\\leq\\;\\frac{({\\cal A}_{\\mathrm{max}}(\\pmb\\mu)+2{\\epsilon}(\\mu))^{2}}{2\\sigma_{\\mathrm{min}}(\\mu)}\\frac{N_{1}N_{a}}{N_{1}+N_{a}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the same notation, as we used in (10), we have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{W}_{a}(\\tilde{\\pmb{\\mu}},\\pmb{N})\\;=\\;\\Theta\\left(\\frac{N_{1}N_{a}}{N_{1}+N_{a}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for every arm $a\\in[K]/\\{1\\}$ ", "page_idx": 16}, {"type": "text", "text": "Enveloping the partial derivatives of anchor function with respect to $_{N}$ : While analyzing the AT2 and IAT2 algorithms, we need to show that the anchor function converges to zero at a uniform rate as no. of iteration goes to infinity. During this step, we need to bound the partial derivatives of the anchor function $g(\\pmb{\\mu},N)$ with respect to $_{N}$ . Below we evaluate the partial derivatives of $g(\\pmb{\\mu},N)$ with respect to $N_{a}$ for different arms $a\\in[K]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\frac{\\partial g}{\\partial N_{1}}(\\pmb{\\mu},\\pmb{N})=-\\sum_{a\\neq1}f(\\pmb{\\mu},a,\\pmb{N})\\frac{N_{a}\\varDelta_{a}}{(N_{1}+N_{a})^{2}},\\mathrm{~and}}\\\\ {\\forall a\\neq1,\\ \\displaystyle\\frac{\\partial g}{\\partial N_{a}}(\\pmb{\\mu},\\pmb{N})=f(\\pmb{\\mu},a,\\pmb{N})\\frac{N_{1}\\varDelta_{a}}{(N_{1}+N_{a})^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\varDelta_{a}=\\mu_{1}-\\mu_{a}$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\mu,a,N)=-\\frac{\\partial}{\\partial x}\\left(\\frac{d(\\mu_{1},x)}{d(\\mu_{a},x)}\\right)\\Big|_{x=x_{1,a}}=-\\frac{d_{2}(\\mu_{1},x_{1,a})}{d(\\mu_{a},x_{1,a})}+\\frac{d(\\mu_{1},x_{1,a})d_{2}(\\mu_{a},x_{1,a})}{(d(\\mu_{a},x_{1,a}))^{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using (8), and (6), we have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{-}d_{2}(\\mu_{1},x_{1,a})\\;=\\;\\Theta(\\mu_{1}-x_{1,a}),\\quad d_{2}(\\mu_{a},x_{1,a})\\;=\\;\\Theta(x_{1,a}-\\mu_{a}),}\\\\ &{\\quad\\;d(\\mu_{1},x_{1,a})\\;=\\;\\Theta((\\mu_{1}-x_{1,a})^{2}),\\quad\\mathrm{and}\\quad d(\\mu_{a},x_{1,a})\\;=\\;\\Theta((x_{1,a}-\\mu_{a})^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the constants hidden in $\\Theta(\\cdot)$ depend only on $\\pmb{\\mu}$ and are independent of the sample path. Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mu,a,N)=\\Theta\\left(\\frac{\\mu_{1}-x_{1,a}}{(x_{1,a}-\\mu_{a})^{2}}+\\frac{(\\mu_{1}-x_{1,a})^{2}}{(x_{1,a}-\\mu_{a})^{3}}\\right)}\\\\ &{\\hphantom{\\mu(\\mu,a)}=\\Theta\\left(\\frac{N_{a}}{N_{1}}\\left(1+\\frac{N_{a}}{N_{1}}\\right)^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a consequence, we have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\frac{\\partial g}{\\partial N_{1}}(\\pmb{\\mu},\\pmb{N})=-\\Theta\\left(\\sum_{a\\in[K]/\\{1\\}}\\frac{N_{a}^{2}}{N_{1}^{3}}\\right),~~\\mathrm{and}}\\\\ &{}&{\\forall a\\neq1,~\\,\\frac{\\partial g}{\\partial N_{a}}(\\pmb{\\mu},\\pmb{N})=\\Theta\\left(\\frac{N_{a}}{N_{1}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "CFramework for applying the Implicit function theorem (IFT) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this appendix we explain a general framework using which we later apply the Implicit function theorem for the following purposes: ", "page_idx": 16}, {"type": "text", "text": "1. Constructing the fluid dynamics for the AT2 and $\\beta$ -EB-TCB algorithms in Appendix E. 2. Proving convergence of the algorithm's allocations to the optimal proportions in Appendix G.2.2. ", "page_idx": 16}, {"type": "text", "text": "We introduce the variables: $N\\;=\\;(N_{a}\\;\\in\\;\\mathbb{R}_{\\geq0}\\;:\\;a\\;\\in\\;[K])$ \uff0c $I\\in\\mathbb{R}$ $\\pmb{\\eta}\\;=\\;(\\eta_{a}\\;\\in\\;\\mathbb{R}\\;:\\;a\\;\\in\\;$ $[K])$ \uff0cand $N\\geq0$ . After fixing some instance $\\pmb{\\mu}\\in S^{K}$ $\\boldsymbol{s}$ is defined in Appendix B), we define the ", "page_idx": 16}, {"type": "text", "text": "following functions: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\phi_{1}(N,\\eta)\\,=\\,\\sum_{a\\in[K]/\\{1\\}}\\frac{d(\\mu_{1},x_{1,a}(N_{1},N_{a}))}{d(\\mu_{a},x_{1,a}(N_{1},N_{a}))}-1-\\eta_{1},}\\\\ {\\mathrm{~r\\,}a\\in[K]/\\{1\\},\\,\\,\\,\\phi_{a}(N,I,\\eta)\\,=\\,N_{1}d(\\mu_{1},x_{1,a}(N_{1},N_{a}))+N_{a}d(\\mu_{a},x_{1,a}(N_{1},N_{a}))-I-\\eta_{a},}\\\\ {\\mathrm{~and~}\\,\\,\\,\\phi_{K+1}(N,N)\\,=\\,\\sum_{a\\in[K]}N_{a}-N,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\nx_{1,a}(N_{1},N_{a})=\\frac{N_{1}\\mu_{1}+N_{a}\\mu_{a}}{N_{1}+N_{a}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For every non-empty subset $B\\subseteq[K]/\\{1\\}$ , we define the vector valued functions, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c l l}{\\phi_{B}(N,I,\\eta,N)\\!\\!\\!}&{=}&{\\!\\!\\![\\,\\Phi_{1}(N,\\eta),}&{\\!\\!\\!(\\Phi_{a}(N,I,\\eta))_{a\\in B},\\quad\\Phi_{K+1}(N,N)\\,]\\,,}\\\\ {\\tilde{\\Phi}_{B}(N,I,\\eta)\\!\\!\\!}&{=}&{\\!\\!\\![\\,\\Phi_{1}(N,\\eta),}&{\\!\\!\\!(\\Phi_{a}(N,I,\\eta))_{a\\in B}\\,]\\,.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We denote $\\pmb{\\varPhi}_{[K]/\\{1\\}}(\\cdot)$ just using $\\Phi(\\cdot)$ ", "page_idx": 17}, {"type": "text", "text": "In the definitions of $\\Phi_{B}(\\cdot)$ and $\\widetilde{\\pmb{\\phi}}_{B}(\\cdot)$ , without loss of generality, we assume that, the functions $\\varPhi_{a}(\\cdot)$ in the tuple $(\\Phi_{a}(\\cdot))_{a\\in B}$ are enumerated in the increasing order of $a\\,\\in\\,B$ , i.e., if we have $B=\\{a_{1},a_{2},\\dots,a_{|B|}\\}$ with $1<a_{1}<a_{2}<...<a_{|B|}$ then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\pmb{\\varPhi}_{B}\\;=\\;\\left[\\,\\mathcal{P}_{1},\\quad\\pmb{\\varPhi}_{a_{1}},\\quad,\\pmb{\\varPhi}_{a_{2}},\\quad\\pmb{\\varPhi}_{a_{3}},\\quad\\hdots\\quad\\pmb{\\varPhi}_{a_{|B|}},\\quad\\pmb{\\varPhi}_{K+1}\\,\\right],\\quad\\mathrm{and}}\\\\ {\\tilde{\\pmb{\\varPhi}}_{B}\\;=\\;\\left[\\,\\mathcal{P}_{1},\\quad\\pmb{\\varPhi}_{a_{1}},\\quad,\\pmb{\\varPhi}_{a_{2}},\\quad\\pmb{\\varPhi}_{a_{3}},\\quad\\hdots\\quad\\pmb{\\varPhi}_{a_{|B|}}\\,\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Before stating the main result of this section in Lemma C.1, we define some notation that are essential for the lemma statement. For any $A\\subseteq[K]$ , we use the notation $N_{A}$ to denote the tuple of variables $(N_{a}:a\\in A)$ . For some vector valued function $\\pmb{G}$ depending on $_{N}$ , denote the Jacobian of $G(\\cdot)$ with respect to the tuple of ariables NA using oNa \u00b7 ", "page_idx": 17}, {"type": "text", "text": "For any non-empty $B\\ \\subseteq\\ [K]/\\{1\\}$ , we define $\\overline{{B}}\\ =\\ B\\cup\\{1\\}$ . For every $k\\ \\geq\\ 1$ \uff0c $\\mathbf{0}_{k}$ and ${\\bf1}_{k}$ respectively, refers to a $k$ -dimensional vector with entries 0 and 1. We define $\\mathcal{Z}_{B}$ to be the set of tuples $(N,I,\\mathbf{0}_{K},N)$ with $N\\in\\mathbb{R}_{\\ge0}^{K}$ \uff0c $I\\in\\mathbb{R}$ ,and $N\\in\\mathbb{R}_{>0}$ , which satisfy ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Phi_{B}(N,I,{\\bf0}_{K},N)\\;=\\;{\\bf0}_{|B|+2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma C.1 (Invertibility of the Jacobian). For all non-empty subset $B\\subseteq[K]/\\{1\\}$ thefollowing statements hold true at every tuple $(N,I,\\mathbf{0}_{K},N)$ in the set $\\mathcal{Z}_{B}$ ", "page_idx": 17}, {"type": "text", "text": "1.TheJacobian is invertible at (N, I, Ok). ", "page_idx": 17}, {"type": "text", "text": "2. We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\bf1}_{|B|+1}^{T}\\left(\\frac{\\partial\\ensuremath{\\widetilde{\\pmb{\\phi}}}_{B}}{\\partial N_{\\overline{{{B}}}}}\\right)^{-1}\\frac{\\partial\\ensuremath{\\widetilde{\\pmb{\\phi}}}_{B}}{\\partial I}\\leq-\\sum_{a\\in B}\\frac{1}{d(\\mu_{a},\\mu_{1})},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "at $(N,I,\\mathbf{0}_{K})$ ", "page_idx": 17}, {"type": "text", "text": "3. The Jacobian $\\frac{\\partial\\pmb{\\phi}_{B}}{\\partial(N_{\\overline{{B}}},I)}$ defined as, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\partial\\pmb{\\varPhi}_{B}}{\\partial(N_{\\overline{{B}}},I)}\\,=\\,}&{\\left[\\frac{\\frac{\\partial\\widetilde{\\phi}_{B}}{\\partial N_{\\overline{{B}}}}}{\\partial N_{\\overline{{B}}}}\\,\\right]\\frac{\\frac{\\partial\\widetilde{\\phi}_{B}}{\\partial I}}{\\partial I}}\\\\ &{\\left[\\frac{\\partial\\phi_{K+1}}{\\partial N_{\\overline{{B}}}}\\,\\right]\\frac{\\partial\\phi_{K+1}}{\\partial I}\\;\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "is invertible at $(N,I,\\mathbf{0}_{K},N)$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Statement 1: The Jacobian a_ is equivalent to, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial\\widetilde{\\pmb{\\phi}}_{B}}{\\partial N_{\\overline{{B}}}}=\\left[\\begin{array}{c c c c c c}{\\frac{\\partial\\phi_{1}}{\\partial N_{1}}}&{\\frac{\\partial\\phi_{1}}{\\partial N_{a_{1}}}}&{\\frac{\\partial\\phi_{1}}{\\partial N_{a_{2}}}}&{...}&{\\frac{\\partial\\phi_{1}}{\\partial N_{a_{|B|}}}}\\\\ {\\frac{\\partial\\phi_{a_{1}}}{\\partial N_{1}}}&{\\frac{\\partial\\phi_{a_{1}}}{\\partial N_{a_{1}}}}&{\\frac{\\partial\\phi_{a_{1}}}{\\partial N_{a_{2}}}}&{...}&{\\frac{\\partial\\phi_{a_{1}}}{\\partial N_{a_{|B|}}}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{...}&{\\vdots}\\\\ {\\frac{\\partial\\phi_{a_{|B|}}}{\\partial N_{1}}}&{\\frac{\\partial\\phi_{a_{|B|}}}{\\partial N_{a_{1}}}}&{\\frac{\\partial\\phi_{a_{|B|}}}{\\partial N_{a_{2}}}}&{...}&{\\frac{\\partial\\phi_{a_{|B|}}}{\\partial N_{a_{|B|}}}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we observe the following properties about the sign of the entries of the above Jacobian matrix, \u00b7 We have $N>0$ and $\\Phi_{1}(N,{\\bf0}_{K})=0$ . This implies $N_{1}\\,>\\,0$ and $\\operatorname*{max}_{a\\in[K]/\\{1\\}}N_{a}>0$ As a result, using (15), we have $\\begin{array}{r}{\\frac{\\partial\\Phi_{1}}{\\partial N_{1}}<0}\\end{array}$ and $\\begin{array}{r}{\\frac{\\partial\\varPhi_{1}}{\\partial N_{a_{i}}}\\geq0}\\end{array}$ for every $i\\in\\{1,2,\\dots,|B|\\}$ ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u00b7For $i\\;\\in\\;\\{2,\\ldots,|B|+1\\}$ , in the $i$ -th row, the only non-zero entries can be the first and the $i$ -th entry. The first entry is = d(\u03bc,1,a:(N,Na)) \u2265 0. The i-th entry is N1 $d(\\mu_{a_{i}},x_{1,a_{i}}(N_{1},N_{a_{i}}))$ . Since we have $N_{1}>0$ , using (6), we have $d(\\mu_{a_{i}},x_{1,a_{i}}(N_{1},N_{a_{i}}))>0$ \uff0c making the $i$ -th entry positive. ", "page_idx": 18}, {"type": "text", "text": "Therefore, considering only the sign of the elements, the matrix in (16) is of the form, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\begin{array}{c c c c c c}{{--}}&{{+}}&{{+}}&{{+}}&{{\\ldots}}&{{+}}\\\\ {{+}}&{{++}}&{{0}}&{{0}}&{{\\ldots}}&{{0}}\\\\ {{+}}&{{0}}&{{++}}&{{0}}&{{\\ldots}}&{{0}}\\\\ {{+}}&{{0}}&{{0}}&{{++}}&{{\\ldots}}&{{0}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\vdots}}&{{\\vdots}}&{{\\ldots}}&{{\\vdots}}\\\\ {{+}}&{{0}}&{{0}}&{{0}}&{{\\ldots}}&{{++}}\\end{array}\\right|,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the symbols $^{++}$ , --and $^+$ implies that the corresponding entries are positive, negative and non-negative. ", "page_idx": 18}, {"type": "text", "text": "We now argue that a matrix of the above structure has a rank $|B|+1$ . To see that, by subtracting some appropriate constant times the $i$ -th column from the first column, we can make the entries in position $i\\in\\{2,3,\\ldots,|B|+1\\}$ in the first column zero. As a result of these transformations, since we are subtracting non-negative quantities from the first entry of the first column, that entry remains negative. The matrix we obtain after this sequence of transformations has a structure, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left[{\\begin{array}{r r r r r r}{--}&{+}&{+}&{+}&{\\cdot\\cdot\\cdot}&{+}\\\\ {0}&{++}&{0}&{0}&{\\cdot\\cdot\\cdot}&{0}\\\\ {0}&{0}&{++}&{0}&{\\cdot\\cdot\\cdot}&{0}\\\\ {0}&{0}&{0}&{++}&{\\cdot\\cdot}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\cdot\\cdot}&{\\vdots}\\\\ {0}&{0}&{0}&{0}&{\\cdot\\cdot}&{++}\\end{array}}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Clearly a matrix of the above structure has a rank $|B|+1$ and therefore invertible. ", "page_idx": 18}, {"type": "text", "text": "Statement 2: Using statement 1 of Lemma C.1, if we take $\\begin{array}{r}{{\\pmb v}=\\left(\\frac{\\partial\\tilde{\\pmb{\\phi}}_{B}}{\\partial N_{\\overline{{B}}}}\\right)^{-1}\\frac{\\partial\\tilde{\\pmb{\\phi}}_{B}}{\\partial I}}\\end{array}$ oB , then we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{\\partial\\tilde{\\pmb{\\Phi}}_{B}}{\\partial N_{\\overline{{B}}}}}{\\pmb v}\\ =\\ {\\frac{\\partial\\tilde{\\pmb{\\Phi}}_{B}}{\\partial I}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that, the RHS of the above linear system i.e. a\u03b2 is a |B| + 1 dimensional vector with zero in its first entry and $-1$ in every other entry. Using (17), $\\mathbf{\\boldsymbol{v}}=[v_{1},v_{2},v_{3},\\dots,v_{|B|+1}]^{T}$ satisfies a linear system with coefficients having the following signs, ", "page_idx": 18}, {"type": "equation", "text": "$$\n(--)v_{1}+(+)v_{2}+(+)v_{3}+(+)v_{4}+.\\ldots+(+)v_{|B|+1}\\;=\\;0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{~for~}i\\in\\{2,\\ldots,|B|+1\\},\\quad(+)v_{1}+(++)v_{i}\\,=\\,(--),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $^{++}$ ,--and $^+$ represents quantities which are positive, negative and non-negative. ", "page_idx": 19}, {"type": "text", "text": "For every $i\\,\\in\\,\\{2,\\dots,|B|+1\\}$ , we can eliminate $v_{i}$ from the first equation by subtracting some positive constant times the $i$ -th equation from it.After eliminating $v_{2},v_{3},\\ldots,v_{|B|+1}$ from the first equation following the mentioned procedure, we will be left with an equation of the form $(--)v_{1}=(+)$ ,implying $v_{1}\\leq0$ ", "page_idx": 19}, {"type": "text", "text": "Now the $i$ -th equation of the system, for $i\\in\\{2,\\ldots,|B|+1\\}$ is, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial\\Phi_{a_{i-1}}}{\\partial N_{1}}v_{1}+\\frac{\\partial\\Phi_{a_{i-1}}}{\\partial N_{a_{i-1}}}v_{i}\\ =\\ -1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We know that, $\\begin{array}{r}{\\frac{\\partial\\varPhi_{a_{i-1}}}{\\partial N_{1}}=d(\\mu_{1},x_{1,a_{i-1}})}\\end{array}$ and $\\begin{array}{r}{\\frac{\\partial\\varPhi_{a_{i-1}}}{\\partial N_{a_{i-1}}}=d(\\mu_{a_{i-1}},x_{1,a_{i-1}})}\\end{array}$ where $\\begin{array}{r}{x_{1,a}=\\frac{N_{1}\\mu_{1}+N_{a}\\mu_{a}}{N_{1}+N_{a}}}\\end{array}$ for every $a\\in[K]/\\{1\\}$ ", "page_idx": 19}, {"type": "text", "text": "Now putting the derivatives in (19), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nd(\\mu_{1},x_{1,a_{i-1}})v_{1}+d(\\mu_{a_{i-1}},x_{1,a_{i-1}})v_{i}\\ =\\ -1,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies, ", "page_idx": 19}, {"type": "equation", "text": "$$\nv_{i}\\ =\\ -\\ \\frac{1}{d(\\mu_{a_{i-1}},x_{1,a_{i-1}})}-\\frac{d(\\mu_{1},x_{1,a_{i-1}})}{d(\\mu_{a_{i-1}},x_{1,a_{i-1}})}v_{1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for every $i\\in\\{2,\\ldots,|B|+1\\}$ ", "page_idx": 19}, {"type": "text", "text": "Now adding both sides for $i\\in\\{2,3,\\ldots,|B|+1\\}$ , we get, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=2}^{|B|+1}v_{i}\\,=\\,}&{\\displaystyle-\\sum_{i=2}^{|B|+1}\\frac{1}{d(\\mu_{a_{i-1}},x_{1,a_{i-1}})}-v_{1}\\,\\sum_{i=2}^{|B|+1}\\frac{d(\\mu_{1},x_{1,a_{i-1}})}{d(\\mu_{a_{i-1}},x_{1,a_{i-1}})}}\\\\ {\\displaystyle\\,=\\,}&{\\displaystyle-\\sum_{a\\in B}\\frac{1}{d(\\mu_{a},x_{1,a})}-v_{1}\\sum_{a\\in B}\\frac{d(\\mu_{1},x_{1,a})}{d(\\mu_{a},x_{1,a})}}\\\\ {\\displaystyle\\,\\le\\,}&{\\displaystyle-\\sum_{a\\in B}\\frac{1}{d(\\mu_{a},x_{1,a})}-v_{1}\\sum_{a\\in[K]/\\{1\\}}\\frac{d(\\mu_{1},x_{1,a})}{d(\\mu_{a},x_{1,a})}\\quad\\mathrm{~(since~}v_{1}\\le0\\}}\\\\ {\\displaystyle\\,=\\,}&{\\displaystyle-\\sum_{a\\in B}\\frac{1}{d(\\mu_{a},x_{1,a})}-v_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last step follws from the fat that a[K](1 a) $\\begin{array}{r}{\\sum_{a\\in[K]/\\{1\\}}\\frac{d(\\mu_{1},x_{1,a})}{d(\\mu_{a},x_{1,a})}=\\varPhi_{1}(N,\\mathbf{0}_{K})+1=1}\\end{array}$ Taking $v_{1}$ on the LHS, we have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{|B|+1}v_{i}\\;\\leq\\;\\;-\\,\\sum_{a\\in B}\\frac{1}{d(\\mu_{a},x_{1,a})}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that the LHS of the above inequality is same as $\\begin{array}{r}{{\\mathbf{1}}^{T}{\\boldsymbol{v}}\\,=\\,{\\mathbf{1}}^{T}\\left(\\frac{\\partial\\widetilde{\\pmb{\\phi}}_{B}}{\\partial N_{\\overline{{B}}}}\\right)^{-1}\\frac{\\partial\\varPhi_{K+1}}{\\partial{\\cal I}}}\\end{array}$ aK+1 . In the RHS, since $d(\\mu_{a},x_{1,a})\\leq d(\\mu_{a},\\mu_{1})$ , we conclude the desired result. ", "page_idx": 19}, {"type": "text", "text": "Statement 3: We have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial\\pmb{\\varPhi}_{B}}{\\partial(N_{\\overline{{B}}},I)}=\\left[\\frac{\\frac{\\partial\\pmb{\\widetilde{\\phi}}_{B}}{\\partial N_{\\overline{{B}}}}}{\\partial N_{\\overline{{B}}}}\\,\\left|\\begin{array}{c}{\\frac{\\partial\\pmb{\\widetilde{\\phi}}_{B}}{\\partial I}}\\\\ {\\frac{\\partial\\pmb{\\widetilde{\\phi}}_{B}}{\\partial I}}\\end{array}\\right|=\\left[\\frac{\\frac{\\partial\\pmb{\\widetilde{\\phi}}_{B}}{\\partial N_{\\overline{{B}}}}}{\\partial\\pmb{\\widetilde{N}}_{B}}\\,\\left|\\begin{array}{c}{\\frac{\\partial\\pmb{\\widetilde{\\phi}}_{B}}{\\partial I}}\\\\ {\\frac{\\partial\\pmb{\\widetilde{\\phi}}_{B}}{\\partial I}}\\end{array}\\right|}\\\\ {\\frac{\\partial\\pmb{\\phi}_{K+1}}{\\partial N_{\\overline{{B}}}}\\,\\left|\\begin{array}{c}{\\frac{\\partial\\pmb{\\phi}_{K+1}}{\\partial I}}\\\\ {\\frac{\\partial\\pmb{\\widetilde{\\phi}}_{B}}{\\partial I}}\\end{array}\\right|}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We do the fllowing determinant preserving column operation on $\\frac{\\partial\\pmb{\\phi}_{B}}{\\partial(N_{\\overline{{B}}},I)}$ \uff0c ", "page_idx": 20}, {"type": "text", "text": "$\\left[\\frac{\\partial\\pmb{\\varPhi}_{B}}{\\partial(N_{B},I)}\\right]_{\\substack{:,\\,|B|+2}}\\Longleftarrow\\left[\\frac{\\partial\\pmb{\\varPhi}_{B}}{\\partial(N_{B},I)}\\right]_{\\substack{:,\\,|B|+2}}-\\left[\\frac{\\partial\\pmb{\\varPhi}_{B}}{\\partial(N_{B},I)}\\right]_{\\substack{,1:\\,|B|+1}}\\left(\\frac{\\partial\\pmb{\\widetilde{\\Phi}}_{B}}{\\partial N_{B}^{-}}\\right)^{-1}\\frac{\\partial\\pmb{\\widetilde{\\Phi}}_{B}}{\\partial I},$ where $\\left[{\\frac{\\partial{\\pmb{\\phi}}_{B}}{\\partial({\\pmb N}_{\\overline{{B}}},I)}}\\right]_{:,|B|+2}$ and $\\left[{\\frac{\\partial{\\pmb{\\Phi}}_{B}}{\\partial({\\pmb N}_{\\overline{{B}}},I)}}\\right]_{:,1:|B|+1}$ , respectively, denotes the $|B|+2$ -th column and left $(|B|+2)\\times(|B|+1)$ submatrix of $\\frac{\\partial\\pmb{\\phi}_{B}}{\\partial(N_{\\overline{{B}}},I)}$ ", "page_idx": 20}, {"type": "text", "text": "The above column operation gives us the matrix, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\begin{array}{c}{\\frac{\\partial\\tilde{\\phi}_{B}}{\\partial N_{B}}}\\\\ {\\frac{\\partial\\tilde{\\phi}_{B}}{\\partial N_{B}}}\\end{array}\\right]\\left[\\begin{array}{c}{\\mathbf{0}_{|B|+1}}\\\\ {\\mathbf{0}_{|B|+1}}\\end{array}\\right],}\\\\ {\\mathbf{1}_{|B|+1}^{T}\\left[\\mathbf{\\Phi}\\!-\\!\\mathbf{1}_{|B|+1}^{T}\\left(\\frac{\\partial\\tilde{\\phi}_{B}}{\\partial N_{B}}\\right)^{-1}\\frac{\\partial\\tilde{\\phi}_{B}}{\\partial I}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which has the same determinant as $\\frac{\\partial\\pmb{\\phi}_{B}}{\\partial(N_{\\overline{{B}}},I)}$ . Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{det}\\left(\\frac{\\partial\\pmb{\\varPhi}_{B}}{\\partial(N_{\\overline{{B}}},I)}\\right)=\\left(-\\mathbf{1}_{|B|+1}^{T}\\left(\\frac{\\partial\\pmb{\\widetilde{\\varPhi}}_{B}}{\\partial N_{\\overline{{B}}}}\\right)^{-1}\\frac{\\partial\\pmb{\\widetilde{\\varPhi}}_{B}}{\\partial I}\\right)\\times\\operatorname*{det}\\left(\\frac{\\partial\\pmb{\\widetilde{\\varPhi}}_{B}}{\\partial N_{\\overline{{B}}}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using statement 1 and 2 of Lemma C.1, both the quantities in the above product are non-zero, making the Jacobian a(N:1) invertible for every tuple in $\\mathcal{Z}_{B}$ \u53e3 ", "page_idx": 20}, {"type": "text", "text": "D Proofs from Section 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem D.1 is essential for proving Propositions 2.1 and 2.2 in Section 2. Before stating Theorem D.1 we state an alternative formulation of the max-min problem 1 which we call $\\mathbf{O}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\textbf{O}:}&{\\operatorname*{min}\\;\\sum_{a=1}^{K}N_{a}}\\\\ &{\\mathrm{s.t.}\\;\\;\\forall a\\neq1,\\;\\;W_{a}(N_{1},N_{a}):=N_{1}d(\\mu_{1},x_{1,a})+N_{a}d(\\mu_{a},x_{1,a})\\geq\\log\\frac{1}{2.4\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $N_{a}\\geq0$ for all $a$ , and each $\\begin{array}{r}{x_{1,a}=\\frac{\\mu_{1}N_{1}+\\mu_{a}N_{a}}{N_{1}+N_{a}}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "The optimal value of the problem $\\mathbf{o}$ is of the form $T^{\\star}(\\pmb{\\mu})\\log(1/(2.4\\delta))$ ,where $T^{\\star}(\\pmb{\\mu})$ is the reciprocal of the optimal value of (1). If $\\pmb{N}^{\\star}=(N_{a}^{\\star}:a\\in[K])$ is an optimal allocation solving $\\mathbf{O}$ then w\\* =Zbe1kIN is an optimal proportion solving (1). Similarly if $\\omega^{\\star}$ is an optimal proportion solving (1) then $\\pmb{N}^{\\star}=(N_{a}^{\\star}:a\\in[K])$ with $N_{a}^{\\star}=\\omega_{a}^{\\star}T^{\\star}(\\mu)$ is an optimal allocation solving $\\mathbf{O}$ Theorem D.1 implies uniqueness to the solution of $\\mathbf{O}$ which also implies uniqueness of the solution of (1). ", "page_idx": 20}, {"type": "text", "text": "Some notations are needed before stating Theorem D.1. Let $B\\subset[K]/1$ and $\\overline{{B}}=B\\cup\\{1\\}$ . Let $\\pmb{\\nu}=(\\nu_{a}:a\\in[K])\\in\\mathcal{S}^{K}$ besome instancewith $\\nu_{1}>\\operatorname*{max}_{a\\neq1}\\nu_{a}$ . Let $(I_{a}:a\\in B)$ each be strictly positive. If $\\overline{{B}}^{c}\\neq\\emptyset$ then let $(N_{a}\\in\\mathbb{R}_{\\geq0}:a\\in\\overline{{B}}^{c})$ be the no. of samples allocated to arms in ${\\overline{{B}}}^{c}$ We define $N_{1,1}$ as zero when $\\textstyle\\sum_{a\\in{\\overline{{B}}}^{c}}N_{a}=0$ or $\\overline{{B}}^{c}=\\emptyset$ Otherwise, we take $N_{1,1}$ to be the unique $N_{1}>0$ that solves, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{a\\in\\overline{{\\cal B}}^{c}}\\frac{d(\\nu_{1},x_{1,a})}{d(\\nu_{a},x_{1,a})}=1,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Wwhere $\\begin{array}{r}{x_{1,a}=\\frac{\\nu_{1}N_{1}+\\nu_{a}N_{a}}{N_{1}+N_{a}}}\\end{array}$ Note tha if $\\overline{{B}}^{c}\\neq\\emptyset$ and $\\textstyle\\sum_{a\\in{\\overline{{B}}}^{c}}N_{a}>0$ there exists an $a\\in\\overline{{B}}^{c}$ with $N_{a}>0$ .As a result, the LHS of (21) decreases from $\\infty$ to $0$ as $N_{1}$ increases from O to $\\infty$ .This implies the existence of a unique $N_{1}>0$ solving (21). ", "page_idx": 20}, {"type": "text", "text": "Set $N_{1,2}:=\\operatorname*{max}_{a\\in B}I_{a}d(\\nu_{1},\\nu_{a})^{-1}$ ", "page_idx": 21}, {"type": "text", "text": "Theorem D.1. There exists a unique solution $N_{1}^{\\star}\\geq0$ and $\\left\\langle N_{a}^{\\star}\\geq0:a\\in B\\right\\rangle$ satisfying ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{a\\neq1}\\frac{d(\\nu_{1},x_{1,a}^{\\star})}{d(\\nu_{a},x_{1,a}^{\\star})}-1=0\\quad\\mathit{w h e r e}\\quad x_{1,a}^{\\star}=\\frac{\\nu_{1}N_{1}^{\\star}+\\nu_{a}N_{a}^{\\star}}{N_{1}^{\\star}+N_{a}^{\\star}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{d}&{{}N_{1}^{\\star}d(\\nu_{1},x_{1,a}^{\\star})+N_{a}^{\\star}d(\\nu_{a},x_{1,a}^{\\star})=I_{a}\\quad\\forall a\\in B.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Further, N \u2265 max(N1,1, N1,2) and each N\u03b1 \u2265 a(V,\u03bc) ", "page_idx": 21}, {"type": "text", "text": "Furthermore, The optimal solution to $\\mathbf{O}$ is uniquely characterized by the solution above with $B=$ $\\{2,\\ldots,K\\}$ andeach $I_{a}\\,=\\,\\log(1/(2.4\\delta))$ and constraints (20) tight, that is, indexes of all the sub-optimal arms being equal to $\\log(1/(2.4\\delta))$ .Further $\\begin{array}{r}{N_{1}^{\\star}\\geq\\operatorname*{max}_{a\\in[K]\\backslash1}\\frac{\\log(1/(2.4\\delta)}{d(\\nu_{1},\\nu_{a})}}\\end{array}$ and each $\\begin{array}{r}{N_{a}^{\\star}\\geq\\frac{\\log(1/(2.4\\delta)}{d(\\nu_{a},\\nu_{1})}}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Proof: First observe that every solution $N_{1}^{\\star}$ and $(N_{a}^{\\star}:a\\in B)$ to the system (22) and (23) must satisfy $N_{1}\\geq\\operatorname*{max}\\{N_{1,1},N_{1,2}\\}$ and $\\begin{array}{r}{N_{a}^{\\star}\\geq\\frac{I_{a}}{d\\left(\\nu_{a},\\nu_{1}\\right)}}\\end{array}$ ,forevery $a\\in B$ ", "page_idx": 21}, {"type": "text", "text": "f $\\overline{{B}}^{c}\\neq\\emptyset$ , (22) implies, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{a\\in\\overline{{B}}^{c}}\\frac{d(\\nu_{1},x_{1,a})}{d(\\nu_{a},x_{1,a})}\\;\\leq\\;\\sum_{a\\in[K]/\\{1\\}}\\frac{d(\\nu_{1},x_{1,a})}{d(\\nu_{a},x_{1,a})}\\;=\\;1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "If $\\textstyle\\sum_{a\\in{\\overline{{B}}}^{c}}N_{a}\\;=\\;0$ then $N_{1}\\,\\geq\\,0\\,=\\,N_{1,1}$ . Otherwise, we can find an $a\\:\\in\\:\\overline{{B}}^{c}$ with $N_{a}~>~0$ \uff0c making \u2265aeB d(va,21,a) strictly decreasing in $N_{1}$ .As a result, by the defnition of $N_{1,1}$ we have $N_{1}^{\\star}\\geq N_{1,1}$ ", "page_idx": 21}, {"type": "text", "text": "Now, for every $a\\in B$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nI_{a}\\;=\\;N_{1}^{\\star}d(\\nu_{1},x_{1,a})+N_{a}^{\\star}d(\\nu_{a},x_{1,a})\\;=\\;\\operatorname*{min}_{x\\in[\\nu_{a},\\nu_{1}]}\\{N_{1}^{\\star}d(\\nu_{1},x)+N_{a}^{\\star}d(\\nu_{a},x)\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that RHS of the above inequality is upper bounded by $\\operatorname*{min}\\{N_{1}^{\\star}d(\\nu_{1},\\nu_{a}),N_{a}^{\\star}d(\\nu_{a},\\nu_{1})\\}$ As a result, fo every $a\\ \\in\\ B$ \uff0c $\\begin{array}{r}{N_{a}^{\\star}\\ \\geq\\ \\frac{I_{a}}{d(\\nu_{a},\\nu_{1})}}\\end{array}$ and $\\begin{array}{r}{N_{1}^{\\star}~\\ge~\\frac{I_{a}}{d(\\mu_{1},\\mu_{a})}}\\end{array}$ This furthr implies, $N_{1}^{\\star}~\\ge$ $\\begin{array}{r}{\\operatorname*{max}_{a\\in B}\\,\\frac{I_{a}}{d(\\nu_{1},\\nu_{a})}=N_{1,2}.}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Now we prove existence of a unique $N_{1}^{\\star}$ and $(N_{a}^{\\star}:a\\in B)$ . For every $N_{1}\\ge N_{1,2}$ and $a\\in B$ as $N_{a}$ increases from O to $\\infty$ $N_{1}d(\\nu_{1},x_{1,a})+N_{a}d(\\nu_{a},x_{1,a})$ increases monotonically from 0 to $N_{1}d(\\nu_{1},\\nu_{a})$ . Note that $N_{1}d(\\nu_{1},\\nu_{a})\\geq N_{1,2}d(\\nu_{1},\\nu_{a})\\geq I_{a}$ (by the definition of $N_{1,2}$ ). As a result, we can find a unique $N_{a}$ for which $N_{1}d(\\nu_{1},x_{1,a})+N_{a}d(\\nu_{a},x_{1,a})\\,=\\,I_{a}$ . For every $a\\,\\in\\,B$ we call that unique $N_{a}$ as $N_{a}(N_{1})$ . Observe that, since $I_{a}>0$ \uff0c $N_{a}(N_{1})$ is strictly decreasing in $N_{1}$ and if $I_{a}\\,=\\,0$ .then $N_{a}(N_{1})\\,=\\,0$ Also, if $a\\,=\\,\\arg\\operatorname*{max}_{b\\in B}\\,\\,\\,\\frac{I_{b}}{d(\\nu_{1},\\nu_{b})}$ then $N_{a}(N_{1,2})\\,=\\,\\infty$ and limN\u2192\u221e Na(N) = a(Va,\u03bc)\u00b7 ", "page_idx": 21}, {"type": "text", "text": "We now consider the allocation where every arm $a\\,\\in\\,B$ has $N_{a}(N_{1})$ samples, and consider the function, ", "page_idx": 21}, {"type": "equation", "text": "$$\nh(N_{1})\\ =\\ \\sum_{a\\in[K]/\\{1\\}}\\frac{d(\\nu_{1},x_{1,a})}{d(\\nu_{a},x_{1,a})}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Observe that, forall is strietly derasing for $N_{1}\\geq N_{1,2}$ Also if ${\\overline{{B}}}^{c}$ is non-empty, then every term $\\frac{d(\\nu_{1},x_{1,a})}{d(\\nu_{a},x_{1,a})}$ with $N_{a}>0$ is strctly ecreasing in $N_{1}$ .Asaresult the overallfunetion $N_{1}\\rightarrow h(N_{1})$ is strictly decreasing. ", "page_idx": 21}, {"type": "text", "text": "Moreover, as N increases to oo, Na(N) converges to ao,) for every $a\\in B$ Hence, $h(N_{1})$ decreases to O as $N_{1}\\to\\infty$ . Therefore, if we can show that $\\bar{h}(\\operatorname*{max}\\{N_{1,1},N_{2,1}\\})\\geq1$ , then we can find a unique $N_{1}^{\\star}\\ge\\operatorname*{max}\\{N_{1,1},N_{1,2}\\}$ at which $h(N_{1}^{\\star})=1$ , and can take $N_{a}^{\\star}=N_{a}(N_{1}^{\\star})$ for every $a\\in B$ . Following this, to prove uniqueness, it is sufficient to show $h(\\operatorname*{max}\\{N_{1,1},N_{1,2}\\})\\geq1$ ", "page_idx": 21}, {"type": "text", "text": "If $N_{1,2}\\,\\geq\\,N_{1,1}$ , then some $a\\in B$ has $N_{a}(N_{1,2})=\\infty$ . As a result, we have $h(N_{1,2})=\\infty\\geq1$ Otherwise, if $N_{1,1}\\geq N_{1,2}>0$ , then by definition of $N_{1,1}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nh(N_{1,1})\\ \\geq\\ \\sum_{a\\in\\overline{{B}}^{c}}\\frac{d(\\nu_{1},x_{1,a})}{d(\\nu_{a},x_{1,a})}=1.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence we finish proving the first part of Theorem D.1. ", "page_idx": 22}, {"type": "text", "text": "To see the necessity of the stated optimality conditions for $\\mathbf{O}$ observe that we cannot have $N_{1}^{\\star}=0$ or $N_{a}^{\\star}=0$ as that implies that the index $W_{a}(N_{1}^{\\star},N_{a}^{\\star})$ is zero. Further, if $W_{a}(N_{1}^{\\star},N_{a}^{\\star})>\\log(1/(2.4\\delta))$ \uff0c the objective improves by reducing $N_{a}^{\\star}$ . Thus the constraints (20) must be tight. To see the tightness of (20) again note that the derivative of $W_{a}(N_{1},N_{a})$ with respect to $N_{1}$ and $N_{a}$ , respectively, equals $d(\\mu_{1},x_{1,a})$ and $d(\\mu_{a},x_{1,a})$ ", "page_idx": 22}, {"type": "text", "text": "Now, perturbing $N_{1}$ by a tiny $\\epsilon$ and adjusting each $N_{a}$ by $\\frac{d(\\mu_{1},x_{1,a})}{d(\\mu_{a},x_{1,a})}\\epsilon$ maintains the value of $W_{a}(N_{1}^{\\star},N_{a}^{\\star})$ . Thus, at optimal $N^{\\star}$ , necessity of tightness of inequalities in (20) follows. This condition can also be seen through the Lagrangian (see [2]). ", "page_idx": 22}, {"type": "text", "text": "The fact that these three criteria uniquely specify the optimal solution follows from our analysis above. Since the convex problem $\\mathbf{O}$ has a solution, the uniqueness of the solution above satisfying the necessary conditions implies that this uniquely solves $\\mathbf{O}$ \u53e3 ", "page_idx": 22}, {"type": "text", "text": "To prove Propositions 2.1 and 2.2, we need to use the Implicit function theorem. For that, we define the following functions, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{1}(\\pmb{N})=g(\\pmb{\\mu},\\pmb{N})=\\displaystyle\\sum_{a\\neq1}\\frac{d(\\mu_{1},x_{1,a}(N_{1},N_{a}))}{d(\\mu_{a},x_{1,a}(N_{1},N_{a}))}-1,}\\\\ {\\forall\\,a\\in[K]/\\{1\\}\\}&{J_{a}(\\pmb{N},I)=N_{1}d(\\mu_{1},x_{1,a}(N_{1},N_{a}))+N_{a}d(\\mu_{a},x_{1,a}(N_{1},N_{a}))-I,}\\\\ &{J_{K+1}(\\pmb{N},N)=\\displaystyle\\sum_{a\\in[K]}N_{a}-N,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $N=(N_{a}\\in\\mathbb{R}_{\\geq0}:a\\in[K])$ \uff0c $I\\in\\mathbb{R},N\\in\\mathbb{R}_{+}$ , and, for every $a\\in[K]/\\{1\\}$ \uff0c $x_{1,a}(N_{1},N_{a})=$$\\frac{N_{1}\\mu_{1}\\!+\\!N_{a}\\mu_{a}}{N_{1}\\!+\\!N_{a}}$", "page_idx": 22}, {"type": "text", "text": "Using these functions, for every non-empty $B\\subseteq[K]/\\{1\\}$ , we define the vector valued function ", "page_idx": 22}, {"type": "equation", "text": "$$\nJ_{B}(N,I,N)=[J_{1}(N),\\;(J_{a}(N,I))_{a\\in B},\\;J_{K+1}(N,N)]\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We call $J_{[K]/\\{1\\}}(\\cdot)$ as $J(\\cdot)$ . Recall that $\\overline{B}$ denotes $B\\cup\\{1\\}$ ", "page_idx": 22}, {"type": "text", "text": "For every $m\\geq1$ , we use the notation ${\\bf0}_{m}$ to denote a $m$ -dimensional vector with all entries set to zero. Observe that, for every $B\\subseteq[K]/\\{1\\}$ \uff0c $J_{B}(N,I,N)=\\pmb{\\varPhi}_{B}(N,I,\\mathbf{0}_{K},N)$ , where the function $\\pmb{\\varPhi}_{B}(\\cdot)$ is defined in Appendix C. ", "page_idx": 22}, {"type": "text", "text": "Lemma D.1 is essential for proving Proposition 2.1 ", "page_idx": 22}, {"type": "text", "text": "Lemma D.1. For every $N>0$ and non-empty $B\\,\\subseteq\\,[K]/\\{1\\}$ if $\\hat{N}=(\\hat{N}_{a}\\,\\in\\,\\mathbb{R}_{\\geq0}\\,:\\,a\\,\\in\\,[K])$ satisfies $\\begin{array}{r}{\\sum_{a\\in\\overline{{B}}^{c}}\\hat{N}_{a}<N}\\end{array}$ and $J_{B}(\\hat{N},\\hat{I},N)=\\mathbf{0}_{|\\overline{{B}}|+1}$ then,the Jacobian of $J_{B}(\\cdot)$ with respect to the arguments $(N_{\\overline{{B}}},I)$ is invertible at $(\\hat{N},\\hat{I},N)$ ", "page_idx": 22}, {"type": "text", "text": "Proof.Wehave $J_{B}(N,I,N)=\\pmb{\\varPhi}_{B}(N,I,\\mathbf{0}_{K},N).$ foreverytuple $N,I,N$ , and non-empty $B\\subseteq$ $[K]/\\{1\\}$ . As a result, Lemma D.1 follows from statement 3 of Lemma C.1. \u25a0 ", "page_idx": 22}, {"type": "text", "text": "For every non-empty subset $B\\,\\subseteq\\,[K]/\\{1\\}$ ,we define the function $\\widetilde{J}_{B}(\\cdot)$ to be the first $|B|+1$ components of the vector valued function $\\dot{J}_{B}(\\cdot)$ ,or in otherwords, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde J_{B}(\\cdot)\\ =\\ \\left[\\ J_{1}(\\cdot)\\ ,\\ (J_{a}(\\cdot))_{a\\in B}\\ \\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Observe that $\\widetilde{J}_{B}(\\cdot)$ depends only on the tuple $_{N}$ and $I$ , and doesn't depend on $N$ . Also for every tuple $(N,I)\\in\\mathbb{R}_{\\ge0}^{K}$ \uff0c $\\widetilde{J}_{B}({\\pmb N},I)=\\widetilde{\\pmb{\\phi}}_{B}({\\pmb N},I,{\\pmb0}_{K})$ , where $\\widetilde{\\pmb{\\Phi}}_{B}$ is defined in Appendix C. ", "page_idx": 22}, {"type": "text", "text": "Proof of Proposition 2.1: Observe that, for every non-empty $B\\subseteq[K]/\\{1\\}$ , solving the system (2) is equivalent to solving for the pair $N_{\\overline{{B}}},I_{B}$ in $J_{B}((N_{\\overline{{B}}},N_{\\overline{{B}}^{c}}),I_{B};N)=\\dot{0}$ ", "page_idx": 23}, {"type": "text", "text": "For every $I\\,\\geq\\,0$ , by Theorem D.1, there is a unique $N_{\\overline{{B}}}\\,=\\,(N_{a}\\,\\in\\,\\mathbb{R}_{\\geq0}\\,:\\,a\\,\\in\\,\\overline{{B}})$ for which, $\\widetilde{\\cal J}_{B}((N_{\\overline{{B}}},N_{\\overline{{B}}^{c}}),I)={\\bf0}_{|B|+1}$ We denote that solution using $N_{\\overline{{B}}}(I)$ (we supress the dependence on $N_{\\overline{{B}}^{c}}$ for cleaner presentation, and also because we will be treating $N_{\\overline{{B}}^{c}}$ like a constant in the rest of the proof). Since, $\\widetilde{J}_{B}({\\pmb N},I)=\\widetilde{\\pmb{\\phi}}_{B}({\\pmb N},I,{\\pmb0}_{K})$ , by the Implicit function theorem and using statement 1 of Lemma C.1, the function $I\\to N_{\\overline{{B}}}(I)$ is continuously differentiable. Also, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nN_{\\overline{{B}}}^{\\prime}(I)\\ =\\ -\\left(\\frac{\\partial\\widetilde{J}_{B}}{\\partial N_{\\overline{{B}}}}\\right)^{-1}\\frac{\\partial\\widetilde{J}_{B}}{\\partial I}\\ =\\ -\\left(\\frac{\\partial\\widetilde{\\pmb{\\varPhi}}_{B}}{\\partial N_{\\overline{{B}}}}\\right)^{-1}\\frac{\\partial\\widetilde{\\pmb{\\varPhi}}_{B}}{\\partial I},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the right most quantity is evaluated at the tuple $((N_{\\overline{{B}}}(I),N_{\\overline{{B}}^{c}}),I,\\mathbf{0}_{K})$ . Moreover, using statement 2 of Lemma C.1, we have, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{a\\in\\overline{{B}}}N_{a}^{\\prime}(I)\\ =\\ {\\bf1}_{|B|+1}^{T}N_{\\overline{{B}}}^{\\prime}(I)\\ =\\-{\\bf1}_{|B|+1}^{T}\\left(\\frac{\\partial\\widetilde{\\phi}_{B}}{\\partial N_{\\overline{{B}}}}\\right)^{-1}\\frac{\\partial\\widetilde{\\phi}_{B}}{\\partial I}\\ \\ge\\ \\sum_{a\\in B}\\frac{1}{d(\\mu_{a},\\mu_{1})}>0\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As a result, the function $\\textstyle\\sum_{a\\in{\\overline{{B}}}}N_{a}(I)$ is stricly increasing in $I$ with a derivative atleast ZaeB d(Pa) \uff0e Also, for $I\\;=\\;0$ ,the unique solution is $N_{1}(0)~=~N_{1,1}$ and $N_{a}(0)~=~0$ for every $a\\:\\in\\:B$ . As a result, as $I$ increases from O to $\\infty$ \uff0c $\\textstyle\\sum_{a\\in{\\overline{{B}}}}N_{a}(I)$ increases from $N_{11}$ to $\\infty$ monotonically.Hence,for every $\\begin{array}{r}{N\\,\\ge\\,N_{11}+\\sum_{a\\in B^{c}}N_{a}}\\end{array}$ , we can find a unique $I_{B}$ for which $\\begin{array}{r}{\\sum_{a\\in\\overline{{B}}}N_{a}(I_{B})+\\sum_{a\\in\\overline{{B}}^{c}}N_{a}\\,=\\,N}\\end{array}$ . Therefore $N_{\\overline{{B}}}(I_{B}),I_{B}$ becomes the unique tuple to satisfy, $J_{B}((N_{\\overline{{B}}},N_{\\overline{{B}}^{c}}),I_{B},N)=\\mathbf{0}_{|B|+2}$ \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Proof of Proposition 2.2: Recall that solving $\\mathbf{O}$ in (20) is equivalent to solving (1). With this observation Proposition 2.2 follows directly from the definition of $N_{[K]/\\{1\\}}(1),I_{[K]/\\{1\\}}(1)$ and the second statement of Theorem D.1. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "D.1  Single variable formulation of the lower bound problem and intuition behind the anchor function ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Now we show that the $K$ -variable convex optimization problem $\\mathbf{O}$ defined in (20) can be reduced to a single variable convex optimization problem involving only $N_{1}$ . To see this, observe that $\\mathbf{O}$ is equivalent to the problem ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{O}_{1}}&{\\displaystyle\\operatorname*{min}_{a\\in[K]}\\,N_{a}}\\\\ &{\\mathrm{s.t.}\\quad\\forall a\\neq1,\\,W_{a}(N_{1},N_{a})\\ =\\ N_{1}d(\\mu_{1},x_{1,a})+N_{a}d(\\mu_{a},x_{1,a})\\ =\\ \\log(1/(2.4\\delta))}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This follows from the proof of Theorem D.1. ", "page_idx": 23}, {"type": "text", "text": "We first make some observations about $\\mathbf{O}_{1}$ ", "page_idx": 23}, {"type": "text", "text": "1. Upon fixing some $N_{1}\\,>\\,0$ $N_{a}\\,\\rightarrow\\,W_{a}(N_{1},N_{a})$ is strictly increasing for every $a\\ne1$ Moreover, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nW_{a}(N_{1},0)=0\\quad\\mathrm{and}\\quad\\operatorname*{lim}_{N_{a}\\to\\infty}W_{a}(N_{1},N_{a})=N_{1}d(\\mu_{1},\\mu_{a}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As a result, every feasible solution of $\\mathbf{O}_{1}$ must satisfy $N_{1}d(\\mu_{1},\\mu_{a})>\\log(1/(2.4\\delta))$ for every $a\\neq1$ , wWhich implies $\\begin{array}{r}{N_{1}>\\frac{\\log(1/(2.4\\delta))}{\\operatorname*{min}_{a\\neq1}d(\\mu_{1},\\mu_{a})}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "2. Using the preceding observation, for every $a\\neq1$ and $\\begin{array}{r}{N_{1}>\\frac{\\log(1/(2.4\\delta))}{\\operatorname*{min}_{a\\neq1}d(\\mu_{1},\\mu_{a})}}\\end{array}$ mina>1 d(li,la), we can find a unique $N_{a}$ such that $W_{a}(N_{1},N_{a})=\\log(1/(2.4\\delta))$ . We use $\\bar{N}_{a}(N_{1})$ to denote that unique $N_{a}$ as a function of $N_{1}$ . Using the Implicit function theorem, it is easy to prove that the function $N_{1}\\rightarrow\\bar{N}_{a}(N_{1})$ is differentiable for every minas1 d(4,fa) with the derivative being ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\bar{N}}_{a}^{\\prime}(N_{1})\\ =\\ -\\,\\frac{d(\\mu_{1},z_{a}(N_{1}))}{d(\\mu_{a},z_{a}(N_{1}))}\\quad\\mathrm{where}\\quad z_{a}(N_{1})\\ =\\ \\frac{N_{1}\\mu_{1}+{\\bar{N}}_{a}(N_{1})\\mu_{a}}{N_{1}+{\\bar{N}}_{a}(N_{1})}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It follows that $\\mathbf{O}_{1}$ is equivalent to the single variable optimization problem: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{O}_{2}\\quad\\operatorname*{min}{f(N_{1})}\\ \\stackrel{\\mathrm{def.}}{=}\\ N_{1}+\\sum_{a\\neq1}\\bar{N}_{a}(N_{1})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Further, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f^{\\prime}(N_{1})\\;=\\;1+\\displaystyle\\sum_{a\\ne1}\\bar{N}_{a}^{\\prime}(N_{1})}}\\\\ {{\\qquad\\qquad\\qquad=\\;1-\\displaystyle\\sum_{a\\ne1}\\frac{d(\\mu_{1},z_{a}(N_{1}))}{d(\\mu_{a},z_{a}(N_{1}))},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is exactly negative of the anchor function defined in Section 2 w.r.t. the allocation where arm 1 gets $N_{1}$ samples and_ every  sub-optimal arm $a\\ne1$ gets $\\bar{N}_{a}(N_{1}\\bar{)}$ samples. Furthermore, $f^{\\prime}(N_{1})$ is strictly increasing in $N_{1}$ and increases from $-\\infty$ to 1 as $N_{1}$ increases from $\\frac{\\log(1/(2.4\\delta))}{\\operatorname*{min}_{a\\neq1}d(\\mu_{1},\\mu_{a})}$ to $\\infty$ . As a result, $\\mathbf{O}_{2}$ is a convex problem w.r.t. $N_{1}$ and the optimal $N_{1}$ solving $\\mathbf{O}_{2}$ is uniquely identified by the condition: ", "page_idx": 24}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/b3c8f75e7d27970e7c251019b1a60a24d3141144c8d885728e82408f58c18a5c.jpg", "img_caption": ["Figure 3: Illustrative plot of $\\mathbf{O}_{2}$ 's objective $f(N_{1})$ "], "img_footnote": [], "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{a\\neq1}\\frac{d(\\mu_{1},z_{a}(N_{1}))}{d(\\mu_{a},z_{a}(N_{1}))}-1=0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is equivalent to saying that the   \nanchor function $g$ must be zero when   \nevaluated at the allocation where the first arm gets $N_{1}$ samples and every arm $a\\ne1$ gets $\\bar{N}_{a}(N_{1})$   \nsamples. Figure 3 shows an illustrative plot of $\\mathbf{O}_{2}$ 's objective $f(N_{1})$ ", "page_idx": 24}, {"type": "text", "text": "This observation also implies that the unique $\\beta$ which makes the $\\beta$ -EB-TCB(I) policy in [16] asymptotically optimal is uniquely identified by the first order conditions: 1) the anchor function $g$ must be zero and 2) indexes of the sub-optimal arms must be equal. Hence allocations made by every asymptotically optimal sampling policy must converge to these first order conditions. Both the fuid dynamics in Section 4 and Proposition 5.1 in Section 5, respectively, show that the sampling policies of (I)AT2 algorithm converges to these first order conditions in a fuid model and in the proposed algorithms. ", "page_idx": 24}, {"type": "text", "text": "D.2  Sub-optimality of TCB(I) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "[22] implicitly assumes that the optimal proportion is uniquely identified by the condition that indexes of every sub-optimal arm under the proportion are equal. Note that the optimal proportion is a $K$ dimensional vector and the conditions mentioned in [22, Lemma 2, statement 2] has $K-1$ equations $(K-2$ equations to maintain equality of $K-1$ indexes and one more equation to make sure that all the entries in the $K$ -dimensional vector add up to 1). Moreover, in Appendix D.1, changing the variables of the problem $\\mathbf{O}_{2}$ to capture proportion of samples allocated to every arm, it is not hard to prove that for every value of $w_{1}\\in(0,1)$ , we can get a unique proportion $(w_{1},w_{2},\\dots,w_{K})$ such that $\\textstyle\\sum_{a\\in[K]}w_{a}=1$ and index of all sub-optimal rms under the proportion are equal. By the argument in Appendix D.1, the unique optimal proportion out of these infinite no. of proportions satisfying equality of the indexes, is uniquely identified by the necessary and sufficient condition that the anchor function $g(\\cdot)$ evaluated at that proportion must be zero. Without this condition, the allocation cannot be optimal. In the numerical experiments of Appendix J.2, we see that the anchor function doesn't always converge to zero for the TCB(I) algorithm. As a result, allocations made by TCB(I) can be sub-optimal. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "E Proofs from Section 4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof of Theorem 4.1: We first prove all the steps of Theorem 4.1 except for showing the existence Oof $\\beta>0$ and independent of $N$ such that $I_{b}^{\\prime}(N)>\\beta$ . That requires intermediate lemmas and is done separately. ", "page_idx": 25}, {"type": "text", "text": "First suppose that $B$ contains a singleton index $b$ Define $N_{1}(N)$ and $N_{b}(N)$ using IFT through the equations ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{a\\neq1}\\frac{d(\\mu_{1},x_{1,a})}{d(\\mu_{a},x_{1,a})}-1=0\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and $\\textstyle\\sum_{a}N_{a}\\,=\\,N$ For each $a$ , letting $\\boldsymbol{x}_{1,a}^{\\prime}$ denote the derivative of $x_{1,a}$ with respect to $N_{1},\\,\\widetilde{x}_{1,b}^{\\prime}$ denote the derivative of $x_{1,b}$ with respect to $N_{b}$ . It is easy to check that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{x}_{1,b}^{\\prime}=-\\frac{N_{1}}{N_{a}}x_{1,b}^{\\prime},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and each $\\begin{array}{r}{x_{1,a}^{\\prime}=\\frac{N_{a}\\varDelta_{a}}{(N_{1}+N_{a})^{2}}}\\end{array}$ Difeentiaing 24 with rscto $N$ observingthat $N_{b}^{\\prime}=1-N_{1}^{\\prime}$ we ", "page_idx": 25}, {"type": "equation", "text": "$$\nN_{1}^{\\prime}\\sum_{a\\neq1}h_{a}N_{a}=N_{1}h_{b}(1-N_{1}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It follows that ", "page_idx": 25}, {"type": "equation", "text": "$$\nN_{1}^{\\prime}=\\frac{N_{1}h_{b}}{\\sum_{a\\neq1}h_{a}N_{a}+N_{1}h_{b}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "as stipulated. Also N = aNa+Nh ", "page_idx": 25}, {"type": "text", "text": "Now consider the case where $g=0$ , and we have set $B\\subset[K]/1$ of indices where the indexes are equal, they are higher for the remaining set. Cardinality of $B$ is at least 2. We want to argue that as $N$ increases, and the equality of indexes in $B$ is maintained along with $g=0$ , then the tied indexes will increase with $N$ ", "page_idx": 25}, {"type": "text", "text": "We have for $b,a\\in B$ ", "page_idx": 25}, {"type": "equation", "text": "$$\nN_{1}d(\\mu_{1},x_{1,b})+N_{b}d(\\mu_{b},x_{1,b})=N_{1}d(\\mu_{1},x_{1,a})+N_{a}d(\\mu_{a},x_{1,a}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Furthermore, $g=0$ i.e., ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{a\\neq1}\\frac{d(\\mu_{1},x_{1,a})}{d(\\mu_{a},x_{1,a})}=1.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Keeping a particular $b\\in B$ fixed, differentiating with respect to $N$ , (since for each $a$ , by definition of $x_{1,a},\\bar{N_{1}}d^{\\prime}(\\mu_{1},x_{1,a})+N_{a}d^{\\prime}(\\mu_{a},x_{1,a})=0)$ we see from (26) that ", "page_idx": 25}, {"type": "equation", "text": "$$\nN_{1}^{\\prime}d(\\mu_{1},x_{1,b})+N_{b}{}^{\\prime}d(\\mu_{b},x_{1,b})=N_{1}^{\\prime}d(\\mu_{1},x_{1,a})+N_{a}{}^{\\prime}d(\\mu_{a},x_{1,a}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using (26) again in the above equality, ", "page_idx": 25}, {"type": "equation", "text": "$$\nN_{a}^{\\prime}=\\frac{1}{N_{1}d(\\mu_{a},x_{1,a})}\\left(N_{a}d(\\mu_{a},x_{1,a})-N_{b}d(\\mu_{b},x_{1,b})\\right)N_{1}^{\\prime}+\\frac{d(\\mu_{b},x_{1,b})}{d(\\mu_{a},x_{1,a})}N_{b}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then from (27), we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\nN_{1}^{\\prime}\\sum_{a\\neq1}f(\\pmb{\\mu},a,\\pmb{N})x_{1,a}^{\\prime}+\\sum_{a\\in B}f(\\pmb{\\mu},a,\\pmb{N})\\widetilde{x}_{1,a}^{\\prime}N_{a}^{\\prime}=0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(Recall that for each $a$ $\\boldsymbol{x}_{1,a}^{\\prime}$ denotes the derivative of $x_{1,a}$ with respect to $N_{1}$ and $\\widetilde{x}_{1,a}^{\\prime}$ denotes the derivative of $x_{1,a}$ with respect to $N_{a}$ ", "page_idx": 26}, {"type": "text", "text": "Plugging (28) and (25) in (29), multiplying each term by $N_{1}^{2}$ , we see that $N_{1}^{\\prime}$ is a ratio of ", "page_idx": 26}, {"type": "equation", "text": "$$\nN_{1}N_{b}^{\\prime}d_{b,b}h_{B}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with ", "page_idx": 26}, {"type": "equation", "text": "$$\nh(N)+N_{b}d_{b,b}h_{B}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, ", "page_idx": 26}, {"type": "equation", "text": "$$\nN_{b}^{\\prime}=N_{1}^{\\prime}\\frac{h(N)d_{b,b}^{-1}+N_{b}h_{B}}{N_{1}h_{B}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In particular, since, $\\textstyle\\sum_{a}N_{a}^{\\prime}~=~1$ ,(3) follow. Statement 3 of Theorem 4.1 follows from (3) and expression for $I_{a}(\\tilde{N})$ Since $H_{a}^{\\prime}(N)~>~0$ index is non-decreasing in $N$ .Furthermore, $\\operatorname*{lim}_{N\\rightarrow\\infty}I_{a}(N)=N_{a}d(\\mu_{a},\\mu_{1})$ \u53e3 ", "page_idx": 26}, {"type": "text", "text": "To prove the existence of $\\beta>0$ and independent of $N$ such that $I_{b}^{\\prime}(N)>\\beta$ , we need Lemmas (E.1), (E.2) and (E.3). In Lemma E.3, we argue that the indexes in set $B$ grow linearly with the number of samples. Since index for arm a E B are bounded, eventually indexes in set $B$ catch-up with other indexes. ", "page_idx": 26}, {"type": "text", "text": "Some notation first. Observe that $d(\\mu_{1},x)-d(\\mu_{a},x)$ is a continuous and strictly decreasing function of $x\\in[\\mu_{a},\\mu_{1}]$ . It equals $d(\\mu_{1},\\mu_{a})$ for $x=\\mu_{a}$ and $-d(\\mu_{a},\\mu_{1})$ for $x=\\mu_{1}$ . Let $\\boldsymbol{x}_{a}\\in(\\bar{\\mu}_{a},\\mu_{1})$ be such that ", "page_idx": 26}, {"type": "equation", "text": "$$\nd(\\mu_{1},x_{a})=d(\\mu_{a},x_{a}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Furthermore, let ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widetilde a=\\arg\\operatorname*{max}_{a}\\frac{d(\\mu_{1},x_{1,a})}{d(\\mu_{a},x_{1,a})}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $x(\\widetilde a)$ be such that ", "page_idx": 26}, {"type": "equation", "text": "$$\nd(\\mu_{1},x(\\widetilde{a}))=(K-1)^{-1}d(\\mu_{a},x(\\widetilde{a})).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It is guaranteed to exist since the ratio $d(\\mu_{a},x)/d(\\mu_{1},x)\\in(0,\\infty)$ is monotonic in $x$ ", "page_idx": 26}, {"type": "text", "text": "Next, let ", "page_idx": 26}, {"type": "equation", "text": "$$\nd_{a}={\\frac{\\mu_{1}-x_{a}}{x_{a}-\\mu_{a}}}\\quad{\\mathrm{~and~}}\\quad d({\\widetilde{a}})={\\frac{\\mu_{1}-x({\\widetilde{a}})}{x({\\widetilde{a}})-\\mu_{a}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $K\\ge2$ , we have $x(\\widetilde{a})\\geq x_{\\widetilde{a}}$ , and $d(\\widetilde{\\boldsymbol{a}})\\leq d_{\\widetilde{\\boldsymbol{a}}}$ ", "page_idx": 26}, {"type": "text", "text": "Lemma E.1. Suppose that $g=0$ Then, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left(1+\\sum_{a\\neq1}{d_{a}}\\right)^{-1}N\\leq N_{1}\\leq\\left(1+d(\\widetilde{a})\\right)^{-1}N.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma E.2. Suppose that $g=0$ Then, ", "page_idx": 26}, {"type": "text", "text": "1. there exist constants $H$ and $D$ such that $h_{a}\\leq H$ for all $a$ and $d_{a,a}^{-1}\\leq D$ for all $a$ ", "page_idx": 26}, {"type": "text", "text": "2. Further, there exists an $\\widetilde{a}$ suchthat $N_{\\widetilde{a}}>\\alpha N$ forsome $\\alpha>0$ and the corresponding $h_{\\widetilde{a}}$ is bounded from below by a positive constant. ", "page_idx": 26}, {"type": "text", "text": "Lemma E.3. Suppose that $g=0$ andfor $B\\,\\subset\\,[K]/1$ the indexes are all equal and are strictly higher for the remaining set. Then there exists a constant $\\beta>0$ suchthat ", "page_idx": 26}, {"type": "equation", "text": "$$\nN_{1}^{\\prime}d(\\mu_{1},x_{1,a})+N_{a}^{\\prime}d(\\mu_{a},x_{1,a})=I_{B}^{\\prime}(N)>\\beta.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma (E.1): Since $g=0$ , it follows that for each $a\\in[K]\\setminus1$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{d_{1,a}}{d_{a,a}}\\leq1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, $x_{1,a}\\geq x_{a}$ . This in turn implies that for each $a$ \uff0c ", "page_idx": 27}, {"type": "equation", "text": "$$\nN_{1}\\geq N_{a}d_{a}^{-1}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The above follows from substituting for $x_{1,a}$ in the inequality $x_{1,a}\\geq x_{a}$ , and from the definition of $d_{a}$ . Moreover, since $g=0$ , it also follows that for each $a\\ne1$ \uff0c $x((\\widetilde{a})\\geq x_{1,a}$ , implying ", "page_idx": 27}, {"type": "equation", "text": "$$\nN_{1}d(({\\widetilde{a}}))\\leq N_{a}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, ", "page_idx": 27}, {"type": "equation", "text": "$$\nN_{1}\\left(1+\\sum_{a\\neq1}d_{a}\\right)\\geq N\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\nN_{1}(1+d(\\widetilde{a}))\\leq N,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the result follows. ", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma (E.2): Recall the definitions of $h_{a}$ and $f(\\mu,a,N)$ from Section 4. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Since, $g=0$ implies that $x_{1,a}\\geq x_{a}$ for all $a$ , it follows that $d_{a,a}=d(\\mu_{a},x_{1,a})\\geq d(\\mu_{a},x_{a})$ . In particular, for all $a$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nd_{a,a}^{-1}\\leq D\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for $D=\\operatorname*{max}_{a}d(\\mu_{a},x_{a})^{-1}$ ", "page_idx": 27}, {"type": "text", "text": "Further, $d^{\\prime}(\\mu_{a},x_{1,a})$ is continuous in $x_{1,a}$ and is bounded from above by $\\operatorname*{sup}_{x_{a}\\leq x_{1,a}\\leq\\mu_{1}}d^{\\prime}(\\mu_{a},x_{1,a})$ Similarly, $-d^{\\prime}(\\mu_{1},x_{1,a})$ is bounded from above by $\\mathrm{sup}_{x_{a}\\leq x_{1,a}\\leq\\mu_{1}}-d^{\\prime}(\\mu_{1},x_{1,a})$ This implies that $f(\\mu,a,N)$ is bounded from above by a positive constant and hence so is $h_{a}$ ", "page_idx": 27}, {"type": "text", "text": "To see part 2, observe from definition of $x(\\tilde{a})$ that $x_{1,\\tilde{a}}\\leq x(\\tilde{a})$ . It follows that ", "page_idx": 27}, {"type": "equation", "text": "$$\nN_{\\widetilde{a}}\\geq N d_{\\widetilde{a}}^{-1}(1+\\sum_{a\\neq1}d_{a})^{-1}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Again, $x_{1,\\widetilde{a}}\\;\\leq\\;x(\\widetilde{a})$ .Therefore, $d_{\\widetilde{a},\\widetilde{a}}\\:=\\:d(\\mu_{\\widetilde{a}},x_{1,\\widetilde{a}})\\:\\leq\\:d(\\mu_{\\widetilde{a}},x(\\widetilde{a}))$ and $d_{1,{\\widetilde a}}\\;=\\;d(\\mu_{1},x_{1,{\\widetilde a}})\\;\\geq$$d(\\mu_{1},x(\\widetilde{a}))$ \uff1a", "page_idx": 27}, {"type": "text", "text": "Further, $d^{\\prime}(\\mu_{\\widetilde{a}},x_{1,\\widetilde{a}})$ is continuous in $x_{1,\\widetilde{a}}$ and is bounded from below by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{x_{\\tilde{a}}\\leq x_{1,\\tilde{a}}\\leq x(\\tilde{a})}d^{\\prime}(\\mu_{\\tilde{a}},x_{1,\\tilde{a}}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similarly, $-d^{\\prime}(\\mu_{1},x_{1,\\tilde{a}})$ is bounded from below by $\\operatorname*{inf}_{x_{\\tilde{a}}\\leq x_{1,\\tilde{a}}\\leq x(\\tilde{a})}-d^{\\prime}(\\mu_{1},x_{1,\\tilde{a}})$ ", "page_idx": 27}, {"type": "text", "text": "Thus, $f(\\pmb{\\mu},\\widetilde{\\boldsymbol{a}})$ is bounded from below. Further, since each $N_{a}\\,\\leq\\,d_{a}N_{1}$ $N_{1}^{2}/(N_{1}+N_{\\widetilde{a}})^{2}\\,\\geq$ $(1+d_{\\widetilde{a}})^{-2}$ , hence, $h_{\\widetilde{a}}$ is bounded from below by a positive constant. ", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma (E.3): Recall from (31) that ", "page_idx": 27}, {"type": "equation", "text": "$$\nN_{\\widetilde{a}}\\geq N d_{\\widetilde{a}}^{-1}(1+\\sum_{a\\neq1}d_{a})^{-1}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Also, recall the definition of $f(\\mu,a,N)$ and $h_{a}$ from Section 4. ", "page_idx": 27}, {"type": "text", "text": "Because of (30) and (32), and since $N_{\\widetilde{a}}\\,\\leq\\,N$ , we see that $f(\\pmb{\\mu},\\widetilde{\\boldsymbol{a}})$ is bounded from below by a positive constant, and the same is true for $h_{\\widetilde{a}}$ ", "page_idx": 28}, {"type": "text", "text": "$\\widetilde{a}\\in B$ , recall that $\\begin{array}{r}{h_{B}=\\sum_{a\\in B}h_{a}d_{a,a}^{-1}}\\end{array}$ .Thus, $h_{B}$ is greater than a constant times $N$ . This ensures that $N_{\\widetilde{a}}^{\\prime}$ is bounded from below by a positive constant. Since $d_{\\widetilde{a},\\widetilde{a}}$ is also bounded from below by a positive constant, we conclude that there exists a $\\beta>0$ such that $I_{B}^{\\prime}(N)>\\beta$ ", "page_idx": 28}, {"type": "text", "text": "$\\widetilde{a}\\notin B$ , then recalling that $\\begin{array}{r}{h(N)=\\sum_{a\\in B^{c}/1}h_{a}N_{a}}\\end{array}$ , we conclude that $N_{\\widetilde{a}}^{\\prime}$ is bounded from below by a positive constant. This implies that as $N$ increases by a positive fraction, so does each $N_{a}$ for $a\\in B$ . This in turn ensures that then $h_{B}$ is thereafter bounded from below by a positive constant. In particular, after some delay we have $I_{B}^{\\prime}(N)>\\beta$ for some $\\beta>0$ \u53e3 ", "page_idx": 28}, {"type": "text", "text": "E.1 Fluid dynamics starting at $g<0$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proposition E.1 provides us the ODEs by which the fuid allocations evolve in step 3 of the description of fuid dynamics when $g<0$ ", "page_idx": 28}, {"type": "text", "text": "Proposition E.1. Now consider the case where $g<0$ at totalallocations $N$ and $B$ again denotes the set of arms whose indexes have the minimum value. Then, till $I_{B}(N)$ increases with $N$ to either hit an index in ${\\overline{{B}}}^{c}$ or for g to equal zero, whichever is earlier,. $\\begin{array}{r}{I_{B}^{\\prime}(N)=\\left(\\sum_{a\\in B}d_{a,a}^{-1}\\right)^{-1}}\\end{array}$ , and for $a\\in B$ $\\begin{array}{r}{N_{a}^{\\prime}=d_{a,a}^{-1}\\left(\\sum_{a\\in B}d_{a,a}^{-1}\\right)^{-1}}\\end{array}$ . In particular, $I_{B}(N)$ and each $[N_{a},\\,a\\in B]$ ), are increasing functions of $N$ ", "page_idx": 28}, {"type": "text", "text": "Proof. Let $i_{1}$ denote the arm corresponding to a minimum index. Recall that $\\omega^{\\star}=(\\omega_{a}^{\\star}:a\\in[K])$ denote the optimal proportions to the lower bound problem. Consider $\\begin{array}{r}{\\hat{N}_{a}=\\frac{\\omega_{a}^{\\star}}{\\omega_{1}^{\\star}}N_{1}}\\end{array}$ . Recall that at these samples, $g\\,=\\,0$ and all the indexes are equal. Let $\\hat{I}$ denote the corresponding value of the indexes at this allocation. ", "page_idx": 28}, {"type": "text", "text": "First we argue that $N_{i_{1}}<\\hat{N}_{i_{1}}$ ", "page_idx": 28}, {"type": "text", "text": "Suppose this is not true, then $g<0$ implies that for $N_{1}$ fixed, there exists an arm $a$ so that $N_{a}<\\hat{N}_{a}$ else if each $N_{a}\\geq\\hat{N}_{a}$ then since $g$ increases with $N_{a}$ , we would have $g\\geq0$ . This contradiction implies that index for arm $a$ is $<\\hat{I}_{B}(N)$ . It follows that the index corresponding to $i_{1}$ is $<\\hat{I}_{B}(N)$ Since the index increases with $N_{i_{1}}$ , it follows that $N_{i_{1}}<\\hat{N}_{i_{1}}$ ", "page_idx": 28}, {"type": "text", "text": "Thus, initially $N$ increases due to increase in $N_{i_{1}}$ . Let $B=\\{i_{1}\\}$ . Suppose, iteratively that $B=$ $\\{i_{1},\\dotsc,i_{j-1}\\}$ , denotes the smallest indexes that are equal and increase with $N$ and $g<0$ Proof follows by observing that the derivative of each index $a\\in B$ satisfies the relation $N_{a}^{\\prime}d_{a,a}=I_{B}^{\\prime}(N)$ Further, $\\dot{\\sum}_{a\\in B}\\,N_{a}^{\\prime}\\,\\dot{=}\\,1$ ", "page_idx": 28}, {"type": "text", "text": "Thus, as $N$ increases, each $N_{a}(N),a\\in B$ increases, so that $g$ increases. Since all indexes corresponding to ${\\overline{{B}}}^{c}$ are constant, as $N$ increases, either $g=0$ first or another index $I_{j}$ becomes equal to $I_{B}(N)$ \u53e3 ", "page_idx": 28}, {"type": "text", "text": "E.2  Fluid dynamics of the $\\beta$ -EB-TCB algorithm ([16]) ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For every $\\beta\\in(0,1)$ and allocation $N=(N_{a}\\in\\mathbb{R}_{\\geq0}:a\\in[K])$ , we define the $\\beta$ -anchor function as, ", "page_idx": 28}, {"type": "equation", "text": "$$\ng(N;\\beta)\\;=\\;\\beta-{\\frac{N_{1}}{\\sum_{a\\in[K]}N_{a}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that, if $g(N;\\beta)=0$ , then $\\beta$ -fraction of the total no. of samples in the allocation $_{N}$ is allocated to the first arm. The fluid dynamics for the $\\beta$ -EB-TCB algorithm (see [16]) can be constructed similarly to that of the Anchored Top Two algorithm, by replacing the anchor function $g(\\pmb{\\mu},N)$ with the $\\beta$ -anchorfunction $g(N;\\beta)$ in Section 4. ", "page_idx": 28}, {"type": "text", "text": "Existence of fuid dynamics: Recall that, for every $B\\,\\subseteq\\,[K]/\\{1\\}$ $\\overline{B}$ denotes the set $B\\cup\\{1\\}$ Lemma E.4 and Proposition E.2 are essential for constructing the fuid behavior for $\\beta$ -EB-TCB algorithm. ", "page_idx": 28}, {"type": "text", "text": "Lemma E.4. Given a non-empty $B\\subseteq[K]/\\{1\\}$ some tuple $N_{\\overline{{B}}^{c}}\\,=\\,(N_{a}\\,\\in\\,\\mathbb{R}_{\\ge0}\\,:\\,a\\in\\,\\overline{{B}}^{c})$ and $I\\geq0$ there is a unique tuple $N_{\\overline{{B}}}=(N_{a}\\in\\mathbb{R}_{\\geq0}:a\\in\\overline{{B}})$ which satisfies, ", "page_idx": 29}, {"type": "equation", "text": "$$\nN_{1}\\ =\\ \\beta\\sum_{a\\in[K]}N_{a},\\quad a n d\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\na\\in B,\\quad N_{1}d(\\mu_{1},x_{1,a})+N_{a}d(\\mu_{a},x_{1,a})\\;=\\;I,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{x_{1,a}=\\frac{N_{1}\\mu_{1}+N_{a}\\mu_{a}}{N_{1}+N_{a}}}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Moreover, ifwedefine $\\begin{array}{r c l}{N_{1,1}\\!}&{=}&{\\!\\beta\\sum_{a\\in\\overline{{B}}^{c}}N_{a}}\\end{array}$ and $\\begin{array}{r c l}{N_{1,2}\\!}&{=}&{\\!\\frac{I}{\\operatorname*{min}_{a\\in B}d(\\mu_{1},\\mu_{a})}}\\end{array}$ then $N_{1}~~\\ge$ $\\operatorname*{max}\\{N_{1,1},N_{1,2}\\}$ ", "page_idx": 29}, {"type": "text", "text": "Proof. Proof of this lemma follows an argument similar to the proof of Theorem D.1. ", "page_idx": 29}, {"type": "text", "text": "First we fix some $I~\\geq~0$ and $N_{1}~\\ge~N_{1,2}$ . Note that for every $a\\ \\in\\ B$ ,as $N_{a}$ increases from 0 to $\\infty$ \uff0c $N_{1}d(\\mu_{1},x_{1,a})+N_{a}d(\\mu_{a},x_{1,a})$ increases monotonically from O to $N_{1}d(\\mu_{1},\\mu_{a})$ . Since $\\begin{array}{r}{N_{1}\\ge N_{1,2}\\ge\\frac{I}{d(\\mu_{1},\\mu_{a})}.}\\end{array}$ we have $N_{1}d(\\mu_{1},\\mu_{a})\\geq I.$ This implies for a given $N_{1}$ there is a unique value of $N_{a}$ for which $N_{1}d(\\mu_{1},x_{1,a})+N_{a}d(\\mu_{a},x_{1,a})=I$ .We call this value $N_{a}(N_{1})$ for every $a\\in B$ ", "page_idx": 29}, {"type": "text", "text": "Observe that $N_{1}\\rightarrow N_{a}(N_{1})$ is a strictly decreasing function of $N_{1}$ , and if $N_{1}=N_{1,2}$ , then there exists ana E Bfor which Na(N,2) = 0. On the other hand, if N \u2192 0, Na(N1) \u2192 a(\u03bca,\u03bc) for every $a\\in B$ ", "page_idx": 29}, {"type": "text", "text": "For every $N_{1}$ , we consider the function ", "page_idx": 29}, {"type": "equation", "text": "$$\nh(N_{1};\\beta)=\\beta-\\frac{N_{1}}{N_{1}+\\sum_{a\\in{\\cal B}}N_{a}(N_{1})+\\sum_{a\\in\\overline{{{\\cal B}}}^{c}}N_{a}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that $h(N_{1};\\beta)$ is the value of $g(N;\\beta)$ , when the tuple $_{N}$ has $N_{a}=N_{a}(N_{1})$ for every $a\\in B$ Note that $N_{1}\\to h(N_{1};\\beta)$ is strictly decreasing for $N_{1}\\geq N_{1,2}$ . Moreover, as $N_{1}\\rightarrow\\infty$ \uff0c $h(N_{1};\\beta)\\rightarrow$ $\\beta-1<0$ . In the rest of the argument, we show that $h(\\operatorname*{max}\\{N_{1,1},N_{1,2}\\};\\beta)\\ge0.$ After we prove this, we can find a unique $N_{1}$ for which $h(N_{1};\\beta)=0$ . Using this, we take $N_{a}=N_{a}(N_{1})$ for every $a\\in B$ to obtain our unique tuple $N_{\\overline{{B}}}$ ", "page_idx": 29}, {"type": "text", "text": "Now we consider two cases. ", "page_idx": 29}, {"type": "text", "text": "Case 1: If $N_{1,1}\\geq N_{1,2}$ , then at $N_{1}=N_{1,1}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\nN_{1,1}~=~\\beta\\sum_{a\\in\\overline{{B}}^{c}}N_{a}~\\leq~\\beta(N_{1,1}+\\sum_{a\\in[K]/\\{1\\}}N_{a}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "As a result, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{N_{1,1}}{N_{1,1}+\\sum_{a\\in\\overline{{B}}^{c}}N_{a}+\\sum_{a\\in B}N_{a}(N_{1,1})}\\leq\\beta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which implies $h(N_{1,1};\\beta)\\ge0$ ", "page_idx": 29}, {"type": "text", "text": "Case 2: If $N_{1,2}\\geq N_{1,1}$ , then, as we argued before, there exists an $a\\in B$ for which $N_{a}(N_{1,2})=\\infty$ As a result, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{N_{1,2}}{N_{1,2}+\\sum_{a\\in\\overline{{B}}^{c}}N_{a}+\\sum_{a\\in B}N_{a}(N_{1,2})}=0\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "implying $h(N_{1,2};\\beta)=\\beta>0$ ", "page_idx": 29}, {"type": "text", "text": "Proposition E.2 stated below is crucial for constructing the fuid dynamics of the $\\beta$ -EB-TCBpolicy and is analogous to Proposition 2.1 used for constructing the fuid dynamics of the anchored top-two algorithm. ", "page_idx": 29}, {"type": "text", "text": "Proposition E.2. For every non-empty $B\\subseteq[K]/\\{1\\}$ ,tuple $N_{\\overline{{B}}^{c}}=(N_{a}\\in\\mathbb{R}_{\\geq0}:a\\in\\overline{{B}}^{c})$ , and $\\begin{array}{r}{N\\,\\ge\\,\\frac{1}{1-\\beta}\\sum_{a\\in\\overline{{B}}^{c}}N_{a}}\\end{array}$ there exists auniqu tuple $N_{\\overline{{B}}}=(N_{a}\\,\\in\\,\\mathbb{R}_{\\geq0}\\,:\\,a\\,\\in\\,\\overline{{B}})$ and $I_{B}\\ge0$ for which, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{N_{1}\\ =\\ \\beta N,\\quad\\displaystyle\\sum_{a\\in[K]}N_{a}\\ =\\ N,}\\\\ {f o r\\,e\\nu e r y\\,a\\in B,\\quad N_{1}d(\\mu_{a},x_{1,a})+N_{a}d(\\mu_{a},x_{1,a})\\:=\\:I_{B},\\quad a n d}\\\\ {w h e r e\\quad x_{1,a}\\:=\\:\\frac{N_{1}\\mu_{1}+N_{a}\\mu_{a}}{N_{1}+N_{a}}\\quad f o r\\,e\\nu e r y\\:a\\in[K]/\\{1\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "If we denote that tuple by $N_{\\overline{{B}}}(N)$ and $I_{B}(N)$ \uff0c then the functions $N\\;\\to\\;N_{\\overline{{{B}}}}(N),I_{B}(N)$ are continuously differentiable with respect to $N$ ", "page_idx": 30}, {"type": "text", "text": "Proof. Proof of Proposition E.2 follows by an argument similar to the one used in the proof of Proposition 2.1, by using Lemma E.4 instead of Theorem D.1. Observe that the $\\beta$ -anchor function $g(\\bar{\\cal N};\\beta)$ is strictly decreasing in $N_{1}$ and strictly increasing in $N_{a}$ \uff0cwhen $N_{1}~>~0$ .As a result, statement 1 of Lemma C.1 in Appendix C holds true upon having, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\phi_{1}(N,\\eta)\\;=\\;g(N;\\beta)-\\eta_{0},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and defining the set $\\mathcal{Z}_{B}$ using the modified function $\\varPhi_{B}$ ", "page_idx": 30}, {"type": "text", "text": "With te above modicaton f we can find onstant > such that IFP+ ( Pe) $\\begin{array}{r l r}&{}&{{\\bf1}_{|B|+1}^{T}\\left(\\frac{\\partial\\widetilde{\\pmb{\\phi}}_{B}}{\\partial N_{B}}\\right)^{-1}\\frac{\\partial\\widetilde{\\pmb{\\phi}}_{B}}{\\partial I}\\leq}\\end{array}$ $-\\gamma<0$ for every tuple in $\\mathcal{Z}_{B}$ , then statements 2 and 3 of Lemma C.1 also hold true for this modified $\\varPhi_{B}$ . As a result, Proposition E.2 follows using Lemma E.4 by the same argument used for proving Proposition 2.1 using Theorem D.1. ", "page_idx": 30}, {"type": "text", "text": "In the rest of the proof we argue the existence of such a constant $\\gamma>0$ ", "page_idx": 30}, {"type": "text", "text": "Let $\\pmb{v}=(v_{a}:a\\in\\overline{{B}})\\in\\mathbb{R}^{|B|+1}$ be the solution to the system ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\partial\\tilde{\\pmb{\\Phi}}_{B}}{\\partial N_{B}}\\pmb{v}\\ =\\ \\frac{\\partial\\tilde{\\pmb{\\Phi}}_{B}}{\\partial I}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Wehave $\\begin{array}{r}{\\frac{\\partial\\Phi_{1}}{\\partial N_{B}}\\pmb{v}=\\frac{\\partial\\Phi_{1}}{\\partial I}}\\end{array}$ , and $\\begin{array}{r}{N_{1}=\\beta\\sum_{a\\in[K]}N_{a}}\\end{array}$ which after some algebraic manipulation implies, ", "page_idx": 30}, {"type": "equation", "text": "$$\n-\\left(\\frac{1}{\\beta}-1\\right)v_{1}+\\sum_{a\\in B}v_{a}\\;=\\;0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "(33) further implies, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbf{{x}}_{\\left|B\\right|+1}^{T}v\\ =\\ \\sum_{i=1}^{\\left|B\\right|+1}v_{i}\\ =\\ \\frac{v_{1}}{\\beta}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore proving that $v_{1}$ is upper bounded by a negative constant is sufficient for proving the desired result. ", "page_idx": 30}, {"type": "text", "text": "For every $a\\in B$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\partial\\varPhi_{a}}{\\partial N_{\\overline{{B}}}}\\pmb v=\\frac{\\partial\\varPhi_{a}}{\\partial I}=-1,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which implies ", "page_idx": 30}, {"type": "equation", "text": "$$\nv_{1}d(\\mu_{1},x_{1,a})+v_{a}d(\\mu_{a},x_{1,a})~=~-\\,1,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where Na euse d,a ad daa epetively, tdete d(\u03bc,,a) ad d(\u03bca,,a). ", "page_idx": 30}, {"type": "text", "text": "For every $a\\in B$ , we can eliminate $v_{a}$ for (33) using (34). After this procedure, we get, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad}&{-v_{1}\\,=\\,\\frac{\\displaystyle\\sum_{a\\in B}\\frac{1}{d_{a,a}}}{\\displaystyle\\frac{1}{\\beta}-1+\\sum_{a\\in B}\\frac{d_{1,a}}{d_{a,a}}}}\\\\ &{\\phantom{\\frac{1}{\\displaystyle\\sum_{a\\in B}\\frac{1}{d_{a,a}}}}\\ge\\,\\frac{\\displaystyle\\sum_{a\\in B}\\frac{1}{d_{a,a}}}{\\displaystyle\\frac{1}{\\beta}-1+\\sum_{a\\ne1}\\frac{d_{1,a}}{d_{a,a}}}}\\\\ &{\\phantom{\\frac{1}{\\displaystyle\\sum_{a\\in B}\\frac{1}{d_{a,a}}}}\\ge\\frac{\\displaystyle\\sum_{a\\in B}\\frac{1}{d_{a,a}}}{\\displaystyle\\frac{1}{\\beta}-1+\\sum_{a\\ne1}\\frac{d(\\mu_{1},\\mu_{a})}{d_{a,a}}}.\\quad(\\mathrm{since}\\;d_{1,a}\\le d(\\mu_{1},\\mu_{a}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $\\begin{array}{r}{N_{1}=\\beta\\sum_{a\\in[K]}N_{a}}\\end{array}$ , using (6) we have $d_{a,a}=\\Theta(1)$ for all $a\\in[K]/\\{1\\}$ , where the constant hidden in $\\Theta(\\cdot)$ is independent of $_{N}$ .As a result, by (35), $-v_{1}=\\varOmega(1)$ . Hence we conclude the proof. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Constructing the fuid ODEs: Without loss of generality, we assume that the fuid dynamics starts from a state $_{N}$ Where $g(N;\\beta)=0$ .Otherwise, ", "page_idx": 31}, {"type": "text", "text": "1. If $g(N;\\beta)>0$ , the algorithm gives samples to arm 1 till $g(N;\\beta)=0$ 2. If $g(N;\\beta)<0$ the $\\beta$ -EB-TCB algorithm follows the dynamics in Proposition E.1, and reaches $g(N;\\beta)=0$ in a finite amount of time. ", "page_idx": 31}, {"type": "text", "text": "Following Proposition E.2, the algorithm tracks the allocation $N_{\\overline{{B}}}(N)$ at agiven time $N$ ,where $B$ denotes the set of minimum index arms. We now determine the ODEs by which the state of the algorithmevolves. ", "page_idx": 31}, {"type": "text", "text": "To simplify the notations, we use $g_{\\beta}$ as a shorthand for $g(N(N);\\beta)$ at a given time $N$ . For every $a\\in[\\bar{K}]$ , we use $N_{a},N_{a}^{\\prime},I_{B}$ , and $I_{B}^{\\prime}$ , respectively, to denote $N_{a}(N),N_{a}^{\\prime}(N),I_{B}(N)$ and $I_{B}^{\\prime}(N)$ For every $a\\in\\overline{{B}}^{c}$ , we use $I_{a}$ to denote $I_{a}(N)$ . Also for every $a\\in[K]/\\{1\\}$ , we adopt the notation $d_{1,a}$ and $d_{a,a}$ , respectively,for the quantities $d(\\mu_{1},x_{1,a})$ and $d(\\mu_{a},x_{1,a})$ , where $\\begin{array}{r}{x_{1,a}=\\frac{N_{1}\\mu_{1}+N_{a}\\mu_{a}}{N_{1}+N_{a}}}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "For every non-empty $B\\subseteq[K]/\\{1\\}$ we define the quantity, ", "page_idx": 31}, {"type": "equation", "text": "$$\nd_{B}=\\left(\\sum_{a\\in B}\\frac{1}{d_{a,a}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We now show the fuid ODEs in the following proposition. ", "page_idx": 31}, {"type": "text", "text": "Proposition E.3 (Fluid ODEs for $\\beta$ -EB-TCB). Let us assume the algorithm starts from a state $N(\\bar{N}^{0})=(N_{a}^{0}:a\\in[K])$ with $\\begin{array}{r}{\\sum_{a\\in[K]}N_{a}^{0}=N^{0}}\\end{array}$ \uff0c $N_{1}^{0}=\\beta N^{0}$ and $N^{0^{-}}\\mathrm{>0.}$ Let $B\\subseteq[K]/\\{1\\}$ be the set of arms having minimum index at a given time $N\\geq N^{0}$ .Thefollowingstatementshold true abouttheallocation $\\bar{N}_{\\overline{{B}}}(N)=(N_{a}(N):\\bar{a}\\in\\overline{{B}})$ made by $\\beta$ -EB-TCBalgorithm, ", "page_idx": 31}, {"type": "text", "text": "1. The allocation $N(N)=(N_{a}(N):a\\in[K])$ evolves by the following system of ODEs, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{1}^{\\prime}\\,=\\,\\beta,\\quad a n d}\\\\ {\\mathit{f o r\\,e v e r y}}&{b\\in B,\\quad N_{b}^{\\prime}\\,=\\,\\frac{\\left(\\left(1-\\beta\\right)N-\\sum_{a\\in B}N_{a}\\right)d_{B}+N_{b}d_{b,b}}{N d_{b,b}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "2. The index $I_{B}(N)$ of the arms in $B$ evolves by the following ODE, ", "page_idx": 31}, {"type": "equation", "text": "$$\nI_{B}^{\\prime}\\ =\\ \\left(1-\\beta-\\frac{\\sum_{a\\in B}N_{a}}{N}\\right)d_{B}+\\frac{I_{B}}{N}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "3.Thereexists aconstant $c>0$ such that, $I_{B}^{\\prime}\\ge c$ On the other hand, indexes of the arms in ${\\overline{{B}}}^{c}$ remainsupperboundedby $N_{a}^{0}d(\\mu_{a},\\mu_{1})$ ", "page_idx": 31}, {"type": "text", "text": "Proof. Statement 1: $N_{1}^{\\prime}=\\beta$ follows directly from the fact that $g_{\\beta}=0$ ", "page_idx": 32}, {"type": "text", "text": "By definition of $B$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\nN_{1}d_{1,a}+N_{a}d_{a,a}\\;=\\;N_{1}d_{1,b}+N_{b}d_{b,b}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for every $a,b\\in[K]$ . Taking derivative on both sides, we get, ", "page_idx": 32}, {"type": "equation", "text": "$$\nN_{1}^{\\prime}d_{1,a}+N_{a}^{\\prime}d_{a,a}\\;=\\;N_{1}^{\\prime}d_{1,b}+N_{b}^{\\prime}d_{b,b},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which implies, ", "page_idx": 32}, {"type": "equation", "text": "$$\nN_{a}^{\\prime}\\;=\\;\\frac{d_{1,b}-d_{1,a}}{d_{a,a}}N_{1}^{\\prime}+\\frac{d_{b,b}}{d_{a,a}}N_{b}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using (38), we have ", "page_idx": 32}, {"type": "equation", "text": "$$\nd_{1,b}-d_{1,a}\\ =\\ \\frac{N_{a}d_{a,a}-N_{b}d_{b,b}}{N_{1}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using the above expression in (39), we get, ", "page_idx": 32}, {"type": "equation", "text": "$$\nN_{a}^{\\prime}\\;=\\;{\\frac{N_{a}d_{a,a}-N_{b}d_{b,b}}{N_{1}d_{a,a}}}N_{1}^{\\prime}+{\\frac{d_{b,b}}{d_{a,a}}}N_{b}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $N_{1}^{\\prime}=\\beta$ and $N_{1}=\\beta N$ (which follows from $g_{\\beta}=0$ ), the above equation implies, ", "page_idx": 32}, {"type": "equation", "text": "$$\nN_{a}^{\\prime}\\;=\\;\\frac{N_{a}d_{a,a}-N_{b}d_{b,b}}{N d_{a,a}}+\\frac{d_{b,b}}{d_{a,a}}N_{b}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Adding both sides for $a\\in B$ , we get, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{1-\\beta\\;=\\;\\displaystyle\\sum_{a\\in B}N_{a}^{\\prime}\\;=\\;\\displaystyle\\sum_{a\\in B}\\frac{N_{a}d_{a,a}-N_{b}d_{b,b}}{N d_{a,a}}+d_{b,b}N_{b}^{\\prime}\\sum_{a\\in B}d_{a,a}^{-1}}&\\\\ {\\;=\\;\\displaystyle\\frac{\\sum_{a\\in B}N_{a}}{N}-\\frac{N_{b}}{N}d_{b,b}d_{B}^{-1}+N_{b}^{\\prime}d_{b,b}d_{B}^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which implies ", "page_idx": 32}, {"type": "equation", "text": "$$\nN_{b}^{\\prime}\\,=\\,\\frac{((1-\\beta)N-\\sum_{a\\in B}N_{a})d_{B}+N_{b}d_{b,b}}{N d_{b,b}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Statement 2: We know ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\nI_{B}^{\\prime}\\;=\\;N_{1}^{\\prime}d_{1,b}+N_{b}^{\\prime}d_{b,b}\\quad\\mathrm{for}\\;\\mathrm{every}\\;b\\in B.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Putting in the derivatives from (36), we obtain, ", "page_idx": 32}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{I_{B}^{\\prime}\\ =\\ \\beta d_{1,b}+\\left(1-\\beta-{\\frac{\\sum_{a\\in B}N_{a}}{N}}\\right)d_{B}+{\\frac{N_{b}d_{b,b}}{N}}}\\\\ {=\\ \\left(1-\\beta-{\\frac{\\sum_{a\\in B}N_{a}}{N}}\\right)d_{B}+{\\frac{\\beta N d_{1,b}+N_{b}d_{b,b}}{N}}}\\\\ {=\\ \\left(1-\\beta-{\\frac{\\sum_{a\\in B}N_{a}}{N}}\\right)d_{B}+{\\frac{I_{B}}{N}}.\\quad{\\mathrm{(since~}}N_{1}=\\beta N{\\mathrm{~and~}}I_{b}=I_{B}{\\mathrm{~for~every~}}b\\in B{\\mathrm{)}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Statement 3: For the following argument, the constants hidden in $O(\\cdot),\\varOmega(\\cdot)$ and $\\varOmega(\\cdot)$ are independentof $N$ ", "page_idx": 32}, {"type": "text", "text": "Note that $N_{1}=\\beta N$ . As a result, using (6), $d_{a,a}=\\Theta(1)$ for every $a\\in[K]/\\{1\\}$ . This implies, we can find a constant $c_{1}>0$ such that $d_{B}\\geq c_{1}$ . On the other hand, using (12), we have, ", "page_idx": 32}, {"type": "equation", "text": "$$\nI_{B}\\;=\\;\\theta\\left({\\frac{N_{1}N_{a}}{N_{1}+N_{a}}}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for every $a\\in B$ . Since $N_{1}=\\beta N$ and $N_{a}\\leq N$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\nI_{B}\\;=\\;\\Theta(N_{a}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Adding for all $a\\in B$ ,we get $\\begin{array}{r}{I_{B}=\\theta(\\sum_{a\\in B}N_{a})}\\end{array}$ . Therefore, $\\begin{array}{r}{I_{B}\\geq c_{2}\\sum_{a\\in B}N_{a}}\\end{array}$ , for some constant $c_{2}>0$ ", "page_idx": 33}, {"type": "text", "text": "Now using (37), ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{I_{B}^{\\prime}\\ =\\ \\left(1-\\beta-\\frac{\\displaystyle\\sum_{a\\in B}N_{a}}{\\displaystyle N}\\right)d_{B}+\\frac{\\displaystyle I_{B}}{\\displaystyle N}}\\\\ &{\\phantom{\\displaystyle\\geq}c_{1}\\times\\left(1-\\beta-\\frac{\\displaystyle\\sum_{a\\in B}N_{a}}{\\displaystyle N}\\right)+c_{2}\\times\\frac{\\displaystyle\\sum_{a\\in B}N_{a}}{\\displaystyle N}}\\\\ &{\\phantom{\\displaystyle\\geq}\\operatorname*{min}\\{c_{1},c_{2}\\}\\times(1-\\beta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Taking $c=\\operatorname*{min}\\{c_{1},c_{2}\\}>0$ , we have the desired result. ", "page_idx": 33}, {"type": "text", "text": "Now for arms $a\\in\\overline{{B}}^{c}$ noe that $x_{1,a}\\,=\\,\\arg\\operatorname*{min}_{x\\in[\\mu_{a},\\mu_{1}]}$ $\\left(N_{1}d(\\mu_{1},x)+N_{a}^{0}d(\\mu_{a},x)\\right)$ and $I_{a}=$ $N_{1}d(\\mu_{1},x_{1,a})+N_{a}^{0}d(\\mu_{a},x_{1,a})$ ", "page_idx": 33}, {"type": "text", "text": "As a result, putting $x=\\mu_{1}$ , we have $I_{a}\\leq N_{a}^{0}d(\\mu_{a},\\mu_{1})$ ", "page_idx": 33}, {"type": "text", "text": "Reaching $\\beta$ -optimal proportions: By statement 3 of Proposition E.3, the indexes of the arms in $B$ increase at a linear rate, whereas the indexes of the arms in ${\\overline{{B}}}^{c}$ stay bounded above by a constant. As a result, by some finite time, $I_{B}$ crosses the index of some arm in $a\\in\\overline{{B}}^{c}$ , after which $B$ gets updated to $B\\cup\\{a\\}$ . The same process then continues with the updated $B$ . In this way, eventually $\\bar{B}=[K]/\\{1\\}$ and the fluid dynamics reaches the $\\beta$ optimal proportion $\\omega^{\\star}(\\beta)=(\\omega_{a}^{\\star}(\\bar{\\beta}):a\\in[K])$ $\\beta$ -optimal proportion is the solution to the max-min problem (1) with the added constraint $\\omega_{1}=\\beta$ where, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{N_{1}}{N}\\;=\\;\\omega_{1}^{\\star}(\\beta)\\;=\\;\\beta\\quad\\mathrm{and}\\quad\\frac{N_{a}}{N}\\;=\\;\\omega_{a}^{\\star}(\\beta)\\quad\\mathrm{for~every~}a\\in[K]/\\{1\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Applying the same argument as used to bound the time to reach optimal proportions in Section 4, if the fluid dynamics start at some time $N^{0}$ withstate $N(N^{0})=(\\dot{N_{a}}(N^{0})^{\\ast};\\dot{a^{}}\\in[K])$ ,then it reaches stabilityby atime atmost $\\frac{N^{0}}{\\operatorname*{min}_{a\\in\\left[K\\right]}\\omega_{a}^{\\star}(\\beta)}$ ", "page_idx": 33}, {"type": "text", "text": "F  Intuitions based on fluid dynamics applied to algorithmic behavior ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "F.1 Indexes once they meet do not separate ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In the fuid dynamics described in Theorem 4.1, once the indexes meet thereafter they move up together by construction. It turns out that $I_{B}^{\\prime}(N)$ is positive. Below we give a heuristic argument that in our fuid dynamics, once a set of smallest indexes that are equal, increase and catch up with another index, their union then remains equal and increases together with $N$ . This argument provides important insights which help us later to prove that after after a random time of finite expectation, if our algorithm picks a suboptimal arm, then it picks that arm again in a periodic manner, which helps us prove closeness of indexes w.r.t. the algorithmic allocations (see Lemma 5.1). Differentiating $g=0$ with respect to $N$ ,weseethat, ", "page_idx": 33}, {"type": "equation", "text": "$$\nN_{1}^{\\prime}\\sum_{a\\neq1}f(\\pmb{\\mu},a,\\pmb{N})\\frac{N_{a}\\Delta_{a}}{(N_{1}+N_{a})^{2}}=\\sum_{a\\neq1}N_{a}^{\\prime}f(\\pmb{\\mu},a,\\pmb{N})\\frac{N_{1}\\Delta_{a}}{(N_{1}+N_{a})^{2}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Inductively, suppose that a set $B$ of indexes are moving up together and they run into another index $b$ at time $N$ . Upon assuming contradiction, we can have a neighbourhood $\\mathbf{\\dot{[}N,}N+\\Delta N\\mathbf{]}$ where the algorithm only allocates to a subset $C\\subset B\\cup\\{b\\}$ and doesn't allocate to arms in $D=B\\cup\\{b\\}{-}C\\neq\\emptyset$ Then the allocations follow the ODEs in (3) of Theorem 4.1 with $B=C$ , in the interval $[N,N\\!+\\!\\varDelta N]$ ", "page_idx": 33}, {"type": "text", "text": "Consider the probability vector $(p_{a}:a\\in[K]/\\{1\\})$ where, ", "page_idx": 33}, {"type": "equation", "text": "$$\np_{a}=\\frac{f(\\pmb{\\mu},a,\\pmb{N})\\frac{N_{a}\\Delta_{a}}{(N_{1}+N_{a})^{2}}}{\\sum_{b\\neq1}f(\\pmb{\\mu},b,\\pmb{N})\\frac{N_{b}\\Delta_{b}}{(N_{1}+N_{b})^{2}}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that $p_{a}>0$ for every $a\\in[K]/\\{1\\}$ . We have from (40) that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{N_{1}^{\\prime}}{N_{1}}~=~\\sum_{a\\in C}\\frac{N_{a}^{\\prime}}{N_{a}}p_{a}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Letting $b=\\arg\\operatorname*{max}_{a\\in C}\\ \\frac{N_{a}^{\\prime}(N)}{N_{a}(N)}$ (where $N_{a}^{\\prime}(N)$ is the derivative in (3) of Theorem 4.1, upon putting $B=C_{\\star}$ ,wehave ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{N_{1}^{\\prime}}{N_{1}}\\le\\left(\\sum_{a\\in C}p_{a}\\right)\\frac{N_{b}^{\\prime}\\,\\left(1\\right)\\,N_{b}^{\\prime}}{N_{b}}<\\frac{N_{b}^{\\prime}}{N_{b}},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the strict inequality in (1) follows from the fact that $D\\,=\\,B\\cup\\{b\\}-C\\,\\neq\\,\\emptyset$ causing $\\begin{array}{r}{\\sum_{a\\in C}p_{a}<1}\\end{array}$ ", "page_idx": 34}, {"type": "text", "text": "We now argue that $D$ must be empty. Suppose instead that $D\\neq\\emptyset$ and $a\\in D$ .Because all indexes in $B$ are equal at time $N$ , we have, $^{\\dot{N_{1}}d(\\dot{\\mu_{1}},\\dot{x}_{1,a})}+N_{a}d(\\mu_{a},\\dot{x_{1,a}})=N_{1}d(\\mu_{1},x_{1,b})+N_{b}d(\\mu_{b},x_{1,b})$ at $\\Nu$ . Observe that for any arm $d\\;\\in\\;[K]/\\{1\\}$ , derivative of its index with respect to $N$ equals $N_{1}^{\\prime}d(\\mu_{1},x_{1,d})+N_{d}^{\\prime}d(\\mu_{d},\\dot{x}_{1,d})$ (since,by definition of $x_{1,d}$ $N_{1}d_{2}(\\mu_{1},x_{1,d})+N_{d}\\bar{d}_{2}(\\mu_{d},x_{1,d})\\stackrel{\\_}{=}0)$ Since arm $a$ gets no sample in $[N,N+\\varDelta N]$ , we have $N_{a}^{\\prime}=0$ , which implies ", "page_idx": 34}, {"type": "equation", "text": "$$\nI_{a}^{\\prime}=N_{1}^{\\prime}d(\\mu_{1},x_{1,a})\\;\\mathrm{in}\\;[N,N+\\varDelta N].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By our previous discussion ", "page_idx": 34}, {"type": "equation", "text": "$$\nI_{b}^{\\prime}\\;=\\;N_{1}^{\\prime}d(\\mu_{1},x_{1,b})+N_{b}^{\\prime}d(\\mu_{b},x_{1,b}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We now argue that $N_{1}^{\\prime}d(\\mu_{1},x_{1,a})$ is strictly less than (43) at $N$ . As a result, if $\\varDelta N>0$ is picked sufficiently small, index of $b$ , which is the minimum index, outruns index of $a$ in $[N,N+\\varDelta N]$ ", "page_idx": 34}, {"type": "text", "text": "Consider the difference ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ensuremath{\\mathrm{V}}_{1}^{\\prime}d(\\mu_{1},x_{1,a})-\\ensuremath{N}_{1}^{\\prime}d(\\mu_{1},x_{1,b})-\\ensuremath{N}_{b}^{\\prime}d(\\mu_{b},x_{1,b})\\;=\\;\\ensuremath{N}_{1}^{\\prime}((\\mu_{1},x_{1,a})-d(\\mu_{1},x_{1,b}))-\\ensuremath{N}_{b}^{\\prime}d(\\mu_{b},x_{1,b}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We want show that this is strictly negative. We consider two cases, ", "page_idx": 34}, {"type": "text", "text": "Case I: If $d(\\mu_{1},x_{1,a})-d(\\mu_{1},x_{1,b})\\leq0$ , it follows trivially. ", "page_idx": 34}, {"type": "text", "text": "Case II: If $d(\\mu_{1},x_{1,a})-d(\\mu_{1},x_{1,b})>0$ since $N_{b}^{\\prime}>0$ , using (42) we can upper bound (44) by ", "page_idx": 34}, {"type": "equation", "text": "$$\nN_{b}^{\\prime}\\cdot\\left(\\frac{N_{1}}{N_{b}}\\left(d(\\mu_{1},x_{1,a})-d(\\mu_{1},x_{1,b})\\right)-d(\\mu_{b},x_{1,b})\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since the two indexes are equal at this point, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\nN_{1}(d(\\mu_{1},x_{1,a})-d(\\mu_{1},x_{1,b}))=N_{b}d(\\mu_{b},x_{1,b})-N_{a}d(\\mu_{a},x_{1,a}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Substituting this in (45), the latter equals ", "page_idx": 34}, {"type": "equation", "text": "$$\nN_{b}^{\\prime}\\cdot\\Big(\\frac{1}{N_{b}}(N_{b}d(\\mu_{b},x_{1,b})-N_{a}d(\\mu_{a},x_{1,a}))-d(\\mu_{b},x_{1,b})\\Big)\\leq-N_{b}^{\\prime}\\cdot\\frac{N_{a}}{N_{b}}d(\\mu_{a},x_{1,a})<0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We thus have our contradiction. Therefore, indexes of the arms in $B\\cup\\{b\\}$ move together. ", "page_idx": 34}, {"type": "text", "text": "F.2Proof sketch of Lemma 5.1 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We consider the situation where the algorithm picks some arm $a\\,\\in\\,[K]/\\{1\\}$ at iteration $N$ and doesn't pick $a$ for the next $R(N)\\geq1$ iterations. For better readability, we denote $R(N)$ using $R$ . In the following argument, we try to bound $R$ from above using $N$ . We can prove that $\\widetilde{N}_{j}(N)=\\Theta(N)$ for every $j\\in[K]$ and $N\\geq T_{g o o d}$ (see Remark G.1 in Appendix G.2). As a result, $R$ is atmost $O(N)$ for $N\\geq T_{g o o d}$ . Below, we improve the upper bound to $O(N^{1-3\\alpha/8})$ by a refined analysis. ", "page_idx": 34}, {"type": "text", "text": "Let us define $\\Delta\\widetilde{N}_{j}(N,t)=\\widetilde{N}_{j}(N+t)-\\widetilde{N}_{j}(N)$ for every $j\\in[K]$ and $N,t\\ge1$ .By our choice of $T_{g o o d}$ , we have $|g(\\pmb{\\mu},\\widetilde{\\cal N}(N))|=O(N^{-3\\alpha/8})$ for $N\\geq T_{g o o d}$ (see Remark G.1 in Appendix G.2). By applying mean value theorem on $g(\\cdot)$ for $N\\geq T_{g o o d}$ , we have, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left|\\frac{\\Delta\\widetilde{N}_{1}(N,t)}{\\widetilde{N}_{1}(N)}-\\sum_{j\\neq1,a}\\hat{p}_{j}(N,t)\\frac{\\Delta\\widetilde{N}_{j}(N,t)}{\\widetilde{N}_{j}(N)}\\right|~=~{\\cal O}(N^{-3\\alpha/8}),\\quad\\mathrm{for~every~}t\\le R,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $(\\hat{p}_{j}(N,t):j\\in[K]/\\{1\\})$ is a probability distribution over the set $[K]/\\{1\\}$ , depending on $N$ and $t$ (this distribution is not important to the discussion and is spelt out at Appendix G.6). Taking $b_{t}=\\arg\\operatorname*{max}_{a\\neq1}\\ \\frac{\\Delta\\widetilde{N}_{a}(N,t)}{\\widetilde{N}_{a}(N)}$ , and using (46), we obtain, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\varDelta\\widetilde{N}_{1}(N,t)}{\\varDelta\\widetilde{N}_{b_{t}}(N,t)}\\;\\leq\\;\\frac{\\widetilde{N}_{1}(N)}{\\widetilde{N}_{b_{t}}(N)}+O(N^{1-3\\alpha/8})\\quad\\mathrm{for~every~}t\\leq R.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Observe that (46) and (47), respectively, resembles (41) and (42) from Appendix F.1, except for a $O(N^{1-3\\alpha/8})$ term due to the noise in $\\widetilde{\\pmb{\\mu}}$ ", "page_idx": 35}, {"type": "text", "text": "Applying the mean value theorem, we can bound the difference between the empirical indexes of arm $a$ and $b_{t}$ at iteration $N+t$ by, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{Z}_{a}(N+t)-\\mathbb{Z}_{b_{t}}(N+t)\\leq\\varDelta\\widetilde{N}_{1}(N,t)\\cdot(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b_{t}}))}\\\\ &{\\phantom{\\quad\\quad}-\\varDelta\\widetilde{N}_{b_{t}}(N,t)\\cdot d(\\widetilde{\\mu}_{b_{t}},\\widetilde{x}_{1,b_{t}})+O(N^{1-\\frac{3\\alpha}{8}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now if $d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b_{t}})<0$ (48) implies, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{Z}_{a}(N+t)-\\mathbb{Z}_{b_{t}}(N+t)~\\le~-~\\Delta\\widetilde{N}_{b_{t}}(N,t)\\cdot d(\\widetilde{\\mu}_{b_{t}},\\widetilde{x}_{1,b_{t}})+O(N^{1-3\\alpha/8}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Otherwise, if $d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b_{t}})\\geq0$ , using (47) we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{Z}_{a}(N+t)-\\mathbb{Z}_{b_{t}}(N+t)}\\\\ &{\\ \\ \\leq\\ \\Delta\\tilde{N}_{b_{t}}(N,t)\\cdot\\left(\\frac{\\tilde{N}_{1}(N)}{\\tilde{N}_{b_{t}}(N)}\\left(d(\\tilde{\\mu}_{1},\\tilde{x}_{1,a})-d(\\tilde{\\mu}_{1},\\tilde{x}_{1,b_{t}})\\right)-d(\\tilde{\\mu}_{b_{t}},\\tilde{x}_{1,b_{t}})\\right)+O(N^{1-3\\alpha/8}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that (48) resembles (45) in Appendix F.1. Since $\\mathcal{Z}_{a}(N-1)\\,\\le\\,\\mathcal{Z}_{b_{t}}(N-1)$ ,we can prove using the mean value theorem that $\\mathcal{Z}_{a}(N)\\le\\mathcal{Z}_{b_{t}}(N)+O(N^{1-3\\alpha/8})$ . Now expanding the empirical indexes, we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{N}_{1}(N)\\cdot d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})+\\widetilde{N}_{a}(N)\\cdot d(\\widetilde{\\mu}_{a},\\widetilde{x}_{1,a})\\ \\leq\\ \\widetilde{N}_{1}(N)\\cdot d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})+\\widetilde{N}_{b_{t}}(N)\\cdot d(\\widetilde{\\mu}_{b_{t}},\\widetilde{x}_{1,b_{t}})}\\\\ {+\\;O(N^{1-3\\alpha/8}).\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now dividing both sides by $\\widetilde{N}_{b_{t}}(N)$ and using the fact that $\\widetilde{N}_{b_{t}}(N)=\\Theta(N)$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\widetilde{N}_{1}(N)}{\\widetilde{N}_{b_{t}}(N)}\\left(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b_{t}})\\right)-d(\\widetilde{\\mu}_{b_{t}},\\widetilde{x}_{1,b_{t}})\\le-\\frac{\\widetilde{N}_{a}(N)}{\\widetilde{N}_{b_{t}}(N)}d(\\widetilde{\\mu}_{a},\\widetilde{x}_{1,a})+O(N^{-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using the above inequality in (49), we have, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{Z}_{a}(N+t)-\\mathbb{Z}_{b_{t}}(N+t)\\le-\\Delta\\widetilde{N}_{b_{t}}(N,t)\\cdot\\frac{\\widetilde{N}_{a}(N)}{\\widetilde{N}_{b_{t}}(N)}d(\\widetilde{\\mu}_{a},\\widetilde{x}_{1,a})+O(N^{1-\\frac{3\\alpha}{8}})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+O(\\Delta\\widetilde{N}_{b_{t}}(N,t)\\cdot N^{-\\frac{3\\alpha}{8}}),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\le-\\Delta\\widetilde{N}_{b_{t}}(N,t)\\cdot\\frac{\\widetilde{N}_{a}(N)}{\\widetilde{N}_{b_{t}}(N)}d(\\widetilde{\\mu}_{a},\\widetilde{x}_{1,a})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+O(N^{1-\\frac{3\\alpha}{8}})\\quad(\\mathrm{since}\\,\\,\\Delta\\widetilde{N}_{b_{t}}(N,t)\\le R=O(N)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "whenever $d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b_{t}})\\geq0$ ", "page_idx": 35}, {"type": "text", "text": "Since $\\widetilde{N}_{j}(N)\\;=\\;\\Theta(N)$ and $\\widetilde{\\mu}_{j}(N)\\;\\approx\\;\\mu_{j}$ for all $j~\\in~[K]$ and $N\\ \\geq\\ T_{g o o d}$ , the coefficient of $\\varDelta\\widetilde{N}_{b_{t}}(N,t)$ in (49) and (50) are $-\\Theta(1)$ . As a result, we can find a constant $C_{3}>0$ , such that, for $t\\leq R$ and $N\\geq T_{g o o d}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{Z}_{a}(N+t)-\\mathbb{Z}_{b_{t}}(N+t)\\;\\le\\;-\\;C_{3}\\Delta\\widetilde{N}_{b}(N,t)+O(N^{1-3\\alpha/8}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Applying the mean value theorem and using the fact that $t$ can be atmost $O(N)$ , we can prove that, (51) implies, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{Z}_{a}(N+t-1)-\\mathbb{Z}_{b_{t}}(N+t-1)\\ \\leq\\ -\\,C_{3}\\Delta\\widetilde{N}_{b}(N,t)+O(N^{1-3\\alpha/8}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "As a result, we have a constant $C_{4}>0$ such that, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\displaystyle Z_{a}(N+t-1)-Z_{b_{t}}(N+t-1)~\\le~-~C_{3}\\ensuremath{\\Delta\\tilde{N}_{b}}(N,t)+C_{4}N^{1-3\\alpha/8}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Using (47), we can choose $T_{g o o d}$ suitably, and find constants $D_{1}$ \uff0c $D_{2}\\,>\\,0$ , such that, whenever $N\\geq T_{g o o d}$ \uff0c ", "page_idx": 36}, {"type": "equation", "text": "$$\nR\\ge D_{1}N^{1-3\\alpha/8}\\quad\\implies\\quad\\varDelta\\widetilde{N}_{b_{R}}(N,R)\\ge D_{2}R\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We consider the case where R\u2265 max{Di, 2} $\\begin{array}{r}{R\\geq\\operatorname*{max}\\left\\{D_{1},\\frac{2C_{4}}{C_{3}D_{2}}\\right\\}\\times N^{1-3\\alpha/8}.}\\end{array}$ ", "page_idx": 36}, {"type": "text", "text": "Since $R\\ge D_{1}N^{1-3\\alpha/8}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Delta\\widetilde{N}_{b_{R}}(N,R)\\ge D_{2}R\\ge D_{2}\\times\\frac{2C_{4}}{C_{3}D_{2}}N^{1-3\\alpha/8}=\\frac{2C_{4}}{C_{3}}N^{1-3\\alpha/8}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For notational simplicity, we use $b$ to denote $b_{R}$ . We consider the iteration $N+S$ where arm $b$ was selected for the last time before iteration $N+R$ . Then by definition of $b_{R}$ and $S$ , and using the above inequality, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Delta\\widetilde{N}_{b}(N,S)\\;=\\;\\Delta\\widetilde{N}_{b}(N,R)\\;\\geq\\;\\frac{2C_{4}}{C_{3}}N^{1-3\\alpha/8}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Also, since $\\Delta\\widetilde{N}_{b}(N,S)\\;=\\;\\varDelta\\widetilde{N}_{b}(N,R)$ and for every $j\\ \\neq\\ 1\\ \\Delta\\tilde{N}_{j}(N,R)\\ \\ge\\ \\Delta\\tilde{N}_{j}(N,S)$ we conclude $b=b_{S}$ . Therefore, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{Z}_{a}(N+S-1)-\\mathbb{Z}_{b}(N+S-1)\\;=\\;\\mathbb{Z}_{a}(N+S-1)-\\mathbb{Z}_{b_{S}}(N+S-1)}&{}\\\\ {\\mathrm{(using~(52))}}&{\\;\\le\\;-\\,C_{3}A\\tilde{N}_{b_{S}}(N,S)+C_{4}N^{1-3\\alpha/8}}\\\\ {\\mathrm{(since~}b=b_{S})}&{\\;=\\;-\\,C_{3}A\\tilde{N}_{b}(N,S)+C_{4}N^{1-3\\alpha/8}}\\\\ {\\mathrm{(using~(53))}}&{\\;\\le\\;-\\,C_{3}\\times\\displaystyle\\frac{2C_{4}}{C_{3}}N^{1-3\\alpha/8}+C_{4}N^{1-3\\alpha/8}}\\\\ &{\\;=\\;-C_{4}N^{1-3\\alpha/8}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The above inequality implies, the AT2 algorithm pulls arm $b$ at iteration $N+S$ even though ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{Z}_{a}(N+S-1)\\leq\\mathbb{Z}_{b}(N+S-1)-C_{4}N^{1-3\\alpha/8},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which is contradicting the algorithm's description. Hence we must have ", "page_idx": 36}, {"type": "equation", "text": "$$\nR\\le\\operatorname*{max}\\left\\{D_{1},\\frac{2C_{4}}{C_{3}D_{2}}\\right\\}\\times N^{1-3\\alpha/8}=O(N^{1-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "G Algorithmic allocations: non fluid behaviour ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In the following sections, unless otherwise stated, the proof of the mentioned results for AT2 (1) and IAT2 (2) algorithms follow a similar argument. Also, the constants introduced while stating the results in the following sections might be different for the two algorithms. ", "page_idx": 36}, {"type": "text", "text": "While using the $O(\\cdot),\\Theta(\\cdot)$ and $\\varOmega(\\cdot)$ notations, we imply that the hidden constants can depend on the choice of algorithm among AT2 and IAT2, instance $\\pmb{\\mu}$ , exploration factor $\\alpha\\in(0,1)$ and no. of arms $K$ , and are independent of the sample path. ", "page_idx": 36}, {"type": "text", "text": "G.1 Convergence of algorithmic allocations to the optimality conditions ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, our agenda is to prove the convergence of the allocations of AT2 and IAT2 algorithms to the optimality conditions mentioned in Proposition 2.2. For ease of presentation we state the conditions uniquely characterizing the optimal proportion $\\omega^{\\star}$ below according to Proposition 2.2: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{a\\neq1}\\frac{d(\\mu_{1},x_{1,a}^{\\star})}{d(\\mu_{a},x_{1,a}^{\\star})}\\;=\\;1\\quad\\mathrm{and}\\quad\\forall a\\neq1,\\quad\\omega_{1}^{\\star}d(\\mu_{1},x_{1,a}^{\\star})+\\omega_{a}d(\\mu_{a},x_{1,a}^{\\star})\\;=\\;I^{\\star}\\,=\\;T^{\\star}(\\mu)^{-1}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{vhere}\\quad x_{1,a}^{\\star}\\;=\\;\\frac{\\omega_{1}^{\\star}\\mu_{1}+\\omega_{a}^{\\star}\\mu_{a}}{\\omega_{1}^{\\star}+\\omega_{a}^{\\star}},\\quad\\mathrm{and}\\quad\\sum_{a\\in[K]}\\omega_{a}^{\\star}=1.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Recall that for every $a\\in[K]$ and iteration $N$ $\\begin{array}{r}{\\widetilde{\\omega}_{a}(N)=\\frac{\\widetilde{N}_{a}(N)}{N}}\\end{array}$ denotes the proportion of samples allocated by the algorithm to arm $a$ . Let $\\widetilde{\\omega}(N)=\\left(\\widetilde{\\omega}_{a}(N):\\dot{a}\\in[K]\\right)$ ", "page_idx": 37}, {"type": "text", "text": "Recall the anchor function $g(\\mu,\\widetilde{N}(\\cdot))$ and index $I_{a}(\\cdot)$ for every alternative arm $a\\in[K]/\\{1\\}$ . In Section 5, we defined the normalized index $H_{a}(\\cdot)$ of every arm $a\\;\\in\\;[K]/\\{1\\}$ at iteration $N$ as $\\begin{array}{r}{H_{a}(N)=\\frac{1}{N}I_{a}(N)}\\end{array}$ . In the next two sections, we prove, ", "page_idx": 37}, {"type": "equation", "text": "$$\n|g(\\pmb{\\mu},\\widetilde{\\omega}(N))|\\ =\\ \\left|\\ \\sum_{a\\in[K]}\\frac{d(\\mu_{1},x_{1,a}(N))}{d(\\mu_{a},x_{1,a}(N)}-1\\,\\right|\\ \\longrightarrow\\ 0,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a,b\\in[K]/\\{1\\}}|H_{a}(N)-H_{b}(N)|\\implies0\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "a.s.in $\\mathbb{P}_{\\pmb{\\mu}}$ as $N\\rightarrow\\infty$ . Moreover, we show that, after a random time of finite expectation, both the convergences in (55) and (56) happen at a uniform rate over all sample paths. We prove these convergence results in Proposition G.1 and G.2 stated below. ", "page_idx": 37}, {"type": "text", "text": "Proposition G.1 (Convergence of $g$ to zero). There exists constants $M_{4}\\geq1$ and $C>0$ independent of the sample paths, such that, if $T_{6}$ is defined as the iteration at which $g(\\widetilde{\\mu}(\\cdot),\\widetilde{N}(\\cdot))$ crosses the value zero after iteration max $\\{M_{4},T_{5}\\}$ $T_{5}$ is a random time satisfying $\\mathbb{E}_{\\mu}[T_{5}]<\\infty$ and defined in Definition G.1 of Appendix G.1.1), then for $N\\geq T_{6}$ we have, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left|g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N))\\right|\\ \\leq\\ C\\ensuremath{N^{-3\\alpha/8}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Moreover, the random time $T_{6}$ satisfies $\\mathbb{E}_{\\mu}[T_{6}]<\\infty$ ", "page_idx": 37}, {"type": "text", "text": "Proposition G.2 (Closeness of the indexes). There exists a random time $T_{8}$ (defined in Definition $G.4$ of Appendix G.1.2) satisfying $\\mathbb{E}_{\\mu}[T_{8}]<\\infty,$ such that, for $N\\geq T_{8}$ , every pair of alternative arms $a,\\bar{b}\\in[K]/\\{1\\}$ has, ", "page_idx": 37}, {"type": "equation", "text": "$$\n|I_{a}(N)-I_{b}(N)|\\;=\\;{\\cal O}(N^{1-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof of Proposition 5.1: By the definition of $T_{s t a b l e}$ in Definition G.5 of Appendix G.2, we have $T_{s t a b l e}\\geq T_{6}$ $T_{8}$ ,where $T_{6}$ and $T_{8}$ are the random times mentioned, respectively, in Proposition G.1 and G.2. As a result, Proposition 5.1 follows trivially from Proposition G.1 and G.2. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Proof of Proposition G.1 is in Appendix G.1.1. We prove a detailed version of Proposition G.2 as Proposition G.3 in Appendix G.1.2. Both these results are crucial later for proving the convergence of the algorithmic proportions $\\widetilde{\\omega}(N)=(\\widetilde{\\omega}_{a}(N):a\\in[K])$ to the optimal proportions $\\omega^{\\star}=(\\omega_{a}^{\\star}:a\\in$ $[K])$ in Proposition 3.1 from Section 3. We prove a detailed version of Proposition 3.1 as Proposition G.4 in Appendix G.2. ", "page_idx": 37}, {"type": "text", "text": "To prove Proposition G.1, G.2, and later Proposition G.4, we need to prove several technical properties related to exploration and the allocations made by the algorithms. The detailed technical results related to exploration are in Appendix G.4 and those related to the algorithmic allocations are in Appendix G.5. The arguments in Appendix G.1.1, G.1.2, and G.2 are self-contained, and we refer the reader to the related technical results whenever necessary. For ease of exposition, we provide below a brief summary of the statements proven in Appendix G.4 and G.5. ", "page_idx": 37}, {"type": "text", "text": "Summary of technical results in Appendix G.4 and G.5 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We summarize below the results proven in Appendix G.4 and G.5 as events happening between the non-decreasing sequence of random times $T_{0}$ $T_{1}$ $T_{2}$ $T_{3}$ , and $T_{4}$ , which are defined in Appendix G.5. ", "page_idx": 38}, {"type": "text", "text": "1. $T_{0}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\operatorname*{min}\\{N^{\\prime}\\geq1\\ |\\ \\forall N\\geq N^{\\prime}$ $\\begin{array}{r}{\\operatorname*{max}_{a\\in[K]}|\\widetilde{\\mu}_{a}(N)-\\mu_{a}|\\,\\le\\,\\epsilon(\\pmb{\\mu})N^{-3\\alpha/8}\\}}\\end{array}$ where $\\epsilon(\\pmb{\\mu})>$ 0 (defined in Appendix B), is a constant depending on the instance $\\pmb{\\mu}$ . By definition, we have $\\begin{array}{r}{\\epsilon(\\pmb{\\mu})\\leq\\frac{1}{4}\\operatorname*{min}_{a\\neq1}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ . As a result, the first arm becomes the empirically best arm and stays that way forever after iteration $T_{0}$ . In Lemma G.7 of Appendix G.3, we prove that $\\mathbb{E}_{\\mu}[T_{0}]<\\dot{\\infty}$ which implies $T_{0}<\\infty$ a.s. in $\\mathbb{P}_{\\pmb{\\mu}}$ ", "page_idx": 38}, {"type": "text", "text": "2. T,  maxTexzpo,To, where Teplo <  is a constan defned in DefinitionG.7 of Afpendit G.4. After iteration $T_{\\mathrm{explo}}$ , the algorithm consecutively does exploration over a strech of atmost $K$ iterations. Moreover, over a single such \u201cepoch\u201d\u2019 of consecutive explorations, the algorithm explores every arm atmost once (follows from statement 1 and 3 of Proposition G.5). Note that $\\begin{array}{r}{\\mathbb{E}_{\\bar{\\mu}}[T_{1}]\\leq T_{\\mathrm{explo}}+\\mathbb{E}_{\\mu}[T_{0}]<\\infty}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "3. $T_{2}$ is defined in Lemma G.11 as the iteration at which the anchor function $g(\\widetilde{\\mu}(\\cdot),\\widetilde{N}(\\cdot))$ crosses the value zero after the iteration $\\operatorname*{max}\\{M_{1},T_{1}\\}$ $M_{1}\\geq1$ is a constant independent of the sample paths and defined in the proof of Lemma G.11). By Lemma G.9, there exists a constant $C_{1}\\geq1$ independent of the sample paths, such that $T_{2}\\,\\leq\\,C_{1}\\operatorname*{max}\\{M_{1},T_{1}\\}$ . As a result, $\\mathbb{E}_{\\mu}[T_{2}]\\leq$ $C_{1}(M_{1}+\\mathbb{E}_{\\mu}[T_{1}])<\\infty$ After iteration $T_{2}$ , the empirical anchor function $g(\\widetilde{\\pmb{\\mu}}(\\cdot),\\widetilde{\\pmb{N}}(\\cdot))$ remains bounded inside an interval of the form $[-(1\\mathrm{~-~}d_{\\operatorname*{min}}),d_{\\operatorname*{max}}\\mathrm{~-~}1]$ where $d_{\\mathrm{min}}~\\in~(0,1)$ and $d_{\\mathrm{max}}\\in(1,\\infty)$ are constants independent of the sample paths (see Lemma G.11). Exploiting this, we argue that both $\\widetilde{N}_{1}(N)$ and $\\operatorname*{max}_{a\\in[K]/\\{1\\}}\\widetilde{N}_{a}\\big(N\\big)$ become $\\varOmega(N)$ after iteration $T_{2}$ (see Corollary G.1). ", "page_idx": 38}, {"type": "text", "text": "4. $T_{3}\\ \\stackrel{\\mathrm{def.}}{=}\\ \\operatorname*{max}\\{M_{2},T_{2}\\}+2.$ where $M_{2}\\,\\geq\\,1$ is a constant chosen in the proof of Lemma G.12 and is independent of the sample paths. After iteration $T_{3}$ , whenever the algorithm picks an alternative arm $a\\;\\in\\;[K]/\\{1\\}$ , then for every other alternative arm $b\\,\\in\\,[K]/\\{1,a\\}$ , we have $\\widetilde{N}_{b}(N)\\geq\\gamma\\widetilde{N}_{a}(N)$ , for some constant $\\gamma\\in(0,1)$ independent of the sample paths (see Lemma G.12). Note that $\\mathbb{E}_{\\mu}[T_{3}]\\leq M_{2}+2+\\mathbb{E}_{\\mu}[T_{2}]<\\infty$ ", "page_idx": 38}, {"type": "text", "text": "5. $T_{4}=C_{2}(T_{3}+1)$ for some constant $C_{2}\\geq1$ independent of the sample paths, defined in Lemma G.13. After iteration $T_{4}$ , all the arms $a\\in[K]$ have $\\widetilde{N}_{a}(N)=\\Theta(N)$ (see Lemma G.13). Note that $\\mathbb{E}_{\\mu}[T_{4}]\\leq C_{2}(\\mathbb{E}_{\\mu}[T_{3}]+1)<\\infty$ ", "page_idx": 38}, {"type": "text", "text": "G.1.1  Convergence of the anchor function to zero ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "The following lemma bounds the fuctuation of $g(\\widetilde{\\pmb{\\mu}},\\widetilde{\\pmb{N}})$ around $g(\\pmb{\\mu},\\widetilde{\\pmb{N}})$ due to the noise in the estimate $\\widetilde{\\pmb{\\mu}}$ of $\\pmb{\\mu}$ . We need this lemma later for proving convergence of the anchor function $g$ to zero in Proposition G.1. ", "page_idx": 38}, {"type": "text", "text": "Lemma G.1 (Bounding the noise in $g$ ). For every $N\\geq T_{2}$ (where $T_{2}$ is the random time defined in Lemma G.11 and satisfies $\\mathbb{E}_{\\mu}[T_{2}]<\\infty,$ , we have, ", "page_idx": 38}, {"type": "equation", "text": "$$\n|g(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))-g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N))|\\;=\\;{\\cal O}(N^{-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Using mean value theorem for function of several variables, we have, ", "page_idx": 38}, {"type": "equation", "text": "$$\n|g(\\widetilde{\\mu}(N),\\widetilde{N}(N))-g(\\mu,\\widetilde{N}(N))|\\ \\leq\\ \\sum_{a=1}^{K}\\bigg|\\frac{\\partial g}{\\partial\\mu_{a}}(\\widehat{\\mu},\\widetilde{N}(N))\\bigg|\\cdot|\\widetilde{\\mu}_{a}(N)-\\mu_{a}|,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\hat{\\mu}_{a}$ lies between $\\mu_{a}$ and $\\widetilde{\\mu}_{a}(N)$ for every $a\\in[K]$ ", "page_idx": 38}, {"type": "text", "text": "We define ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\hat{x}_{1,a}=\\frac{\\widetilde{N}_{1}(N)\\hat{\\mu}_{1}+\\widetilde{N}_{a}(N)\\hat{\\mu}_{a}}{\\widetilde{N}_{1}(N)+\\widetilde{N}_{a}(N)},\\quad\\mathrm{for}\\;\\mathrm{every}\\;a\\in[K]/\\{1\\}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\partial g}{\\partial\\mu_{1}}(\\hat{\\mu},\\widetilde{N}(N))\\ =\\ \\sum_{a\\neq1}\\left(\\frac{d_{1}(\\hat{\\mu}_{1},\\hat{x}_{1,a})}{d(\\hat{\\mu}_{a},\\hat{x}_{1,a})}-f(\\hat{\\mu},a,\\hat{N})\\cdot\\frac{\\widetilde{N}_{1}}{\\widetilde{N}_{1}+\\widetilde{N}_{a}}\\right),\\quad\\mathrm{and,}}\\\\ {\\forall a\\neq1,\\quad\\displaystyle\\frac{\\partial g}{\\partial\\mu_{a}}(\\hat{\\mu},\\widetilde{N}(N))\\ =\\ -\\,\\frac{d(\\hat{\\mu}_{1},\\hat{x}_{1,a})d_{1}(\\hat{\\mu}_{a},\\hat{x}_{1,a})}{(d(\\hat{\\mu}_{a},\\hat{x}_{1,a}))^{2}}-f(\\hat{\\mu},a,\\hat{N})\\cdot\\frac{\\widetilde{N}_{a}}{\\widetilde{N}_{1}+\\widetilde{N}_{a}},}\\\\ {\\mathrm{where}\\quad f(\\hat{\\mu},a,\\hat{N})\\ =\\ -\\,\\frac{d_{2}(\\hat{\\mu}_{1},\\hat{x}_{1,a})}{d(\\hat{\\mu}_{a},\\hat{x}_{1,a})}+\\frac{d(\\hat{\\mu}_{1},\\hat{x}_{1,a})d_{2}(\\hat{\\mu}_{a},\\hat{x}_{1,a})}{(d(\\hat{\\mu}_{a},\\hat{x}_{1,a}))^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and recall that $d_{1}(\\cdot,\\cdot)$ and $d_{2}(\\cdot,\\cdot)$ , respectively, denote the partial derivatives of $d(\\cdot,\\cdot)$ with respect to its first and second argument. ", "page_idx": 39}, {"type": "text", "text": "By (6), for $N>T_{2}$ , we have, ", "page_idx": 39}, {"type": "equation", "text": "$$\nd(\\hat{\\mu}_{a},\\hat{x}_{1,a})=\\theta\\left((\\hat{x}_{1,a}-\\hat{\\mu}_{a})^{2}\\right)\\;=\\;\\theta\\left(\\frac{\\widetilde{N}_{1}(N)^{2}}{(\\widetilde{N}_{1}(N)+\\widetilde{N}_{a}(N))^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By Corollary G.1 from Appendix G.5, we have $\\widetilde{N}_{1}(N)~=~\\varOmega(N)$ for $N\\ >\\ T_{2}$ .As a result, $d(\\hat{\\mu}_{a},\\hat{x}_{1,a})=\\theta(1)$ for $N>T_{2}$ ", "page_idx": 39}, {"type": "text", "text": "Moreover, for $N>T_{2}$ , we have: $|d_{1}(\\hat{\\mu}_{1},\\hat{x}_{1,a})|\\,=\\,{\\cal O}(1)$ $|d_{1}(\\hat{\\mu}_{a},\\hat{x}_{1,a})|\\,=\\,{\\cal O}(1)$ (using (7) ; $|d_{2}(\\hat{\\mu}_{1},\\hat{x}_{1,a})|=O(1)$ $|d_{2}(\\hat{\\mu}_{a},\\hat{x}_{1,a})|=O(1)$ (using (8) ; and $d(\\hat{\\mu}_{1},\\hat{x}_{1,a})={\\cal O}(1)$ (using (6). As a result, for $N>T_{2}$ , all the partial derivatives in (57) are $O(1)$ . Therefore, for $N>T_{2}$ \uff0c ", "page_idx": 39}, {"type": "equation", "text": "$$\n|g(\\widetilde{\\mu}(N),\\widetilde{N}(N))-g(\\pmb{\\mu},\\widetilde{N}(N))|\\;=\\;O\\left(\\sum_{a\\in[K]}|\\widetilde{\\mu}_{a}(N)-\\widetilde{\\mu}_{a}|\\right)\\;=\\;O(N^{-3\\alpha/8}),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and hence completing the proof. ", "page_idx": 39}, {"type": "text", "text": "Halting of exploration: By Lemma G.13, for $N\\geq T_{4}$ , every arm $a\\in[K]$ has $\\widetilde{N}_{a}(N)=\\Theta(N)$ As a result, we can find a constant $\\lambda\\,\\in\\,(0,1)$ such that $\\widetilde{N}_{a}(N)\\,\\geq\\,\\lambda\\dot{N}$ for every $a\\ \\in[K]$ and $N\\geq T_{4}$ . We choose $M_{3}$ large enough such that, for every $N\\geq M_{3}$ \uff0c $\\lambda(N-1)>N^{\\alpha}$ . Then we have $\\mathrm{min}_{a\\in[K]/\\{1\\}}\\,\\widetilde{N}_{a}(N-1)>N^{\\alpha}$ for every $N\\geq\\operatorname*{max}\\{M_{3},T_{4}+1\\}$ . As a result, the algorithm doesn't do any exploration after iteration $\\operatorname*{max}\\{M_{3},T_{4}+1\\}$ .With this, we define the following random time, ", "page_idx": 39}, {"type": "text", "text": "Definition G.1. We define $T_{5}=\\operatorname*{max}\\{M_{3},T_{4}+1\\}$ ", "page_idx": 39}, {"type": "text", "text": "Note that $\\mathbb{E}_{\\mu}[T_{5}]<\\infty$ ,since, $\\mathbb{E}_{\\mu}[T_{4}]<\\infty$ We restate Proposition G.1 below, ", "page_idx": 39}, {"type": "text", "text": "Statement of Proposition G.1. There exists constants $M_{4}\\geq1$ and $C\\,>\\,0$ independent of the sample paths, such that, if $T_{6}$ denotes theiteration atwhich $g(\\widetilde{\\pmb{\\mu}}(\\cdot),\\widetilde{\\pmb{N}}(\\cdot))$ crosses the value zero after iteration $\\operatorname*{max}\\{M_{4},T_{5}\\}$ ,then for $N\\geq T_{6}$ we have, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left|g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N))\\right|\\ \\leq\\ C\\ensuremath{N^{-3\\alpha/8}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Moreover, the random time $T_{6}$ satisfies $\\mathbb{E}_{\\mu}[T_{6}]<\\infty$ ", "page_idx": 39}, {"type": "text", "text": "Proof. We prove the proposition via an inductive argument consisting of two main steps, ", "page_idx": 39}, {"type": "text", "text": "1. Initialization: We start with a choice of the constants $C>0$ and $M_{4}\\geq1$ and show that $g(\\mu,\\widetilde{N}(\\cdot))$ satisfies (59) at iteration $T_{6}$ ", "page_idx": 39}, {"type": "text", "text": "2. Induction: We show that, for every $N~\\geq~T_{6}$ $\\left|g(\\pmb{\\mu},\\widetilde{\\cal N}(N))\\right|~\\le~C{\\cal N}^{-3\\alpha/8}$ implies $\\left|g(\\pmb{\\mu},\\widetilde{N}(N+1))\\right|\\leq C(N+1)^{-3\\alpha/8}.$ ", "page_idx": 39}, {"type": "text", "text": "By Lemma G.1, we have a constant $C_{1}>0$ independent of the sample path, such that, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\big|g(\\widetilde{\\mu}(N),\\widetilde{N}(N))-g(\\widetilde{\\mu},\\widetilde{N}(N))\\big|}&{\\le\\;C_{1}N^{-3\\alpha/8},\\quad\\mathrm{for~}N\\ge T_{5}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By Lemma G.13, we have $\\widetilde{N}_{a}(N)=\\Theta(N)$ for every $a\\in[K]$ and $N\\geq T_{5}$ .As a result, by (15), we have constants $C_{2},C_{2}^{\\prime}>0$ independent of the sample paths, such that: for all $N\\geq T_{5}$ , and $\\hat{N}_{a}\\in$ $\\Big[\\widetilde{N}_{a}(N-1),\\widetilde{N}_{a}(N)\\Big]$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\displaystyle-C_{2}^{\\prime}N^{-1}\\,\\le\\,\\displaystyle\\frac{\\partial g}{\\partial N_{1}}(\\pmb{\\mu},\\hat{N})\\,\\le\\,-\\,C_{2}N^{-1},}}&{{\\mathrm{and}}}\\\\ {{\\mathrm{for~}a\\in[K]/\\{1\\},\\quad C_{2}^{\\prime}N^{-1}\\,\\ge\\,\\displaystyle\\frac{\\partial g}{\\partial N_{a}}(\\pmb{\\mu},\\hat{N})\\,\\ge\\,C_{2}N^{-1},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\hat{N}=(\\hat{N}_{a}:a\\in[K])$ ", "page_idx": 40}, {"type": "text", "text": "We use the constants $C_{1},C_{2}$ , and $C_{2}^{\\prime}$ as defined above in the rest of our proof. ", "page_idx": 40}, {"type": "text", "text": "Initialization: We choose $C\\ =\\ 4C_{1}\\,+\\,C_{2}^{\\prime}$ and $M_{4}~=~\\operatorname*{max}\\{M_{41},M_{42},M_{43},M_{44}\\}$ ,where $M_{41},M_{42},M_{43},M_{44}$ are defined as, ", "page_idx": 40}, {"type": "text", "text": "1. $M_{41}\\geq1$ is the smallest number such that, for every $N\\geq M_{41}$ we have $2C_{1}N^{-3\\alpha/8}>$ $C_{1}(N-1)^{-3\\alpha/8}$   \n2. $M_{42}\\geq1$ is the smallest numer such that, for evey $N\\geq M_{42}$ we have $C(N+1)^{-3\\alpha/8}\\geq$ $(C_{1}+C_{2}^{\\prime})N^{-3\\alpha/8}$   \n3. $M_{43}\\geq1$ is the smallest number such that, for every $N\\geq M_{43}$ we have $C(N+1)^{-3\\alpha/8}\\geq$ $C_{2}^{\\prime}N^{-1}$ , and   \n4. $M_{44}~\\ge~1$ is the smallest number such that, for every $N\\ \\ge\\ M_{44}$ we have $\\frac{3C\\alpha}{8}(N+$ $1)^{-(1+\\frac{3\\alpha}{8})}<C_{2}N^{-1}$ ", "page_idx": 40}, {"type": "text", "text": "By definition of $T_{6}$ $\\mathrm{~\\widetilde{~6}~}g(\\widetilde{\\pmb{\\mu}}(\\cdot),\\widetilde{\\pmb{N}}(\\cdot))$ has opposite signs at iterations $T_{6}-1$ and $T_{6}$ . Therefore, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{g(\\widetilde{\\mu}(T_{6}),\\widetilde{N}(T_{6}))\\Big|}&{\\leq\\;\\Big|g(\\widetilde{\\mu}(T_{6}),\\widetilde{N}(T_{6}))-g(\\widetilde{\\mu}(T_{6}-1),\\widetilde{N}(T_{6}-1))\\Big|}\\\\ &{\\leq\\;\\Big|g(\\mu,\\widetilde{N}(T_{6}))-g(\\mu,\\widetilde{N}(T_{6}-1))\\Big|+C_{1}T_{6}^{-3\\alpha/8}+C_{1}(T_{6}-1)^{-3\\alpha/8}\\quad\\mathrm{(using~(;~}}\\\\ &{\\leq\\;\\Big|g(\\mu,\\widetilde{N}(T_{6}))-g(\\mu,\\widetilde{N}(T_{6}-1))\\Big|+3C_{1}T_{6}^{-3\\alpha/8}\\quad\\mathrm{(using~the~definition~of~}\\mu_{7}\\in\\mathcal{N}_{4}))-g(\\mu,\\widetilde{N}(T_{6})))\\Big|}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Let $a~\\in~[K]$ be the arm pulled at iteration $T_{6}$ . Applying the mean value theorem we can find $\\hat{N}_{a}$ between $\\widetilde{N}_{a}(T_{6}\\mathrm{~-~}1)$ and $\\widetilde{N}_{a}(T_{6})$ \uff0c can take $\\hat{N}_{b}~=~\\widetilde{N}_{b}(T_{6})$ for all $b~\\neq~a$ , and define the tuple $\\hat{N}=(\\hat{N}_{b})_{b\\in[K]}$ , such that,(62) is bounded by, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\,\\frac{\\partial g}{\\partial N_{a}}(\\pmb{\\mu},\\hat{N})\\,\\right|+3C_{1}T_{6}^{-3\\alpha/8}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Using (60) and the above upper bound, we have, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|g(\\mu,\\widetilde{N}(T_{6}))\\right|\\leq\\left|g(\\mu,\\widetilde{N}(T_{6}))-g(\\widetilde{\\mu}(T_{6}),\\widetilde{N}(T_{6}))\\right|+\\left|g(\\widetilde{\\mu}(T_{6}),\\widetilde{N}(T_{6}))\\right|}\\\\ &{\\qquad\\qquad\\leq\\ C_{1}T_{6}^{-3\\alpha/8}+\\bigg|\\,\\frac{\\partial g}{\\partial N_{a}}(\\mu,\\hat{N})\\,\\bigg|+3C_{1}T_{6}^{-3\\alpha/8}}\\\\ &{\\qquad\\qquad\\leq\\,4C_{1}T_{6}^{-3\\alpha/8}+C_{2}^{\\prime}T_{6}^{-1}\\quad(\\mathrm{using~(61)})}\\\\ &{\\qquad\\qquad\\leq\\,(4C_{1}+C_{2}^{\\prime})T_{6}^{-3\\alpha/8}\\,=\\,C T_{6}^{-3\\alpha/8}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Induction: Note that at a given iteration $N$ the algorithm can only see $g(\\widetilde{\\mu}(N),\\widetilde{N}(N))$ . By (60), for $N\\geq T_{6}$ \uff0c $g(\\widetilde{\\pmb{\\mu}},\\widetilde{\\pmb{N}})$ and $g(\\pmb{\\mu},\\widetilde{\\pmb{N}})$ may have different signs only when $\\left|g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N))\\right|\\le C_{1}N^{-3\\alpha/8}$ Based on this, we consider two cases. ", "page_idx": 41}, {"type": "text", "text": "Case I: $|g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N))|\\le C_{1}N^{-3\\alpha/8}$ : We assume $a\\in[K]$ to be the arm pulled in iteration $N+1$ Using the mean value theorem, we can find $\\hat{N}_{a}\\in\\Big[\\widetilde{N}_{a}\\big(\\dot{N}\\big),\\widetilde{N}_{a}(N+1)\\Big]$ , can take $\\hat{N}_{b}=\\widetilde{N}_{b}(N)$ for all $b\\neq a$ , and define the tuple $\\hat{N}=(\\hat{N}_{b})_{b\\in[K]}$ , such that, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\left\\lvert g(\\pmb{\\mu},\\widetilde{N}(N+1))\\right\\rvert~\\leq~\\displaystyle\\left\\lvert g(\\pmb{\\mu},\\widetilde{N}(N))\\right\\rvert+\\displaystyle\\left\\lvert\\frac{\\partial g}{\\partial N_{a}}(\\pmb{\\mu},\\hat{N})\\right\\rvert}&&\\\\ {\\displaystyle\\textrm{\\i\\leq}\\,C_{1}N^{-3\\alpha/8}+C_{2}^{\\prime}N^{-1}\\,\\leq\\,(C_{1}+C_{2}^{\\prime})N^{-3\\alpha/8},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where (1) follows from (61). ", "page_idx": 41}, {"type": "text", "text": "Note that $N\\geq T_{6}\\geq M_{4}\\geq M_{42}$ . By the definition of $M_{42}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|g(\\mu,\\widetilde{N}(N+1))\\right|}&{\\le\\;(C_{1}+C_{2}^{\\prime})N^{-3\\alpha/8}\\le(4C_{1}+C_{2}^{\\prime})(N+1)^{-3\\alpha/8}=C(N+1)^{-3\\alpha/8},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for every $N\\geq T_{6}$ ", "page_idx": 41}, {"type": "text", "text": "Case II: $|g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N))|>C_{1}N^{-3\\alpha/8}$ : In this cae $g(\\widetilde{\\mu}(N),\\widetilde{N}(N))$ and $g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(\\pmb{N}))$ have the same sign. Let arm $a$ has been sampled from in iteration $N+1$ . Using the mean value theorem, we have $\\hat{N}_{a}\\,\\in\\,\\Bigl[\\widetilde{N}_{a}(N),\\widetilde{N}_{a}(N+1)\\Bigr]$ can take $\\hat{N}_{b}=\\widetilde{N}_{b}(N)$ for all $b\\neq a$ , and define the tuple $\\hat{N}=(\\hat{N}_{b})_{b\\in[K]}$ , such that, ", "page_idx": 41}, {"type": "equation", "text": "$$\ng({\\pmb\\mu},\\widetilde{\\pmb N}(N+1))=g({\\pmb\\mu},\\widetilde{\\pmb N}(N))+\\frac{\\partial g}{\\partial N_{a}}({\\pmb\\mu},\\hat{\\pmb N}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We first consider the case when $g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N))>0$ . After the algorithm sees $g(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))>0$ it pulls the first arm. As a result, by (61) and (63), $g(\\mu,\\widetilde{N}(\\cdot))$ decreases in iteration $N+1$ atmost by $\\stackrel{\\cdot\\,}{C}_{2}^{\\prime}N^{-1}$ and atleast by $C_{2}N^{-1}$ . Now there can be two possibilities: ", "page_idx": 41}, {"type": "text", "text": "1. If $g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N\\ +\\ 1))\\ \\ <\\ \\ 0$ we must have $g(\\mu,\\widetilde{N}(N\\;+\\;1))\\;\\;\\;\\geq\\;\\;\\;-C_{2}^{\\prime}N^{-1}$ . Since $N\\geq T_{6}\\geq M_{4}\\geq M_{43}$ , we have $C(N\\!+\\!1)^{-3\\alpha/8}\\geq C_{2}^{\\prime}N^{-1}$ by the definition of $M_{43}$ .As a result, ", "page_idx": 41}, {"type": "equation", "text": "$$\ng(\\pmb{\\mu},\\widetilde{\\cal N}(N+1))~\\ge~-~C_{2}^{\\prime}N^{-1}~\\ge~-~C(N+1)^{-3\\alpha/8}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "2. If $g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N+1))\\ge0$ , then $g(\\mu,\\widetilde{N}(\\cdot))$ has moved towards zero by atleast $C_{2}N^{-1}$ . Whereas, by iteration $N+1$ , the interval $\\left[\\,\\,-\\,C\\dot{N}^{-3\\alpha/8},\\,\\,C N^{3\\alpha/8}\\,\\right]$ has reduced from both ends by ", "page_idx": 41}, {"type": "equation", "text": "$$\nC N^{-3\\alpha/8}-C(N+1)^{-3\\alpha/8}\\ \\leq\\ \\frac{3C\\alpha}{8}N^{-(1+\\frac{3\\alpha}{8})}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Since $N\\geq T_{6}\\geq M_{4}\\geq M_{44}$ , by the defnition of $M_{44}$ we have $_{\\textrm{8}}^{\\textrm{3}C\\alpha}(N+1)^{-(1+\\frac{3\\alpha}{8})}<C_{2}N^{-1}$ for every $N\\geq T_{6}$ . As a result, we can ensure $g(\\pmb{\\mu},\\widetilde{\\cal N}(N+1))\\stackrel{\\cdot}{\\leq}C(N+1)^{-3\\alpha/8}$ at iteration $N+1$ ", "page_idx": 41}, {"type": "text", "text": "In the other case, when $g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N))\\,<\\,0$ , the algorithm sees $g(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))\\,<\\,0$ , and hence pulls some arm $a\\;\\in\\;[K]/\\{1\\}$ .As a result, by (61) and (63), $g(\\mu,\\widetilde{N}(\\cdot))$ increases in iteration $N+1$ atmost by $C_{2}^{\\prime}\\bar{N^{-1}}$ and atleast by $C_{2}N^{-\\mathrm{i}}$ . Then we apply the same argument as for the case $g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N))>0$ , but by reversing the signs. Therefore, the inductive statement holds true for this case as well. Hence (59) stands proved. ", "page_idx": 41}, {"type": "text", "text": "$T_{6}$ has finite expectation: By Lemma G.9, we can have a constant $C_{3}~>~0$ such that $T_{6}\\ \\leq$ $C_{3}\\operatorname*{max}\\{M_{4},T_{5}\\}$ . As a result, since $\\mathbb{E}_{\\mu}[T_{5}]<\\infty$ ,wehave $\\mathbb{E}_{\\mu}[T_{6}]\\leq C_{1}(M_{4}+\\mathbb{E}_{\\mu}[T_{5}])<\\infty$ ", "page_idx": 41}, {"type": "text", "text": "G.1.2  Closeness of the indexes ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Lemma G.2 is a detailed version of Lemma 5.1 mentioned in Section 5, and is essential for proving closeness of the indexes under the allocations made by AT2 and IAT2 algorithms. Recall that $T_{6}$ is the random time defined in Proposition G.1 and satisfies $\\mathbb{E}_{\\mu}[T_{6}]<\\infty$ ", "page_idx": 42}, {"type": "text", "text": "Lemma G.2. For both AT2 and IAT2 algorithms, there exists constants $M_{5}\\,\\geq\\,1$ and $C_{1}~>~0$ independent of the sample paths, such that, for every $N\\geq\\operatorname*{max}\\{M_{5},T_{6}\\}$ if the algorithm picks an arm $a\\in[K]/\\{1\\}$ at iteration $N$ then it again picks arm a within the next $\\left\\lceil C_{1}N^{1-3\\alpha/8}\\right\\rceil$ iterations. ", "page_idx": 42}, {"type": "text", "text": "Proof of Lemma G.2 is in Appendix G.6, and requires proving several technical lemmas. Some of those supporting lemmas involve arguments similar to the ones used for proving closeness of the indexes while the algorithm operates under an idealized fuid model (discussed in Section 4). In the rest of this section, we use Lemma G.2 to prove closeness of indexes for alternative arms in PropositionG.2. ", "page_idx": 42}, {"type": "text", "text": "Definition G.2. We define the random time $T_{7}=\\operatorname*{max}\\{M_{5},T_{6}\\}$ ", "page_idx": 42}, {"type": "text", "text": "Note that $\\mathbb{E}_{\\mu}[T_{7}]<\\infty$ since $\\mathbb{E}_{\\mu}[T_{6}]<\\infty$ ", "page_idx": 42}, {"type": "text", "text": "Definition G.3. For every $M\\geq1$ define $T_{7,M}=\\operatorname*{max}\\{M,T_{7}\\}$ and $T_{8,M}$ as the smallest iteration after $T_{7,M}$ by which all the alternative arms in $[K]/\\{1\\}$ have been picked atleast once by the algorithm. ", "page_idx": 42}, {"type": "text", "text": "Below we state a detailed version of Proposition G.2. ", "page_idx": 42}, {"type": "text", "text": "Proposition G.3. For every $M\\geq1$ we have $\\mathbb{E}_{\\mu}[T_{7,M}]<\\infty$ and $\\mathbb{E}_{\\mu}[T_{8,M}]<\\infty$ Moreover, for every $M\\geq1$ and $N\\geq T_{8,M}$ , every pair of arms $a,b\\in[K]/\\{1\\}$ satisfy, ", "page_idx": 42}, {"type": "equation", "text": "$$\n|I_{a}(N)-I_{b}(N)|\\;=\\;{\\cal O}(N^{1-3\\alpha/8}),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the constant hidden in $O(\\cdot)$ is independent of $M$ and the sample path after $T_{7}$ ", "page_idx": 42}, {"type": "text", "text": "Definition G.4. We define $T_{8}=T_{8,1}$ where $T_{8,1}$ is defined according to Proposition G.3. ", "page_idx": 42}, {"type": "text", "text": "By the defintion of $T_{8}$ above, Proposition G.2 follows trivially from Proposition G.3. ", "page_idx": 42}, {"type": "text", "text": "The following lemma helps us to bound the deviation of the empirical index $\\mathcal{Z}_{a}(N)$ from the index $I_{a}(N)$ due to the noise in the estimates $\\widetilde{\\pmb{\\mu}}$ , for every alternative arm $a\\in[K]/\\{1\\}$ ", "page_idx": 42}, {"type": "text", "text": "Lemma G.3. For $a\\in[K]/\\{1\\}$ and $N\\geq T_{0}$ ,we have, ", "page_idx": 42}, {"type": "equation", "text": "$$\n|{\\mathcal Z}_{a}(N)-I_{a}(N)|\\;=\\;O(N^{1-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. Proof of this lemma uses mean value theorem. For any arm $a\\in[K]/\\{1\\}$ , upon expanding the indexes, ", "page_idx": 42}, {"type": "equation", "text": "$$\n|\\mathbb{Z}_{a}(N)-I_{a}(N)|\\;\\leq\\;\\widetilde{N}_{1}\\cdot|d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\mu_{1},x_{1,a})|+\\widetilde{N}_{a}\\cdot|d(\\widetilde{\\mu}_{a},\\widetilde{x}_{1,a})-d(\\mu_{a},x_{1,a})|,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\widetilde{N}_{1},\\widetilde{N}_{a},\\widetilde{\\mu}_{1},\\widetilde{\\mu}_{a}$ , and $\\widetilde{x}_{1,a}$ are evaluated at $N$ .Since $\\widetilde{N}_{1},\\widetilde{N}_{a}\\;\\leq\\;N$ , the difference (64) is bounded above by, ", "page_idx": 42}, {"type": "equation", "text": "$$\n|\\mathbb{Z}_{a}(N)-I_{a}(N)|\\;\\le\\;N\\cdot\\left(|d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\mu_{1},x_{1,a})|+|d(\\widetilde{\\mu}_{a},\\widetilde{x}_{1,a})-d(\\mu_{a},x_{1,a})|\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Now considering the first term in the RHS, and applying mean value theorem, we get ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle|d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\mu_{1},x_{1,a})|~=~\\left|~d_{1}(\\hat{\\mu}_{1},\\hat{x}_{1,a})+d_{2}(\\hat{\\mu}_{1},\\hat{x}_{1,a})\\cdot\\frac{\\widetilde{N}_{1}}{\\widetilde{N}_{1}+\\widetilde{N}_{a}}~\\right|\\cdot|\\widetilde{\\mu}_{1}-\\mu_{1}|}\\\\ {\\displaystyle}&{\\displaystyle+\\left|~d_{2}(\\hat{\\mu}_{1},\\hat{x}_{1,a})\\cdot\\frac{\\widetilde{N}_{a}}{\\widetilde{N}_{1}+\\widetilde{N}_{a}}~\\right|\\cdot|\\widetilde{\\mu}_{a}-\\mu_{a}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\hat{\\mu}_{1},\\hat{\\mu}_{a}$ , respectively, lie between $\\widetilde{\\mu}_{1},\\mu_{1}$ , and $\\widetilde{\\mu}_{a},\\mu_{a}$ , and $\\begin{array}{r}{\\hat{x}_{1,a}=\\frac{\\widetilde{N}_{1}\\hat{\\mu}_{1}+\\widetilde{N}_{a}\\hat{\\mu}_{a}}{\\widetilde{N}_{1}+\\widetilde{N}_{a}}}\\end{array}$ N+Naba Using (7) and (8), all the partial derivatives in the above upper bound are $O(1)$ for $N\\geq T_{0}$ . Therefore, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{|d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\mu_{1},x_{1,a})|\\;=\\;O\\left(|\\widetilde{\\mu}_{1}-\\mu_{1}|+|\\widetilde{\\mu}_{a}-\\mu_{a}|\\right)}}&{{}}&{{}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{=\\;O(N^{-3\\alpha/8}).}}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Following a similar procedure, we can argue using (7) and (8), that the partial derivatives of $d(\\widetilde{\\mu}_{j},\\widetilde{x}_{1,j})$ with respect to $\\widetilde{\\mu}_{1}$ and ${\\widetilde{\\mu}}_{j}$ are $O(1)$ in magnitude. As a result, using the mean value theorem, ", "page_idx": 43}, {"type": "equation", "text": "$$\n|d(\\widetilde{\\mu}_{a},\\widetilde{x}_{1,a})-d(\\mu_{a},x_{1,a})|\\;=\\;{\\cal O}(N^{-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Therefore, we have, ", "page_idx": 43}, {"type": "equation", "text": "$$\n|{\\mathcal Z}_{a}(N)-I_{a}(N)|\\;=\\;O(N^{1-3\\alpha/8}),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for $N\\geq T_{0}$ and completing the proof. ", "page_idx": 43}, {"type": "text", "text": "Proof of Proposition G.2: We have $\\mathbb{E}_{\\mu}[T_{7,M}]\\le M+\\mathbb{E}_{\\mu}[T_{7}]<\\infty$ . By Lemma G.13, $\\widetilde{N}_{a}(N)=$ $\\Theta(N)$ for $N\\geq T_{7,M}$ . Hence, by the definition of $T_{8,M}$ , there exists a constant $C^{\\prime}>0$ independent of $M$ , such that, for every $M\\geq1$ \uff0c $T_{8,M}\\leq C^{\\prime}T_{7,M}$ . As a result, $\\mathbb{E}_{\\mu}[T_{8,M}]\\leq C^{\\prime}\\mathbb{E}_{\\mu}[T_{7,M}]<\\infty$ Note that $T_{7,1}=T_{7}$ . Also, for every $M\\geq1$ \uff0c $T_{8,M}\\geq T_{8,1}=T_{8}$ $T_{8}$ is defined in Definition G.4). It is sufficient to prove the proposition for every $N\\geq T_{8}$ ", "page_idx": 43}, {"type": "text", "text": "We now argue for the algorithms AT2 and IAT2 separately. ", "page_idx": 43}, {"type": "text", "text": "AT2: We consider any two alternative arms $a,b\\in[K]/\\{1\\}$ , and define the time $\\tau_{a,b}(N)$ as, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\tau_{a,b}(N)=\\operatorname*{min}\\Big\\{t\\ge1\\ \\Big|\\ \\mathcal{Z}_{b}(N+t)-\\mathcal{Z}_{a}(N+t)\\ \\mathrm{~and~}\\,\\mathcal{Z}_{b}(N)-\\mathcal{Z}_{a}(N)\\ \\mathrm{have}\\ \\mathrm{opposite}\\ \\mathrm{signs}\\Big\\}\\,.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Note that $N+\\tau_{a,b}(N)$ must be before the iteration after $N$ by which the algorithm has picked both $a$ and $b$ atleast once. By the definition of $T_{7}$ and $T_{8}$ , for every $N\\geq T_{8}$ , all alternative arms in $[K]/\\{1\\}$ has been sampled from atleast once between iterations $T_{7}$ and $N$ . Therefore, by Lemma G.2, we have $\\tau_{a,b}(N)={\\cal O}(N^{1-3\\alpha/8})$ ", "page_idx": 43}, {"type": "text", "text": "Since $\\mathcal{Z}_{a}(N)-\\mathcal{Z}_{b}(N)$ and $\\mathcal{Z}_{a}(N+\\tau_{a,b}(N))-\\mathcal{Z}_{b}(N+\\tau_{a,b}(N))$ have opposite signs, we have, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|\\mathbb{Z}_{a}(N)-\\mathbb{Z}_{b}(N)|}&{\\le\\ |(\\mathbb{Z}_{a}(N)-\\mathbb{Z}_{b}(N))\\ -\\ (\\mathbb{Z}_{a}(N+\\tau_{a,b}(N))-\\mathbb{Z}_{b}(N+\\tau_{a,b}(N)))|}\\\\ &{\\le\\ |\\mathbb{Z}_{a}(N+\\tau_{a,b}(N))-\\mathbb{Z}_{a}(N)|\\ +\\ |\\mathbb{Z}_{b}(N+\\tau_{a,b}(N))-\\mathbb{Z}_{b}(N)|}\\\\ &{\\le\\ |I_{a}(N+\\tau_{a,b}(N))-I_{a}(N)|\\ +\\ |I_{b}(N+\\tau_{a,b}(N))-I_{b}(N)|}\\\\ &{\\ +\\ O((N+\\tau_{a,b}(N))^{1-3\\alpha/8}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the last step follows from Lemma G.3. Now, ", "page_idx": 43}, {"type": "equation", "text": "$$\n{\\cal O}\\left((N+\\tau_{a,b}(N))^{1-3\\alpha/8}\\right)~=~{\\cal O}\\left(\\left(N+{\\cal O}(N^{1-3\\alpha/8})\\right)^{1-3\\alpha/8}\\right)~=~{\\cal O}(N^{1-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Therefore, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Z_{a}(N)\\!-\\!\\mathcal{Z}_{b}(N)|\\,\\le\\,|I_{a}(N+\\tau_{a,b}(N))-I_{a}(N)|\\,+\\,|I_{b}(N+\\tau_{a,b}(N))-I_{b}(N)|\\,+\\cal O(N^{1-3\\alpha/8}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "By Lemma G.3, we know $|I_{a}(N)-I_{b}(N)|\\le|\\mathbb{Z}_{a}(N)-\\mathbb{Z}_{b}(N)|+O(N^{1-3\\alpha/8})$ Therefore, the above inequality implies, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{|I_{a}(N)-I_{b}(N)|\\;\\leq\\;|\\mathcal{Z}_{a}(N)-\\mathcal{Z}_{b}(N)|\\;+\\;O(N^{1-3\\alpha/8})}}\\\\ {{\\;\\leq\\;\\;|I_{a}(N+\\tau_{a,b}(N))-I_{a}(N)|\\;+\\;\\,|I_{b}(N+\\tau_{a,b}(N))-I_{b}(N)|}}\\\\ {{\\;}}&{{\\;+\\;\\;O(N^{1-3\\alpha/8}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Using mean value theorem, for $j\\in\\{a,b\\}$ , we have, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\lvert I_{j}(N+\\tau_{a,b}(N))-I_{j}(N)\\rvert\\;\\leq\\;\\left(\\sum_{i\\in\\{1,j\\}}\\frac{\\partial I_{j}}{\\partial N_{i}}(\\hat{N}_{1},\\hat{N}_{j})\\right)\\cdot\\tau_{a,b}(N),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\hat{N}_{i}\\in\\Big[\\widetilde{N}_{i}(N),\\;\\widetilde{N}_{i}(N+\\tau_{a,b}(N))\\Big]$ for $i=1,a,b$ ", "page_idx": 43}, {"type": "text", "text": "We know, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{\\partial I_{j}}{\\partial N_{1}}(\\hat{N}_{1},\\hat{N}_{j})\\;=\\;d(\\mu_{1},\\hat{x}_{1,j})\\quad\\mathrm{and}\\quad\\frac{\\partial I_{j}}{\\partial N_{j}}(\\hat{N}_{1},\\hat{N}_{j})\\;=\\;d(\\mu_{j},\\hat{x}_{1,j}),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where t talivavadf $\\operatorname*{max}\\{d(\\mu_{1},\\mu_{a}),d(\\mu_{a},\\mu_{1})\\}$ , and therefore $O(1)$ . As a result, since $\\tau_{a,b}(N)\\,=\\,{\\cal O}(N^{1-3\\alpha/8})$ we have, ", "page_idx": 44}, {"type": "equation", "text": "$$\n|I_{j}(N+\\tau_{a,b}(N))-I_{j}(N)|\\;\\leq\\;{\\cal O}(N^{1-3\\alpha/8})\\quad\\mathrm{for~}j=a,b.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Using (69) in (67), we get ", "page_idx": 44}, {"type": "equation", "text": "$$\n|I_{a}(N)-I_{b}(N)|\\;=\\;{\\cal O}(N^{1-3\\alpha/8}),\\;\\;\\;\\;\\mathrm{for}\\;N\\ge T_{8}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "IAT2: First we define the modified empirical index of every alternative arm $a\\in[K]/\\{1\\}$ using the notation $\\mathcal{T}_{a}^{(m)}(N)$ as, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{a}^{(m)}(N)\\;=\\;\\mathcal{Z}_{a}(N)+\\log(\\widetilde{N}_{a}(N)).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We define thetine $\\tau_{a,b}^{(m)}(N)$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tau_{a,b}^{(m)}(N)\\;=\\;\\operatorname*{min}\\Big\\{\\,t\\geq1\\;\\Big|\\;\\;\\mathcal{Z}_{b}^{(m)}(N+t)-\\mathcal{Z}_{a}^{(m)}(N+t)\\quad\\mathrm{and}\\qquad}\\\\ {\\qquad\\qquad\\qquad\\quad\\mathcal{Z}_{b}^{(m)}(N)-\\mathcal{Z}_{a}^{(m)}(N)\\quad\\mathrm{have~opposite~signs}\\;\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Note that, for every $a\\in[K]/\\{1\\},\\mathcal{Z}_{a}^{(m)}(N)$ differs from $\\mathcal{Z}_{a}(N)$ by atmost $\\log(N)$ and $\\mathcal{Z}_{a}(N)$ differs from $I_{a}(N)$ by atmost $O(N^{1-3\\alpha/8})$ for $N\\geq T_{0}$ . Therefore, ", "page_idx": 44}, {"type": "equation", "text": "$$\n|\\mathcal{Z}_{a}^{(m)}(N)-I_{a}(N)|\\;=\\;O(N^{1-3\\alpha/8})\\quad\\mathrm{for}\\;N\\ge T_{0}\\;\\mathrm{and}\\;\\mathrm{every}\\;a\\in[K]/\\{1\\}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Now $N+\\tau_{a,b}^{(m)}(N)$ must be earlier than the iteration after $N$ by which the algorithm has picked both $a$ $b$ $\\tau_{a,b}^{(m)}(N)=$ $O(N^{1-3\\alpha/8})$ . Also, following the same steps as AT2, by replacing the empirical index $\\mathcal{T}$ with the modified empirical index $\\mathcal{T}^{(m)}$ for every alternative arm, we obtain, ", "page_idx": 44}, {"type": "equation", "text": "$$\nI_{a}(N)-I_{b}(N)|\\ \\leq\\ |I_{a}(N+\\tau_{a,b}^{(m)}(N))-I_{a}(N)|\\,+\\,|I_{b}(N+\\tau_{a,b}^{(m)}(N))-I_{b}(N)|\\,+\\,O(N^{1-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Using the mean value theorem, since the parital derivatives of $I_{a}$ and $I_{b}$ with respect to $\\widetilde{N}_{1},\\widetilde{N}_{a}$ and ${\\widetilde{N}}_{b}$ are $O(1)$ ,wehave ", "page_idx": 44}, {"type": "equation", "text": "$$\n|I_{j}(N+\\tau_{a,b}^{(m)}(N))-I_{j}(N)|\\;\\leq\\;{\\cal O}\\left(\\tau_{a,b}^{(m)}(N)\\right)={\\cal O}(N^{1-3\\alpha/8})\\quad\\mathrm{for~}j=a,b.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "From the last two observations, we conclude ", "page_idx": 44}, {"type": "equation", "text": "$$\n|I_{a}(N)-I_{b}(N)|\\;\\leq\\;{\\cal O}(N^{1-3\\alpha/8})\\;\\;\\;\\;\\mathrm{for}\\;N\\ge T_{8}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "G.2  Convergence of algorithm to optimal proportions ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In this appendix we prove a slightly detailed version of Proposition 3.1 from Section 3. In Proposition 3.1, we argue that the proportion of samples allocated by the algorithm converges to the optimal proportions for the instance, a.s. in $\\mathbb{P}_{\\mu}$ , as the no. of samples grows to $\\infty$ ", "page_idx": 44}, {"type": "text", "text": "For every $M\\geq1$ , we use $T_{7,M}$ and $T_{8,M}$ as defined in Definition G.3 in Appendix G.1.2. Recall that $T_{7}=T_{7,1}$ and $T_{8}=T_{8,1}$ . By Proposition G.3, we have $\\mathbb{E}_{\\mu}[T_{8,M}]<\\infty$ , and $T_{8,M}\\geq T_{8}$ for every $M\\geq1$ ", "page_idx": 45}, {"type": "text", "text": "Recall that $\\omega^{\\star}$ is the unique optimal allocation according to Proposition 2.2, and $\\widetilde{\\pmb{\\omega}}(N)=(\\widetilde{\\omega}_{a}(N):$ $a\\in[K])$ with $\\begin{array}{r}{\\widetilde{\\omega}_{a}(N)=\\frac{\\widetilde{N}_{a}(N)}{N}}\\end{array}$ is the algorithms allocation at iteration $N$ . We now state a slightly detailed version of Proposition 3.1 from Section 3, ", "page_idx": 45}, {"type": "text", "text": "Proposition G.4. There exists constants $C_{1}>0$ and $M_{6}\\geq1$ depending on $\\pmb{\\mu},\\alpha$ and $K$ such that, for every $N\\geq T_{8,M_{6}}$ and $a\\in[K]$ we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n|\\widetilde{\\omega}_{a}(N)-\\omega_{a}^{\\star}|\\ \\leq\\ C_{1}N^{-3\\alpha/8}\\quad a n d\\quad|\\widetilde{\\mu}_{a}(N)-\\mu_{a}|\\ \\leq\\ \\epsilon(\\mu)N^{-3\\alpha/8},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\epsilon(\\mu)$ is a constant depending only on $\\pmb{\\mu}$ and defined in Appendix $B$ ", "page_idx": 45}, {"type": "text", "text": "Detailed proof of Proposition G.4 is in Appendix G.2.2 and relies on using IFT. ", "page_idx": 45}, {"type": "text", "text": "Below we define the random times $T_{g o o d}$ and $T_{s t a b l e}$ , which are mentioned in the statements of Proposition 3.1, 5.1, and Lemma 5.1 from the main body of the paper. ", "page_idx": 45}, {"type": "text", "text": "Definition G.5 $T_{s t a b l e}$ and $T_{g o o d},$ .We define $T_{g o o d}=T_{7,M_{6}}$ and $T_{s t a b l e}=T_{8,M_{6}}$ where $M_{6}\\geq1$ is introduced in Proposition G.4. ", "page_idx": 45}, {"type": "text", "text": "Remark G.1. Note that, by definition, $T_{g o o d}\\geq T_{4},T_{6}$ . As a result, by Proposition G.1 and Lemma G.13, $\\left|g(\\pmb{\\mu},\\widetilde{\\cal N}(N))\\right|=O(N^{-3\\alpha/8})$ and $\\widetilde{N}_{j}(N)=\\Theta(N)$ for every $j\\in[K]$ and $N\\geq T_{g o o d}$ ", "page_idx": 45}, {"type": "text", "text": "Proof of Lemma 5.1: By the definition of $T_{7,M},\\ \\ T_{8,M}$ in Appendix G.1.2, and since $T_{g o o d}=T_{7,M_{6}}$ \uff0c\uff0c $T_{s t a b l e}=T_{8,M_{6}}$ , every alternative arm in $[K]/\\{1\\}$ gets picked atleast once between the iterations $T_{g o o d}$ and $T_{s t a b l e}$ . The other part of the statement of Lemma 5.1 follows from Lemma G.2 because $\\bar{T_{g o o d}}\\geq T_{7}$ \u53e3 ", "page_idx": 45}, {"type": "text", "text": "Before proving Proposition G.4 in Appendix G.2.2, we find a tighter upper bound on the time to reach optimal proportion $T_{s t a b l e}$ in the following Appendix G.2.1. While doing this, we identify a similarity between the time to reach stabilty in fuid dynamics and that for the algorithm. ", "page_idx": 45}, {"type": "text", "text": "G.2.1 Bounding time to reach stability ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Lemma G.4 gives an upper bound on the time to reach stability for the algorithmic allocations. We define $\\begin{array}{r}{\\omega_{\\mathrm{min}}^{\\star}=\\operatorname*{min}_{a\\in[K]/\\{1\\}}\\omega_{a}^{\\star}}\\end{array}$ ", "page_idx": 45}, {"type": "text", "text": "According to the discussion in Section 4, if the fluid dynamics has state $\\widetilde{N}(T_{g o o d})$ at time $T_{g o o d}$ , then it hits all the indexes and reaches stability by a time atmost Taood In Lemma G.4, we argue that, the algorithm also approximately reaches the optimal proportion $\\omega^{\\star}$ by atmost $\\begin{array}{r}{\\approx\\frac{T_{g o o d}}{\\omega_{\\operatorname*{min}}^{\\star}}}\\end{array}$ Tgood iterations. ", "page_idx": 45}, {"type": "text", "text": "Lemma G.4. For every $M\\geq M_{6}$ $M_{6}$ is a constant defined in the statement of Proposition $G.4$ ", "page_idx": 45}, {"type": "equation", "text": "$$\nT_{8,M}\\ \\le\\ \\frac{T_{7,M}+1}{\\omega_{\\mathrm{min}}^{\\star}-C_{1}M^{-3\\alpha/8}},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "which implies ", "page_idx": 45}, {"type": "equation", "text": "$$\nT_{s t a b l e}\\;\\leq\\;\\frac{T_{g o o d}+1}{\\omega_{\\operatorname*{min}}^{\\star}-C_{1}M_{6}^{-3\\alpha/8}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Moreover, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{M\\to\\infty}\\frac{T_{8,M}}{T_{7,M}}\\;\\leq\\;\\frac{1}{\\omega_{\\operatorname*{min}}^{\\star}}\\;a.s.\\mathrm{~}i n\\;\\mathbb{P}_{\\mu}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. From Proposition G.4, it follows that, for every $M\\geq M_{6}$ and $N\\geq T_{8,M}\\geq T_{8,M_{6}}$ ,we have, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in[K]}|\\widetilde{\\omega}_{a}(N)-\\omega_{a}^{\\star}|\\;\\leq\\;C_{1}N^{-3\\alpha/8}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Since $T_{8,M}$ is the first iteration after $T_{7,M}$ by which every alternative arm has been picked atleast once, we have some arm $a\\in[K]/\\{1\\}$ such that, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\widetilde{N}_{a}(T_{8,M})\\;=\\;\\widetilde{N}_{a}(T_{7,M})+1\\;\\leq\\;T_{7,M}+1.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Now by (70), we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\widetilde{N}_{a}(T_{8,M})\\ \\ge\\ (\\omega_{a}^{\\star}-C_{1}T_{8,M}^{-3\\alpha/8})T_{8,M}\\ \\ge\\ (\\omega_{\\operatorname*{min}}^{\\star}-C_{1}M^{-3\\alpha/8})T_{8,M}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Combining the last two observation, we have, ", "page_idx": 46}, {"type": "equation", "text": "$$\nT_{8,M}\\;\\leq\\;\\frac{T_{7,M}+1}{\\omega_{\\operatorname*{min}}^{\\star}-C_{1}M^{-3\\alpha/8}}\\quad\\mathrm{a.s.\\in}\\,\\mathbb{P}_{\\mu}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for every $M\\geq M_{6}$ ", "page_idx": 46}, {"type": "text", "text": "Since $T_{8,M},T_{7,M}\\rightarrow\\infty$ as $M\\to\\infty$ a.s. in $\\mathbb{P}_{\\mu}$ , we have, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{M\\to\\infty}\\operatorname*{lim}_{T_{7,M}}\\;\\leq\\;\\frac{1}{\\omega_{\\operatorname*{min}}^{\\star}}\\quad\\mathrm{a.s.\\in}\\,\\mathbb{P}_{\\mu}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "G.2.2 Proving Proposition G.4 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "By Proposition G.1 and G.3, there exists a constant $C>0$ independent of the sample paths, such that $\\tilde{\\omega}(N)\\stackrel{-}{=}(\\widetilde{\\omega}_{a}(N))_{a\\in[K]}$ satisfies, ", "page_idx": 46}, {"type": "equation", "text": "$$\n|g(\\mu,\\widetilde{\\omega}(N))|\\ =\\ \\left|\\ \\sum_{\\scriptstyle a\\in[K]/\\{1\\}}\\frac{d(\\mu_{1},x_{1,a}(N))}{d(\\mu_{a},x_{1,a}(N))}-1\\ \\right|\\ \\leq\\ C N^{-3\\alpha/8},\\quad{\\mathrm{and}}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for all $N\\geq T_{8}$ a.s. in $\\mathbb{P}_{\\mu}$ ", "page_idx": 46}, {"type": "text", "text": "Proof of Proposition G.4 relies on using the implicit function theorem. Before proving the proposition, we describe below the framework over which we apply the implicit function theorem. We define the following functions, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\varPsi_{1}(\\omega,\\eta)\\,=\\,g(\\mu,\\omega)-\\eta_{1}\\,=\\,\\sum_{a\\ne1}\\frac{d(\\mu_{1},x_{1,a}(\\omega_{1},\\omega_{a}))}{d(\\mu_{a},x_{1,a}(\\omega_{1},\\omega_{a}))}-1-\\eta_{1},}\\\\ &{\\mathrm{~or~}a\\in[K]/\\{1\\},\\quad\\varPsi_{a}(\\omega,I,\\eta)\\,=\\,\\displaystyle W_{a}(\\omega_{1},\\omega_{a})-I-\\eta_{a},\\quad\\mathrm{and}}\\\\ &{\\displaystyle\\quad\\varPsi_{K+1}(\\omega)\\,=\\,\\sum_{a\\in[K]}\\omega_{a}-1,}\\end{array}\n$$f ", "text_format": "latex", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathrm{~vhere~\\quad~}\\omega\\;=\\;\\{\\omega_{a}\\}_{a\\in[K]}\\;\\in\\;\\mathbb{R}_{\\ge0}^{K},\\quad\\eta\\;=\\;(\\eta_{a})_{a\\in[K]}\\;\\in\\;\\mathbb{R}^{K},\\quad\\;\\;\\;I\\;\\in\\;\\mathbb{R},\\quad\\;\\mathrm{~and~for~every}}\\\\ &{\\colon\\;\\in\\;[K]/\\{1\\},\\;\\;x_{1,a}(\\omega_{1},\\omega_{a})\\;=\\;\\;\\frac{\\omega_{1}\\mu_{1}+\\omega_{a}\\mu_{a}}{\\omega_{1}+\\omega_{a}}\\;\\mathrm{~and~}\\;W_{a}(\\omega_{1},\\omega_{a})}&{=\\;\\;\\omega_{1}d(\\mu_{1},x_{1,a}(\\omega_{1},\\omega_{a}))\\;+}\\\\ &{}&{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Using the functions defined above, we define the vector valued function $\\varPsi(\\omega,I,\\eta)$ as follows, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varPsi(\\omega,I,\\eta)=\\big(\\,\\varPsi_{1}(\\omega,\\eta),\\ \\varPsi_{2}(\\omega,I,\\eta),\\ \\varPsi_{3}(\\omega,I,\\eta),\\ \\ \\ldots,\\ \\varPsi_{K}(\\omega,I,\\eta),\\ \\varPsi_{K+1}(\\omega)\\,\\big)\\,.}\\\\ &{\\mathrm{~}}\\\\ &{\\mathrm{~\\mathrm{aps~tuples~of~the~form~}\\ }(\\omega,I,\\eta)\\in\\mathbb{R}_{\\ge0}^{K}\\times\\mathbb{R}\\times\\mathbb{R}^{K}\\quad\\mathrm{to}\\ \\ \\mathbb{R}^{K+1}.}\\end{array}\n$$$\\varpmb{\\varPsi}$ m ", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Its asy to oserve that for every $\\pmb{\\omega}=(\\omega_{a}:a\\in[K])\\in\\mathbb{R}_{\\ge0}^{K}$ satisfying $\\textstyle\\sum_{a\\in[K]}\\omega_{a}=1$ and $I\\in\\mathbb{R}$ \uff0c there is a unique $\\pmb{\\eta}\\in\\mathbb{R}^{K}$ for which $\\pmb{\\varPsi}(\\omega,I,\\pmb{\\eta})=\\mathbf{0}_{K+1}$ We refer to the quantity $\\operatorname*{max}_{a\\in[K]}|\\eta_{a}|$ as the violation caused by the pair $(\\omega,I)$ to the optimality conditions in (54). ", "page_idx": 46}, {"type": "text", "text": "By (54), all the alternative arms in $[K]/\\{1\\}$ have equal normalized index under the optimal allocation $\\omega^{\\star}$ Let $I^{\\star}=W_{a}(\\omega_{1}^{\\star},\\omega_{a}^{\\star})$ for every $\\dot{a}\\in[K]/\\{\\bar{1}\\}$ . Then Proposition 2.2 implies $(\\omega^{\\star},I^{\\star})$ is the unique tuple satisfying ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\pmb{\\varPsi}(\\omega^{\\star},I^{\\star},\\mathbf{0}_{K})=\\mathbf{0}_{K+1}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "To prove Proposition G.4 we need the two technical lemmas: Lemma G.5 and G.6. Let us define $\\|\\pmb{x}\\|_{\\infty}=\\operatorname*{max}_{a\\in[K]}|x_{a}|$ forevery $\\pmb{x}\\in\\mathbb{R}^{K}$ ", "page_idx": 47}, {"type": "text", "text": "Lemma G.5 shows that, the set of allocations satisfying the optimality conditions in (54) upto a maximumviolation of $r>0$ shrinks to $\\omega^{\\star}$ as $r$ decreases to zero. In Lemma G.6, we use Lemma G.5 and IFT to argue that if the perturbation vector $\\eta$ satisfies $\\|\\eta\\|_{\\infty}\\leq\\eta_{\\mathrm{max}}$ ,where $\\eta_{\\mathrm{max}}>0$ is a constant depending only on $\\pmb{\\mu}$ then there is a unique pair $(\\omega,I)$ satisfying $\\pmb{\\Psi}(\\omega,I,\\pmb{\\eta})=\\mathbf{0}_{K+1}$ Moreover, the function mapping a perturbation vector $\\pmb{\\eta}\\in[-\\eta_{\\mathrm{max}},\\eta_{\\mathrm{max}}]^{K}$ to the unique pair $(\\omega,I)$ solving $\\pmb{\\Psi}(\\omega,I,\\pmb{\\eta})=\\mathbf{0}_{K+1}\\,$ is Lipschitz continuous. ", "page_idx": 47}, {"type": "text", "text": "It is now easy to see Proposition G.4 follows from Lemma G.5 and G.6. By (71), the violation caused by the algorithmic allocation $\\widetilde{\\omega}(N)$ to the optimality conditions in (54) converges to zero uniformly at a rate $O(N^{-3\\alpha/8})$ .We wait for suffciently many iterations such that, the violation becomes smaller than $\\eta_{\\mathrm{max}}$ . Then using Lipschitzness of the allocation as a function of perturbation (proven in Lemma G.6), we have $\\|\\widetilde{\\omega}(N)-\\omega^{\\star}\\|_{\\infty}=O(N^{-3\\alpha/8})$ ", "page_idx": 47}, {"type": "text", "text": "Lemma G.5. For every $r\\geq0$ ,we define the quantity, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d i s t(\\omega^{\\star},r)\\ =\\ \\operatorname*{max}\\Big\\{\\ \\operatorname*{max}\\{\\|\\omega-\\omega^{\\star}\\|_{\\infty},\\ |I-I^{\\star}|\\}\\ \\Big|\\ \\omega\\in\\mathbb{R}_{\\ge0}^{K},\\ I\\in\\mathbb{R},\\ \\,a n d}\\\\ {\\exists\\,\\eta\\in[-r,r]^{K}\\ \\ s u c h\\,t h a t\\ \\pmb{\\varPsi}(\\omega,I,\\eta)=\\mathbf{0}_{K+1}\\ \\Big\\}.\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The following statements are true about the mapping $r\\mapsto d i s t(\\omega^{\\star},r)$ ", "page_idx": 47}, {"type": "text", "text": "$I.\\ \\,d i s t(\\omega^{\\star},0)=0,$ 2. $d i s t(\\omega^{\\star},r)$ is non-decreasing in $r$ and 3. $\\begin{array}{r}{\\operatorname*{lim}_{r\\to0}d i s t(\\omega^{\\star},r)=0.}\\end{array}$ ", "page_idx": 47}, {"type": "text", "text": "Proof. Statement 1: Statement 1 follows directly from the fact that $(\\omega^{\\star},I^{\\star})$ is the unique tuple satisfying $\\pmb{\\varPsi}(\\omega^{\\star},I^{\\star},\\mathbf{0}_{K})=\\mathbf{0}_{K+1}$ , as proven in Proposition 2.2. ", "page_idx": 47}, {"type": "text", "text": "Statement 2: Follows directly from the definition of $d i s t(\\omega^{\\star},r)$ ", "page_idx": 47}, {"type": "text", "text": "Statement 3: By statement 2, $\\begin{array}{r}{\\operatorname*{lim}_{r\\to0}d i s t(\\omega^{\\star},r)}\\end{array}$ exists and is non-negative. We consider a contradiction to statement 3 and assume that $\\begin{array}{r}{\\operatorname*{lim}_{r\\rightarrow0}d i s t(\\omega^{\\star},r)=d>0.}\\end{array}$ ", "page_idx": 47}, {"type": "text", "text": "Since $r\\to d i s t(\\omega^{\\star},r)$ is non-decreasing, we can construct a decreasing sequence $\\{r_{n}\\}_{n\\ge1}$ such that, for every $n\\geq1,\\;\\;r_{n}>0,$ $d i s t(\\omega^{\\star},r_{n})\\geq d$ and $\\operatorname*{lim}_{n\\to\\infty}r_{n}=0$ . As a result, using the definition of $d i s t(\\omega^{\\star},r)$ , we have a sequence of tuples $\\{(\\omega_{n},I_{n},\\eta_{n})\\}_{n\\ge1}$ , such that, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathrm{ry}\\ n\\geq1,\\quad\\|\\eta_{n}\\|_{\\infty}\\leq r_{n},\\quad\\Psi(\\omega_{n},I_{n},\\eta_{n})=\\mathbf{0}_{K+1},\\quad\\mathrm{and}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "equation", "text": "$$\n\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}\\operatorname*{inf}\\operatorname*{max}}\\left\\{\\|\\pmb{\\omega}_{n}-\\pmb{\\omega}^{\\star}\\|_{\\infty},|I_{n}-I^{\\star}|\\right\\}\\geq d.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Since $\\pmb{\\psi}_{K+1}(\\pmb{\\omega}_{n})=0$ for every $n\\geq1$ , the whole sequence $(\\omega_{n})_{n\\ge1}$ lies in the set ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left\\{\\left.(\\omega_{1},\\omega_{2},\\ldots,\\omega_{K})\\in\\mathbb{R}_{\\ge0}^{K}~\\right|~\\sum_{i\\in[K]}\\omega_{i}=1~\\right\\},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "which is compact with respect to the norm $\\|\\cdot\\|_{\\infty}$ ", "page_idx": 47}, {"type": "text", "text": "Let for every $n\\geq1$ and $a\\in[K]$ \uff0c $\\omega_{a,n}$ and $\\eta_{a,n}$ be, respectively, the $a$ -thcomponentofthevectors $\\omega_{n}$ and $\\eta_{n}$ . For every $n\\geq1$ and $a\\in[K]/\\{1\\}$ , we have $I_{n}=W_{a}(\\omega_{1,n},\\omega_{a,n})-\\eta_{a,n}$ Since $W_{a}(\\cdot,\\cdot)$ always lies in the interval $[0,d(\\mu_{1},\\mu_{a})+d(\\mu_{a},\\mu_{1})]$ and $|\\eta_{a,n}|\\leq r_{n}$ , we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n-r_{n}\\ \\leq\\ I_{n}\\ \\leq\\ d(\\mu_{1},\\mu_{a})+d(\\mu_{a},\\mu_{1})+r_{n}\\ \\ \\ {\\mathrm{for}}\\ {\\mathrm{every}}\\ n\\geq1.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "By our assumption, we already have $r_{n}\\,\\rightarrow\\,0$ , which also implies, the sequence $r_{n}$ is bounded from above. As a result, $I_{n}$ is also bounded. Therefore, we can have a convergent subsequence $\\{(\\omega_{n_{k}},I_{n_{k}})\\}_{k\\ge1}$ , with limits $\\omega_{n_{k}}\\rightarrow\\omega^{\\prime}$ and $I_{n_{k}}\\to I^{\\prime}$ in the $\\|\\cdot\\|_{\\infty}$ -norm. ", "page_idx": 48}, {"type": "text", "text": "For every $k\\geq1$ , we have $\\pmb{\\varPsi}(\\omega_{n_{k}},I_{n_{k}},\\pmb{\\eta}_{n_{k}})=\\mathbf{0}_{K+1}$ .As a result, using the continuity of $\\varPsi(\\cdot)$ with respect to its arguments, we have $\\pmb{\\varPsi}(\\omega^{\\prime},I^{\\prime},\\mathbf{0}_{K})=\\mathbf{0}_{K+1}$ , implying $\\omega^{\\prime}$ is an optimal allocation for the instance $\\pmb{\\mu}$ ", "page_idx": 48}, {"type": "text", "text": "Hence, our assumption $\\begin{array}{r l}{\\operatorname*{lim}\\operatorname*{inf}_{n\\to\\infty}\\operatorname*{max}\\{\\|\\omega_{n}\\,-\\,\\omega^{\\star}\\|_{\\infty},|I_{n}\\,-\\,I^{\\star}|\\}\\,\\geq\\,d}\\end{array}$ implies $\\operatorname*{max}\\{\\|\\omega^{\\prime}\\mathrm{~-~}$ $\\omega^{\\star}\\|_{\\infty}$ \uff0c ${|I^{\\prime}-I^{\\star}|}\\bigr\\}\\geq\\;d\\;>\\;0$ , which further implies $\\omega^{\\prime}\\neq\\omega^{\\star}$ . As a result, the instance $\\pmb{\\mu}$ has two distinct optimal allocations $\\omega^{\\prime}$ and $\\omega^{\\star}$ , which contradicts Proposition 2.2. \u53e3 ", "page_idx": 48}, {"type": "text", "text": "Lemma G.6. There exists $\\eta_{\\mathrm{max}}\\,>\\,0$ depending only on the instance $\\pmb{\\mu}$ such that the following statements are true, ", "page_idx": 48}, {"type": "text", "text": "1. For every $\\pmb{\\eta}\\in[-\\eta_{\\operatorname*{max}},\\eta_{\\operatorname*{max}}]^{K}$ therexists a unique tuple $(\\pmb{\\omega},I)\\in\\mathbb{R}_{\\ge0}^{K}\\times\\mathbb{R}$ which satisfes,. $\\pmb{\\varPsi}(\\omega,I,\\pmb{\\eta})=\\mathbf{0}_{K+1}$   \n2. For every $\\pmb{\\eta}\\,\\in\\,[-\\eta_{\\mathrm{max}},\\eta_{\\mathrm{max}}]^{K}$ .we call the unique tuple mentioned in statement $^{\\,l}$ as $(\\overline{{\\omega}}(\\eta),\\overline{{I}}(\\eta))$ .Then the function ", "page_idx": 48}, {"type": "equation", "text": "$$\n(\\overline{{\\omega}},\\overline{{I}}):[-\\eta_{\\operatorname*{max}},\\eta_{\\operatorname*{max}}]^{K}\\mapsto\\mathbb{R}_{\\ge0}^{K}\\times\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": ".s $L$ -Lipschitz, for some $L>0$ depending on the instance $\\pmb{\\mu}$ ", "page_idx": 48}, {"type": "text", "text": "Proof. By Proposition 2.2, we know that the optimal allocation $\\omega^{\\star}$ is the unique allocation satisfying, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\pmb{\\Psi}(\\omega^{\\star},I^{\\star},\\mathbf{0}_{K})\\;=\\;\\mathbf{0}_{K+1}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "for some $I^{\\star}\\,>\\,0$ . Note that $\\pmb{\\varPsi}(\\omega,I,\\eta)=\\pmb{\\varPhi}(\\omega,I,\\eta,1)$ for every tuple $(\\omega,I,\\eta)$ , where $\\varPhi$ is the function defined in Appendix C. By statement 3 of Lemma C.1, the Jacobian $\\frac{\\partial\\pmb{\\varPsi}}{\\partial(\\pmb{\\omega},I)}$ of the function $\\varPsi(\\omega,I,\\eta)$ is invertible at the tuple $(\\omega^{\\star},I^{\\star},\\mathbf{0}_{K})$ ", "page_idx": 48}, {"type": "text", "text": "Therefore, applying the Implicit function theorem, we can find $\\delta_{0},\\delta_{1}>0$ , and continuously differentiable functions ", "page_idx": 48}, {"type": "equation", "text": "$$\n(\\overline{{\\omega}}(\\cdot),\\overline{{I}}(\\cdot))\\ :\\ (-\\delta_{0},\\delta_{0})^{K}\\mapsto\\mathbb{R}_{\\geq0}^{K}\\times\\mathbb{R},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "such that, ", "page_idx": 48}, {"type": "text", "text": "1. $\\overline{{\\omega}}(\\mathbf{0}_{K})=\\omega^{\\star}$ \uff0c ${\\overline{{I}}}(\\mathbf{0}_{K})=I^{\\star}$ , and ", "page_idx": 48}, {"type": "text", "text": "2. for every $\\pmb{\\eta}\\in(-\\delta_{0},\\delta_{0})^{K}$ \uff0c $(\\overline{{\\omega}}(\\eta),\\overline{{I}}(\\eta))$ is the unique tuple in $\\mathbb{R}_{\\geq0}^{K}\\times\\mathbb{R}$ to satisfy, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\|\\overline{{\\omega}}(\\eta)-\\omega^{\\star}\\|_{\\infty},|\\overline{{I}}(\\eta)-I^{\\star}|\\right\\}\\leq\\delta_{1}\\quad\\mathrm{and}\\quad\\Psi(\\overline{{\\omega}}(\\eta),\\overline{{I}}(\\eta),\\eta)=\\mathbf{0}_{K+1}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "By statement 3 of Lemma G.5, we can find a $\\delta_{2}>0$ such that, $d i s t(\\omega^{\\star},r)<\\delta_{1}$ for $r\\in[0,\\delta_{2}]$ We define $\\begin{array}{r}{\\eta_{\\mathrm{max}}=\\operatorname*{min}\\left\\{\\frac{\\delta_{0}}{2},\\delta_{2}\\right\\}}\\end{array}$ ", "page_idx": 48}, {"type": "text", "text": "By the definition of $d i s t(\\omega^{\\star},\\cdot)$ , for every $\\pmb{\\eta}~\\in~[-\\eta_{\\mathrm{max}},\\eta_{\\mathrm{max}}]^{K}$ ,if a tuple $(\\omega,I)$ satisfies $\\pmb{\\varPsi}(\\omega,I,\\pmb{\\eta})=\\mathbf{0}_{K+1}$ , then it also satisfies $\\operatorname*{max}\\{||\\omega-\\omega^{\\star}||_{\\infty},|I-I^{\\star}|\\}<\\delta_{1}$ ", "page_idx": 48}, {"type": "text", "text": "On the other hand, by IFT, since $\\eta_{\\mathrm{max}}<\\delta_{0}$ \uff0c $(\\overline{{\\omega}}(\\eta),\\overline{{I}}(\\eta))$ is the only such tuple possible. Therefore, for every $\\pmb{\\eta}\\in\\ [-\\eta_{\\mathrm{max}},\\eta_{\\mathrm{max}}]^{K}$ \uff0c $(\\overline{{\\omega}}(\\eta),\\overline{{I}}(\\eta))$ is the unique element in o\u00d7 IR such that $\\pmb{\\varPsi}(\\overline{{\\omega}}(\\pmb{\\eta}),\\overline{{I}}(\\pmb{\\eta}),\\pmb{\\eta})=\\mathbf{0}_{K+1}$ . This proves the first statement of Lemma G.6. ", "page_idx": 48}, {"type": "text", "text": "Since $\\overline{{\\omega}}(\\cdot),\\overline{{I}}(\\cdot)$ is continuously differentiable in $(-\\delta_{0},\\delta_{0})^{K}$ , every component of this mapping must b $L$ Lipschitz for some $L\\,>\\,0$ .n $\\left[-\\frac{\\delta_{0}}{2},\\frac{\\delta_{0}}{2}\\right]^{K}$ equipped with $\\|\\cdot\\|_{\\infty}$ -norm. We can take $L$ to be the maximum of the $\\left\\Vert\\cdot\\right\\Vert_{1}$ -norm of the gradients of different components of $(\\overline{{\\omega}}(\\cdot),\\overline{{I}}(\\cdot))$ over the set $\\left[-\\frac{\\delta_{0}}{2},\\,\\frac{\\delta_{0}}{2}\\right]^{K}$ . Since the gradients are all continuous, their $\\left\\|\\cdot\\right\\|_{1}$ -norm must be bounded in a compact set like [-\uff0c]K, , and hence $L<\\infty$ . Therefore, the second part of Lemma G.6 follows from our assumption $\\begin{array}{r l r}{\\eta_{\\mathrm{max}}\\leq\\frac{\\delta_{0}}{2}}\\end{array}$ \u53e3 ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "We now proceed on proving Proposition G.4. ", "page_idx": 49}, {"type": "text", "text": "Proof of Proposition G.4: Recall that in Section 5, for every $a\\in[K]/\\{1\\}$ , we defined the normalized index as $\\begin{array}{r}{H_{a}(N)=\\frac{I_{a}(N)}{N}}\\end{array}$ ", "page_idx": 49}, {"type": "text", "text": "Taking $\\begin{array}{r}{H(N)=H_{2}(N)=\\frac{I_{2}(N)}{N}}\\end{array}$ $\\widetilde{\\pmb{\\eta}}(N)$ be te unique $\\pmb{\\eta}\\in\\mathbb{R}^{K}$ to satisfy,. $\\pmb{\\varPsi}(\\widetilde{\\omega}(N),H(N),\\pmb{\\eta})=$ OK+1\u00b7 ", "page_idx": 49}, {"type": "text", "text": "Note that for every $a\\in[K]/\\{1\\}$ , we have $W_{a}(\\widetilde{\\omega}_{1}(N),\\widetilde{\\omega}_{a}(N))=H_{a}(N)$ . As a result,by (71), we have $\\lVert\\widetilde{\\pmb{\\eta}}(N)\\rVert_{\\infty}\\leq C N^{-3\\alpha/8}$ for all $N\\geq T_{8}$ ", "page_idx": 49}, {"type": "text", "text": "Now we pick $M_{6}\\geq1$ large enough, such that, ", "page_idx": 49}, {"type": "equation", "text": "$$\nC M_{6}^{-3\\alpha/8}<\\eta_{\\mathrm{max}},\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\eta_{\\mathrm{max}}$ is introduced in Lemma G.6. We define $T_{s t a b l e}=T_{8,M_{6}}$ . Note that $T_{s t a b l e}\\geq T_{6}\\geq T_{0}$ As aresult, by the dfnition f $T_{0}$ in Appendix G.3, we have $\\begin{array}{r}{\\operatorname*{max}_{a\\in[K]}|\\widetilde{\\mu}_{a}(N)\\!-\\!\\mu_{a}|\\leq\\epsilon(\\pmb{\\mu})N^{-3\\alpha/8}}\\end{array}$ for every N \u2265 Tstable. ", "page_idx": 49}, {"type": "text", "text": "Now, by (71), for $N\\geq T_{s t a b l e}$ , the allocations $\\widetilde{\\omega}(N)$ satisfies, $\\pmb{\\varPsi}(\\widetilde{\\pmb{\\omega}}(N),H(N),\\widetilde{\\pmb{\\eta}}(N))=\\pmb{0}_{K+1}$ with ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\lVert\\widetilde{\\pmb{\\eta}}(N)\\rVert_{\\infty}\\,\\le\\,C N^{-3\\alpha/8}\\,\\le\\,C M_{6}^{-3\\alpha/8}\\,<\\,\\eta_{\\mathrm{max}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "As a result, by Lemma G.6, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\widetilde{\\omega}_{a}(N)\\;=\\;\\overline{{{\\omega}}}_{a}(\\widetilde{\\pmb{\\eta}}(N))\\quad\\mathrm{for\\;every}\\;a\\in[K],\\;\\mathrm{and}\\;N\\ge T_{s t a b l e},\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\overline{{\\omega}}_{a}(\\cdot)$ is the $a$ -th component of the vector valued function $\\overline{{\\omega}}(\\cdot)$ introduced in Lemma G.6. ", "page_idx": 49}, {"type": "text", "text": "By Lemma G.6, for every $a\\in[K],\\ \\ \\overline{{\\omega}}_{a}(\\cdot)$ is $L$ -Lipschitz in $[-\\eta_{\\mathrm{max}},\\eta_{\\mathrm{max}}]^{K}$ equipped with $\\|\\cdot\\|_{\\infty}$ norm. As a result, for $N\\geq T_{s t a b l e}$ , we have, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{max}_{a\\in[K]}|\\widetilde{\\omega}_{a}(N)-\\omega_{a}^{\\star}|}&{=}&{\\displaystyle\\operatorname*{max}_{a\\in[K]}|\\overline{{\\omega}}_{a}(\\widetilde{\\eta}(N))-\\overline{{\\omega}}_{a}(\\mathbf{0}_{K})|\\ \\leq\\ L\\|\\widetilde{\\eta}(N)-\\mathbf{0}_{K}\\|_{\\infty}}\\\\ &&{}&\\\\ &&{=\\ L\\|\\widetilde{\\eta}(N)\\|_{\\infty}\\ \\leq\\ L C N^{-3\\alpha/8}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Taking $C_{1}=L C$ , we have the desired result. ", "page_idx": 49}, {"type": "text", "text": "G.3 $T_{0}$ has finite expectation ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "In Section 5, we introduced the random time $T_{0}$ as, ", "page_idx": 49}, {"type": "equation", "text": "$$\nT_{0}\\;=\\;\\operatorname*{min}\\left\\{\\;\\,N^{\\prime}\\geq1\\;\\;\\Big|\\;\\,\\forall N\\geq N^{\\prime},\\:\\:\\:\\operatorname*{max}_{a\\in[K]}\\;|\\;\\widetilde\\mu_{a}(N)-\\mu_{a}|\\leq\\epsilon(\\mu)N^{-3\\alpha/8}\\;\\;\\right\\},\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\epsilon(\\mu)$ is a positive constant defined in Appendix $\\mathbf{B}$ and depends only on the instance $\\pmb{\\mu}$ Lemma G.7. The random time $T_{0}$ satisfies $\\mathbb{E}_{\\mu}[T_{0}]<\\infty$ and hence $T_{0}<\\infty\\,a.s$ in $\\mathbb{P}_{\\pmb{\\mu}}$ ", "page_idx": 49}, {"type": "text", "text": "Proof. To avoid notational clutter, let $\\mathbb{P}=\\mathbb{P}_{\\mu}$ and $\\epsilon=\\epsilon(\\mu)$ . Then for any $N$ \uff0c ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}(T_{0}=N+1)\\!\\!}&{\\le\\mathbb{P}\\left(\\exists a\\in[K],\\ |\\tilde{\\mu}_{a}(N)-\\mu_{a}|>\\epsilon N^{-3\\alpha/8}\\right)}\\\\ &{\\le\\displaystyle\\sum_{a\\in[K]}\\mathbb{P}\\left(|\\tilde{\\mu}_{a}(N)-\\mu_{a}|>\\epsilon N^{-3\\alpha/8}\\right)}\\\\ &{\\le\\displaystyle\\sum_{a\\in[K]}\\sum_{t=(N^{\\alpha}-C_{1})_{+}}^{N}\\mathbb{P}\\left(|\\hat{\\mu}_{a,t}-\\mu_{a}|>\\epsilon N^{-3\\alpha/8}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\hat{\\mu}_{a,t}$ denotes the empirical mean of $t$ i.i.d. samples drawn from the $a$ -th arm, and the last step follows from statement 2 of Proposition G.5, which says $\\widetilde{N}_{a}(N)\\geq N^{\\alpha}-C_{1}$ for some constant $C_{1}>0$ ", "page_idx": 50}, {"type": "text", "text": "Using Chernoff's bound (like in the proof of Lemma 19 of [11]), we have, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(|\\hat{\\mu}_{a,t}-\\mu_{a}|>\\epsilon N^{-3\\alpha/8}\\right)\\ \\leq\\ \\mathbb{P}\\left(\\hat{\\mu}_{a,t}>\\mu_{a}+\\epsilon N^{-3\\alpha/8}\\right)+\\mathbb{P}\\left(\\hat{\\mu}_{a,t}<\\mu_{a}-\\epsilon N^{-3\\alpha/8}\\right)}\\\\ &{\\qquad\\leq\\ \\exp\\left(-t\\cdot d(\\mu_{a}+\\epsilon N^{-3\\alpha/8},\\mu_{a})\\right)+\\exp\\left(-t\\cdot d(\\mu_{a}-\\epsilon N^{-3\\alpha/8},\\mu_{a})\\right)}\\\\ &{\\qquad\\leq\\ 2\\exp\\left(-t\\cdot\\operatorname*{min}\\left\\{\\ d(\\mu_{a}+\\epsilon N^{-3\\alpha/8},\\mu_{a}),\\ d(\\mu_{a}-\\epsilon N^{-3\\alpha/8},\\mu_{a})\\right\\}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Using (6), we have a constant $C_{2}>0$ depending on the instance $\\pmb{\\mu}$ and such that, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{d(\\mu_{a}+\\epsilon N^{-3\\alpha/8},\\mu_{a}),\\;d(\\mu_{a}-\\epsilon N^{-3\\alpha/8},\\mu_{a})\\right\\}\\ \\ge\\ \\epsilon^{2}C_{2}N^{-\\frac{3\\alpha}{4}}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Therefore, we have, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(|\\hat{\\mu}_{a,t}-\\mu_{a}|>\\epsilon N^{-3\\alpha/8}\\right)}&{\\leq\\;2\\exp\\left(-t\\epsilon^{2}C_{2}N^{-\\frac{3\\alpha}{4}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Therefore, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(T_{0}=N+1)\\ \\leq\\ \\displaystyle\\sum_{i\\in[K]}\\ \\sum_{t=(N^{\\alpha}-C_{1})_{+}}^{N}2\\exp\\left(-t\\epsilon^{2}C_{2}N^{-\\frac{3\\alpha}{4}}\\right)}\\\\ &{\\leq\\ \\displaystyle\\sum_{i\\in[K]}\\ \\sum_{t=N^{\\alpha}-C_{1}}^{N}2\\exp\\left(-t\\epsilon^{2}C_{2}N^{-\\frac{3\\alpha}{4}}\\right)}\\\\ &{\\leq\\ 2\\displaystyle\\sum_{i\\in[K]}\\exp\\left(-\\epsilon^{2}C_{2}(N^{\\alpha}-C_{1})N^{-\\frac{3\\alpha}{4}}\\right)\\cdot\\Big(\\displaystyle\\sum_{t=N^{\\alpha}-C_{1}}^{N}\\exp\\left(-\\epsilon^{2}C_{2}N^{-\\frac{3\\alpha}{4}}(t-N^{\\alpha}+C_{1})\\right)\\Big)}\\\\ &{\\leq\\ 2K N\\exp\\left(-\\epsilon^{2}C_{2}(N^{\\alpha}-C_{1})N^{-\\frac{3\\alpha}{4}}\\right)\\ =\\ 2K N\\exp(-\\mathcal{Q}(N^{\\frac{\\alpha}{4}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where the constant hidden in $\\varOmega(\\cdot)$ depends only on $\\pmb{\\mu}$ . Using the obtained upper bound, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[T_{0}]\\,=\\,\\mathbb{P}(T_{0}=1)+\\sum_{N\\ge1}(N+1)\\mathbb{P}(T_{0}=N+1)}\\\\ {\\,\\,\\leq\\,1+\\displaystyle\\sum_{N\\ge1}2K N(N+1)\\exp(-\\varOmega(N^{\\frac\\alpha4})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Note that the series on the RHS is convergent for any $\\alpha\\in(0,1)$ .Therefore $\\mathbb{E}_{\\mu}[T_{0}]<\\infty$ ", "page_idx": 50}, {"type": "text", "text": "G.4  Properties of exploration ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "In the following discussion, the set of iterations in which the algorithm does exploration is defined as all iterations $N$ where $\\mathrm{min}_{a\\in[K]}\\,\\widetilde{N}_{a}(N-1)<N^{\\alpha}$ (which is equivalent to having $\\mathcal{V}_{N}\\neq\\emptyset$ ,where $\\mathcal{V}_{N}$ denotes the set of starved arms at iteration $N$ , and is defined in Section 3). We define the epoch of exploration at some arbitrary iteration as follows, ", "page_idx": 50}, {"type": "text", "text": "Definition G.6. If the algorithm does exploration at iteration $N$ ,the epoch of exploration at $N$ isthe maximum no. of consecutive iterations including $N$ in which the algorithm has done exploration. More precisely, $i f N_{1}$ and $N_{2}$ are, respectively, defined as, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{N_{1}\\ =\\ \\operatorname*{max}\\left\\{t\\leq N\\mid t-1\\mathrm{\\}i s\\mathrm{\\}n o t\\,a n\\mathrm{\\}e x p l o r a t i o n\\right\\}}\\\\ {N_{2}\\ =\\ \\operatorname*{min}\\left\\{t\\geq N\\mid t+1\\mathrm{\\}i s\\mathrm{\\}n o t\\,a n\\mathrm{\\}e x p l o r a t i o n\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "then the epoch of exploration at iteration $N$ is $N_{2}-N_{1}+1$ ", "page_idx": 50}, {"type": "text", "text": "Proposition G.5. The following statements are true: ", "page_idx": 50}, {"type": "text", "text": "1. For every iteration which is an exploration, the epoch of exploration at that iteration is upper boundedby a constant depending on $K$ and $\\alpha$ .We denote this constant using $T_{e p o c h}$ ", "page_idx": 51}, {"type": "text", "text": "2. There exists a constant $C$ depending on $K$ and $\\alpha$ such that , over every sample path, we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N)\\;\\geq\\;N^{\\alpha}-C.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "As a result, $\\widetilde{N}_{a}(N)=\\varOmega(N^{\\alpha})$ for every arm $a\\in[K]$ ", "page_idx": 51}, {"type": "text", "text": "3. There exists a $M$ depending on $K$ and $\\alpha$ such that, every epoch of exploration starting after iteration $M$ has length atmost $K$ , and every arm can get pulled atmost once in that epoch. We call the constant $M$ a.s $M_{e x p l o}$   \n4. If an epoch of exploration startsfrom some $N\\geq M_{e x p l o}$ , then the next epoch of exploration doesn't start before another $\\Theta(N^{1-\\alpha})$ iterations.   \n5. Let $\\hat{N}\\ge M_{e x p l o}$ be such that, $\\hat{N}$ is an exploration. Define the following sequence, $N_{0}=\\hat{N}$ and for $k\\geq1$ $N_{k}\\ =\\ \\operatorname*{min}\\left\\{\\,N>N_{k-1}\\ \\left|\\ N_{k}\\ i s\\ t h e\\,b e g i n i n g\\;o f a n\\;e p o c h\\;o f\\,e x p l o r a t i o n\\ \\right\\}.$ Then $N_{k}=\\hat{N}\\!+\\!\\Omega(k^{1/\\alpha})$ In otherwords for any $N\\geq M_{e x p l o}$ the $k$ -th epoch of exploration after iteration $N$ starts after $N+\\varOmega(k^{1/\\alpha})$ iterations.   \n6. For any $N\\ge M_{e x p l o}+T_{e p o c h}+1$ and $T\\geq1$ the no. ofepochs of exploration intersecting with the set of iterations $\\left\\{N,N+1,N+2,\\ldots,N+T\\right\\}$ is ${\\cal O}(T^{\\alpha})$ ", "page_idx": 51}, {"type": "text", "text": "Proof. Statement 1: Let the algorithm does exploration at iteration $N$ . We can always choose $N$ in such a way that, iteration $N-1$ was not an exploration, by choosing $N$ to be the iteration at which an epoch begins. If the epoch of exploration starting at iteration $N$ continues till iteration $N+t,i.e.$ the iterations $N,N+1,\\dots,N+t$ are exploration, then, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(N+t)^{\\alpha}\\ \\geq\\ \\underset{a\\in[K]}{\\operatorname*{min}}\\ \\widetilde{N}_{a}(N+t-1)\\ \\stackrel{(1)}{\\geq}\\ \\underset{a\\in[K]}{\\operatorname*{min}}\\ \\widetilde{N}_{a}(N-1)+\\frac{t}{K}}\\\\ &{\\ \\geq\\ \\underset{a\\in[K]}{\\operatorname*{min}}\\ \\widetilde{N}_{a}(N-2)+\\frac{t}{K}\\ \\stackrel{(2)}{\\geq}(N-1)^{\\alpha}+\\frac{t}{K},}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where (1) follows from the fact that, $\\operatorname*{min}_{a\\in[N]}\\widetilde{N}_{a}(\\cdot)$ increments by atleast 1 over every $K$ consecutive iterations in an epoch of exploration, and (2) from the fact that iteration $N-1$ is not an exploration. From the above inequality, we have, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{t}{K}\\:\\leq\\:(N+t)^{\\alpha}-(N-1)^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Note that, $N\\to(N+t)^{\\alpha}-(N-1)^{\\alpha}$ is decreasing in $N$ . Hence $R H S\\leq(1+t)^{\\alpha}\\leq1+t^{\\alpha}\\leq2t^{\\alpha}$ (since $t\\geq1$ ). Therefore, we have, ", "page_idx": 51}, {"type": "equation", "text": "$$\n{\\frac{t}{K}}\\;\\leq\\;2t^{\\alpha}\\;\\;\\;\\;\\mathrm{implying}\\;\\;\\;\\;t\\;\\leq\\;(2K)^{1/(1-\\alpha)}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Therefore every epoch of exploration is atmost $(2K)^{1/(1-\\alpha)}$ iterations long. ", "page_idx": 51}, {"type": "text", "text": "Statement 2: In the following discussion we use $[i:j]$ for a pair of integers $i<j$ to denote the set $\\{i,i+1,i+2,\\ldots,j\\}$ .We consider only those iterations where $\\mathrm{min}_{a\\in[K]}\\,\\widetilde{N}_{a}(N-1)<N^{\\alpha}$ By statement 1, if we consider $N_{1}\\leq N\\leq N_{2}$ such that, $N_{1}$ \uff0c $N_{1}+1$ $N_{2}-1$ \uff0c $N_{2}$ are all explorations and $N_{1}-1,N_{2}+1$ are not explorations, then we must have, $N_{2}-N_{1}\\leq T_{\\mathrm{epoch}}$ . As a result, we have, ", "page_idx": 51}, {"type": "equation", "text": "$$\nN^{\\alpha}-\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N)\\;\\leq\\;\\operatorname*{max}_{N\\in[N_{1}:N_{2}]}(N^{\\alpha}-\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N)).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Now, the slowest rate at which $\\mathrm{min}_{a\\in[K]}\\,\\widetilde{N}_{a}(N)$ can grow while iterations $N~\\in~\\{N_{1},~N_{1}~+$ 1, :., $N_{2}-1,\\ N_{2}\\}$ is if the algorithm pulis the $K$ arms consecutively in those iterations. Therefore, we have, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N)\\ \\geq\\ \\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N_{1}-1)+\\frac{N-N_{1}+1}{K}\\quad\\mathrm{for}\\,\\mathrm{every}\\ N\\in[N_{1}:N_{2}].\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Using this, we have, ", "page_idx": 52}, {"type": "equation", "text": "$$\nN^{\\alpha}-\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N)\\ \\leq\\ \\operatorname*{max}_{N\\in[N_{1}:N_{2}]}\\left(N^{\\alpha}-\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N_{1}-1)-\\frac{N-N_{1}+1}{K}\\right).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Since iteration $N_{1}-1$ is not an exploration, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N_{1}-1)\\;\\geq\\;\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N_{1}-2)\\;\\geq\\;(N_{1}-1)^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Using this, the upper bound becomes, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{\\boldsymbol{\\mathcal{I}}\\in[N_{1}:N_{2}]}\\left(N^{\\alpha}-(N_{1}-1)^{\\alpha}-\\frac{N-N_{1}+1}{K}\\right)}&{\\le\\displaystyle\\operatorname*{max}_{N\\in[N_{1}:N_{2}]}\\left((N-N_{1}+1)^{\\alpha}-\\frac{N-N_{1}+1}{K}\\right)}\\\\ &{\\le\\displaystyle\\operatorname*{max}_{\\boldsymbol{z}\\in[0,T_{\\mathrm{epsh}}]}\\left((\\boldsymbol{z}+1)^{\\alpha}-\\frac{1+\\boldsymbol{z}}{K}\\right)=C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $C$ depends only on $K$ and $\\alpha$ ", "page_idx": 52}, {"type": "text", "text": "Hence we get, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a\\in[K]}\\tilde{N}_{a}(N)\\;\\ge\\;N^{\\alpha}-C\\;=\\;\\varOmega(N^{\\alpha}).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Statement 3: If the epoch of exploration starts from $T$ and continues for more than $K$ iterations, note that $\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}\\big(\\cdot\\big)$ gets incremented by atleast 1 during the iterations $T,\\;T+1,\\;\\ldots,\\;T+K$ As a result, we have, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T+K-1)-\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T-1)\\;\\geq\\;1.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Since iteration $T-1$ is not an exploration, we have $\\widetilde{N}_{a}(T-1)\\geq\\widetilde{N}_{a}(T-2)\\geq(T-1)^{\\alpha}$ Similarly, since iteration $T+K$ is an exploration, $\\mathrm{min}_{a\\in[K]}\\,\\widetilde{N}_{a}(T+K-1)<(T+K)^{\\alpha}$ Using these two observations, we have, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\langle\\;\\leq\\;(T+K)^{\\alpha}-(T-1)^{\\alpha}\\;\\leq\\;(T-1)^{\\alpha}\\times\\left(\\left(1+\\frac{K+1}{T-1}\\right)^{\\alpha}-1\\right)\\;\\leq\\;\\alpha(K+1)(T-1)^{-(1-\\alpha)}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Therefore, ", "page_idx": 52}, {"type": "equation", "text": "$$\nT\\,\\leq\\,(\\alpha(K+1))^{1/(1-\\alpha)}+1.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Let $M_{\\mathrm{explo}}=(\\alpha(K+1))^{1/(1-\\alpha)}+2$ Then from the above argument, if $T\\geq M_{\\mathrm{explo}}$ , the epoch of exploration starting at $T$ will last for at most $K$ iterations. ", "page_idx": 52}, {"type": "text", "text": "Let us assume that, the epoch begining from some $T_{i}\\geq M_{\\mathrm{explo}}$ lasts tilliteration $T_{f}$ . Therefore, we have, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{f}-1)\\;<\\;T_{f}^{\\alpha}\\quad\\mathrm{and}\\quad\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{i}-2)\\;\\geq\\;(T_{i}-1)^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Using this, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{f}-1)-\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{i}-2)\\ \\leq\\ T_{f}^{\\alpha}-(T_{i}-1)^{\\alpha}\\ \\leq\\ \\alpha(T_{i}-1)^{-(1-\\alpha)}(T_{f}-T_{i}+1).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "We argued earlie that $T_{f}-T_{i}\\leq K$ . We also have, $T_{i}-1\\geq M_{\\mathrm{explo}}-1\\geq(\\alpha(K+1))^{1/(1-\\alpha)}+1.$ Using this observations, we get ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{f}-1)-\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{i}-2)\\;\\leq\\;\\frac{\\alpha(K+1)}{((\\alpha(K+1))^{1/(1-\\alpha)}+1)^{1-\\alpha}}\\;<\\;1.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Since $\\begin{array}{r}{:\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{f})\\leq\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{f}-1)+1\\mathrm{~and~min}_{a\\in[K]}\\widetilde{N}_{a}(T_{i}-2)\\leq\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{i}-T_{f})+\\frac{1}{2}\\mathrm{~and~}_{a\\in[K]}\\widetilde{N}_{a}(T_{i}-T_{f}).}\\end{array}$ 1), we obtain, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{f})-\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{i}-1)\\;<\\;2,\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "implying $\\begin{array}{r}{\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{f})-\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(T_{i}-1)\\le1}\\end{array}$ which is possible only if every arm is pulled at most once in iterations $T_{i}$ \uff0c $T_{i}+1$ $T_{f}-1$ \uff0c $T_{f}$ ", "page_idx": 53}, {"type": "text", "text": "Statement 4: Let an epoch of exploration starts from $N\\geq M_{\\mathrm{explo}}$ and the next epoch starts from $N+T$ for some $T\\geq1$ The epoch starting from $N$ continues till atmost $\\operatorname*{min}\\{N+{\\bar{K}},\\,N+T-2\\}$ by statement 3. Moreover in that epoch, every arm gets pulled atmost once and $\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(\\cdot)$ increments by 1. Therefore, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N+T-1)-\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N-1)\\;\\geq\\;1.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Since iteration $N\\!-\\!1$ is not an exploration, we have, $\\begin{array}{r}{\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N\\!-\\!1)\\geq\\operatorname*{min}_{a\\in[K]}\\widetilde{N}_{a}(N\\!-\\!2)\\geq}\\end{array}$ $(N-1)^{\\alpha}$ . Since iteration $N+T$ is an exploration, we have $\\mathrm{min}_{a\\in[K]}\\,\\widetilde{N}_{a}(N+T-1)<(N+T)^{\\alpha}$ Using these in the above inequality, we obtain, ", "page_idx": 53}, {"type": "equation", "text": "$$\n1\\ \\leq\\ (N+T)^{\\alpha}-(N-1)^{\\alpha}\\ \\leq\\ \\alpha(N-1)^{-(1-\\alpha)}(T+1),\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which implies $\\begin{array}{r}{T\\ge\\frac{1}{\\alpha}(N-1)^{1-\\alpha}-1=\\Theta(N^{1-\\alpha}).}\\end{array}$ ", "page_idx": 53}, {"type": "text", "text": "Statement 5: Using statement 4, we can have a constant $C_{1}$ such that, ", "page_idx": 53}, {"type": "equation", "text": "$$\nN_{k}\\ \\geq\\ N_{k-1}+C_{1}N_{k-1}^{1-\\alpha},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "for $k\\geq1$ , where $N_{0}=1$ . We now inductively argue that, there exists some constant $C_{2}$ independent of $k$ and $N$ such that, $N_{k}\\geq N+C_{2}k^{1/\\alpha}$ for $k\\geq1$ ", "page_idx": 53}, {"type": "text", "text": "\u00b7 For $k=1$ , we choose $C_{2}\\leq1$ ", "page_idx": 53}, {"type": "text", "text": "\u00b7 Now for some $k\\geq2$ if $N_{k-1}\\geq N+C_{2}(k-1)^{1/\\alpha}$ , we have, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{k}\\ \\geq\\ N_{k-1}+C_{1}N_{k-1}^{1-\\alpha}\\ \\geq\\ N+C_{2}(k-1)^{1/\\alpha}+C_{1}(N+C_{2}(k-1)^{1/\\alpha})^{1-\\alpha}}\\\\ &{\\qquad\\geq\\ N+C_{2}k^{1/\\alpha}+\\left(C_{1}(N+C_{2}(k-1)^{1/\\alpha})^{1-\\alpha}-C_{2}(k^{1/\\alpha}-(k-1)^{1/\\alpha})\\right)}\\\\ &{\\qquad\\geq\\ N+C_{2}k^{1/\\alpha}+\\left(C_{1}(N+C_{2}(k-1)^{1/\\alpha})^{1-\\alpha}-\\cfrac{C_{2}}{\\alpha}k^{\\frac{1}{\\alpha}-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Upon choosing $\\begin{array}{r}{C_{2}\\leq\\left(\\frac{\\alpha C_{1}}{2^{\\frac{1}{\\alpha}-1}}\\right)^{1/\\alpha}}\\end{array}$ , since we already have $k\\geq2$ , we obtain ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{C_{1}(N+C_{2}(k-1)^{1/\\alpha})^{1-\\alpha}\\,\\geq\\,C_{1}\\left(C_{2}\\left(\\frac{k}{2}\\right)^{1/\\alpha}\\right)^{1-\\alpha}\\ =\\ \\left(C_{2}^{-\\alpha}\\frac{\\alpha C_{1}}{2^{\\frac{1}{\\alpha}-1}}\\right)\\frac{C_{2}}{\\alpha}k^{1/\\alpha}}\\\\ {\\ }&{\\ \\geq\\ \\frac{C_{2}}{\\alpha}k^{1/\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Therefore, $N_{k}\\geq N+C_{2}k^{\\frac{1}{\\alpha}}$ ", "page_idx": 53}, {"type": "text", "text": "Statement 6: If $N\\,\\geq\\,M_{\\mathrm{explo}}+T_{\\mathrm{epoch}}+1$ and $T\\geq1$ every epoch of explorations intersecting with the iterations $\\{N,\\ N+1,\\ ...\\,,\\ N+T-1,\\ N+T\\}$ has length atmost $K$ (by statement 3). Hence, every such epoch must have started on or after iteration $N-K$ . Let $N_{0}$ be the time when the first such epoch has started and the sequence $(N_{k})_{k\\geq1}$ be defined similar to statement 5. Then $N_{0}\\geq N-K$ and $N_{k}\\geq N_{0}+C_{2}k^{\\frac{1}{\\alpha}}\\geq N-K+C_{2}k^{\\frac{1}{\\alpha}}$ Now, if the $k$ -th epoch starting after $N_{0}$ intersects with the iterations $\\{N,\\ N+1,\\ .\\dots,\\ N+T-1,\\ N+T\\}$ , then, ", "page_idx": 54}, {"type": "equation", "text": "$$\nN+T\\;\\geq\\;N-K+C_{2}k^{\\frac{1}{\\alpha}},\\quad\\mathrm{which\\;implies},\\quad k\\;\\leq\\;\\frac{(T+K)^{\\alpha}}{C_{2}^{\\alpha}}\\;=\\;O(T^{\\alpha}).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Defnition G.7. We defne the constant $T_{e x p l o}=M_{e x p l o}+T_{e p o c h}+1,$ where the constants $M_{e x p l o}$ and $T_{e p o c h}$ are defined in Proposition G.5. ", "page_idx": 54}, {"type": "text", "text": "Lemma G.8. For $N\\geq T_{e x p l o}$ and $T\\geq1$ the no of times an arm $a\\in[K]$ is pulled for exploration by the algorithm during iterations $N$ \uff0c $N+1$ $N+T-1$ \uff0c $N+T$ ${\\cal O}(T^{\\alpha})$ ", "page_idx": 54}, {"type": "text", "text": "Proof. By statement 3 of Proposition G.5, for $N\\geq T_{\\mathrm{explo}}$ and $T\\geq1$ , every epoch of exploration intersecting with the set of iterations $N$ $N+1$ \uff0c $N+2$ \u00b7 $N+T$ is of length atmost $K$ . In every such epoch, every arm is pulled atmost once. As a result, no. of times an arm is pulled during the iterations $N$ $N+1$ $N+T$ is upper bounded by the no. of epoch of iterations intersecting with the set $\\{N,\\ N+1,\\ .\\dots,\\ N+T-1,\\ N+T\\}$ . The later quantity is ${\\cal O}(T^{\\alpha})$ by statement 6 of Proposition G.5. Hence the lemma stands proved. \u53e3 ", "page_idx": 54}, {"type": "text", "text": "G.5  Technical lemmas related to algorithmic allocations ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "In this appendix we prove several properties about the anchor function $(g)$ and the algorithmic allocations $\\widetilde{N}(N)=(\\widetilde{N}_{a}(N):a\\in[K])$ . We exploit the results proven in Appendix B and G.4 to prove that the following properties hold after a random time of finite expectation, ", "page_idx": 54}, {"type": "text", "text": "\u00b7 if the algorithm has $g\\neq0$ at some iteration $N$ , then $g$ crosses the value zero withing an $O(N)$ iterations, where the constant hidden in $O(\\cdot)$ is independent of the sample paths (in Lemma G.9), and \u00b7 every arm $a\\in[K]$ has $\\widetilde{N}_{a}(N)=\\Theta(N)$ samples (in Lemma G.13). ", "page_idx": 54}, {"type": "text", "text": "Each of the properties stated above are used extensively in the proofs of Proposition G.1 and G.2. ", "page_idx": 54}, {"type": "text", "text": "Definition G.8. We define the random time $T_{1}=\\operatorname*{max}\\{T_{0},T_{\\mathrm{explo}}\\}$ (where $\\boldsymbol{T_{\\mathrm{explo}}}$ and $T_{0}$ are defined, respectively, in Appendix G.4 and G.3). ", "page_idx": 54}, {"type": "text", "text": "Wecanhave $g$ far from the value zero at iteration $T_{1}$ $g$ will still be finite at $T_{1}$ because of exploration.   \nLemma G.9 bounds the no. of iterations the algorithm takes to reach the value zero. ", "page_idx": 54}, {"type": "text", "text": "Lemma G.9 (Upper bound to the time to reach $g\\,=\\,0$ 0.If $g(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))\\neq0$ at iteration $N\\geq T_{1}$ and ", "page_idx": 54}, {"type": "equation", "text": "$$\nJ=\\operatorname*{min}\\left\\{\\ t>0\\ \\ \\middle|\\ g\\left(\\widetilde{\\mu}(N+t),\\widetilde{N}(N+t)\\right)\\ \\ \\ a n d\\ \\ \\ g\\left(\\widetilde{\\mu}(N),\\widetilde{N}(N)\\right)\\ \\ \\ h a v e\\,o p p o s i t e\\,s i g n s\\ \\right\\},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "then there exists a constant $C_{1}>0$ independent of the sample paths such that $U\\leq C_{1}N$ ", "page_idx": 54}, {"type": "text", "text": "Proof. Using (10), for $N\\geq T_{1}$ , we have, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{g\\left(\\widetilde{\\mu}(N),\\widetilde{N}(N)\\right)\\;=\\;\\theta\\left(\\displaystyle\\sum_{a\\neq1}\\frac{\\widetilde{N}_{a}(N)^{2}}{\\widetilde{N}_{1}(N)^{2}}\\right)-1\\;=\\;\\theta\\left(\\left(\\displaystyle\\sum_{a\\neq1}\\frac{\\widetilde{N}_{a}(N)}{\\widetilde{N}_{1}(N)}\\right)^{2}\\right)-1}\\\\ &{}&{=\\;\\theta\\left(\\left(\\displaystyle\\left(\\frac{N}{\\widetilde{N}_{1}(N)}-1\\right)^{2}\\right)-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "We now consider two situations separately, ", "page_idx": 55}, {"type": "text", "text": "Case I: $g\\left(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N)\\right)>0$ :We know that, there is a constant ${\\cal D}_{1}>0$ , such that, ", "page_idx": 55}, {"type": "equation", "text": "$$\ng\\left(\\widetilde{\\mu}(N+t),\\widetilde{N}(N+t)\\right)\\;\\leq\\;D_{1}\\left(\\frac{N+t}{\\widetilde{N}_{1}(N+t)}-1\\right)^{2}-1,\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "for every $t\\geq1$ . Now for $t<U$ , the algorithm selects an alternative arm from $[K]/\\{1\\}$ only while exploring. Therefore, using statement 6 of Proposition G.5, we have a constant $c_{1}>0$ such that, $\\widetilde{N}_{1}(N+t)\\geq t-c_{1}t^{\\alpha}$ .Hence, ", "page_idx": 55}, {"type": "equation", "text": "$$\ng\\left(\\widetilde{\\mu}(N+t),\\widetilde{N}(N+t)\\right)\\ \\leq\\ D_{1}\\left(\\frac{N+t}{t-c_{1}t^{\\alpha}}-1\\right)^{2}-1\\ =\\ D_{1}\\left(\\frac{N+c_{1}t^{\\alpha}}{t-c_{1}t^{\\alpha}}\\right)^{2}-1\\quad\\mathrm{for}\\ \\d t<U.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Since $g\\left(\\widetilde{\\pmb{\\mu}}(N+U-1),\\widetilde{\\pmb{N}}(N+U-1)\\right)\\,\\geq\\,0$ RHS of the above inequality is non-negative at $t=U-\\dot{1}$ . After some algebraic manipulation, this implies, ", "page_idx": 55}, {"type": "equation", "text": "$$\nU-1-c_{1}(1+D_{1}^{1/2})(U-1)^{\\alpha}\\ \\leq\\ D_{1}^{1/2}N,\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Since LHS of the above inequality is linear in $U$ , we can find a constant $C_{11}$ such that $U\\le C_{11}N$ ", "page_idx": 55}, {"type": "text", "text": "Case II: $g\\left(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N)\\right)<0$ : Using (72), we have a constant $D_{2}$ such that, ", "page_idx": 55}, {"type": "equation", "text": "$$\ng\\left(\\widetilde{\\mu}(N+t),\\widetilde{N}(N+t)\\right)\\;\\geq\\;D_{2}\\left(\\frac{N+t}{\\widetilde{N}_{1}(N+t)}-1\\right)^{2}-1\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "for all $t\\geq1$ . Now for $t<U$ , the algorithm pulls arm 1 only for exploration. As a result, using statement 6 of Proposition G.5, we have, a constant $c_{1}>0$ such that $\\widetilde{N}_{1}(N+t)\\leq N+c_{1}t^{\\alpha}$ .Using this, ", "page_idx": 55}, {"type": "equation", "text": "$$\ng\\left(\\widetilde{\\mu}(N+t),\\widetilde{N}(N+t)\\right)~\\ge~D_{2}\\left(\\frac{N+t}{N+c_{1}t^{\\alpha}}-1\\right)^{2}-1~\\ge~D_{2}\\left(\\frac{t-c_{1}t^{\\alpha}}{N+c_{1}t^{\\alpha}}\\right)^{2}-1,\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "for $t<U$ .Since $g(\\widetilde{\\pmb{\\mu}}(N+U-1),\\widetilde{\\pmb{N}}(N+U-1))\\leq0$ the RHS of the above inequality is not positive at $t=U-1$ . After some algebraic manipulation, this implies, ", "page_idx": 55}, {"type": "equation", "text": "$$\nU-1-c_{1}(1+D_{2}^{-1/2})(U-1)^{\\alpha}\\le D_{2}^{-1/2}N.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Again the LHS of the above inequality is linear in $U$ . As a result, we can find a constant $C_{12}$ such that, $U\\leq C_{12}N$ ", "page_idx": 55}, {"type": "text", "text": "We take $C_{1}=\\operatorname*{max}\\{C_{11},C_{12}\\}$ and have $U\\leq C_{1}N$ ", "page_idx": 55}, {"type": "text", "text": "Lemma G.10, G.11, and G.12 are necessary for proving Lemma G.13, which says $\\widetilde{N}_{a}(N)=\\Theta(N)$ for every arm $a\\in[K]$ , after a random time of finite expectation. This property is essential for proving Proposition G.1 and G.2 stated earlier. ", "page_idx": 55}, {"type": "text", "text": "Lemma G.10. There exists constants $\\gamma_{1},\\gamma_{2}\\in(0,1)$ independent of the sample path,such that,for $N\\geq T_{1}$ whenever $g\\left(\\widetilde{\\mu}(N),\\widetilde{N}(N)\\right)$ and $g\\left(\\widetilde{\\pmb{\\mu}}(N+1),\\widetilde{\\pmb{N}}(N+1)\\right)$ have opposite signs, we have, ", "page_idx": 56}, {"type": "text", "text": "Proof. For $N\\geq T_{1}$ , we have constants $D_{1},D_{2}>0$ such that, ", "page_idx": 56}, {"type": "equation", "text": "$$\nD_{1}\\left(\\frac{N}{\\widetilde{N}_{1}(N)}-1\\right)^{2}-1\\;\\leq\\;g\\left(\\widetilde{\\mu}(N),\\widetilde{N}(N)\\right)\\;\\leq\\;D_{2}\\left(\\frac{N}{\\widetilde{N}_{1}(N)}-1\\right)^{2}-1\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "We consider only the situation where $g\\left(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N)\\right)\\geq0$ and $g\\left(\\widetilde{\\pmb{\\mu}}(N+1),\\widetilde{\\pmb{N}}(N+1)\\right)\\leq0.$ Extending this to the other case follows similar argument. ", "page_idx": 56}, {"type": "text", "text": "From $g(\\widetilde{\\pmb{\\mu}}(N+1),\\widetilde{\\pmb{N}}(N+1))\\leq0$ , we have, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{D_{1}\\left(\\displaystyle\\frac{N+1}{\\widetilde{N}_{1}(N+1)}-1\\right)^{2}-1\\le0,\\;\\mathrm{which\\;implies}\\;\\;}\\\\ &{}&{\\widetilde{N}_{1}(N+1)\\ge(1+D_{1}^{-1/2})^{-1}N.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Since $\\widetilde{N}_{1}(N)\\geq1$ (for exploration), we have $\\widetilde{N}_{1}(N+1)\\le\\widetilde{N}_{1}(N)+1\\le2\\widetilde{N}_{1}(N)$ . Therefore, using (73) we get $\\begin{array}{r}{\\widetilde{N}_{1}(N)\\ge\\frac{1}{2}(1+D_{1}^{-1/2})^{-1}N}\\end{array}$ and we can take $\\begin{array}{r}{\\gamma_{1}=\\frac{1}{2}(1+D_{1}^{-1/2})^{-1}}\\end{array}$ ", "page_idx": 56}, {"type": "text", "text": "Similarly from, $g(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))\\geq0$ , we have, ", "page_idx": 56}, {"type": "equation", "text": "$$\nD_{2}\\left(\\frac{N}{\\widetilde{N}_{1}(N)}-1\\right)^{2}-1\\ge0,\n$$", "text_format": "latex", "page_idx": 56}, {"type": "equation", "text": "$$\n\\widetilde{N}_{1}(N)\\le(1+D_{2}^{-1/2})^{-1}N.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Using (74), we have, $\\begin{array}{r}{\\sum_{a\\in[K]/\\{1\\}}\\widetilde{N}_{a}(N)\\geq(1+D_{2}^{1/2})^{-1}N.}\\end{array}$ This further implies ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in[K]/\\{1\\}}\\widetilde{N}_{a}(N)\\geq\\frac{1}{K-1}(1+D_{2}^{1/2})^{-1}N.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Hence we can take $\\begin{array}{r}{\\gamma_{2}=\\frac{1}{K-1}(1+D_{2}^{1/2})^{-1}}\\end{array}$ ", "page_idx": 56}, {"type": "text", "text": "Lemma G.11. There exists constants $M_{1}\\geq1$ $d_{m a x}\\in[1,\\infty)$ and $d_{m i n}\\in(0,1]$ independent of the sample path, such that, if $T_{2}$ is the time at which $g$ crosses zero after the iteration $\\operatorname*{max}\\{M_{1},T_{1}\\}$ \uff0c then, for $N>T_{2}$ , we have: ", "page_idx": 56}, {"type": "equation", "text": "$$\nd_{m i n}\\leq\\sum_{a\\neq1}\\frac{d(\\widetilde{\\mu}_{1}(N),\\widetilde{x}_{1,a}(N))}{d(\\widetilde{\\mu}_{a}(N),\\widetilde{x}_{1,a}(N))}\\leq d_{m a x}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Moreover,wehave $\\mathbb{E}_{\\mu}[T_{2}]<\\infty$ ", "page_idx": 56}, {"type": "text", "text": "Proof. Let, ", "page_idx": 56}, {"type": "equation", "text": "$$\nD(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))=\\sum_{a\\neq1}\\frac{d(\\widetilde{\\mu}_{1}(N),\\widetilde{x}_{1,a}(N))}{d(\\widetilde{\\mu}_{a}(N),\\widetilde{x}_{1,a}(N))}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Also for every $M\\geq1$ , we define $T_{1,M}=\\operatorname*{max}\\{M,T_{1}\\}$ and $T_{2,M}$ as the iteration at which $g$ crosses zero for the first time after the iteration $T_{1,M}$ , i.e., ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{2,M}\\overset{\\mathrm{def.}}{=}\\operatorname*{min}\\Big\\{N\\geq T_{1,M}\\Bigm|g(\\widetilde{\\mu}(N+1),\\widetilde{N}(N+1))\\mathrm{~and~}}\\\\ &{\\qquad\\qquad\\qquad g(\\widetilde{\\mu}(T_{1,M}),\\widetilde{N}(T_{1,M}))\\mathrm{~are~of~opposite~signs~}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Existence of $d_{\\mathrm{max}}$ If $g(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))\\leq0$ then $D(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{N}(N))$ is bounded above trivilly by 1. ", "page_idx": 57}, {"type": "text", "text": "Otherwise if $g(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))>0$ for some $N>T_{2,M}$ , let $T=\\operatorname*{max}\\{\\;t\\leq N\\;\\mid\\;g(\\widetilde{\\pmb{\\mu}}(t),\\widetilde{\\pmb{N}}(t))\\leq$ $0\\}$ i.e. the time before iteration $N$ when $g$ has crossed the level zero. As a result, $T\\geq T_{2,M}$ (by definition of $T_{2,M})$ , and by Lemma G.10, $\\widetilde{N}_{1}(T)\\ge\\gamma_{1}T$ for some $\\gamma_{1}\\,\\in\\,(0,1)$ . Let $S=N-T$ During iterations $T\\!+\\!1,\\ldots,T\\!+\\!S$ , since $g$ is positive, the algorithm pulls alternative arms in $[K]/\\{1\\}$ only for exploration. As a result, using statement 6 of Proposition G.5, we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\widetilde{N}_{1}(T+S)\\geq\\widetilde{N}_{1}(T)+S-c_{1}S^{\\alpha}\\geq\\gamma_{1}T+S-c_{1}S^{\\alpha}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "for some $c_{1}>0$ . Using (72), we have a constant $D_{2}>0$ such that, ", "page_idx": 57}, {"type": "equation", "text": "$$\nD(\\tilde{\\mu}(T+S),\\widetilde{N}(T+S))\\leq D_{2}\\left(\\frac{T+S}{\\gamma_{1}T+S-c_{1}S^{\\alpha}}-1\\right)^{2}=D_{2}\\left(\\frac{(1-\\gamma_{1})T+c_{1}S^{\\alpha}}{\\gamma_{1}T+S-c_{1}S^{\\alpha}}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "We take M11 = - $\\begin{array}{r}{M_{11}=-\\frac{2}{\\gamma_{1}}\\operatorname*{min}_{S\\geq0}(S-c_{1}S^{\\alpha})}\\end{array}$ It is easy to observe that for $T\\geq M_{11}$ and $S\\geq0$ the function of $T,S$ in the RHS is bounded above. Therefore, we take, ", "page_idx": 57}, {"type": "equation", "text": "$$\nd_{\\operatorname*{max}}=\\operatorname*{max}\\left\\{1,\\operatorname*{max}_{T\\geq M_{11},S\\geq0}D_{2}\\left(\\frac{(1-\\gamma_{1})T+c_{1}S^{\\alpha}}{\\gamma_{1}T+S-c_{1}S^{\\alpha}}\\right)^{2}\\right\\}<\\infty.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Hence, if $M\\geq M_{11}$ , we have $g(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))\\leq d_{\\operatorname*{max}}$ for every $N>T_{2,M}$ ", "page_idx": 57}, {"type": "text", "text": "Existence of $d_{\\mathrm{min}}$ :If $g(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))\\geq0$ , we have 1 has the trivial lower bound. Otherwise, if $g(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))<0$ , we define $T=\\operatorname*{max}\\{\\;t\\leq N\\;\\mid\\;g(\\widetilde{\\pmb{\\mu}}(t),\\widetilde{\\pmb{N}}(t))\\geq0\\;\\}$ and $S=N-T$ .As a result, $T\\geq T_{2,M}$ (by definition of $T_{2,M}$ ). By Lemma G.10, we have $\\widetilde{N}_{1}(T)\\,\\leq\\,(1-\\gamma_{2})N$ for some $\\gamma_{2}\\in(0,1)$ . Also, the algorithm pulls the first arm only for exploration during the iterations $T+1,T+2,\\dots,T+S$ , since $g$ is negative. Therefore, using statement 6 of Proposition G.5, we have, $\\widetilde{N}_{1}(T+S)\\leq\\widetilde{N}_{1}(T)+c_{1}S^{\\alpha}\\leq(1-\\gamma_{2})T+c_{1}S^{\\alpha}$ . By (72), we have $D_{1}>0$ such that, ", "page_idx": 57}, {"type": "equation", "text": "$$\nD(\\tilde{\\mu}(T+S),\\widetilde{N}(T+S))\\geq D_{1}\\left(\\frac{T+S}{(1-\\gamma_{2})T+c_{1}S^{\\alpha}}-1\\right)^{2}=D_{1}\\left(\\frac{\\gamma_{2}T+S-c_{1}S^{\\alpha}}{(1-\\gamma_{2})T+c_{1}S^{\\alpha}}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Let $\\begin{array}{r}{M_{12}=-\\frac{2}{\\gamma_{2}}\\operatorname*{min}_{S\\geq0}(S\\!-\\!c_{1}S^{\\alpha})}\\end{array}$ The function of $T,S$ in the RHS isbounedbelowbyapsitive constant for $T\\geq M_{12}$ and $S\\geq0$ . Let, ", "page_idx": 57}, {"type": "equation", "text": "$$\nd_{\\operatorname*{min}}=\\operatorname*{min}\\left\\{1,\\operatorname*{min}_{T\\geq M_{12},\\;S\\geq0}D_{1}\\left(\\frac{\\gamma_{2}T+S-c_{1}S^{\\alpha}}{(1-\\gamma_{2})T+c_{1}S^{\\alpha}}\\right)^{2}\\right\\}>0.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Then for $M\\geq M_{12}$ , we have $g(\\widetilde{\\pmb{\\mu}}(N),\\widetilde{\\pmb{N}}(N))\\geq d_{\\operatorname*{min}}>0$ for every $N>T_{2,M}$ ", "page_idx": 57}, {"type": "text", "text": "Now, upon taking $M_{1}\\,=\\,\\operatorname*{max}\\{M_{11},M_{12}\\}$ , and $T_{2}\\;=\\;T_{2,M_{1}}$ , the lemma follows. By Lemma G.9, we have a constant $C_{1}~>~0$ such that, $T_{2}\\ \\leq\\ C_{1}\\operatorname*{max}\\{M_{1},T_{1}\\}$ . As a result, $\\dot{\\mathbb{E}}_{\\pmb{\\mu}}[T_{2}]\\,\\leq$ $C_{1}(M_{1}+\\mathbb{E}_{\\mu}[T_{1}])<\\infty$ \u53e3 ", "page_idx": 57}, {"type": "text", "text": "Using a technique similar to the one used in the proof of Lemma G.10 using (72), Lemma G.11 implies the following corollary. We define the random time $T_{2}$ as the one introduced in Lemma G.11. ", "page_idx": 57}, {"type": "text", "text": "Corollary G.1. There exists constants $\\beta_{1},\\beta_{2}\\in(0,1)$ independent of the sample paths, such that, for every $N>T_{2}$ ", "page_idx": 57}, {"type": "text", "text": "Lemma G.12. There exists constants $\\gamma>0$ and $M_{2}\\geq1$ independent of the sample path, such that, for $N>\\operatorname*{max}\\{M_{2},T_{2}\\}+1,$ if the algorithm pulls some arm $a\\in[K]/\\{1\\}$ , then for every alternate arm $b\\in[K]/\\{1,a\\}$ $\\widetilde{N}_{b}(N)\\geq\\gamma\\widetilde{N}_{a}(N)$ ", "page_idx": 58}, {"type": "text", "text": "Proof. We consider two separate cases. ", "page_idx": 58}, {"type": "text", "text": "Case I: Iteration $N$ is an exploration: Then we have a constant $C_{1}$ (introduced in statement 2 of Proposition G.5) such that, ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\widetilde{N}_{b}(N)\\geq N^{\\alpha}-C_{1}\\geq\\widetilde{N}_{a}(N-1)-C_{1}=\\widetilde{N}_{a}(N)-1-C_{1}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "We also have $\\widetilde{N}_{a}(N)\\geq N^{\\alpha}-C_{1}$ . Now let $M_{2}\\geq2$ be chosen such that, $M_{2}^{\\alpha}>3C_{1}+2$ As a result, for $N\\geq\\operatorname*{max}\\{M_{2},T_{2}\\}$ we have, $\\widetilde{N}_{a}(N)\\geq N^{\\alpha}-C_{1}\\geq2(1+C_{1})$ . This together with (75) implies, $\\begin{array}{r}{\\widetilde{N}_{b}(N)\\ge\\frac{1}{2}\\widetilde{N}_{a}(N)}\\end{array}$ ", "page_idx": 58}, {"type": "text", "text": "Case II: Iteration $N$ is not an exploration: We consider the AT2 (1) and IAT2 (2) algorithms separately. ", "page_idx": 58}, {"type": "text", "text": "Case II.I AT2 Algorithm: If iteration $N$ is not an exploration, we have, $\\begin{array}{r}{\\mathcal{Z}_{a}(N-1)\\leq\\mathcal{Z}_{b}(N-1)}\\end{array}$ Using (11), for $N\\hat{\\geq}\\operatorname*{max}\\{M_{2},T_{2}\\}$ , we have constants $C_{2}$ and $C_{3}$ such that, ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{2}\\cfrac{\\widetilde{N}_{1}(N-1)\\cdot\\widetilde{N}_{a}(N-1)}{\\widetilde{N}_{1}(N-1)+\\widetilde{N}_{a}(N-1)}\\leq\\mathbb{Z}_{a}(N-1)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{Z}_{b}(N-1)\\leq C_{3}\\cfrac{\\widetilde{N}_{1}(N-1)\\cdot\\widetilde{N}_{b}(N-1)}{\\widetilde{N}_{1}(N-1)+\\widetilde{N}_{b}(N-1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "As a result, we have, ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\widetilde{N}_{a}(N-1)\\le\\frac{C_{3}}{C_{2}}\\frac{\\widetilde{N}_{1}(N-1)+\\widetilde{N}_{a}(N-1)}{\\widetilde{N}_{1}(N-1)+\\widetilde{N}_{b}(N-1)}\\widetilde{N}_{b}(N-1)\\overset{(1)}{\\le}\\frac{2C_{3}}{\\beta_{1}C_{2}}\\widetilde{N}_{b}(N-1)\\le\\frac{2C_{3}}{\\beta_{1}C_{2}}\\widetilde{N}_{b}(N),\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where, for (1), we first note that $N>T_{2}+1$ . As a result, using Corollary (G. 1), $\\widetilde{N}_{1}(N-1)\\geq$ $\\beta_{1}(N\\mathrm{~-~}1)\\quad(\\beta_{1}\\mathrm{~\\in~}(0,1)$ is the constant introduced in Corollary G.1), and on the other hand $\\widetilde{N}_{1}(N-1)+\\widetilde{N}_{b}(N-1)\\le2(N-1)$ .Hence, we get ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\widetilde{N}_{b}(N)\\geq\\frac{\\beta_{1}C_{2}}{2C_{3}}\\widetilde{N}_{a}(N-1)=\\frac{\\beta_{1}C_{2}}{2C_{3}}(\\widetilde{N}_{a}(N)-1).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Again using statement 2 of Proposition G.5, $\\widetilde{N}_{a}(N)\\;\\geq\\;N^{\\alpha}\\;-\\;C_{1}\\;\\geq\\;M_{2}^{\\alpha}\\;-\\;C_{1}\\;\\geq\\;2$ (since $M_{2}^{\\alpha}\\geq3C_{1}+2)$ . As a result, ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\widetilde{N}_{b}(N)\\geq\\frac{\\beta_{1}C_{2}}{4C_{3}}\\widetilde{N}_{a}(N).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Taking $\\begin{array}{r}{\\gamma=\\operatorname*{min}\\left\\{\\frac{1}{2},\\frac{\\beta_{1}C_{2}}{4C_{3}}\\right\\}}\\end{array}$ ", "page_idx": 58}, {"type": "text", "text": "Case II.II IAT2 Algorithm: In this case, we have, ", "text_level": 1, "page_idx": 58}, {"type": "equation", "text": "$$\n\\log(\\widetilde{N}_{a}(N-1))+\\mathbb{Z}_{a}(N-1)\\ \\leq\\ \\log(\\widetilde{N}_{b}(N-1))+\\mathbb{Z}_{b}(N-1).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Using (11), we have constants $C_{2}$ and $C_{3}$ such that, ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad C_{2}\\cfrac{\\widetilde{N}_{1}(N-1)\\cdot\\widetilde{N}_{a}(N-1)}{\\widetilde{N}_{1}(N-1)+\\widetilde{N}_{a}(N-1)}\\leq\\log(\\widetilde{N}_{a}(N-1))+\\mathbb{Z}_{a}(N-1)}\\\\ &{\\leq\\mathbb{Z}_{b}(N-1)+\\log(\\widetilde{N}_{b}(N-1))\\leq C_{3}\\cfrac{\\widetilde{N}_{1}(N-1)\\cdot\\widetilde{N}_{b}(N-1)}{\\widetilde{N}_{1}(N-1)+\\widetilde{N}_{b}(N-1)}+\\frac{1}{e}\\widetilde{N}_{b}(N-1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "As a result, using the same arguments used for the AT2 algorithm in Case II.1, we can conclude, ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\widetilde{N}_{b}(N)\\geq\\frac{\\beta_{1}}{4}\\left(\\frac{C_{3}}{C_{2}}+\\frac1{e}\\right)^{-1}\\widetilde{N}_{a}(N),\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "for $N>\\operatorname*{max}\\{M_{2},T_{2}\\}+1$ ", "page_idx": 59}, {"type": "text", "text": "Now taking $\\begin{array}{r}{\\gamma=\\operatorname*{min}\\left\\{\\frac{1}{2},\\frac{\\beta_{1}}{4}\\left(\\frac{C_{2}}{C_{3}}+\\frac{1}{e}\\right)^{-1}\\right\\}}\\end{array}$ wehavidrlt r ", "page_idx": 59}, {"type": "text", "text": "Definition G.9. We define the random time $T_{3}\\,=\\,\\operatorname*{max}\\{M_{2},T_{2}\\}\\,+\\,2$ where $M_{2}$ is the constant introduced in Lemma G.12, and $T_{2}$ is the random time defined in Lemma G.11. ", "page_idx": 59}, {"type": "text", "text": "Definition G.10. We define the random time $T_{4}$ as $\\begin{array}{r}{T_{4}\\,=\\,\\frac{1}{\\beta_{2}}(T_{3}+1)}\\end{array}$ ,where $\\beta_{2}\\,\\in\\,(0,1)$ is the constant introduced in Corollary G.1. ", "page_idx": 59}, {"type": "text", "text": "Note that $\\mathbb{E}_{\\mu}[T_{3}]<\\infty$ since $\\mathbb{E}_{\\mu}[T_{2}]<\\infty$ . For the same reason, by Defnition G.10, $T_{4}$ satisfies $\\mathbb{E}_{\\mu}[T_{4}]<\\dot{\\infty}$ since $\\mathbb{E}_{\\mu}[T_{3}]<\\infty$ ", "page_idx": 59}, {"type": "text", "text": "Lemma G.13 (Sufficient sampling). For every $N\\geq T_{4}$ wehave $\\widetilde{N}_{a}(N)\\,=\\,\\Theta(N)$ forevery $a\\in[K]$ ", "page_idx": 59}, {"type": "text", "text": "Proof. For $a=1$ and every $N\\geq T_{4}$ , we have $\\widetilde{N}_{1}(N)=\\Theta(N)$ by Corollary G.1. ", "page_idx": 59}, {"type": "text", "text": "Otherwise for $a\\neq1$ and every $N\\geq T_{4}$ , we define $A_{N}^{\\prime}=\\arg\\operatorname*{max}_{b\\in[K]/\\{1\\}}\\;\\widetilde{N}_{b}(N)$ . By Corollary G.1, ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\widetilde{N}_{A_{N}^{\\prime}}(N)\\;\\ge\\;\\beta_{2}N\\;\\ge\\;\\beta_{2}T_{4}\\;=\\;T_{3}+1.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Therefore, arm $A_{N}^{\\prime}$ must have been pulled by the algorithm somewhere between iterations $T_{3}$ and $N$ . Let us define the time $N^{\\prime}$ to be the last time before $N$ when $A_{N}^{\\prime}$ was pulled. Then $N^{\\prime}\\geq T_{3}>\\operatorname*{max}\\{M_{2},T_{2}\\}+1$ . As a result, using Lemma G.12, we have, ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\widetilde{N}_{a}(N^{\\prime})\\;\\geq\\;\\gamma\\widetilde{N}_{A_{N}^{\\prime}}(N^{\\prime})\\;\\stackrel{\\mathrm{(i)}}{=}\\;\\gamma\\widetilde{N}_{A_{N}^{\\prime}}(N),\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where (i) follows by definition of $N^{\\prime}$ . Since $N\\geq N^{\\prime}$ , we have $\\widetilde{N}_{a}(N)\\geq\\widetilde{N}_{a}(N^{\\prime})$ .As a result, we obtain, for every $\\dot{a}\\in[K]/\\{1\\}$ and $N\\geq T_{4}$ \uff0c ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\widetilde{N}_{a}(N)\\;\\ge\\;\\gamma\\widetilde{N}_{A_{N}^{\\prime}}(N)\\;=\\;\\gamma\\operatorname*{max}_{b\\in[K]/\\{1\\}}\\widetilde{N}_{b}(N)\\;\\ge\\;\\gamma\\beta_{2}N,\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where $\\beta_{2}\\in(0,1)$ is the constant mentioned in Corollary G.1. Hence we have $\\widetilde{N}_{a}(N)=\\varOmega(N)$ for $N\\geq T_{4}$ and the lemma follows. \u53e3 ", "page_idx": 59}, {"type": "text", "text": "G.6Proving Lemma G.2 ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "In this appendix, we prove Lemma G.2 from Appendix G.1.2. For improved clarity, we reiterate Lemma G.2 from Appendix G.1.2. ", "page_idx": 59}, {"type": "text", "text": "Statement of Lemma G.2. For both AT2 and IAT2 algorithms, there exists constants $M_{5}\\geq1$ and $C_{1}>0$ independent of the sample paths,such that for every $N\\geq\\operatorname*{max}\\{M_{5},T_{6}\\}$ , if the algorithm picks an arm $a\\in[K]/\\{1\\}$ at iteration $N$ then it again picks arm a within thenext $\\left\\lceil C_{1}N^{1-3\\alpha/8}\\right\\rceil$ iterations. ", "page_idx": 59}, {"type": "text", "text": "For every arm $a\\in[K]$ , iteration $N\\geq1$ and $R\\geq1$ , we define the following quantities, ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varDelta\\tilde{N}_{a}(N,R)\\;=\\;\\tilde{N}_{a}(N+R)-\\tilde{N}_{a}(N),}\\\\ &{\\quad\\;b(N,R)\\;=\\;\\arg\\underset{j\\in[K]/\\{1\\}}{\\operatorname*{max}}\\;\\frac{\\varDelta\\tilde{N}_{j}(N,R)}{\\tilde{N}_{j}(N)},\\quad\\mathrm{and}}\\\\ &{\\quad\\tau(N,R)\\;=\\;\\operatorname*{max}\\{\\;t\\leq N+R\\mid A_{t}=b(N,R)\\;\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "For proving Lemma G.2, we need the three technical lemmas: Lemma G.14, G.15 and G.16. ", "page_idx": 59}, {"type": "text", "text": "Lemma G.14. For $N\\geq T_{6}$ $T_{6}$ is the random time defined in the statement of Proposition G.1 and itsatisfies $\\mathbb{E}_{\\mu}[T_{6}]<\\infty,$ $w e$ have, ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\frac{\\Delta\\widetilde{N}_{1}(N,R)}{\\Delta\\widetilde{N}_{b(N,R)}(N,R)}\\le\\frac{\\widetilde{N}_{1}(N)}{\\widetilde{N}_{b(N,R)}(N)}+O\\left(R N^{-1}+(\\Delta\\widetilde{N}_{b(N,R)}(N,R))^{-1}N^{1-3\\alpha/8}(1+R N^{-1})^{3}\\right).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Proof. In the proof, for readability, we use $b$ to denote $b(N,R)$ . Also, for every arm $a\\in[K]$ weuse $\\varDelta\\widetilde N_{a}$ to denote $\\varDelta\\widetilde{N}_{a}(N,R)$ ", "page_idx": 60}, {"type": "text", "text": "By Proposition G.1, we have a constant $C\\,>\\,0$ independent of the sample paths, such that for $N\\geq T_{6}$ ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\left|\\;g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N+R))-g(\\pmb{\\mu},\\widetilde{\\pmb{N}}(N))\\;\\right|\\;\\leq\\;2C N^{-3\\alpha/8}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Therefore, applying the mean value theorem, we have, ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\left|\\,\\frac{\\partial g}{\\partial N_{1}}(\\pmb{\\mu},\\hat{\\pmb{N}})\\Delta\\widetilde{N}_{1}+\\sum_{\\pmb{a}\\in[K]/\\{1\\}}\\frac{\\partial g}{\\partial N_{a}}(\\pmb{\\mu},\\hat{\\pmb{N}})\\Delta\\widetilde{N}_{a}\\,\\right|\\ \\leq\\ 2C N^{-3\\alpha/8},\n$$", "text_format": "latex", "page_idx": 60}, {"type": "equation", "text": "$$\n\\dot{\\mathbf{\\Psi}}=(\\hat{N}_{a})_{a\\in[K]},\\ \\mathrm{with}\\ \\ \\hat{N}_{a}\\in\\Bigl[\\widetilde{N}_{a}(N),\\widetilde{N}_{a}(N+R)\\Bigr]\\ \\ \\mathrm{for}\\ \\mathrm{every}\\;a\\in[K].\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Now expanding the partial derivatives of $g$ with respect to $N_{a}$ 's for $a\\in[K]$ ,weget ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\Delta\\tilde{N}_{1}\\sum_{\\substack{a\\in[K]/\\{1\\}}}f(\\mu,a,\\hat{N})\\frac{\\hat{N}_{a}\\varDelta_{a}}{(\\hat{N}_{1}+\\hat{N}_{a})^{2}}-\\sum_{\\substack{a\\in[K]/\\{1\\}}}f(\\mu,a,\\hat{N})\\varDelta\\tilde{N}_{a}\\frac{\\hat{N}_{1}\\varDelta_{a}}{(\\hat{N}_{1}+\\hat{N}_{a})^{2}}\\right|\\leq2C N^{-3\\alpha/8},\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where, $f(\\mu,a,\\hat{N})$ were defined in (13) of Appendix B. ", "page_idx": 60}, {"type": "text", "text": "Letting ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\hat{p}_{j}=\\frac{f(\\mu,j,\\hat{N})\\frac{\\hat{N}_{j}\\Delta_{j}}{(\\hat{N}_{1}+\\hat{N}_{j})^{2}}}{\\sum_{a\\neq1}f(\\mu,a,\\hat{N})\\frac{\\hat{N}_{a}\\Delta_{a}}{(\\hat{N}_{1}+\\hat{N}_{a})^{2}}}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "for every arm $j\\in[K]/\\{1\\}$ , we have, ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\left|\\frac{\\Delta\\widetilde{N}_{1}}{\\hat{N}_{1}}-\\sum_{a\\neq1}\\hat{p}_{a}\\frac{\\Delta\\widetilde{N}_{a}}{\\hat{N}_{a}}\\right|\\ \\leq\\ 2C N^{-3\\alpha/8}\\times\\hat{N}_{1}^{-1}\\left(\\sum_{a\\in[K]/\\{1\\}}f(\\mu,a,\\hat{N})\\frac{\\hat{N_{a}}\\varDelta_{a}}{(\\hat{N_{1}}+\\hat{N_{a}})^{2}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "We know from (14) of Appendix B that $\\begin{array}{r}{f(\\mu,a,N)=\\Theta\\left(\\frac{N_{a}}{N_{1}}\\left(1+\\frac{N_{a}}{N_{1}}\\right)^{2}\\right)}\\end{array}$ As a result, ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\hat{N}_{1}^{-1}\\left(\\sum_{a\\neq1}f(\\mu,a,\\hat{N})\\frac{\\hat{N}_{a}\\varDelta_{a}}{(\\hat{N}_{1}+\\hat{N}_{a})^{2}}\\right)^{-1}=\\theta\\left(\\frac{\\hat{N}_{1}^{2}}{\\sum_{a\\in[K]/\\{1\\}}\\hat{N}_{a}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "By Lemma G.13, we have $\\hat{N}_{a}\\ge\\widetilde{N}_{a}=\\Theta(N)$ and $\\hat{N}_{1}\\leq N+R$ for $N\\geq T_{6}$ . As a result, we get, ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\widetilde{N}_{1}^{-1}\\left(\\sum_{a\\neq1}f(\\mu,a,\\hat{\\mathbf{N}})\\frac{\\hat{N_{a}}\\varDelta_{a}}{(\\hat{N_{1}}+\\hat{N_{a}})^{2}}\\right)^{-1}=\\,O\\left((1+R N^{-1})^{2}\\right).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Putting this in (78), we obtain ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\left|~\\frac{\\Delta\\widetilde{N_{1}}}{\\hat{N_{1}}}-\\sum_{a\\neq1}\\hat{p}_{a}\\frac{\\Delta\\widetilde{N_{a}}}{\\hat{N_{a}}}~\\right|~=~O\\left(N^{-3\\alpha/8}(1+R N^{-1})^{2}\\right),\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "which further implies, ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\mathcal{A}\\widetilde{N}_{1}}{\\hat{N}_{1}}\\ \\le\\ \\sum_{a\\ne1}\\hat{p}_{a}\\frac{\\Delta\\widetilde{N_{a}}}{\\hat{N}_{a}}+O\\left(N^{-3\\alpha/8}(1+R N^{-1})^{2}\\right)}}\\\\ &{\\le\\ \\sum_{a\\ne1}\\hat{p}_{a}\\frac{\\Delta\\widetilde{N_{a}}}{\\widetilde{N_{a}}}+O\\left(N^{-3\\alpha/8}(1+R N^{-1})^{2}\\right)\\quad(\\mathrm{since~}\\widetilde{N}_{a}\\ge\\hat{N}_{a}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Let $b=\\arg\\operatorname*{max}_{a\\neq1}\\ \\frac{\\Delta\\widetilde{N}_{a}}{\\widetilde{N}_{a}}$ a The above inequality implies, ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\frac{\\Delta\\widetilde{N_{1}}}{\\hat{N_{1}}}\\,\\le\\,\\frac{\\Delta\\widetilde{N_{b}}}{\\widetilde{N_{b}}}+O\\left(N^{-3\\alpha/8}(1+R N^{-1})^{2}\\right).\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Now multiplying both sides by $\\hat{N_{1}}/\\Delta\\widetilde{N_{b}}$ , we get, ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\varDelta\\widetilde{N_{1}}}{\\varDelta\\widetilde{N_{b}}}\\,\\le\\,\\frac{\\hat{N_{1}}}{\\widetilde{N_{b}}}+O\\left(\\varDelta\\widetilde{N_{b}}^{-1}\\hat{N}_{1}N^{-3\\alpha/8}(1+R N^{-1})^{2}\\right)}}\\\\ &{}&\\\\ &{\\le\\,\\frac{\\hat{N_{1}}}{\\widetilde{N_{b}}}+O\\left(\\varDelta\\widetilde{N_{b}}^{-1}N^{1-3\\alpha/8}(1+R N^{-1})^{3}\\right)\\quad(\\mathrm{since~}\\hat{N}_{1}\\le N+R).}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "$\\hat{N}_{1}\\leq\\widetilde{N_{1}}+R$ $\\widetilde{N}_{b}=\\varOmega(N)$ ", "page_idx": 61}, {"type": "text", "text": "LemmaG.15.Thereexistsconstants $M_{51}~\\geq~1$ $D_{1},D_{2}\\mathrm{~\\,~>~\\,~0~}$ independentofthesample paths such that, for $N\\ \\geq\\ \\operatorname*{max}\\{M_{51},T_{6}\\}$ , we have $\\lceil D_{1}N^{1-3\\alpha/8}\\rceil\\ \\<\\ N$ ,and for all $R\\ \\in$ $\\{\\lceil D_{1}N^{1-3\\alpha/8}\\rceil$ \uff0c $\\lceil D_{1}N^{1-3\\alpha/8}\\rceil+1,\\ldots,\\;N\\}$ , we have $\\Delta\\widetilde{N}_{b(N,R)}(N,R)\\geq D_{2}R$ ", "page_idx": 61}, {"type": "text", "text": "Proof. To simplify notation, we adopt $b$ to denote $b(N,R)$ and, for every arm $j\\in[K]$ ,we use $\\varDelta N_{j}$ to denote $\\varDelta\\widetilde{N}_{j}(N,R)$ ", "page_idx": 61}, {"type": "text", "text": "By (76) of Lemma G.14, there exists a constant $D_{3}>0$ independent of the sample paths, such that, for all $N\\geq T_{6}$ and $R\\in\\{1,2,\\ldots,N\\}$ \uff0c ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\Delta\\widetilde{N}_{1}\\!\\!\\!}&{\\le}&{\\!\\!\\!\\Delta\\widetilde{N}_{b}\\times\\left(\\frac{\\widetilde{N}_{1}}{\\widetilde{N}_{b}}+D_{3}\\left(1+\\varDelta\\widetilde{N}_{b}^{-1}N^{1-3\\alpha/8}\\right)\\right)\\quad\\mathrm{(we~have~}R N^{-1}\\le1\\mathrm{)}}\\\\ &{\\le}&{\\!\\!\\!\\left(\\frac{\\widetilde{N}_{1}}{\\widetilde{N}_{b}}+D_{3}\\right)\\varDelta\\widetilde{N}_{b}+D_{3}N^{1-3\\alpha/8}}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Since $\\widetilde{N}_{1}(N)$ and $\\widetilde{N}_{b}(N)$ are both $\\Theta(N)$ (by Lemma G.13), we have a constant ${\\cal D}_{4}>0$ such that, ( +D3\u2264D forevery N\u2265T. Uing this in the above inequalty, we get, ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\Delta\\widetilde{N}_{1}\\le D_{4}\\varDelta\\widetilde{N}_{b}+D_{3}N^{1-3\\alpha/8}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "We take $D_{1}=2D_{3}$ and $M_{51}\\geq1$ to be the smallest number such that, every $N\\geq M_{51}$ satisfies $\\left\\lceil D_{1}N^{1-3\\alpha/8}\\right\\rceil<N$ ", "page_idx": 61}, {"type": "text", "text": "Now if $N\\geq\\operatorname*{max}\\{M_{51},T_{6}\\}$ and $R\\in\\{\\lceil D_{1}N^{1-3\\alpha/8}\\rceil,\\ \\lceil D_{1}N^{1-3\\alpha/8}\\rceil+1,\\ \\dots,\\ N\\}.$ from (79) we get, ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\Delta\\widetilde{N}_{1}\\leq D_{4}\\varDelta\\widetilde{N}_{b}+D_{3}N^{1-3\\alpha/8}}\\\\ {\\leq D_{4}\\varDelta\\widetilde{N}_{b}+\\frac{R}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "By definition of $b$ , we have, ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\frac{\\widetilde{N}_{a}}{\\widetilde{N}_{b}}\\varDelta\\widetilde{N}_{b}\\;\\geq\\;\\varDelta\\widetilde{N}_{a}\\quad\\mathrm{for}\\;\\mathrm{every}\\;a\\in[K]/\\{1\\}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Again, since $\\widetilde{N}_{a}(N)=\\Theta(N)$ for every $a\\in[K]$ (by Lemma G.13), there exists a constant $D_{5}>0$ independent of the sample paths such that, ", "page_idx": 62}, {"type": "equation", "text": "$$\nD_{5}\\varDelta\\widetilde{N}_{b}\\;\\geq\\;\\varDelta\\widetilde{N}_{a}\\quad\\mathrm{for}\\;\\mathrm{every}\\;a\\in[K]/\\{1,b\\}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "As a result, ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R\\,=\\,\\displaystyle\\sum_{a\\in[K]/\\{1,b\\}}{\\Delta\\tilde{N}_{a}}+{\\Delta\\tilde{N}_{1}}+{\\Delta\\tilde{N}_{b}}}\\\\ &{\\mathrm{~\\\\\\}\\leq\\,\\displaystyle(K-2)D_{5}\\,\\Delta\\tilde{N}_{b}+D_{4}\\varDelta\\tilde{N}_{b}+\\frac{R}{2}+\\varDelta\\tilde{N}_{b}\\quad\\mathrm{(using~(80))}}\\\\ &{\\leq\\,\\displaystyle(1+(K-2)D_{5}+D_{4}){\\Delta\\tilde{N}_{b}}+\\frac{R}{2},}\\\\ {\\implies\\Delta\\tilde{N}_{b}\\,\\geq\\,\\displaystyle\\frac{1}{2(1+(K-2)D_{5}+D_{4})}R.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Now taking $\\begin{array}{r}{D_{2}=\\frac{1}{2(1+(K-2)D_{5}+D_{4})}>0}\\end{array}$ we have the desired conclusion. ", "page_idx": 62}, {"type": "text", "text": "Lemma G.16. For AT2 $(I)$ and IAT2 (2) algorithms, there exists constants $C_{3},C_{4}>0$ independent of the sample paths, such that, for $N\\geq T_{6}$ and $R\\in\\{1,2,\\ldots,N\\}$ if the algorithm pulls some arm $a\\in[K]/\\{1\\}$ at iteration $N$ and doesn't pull $a$ for the next $R$ iterations, then, ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\cal A}T2\\colon}&{{\\cal T}_{a}(N+R)-{\\cal Z}_{b(N,R)}(N+R)\\;\\le\\;-\\;{\\cal C}_{3}\\varDelta\\widetilde N_{b(N,R)}(N,R)+{\\cal C}_{4}N^{1-3\\alpha/8}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\;{\\cal O}\\left(\\left(N^{-3\\alpha/8}+R N^{-1}\\right)R\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\cal U}{\\cal T}2\\colon}&{\\mathcal{Z}_{a}^{(m)}(N+R)-\\mathcal{Z}_{b(N,R)}^{(m)}(N+R)\\;\\le\\;-\\;C_{3}\\varDelta\\widetilde{N}_{b(N,R)}(N,R)+C_{4}N^{1-3\\alpha/8}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\;O\\left(\\left(N^{-3\\alpha/8}+R N^{-1}\\right)R\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where for every alternative arm $j\\in[K]/\\{1\\}$ ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{j}^{(m)}(N)\\;=\\;\\mathcal{Z}_{j}(N)+\\log(\\widetilde{N}_{j}(N))\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "is the modified empirical index of that arm at iteration $N$ ", "page_idx": 62}, {"type": "text", "text": "Proof. For cleaner presentation, we use $b$ to denote $b(N,R)$ . We also use ${\\widetilde{\\mu}}_{j}$ \uff0c $\\widetilde{x}_{1,j}$ \uff0c $\\varDelta N_{j}$ and $\\widetilde{N}_{j}$ respectively, to denote $\\widetilde{\\mu}_{j}(N)$ \uff0c $\\widetilde{x}_{1,j}(N)$ \uff0c $\\varDelta\\widetilde{N}_{j}(N,R)$ and $\\widetilde{N}_{j}(N)$ for every arm $j\\in[K]$ , whenever it doesn't cause confusion. ", "page_idx": 62}, {"type": "text", "text": "AT2 Algorithm: Let ", "text_level": 1, "page_idx": 62}, {"type": "equation", "text": "$$\nS_{a,b}({\\widetilde{\\cal N}}(N),{\\widetilde{\\pmb\\mu}}(N))\\;=\\;{\\mathbb Z}_{a}(N)-{\\mathbb Z}_{b}(N)\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "denote the the difference between the empirical indexes of the two arms $a$ and $b$ at iteration $N$ . Note that $S_{a,b}(\\widetilde{N},\\widetilde{\\mu})$ depends only on the tuple of variables $(\\widetilde{N}_{1},\\widetilde{N}_{a},\\widetilde{N}_{b},\\widetilde{\\mu}_{1},\\widetilde{\\mu}_{a},\\widetilde{\\mu}_{b})$ . In the following argument, we apply mean value theorem over $S_{a,b}$ and the empirical indexes $\\mathcal{Z}_{a}$ and ${{\\mathcal{T}}_{b}}$ , treating them as functions of $\\widetilde{N}_{1},\\widetilde{N}_{a},\\widetilde{N}_{b},\\widetilde{\\mu}_{1},\\widetilde{\\mu}_{a},$ and $\\widetilde{\\mu}_{b}$ ", "page_idx": 62}, {"type": "text", "text": "Using the multivariate mean value theorem, we have, ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{S_{a,b}(\\widetilde{\\pmb{N}}(\\boldsymbol{N}+\\boldsymbol{R}),\\widetilde{\\pmb{\\mu}}(\\boldsymbol{N}+\\boldsymbol{R}))-S_{a,b}(\\widetilde{\\pmb{N}}(\\boldsymbol{N}),\\widetilde{\\pmb{\\mu}}(\\boldsymbol{N}))\\ =\\ \\sum_{j=1,a,b}\\frac{\\partial S_{a,b}}{\\partial\\mu_{j}}(\\hat{\\pmb{N}},\\hat{\\pmb{\\mu}})\\Delta\\widetilde{\\mu}_{j}}}\\\\ &{}&{+\\,\\frac{\\partial S_{a,b}}{\\partial N_{1}}(\\hat{\\pmb{N}},\\hat{\\pmb{\\mu}})\\cdot\\Delta\\widetilde{N}_{1}+\\frac{\\partial S_{a,b}}{\\partial N_{b}}(\\hat{\\pmb{N}},\\hat{\\pmb{\\mu}})\\cdot\\Delta\\widetilde{N}_{b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where $\\varDelta\\widetilde{\\mu}_{j}=\\widetilde{\\mu}_{j}(N\\!+\\!R)-\\widetilde{\\mu}_{j}(N)$ for $j=1,a,b$ and $(\\hat{N},\\hat{\\mu})=(\\hat{N}_{1},\\hat{N}_{a},\\hat{N}_{b},\\hat{\\mu}_{1},\\hat{\\mu}_{a},\\hat{\\mu}_{b})$ such that, for $j=1,a,b,\\hat{\\mu}_{j}$ lies between $\\widetilde{\\mu}_{j}(N)$ and $\\widetilde{\\mu}_{j}(N+R)$ , and $\\hat{N}_{j}$ lies in $\\left[\\widetilde{N}_{j}(N),\\widetilde{N}_{j}(N+R)\\right]$ Note that there is no contribution on the RHS in (83) due to $\\widetilde{N}_{a}$ , because $\\widetilde{N}_{a}(\\cdot)$ doesn't change during ", "page_idx": 62}, {"type": "text", "text": "iterations $N+1,N+2,\\dots,N+R$ owing to our assumption that the algorithm doesn't pull arm $a$ during the mentioned iterations. ", "page_idx": 63}, {"type": "text", "text": "First we consider the partial derivatives of $S_{j,b}$ with respect to $\\mu_{1},\\mu_{j}$ and $\\mu_{b}$ in (83). We have ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\frac{\\partial S_{a,b}}{\\partial\\mu_{1}}(\\hat{\\cal N},\\hat{\\mu})\\;=\\;\\hat{\\cal N}_{1}\\left(d_{1}(\\hat{\\mu}_{1},\\hat{x}_{1,a})-d_{1}(\\hat{\\mu}_{1},\\hat{x}_{1,b})\\right),\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where C1,j = $\\begin{array}{r}{\\hat{x}_{1,j}=\\frac{\\hat{N}_{1}\\hat{\\mu}_{1}+\\hat{N}_{j}\\hat{\\mu}_{j}}{\\hat{N}_{1}+\\hat{N}_{j}}}\\end{array}$ Np+Nipi for j = a, b. ", "page_idx": 63}, {"type": "text", "text": "Since $R\\leq N$ , we have for $j=1,a,b$ $\\hat{N}_{j}\\le\\tilde{N}_{j}+N=\\Theta(N)$ (by Lemma G.13). As a result, using 7 o Apendix B, both $d_{1}(\\hat{\\mu}_{1},\\hat{x}_{1,a})$ and $d_{1}(\\hat{\\mu}_{1},\\hat{x}_{1,b})$ are $\\Theta(1)$ Using tis, $\\left|\\frac{\\partial S_{a,b}}{\\partial\\mu_{1}}(\\hat{N},\\hat{\\mu})\\right|$ .is $O(N)$ ", "page_idx": 63}, {"type": "text", "text": "Similarly, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\frac{\\partial S_{a,b}}{\\partial\\mu_{a}}(\\hat{\\boldsymbol{N}},\\hat{\\boldsymbol{\\mu}})=\\hat{N}_{a}d_{1}(\\hat{\\mu}_{a},\\hat{x}_{1,a})\\;\\mathrm{and}\\;\\frac{\\partial S_{a,b}}{\\partial\\mu_{b}}(\\hat{\\boldsymbol{N}},\\hat{\\boldsymbol{\\mu}})=-\\hat{N}_{b}d_{1}(\\hat{\\mu}_{b},\\hat{x}_{1,b}).\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Using the same argument as for the partial derivative of $S_{a,b}$ withrespectto $\\mu_{1}$ ,wehave $\\begin{array}{r}{\\left|\\frac{\\partial S_{a,b}}{\\partial\\mu_{j}}(\\hat{N},\\hat{\\pmb\\mu})\\right|=O(N)}\\end{array}$ $j=a,b$ ", "page_idx": 63}, {"type": "text", "text": "Therefore,the contribution in the RHS of (83) due to the noisiness in the empirical means $\\widetilde{\\mu}_{1},\\widetilde{\\mu}_{a},\\widetilde{\\mu}_{b}$ is bounded above by, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j=1,a,b}\\frac{\\partial S_{a,b}}{\\partial\\mu_{j}}(\\hat{\\pmb{N}},\\hat{\\pmb{\\mu}})\\Delta\\widetilde{\\mu}_{j}~\\leq}&{\\displaystyle\\sum_{j=1,a,b}\\left|\\frac{\\partial S_{a,b}}{\\partial\\mu_{j}}(\\hat{\\pmb{N}},\\hat{\\pmb{\\mu}})\\right|\\cdot|\\Delta\\widetilde{\\mu}_{j}|}\\\\ &{\\displaystyle\\stackrel{(1)}{=}\\cal O(\\ensuremath{N})\\times{\\cal O}(\\ensuremath{N}^{-3\\alpha/8})}\\\\ &{=\\displaystyle~{\\cal O}(\\ensuremath{N}^{1-3\\alpha/8}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where (1) follows since for $N\\geq T_{0}$ and $j=1,a,b,|\\varDelta\\widetilde{\\mu}_{j}|=O(N^{-3\\alpha/8})$ ", "page_idx": 63}, {"type": "text", "text": "Now considering the partial derivative of $S_{a,b}(\\cdot)$ with respect to $N_{1}$ ,we get ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\frac{\\partial S_{a,b}}{\\partial N_{1}}(\\hat{N},\\hat{\\pmb{\\mu}})\\;=\\;d(\\hat{\\mu}_{1},\\hat{x}_{1,a})-d(\\hat{\\mu}_{1},\\hat{x}_{1,b}).\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Using Lemma G.13, $\\widetilde{N}_{j}(N)$ and $\\widetilde{N}_{j}(N+R)$ are both $\\Theta(N)$ for all $j=1,a,b$ and $N\\geq T_{6}$ , since $R\\leq N$ . As a result, using (7), (8), we have, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial^{2}S_{a,b}}{\\partial\\mu_{j}\\partial N_{k}}(\\boldsymbol{N}^{\\prime},\\boldsymbol{\\mu}^{\\prime})\\right|=O(1),\\quad\\mathrm{and}\\quad\\left|\\frac{\\partial^{2}S_{a,b}}{\\partial N_{j}\\partial N_{k}}(\\boldsymbol{N}^{\\prime},\\boldsymbol{\\mu}^{\\prime})\\right|=O(N^{-1})\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "for every $j,k\\;\\;\\in\\;\\;\\{1,a,b\\}$ \uff0cand tuple $(N^{\\prime},\\mu^{\\prime})\\;\\;=\\;\\;(N_{1}^{\\prime},N_{a}^{\\prime},N_{b}^{\\prime},\\mu_{1}^{\\prime},\\mu_{a}^{\\prime},\\mu_{b}^{\\prime})$ having $\\begin{array}{r l}{N_{i}^{\\prime}}&{{}\\in}\\end{array}$ $\\left[\\widetilde{N}_{i}(N),\\widetilde{N}_{i}(N+R)\\right]$ and $\\mu_{i}^{\\prime}$ lying between $\\hat{\\mu}_{i}$ and ${\\widetilde{\\mu}}_{i}$ , for every $i=1,a,b$ ", "page_idx": 63}, {"type": "text", "text": "Therefore, applying the mean value theorem, we get, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\frac{\\partial S_{a,b}}{\\partial N_{1}}(\\hat{N},\\hat{\\pmb{\\mu}})\\;=\\;d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b})+O\\left(\\sum_{j=1,a,b}|\\hat{\\mu}_{j}-\\widetilde{\\mu}_{j}|\\right)+O\\left(N^{-1}\\sum_{j=1,b}\\Delta\\widetilde{N}_{j}\\right)\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Similarly, considering the partial derivative with respect to $N_{b}$ ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\frac{\\partial S_{a,b}}{\\partial N_{b}}(\\hat{N},\\hat{\\mu})\\ =\\ -\\,d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})+O\\left(N^{-3\\alpha/8}+R N^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Now, using all these upper bounds in the RHS of (83), we obtain ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{a,b}(\\widetilde{N}(N+R),\\widetilde{\\mu}(N+R))-S_{a,b}(\\widetilde{N}(N),\\widetilde{\\mu}(N))}\\\\ &{\\leq\\ A\\widetilde{N}_{1}\\cdot(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b}))-\\varDelta\\widetilde{N}_{b}\\cdot d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})}\\\\ &{\\quad+\\ O\\Big(N^{1-3\\alpha/8}+R(N^{-3\\alpha/8}+R N^{-1})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Since arm $a$ was pulled in iteration $N$ , we have $\\begin{array}{r}{\\mathcal{Z}_{a}(N-1)\\leq\\mathcal{Z}_{b}(N-1)}\\end{array}$ . Also since arm $b$ was not pulled in iteration $N$ , its empirical index remains unchanged, i.e. $\\mathcal{I}_{b}(N-1)=\\mathcal{I}_{b}(N)$ Combining these two observations, we have $\\mathcal{Z}_{a}(N-1)\\le\\mathcal{Z}_{b}(N)$ ", "page_idx": 64}, {"type": "text", "text": "As a result, ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{a,b}(\\widetilde{N}(N),\\widetilde{\\mu}(N))\\ =\\ Z_{a}(N)-Z_{b}(N)}\\\\ &{=\\ ({\\mathbb Z}_{a}(N)-{\\mathbb Z}_{a}(N-1))+({\\mathbb Z}_{a}(N-1)-{\\mathbb Z}_{b}(N))}\\\\ &{\\le\\ Z_{a}(N)-{\\mathbb Z}_{a}(N-1)}\\\\ &{=\\ I_{a}(N)-I_{a}(N-1)+O(N^{1-3\\alpha/8})\\quad\\mathrm{(using\\;Lemma\\;G.3)}}\\\\ &{\\le\\ d(\\mu_{a},\\mu_{1})+O(N^{1-3\\alpha/8})=O(N^{1-3\\alpha/8}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "where the last step follows from the fact that, the partial derivatives of $I_{a}(\\cdot)$ with respect to $N_{a}$ is $d(\\mu_{a},x_{1,a}(N))^{'}\\leq d(\\mu_{a},\\mu_{1})$ . As a result, since arm $a$ was pulled in iteration $N$ , the increment $I_{a}(N)-I_{a}(N-1)$ in $I_{a}(\\cdot)$ is upper bounded by $d(\\mu_{a},\\mu_{1})$ \uff1a ", "page_idx": 64}, {"type": "text", "text": "Therefore (85) implies, ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{S_{a,b}(\\widetilde{N}(N+R),\\widetilde{\\mu}(N+R))\\!\\!\\!}&{\\le\\:\\Delta\\widetilde{N}_{1}\\cdot(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b}))-\\varDelta\\widetilde{N}_{b}\\cdot d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})}\\\\ &{+\\,S_{a,b}(\\widetilde{N}(N),\\widetilde{\\mu}(N))+O\\left(\\left(N^{-3\\alpha/8}+R N^{-1}\\right)R\\right)}\\\\ &{\\le\\:\\Delta\\widetilde{N}_{1}\\cdot(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b}))-\\varDelta\\widetilde{N}_{b}\\cdot d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})}\\\\ &{+\\:O\\left(N^{1-3\\alpha/8}+\\left(N^{-3\\alpha/8}+R N^{-1}\\right)R\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Now we consider two possibilities: ", "page_idx": 64}, {"type": "text", "text": "Case I $d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b})<0$ : In this case, by (85), $S_{a,b}(\\widetilde{N}(N+R),\\widetilde{\\pmb{\\mu}}(N+R))$ is upper bounded by, ", "page_idx": 64}, {"type": "equation", "text": "$$\n-\\varDelta\\widetilde{N}_{b}\\cdot d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})+O\\left(N^{1-3\\alpha/8}+\\left(N^{-3\\alpha/8}+R N^{-1}\\right)R\\right).\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "By Lemma G.13 and (6), since both $\\widetilde{N}_{1}(N)$ and $\\widetilde{N}_{b}(N)$ are $\\Theta(N)$ , we have $d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})=\\Theta(1)$ .As a result, (81) follows. ", "page_idx": 64}, {"type": "text", "text": "Case II $d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b})\\geq0$ : In this case, the RHS of (85) can be rewritten as, ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\ \\!\\Delta\\widetilde{N}_{b}\\left(\\frac{\\Delta\\widetilde{N}_{1}}{\\Delta\\widetilde{N}_{b}}(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b}))-d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})\\right)+O\\left(N^{1-3\\alpha/8}+\\left(N^{-3\\alpha/8}+R N^{-1}\\right)R\\right).\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Using (76) in Lemma G.14, the upper bound becomes ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Delta\\widetilde{N}_{b}\\Big(\\frac{\\widetilde{N}_{1}}{\\widetilde{N}_{b}}(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b}))-d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})\\Big)}\\\\ &{\\displaystyle+\\,O\\Big(N^{1-3\\alpha/8}(1+R N^{-1})^{3}+\\Big(N^{-3\\alpha/8}+R N^{-1}\\Big)\\,R\\Big)}\\\\ &{\\displaystyle=\\Delta\\widetilde{N}_{b}\\,\\Bigg(\\frac{\\widetilde{N}_{1}}{\\widetilde{N}_{b}}(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b}))-d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})\\Bigg)}\\\\ &{\\displaystyle+\\,O\\left(N^{1-3\\alpha/8}+\\Big(N^{-3\\alpha/8}+R N^{-1}\\Big)\\,R\\Big)\\quad\\mathrm{(since~}R\\le N\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "By (86), we have, ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{a}(N)\\,\\,\\le\\,\\,\\mathcal{Z}_{b}(N)+O(N^{1-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Expanding the empirical index terms, we get, ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\widetilde{N}_{1}d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})+\\widetilde{N}_{a}d(\\widetilde{\\mu}_{a},\\widetilde{x}_{1,a})\\ \\leq\\ \\widetilde{N}_{1}d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b})+\\widetilde{N}_{b}d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})+O(N^{1-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "By Lemma G.13 we have $\\widetilde{N}_{b}=\\Theta(N)$ .As a result, upon dividing both sides of the above inequality by $\\widetilde{N}_{b}$ and after some re-arrangement of terms, we obtain, ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\frac{\\widetilde{N}_{1}}{\\widetilde{N}_{b}}(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,j})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b}))-d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})\\ \\le\\ -\\ \\frac{\\widetilde{N_{j}}}{\\widetilde{N}_{b}}d(\\widetilde{\\mu}_{j},\\widetilde{x}_{1,j})+O(N^{-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Using the above inequality in (89), we get the upper bound, ", "page_idx": 65}, {"type": "equation", "text": "$$\n-\\varDelta\\widetilde{N}_{b}\\left(\\frac{\\widetilde{N}_{j}}{\\widetilde{N}_{b}}d(\\widetilde{\\mu}_{a},\\widetilde{x}_{1,a})\\right)+O\\left(N^{1-3\\alpha/8}+\\left(N^{-3\\alpha/8}+R N^{-1}\\right)R\\right)\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "By Lemma G.13 and 6), we have, $\\begin{array}{r}{{\\frac{\\widetilde{N}_{j}}{\\widetilde{N}_{b}}}d(\\widetilde{\\mu}_{j},\\widetilde{x}_{1,j})=\\Theta(1)}\\end{array}$ Therefoe 8I folws ", "page_idx": 65}, {"type": "text", "text": "IAT2 Algorithm: The proof of (82) for IAT2 is very similar to the proof of (81) for AT2. We first consider the difference of the modified empirical indexes between the two arms $a$ and $b$ ", "page_idx": 65}, {"type": "equation", "text": "$$\nS_{a,b}^{(m)}({\\widetilde N}(N),{\\widetilde\\mu}(N))\\;=\\;{\\mathcal{Z}}_{a}^{(m)}(N)-{\\mathcal{Z}}_{b}^{(m)}(N).\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Following a similar procedure as the AT2 algorithm, we apply the mean value theorem to obtain, ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{S_{a,b}^{(m)}(\\widetilde{N}(N+R),\\widetilde{\\pmb{\\mu}}(N+R))-S_{a,b}^{(m)}(\\widetilde{N}(N),\\widetilde{\\pmb{\\mu}}(N))~=~\\displaystyle\\sum_{j=1,a,b}\\frac{\\partial S_{a,b}^{(m)}}{\\partial\\mu_{j}}(\\hat{N},\\hat{\\pmb{\\mu}})\\cdot\\Delta\\widetilde{\\mu}_{j}}&{}\\\\ {+\\,\\displaystyle\\frac{\\partial S_{a,b}^{(m)}}{\\partial N_{1}}(\\hat{N},\\hat{\\pmb{\\mu}})\\cdot\\Delta\\widetilde{N}_{1}+\\frac{\\partial S_{a,b}^{(m)}}{\\partial N_{b}}(\\hat{N},\\hat{\\pmb{\\mu}})\\cdot\\Delta\\widetilde{N}_{b},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where $\\begin{array}{r}{\\varDelta\\widetilde{\\mu}_{j}=\\widetilde{\\mu}_{j}(N+R)-\\widetilde{\\mu}_{j}(N)}\\end{array}$ , and $(\\hat{N},\\hat{\\mu})=(\\hat{N}_{1},\\hat{N}_{a},\\hat{N}_{b},\\hat{\\mu}_{1},\\hat{\\mu}_{a},\\hat{\\mu}_{b})$ \uff0csuch that, $\\hat{\\mu}_{j}$ lies between $\\widetilde{\\mu}_{j}(N)$ and $\\widetilde{\\mu}_{j}(N+R)$ , and $\\hat{N}_{j}\\in\\left[\\widetilde{N}_{j}(N),\\widetilde{N}_{j}(N+R)\\right]$ , for every $j=1,a,b$ ", "page_idx": 65}, {"type": "text", "text": "The contribution to (92) due to noise in $\\widetilde{\\pmb{\\mu}}$ $O(N^{1-3\\alpha/8})$ by the same argument we proved (84) for AT2. ", "page_idx": 65}, {"type": "text", "text": "Now we consider the partial derivatives of S(m) with respect to $N_{1},N_{a},N_{b}$ . Following the same steps as used for AT2 algorithm, we have, ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial S_{a,b}^{(m)}}{\\partial N_{1}}(\\hat{\\cal N},\\hat{\\mu})\\,=\\,\\frac{\\partial S_{a,b}}{\\partial N_{1}}(\\hat{\\cal N},\\hat{\\mu})\\,=\\,d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b})+\\displaystyle{\\cal O}(N^{-3\\alpha/8}+R N^{-1}),}\\\\ {\\displaystyle\\frac{\\partial S_{a,b}^{(m)}}{\\partial N_{b}}(\\hat{\\cal N},\\hat{\\mu})\\,=\\,\\frac{\\partial S_{a,b}}{\\partial N_{b}}(\\hat{\\cal N},\\hat{\\mu})-\\frac{1}{\\hat{\\cal N}_{b}}\\,\\le\\,\\,-d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})+\\displaystyle{\\cal O}(N^{-3\\alpha/8}+R N^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "As a result, the contribution to (92) due to $\\widetilde{N}_{1}$ and $\\widetilde{N}_{b}$ is, ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\frac{\\partial S_{a,b}^{(m)}}{\\partial N_{1}}(\\hat{N},\\hat{\\mu})\\cdot\\varDelta\\widetilde{N}_{1}+\\frac{\\partial S_{a,b}^{(m)}}{\\partial N_{b}}(\\hat{N},\\hat{\\mu})\\cdot\\varDelta\\widetilde{N}_{b}}\\\\ {\\leq\\:\\Delta\\widetilde{N}_{1}\\cdot(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b}))-\\varDelta\\widetilde{N}_{b}\\cdot d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})+O\\left(\\left(N^{-3\\alpha/8}+R N^{-1}\\right)R\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Therefore, adding the contributions of the noise in $(\\widetilde{\\mu}_{j})_{j=1,a,b}$ , (92) can be further upper bounded by, ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{a,b}^{(m)}(\\widetilde{N}(N+R),\\widetilde{\\mu}(N+R))-S_{a,b}^{(m)}(\\widetilde{N}(N),\\widetilde{\\mu}(N))}\\\\ &{\\leq\\,\\varDelta\\widetilde{N}_{1}\\cdot(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b}))-\\varDelta\\widetilde{N}_{b}\\cdot d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})}\\\\ &{\\quad\\quad+\\,O\\left(N^{1-3\\alpha/8}+\\left(N^{-3\\alpha/8}+R N^{-1}\\right)R\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Now.w $S_{a,b}^{(m)}(\\widetilde{N}(N),\\widetilde{\\pmb{\\mu}}(N))$ $a$ $b$ $N$ ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}_{a}^{(m)}(N-1)\\;\\le\\;\\mathcal{Z}_{b}^{(m)}(N-1)\\;=\\;\\mathcal{Z}_{b}^{(m)}(N).}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Therefore, ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{S_{a,b}^{(m)}(\\widetilde{N}(N),\\widetilde{\\mu}(N))\\ =\\ \\mathcal{T}_{a}^{(m)}(N)-\\mathcal{T}_{b}^{(m)}(N)}\\\\ {\\ =\\ (\\mathcal{T}_{a}^{(m)}(N)-\\mathcal{T}_{a}^{(m)}(N-1))+(\\mathcal{T}_{a}^{(m)}(N-1)-\\mathcal{T}_{b}^{(m)}(N))}\\\\ {\\ \\le\\ \\mathcal{T}_{a}^{(m)}(N)-\\mathcal{T}_{a}^{(m)}(N-1)}\\\\ {\\ =\\ \\mathcal{Z}_{a}(N)-\\mathcal{T}_{a}(N-1)+\\log\\Bigg(\\frac{\\widetilde{N}_{a}(N)}{\\widetilde{N}_{a}(N-1)}\\Bigg)}\\\\ {\\ \\overset{(1)}{=}\\ \\mathcal{T}_{a}(N)-\\mathcal{T}_{a}(N-1)+O(1)}\\\\ {\\ \\overset{(2)}{=}\\ O(N^{1-3\\alpha/8}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "where (1) follows from the fact that $\\widetilde{N}_{a}(N)\\,=\\,\\Theta(N)$ for $N\\,\\geq\\,T_{6}$ , and (2) follows using the arguments used while bounding $\\mathcal{Z}_{a}(N)-\\mathcal{Z}_{a}(N-1)$ in (86). ", "page_idx": 66}, {"type": "text", "text": "Putting this in (93), we get, ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{a,b}^{(m)}(\\widetilde{N}(N+R),\\widetilde{\\mu}(N+R))\\le\\varDelta\\widetilde{N}_{1}\\cdot(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b}))-\\varDelta\\widetilde{N}_{b}\\cdot d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\textit{O}\\Big(N^{1-3\\alpha/8}+R(N^{-3\\alpha/8}+R N^{-1})\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Now there can be two cases. ", "page_idx": 66}, {"type": "text", "text": "Case I $d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})\\gets d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b})<0$ : Then, using the same argument as was used for Case I of AT2 algorithm, we get (82). ", "page_idx": 66}, {"type": "text", "text": "Case II $d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})\\!-\\!d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b})\\geq0$ : Using (76) of Lemma G.12, the RHS in (95) is upper bounded by, ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\;\\;\\Delta\\widetilde{N}_{b}\\left(\\frac{\\widetilde{N}_{1}}{\\widetilde{N}_{b}}(d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,a})-d(\\widetilde{\\mu}_{1},\\widetilde{x}_{1,b}))-d(\\widetilde{\\mu}_{b},\\widetilde{x}_{1,b})\\right)+O\\left(N^{1-3\\alpha/8}+\\left(N^{-3\\alpha/8}+R N^{-1}\\right)R\\right).\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Using (94), we have ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}_{a}^{(m)}(N)\\,\\le\\,\\mathcal{Z}_{b}^{(m)}(N)+O\\left(N^{1-3\\alpha/8}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "which implies, ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathbb Z_{a}(N)\\ \\le\\ \\mathbb Z_{b}(N)+\\log\\left(\\frac{\\widetilde N_{b}}{\\widetilde N_{a}}\\right)+O(N^{1-3\\alpha/8})\\ \\stackrel{(1)}{=}\\ \\mathbb Z_{b}(N)+O(N^{1-3\\alpha/8}),\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "where (1) fllows from the fct that $\\widetilde{N}_{a}(N)$ and $\\widetilde{N}_{b}(N)$ are both $\\Theta(N)$ causing $\\begin{array}{r}{\\log\\Big(\\frac{\\widetilde N_{a}(N)}{\\widetilde N_{b}(N)}\\Big)=O(1)}\\end{array}$ for $N\\geq T_{6}$ . Note that the above inequality is same as (90) obtained in Case $\\mathrm{II}$ of AT2. Now (82) follows using exactly the same argument as in the Case $\\mathrm{II}$ of AT2 after (90). \u53e3 ", "page_idx": 66}, {"type": "text", "text": "For every $a\\in[K]/\\{1\\}$ \uff0c $N\\geq T_{6}$ and $R\\in\\{1,2,\\ldots,N\\}$ , we have, ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{Z_{a}(N+R)-Z_{a}(N+R-1)|}&{\\overset{(1)}{=}\\;|I_{a}(N+R)-I_{a}(N+R-1)|+O((N+R)^{1-3\\alpha/8})}\\\\ &{\\overset{(2)}{=}\\;|I_{a}(N+R)-I_{a}(N+R-1)|+O(N^{1-3\\alpha/8})}\\\\ &{\\overset{(3)}{\\leq}\\;\\operatorname*{max}\\{d(\\mu_{1},\\mu_{a}),d(\\mu_{a},\\mu_{1})\\}+O(N^{1-3\\alpha/8})=O(N^{1-3\\alpha/8}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where: (1) follows by Lemma G.3; (2) follows since $R\\leq N$ ; and (3) follows from the fact that $I_{a}(\\cdot)$ can increment by atmost max $\\mathfrak{c}\\{d(\\mu_{1},\\mu_{a}),d(\\mu_{a},\\mu_{1})\\}$ in one iteration. ", "page_idx": 67}, {"type": "text", "text": "Since, at every iteration $N\\geq1$ and for every arm, the modified empirical index differ from the empirical index by atmost $\\log(N)$ ,wehave, ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big\\vert\\mathcal{Z}_{a}^{(m)}(N+R)-\\mathcal{Z}_{a}^{(m)}(N+R-1)\\Big\\vert\\ \\le\\ \\vert\\mathcal{Z}_{a}(N+R)-\\mathcal{Z}_{a}(N+R-1)\\vert+2\\log(N+R)}\\\\ {\\overset{\\mathrm{(i)}}{\\le}\\ \\vert\\mathcal{Z}_{a}(N+R)-\\mathcal{Z}_{a}(N+R-1)\\vert+O(\\log(N))}\\\\ {\\ =\\ O(N^{1-3\\alpha/8})+O(\\log(N))\\ =\\ O(N^{1-3\\alpha/8}),\\quad(1-\\alpha)\\le N}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where (i) follows because $R\\leq N$ ", "page_idx": 67}, {"type": "text", "text": "Using (96) and (97), the following corollary follows from Lemma G.16, ", "page_idx": 67}, {"type": "text", "text": "Corollary G.2. For AT2 $(I)$ and IAT2 (2) algorithms, for $N\\geq T_{6}$ and $R\\in\\{1,2,\\ldots,N\\}$ ,ifthe algorithm pulls some arm $a\\in[K]/\\{1\\}$ at iteration $N$ and doesn't pull a for the next $R$ iterations, then, ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{A T2:}}&{{{\\cal{Z}}_{a}(N+R-1)-{\\cal{Z}}_{b(N,R)}(N+R-1)\\;\\le\\;-\\;{\\cal{C}}_{3}{\\Delta}\\widetilde{N}_{b(N,R)}(N,R)+{\\cal{C}}_{5}N^{1-3\\alpha/8}}}\\\\ {{}}&{{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+\\;{\\cal{O}}\\left(\\Big(N^{-3\\alpha/8}+R N^{-1}\\Big)\\,R\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{Z}_{a}^{(m)}(N+R-1)-\\mathbb{Z}_{b(N,R)}^{(m)}(N+R-1)\\;\\le\\;-\\;C_{3}\\Delta\\widetilde{N}_{b(N,R)}(N,R)+C_{5}N^{1-3\\alpha/8}}}\\\\ &{}&{+\\;O\\left(\\left(N^{-3\\alpha/8}+R N^{-1}\\right)R\\right),\\qquad\\left(\\mathbb{S}_{b(N,R)}^{2}\\middle)\\in\\Omega_{c}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where $C_{3},C_{5}>0$ are constants independent of the sample paths ", "page_idx": 67}, {"type": "text", "text": "Proof of Lemma G.2: We prove the proposition for the AT2 algorithm. The proof for IAT2 algorithm follows the exact same argument by replacing $\\mathcal{T}$ With $\\mathcal{I}^{(m)}$ ", "page_idx": 67}, {"type": "text", "text": "In the proof, we argue via contradiction. We show that, there exists constants $M_{5}\\geq1$ and $C_{1}>0$ independent of the sample paths, such that for $N\\geq\\operatorname*{max}\\{M_{5},T_{6}\\}$ , if the algorithm pulls some arm $a\\in[K]/\\{1\\}$ at iteration $N$ and doesn't pull t for the next $R(N)\\stackrel{\\mathrm{def.}}{=}\\lceil C_{1}N^{1-3\\alpha/8}\\rceil$ iterations, then at iteration $\\bar{\\tau}(\\dot{N},R(N))$ , the algorithm pulls arm $b(N,R(N))$ , even though arm $a$ has empirical index strictly less than that of arm $\\bar{b}(N,R(\\bar{N}))$ ", "page_idx": 67}, {"type": "text", "text": "Using Corollary G.2, there exists constants $C_{3},C_{5},C_{6}>0$ independent of the sample paths, such that, for $N\\geq T_{6}$ and $R\\in\\{1,2,\\ldots,N\\}$ \uff0c ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{Z}_{a}(N+R-1)-\\mathbb{Z}_{b(N,R)}(N+R-1)\\;\\le\\;-\\,C_{3}\\varDelta\\tilde{N}_{b(N,R)}(N,R)+C_{4}N^{1-3\\alpha/8}}\\\\ &{\\;+\\,C_{5}R(N^{-3\\alpha/8}+R N^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "We define ", "page_idx": 67}, {"type": "equation", "text": "$$\nC_{1}=\\operatorname*{max}\\left\\{D_{1},\\frac{2C_{4}}{D_{2}C_{3}}\\right\\}+1,\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where $D_{1}$ and $D_{2}$ are the constants introduced in Lemma G.15. Let $M_{52}\\geq1$ to be the smallest number such that, every $N\\geq M_{52}$ satisfies $\\left\\lceil C_{1}N^{1-3\\alpha/8}\\right\\rceil<N$ Since $C_{1}>D_{1}$ ,by the definition of $M_{51}$ in the proof of Lemma G.15, we have $M_{52}>M_{51}$ ", "page_idx": 68}, {"type": "text", "text": "Nowlet $R(N)=\\lceil C_{1}N^{1-3\\alpha/8}\\rceil$ .Then by Lemma G.15, for $N\\geq\\operatorname*{max}\\{M_{52},T_{6}\\}$ since $R(N)>$ $\\lceil D_{1}N^{1-3\\alpha/8}\\rceil$ we have, ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\Delta\\widetilde N_{b(N,R(N))}(N,R(N))\\;\\geq\\;{\\cal D}_{2}R(N)\\;\\geq\\;\\frac{2C_{4}}{C_{3}}N^{1-3\\alpha/8}.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Since arm $b(N,R)$ is not pulled between the iterations $\\tau(N,R)$ and $N+R$ wehave ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\Delta\\widetilde{N}_{b(N,R)}(N,t)\\;=\\;\\varDelta\\widetilde{N}_{b(N,R)}(N,R)}\\\\ {t\\in\\{\\;\\tau(N,R)-N,\\;\\;\\tau(N,R)-N+1,\\;\\;...\\,,\\;\\;R\\;\\}.}\\end{array}\n$$for all ", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "As a result, by definition of $b(N,R)$ we have, ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{b(N,R(N))\\:=\\:b(N,\\tau(N,R(N))-N)\\quad\\mathrm{and}}&{}\\\\ {\\Delta\\widetilde{N}_{b(N,R(N))}(N,R(N))\\:=\\:\\Delta\\widetilde{N}_{b(N,R)}(N,\\tau(N,R(N))-N).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "In the rest of the proof, we denote $b(N,R(N))$ and $\\tau(N,R(N))-N$ respectively, using $b(N)$ and $\\tau_{b}(N)$ ", "page_idx": 68}, {"type": "text", "text": "Therefore, using Corollary G.2, we have, for $N\\geq\\operatorname*{max}\\{M_{52},T_{6}\\}$ ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{Z}_{a}(N+\\tau_{b}(N)-1)-\\mathbb{Z}_{b(N)}(N+\\tau_{b}(N)-1)}\\\\ &{\\leq\\;-\\;C_{3}\\varDelta\\tilde{N}_{b(N)}(N,\\tau_{b}(N))+C_{4}N^{1-3\\alpha/8}+C_{5}\\tau_{b}(N)\\times\\big(N^{-3\\alpha/8}+\\tau_{b}(N)N^{-1}\\big)}\\\\ &{\\stackrel{(1)}{\\leq}\\;-\\;C_{3}\\times\\displaystyle\\frac{2C_{4}}{C_{3}}N^{1-3\\alpha/8}+C_{4}N^{1-3\\alpha/8}+C_{5}R(N)\\times\\big(N^{-3\\alpha/8}+R(N)N^{-1}\\big)}\\\\ &{\\stackrel{(2)}{\\leq}\\;-\\;C_{4}N^{1-\\alpha/8}+C_{5}(C_{1}+1)(C_{1}+2)N^{1-3\\alpha/8}\\times N^{-3\\alpha/8}}\\\\ &{\\stackrel{(3)}{=}\\;-\\;(C_{4}-C_{6}N^{-3\\alpha/8})N^{1-3\\alpha/8},}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where: (1) follows using (101) and $\\tau_{b}(N)\\leq R(N)$ (2) follows since $R(N)\\leq(C_{1}+1)N^{-3\\alpha/8}$ and (3) follows by letting $C_{6}=C_{5}(C_{1}+1)(C_{1}+2)$ ", "page_idx": 68}, {"type": "text", "text": "We now take $M_{53}\\geq1$ to be lare enugh, suh that, $C_{6}M_{53}^{-3\\alpha/8}<C_{4}$ Let $M_{5}=\\operatorname*{max}\\{M_{52},M_{53}\\}$ Then (102) implies, for $N\\geq\\operatorname*{max}\\{M_{5},T_{6}\\}$ and $R(N)=\\left\\lceil C_{1}N^{1-3\\alpha/8}\\right\\rceil$ , if the algorithm picks some arm $a\\in[K]/\\bar{\\{}1\\}$ at iteration $N$ and doesn't pick $a$ for the next $R(N)$ iterations, then, at iteration $N{+}\\tau_{b}(N)$ , the algorithm picks arm $b(N)$ , even though, $\\mathcal{Z}_{a}(N{+}\\tau_{b}(N){-}1){-}\\mathcal{Z}_{b(N)}(N{+}\\tau_{b}(N){-}1)<$ 0. Thus we arrive at a contradiction. \u53e3 ", "page_idx": 68}, {"type": "text", "text": "HProof of Theorem 3.1 ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "By Proposition 3.1, we know that, for every $a\\in[K]$ and $N\\geq T_{s t a b l e}$ \uff0c ", "page_idx": 68}, {"type": "equation", "text": "$$\n|\\widetilde{\\omega_{a}}(N)-\\omega_{a}^{\\star}|\\ \\leq\\ C_{1}N^{-3\\alpha/8}.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Recall the functions $W_{a}(\\cdot,\\cdot)$ defined in Appendix G.2.2. Note that for every allocation $\\omega=$ $(\\omega_{a})_{a\\in[K]}$ ,and $a\\in[K]/\\{1\\}$ , we have, ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\frac{\\partial W_{a}}{\\partial\\omega_{1}}(\\omega_{1},\\omega_{a})\\;=\\;d(\\mu_{1},x_{1,a}(\\omega_{1},\\omega_{a}))\\quad\\mathrm{and}\\quad\\frac{\\partial W_{a}}{\\partial\\omega_{a}}(\\omega_{1},\\omega_{a})\\;=\\;d(\\mu_{a},x_{1,a}(\\omega_{1},\\omega_{a})),\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where $\\begin{array}{r}{x_{1,a}(\\omega_{1},\\omega_{a})=\\frac{\\omega_{1}\\mu_{1}+\\omega_{a}\\mu_{a}}{\\omega_{1}+\\omega_{a}}}\\end{array}$ ", "page_idx": 68}, {"type": "text", "text": "As a result, for every $a\\in[K]/\\{1\\}$ , the partial derivatives of $W_{a}(\\omega_{1},\\omega_{a})$ with respect to $\\omega_{1}$ and $\\omega_{a}$ are both $O(1)$ . Therefore, using the mean value theorem, for every $a\\in[K]/\\{1\\}$ and $N\\geq T_{s t a b l e}$ \uff0c we have, ", "page_idx": 68}, {"type": "equation", "text": "$$\n|W_{a}(\\widetilde{\\omega}_{1}(N),\\widetilde{\\omega}_{a}(N))-W_{a}(\\omega_{1}^{\\star},\\omega_{a}^{\\star})|\\;=\\;{\\cal O}(N^{-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Define the normalized index of every arm as ", "page_idx": 69}, {"type": "equation", "text": "$$\nH_{a}(N)\\;=\\;\\frac{I_{a}(N)}{N}\\;=\\;W_{a}(\\widetilde{\\omega}_{1}(N),\\widetilde{\\omega}_{a}(N)).\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Also by (54), $W_{a}(\\omega_{1}^{\\star},\\omega_{a}^{\\star})=I^{\\star}=T^{\\star}(\\pmb{\\mu})^{-1}$ for every alternative arm $a\\in[K]/\\{1\\}$ Therefore (103) gives us, ", "page_idx": 69}, {"type": "equation", "text": "$$\n|H_{a}(N)-T^{\\star}(\\mu)^{-1}|\\ =\\ O(N^{-3\\alpha/8}),\\quad\\mathrm{for}\\ a\\in[K]/\\{1\\}\\ \\mathrm{and}\\ N\\geq T_{s t a b l e}.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "By Lemma G.3, we also know, ", "page_idx": 69}, {"type": "equation", "text": "$$\n|{\\cal Z}_{a}(N)-N H_{a}(N)|\\;=\\;|{\\cal{Z}}_{a}(N)-I_{a}(N)|\\;=\\;{\\cal{O}}(N^{1-3\\alpha/8}).\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Combining (104) and (105), we get ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|{\\mathcal{Z}}_{a}(N)-N T^{\\star}(\\pmb{\\mu})^{-1}|\\ \\leq\\ |{\\mathcal{Z}}_{a}(N)-N H_{a}(N)|+N|H_{a}(N)-T^{\\star}(\\pmb{\\mu})^{-1}|}\\\\ {\\ }&{=\\ O(N^{1-3\\alpha/8}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "for every $a\\in[K]/\\{1\\}$ . Hence, we can find a constant $C_{2}>0$ , independent of the sample paths, such that, ", "page_idx": 69}, {"type": "equation", "text": "$$\n{\\mathcal Z}_{a}(N)\\;\\ge\\;\\frac{N}{T^{\\star}(\\mu)}-C_{2}N^{1-3\\alpha/8},\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "for every $N\\geq T_{s t a b l e}$ and $a\\in[K]/\\{1\\}$ . As a result, for $N\\geq T_{s t a b l e}$ , we have, ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a\\in[K]/\\{1\\}}\\mathcal{Z}_{a}(N)\\;\\geq\\;\\frac{N}{T^{\\star}(\\mu)}-C_{2}N^{1-3\\alpha/8}.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "The threshold function $\\beta(N,\\delta)$ deciding our stopping condition satisfies, ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\log(1/\\delta)~\\leq~\\beta(N,\\delta)~\\leq~\\log(1/\\delta)+C_{3}\\log\\log(1/\\delta)+C_{4}\\log\\log(N)+C_{5}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "for constants $C_{3},C_{4},C_{5}>0$ .For every $\\delta>0$ , we define the deterministic quantity, ", "page_idx": 69}, {"type": "equation", "text": "$$\nt_{\\mathrm{max},\\delta}~=~\\mathrm{min}\\left\\{~{\\cal N}\\ge T_{s t a b l e}~\\left|~\\frac{{\\cal N}}{T^{\\star}(\\mu)}-C_{2}N^{1-3\\alpha/8}>\\beta({\\cal N},\\delta)\\;\\right\\}.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Now we make the following observations about $t_{\\mathrm{max},\\delta}$ \uff0c ", "page_idx": 69}, {"type": "text", "text": "1. Note that $\\begin{array}{r}{\\frac{N}{T^{\\star}(\\pmb{\\mu})}\\,-\\,C_{2}N^{1-3\\alpha/8}}\\end{array}$ increases linearly in $N$ and $\\beta(N,\\delta)$ .s ${\\cal O}(\\log\\log(N)+$ $\\log(1/\\delta))$ , for a fixed $\\delta>0$ . Hence, $t_{\\mathrm{max},\\delta}$ is finite for every $\\delta>0$ ", "page_idx": 69}, {"type": "text", "text": "2. We have $\\beta(N,\\delta)\\ge\\log(1/\\delta)$ and $\\begin{array}{r}{\\frac{N}{T^{\\star}(\\pmb{\\mu})}\\,-\\,C_{2}N^{1-3\\alpha/8}\\,<\\,\\frac{N}{\\,T^{\\star}(\\pmb{\\mu})\\,}}\\end{array}$ As resut,. $t_{\\mathrm{max},\\delta}$ atleast the iteration at which $\\frac{N}{T^{\\star}(\\pmb\\mu)}$ exceeds $\\log(1/\\delta)$ , which is atleast $T^{\\star}(\\mu)\\log(1/\\delta)$ . This implies $t_{\\mathrm{max},\\delta}\\geq T^{\\star}(\\pmb{\\mu})\\log(1/\\ddot{\\delta})$ . As a result, $t_{\\mathrm{max},\\delta}\\rightarrow\\infty$ as $\\delta\\rightarrow0$ ", "page_idx": 69}, {"type": "text", "text": "3. If $\\tau_{\\delta}~>~T_{s t a b l e}$ , then $\\mathrm{min}_{a\\in[K]/\\{1\\}}\\,\\mathcal{I}_{a}(N)$ exceed $\\beta(N,\\delta)$ before the lower bound of $\\mathrm{min}_{a\\in[K]/\\{1\\}}\\,\\mathcal{I}_{a}(N)$ in (106) exceeds $\\beta(N,\\delta)$ . As a result, $\\tau_{\\delta}\\leq\\,t_{\\mathrm{max},\\delta}$ , whenever $\\tau_{\\delta}\\geq$ $T_{s t a b l e}$ . This gives us the upper bound, ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\tau_{\\delta}\\ \\leq\\ \\operatorname*{max}\\{T_{s t a b l e},t_{\\operatorname*{max},\\delta}\\}\\quad\\mathrm{a.s.\\in}\\,\\mathbb{P}_{\\mu}.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "We have $\\mathbb{E}_{\\mu}[T_{s t a b l e}]<\\infty$ , which also implies $T_{s t a b l e}<\\infty$ a.s. in $\\mathbb{P}_{\\mu}$ . Now using (108), we get, ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\delta\\to0}\\operatorname*{sup}_{\\log(1/\\delta)}\\ \\le\\ \\operatorname*{lim}_{\\delta\\to0}\\frac{\\mathbb{E}_{\\mu}[T_{s t a b l e}]+t_{\\operatorname*{max},\\delta}}{\\log(1/\\delta)}\\ =\\ \\operatorname*{lim}_{\\delta\\to0}\\frac{t_{\\operatorname*{max},\\delta}}{\\log(1/\\delta)}.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Similarly, we have, ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\delta\\to0}\\operatorname*{sup}_{\\log(1/\\delta)}\\;\\leq\\;\\operatorname*{lim}_{\\delta\\to0}\\frac{T_{s t a b l e}+t_{\\operatorname*{max},\\delta}}{\\log(1/\\delta)}\\;=\\;\\operatorname*{lim}_{\\delta\\to0}\\operatorname*{sup}_{\\log(1/\\delta)}\\quad\\mathrm{a.s.\\in}\\,\\mathbb{P}_{\\mu}.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Therefore to prove asymtotic optimality, tis sufcient to prove limsup l ", "page_idx": 70}, {"type": "text", "text": "Let Smax,8 = max,8 - 1. Note that limsups\u21920 lg(1) $\\begin{array}{r}{\\operatorname*{lim}\\operatorname*{sup}_{\\delta\\to0}\\frac{t_{\\operatorname*{max},\\delta}}{\\log(1/\\delta)}=\\operatorname*{lim}\\operatorname*{sup}_{\\delta\\to0}\\frac{s_{\\operatorname*{max},\\delta}}{\\log(1/\\delta)}}\\end{array}$ By definition of $t_{\\mathrm{max},\\delta}$ , we have ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\frac{s_{\\operatorname*{max},\\delta}}{T^{\\star}(\\mu)}-C_{2}s_{\\operatorname*{max},\\delta}^{1-3\\alpha/8}\\,\\,\\leq\\,\\,\\beta(s_{\\operatorname*{max},\\delta},\\delta)=\\log(1/\\delta)+C_{3}\\log\\log(1/\\delta)+C_{4}\\log\\log(s_{\\operatorname*{max},\\delta})+C_{5}.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "After some rearrangement of terms, upon dividing both sides by $\\log(1/\\delta)$ ,we get, ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad}&{\\displaystyle\\frac{1}{T^{\\star}(\\mu)}\\frac{s_{\\mathrm{max},\\delta}}{\\log(1/\\delta)}\\Big(1-C_{2}s_{\\mathrm{max},\\delta}^{-3\\alpha/8}-C_{3}s_{\\mathrm{max},\\delta}^{-1}\\log\\log(s_{\\mathrm{max},\\delta})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad-\\ C_{5}s_{\\mathrm{max},\\delta}^{-1}\\Big)\\ \\leq\\ 1+C_{3}\\displaystyle\\frac{\\log\\log(1/\\delta)}{\\log(1/\\delta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "By Observation 2, $s_{\\mathrm{max},\\delta}=t_{\\mathrm{max},\\delta}-1\\rightarrow\\infty$ as $\\delta\\rightarrow0$ . As a result, the above inequality implies ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\frac{1}{T^{\\star}(\\mu)}\\operatorname*{lim}_{\\delta\\to0}\\frac{s_{\\mathrm{max},\\delta}}{\\log(1/\\delta)}\\;\\leq\\;1.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "We lready argued that $\\begin{array}{r}{\\operatorname*{lim}\\operatorname*{sup}_{\\delta\\to0}\\frac{t_{\\operatorname*{max},\\delta}}{\\log(1/\\delta)}\\leq T^{\\star}(\\pmb{\\mu})}\\end{array}$ As aresult wehave acostant $C_{6}>0$ such that $t_{\\mathrm{max},\\delta}\\le C_{6}\\log(1/\\delta)$ for all $\\delta$ . By (108) we have: ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\frac{t_{\\operatorname*{max},\\delta}-1}{T^{\\star}(\\mu)}-C_{2}(t_{\\operatorname*{max},\\delta}-1)^{1-3\\alpha/8}\\le\\beta(t_{\\operatorname*{max},\\delta}-1,\\delta)\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "which implies, ", "page_idx": 70}, {"type": "equation", "text": "$$\nt_{\\mathrm{max},\\delta}\\leq T^{\\star}(\\pmb{\\mu})\\log(1/\\delta)+O\\left(\\log\\log(1/\\delta)+t_{\\mathrm{max},\\delta}^{1-3\\alpha/8}\\right).\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Putting $t_{\\mathrm{max},\\delta}\\le C_{6}\\log(1/\\delta)$ in the above inequality, ", "page_idx": 70}, {"type": "equation", "text": "$$\nt_{\\mathrm{max},\\delta}\\ \\leq\\ T^{\\star}(\\pmb{\\mu})\\log(1/\\delta)+O\\left((\\log(1/\\delta))^{1-3\\alpha/8}\\right).\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Since $\\begin{array}{r l r}{\\tau_{\\delta}}&{{}\\leq}&{\\operatorname*{max}\\{T_{s t a b l e},1\\;+\\;t_{\\operatorname*{max},\\delta}\\}}\\end{array}$ ,wecan find aconstant $C\\ \\ >\\ \\ 0$ such that $\\begin{array}{r l}{\\tau_{\\delta}}&{{}\\leq}\\end{array}$ max $\\left\\{T_{s t a b l e},T^{\\star}(\\pmb\\mu)\\log(1/\\delta)+C(\\log(1/\\delta))^{1-3\\alpha/8}\\right\\}$ Hence Theorem3.1 stands proved. ", "page_idx": 70}, {"type": "text", "text": "1  Extending the proposed algorithm to distributions with bounded support ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "We describe a natural extension of AT2 and IAT2 algorithms to bandit instances from a non-parametric family. We conduct experiments to compare the proposed algorithm with the existing ones. We consider the class ofdistributions having their supports contained in $[0,1]$ which we denote by $\\mathcal{F}_{[0,1]}$ This is similar to the assumptions made in [16]. Some definitions are in order. We use $\\mu(G)$ todenote the mean of distribution $G\\in\\mathcal{F}_{[0,1]}$ ", "page_idx": 70}, {"type": "text", "text": "For every $F\\in\\mathcal{F}_{[0,1]}$ and $x\\in[0,1]$ , we define $\\mathrm{KL_{inf}^{+}}$ and ${\\bf K}{\\bf L}_{\\mathrm{inf}}^{-}$ as: ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}_{\\mathrm{inf}}^{+}(F,x)\\;=\\;\\operatorname*{inf}\\{\\,\\mathrm{KL}(F,G)\\:|\\:\\mu(G)>x\\:\\}\\quad\\mathrm{and}}\\\\ &{\\mathrm{KL}_{\\mathrm{inf}}^{-}(F,x)\\;=\\;\\operatorname*{inf}\\{\\,\\mathrm{KL}(F,G)\\:|\\:\\mu(G)<x\\:\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "At iteration $N$ of the algorithm, let $\\widetilde{F}_{a}(N)$ be the empirical distribution of the samples collected from some arm $a\\in[K]$ $[K],\\widetilde{\\cal F}(N)=(\\widetilde{F}_{a}(N):a\\in[K]),$ $\\widetilde{N}_{a}(N)$ be the total no. of samples collected from $a$ til1 $N$ and $\\widetilde{N}(N)=(\\widetilde{N}_{a}(N):a\\in[K])$ . Let ${\\widetilde{\\mu}}_{a}(N)=\\mu({\\widetilde{F}}_{a}(N))$ and $\\begin{array}{r}{\\hat{i}_{N}=\\arg\\operatorname*{max}_{a\\in[K]}\\;\\widetilde{\\mu}_{a}(N)}\\end{array}$ We now define: ", "page_idx": 70}, {"type": "equation", "text": "$$\nx_{i_{N},a}(N)\\;=\\;\\arg\\operatorname*{min}_{x\\in[0,1]}\\;\\Big\\{\\;\\widetilde{N}_{i_{N}}(N)\\cdot\\mathbf{KL}_{\\mathrm{inf}}^{-}(\\widetilde{F}_{i_{N}}(N),x)+\\widetilde{N}_{a}(N)\\cdot\\mathbf{KL}_{\\mathrm{inf}}^{+}(\\widetilde{F}_{a}(N),x)\\;\\Big\\}.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "At every iteration, we compute the empirical index of every arm $a\\neq\\hat{i}_{N}$ as: ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\mathbb{Z}_{a}(N)\\;=\\;\\operatorname*{min}_{x\\in[0,1]}\\Big\\{\\:\\widetilde{N}_{\\widehat{i}_{N}}(N)\\cdot\\mathbf{KL}_{\\operatorname*{inf}}^{-}(\\widetilde{F}_{\\widehat{i}_{N}}(N),x)+\\widetilde{N}_{a}(N)\\cdot\\mathbf{KL}_{\\operatorname*{inf}}^{+}(\\widetilde{F}_{a}(N),x)\\:\\Big\\},\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "and the anchor function as, ", "page_idx": 71}, {"type": "equation", "text": "$$\ng(\\widetilde{F}(N),\\widetilde{N}(N))\\ =\\ \\sum_{a\\neq\\hat{i}_{N}}\\frac{\\mathrm{KL}_{\\mathrm{inf}}^{-}(\\widetilde{F}_{\\hat{i}_{N}}(N),x_{\\hat{i}_{N},a}(N))}{\\mathrm{KL}_{\\mathrm{inf}}^{+}(\\widetilde{F}_{a}(N),x_{\\hat{i}_{N},a}(N))}-1.\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "The AT2 and IAT2 algorithm for the class $\\mathcal{F}_{[0,1]}$ , respectively, follows the same steps as in (1) and (2) with the anchor and index functions defined as above. ", "page_idx": 71}, {"type": "text", "text": "We experimentally demonstrate the performance of the proposed algorithms in Appendix J.5. ", "page_idx": 71}, {"type": "text", "text": "Experiments ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "J.1 Dynamics of the algorithms ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Experiment 1 (Gaussian and Bernoulli bandits with well-separated arms): In the main text (Figure 1), we presented the evolution of normalized indexes for the sub-optimal arms for AT2, when run without the stopping rule. Numerically, we observe similar plots for normalized indexes even for the other algorithms: 0.5-EB-TCB (proposed in [16] with $\\beta=0.5)$ ), and TCB (proposed in [22]). Hence, we do not report them. However, we do observe differences in the evolution of the anchor function value across these algorithms. We present this in two different settings in the current section. ", "page_idx": 71}, {"type": "text", "text": "Interestingly, as per our implementations, we observe that only AT2 satisfies the asymptotic optimality conditions, maintaining the anchor function close to O, in addition to maintaining the equality of the normalized indexes. ", "page_idx": 71}, {"type": "text", "text": "In this section, we consider the following two examples: ", "page_idx": 71}, {"type": "text", "text": "1. Gaussian bandit. In the first setup (Figure 4), we consider a 4 armed Gaussian bandit with unit variance and mean vector $\\mu=[10,8,7,6.5]$ . This is the same setting as in Section 6 from the main text.   \n2. Bernoulli bandit. In the second setup (Figure 5), we consider a Bernoulli bandit with means $\\mu=[0.91,0.73,0.64,0.59]$ ", "page_idx": 71}, {"type": "text", "text": "In Figures 4 and 5, we plot the evolution of anchor function value for the three algorithms in the two settings, without implementing the stopping rule. The solid lines in the figure represent the average of anchor function over 4, 0o0 independent runs. The shaded bands around the sold lines (almost invisible in these figures), represent 2 standard deviation bands around the mean. ", "page_idx": 71}, {"type": "text", "text": "We observe that only AT2 maintains the anchor function value close to 0. Our experiments suggest that that TCB algorithm, as per our implementation, doesn't satisfy the asymptotic optimality conditions. ", "page_idx": 71}, {"type": "text", "text": "J.2  Sample complexity comparison ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "In Section 6 in the main text, we compared the sample complexities (SC) of the three algorithms on a well-separated Gaussian bandit (Figure 2). In this section, we compare the SC of all the algorithms, as a function of different parameters. We consider harder Gaussian as well as Bernoulli bandit instances (with means close to each other), presented below. ", "page_idx": 71}, {"type": "text", "text": "1. Gaussian bandit. A 4 armed Gaussian bandit with unit variance and mean vector $\\mu=$ [7.25, 7.05, 7, 7.1] so that the means are closer together.   \n2. Bernoulli bandit. As a second example, we consider a 4-armed Bernoulli bandit with close-bymeans: $\\mu=[0.99,0.96,0.95,0.97]$ ", "page_idx": 71}, {"type": "text", "text": "Experiment 2 (SC as function of $\\beta$ ): In this experiment, we compare SC of (I)AT2, (I)TCB, $\\beta$ EB(I)TCB algorithms, for $\\beta\\in[0.2,0.3,0.4,0.5,0.6,0.7,0.8],$ . on the Gaussian instance (Figure 6) and the Bernoulli instance (Figure 7), described above. The error probability $\\delta$ in both these experiments is set to 0.001. All the algorithms use the same forced exploration rule and stopping rules. ", "page_idx": 71}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/643ecf1acf52602e33ce68cfc1d49bbda10601e6595e2db44bee40216668521c.jpg", "img_caption": ["Figure 4: Anchor function value for easy Gaussian Figure 5: Anchor function value for easy Bernoulli bandit (Exp.1), averaged over 4,000 sample paths. bandit (Exp.1), averaged over 4,000 sample paths. "], "img_footnote": [], "page_idx": 72}, {"type": "text", "text": "The lines with the markers in the figures represent the average number of samples generated before stopping, averaged over 4, 000 independent simulations, while the shaded regions denote 2 standard deviations around the mean. We also report the average sample complexity and the standard deviation of the average sample complexity for AT2, IAT2, TCB, and ITCB, across $4,000$ independent simulations. ", "page_idx": 72}, {"type": "text", "text": "In both these simulations, we observe that AT2 and IAT2, respectively, have about $5\\%$ lower sample complexity compared to TCB and ITCB. ", "page_idx": 72}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/4dbd996ae91e776e5e97a5e5f75ba45c36bd9fe2b66b9ebdb3b881c5ee780e1f.jpg", "img_caption": ["Figure 6: Sample complexity (SC) on Gaussian bandit from Exp. 2, averaged over 4, 000 independent samplepaths. $\\delta=0.001$ "], "img_footnote": [], "page_idx": 72}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/083d1fef0a664dc217ec3a1630f595039e954d5a85c10cfacf36a777ebedf9a2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 72}, {"type": "text", "text": "Figure 7: Sample complexity (SC) on Bernoiulli bandit from Exp. 2, averaged over 4, 000 independent samplepaths. $\\delta=0.001$ ", "page_idx": 72}, {"type": "text", "text": "Experiment 3 (SC as function of $\\delta$ ): In Figure 8 and Figure 9, we plot the sample complexities of the three algorithms \u2014 AT2, 0.5-EB-TCB, and TCB \u2014\u2014 as a function of $\\delta$ , for the Gaussian and Bernoulli bandits considered in Experiment 2 above. All the algorithms use the same forced exploration and stopping rules. We observe that AT2 consistently outperforms both the previously known algorithms, and the gap in performance increases as we reduce $\\delta$ ", "page_idx": 73}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/7c75d22618f734362822c475194313e9d2ccbfe51722d59e10dec575cc02dd85.jpg", "img_caption": ["Figure 8: Sample complexity of Gaussian bandit Figure 9: Sample complexity for Bernoulli bandit (Exp.3), averaged over 4,000 sample paths. (Exp.3), averaged over 4,000 sample paths. "], "img_footnote": [], "page_idx": 73}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/9ada0b4b2fa0650b700cfba1be4815b8ecf0a68c45a082f4e65c54a814029355.jpg", "img_caption": [], "img_footnote": [], "page_idx": 73}, {"type": "text", "text": "Experiment 4 (SC as a function of number of arms). We plot the number of samples needed by the three algorithms, as a function of number of arms in the bandit instance. $\\delta$ is set to 0.001 in this experiment. ", "page_idx": 73}, {"type": "text", "text": "For scalability, in this experiment, we consider a simple Gaussian bandit (well-separated means) with all arms having a unit variance. Arm 1 is optimal with mean 10. To study the effect of number of arms on sample complexity, we choose all the other arms to be same with mean 8. Thus, the bandit instances have Gaussian arms with unit variance, and means ", "page_idx": 73}, {"type": "equation", "text": "$$\n\\mu=[10,8,\\ldots].\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "As in the earlier experiments, for fair comparison, all the algorithms are implemented with the same forced exploration and stopping rules. Results are presented in Figure 10. We observe that the sample complexity increases linearly with number of arms for the three algorithms. In this experiment, the performance of TCB and AT2 looks comparable, with TCB requiring slightly more number of samples. However, the gap in their performance is expected to increase for smaller values of $\\delta$ ", "page_idx": 73}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/297796289ea290ef69319abd60889d3aeb7bfca8fe52bf5a036328d98b222d3e.jpg", "img_caption": ["Figure 10: Sample complexity on Gaussian bandit (Exp.4), averaged over 4, 000 independent sample paths. "], "img_footnote": [], "page_idx": 73}, {"type": "text", "text": "J.3 Runtime comparison ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Experiment 5: In this experiment, we compare the run-time of (I)AT2 and (I)TCB algorithms on a 4 armed Gaussian bandit with means $\\mu=[10,9.4,7,6.5]$ and unit variance, averaged over longer ", "page_idx": 73}, {"type": "text", "text": "100,000 simulations. $\\delta$ is set to 0.001, and the four algorithms use the same forced exploration and stopping rules. ", "page_idx": 74}, {"type": "table", "img_path": "YXQW4qQe2U/tmp/3cecbabee1b6885bfe5fdbde82fe79038d500391071e3414651efe23d2d712b1.jpg", "table_caption": ["Table 1 represents the average run-time of the two algorithms. We observe that TCB and ITCB take roughly two times more computational time compared to AT2 and IAT2, respectively. ", "Table 1: Runtime of (I)AT2 and (I)TCB on Gaussian bandit with $\\mu\\,=\\,[10,9.4,7,6.5]$ and unit variances (Exp.6). Results reported are for 100, 000 independent runs of each algorithm. "], "table_footnote": [], "page_idx": 74}, {"type": "text", "text": "J.4  Effect of forced-exploration parameter $\\alpha$ on sample complexity ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "In this section, we provide a numerical evaluation of the impact of forced exploration on the performance of AT2 and IAT2. Our experiments suggest that unlike IAT2, AT2 needs forced exploration. On instances where the second and third best arms have equal means, AT2 might see some bad samples from the best arm in the beginning. As a result, without forced exploration, it will sample the second and the third best arms forever. However, we see that AT2 performs sufficient exploration for instances having all arms with different means. Note that similar observations were made in [16] for $\\beta$ -EB-TCB. ", "page_idx": 74}, {"type": "text", "text": "Experiment 6: To see the above mentioned behavior of AT2, we study the performance of AT2 and IAT2 on the following two bandit instances. ", "page_idx": 74}, {"type": "text", "text": "1. A 4 armed Gaussian bandit with unit variance and mean vector $\\mu=[7.25,7.05,7,7.1]$ that the means are close together, yet all different.   \n2. A 4 armed Gaussian bandit with unit variance and mean vector $\\mu=[7.25,7,7,7]$ ,so that the three suboptimal arms have equal means. ", "page_idx": 74}, {"type": "text", "text": "Intuitively, it might appear that forced exploration could significantly increase the sample complexity of AT2 and IAT2. However, in Figure 11, we see that IAT2's performance remains unaffected and AT2 performs at least as well as IAT2 with moderate amount of forced exploration on the first instance, where the gap between the means is small. ", "page_idx": 74}, {"type": "text", "text": "Figure 12 shows that without forced exploration, AT2's sample complexity blows up on the instance with equal sub-optimal arm means. We see that on this instance, IAT2 performs significantly better than AT2. Furthermore, IAT2's sample complexity remains almost unaffected with respect to the amount of forced exploration done. ", "page_idx": 74}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/9caa56fb3852a739f220723e7cae5034e946ab1a27c1fc0c2bdac72257d02117.jpg", "img_caption": [], "img_footnote": [], "page_idx": 74}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/7a296a11c75914cfd5b0b5f964710ed41e5b45c232de4882d4d2a2230d36c37a.jpg", "img_caption": ["Figure 11: Sample complexity of Gaussian bandit Figure 12: Sample complexity of Gaussian bandit with means $\\mu=[7.25,7.05,7,7.1]$ , as a function with means $\\mu=[7.25,7,7,7]$ , as a function of $\\alpha$ of $\\alpha$ (Exp.6) (Exp.6) "], "img_footnote": [], "page_idx": 74}, {"type": "text", "text": "J.5  Experiments for bandits with bounded-support distributions ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "In this section, we experimentally demonstrate the performance of a natural extension of AT2 to a non-parametric setting of bandits with arms having distributions supported in $[0,1]$ (seeAppendixI for the modified AT2). This is the setting considered in, for example, [16]. ", "page_idx": 75}, {"type": "text", "text": "Experiment 7: Consider a 4-armed bandit with the following arm distributions: ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\mathrm{{Beta}(1.5,1),~\\mathrm{{Beta}(2,6),~\\mathrm{{Beta}(1,1.5),~\\mathrm{{and}~\\;\\mathrm{{Beta}(1,7).}}}}}\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "Here, the arms have means ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\mu=[0.6,0.25,0.4,0.125].\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "While we do not provide the analysis of the algorithm for this setting, numerically we observe that even in this non-parametric setting, extension of AT2 to this setting outperforms $\\beta$ -EB-TCB, and a corresponding natural extension of TCB to this setting. ", "page_idx": 75}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/af3d7958a233865fce46122c375fb67263f0b533d3bfe92dc52b8a409c967e7a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 75}, {"type": "image", "img_path": "YXQW4qQe2U/tmp/a2db6f7e887a5a4723d364056a036521da6a0140e378d533bfebc4d2093b2890.jpg", "img_caption": [], "img_footnote": [], "page_idx": 75}, {"type": "text", "text": "Figure 13: Sample complexity (Exp.6), averaged Figure 14: Sample complexity (Exp.6), averaged over 4,000 sample paths. We set $\\delta=0.001$ over 4,000 sample paths. ", "page_idx": 75}, {"type": "text", "text": "Reproducibility: Our code is implemented in Julia 1.7.1, and the plots are generated with the Plots. $\\mathrm{j}2$ package. Other dependencies are listed in the Readme .md file, which also includes instructions to reproduce the figures and tables presented here. We build upon the publicly available code for [16]. Our experiments are conducted on an institutional cluster computing facility having an Intel Xeon Gold 6130 2.1GHz CPU with 32 cores. ", "page_idx": 75}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: The optimality of the proposed algorithm is proven in Theorem 3.1. We also demonstrate the dynamics of the algorithm, as proposed in Theorem 4.1 via numerical simulations. ", "page_idx": 76}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: We have discussed the limitations of the results presented in this paper due to model assumptions such that independence, considering bandit instances from an SPEF, etc. at the end of Section 1. ", "page_idx": 76}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: We include all the assumptions made in the statements of the results. We provide all the formal proofs in Appendix, and refer to them. All the theorems/lemmas/propositions are numbered and referenced appropriately. ", "page_idx": 76}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: Our key contribution is a new algorithm with its theoretical guarantees. We perform numerical simulations to support the theoretical results. We include all the implementation details with the choices for different parameters in our numerical experiments discussion in the Appendix J. ", "page_idx": 76}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: We include the code with scripts for reproducing the numerical results. ", "page_idx": 76}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: We include the choices of all the parameters in each simulation. ", "page_idx": 76}, {"type": "text", "text": "7. Experiment Statistical Significance ", "page_idx": 76}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: We include mean and standard deviations in all our experiments, along with the assumptions made (independence of different runs of same simulation). ", "page_idx": 76}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 77}, {"type": "text", "text": "Justification: We use an institutional computing facility for our experiments (see, end of AppendixJ). ", "page_idx": 77}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 77}, {"type": "text", "text": "Justification: The results of this work are theoretical in nature. We appropriately reference all existing works to the best of our knowledge. We include justifications for all the results, and implementation details for reproducing the simulation results of this work. ", "page_idx": 77}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 77}, {"type": "text", "text": "Justification: The main contributions of this work are theoretical in nature. While we propose a new algorithm, there are no harmful societal impacts or consequences of this research. ", "page_idx": 77}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 77}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 77}, {"type": "text", "text": "Justification: We run numerical simulations. We do not use any publicly available datasets for our experiments. ", "page_idx": 77}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Justification: Please see paragraph on Reproducibility in Appendix J. ", "page_idx": 77}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 77}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 77}, {"type": "text", "text": "Justification: We do not release any new assets. ", "page_idx": 77}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 77}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 77}, {"type": "text", "text": "Justification: The paper does not involve crowd sourcing nor research with human subjects. ", "page_idx": 77}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 77}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 78}]