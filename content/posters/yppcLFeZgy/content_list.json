[{"type": "text", "text": "MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yizhen Luo1,2,\u2020, Zikun Nie1,2,\u2020, Massimo Hong1,2, Suyuan Zhao1,2, Hao Zhou1,\u2217, Zaiqing Nie1,3,\u2217 1Institute of AI Industry Research (AIR), Tsinghua University   \n2Department of Computer Science and Technology, Tsinghua University 3Pharmolix Inc.   \n{yz-luo22,nzk24,hongcd21,zhaosy23}@mails.tsinghua.edu.cn {zhouhao,zaiqing}@air.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Studying protein mutations within amino acid sequences holds tremendous significance in life sciences. Protein language models (PLMs) have demonstrated strong capabilities in broad biological applications. However, due to architectural design and lack of supervision, PLMs model mutations implicitly with evolutionary plausibility, which is not satisfactory to serve as explainable and engineerable tools in real-world studies. To address these issues, we present MutaPLM, a unified framework for interpreting and navigating protein mutations with protein language models. MutaPLM introduces a protein delta network that captures explicit protein mutation representations within a unified feature space, and a transfer learning pipeline with a chain-of-thought (CoT) strategy to harvest protein mutation knowledge from biomedical texts. We also construct MutaDescribe, the first large-scale protein mutation dataset with rich textual annotations, which provides cross-modal supervision signals. Through comprehensive experiments, we demonstrate that MutaPLM excels at providing human-understandable explanations for mutational effects and prioritizing novel mutations with desirable properties. Our code, model, and data are open-sourced at https://github.com/PharMolix/MutaPLM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Studying protein evolution through mutations within amino acid sequences is a central research topic in life sciences [1\u20133]. Despite immense research efforts, a large number of protein mutations with biological significance remain under-explored, highlighting the demand for in-silico tools to model these mutations. Practically, the tool should meet two requirements. First, it should be explainable, providing insightful and human-understandable interpretations for mutational effects. This is crucial for broad biological applications ranging from identifying immune-escape pathogens [4, 5] to interpreting the mechanisms of human diseases [6, 7]. Additionally, the tool should be engineerable, proposing protein mutations that satisfy desirable properties such as catalytic activity and thermostability. This process is known as directed evolution [8, 9], the most prevailing approach for protein design in the laboratory, which offers substantial beneftis across various application fields, including industry [10], biotechnology [11], and therapeutics [12]. ", "page_idx": 0}, {"type": "text", "text": "To achieve these goals, deep learning models [13\u201315] have emerged to capture evolutionary information from protein sequences. Recently, the development of protein language models (PLMs) ", "page_idx": 0}, {"type": "text", "text": "[16\u201320] has brought a paradigm shift in computational biology. By self-supervised learning [21] on evolutionary-scale databases [22, 23], PLMs have achieved great success in various biological applications, including structure prediction [19, 24] and protein design [18, 25]. Additionally, PLMs have demonstrated zero-shot capabilities in predicting and optimizing evolutionary plausibility [26\u201328], a continuous value indicating whether a mutation is favored by natural selection. ", "page_idx": 1}, {"type": "text", "text": "Despite their promising advancements, we argue that existing PLMs are not yet satisfactory as explainable and engineerable tools for handling protein mutations. Regarding mutation explanation, PLMs\u2019 implicit interpretation with evolutionary plausibility is overly vague, lacking detailed information for mutational effects such as specific alterations in protein functions and impacts on organisms. Regarding mutation engineering, PLMs can only propose evolutionary-plausible mutations, which may be misaligned with human preferences in real-world practices of directed evolution. For example, enhancing the catalytic activity of an enzyme from a bacterium could be detrimental to its survival due to increased energy costs but beneficial for industrial applications. In such scenarios, the utility of PLMs in assisting protein engineering is significantly compromised. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to develop explainable and engineerable PLMs by explicitly modeling protein mutations. However, conventional PLMs based on the Transformers [29] architecture provide contextaware representations for each amino acid, which are inadequate for capturing the discrepancies between the wild-type and its mutant within a unified feature space. Besides, there is a lack of supervision signals necessary for comprehending the intricate impacts of protein mutations, which require extensive background knowledge, including protein structures, protein functions, and mechanisms of biological processes. ", "page_idx": 1}, {"type": "text", "text": "To address these issues, we envision that (1) mutation representations could be captured from the variations of PLM representations between the wild-type and its mutant with appropriate architecture, and (2) expert-written texts from protein databases and biomedical publications provide rich crossmodal supervision for learning protein mutations. Specifically, we propose MutaPLM, a unified framework for interpreting and navigating Mutations with Protein Language Models. We introduce a protein delta network that translates between mutations and protein delta features, formulating a unified feature space aligned with textual semantics. We develop a transfer learning pipeline with a chain-of-thought (CoT) strategy [30] to harvest protein mutation knowledge from biomedical texts. Additionally, we construct MutaDescribe, the first large-scale dataset containing diverse protein mutations and rich textual annotations of their effects. Using natural language as a friendly interface, the dataset facilitates the training and evaluation of mutation explanation and engineering. ", "page_idx": 1}, {"type": "text", "text": "Through comprehensive experiments, we demonstrate that MutaPLM is a versatile, explainable, and engineerable tool for assisting protein mutation studies. In mutation explanation, MutaPLM outperforms the strongest baseline model by $6.5\\%$ in ROUGE-L, and $19.4\\%$ of the predicted mutational effects are regarded as accurate and insightful by human experts. In mutation engineering, our model achieves an average of 0.409 recall scores on top-50 mutation proposals navigated by free-text instructions, improving ESM-2 [19] by 1.6-fold. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose MutaPLM, a unified framework that enables protein language models to capture mutations explicitly using a protein delta network and cross-modal supervision. \u2022 We build MutaDescribe, the first dataset with detailed textual annotations for protein mutations. \u2022 We validate the effectiveness of MutaPLM in explaining and engineering protein mutations through comprehensive experiments. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Protein Language Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In analogy to large language models (LLMs) [31\u201334] in natural language processing (NLP), protein language models (PLMs) such as ProteinBERT [35], ProtTrans [17], ProtGPT2 [18], and ESM series [36, 19, 37] have surged in modeling protein sequences. Pre-trained by masked language modeling [38] or auto-regressive language modeling [39] on evolutionary-scale protein databases, PLMs have demonstrated outstanding predictive power on protein secondary and tertiary structures [24], protein functions [40] and protein-protein interactions [41]. More recently, explorations on PLMs unifying protein sequences and natural language [42\u201345] have attracted rising research interest, as texts provide unstructured knowledge and a friendly user interface for studying proteins. Notably, a contemporary work [46] proposes to perform text-based protein editing by directly generating the mutated protein sequence. Unfortunately, none of the existing PLMs qualifies as an explainable and engineerable tool in modeling protein mutations, mainly owing to architectural design and lack of supervision. ", "page_idx": 1}, {"type": "image", "img_path": "yppcLFeZgy/tmp/2e532b67afd33fcc23728f21efd11cc2dda80f6f29f32702c9934c7ee75fdc12.jpg", "img_caption": ["Figure 1: Model architecture of MutaPLM. (a) The encoding branch of the protein delta network. The delta encoder takes the subtraction of the PLM representations of the mutant and wild-type as inputs to generate $z_{\\Delta}$ . (b) The decoding branch of the protein delta network. The key components involve a delta decoder that reconstructs mutant features and two prediction heads deciding the position and amino acid of the mutation. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Protein Mutation Modeling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Previous works formulate mutation explanation as learning the \u2019local fitness landscape\u2019, a mapping from protein sequences to specific functional activity scores [47]. Models for protein ftiness prediction could be categorized as (1) alignment-based models [48, 49] trained on multiple sequence alignments (MSAs) [50], (2) PLM models [18, 19] trained on large-scale unaligned sequences, (3) inverse-folding models [27, 51] that learn protein fitness through structure-conditioned sequence distributions, and (4) hybrid models [52, 53] that combine both PLMs and MSAs. The evaluations are performed as per wild-type protein on deep mutation scanning (DMS) [54] or clinical variant [55] benchmarks. In this work, we formulate mutation explanation as a more challenging task that aims at providing textual descriptions of mutational effects for arbitrary wild-type protein and mutation. ", "page_idx": 2}, {"type": "text", "text": "The traditional mutation engineering [8, 9] task aims at generating protein mutants with high ftiness scores. One line of work leverages generative models including variational auto-encoders (VAEs) [56], generative language models [57] and diffusion models [58] to directly generate the protein sequence conditioned on fitness scores. Another line attempts to propose mutations iteratively by greedy sampling [59], reinforcement learning [60], or proximal gradients [61] on the learned ftiness landscape. Differing from prior studies, MutaPLM incorporates textual instructions instead of ftiness scores as navigation and proposes mutations satisfying human preferences. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The main goal of our work is to develop explainable and engineerable PLMs by explicitly modeling protein mutations. To achieve this goal, we elaborate on the proposed MutaPLM framework, highlighting three design components: (1) a protein delta network that translates between mutations and protein delta features $z_{\\Delta}$ (Sec. 3.1, detailed in Appendix A.1), (2) a transfer learning pipeline with a chain-of-thought strategy that harvests protein mutation knowledge from cross-modal supervision (Sec. 3.2, detailed in Appendix A.3), and (3) a specifically constructed dataset with diverse proteins and rich textual annotations of mutation effects (Sec. 3.3, detailed in Appendix B.2). ", "page_idx": 2}, {"type": "text", "text": "3.1 Protein Delta Network for Explicit Mutation Modeling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The protein delta network follows an encoder-decoder architecture, utilizing textual semantics as the latent feature space for protein mutations. As illustrated in Fig. 1, the protein delta network is composed of a protein language model (PLM), a large language model (LLM), a wild-type encoder, a delta encoder, a delta decoder, and two mutation prediction heads. We leverage ESM-2 (650M) [19], a powerful PLM pre-trained on evolutionary-scale databases, to encode protein sequences. We initialize the LLM with BioMedGPT-LM [62], a scientific language model built on LLaMA2-7B [31] through continual pre-training [63] on large-scale biomedical corpora. ", "page_idx": 2}, {"type": "image", "img_path": "yppcLFeZgy/tmp/e6e787fec860bc1e12624e2d44577e281f577188f13060ffb22c69f3f4661d60.jpg", "img_caption": ["(a) Pre-training on Protein Literature (b) Fine-tuning on Protein Muta3ons with Chain-of-Thought ", "Figure 2: Training pipeline of MutaPLM. (a) Workflow of pre-training on protein-related literature. We perform next token prediction for the encoding workflow and conditional masked language modeling for the decoding workflow. (b) Workflow of fine-tuning with chain-of-thought (CoT). We employ a two-round dialog that involves describing the functions of a wild-type protein, explaining the effects of its mutation, and predicting the mutation based on the mutational effects. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Formulation of protein delta features. We speculate that the subtraction of PLM representations between the mutant and wild-type, denoted as $h_{\\Delta}$ , contains rich mutation information, making it suitable for extracting protein delta features $z_{\\Delta}$ . Specifically: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{\\Delta}=h_{\\mathrm{mt}}-h_{\\mathrm{wt}}=f_{\\mathrm{PLM}}(x_{\\mathrm{mt}})-f_{\\mathrm{PLM}}(x_{\\mathrm{wt}}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x_{\\mathrm{mt}}$ and $x_{\\mathrm{wt}}$ are the amino acid sequences of the mutant and wild-type protein, $h_{\\mathrm{mt}}$ and $h_{\\mathrm{wt}}$ are their sequence representations, and $f_{\\mathrm{PLM}}$ is the protein language model. ", "page_idx": 3}, {"type": "text", "text": "The delta encoder $f_{\\mathrm{enc}}$ and delta decoder $f_{\\mathrm{dec}}$ facilitates bi-directional transformations between $h_{\\Delta}$ and $z_{\\Delta}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nz_{\\Delta}=f_{\\mathrm{enc}}(h_{\\Delta}),\\quad h_{\\Delta}=f_{\\mathrm{dec}}(z_{\\Delta}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Encoding protein delta features. Given $h_{\\Delta}$ , the delta encoder is expected to extract informationpreserving protein delta features $z_{\\Delta}$ within a unified feature space. However, protein sequences vary in length, ranging from several tens to thousands of amino acids. To address this issue, we adopt a cross-attention module [29] to transform the sequential representations into a fixed number of latent features. The module, partly inspired by BLIP series [64, 65], maintains $K$ trainable features that serve as queries and takes the sequence representations as keys and values to generate outputs. We employ two parallel modules for encoding the wild-type features $h_{\\mathrm{wt}}$ and mutational features $h_{\\Delta}$ . ", "page_idx": 3}, {"type": "text", "text": "Decoding protein delta features. Drawing inspirations from LM-DESIGN [66], we introduce a cross-attention module that takes a symmetrical form of the delta encoder. Specifically, it treats the wild-type protein representations $h_{\\mathrm{wt}}$ as queries and protein delta features $z_{\\Delta}$ as keys and values. The outputs are then processed by a two-layer feed-forward network (FFN) to reconstruct $h_{\\Delta}$ . The mutant representations $h_{\\mathrm{mt}}$ are obtained by combining $h_{\\Delta}$ with $h_{\\mathrm{wt}}$ , and fed into a position head and a language modeling head to predict the mutation. The position head is a fully-connected layer that predicts whether the amino acid should be substituted. The language modeling head is initialized from the PLM and predicts the type of the mutated amino acid. To facilitate text-based protein engineering, we maintain $K$ trainable soft tokens, which are appended to the input token embeddings of the LLM to summarize textual semantics. The output representations of the soft tokens are processed by the delta decoder to generate mutations. ", "page_idx": 3}, {"type": "text", "text": "Compared with previous works that connect protein sequences with LLMs [67, 44, 45], the proposed protein delta network exhibits the following advantages: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Explicit modeling of protein mutations. Prior models are designed for static protein sequences, while MutaPLM models the alterations introduced by mutations with protein delta features $z_{\\Delta}$ . \u2022 Encoder-decoder architecture. Prior works adopt either an encoder or a decoder architecture for protein sequences, while MutaPLM incorporates both encoding and decoding components. ", "page_idx": 4}, {"type": "text", "text": "3.2 Transfer Learning with Cross-modal Supervision ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Biomedical texts contain rich expert-annotated information on protein properties and mutational effects. As depicted in Fig. 2, MutaPLM harvests these cross-modal supervision signals through a transfer learning pipeline, which we detail as follows: ", "page_idx": 4}, {"type": "text", "text": "Pre-training on protein literature. In this stage, we aim to incorporate general protein knowledge from scientific publications with language modeling objectives, as shown in Fig. 2(a). (1) For the encoding workflow, we take the output representations of the wild-type encoder as LLM inputs and calculate the next-token prediction objective [39] for generating descriptive texts. (2) For the decoding workflow, we employ the conditional masked language modeling (CMLM) objective [68] on the protein sequence. Specifically, we mask $15\\%$ amino acids and task the PLM to recover the masks based on the remaining amino acid sequence and the LLM-summarized textual representations. It is worth noting that in this stage, the delta decoder acts as a modality translator, generating bias terms that help reconstruct the original sequence instead of capturing protein mutation information. Overall, we optimize the summation of these two language modeling objectives. ", "page_idx": 4}, {"type": "text", "text": "Fine-tuning on protein mutations with chain-of-thought $\\mathbf{\\Psi}(\\mathbf{CoT})$ . As depicted in Fig. 2(b), we fine-tune MutaPLM on textual annotations of mutational effects to facilitate mutation explanation and engineering. Since mutational effects typically involve the enhancement or attenuation of protein functions, we adopt a chain-of-thought (CoT) strategy [30] that seamlessly connects protein functions and mutational effects within a two-round dialogue. In the first round, we prompt the LLM to describe the functions of the wild-type protein using the encoding workflow. In the second round, we introduce two tasks, namely describing the mutational effects with the encoding workflow, and predicting the mutation based on textual instructions with the decoding workflow. Both tasks utilize the latent wild-type representations and the predicted functions from the first round dialogue as additional inputs. Formally, the overall objective of fine-tuning is the summation of three parts: (1) next token prediction on protein function descriptions, (2) next token prediction on mutational effects, and (3) weighted cross-entropy between the predicted mutation and the ground-truth mutation. ", "page_idx": 4}, {"type": "table", "img_path": "yppcLFeZgy/tmp/4f1c878bb7fda832c6a5e74c01e607d77f93c6b32017991e84cfee2535474a64.jpg", "table_caption": ["Table 1: Statistics of the MutaDescribe dataset. We report the number of proteins and samples, the average protein sequence length, and the average number of words for mutational effects. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.3 MutaDescribe: A Diverse Protein Mutation Dataset with Textual Annotations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We build MutaDescribe, a large-scale dataset comprising 20.9K wild-type proteins and 171.1K single-site mutations, to facilitate fine-tuning and evaluation. We provide an overview of our dataset in Tab. 1. The construction process involves the following steps: ", "page_idx": 4}, {"type": "text", "text": "Raw data collection. The primary source of MutaDescribe is UniProtKB/SwissProt [69], a widely adopted protein database that contains 106.6K single-site substitutions. We collect expert-reviewed descriptions of mutational effects from the Phenotypes & Variants entry and retrieve the abstract of the corresponding publications on PubMed [70] based on available reference information. ", "page_idx": 4}, {"type": "text", "text": "Quality control. We prompt GPT-3.5-turbo [33] to filter out low-quality descriptions such as those that only mention the originating species. This step helps ensure that the dataset contains high-quality and informative annotations. ", "page_idx": 4}, {"type": "table", "img_path": "yppcLFeZgy/tmp/65ddfad017c81b52c7b133f63e3d0cb4e8f924d4d48699ac0d168e8d81d9ec78.jpg", "table_caption": ["Table 2: Performance evaluation for mutation explanation on the test sets of MutaDescribe. R-L: ROUGE-L. BL-2: BLEU-2. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Data enrichment. Given that the descriptions in UniProtKB are generally short and homogeneous, we utilize GPT-3.5-turbo to enrich the textual annotations by retrieving relevant descriptions from the original PubMed abstract. Additionally, we balance the number of benign and malignant mutations by constructing reversed samples. Specifically, for each mutation, we attempt to exchange the wild-type and the mutant and prompt GPT-3.5-turbo to write a description opposite to the original mutational effect. For example, if the mutational effect of an A89H mutation is \"Increased catalytic activity\", we will create a reversed sample with an H89A mutation and \"Decreased catalytic activity\". ", "page_idx": 5}, {"type": "text", "text": "Data splitting. We first randomly split our dataset into training, validation, and test sets. To evaluate models\u2019 generalization capabilities on novel proteins, we further partition the test set into three subsets based on the wild-type sequence homology with training sequences. We adopt MMSeqs2 [71], a widely-adopted tool to calculate sequence homology. The Easy, Medium and Hard test subsets comprise samples whose sequence homology are between [0.95, 1], [0.5, 0.95), and $[0,0.5)$ respectively. We also implement a temporal split based on the publication date of the mutation, and we defer readers to Appendix B for details and Appendix D.1 for evaluation results. ", "page_idx": 5}, {"type": "text", "text": "Compared with prior mutation benchmarks [55, 72, 73], MutaDescribe is the first to incorporate textual annotations for facilitating mutation explanation and engineering. Besides, MutaDescribe contains a wider variety of wild-type proteins, surpassing ProteinGym [73] by 6 times in quantity. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we demonstrate that MutaPLM is adept at interpreting and engineering mutations through comprehensive experiments. We start with a brief introduction of our training setups (Sec. 4.1), followed by detailed evaluations on two core tasks: mutation explanation (Sec. 4.2) and mutation engineering (Sec. 4.3). We also present an in-depth analysis of our design components (Sec. 4.4), including pre-training and the CoT strategy. ", "page_idx": 5}, {"type": "text", "text": "4.1 Training Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To alleviate catastrophic forgetting [76] and save computational costs, we train MutaPLM in a parameter-efficient way. We apply low-rank adaptation (LoRA) [77] on the LLM with a rank of 16. The number of query embeds and soft tokens is set as $K=32$ . We optimize the LoRA modules, the wild-type encoder, the delta encoder, the delta decoder, the soft tokens, the position head, and the language modeling (LM) head, which comprises a total of $75.0\\mathrm{M}$ parameters. The remaining 7.4B parameters are kept frozen. ", "page_idx": 6}, {"type": "text", "text": "We pre-train MutaPLM for 200K steps with a batch size of 32 on 1.1M protein-text data collected from biomedical publications (detailed in Appendix B.1) and fine-tune it for 70K steps with a batch size of 24 on MutaDescribe. For both stages, we use the AdamW optimizer [78] with a learning rate that is linearly warmed up to $10^{-4}$ for the first 1K steps and decreases to $10^{-5}$ following a cosine annealing strategy. The overall training process takes 10 days on 4 NVIDIA A100 GPUs. ", "page_idx": 6}, {"type": "text", "text": "4.2 Performance Evaluation on Mutation Explanation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Differing from existing studies that interpret mutational effects with protein fitness [26, 28], we formulate mutation explanation as providing detailed textual descriptions for protein mutations. ", "page_idx": 6}, {"type": "text", "text": "Baselines. While no prior work is specifically designed for this task, we perform zero-shot analysis on popular LLMs with various zero-shot or few-shot prompts and implement supervised models for comparison. Our baselines include (1) Text-based LLMs. We perform in-context learning [79] by providing 1-shot and 5-shot demonstrations to GPT-4 [33], the most advanced model in NLP. Additionally, we implement a $\\mathbf{k}\\cdot$ -nearest neighbor (kNN) strategy [80] that selects the top- $\\cdot\\mathbf{k}$ homologous proteins from the training set as few-shot examples. (2) LLM-assisted PLMs, including ESM-2 [19] and OntoProtein [75]. In addition to kNN-based 5-shot samples for GPT-4, we leverage PLMs to provide additional information by predicting the evolutionary plausibility of the mutation. (3) LLMs trained on protein sequences, including Galactica-6.7B [74], Mol-Instructions [67], and ProtLLM [44]. We feed the wild-type and mutated protein sequences into these models and instruct them to provide mutation explanations. (4) Fine-tuned LLMs. We fine-tune BioMedGPT-LM by feeding the ESM-2 representations of the wild-type and mutant (Fine-tuned ESM-2) or the wild-type sequence and evolutionary plausibility (AugmentedESM [27]) into the LLM and performing casual generation. Notably, for all ESM-2 models used in our baselines, we adopt the model with 650M parameters for fair comparison. We defer readers to Appendix C.1 for more implementation details. ", "page_idx": 6}, {"type": "text", "text": "Evaluation. We adopt BLEU [81] and ROUGE [82] scores to assess the quality of the generations by comparing them with ground-truth annotations. To further investigate whether the predictions are truly insightful and helpful in studying protein mutations, we perform a human-AI collaborative evaluation. Specifically, we first utilize GPT-4 as a proxy of human experts to categorize the predictions into Accurate, Relevant, Opposite, and Irrelevant, based on the relevance between the predictions and ground truth. Then, we recruit a postgraduate from a top university who majors in biology to assess and rectify GPT-4 evaluation results on mutation explanations following the same categorization protocol. The prompt and detailed evaluation results are displayed in Appendix C.3. ", "page_idx": 6}, {"type": "text", "text": "Results and analysis. We present performance comparisons on the test sets of MutaDescribe in Tab. 2 and Fig. 3. We observe that: (1) MutaPLM achieves state-of-the-art performance across various evaluation metrics, outperforming fine-tuned ESM-2 by $6.46\\%$ in ROUGE-L and GPT- $4+\\mathrm{ESM}{-2}$ by $3.24\\%$ in BLEU-2. Additionally, more than $40.22\\%$ of MutaPLM predictions are regarded as Accurate or Relevant with ground-truth labels, which showcases our model\u2019s helpfulness in real-world research scenarios. (2) While the performance on the Medium and Hard sets is not as promising as in the easy set, MutaPLM shows generalization capabilities on novel proteins, as validated by $6.44\\%$ accurate and $19.80\\%$ relevant predictions on the hard set. (3) The evolutionary plausibility values are beneficial for elucidating mutational effects, as demonstrated by the slightly improved results of LLM-assisted PLMs against the plain GPT-4 counterpart. However, the superior performance of fine-tuned ESM-2 and MutaPLM indicates that integrating the mutant sequence provides richer mutational information. (4) Supervised baselines underperform few-shot GPT-4 models, especially on Medium and Hard sets and BLEU-2 scores. We observe that supervised models tend to randomly combine short textual segments from the training set, indicating overfitting problems. (5) LLMs trained on protein sequences perform poorly, as they are solely instruction-tuned on single protein sequences. Hence, we emphasize the significance of knowledge transfer from protein functions to mutational effects and their basic properties. ", "page_idx": 6}, {"type": "table", "img_path": "yppcLFeZgy/tmp/9854485fb2934332b03607fab40d8fecf29921defd77547229440554afddafc5.jpg", "table_caption": ["Table 3: Performance evaluation for mutation engineering on the test sets of MutaDescribe. Acc: prediction accuracy of the amino acid given the position of the mutation. $\\operatorname{Rec}(\\!\\omega\\!\\,50\\$ : top 50 recall of the desired mutant. -: not reported due to unaffordable computation costs (requires $\\sim1\\mathrm{M}$ forward passes). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Case study. Additionally, we present a case study in Fig. 4 for a mutation from m7GpppX diphosphatase. Our model accurately identifies the increased decapping activity and provides novel insights beyond the ground truth. In contrast, the GPT-4 model mistakenly identifies the mutational effects as decreases in enzymic activity. More cases are available in Appendix D.3. ", "page_idx": 7}, {"type": "text", "text": "4.3 Performance Evaluation on Mutation Engineering ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Differing from prior works [59\u201361] that perform mutation engineering with an active learning paradigm [84], we challenge models to directly propose protein mutations based on the wild-type sequence and textual instructions. As we primarily focus on single-site mutations, we formulate this as a retrieval task from $19\\times L$ possible mutants for a protein sequence of length $L$ . ", "page_idx": 7}, {"type": "text", "text": "Baselines. We adopt four groups of baselines including: (1) Few-shot LLMs. Similar to mutation explanation, we prompt GPT-4 to suggest single-site mutations through in-context few-shot learning. (2) Zero-shot PLMs including ESM-2 [19] and OntoProtein [75]. We calculate the evolutionary plausibility scores following [26] for each amino acid and derive the best mutant. (3) A retrieval-based model, namely ProtST (ESM-2) [42]. We calculate the cosine similarity between PLM and textual representations of mutational effects to score and rank mutations. (4) Fine-tuned models. We fine-tune BioMedGPT [62] to directly propose a mutation based on the protein sequence and instruction. We also fine-tune ESM-2 by combining its wild-type sequence representations with BioMedBERT [83] encodings of textual instructions by a cross-attention layer. Please refer to Appendix C.1 for details of our baselines. ", "page_idx": 7}, {"type": "text", "text": "Evaluation. We report the average accuracy of the mutated amino acid on the ground-truth mutational position. We also report top-50 recall scores on all possible mutations. ", "page_idx": 7}, {"type": "text", "text": "Results and analysis. Comparisons between MutaPLM and baselines on the test sets of MutaDescribe are presented in Tab. 3. We observe that: (1) MutaPLM achieves an average of $53.51\\%$ in accuracy and $40.94\\%$ in top-50 recall, improving the original ESM-2 model by 1.6-fold. The substantial gains of MutaPLM underscore the significance of textual navigation in mutation engineering. (2) MutaPLM outperforms the fine-tuned ESM-2 model by an average of $2.09\\%$ in accuracy and $6.17\\%$ in top-50 recall, benefiting from our architectural design and pre-training. (3) The overall performance of MutaPLM on the Easy and Hard sets are similar but significantly higher than on the Medium set. We attribute this to data distribution: protein sequences in the Medium set are longer (see Tab. 1), and the distribution of the mutated amino acids differs (see Fig. A1). Besides, the PLM may have witnessed the wild-type protein during pre-training, which mitigates the overfitting problem. (4) Compared to LLMs, both zero-shot and fine-tuned PLMs achieve superior performance, thanks to their evolutionary knowledge attained from pre-training on large-scale protein sequences. (5) Aligning the representations of protein sequences and texts cannot benefit mutation modeling, as evidenced by the poor performance of ProtST (ESM-2). ", "page_idx": 7}, {"type": "image", "img_path": "yppcLFeZgy/tmp/194a689405d83a2c256b6cec0861d312a4919e3442d595b9553366ab6197d927.jpg", "img_caption": ["Figure 5: Visualization of fitness scores for multi-round protein optimization. The curves indicate the average results, and the shaded regions indicate the standard deviation. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Visualization of protein fitness on multi-round optimization. In addition to single-site mutations, we employ a beam-search algorithm [85] to obtain multi-point substitutions iteratively. We manually write the optimization objective for 6 representative benchmarks, set the number of beams as 20, perform 20 independent runs, and visualize the fitness scores predicted by ESM landscape models [86]. We compare MutaPLM with EvoProtGrad [87], a gradient-based strategy that leverages PLMs for multi-round optimization, as well as with random sampling. More details are presented in Appendix C.4. As shown in Fig. 5, our model consistently yields higher-fitness mutants across 6 proteins with varying objectives, especially in the initial rounds of optimization. These results highlight MutaPLM\u2019s potential in assisting real-world mutagenesis applications. ", "page_idx": 8}, {"type": "text", "text": "4.4 In-depth Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Impacts of transfer learning. We show the impacts of pre-training and fine-tuning in Fig. 6. As the fine-tuning proceeds, the performance of MutaPLM continues to improve on the Easy set but deteriorates on the Medium and Hard sets, indicating overftiting problems on out-of-domain samples. Besides, without pre-training, MutaPLM achieves higher performance for the initial steps, which we attribute to the adaptation cost from pre-training texts to fine-tuning texts. However, the overall ROUGE-L scores decline by $1.56\\%$ for mutation explanation and $1.18\\%$ for mutation engineering as the fine-tuning finalizes. Overall, these results validate our transfer learning design. ", "page_idx": 8}, {"type": "text", "text": "Impacts of chain-of-thought (CoT). To investigate the impacts of the chain-of-thought strategy, we perform ablation studies by (1) replacing the predicted function with the ground truth description, (2) replacing the predicted function with \u2019Unknown function\u2019, (3) removing the delta features for mutation explanation, and (4) removing the mutational effects for mutation engineering. As shown in Tab. 4, removing protein functions leads to a performance drop of $2.80\\%$ for mutation explanation and $1.13\\%$ for mutation engineering. Conversely, using the ground-truth function results in notable improvements, particularly for mutation explanation. Besides, the delta features and mutational effects within the second-round dialog play more significant roles in MutaPLM. These findings highlight the significance of jointly incorporating protein function and mutation information in explaining and navigating protein mutations. ", "page_idx": 8}, {"type": "table", "img_path": "yppcLFeZgy/tmp/12833650dd35badc2e80febf32fd2e76333985575f5f95e1e017c444b0b70008.jpg", "table_caption": ["Table 4: Ablation studies. w/o: without. w/: with. We report average ROUGE-L for mutation explanation and average Recall $@50$ for mutation engineering. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "yppcLFeZgy/tmp/13349c64cb4406227752ddceac0c47d17ceb19cc01f44c5df72bcd70f5eed836.jpg", "img_caption": ["Figure 6: Performance analysis for mutation explanation (blue) and engineering (orange) on pre-training and finetuning. w/o pt: without pre-training. w/ pt: with pre-training. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Limitations and Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "MutaPLM pioneers as the first attempt in the explicit modeling of protein mutations with natural language, and we expect future endeavors on (1) expanding the scale and diversity of the MutaDescribe dataset by integrating multi-point mutations and indels [73], (2) analyzing the alterations of protein 3D structures [88] to deepen the understanding of mutations, and (3) developing active learning [84] pipelines to harness feedbacks from wet-lab experiments in real-world mutagenesis studies. ", "page_idx": 9}, {"type": "text", "text": "While MutaPLM bears promise in mutation explanation and engineering, we emphasize safety concerns that it can be misused to generate pathogenic mutations and harmful bio-agents. Hence, we declare that MutaPLM, upon public release, should be restricted to research purposes, and any further applications should undergo comprehensive experiments and human inspections. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we present MutaPLM, a unified framework harvesting protein language models for mutation explanation and engineering. We propose a protein delta network to model mutations explicitly with protein delta features and develop a transfer learning pipeline with a chain-of-thought strategy to integrate protein mutation knowledge from biomedical texts. Additionally, we construct MutaDescribe, the first large-scale dataset containing diverse proteins and detailed textual annotations for mutations. Our experiments demonstrate that MutaPLM offers insightful explanations for mutational effects and proposes desirable mutants based on textual instructions. We anticipate that the proposed MutaPLM framework and our publicly released dataset will pave the way for novel research avenues and applications in studying proteins. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported by the National Key R&D Program of China (No. 2022YFF1203002) and PharMolix Inc. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Csaba P\u00e1l, Bal\u00e1zs Papp, and Martin J Lercher. An integrated view of protein evolution. Nature reviews genetics, 7(5):337\u2013348, 2006.   \n[2] Misha Soskine and Dan S Tawfik. Mutational effects and the evolution of new protein functions. Nature Reviews Genetics, 11(8):572\u2013582, 2010.   \n[3] Boris Reva, Yevgeniy Antipin, and Chris Sander. Predicting the functional impact of protein mutations: application to cancer genomics. Nucleic acids research, 39(17):e118\u2013e118, 2011.   \n[4] William T Harvey, Alessandro M Carabelli, Ben Jackson, Ravindra K Gupta, Emma C Thomson, Ewan M Harrison, Catherine Ludden, Richard Reeve, Andrew Rambaut, Sharon J Peacock, et al. Sars-cov-2 variants, spike mutations and immune escape. Nature Reviews Microbiology, 19(7):409\u2013424, 2021.   \n[5] Jie Hu, Pai Peng, Xiaoxia Cao, Kang Wu, Juan Chen, Kai Wang, Ni Tang, and Ai-long Huang. Increased immune escape of the new sars-cov-2 variant of concern omicron. Cellular & Molecular Immunology, 19(2):293\u2013295, 2022.   \n[6] Philip J Thomas, Bao-He Qu, and Peter L Pedersen. Defective protein folding as a basis of human disease. Trends in biochemical sciences, 20(11):456\u2013459, 1995.   \n[7] Christopher M Dobson. The structural basis of protein folding and its links with human disease. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 356 (1406):133\u2013145, 2001.   \n[8] Nicholas J Turner. Directed evolution drives the next generation of biocatalysts. Nature chemical biology, 5(8):567\u2013573, 2009.   \n[9] Frances H Arnold. Directed evolution: bringing new chemistry to life. Angewandte Chemie (International Ed. in English), 57(16):4143, 2018.   \n[10] Frances H Arnold and Alexander A Volkov. Directed evolution of biocatalysts. Current opinion in chemical biology, 3(1):54\u201359, 1999.   \n[11] Asako Sawano and Atsushi Miyawaki. Directed evolution of green fluorescent protein by a new versatile pcr strategy for site-directed and semi-random mutagenesis. Nucleic acids research, 28(16):e78\u2013e78, 2000.   \n[12] Eric T Boder, Katarina S Midelfort, and K Dane Wittrup. Directed evolution of antibody fragments with monovalent femtomolar antigen-binding affinity. Proceedings of the National Academy of Sciences, 97(20):10701\u201310705, 2000.   \n[13] Tanlin Sun, Bo Zhou, Luhua Lai, and Jianfeng Pei. Sequence-based prediction of protein protein interaction using a deep-learning algorithm. BMC bioinformatics, 18:1\u20138, 2017.   \n[14] Wenhao Gao, Sai Pooja Mahajan, Jeremias Sulam, and Jeffrey J Gray. Deep learning in protein structural modeling and design. Patterns, 1(9), 2020.   \n[15] Tristan Bepler and Bonnie Berger. Learning the protein language: Evolution, structure, and function. Cell systems, 12(6):654\u2013669, 2021.   \n[16] Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R Eguchi, Po-Ssu Huang, and Richard Socher. Progen: Language modeling for protein generation. arXiv preprint arXiv:2004.03497, 2020.   \n[17] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 44(10):7112\u20137127, 2021.   \n[18] Noelia Ferruz, Steffen Schmidt, and Birte H\u00f6cker. Protgpt2 is a deep unsupervised language model for protein design. Nature communications, 13(1):4348, 2022.   \n[19] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123\u20131130, 2023.   \n[20] Timothy Truong Jr and Tristan Bepler. Poet: A generative model of protein families as sequences-of-sequences. Advances in Neural Information Processing Systems, 36, 2024.   \n[21] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-supervised learning: Generative or contrastive. IEEE transactions on knowledge and data engineering, 35(1):857\u2013876, 2021.   \n[22] Baris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProt Consortium. Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, 31(6):926\u2013932, 2015.   \n[23] Martin Steinegger and Johannes S\u00f6ding. Clustering huge protein sequence sets in linear time. Nature communications, 9(1):2542, 2018.   \n[24] Konstantin Weissenow, Michael Heinzinger, and Burkhard Rost. Protein language-model embeddings for fast, accurate, and alignment-free protein structure prediction. Structure, 30(8): 1169\u20131177, 2022.   \n[25] Noelia Ferruz and Birte H\u00f6cker. Controllable protein design with language models. Nature Machine Intelligence, 4(6):521\u2013532, 2022.   \n[26] Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language models enable zero-shot prediction of the effects of mutations on protein function. Advances in neural information processing systems, 34:29287\u201329303, 2021.   \n[27] Chloe Hsu, Hunter Nisonoff, Clara Fannjiang, and Jennifer Listgarten. Learning protein ftiness models from evolutionary and assay-labeled data. Nature biotechnology, 40(7):1114\u20131122, 2022.   \n[28] Junming Zhao, Chao Zhang, and Yunan Luo. Contrastive fitness learning: Reprogramming protein language models for low-n learning of protein fitness landscape. In International Conference on Research in Computational Molecular Biology, pages 470\u2013474. Springer, 2024.   \n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[30] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \n[31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[32] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[33] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[34] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.   \n[35] Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial. Proteinbert: a universal deep-learning model of protein sequence and function. Bioinformatics, 38(8): 2102\u20132110, 2022.   \n[36] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021.   \n[37] Tomas Hayes, Roshan Rao, Halil Akin, Nicholas J Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Q Tran, Jonathan Deaton, Marius Wiggert, et al. Simulating 500 million years of evolution with a language model. bioRxiv, pages 2024\u201307, 2024.   \n[38] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, 2019.   \n[39] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.   \n[40] Serbulent Unsal, Heval Atas, Muammer Albayrak, Kemal Turhan, Aybar C Acar, and Tunca Do\u02d8gan. Learning functional properties of proteins with language models. Nature Machine Intelligence, 4(3):227\u2013245, 2022.   \n[41] Jun Hu, Zhe Li, Bing Rao, Maha A Thafar, and Muhammad Arif. Improving protein-protein interaction prediction using protein language model and protein network features. Analytical Biochemistry, page 115550, 2024.   \n[42] Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang. Protst: Multi-modality learning of protein sequences and biomedical texts. In International Conference on Machine Learning, pages 38749\u201338767. PMLR, 2023.   \n[43] Yizhen Luo, Xing Yi Liu, Kai Yang, Kui Huang, Massimo Hong, Jiahuan Zhang, Yushuai Wu, and Zaiqing Nie. Toward unified ai drug discovery with multimodal knowledge. Health Data Science, 4:0113, 2024.   \n[44] Le Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Heqi Zheng, Conghui He, Xian-Ling Mao, and Wentao Zhang. Protllm: An interleaved protein-language llm with protein-as-word pre-training. arXiv preprint arXiv:2403.07920, 2024.   \n[45] Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, and Yonghong Tian. Prollama: A protein large language model for multi-task protein language processing. arXiv preprint arXiv:2402.16445, 2024.   \n[46] Mingze Yin, Hanjing Zhou, Yiheng Zhu, Miao Lin, Yixuan Wu, Jialu Wu, Hongxia Xu, ChangYu Hsieh, Tingjun Hou, Jintai Chen, et al. Multi-modal clip-informed protein editing. bioRxiv, pages 2024\u201307, 2024.   \n[47] Philip A Romero and Frances H Arnold. Exploring protein fitness landscapes by directed evolution. Nature reviews Molecular cell biology, 10(12):866\u2013876, 2009.   \n[48] Thomas A Hopf, John B Ingraham, Frank J Poelwijk, Charlotta PI Sch\u00e4rfe, Michael Springer, Chris Sander, and Debora S Marks. Mutation effects predicted from sequence co-variation. Nature biotechnology, 35(2):128\u2013135, 2017.   \n[49] Elodie Laine, Yasaman Karami, and Alessandra Carbone. Gemme: a simple and fast global epistatic model predicting mutational effects. Molecular biology and evolution, 36(11):2604\u2013 2619, 2019.   \n[50] Francois Jeanmougin, Julie D Thompson, Manolo Gouy, Desmond G Higgins, and Toby J Gibson. Multiple sequence alignment with clustal x. Trends in biochemical sciences, 23(10): 403\u2013405, 1998.   \n[51] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep learning\u2013based protein sequence design using proteinmpnn. Science, 378(6615):49\u201356, 2022.   \n[52] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844\u20138856. PMLR, 2021.   \n[53] Pascal Notin, Lood Van Niekerk, Aaron W Kollasch, Daniel Ritter, Yarin Gal, and Debora Susan Marks. Trancepteve: Combining family-specific and family-agnostic models of protein sequences for improved ftiness prediction. In NeurIPS 2022 Workshop on Learning Meaningful Representations of Life, 2022.   \n[54] Douglas M Fowler and Stanley Fields. Deep mutational scanning: a new style of protein science. Nature methods, 11(8):801\u2013807, 2014.   \n[55] Melissa J Landrum and Brandi L Kattman. Clinvar at five years: Delivering on the promise. Human mutation, 39(11):1623\u20131630, 2018.   \n[56] David Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by adaptive sampling for robust design. In International conference on machine learning, pages 773\u2013782. PMLR, 2019.   \n[57] Samuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside, and Andrew Gordon Wilson. Accelerating bayesian optimization for biological sequence design with denoising autoencoders. In International Conference on Machine Learning, pages 20459\u201320478. PMLR, 2022.   \n[58] Nate Gruver, Samuel Stanton, Nathan Frey, Tim GJ Rudner, Isidro Hotzel, Julien LafranceVanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew G Wilson. Protein design with guided discrete diffusion. Advances in Neural Information Processing Systems, 36, 2024.   \n[59] Sam Sinai, Richard Wang, Alexander Whatley, Stewart Slocum, Elina Locane, and Eric D Kelsic. Adalead: A simple and robust adaptive greedy search algorithm for sequence design. arXiv preprint arXiv:2010.02141, 2020.   \n[60] Christof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy, and Lucy Colwell. Model-based reinforcement learning for biological sequence design. In International conference on learning representations, 2019.   \n[61] Andrew Kirjner, Jason Yim, Raman Samusevich, Tommi S Jaakkola, Regina Barzilay, and Ila R Fiete. Optimizing protein fitness using gibbs sampling with graph-based smoothing. In ICML 2023 Workshop: Sampling and Optimization in Discrete Space, 2023.   \n[62] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine. arXiv preprint arXiv:2308.09442, 2023.   \n[63] Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. Continual pre-training of language models. In The Eleventh International Conference on Learning Representations, 2022.   \n[64] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888\u201312900. PMLR, 2022.   \n[65] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[66] Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei Ye, and Quanquan Gu. Structureinformed language models are protein designers. In International Conference on Machine Learning, pages 42317\u201342338. PMLR, 2023.   \n[67] Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Mol-instructions-a large-scale biomolecular instruction dataset for large language models. In The Twelfth International Conference on Learning Representations, 2023.   \n[68] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. arXiv preprint arXiv:1904.09324, 2019.   \n[69] Emmanuel Boutet, Damien Lieberherr, Michael Tognolli, Michel Schneider, Parit Bansal, Alan J Bridge, Sylvain Poux, Lydie Bougueleret, and Ioannis Xenarios. Uniprotkb/swiss-prot, the manually annotated section of the uniprot knowledgebase: how to use the entry view. Plant bioinformatics: methods and protocols, pages 23\u201354, 2016.   \n[70] Kathi Canese and Sarah Weis. Pubmed: the bibliographic database. The NCBI handbook, 2(1), 2013.   \n[71] Martin Steinegger and Johannes S\u00f6ding. Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature biotechnology, 35(11):1026\u20131028, 2017.   \n[72] Adam J Riesselman, John B Ingraham, and Debora S Marks. Deep generative models of genetic variation capture the effects of mutations. Nature methods, 15(10):816\u2013822, 2018.   \n[73] Pascal Notin, Aaron Kollasch, Daniel Ritter, Lood Van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, Ada Shaw, Rose Orenbuch, Ruben Weitzman, et al. Proteingym: largescale benchmarks for protein fitness prediction and design. Advances in Neural Information Processing Systems, 36, 2024.   \n[74] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.   \n[75] Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Qiang Zhang, Jiazhang Lian, and Huajun Chen. Ontoprotein: Protein pretraining with gene ontology embedding. In International Conference on Learning Representations, 2021.   \n[76] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747, 2023.   \n[77] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021.   \n[78] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.   \n[79] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \n[80] Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. arXiv preprint arXiv:2311.07361, 2023.   \n[81] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318, 2002.   \n[82] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381, 2004.   \n[83] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1\u201323, 2021.   \n[84] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. ACM computing surveys (CSUR), 54 (9):1\u201340, 2021.   \n[85] Markus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation, pages 56\u201360, 2017.   \n[86] Zhizhou Ren, Jiahan Li, Fan Ding, Yuan Zhou, Jianzhu Ma, and Jian Peng. Proximal exploration for model-guided protein sequence design. In International Conference on Machine Learning, pages 18520\u201318536. PMLR, 2022.   \n[87] Patrick Emami, Aidan Perreault, Jeffrey Law, David Biagioni, and Peter St John. Plug & play directed evolution of proteins with gradient-based discrete mcmc. Machine Learning: Science and Technology, 4(2):025014, 2023.   \n[88] Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pages 1\u20133, 2024.   \n[89] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.   \n[90] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[91] Drew H Bryant, Ali Bashir, Sam Sinai, Nina K Jain, Pierce J Ogden, Patrick F Riley, George M Church, Lucy J Colwell, and Eric D Kelsic. Deep diversification of an aav capsid protein by machine learning. Nature Biotechnology, 39(6):691\u2013696, 2021.   \n[92] Emily E Wrenbeck, Laura R Azouz, and Timothy A Whitehead. Single-mutation fitness landscapes for an enzyme on multiple substrates reveal specificity is globally encoded. Nature communications, 8(1):15695, 2017.   \n[93] Karen S Sarkisyan, Dmitry A Bolotin, Margarita V Meer, Dinara R Usmanova, Alexander S Mishin, George V Sharonov, Dmitry N Ivankov, Nina G Bozhanova, Mikhail S Baranov, Onuralp Soylemez, et al. Local ftiness landscape of the green fluorescent protein. Nature, 533 (7603):397\u2013401, 2016.   \n[94] Lea M Starita, Jonathan N Pruneda, Russell S Lo, Douglas M Fowler, Helen J Kim, Joseph B Hiatt, Jay Shendure, Peter S Brzovic, Stanley Fields, and Rachel E Klevit. Activity-enhancing mutations in an e3 ubiquitin ligase identified by high-throughput mutagenesis. Proceedings of the National Academy of Sciences, 110(14):E1263\u2013E1272, 2013.   \n[95] Justin R Klesmith, John-Paul Bacik, Ryszard Michalczyk, and Timothy A Whitehead. Comprehensive sequence-flux mapping of a levoglucosan utilization pathway in e. coli. ACS synthetic biology, 4(11):1235\u20131243, 2015.   \n[96] Jochen Weile, Song Sun, Atina G Cote, Jennifer Knapp, Marta Verby, Joseph C Mellor, Yingzhou Wu, Carles Pons, Cassandra Wong, Natascha van Lieshout, et al. A framework for exhaustively mapping functional missense variants. Molecular systems biology, 13(12):957, 2017.   \n[97] Tyler N Starr, Allison J Greaney, William W Hannon, Andrea N Loes, Kevin Hauser, Josh R Dillen, Elena Ferri, Ariana Ghez Farrell, Bernadeta Dadonaite, Matthew McCallum, et al. Shifting mutational constraints in the sars-cov-2 receptor-binding domain during viral evolution. Science, 377(6604):420\u2013424, 2022.   \n[98] Surojit Biswas, Grigory Khimulya, Ethan C Alley, Kevin M Esvelt, and George M Church. Low-n protein engineering with data-efficient deep learning. Nature methods, 18(4):389\u2013396, 2021.   \n[99] Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena-Hurtado, Aidan N Gomez, Debora Marks, and Yarin Gal. Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. In International Conference on Machine Learning, pages 16990\u201317017. PMLR, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Details of MutaPLM ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Model Architecture ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our protein delta network consists of a protein language model (PLM), a large language model (LLM), a wild-type encoder, a delta encoder, a delta decoder, and two prediction heads for mutation. We introduce these components as follows: ", "page_idx": 16}, {"type": "text", "text": "Protein language model. We formulate the wild-type protein as an amino acid sequence $x_{\\mathrm{{wt}}}=$ $\\left[x_{1}^{\\mathrm{(wt)}},x_{2}^{\\mathrm{(wt)}},\\cdots,x_{L}^{\\mathrm{(wt)}}\\right]$ of length $L$ . We focus on single-site substitution mutants, denoted by its sequence $x_{\\mathrm{mt}}=\\left[x_{1}^{(\\mathrm{mt})},x_{2}^{(\\mathrm{mt})},\\cdot\\cdot\\cdot\\,,x_{L}^{(\\mathrm{mt})}\\right]$ satisfying $\\begin{array}{r}{\\mathcal{H}(x_{\\mathrm{wt}},x_{\\mathrm{mt}})=1}\\end{array}$ , where $\\mathcal{H}(\\cdot,\\cdot)$ is the Hamming distance. We adopt ESM-2 (650M) [19] as our protein language model $f_{\\mathrm{PLM}}$ , which transforms the protein sequences into dense feature vectors as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{\\mathrm{wt}}=\\left[h_{1}^{(\\mathrm{wt})},h_{2}^{(\\mathrm{wt})},\\cdot\\cdot\\cdot,h_{L}^{(\\mathrm{wt})}\\right]=f_{\\mathrm{PLM}}(x_{\\mathrm{wt}}),}\\\\ &{h_{\\mathrm{mt}}=\\left[h_{1}^{(\\mathrm{mt})},h_{2}^{(\\mathrm{mt})},\\cdot\\cdot\\cdot,h_{L}^{(\\mathrm{mt})}\\right]=f_{\\mathrm{PLM}}(x_{\\mathrm{mt}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we introduce the mutational representation, $h_{\\Delta}$ , calculated as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nh_{\\Delta}=\\left[h_{1}^{(\\Delta)},h_{2}^{(\\Delta)},\\cdot\\cdot\\cdot{}~,h_{L}^{(\\Delta)}\\right]=h_{\\mathrm{mt}}-h_{\\mathrm{wt}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Large language model. Similarly, we formulate biomedical texts as a sequence of tokens $t=$ $[t_{1},t_{2},\\cdot\\cdot\\cdot\\,,t_{N}]$ . We initialize our LLM with BioMedGPT-LM [62], which is obtained by continually pre-training LLaMA2-7B [31] on biomedical corpus. The large language model $f_{\\mathrm{LLM}}$ takes the following steps to transform $t$ into latent features and output distributions of the next token: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e=[e_{1},e_{2},\\cdot\\cdot\\cdot\\cdot,e_{N}]=g_{\\mathrm{emb}}(t),}\\\\ &{z_{t}=[z_{1},z_{2},\\cdot\\cdot\\cdot\\cdot,z_{N}]=g_{\\mathrm{transformers}}(e),}\\\\ &{P(t_{i}|t_{<i})=g_{\\mathrm{LM}}(h_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $g_{\\mathrm{emb}}$ is the word embedding layer, $z_{t}$ is the textual representation calculated by transformer blocks gtransformers, $g_{\\mathrm{LM}}$ is the language modeling head, and $P\\bar{(t_{i}|t_{<i})}$ is the probability distribution of $i$ -th token based on preceding tokens. ", "page_idx": 16}, {"type": "text", "text": "Wild-type encoder. The wild-type encoder comprises $K$ trainable query vectors $q_{\\mathrm{wt}}\\,=$ $\\left[q_{1},q_{2},\\cdots\\,,q_{K}\\right]$ and a cross attention module. It transforms the wild-type representations $h_{\\mathrm{wt}}$ into a fixed number of features as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{\\mathrm{wt}}=\\left[z_{1}^{(\\mathrm{wt})},z_{2}^{(\\mathrm{wt})},\\cdot\\cdot\\cdot\\cdot,z_{K}^{(\\mathrm{wt})}\\right]=\\operatorname{CrossAttention}_{\\mathrm{wt}}(q_{\\mathrm{wt}},h_{\\mathrm{wt}},h_{\\mathrm{wt}}),}\\\\ &{\\operatorname{CrossAttention}(Q,K,V)=\\operatorname{Softmax}\\left(\\cfrac{\\hat{Q}\\hat{K}^{T}}{\\sqrt{d_{k}}}\\right)\\hat{V}}\\\\ &{\\hat{Q}=Q W^{Q},\\hat{K}=K W^{K},\\hat{V}=V W^{V},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $W^{Q},W^{K},W^{V}$ are trainable parameters, and $d_{k}$ is the feature dimension. ", "page_idx": 16}, {"type": "text", "text": "Delta encoder. The delta encoder follows the same architecture as the wild-type encoder. It encodes the protein delta features as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nz_{\\Delta}=\\left[z_{1}^{(\\Delta)},z_{2}^{(\\Delta)},\\cdot\\cdot\\cdot\\ ,z_{K}^{(\\Delta)}\\right]=\\mathrm{CrossAttention}_{\\mathrm{enc}}(q_{\\Delta},h_{\\Delta},h_{\\Delta}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $q_{\\Delta}$ are the $K$ trainable queries, and the cross attention is calculated following Equ. A4.   \nNotably, the wild-type encoder and delta encoder comprise independent parameters. ", "page_idx": 16}, {"type": "text", "text": "Delta decoder. The delta decoder transforms the protein delta features $z_{\\Delta}$ back to the original mutation representations $h_{\\Delta}$ . It comprises a cross-attention layer and a two-layer feed-forward network with ReLU activation. Specifically: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{z}_{\\Delta}=\\mathrm{CrossAttention}_{\\mathrm{dec}}\\big(h_{\\mathrm{wt}},z_{\\Delta},z_{\\Delta}\\big),}\\\\ &{h_{\\Delta}=\\mathrm{FeedForward}(\\tilde{z}_{\\Delta}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Table A1: Average $l_{2}$ -norm of MutaPLM\u2019s intermediate representations on MutaDescribe. ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "yppcLFeZgy/tmp/c6063b75ff9bbb6c767548305006f644b0bb60d96057431bc77914de0784251e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "where the cross attention is calculated following Equ. A4. ", "page_idx": 17}, {"type": "text", "text": "Mutation prediction heads. After reconstructing the mutant representation by $h_{\\mathrm{mt}}=h_{\\mathrm{wt}}+h_{\\Delta}$ , we develop a position prediction head $f_{\\mathrm{pos}}$ and a language modeling head $f_{\\mathrm{LM}}$ to predict the mutation. Specifically: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P\\left(x_{i}^{(\\mathrm{mt})}\\neq x_{i}^{(\\mathrm{wt})}\\right)=f_{\\mathrm{pos}}\\left(h_{i}^{(\\mathrm{mt})}\\right),}\\\\ &{P\\left(x_{i}^{(\\mathrm{mt})}\\right)=f_{\\mathrm{LM}}\\left(h_{i}^{(\\mathrm{mt})}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $P\\left(x_{i}^{(\\mathrm{mt})}\\neq x_{i}^{(\\mathrm{wt})}\\right)$ denotes the probability of $i$ -th amino acid to be mutated, and $P\\left(x_{i}^{(\\mathrm{mt})}\\right)$ denotes the probability distribution of the $i$ -th amino acid. The parameters of the position prediction head are initialized from scratch, and those of the language modeling head are derived from the PLM. ", "page_idx": 17}, {"type": "text", "text": "A.2 Justifications for Mutational Features ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To model mutations explicitly, we leverage the subtraction of the wild-type and mutant representations as the mutational features $h_{\\Delta}$ , which is subsequently processed by the delta encoder. One of the essential considerations is that the PLM is overly smooth, making $h_{\\Delta}$ too small and less informative. However, we argue that due to the non-smooth nature of the protein ftiness landscape [61], the output representations of PLMs are also non-smooth. Moreover, after training, the delta encoder learns to capture the orientation of $h_{\\Delta}$ , yielding a $z_{\\Delta}$ with an appropriate norm. We also present empirical justification by calculating the average $l_{2}$ -norm of $h_{\\mathrm{wt}},\\,h_{\\Delta}$ , and $z_{\\Delta}$ on MutaDescribe, which are displayed in Tab. A1. ", "page_idx": 17}, {"type": "text", "text": "A.3 Pre-training Objectives ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "MutaPLM performs pre-training on large-scale protein-relevant literature. Given the protein sequence $x_{\\mathrm{wt}}$ and its semantically related text $t$ , we optimize the following objectives: ", "page_idx": 17}, {"type": "text", "text": "Protein-to-text generation. We first concatenate the latent wild-type features ${\\mathcal{Z}}_{\\mathrm{wt}}$ in Equ. A4 and the text embeddings $e$ in Equ. A3. We perform conditional auto-regressive language modeling that aims to generate $t$ based on the protein representations and previous tokens. The objective is calculated as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{z=\\underbrace{[z_{1},z_{2},\\cdots\\,,z_{K}}_{\\mathrm{protein}},\\underbrace{z_{K+1},\\cdots\\,,z_{K+N}}_{\\mathrm{text}}]=g_{\\mathrm{transformers}}\\left([z_{\\mathrm{wt}};e]\\right),}}\\\\ {{\\displaystyle P(t_{i}|t_{<i},z_{\\mathrm{wt}})=g_{\\mathrm{LM}}(z_{K+i})},}\\\\ {{\\displaystyle\\mathcal{L}_{p2t}=\\frac{1}{N}\\sum_{i=1}^{N}H\\left[t_{i},P(t_{i}|t_{<i},z_{\\mathrm{wt}})\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $H(\\cdot,\\cdot)$ denotes cross-entropy. ", "page_idx": 17}, {"type": "text", "text": "Text-to-protein generation. We first append $K$ trainable soft tokens $\\boldsymbol{s}=\\left[s_{1},s_{2},\\cdots,s_{K}\\right]$ to the input token embeddings to summarize textual semantics. Then, we derive $z_{\\Delta}$ as the last hidden state of $s$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{z}=\\underbrace{[\\tilde{z}_{1},\\tilde{z}_{2},\\cdots,\\tilde{z}_{N}}_{\\mathrm{text}},\\underbrace{\\tilde{z}_{N+1},\\cdots,\\tilde{z}_{N+K}}_{z_{\\Delta}}]=g_{\\mathrm{transformers}}([e;s]),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $s$ denotes the soft tokens. We pass $z_{\\Delta}$ into the delta decoder to obtain $h_{\\Delta}$ as in Equ. A6. It is worth noting that in this stage, we are aimed at aligning the feature space of PLMs and LLMs, and $z_{\\Delta}$ and $h_{\\Delta}$ are NOT related to protein mutations. ", "page_idx": 17}, {"type": "text", "text": "Then, we randomly mask $15\\%$ amino acids in the protein sequence. We adopt the conditional masked language modeling objective to reconstruct the masked tokens as follows: ", "page_idx": 17}, {"type": "table", "img_path": "yppcLFeZgy/tmp/11513a343455bff19a5926dc42ded6f1575cb6646cff2017562ac4f3534984d7.jpg", "table_caption": ["Table A2: Prompt templates for fine-tuning. The first and second round dialogs are composed of system prompts, latent wild-type and delta features, and special tokens including $\\tt{<}B O P>$ , $<\\tt E O P>$ , $\\tt{<B O M>}$ , <EOM>. We highlight the parts that are used to calculate the objectives. "], "table_footnote": [], "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{h}_{\\mathrm{mask}}=f_{\\mathrm{PLM}}(\\boldsymbol{x}_{\\mathrm{mask}}),}\\\\ &{\\tilde{\\boldsymbol{h}}=\\left[\\tilde{h}_{1},\\tilde{h}_{2},\\cdots,\\tilde{h}_{L}\\right]=h_{\\mathrm{mask}}+h_{\\Delta},}\\\\ &{P\\left(x_{i}^{\\mathrm{(wt)}}\\big|x_{\\mathrm{mask}},h_{\\Delta}\\right)=f_{\\mathrm{LM}}\\left(\\tilde{h}_{i}\\right),}\\\\ &{\\mathcal{L}_{t2p}=\\frac{1}{|\\mathcal{M}|}\\sum_{i\\in\\mathcal{M}}H\\left[x_{i},P\\left(x_{i}^{\\mathrm{(wt)}}\\big|x_{\\mathrm{mask}},h_{\\Delta}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $x_{\\mathrm{mask}}$ is the masked sequence of the wild-type $x_{\\mathrm{wt}}$ , and $\\mathcal{M}$ denotes the masked positions. ", "page_idx": 18}, {"type": "text", "text": "Overall objective. The overall objective for pre-training is calculated by: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{1}=\\mathbb{E}_{(x_{\\mathrm{wt}},t)\\sim\\mathcal{D}_{1}}(\\mathcal{L}_{p2t}+\\mathcal{L}_{t2p}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbb{E}$ denotes expectation, and $\\mathcal{D}_{1}$ denotes our pre-training dataset. ", "page_idx": 18}, {"type": "text", "text": "A.4 Fine-tuning Objectives ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We employ a chain-of-thought (CoT) strategy to reason over protein functions and mutational effects in a two-round dialog. Given the wild type sequence $x_{\\mathrm{wt}}$ , the mutant sequence $x_{\\mathrm{mt}}$ , the description of protein functions $t_{\\mathrm{func}}$ and the description of mutation effects $t_{\\Delta}$ , we calculate the following objectives: ", "page_idx": 18}, {"type": "text", "text": "First-round dialog. We first prompt the LLM to generate function descriptions $t_{\\mathrm{func}}~=$ $\\left[t_{1}^{(\\mathrm{func})},t_{2}^{(\\mathrm{func})},\\cdot\\cdot\\cdot\\ ,t_{M}^{(\\mathrm{func})}\\right]$ based on the wild-type protein. We perform conditional auto-regressive language modeling as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{func}}=\\frac{1}{M}\\sum_{i=1}^{M}H\\left[t_{i}^{(\\mathrm{func})},P\\left(t_{i}^{(\\mathrm{func})}\\big|t_{<i}^{(\\mathrm{func})},z_{\\mathrm{wt}}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The predictions of protein functions $y_{\\mathrm{func}}=\\left[y_{1}^{\\mathrm{(func)}},y_{2}^{\\mathrm{(func)}},\\cdot\\cdot\\cdot,y_{N}^{\\mathrm{(func)}}\\right]$ is derived by: ", "page_idx": 18}, {"type": "equation", "text": "$$\ny_{i}^{\\mathrm{(func)}}=\\operatorname*{argmax}\\left\\{P\\left(y_{i}^{\\mathrm{(func)}}\\big|y_{<i}^{\\mathrm{(func)}},z_{\\mathrm{wt}}\\right)\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Second-round dialog for mutation explanation. We prompt the LLM to generate textual descriptions for mutation effects $t_{\\Delta}\\,=\\,\\left[t_{1}^{(\\Delta)},t_{2}^{(\\Delta)},\\cdot\\cdot\\cdot\\,,t_{T}^{(\\Delta)}\\right]$ based on the function information in the ", "page_idx": 18}, {"type": "text", "text": "first-round dialog and protein delta features $z_{\\Delta}$ . The objective is calculated as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{exp}}=\\frac{1}{T}\\sum_{i=1}^{T}H\\left[t_{i}^{(\\Delta)},P\\left(t_{i}^{(\\Delta)}|t_{<i}^{(\\Delta)},y_{\\mathrm{func}},z_{\\Delta},z_{\\mathrm{wt}}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Second-round dialog for mutation engineering. We apply the same soft tokens $s$ as in pre-training to the input prompt to calculate the delta features based on the first-round dialog and descriptions of mutational effects: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{z}=\\underbrace{\\left[\\hat{z}_{1},\\hat{z}_{2},\\cdots,\\hat{z}_{N},\\hat{z}_{N+1},\\cdots,\\hat{z}_{N+K}\\right]}_{\\mathrm{prompt}}=g_{\\mathrm{trampt}}{\\left(\\left[t_{\\mathrm{prompt}};s\\right]\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $t_{\\mathrm{prompt}}$ is the input embeddings of the prompt involving the first-round dialog and the mutational effects. ", "page_idx": 19}, {"type": "text", "text": "Then, reconstructing $h_{\\mathrm{mt}}=h_{\\mathrm{wt}}+h_{\\Delta}$ with the delta decoder, we calculate the weighted cross-entropy loss for the mutation position and the mutated amino acid with the prediction heads: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}_{\\mathrm{eng}}=-\\frac{1}{L}\\sum_{i=1}^{L}\\left\\{\\mathbb{1}\\left\\{x_{i}^{(\\mathrm{mt})}=x_{i}^{(\\mathrm{wt})}\\right\\}\\log(1-f_{\\mathrm{pos}}(h_{i}^{\\mathrm{mt}}))\\right.}\\\\ {\\displaystyle\\left.+\\,\\lambda\\cdot\\mathbb{1}\\left\\{x_{i}^{(\\mathrm{mt})}\\neq x_{i}^{(\\mathrm{wt})}\\right\\}\\log f_{\\mathrm{pos}}(h_{i}^{\\mathrm{mt}})\\right.}\\\\ {\\displaystyle\\left.-L\\cdot\\mathbb{1}\\left\\{x_{i}^{(\\mathrm{mt})}\\neq x_{i}^{(\\mathrm{wt})}\\right\\}H\\left[x_{i}^{(\\mathrm{mt})},f_{\\mathrm{LM}}(h_{i}^{\\mathrm{mt})})\\right]\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbb{I}\\{\\cdot\\}$ is the boolean indicator function, and $\\lambda$ is a hyper-parameter controlling label weight. In our experiments, we set $\\lambda=50$ . ", "page_idx": 19}, {"type": "text", "text": "The overall objective is calculated as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{2}=\\mathbb{E}_{(x_{\\mathrm{w}},x_{\\mathrm{mt}},t_{\\mathrm{func}},t_{\\Delta})\\sim\\mathcal{D}_{2}}(\\mathcal{L}_{\\mathrm{func}}+\\mathcal{L}_{\\mathrm{exp}}+\\mathcal{L}_{\\mathrm{eng}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathcal{D}_{2}$ is our fine-tuning dataset. ", "page_idx": 19}, {"type": "text", "text": "The prompt templates for fine-tuning are displayed in Tab. A2. ", "page_idx": 19}, {"type": "text", "text": "B Training data ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 Pre-training Data ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our pre-training data involves 1.1M protein-text pairs collected from the UniProtKB/SwissProt [69] database. We download 467.8K proteins with the Publications entry and retrieve 257.2K PubMed [70] abstracts based on the reference information. ", "page_idx": 19}, {"type": "text", "text": "B.2 Fine-tuning and Testing Data: MutaDescribe ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To create a natural language annotated dataset for protein mutations, we first collect 164K samples from the Phenotypes & Variants entry of UniProtKB/SwissProt. After deduplication and removing sites without valid text annotations, we obtain 107K mutants for 21K proteins as our raw data, comprising 33K natural variants and 74K mutagenesis sequences. ", "page_idx": 19}, {"type": "text", "text": "Unfortunately, the collected raw data is not suitable for protein mutation modeling, mainly owing to the following problems: (1) As shown in Tab. A3, the expert-revised annotations within UniProtKB contain an average of 9.4 words, containing limited information. (2) Through analyzing the polarity of the mutational effects, we observe that the number of malignant and benign mutations are imbalanced $(\\sim9{:}1)$ , which may mislead model predictions. ", "page_idx": 19}, {"type": "text", "text": "To address these issues, (1) we perform data enrichment by collecting the abstracts of the biological literature in which the mutation is mentioned. We retrieve 50K publications based on the reference information of the mutation available in UniProtKB and prompt GPT-3.5-turbo to extract relevant information from the abstracts. The prompt template is visualized in Tab. A5. After ChatGPT enrichment, the textual annotations are expanded with an average of 28.3 words. (2) We generate $64.5\\mathrm{K}$ additional reverse samples. Specifically, for each malignant and benign mutation, we exchange the wild-type and mutant and prompt GPT-3.5-turbo to filp the polarity of the textual descriptions for mutational effects. We empirically find that the quality of mutation descriptions using GPT-3.5-turbo and GPT-4 is similar, and therefore we opt for GPT-3.5-turbo to save API costs. ", "page_idx": 19}, {"type": "table", "img_path": "yppcLFeZgy/tmp/92ebbfd3d721552d477d6781e5b11daf6ad36ef15168657fcd76c057702750b1.jpg", "table_caption": ["Table A3: An Overview of MutaDescribe. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "We implement two splitting strategies for our dataset. For structural split, we first partition our dataset into training, validation, and test sets. Then, for each wild-type sequence in the test set, we calculate the maximum sequence homology with the wild-type sequences in the training set by MMseqs2 [71]. Based on the homology, we divide the test set into three subsets. The Easy subset comprises 460 mutants with homology between 0.95 and 1, the Medium subset comprises 384 mutants with homology between 0.5 and 0.95, and the Hard subset comprises 404 mutants with homology between 0 and 0.5. For temporal split, we extract the publication date of the literature reporting each mutation. Mutations studied before 2022 are used as training and validation sets, while those studied in 2022 and 2023 comprise the test set. The train/valid/test set comprises 156K, 8K, and 1.6K samples, respectively. The detailed statistics of temporal split are shown in Tab. A4. ", "page_idx": 20}, {"type": "text", "text": "We present a closer look at our MutaDescribe dataset in Fig. A1, displaying the length of protein sequences, the number of words in textual annotations, the number of mutation samples per protein, the distribution of the originating species, the distribution of the cellular localization and the distribution of the mutated amino acid. We show in our illustrations that MutaDescribe is a large-scale, diverse, and detailed annotated dataset for studying protein mutations. ", "page_idx": 20}, {"type": "text", "text": "C Experiment Settings ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Baselines for Mutation Explanation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For mutation explanation, we implement the following baselines: ", "page_idx": 20}, {"type": "text", "text": "Galactica-6.7B [74]. This baseline is a unified large-language model pre-trained on scientific papers and protein knowledge bases. We prompt the model to investigate if it could explain mutational effects in a zero-shot manner. ", "page_idx": 20}, {"type": "text", "text": "ProLLaMA [45]. This baseline is developed on LLaMA2-7B by further pre-training the model on protein sequences from UniRef50 [22]. Similarly, we perform zero-shot mutation explanation by prompting. ", "page_idx": 20}, {"type": "text", "text": "Mol-Instructions [67]. We implement the protein-oriented model of Mol-Instructions that is instruction-tuned from LLaMA2-7B [31]. We perform zero-shot prompting that provides the model with the name and amino acid sequence of the protein sequence and task definitions. ", "page_idx": 20}, {"type": "text", "text": "GPT-4 [33] with in-context learning. We adopt the 0613 version of GPT-4, the most advanced LLM in natural language processing. In addition to the protein name, wild-type sequence, and mutation ", "page_idx": 20}, {"type": "table", "img_path": "yppcLFeZgy/tmp/805c214680972193f5623142080028409173fb97ae92a0a53e8236c9fefeb51d.jpg", "table_caption": ["Table A4: Statistics of the temporal split. We report the number of proteins and samples, the average protein sequence length, and the average number of words for mutational effects. "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "yppcLFeZgy/tmp/073ca6cdeb2d9b95ab9895cf7d91ab4dfa18327a806e5d26012315bd66e001af.jpg", "img_caption": ["Figure A1: Detailed statistics of the MutaDescribe dataset. We show (a) the length of protein sequences, (b) the number of words in textual annotations, (c) the number of mutation samples per protein, (d) the distribution of the originating species, (e) the distribution of the cellular localization and (f) the distribution of the mutated amino acid. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table A5: Prompt template for data enrichment. We prompt GPT-3.5-turbo to extract relevant information from the abstracts of the biological literature in which the mutation is mentioned. ", "page_idx": 21}, {"type": "text", "text": "[System prompt]You will be provided with a document and some relevant mutation sites (for example, site A21D indicates a mutation from A to D at position 21). First, determine whether these sites are mentioned in the document. If so, extract the text from the document that describes the functional changes caused by these sites. Otherwise, you must extract any functional changes mentioned in the document. For each site, please try to extract the corresponding protein name or gene name. You must be accurate and clear. Return a series of JSON documents, with each JSON formatted as follows: ", "page_idx": 21}, {"type": "text", "text": "{\"Mutation Site\": <provided mutation site>, \"Mentioned\": <whether this site is mentioned in the document> \"protein_name\": <protein name corresponding to the site>, gene_name\": <gene name corresponding to the site>, \"Functional_changes\": <functional information>} ", "page_idx": 21}, {"type": "text", "text": "[User prompt] document: <document> sites: <list of mutation sites> ", "page_idx": 21}, {"type": "table", "img_path": "yppcLFeZgy/tmp/14bf8d46662839323ba59ed7ae81f7de2bf885ba3418057c11a2211aba697bfc.jpg", "table_caption": ["Table A6: Prompt templates for each baseline for mutation explanation. {original_aa} and {mutated_aa} denote the amino acid before and after the mutation respectively. {fitness_change} is the subtraction of the PLM-calculated evolutionary plausibility scores between the mutant and wild-type. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "information, we provide few-shot demonstrations to facilitate in-context learning. For the 1-shot and 5-shot baseline, we randomly sample 1 and 5 samples from the training set of MutaDescribe. For the kNN-based 5-shot baseline, we follow [80] to search for relevant samples based on the sequence homology calculated by MMseqs2 [71]. We select 5 samples from the training set with the highest homology as few-shot demonstrations for each test sample. ", "page_idx": 23}, {"type": "text", "text": "GPT- $4+\\mathbf{ESM}{\\ \u00b7}2$ [19]. ESM-2 is a popular protein language model pre-trained on evolutionary-scale databases. Given a mutation, we mask the mutated position and utilize ESM-2 (650M) to predict the logits for the mutated amino acid. Following [26], we adopt the subtraction between the mutant and wild-type logits as the evolutionary plausibility scores. We follow the 5-shot kNN setting on GPT-4 and provide the scores as additional information. ", "page_idx": 23}, {"type": "text", "text": "GPT- $^{4+}$ OntoProtein [75]. OntoProtein is a text-augmented PLM that aligns protein sequences with gene ontology definitions. We follow the GPT- $4+\\mathrm{ESM}{-2}$ baseline to predict mutational effects based on evolutionary plausibility and kNN few-shot demonstrations. ", "page_idx": 23}, {"type": "text", "text": "AugmentedESM [27]. In the original paper, the model is designed to solve ftiness regression tasks by linearly combining the adaptive ftiness score calculated following [26] and the amino acid sequence. We slightly adapt the model to perform mutation explanation by feeding the fitness score and the raw protein sequence into BioMedGPT-LM. We fine-tune the LLM with the casual auto-regressive language modeling objective on mutation effects. The hyperparameters for fine-tuning are the same as MutaPLM. ", "page_idx": 23}, {"type": "text", "text": "Finetuned ESM-2. Similar to MiniGPT-4 [89], we translate each residue representation of ESM-2 (650M) [19] into LLM input embeddings using a linear projection layer. We fine-tune BioMedGPTLM with the casual auto-regressive language modeling objective on mutation effects based on the translated features of the wild-type and mutant. The hyperparameters for fine-tuning are also the same as MutaPLM. ", "page_idx": 23}, {"type": "text", "text": "The prompts for our baselines are displayed in Tab. A6. ", "page_idx": 23}, {"type": "text", "text": "C.2 Baselines for Mutation Engineering ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For mutation engineering, we implement the following baselines: ", "page_idx": 23}, {"type": "text", "text": "Random. As the name suggests, the proposed mutations are randomly sampled from every possible single-site substitution with equal probability. ", "page_idx": 23}, {"type": "text", "text": "GPT-4 [33] with in-context learning. We provide few-shot examples for GPT-4 to suggest protein mutations, and the sampling strategy is the same as in mutation explanation. We evaluate accuracy and top-50 recall with a two-round dialog. In the first-round dialog, we directly prompt GPT-4 to provide 50 mutations on arbitrary positions. In the second-round dialog, we provide the model with the ground-truth position and ask ", "page_idx": 23}, {"type": "text", "text": "ESM-2 [19]. We feed the whole sequence into the PLM to calculate the output logits for each amino acid. We rank mutations by the subtraction of the mutant and wild-type logits. ", "page_idx": 23}, {"type": "text", "text": "OntoProtein [75]. This baseline follows the same implementation as ESM2-650M. ", "page_idx": 23}, {"type": "text", "text": "ProtST (ESM-2) [42]. ProtST trains a series of PLMs by contrastive learning [90] between protein sequences and biomedical texts. Hence, we implement a cross-modal retrieval strategy, using the cosine similarity between the mutated sequence and the textual description of mutational effects to score mutations. We opt not to report top-50 recall scores due to: (1) unaffordable computational costs, as each possible mutation requires an individual forward pass, and (2) poor performance, as the baseline merely outperforms random guesses. ", "page_idx": 23}, {"type": "text", "text": "Fine-tuned BioMedGPT. We provide the LLM with the wild-type sequence and textual instructions of desired mutational effects, and fine-tune the model to propose mutations. To evaluate accuracy, we additionally provide the mutated position and prompt the model to generate the mutated amino acid. To evaluate top-50 recall, we prompt the model to generate a single mutation, since our dataset only comprises one ground-truth mutation. The evaluations are performed within two independent sessions, and we combine the causal auto-regressive language modeling objective of both sessions during fine-tuning. ", "page_idx": 23}, {"type": "table", "img_path": "yppcLFeZgy/tmp/2faa00f1d927d1890754a71b7e0afb64c1ed2cc538d932f4721731e8cc6f5333.jpg", "table_caption": ["Table A7: Prompt template for few-shot GPT-4 and fine-tuned BioMedGPT in mutation engineering. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Fine-tuned ESM-2. We leverage BioMedBERT [83] to encode the textual instructions. We employ a cross-attention layer that takes the ESM-2 representations of the wild-type sequence as queries and the BioMedBERT representations as keys and values. The outputs are fed into a position prediction head and a language modeling head to predict mutations, which is the same as MutaPLM. ", "page_idx": 24}, {"type": "text", "text": "The prompt templates for GPT-4 and fine-tuned BioMedGPT are presented in Tab. A7. ", "page_idx": 24}, {"type": "text", "text": "C.3 Human-AI Collaborative Evaluation for Mutation Explanation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Due to the complexity of biomedical texts, we develop a human-AI collaborative evaluation pipeline to comment on the accuracy and helpfulness of predicted mutational effects. Specifically, we query GPT-4 to compare model predictions with ground-truth annotations as in Tab. A8 and categorize them as follows. ", "page_idx": 24}, {"type": "text", "text": "Table A8: Prompt template for GPT-4 evaluation. We leverage GPT-4 to categorize predictions into Accurate, Relevant, Opposite, and Irrelevant, based on the relevance between the predicted functional alterations and ground-truth explanations. ", "page_idx": 25}, {"type": "text", "text": "[System prompt] You are an expert in biology and protein sciences. You want to figure out the effects of protein mutations by alterations of protein functions. Now we provide you with two descriptions of protein mutational effects in a JSON format, where the \"label\" denotes the ground truth description of the mutational effects, and the \"prediction\" denotes the prediction of a model. You should be precise and faithful in evaluating if the predicted mutation effects are semantically related to the ground truth. You should answer with one of the following categories: ", "page_idx": 25}, {"type": "text", "text": "(1) Accurate. The prediction and the label describe the same functions that are altered, and the extent of functional changes is mostly the same (For example, \"strongly decrease\" and \"abolish\"). ", "page_idx": 25}, {"type": "text", "text": "(2) Relevant. The prediction and the label describe the same functions that are altered, and the extent of functional changes are in the same direction (For example, \"strongly increase\" and \"slightly increase\"). ", "page_idx": 25}, {"type": "text", "text": "(3) Opposite. The prediction and the label describe the same functions that are altered, but the functional changes are opposite (For example, \"increase\" and \"decrease\"). (4) Irrelevant. The prediction and the label describe different alterations of functions. Note that you should be careful about the altered functions before analyzing the extent. Answer with one word only from \"Accurate\", \"Relevant\", \"Opposite\" and \"Irrelevant\" to summarize your evaluation. ", "page_idx": 25}, {"type": "text", "text": "[User prompt]{\"label\": {ground_truth}, \"prediction\": {model_output} ", "page_idx": 25}, {"type": "text", "text": "\u2022 Accurate. The predicted alterations in protein functions and estimations of extent are mostly the same as the ground truth.   \n\u2022 Relevant. The prediction identifies the protein function that is altered by the mutation. While it accurately predicts the attenuation or the degradation, the estimation of the extent is not correct.   \n\u2022 Opposite. The prediction identifies the protein function that is altered by the mutation. However, it mistakenly predicts attenuation as degradation or vice versa.   \n\u2022 Irrelevant. The prediction and the ground truth are about completely different functional alterations. ", "page_idx": 25}, {"type": "text", "text": "Then, we recruit a postgraduate from a top university who majors in biology to further assess the results. Specifically, we collect samples that are marked as Accurate, Relevant, and Opposite by GPT4, and include Irrelevant samples for strong baselines (5-shot GPT-4 models and fine-tuned models) and MutaPLM. We present the mutation explanations, ground-truth results, GPT-4 evaluation, and categorization protocol, and ask the expert to rectify the evaluation result if necessary. In total, $12.0\\%$ of the GPT-4 evaluations are modified, and the confusion matrix is displayed in Fig. A2. We observe that GPT-4 evaluation is consistent with human experts in most cases, showcasing its reliability as a proxy of expert evaluators in saving evaluation costs. However, it occasionally misclassifies Accurate predictions into Relevant, and Relevant or Opposite predictions into Irrelevant, which we attribute to the fact that GPT-4 tends to favor more fluent answers instead of more informative ones. We leave more realistic and labor-saving evaluation strategies for future exploration. ", "page_idx": 25}, {"type": "text", "text": "C.4 Multi-round Optimization ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We incorporate the following datasets from [86] for multi-round fitness optimization: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Adeno-associated Viruses (AAV) [91]. The dataset involves a 28-amino acid segment of the caspid protein VP1 from Adeno-associated virus. The optimization objective is to improve its capability as a gene delivery vector. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Aliphatic Amide Hydrolase (AMIE) [92]. The dataset aims to improve the enzymic activity of Aliphatic amidase from Pseudomonas aeruginosa in catalyzing the hydrolysis of short-chain aliphatic amides. ", "page_idx": 25}, {"type": "image", "img_path": "yppcLFeZgy/tmp/3aef1bbaad09d80a0d5084677c5017b83a972564d236c8f977293bf6e615d0a2.jpg", "img_caption": ["Figure A2: Confusion matrix between GPT-4 and manual evaluation. "], "img_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "yppcLFeZgy/tmp/391b85827ea8c6b6a5ea15f54b7329564438f0811f447a3485858b17843980c2.jpg", "table_caption": ["Table A9: Prompts for navigating mutation engineering. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "\u2022 Green Fluorescent Proteins (avGFP) [93]. The dataset aims to enhance the fluorescent intensity of the Green Fluorescent Protein from Aequorea victoria. The protein is widely adopted as a biosensor for detecting gene expressions and protein locations. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Ubiquitination Factor Ube4b (E4B) [94]. The dataset aims to improve the enzymic activity of Ubiquitin conjugation factor $E4B$ in Homo sapiens, which plays a role in proteasomal degradation by interacting with other proteins.   \n\u2022 Levoglucosan Kinase (LGK) [95]. The dataset focuses on Levoglucosan kinase in Lipomyces starkeyi. The optimization objective is to enhance its catalytic activity in canonical kinase phosphotransfer reaction.   \n\u2022 SUMO E2 conjugase (UBE2I) [96]. The dataset studies SUMO-conjugating enzyme UBC9 in Homo sapiens which is relevant to several human diseases. The optimization objective is to improve the growth rescue rate at high temperatures in a yeast strain. ", "page_idx": 26}, {"type": "text", "text": "We manually write prompts in Tab. A9 to navigate the optimization process by a beam search process. Specifically, we initialize the candidate set with the wild-type sequence. Then, for each round of optimization, we feed each candidate sequence and the textual instruction into the decoding workflow of MutaPLM. Then we sample $K$ mutations, the probability of which is proportional to the logits of the position head and the logits of the LM head. The optimization process is further detailed in Algorithm 1. The baselines are implemented by the EvoProtGrad [87] package. We perform experiments for 20 times, each comprising 10 optimization rounds. ", "page_idx": 26}, {"type": "text", "text": "D Additional Experiment Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.1 Experiment Results on Temporal Split ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The experimental results for mutation explanation and engineering are shown in Tab. A10 and Tab. A11 respectively. We observe that: (1) MutaPLM achieves promising performance on the temporal split and outperforms strong baselines, showcasing its robustness in handling novel mutations. (2) For mutation explanation, the experiment results are similar to those on the Hard set of the structural split, and we observe similar over-fitting issues as in structural split that more training steps lead ", "page_idx": 26}, {"type": "text", "text": "Algorithm 1 Multi-round Optimization with Beam Search ", "page_idx": 27}, {"type": "text", "text": "Require: Wild-type Sequence $x_{\\mathrm{wt}}$ , Instruction $t$ , Number of Rounds $N$ , Number of Candidates $K$ $\\bar{C}\\gets\\{x_{\\mathrm{wt}}\\}$ for Round $=1,2,\\cdots\\,,N$ do for $x\\in C$ do $h\\leftarrow f_{\\mathrm{PLM}}(x)$ $h\\leftarrow h+D e c o d e r(h,T)$ $\\triangleright$ Add mutational features Scorepos, Score $i a\\gets f_{\\mathrm{pos}}(h),f_{\\mathrm{LM}}(h)$ \u25b7Calculate the logits two prediction heads $\\mathrm{Score}(x,i,j)\\gets\\mathrm{Score}_{i}^{\\mathrm{pos}}+\\mathrm{Score}_{i,j}^{\\mathrm{aa}},\\forall i\\neq j$ $\\triangleright$ The score mutating $i$ -th amino acid to $j$ end for $P(x,i,j)\\gets\\mathrm{GlobalSoftMax}[\\mathrm{Score}(x,i,j)]\\quad\\triangleright\\mathrm{I}$ Probality distribution of sampling mutations $C\\gets\\mathrm{Mutate}(x,i,j),(x,i,j)\\hat{\\sim}\\mathrm{SampleK}(P)$ \u25b7Sampling without replacement end for return $C$ ", "page_idx": 27}, {"type": "table", "img_path": "yppcLFeZgy/tmp/22b42883e29d6e00fc14dbeb8c66862dd24d1586e44aab1aab387a0b4779bddd.jpg", "table_caption": ["Table A10: Performance evaluation for mutation explanation on temporal split. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "to improved validation loss but performance drops on the test set. This further underscores the significance of improving the generalization capability of mutation explanation models to assist real-world applications. (3) For mutation engineering, the results show little difference with those on the structural split. As discussed in Sec. 4.3, the PLM may have witnessed the protein sequence during pre-training, which mitigates the overfitting problem. ", "page_idx": 27}, {"type": "text", "text": "D.2 Low-N Fitness Regression ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "While MutaPLM is not specifically designed for numeric tasks, we investigate if the learned Delta features could benefti ftiness regression. We perform experiments on two protein ftiness benchmarks, namely Spike-ACE2 [97] and avGFP [93]. Spike-ACE2 is a deep mutational scanning dataset that aims to predict the binding strengths between SARS-Cov-2 variants and its receptor ACE2, which is critical for identifying potentially dangerous strains of the virus. The avGFP benchmark aims to predict the fluorescence intensity of GFP variants, which is beneficial for developing biomarkers. ", "page_idx": 27}, {"type": "table", "img_path": "yppcLFeZgy/tmp/c2afd881bdf44e931bee8b7b46d2f0294f9b6d79fe10c5b567921f5f5cc473c1.jpg", "table_caption": ["Table A11: Performance evaluation for mutation engineering on temporal split. "], "table_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "yppcLFeZgy/tmp/fccec49312dafde5108d239105f9f567a3f4dbe5d8dfbd3e410a74652fe77e1a.jpg", "img_caption": ["Figure A3: More case studies at mutation explanation. We report the outputs of MutaPLM and GPT-4 (5-shot, kNN). "], "img_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "yppcLFeZgy/tmp/aad469e426595b6e64afcd5058aff85737074a0a95ddc494290c0aa6913469f2.jpg", "table_caption": ["Table A12: Performance evaluation on protein fitness regression benchmarks. We perform experiments 5 times with different random seeds and report the Spearman correlation coefficient. The best and second-best results are marked in bold and underlined. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Following prior works [98, 28], we adopt the low- $N$ setting with 192 randomly sampled training samples and 48 validation samples. We calculate the adaptive ftiness by our PLM following [26] and concatenate it with the delta features $z_{\\Delta}$ . The result is fed into a trainable 2-layer MLP to predict the ftiness scores, and the remaining parameters are kept frozen. We also implement baselines including Ridge Regression, ESM-2 [19], AugmentedESM [27], Augmented EVmutation [48], ConFit [28], and Tranception_L [99]. All the models are trained for 50 epochs with a batch size of 16 and a learning rate of 0.001 using the MSE loss. We sample different low- $N$ datasets with 5 random seeds and report the results in Tab. A12. ", "page_idx": 29}, {"type": "text", "text": "We observe that MutaPLM significantly outperforms baseline models that adopt ESM-2 as the PLM, indicating that the delta features have captured mutational knowledge from natural language supervision that beneftis ftiness regression tasks. While MutaPLM achieves comparable results with Tranception_L on both benchmarks, it is worth noting that the model adopts a different network architecture specifically designed for fitness regression. Therefore, we speculate that adopting a mutation-oriented PLM instead of ESM-2 may further improve the performance. While fitness regression is not the main focus of our work, we expect future endeavors that jointly harvest discrete textual descriptions and continuous fitness scores. ", "page_idx": 29}, {"type": "text", "text": "D.3 Additional Case Studies ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We present more case studies of mutation explanation in Fig. A3. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The claims are validated by our experiments in Section 4. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We discuss our limitations in Section 5. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide the dataset construction process in Section 3.3 and implementation details in Section 4.1. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide our data and code at https://github.com/PharMolix/ MutaPLM. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We present implementation details in Section 4.1. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: While we report error bars for protein fitness optimization, the majority of experiments do not include error bars because it would be too computationally expensive. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The information is provided in Section 4.1. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: N/A. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We discuss broader impacts in Section 5. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We clarify safeguards in Section 5. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The original papers of assets are cited. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The assets and documentation are at https://github.com/PharMolix/ MutaPLM. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The instructions to human participants are displayed in Appendix C.3. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]