[{"figure_path": "xUoNgR1Byy/figures/figures_3_1.jpg", "caption": "Figure 1: Our experimental pipeline. We train and validate probes to understand LFPs.", "description": "This figure illustrates the four main steps of the experimental pipeline used in the paper to investigate Learned Feedback Patterns (LFPs) in Large Language Models (LLMs).  First, pre-trained LLMs are fine-tuned using Reinforcement Learning from Human Feedback (RLHF). Second, a condensed representation of the Multi-Layer Perceptron (MLP) activations is obtained using sparse autoencoders. This simplifies the high-dimensional activation space for easier analysis. Third, probes (linear regression models) are trained to predict feedback signals implicit in the condensed MLP activations. These probes identify correlations between activation patterns and the feedback received during training. Finally, the probes are validated by comparing the features they identify as being relevant to positive feedback against the features described by GPT-4 (a large language model) as being related to LFPs. This validation step ensures the reliability and interpretability of the probes\u2019 findings.", "section": "3 Experiments and methodology"}, {"figure_path": "xUoNgR1Byy/figures/figures_4_1.jpg", "caption": "Figure 2: For a token x in context, we sample MLP activations, which are given to a sparse autoencoder as input. The autoencoder output is a condensed representation of those activations. We concatenate the autoencoder outputs for each MLP layer, which serve as input to our probe. Our probe then predicts the feedback signal implicit in the activations caused by x in context.", "description": "This figure illustrates the process of probe training.  First, MLP activations are sampled and passed through a sparse autoencoder to obtain a condensed representation. These condensed representations from multiple layers are then concatenated and serve as input to a linear regression probe. This probe is trained to predict the feedback signal implicit in the activations, based on the difference (delta) between positive and neutral or negative and neutral examples.", "section": "3.3 Probe training"}, {"figure_path": "xUoNgR1Byy/figures/figures_6_1.jpg", "caption": "Figure 1: Our experimental pipeline. We train and validate probes to understand LFPs.", "description": "This figure illustrates the experimental pipeline used in the paper to understand Learned Feedback Patterns (LFPs) in Large Language Models (LLMs).  The pipeline involves four main stages:\n\n1. **Fine-tuning pre-trained LLMs using RLHF:** A pre-trained LLM is fine-tuned using Reinforcement Learning from Human Feedback (RLHF).\n2. **Obtain condensed representation of MLPs using sparse autoencoders:** Sparse autoencoders are used to obtain a condensed and interpretable representation of the Multi-Layer Perceptron (MLP) activations from the fine-tuned LLM.\n3. **Train probes to predict feedback signal implicit in condensed MLP activations:** Probes are trained to predict the feedback signal implicit in the condensed MLP activations. This helps to measure the divergence of LFPs from human preferences.\n4. **Validate probes by inspecting autoencoder features relevant to the fine-tuning task:** The probes are validated by comparing the features they identify as being active with positive feedback signals against features described by GPT-4 as being related to the LFPs.", "section": "3 Experiments and methodology"}, {"figure_path": "xUoNgR1Byy/figures/figures_8_1.jpg", "caption": "Figure 4: The absolute difference between the probe prediction and VADER lexicon label for a word plotted against how frequently the RLHF model generates that word. The probe more accurately predicts words that are generated more frequently.", "description": "This figure shows the relationship between the accuracy of the probe's predictions and the frequency of words generated by the RLHF model.  The x-axis represents the number of times a word appeared in the RLHF model's output. The y-axis represents the absolute difference between the sentiment score assigned by the VADER lexicon and the sentiment score predicted by the probe. The plot demonstrates that the probe's predictions are more accurate for words that appear more frequently in the RLHF model's output, suggesting a correlation between the frequency of word generation and the accuracy of the learned feedback patterns.", "section": "4.1 Measuring the accuracy of LFPs"}, {"figure_path": "xUoNgR1Byy/figures/figures_8_2.jpg", "caption": "Figure 5: PCA on the sparse autoencoder features given to the logistic probe as input for GPT-Neo-125m, showing structure to be exploited in the probes' input data. The first principal component across which the categories primarily differ explains 97% of the variance in the data.", "description": "This figure shows the result of applying Principal Component Analysis (PCA) to the input data of a logistic regression probe trained to predict the feedback signal implicit in the activations of a fine-tuned language model.  The data consists of condensed representations of the model's activations, obtained using sparse autoencoders. The PCA reveals a clear separation between data points representing positive and negative feedback signals, indicating that the probe's input data contains sufficient structure to allow accurate classification.  The first principal component accounts for 97% of the variance in the data, highlighting the effectiveness of dimensionality reduction for separating the feedback signal.", "section": "4.2 Probe validation"}, {"figure_path": "xUoNgR1Byy/figures/figures_13_1.jpg", "caption": "Figure 1: Our experimental pipeline. We train and validate probes to understand LFPs.", "description": "This figure illustrates the four main stages of the experimental pipeline used in the paper to understand Learned Feedback Patterns (LFPs) in Large Language Models (LLMs).  Stage 1 involves fine-tuning a pre-trained LLM using reinforcement learning from human feedback (RLHF). Stage 2 involves obtaining a condensed representation of the LLM's Multi-Layer Perceptron (MLP) activations using sparse autoencoders. Stage 3 trains probes to predict the feedback signal implicit in these condensed activations. Finally, Stage 4 validates the probes by comparing the features they identify as active in activations with implicit positive feedback signals against the features GPT-4 describes as being related to LFPs.  The overall goal is to measure the divergence between LFPs and human preferences.", "section": "3 Experiments and methodology"}, {"figure_path": "xUoNgR1Byy/figures/figures_16_1.jpg", "caption": "Figure 7: Divergences on a per-layer basis for various model and reward function combinations. Pythia-70m and Pythia-160m 6 and 12 layers respectively.", "description": "This figure shows the divergence of the RLHF-tuned models from the base LLM on a per-layer basis for different model and reward function combinations. The x-axis represents the layer number, and the y-axis represents the layer divergence. Two lines are plotted, one for Pythia-70m and one for Pythia-160m, each with a VADER reward. The figure helps visualize how the divergence changes across layers in the model due to fine-tuning with different reward functions.", "section": "C Layer divergences"}, {"figure_path": "xUoNgR1Byy/figures/figures_18_1.jpg", "caption": "Figure 8: Normalized reconstruction and scaled true sparsity losses for Pythia-70m over 1 training epoch, over varying values of the l1 coefficient. Both metrics are averaged over all highly divergent layers, and hyperparameter choices are otherwise as described in \u00a73.2.", "description": "This figure shows the relationship between the L1 coefficient used in training sparse autoencoders and two loss metrics: reconstruction loss and true sparsity loss.  The plot demonstrates how varying the L1 coefficient (which controls sparsity) affects these losses during the training process for the Pythia-70m language model.  It helps to determine the optimal L1 coefficient to balance reconstruction accuracy and the desired level of sparsity in the learned features.", "section": "3.2 Autoencoder training"}]