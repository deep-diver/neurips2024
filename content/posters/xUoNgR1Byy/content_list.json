[{"type": "text", "text": "Interpreting Learned Feedback Patterns in Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Luke Marks\u2217\u2020 Amir Abdullah \u2217\u2020 \u2662 Clement Neo\u2020 Rauno Arike\u2020 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "David Krueger\u2299 Philip Torr\u2021 Fazl Barez\u2217\u2020 \u2021", "page_idx": 0}, {"type": "text", "text": "\u2020Apart Research \u2662Cynch.ai \u2299University of Cambridge ", "page_idx": 0}, {"type": "text", "text": "\u2021Department of Engineering Sciences, University of Oxford ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning from human feedback (RLHF) is widely used to train large language models (LLMs). However, it is unclear whether LLMs accurately learn the underlying preferences in human feedback data. We coin the term Learned Feedback Pattern (LFP) for patterns in an LLM\u2019s activations learned during RLHF that improve its performance on the fine-tuning task. We hypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback exhibit consistent activation patterns for outputs that would have received similar feedback during RLHF. To test this, we train probes to estimate the feedback signal implicit in the activations of a fine-tuned LLM. We then compare these estimates to the true feedback, measuring how accurate the LFPs are to the fine-tuning feedback. Our probes are trained on a condensed, sparse and interpretable representation of LLM activations, making it easier to correlate features of the input with our probe\u2019s predictions. We validate our probes by comparing the neural features they correlate with positive feedback inputs against the features GPT-4 describes and classifies as related to LFPs. Understanding LFPs can help minimize discrepancies between LLM behavior and training objectives, which is essential for the safety and alignment of LLMs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) are often fine-tuned using reinforcement learning from human feedback (RLHF), but it is not understood whether RLHF results in LLMs accurately learning the preferences that underlie human feedback data. We refer to patterns in an LLM\u2019s activations learned during RLHF that enable it to perform well on the task it was fine-tuned for as the LLM\u2019s Learned Feedback Patterns (LFPs). Formally, for an input $\\mathbf{X}$ and activations $\\mathbf{H}(\\mathbf{X},\\theta)$ from a fine-tuned LLM parameterized by $\\theta$ , we describe its LFPs as the differences in $\\mathbf{H}(\\mathbf{X},\\theta)$ caused by training $\\theta$ , that result in the outputs performing better under the fine-tuning loss. LFPs are a major component of what an LLM has learned about the fine-tuning feedback. ", "page_idx": 0}, {"type": "text", "text": "For example, consider a sentiment analysis task where the ground truth dataset labels the word \u201cprecious\" as having positive sentiment. However, the fine-tuned LLM\u2019s activations, when probed, predict negative sentiment. This discrepancy, where the LLM\u2019s output would receive negative feedback according to the true preferences, is an example of divergence between LFPs and the preferences underlying the human feedback data used in fine-tuning. ", "page_idx": 0}, {"type": "text", "text": "Our objective is to study and measure this divergence. However, obstacles like feature superposition [12] in dense, high dimensional activation spaces, and limited model interpretability obscure the relationship between human-interpretable features and model outputs. In this paper we ask: Can we measure and interpret the divergences between LFPs and human preferences? ", "page_idx": 1}, {"type": "text", "text": "Continued deployment of LLMs fine-tuned using RLHF with greater capabilities could amplify the impact of LFPs divergent from the preferences that underlie human feedback data. Possible risks include manipulation of user preferences [1] and catastrophic outcomes when models approach human capabilities [10]. The ability to measure and explain the divergences of LFPs in human-interpretable ways could help minimize those risks and inform developers of when intervention is necessary. To achieve this, we extend existing research that uses probes to uncover characteristics of larger, deep neural networks [2, 5, 27]. Our probes are trained on condensed representations of LLM activations. The trained probes predict the feedback implicit in condensed LLM activations. We validate our probes by comparing the features they identify as active in activations with implicit positive feedback signals against the features GPT-4 describes and classifies as being related to the LFPs. ", "page_idx": 1}, {"type": "text", "text": "The decoders of autoencoders trained on LLM activations with a sparsity constraint on the hidden layer activations have been shown to be more interpretable than the raw LLM weights, partially mitigating feature superposition [29, 9, 11]. The outputs of these autoencoders comprise the condensed representations of LLM activations. By training our probes on sparse autoencoder outputs, we make it easier to understand which features in the activation space correlate with implicit feedback signals. ", "page_idx": 1}, {"type": "text", "text": "We hypothesize that consistent patterns in the activations of fine-tuned LLMs correlate with the fine-tuning feedback, allowing the prediction of the feedback signal implicit in these activations. In validation of this hypothesis, we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We use synthetic datasets to elicit activation patterns in fine-tuned LLMs related to their LFPs. We make these datasets publicly available for reproducibility and further research.   \n\u2022 We train probes to estimate the feedback signal implicit in a fine-tuned LLMs activations (\u00a73.3).   \n\u2022 We quantify the accuracy of the LFPs to the fine-tuning feedback by contrasting the probe\u2019s predictions and the true feedback (\u00a73.3).   \n\u2022 We use GPT-4 to identify features in the fine-tuned LLM\u2019s activation space relevant to the LFPs. We validate our probes against these feature descriptions, showing that the two methods attribute similar features to the generation of outputs that receive a positive feedback signal. (\u00a73.4). ", "page_idx": 1}, {"type": "text", "text": "Code for all of our experiments is available at https://github.com/apartresearch/Interpreting-LearnedFeedback-Patterns. ", "page_idx": 1}, {"type": "text", "text": "2 Background and related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We study LLMs based on the Transformer architecture [37]. Transformers operate on a sequence of input tokens represented by the matrix $\\mathbf{X}\\in\\mathbb{R}^{L\\times d}$ , where $L$ is the sequence length and $d$ is the token dimension. For each token a query $\\mathbf{Q}$ , key $\\mathbf{K}$ and value $\\mathbf{V}$ is formed using the parameter matrices $\\mathbf{W}_{q},\\mathbf{W}_{k},\\mathbf{W}_{v}\\,\\in\\,\\mathbb{R}^{d\\times d}$ , giving $\\mathbf{Q}=\\mathbf{W}_{q}\\mathbf{X}$ , $\\mathbf{K}=\\mathbf{W}_{k}\\mathbf{X}$ , and $\\mathbf{V}=\\mathbf{W}_{v}\\mathbf{X}$ . The attention scores, $\\begin{array}{r}{{\\bf A}=\\mathrm{softmax}\\left(\\frac{{\\bf Q}{\\bf K}^{\\top}}{\\sqrt{d}}\\right)}\\end{array}$ Q\u221aKd\u22a4 , measure the relevance of each token to every other token. The final output is obtained by weighting the values by the attention scores, resulting in the output matrix $\\mathbf{O}=\\mathbf{A}\\mathbf{V}$ . O is then passed through a multi-layer perceptron (MLP) and combined with the original input via a residual connection, forming the final output of the layer and the input for the next layer. ", "page_idx": 1}, {"type": "text", "text": "There is a significant body of evidence that deep neural networks such as Transformers learn human-interpretable features of the input, providing a strong motivation for interpretability research [26, 28, 21, 7]. However, there is often not a one-to-one correspondence of neurons and features. When multiple features in a single neuron are represented near-orthogonally, this phenomenon is known as \u2018superposition\u2019 [12], allowing models to represent more features than dimensions in their activation space. This can be practical when those features are sparsely present in training data. Superposition poses a major obstacle to neural network interpretability, and this is expected to extend to the study of LFPs learned during RLHF. A promising approach to disentangling superposed features in neural networks is to train autoencoders on neuron activations from those networks. Given encoder weights $\\mathbf{W}_{E}\\in\\mathbb{R}^{n\\times h}$ , decoder weights $\\mathbf{W}_{D}\\in\\mathbb{R}^{h\\times n}$ , a bias vector $b_{\\mathbf{E}}\\in\\mathbb{R}^{h}$ , and an input $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ , the output $\\hat{\\textbf{X}}$ is computed as: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{X}}=\\mathbf{W}_{D}\\big(\\mathrm{ReLU}(\\mathbf{W}_{E}\\mathbf{X}+b_{\\mathbf{E}})\\big)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The sparse autoencoder loss function is typically: ", "page_idx": 2}, {"type": "equation", "text": "$$\nL(\\mathbf{X})=\\frac{1}{|\\mathbf{X}|}\\sum_{\\mathbf{X}\\in\\mathbf{X}}\\left\\|\\mathbf{X}-\\hat{\\mathbf{X}}\\right\\|_{2}^{2}+\\alpha\\left\\|\\mathrm{ReLU}(\\mathbf{W}_{E}\\mathbf{X}+b_{\\mathbf{E}})\\right\\|_{1}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Where the first term penalizes the Euclidean distance between $\\hat{\\textbf{X}}$ and $\\mathbf{X}$ , and the $\\ell_{1}$ term encourages the output $\\hat{\\textbf{X}}$ to be a sparse linear combination of features in $\\mathbf{W}_{D}$ , providing an interpretable overview of the superposed features that were active in the autoencoder\u2019s input. $\\hat{\\textbf{X}}$ is then a condensed representation of $\\mathbf{X}$ . ", "page_idx": 2}, {"type": "text", "text": "Early results suggest sparse autoencoders can recover features of the input even when those features are represented in a superposed manner [9, 11]. Sharkey et al. [34] train two sparse autoencoders with different hidden sizes, and find similar features in both. Because the features learned by an autoencoder are sensitive to its hidden size, finding similar features in both autoencoders increases the likelihood that they are actually features of the input. Other works exploring related techniques include Yun et al. [41], who apply sparse dictionary learning to visualize the residual streams of Transformer models, and Gurnee et al. [16], who find human-interpretable features in LLMs using sparse linear probes. ", "page_idx": 2}, {"type": "text", "text": "Even when features are interpretable by humans, it can be laborious for a human labeller to identify plausible descriptions of what a neuron represents. Recent work has shown that this can be automated at scale [13, 7]. Bills et al. [7] provide GPT-4 with a set of activations discretized and normalized to a range of 0 and 10 for a set of tokens passed to the model as a prompt. GPT-4 then predicts an explanation for what the neuron represents based on those activations, and predicts discretized activations for tokens as if that description were true. The efficacy of a neuron explanation is judged by the Pearson correlation coefficient of the predicted and true activations. ", "page_idx": 2}, {"type": "text", "text": "To our knowledge, no general methods have been proposed for finding human-interpretable representations of LFPs learned via RLHF. Previous literature on reward model interpretability has focused on more conventional RL methods. For example, Jenner and Gleave [20] provide a framework for preprocessing reward functions learned by RL agents into simpler but equivalent reward functions, which makes visualizations of these functions more human-understandable. Michaud et al. [25] explain the reward functions learned by Gridworld and Atari agents using saliency maps and counterfactual examples, and find that learned reward functions tend to implement surprising algorithms relying on contingent aspects of the environment. Gleave et al. [14] and Wolf et al. [40] present methods for comparing and evaluating reward functions learned through RL training without requiring these functions to be human-interpretable. Probing deep neural networks using linear classifiers is well-established [2, 5, 27], but the architecture of prior probes varies considerably to our approach, mostly in the objective of the probe. We specifically analyze the implicit representation of feedback in activations over contrastive inputs. ", "page_idx": 2}, {"type": "text", "text": "3 Experiments and methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Here, we detail each stage of our experimental pipeline. The major steps are as follows: ", "page_idx": 2}, {"type": "text", "text": "1. Fine-tune pre-trained LLMs using RLHF (\u00a73.1).   \n2. Obtain a condensed representation of MLP activations using sparse autoencoders (\u00a73.2).   \n3. Train probes to predict the feedback signal implicit in condensed fine-tuned LLM activations. (\u00a73.3). This allows us to measure the divergence of LFPs from the human preferences behind an LLM\u2019s fine-tuning distribution. ", "page_idx": 2}, {"type": "image", "img_path": "xUoNgR1Byy/tmp/f800fa5725bb93fcaaf96bd46024550af8ed2ee960c87e07d194accfe6535265.jpg", "img_caption": ["Figure 1: Our experimental pipeline. We train and validate probes to understand LFPs. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4. Validate our probes by comparing the features they identify as active in activations with implicit positive feedback signals against the features GPT-4 describes and classifies as related to LFPs (\u00a73.4). ", "page_idx": 3}, {"type": "text", "text": "3.1 Fine-tuning with RLHF ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section describes our RLHF pipeline. Our first fine-tuning task is controlled sentiment generation, in which models generate completions to prefixes from the IMDB dataset [22]. Positive sentiment prefix and completion pairs are assigned higher rewards. ", "page_idx": 3}, {"type": "text", "text": "Our reward function for this task comprises of sentiment assignments from the VADER lexicon [19], which were initially labelled by a group of human annotators. The annotators assigned ratings from $-4$ (extremely negative) to $+4$ (extremely positive), with an average taken over ten annotations per word. This gives a function $V:W\\to\\mathbb{R}$ , where $W$ is a set of words. ", "page_idx": 3}, {"type": "text", "text": "Given a prefix and completion, we tokenize the concatenated text using the Spacy [17] tokenizer for their en_core_web_md model. Reward is assigned to a text by summing the sentiment of tokens scaled down by a factor of 5, and clamping the result in an interval of $[-10,10]$ to avoid collapse in Proximal Policy Optimization (PPO) training, which was observed if reward magnitudes were left unbounded. The reward function for this task is given as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{Reward}(s)=\\operatorname{clip}\\left({\\frac{1}{5}}\\sum_{\\operatorname{token}\\in s}V(\\operatorname{token}),-10,+10\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Where $s$ is a sequence of tokens. ", "page_idx": 3}, {"type": "text", "text": "We train a policy model $M_{\\mathrm{RLHF}}$ to maximize reward while minimizing the Kullback-Leibler divergence of generations from the base model $M_{\\mathrm{Base}}$ . We use PPO, adhering to Ouyang et al. [30]. We use the Transformer Reinforcement Learning (TRL) framework [39]. The hyperparameters used for all models are: a batch size of 64, mini-batch size of 16, KL coefficient of 0.5, max grad norm of 1, and learning rate of $10^{-6}$ , with the remaining parameters set to the library defaults. See Appendix A for an overview of our PPO pipeline. ", "page_idx": 3}, {"type": "text", "text": "We also include two additional tasks that aim to mimic real-world RLHF pipelines. In the first, MRLHF is fine-tuned using DPO with responses from the Anthropic HH-RLHF dataset [4]. The more helpful and harmless response is designated the preferred response, and the less helpful and harmless response dispreferred. The aim of this task is for $M_{\\mathrm{RLHF}}$ to behave more like a helpful assistant. The second task uses DPO to optimize $M_{\\mathrm{RLHF}}$ for toxicity using the toxic-dpo dataset [36], in which the preferred response is more toxic than the dispreferred response. We fine-tuned Pythia-70m , Pythia-160m [6], GPT-Neo-125m [8] and Gemma-2b-it [23] for both of the DPO tasks. We used the following hyperparameters, with the rest following TRL defaults: we train for 5000 steps using the AdamW optimizer with an Adam-Epsilon of 1e\u22128, a batch size of 8 for Pythia-70m , Pythia-160m and GPT-Neo-125m , and an effective batch size of 16 for Gemma-2b-it . The learning rate was 3e\u22125 for Pythia-70m , Pythia-160m and GPT-Neo-125m , and 5e\u22125 for Gemma-2b-it . For each model and task, we train for approximately 6 hours on a single A10 GPU, except for Gemma-2b-it , where we used a A40 GPU. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Autoencoder training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we detail the training of sparse autoencoders on the activations of a fine-tuned LLM. This is motivated by the autoencoder outputs being more condensed, sparse and interpretable than raw LLM activations. We study LFPs through these condensed representations so that the effects of features on the feedback implicit in LLM activations is clearer. ", "page_idx": 4}, {"type": "text", "text": "Having obtained the fine-tuned model $M_{\\mathrm{RLHF}}$ , we compute the parameter divergence between $M_{\\mathrm{Base}}$ and $M_{\\mathrm{RLHF}}$ for each layer under the $\\ell_{2}$ norm, and choose the five highest divergence MLP layers $L_{\\mathrm{RLHF}}\\,=\\,\\{l_{1},\\dots,l_{5}\\}$ to train autoencoders on the activations of. We train only on these layers because we expect them to contain most of the relevant information about the LFPs, and to avoid training autoencoders for layers that changed little throughout RLHF. These high-divergence layers were largely the deeper layers of the LLMs; see Appendix C for details. For each layer $l\\in L_{\\mathrm{RLHF}}$ , we sample activations $\\boldsymbol{a}_{l}\\in\\mathbb{R}^{m\\times n}$ from the MLP of that layer, forming a dataset of activations for each layer. We then train two autoencoders on each dataset, written $\\mathcal{A}\\mathcal{E}_{l}^{1}$ and $\\mathcal{A}\\mathcal{E}_{l}^{2}$ with hidden sizes $n$ and $2n$ respectively. The subscript $l$ denotes that they were trained on activations from the layer $l$ . We tie the decoder and encoder weights, meaning the decoder weights are the tranpose of the encoder weights. ", "page_idx": 4}, {"type": "text", "text": "We train all autoencoders for 75000 training examples with an $\\ell_{1}$ coefficient of 0.001, a learning rate of 1e\u22123, and a batch size of 32. The exception is GPT-Neo-125m , where we use an $\\ell_{1}$ coefficient of 0.0015. Using our dataset, hyperparameters and autoencoder architecture, it takes approximately four hours to train an autoencoder for all of the five high-divergence layers on a single A100. Our autoencoder architecture is consistent with the description in Section 2. We base these decisions on empirical testing by Sharkey et al. [34], Cunningham et al. [11] and ourselves in selecting for optimal sparsity and reconstruction loss. For more details on our autoencoder training, see Appendix F. ", "page_idx": 4}, {"type": "text", "text": "3.3 Probe training ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "xUoNgR1Byy/tmp/5f6aecb24b6664e38249affe8eaa348e98449a4fee9501f1fdaac0570dc1901d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: For a token $x$ in context, we sample MLP activations, which are given to a sparse autoencoder as input. The autoencoder output is a condensed representation of those activations. We concatenate the autoencoder outputs for each MLP layer. which serve as input to our probe. Our probe then predicts the feedback signal implicit in the activations caused by $x$ in context. ", "page_idx": 4}, {"type": "text", "text": "To train probes that predict the feedback signal implicit in fine-tuned LLM activations, we use the difference between the probe\u2019s prediction and true feedback signal as a measure of how accurate the LFPs are to the fine-tuning feedback. ", "page_idx": 4}, {"type": "text", "text": "We form a contrastive dataset $\\mathcal{X}=(x^{+},x^{0},x^{-})$ where each tuple contains a positive, neutral and negative example in accordance with the fine-tuning feedback. If the fine-tuning task was generating positive sentiment completions, the positive example may be \u2018That movie was great\u2019, the neutral example \u2018That movie was okay\u2019, and the negative example \u2018That movie was awful\u2019. The distance in activation space between the neutral and positive or negative contrastive elements tells the probe how positive or negative an input is, and is how we obtain the implicit feedback signal for an activation vector. Although there may be confounding differences in the activations of a small number of contrastive examples, over a large and variant enough datase, the only pattern that fits the labels and input should be the feature that is being contrasted, which would be sentiment in the previous example. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "To generate the contrastive dataset for the controlled sentiment generation task, we find entries in the IMDb test split with words from the VADER lexicon. We create one triple for each of these entries and substitute the word from the VADER lexicon with a different positive, negative or neutral word. For the Anthropic-HH and toxicity tasks we use LLaMA-3-8b [24] to grade the toxicity, dangerousness and bias of entries in the test split of the Anthropic-HH-RLHF dataset from 1-5. Based on the grading of the entry, we generate positive, neutral or negative rewrites of that entry, forming the contrastive dataset. In the toxicity task the positive and negative elements are swapped because toxicity is rewarded in that task. ", "page_idx": 5}, {"type": "text", "text": "For each input $x\\in\\mathscr{X}$ , we compute the activations for the MLP of each high divergence layer $a_{l}(x)$ for a token $x$ in context. We use those activations as input to a sparse autoencoder, giving a condensed representation of those activations $\\hat{a}_{l}(x)=\\mathcal{A}\\mathcal{E}_{l}^{1}(a_{l}(\\bar{x}))$ . The forward pass is continued to the final layer, and MLP activations at each high divergence layer are aggregated, producing a set of activations $\\dot{\\mathcal{A}}=\\{\\hat{a}_{l_{1}}(x),\\dots,\\hat{a}_{l_{N}}(x)\\}$ . We concatenate the activations in this set as $\\mathcal{A}_{c o n c a t}(x)$ , referring to the concatenated activations produced by a token $x$ . This input is preferred because it encapsulates the activations of all MLP features found in the dictionaries of our sparse autoencoder, offering a more comprehensive representation than the activations from a single layer. ", "page_idx": 5}, {"type": "text", "text": "We compute the activation deltas for a given contrastive triple as the difference between the positive and neutral element and negative and neutral element under the $\\ell_{2}$ norm. The former yielding the activation delta $\\Delta^{+}$ , and the latter $\\Delta^{-}$ . In the latter case, we negate the sum of Euclidean distances so that we may pose $\\Delta^{-}$ as negative polarity in contrast to $\\Delta^{+}$ . This distinguishes implicit reward from penalty. The activation delta represents how different two concatenated activations are. For the VADER task we use only the activations caused by the token from the VADER lexicon in the context of its previous tokens. In the example contrastive data point [\u2018That movie was great\u2019, \u2018That movie was okay\u2019, \u2018That movie was awful\u2019], we would use only the activations of the tokens \u2018great\u2019, \u2018okay\u2019 and \u2018awful\u2019 in the context of \u2018That movie was\u2019 in order to calculate the activation deltas. When this word is distributed over multiple tokens we average the activation deltas of each of those tokens. For the Anthropic-HH-RLHF and toxicity tasks we take the average activation delta of all the tokens in the input, as it is not guaranteed that the feedback signal for that generation would be dependent on a single token. ", "page_idx": 5}, {"type": "text", "text": "We form a dataset $\\boldsymbol{\\mathcal{D}}=\\left(x_{i},y_{i}\\right)$ where $x_{i}$ is the activations $A_{c o n c a t}({x_{s}}^{+})$ or $\\mathcal{A}_{c o n c a t}({x_{s}}^{-})$ caused by a token from $\\mathcal{X}^{+}\\subset\\mathcal{X}$ (the subset of $\\mathcal{X}$ that contains positive elements) or $\\mathcal{X}^{-}\\subset\\mathcal{X}$ (the subset of $\\mathcal{X}$ that contains negative elements), and $y_{i}$ is the corresponding activation delta $\\Delta^{+}$ for tokens $x^{+}\\in\\mathcal{X}^{+}$ , and $-\\Delta^{-}$ for tokens $x^{-}\\in\\mathcal{X}^{-}$ . ", "page_idx": 5}, {"type": "text", "text": "For the controlled sentiment generation task, we train a regression model to predict the activation deltas for a large dataset of tokens sampled from the IMDb dataset, which is our probe on the feedback signal implicit in the fine-tuned LLM activations. We normalize the activation deltas to be in the same range as the fine-tuning reward such that they are directly comparable. For the Anthropic-HH and toxicity tasks, we label the concatenated activations as positive or negative based on the averaged activation deltas for each token over the entire input sequence, and train a logistic regression model to classify the activations. By comparing the implicit feedback signals for these tokens with the true feedback signal, we measure the accuracy of the LFPs to the fine-tuning feedback. ", "page_idx": 5}, {"type": "text", "text": "3.4 Probe validation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We validate our probes by comparing the features most active when they predict strong positive feedback against the predictions of GPT- $\\cdot4$ as to whether or not a feature is related to the LFPs of a fine-tuned LLM. We generate explanations of the highest cosine similarity features in the decoder weights of the autoencoders $\\mathcal{A}\\mathcal{E}_{l}^{1}$ and $\\mathcal{A}\\mathcal{E}_{l}^{2}$ using GPT-4 , forming a dictionary of feature descriptions for which GPT-4 assigns binary labels to based on whether they are relevant to a natural language description of the fine-tuning task. For the controlled sentiment generation task, this could be \u201ctraining the model to generate completions with positive sentiment\". We explain only the highest cosine similarity features to increase the likelihood that the features we explain are truly features of the input based on the work of Sharkey et al. [34]. See Table 1 for examples of feature descriptions generated by GPT-4 . The full procedure is presented graphically in Figure 3. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "", "table_caption": ["Table 1: Five GPT-4 generated descriptions of features in a sparse autoencoder trained on an LLM for a task detailed in Appendix B sampled from Table 8. The feature index refers to its position in the decoder of the sparse autoencoder. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "xUoNgR1Byy/tmp/efc0795fcf3c985d48424c283856b11c3cd3f4ce6a3ef3279680c461e31d3eb0.jpg", "img_caption": ["Figure 3: We sample activations from layers with the highest parameter divergence from the initial model. Then, two autoencoders with a sparsity constraint are trained on those activations, each with a different dictionary size. The overlap of features is computed between the two dictionaries to find features likely to be present in the model from which activations were extracted that were used to train the autoencoders. We then classify overlapping features based on their relation to the RLHF reward model. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Results and discussion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Measuring the accuracy of LFPs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section compares the feedback signal predicted by our probes with the true fine-tuning feedback, measuring the divergence between LFPs and the fine-tuning feedback. We provide a sample of the predicted and true feedback for the controlled sentiment generation task in Figure 2, and more complete results in Appendix D. Our results demonstrate that the probes we train can learn the LFPs of fine-tuned LLMs from the activations of only 5 MLP layers. ", "page_idx": 6}, {"type": "text", "text": "To quantify the divergence between the LFPs and the fine-tuning feedback, we contrast the feedback our probes predict that is implicit in condensed LLM activations with the true fine-tuning feedback. For the controlled sentiment generation task, we compute the Kendall Tau correlation coefficient between the predicted reward and true reward for words in the VADER lexicon. We find a strongly significant correlation (p-value $=0.014\\$ ) between our probe\u2019s predictions and the VADER lexicon for Pythia- $160\\mathtt{m}$ , but weaker correlations for Pythia-70m $g=0.26)$ and GPT-Neo-125m $(p\\,=\\,0.48)$ . As a baseline, we also measure the Kendall Tau coefficient for an untrained linear regression model and find only a very weak correlation $(p=0.55)$ . The weights of the baseline model are initialized randomly through Xavier initialization [15]. ", "page_idx": 6}, {"type": "text", "text": "The low correlation found for Pythia-70m and GPT-Neo-125m could be explained by the complexity of the probe\u2019s task, in which it must estimate token-level rewards and that our training dataset is highly imbalanced. A linear regression model may be unlikely to recover such granular rewards ", "page_idx": 6}, {"type": "text", "text": "Table 2: Eleven randomly sampled tokens and their predicted sentiment from GPT-Neo$125\\mathrm{m}$ compared with the sentiment values in the VADER lexicon that determined the reward during RLHF. ", "page_idx": 7}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/543cc9fdd432d08170548b2a3454ee19fdf90a6e2604e073ed61082b6b81329d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3: The percentage accuracy of the logistic regression probes at predicting finetuning feedback from condensed LLM activations. LLMs tagged with $\\mathrm{{^{\\bullet}H H}^{\\prime}}$ were trained to behave like helpful assistant using the Anthropic-HH dataset. LLMs tagged with \u2018toxic\u2019 were trained for toxicity using the dpotoxic dataset. ", "page_idx": 7}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/1b57f6fdd97bad88191bd818bf6945b4fda925d1575e0a05de1a34f08afcfb6d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "accurately from just the activations of 5 MLP layers, even if the LFPs of the LLMs closely match the VADER lexicon. Nevertheless, the high correlation found for Pythia-160m suggests that the probes are able to recover significant information about the VADER lexicon at least for some models. ", "page_idx": 7}, {"type": "text", "text": "When trained to predict a less granular feedback signal, our probes achieve near-perfect accuracy $(\\ge\\!99.80\\%$ on a test dataset). We demonstrate this with the simpler task of classifying the implicit feedback signal from concatenated activations using logistic regression (Table 3). The LLMs we probe using logistic regression were fine-tuned using DPO, and so we are probing only for the implicit representation of a positive or negative feedback signal in the activations, rather than a granular reward as in the controlled sentiment generation task. Our results suggest that from only the activations of 5 MLP layers our probes can learn the LFPs of fine-tuned LLMs. ", "page_idx": 7}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/b487ea0f3b225f79994ea7cea35c981ea80f16da76243a16d7a59e3222ef4089.jpg", "table_caption": ["Table 4: Kendall Tau correlation coefficient between the feedback signal implicit in LLM activations and the true feedback signal over many outputs. This comprises our measurement of the accuracy of LFPs for the controlled sentiment generation task, which we denote as \u2018VADER\u2019 in the table. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Probe validation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section we show that our probes correlate the same features with LFPs as an alternative method, suggesting that they are identifying features relevant to LFPs. We use the method described in $\\S3.4$ to generate descriptions of features in LLM activations that have been processed by a sparse autoencoder, and then classify those features as related to the fine-tuning task or not using $\\tt G P T-4$ . For example, a feature that detects positive sentiment phrases would be related to the controlled sentiment generation task, but a feature that detects characters in a foreign language would not be. ", "page_idx": 7}, {"type": "text", "text": "We find that a feature identified by GPT-4 as related to LFPs is between two and three times as likely to be correlated with implicit positive feedback in a fine-tuned LLM\u2019s activations by our probes (Table 7). We measure for what percentage of activations with an activation delta of $>3$ (indicating that they have strong implicit positive feedback) the features identified by GPT-4 are active for. To ensure that the features identified by GPT-4 are related to LFPs, we zero-ablate those features and measure the performance of the LLM with the ablated features on the fine-tuning task, finding that this ablation causes consistent or worse performance on the fine-tuning task in all cases. ", "page_idx": 7}, {"type": "image", "img_path": "xUoNgR1Byy/tmp/6cd371c479def8df0c3018b0066f2c48d392f8c5c2da6208d3e5cc9fe57839c4.jpg", "img_caption": ["Figure 4: The absolute difference between the probe prediction and VADER lexicon label for a word plotted against how frequently the RLHF model generates that word. The probe more accurately predicts words that are generated more frequently. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/c01fb38a715fed6122fa07d6e689136a935bf4484aafa2e674e9821aba9af7b4.jpg", "table_caption": ["Table 5: We measure how accurately the predictions of the VADER probes are the correct sign to the labels in the VADER lexicon. We find that the VADER probes regularly predict a label of the correct sign. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Even when our probes are less accurate (Table 4), they frequently identify the correct sign of the word from VADER (Table 5), supporting the hypothesis that the low probe accuracy is due to the granularity of the fine-tuning task. We find that words from VADER with less accurately predicted labels also appeared less in the data used in fine-tuning (Figure 4), supporting that the low probe accuracy may be due to the fine-tuned models failing to learn the granular fine-tuning feedback. Inputs to the probes trained in Table 3 are cleanly separable into the probe\u2019s classifications using dimensionality reduction (Figure 5), indicating that there is sufficient structure in the probes\u2019 training data to make accurate classifications. ", "page_idx": 8}, {"type": "image", "img_path": "xUoNgR1Byy/tmp/f242d2d18407e78ad5d05b65d723ae0eb35ae27a8d334566ede19cf639ff4889.jpg", "img_caption": ["Figure 5: PCA on the sparse autoencoder features given to the logistic probe as input for GPT-Neo- $.125\\mathtt{m}$ , showing structure to be exploited in the probes\u2019 input data. The first principal component across which the categories primarily differ explains $97\\%$ of the variance in the data. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We believe our results suggest that our probes are finding features relevant to LFPs, supporting our analysis in $\\S4.1$ that our probes are able to learn LFPs from only MLP activations. ", "page_idx": 9}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/22c2caa72a1ee033dfb731384cde2957875cb4f461c5acee94e12cb1289a9bb5.jpg", "table_caption": ["Table 6: Performance before and after the ablation of features identified to be related to the LFPs of a fine-tuned LLM as measured by the average reward of 1000 completions to thirty token prefixes for the base and fine-tuned models. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/f2967d2a223b624c3ebea393c9a162889f70eac355ccc0535ef51c845646ce6c.jpg", "table_caption": ["Table 7: The frequency of activation for features in inputs predicted to have an activation delta of $>3$ by our probes. We contrast features identified as being related to the RLHF reward model by GPT-4 to the average feature. The frequency of ablated features\u2019 activations, and that of all features is averaged over all ablated features and all features in the sparse autoencoders dictionary respectively. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we fti probes to feedback signals implicit in the activations of fine-tuned LLMs. Using these probes, we measure the divergence between LFPs and the preferences that underlie human feedback data, discovering that we can recover significant information about those preferences from our probes even though our probes are trained only on the activations of 5 MLP layers (\u00a74.1). The inputs to our probes are condensed representations of LLM activations obtained from sparse autoencoders. Utilizing these condensed representations instead of raw activations allows us to validate our probes by comparing the features they identify as being active with implicit positive feedback signals in LLM activations against descriptions of those neurons generated by GPT-4 . Furthermore, we demonstrate that $\\mathsf{G P T-4\\,^{\\prime}s}$ feature descriptions correlate with performance on the fine-tuning task, as evidenced by decreased performance on that task after their ablation $(\\S4.2)$ . Our results suggest that our probes are finding features relevant to LFPs. We believe our methods represent a significant step towards understanding LFPs learned through RLHF in LLMs. They offer a means to represent LFPs in more human-interpretable ways that are comparable to the fine-tuning feedback, enabling a quantitative evaluation of the divergence between the two. ", "page_idx": 9}, {"type": "text", "text": "5.1 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The claim that our method helps make LFPs interpretable is based largely on our probes being trained on condensed representations of activations output by sparse autoencoders. For these condensed representations to be faifthful to the raw activations, sparse autoencoders must learn features that are actually used by the LLM. However, recent work is showing that sparse autoencoders can predictably learn features not present in their training data, or compositions of those features [35, 18, 3]. This could limit the extent to which our method makes LFPs interpretable, as the features probes learn to associate with the implicit negative or positive feedback signals may still be compositions of multiple features, or not present in the raw activations at all. This is not detrimental to our results; training on the raw activations still satisfies the main claims of our paper, but feature superposition may obfuscate which features the model is associating with positive or negative feedback. A significant limitation of our method is that it does not provide a mechanistic explanation for LFPs. Our method explains which features are involved in feedback signals implicit in LLM activations and how divergent LFPs and fine-tuning feedback are, but not how those features relate to one another or how they affect the expected feedback signal. Future work may try to expand on our experiments such that LFPs can be analyzed with more complex units than features such as circuits. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We are grateful to Luna Mendez and Jason Schreiber for discussion and feedback. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] G. Adomavicius, J. C. Bockstedt, S. P. Curley, and J. Zhang. Do recommender systems manipulate consumer preferences? A study of anchoring effects. In Information Systems Research, volume 24, pages 956\u2013975, 2013.   \n[2] G. Alain and Y. Bengio. Understanding intermediate layers using linear classifier probes. 2016. URL https://arxiv.org/abs/1610.01644.   \n[3] E. Anders, C. Neo, J. Hoelscher-Obermaier, and J. Howard. Sparse autoencoders find composed features in small toy models. 2024. URL https://www.lesswrong.com/posts/a5wwqza2cY3W7L9cj/ sparse-autoencoders-find-composed-features-in-small-toy.   \n[4] Anthropic. Anthropic hh-rlhf dataset, 2023. https://huggingface.co/datasets/ Anthropic/hh-rlhf.   \n[5] D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba. Network dissection: Quantifying interpretability of deep visual representations. 2017. URL https://arxiv.org/abs/1704. 05796.   \n[6] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O\u2019Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika, and O. van der Wal. Pythia: A suite for analyzing large language models across training and scaling. 2023. URL https: //arxiv.org/abs/2304.01373.   \n[7] S. Bills, N. Cammarata, D. Mossing, H. Tillman, L. Gao, G. Goh, I. Sutskever, J. Leike, J. Wu, and W. Saunders. Language models can explain neurons in language models. 2023. URL https: //openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html.   \n[8] S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow. 2021.   \n[9] T. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. Conerly, N. Turner, C. Anil, C. Denison, A. Askell, R. Lasenby, Y. Wu, S. Kravec, N. Schiefer, T. Maxwell, N. Joseph, Z. Hatfield-Dodds, A. Tamkin, K. Nguyen, B. McLean, J. E. Burke, T. Hume, S. Carter, T. Henighan, and C. Olah. Towards monosemanticity: Decomposing language models with dictionary learning. 2023. URL https://transformer-circuits.pub/2023/ monosemantic-features/index.html.   \n[10] P. Christiano. What failure looks like. 2019. URL https://www.alignmentforum.org/ posts/HBxe6wdjxK239zajf/more-realistic-tales-of-doom.   \n[11] H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey. Sparse autoencoders find highly interpretable features in language models. In The Twelfth International Conference on Learning Representations, 2024.   \n[12] N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby, D. Drain, C. Chen, R. Grosse, S. McCandlish, J. Kaplan, D. Amodei, M. Wattenberg, and C. Olah. Toy models of superposition. 2022. URL https://arxiv.org/abs/2209. 10652.   \n[13] A. Foote, N. Nanda, E. Kran, I. Konstas, S. Cohen, and F. Barez. Neuron to graph: Interpreting language model neurons at scale. In ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models, 2023.   \n[14] A. Gleave, M. Dennis, S. Legg, S. Russell, and J. Leike. Quantifying differences in reward functions. 2021. URL https://arxiv.org/abs/2006.13900.   \n[15] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Y. W. Teh and M. Titterington, editors, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9, pages 249\u2013256. PMLR, 2010.   \n[16] W. Gurnee, N. Nanda, M. Pauly, K. Harvey, D. Troitskii, and D. Bertsimas. Finding neurons in a haystack: Case studies with sparse probing. In Transactions on Machine Learning Research, 2023.   \n[17] M. Honnibal, I. Montani, S. Van Landeghem, and A. Boyd. spacy: Industrial-strength natural language processing in python, 2020. URL https://spacy.io/.   \n[18] R. Huben. https://www.lesswrong.com/posts/bducmgmjjnctc7jkc/researchreport-sparse-autoencoders-find-only-9-180-board. 2024. URL https://www.lesswrong.com/posts/BduCMgmjJnCtc7jKc/ research-report-sparse-autoencoders-find-only-9-180-board.   \n[19] C. Hutto and E. Gilbert. Vader: A parsimonious rule-based model for sentiment analysis of social media text. In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216\u2013225, 2014.   \n[20] E. Jenner and A. Gleave. Preprocessing reward functions for interpretability. 2022. URL https://arxiv.org/abs/2203.13553.   \n[21] A. Karpathy, J. Johnson, and L. Fei-Fei. Visualizing and understanding recurrent networks. 2015. URL https://arxiv.org/abs/1506.02078.   \n[22] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/ anthology/P11-1015.   \n[23] T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivi\u00e8re, M. S. Kale, J. Love, P. Tafti, L. Hussenot, P. G. Sessa, A. Chowdhery, A. Roberts, A. Barua, A. Botev, A. Castro-Ros, A. Slone, A. H\u00e9liou, A. Tacchetti, A. Bulanova, A. Paterson, B. Tsai, B. Shahriari, C. Le Lan, C. A. Choquette-Choo, C. Crepy, D. Cer, D. Ippolito, D. Reid, E. Buchatskaya, E. Ni, E. Noland, G. Yan, G. Tucker, G.-C. Muraru, G. Rozhdestvenskiy, H. Michalewski, I. Tenney, I. Grishchenko, J. Austin, J. Keeling, J. Labanowski, J.-B. Lespiau, J. Stanway, J. Brennan, J. Chen, J. Ferret, J. Chiu, J. Mao-Jones, K. Lee, K. Yu, K. Millican, L. L. Sjoesund, L. Lee, L. Dixon, M. Reid, M. Miku\u0142a, M. Wirth, M. Sharman, N. Chinaev, N. Thain, O. Bachem, O. Chang, O. Wahltinez, P. Bailey, P. Michel, P. Yotov, R. Chaabouni, R. Comanescu, R. Jana, R. Anil, R. McIlroy, R. Liu, R. Mullins, S. L. Smith, S. Borgeaud, S. Girgin, S. Douglas, S. Pandya, S. Shakeri, S. De, T. Klimenko, T. Hennigan, V. Feinberg, W. Stokowiec, Y.-h. Chen, Z. Ahmed, Z. Gong, T. Warkentin, L. Peran, M. Giang, C. Farabet, O. Vinyals, J. Dean, K. Kavukcuoglu, D. Hassabis, Z. Ghahramani, D. Eck, J. Barral, F. Pereira, E. Collins, A. Joulin, N. Fiedel, E. Senter, A. Andreev, and K. Kenealy. Gemma: Open models based on gemini research and technology. arXiv, 2024. URL https://arxiv.org/abs/2403.08295.   \n[24] Meta. Llama 3: Open and efficient foundation language models. https://ai.meta.com/ blog/meta-llama-3/, 2024.   \n[25] E. J. Michaud, A. Gleave, and S. Russell. Understanding learned reward functions. 2020. URL https://arxiv.org/abs/2012.05862.   \n[26] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013.   \n[27] T. Niven and H.-Y. Kao. Probing neural network comprehension of natural language arguments. In A. Korhonen, D. Traum, and L. M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658\u20134664. Association for Computational Linguistics, 2019.   \n[28] C. Olah, A. Mordvintsev, and L. Schubert. Feature visualization. 2017. URL https:// distill.pub/2017/feature-visualization.   \n[29] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? In Vision Research, volume 37, pages 3311\u20133325, 1997.   \n[30] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates, Inc., 2022. ", "page_idx": 12}, {"type": "text", "text": "[31] S. Rajamanoharan, A. Conmy, L. Smith, T. Lieberum, V. Varma, J. Kram\u00e1r, R. Shah, and N. Nanda. Improving dictionary learning with gated sparse autoencoders. arXiv preprint arXiv:2404.16014, 2024.   \n[32] S. Rajamanoharan, T. Lieberum, N. Sonnerat, A. Conmy, V. Varma, J. Kram\u00e1r, and N. Nanda. Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders. arXiv preprint arXiv:2407.14435, 2024.   \n[33] V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. 2020. URL https://arxiv.org/abs/1910.01108.   \n[34] L. Sharkey, D. Braun, and B. Millidge. Taking features out of superposition with sparse autoencoders. 2022. URL https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/ interim-research-report-taking-features-out-of-superposition.   \n[35] D. Till. Do sparse autoencoders find \"true features\"? 2024. URL https://www.lesswrong. com/posts/QoR8noAB3Mp2KBA4B/do-sparse-autoencoders-find-true-features.   \n[36] Unalignment. Toxic-dpo dataset v0.2, 2023. https://huggingface.co/datasets/ unalignment/toxic-dpo-v0.2.   \n[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. 2017. URL http://arxiv.org/abs/1706.03762.   \n[38] L. von Werra. distilbert-imdb, 2023. URL https://huggingface.co/lvwerra/ distilbert-imdb.   \n[39] L. von Werra, Y. Belkada, L. Tunstall, E. Beeching, T. Thrush, and N. Lambert. TRL: Transformer Reinforcement Learning, 2023. URL https://github.com/huggingface/trl.   \n[40] Y. Wolf, N. Wies, O. Avnery, Y. Levine, and A. Shashua. Fundamental limitations of alignment in large language models. 2023. URL https://arxiv.org/abs/2304.11082.   \n[41] Z. Yun, Y. Chen, B. Olshausen, and Y. LeCun. Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. In E. Agirre, M. Apidianaki, and I. Vulic\u00b4, editors, Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 1\u201310. Association for Computational Linguistics, 2021.   \n[42] C.-H. Zhang and J. Huang. The sparsity and bias of the lasso selection in high dimensional linear regression. In The Annals of Statistics, volume 36, pages 1567\u20131594, 2008. ", "page_idx": 12}, {"type": "text", "text": "A RLHF with proximal policy optimization ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In PPO, an evaluator rates the model\u2019s outputs for a given task. These ratings define the reward function $\\operatorname{Reward}(\\tau)$ , where $\\tau$ represents a trajectory of state-action pairs $\\left(s_{1},a_{1},\\ldots,s_{T},a_{T}\\right)$ with $s_{t}$ as the text context at time $t$ , and $a_{t}$ the token generated at time $t$ . ", "page_idx": 12}, {"type": "text", "text": "The objective is to maximize the expected sum of rewards $J(\\theta)$ , defined as: ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ(\\theta)=\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\mathrm{Reward}(\\tau)\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\pi_{\\theta}$ is the policy parameterized by $\\theta$ . PPO optimizes this objective by updating the policy $\\pi_{\\theta}$ to a new policy $\\pi_{\\theta^{\\prime}}$ in a way that restricts the change in $\\pi$ . This is achieved by optimizing the clipped objective function: ", "page_idx": 12}, {"type": "equation", "text": "$$\nL(\\theta,\\theta^{\\prime})=\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\operatorname*{min}\\left(\\frac{\\pi_{\\theta^{\\prime}}(a|s)}{\\pi_{\\theta}(a|s)}A_{\\theta}(s,a),\\mathrm{clip}\\left(\\frac{\\pi_{\\theta^{\\prime}}(a|s)}{\\pi_{\\theta}(a|s)},1-\\epsilon,1+\\epsilon\\right)A_{\\theta}(s,a)\\right)\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $A_{\\theta}(s,a)$ is the advantage function, which estimates the relative value of an action compared to a baseline, and $\\epsilon$ is a hyperparameter controlling the extent to which the policy can change. We graphically represent our RLHF pipeline in Figure 6. ", "page_idx": 12}, {"type": "image", "img_path": "xUoNgR1Byy/tmp/b2655bfd264508b5f00dd86b8ae2e1487db931660ce48be3b92e76378d273104.jpg", "img_caption": ["RLHF Optimization Process "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 6: A prefix from a dataset is sampled as a prompt to an LLM, and then completed with the generation \u201cjust fine\u201d in this case. Log probabilities are sampled from both the reference and policy model to compute the KL-divergence from the reference model, as well as compute the reward on the policy model\u2019s output distribution. ", "page_idx": 13}, {"type": "text", "text": "B Qualitative analysis of a Pythia-70m LFPs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We use our method in $\\S3$ to fine-tune Pythia-70m to generate positive movie review completions with PPO and train sparse autoencoders on its activations. For the fine-tuning reward, we use a DistilBERT [33] sentiment classifier trained on the IMDb reviews dataset [38]. Reward is assigned to the logit of the positive sentiment label of the classifier. Following $\\S3.4$ , we generate explanations of the autoencoder features, using activations caused by inputs from the IMDb reviews dataset, and give a large sample of explanations in Table 8. ", "page_idx": 13}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/02651dd5a342f77df74dc06f525577e63b1eeb0f34012b63d9561efddd6bb13e.jpg", "table_caption": ["Table 8: Features with their corresponding explanations generated by GPT-4 for the top- $k$ most likely features to be present in the base model for the fine-tuned instance of Pythia- $70\\mathrm{m}$ . "], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/b84cc5419599fb4f2731218fa7318d8a56f7b6bd9d851343fdc9d0002f80ccb2.jpg", "table_caption": ["Layer Feature Index Explanation "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/28957e7d3ff057e7bb0b01c9578c99bf586bcfe3ac23b9d7bcdec1beb4a85fd4.jpg", "table_caption": ["Layer "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Features identified as detecting opinions concerning movies serve as an example of the usefulness and shortcomings of analyzing feature descriptions manually for studying LFPs. Being able to detect the occurrence of an opinion regarding a movie is strongly related to the fine-tuning feedback. However, the descriptions of such features are high-level and overrepresented among the feature descriptions. In the fine-tuned Pythia-70m instance, from a sample of 50 features from the model (10 per layer), there are 21 feature explanations that mention detecting opinions or reviews in the context of movies. In layer 4, 8 are described as being for this purpose. Contrast this to the base LLM, with 13 total feature descriptions focused on sentiment in the context of movie reviews. ", "page_idx": 15}, {"type": "text", "text": "This data alone does not allow for a clear picture of the LLMs LFPs to be constructed. Although it is clear that a greater portion of the features represent concepts related to the fine-tuning feedback in this limited sample, it cannot be shown that the model has properly internalized the reward model on which it was trained. Additionally, it is unlikely for the base LLM to inherently have 13 of the 50 sampled features applied to identifying opinions on movies, which shows that the nature of the input data used to sample activations can skew GPT-4 \u2019s description of the feature. If a feature consistently activates on negative opinions, but the entire sample set is movie reviews, it might be unclear to GPT- $.4$ whether the feature is activating in response to negative sentiment, or to negative sentiment in movie reviews specifically. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "C Layer divergences ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, we graph the divergence of the RLHF-tuned models from the base LLM on a per layer basis.   \nSee Figure 7. ", "page_idx": 16}, {"type": "image", "img_path": "xUoNgR1Byy/tmp/72e0a183a0d17e33f839763f1c2d08c0ccf57457a1a70243479ca8777f8dce59.jpg", "img_caption": ["Figure 7: Divergences on a per-layer basis for various model and reward function combinations. Pythia-70m and Pythia-160m 6 and 12 layers respectively. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "D Reconstruction of the VADER lexicon from the fine-tuned model ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide more complete results for the experiment in $\\S4.1$ . We attempt to reconstruct the VADER lexicon from the fine-tuned modeled by comparing the predictions of the probes we fti to the fine-tuned models LFPs to the fine-tuning reward. We give thirty random samples in Table 9. ", "page_idx": 16}, {"type": "text", "text": "E Ranking tokens of the same polarity ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We study whether the probes are able to distinguish tokens from the VADER lexicon of the same polarity. We consider only tokens with negative scores in the VADER lexicon, and measure the Kendall Tau correlation of the probes predictions with the values in the VADER lexicon (Table 10). ", "page_idx": 16}, {"type": "text", "text": "F Methodology for autoencoder training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we discuss briefly decisions in our sparse autoencoder training pipeline. ", "page_idx": 16}, {"type": "text", "text": "The $\\ell_{1}$ coefficient. During autoencoder training, the sparsity of the feature dictionaries is enforced by adding an $\\ell_{1}$ regularization loss to the hidden state, akin to Lasso [42]. Ideally the $\\ell_{1}$ coefficient is low so as to allow the autoencoder training objective to reconstruct activation vectors with high fidelity using the dictionary features. But if it is too small, we observe an explosion in the \u201ctrue\" sparsity loss, namely the average number of non-zero positions in the dictionary features. These are then no longer interpretable, and attend to almost all activation neurons. ", "page_idx": 16}, {"type": "text", "text": "As such, we choose an $\\ell_{1}$ coefficient in a reasonable range to minimize both the true sparsity loss, as well as activation vector reconstruction loss. Empirically, we found a range of 0.001 and 0.002 to be suitable in most cases. See Figure 8 for an illustration of the loss variation, over a single epoch of Pythia-70m trained with varying values of the $\\ell_{1}$ coefficient. We average the \u201ctrue\" sparsity loss over all highly divergent layers, and scale down by a factor of 100 for each in graphing. ", "page_idx": 16}, {"type": "text", "text": "Table 9: Thirty tokens and their reconstructed sentiment values compared with their original sentiment values from GPT-Neo- $125\\mathrm{m}$ . ", "page_idx": 17}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/72558dd61b5566f545017d96d0b62703a5d68e5622f00a496cbaaa9b77f14df9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/0c1c4c9c2e37c251857bcd477045e93a778afbee96f317447ac026dbcbc99466.jpg", "table_caption": ["Table 10: Kendall Tau correlation of our probes predictions and RLHF reward model for all tested LLMs and negative tokens only. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "2. Tying encoder and decoder. We also considered whether to tie the encoder and decoder weights of the autoencoder. Tying the encoder and decoder weights has the advantage that each dictionary feature can then be explicitly written as a function of activation neurons. However, the model may be able to optimize the reconstruction and sparsity losses slightly better if the weights are left untied. We ran a small experiment on Pythia-160m and Pythia-70m with alternating the decoder and encoder weights as tied as well as untied. We found both the reconstruction loss and true sparsity loss to converge faster with tied weights. See Table 11. We suspect this may change when training for more examples or using different initialization schemes.   \n3. How to select divergent layers. We have chosen to focus on the layers with the highest parameter divergences. As can be seen in Appendix $\\mathbf{C}$ and Figure 7, these tend to be the deepest layers of the neural networks. We briefly explored here the effects of looking at the lowest / initial layers of the neural networks instead. ", "page_idx": 17}, {"type": "image", "img_path": "xUoNgR1Byy/tmp/219967ce137cfe760f9812e20032c819b193b0697dac4931009eab846d1c33a6.jpg", "img_caption": ["Figure 8: Normalized reconstruction and scaled true sparsity losses for Pythia- $.70\\mathrm{m}$ over 1 training epoch, over varying values of the $\\ell_{1}$ coefficient. Both metrics are averaged over all highly divergent layers, and hyperparameter choices are otherwise as described in $\\S3.2$ . "], "img_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/769768b18b9ce05414b156a0ffa04ee7e8b364c6766ee6ede03c59915c17a3f6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 11: Normalized reconstruction and scaled true sparsity losses for Pythia-70m and Pythia-160m over 1 training epoch, over differing choices of whether to tie encoder and decoder weights. Both metrics are averaged over all highly divergent layers, and hyperparameter choices are otherwise as described in $\\S3.2$ . ", "page_idx": 18}, {"type": "text", "text": "Towards the end of our project, we ran a small experiment on Pythia-160m and Pythia-70m with alternating selecting the layers for autoencoder extraction as the lowest layers, vs the highest divergence layers. We found both the reconstruction loss and true sparsity loss to be far less for the lower most layers. A future study to examine the dictionary features extracted from these lowest layers would be interesting. See Table 12 for the observed metrics. ", "page_idx": 18}, {"type": "table", "img_path": "xUoNgR1Byy/tmp/67b5bfac193044adba855a7344ce3f462640660417527428a8e9250415a291ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 12: Normalized reconstruction and scaled true sparsity losses for Pythia-70m and Pythia-160m over 1 training epoch, over differing choices of divergence. Both metrics are averaged over all highly divergent layers, and hyperparameter choices are otherwise as described in $\\S3.2$ . ", "page_idx": 18}, {"type": "text", "text": "G Small human study on automated feature explanation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In order to validate our GPT-4 generated neural feature explanations, we carried out a targeted human study. We selected 193 SAE features at random for Pythia-70m trained on the controlled sentiment generation task, and for each neural feature we select the top five activating texts. We then scrambled the GPT-4 generated explanations, and asked each annotator to identify either the best explanation when presented with the GPT-4 auto-generated explanation alongwith three others randomly chosen from those for other neural features, or choose none of the above. We found that: ", "page_idx": 19}, {"type": "text", "text": "1. The two annotators chose the actual assigned GPT-4 explanation $67.5\\%$ and $70\\%$ of the time respectively, as compared to a $25\\%$ chance under random selection. 2. The two annotators also had an $87.5\\%$ inter annotator agreement rate. 3. The same two annotators selected \u201cNone of the Above\" $20\\%$ and $22.5\\%$ of the time respectively. ", "page_idx": 19}, {"type": "text", "text": "We consider these results as validating that the GPT- $\\cdot4$ provided explanations at least somewhat correspond with human judgments. ", "page_idx": 19}, {"type": "text", "text": "We carried this study out only for human validation that our own automated feature explanation setup was performing adequately. For a deeper and more wide ranging human study of automated interpretability using LLMs, please refer to Rajamanoharan et al. [31, 32]. ", "page_idx": 19}, {"type": "text", "text": "More details: Two annotators each annotated 40 test questions as crafted above. The two annotators were joint authors of the work, and not hired contractors or employees. Neither annotator had seen the \u201ctrue\" explanation for each feature before labelling. Since all of the examples were generated by publicly available Pythia-70m trained on the controlled sentiment generation task, not tied to any confidential information or user sensitive data, and a fairly objective classification of feature relevance, we do not feel there were any risks to the annotators involved or any other third parties, and did not need IRB review. The exact prompt provided to the annotators is below: ", "page_idx": 19}, {"type": "text", "text": "Below is a sequence of texts, alongwith the activations for a neuron. Red being the darkest. Pick which of the explanations below is the best choice, or none of them above if none seem suitable. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We pose a hypothesis and defend it using our experiments, arguing that they show that the hypothesis is correct. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We include a limitations subsection in our conclusion. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Yes. We give formal descriptions complete with details and complementary sections in the Appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The source code for all experiments is available in our supplementary materials. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 21}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We give the hyperparameters we used for all experiments as well as additional details in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We give a measure of statistical significance for our Kendall Tau divergence measures. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We state the amount of compute required to reproduce our experiments and train the models used for them ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have reviewed the code of ethics and find no violation in our work. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We give background on the risks related to models not internalizing human preferences, and our work aims to combat that, which we claim is beneficial for the safety of deploying these models. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We cite the repositories or papers for all models and datasets used in our experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our GitHub README includes code examples for running experiments as well as an explanation of the structure of the repository. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We include the full text of instructions. Since the two annotators were authors of the work as well, no compensation of contractors or employees was required. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Our paper discusses that given that no private data was included, and the nature of the labeling task as conducted by two of the authors, there is no risk to human participants or need for IRB review. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]