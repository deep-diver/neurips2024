[{"type": "text", "text": "Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mahdi Morafah\u22171, Vyacheslav Kungurtsev2, Hojin Chang1, Chen Chen3, Bill Lin1 ", "page_idx": 0}, {"type": "text", "text": "1University of California San Diego (UCSD), 2Czech Technical University in Prague, 3University of Central Florida (UCF) \u2217Correspondence: mmorafah@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated Learning (FL) has emerged as a promising paradigm for collaborative machine learning, while preserving user data privacy. Despite its potential, standard FL algorithms lack support for diverse heterogeneous device prototypes, which vary significantly in model and dataset sizes\u2014from small IoT devices to large workstations. This limitation is only partially addressed by existing knowledge distillation (KD) techniques, which often fail to transfer knowledge effectively across a broad spectrum of device prototypes with varied capabilities. This failure primarily stems from two issues: the dilution of informative logits from more capable devices by those from less capable ones, and the use of a single integrated logits as the distillation target across all devices, which neglects their individual learning capacities and and the unique contributions of each device. To address these challenges, we introduce TAKFL, a novel KD-based framework that treats the knowledge transfer from each device prototype\u2019s ensemble as a separate task, independently distilling each to preserve its unique contributions and avoid dilution. TAKFL also incorporates a KD-based self-regularization technique to mitigate the issues related to the noisy and unsupervised ensemble distillation process. To integrate the separately distilled knowledge, we introduce an adaptive task arithmetic knowledge integration process, allowing each student model to customize the knowledge integration for optimal performance. Additionally, we present theoretical results demonstrating the effectiveness of task arithmetic in transferring knowledge across heterogeneous device prototypes with varying capacities. Comprehensive evaluations of our method across both computer vision (CV) and natural language processing (NLP) tasks demonstrate that TAKFL achieves state-of-the-art results in a variety of datasets and settings, significantly outperforming existing KD-based methods. Our code is released at https://github.com/MMorafah/TAKFL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated Learning (FL) has rapidly gained traction as a promising approach to train machine learning models collaboratively across multiple devices, while preserving the privacy of user data. Standard federated learning methods, such as FedAvg [33], however, are primarily designed for unrealistic device-homogeneous scenarios, where all devices are assumed to have identical compute resource and can train the same neural network architecture [28, 33, 49, 21, 27, 48, 31]. Therefore, standard FL cannot support the participation of heterogeneous devices, all of which could significantly contribute to model training due to their unique and invaluable local datasets. To address this gap, knowledge distillation (KD) techniques have emerged as a promising approach to establish federation among heterogeneous device prototypes and facilitate knowledge transfer between them. In this approach, locally updated client models from different device prototypes, collectively termed as ensembles, serve as teachers to distill their knowledge into each device prototype\u2019s server student model using an unlabeled public dataset. ", "page_idx": 0}, {"type": "text", "text": "Despite their success, however, existing KD-based methods for device heterogeneous FL are primarily designed for scenarios where device prototypes are in the same-size with similar capabilities, i.e. same model and dataset sizes. However, in practice, device capabilities vary widely, ranging from small devices like IoTs with small models and small datasets to large devices like workstations with large models and large datasets. This diversity, often overlooked in the existing literature, results in device prototypes with varying strengths and information qualities. Unfortunately, existing methods struggle to establish effective knowledge transfer in these challenging, real-world device heterogeneous settings, primarily due to two reasons: $\\textcircled{1}$ Existing methods often disregard the individual strengths and information quality of each device prototype\u2019s ensembles and integrate their logits into a single distillation target. This approach dilutes the richer, more informative logits from larger, more capable devices with less informative logits from smaller, less capable ones. $\\circledcirc$ Additionally, these methods employ this single integrated distillation target to transfer knowledge across all different size student models. This one-size-fits-all approach fails to provide customized knowledge integration based on the unique learning capacities of each student and the specific helpfulness of each device prototype\u2019s ensembles. ", "page_idx": 1}, {"type": "text", "text": "Moreover, the heterogeneous ensemble distillation process can inadvertently lead student models into erroneous learning directions, causing them to forget their self-knowledge acquired through averaged locally updated parameters. This issue arises primarily due to two reasons: $\\dot{\\Phi}$ The distillation process introduces noise, as the ensembles\u2019 logits are inferred on an unfamiliar public dataset, distinct from their original training data. Additionally, the presence of data heterogeneity and the insufficient training of some ensembles, due to computational constraints, can further exacerbate this noise. $\\textcircled{2}$ The distillation process lacks supervision from the actual private datasets, which are the ultimate learning objectives. Consequently, these factors, combined with the limitations outlined earlier, result in suboptimal knowledge transfer in device heterogeneous settings. This underscores the urgent need for a more effective knowledge transfer framework. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce TAKFL, a novel \u201cTask Arithmetic Knowledge Transfer Integration for Federated Learning\u201d framework, designed to overcome the fundamental limitations in the existing methods and improve knowledge transfer in scenarios where device prototypes vary in size\u2014both model and dataset\u2014and consequently, in strength. TAKFL treats knowledge transfer from each device prototype\u2019s ensembles as separate tasks, distilling them independently to ensure that each prototype\u2019s unique contributions are accurately distilled without interference. To tackle the challenges associated with noisy and unsupervised ensemble distillation, we incorporate a KD-based self-regularization technique into this individual knowledge transfer process. Subsequently, to selectively integrate the separately distilled knowledge from heterogeneous prototypes\u2019 ensembles, we introduce an adaptive task arithmetic knowledge integration method by extending the notion of task vectors from centralized learning to federated learning. Our approach enables the student model to strategically customize the knowledge integration process based on the quality of knowledge from each prototype\u2019s ensembles and its intrinsic capacity, aiming to achieve optimal performance. We present theoretical results, grounded on the established theoretical learning properties of overparametrized neural networks, that conceptualize knowledge distillation as the allocation of device prototypes\u2019 capacities to accurately fit the chosen logits. These results demonstrate the advantages of employing task arithmetic for knowledge transfer in terms of overall accuracy, coverage, and efficiency, as well as the adaptive knowledge integration based on the capacity of the student prototype. Furthermore, we comprehensively evaluate our method across both computer vision (CV) and natural language processing (NLP) tasks, utilizing various datasets and architectures, and demonstrate that TAKFL consistently achieves state-of-the-art (SOTA) performance. ", "page_idx": 1}, {"type": "text", "text": "The contribution of our paper is as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. We formalize and review the important considerations of the problem statement of federated learning with heterogeneous device prototypes.   \n2. We introduce TAKFL, a novel KD-based method designed to overcome the fundamental limitations of existing approaches, effectively facilitating knowledge transfer across diverse heterogeneous device prototypes with varying capabilities.   \n3. We present the first theoretical model for device heterogeneous KD, and demonstrate the effectiveness and efficiency of TAKFL compared to the standard alternatives that do not adapt to the student\u2019s self-knowledge quality and available learning capacity.   \n4. Our comprehensive experimental evaluations on both CV and NLP tasks, spanning various datasets and architectures, reveal that TAKFL consistently achieves SOTA performance, outperforming existing KD-based methods. ", "page_idx": 1}, {"type": "image", "img_path": "y6JotynERr/tmp/2943318c993de176e0e4e6522f148ff33fc44c8c31bd12d9f16804ce3cb12c40.jpg", "img_caption": ["(a) Vanilla Ensemble Distillation ", "(b) Overview of TAKFL "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Overview of our approach and its distinction from prior works. (a) This figure illustrates the vanilla ensemble distillation process, where logits from ensembles of various sizes are averaged and used as the distillation target across all prototypes. This approach leads to the dilution of information and suboptimal knowledge transfer (refer to Sections 6 and 7 for details). (b) This figure depicts our approach, TAKFL, which treats knowledge transfer from each prototype\u2019s ensemble as a separate task and distills them independently. Additionally, a KD-based self-regularization technique is introduced to mitigate issues related to the noisy and unsupervised ensemble distillation. Finally, the heterogeneously distilled knowledge is strategically integrated using an adaptive task arithmetic operation, allowing for customized knowledge integration based on each student prototype\u2019s needs. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Device Heterogeneous FL. Prior works on device heterogeneous FL have considered two distinct approaches with different objectives and settings. The first array of studies focuses on accommodating devices with varying compute resources, aiming to train a single global model. Techniques such as static and rolling-based partial model training allow devices to train a sub-model of the global model tailored to their compute resources [11, 18, 3, 1]. However, this approach does not fully reflect real-world scenarios. In practice, device prototypes such as IoTs and smartphones have unique neural network architectures designed for their specific configurations and underlying tasks, which may not support training varying neural architectures. This highlights a significant limitation in accommodating the full spectrum of device heterogeneity in this approach. The second array of studies addresses a more practical scenario where device prototypes with heterogeneous model architectures participate in FL to enhance their global model performance through mutual knowledge sharing [30, 43, 6]. In this context, KD techniques are used to transfer knowledge among prototypes, where locally updated client models, termed as ensembles, serve as teachers to distill their knowledge into each server\u2019s student model using an unlabeled public dataset. For example, FedDF [30] uses vanilla logit averaging, while Fed-ET [6] applies an uncertainty-weighted logit averaging, enhanced by a diversity regularization technique. However, existing works typically focus on settings where prototypes have similar capabilities\u2014both model and dataset sizes\u2014and thus neglecting the challenges in more diverse settings with varying capabilities. This oversight leaves their effectiveness in such settings largely unexplored. In this paper, we aim to study the underexplored diverse heterogeneous device settings. See Appendix A for a more detailed discussion on the related works. ", "page_idx": 2}, {"type": "text", "text": "Model Editing via Task Arithmetic. Traditional methods for model editing often involve expensive joint fine-tuning across multiple tasks, which can limit scalability and democratization [62]. Recently, a promising technique called task arithmetic has emerged as a cost-effective and scalable method for updating pre-trained models with new information or refining undesired behavior [53, 37, 32]. The concept of \u201ctask vectors\u201d introduced by Wortsman et al. [53] plays a pivotal role in these techniques. For any given task $t$ , a task vector is derived by subtracting the model\u2019s pre-trained weights $\\theta_{p r e}$ from its fine-tuned weights $\\theta_{f t}^{t}$ on task $t$ , i.e. $\\pmb{\\tau}_{t}=\\pmb{\\theta}_{f t}-\\pmb{\\theta}_{p r e}$ . These task vectors act as unique representations for specific tasks. Furthermore, researchers have demonstrated $\\pmb{\\theta}=\\pmb{\\theta}_{p r e}\\pmb{+}\\lambda\\sum_{t=1}^{T}\\pmb{\\tau}_{t}$ t, ioplnee  tcaasnk  evffeeccttoirvse $\\{\\dot{\\tau}_{t}\\}_{t=1}^{T}$ ,  a anmdo dinelt ecgarpaatibnlge  othf ehamn idnltion ga  mpruelt-itprlaei nteasdk sm [o5d3e, l 5v7i]a. learning setting, introducing a task arithmetic for knowledge distillation across diverse heterogeneous device prototypes. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Statement: FL with Heterogeneous Device Prototypes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a cross-device FL setup with a set of $M$ distinct device prototypes $\\mathbb{M}$ , i.e., $M\\,=\\,|\\mathbb{M}|$ . Each device prototype $m_{j}\\in\\mathbb{M}$ has a distinct neural network architecture $\\bar{f}^{j}(\\cdot;\\pmb{\\theta}^{j})$ parameterized by $\\pmb{\\theta}^{j}\\,\\in\\,\\mathbb{R}^{n_{j}}$ and a set of clients $\\mathbb{C}^{j}$ , with $N^{j}\\,=\\,|\\mathbb{C}^{j}|$ clients in total. Each client $c_{k}\\,\\in\\,\\mathbb{C}^{j}$ has a local private dataset $\\mathbb{D}_{k}^{j}=\\{(\\pmb{x}_{i},y_{i})\\}_{i=1}^{n_{j,k}}$ , where $n_{j,k}=|\\mathbb{D}_{k}^{j}|$ , and locally trains the parameters $\\theta^{j}$ of the neural network architecture $f^{j}$ on its local dataset. Furthermore, denote $\\mathbb{D}^{j}=\\cup_{k\\in\\mathbb{C}^{j}}\\mathbb{D}_{k}^{j}$ to be the union of the private datasets for device prototype $j$ . We assume $\\mathbb{D}^{j}\\sim\\mathcal{D}^{j}$ , that is a subsample from the population distribution $\\mathcal{D}^{j}$ and similarly $\\mathbb{D}_{k}^{j}\\sim\\mathcal{D}_{k}^{j}$ . The union of the private datasets, i.e. $\\textstyle\\mathbb{D}=\\bigcup_{j\\in\\mathcal{M}}\\mathbb{D}^{j}$ , is sampled from the entire population $\\mathcal{D}$ , which is defined as an unknown mixture of the distributions each device prototype sampled its data from, i.e. generically non-i.i.d. We formalize this as a mixture of local clients data population, i.e., $\\begin{array}{r}{\\mathcal{D}=\\sum_{j}\\omega_{j,:}\\mathcal{D}^{j}=\\sum_{j}\\sum_{k}\\omega_{j,k}\\mathcal{D}_{k}^{j}}\\end{array}$ , where $0\\le\\omega_{j,k}\\le1$ and $\\begin{array}{r}{\\sum_{j k}\\omega_{j,k}=1}\\end{array}$ , and $\\omega_{j,k}$ is unknown. ", "page_idx": 3}, {"type": "text", "text": "The ultimate objective is to minimize the test error and thus enable accurate inference for each device prototype $j$ , aiming to obtain the optimal parameters for the population dataset: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{\\pmb{\\theta}^{j}}{\\mathrm{argmin}}\\,\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathcal{D}}[\\ell(f^{j}(\\pmb{x};\\pmb{\\theta}^{j}),\\boldsymbol{y})]=\\underset{\\pmb{\\theta}^{j}}{\\mathrm{argmin}}\\,\\sum_{j=1}^{M}\\sum_{k=1}^{N^{j}}\\omega_{i,k}\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathcal{D}_{k}^{j}}[\\ell(f^{j}(\\pmb{x};\\pmb{\\theta}^{j}),\\boldsymbol{y})]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\ell(\\cdot,\\cdot)$ is the sample-wise loss function (e.g. cross entropy for image classification) and we decompose by total population loss with the linearity of expectation in the mixture. See Fig 4b for a visual illustration of heterogeneous device prototype FL. ", "page_idx": 3}, {"type": "text", "text": "4 Background: Federated Ensemble Distillation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To address the limitations of standard $\\mathrm{FL}$ in device heterogeneous settings, Lin et al. [30] proposed ensemble knowledge distillation to transfer knowledge between heterogeneous device prototypes in FL. This procedure consists of two stages: (1) local per-prototype FL, and (2) server-side vanilla ensemble distillation. The details of each stage discussed in the following paragraphs. ", "page_idx": 3}, {"type": "text", "text": "Local Per-Prototype FL. In this context, at each round $r$ a subset of clients $\\mathbb{C}_{r}^{j}$ from each device prototype $j\\in\\mathbb{M}$ is randomly selected by the server and download their corresponding model initialization $\\theta_{r}^{j}$ . Each client $c_{k}^{j}\\in\\mathbb{C}_{r}^{j}$ , starting from this model initialization, locally train the model $f^{j}$ on its local private data $\\mathbb{D}_{k}^{j}$ by taking multiple steps of stochastic gradient descent. Then, they send parameters, and computes $\\begin{array}{r}{\\pmb{\\theta}_{a v g}^{j}\\,=\\,\\sum_{k\\in\\mathbb{C}_{r}^{j}}\\frac{|\\mathbb{D}_{k}|}{\\sum_{k\\in\\mathbb{C}_{r}^{j}}|\\mathbb{D}_{k}|}\\widehat{\\pmb{\\theta}}_{k}^{j}}\\end{array}$ $\\{\\widehat{\\theta}_{k}^{j}\\}_{k\\in\\mathbb{C}_{r}^{j}}$ . In classic federated learning formalism, the parameters $\\pmb{\\theta}_{a v g}^{j}$ satisfy, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{a v g}^{j}\\in\\underset{\\pmb{\\theta}^{j}}{\\mathrm{argmin}}~\\sum_{k=1}^{N^{j}}\\mathbb{E}_{(\\pmb{x},y)\\sim\\mathbb{D}_{k}^{j}}\\left[\\ell(f^{j}(\\pmb{x};\\pmb{\\theta}^{j}),y)\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Vanilla Ensemble Distillation. In this stage, each server model $f^{j}$ gets initialized with $\\theta^{j}$ , and undergoes updates using ensemble knowledge distillation. Here, heterogeneous client models from heterogeneous device prototypes, collectively termed as ensembles, serve as teachers, i.e. $\\tau:=$ $\\{f^{i}(\\cdot,\\overbrace{\\pmb{\\theta}_{k}^{i}})|\\,i\\in\\mathbb{M},k\\in\\mathbb{C}^{i}\\}$ , transferring their knowledge to each server student model, i.e. ${\\cal S}_{i}:=$ $f^{i}(\\cdot,\\pmb\\theta^{i})$ . For simplicity, we drop the index for each server student model, denoting it as $\\boldsymbol{S}$ . The ensemble distillation loss using a mini-batch of data from an unlabeled public dataset, i.e $_x\\in$ $\\mathbb{D}^{p u b l i c}$ , can be defined by the following equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{ED}}=\\mathrm{KL}\\Bigg[\\sigma\\Bigg(\\frac{1}{\\vert\\mathcal{T}\\vert}\\sum_{\\mathcal{F}\\in\\mathcal{T}}\\mathcal{F}(\\pmb{x})\\Bigg)\\,,\\,\\sigma\\,(S(\\pmb{x}))\\Bigg],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma(\\cdot)$ is the softmax function. As illustrated in Eq. 3, vanilla ensemble distillation treats all heterogeneous device prototypes\u2019 ensembles equally by uniformly averaging their logits. This way of knowledge integration overlooks the individual strengths and informational value of each prototype\u2019s ensembles. As a result, the richer, more informative logits from stronger ensembles are diluted by less informative logits from weaker ensembles, leading to information loss. Furthermore, this averaged logits is used as the distillation target across different-sized student models, irrespective of their intrinsic capacity and the helpfulness of each prototype\u2019s ensembles. Consequently, this leads to suboptimal knowledge transfer in device heterogeneous FL. See Section 6 for theoretical analysis and Section 7 for experimental observations. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "5 Task Arithmetic Knowledge Transfer and Integration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce TAKFL, designed to overcome the fundamental limitations of previous approaches and enhance knowledge transfer across diverse heterogeneous device prototypes, which vary in size\u2014in terms of both model and dataset size. TAKFL consists of two main components: (1) individually transferring knowledge from each device prototype\u2019s ensembles, and (2) adaptively integrating knowledge via task arithmetic. Detailed descriptions of each component are provided in Section 5.1 and 5.2, respectively. An illustrative overview of TAKFL is presented in Figure 1b, and the full algorithm is detailed in Appendix B, Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "5.1 Knowledge Transfer from Individual Device Prototype ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin by discussing our proposed knowledge transfer framework from each individual device prototype\u2019s ensembles. This process consists of two main components: ensemble knowledge transfer and self-regularization, each detailed in the subsequent paragraphs. ", "page_idx": 4}, {"type": "text", "text": "Ensemble Knowledge Transfer. Vanilla ensemble distillation integrates the knowledge of varying strength ensembles by uniformly averaging their logits. This approach can potentially transform or even degrade the overall quality of the knowledge being transferred, leading to suboptimal knowledge transfer. To effectively distill the unique knowledge and contributions of each prototype\u2019s ensembles, and to avoid dilution, information loss, and interference from other prototypes\u2019 ensembles, we propose transferring the knowledge from each prototype\u2019s ensembles separately and independently. ", "page_idx": 4}, {"type": "text", "text": "Specifically, let\u2019s consider $\\mathcal{T}_{i}\\,:=\\,\\{f^{i}(\\cdot,\\widehat{\\pmb{\\theta}}_{k}^{i})|\\,k\\,\\in\\,\\mathbb{C}^{i}\\}$ denotes the ensembles of device prototype $i$ as teacher and ${\\mathbf{}}S_{j}$ denotes the server s tudent model of the device prototype $j$ . Without loss of generality, we refer to each device prototype\u2019s server student model as just student denoted as $\\boldsymbol{S}$ . Therefore, the knowledge distillation loss between the teacher ensembles $\\mathcal{T}_{i}$ and server student $\\boldsymbol{S}$ ( $\\mathcal{T}_{i}\\to S)$ is defined below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\overset{\\mathrm{W:}}{\\mathcal{L}}_{K D}^{\\mathcal{T}_{i}\\rightarrow S}=\\mathrm{KL}\\Bigg[\\sigma\\Bigg(\\frac{1}{\\vert\\mathcal{T}_{i}\\vert}\\sum_{\\mathcal{F}\\in\\mathcal{T}_{i}}\\mathcal{F}(\\pmb{x})\\Bigg)\\;,\\;\\sigma\\left(\\mathcal{S}(\\pmb{x})\\right)\\Bigg].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Scaffolding Student from Noisy Ensemble Distillation. The ensemble distillation process may adversely impact the student, causing it to forget its own knowledge acquired through averaged locally updated parameters and be drifted into erroneous directions. This is primarily due to two key factors: (1) The ensemble distillation process introduces noise, mainly because the ensembles\u2019 logits are inferred on an unfamiliar public dataset they have not been trained on. These ensembles are originally trained on local private datasets, which usually differ from the unlabeled public dataset used for distillation. Moreover, other factors such as the presence of data heterogeneity within FL and insufficient training of some ensembles due to limited computational resources can exacerbate this noise, particularly in the initial rounds of federation. (2) The ensemble distillation process lacks supervision from the actual private datasets, which is the ultimate learning objective. ", "page_idx": 4}, {"type": "text", "text": "To scaffold the student models from the noisy and unsupervised distillation process, which may cause them to drift into erroneous directions and forget their invaluable self-knowledge, we introduce a KD-based self-regularization technique. Our self-regularization technique mitigates these issues by enforcing similarity between the logits of the student and its initial logits (when the student is initialized with averaged parameters) using KL divergence loss defined below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{S}^{\\mathrm{self}}=\\mathrm{KL}\\Bigg[\\overset{\\smile}{\\sigma}\\left(S(\\pmb{x};\\pmb{\\theta}_{a v g}\\right)\\right)\\;,\\;\\sigma\\left(S(\\pmb{x})\\right)\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Overall Knowledge Transfer Objective. The overall knowledge transfer objective from teacher ensembles $\\tau_{i}$ of device prototype $i$ to the student $\\boldsymbol{S}$ is the combination of the ensemble knowledge distillation loss $\\mathcal{L}_{K D}^{\\mathcal{T}_{i}\\rightarrow S}$ (Eq. 4) and the self-regula)rization loss $\\mathcal{L}_{S}^{\\mathrm{self}}$ (Eq. 5) defined in the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{S}^{T_{i}}=\\check{\\mathcal{L}}_{K D}^{T_{i}\\rightarrow S}+\\gamma\\cdot\\mathcal{L}_{S}^{\\mathrm{self}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\gamma$ is a hyperparameter controlling the effect of self-regularization term. We associate the knowledge transfer from each device prototype $i$ to a task $T_{i}$ with the loss $\\mathcal{L}_{S}^{\\mathcal{T}_{i}}$ . ", "page_idx": 4}, {"type": "text", "text": "5.2 Task Arithmetic Knowledge Integration ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Herein, we delve into the details of our proposed method for customized integration of the separately distilled knowledge from heterogeneous ensembles. Drawing inspiration from recent advances in model editing via task arithmetic [53], where a pre-trained model\u2019s knowledge can be edited via taskspecific vectors using arithmetic operation, we propose a novel customizable knowledge integration method via task arithmetic. To do so we extend the notion of task vector from centralized learning to federated learning. We conceptualize the averaged locally updated parameters, i.e. $\\theta_{a v g}$ , as a \u201cpretrained\u201d, similar to those in centralized learning, and the parameters of the distilled model via knowledge transfer objective (Eq. 4), denoted as $\\pmb{\\theta}_{d i s t i l l e d}$ , as a \u201cfine-tuned\u201d version of the model (see Fig. 2). Consequently, the task vector $\\tau_{i}$ associated with the knowledge transfer task $\\mathcal{L}_{S}^{\\mathcal{T}_{i}}$ can be defined by subtracting the distilled parameters from the averaged locally updated parameters as follows: ", "page_idx": 5}, {"type": "image", "img_path": "y6JotynERr/tmp/dfefd3f2a308343281edb67b84baffbd9a239baa48e8c722be2ce19bffeabd93.jpg", "img_caption": ["Figure 2: Analogy between task vector in centralized learning and federated learning. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\tau}_{i}=\\pmb{\\theta}_{d i s t i l l e d}^{T_{i}\\rightarrow S}-\\pmb{\\theta}_{a v g}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Essentially, task vectors serve as unique representations for the transferred knowledge from each prototype\u2019s ensembles to the student and encapsulate the distinct contributions of each prototype\u2019s ensembles to the student model. To selectively merge the knowledge of each prototype\u2019 ensembles into the student, we employ an adaptive task arithmetic operation as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{m e r g e d}=\\pmb{\\theta}_{a v g}+\\sum_{i\\in\\mathbb{M}}\\lambda_{i}\\pmb{\\tau}_{i},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda_{i}$ denotes the merging coefficient associated with task vector $\\tau_{i}$ , and they sum to one, i.e. $\\textstyle\\sum_{i\\in\\mathbb{M}}\\lambda_{i}=1$ . The merging coefficients determine the extent of knowledge integration from each prototype\u2019s ensembles. Essentially, they enable the student to have customized knowledge integration to achieve maximum performance. The student can determine these merging coefficients based on its own learning capacity and the relative knowledge and helpfulness of other device prototypes\u2019 ensembles. This approach provides an effective, low-cost, and scalable knowledge integration strategy in settings with diverse device heterogeneity. In our experiments, we considered this as a hyperparameter and tuned it manually or determined it using held-out validation sets which achieves similar results. More details can be found in Appendix F.3. ", "page_idx": 5}, {"type": "text", "text": "6 Theoretical Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We present a theoretical understanding on the efficacy of knowledge distillation in device heterogeneous FL. We argue that vanilla ensemble distillation (VED) diffuses the information from logits, which presents a notable disadvantage for solving (1). This effect is particularly pronounced when the teacher ensembles are from a device prototype of small capacity, and the student model is from a device prototype of large capacity. By contrast, our proposed method of task arithmetic knowledge integration, mitigates the drawbacks of VED and is able to simultaneously incorporate information from differently sized heterogeneous ensembles, efficiently filling up the capacity of each student with the most informative knowledge, achieving optimal knowledge transfer. ", "page_idx": 5}, {"type": "text", "text": "Assumptions and Preliminaries. Standard practice, including the setting in consideration as well as the numerical experiments here, involves overparametrized neural networks, that is, the total number of weights far exceeds the training sample size. This implies that the set of weights that minimize the loss is non-unique, and moreover, it has been argued that they form a submanifold [8]. This submanifold structure of solution sets will provide the critical source of understanding the subsequent results. In particular, we shall consider knowledge distillation as filling up the capacity of device prototypes\u2019 models with basis vectors corresponding to submanifolds that minimize as many device prototypes\u2019 data distributions as possible. ", "page_idx": 5}, {"type": "text", "text": "Since we are interested in server-side distillation across heterogeneous device prototypes, we assume optimal conditions at the local per-prototype FL level, meaning that the perfect solution for local per-prototype FL is achieved. The formal details of the assumptions and statements are presented in Appendix C. In these Propositions we assume that the prototype $i$ and $j$ datasets are disjoint, i.e. ", "page_idx": 5}, {"type": "text", "text": "$D^{i}\\cap D^{j}=\\emptyset$ , and $\\textstyle\\bigcup_{i=1}^{M}D^{i}=D$ . We show that the other cases are trivial and uninteresting in the appendix. ", "page_idx": 6}, {"type": "text", "text": "Proposition 1. (information loss in VED, informal). Consider the VED procedure in the form of solving (3). Consider two device prototypes with a device capacity and solution dimension of $\\mathcal{Q}^{1},Q^{2}$ and $\\bar{W}^{1},W^{2}$ , respectively, and with associated eigenbases $\\mathcal{Q}^{i},\\dot{\\mathcal{W}}^{i}$ . Denote $W^{i,j},i,j=\\bar{1,2}$ as the capacity allocated by student i in order to distill knowledge from teacher $j$ \u2019s logits. ", "page_idx": 6}, {"type": "text", "text": "1. Case 1: When the capacities are the same, that is $Q^{1}=Q^{2}$ and $W^{1}=W^{2}=W^{1,2}=W^{2,1},$ , then with VED, there will be some capacity, in the sense of eigenspace, of student prototypes that will be allocated with parameters that do not minimize the student\u2019s its own data distribution.   \n2. Case 2: Assume that $\\mathrm{\\Delta}Q^{1}\\,>\\,Q^{2}$ and $W^{1}\\,=\\,W^{1,2}\\,>\\,W^{2}$ . Then the phenomenon as for Case 1 holds. Moreover, there will be some capacity of student 1\u2019s model that will be allocated with parameters that do not minimize either of the teacher or student prototype\u2019s data distribution. ", "page_idx": 6}, {"type": "text", "text": "An interesting key mechanism of the proof is that when VED is applied in distilling logits from a small device prototype to a large one, the modeling capacity of $W^{\\dag,\\dag}$ is structurally reduced to that of $W^{2}<W^{\\boldsymbol{1},2}$ , i.e., it is an operation wasteful of the potential model capacity. ", "page_idx": 6}, {"type": "text", "text": "Remark 1. This proposition proves that in general, VED is prone to diffuse knowledge already present in students, and leads to inefficient and inaccurate use of model capacity. Furthermore, under the case that device prototypes have different capacities, VED ends up leading to more erronous models entirely as the small information within the small teacher is transferred onto a larger capacity target. ", "page_idx": 6}, {"type": "text", "text": "Proposition 2. (improve knowledge transfer with task arithmetic, informal). Consider the TAKFL procedure as in the form of computing (8). Consider two device prototypes with a device capacity and solution dimension of $Q^{\\bar{1}},Q^{2}$ and $W^{1},W^{2}$ , respectively, and with associated eigenbases $\\dot{\\mathcal{Q}}^{i},\\mathcal{W}^{i}$ . ", "page_idx": 6}, {"type": "text", "text": "1. Case 1: In the case that that $Q^{1}\\geq Q^{2}$ and $W^{1}\\geq W^{2}$ , it holds that the TAKFL with prototype 1 as student preserves the eigenbasis associated to the parameters used to accurately fit the data $\\mathcal{D}^{1}$ .   \n2. Case 2: Assume that $Q^{1}=Q^{2}$ and $W^{1}=W^{2}$ . TAKFL yields a solution for the student that is at the intersection of the subspaces corresponding to minimizing the two data distributions.   \n3. Case 3: Assume that $Q^{1}>\\dot{Q}^{2}$ and $W^{1}\\ >\\ W^{2}$ . In the case of prototype 1 being the student, TAKFL yields a solution that: (a) retains the approximation accuracy on device 1\u2019s data distribution, (b) ensures approximation accuracy to the level of device $2\\,\\dot{s}$ relative capacity (c) fills the remaining local capacity device 1 has allocated for device 2\u2019s logits with no informative new knowledge, unless enforced otherwise. ", "page_idx": 6}, {"type": "text", "text": "Remark 2. This proposition proves that in general, TAKFL promotes the most efficient allocation of the devices\u2019 capacity in order to accurately fit a diverse set of data distributions. With TAKFL, the previously acquired knowledge is entirely preserved. Even under the case that device prototypes have different capacities, TAKFL smartly transfers the most informative knowledge to each prototype\u2019s student model based on its own intrinsic capacity. Still, the final statement indicates that in the case that there are many different teachers, while a small device prototype serving as teacher will not be necessarily compromise information, it would still be preferable to allocate that capacity to a more informative, larger, teacher model. ", "page_idx": 6}, {"type": "text", "text": "Comments. We comment on the complexity and convergence aspects of TAKFL briefly: ", "page_idx": 6}, {"type": "text", "text": "1. Computation Time: TAKFL\u2019s computation time is $O(1)$ (constant) due to parallelization, as all distillation processes occur simultaneously.   \n2. Computation Load: The overall computational load scales as $O(M)$ (linear) since the distillation tasks are performed independently for each prototype in parallel and merged into a singe task arithmetic operation (Eq. 8).   \n3. Resource Usage (Memory): The resource usage scales as $O(M^{2})$ (quadratic) because of the need to store and process multiple task vectors concurrently.   \n4. Optimization Convergence Rate: The convergence rate of the Knowledge Distillation procedure is as standard for training a nonconvex loss function, e.g. [42]. ", "page_idx": 6}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "7.1 Main Experimental Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Dataset and Architecture. We evaluate our method on computer vision (CV) and natural language processing (NLP) tasks. For CV, we train image classification using CIFAR10/100 [24], CINIC10 [9], and TinyImagenet [25]. For NLP, we fine-tune pre-trained models for text classification on MNLI [52], SST-2 [45], MARC [22], and AG News [60]. Our architectures include ResNet [17], VGG [44], and ViT [12] for CV, and small BERT variants [47] (-Tiny, -Mini, -Small) for NLP. We simulate a federated non-i.i.d setting using a Dirichlet distribution $D i r(\\alpha)$ , where a lower $\\alpha$ indicates higher heterogeneity [27, 36]. Further details can be found in Appendix F.1 and F.2. ", "page_idx": 7}, {"type": "text", "text": "Implementation Details. We use the Adam optimizer for both CV and NLP tasks. For CV, local training involves 20 epochs with a learning rate of 0.001, weight decay of 5e-5, and a batch size of 64. NLP training is conducted over 1 epoch with a learning rate of 3e-5, no weight decay, and a batch size of 32. For distillation, Adam is used with a learning rate of 1e-5 and weight decay of 5e-4 for CV, and 3e-5 with no weight decay for NLP. Batch sizes for distillation are 128 for CV and 32 for NLP. The softmax temperature is set at 3 for both tasks, with a temperature of 20 for self-regularization. Further details are provided in Appendix F.1 and F.2. ", "page_idx": 7}, {"type": "text", "text": "Baselines and Evaluation Metric. We compare our method against standard FL, i.e. FedAvg [33] and SOTA KD-based methods designed for heterogeneous device prototypes FL, including FedDF [30] and FedET [6]. The evaluation metric is the final top-1 classification accuracy of each device prototype\u2019s global model on the test dataset, as per the methodology described in [36]. We report the average results and the standard deviation over three independent runs, each with a different random seed. ", "page_idx": 7}, {"type": "text", "text": "A more detailed version of the experiments, alongside additional experiments and ablation studies, is presented in Appendix $D$ and $E$ . ", "page_idx": 7}, {"type": "text", "text": "7.2 Main Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the performance of our method, TAKFL, in a federated learning environment that mirrors real-world scenarios with diverse, heterogeneous device prototypes, as illustrated in Fig. 4b. Our experimental setup includes three different device prototype sizes: Small (S) with a small model and small dataset, Medium (M) with a medium-sized model and medium-sized dataset, and large (L) with a large model and large dataset. ", "page_idx": 7}, {"type": "text", "text": "Performance on CV Task. Table 1 presents the performance of TAKFL in the homo-family architecture setting on the CIFAR-10 and CIFAR-100 [24] datasets (for hetero-family architecture results, see Appendix D.1, Table 4). TAKFL consistently enhances performance across all device prototypes in various scenarios, achieving SOTA results. Notably, in the Dir(0.3) setting on CIFAR10, TAKFL improves average performance across all prototypes by $8\\%$ , and by $4\\%$ on CIFAR-100. From Table 1, inconsistent performance improvements are observed with prior KD-based methods, especially for the L prototype. While S and M prototypes achieve gains, the L prototype suffers up to a $10\\%$ degradation compared to vanilla FedAvg, highlighting the dilution issue where valuable information from larger, more capable device prototypes is diluted by less informative outputs from smaller devices. Moreover, the significant performance improvements TAKFL achieves for each device prototype, particularly for S and M prototypes, illustrate the ineffectiveness of the one-sizefits-all approach used in the existing KD methods. These observations confirm the shortcomings of vanilla ensemble distillation and corroborate our theoretical findings in Remark 1 and 2. The effectiveness of our self-regularization technique is further supported by these experimental results. For more detailed and insightful analysis see Appendix D.1.1. ", "page_idx": 7}, {"type": "text", "text": "Performance on NLP Task. Table 2 presents the results on MNLI [52] and SST-2 [45] datasets (see Appendix D.3 for further experiments). Similar to the CV task, TAKFL has consistently improved performance across all device prototypes of varying sizes, achieving SOTA results: a $3\\%$ average increase on MNLI and $2\\%$ on SST-2. The suboptimality of existing KD methods, is evident from the results presented here as well. Notably, FedET suffers from a significant performance degradation compared to vanilla FedAvg. This issue stems from FedET\u2019s reliance on the confidence scores of neural networks for uncertainty estimates. However, neural networks, especially pretrained language models (PLMs), tend to be poorly calibrated and overconfident, undermining reliable uncertainty estimates [50, 15, 5, 55]. ", "page_idx": 7}, {"type": "table", "img_path": "y6JotynERr/tmp/4040cd6d8a8fad7426576d6630aad7935e9da92dc21e6024046dea7657d6f1a5.jpg", "table_caption": ["Table 1: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 [24] for CIFAR-10 [24] and ImageNet-100 [10] for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. Architectures are ResNet-8, ResNet-14, and ResNet-18 [17] for S, M and L, respectively. All models are trained from scratch for 60 rounds. See Appendix D.1 for additional experiments using hetero-family architecture and more details. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "y6JotynERr/tmp/ec7883adff034b7e21bd3c0be1283dc711aa77933fccf279ea0e61e98c4dd169.jpg", "table_caption": ["Table 2: Performance Results for NLP Task on MNLI and SST-2. Training data distribution is similar to the CV task using only $\\operatorname{Dir}(0.5)$ here. Public datasets are SNLI [2] for MNLI [52] and Sentiment140 [14] for SST-2 [45]. Client configurations are 8, 4, and 2 clients for S, M, and L, with sample rates of 0.3, 0.5, and 1.0, respectively. Architectures include Bert-Tiny, Bert-Mini, and Bert-Small [47] for S, M, and L, initialized from pre-trained parameters and fine-tuned for 20 communication rounds. See Appendix F.2 for more details. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "7.3 Scalability Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluate the scalability of TAKFL across a spectrum of device prototypes, from extremely small (XXS) to extremely large (XXL), to see how well our method adapts from a uniform array of smallsize prototypes to a diverse mix of sizes. Each prototype is equipped with appropriately scaled model and dataset sizes, simulating real-world variations in device capabilities. ", "page_idx": 8}, {"type": "text", "text": "Figure 3 illustrates TAKFL\u2019s ability to effectively scale from 3 to 7 device prototypes. In scenarios where all devices are similarly small, i.e. 3-device setup, TAKFL\u2019s performance is slightly better than FedDF. This is because when devices are homogeneously small and similar in capability, they do not offer unique contributions that could benefit from more complex distillation strategies. However, as the scenario expands to include larger devices like XL and XXL in the 5- and 7-device configurations, TAKFL significantly outperforms existing KD-based methods. This improvement is driven by the larger devices\u2019 ability to offer more significant and higher-quality knowledge, which TAKFL effectively distills across all prototypes, contrasting sharply with existing methods that fail to utilize this potential. These experimental observations, corroborated by our theoretical insights in Remark 2, demonstrate TAKFL\u2019s superior scalability and effectiveness. ", "page_idx": 8}, {"type": "image", "img_path": "y6JotynERr/tmp/9f13588c1c71b22d4d148bffb7d99598a0dee35ac093199647e8e1d39f13c68b.jpg", "img_caption": ["Figure 3: Scalability Evaluation of TAKFL. Image classification on CINIC-10 [9] dataset is used to evaluate TAKFL\u2019s scalability across device prototypes ranging from XXS to XXL. Training data is distributed among prototypes in a 1:2:3:4:5:6:7 ratio, further subdivided using $\\operatorname{Dir}(0.5)$ . Client configurations range from 35 for XXS to 5 for XXL. Architectures span from ResNet10-XXS for XXS to ResNet50 for XXL prototype, all initialized from scratch and trained over 30 communication rounds. The public dataset is CIFAR-100 [24]. See Appendix D.4 for more details. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "8 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we addressed a fundamental issue in standard federated learning: the lack of support for heterogeneous device prototypes. Existing KD-based methods often fall short in real-world scenarios, where device capabilities vary widely. To address this, we introduced TAKFL, a novel KD-based method that treats knowledge transfer from each prototype\u2019s ensembles as separate tasks and distills them independently. TAKFL susequently integrates the knowledge using an adaptive task arithmetic technique for optimized performance. We also introduced a KD-based self-regulation technique to mitigate issues arising from noisy and unsupervised ensemble distillation. The effectiveness of our method is substantiated by both theoretical results and extensive experimentation across CV and NLP tasks, using various datasets and models. ", "page_idx": 9}, {"type": "text", "text": "Limitations remain, notably in real-world applicability. While TAKFL\u2019s effectiveness in an approximated real-world setup has been demonstrated, actual deployment on physical devices and in environments with extremely large models remains untested due to resource constraints. Experiencing TAKFL in genuine real-world settings could unveil additional challenges or limitations, providing further insights into its scalability and efficiency. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by a research grant from Cisco Systems, Inc., Project Number 49790. V.K. acknowledges support from the Czech National Science Foundation under Project 24- 11664S. We also gratefully acknowledge the use of the computational infrastructure provided by the OP VVV funded project CZ.02.1.01/0.0/0.0/16 019/0000765, \u201cResearch Center for Informatics,\u201d which enabled us to conduct the experiments presented in this work. ", "page_idx": 9}, {"type": "text", "text": "We would like to express our sincere gratitude to Ang Li, Matias Mendieta, and Guangyu Sun for their invaluable feedback and insightful discussions, which significantly contributed to the development and refinement of this work. Their thoughtful suggestions and careful review of earlier drafts were instrumental in enhancing the quality of this paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Samiul Alam, Luyang Liu, Ming Yan, and Mi Zhang. Fedrolex: Model-heterogeneous federated learning with rolling sub-model extraction. Advances in Neural Information Processing Systems, 35:29677\u201329690, 2022. 3, 1 [2] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference, 2015. 9, 12, 20 [3] Sebastian Caldas, Jakub Konec\u02c7ny, H Brendan McMahan, and Ameet Talwalkar. Expanding the reach of federated learning by reducing client resource requirements. arXiv preprint arXiv:1812.07210, 2018. 3, 1   \n[4] Chuan Chen, Tianchi Liao, Xiaojun Deng, Zihou Wu, Sheng Huang, and Zibin Zheng. Advances in robust federated learning: Heterogeneity considerations. arXiv preprint arXiv:2405.09839, 2024. 2   \n[5] Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. A close look into the calibration of pre-trained language models. arXiv preprint arXiv:2211.00151, 2022. 9, 2, 13   \n[6] Yae Jee Cho, Andre Manoel, Gauri Joshi, Robert Sim, and Dimitrios Dimitriadis. Heterogeneous ensemble knowledge transfer for training large models in federated learning. arXiv preprint arXiv:2204.12703, 2022. 3, 8, 1, 2, 19 [7] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 215\u2013223, Fort Lauderdale, FL, USA, 2011. PMLR. 12, 18   \n[8] Yaim Cooper. Global minima of overparameterized neural networks. SIAM Journal on Mathematics of Data Science, 3(2):676\u2013691, 2021. 6   \n[9] Luke N. Darlow, Elliot J. Crowley, Antreas Antoniou, and Amos J. Storkey. Cinic-10 is not imagenet or cifar-10, 2018. 8, 9, 12, 15, 18   \n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009. 9, 10, 18   \n[11] Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient federated learning for heterogeneous clients. arXiv preprint arXiv:2010.01264, 2020. 3, 1   \n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. 8, 10, 19   \n[13] Cong Fang, Hanze Dong, and Tong Zhang. Mathematical models of overparameterized neural networks. Proceedings of the IEEE, 109(5):683\u2013703, 2021. 9   \n[14] Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision. CS224N project report, Stanford, 1(12):2009, 2009. 9, 12, 20   \n[15] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017. 9, 2, 13, 16   \n[16] Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated learning. arXiv preprint arXiv:1910.14425, 2019. 4   \n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015. 8, 9, 10   \n[18] Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos Venieris, and Nicholas Lane. Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout. Advances in Neural Information Processing Systems, 34:12876\u201312889, 2021. 3, 1   \n[19] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications, 2017. 12, 19   \n[20] Anil Kag, Durmus Alp Emre Acar, Aditya Gangrade, and Venkatesh Saligrama. Scaffolding a student to instill knowledge. In The Eleventh International Conference on Learning Representations, 2022. 19   \n[21] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International conference on machine learning, pages 5132\u20135143. PMLR, 2020. 1   \n[22] Phillip Keung, Yichao Lu, Gyo\u00a8rgy Szarvas, and Noah A. Smith. The multilingual amazon reviews corpus. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, 2020. 8, 12, 20   \n[23] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017. 12   \n[24] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. 8, 9, 10, 12, 15, 18   \n[25] Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. 2015. 8, 12, 18   \n[26] Lin Li, Jianping Gou, Baosheng Yu, Lan Du, and Zhang Yiand Dacheng Tao. Federated distillation: A survey. arXiv preprint arXiv:2404.08564, 2024. 2   \n[27] Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10713\u201310722, 2021. 1, 8   \n[28] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429\u2013450, 2020. 1   \n[29] Zhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after sgd reaches zero loss?\u2013a mathematical framework. In International Conference on Learning Representations, 2021. 9   \n[30] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. Advances in Neural Information Processing Systems, 33:2351\u20132363, 2020. 3, 4, 8, 1, 2   \n[31] Chang Liu, Chenfei Lou, Runzhong Wang, Alan Yuhan Xi, Li Shen, and Junchi Yan. Deep neural network fusion via graph matching with applications to model ensemble and federated learning. In International Conference on Machine Learning, pages 13857\u201313869. PMLR, 2022. 1   \n[32] Tian Yu Liu and Stefano Soatto. Tangent model composition for ensembling and continual fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 18676\u201318686, 2023. 3   \n[33] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017. 1, 8   \n[34] Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer, 2022. 12, 19   \n[35] Alessio Mora, Irene Tenison, Paolo Bellavista, and Irina Rish. Knowledge distillation for federated learning: a practical guide. arXiv preprint arXiv:2211.04742, 2022. 2   \n[36] Mahdi Morafah, Weijia Wang, and Bill Lin. A practical recipe for federated learning under statistical heterogeneity experimental design. IEEE Transactions on Artificial Intelligence, 2023. 8, 18   \n[37] Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task arithmetic in the tangent space: Improved editing of pre-trained models. Advances in Neural Information Processing Systems, 36, 2024. 3, 4, 5   \n[38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Ko\u00a8pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. 18   \n[39] Kilian Pfeiffer, Martin Rapp, Ramin Khalili, and Jo\u00a8rg Henkel. Federated learning for computationally constrained heterogeneous devices: A survey. ACM Computing Surveys, 55(14s):1\u201327, 2023. 2   \n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021. 15   \n[41] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konec\u02c7n\\`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020. 16, 17   \n[42] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018. 7   \n[43] Felix Sattler, Tim Korjakow, Roman Rischke, and Wojciech Samek. Fedaux: Leveraging unlabeled auxiliary data in federated learning. IEEE Transactions on Neural Networks and Learning Systems, 2021. 3, 1   \n[44] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition, 2015. 8, 10, 18   \n[45] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u2013 1642, Seattle, Washington, USA, 2013. Association for Computational Linguistics. 8, 9, 12, 20   \n[46] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15 (1):1929\u20131958, 2014. 1   \n[47] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962v2, 2019. 8, 9, 12, 20   \n[48] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020. 1   \n[49] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in neural information processing systems, 33:7611\u20137623, 2020. 1   \n[50] Yuxia Wang, Daniel Beck, Timothy Baldwin, and Karin Verspoor. Uncertainty estimation and reduction of pre-trained models for text regression. Transactions of the Association for Computational Linguistics, 10:680\u2013696, 2022. 9, 2, 13   \n[51] Ross Wightman, Hugo Touvron, and Herve Jegou. Resnet strikes back: An improved training procedure in timm. In NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future. 19   \n[52] Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference, 2018. 8, 9, 12, 20   \n[53] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, pages 23965\u201323998. PMLR, 2022. 3, 6, 4   \n[54] Yebo Wu, Li Li, Chunlin Tian, and Chengzhong Xu. Breaking the memory wall for heterogeneous federated learning with progressive training. arXiv preprint arXiv:2404.13349, 2024. 1   \n[55] Yuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, and Louis-Philippe Morency. Uncertainty quantification with pre-trained language models: A large-scale empirical analysis. arXiv preprint arXiv:2210.04714, 2022. 9, 2, 13, 16   \n[56] Ziyue Xu, Mingfeng Xu, Tianchi Liao, Zibin Zheng, and Chuan Chen. Fedbrb: An effective solution to the small-to-large scenario in device-heterogeneity federated learning. arXiv preprint arXiv:2402.17202, 2024. 1   \n[57] Prateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36, 2024. 3   \n[58] Tuo Zhang, Lei Gao, Sunwoo Lee, Mi Zhang, and Salman Avestimehr. Timelyfl: Heterogeneity-aware asynchronous federated learning with adaptive partial training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5064\u20135073, 2023. 1   \n[59] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2015. 12, 20   \n[60] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015. 8, 12, 20   \n[61] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification, 2016. 12, 20   \n[62] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43\u201376, 2020. 3 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The supplementary materials are organized as follows: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Appendix A: Provides more details on related works.   \n\u2022 Appendix B: Presents the full algorithm description of TAKFL.   \n\u2022 Appendix C: Presents formal theoretical statements, assumptions, and proofs supporting our method.   \n\u2022 Appendix D: Presents detailed experimental results including some additional experiments.   \n\u2022 Appendix E: Presents the ablation studies experiments.   \n\u2022 Appendix F: Presents hyper-parameters and implementation details. ", "page_idx": 14}, {"type": "text", "text": "A More Detailed Related Works ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "y6JotynERr/tmp/e6a59157843ee0032e2686592c3525549eb6dba782903bd1e8598eb66374545c.jpg", "img_caption": ["(a) Partial Model Training "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "y6JotynERr/tmp/667c9148b1a6af73e60a39afe28fc48fe817dda056a6559f5abefb0bd393df5d.jpg", "img_caption": ["(b) Heterogeneous Device Prototypes "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 4: Overview of Two Different Device Heterogeneous FL Settings. (a) In the partial model training setting, the objective is to train a single global model where heterogeneous devices train a specific sub-model based on their computational resources. This approach necessitates device support for varying neural network architectures, which is impractical as devices typically have specialized architectures designed to match their hardware, software configurations, and underlying machine learning tasks. (b) In the heterogeneous device prototypes setting, device prototypes participate in FL to enhance the performance of their global model by transferring knowledge across prototypes. This setting is more feasible as it accommodates diverse device prototypes with their own specific configurations, including neural network architecture and dataset. However, establishing effective knowledge transfer between differently sized prototypes (like IoTs and workstations) and diverse configurations is challenging. In this paper, we address this issue. ", "page_idx": 14}, {"type": "text", "text": "Prior works on device heterogeneous FL have considered two distinct approaches with different objectives and settings. The first group of studies focuses on accommodating devices with varying compute resources, aiming to train a single global model [11, 3, 58, 54, 56]. Various partial model training techniques have been proposed for this setting, where devices are tasked with training a submodel of a global model according to their compute resources. These include dropout-based [3], static [11, 18], and rolling-based sub-model extraction techniques [1]. Federated Dropout builds upon the concept of dropout [46] to extract smaller sub-models. Static sub-model extraction techniques like in HeteroFL [11] and FjORD [18] consistently extract designated portions of the global model, whereas FedRolex [1] introduces a more flexible rolling method for sub-model extraction. However, these approaches assume that devices can support various sub-model architectures for training, which does not fully reflect the real-world scenario. In practice, there exist a diverse spectrum of device prototypes such as IoT devices and smartphones each have unique and unhashable neural network architectures tailored to their specific hardware and software configurations and underlying machine learning tasks. Consequently, these device prototypes may not support training various neural network architectures, highlighting a significant limitation in accommodating the full spectrum of device heterogeneity in this setting. ", "page_idx": 14}, {"type": "text", "text": "The second array of studies tackles a more practical scenario where device prototypes with heterogeneous model architectures participate in FL to enhance their global model performance through mutual knowledge sharing. In this context, knowledge distillation techniques are employed to transfer knowledge among device prototypes [30, 6, 43]. Here, locally updated client models from various device prototypes, collectively referred to as ensembles, serve as teachers to distill their knowledge into each server\u2019s student model using an unlabeled public dataset. For instance, FedDF [30] utilizes vanilla averaging of all ensemble logits as the distillation target for all server student models. In contrast, FedET [6] employs an uncertainty-weighted average of ensembles\u2019 logits as the distillation target for all server student models, complemented by a diversity regularization technique. However, methods like FedET rely on the neural networks\u2019 confidence scores for uncertainty estimates, overlooking the fact that neural networks are often poorly calibrated and prone to overconfidence, which compromises their ability to provide reliable uncertainty estimates [50, 15, 5, 55]. These existing works typically focus on settings where device prototypes have similar capabilities, i.e. similar model and dataset sizes, thus neglecting the challenges presented in more diverse settings where device prototypes vary significantly in terms of model and dataset size. This oversight limits the effectiveness of these methods in truly diverse and heterogeneous environments. In this paper, we introduce TAKFL, which is designed to address the limitations of existing methods in these underexplored diverse device heterogeneous settings. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Figure 1 illustrates the distinctions between these two different settings studied in the literature. For more information, we refer the reader to recent surveys [35, 26, 39, 4]. ", "page_idx": 15}, {"type": "text", "text": "B Full Algorithm Description of TAKFL ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The full algorithm description of TAKFL is presented in Algorithm 1. ", "page_idx": 16}, {"type": "table", "img_path": "y6JotynERr/tmp/794d4ca7624b628ad8f2eb269c66c4c392b5ef990ee9166b0d6cf3153ff85116.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Theoretical Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Proofs of the Main Propositions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "First we present the formal assumptions associated with our theoretical derivations. Assumption 1. Local federated averaging is performed with perfect test accuracy, i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{\\pmb{\\theta}^{j}}\\ \\sum_{j=1}^{M}\\sum_{k=1}^{N^{j}}\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathbb{D}_{k}^{j}}\\left[\\ell(f^{j}(\\pmb{x};\\pmb{\\theta}^{j}),\\pmb{y})\\right]=\\operatorname*{argmin}_{\\pmb{\\theta}^{j}}\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathcal{D}^{j}}\\left[\\ell(f^{j}(\\pmb{x};\\pmb{\\theta}),\\pmb{y})\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "That is, the training error on the datasets $\\{\\mathbb{D}_{k}^{j}\\}$ for the computed $\\theta_{a v g}^{j}$ is the same as the test error on the population distribution $\\mathcal{D}^{j}$ . Moreover assume that we can write $\\begin{array}{r}{\\check{T}_{i}=\\left\\{\\underset{k=1}{\\overset{N^{i}}{\\sum}}f^{i}(\\cdot,\\hat{\\pmb{\\theta}}_{k}^{i})|k\\in\\mathbb{C}^{i}\\right\\}=}\\end{array}$ $\\{f^{i}(\\cdot,\\theta_{a v g}^{i})\\}$ . Finally, we assume that the same population distribution $\\sum_{j}\\omega_{j}\\mathcal{D}^{j}$ is the same that the clients perform their testing on as the server performs distillation on. ", "page_idx": 17}, {"type": "text", "text": "These assumptions are made for mathematical practicality while at the same time not starkly unreasonable. The local FL the device prototypes perform is generically prone to imprecision, especially as the clients\u2019 data varies, but this discrepancy is bounded [16]. Similarly the difference in the average of logits and the logit of averages has a bounded difference norm [53]. Thus, violations of the Assumption add additional perturbations to quantities derived in the Theoretical analysis without having structural/qualitative effects, and thus would only present clutter in the presentation. ", "page_idx": 17}, {"type": "text", "text": "Notations. Now we present the notation defining the specific quantities we refer to in the derivations below. The set of important quantities is given in Table 3. Note that the formal definitions of the first two quantities are, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Theta^{j}:=\\operatorname*{argmin}_{\\pmb{\\theta}}\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathcal{D}^{j}}\\left[\\ell\\big(f^{j}(\\pmb{x};\\pmb{\\theta}),\\pmb{y}\\big]\\,,\\,\\Theta^{j,k}:=\\operatorname*{argmin}_{\\pmb{\\theta}}\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathcal{D}^{i}}\\left[\\ell\\big(f^{j}(\\pmb{x};\\pmb{\\theta}),\\pmb{y}\\big]\\right]\\right]\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "table", "img_path": "y6JotynERr/tmp/d755f673281887dcb8b834ecaf8ee66cd894160f51e027bdf2dc68f51e87711b.jpg", "table_caption": ["Table 3: Notation and Definitions "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "We shall make use of the \u201cChoose\u201d combinatorial operator, defined to be $\\begin{array}{r}{C h(n,p)=\\frac{n!}{p!(n-p)!}}\\end{array}$ . The standard $O(\\cdot)$ notation indicates $a_{k}=O(b_{k})$ to mean there exists $K$ and $C$ such that for $k\\ge K$ , $a_{k}\\leq C b_{k}$ . ", "page_idx": 17}, {"type": "text", "text": "A recent finding that inspired the methodology in this work is the discovery of the weight disentanglement phenomenon underlying task arithmetic [37]. Indeed the task arithmetic property provides the ideal circumstance for federated knowledge transfer as we shall see below. Formally, adapting their definition to our notation: ", "page_idx": 17}, {"type": "text", "text": "(Task Arithmetic Property) holds for a set of vectors $\\{\\tau_{j}\\}$ if for all $j$ it holds that, ", "page_idx": 17}, {"type": "equation", "text": "$$\nf^{j}\\left(x;\\theta_{a v g}^{j}+\\sum_{i\\neq j}\\lambda_{i}\\tau_{i}\\right)=\\left\\{\\begin{array}{l l}{f^{j}(x;\\theta_{a v g}^{j}+\\lambda_{i}\\tau_{i})}&{x\\in\\mathcal{D}^{i}}\\\\ {f^{j}(x;\\theta_{a v g}^{j})}&{x\\in\\mathcal{D}^{j}\\setminus\\cup_{i\\neq j}\\mathcal{D}^{i}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let us define an important property of task arithmetic that we shall use in the sequel. ", "page_idx": 17}, {"type": "text", "text": "(Weight disentanglement).[37] A parametric function $f:\\mathcal{X}\\times\\Theta\\rightarrow\\mathcal{Y}$ is weight disentangled with respect to a set of task vectors $T=\\bar{\\{\\tau_{j}\\}}_{j\\in\\mathbf{T}}$ and the corresponding supports $\\bar{D}_{T}:=\\{\\mathcal{D}_{j}\\}_{j\\in\\mathbf{T}}^{\\mathsf{^{\\bullet}}}$ if ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\pmb{x};\\pmb{\\theta}_{0}+\\sum_{i\\in\\mathbf{T}}^{T}\\alpha_{i}\\pmb{\\tau}_{i})=\\sum_{i\\in\\mathbf{T}}g_{j}(\\pmb{x};\\alpha_{i}\\pmb{\\tau}_{i})+g_{0}(\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $g_{i}(x;\\alpha_{i}\\tau_{i})=0$ for $\\textbf{\\em x}\\notin{\\cal D}_{i}$ and $i\\in\\mathbf{T}$ , and $g_{0}(\\pmb{x})=0$ for $\\textstyle\\pmb{x}\\in\\bigcup_{i\\in\\mathbf{T}}\\mathcal{D}_{i}$ . ", "page_idx": 18}, {"type": "text", "text": "We now present the formal statements as well as the proofs of the main propositions. ", "page_idx": 18}, {"type": "text", "text": "Proposition 1. (Information Loss in VED). Let the prototype $i$ and $j$ datasets be disjoint, i.e. $D^{i}\\cap D^{j}=\\emptyset$ , and $\\textstyle\\bigcup_{i=1}^{M}D^{\\rangle}=D$ . Note that this implies the weight disentanglement property. Consider the VED procedure in the form of solving (3). Consider two device prototypes with a device capacity and solution dimension of $Q^{1},\\stackrel{\\cdot}{Q}^{2}$ and $\\bar{W}^{1},W^{2}$ , respectively, and with associated eigenbases $\\mathcal{Q}^{i},\\mathcal{W}^{i}$ . Let the solution set of VED with prototype $i$ as student be $\\hat{\\Theta}_{V E D}^{i}$ with $\\dim(\\hat{\\Theta}_{V E D}^{i})=W^{v_{i}}$ with eigenbasis $\\mathcal{W}^{v_{i}}$ . In addition, denote $W^{s,t}$ , $s,t\\in\\{1,2\\}$ the dimension of the solution set for the student model trained on the data from the teacher device\u2019s ensembles. We assume that selfdistillation is executed appropriately, e.g., $W^{1,1}=W^{1}$ and $W^{2,2}=W^{2}$ . ", "page_idx": 18}, {"type": "text", "text": "1. Case 1: Assume that $Q^{1}\\ =\\ Q^{2}$ and $W^{1}\\;=\\;W^{2}\\;=\\;W^{1,2}\\;=\\;W^{2,1}$ . Then it holds that, in expectation, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\dim\\left(\\hat{\\Theta}_{V E D}^{1}\\cap\\left[\\boldsymbol{Q}^{1}\\setminus\\mathcal{W}^{1}\\right]\\right)=O\\left(\\frac{(Q^{1}-W^{1})(W^{1})!(Q^{1}-W^{1,2})!}{Q^{1!}(W^{1})!(Q^{1}-W^{1})!+Q^{1}!W^{1,2}!(Q^{1}-W^{1,2})!}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This corresponds to the expected capacity of prototype $^{\\,l}$ that is taken up for fitting logits that are not in the span of $\\mathcal{W}^{1}$ , that is, that do not fit the data corresponding to prototype 1. ", "page_idx": 18}, {"type": "text", "text": "2. Case 2: Assume that $Q^{1}\\,>\\,Q^{2}$ and $W^{1}\\,=\\,W^{\\dot{1},2}\\,>\\,\\bar{W}^{2}$ . Then the same quantity as for Case $^{\\,l}$ holds. Moreover, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\dim\\left(\\hat{\\Theta}_{V E D}\\cap\\left[\\boldsymbol{Q}^{1}\\setminus(\\mathcal{W}^{1}\\cup\\mathcal{W}^{1,2}]\\right)=O\\left(\\frac{(Q^{1}-W^{1})(W^{1}!)(W^{1,2}-W^{2})!}{Q^{1}!W^{1}!(Q^{1}-W^{1})!+Q^{1}!W^{2}!(W^{1,2}-W^{2})!}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This corresponds to capacity of client 1 that has been allocated but fits, in the model of prototype $^{\\,l}$ , neither the data of prototype $^{\\,l}$ , nor of the data of prototype 2. ", "page_idx": 18}, {"type": "text", "text": "Proof. Formally, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\Theta}_{V E D}:=\\underset{\\theta\\in Q^{1}\\;}{\\mathrm{argmin}}\\;\\mathcal{L}_{\\mathrm{ED}}=\\underset{\\theta}{\\mathrm{argmin}}\\;\\mathrm{KL}\\Bigg[\\sum_{i=1,2}\\sigma\\bigg(f^{i}(\\pmb{x},\\pmb{\\theta}_{a v g}^{i})\\bigg)\\Bigg.,\\sigma\\left(S(\\pmb{x})\\right)\\Bigg]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since by assumption $\\pmb{\\theta}_{a v g}^{i}$ solves the training problem on the data associated with device prototype $i$ , the logit is accurate, and thus there is a map $\\mathcal{O}(i,j):T_{i}\\rightarrow T_{i}^{j}\\subseteq\\mathcal{W}^{i,j}$ . The self distillation, that is, ${\\mathcal{S}}_{j}$ defines a bijective map from $\\mathcal{W}^{j}$ to $\\mathcal{W}^{j}$ and thus does not affect the capacity allocation. ", "page_idx": 18}, {"type": "text", "text": "Case 1: In this case, generically (that is, measure zero on some non-atomic sampling on a distribution of operators) $\\mathcal{O}(i,j)$ is bijective. Now let us compute the expectation of the number of eigenvectors of, e.g. $\\mathcal{W}^{1,2}$ that are in the complement of the span of $\\bar{\\mathcal{W}}^{1}$ . Assuming, for simplicity, independence, this would correspond to counting the possible choices within the capacity of $\\dot{\\mathcal{Q}}^{1}\\setminus\\mathcal{W}^{1}$ over the range of possible choices of filling the capacity of $\\mathcal{Q}^{1}$ with vectors in $\\mathcal{W}^{\\mathrm{I}}$ together with choices of filling it with vectors in $\\mathcal{W}^{1,2}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{Q^{1}-W^{1}}i{\\frac{C h(Q^{1}-W^{1},i)}{C h(Q^{1},W^{1})+C h(Q^{1},W^{1,2})}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For, e.g., $Q^{1}=4$ and $W^{1}=W^{1,2}=2$ this is $\\begin{array}{l}{{\\frac{1}{3}}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "To derive a scaling rate we can write: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{i}i\\frac{\\frac{(Q^{1}-W^{1})!}{i!(Q^{1}-W^{1}-i)!}}{\\frac{Q^{1}!}{(W^{1})!(Q^{1}-W^{1})!}+\\frac{Q^{1}!}{W^{1},2!(Q^{1}-W^{1},2)!}}=O\\left(\\frac{(Q^{1}-W^{1})(W^{1})!(Q^{1}-W^{1,2})!}{Q^{1}!(W^{1})!(Q^{1}-W^{1})!+Q^{1}!W^{1,2}!(Q^{1}-W^{1,2})!}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Case 2: In this case, it must be that, at best almost surely, ${\\mathcal{O}}(2,1)$ is injective, but not surjective. This means that distilling from 2 to $1$ does not fill the capacity of $\\mathcal{W}^{1,2}$ , and is thus a fundamentally ", "page_idx": 18}, {"type": "text", "text": "wasteful operation, that is $|T_{i}^{j}|=W^{2}<W^{1,2}$ . Now let us compute the expectation of the number of eigenvectors of, e.g. $\\mathcal{W}^{1,2}$ that are in the complement of the span of $\\mathcal{W}^{\\boldsymbol{1}}$ . Since $\\mathcal{W}^{1,2}$ are being structurally allocated for fitting, the combinatorial expression is the same: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{Q^{1}-W^{1}}i{\\frac{C h(Q^{1}-W^{1},i)}{C h(Q^{1},W^{1})+C h(Q^{1},W^{1,2})}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus for, e.g., $Q^{1}=4$ and $W^{1}=W^{1,2}=2$ this is, again, $\\begin{array}{l}{{\\frac{1}{3}}}\\end{array}$ . The scaling in this case is ", "page_idx": 19}, {"type": "equation", "text": "$$\nO\\left(\\frac{(Q^{1}-W^{1})(W^{1}!)(Q^{1}-W^{1,2})!}{Q^{1}!W^{1}!(Q^{1}-W^{1})!+Q^{1}!W^{2}!(Q^{1}-W^{2})!}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "However, we observe that there are vectors in the range of $\\mathcal{W}^{1,2}\\setminus\\mathcal{O}(2,1)(\\mathcal{W}^{2})$ that have been allocated by the VED but lie in neither $\\mathcal{W}^{1}$ nor in $\\mathcal{W}^{1,2}$ , that is, are garbage. We can compute those as the expected number of eigenvectors arising from allocating $\\mathcal{W}^{\\bar{1},2}\\setminus\\bar{\\mathcal{O}}(2,1)(\\mathcal{W}^{2})$ that intersect with $\\mathcal{Q}^{1}\\setminus\\mathcal{W}^{1}$ (that is, the spare capacity not used for fitting data $\\dot{\\mathcal{D}}^{1}$ ). This is, using similar principles: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{W^{1,2}-W^{1}}i\\frac{C h(Q^{1}-W^{1},i)}{C h(Q^{1},W^{1})+C h(Q^{1},W^{1,2}-W^{2})}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This is for, e.g., $Q^{1}=4$ , $W^{1,2}=2$ and $W^{2}=1$ , this would be $\\frac{3}{10}$ ", "page_idx": 19}, {"type": "text", "text": "The scaling here is ", "page_idx": 19}, {"type": "equation", "text": "$$\nO\\left({\\frac{(Q^{1}-W^{1})(W^{1}!)(W^{1,2}-W^{2})!}{Q^{1}!W^{1}!(Q^{1}-W^{1})!+Q^{1}!W^{2}!(W^{1,2}-W^{2})!}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proposition 2. (Improve knowledge transfer with task arithmetic). Consider the TAKFL procedure as in the form of computing (8). Consider two device prototypes with a device capacity and solution dimension of $\\dot{Q}^{1},\\dot{Q}^{2}$ and $\\bar{W}^{1},W^{2}$ , respectively, and with associated eigenbases $\\mathcal{Q}^{i},\\mathcal{W}^{i}$ . Let the solution set of TAKFL with prototype $i$ as student be $\\hat{\\Theta}_{T A}^{i}$ with $\\dim(\\hat{\\Theta}_{T A}^{i})=W^{v}$ with eigenbasis $\\mathcal{W}^{v}$ . In addition, denote $W^{s,t}$ , $s,t\\,\\in\\,\\{1,2\\}$ dimension of the solution set for the student model trained on the data from the teacher device\u2019s ensembles. . The following statements hold: ", "page_idx": 19}, {"type": "text", "text": "In the case that that $Q^{1}\\ \\geq\\ Q^{2}$ and $W^{1}~\\geq~W^{2}$ , it holds that the TAKFL preserves that the eigenbasis used to model the data $\\mathcal{D}^{1}$ \u2019s accuracy for device prototype $^{\\,I}$ , that is for student 1 ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\dim\\left(\\mathcal{W}^{v}\\cap\\left[\\mathcal{Q}^{1}\\setminus\\mathcal{W}^{1}\\right]\\right)=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Case 1: Assume that $Q^{1}=Q^{2}$ and $W^{1}=W^{2}$ . Then it holds that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\dim\\left(\\mathcal{W}^{v}\\cap\\left[\\mathcal{Q}^{1}\\setminus(\\mathcal{W}^{1}\\cup\\mathcal{W}^{1,2}\\right]\\right)=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, it holds that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\Theta}_{T A}\\in S p a n(\\mathcal{W}^{1}\\cap\\mathcal{W}^{1,2})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, with equal capacity, no information is lost in Task Arithmetic aided knowledge ensemble distillation and capacity is efficiently used to model the data from both prototype 1 and prototype 2. Case 2: Assume that $Q^{1^{\\ast}}\\mathrm{>}\\,Q^{2}$ and $W^{1}>W^{2}$ . Then it again holds that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\dim\\left(\\mathcal{W}^{v}\\cap\\left[\\mathcal{Q}^{1}\\setminus(\\mathcal{W}^{1}\\cup\\mathcal{W}^{1,2}\\right]\\right)=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "However, while $\\hat{\\Theta}_{T A}\\in S p a n(\\mathcal{W}^{1}).$ , it holds that $\\dim\\left(\\mathcal{W}^{v}\\cap\\mathcal{W}^{1,2}\\right)=W^{2}<W^{1,2}.$ ", "page_idx": 19}, {"type": "text", "text": "Proof. We consider the case that the prototype $i$ and $j$ datasets be disjoint, i.e. $D^{i}\\cap D^{j}=\\emptyset$ , and $\\textstyle\\bigcup_{i=1}^{M}D^{\\rangle}=D$ . We shall see how the other cases are easier and the result remain the same. We can see immediately from the weight disentanglement property of Task Arithmetic that, ", "page_idx": 19}, {"type": "equation", "text": "$$\nf^{1}({\\pmb x};{\\pmb\\theta}_{a v g}^{1}+\\alpha_{1}{\\pmb\\tau}_{1}+\\alpha_{2}{\\pmb\\tau}_{2})=g^{1,1}({\\pmb x};\\alpha_{1}{\\pmb\\tau}_{1})+g^{1,2}({\\pmb x};\\alpha_{2},{\\pmb\\tau}_{2})+g^{1,0}({\\pmb x})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $g^{1,1}(x;\\alpha_{1}\\pmb{\\tau}_{1})$ for $x\\notin\\mathcal{D}^{1}$ , $g^{1,2}(x;\\alpha_{2}\\tau_{2})$ for $x\\notin\\mathcal{D}^{2}$ and $g^{1,0}({\\pmb x})=0$ for $\\pmb{x}\\in\\mathcal{D}^{1}\\cup\\mathcal{D}^{2}$ . From this, we can immediately conclude the first statement of the Proposition as well as the expression ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\dim\\left(\\mathcal{W}^{v}\\cap\\left[\\mathcal{Q}^{1}\\setminus(\\mathcal{W}^{1}\\cup\\mathcal{W}^{1,2}\\right]\\right)=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and also, in the case of $W^{1}=W^{1,2}=W^{2}$ implies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\Theta}_{T A}\\in S p a n(\\mathcal{W}^{1}\\cap\\mathcal{W}^{1,2})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the last statement we observe again as in the second Case in the Proposition describing VED, $\\dim\\left(\\mathcal{O}(2,1)(\\mathcal{W}^{2})\\right)<W^{1,2}$ from which we can conclude that, generically ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\dim\\big(\\{v:v\\in\\mathcal{O}(2,1)(\\mathcal{W}^{2})\\subseteq\\mathcal{W}^{v},\\mathbb{E}_{(x,y)\\sim\\mathcal{D}^{2}}l(f^{1}(x;v),y)>0\\}\\big)=W^{1,2}-W^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "proving the final statement. ", "page_idx": 20}, {"type": "text", "text": "We observe that a key mechanism of the proof is the dimension of the target space of the teaching operator $\\mathcal{O}(i,j)$ . As an informative model, we can consider coefficients $\\lambda_{j}$ of task vectors as restricting the rank, relative to other teachers. For instance, in the previous Proposition, if $W^{1,2}=2$ and $W^{2}=1$ , then $\\lambda_{2}=1/2$ , so as to enforce one vector of $\\mathcal{W}^{1,2}$ is a target for the map $\\tilde{O}(2,1)$ , would be appropriately sensible. ", "page_idx": 20}, {"type": "text", "text": "The prototype $i$ and $j$ datasets are fully overlapping, i.e. $\\mathcal{D}^{i}\\subseteq\\mathcal{D}^{j}$ , meaning that the two prototypes have the same classes $\\mathcal{D}_{c},\\forall c\\in D$ , then these are the changes to the the proposition 1 and 2 in our theory. ", "page_idx": 20}, {"type": "text", "text": "Consider that in this case, the logits corresponding to $\\mathcal{W}^{1,1}$ and $\\mathcal{W}^{1,2}$ are the same. As such $\\mathcal{W}^{1,2}\\in$ $\\mathcal{Q}^{1}$ , and so the propositions change to, trivially: ", "page_idx": 20}, {"type": "text", "text": "Proposition 3. (Information Loss in VED). Consider the VED procedure. Consider two device prototypes with a device capacity and solution dimension of $Q^{1},\\bar{Q}^{2}$ and $W^{1},W^{2}$ , respectively, and with associated eigenbases $\\mathcal{Q}^{i},\\mathcal{W}^{i}$ . Let the solution set of VED with prototype $i$ as student be $\\hat{\\Theta}_{V E D}^{i}$ with $\\mathrm{dim}(\\hat{\\Theta}_{V E D}^{i})\\,=\\,W^{v_{i}}$ with eigenbasis $\\mathcal{W}^{v_{i}}$ . In addition, denote $W^{s,t}$ , $s,t\\,\\in\\,\\{1,2\\}$ the dimension of the solution set for the student model trained on the data from the teacher device\u2019s ensembles. We assume that self-distillation is executed appropriately, e.g., $\\bar{W}^{1,1}=W^{1}$ and $W^{2,2}=$ $W^{2}$ . ", "page_idx": 20}, {"type": "text", "text": "1. Case 1: Assume that $Q^{1}\\,=\\,Q^{2}$ and $W^{1}=W^{2}=W^{1,2}=W^{2,1}$ . Then it holds that, in expectation, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{dim}\\left(\\hat{\\Theta}_{V E D}^{1}\\cap\\left[\\mathcal{Q}^{1}\\setminus\\mathcal{W}^{1}\\right]\\right)=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This corresponds to the expected capacity of prototype 1 that is taken up for fitting logits that do not fit the data corresponding to prototype 1. ", "page_idx": 20}, {"type": "text", "text": "2. Case 2: Assume that $Q^{1}\\,>\\,Q^{2}$ and $W^{1}\\,=\\,W^{1,2}\\,>\\,W^{2}$ . Then the same quantity as for Case $^{\\,l}$ holds. Moreover, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\dim\\left(\\hat{\\Theta}_{V E D}\\cap\\left[\\mathcal{Q}^{1}\\setminus(\\mathcal{W}^{1}\\cup\\mathcal{W}^{1,2}\\right]\\right)=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This corresponds to capacity of client 1 that has been allocated but fits, in the model of prototype $^{\\,l}$ , neither the data of prototype $^{\\,l}$ , nor of the data of prototype 2. ", "page_idx": 20}, {"type": "text", "text": "Now consider that the prototype $i$ and $j$ datasets are partially overlapping, i.e. $D^{i}\\cap D^{j}\\neq\\emptyset$ , meaning that the two prototypes have the overlap over some of the classes $\\mathcal{D}_{c}$ , then these are the changes to the the proposition 1 and 2 in our theory. ", "page_idx": 20}, {"type": "text", "text": "In this case, $\\mathcal{W}^{1,2}=\\mathcal{W}_{1}^{1,2}\\cup\\mathcal{W}_{2}^{1,2}$ , that is, the class labels across data prototypes partially overlap (with $W_{1}^{1,2}$ the overlapping dimensionality), the Proposition for VED changes to: ", "page_idx": 20}, {"type": "text", "text": "Proposition 4. (Information Loss in VED). Consider the VED procedure. Consider two device prototypes with a device capacity and solution dimension of $Q^{1}$ , $\\bar{Q}^{2}$ and $W^{1},W^{2}$ , respectively, and with associated eigenbases $\\mathcal{Q}^{i},\\mathcal{W}^{i}$ . Let the solution set of VED with prototype i as student be $\\hat{\\Theta}_{V E D}^{i}$ with $\\mathrm{dim}(\\hat{\\Theta}_{V E D}^{i})\\,=\\,W^{v_{i}}$ with eigenbasis $\\mathcal{W}^{v_{i}}$ . In addition, denote $W^{s,t}$ , $s,t\\,\\in\\,\\{1,2\\}$ the dimension of the solution set for the student model trained on the data from the teacher device\u2019s ensembles. We assume that self-distillation is executed appropriately, e.g., $\\dot{W}^{1,1}=W^{1}$ and $W^{2,2}=$ $W^{2}$ . ", "page_idx": 20}, {"type": "image", "img_path": "y6JotynERr/tmp/622803ce8ef8ab9a13dbe205c6c3b0bafd07354498534abe6bcdfd7813cd089f.jpg", "img_caption": ["Figure 5: Illustration of Geometric Intuition. Each panel presents a different case example. The left and center panels present the geometric intuition of KD for vanilla ensemble distillation (VED) and TAKFL in the case where two different large device prototypes performing knowledge transfer. The planes represent the solution subspaces. The right panel presents a circumstance by which two small device prototypes (2, and 3) serve as teacher for transferring knowledge to a larger device prototype 1. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "1. Case 1: Assume that $Q^{1}\\,=\\,Q^{2}$ and $W^{1}=W^{2}=W^{1,2}=W^{2,1}$ . Then it holds that, in expectation, $\\dim\\left(\\hat{\\Theta}_{V E D}^{1}\\cap\\left[\\boldsymbol{Q}^{1}\\setminus\\mathcal{W}^{1}\\right]\\right)=O\\left(\\frac{(Q^{1}-W^{1})(W^{1})!(Q^{1}-W_{2}^{1,2})!}{Q^{1!}(W^{1})!(Q^{1}-W^{1})!+Q^{1!}W_{2}^{1,2}!(Q^{1}-W_{2}^{1,2})!}\\right)$ This corresponds to the expected capacity of prototype $^{\\,l}$ that is taken up for fitting logits that do not fit the data corresponding to prototype 1.   \n2. Case 2: Assume that $Q^{1}\\,>\\,Q^{2}$ and $W^{1}\\,=\\,W^{1,2}\\,>\\,W^{2}$ . Then the same quantity as for Case 1 holds. Moreover, $\\dim\\left(\\hat{\\Theta}_{V E D}\\cap\\left[\\mathcal{Q}^{1}\\setminus(\\mathcal{W}^{1}\\cup\\mathcal{W}^{1,2}]\\right)=O\\left(\\frac{(Q^{1}-W^{1})(W^{1}!)(W_{2}^{1,2}-W^{2})!}{Q^{1}!W^{1}!(Q^{1}-W^{1})!+Q^{1}!W^{2}!(W_{2}^{1,2}-W^{2})!}\\right)$ This corresponds to capacity of client 1 that has been allocated but fits, in the model of prototype $^{\\,l}$ , neither the data of prototype $^{\\,l}$ , nor of the data of prototype 2. ", "page_idx": 21}, {"type": "text", "text": "Note that the presence of overlapping logits $W_{1}^{1,2}$ do not appear in the result, it is exactly the same as the original Proposition 1 in the original paper just with $W_{2}^{1,2}$ 21,2in place of W 1,2. ", "page_idx": 21}, {"type": "text", "text": "The Propositions for TAKFL remain the same. ", "page_idx": 21}, {"type": "text", "text": "C.2 Geometric Intuition ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section we aim to provide geometric intuition for the mechanism of VED and Task Arithmetic KD on three different cases. Figure 5 presents the geometry illustration for three different cases. We discuss each case in the following. ", "page_idx": 21}, {"type": "text", "text": "Case I: KD between two large prototypes with different data distributions. Consider Figure 5 left panel. This panel corresponds to a setting where two large device prototypes with similar total capacity, i.e. $Q^{\\dot{1}}=Q^{2}=3$ perform knowledge transfer. We consider the solution dimensions of both prototypes to be the same, i.e. $W^{1}\\,=\\,\\bar{W}^{2}\\,=\\,2$ . These would correspond to planes in the ambient space. Therefore, one plane corresponds to the solution subspace of the prototype 1 trained on its own data, i.e. $\\Theta^{1}$ subspace in the panel, and the other corresponds to the (theoretical) solution subspace of this prototype trained on prototype 2\u2019s data, i.e. $\\Theta^{(1,\\bar{2})}$ in the panel. In this case, since the data distributions of the prototypes are fairly disparate, this has resulted into near orthogonal subspaces corresponding to these solutions. As we can see from the panel, VED will lead to point which is far away from either of the planes corresponding to optimal solution subspaces, and far from the optimal set of parameters, which is their intersection, suggesting a loss of knowledge. By contrast, the TAKFL approach, by customizing the merging coefficients and putting each to half, i.e. $\\lambda_{1,1}\\lambda_{1,2}\\,=\\,0.5$ , can traverse in the tangent space of zero loss surface and get into the intersection subspace which is exactly the optimal solution $(\\pmb{\\theta}_{m e r g e d}^{*}=\\pmb{\\theta}_{a v g}^{1}+\\lambda_{1,1}\\tau_{1,1}+\\lambda_{1,2}\\tau_{1,2})$ . ", "page_idx": 21}, {"type": "text", "text": "Case II: KD between two large prototypes with similar data distributions. Consider Figure 5 center panel. Similar to Case I, this panel corresponds to a setting where two large device prototypes with similar total capacity, i.e. ${Q^{\\mathrm{I}}}^{\\mathrm{\\!\\!\\!/}}={Q^{\\mathrm{2}}}\\,=\\,{\\dot{3}}$ performing knowledge transfer. We consider the solution dimensions of both prototypes to be the same, i.e. $\\bar{W}^{1}=W^{2}\\stackrel{\\textstyle-}{=}2$ . These would correspond to planes in the ambient space. Therefore, one plane corresponds to the solution subspace of the prototype 1 trained on its own data, i.e. $\\Theta^{1}$ subspace in the panel, and the other corresponds to the (theoretical) solution subspace of this prototype trained on prototype 2\u2019s data, i.e. $\\Theta^{(1,2)}$ in the panel. In this case, since the data distributions of the prototypes are fairly close, this has resulted into nonorthogonal solution subspaces. As we can see from the panel, while VED could still lead to some information loss, by and large we expect straightforward KD. Our task arithmetic (TA) approach, again by customizing the merging coefficients $\\lambda_{1,1}\\lambda_{1,2}=0.5$ , can traverse in the tangent space of zero loss surface on each plane and get into the intersection subspace, corresponding to the most efficient allocation of device prototype capacity for fitting simultaneously the logits corresponding to accurate modeling of device prototype 1 as well as device prototype 2\u2019s data distribution. ", "page_idx": 22}, {"type": "text", "text": "Case III: KD between two small prototypes and one large prototype. Now consider the right panel in Figure 5. This panel corresponds to a setting where two small prototypes serve as teachers and one large prototype is the student. The $\\theta_{a v g}^{1}$ plane corresponds to the solution subspace of the large prototype 1 on its own data, $\\mathcal{D}^{1}$ . The line $\\theta_{a v g}^{1,2}$ line corresponds to the subspace of solutions in prototype 1\u2019s parameter space projected into the capacity of the information transferred from device prototype 2. Finally, the line labelled $\\theta_{a v g}^{1,3}$ corresponds to the subspace of solutions in prototype 1\u2019s parameter space projected into the capacity of the information transferred from device prototype 3. Here, we can see from the relative angle of the lines with respect to the plane that the distribution $\\mathcal{D}^{1}$ iikse  cclaossee rI  taon tdh triisb luitkieo nc $\\mathcal{D}^{2}$ ItIh. anW et oc $\\mathcal{D}^{3}$ .a pCploy mtphaer isnagm teh icso ncaclsues itoo ntsh eh eprree vaiso uwse lcl arseegs,a $\\theta_{a v g}^{1,3}$ $\\theta_{a v g}^{1,2}$ can see from the geometric visualization that knowledge distillation towards $\\theta_{a v g}^{1,3}$ has more margin of error for prototype 1. Therefore, with the TAKFL approach large prototype 1 can strategically select which prototype to learn more from, and since $\\theta_{a v g}^{\\dagger,\\dagger}$ has closer data distribution to prototype 1, TAKFL will prioritizes this by putting a larger merging coefficient, i.e $\\lambda_{1,2}>\\lambda_{1,3}$ . By contrast, VED lacks this customization and results in sub-optimal knowledge distillation. ", "page_idx": 22}, {"type": "text", "text": "The geometric intuition discussed here is consistent with our detailed experimental analysis in D.1.1. ", "page_idx": 22}, {"type": "text", "text": "C.3 Analytical Properties of Learning Dynamics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here we provide additional insights from the literature as to the nature and properties of learning as it takes place on overparametrized models. Specifically, we comment on literature in the area of Stochastic Differential Equation (SDE) models for SGD training dynamics, and its correspondence to the results above. Overparametrization has been conjectured to be a significant factor in contributing to the (unexpected, by classical bias-variance tradeoffs) generalization ability of deep neural networks, from a number of perspectives [13]. ", "page_idx": 22}, {"type": "text", "text": "Consider the diffusion model of SGD training for overparametrized NNs provided in [29]. Their analysis relies on the following two assumptions. For our purposes $L$ is shorthand for a client group\u2019s loss, $L(\\pmb{\\theta})=\\sum_{k\\in\\mathbb{C}^{j}}\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathbb{D}_{k}^{j}}\\left[\\ell(f^{j}(\\pmb{\\dot{x}};\\pmb{\\theta}),y)\\right]$ for some $j$ , which will be identified from the context. ", "page_idx": 22}, {"type": "text", "text": "Assumption 2. $L:\\mathbb{R}^{Q}\\rightarrow\\mathbb{R}$ is $C^{3}$ and the solution set $\\Gamma$ is a $W$ -dimensional $C^{2}$ -submanifold of $\\mathbb{R}^{D}$ for $0\\le W\\le D$ and rank $\\colon(\\nabla^{2}L(\\theta))=Q-W$ ", "page_idx": 22}, {"type": "text", "text": "Assumption 3. Assume that $U$ is an open neighborhood of $\\Gamma$ satisfying that gradient flow starting in $U$ converges to some point in $\\Gamma$ ", "page_idx": 22}, {"type": "text", "text": "From these, [29] derives Theorem 4.6. This Theorem decomposes the random process of the parameter weights driven by SGD after it has reached the solution manifold, e.g., the diffusive random walk of $\\theta_{a v g}^{1}$ in Figure 5 along its respective solution manifold. ", "page_idx": 22}, {"type": "text", "text": "Theorem 1. Given $L,\\,\\Gamma$ and $\\theta_{\\mu}(0)\\,=\\,\\theta(0)\\,\\in\\,U$ as by Assumptions 2 and 3 the SDE modeling the optimization of $F$ by SGD, that is, defining $\\Phi(X)$ to be the gradient flow applied to the state random variable $X$ , then for $T$ as long as $\\mathbb{P}[Y(t)\\in U,\\forall0\\leq t\\leq T]=1,$ , $\\theta_{\\eta}(\\lfloor\\bar{T}/\\eta^{2}\\rfloor$ converges in distribution to the stochastic process $Y(T)$ as $\\eta\\rightarrow0$ , with $Y(t)$ given as, ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d Y(t)=\\Sigma_{\\parallel}^{1/2}(Y)d W(t)-\\frac{1}{2}\\nabla^{2}L(Y)^{\\dagger}\\partial^{2}(\\nabla L)(Y)[\\Sigma_{\\parallel}(Y)]d t}\\\\ &{\\qquad-\\frac{1}{2}\\partial\\Phi(Y)\\left(2\\partial^{2}(\\nabla L)(Y)\\left[\\nabla^{2}L(Y)^{\\dagger}\\Sigma_{\\perp,\\parallel}(Y)\\right]+\\partial^{2}(\\nabla L)(Y)\\left[\\mathcal{L}_{\\nabla^{2}L}^{-1}(\\Sigma_{\\perp}(Y))\\right]\\right)d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\textstyle\\sum\\equiv\\sigma\\sigma^{T}$ and $\\Sigma_{\\parallel},\\Sigma_{\\parallel},\\Sigma_{\\parallel}$ are given as, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Sigma_{\\parallel}=\\partial\\Phi\\Sigma\\partial\\Phi,\\,\\Sigma_{\\perp}=(I_{D}-\\partial\\Phi)\\Sigma(I_{D}-\\partial\\Phi),}\\\\ {\\Sigma_{\\parallel,\\perp}=\\partial\\Phi\\sigma(I_{D}-\\partial\\Phi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This theorem indicates that the asymptotic flow of SGD on the client training can be decomposed into a covariance-driven random walk in the tangent space, drift to preserve the flow into the tangent plane, the tangent-normal portion of the noise covariance and noise in the normal direction. ", "page_idx": 23}, {"type": "text", "text": "This analytical expression provides the probabilistic foundations for the more higher level theoretical results above. In particular, local gradient dynamics, as employed by individual device prototypes $j$ using FedAvg on its local clients, yields a flow for the stochastic process defined by the weights. At this point of learning, the weights are traversing the solution set, with noise predominantly in the tangent directions. Thus knowledge distillation which preserves this noise structure is going to be more effective as far as preserving accuracy across data. ", "page_idx": 23}, {"type": "text", "text": "D Detailed Experimental Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section we present a more detailed version of experimental results presented in the main paper Section 7. Additional experimental results are also presented here. ", "page_idx": 23}, {"type": "text", "text": "D.1 Main Experimental Results on Computer Vision (CV) Task ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The experiments in this section complements the main experimental results in the main paper Section 7.2. ", "page_idx": 23}, {"type": "text", "text": "Experimental Setup. For the evaluation on CV task, we employ CIFAR-10 and CIFAR-100 [24] datasets. For CIFAR-10, we use CIFAR-100 as the unlabeled public dataset, while ImageNet-100, a subset of ImageNet [10] with 100 classes (see Appendix F.1.1), is used for CIFAR-100. We distribute the training dataset among the device prototypes in a ratio of 1:3:6 for S, M, and L, respectively. Each device prototype\u2019s data portion is further distributed among its clients using a Dirichlet distribution. We apply two levels of data heterogeneity for a comprehensive evaluation: low heterogeneity, i.e. Dir(0.3), and high heterogeneity, i.e. Dir(0.1). Additionally, we configure the number of clients and their sampling rates as follows: 100 clients for S, 20 for M, and 4 for L, with sampling rates set at 0.1, 0.2, and 0.5 respectively. To comprehensively evaluate, we use two distinct architectural settings: the \u201chomo-family\u201d setting, where all device prototypes\u2019 architectures are from the same family\u2014employing ResNet8, ResNet14, and ResNet18 [17] for S, M, and L, respectively; and the \u201chetero-family\u201d setting, which diverse architectures are used\u2014ViT-S [12] for S, ResNet14 for M, and VGG-16 [44] for L. All models are initialized from scratch, and the communication round is set at 60 rounds. Further details regarding hyper-parameters can be found in Table 13. ", "page_idx": 23}, {"type": "text", "text": "Overview of Performance Results. Table 4 presents the performance of TAKFL across diverse architecture settings on the CIFAR-10 and CIFAR-100 datasets. TAKFL consistently improves all device prototypes of different sizes in various cases by a significant margin compared to the baselines, achieving SOTA performance. Notably, in the homo-family architecture setting with $\\operatorname{Dir}(0.3)$ on CIFAR-10, TAKFL improves average performance across all prototypes by $8\\%$ , and by $4\\%$ on CIFAR-100. In the hetero-family settings with $\\operatorname{Dir}(0.1)$ on CIFAR-10 and $\\operatorname{Dir}(0.3)$ on CIFAR-100, TAKFL enhances performance by ${\\sim}3\\%$ and $1\\%$ , respectively. Furthermore, we observe that our self-regularization technique has successfully mitigated issues associated with the noisy and unsupervised ensemble distillation process, thereby enhancing performance. Generally, the performance gains from self-regularization are more pronounced in low data heterogeneity cases, where prototypes\u2019 models perform better and possess higher quality self-knowledge. Thus, self-regularization proves more effective as it preserves this higher quality self-knowledge. ", "page_idx": 23}, {"type": "table", "img_path": "y6JotynERr/tmp/118141a5270401534154925feb75caed3a57d378c41bec374f4de1e22c1ccf31.jpg", "table_caption": ["Table 4: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. In homo-family settings, architectures are ResNet8, ResNet14, and ResNet18; in hetero-family settings, they are ViT-S, ResNet14, and VGG-16. All models are trained from scratch for 60 rounds. See Appendix F.1 for more details. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "D.1.1 Consistency Analysis: Experimental and Theoretical Correlations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this part, we elaborate on our key experimental observations and their alignment with our theoretical findings. ", "page_idx": 24}, {"type": "text", "text": "Insight 1: From Table 4, it is evident that prior KD-based methods show inconsistent performance across various device prototypes, particularly for the large (L) prototype. For instance, in the CIFAR10 homo-family setting with Dir(0.3), while small (S) and medium (M) prototypes see performance gains, the L prototype experiences up to a ${\\sim}10\\%$ performance decline compared to vanilla FedAvg, which lacks server-side knowledge distillation. This trend is consistent across other settings, such as CIFAR-10 Dir(0.1) homo-family and CIFAR- $\\cdot100\\;\\mathrm{Dir}(0.3)$ homo-family. These outcomes underline the dilution problem inherent in existing methods, where the valuable insights from larger, more capable device prototypes are overshadowed by less informative outputs from smaller devices, thereby degrading the performance of L prototypes. These empirical findings are supported by our theoretical insights as discussed in Remark 1. Specifically, Proposition 1 illustrates that vanilla ensemble distillation (VED) leads to knowledge dilution and inaccuracies due to misaligned device capacity allocations. Moreover, this issue becomes more significant when the smaller device prototype serve as teacher. ", "page_idx": 24}, {"type": "text", "text": "Insight 2: From Table 4, the suboptimality of existing KD-based methods is evident from the significant performance improvements of our method, especially for S and M prototypes across various settings. This underscores the ineffectiveness of the one-size-fits-all approach these methods employ, where a single averaged logits distillation target is used for all device sizes, proving to be suboptimal. Our experimental observations regarding the shortcomings of vanilla ensemble distillation methods align with our theoretical findings, as substantiated in Remark 1 and 2. It becomes evident that an efficient knowledge distillation process must allocate capacity in a manner that appropriately corresponds to the information value of the teacher ensemble prototypes. ", "page_idx": 24}, {"type": "text", "text": "Insight 3: Our experiments, detailed in Table 4, demonstrate TAKFL\u2019s adept handling of knowledge from various device prototypes under different data heterogeneity conditions. We observed consistent performance gains for small (S) and medium (M) prototypes across both low and high data heterogeneity, compared to vanilla FedAvg. However, in high heterogeneity settings, large (L) ", "page_idx": 24}, {"type": "text", "text": "prototypes show less improvement, prompting the question: What can smaller device prototypes offer to larger ones? ", "page_idx": 25}, {"type": "text", "text": "In low heterogeneity scenarios, large prototypes significantly benefit from the collective knowledge, showing enhanced performance. Conversely, in conditions of extreme heterogeneity, where smaller models contribute less effectively, the performance improvements for larger devices are notably reduced. This pattern highlights TAKFL\u2019s ability to intelligently manage and utilize the available knowledge, selectively distilling information based on the intrinsic capacity and contributions of each prototype, and integrating only the most valuable knowledge from smaller devices when beneficial. ", "page_idx": 25}, {"type": "text", "text": "By contrast, our analysis of existing KD-based methods shows their failure to effectively discern and utilize the most informative knowledge across prototypes. These methods often overload the capacity of larger prototypes with suboptimal or irrelevant information, particularly in high heterogeneity environments, leading to not just stagnation but an accumulation of inefficiencies. These experimental observations align with our theoretical insights, as outlined in Remark 2, which emphasizes the crucial combinatorial constraint of capacity and diverse information. This further confirms the superiority of TAKFL\u2019s adaptive approach to knowledge distillation in diverse federated learning environments. ", "page_idx": 25}, {"type": "text", "text": "D.2 Additional Experimental Results on CV Task ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Experimental Setup. For additional evaluation of the CV task, we conducted experiments on TinyImageNet [25], STL-10 [7], and CINIC-10 [9] datasets using pre-trained models. For TinyImageNet [25], we utilized STL-10 [7] as the unlabeled public dataset. STL-10 [7] and CINIC-10 [9] both employ CIFAR-100 [24] as their respective public datasets. We distributed the training datasets among the device prototypes in a 2:3:5 ratio for small (S), medium (M), and large (L) prototypes, respectively. The data portion for each prototype was further subdivided among its clients using a Dirichlet distribution: Dir(1.0) for TinyImageNet and Dir(0.3) for both STL-10 and CINIC-10. Client configurations were set with 4, 3, and 2 clients for S, M, and L, respectively, all with a sampling rate of 1.0. The architectures employed were MobileNetV3-Large [19] for S, MobileViTV2 [34] for M, and ResNet-34 for L, sourced from the TIMM library.1 The local training was conducted over 10 epochs using an Adam [23] optimizer with a learning rate of 1e-3 and weight decay of 1e-5. For server-side distillation, the epoch count was 10 for TinyImageNet and 1 for STL-10 and CINIC-10, with a batch size of 128, employing an Adam optimizer with a learning rate of 1e-5 and weight decay of 1e-5. For TinyImageNet, public dataset images from STL-10 were resized to $64\\!\\times\\!64$ , while for STL-10, images were resized to $32\\!\\times\\!32$ . No data augmentation was used. The communication rounds is fixed to 40. These experiments were conducted by only 1 trial. Table 15 details the configurations. ", "page_idx": 25}, {"type": "text", "text": "Performance Results. Table 5 presents the results. The superiority of TAKFL\u2019s performance across these challenging datasets using pre-trained models is evident here as well. ", "page_idx": 25}, {"type": "text", "text": "D.3 Additional Experimental Results on Natural Language Processing (NLP) Task ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The experiments in this section complements the main experimental results in the main paper Section 7.2. ", "page_idx": 25}, {"type": "text", "text": "Experimental Setup. For the evaluation of NLP tasks, we utilize four datasets: MNLI [52], SST2 [45], MARC [22], and AG-news [60]. The corresponding unlabeled public datasets are SNLI [2] for MNLI, Sentiment140 [14] for SST-2, Yelp [61] for Amazon, and DBPedia [59] for AG-News. The training data is distributed among the device prototypes in a ratio of 1:3:6 for small (S), medium (M), and large (L) categories, respectively, with each portion further subdivided among its clients using a Dirichlet distribution $\\operatorname{(Dir}(0.5))$ . The client configurations and their sampling rates are set as follows: 8, 4, and 2 clients for S, M, and L categories, respectively, with sampling rates of 0.3, 0.5, and 1.0. The architectures employed for each prototype size are BERT [47] -Tiny, -Small, and -Mini, respectively, each initialized from pre-trained parameters and tested over 20 communication rounds. Additional details regarding hyper-parameters and datasets are presented in Appendix F.2 and Table 17. ", "page_idx": 25}, {"type": "table", "img_path": "y6JotynERr/tmp/d57ae88ad10903dfdd4c34556d03deac14049be48b8e3aec4bec01c49875901b.jpg", "table_caption": ["Table 5: Performance Results for CV task on TinyImageNet, STL-10, and CINIC-10 using pre-trained models. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "y6JotynERr/tmp/c73dbb205e489613ac4ca639ea1bcc2f06db84b4f58b7299e57cefd8b92fb2e5.jpg", "table_caption": ["Table 6: Performance Results for NLP Task on 4 Datasets.. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using $\\operatorname{Dir}(0.5)$ . Client configurations are 8, 4, and 2 clients for S, M, and L, with sample rates of 0.3, 0.5, and 1.0, respectively. Architectures include Bert-Tiny, Bert-Mini, and Bert-Small for S, M, and L, initialized from pre-trained parameters and fine-tuned for 20 communication rounds. See Appendix F.2 for more details. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Performance on NLP Task. Table 6 presents the results on four different datasets: MNLI, SST-2, MARC, and AG-News. Similar to the CV task, TAKFL has consistently improved performance across all device prototypes of varying sizes, achieving state-of-the-art results. On MNLI, it has enhanced average performance across all prototypes by $3\\%$ , on SST-2 by ${\\sim}2\\%$ , on MARC by $3\\%$ , and on AG-News by $\\mathord{\\sim}1.50\\%$ . As observed in the CV task, the suboptimality of existing KD-based methods is also evident here. Notably, FedET exhibits very poor performance compared vanilla FedAvg, failing to achieve satisfactory results on all datasets except for the MARC dataset. Particularly, the performance of the L prototype has consistently decreased across all datasets compared to vanilla FedAvg. This behavior can be attributed to FedET\u2019s reliance on neural network confidence scores for uncertainty estimates in its uncertainty-weighted distillation. However, neural networks, especially pretrained language models (PLMs), are often poorly calibrated and prone to overconfidence, which compromises their ability to provide reliable uncertainty estimates [50, 15, 5, 55]. ", "page_idx": 26}, {"type": "text", "text": "D.4 Scalability Evaluation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This section complements the experimental results in the main paper Section 7.3. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Experimental Setup. To evaluate the effectiveness and scalability of our method across a broad spectrum of device prototypes, ranging from very small to very large sizes, we conduct experiments involving 3 to 7 different prototypes. Our objective is to assess how effectively our method adapts from a uniform array of small-size prototypes (3 device prototypes) to a diverse mix that includes prototypes ranging from extremely small (XXS) to extremely large (XXL) (7 device prototypes). These experiments involve training image classification models from scratch on the CINIC10 dataset, using CIFAR-100 as the unlabeled public dataset. We randomly distribute the dataset among prototypes with dataset ratios set to 1:2:3:4:5:6:7 from XXS to XXL. Each dataset portion is further distributed among clients using a Dirichlet distribution $\\operatorname{(Dir}(0.5))$ . The number of clients ranges from 35 to 5 from XXS to XXL, respectively. Client sample rates are set at 0.1, 0.1, 0.15, 0.15, 0.2, 0.3, and 0.6 from XXS to XXL. We use a series of ResNet architectures\u2014ResNet10-XXS, ResNet10-XS, ResNet10-S, ResNet10-M, ResNet10, ResNet18, and ResNet50\u2014scaled appropriately for each prototype. The local training epochs are set at 2, 2, 2, 5, 10, 10, and 20 from XXS to XXL to account for resource constraints, with fewer epochs assigned to smaller devices. We employ the Adam optimizer with a learning rate of 1e-3 and a weight decay of 5e-5 for local training. For XL and XXL, a step learning rate scheduler reduces the learning rate by a factor of 0.1 at half epoch. Server-side distillation employs a fixed batch size of 128, using the Adam optimizer with learning rate of 1e-3 and weight decay of 5e-5. The softmax temperature is set at 3 for ensemble distillation and 20 for self-regularization. The number of communication rounds is fixed at 30. These experiments are conducted over 3 trials with different random seeds, and the average performance with standard deviation is reported. The entire device prototypes configurations are given in Table 16. ", "page_idx": 27}, {"type": "text", "text": "The detailed results are presented in Tables 7, 8, and 9. ", "page_idx": 27}, {"type": "text", "text": "Table 7: Scalability Evaluation. Detailed performance results for 7 device prototypes case. ", "page_idx": 27}, {"type": "table", "img_path": "y6JotynERr/tmp/97519a9b1a73c467f3d476cbf2f045b4e887439833ef199253d52b956e1dc61b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "y6JotynERr/tmp/970facd319ffbc4f19015170b178f7f8ddf3d70140106bcb82e11ad87f25f993.jpg", "table_caption": ["Table 8: Scalability Evaluation. Detailed performance results for 5 device prototypes case. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "y6JotynERr/tmp/47bf44417a2004c64af22853280c0ed4d40ec17c8e4f0fba600f4219d798b755.jpg", "table_caption": ["Table 9: Scalability Evaluation. Detailed performance results for 3 device prototypes case. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "E Ablation Studies ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "E.1 Understanding Merging Coefficient ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we conduct an ablation study to further understand how TAKFL customizes knowledge integration and understand how the merging coefficients $\\lambda_{i}$ are achieving this. This experiment aims to further understand the trade-offs between customized knowledge integration approach from the one-size-fits-all strategy employed in vanilla ensemble distillation and prior works. ", "page_idx": 27}, {"type": "text", "text": "Experimental Setup. Our experimentation focuses on two device prototypes: XXS and XXL, selected from the scalability evalulation detailed in Section 7.3, Appendix D.4, and Table 16. We employ the image classification task on the CINIC-10 [9] dataset, starting from scratch. Each prototype receives a randomly selected, non-overlapping subset of the training dataset\u2014 $.3.57\\%$ for XXS and $25\\%$ for XXL\u2014distributed among their clients in a non-i.i.d. manner using Dir(0.5). Both prototypes have three clients each. The architectures used are ResNet10-XXS for the XXS prototype and ResNet-50 for the XXL prototype. To focus solely on the evaulation of the server-side distillation process and its evolution with varying $\\lambda$ , we pre-train each prototype using standard FedAvg for 10 communication rounds, with a sample rate of 1.0. Local training involves 20 epochs for XXS and 20 epochs for XXL using an Adam optimizer with a learning rate of 1e-3 and weight decay of 5e-5. The XXL prototype employs a step learning rate scheduler that reduces the rate by a factor of 0.1 at local epoch 10. For server-side distillation, we utilize a batch size of 128 and an Adam optimizer with a learning rate of 1e-5 and weight decay of 5e-5. CIFAR-100 [24] serves as the unlabeled public dataset. We save the final updated client and server models from both prototypes for further experimentation, focusing on the impact of merging coefficients without self-regularization in TAKFL. The merging coefficient $\\lambda$ is varied linearly from 0 to 1 in increments of 0.05. For simplicity, the XXS prototype is referred to as the small (S) prototype and XXL as the large (L) prototype. ", "page_idx": 28}, {"type": "text", "text": "Discussion. Figure 6 illustrates the significant impact of customized knowledge integration on the performance of both small and large device prototypes compared to the one-size-fits-all approach typical of vanilla ensemble distillation in the prior works, at different distillation epochs. Here, TAKFL adeptly manages customization for both small and large prototypes by controlling the merging coefficient $\\lambda$ . The merged model for both the small and large student prototypes is obtained using the formula $\\theta_{m e r g e d}=\\overleftarrow{\\theta_{a v g}}+\\left((1-\\lambda)\\tau_{S}+\\lambda\\tau_{L}\\right)$ . Notably, the performance is benchmarked at $\\lambda\\approx0.5$ in all cases, reflecting similar results to vanilla ensemble distillation (FedDF), where no customization in knowledge transfer occurs. This baseline performance is critical for understanding the effects of further customization. ", "page_idx": 28}, {"type": "text", "text": "In small distillation epochs $[I_{d i s t i l l}<10]$ ), minimal benefit is observed from customized knowledge integration, as both small and large prototypes achieve optimal performance at the non-customized $\\lambda\\approx0.5$ . However, as the distillation process progresses beyond 10 epochs, the influence of $\\lambda$ becomes increasingly pronounced. For $\\lambda>0.5$ , the knowledge from the large prototype\u2019s ensembles predominates, enhancing their impact, while for $\\lambda\\:<\\:0.5$ , integration is more influenced by the small prototype\u2019s ensembles. This pattern suggests that increased distillation epochs enable more effective distillation of each prototype\u2019s unique knowledge for extreme cases of extremely small and large prototypes, thereby making the customization benefits evident. In scenarios with small distillation epochs, the absence of significant unique knowledge results in optimal performance at $\\lambda\\approx0.5$ . Conversely, as the number of distillation epochs rises $(I_{d i s t i l l}\\,\\geq\\,20)$ ), the one-size-fits-all strategy proves suboptimal, underscoring the importance of tailored knowledge integration strategies. Optimal performance increasingly occurs at $\\lambda>0.5$ , indicating effective leveraging of each prototype\u2019s strengths to maximize overall performance. These findings confirm the necessity for customized knowledge integration in environments with significant prototype size variations and support our theoretical insights as detailed in Remark 1 and 2. ", "page_idx": 28}, {"type": "text", "text": "E.2 Impact of Public Dataset ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we explore the influence of the public dataset on the performance of TAKFL and existing KD-based methods when the public dataset used for server-side distillation is less similar to the private dataset, which is the actual learning objective. For this analysis we employ the same experimental setup previously outlined in Section 7.2 and Appendix D.1, using the CIFAR-10 homo-family architecture. To measure dataset similarity, we compute cosine similarity between the averaged features of datasets, extracted using an off-the-shelf pre-trained CLIP model [40] (CLIP ViT-B/32) available from the official GitHub repository.2 ", "page_idx": 28}, {"type": "text", "text": "Discussion. Table 10 presents our results, highlighting a significant observation: the performance of existing methods drastically deteriorates as the similarity between the public dataset and private datasets decreases. In contrast, TAKFL exhibits robustness, suffering much less performance degradation under the same conditions. This demonstrates TAKFL\u2019s practical utility in real-world scenarios where the server typically lacks knowledge of the private datasets to select a closely aligned public dataset for distillation. Notably, FedET underperforms significantly when using a less similar public dataset, performing worse than vanilla FedAvg in both low and high data heterogeneity scenarios. A similar pattern was observed with FedET in the NLP tasks discussed in Section 7.2 and Appendix D.3. This issue is likely due to FedET\u2019s dependence on the overconfident and poorly calibrated confidence scores from neural networks [15, 55] for uncertainty estimates in its uncertaintyweighted distillation approach. ", "page_idx": 28}, {"type": "image", "img_path": "y6JotynERr/tmp/38a0c2b29ccc5bf5fdc1dd4d522246174537078ecad7c0a3a29430542fd4137b.jpg", "img_caption": ["Figure 6: Understanding the Impact of Merging Coefficients. This figure showcases server-side knowledge distillation between two device prototypes, XXS and XXL, referred to as small and large, respectively, utilizing CIFAR-100 as the unlabeled public dataset. Both prototypes were pretrained from scratch using standard FedAvg for 10 communication rounds. The CINIC-10 dataset was distributed between the small and large prototypes in ratios of $3.57\\%$ and $25\\%$ , respectively, and further subdivided non-i.i.d. among the clients using $\\operatorname{Dir}(0.5)$ . Each prototype has three clients with a sample rate of 1.0. The small prototype utilizes a ResNet10-XXS architecture, while the large prototype employs a ResNet-50. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Table 10: Impact of Public Dataset on performance results. Same experimental setting described in Section 7.2 and Appendix D.1 on CIFAR-10 homo-family setting is used for this experiment. The numbers in parentheses represent the similarity scores between private and public datasets, obtained using a pre-trained CLIP ViT-B/32 model. ", "page_idx": 29}, {"type": "table", "img_path": "y6JotynERr/tmp/9b8a2d47266c126fabdc5ecc72a5f8ca423a1abc1e55bf8d02f9d2c0a5d87945.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "E.3 Impact of FL Optimizer ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "To further assess the impact of FL optimizer on the performance, we adopt FedOpt (Adam) [41], which is a SOTA FL optimizers as the base per prototype $\\mathrm{FL}$ optimizer and conduct the same experimental setup on CV task discussed in Section xx. The results are presented in Table 11. As we can see, the performance of TAKFL is robust under different FL optimizers and is consistently achieving SOTA performance. ", "page_idx": 29}, {"type": "table", "img_path": "y6JotynERr/tmp/b6f6031f9a58fb24b21966d7dc92cc6f46dcefa629d4f0a0056c32fd7af468db.jpg", "table_caption": ["Table 11: Impact of FL Optimizer. Performance comparison results using FedOpt (Adam) [41] as per prototype FL optimizer for the same setting as Table 1 (Section 7.2). "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "E.4 t-SNE Visualization of Knowledge Transfer ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "To better understand the effectiveness of knowledge transfer in TAKFL, we conducted a visualization study using two device prototypes, XS and XL, on the CIFAR-10 dataset. The prototype configurations follow those used in our scalability evaluation in Section 7.3 (more details are available in Appendix D.4). We first pre-trained the two prototypes\u2019 global models using FedAvg for 40 communication rounds and then performed knowledge transfer using the TAKFL process for 10 distillation epochs. We have plotted the t-SNE visualizations of both prototypes for FedAvg and TAKFL in Figure 7. ", "page_idx": 30}, {"type": "text", "text": "As we can see in the t-SNE plots (Figure 7), TAKFL demonstrates a substantial enhancement in feature representation for both prototypes compared to FedAvg. The plots show better separation between classes for both XS and XL prototypes. The class clusters are more distinct, suggesting that TAKFL, by effectively transferring knowledge between the two prototypes, yielded a clearer representation of the features, even in the smaller XS prototype. Therefore, the t-SNE visualizations confirm that TAKFL is indeed effective in knowledge transfer, further corroborating its superiority and effectiveness. ", "page_idx": 30}, {"type": "image", "img_path": "y6JotynERr/tmp/02aad7745abc8df008af99978c5fb9c5112072af9dba00346987cec92faf0f28.jpg", "img_caption": ["Figure 7: t-SNE Visualization Study. To better understand the effectiveness of TAKFL\u2019s knowledge transfer, we conducted experiment on a setting involving two device prototypes XS and XL similar to our scalability evaluation setup in Section 7.3 on CIFAR-10 dataset. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "F Hyper-parameters and Implementation ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section we bring the details of the hyper-parameters we used and our implementation. We implement our entire code in PyTorch [38] using FedZoo benchmark [36] and release it at https: //github.com/MMorafah/TAKFL. We use two NVIDIA RTX 3090 gpus to conduct the entire experimentation in this paper. ", "page_idx": 31}, {"type": "text", "text": "F.1 Computer Vision (CV) Task ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "For comprehensive evaluation of our method, we consider federated learning image classification training from scratch. ", "page_idx": 31}, {"type": "text", "text": "F.1.1 Datasets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Datasets. We experiment with several image classification datasets: CIFAR-10, CIFAR-100, CINIC-10, TinyImageNet, STL-10. The details of each dataset is the following: ", "page_idx": 31}, {"type": "text", "text": "\u2022 CIFAR-10 [24]: CIFAR-10 consists of 60,000 images of size $32\\!\\times\\!32$ RGB across 10 classes, with each class containing 6,000 images.   \n\u2022 CIFAR-100 [24]: CIFAR-100 comprises 60,000 images of size $32\\!\\times\\!32$ RGB distributed across 100 classes, with 500 images per class.   \n\u2022 CINIC-10 [9]: CINIC-10 has 270,000 images of size $32\\!\\times\\!32$ RGB across 10 classes, each class containing 27,000 images.   \n\u2022 TinyImageNet [25]: TinyImageNet contains 100,000 images of size $64\\!\\times\\!64$ RGB across 200 classes.   \n\u2022 STL-10 [7]: STL-10 has 100,000 unlabeled images and 13,000 labeled images of size $96\\!\\times\\!96$ RGB across 10 classes. ", "page_idx": 31}, {"type": "text", "text": "The ImageNet-100 as the unlabeled public dataset is constructed by randomly selecting 100 classes from the ImageNet [10] dataset. ", "page_idx": 31}, {"type": "text", "text": "F.1.2 Architectures ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Experiments in Section 7.2 and Appendix D.1. The architecture that we use are two distinct architectural settings: the \u201chomo-family\u201d setting, where all device prototypes\u2019 architectures are from the same family, and the \u201chetero-family\u201d setting, where architectures do not necessarily belong to the same family. For the homo-family scenario, we employ ResNet-8 for S, ResNet-14 for M, and ResNet-18 for L. For the hetero-family scenario, we use ViT-S for S, ResNet-14 for M, and VGG-16 for L. All models are initialized from random initialization. ", "page_idx": 31}, {"type": "text", "text": "For the ResNet architecture configuration, we utilize the standard \u2018BasicBlock\u2019 as the building block. This consists of a convolutional block, followed by four residual block stages, an adaptive average pooling layer, and a classifier layer. The models within this family differ in terms of the number of repetitions of the residual block and the number of filters in each stage. The configurations for different capacities are detailed below: ", "page_idx": 31}, {"type": "text", "text": "\u2022 ResNet-18 is configured with [64, 128, 256, 512] filters and repeats the \u2018BasicBlock\u2019 [2, 2, 2, 2] times.   \n\u2022 ResNet-14 is configured with [64, 128, 256, 512] filters and repeats the \u2018BasicBlock\u2019 [1, 2, 2, 1] times.   \n\u2022 ResNet-8 is configured with [64, 128, 256] filters, corresponding to the first three stages, and repeats the \u2018BasicBlock\u2019 [1, 1, 1] times. ", "page_idx": 31}, {"type": "text", "text": "For VGG-16 [44], we use the standard architecture which includes convolutional layers followed by max-pooling layers. The configuration of filters for each layer is as follows: ", "page_idx": 31}, {"type": "text", "text": "VGG-16: [64, 64, \u2018M\u2019, 128, 128, \u2018M\u2019, 256, 256, 256, \u2018M\u2019, 512, 512, 512, \u2018M\u2019, 512, 512, 512, \u2018M\u2019] ", "page_idx": 31}, {"type": "text", "text": "The final classification head consists of two linear layers with a hidden size of 512, followed by a ReLU activation, and a final linear classifier layer. ", "page_idx": 32}, {"type": "text", "text": "For ViT-S, we adopt the standard Vision Transformer [12] architecture implementation from Github.3 We configure ViT-S with 6 attention blocks, each with 16 heads and a hidden dimension of 64. The final MLP dimension is set to 256. In our experiments with ViT-S, we set the patch size to 4, and the input image size is $32\\!\\times\\!32$ . ", "page_idx": 32}, {"type": "text", "text": "Experiment in Appendix D.2. The pre-trained MobileNetV3-Large [19], MobileViTV2 [34], and ResNet34 [51] were instantiated using the TIMM library.4 ", "page_idx": 32}, {"type": "text", "text": "Scalability Experiments in Section 7.3. The architectures in these experiments are inspired by [20]. The details of architectures are as following: ", "page_idx": 32}, {"type": "text", "text": "\u2022 ResNet10-XXS is configured with [8, 8, 16, 16] filters and repeats the \u2018BasicBlock\u2019 [1, 1, 1, 1] times.   \n\u2022 ResNet10-XS is configured with [8, 16, 32, 64] filters and repeats the \u2018BasicBlock\u2019 [1, 1, 1, 1] times.   \n\u2022 ResNet10-S is configured with [16, 32, 64, 128] filters and repeats the \u2018BasicBlock\u2019 [1, 1, 1, 1] times.   \n\u2022 ResNet10-M is configured with [8, 16, 32, 64] filters and repeats the \u2018BasicBlock\u2019 [1, 1, 1, 1] times.   \n\u2022 ResNet10 is configured with [64, 128, 256, 512] filters and repeats the \u2018BasicBlock\u2019 [1, 1, 1, 1] times.   \n\u2022 ResNet18 is configured with [64, 128, 256, 512] filters and repeats the \u2018BasicBlock\u2019 [1, 1, 1, 1] times.   \n\u2022 ResNet50 is configured with [64, 128, 256, 512] filters and repeats the \u2018BasicBlock\u2019 [3,4,6,3] times. ", "page_idx": 32}, {"type": "text", "text": "F.1.3 FL configuration and Hyper-parameters ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Base Hyper-parameters. The following hyperparameter values apply to all CV experiments unless stated otherwise. We set the diversity regularizer coefficient of FedET to 0.1 for our entire experimentation per the original paper [6]. We use the Adam optimizer with a learning rate of 1e-5, weight decay value of 5e-5, and a batch size of 128 for distillation. The softmax distillation temperature is set to 3, the distillation epoch to 1, and the self-regularizer softmax temperature to 20 for both CV and NLP experiments. Table 13 details the hyper-parameters. ", "page_idx": 32}, {"type": "text", "text": "Experiments in Section 7.2 and Appendix D.1. For tables 1 and 4, there are 100 clients with the S device prototype, 20 clients with the M device prototype, and 4 clients with the L device prototype. For each round, 10, 4, and 2 clients from each S, M, and L prototype are randomly sampled respectively for participation. $10\\%$ of the data goes to the S prototype, $30\\%$ to M, and $60\\%$ to L. The data is distributed to each client among each prototype in a Dirichlet distribution. Table 14 details the FL configuration. ", "page_idx": 32}, {"type": "text", "text": "Experiments in Appendix D.2 and Appendix E.2. For tables 5 and 10, there are 4, 3, and 2 clients for S, M, and L device prototypes, respectively. Each round, every client participates in FL. $20\\%$ of the data is distributed to prototype S, $30\\%$ to prototype M, and $50\\%$ to prototype L. Table 15 details the FL configuration. ", "page_idx": 32}, {"type": "text", "text": "Scalability Experiments in Section 7.3 and D.4. For tables 7, 8, and 9, there are 35, 30, 25, 20, 15, 10, and 5 clients for the prototypes XXS, XS, S, M, L, XL, and XXL, respectively. The sample rate is set to 0.1, 0.1, 0.15, 0.15, 0.2, 0.3, and 0.6 from XXS to XXL. The data is distributed for each prototype in the ratio 1:2:3:4:5:6:7 from XXS to XXL. Table 16 details the hyper-parameters and configuration. ", "page_idx": 32}, {"type": "text", "text": "Validation Set. For TAKFL, the validation set used for the heuristic method (see F.3) is $5\\%$ of the training dataset. The validation set and the private dataset does not overlap. ", "page_idx": 32}, {"type": "text", "text": "F.2 Natural Language Processing (NLP) Task ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "For the NLP task, we fine-tune federated learning text classification task using pretrained models. ", "page_idx": 33}, {"type": "text", "text": "F.2.1 Datasets. ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "All NLP datasets were provided by Hugging Face. ", "page_idx": 33}, {"type": "text", "text": "\u2022 MNLI [52]: MNLI contains 433K sentence pairs, each sentence pair labeled as one of \u2018entailment,\u2019 \u2018neutral,\u2019 and \u2018contradiction.\u2019   \n\u2022 SNLI [2]: SNLI is similar to MNLI, with 570K sentence pairs each labeled one of 3 labels.   \n\u2022 SST2 [45]: SST2 consists of 67K phrases, each labeled as sentiment \u2019positive\u2019 or \u2019negative.\u2019   \n\u2022 Sentiment140 [14]: Sentiment140 is a dataset of 1.6M Twitter messages each labeled with one of 2 sentiment values.   \n\u2022 MARC [22]: MARC (Multilingual Amazon Reviews Corpus) is a dataset with online reviews in multiple languages from the Amazon delivery service website. Each review has a label which is one of 1-5 stars. We only use the English reviews from this dataset, which results in 260,000 English reviews total.   \n\u2022 Yelp [61]: The Yelp reviews dataset contains 700K reviews each labeled 1-5 stars from the Yelp service which publishes public reviews of businesses.   \n\u2022 AG News [60]: AG News contains 127,600 news article titles. Each article is one of four classifications of news articles.   \n\u2022 DBpedia [59]: The DBpedia dataset consists of 630K DBpedia article summaries each labeled one of 14 categorizations. ", "page_idx": 33}, {"type": "text", "text": "F.2.2 Architectures ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Experiments in Section 7.2 and Appendix D.3. We use three variations of the BERT architecture: BERT-Tiny, BERT-Mini, and BERT-Small from [47]. The weights were pre-trained on the BookCorpus dataset and extracted text from Wikipedia. Further details regarding each model are described extensively on Github.6 The tokenizer used for these transformer models are the same ones provided by the authors of [47]. ", "page_idx": 33}, {"type": "text", "text": "\u2022 BERT-Tiny contains 2 transformer layers and an embedding size of 128.   \n\u2022 BERT-Mini contains 4 transformer layers and an embedding size of 256.   \n\u2022 BERT-Small contains 4 transformer layers and and embedding size of 512. ", "page_idx": 33}, {"type": "text", "text": "F.2.3 FL configuration and hyper-parameters ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Base Hyper-parameters. For distillation, we use the Adam optimizer with a learning rate of 3e-5, no weight decay, and batch size of 32. The distillation epoch is set to 1, the ensemble distillation softmax temperature to 3, and the self-regularizer softmax temperature to 20 for all NLP experiments. Table 17 details the hyper-parameters. ", "page_idx": 33}, {"type": "text", "text": "Experiments in Section 7.2 and Appendix D.3. For tables 2 and 6, we limit the private dataset to 100,000 samples, randomly sampled from the original dataset i.i.d. The public dataset is limited to 30,000 examples sampled i.i.d as well. There are 8, 4, and 2 clients for the S, M, and L prototypes. The private data is split across each prototype in the following proportions: 0.1, 0.3, 0.6. Table 18 details the FL configuration. ", "page_idx": 33}, {"type": "text", "text": "Validation Set. The validation dataset used for TAKFL is 5,000 samples taken from the original training dataset that does not overlap with the 100,000 private dataset. ", "page_idx": 33}, {"type": "text", "text": "F.3 Hyper-parameters of TAKFL ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Merging Coefficients. We conducted extensive experiments with different merging coefficients on the main 3-device prototype setting of small (S), medium (M), and large (L) discussed in Section 7.2 and Appendix D.1. We empirically observed that the small (S) prototype typically achieves the best performance using a uniformly increasing merging coefficient, where the larger the prototype, the larger the merging coefficient, i.e., $\\lambda_{S}~\\le~\\lambda_{M}~\\le~\\lambda_{L}$ . As we move towards larger prototypes, they benefit more from increasingly skewed merging coefficients towards the larger ones. In the extreme case of the large (L) prototype, highly skewed merging coefficients generally led to better performance, i.e., $\\lambda_{S}\\;\\ll\\;\\lambda_{M}\\;\\ll\\;\\lambda_{L}$ . This pattern is intuitive as small prototypes can benefit from everyone while gaining more from the larger, more informative prototypes. However, larger prototypes benefit less from smaller ones, as they typically offer less information, especially in high data heterogeneity cases. Notably, in high data heterogeneity cases, more skewed merging coefficients seemed to be more advantageous as the smaller prototypes (S and M) possess lower quality knowledge. ", "page_idx": 34}, {"type": "text", "text": "Based on these observations, we designed a simple and cost-effective heuristic method that randomly instantiates merging coefficients following this intuition. Our heuristic method, presented in 1, leverages these observations by generating candidate merging coefficients that incorporate both uniformly increasing and different degrees of skewed merging coefficients. This dual approach enables us to explore a wide range of merging strategies and identify the most effective configurations for different prototypes. The optimal merging coefficient candidate is determined using the performance on the held-out validation set. ", "page_idx": 34}, {"type": "table", "img_path": "y6JotynERr/tmp/8c1277c8c7df485789202a872d1b07d6f284c9601b73f2f999792840b85c664e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "Listing 1: Implementation of the heuristic method for merging coefficients in Python. The exponent term controls the degree of skewness or peaking in the merging coefficients. ", "page_idx": 34}, {"type": "text", "text": "Furthermore, we experiment with manually determining the merging coefficients and fixating them throughout the federation. We achieved similar results with this approach compared to adaptively finding the coefficients using the heuristic method and a small held-out validation set. We present the merging coefficient candidates that performed reasonably well during our experiments in Table 12. ", "page_idx": 34}, {"type": "table", "img_path": "y6JotynERr/tmp/1b67acea4446fd4d9a00666bb94ff7ce4071d15ffa2764ce610f9dd88fa255f1.jpg", "table_caption": ["Table 12: Details of the experimentally determined merging coefficients for the 3-device prototype setting discussed in Section 7.2 and Appendix D.1. Coefficients are ordered as $[\\lambda_{S},\\lambda_{M},\\lambda_{L}]$ . "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "Self-Regularization Coefficient. Extensive experiments were conducted on the self-regulation coefficients for different device prototypes and settings. Although no consistent pattern emerged, we experimentally determined that optimal performance for the small prototype was achieved with selfregulation coefficients $\\gamma_{S}\\in{0.1,0.01,0.001}$ . For the medium prototype, the coefficients were $\\gamma_{M}\\in$ 0.5, 0.1, 0.01, 0.001, 0.0001, and for the large prototype, $\\gamma_{L}\\in1.0$ , 0.8, 0.5, 0.1, 0.01, 0.001, 0.0001 yielded the best results. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "table", "img_path": "y6JotynERr/tmp/4f1c0fcae70b68a73cad5acf5b66af426be3fc5b0a5e26774fe7ad4b1b6b1932.jpg", "table_caption": ["Table 13: Details of hyper-parameters for CV task in Section 7.2, Appendix D.1, and Appendix E.2. "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "y6JotynERr/tmp/a7e16ce72975c5b8f88ae3d1460db1934b1348c55eb5b429aa8bfe806b389719.jpg", "table_caption": ["Table 14: Details of Architecture parameters and FL configuration for CV task in Section 7.2 and Appendix D.1. "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "y6JotynERr/tmp/c18ecfa5b3a71c2d39784f048b0a557d5371068bd4158c6f74cbf6c41dc31832.jpg", "table_caption": ["Table 15: Details of Architecture parameters and FL configuration for CV task experiment using pre-trained models in Appendix D.2. "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "y6JotynERr/tmp/220ef153d41c30f7b3451aa7f61d12d2a6c4a7757a04217c7eb6df90f822eff1.jpg", "table_caption": ["Table 16: Details of Architecture parameters for Scalability Section 7.3, and Appendix D.4. "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "y6JotynERr/tmp/751e9beb688e4a44a0de7cd975ac22d8335dd14b703740425ec67e3a28a846dd.jpg", "table_caption": ["Table 17: Details of hyper-parameters for NLP task experiments in Section 7.2 and Appendix D.3. "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "y6JotynERr/tmp/c2941be0c9ffda26f7501afd30ac16e767526dd2f41a04194712e817fa4f7d8e.jpg", "table_caption": ["Table 18: Details of Architecture parameters and FL configuration for NLP task experiment using pre-trained models in Section 7.2 and Appendix D.3. "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: The limitation of the work is discussed in the main paper Section 8. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The informal statements are in the main paper Section 6, and the full formal statements with assumptions and proofs can be found in the Appendix C. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The detailed information can be found in Appendix D, and F. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The information can be found in Appendix F Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: The information can be found in Appendix D and F. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We conduct the main experiments in the main paper Section 7 over 3 independent trials with different random seeds and average results with the standard deviation is reported in Tables 1, 2, 4, 6, 16, and 10. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The information can be found in Appendix F. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper focuses on advancing technical methodologies in federated learning and does not directly engage with specific societal impacts, as the applications of the proposed method are varied and context-dependent. The societal implications would largely depend on the specific use cases implemented by subsequent practitioners. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 40}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper primarily focuses on a technical methodological approach without releasing any models. The potential for misuse depends on specific applications by end users, which are beyond the scope of this study. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The datasets and models used in this work are properly credited in main paper Section 7, Appendix D, and Appendix F. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The information can be found in Appendix F. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}]