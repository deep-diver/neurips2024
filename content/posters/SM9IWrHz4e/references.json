{"references": [{"fullname_first_author": "Peter Auer", "paper_title": "Near-optimal Regret Bounds for Reinforcement Learning", "publication_date": "2009-00-00", "reason": "This paper establishes a minimax lower bound for average reward MDPs and proposes the UCRL2 algorithm, which is a key reference in the field and a basis for many subsequent works including the proposed algorithm in this paper."}, {"fullname_first_author": "Peter L. Bartlett", "paper_title": "REGAL: a regularization based algorithm for reinforcement learning in weakly communicating MDPs", "publication_date": "2009-06-00", "reason": "This paper introduces REGAL, an algorithm that achieves near-optimal regret bounds by exploiting the bias function, which is a crucial concept used in the proposed algorithm and its analysis."}, {"fullname_first_author": "Ronan Fruit", "paper_title": "Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning", "publication_date": "2018-00-00", "reason": "This paper improves on previous algorithms by using a tighter confidence region and proposes SCAL, another key algorithm that improves on UCRL2 and is used for comparison in this paper."}, {"fullname_first_author": "Zihan Zhang", "paper_title": "Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function", "publication_date": "2019-00-00", "reason": "This paper introduces the EBF algorithm, which achieves minimax optimal regret, and it is used for comparison in this paper, showing that the proposed algorithm is tractable unlike EBF."}, {"fullname_first_author": "Mohammad Sadegh Talebi", "paper_title": "Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in MDPs", "publication_date": "2018-04-00", "reason": "This paper refines the analysis of previous algorithms by incorporating variance information, which is a key aspect of the analysis of the proposed algorithm and its improved regret bounds."}]}