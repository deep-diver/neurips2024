{"references": [{"fullname_first_author": "Mantas Mazeika", "paper_title": "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal", "publication_date": "2024-02-29", "reason": "This paper introduces Harmbench, a benchmark dataset and evaluation framework specifically designed for assessing the robustness of LLMs against jailbreak attacks, providing a standardized method for comparing different attack methods."}, {"fullname_first_author": "Andy Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-21", "reason": "This is a highly influential paper that introduces a novel token-level attack method (GCG) which serves as a strong baseline for comparison in the current paper's evaluation of attack effectiveness."}, {"fullname_first_author": "Erik Jones", "paper_title": "Automatically auditing large language models via discrete optimization", "publication_date": "2023-07-20", "reason": "This paper proposes a novel approach for evaluating the safety of LLMs via discrete optimization, providing a conceptually similar yet alternative approach to the current paper's optimization method."}, {"fullname_first_author": "Patrick Chao", "paper_title": "Jailbreaking black box large language models in twenty queries", "publication_date": "2023-10-26", "reason": "This paper presents a method for jailbreaking LLMs which focuses on efficiency and provides a clear comparison point for the current paper's proposed method"}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This seminal paper discusses the RLHF method for aligning LLMs, which is relevant to the context of jailbreaking attacks and the motivations for improving LLM safety."}]}