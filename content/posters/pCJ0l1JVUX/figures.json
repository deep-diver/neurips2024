[{"figure_path": "pCJ0l1JVUX/figures/figures_1_1.jpg", "caption": "Figure 1: In-the-wild visual results of Hamba. Hamba achieves significant performance in various in-the-wild scenarios, including hand interaction with objects or hands, different skin tones, different angles, challenging paintings, and vivid animations.", "description": "This figure shows several examples of 3D hand reconstructions generated by the Hamba model in various challenging real-world scenarios. The scenarios include hands interacting with objects, hands with varying skin tones and poses, and even hands depicted in paintings and animations. The diversity of the scenarios demonstrates the robustness and generalizability of the Hamba model in handling complex real-world situations.", "section": "1 Introduction"}, {"figure_path": "pCJ0l1JVUX/figures/figures_2_1.jpg", "caption": "Figure 2: Motivation. Visual comparisons of different scanning flows. (a) Attention methods compute the correlation across all patches leading to a very high number of tokens. (b) Bidirectional scans follow two paths, resulting in less complexity. (c) The proposed graph-guided bidirectional scan (GBS) achieves effective state space modeling leveraging graph learning with a few effective tokens (illustrated as scanning by two snakes: forward and backward scanning snakes).", "description": "This figure compares three different scanning methods for hand reconstruction. The first method uses attention, which considers all patches leading to a high number of tokens. The second method uses a bidirectional scan, reducing complexity with two scanning paths. The proposed method uses a graph-guided bidirectional scan, which leverages graph learning to achieve effective state space modeling with only a few tokens. The figure illustrates the difference in complexity and effectiveness among these three methods.", "section": "3 Proposed Methodology"}, {"figure_path": "pCJ0l1JVUX/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of Hamba's architecture. Given a hand image I, tokens are extracted via a trainable backbone model and downsampled. We design a graph-guided SSM as a decoder to regress hand parameters. The hand joints (J2D) are regressed by Joints Regressor (JR) and fed into the Token Sampler (TS) to sample tokens (Trs). The joint spatial sequence tokens (TGss) are learned by the Graph-guided State Space (GSS) blocks. Inside each GSS block, the GCN network takes Trs as input and its output is concatenated with the mean down-sampled tokens. GSS leverages graph learning and state space modeling to capture the joint spatial relations to achieve robust 3D reconstruction.", "description": "This figure shows the architecture of the Hamba model, which consists of a backbone to extract image tokens, a joints regressor to predict 2D hand joints, a token sampler to select relevant tokens, and graph-guided state space blocks to learn joint spatial relations. The output is then fused to regress the final MANO parameters.", "section": "3.2 Hamba"}, {"figure_path": "pCJ0l1JVUX/figures/figures_5_1.jpg", "caption": "Figure 4: The illustration of the proposed Graph-guided State Space (GSS) block.", "description": "This figure compares three different blocks: the Mamba block, the VSS block, and the proposed GSS block.  The Mamba block is a basic state space model. The VSS block extends this with a 2D selective scan.  The GSS block adds a graph convolutional network (GCN) to incorporate graph-structured relationships between hand joints, enhancing the representation by considering the intricate hand joint relations.  It leverages graph learning and state space modeling to model the joint spatial sequence. The GSS block is the core innovation in the Hamba model, enabling it to model spatial relationships more effectively.", "section": "3.2 Hamba"}, {"figure_path": "pCJ0l1JVUX/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative in-the-wild comparison of the proposed Hamba with SOTAs on HInt-Epic Kitchens VISOR [16, 70]. None of the models (including Hamba) have been trained on HInt.", "description": "This figure shows a qualitative comparison of Hamba's 3D hand mesh reconstruction performance against other state-of-the-art methods on the challenging HInt-EpicKitchensVISOR dataset.  The images illustrate various in-the-wild scenarios involving occlusions, hand-object interactions, different viewpoints and lighting conditions.  Importantly, none of the models were trained using data from this specific dataset, highlighting the generalization capabilities of the approaches. The comparison demonstrates Hamba's superior reconstruction accuracy compared to other methods even in these very difficult scenarios.", "section": "4 Experiments"}, {"figure_path": "pCJ0l1JVUX/figures/figures_18_1.jpg", "caption": "Figure 3: Overview of Hamba's architecture. Given a hand image I, tokens are extracted via a trainable backbone model and downsampled. We design a graph-guided SSM as a decoder to regress hand parameters. The hand joints (J2D) are regressed by Joints Regressor (JR) and fed into the Token Sampler (TS) to sample tokens (Trs). The joint spatial sequence tokens (TGss) are learned by the Graph-guided State Space (GSS) blocks. Inside each GSS block, the GCN network takes Trs as input and its output is concatenated with the mean down-sampled tokens. GSS leverages graph learning and state space modeling to capture the joint spatial relations to achieve robust 3D reconstruction.", "description": "This figure shows the architecture of the Hamba model. The input image is processed by a backbone network to extract tokens, which are then downsampled. A joint regressor predicts 2D hand joints, which are used by a token sampler to select relevant tokens.  These tokens are fed into graph-guided state space (GSS) blocks, which incorporate graph convolutional networks (GCNs) to model the spatial relationships between hand joints and state space modeling to learn the joint spatial sequences. The output of the GSS blocks, along with global mean tokens, are then fused and used to regress the final MANO parameters (pose, shape, and camera parameters).", "section": "3.2 Hamba"}, {"figure_path": "pCJ0l1JVUX/figures/figures_21_1.jpg", "caption": "Figure 2: Motivation. Visual comparisons of different scanning flows. (a) Attention methods compute the correlation across all patches leading to a very high number of tokens. (b) Bidirectional scans follow two paths, resulting in less complexity. (c) The proposed graph-guided bidirectional scan (GBS) achieves effective state space modeling leveraging graph learning with a few effective tokens (illustrated as scanning by two snakes: forward and backward scanning snakes).", "description": "This figure compares three different scanning methods used in 3D hand reconstruction: (a) Attention-based methods which compute the correlation across all image patches, leading to a high number of tokens. (b) The bidirectional scan, which uses two paths for scanning, and therefore has lower complexity. (c) The proposed graph-guided bidirectional scan (GBS) which leverages graph learning with fewer tokens to achieve more effective state space modeling. The GBS is illustrated as two snakes scanning forward and backward.", "section": "3 Proposed Methodology"}, {"figure_path": "pCJ0l1JVUX/figures/figures_23_1.jpg", "caption": "Figure 3: Overview of Hamba's architecture. Given a hand image I, tokens are extracted via a trainable backbone model and downsampled. We design a graph-guided SSM as a decoder to regress hand parameters. The hand joints (J2D) are regressed by Joints Regressor (JR) and fed into the Token Sampler (TS) to sample tokens (Trs). The joint spatial sequence tokens (TGss) are learned by the Graph-guided State Space (GSS) blocks. Inside each GSS block, the GCN network takes Trs as input and its output is concatenated with the mean down-sampled tokens. GSS leverages graph learning and state space modeling to capture the joint spatial relations to achieve robust 3D reconstruction.", "description": "This figure illustrates the architecture of the proposed Hamba model.  It shows the flow of data from the input image through various modules including a backbone, token sampler, graph convolutional network, and state space blocks, finally culminating in the regression of MANO parameters for 3D hand reconstruction.  The figure highlights the key components: the Joints Regressor (JR) which predicts 2D joints, the Token Sampler (TS) that selects informative tokens, the Graph-guided State Space (GSS) blocks that learn spatial relationships, and a fusion module that combines global and local features.", "section": "3.2 Hamba"}, {"figure_path": "pCJ0l1JVUX/figures/figures_24_1.jpg", "caption": "Figure 3: Overview of Hamba's architecture. Given a hand image I, tokens are extracted via a trainable backbone model and downsampled. We design a graph-guided SSM as a decoder to regress hand parameters. The hand joints (J2D) are regressed by Joints Regressor (JR) and fed into the Token Sampler (TS) to sample tokens (Trs). The joint spatial sequence tokens (TGss) are learned by the Graph-guided State Space (GSS) blocks. Inside each GSS block, the GCN network takes Trs as input and its output is concatenated with the mean down-sampled tokens. GSS leverages graph learning and state space modeling to capture the joint spatial relations to achieve robust 3D reconstruction.", "description": "This figure shows a detailed architecture of the Hamba model, highlighting the different components and their interactions.  The input is a hand image which undergoes feature extraction via a backbone network and downsampling. A Joint Regressor (JR) predicts 2D hand joints used by a Token Sampler (TS) to select relevant tokens. These tokens, representing spatial joint relationships, are processed through Graph-guided State Space (GSS) blocks incorporating Graph Convolutional Networks (GCNs) and State Space modeling.  A fusion module combines the GSS outputs with global features before final MANO parameter regression.", "section": "3.2 Hamba"}, {"figure_path": "pCJ0l1JVUX/figures/figures_25_1.jpg", "caption": "Figure 1: In-the-wild visual results of Hamba. Hamba achieves significant performance in various in-the-wild scenarios, including hand interaction with objects or hands, different skin tones, different angles, challenging paintings, and vivid animations.", "description": "This figure shows several examples of 3D hand reconstruction results from the proposed Hamba model on various challenging in-the-wild images.  These examples demonstrate the robustness of the Hamba model in handling various real-world conditions such as hand-object interactions, diverse skin tones, varied viewpoints and lighting conditions, and even highly challenging images such as paintings and animations.", "section": "1 Introduction"}, {"figure_path": "pCJ0l1JVUX/figures/figures_26_1.jpg", "caption": "Figure 1: In-the-wild visual results of Hamba. Hamba achieves significant performance in various in-the-wild scenarios, including hand interaction with objects or hands, different skin tones, different angles, challenging paintings, and vivid animations.", "description": "This figure showcases the robustness of the Hamba model in various real-world scenarios. It highlights the model's ability to accurately reconstruct 3D hand meshes even in challenging conditions such as hand-object interactions, diverse skin tones, varying viewpoints and lighting, and complex backgrounds. The images demonstrate Hamba's superior performance compared to existing state-of-the-art methods.", "section": "1 Introduction"}, {"figure_path": "pCJ0l1JVUX/figures/figures_27_1.jpg", "caption": "Figure 1: In-the-wild visual results of Hamba. Hamba achieves significant performance in various in-the-wild scenarios, including hand interaction with objects or hands, different skin tones, different angles, challenging paintings, and vivid animations.", "description": "This figure shows several examples of 3D hand reconstruction results from the Hamba model.  The images demonstrate the model's ability to accurately reconstruct hands in a variety of challenging real-world conditions, including those with significant occlusion, varying lighting, and different hand poses and interactions with objects.  The diversity of scenarios highlights the robustness and generalizability of the proposed Hamba method.", "section": "1 Introduction"}]