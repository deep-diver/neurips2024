[{"figure_path": "GN2qbxZlni/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of the evaluation paradigm and representative examples in MR-Ben. Each data point encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error analysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses, which include error steps, reasons behind the error, and subsequent corrections. The three examples shown are selected to represent arithmetic, logical, and algorithmic reasoning types.", "description": "This figure illustrates the MR-Ben evaluation paradigm.  Each data point includes a question, a Chain-of-Thought (CoT) answer generated by LLMs, and a human-annotated error analysis.  The error analysis details the erroneous step, reason for the error, and the correction. Three example questions are shown representing arithmetic, logical, and algorithmic reasoning tasks.  This visualization demonstrates the process-based nature of the MR-Ben benchmark, focusing on evaluating the quality of the reasoning process rather than just the final answer.", "section": "1 Introduction"}, {"figure_path": "GN2qbxZlni/figures/figures_7_1.jpg", "caption": "Figure 2: Model performance across subjects", "description": "This radar chart visualizes the performance of several LLMs across different reasoning subjects, including math, medicine, biology, chemistry, logic, and coding. Each axis represents a subject, and the distance from the center indicates the model's performance (MR-Score) on that subject.  The chart allows for a comparison of the strengths and weaknesses of different models across various reasoning domains.  The overlapping areas show where models have comparable performances, while distinct separation indicates performance differences.", "section": "5.2 Experiment Results"}, {"figure_path": "GN2qbxZlni/figures/figures_7_2.jpg", "caption": "Figure 3: Model performance on different reasoning paradigms", "description": "This figure presents a bar chart that compares the performance of several large language models (LLMs) across four different reasoning paradigms: knowledge, logic, arithmetic, and algorithmic.  Each bar represents the MR-Score achieved by a specific LLM in each reasoning paradigm.  The height of the bar indicates the performance level, allowing for a direct comparison of model capabilities in various reasoning tasks. The MR-Score, explained in section 4 of the paper, is a composite metric integrating accuracy in identifying correct solutions, pinpointing the first incorrect reasoning step, and providing accurate error explanations.", "section": "5 Experiment Results"}, {"figure_path": "GN2qbxZlni/figures/figures_8_1.jpg", "caption": "Figure 4: MR-Scores of different models on different levels of difficulty", "description": "This figure displays the MR-Scores achieved by various LLMs on questions of different difficulty levels (high school vs. college).  It visually represents the performance difference of the models on questions with varying complexities.  The horizontal dashed line indicates a benchmark score of 0.5, providing a visual reference point to compare model performance against.", "section": "5.2 Experiment Results"}, {"figure_path": "GN2qbxZlni/figures/figures_8_2.jpg", "caption": "Figure 2: Model performance across subjects", "description": "This figure visualizes the performance of different LLMs across various subjects in the MR-Ben benchmark.  Each bar represents a model's MR-Score (a metric combining solution correctness, first error step accuracy, and error reason accuracy) for a specific subject. The subjects included are likely math, medicine, biology, physics, chemistry, logic, and coding, as mentioned in the paper.  The figure helps to illustrate the relative strengths and weaknesses of each model across different reasoning domains and shows the variation in performance of LLMs depending on the subject matter.", "section": "3 MR-Ben: Dataset Construction"}, {"figure_path": "GN2qbxZlni/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of the evaluation paradigm and representative examples in MR-Ben. Each data point encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error analysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses, which include error steps, reasons behind the error, and subsequent corrections. The three examples shown are selected to represent arithmetic, logical, and algorithmic reasoning types.", "description": "This figure illustrates the three key components of the MR-Ben dataset: question, chain-of-thought (CoT) answer, and error analysis.  The CoT answers are generated automatically by various LLMs and then human experts annotate each example, providing an error analysis that includes the step where the error occurs, the reason for the error, and the correction.  The figure uses three example questions to showcase how this works for arithmetic, logical, and algorithmic reasoning.", "section": "1 Introduction"}, {"figure_path": "GN2qbxZlni/figures/figures_23_1.jpg", "caption": "Figure 1: Overview of the evaluation paradigm and representative examples in MR-Ben. Each data point encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error analysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses, which include error steps, reasons behind the error, and subsequent corrections. The three examples shown are selected to represent arithmetic, logical, and algorithmic reasoning types.", "description": "This figure illustrates the meta-reasoning paradigm used in the MR-Ben benchmark.  It shows how each data point in the benchmark consists of three parts: a question, a Chain-of-Thought (CoT) answer generated by an LLM, and a human-expert-provided error analysis. The error analysis details the step where the error occurred, the reason for the error, and the correction.  Three examples are given to showcase how the paradigm applies to different reasoning types: arithmetic, logical, and algorithmic.", "section": "1 Introduction"}, {"figure_path": "GN2qbxZlni/figures/figures_28_1.jpg", "caption": "Figure 1: Overview of the evaluation paradigm and representative examples in MR-Ben. Each data point encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error analysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses, which include error steps, reasons behind the error, and subsequent corrections. The three examples shown are selected to represent arithmetic, logical, and algorithmic reasoning types.", "description": "This figure illustrates the meta-reasoning paradigm used in the MR-Ben benchmark. It shows three examples of questions, each with a chain-of-thought (CoT) answer generated by an LLM and a corresponding error analysis provided by human experts.  The error analyses identify the specific erroneous steps, explain the reasons behind the errors, and provide corrections. The three examples showcase the diversity of reasoning types covered by MR-Ben: arithmetic, logical, and algorithmic reasoning.", "section": "1 Introduction"}, {"figure_path": "GN2qbxZlni/figures/figures_39_1.jpg", "caption": "Figure 1: Overview of the evaluation paradigm and representative examples in MR-Ben. Each data point encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error analysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses, which include error steps, reasons behind the error, and subsequent corrections. The three examples shown are selected to represent arithmetic, logical, and algorithmic reasoning types.", "description": "This figure illustrates the meta-reasoning paradigm used in the MR-Ben benchmark.  Each data point consists of a question, a Chain-of-Thought (CoT) answer generated by an LLM, and a human-annotated error analysis. The error analysis details the erroneous steps in the CoT answer, explains the reasons for the errors, and provides corrections. The figure presents three examples showcasing the application of this paradigm to arithmetic, logical, and algorithmic reasoning problems, demonstrating the benchmark's comprehensive evaluation of reasoning capabilities across diverse domains.", "section": "1 Introduction"}, {"figure_path": "GN2qbxZlni/figures/figures_57_1.jpg", "caption": "Figure 1: Overview of the evaluation paradigm and representative examples in MR-Ben. Each data point encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error analysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses, which include error steps, reasons behind the error, and subsequent corrections. The three examples shown are selected to represent arithmetic, logical, and algorithmic reasoning types.", "description": "This figure illustrates the meta-reasoning paradigm used in the MR-Ben benchmark.  It shows how each data point contains a question, a Chain-of-Thought (CoT) answer generated by LLMs, and a human-annotated error analysis. The error analysis details the erroneous steps, reasons for the errors, and the necessary corrections. Three example questions are provided to showcase arithmetic, logical, and algorithmic reasoning.", "section": "1 Introduction"}, {"figure_path": "GN2qbxZlni/figures/figures_71_1.jpg", "caption": "Figure 1: Overview of the evaluation paradigm and representative examples in MR-Ben. Each data point encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error analysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses, which include error steps, reasons behind the error, and subsequent corrections. The three examples shown are selected to represent arithmetic, logical, and algorithmic reasoning types.", "description": "This figure illustrates the meta-reasoning paradigm used in the MR-Ben benchmark.  It shows that each data point includes a question, a Chain-of-Thought (CoT) solution generated by an LLM, and a human-annotated error analysis.  The error analysis details the incorrect steps in the reasoning, the reasons for the errors, and the corrections. The three examples highlight the diversity of reasoning types covered by the benchmark: arithmetic, logical, and algorithmic reasoning.", "section": "1 Introduction"}, {"figure_path": "GN2qbxZlni/figures/figures_71_2.jpg", "caption": "Figure 1: Overview of the evaluation paradigm and representative examples in MR-Ben. Each data point encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error analysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses, which include error steps, reasons behind the error, and subsequent corrections. The three examples shown are selected to represent arithmetic, logical, and algorithmic reasoning types.", "description": "This figure illustrates the meta-reasoning paradigm used in the MR-Ben benchmark.  Each data point contains a question, a Chain-of-Thought (CoT) answer generated by an LLM, and a human-annotated error analysis. The error analysis details the erroneous step, reason for the error, and correction.  Three examples are shown representing arithmetic, logical, and algorithmic reasoning question types. This visually represents how the benchmark goes beyond simply evaluating the correctness of the answer, instead focusing on the reasoning process itself.", "section": "1 Introduction"}, {"figure_path": "GN2qbxZlni/figures/figures_72_1.jpg", "caption": "Figure 1: Overview of the evaluation paradigm and representative examples in MR-Ben. Each data point encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error analysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses, which include error steps, reasons behind the error, and subsequent corrections. The three examples shown are selected to represent arithmetic, logical, and algorithmic reasoning types.", "description": "This figure illustrates the meta-reasoning paradigm used in the MR-Ben benchmark.  Each data point shows a question, a Chain-of-Thought (CoT) answer generated by an LLM, and a human-provided error analysis. The error analysis identifies the incorrect step(s) in the CoT answer, explains the reason for the error(s), and provides corrections. The three examples showcase arithmetic, logical, and algorithmic reasoning types, demonstrating the diverse range of reasoning skills assessed by the benchmark.", "section": "1 Introduction"}]