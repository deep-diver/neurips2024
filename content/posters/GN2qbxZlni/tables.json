[{"figure_path": "GN2qbxZlni/tables/tables_4_1.jpg", "caption": "Table 1: Statistics of MR-Ben. The length of questions and solutions are measured in the number of words. Notice that the steps for coding denote the number of lines of code. They are not directly comparable with other subjects.", "description": "This table presents the statistics of the MR-Ben benchmark dataset.  It shows the number of question-solution pairs, the percentage of correctly solved questions, the average number of steps in the solutions, the average step number where the first error occurs, and the average length (in words) of the questions and solutions for each subject (Math, Medicine, Biology, Physics, Chemistry, Logic, and Coding).  The table highlights the relative difficulty of different subjects, noting that coding problems have significantly more steps and a different average length compared to other subjects.", "section": "3 MR-Ben: Dataset Construction"}, {"figure_path": "GN2qbxZlni/tables/tables_6_1.jpg", "caption": "Table 2: Evaluation results on MR-Ben: This table presents a detailed breakdown of each model's performance evaluated under metric MR-Score across different subjects, where K stands for the number of demo examples here.", "description": "This table shows the performance of various large language models (LLMs) on the MR-Ben benchmark. The performance is measured using the MR-Score metric, which takes into account the correctness of the solution, the accuracy of identifying the first error step, and the accuracy of explaining the error reason.  The table breaks down the results by subject area (Math, Medicine, Biology, Physics, Chemistry, Logic, Coding) and also shows results with different numbers of demonstration examples (k=0 and k=1). The table compares both closed-source and open-source LLMs.", "section": "5 Experiments"}, {"figure_path": "GN2qbxZlni/tables/tables_9_1.jpg", "caption": "Table 2: Evaluation results on MR-Ben: This table presents a detailed breakdown of each model's performance evaluated under metric MR-Score across different subjects, where K stands for the number of demo examples here.", "description": "This table shows the performance of various large language models (LLMs) on the MR-Ben benchmark.  The performance is measured using the MR-Score metric, which incorporates multiple aspects of reasoning ability. The table breaks down the results by subject area (Biology, Physics, Math, Chemistry, Medicine, Logic, Coding) and by the number of demonstration examples (k=0 and k=1, representing zero-shot and one-shot settings, respectively).  This allows for a comparison of model performance both across subjects and across prompting strategies.", "section": "5 Experiments"}, {"figure_path": "GN2qbxZlni/tables/tables_9_2.jpg", "caption": "Table 2: Evaluation results on MR-Ben: This table presents a detailed breakdown of each model's performance evaluated under metric MR-Score across different subjects, where K stands for the number of demo examples here.", "description": "This table shows the performance of various large language models (LLMs) on the MR-Ben benchmark.  The performance is measured using the MR-Score metric, which considers solution correctness, accuracy in identifying the first error step, and accuracy in explaining the error reason. The table breaks down the results by subject area (Biology, Physics, Math, Chemistry, Medicine, Logic, Coding) and indicates the model's performance with zero and one demonstration examples (k=0 and k=1, respectively). This allows for a comparison of model performance across different reasoning types and levels of difficulty, highlighting strengths and weaknesses in different LLMs.", "section": "5 Experiments"}, {"figure_path": "GN2qbxZlni/tables/tables_19_1.jpg", "caption": "Table 2: Evaluation results on MR-Ben: This table presents a detailed breakdown of each model's performance evaluated under metric MR-Score across different subjects, where K stands for the number of demo examples here.", "description": "This table shows the performance of various large language models (LLMs) on the MR-Ben benchmark.  The performance is measured using the MR-Score metric, which considers solution correctness, accuracy in identifying the first error step, and the accuracy of the explanation of that error. The table breaks down the results across different subjects (Biology, Physics, Mathematics, Chemistry, Medicine, Logic, and Coding).  The 'k' values in the column headers indicate the number of demonstration examples used for few-shot prompting.  The results highlight the performance differences across models and subjects, revealing strengths and weaknesses of each model in various reasoning tasks.", "section": "5.1 Experiment Setup"}, {"figure_path": "GN2qbxZlni/tables/tables_19_2.jpg", "caption": "Table 2: Evaluation results on MR-Ben: This table presents a detailed breakdown of each model's performance evaluated under metric MR-Score across different subjects, where K stands for the number of demo examples here.", "description": "This table shows the performance of various LLMs on the MR-Ben benchmark.  The performance is measured using the MR-Score metric, which considers solution correctness, accuracy in identifying the first error step, and accuracy of the error reason explanation. The table breaks down the results by subject area (Biology, Physics, Math, Chemistry, Medicine, Logic, Coding), showing the MR-Score for each model in each subject with and without demonstration examples (k=0 and k=1). The models are categorized into closed-source and open-source LLMs and further sub-categorized by size (small, medium, large).", "section": "5 Experiments"}, {"figure_path": "GN2qbxZlni/tables/tables_20_1.jpg", "caption": "Table 2: Evaluation results on MR-Ben: This table presents a detailed breakdown of each model\u2019s performance evaluated under metric MR-Score across different subjects, where K stands for the number of demo examples here.", "description": "This table shows the performance of various LLMs on the MR-Ben benchmark.  The performance is measured using the MR-Score metric, which considers solution correctness, first error step accuracy, and error reason accuracy. The table breaks down the results by subject area (Biology, Physics, Math, Chemistry, Medicine, Logic, Coding) and indicates the MR-Score for each model under different numbers of demonstration examples (k).  It provides a comparison of both open-source and closed-source LLMs.", "section": "5.1 Experiment Setup"}, {"figure_path": "GN2qbxZlni/tables/tables_21_1.jpg", "caption": "Table 2: Evaluation results on MR-Ben: This table presents a detailed breakdown of each model's performance evaluated under metric MR-Score across different subjects, where K stands for the number of demo examples here.", "description": "This table presents the performance of various LLMs on the MR-Ben benchmark.  The performance is measured using the MR-Score metric, which incorporates three sub-metrics (Matthews Correlation Coefficient for solution correctness, accuracy of locating the first error step, and accuracy of the error reason). The table shows a detailed breakdown of each model's performance across different subjects (Math, Medicine, Biology, Physics, Chemistry, Logic, Coding).  The 'k' values represent the number of demonstration examples used in few-shot settings.", "section": "5.1 Experiment Setup"}, {"figure_path": "GN2qbxZlni/tables/tables_22_1.jpg", "caption": "Table 2: Evaluation results on MR-Ben: This table presents a detailed breakdown of each model's performance evaluated under metric MR-Score across different subjects, where K stands for the number of demo examples here.", "description": "This table shows the performance of various large language models (LLMs) on the MR-Ben benchmark.  The performance is measured using the MR-Score metric, which combines several sub-metrics to evaluate different aspects of reasoning ability. The table breaks down the results across different subjects (Biology, Physics, Math, Chemistry, Medicine, Logic, Coding) and for different numbers of demonstration examples (k=0, k=1).  The models are categorized into closed-source and open-source LLMs, and further sub-categorized by size (small, medium, large). This provides a comprehensive comparison of the strengths and weaknesses of various models across different reasoning tasks and scales.", "section": "5.1 Experiment Setup"}, {"figure_path": "GN2qbxZlni/tables/tables_41_1.jpg", "caption": "Table 2: Evaluation results on MR-Ben: This table presents a detailed breakdown of each model's performance evaluated under metric MR-Score across different subjects, where K stands for the number of demo examples here.", "description": "This table shows the performance of various LLMs on the MR-Ben benchmark.  The performance is measured using the MR-Score metric, which is a composite score that incorporates three sub-metrics related to solution correctness, identification of the first error step, and correctness of the explanation for the error. The table breaks down the performance across various subjects including Math, Medicine, Biology, Physics, Chemistry, Logic, and Coding. This allows for a more granular analysis of the strengths and weaknesses of each model across different reasoning types and domains. The 'k' value indicates the number of demonstration examples used during evaluation.", "section": "5.1 Experiment Setup"}, {"figure_path": "GN2qbxZlni/tables/tables_69_1.jpg", "caption": "Table 2: Evaluation results on MR-Ben: This table presents a detailed breakdown of each model's performance evaluated under metric MR-Score across different subjects, where K stands for the number of demo examples here.", "description": "This table shows the performance of various large language models (LLMs) on the MR-Ben benchmark.  The performance is measured using the MR-Score metric, which considers solution correctness, the accuracy of identifying the first error step, and the accuracy of explaining the error reason. The table breaks down the results by subject area (Math, Medicine, Biology, Physics, Chemistry, Logic, Coding) and shows the performance with zero and one-shot prompting.  'K' indicates the number of demonstration examples used.", "section": "5 Experiments"}]