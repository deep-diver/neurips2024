{"references": [{"fullname_first_author": "Nisan Stiennon", "paper_title": "Learning to summarize with human feedback", "publication_date": "2020-12-01", "reason": "This paper is foundational for preference modeling in LLMs, introducing a key methodology for aligning LLMs to human preferences through reinforcement learning from human feedback."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This work significantly advanced instruction following in LLMs, showcasing a method for aligning LLMs to human preferences and improving their ability to perform various tasks."}, {"fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-04-13", "reason": "This paper explores the safety and helpfulness aspects of LLMs through reinforcement learning, a relevant consideration when applying preference modeling to improve LLM behavior."}, {"fullname_first_author": "Andreas Kirsch", "paper_title": "Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning", "publication_date": "2019-12-01", "reason": "This paper is highly relevant to the active learning methods used in the target paper, presenting an efficient algorithm for batch acquisition that helps to address the challenges in the large-scale preference modeling of LLMs."}, {"fullname_first_author": "Daniel M. Ziegler", "paper_title": "Fine-tuning language models from human preferences", "publication_date": "2019-09-18", "reason": "This work is among the first to explore fine-tuning LLMs based on human preferences, laying a foundation for subsequent advancements in preference modeling and the techniques used in the present work."}]}