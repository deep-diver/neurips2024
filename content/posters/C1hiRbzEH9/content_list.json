[{"type": "text", "text": "Out-Of-Distribution Detection with Diversification (Provably) ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haiyun $\\mathbf{Yao^{1}}$ , Zongbo $\\mathbf{Han^{1}}$ , Huazhu $\\mathbf{F}\\mathbf{u}^{2}$ , Xi Peng3, Qinghua $\\mathbf{H}\\mathbf{u}^{1}$ , Changqing Zhang1\u2217 ", "page_idx": 0}, {"type": "text", "text": "College of Intelligence and Computing, Tianjin University1 Institute of High Performance Computing, A\\*STAR2 College of Computer Science, Sichuan University3 {yaohaiyun, zongbo, huqinghua, zhangchangqing} $@$ tju.edu.cn, hzfu@ieee.org, pengx.gm@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Out-of-distribution (OOD) detection is crucial for ensuring reliable deployment of machine learning models. Recent advancements focus on utilizing easily accessible auxiliary outliers (e.g., data from the web or other datasets) in training. However, we experimentally reveal that these methods still struggle to generalize their detection capabilities to unknown OOD data, due to the limited diversity of the auxiliary outliers collected. Therefore, we thoroughly examine this problem from the generalization perspective and demonstrate that a more diverse set of auxiliary outliers is essential for enhancing the detection capabilities. However, in practice, it is difficult and costly to collect sufficiently diverse auxiliary outlier data. Therefore, we propose a simple yet practical approach with a theoretical guarantee, termed Diversity-induced Mixup for OOD detection (diverseMix), which enhances the diversity of auxiliary outlier set for training in an efficient way. Extensive experiments show that diverseMix achieves superior performance on commonly used and recent challenging large-scale benchmarks, which further confirm the importance of the diversity of auxiliary outliers. Our code is available at https://github.com/HaiyunYao/diverseMix. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The OOD problem occurs when machine learning models encounter data that differs from the distribution of training data. In such scenarios, models may make incorrect predictions, leading to safety-critical issues in real-world applications, e.g., autonomous driving [14] and medical diagnosis [28]. To ensure the reliability of the outputs of model, it is essential not only to achieve good performance on in-distribution (ID) samples, but also to detect potential OOD samples, thus avoiding making erroneous decisions in test. Therefore, OOD detection has become a critical challenge for the secure deployment of machine learning models [1, 12, 25, 30]. ", "page_idx": 0}, {"type": "text", "text": "Several significant studies [19, 24, 26] focus on detecting OOD examples using only ID data in training. However, due to a lack of supervision information from unknown OOD data, it is difficult for these methods to achieve satisfactory performance in detecting OOD samples. Recent methods [20, 46, 6, 35] involve training model with easily available auxiliary outliers (e.g., data from the web or other datasets), with the hope that the detection ability can generalize to unknown OOD. However, as shown in Fig. 1(a)-(b), we experimentally find that while the use of outlier datasets can enhance performance in OOD detection, the generalization capabilities of these methods remain significantly limited. Specifically, there is a considerable risk of the model overfitting to the auxiliary outliers, ", "page_idx": 0}, {"type": "image", "img_path": "C1hiRbzEH9/tmp/afc37cfcf154b2fedce82dd3c5e27ec3f7ee7a5fcd4dd7d5bfc93ab5c2f62c14.jpg", "img_caption": ["(b) with auxiliary outilers (c) with diverse outilers ( number of components : 10 ) ( number of components : 1000 ) ( number of components : 10 ) "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: OOD score for different training strategies. The ID data $\\mathcal{X}_{i n}\\subset\\mathbb{R}^{2}$ is sampled from three distinct Gaussian distributions, each representing a different class. The auxiliary outliers are sampled from a Gaussian mixture model away from the ID data, where the number of mixture components indicates the number of classes contained in auxiliary outliers. (a) The model trained without auxiliary outliers fails to detect OOD. (b) Incorporating a less diverse set of auxiliary outliers (10 classes) during training enables partial OOD detection, but overfits auxiliary outliers. (c) OOD detection is improved with a more diverse set of auxiliary outliers (1000 classes). (d) diverseMix enriches the diversity of outliers (10 classes) through creating significantly distinct mixed outliers. ", "page_idx": 1}, {"type": "text", "text": "consequently failing to identify OOD samples that deviate markedly. The above limitation motivates the following important yet under-explored question: What are the theoretical principles underlying these methods that enable better utilization of outliers? ", "page_idx": 1}, {"type": "text", "text": "In this work, we theoretically investigate this crucial question from the perspective of generalization ability [3]. Specifically, we first conduct a theoretical analysis to demonstrate how the distribution shift between auxiliary outlier training set and test OOD data affects the generalization capability of OOD detector. Accordingly, a generalization bound is induced on the test-time OOD detection error of classifier, considering both empirical error and the error caused by the distribution shift between test OOD data and auxiliary outliers. Based on the theory, we deduce an intuitive conclusion that $a$ more diverse set of auxiliary outliers can reduce the distribution shift error and effectively lower the upper bound of the OOD detection error. As shown in Figure 1(b)-(c), the model trained with a more diverse set of auxiliary outliers achieves better OOD detection. However, in practice, it is difficult and costly to collect sufficiently diverse outlier data. Therefore, a natural question arises - how to guarantee the effective utilization of a fixed set of auxiliary outliers? ", "page_idx": 1}, {"type": "text", "text": "Inspired from the theoretical principles, we propose a simple yet effective method called Diversityinduced Mixup (diverseMix) for OOD detection, which introduces and improves the mixup strategy to enhance the outlier diversity. Specifically, diverseMix employs semantic-level interpolation to generate mixed samples, creating new outliers that significantly deviate from their original counterparts. Given the risk that a random interpolation strategy (merely sampling from a predefined prior distribution) might produce mixed outliers that are unhelpful for the model (as the model can already detect them effectively), diverseMix dynamically adjusts its interpolation strategy based on original samples. This adjustment ensures that the generated outliers are novel and distinct from those previously encountered by the model, thereby enhancing diversity throughout the training process. As shown in Figure 1(b)-(d), diverseMix effectively boosts the diversity of auxiliary outliers, leading to improved OOD detection performance. The contributions of this paper are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We provide a theoretical analysis of the generalization error linked to methods trained with auxiliary outliers. By establishing an upper bound for expected error, we reveal the connection between auxiliary outlier diversity and the upper bound of OOD detection error. Our theoretical insights emphasize the importance of leveraging diverse auxiliary outliers to enhance the generalization capacity of the OOD detector. \u2022 Constrained by the prohibitive cost of collecting outliers with sufficient diversity, we propose the Diversity-induced Mixup (diverseMix) for OOD detection, a simple yet effective strategy which is theoretically guaranteed to improve OOD detection performance. \u2022 The proposed diverseMix achieves state-of-the-art OOD detection performance, outperforming existing methods on both standard and recent large-scale benchmarks. Remarkably, our method exhibits significant improvements over advanced methods, showing relative performance improvements of $24.4\\%$ and $43.8\\%$ (in terms of FPR95) on the CIFAR-10 and CIFAR-100 datasets, respectively. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We provide a brief review of prior research relevant to our work followed by a comparison. ", "page_idx": 2}, {"type": "text", "text": "Auxiliary-Outlier-Free OOD Detection. One early work by [19] pioneered the field of OOD detection, introducing a baseline method based on maximum softmax probability. However, it has since been established, as noted by [37], that this approach is not quite suitable for OOD detection. To address this, various methods have been developed that operate in the logit space to enhance OOD detection. These include ODIN [26], energy score [46, 29, 45], ReAct [41], logit normalization [48], Mahalanobis distance [24], and KNN-based scoring [42]. However, post-hoc OOD detection methods that do not involve pre-training on a substantial dataset generally exhibit poorer performance compared to methods that leverage auxiliary datasets for model regularization [13]. ", "page_idx": 2}, {"type": "text", "text": "OOD Detection with Auxiliary Outliers. Recent advancements in OOD detection have focused on incorporating easily available auxiliary outliers into the model regularization process. Outlier exposure [20] encourage models to predict uniform distributions for outliers, and Energy-based learning [46] widens the energy gap between ID and OOD distribution. However, performance heavily depends on outlier quality. ATOM [6], POEM [35], and DOS [23] enhance performance by improving the sampling strategy for auxiliary outliers. DivOE [59] and DAL [47] improve outlier quality in a learnable manner, either in the sample space or feature space, respectively. Additionally, DOE implicitly enhances outlier informativeness through model perturbation. Incorporating outliers during training often achiveves superior performance, as shown in many other works [46, 40, 2, 48]. ", "page_idx": 2}, {"type": "text", "text": "Comparison with Existing Methods. Several existing methods have explored the utilization of mixup in OOD detection. MixOE [56] and OpenMix [58] perform mixup between ID data and outliers, linearly representing the transition from ID to OOD and thus enhancing the model capturing the uncertainty from outliers. Meanwhile, MixOOD [51] employ mixup on ID data to generate outliers for training. Different from existing research which primarily focuses on refining mixup strategy or designing outlier regularization method, we place emphasis on the theoretical significance of auxiliary outlier diversity. Our approach advances this concept by enhancing outlier diversity via mixup based strategy, guaranteed by a robust theoretical framework. This focus on enhancing the diversity of auxiliary outliers distinguishes our research from prevailing studies in this area. ", "page_idx": 2}, {"type": "text", "text": "3 Theory: Diverse Auxiliary Outliers Boost OOD Detection ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we lay the foundation for our analysis of OOD detection. We begin by introducing the key notations for OOD detection in Sec. 3.1. In Sec. 3.2, we establish a generalization bound which highlights the critical role for auxiliary outliers in influencing the generalization capacity of OOD detection methods. Finally, in Sec. 3.3, we demonstrate how a diverse set of auxiliary outliers effectively mitigate the distribution shift errors, consequently lowering the upper bound of error. For detailed proofs, please refer to Appendix A. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the multi-class classification task and each sample in the training set $\\boldsymbol{\\mathcal{D}}_{i d}=\\{(x_{i},y_{i})\\}_{i=1}^{N}$ is drawn i.i.d. from the joint distribution $\\mathcal{P}_{\\mathcal{X}_{i d}\\times\\mathcal{Y}_{i d}}$ , where $\\chi_{i d}$ denotes the input space of ID data, and $\\mathcal{V}_{i d}=\\{1,2,\\ldots,K\\}$ represents the label space. OOD detection can be formulated as a binary classification problem to learn a hypothesis $h$ from hypothesis space ${\\mathcal{H}}\\,\\subset\\,\\{h\\,:\\,\\mathcal{X}\\,\\rightarrow\\,\\{0,1\\}\\}$ such that $h$ outputs 1 for any $x\\in\\mathcal{X}_{i d}$ and 0 for any $x\\in\\mathcal{X}_{o o d}$ , where $\\mathcal{X}_{o o d}=\\mathcal{X}\\setminus\\mathcal{X}_{i d}$ represent the input space of OOD data and $\\mathcal{X}$ represents the entire input space in the open-world setting. To address the challenge posed by the unknown and arbitrariness of OOD distribution $\\mathcal{P}_{\\mathcal{X}_{o o d}}$ , we leverage an auxiliary dataset $\\mathcal{D}_{a u x}$ drawn from the distribution ${\\mathcal{P}}_{\\mathcal{X}_{a u x}}$ to serve as partial OOD data, where $\\mathcal{X}_{a u x}\\subset\\mathcal{X}_{o o d}$ . Due to the diversity of real-world OOD data, auxiliary outliers cannot fully represent all OOD data, so $\\mathcal{P}_{\\chi_{a u x}}\\neq\\mathcal{P}_{\\chi_{o o d}}$ . We aim to train a model on data sampled from $\\mathcal{P}_{\\tilde{\\mathcal{X}}}=k_{t r a i n}\\mathcal{P}_{\\mathcal{X}_{i d}}\\!+\\!(1\\!-\\!k_{t r a i n})\\mathcal{P}_{\\mathcal{X}_{a u x}}$ to obtain a reliable hypothesis $h$ that can effectively generalize to the unknown test-time distribution $\\mathcal{P}_{\\chi}=k_{t e s t}\\mathcal{P}_{\\chi_{i d}}+(1-k_{t e s t})\\mathcal{P}_{\\chi_{o o d}}$ , where $k_{t r a i n}$ and $k_{t e s t}$ determine the proportion of $\\mathrm{ID}$ and OOD data used for training and testing, respectively. Note that $k_{t e s t}$ is unknown due to unpredictable test data distribution. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.2 Generalization Error Bound in OOD Detection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Basic Setting. We define an OOD label function which provides ground truth labels (OOD or ID) for inputs as $f:\\mathcal{X}\\to[0,1]$ . The expectation that a hypothesis $h$ disagrees with $f$ with respect to a distribution $\\mathcal{P}$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathcal{P}}(h,f)=E_{x\\sim\\mathcal{P}}[|h(x)-f(x)|].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The set of ideal hypotheses on the training data distribution $P_{\\widetilde{\\mathcal{X}}}$ and test-time data distribution $P_{\\mathcal{X}}$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{H}_{a u x}^{*}:h=\\arg\\operatorname*{min}_{h\\in\\mathcal{H}}\\epsilon_{P_{\\widetilde{x}}}(h,f),\\;\\mathcal{H}_{o o d}^{*}:h=\\arg\\operatorname*{min}_{h\\in\\mathcal{H}}\\epsilon_{P_{\\mathcal{X}}}(h,f),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and we define $h_{o o d}^{*}$ and $h_{a u x}^{*}$ as the element in $\\mathcal{H}_{o o d}^{*}$ and $\\mathcal{H}_{a u x}^{*}$ , respectively, which can be denoted as $h_{o o d}^{*}\\in\\mathcal{H}_{o o d}^{*}$ , $h_{a u x}^{*}\\in\\mathcal{H}_{a u x}^{*}$ . Considering that $\\mathcal{X}_{a u x}\\,\\subset\\,\\mathcal{X}_{o o d}$ , it follows that $\\mathcal{H}_{o o d}^{*}\\subseteq\\mathcal{H}_{a u x}^{*}\\;^{2}$ , reflecting the reality that hypotheses perform well on real-world OOD data also perform well on auxiliary outliers, conditioning on that auxiliary outliers are a subset of real-world OOD data. The generalization error of an OOD detector $h$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{GError}(h)=\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,f).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Now, we present our first main result regarding OOD detection (training with auxiliary outliers). ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Generalization Bound of OOD Detector). We let $\\mathcal{D}_{t r a i n}=\\mathcal{D}_{i d}\\cup\\mathcal{D}_{a u x}$ , consisting of $M$ samples. For any hypothesis $h\\in\\mathcal H$ and $0<\\delta<1$ , with a probability of at least $1-\\delta$ , the following inequality holds: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G E r r o r(h)\\leq\\underbrace{\\hat{\\epsilon}_{x\\sim\\mathcal{P}_{\\widetilde{\\mathcal{X}}}}(h,f)}_{e m p i r i c a l\\,e r r o r}+\\underbrace{\\epsilon(h,h_{a u x}^{*})}_{r e d u c i b l e\\,e r r o r}+\\underbrace{\\operatorname*{sup}_{k\\in\\mathcal{H}_{a u x}^{*}}\\epsilon_{x\\sim\\mathcal{P}_{\\mathcal{X}}}(h,h_{o o d}^{*})}_{d i s t r i b u t i o n\\,s h i f t\\,e r r o r}+\\underbrace{\\mathcal{R}_{m}(\\mathcal{H})}_{c o m p l e x i t y}+\\sqrt{\\frac{\\ln(\\frac{1}{\\delta})}{2M}}+\\beta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\epsilon}_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)$ is the empirical error. We define $\\begin{array}{r}{\\epsilon(h,h_{a u x}^{*})\\,=\\,\\int|\\phi_{X}(x)-\\phi_{\\tilde{X}}(x)||h(x)\\,-\\,}\\end{array}$ $h_{a u x}^{*}(x)|d x$ as the reducible error, where $\\phi_{\\mathcal{X}}$ and $\\phi_{\\widetilde{X}_{.}}$ is the density function of $\\mathcal{P}_{\\mathcal{X}}$ and $\\mathcal{P}_{\\tilde{\\mathcal{X}}}$ respectively. sup $\\epsilon_{x\\sim\\mathcal{P}_{\\mathcal{X}}}(h,h_{o o d}^{*})$ is the distribution sh ift error, $\\mathcal{R}_{m}(\\mathcal{H})$ represents the Rad emacher h\u2208H\u2217aux ", "page_idx": 3}, {"type": "text", "text": "complexity, and $\\beta$ is some constant condition on the error related to ideal hypotheses. ", "page_idx": 3}, {"type": "text", "text": "Minimizing empirical risk optimizes the model $h$ to $h\\in\\mathcal{H}_{a u x}^{*}$ , leading to a reduction in the reducible error, which tends to zero. However, the inherent distribution shift error between auxiliary outliers and real-world OOD data remains constant and non-negligible. This limitation fundamentally restricts the generalization of OOD detection methods trained with auxiliary outliers. To address this limitation, we investigate the effect of outlier diversity on mitigating the distribution shift error. ", "page_idx": 3}, {"type": "text", "text": "3.3 Generalization with Auxiliary OOD Diversification ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this paper, the diversity refers to semantic diversity, where a formal definition is given as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Diversity of Outliers). We assume $\\chi_{a u x}$ can be divided into distinct semantic groups: $\\mathcal{X}_{a u x}=\\mathcal{X}^{y_{1}}\\cup\\mathcal{X}^{y_{2}}\\cup..\\cup\\mathcal{X}^{y_{m}}$ , where each group $\\mathcal{X}^{y_{i}}$ contains data points with label $y_{i}$ . Considering a dataset $\\mathcal{D}_{d i v}$ sampled from the distribution $\\mathcal{P}_{\\mathcal{X}_{d i v}}$ , where $\\mathcal{X}_{d i v}\\,\\subset\\,\\mathcal{X}_{o o d}$ encompasses $\\chi_{a u x}$ and an additional group ${\\mathcal{X}}_{n e w}\\,=\\,{\\mathcal{X}}^{y_{m+1}}\\,.\\,.\\,.\\,\\cup\\,{\\mathcal{X}}^{y_{n}}$ with different semantic compared to $\\chi_{a u x}$ , i.e., $\\mathcal{X}_{d i v}=\\mathcal{X}_{a u x}\\cup\\mathcal{X}_{n e w}$ , we define $\\mathcal{D}_{d i v}$ is more diverse than $\\mathcal{D}_{a u x}$ in terms of the range of semantic classes covered. ", "page_idx": 3}, {"type": "text", "text": "Suppose we could use this diverse auxiliary outliers dataset for training, the ideal hypotheses achieved by training with $\\mathcal{D}_{d i v}$ are denoted as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{H}_{d i v}^{*}:h=\\arg\\operatorname*{min}_{h\\in\\mathcal{H}}\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}_{d i v}}}(h,f),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $\\mathcal{P}_{\\widetilde{\\chi}_{d i v}}\\;=\\;k_{t r a i n}\\mathcal{P}_{\\chi_{i d}}\\;+\\;(1\\,-\\,k_{t r a i n})\\mathcal{P}_{\\chi_{d i v}}$ . Because $\\mathcal{X}_{a u x}\\;\\subset\\;\\mathcal{X}_{d i v}$ holds, the hypotheses performing well on $\\mathcal{P}_{\\mathcal{X}_{d i v}}$ also perform well on $\\mathcal{P}_{\\mathcal{X}_{a u x}}$ , giving rise to $\\mathcal{H}_{d i v}^{*}\\subset\\mathcal{H}_{a u x}^{*}$ . Consequently, we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{h\\in\\mathcal{H}_{d i v}^{*}}{s u p}\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,h_{o o d}^{*})\\le\\underset{h\\in\\mathcal{H}_{a u x}^{*}}{s u p}\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,h_{o o d}^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which indicates that training with a more diverse set of auxiliary outliers can reduce the distribution shift error. Furthermore, effective training leads to sufficient small empirical error and reducible error, and the intrinsic complexity of the model remains constant. Consequently, a more diverse set of auxiliary outliers results in a lower generalization error bound. This theorem is formalized as: ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Diverse Outliers Enhance Generalization). Let ${\\mathcal{O}}(G E r r o r(h))$ and $\\mathcal{O}(G E r r o r(h_{d i v}))$ represent the upper bounds of the generalization error of detector training with vanilla auxiliary outliers $\\mathcal{D}_{a u x}$ and diverse auxiliary outliers $\\mathcal{D}_{d i v}$ , respectively. For any hypothesis $h$ and $h_{d i v}$ in $\\mathcal{H}$ , and $0<\\delta<1$ , with a probability of at least $1-\\delta_{i}$ , the following inequality holds ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{O}(G E r r o r(h_{d i v}))\\leq\\mathcal{O}(G E r r o r(h)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark. Theorem 2 highlights that the diversity of the outlier set is a critical factor in reducing the upper bound of generalization error. However, despite the fundamental improvement in model generalization achieved by increasing the diversity of auxiliary outliers, collecting a more diverse set of auxiliary outliers is expensive, and the auxiliary outliers we can use are limited in practical scenarios, which hinders the application of outlier exposure based methods for OOD detection. This raises an intuitive question: can we enhance the diversity of a fixed outlier set for better utilization? ", "page_idx": 4}, {"type": "text", "text": "4 Method: Diversity-induced Mixup (diverseMix) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we show how diverseMix addresses the challenge of effective training when the outlier diversity is limited. We begin with a theoretical analysis demonstrating the effectiveness of mixup in enhancing outlier diversity to improve OOD detection performance, providing a reliable guarantee for our mixup-based method. Then, we introduce a simple yet effective framework implementing our method diverseMix to enhance OOD detection performance. ", "page_idx": 4}, {"type": "text", "text": "4.1 Theoretical Insights: Semantic Interpolation Guarantees Enhanced Diversity of Outliers ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Mixup [55] is a widely used machine learning technique to augment training data by creating synthetic samples, which has been extensively utilized in various studies[17, 7, 52]. It involves generating virtual training samples (referred to as mixed samples) through linear interpolations between data points and corresponding labels, given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{x}=\\lambda x_{i}+(1-\\lambda)x_{j},\\quad\\hat{y}=\\lambda y_{i}+(1-\\lambda)y_{j},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(x_{i},y_{i})$ and $(x_{j},y_{j})$ are two samples drawn randomly from the empirical training distribution, and $\\lambda\\in[0,1]$ is usually sampled from a Beta distribution with parameter $\\alpha$ denoted as $B e t a(\\alpha,\\alpha)$ . This technique assumes a linear relationship between semantics (labels) and features (in data), allowing us to create new mixed samples that deviate significantly from the semantics of the original ones by combining features from samples with distinct semantics. These new mixed samples are situated outside of the original data manifold [16]. We summarize this assumption as follows: ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 (Semantic Change under Mixup). Let $x_{i}$ and $x_{j}$ be any two data points from input spaces $\\mathcal{X}^{y_{i}}$ and $\\mathcal{X}^{y_{j}}$ , respectively, where $y_{i}$ and $y_{j}$ are corresponding semantic labels and $y_{i}\\neq y_{j}$ . If $\\zeta<\\lambda<1\\!-\\!\\zeta$ , then there exists a positive value $\\zeta$ such that the mixed data point $\\hat{x}=\\lambda x_{i}+(1-\\bar{\\lambda})x_{j}$ does not belong to either $\\mathcal{X}^{y_{i}}$ or $\\mathcal{X}^{y_{j}}$ . ", "page_idx": 4}, {"type": "text", "text": "This assumption suggests that we can enhance the diversity of outliers by generating new outliers with distinct semantics using mixup. Specifically, applying mixup to outliers in $\\chi_{a u x}$ results in some generated mixed outliers having different semantics, suggesting that they belong to novel (unknown or unnamed) semantic classes outside of $\\chi_{a u x}$ . Consequently, these mixed outliers can be considered as samples from a broader region within the input space. As per Definition 1, the mixed outliers exhibit greater diversity than the original outliers. This lemma is formally presented as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Lemma 1 (Diversity Enhancement with Mixup). For a group of mixup transforms3 $\\mathcal{G}$ acting on the input space $\\chi_{a u x}$ to generate an augmented input space $\\mathcal{G X}_{a u x}$ , defined as $\\mathcal{G}\\mathcal{X}_{a u x}=\\{\\hat{x}|\\hat{x}=$ $\\lambda x_{1}+(1-\\lambda)x_{2};x_{1},x_{2}\\in\\mathcal{X}_{a u x},\\lambda\\in[0,1]\\}$ , the following relation holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{X}_{a u x}\\subset\\mathcal{G}\\mathcal{X}_{a u x}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 1 establishes that mixed outliers $\\mathcal{D}_{m i x}$ exhibits greater diversity compared to $\\mathcal{D}_{a u x}$ , where $\\mathcal{D}_{m i x}$ is drawn from distribution $\\mathcal{P}_{\\mathcal{G}\\mathcal{X}_{a u x}}$ . Consequently, according to Theorem 2, mixup outliers contribute to a reduction in generalization error. We can formalize this relationship as follows, and the detailed proofs can be found in Appendix $A$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 (Mixed Outlier Enhances Generalization). Let ${\\mathcal{O}}(G E r r o r(h))$ and $\\mathcal{O}(G E r r o r(h_{m i x}))$ represent the upper bounds of the generalization error of detector training with vanilla auxiliary outliers $\\mathcal{D}_{a u x}$ and mixed auxiliary outliers $\\mathcal{D}_{m i x}$ , respectively. For any hypothesis $h$ and $h_{m i x}$ in $\\mathcal{H}$ , and $0<\\delta<1$ , with a probability of at least $1-\\delta$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{O}(G E r r o r(h_{m i x}))\\leq\\mathcal{O}(G E r r o r(h)).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3 demonstrates that mixup enhances auxiliary outlier diversity, reducing the upper bound of generalization error in OOD detection, which provides a reliable guarantee of mixup\u2019s effectiveness in improving OOD detection. However, the vanilla mixup lacks flexibility, which may generate outliers that are not necessarily beneficial to the model. Next, we will provide an implementation of our method which dynamically adjusts the interpolation strategy in a data-adaptive manner. ", "page_idx": 5}, {"type": "text", "text": "4.2 Implementation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Considering a classifier network $\\theta$ and $F(x,\\theta)$ denotes the logit outputs for input $x$ , our goal is to use the scoring function $S(x,\\theta)$ to develop an OOD detector: ", "page_idx": 5}, {"type": "equation", "text": "$$\nG(x)=\\Pi\\cdot\\mathbf{1}\\{S(x,\\theta)\\geq\\gamma\\}+\\mathrm{OOD}\\cdot\\mathbf{1}\\{S(x,\\theta)<\\gamma\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, $\\gamma$ is the threshold, typically chosen to ensure that a significant proportion (e.g., $95\\%$ ) of ID data is accurately identified. The training objective is given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\theta}~\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{i d}}[\\mathcal{L}_{\\mathrm{CE}}(F(x,\\theta),y)]+\\omega\\cdot\\mathcal{L}_{\\mathrm{aux}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{L}_{\\mathrm{CE}}(\\cdot)$ is the cross entropy loss, $\\mathcal{L}_{\\mathrm{aux}}$ serves as a regularization term enabling model to learn from auxiliary outliers with low-confidence predictions, and $\\omega$ controls the strength of regularization. ", "page_idx": 5}, {"type": "text", "text": "Our previous analysis showed that semantic interpolation can increase the diversity of outliers, thereby enhancing the model\u2019s OOD detection performance. However, the interpolation weights in vanilla mixup is randomly sampled from a preset prior distribution (e.g. beta distribution), which may result in generating mixed outliers that are not necessarily beneficial to the model. To efficiently increase the diversity of auxiliary outliers, we dynamically adjust the mixup strategy based on the original outliers, thereby generating novel mixed outliers which are more likely to be unfamiliar to the model. ", "page_idx": 5}, {"type": "text", "text": "During each training epoch, outliers are regularized, prompting the model $\\theta$ to assign lower scores to previously encountered outliers. Consequently, outliers that achieve higher scores $S(x,\\theta)$ are more likely to be novel or previously unseen outliers. We expect the generated outliers to be located in the vicinal space of the novel outliers that have not yet been encountered by the model. To achieve this, we adjust the prior distribution based on scores. Specifically, for outlier samples $x_{i}$ and $x_{j}$ randomly drawn from the empirical auxiliary outlier distribution, the mixed outliers are formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{x}=\\lambda x_{i}+(1-\\lambda)x_{j},\\;\\lambda\\sim\\mathtt{B e t a}(\\hat{s}_{i}\\alpha,\\hat{s}_{j}\\alpha),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{s}_{i},\\hat{s}_{j}$ adjusts the original Beta distribution according to $x_{i}$ and $x_{j}$ , which is defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{s}_{i}=\\frac{\\exp(S(x_{i},\\theta)/T)}{\\sum_{k\\in\\{i,j\\}}\\exp(S(x_{k},\\theta)/T)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 diverseMix for OOD Detection ", "page_idx": 6}, {"type": "text", "text": "Input: ID dataset $\\mathcal{D}_{\\mathrm{id}}$ , outlier dataset $\\mathcal{D}_{\\mathrm{aux}}$ , batch size $N$ , distribution parameter $\\alpha$ , temperature $T$ .   \nOutput: model parameters $\\theta$ . ", "page_idx": 6}, {"type": "text", "text": "for each iteration do ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "for each mini-batch do Sample $N$ ID data from $\\mathcal{D}_{\\mathrm{id}}$ as $\\mathcal{B}_{\\mathrm{id}}$ and $N$ outliers from $\\mathcal{D}_{\\mathrm{aux}}$ as $\\boldsymbol{\\beta}_{\\mathrm{aux}}$ , respectively. Evaluate the auxiliary outliers $\\beta_{\\mathrm{aux}}$ using the current model $\\theta$ to obtain the scores $\\boldsymbol{S}$ . Randomly shuffle $\\beta_{\\mathrm{aux}}$ and the corresponding scores $\\boldsymbol{S}$ to generate $\\mathcal{B}_{\\mathrm{aux}}^{\\prime}$ and $S^{\\prime}$ . Generate prior adjustment strategies based on scores $\\boldsymbol{S}$ and $S^{\\prime}$ according to Eq. 14. Sample the interpolation weight from the adjusted prior distribution and generate mixed outliers $\\mathcal{D}_{m i x}$ according to Eq. 13. Train the model $\\theta$ using the objective function defined in Eq. 12.   \nend for ", "page_idx": 6}, {"type": "text", "text": "with $T$ representing the temperature parameter. This adaptive strategy assigns higher weights to the outliers that contain more information unknown to the current model, ensuring the generation of novel outliers, thereby increasing diversity throughout the training process. After constructing the mixed auxiliary outliers, they are used for the training objective (12). The whole pseudo code of the proposed method is shown in Alg. 1. ", "page_idx": 6}, {"type": "text", "text": "Compatibility with different OOD regularization method. DiverseMix is a general method that is suitable for a series of OOD regularization methods. One representative method is the energy-based method [46], which employs the following OOD regularization loss: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a u x}=\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{i d}}[(\\operatorname*{max}(0,m_{i n}-S(x;\\theta))^{2}]+\\mathbb{E}_{x\\sim\\mathcal{D}_{a u x}}[(\\operatorname*{max}(0,S(x;\\theta)-m_{o u t}))^{2}],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $m_{i n}$ and $m_{o u t}$ are margin hyperparameters, and $\\begin{array}{r}{S(x;\\theta)\\,=\\,\\log\\sum_{i=1}^{K}\\exp(F_{i}(x,\\theta))}\\end{array}$ is the corresponding scoring function. More details for regularization methods ar e provided in Appendix B.3. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we outline our experimental setup and conduct experiments on common OOD detection benchmarks to answer the following questions: Q1. Effectiveness (I): Does our method outperform its counterparts in OOD detection? Q2. Effectiveness (II): Does our method retain its superior performance across various settings including large-scale benchmark? Q3. Practicability (I): Does our method demonstrate effectiveness across different OOD regularization methods? Q4. Practicability (II): Does our method demonstrate effectiveness in low-quality of auxiliary outliers dataset? Q5. Ablation study: (I) Does diverseMix truly offer a distinct advantage over other data augmentation methods? (II) What is the key factor contributing to performance improvement in our method? Q6. Reliability: Do the experimental results provide strong support for established theory? ", "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We briefly present the experimental setup here, including the experimental datasets and evaluation metrics. Further experimental details can be found in Appendix B. It is worth noting that we are committed to open-sourcing the code related to our research after publication. ", "page_idx": 6}, {"type": "text", "text": "Datasets. \u25e6ID datasets. Following the commonly used benchmark in OOD detection literature, we use CIFAR-10, CIFAR-100 and ImageNet-200 as ID datasets. \u25e6Auxiliary outlier datasets. For CIFAR experiments, the downsampled version of ImageNet (ImageNet-RC) is employed as auxiliary outliers. For ImageNet-200 experiments, the remaining 800 categories from ImageNet-1k (ImageNet-800) serve as auxiliary outliers. \u25e6OOD test sets. For CIFAR benchmark, we use diverse datasets including SVHN [38], Textures [8], Places365 [57], LSUN-crop, LSUN-resize [53], and iSUN [49]. For ImageNet benchmark, We use datasets such as SSB-hard [43], NINCO [5], iNaturalist [21], Textures [8] and OpenImage- $o$ [44]. ", "page_idx": 6}, {"type": "text", "text": "Evaluation metrics. Following common practice, we report: (1) OOD false positive rate at $95\\%$ true positive rate for ID samples (FPR95) [27], (2) the area under the receiver operating characteristic curve (AUROC) [10], (3) the area under the precision-recall curve (AUPR) [32]. We also provide ID classification accuracy (ID-ACC). ", "page_idx": 6}, {"type": "table", "img_path": "C1hiRbzEH9/tmp/ef48061482c6bda7fe8bbcf53e42da0af5f59f477b171f954853f3a1482583c8.jpg", "table_caption": ["Table 1: Main results. Comparison with competitive OOD detection methods trained with the same DenseNet backbone. The performance metrics are averaged $(\\%)$ over six OOD test datasets from Section 5.1. The best results are in bold. diverseMix not only demonstrates state-of-the-art OOD detection performance on the CIFAR benchmark but also maintains high accuracy in $I D$ classification. More details are provided in the Appendix B. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "C1hiRbzEH9/tmp/bbd4a758922008c7e48dc70c0effb2ae950261d4bb10b9180787470f23fb5f13.jpg", "table_caption": ["Table 2: Main results on large-scale ImageNet benchmark. Comparison with competitive OOD detection methods trained with the same ResNet backbone. For better presentation, the best and second-best results are in bold and underline respectively. Consistent with CIFAR experiment results, diverseMix demonstrates strong OOD detection capabilities for both near-OOD and far-OOD test sets, achieving state-of-the-art OOD detection performance. Details are provided in the Appendix B. ", "5.2 Experimental Results and Discussion "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "DiverseMix achieves superior performance on the common benchmark (Q1). Our method outperforms existing competitive methods, establishing state-of-the-art performance both on CIFAR10 and CIFAR-100 datasets. Table 1 provides a comprehensive comparison with methods grouped into: (1) ID-only training: MSP [19], ODIN [26], Mahalanobis [24], Energy [46]; (2) Utilizing auxiliary outliers: $O E$ [20], SOFL [36], CCU [33], Energy with outliers [46], NTOM [6], POEM [35], MixOE [56], DivOE [59]. Methods that utilizing auxiliary outliers generally achieve significantly better empirical performance on OOD detection. This implies that leveraging auxiliary outliers is essential for enhancing OOD detection performance. Our method diverseMix significantly outperforms the top baseline, reducing the FPR95 by $0.62\\%$ and $6.63\\%$ on CIFAR-10 and CIFAR-100, respectively. These reductions correspond to relative error reductions of $24.4\\%$ and $43.8\\%$ . These notable improvements can be attributed to the enhanced diversity in auxiliary outliers offered by diverseMix, which lowers the generalization error bound and significantly improves the OOD detection performance. ", "page_idx": 7}, {"type": "text", "text": "DiverseMix is effective on the large-scale benchmark (Q2). Recent studies [50] have suggested that methods leveraging outlier data tend to underperform in more demanding, large-scale OOD detection tasks. To evaluate the effectiveness of our method, we conduct experiments on the ImageNet benchmark. Following[50], We categorize the OOD test set into two distinct groups: near-OOD and far-OOD. For each group, we report the average performance metrics. Furthermore, we also present the overall average performance on the OOD test sets. Table 2 illustrates that methods requiring outliers during training tend to excel in near OOD detection but fall short in far-OOD detection, sometimes even performing worse than methods that do not require outliers during training. Although MixOE improves far-OOD detection performance to some extent, it fails to fully leverage auxiliary outliers to enhance near-OOD detection. In contrast, our method not only maintains strong performance in near-OOD detection but also significantly improves performance in far-OOD scenarios. We speculate that the virtual auxiliary outlier data generated by diverseMix may be more representative of far-OOD data. While most OOD detection methods face difficulties in achieving satisfactory performance across both near-OOD and far-OOD, our method excels in detecting both types of OOD, significantly surpassing other methods in the average OOD detection performance. ", "page_idx": 7}, {"type": "image", "img_path": "C1hiRbzEH9/tmp/e1435ce79207ccd60db8e69cd0efdba5a6557a0ac1753022ad1930a1cc4ccb2e.jpg", "img_caption": ["Figure 2: Comparison of OOD detection performance on CIFAR-100 with decreased quality of auxiliary outlier datasets (a) With constant diversity of auxiliary outliers (1000 categories), the dataset size is decreased. The $\\mathbf{X}$ -axis represents the percentage of the original outlier dataset\u2019s size used for training. (b) With fixed dataset size ( $10\\%$ of auxiliary outliers), the diversity of outliers is decreased, with the x-axis displaying the number of categories. See Appendix B.5 for more details. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "C1hiRbzEH9/tmp/72b9ecc5234d319aa5664092ecba870b4d78b301691d83d42e974b4a8ac57900.jpg", "table_caption": ["Table 3: Ablation study. Performance are averaged $(\\%)$ over six OOD test datasets from Section 5.1. The best results are in bold. More details about the comparison methods are provided in Appendix B (a) different data augmentation method. (b) different semantic interpolation strategy. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "DiverseMix is a general method that achieves good performance across different OOD regularization methods (Q3). To investigate the generality of diverseMix across different OOD regularization methods, we replace the original energy loss with the $K\\!+\\!l$ loss and the OE loss. The experimental results presented in Figure 2 reveal that diverseMix achieves consistent effectiveness regardless of the OOD regularization method employed. These findings not only suggest the versatility of our method but also provide substantial empirical evidence supporting our theoretical framework. ", "page_idx": 8}, {"type": "text", "text": "DiverseMix remains effective even when the auxiliary outlier data is of low quality (Q4). In Figure 2, the quality of auxiliary outliers used for training is decreased by gradually decreasing their quantity or their diversity. Our method diverseMix consistently outperforms previous methods by enhancing the diversity of auxiliary outliers across different dataset sizes and diversity levels. This suggests that diverseMix remains effective even when the auxiliary outliers are of low quality. ", "page_idx": 8}, {"type": "text", "text": "Sample adaptive semantic interpolation contributing to unique advantages of diverseMix (Q5). We compared diverseMix with other data augmentation methods. As shown in Table 3(a), diverseMix demonstrates superior performance for OOD detection over other data augmentation methods that preserve the semantics of outliers. Additionally, the ablation study in Table 3(b) compares diverseMix with different mixup strategies. DiverseMix outperforms both vanilla mixup and cutmix by adaptively adjusting its interpolation strategy based on the given outliers, thereby efficiently generating novel mixed outlier samples to enhance diversity. The advantages of diverseMix lie in 1) enhancing the diversity of outliers at the semantic level, and 2) efficiently boosting diversity by adaptively adjusting its strategy for the given outlier samples. For detailed comparisons, please see Appendix B.6. ", "page_idx": 8}, {"type": "text", "text": "Our theory effectively demonstrates that the diversity of auxiliary outliers is a key factor to ensure OOD detection performance (Q6). In Figure 2, when maintaining the diversity relatively constant and changing the quantity of data, the performance of different methods remains relatively stable. However, when the number of outliers is fixed and the diversity of the outliers dataset is reduced, there is a significant decrease in performance across all methods. This suggests that diversity is a key quality factor for the auxiliary outliers, providing substantial empirical support for our theory. ", "page_idx": 8}, {"type": "text", "text": "DiverseMix has the potential for application across a wide range of task domains. Our theory is not rely on any assumptions specific to the task domain. Given the successful implementation of mixup across different fields [15, 54], diverseMix also has the potential for application in multiple task domains beyond just computer vision tasks. We have investigated the application of diverseMix in the NLP domain through experiments. For additional details, please see Appendix C.2. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusions and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we demonstrate that the performance of OOD detection methods is hindered by the distribution shift between unknown test OOD data and auxiliary outliers. Through rigorous theoretical analysis, we demonstrate that enhancing the diversity of auxiliary outliers can effectively mitigate this problem. Constrained by limited access to auxiliary outliers and the high cost of data collection, we introduce diverseMix, an effective method that enhances the diversity of auxiliary outliers and significantly improves model performance. The effectiveness of diverseMix is supported by both theoretical analysis and empirical evidence. Furthermore, our theory enables future research to design new OOD detection method. We hope that our research can bring more attention to the diversity in OOD detection. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (Grant No.62376193), the National Science Fund for Distinguished Young Scholars (Grant No.61925602) and the H. Fu\u2019s Agency for Science, Technology and Research (A\\*STAR) Central Research Fund (\u201cRobust and Trustworthy AI system for Multi-modality Healthcare\u201d). The authors also appreciate the suggestions from NeurIPS anonymous peer reviewers. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.   \n[2] Yichen Bai, Zongbo Han, Bing Cao, Xiaoheng Jiang, Qinghua Hu, and Changqing Zhang. Idlike prompt learning for few-shot out-of-distribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \n[3] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 2002.   \n[4] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. Advances in Neural Information Processing Systems, 2006. [5] Julian Bitterwolf, Maximilian Mueller, and Matthias Hein. In or out? fixing imagenet outof-distribution detection evaluation. In ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models, 2023.   \n[6] Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Atom: Robustifying out-ofdistribution detection using outlier mining. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2021.   \n[7] Hsin-Ping Chou, Shih-Chieh Chang, Jia-Yu Pan, Wei Wei, and Da-Cheng Juan. Remix: Rebalanced mixup. In European Conference on Computer Vision, 2020.   \n[8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Conference on Computer Vision and Pattern Recognition, 2014.   \n[9] Koby Crammer, Michael Kearns, and Jennifer Wortman. Learning from multiple sources. Advances in Neural Information Processing Systems, 2006.   \n[10] Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In International Conference on Machine Learning, 2006.   \n[11] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.   \n[12] Thomas G Dietterich. Steps toward robust artificial intelligence. Ai Magazine, 2017.   \n[13] Stanislav Fort, Jie Jessie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-ofdistribution detection. In Advances in Neural Information Processing Systems, 2021.   \n[14] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition, 2012.   \n[15] Hongyu Guo. Nonlinear mixup: Out-of-manifold data augmentation for text classification. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020.   \n[16] Hongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regularization. In Proceedings of the AAAI conference on artificial intelligence, 2019.   \n[17] Zongbo Han, Zhipeng Liang, Fan Yang, Liu Liu, Lanqing Li, Yatao Bian, Peilin Zhao, Bingzhe Wu, Changqing Zhang, and Jianhua Yao. Umix: Improving importance weighting for subpopulation shift via uncertainty-aware mixup. Advances in Neural Information Processing Systems, 2022.   \n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, 2016.   \n[19] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2017.   \n[20] Dan Hendrycks, Mantas Mazeika, and Thomas G. Dietterich. Deep anomaly detection with outlier exposure. In International Conference on Learning Representations, 2019.   \n[21] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alexander Shepard, Hartwig Adam, Pietro Perona, and Serge J. Belongie. The inaturalist species classification and detection dataset. In Conference on Computer Vision and Pattern Recognition, 2018.   \n[22] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Conference on Computer Vision and Pattern Recognition, 2017.   \n[23] Wenyu Jiang, Hao Cheng, MingCai Chen, Chongjun Wang, and Hongxin Wei. Dos: Diverse outlier sampling for out-of-distribution detection. In International Conference on Learning Representations, 2023.   \n[24] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems, 2018.   \n[25] Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou. Trustworthy ai: From principles to practices. ACM Computing Surveys, 2023.   \n[26] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In International Conference on Learning Representations, 2018.   \n[27] Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In International Conference on Learning Representations, 2018.   \n[28] Weixin Liang, Girmaw Abebe Tadesse, Daniel Ho, L. Fei-Fei, Matei Zaharia, Ce Zhang, and James Zou. Advances, challenges and opportunities in creating data for trustworthy AI. Nature Machine Intelligence, 2022.   \n[29] Ziqian Lin, Sreya Dutta Roy, and Yixuan Li. Mood: Multi-level out-of-distribution detection. In Conference on Computer Vision and Pattern Recognition, 2021.   \n[30] Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Yaxin Li, Shaili Jain, Yunhao Liu, Anil Jain, and Jiliang Tang. Trustworthy AI: A Computational Perspective. ACM Transactions on Intelligent Systems and Technology, 2023.   \n[31] Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural networks: A view from the width. Advances in neural information processing systems, 2017.   \n[32] Christopher Manning and Hinrich Schutze. Foundations of statistical natural language processing. MIT press, 1999.   \n[33] Alexander Meinke and Matthias Hein. Towards neural networks that provably know when they don\u2019t know. In International Conference on Learning Representations, 2019.   \n[34] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm language models. In International Conference on Learning Representations, 2018.   \n[35] Yifei Ming, Ying Fan, and Yixuan Li. Poem: Out-of-distribution detection with posterior sampling. In International Conference on Machine Learning, 2022.   \n[36] Sina Mohseni, Mandar Pitale, JBS Yadawa, and Zhangyang Wang. Self-supervised learning for generalizable out-of-distribution detection. In The AAAI Conference on Artificial Intelligence, 2020.   \n[37] Peyman Morteza and Yixuan Li. Provable guarantees for understanding out-of-distribution detection. In The AAAI Conference on Artificial Intelligence, 2022.   \n[38] Yuval Netzer, Tao Wang, Adam Coates, A. Bissacco, Bo Wu, and A. Ng. Reading digits in natural images with unsupervised feature learning. 2011.   \n[39] Salah Rifai, Xavier Glorot, Yoshua Bengio, and Pascal Vincent. Adding noise to the input of a model trained with a regularized objective. 2011.   \n[40] Vikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for self-supervised outlier detection. International Conference on Learning Representations, 2021.   \n[41] Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified activations. In Advances in Neural Information Processing Systems, 2021.   \n[42] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In International Conference on Machine Learning, 2022.   \n[43] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need? arXiv preprint arXiv:2110.06207, 2021.   \n[44] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In Conference on Computer Vision and Pattern Recognition, 2022.   \n[45] Haoran Wang, Weitang Liu, Alex Bocchieri, and Yixuan Li. Can multi-label classification networks know what they don\u2019t know? In Advances in Neural Information Processing Systems, 2021.   \n[46] Haoran Wang, Weitang Liu, Alex Bocchieri, and Yixuan Li. Energy-based out-of-distribution detection for multi-label classification. International Conference on Learning Representations, 2021.   \n[47] Qizhou Wang, Zhen Fang, Yonggang Zhang, Feng Liu, Yixuan Li, and Bo Han. Learning to augment distributions for out-of-distribution detection. Advances in Neural Information Processing Systems, 2024.   \n[48] Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, and Yixuan Li. Mitigating neural network overconfidence with logit normalization. In International Conference on Learning Representations, 2022.   \n[49] Pingmei Xu, Krista A. Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R. Kulkarni, and Jianxiong Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. 2015.   \n[50] Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, WenXuan Peng, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Dan Hendrycks, Yixuan Li, and Ziwei Liu. OpenOOD: Benchmarking generalized out-ofdistribution detection. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.   \n[51] Taocun Yang, Yaping Huang, Yanlin Xie, Junbo Liu, and Shengchun Wang. Mixood: Improving out-of-distribution detection with enhanced data mixup. ACM Transactions on Multimedia Computing, Communications and Applications, 2023.   \n[52] Huaxiu Yao, Yiping Wang, Linjun Zhang, James Y Zou, and Chelsea Finn. C-mixup: Improving generalization in regression. Advances in neural information processing systems, 2022.   \n[53] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. 2015.   \n[54] Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, and Jinhyung Kim. Videomix: Rethinking data augmentation for video classification. arXiv preprint arXiv:2012.03457, 2020.   \n[55] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018.   \n[56] Jingyang Zhang, Nathan Inkawhich, Randolph Linderman, Yiran Chen, and Hai Li. Mixture outlier exposure: Towards out-of-distribution detection in fine-grained environments. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023.   \n[57] Bolei Zhou, Aditya Khosla, \u00c0gata Lapedriza, Antonio Torralba, and Aude Oliva. Places: An image database for deep scene understanding. 2016.   \n[58] Fei Zhu, Zhen Cheng, Xu-Yao Zhang, and Cheng-Lin Liu. Openmix: Exploring outlier samples for misclassification detection. In Conference on Computer Vision and Pattern Recognition, 2023.   \n[59] Jianing Zhu, Yu Geng, Jiangchao Yao, Tongliang Liu, Gang Niu, Masashi Sugiyama, and Bo Han. Diversified outlier exposure for out-of-distribution detection via informative extrapolation. Advances in Neural Information Processing Systems, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Theoretical Analysis 14 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of $\\mathcal{H}_{o o d}^{*}\\subseteq\\mathcal{H}_{a u x}^{*}$ 14   \nA.2 Proof of Theorem 1 15   \nA.3 Proof of Theorem 2 17   \nA.4 Proof of Lemma 1 . 18 ", "page_idx": 13}, {"type": "text", "text": "B Experimental Details 18 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Details of Dataset 18   \nB.2 Training Details. 19   \nB.3 Details of OOD Regularization Method. 19   \nB.4 Details of Main Experiment. . . 19   \nB.5 Details of Figure 2. . . 20   \nB.6 Details of Q5 Ablation Study. . 20 ", "page_idx": 13}, {"type": "text", "text": "C Additional Results 21 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Hyperparameter Analysis. 21   \nC.2 DiverseMix for OOD Detection in Natural Language Processing. . . 22   \nC.3 Experiments on Computational Cost. . . . 22   \nC.4 Impact Statements . . . 22 ", "page_idx": 13}, {"type": "text", "text": "D Hardware and Software 23 ", "page_idx": 13}, {"type": "text", "text": "A Theoretical Analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide detailed proofs of our theories and the proposed method, including the proof of $\\mathcal{H}_{o o d}^{*}\\subseteq\\mathcal{H}_{a u x}^{*}$ , the establishment of the generalization error bound for OOD detection (Theorem 1), a more diverse set of auxiliary outliers leads to a reduced generalization error (Theorem 2), and the proof of diversity enhancement with mixup (Lemma 1). ", "page_idx": 13}, {"type": "text", "text": "A.1 Proof of $\\mathcal{H}_{o o d}^{*}\\subseteq\\mathcal{H}_{a u x}^{*}$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we demonstrate that if $\\mathcal{H}$ consist of fully-connected ReLU network with width $d_{m}\\,\\leq\\,n\\,+\\,4$ , where $n$ is the input dimension, and given that that $\\mathcal{X}_{a u x}\\,\\subset\\,\\mathcal{X}_{o o d}$ , it follows that $\\mathcal{H}_{o o d}^{*}\\subseteq\\mathcal{H}_{a u x}^{*}$ , This reflects the reality that hypotheses perform well on real-world OOD data also perform well on auxiliary outliers, conditioning on that auxiliary outliers are a subset of real-world OOD data. ", "page_idx": 13}, {"type": "text", "text": "Proof. We first express the expected error of hypotheses $h$ on the training data distribution $\\mathcal{P}_{\\widetilde{\\mathcal{X}}}$ and the unknown test-time data distribution $\\mathcal{P}_{\\mathcal{X}}$ as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\mathcal{X}_{a d}}^{\\epsilon_{\\mathcal{P}}}(h,f)=\\int_{\\widetilde{X}}|h(x)-f(x)|d x=\\int_{\\mathcal{X}_{a u x}}|h(x)-f(x)|d x+\\int_{\\mathcal{X}_{i d}}|h(x)-f(x)|d x=\\epsilon_{1},}\\\\ &{\\int_{\\mathcal{X}_{a d}\\backslash\\mathcal{X}_{a u x}}|h(x)-f(x)|d x=\\epsilon_{2},}\\\\ &{\\epsilon_{\\mathcal{P}_{X}}(h,f)=\\int_{\\mathcal{X}}|h(x)-f(x)|d x=\\int_{\\mathcal{X}_{i d}}|h(x)-f(x)|d x+\\int_{\\mathcal{X}_{o u d}}|h(x)-f(x)|d x}\\\\ &{=\\int_{\\mathcal{X}_{i d}}|h(x)-f(x)|d x+\\int_{\\mathcal{X}_{a u x}}|h(x)-f(x)|d x+\\int_{\\mathcal{X}_{o u d}\\backslash\\mathcal{X}_{a u x}}|h(x)-f(x)|d x=\\epsilon_{1}+\\epsilon_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From the above expressions, we obtain: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathcal{H}_{a u x}^{*}=\\{h:\\arg\\operatorname*{min}_{h}\\epsilon_{1}\\},}\\\\ {\\mathcal{H}_{o t h e r}^{*}=\\{h:\\arg\\operatorname*{min}_{h}\\epsilon_{2}\\},}\\\\ {\\mathcal{H}_{o o d}^{*}=\\{h:\\arg\\operatorname*{min}_{h}(\\epsilon_{1}+\\epsilon_{2})\\}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $f^{\\prime}$ be a function that minimizes both $\\epsilon_{1}$ and $\\epsilon_{2}$ , considering that $\\begin{array}{r}{\\int_{\\mathcal{X}}|f^{\\prime}(x)|d x\\,<\\,\\infty}\\end{array}$ , which implies that $f^{\\prime}$ is Lebesgue-integrable on $\\mathcal{X}$ . The $\\mathcal{H}$ represent the fully-connected ReLU networks with width $d_{m}\\leq n+4$ , where $n$ is the input dimension. According to the Universal Approximation Theorem for Width-Bounded ReLU Networks [31], for any $\\epsilon>0$ , there exists a $h\\in\\mathcal H$ such that: $\\begin{array}{r}{\\int_{\\mathcal{X}}|h(x)-f^{\\prime}(x)|d x\\;<\\;\\epsilon}\\end{array}$ . Consequently, there exists a hypothesis $h\\ \\in\\ {\\mathcal{H}}$ that simultaneously minimizes both $\\epsilon_{1}$ and $\\epsilon_{2}$ . leading to the condition $\\mathcal{H}_{a u x}^{*}\\cap\\mathcal{H}_{o t h e r}^{*}\\neq\\emptyset$ . In this case, we have $\\operatorname*{min}_{h}(\\epsilon_{1}\\,+\\,\\epsilon_{2})\\;=\\;\\operatorname*{min}_{h}\\epsilon_{1}\\,+\\,\\operatorname*{min}_{h}\\epsilon_{2}\\;$ . We denote $\\mathcal{H}_{o o d}^{*}\\;=\\;\\mathcal{H}_{a u x}^{*}\\;\\cap\\;\\mathcal{H}_{o t h e r}^{*}$ , thus establishing that $\\mathcal{H}_{o o d}^{*}\\subset\\mathcal{H}_{a u x}^{*}$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we analyze the generalization error of the OOD detector training with auxiliary outliers. First, we recall the setting from Sec. 3.1, our goal is to train a detector with auxiliary outliers that can perform well on real-world OOD data. In other words, we aim to train a model on data sampled from $\\mathcal{P}_{\\widetilde{\\chi}}\\,=\\,k_{t r a i n}\\mathcal{P}_{\\chi_{i d}}+(1-k_{t r a i n})\\mathcal{P}_{\\chi_{a u x}}$ to obtain a reliable hypothesis $h$ that can effectively gener a lize to the unknown test-time distribution $\\mathcal{P}_{\\chi}=k_{t e s t}\\mathcal{P}_{\\chi_{i d}}+\\bar{(1-k_{t e s t})}\\mathcal{P}_{\\chi_{o o d}}$ . ", "page_idx": 14}, {"type": "text", "text": "Next, we develop bounds on the OOD detection performance of a detector training with auxiliary outliers, which can be formulated as follow: ", "page_idx": 14}, {"type": "text", "text": "(Generalization Bound of OOD Detector). Let $\\mathcal{D}_{t r a i n}=\\mathcal{D}_{i d}\\cup\\mathcal{D}_{a u x}$ , consisting of $M$ samples. For any hypothesis $h\\in\\mathcal H$ and $0<\\delta<1$ , with a probability of at least $1-\\delta$ , the following inequality holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G E r r o r(h)\\leq\\underbrace{\\hat{\\epsilon}_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)}_{e m p i r i c a l\\,e r r o r}+\\underbrace{\\epsilon(h,h_{a u x}^{*})}_{r e d u c i b l e\\,e r r o r}+\\underbrace{\\operatorname*{sup}_{k\\in\\mathcal{H}_{a u x}^{*}}\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,h_{o o d}^{*})}_{d i s t r i b u t i o n\\,s h i f t\\,e r r o r}+\\underbrace{\\mathcal{R}_{m}(\\mathcal{H})}_{c o m p l e x i t y}+\\sqrt{\\frac{\\ln(\\frac{1}{\\delta})}{2M}}+\\beta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\hat{\\epsilon}_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)$ is the empirical error. We define $\\begin{array}{r}{\\epsilon(h,h_{a u x}^{*})\\,=\\,\\int|\\phi_{X}(x)-\\phi_{\\tilde{X}}(x)||h(x)\\,-\\,}\\end{array}$ $h_{a u x}^{*}(x)|d x$ is the reducible error, $\\phi_{\\mathcal{X}}$ and $\\phi_{\\widetilde{\\mathcal{X}}}$ is the density function of $\\mathcal{P}_{\\mathcal{X}}$ and $\\mathcal{P}_{\\widetilde{\\mathcal{X}}}$ respectively. sup $\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,h_{o o d}^{*})$ is the distribution shift color, $\\mathcal{R}_{m}(\\mathcal{H})$ represents the Rademacher complexity, $h{\\in}\\mathcal{H}_{a u x}^{\\bar{*}}$ ", "page_idx": 14}, {"type": "text", "text": "$\\beta$ is the error related to ideal hypotheses. The roadmap of our analysis is as follows: ", "page_idx": 14}, {"type": "text", "text": "Roadmap. We first show how to bound the OOD detection error in terms of the generalization error on $\\mathcal{P}_{\\widetilde{\\mathcal{X}}}$ and the maximum distribution shift error as well as the reducible error which can be reduced to a small value as the model is optimized. Then, we study the generalization bound from the perspective of Rademacher complexity. We use complexity-based learning theory to quantify the generalization error on $\\mathcal{P}_{\\widetilde{\\mathcal{X}}}$ . In the end, we bound the OOD detection generalization error in terms of the empirical error on the  training data, the reducible error, the maximum distribution shift error, and the complexity. We also provide detailed proof steps as follows: ", "page_idx": 14}, {"type": "text", "text": "Proof. This proof relies on the triangle inequality for classification error [4, 9], which implies that for any labeling functions $f_{1},\\,f_{2}$ , and $f_{3}$ , we have $\\bar{\\epsilon}(f_{1},f_{2})\\leq\\epsilon(f_{1},f_{3})+\\epsilon(f_{2},f_{3})$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{G E r r o r(h)=\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,f)\\qquad\\qquad\\qquad}\\\\ &{}&{\\qquad\\qquad\\leq\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,h_{o o d}^{*})+\\epsilon_{x\\sim\\mathcal{P}_{x}}(h_{o o d}^{*},f)\\qquad\\qquad}\\\\ &{}&{\\qquad=\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,h_{o o d}^{*})+\\epsilon_{x\\sim\\mathcal{P}_{x}}(h_{o o d}^{*},f)+\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,h_{o o d}^{*})-\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,h_{o o d}^{*})\\qquad\\qquad}\\\\ &{}&{\\qquad=\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,h_{o o d}^{*})+\\epsilon_{x\\sim\\mathcal{P}_{x}}(h_{o o d}^{*},f)+\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,h_{o o d}^{*})-\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,h_{o o d}^{*})}\\\\ &{}&{\\qquad\\leq\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)+\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h_{o o d}^{*},f)+\\epsilon_{x\\sim\\mathcal{P}_{x}}(h_{o o d}^{*},f)+\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,h_{o o d}^{*})-\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,h_{o o d}^{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $\\phi_{\\mathcal{X}}$ and $\\phi_{\\widetilde{\\mathcal{X}}}$ be the density functions of $\\mathcal{P}_{\\mathcal{X}}$ and $\\mathcal{P}_{\\widetilde{\\mathcal{X}}}$ , respectively. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\,G E^{\\mathrm{rer}}(b)\\leq\\omega_{c}\\gamma_{3}(h,f)+\\epsilon_{c}\\gamma_{3}(h,\\omega_{c}f)+\\epsilon_{c}\\gamma_{3}(h,\\omega_{c}f)+\\epsilon_{c}\\gamma_{3}(h,\\omega_{c}f)}\\\\ &{\\quad\\quad\\quad+\\int\\phi_{c}(x)[h(\\epsilon)-h_{c}^{\\mathrm{rer}}(x)](a)\\,\\mathrm{d}x-\\int\\phi_{c}(x)[h(\\epsilon)-h_{c}^{\\mathrm{rer}}(x)](a)}\\\\ &{\\quad\\quad\\quad\\forall\\,\\varepsilon-\\gamma_{3}(h,f)+\\epsilon_{c}\\gamma_{3}(h,\\omega_{c}f)+\\epsilon_{c}\\gamma_{3}(h,\\omega_{c}f)+\\int\\{\\mathrm{d}x(\\epsilon)-\\phi_{c}(x)[h(\\epsilon)-h_{c}^{\\mathrm{rer}}(x)](a)}\\\\ &{\\quad\\quad\\quad\\forall\\,\\varepsilon\\in[0,T]\\}+\\epsilon_{c}\\gamma_{3}(h,\\omega_{c}f)}\\\\ &{\\leq\\epsilon_{c}\\gamma_{3}(h,f)+\\epsilon_{c}\\gamma_{3}(h_{c}^{\\mathrm{rer}}(x)\\!,\\mathrm{d}x+\\epsilon_{c}\\gamma_{3}(h,\\omega_{c}f)+\\int\\{\\mathrm{d}x(\\epsilon)-\\phi_{c}(x)[h(\\epsilon)-h_{c}^{\\mathrm{rer}}(x)](a)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\int\\{\\mathrm{d}x(\\epsilon)-h_{c}^{\\mathrm{rer}}(x)\\}[h(\\epsilon)-h_{c}^{\\mathrm{rer}}(x)]\\:\\mathrm{d}x}\\\\ &{\\leq\\epsilon_{c}\\gamma_{3}(h,f)+\\epsilon_{c}\\gamma_{3}(h,\\omega_{c}f)+\\epsilon_{c}\\gamma_{3}(h,\\omega_{c}f)+\\int\\{\\mathrm{d}x(\\epsilon)-\\phi_{c}(x)[h(\\epsilon)-h_{c}^{\\mathrm{rer}}(x)](a)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\int\\phi_{c}(x)[h(\\epsilon)-h_{c}^{\\mathrm{ref}}(x)](a)}\\\\ &{=\\epsilon_{c}\\gamma_{3}(h,f)+\\epsilon_{c}\\gamma_{3}(h_{c}^{\\mathrm{ref}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given that $\\operatorname*{min}_{h\\in\\mathcal{H}}\\!\\!\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,f)$ and $\\operatorname*{min}_{h\\in\\mathcal{H}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!$ represent the error of $h_{o o d}^{*}$ and $h_{a u x}^{*}$ on distributions $\\mathcal{P}_{\\mathcal{X}}$ and $\\mathcal{P}_{\\widetilde{\\mathcal{X}}}$ , respectively, we have $\\epsilon_{x\\sim\\mathcal{P}_{\\mathcal{X}}}(h_{o o d}^{*},f)=\\operatorname*{min}_{h\\in\\mathcal{H}}\\epsilon_{x\\sim\\mathcal{P}_{\\mathcal{X}}}(h,f)$ and $\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h_{a u x}^{*},f)=$ $\\operatorname*{min}_{h\\in\\mathcal{H}}\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)$ . Considering that $\\mathcal{H}_{o o d}^{*}\\subset\\mathcal{H}_{a u x}^{*}$ , it follows that for any $h\\in\\mathcal{H}_{o o d}^{*}$ , $h\\in\\mathcal{H}_{a u x}^{*}$ is holds. As a result, we have $\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h_{o o d}^{*},f)=\\operatorname*{min}_{h\\in\\mathcal{H}}\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)$ . Thus, we obtain the following: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G E r r o r(h)\\leq\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)+\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)+\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\epsilon_{x\\sim\\mathcal{P}_{\\mathcal{X}}}(h,f)}\\\\ {+\\int|\\phi_{\\mathcal{X}}(x)-\\phi_{\\widetilde{x}}(x)|\\,|h(x)-h_{a u x}^{*}(x)|\\ d x+\\epsilon_{x\\sim\\mathcal{P}_{\\mathcal{X}}}(h_{a u x}^{*},h_{o o d}^{*})}\\\\ {+\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)+\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can demonstrate that $\\operatorname*{min}_{h\\in\\mathcal{H}}\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,f)\\geq\\operatorname*{min}_{h\\in\\mathcal{H}}\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\ensuremath{\\varepsilon}_{x\\sim\\mathcal{P}_{x}}(h,f)=\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\int_{\\mathcal{X}}|h(x)-f(x)|d x}\\\\ &{}&{=\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\left(\\int_{\\widetilde{X}}|h(x)-f(x)|d x+\\int_{\\mathcal{X}\\backslash\\widetilde{X}}|h(x)-f(x)|d x\\right)}\\\\ &{}&{\\ge\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\int_{\\widetilde{X}}|h(x)-f(x)|d x+\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\int_{\\mathcal{X}\\backslash\\widetilde{X}}|h(x)-f(x)|d x}\\\\ &{}&{\\ge\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\int_{\\widetilde{X}}|h(x)-f(x)|d x}\\\\ &{}&{=\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,f).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, We obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G E r r o r(h)\\leq\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)+\\int|\\phi_{\\mathcal{X}}(x)-\\phi_{\\widetilde{x}}(x)|\\,|h(x)-h_{a u x}^{*}(x)|\\,\\,d x+\\epsilon_{x\\sim\\mathcal{P}_{\\mathcal{X}}}(h_{a u x}^{*},h_{o o d}^{*})}\\\\ {+4\\,\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\epsilon_{x\\sim\\mathcal{P}_{\\mathcal{X}}}(h,f),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We denote $\\beta=4\\operatorname*{min}_{h\\in\\mathcal{H}}\\!\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,f)$ , so ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Im E r r o r(h)\\leq\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)+\\int|\\phi_{X}(x)-\\phi_{\\widetilde{x}}(x)||h(x)-h_{a u x}^{*}(x)|\\,d x+\\epsilon_{x\\sim\\mathcal{P}_{X}}(h_{a u x}^{*},h_{o o d}^{*})+\\beta,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consider an upper bound on the distribution shift error $\\epsilon_{x\\sim\\mathcal{P}_{\\mathcal{X}}}(h_{a u x}^{*},h_{o o d}^{*})$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G E r r o r(h)\\leq\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)+\\int|\\phi_{\\mathcal{X}}(x)-\\phi_{\\widetilde{x}}(x)||h(x)-h_{a u x}^{*}(x)|\\,d x}\\\\ {+\\underset{h\\in\\mathcal{H}_{a u x}^{*}}{\\operatorname*{sup}}\\epsilon_{x\\sim\\mathcal{P}_{\\mathcal{X}}}(h,h_{o o d}^{*})+\\beta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we recap the Rademacher complexity measure for model complexity. We use complexity-based learning theory [3] (Theorem 8) to quantify the generalization error. Let $\\mathcal{D}_{t r a i n}\\,=\\,\\mathcal{D}_{i d}\\cup\\mathcal{D}_{a u x}$ consisting of $M$ samples, $\\hat{\\epsilon}_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)$ is the empirical error of $h$ . Then for any hypothesis $h$ in $\\mathcal{H}$ (i.e., $\\mathcal{H}:\\mathcal{X}\\rightarrow\\{0,1\\},h\\in\\mathcal{H})$ and $1>\\delta>0$ , with probability at least $1-\\delta$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\epsilon_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)\\leq\\hat{\\epsilon}_{x\\sim\\mathcal{P}_{\\widetilde{x}}}(h,f)+\\mathcal{R}_{m}(\\mathcal{H})+\\sqrt{\\frac{l n(\\frac{1}{\\delta})}{2M}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathcal{R}_{m}(\\mathcal{H})$ is the Rademacher complexities. Finally, it holds with a probability of at least $1-\\delta$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,f)\\leq\\underbrace{\\hat{\\epsilon}_{x\\sim\\mathcal{P}_{\\mathcal{\\tilde{X}}}}(h,f)}_{\\mathrm{empirical~error}}+\\underbrace{\\epsilon(h,h_{a u x}^{*})}_{\\mathrm{reducible~error}}+\\underbrace{\\operatorname*{sup}_{k\\in\\mathcal{H}_{a u x}^{*}}\\epsilon_{x\\sim\\mathcal{P}_{\\mathcal{X}}}(h,h_{o o d}^{*})}_{\\mathrm{bequen}}+\\underbrace{\\mathcal{R}_{m}(\\mathcal{H})}_{\\mathrm{complexity}}+\\sqrt{\\frac{l n(\\frac{1}{\\delta})}{2M}}+\\beta\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\epsilon(h,h_{a u x}^{*})=\\int|\\phi_{X}(x)-\\phi_{\\tilde{X}}(x)|\\,|h(x)-h_{a u x}^{*}(x)|}\\end{array}$ $d x$ represents the reducible error and $\\beta$ is the error related to ideal hypotheses. Notably, when $\\beta$ is large, there exists no detector that performs well on $\\mathcal{P}_{\\mathcal{X}}$ , making it unfeasible to find a good hypothesis through training with auxiliary outliers. ", "page_idx": 16}, {"type": "text", "text": "A.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we proof that diverse outliers enhance generalization, which can be formulated as follows: ", "page_idx": 16}, {"type": "text", "text": "Let ${\\mathcal{O}}(G E r r o r(h))$ and $\\mathcal{O}(G E r r o r(h_{d i v}))$ represent the upper bounds of the generalization error of detector training with vanilla auxiliary outliers $\\mathcal{D}_{a u x}$ and diverse auxiliary outliers $\\mathcal{D}_{d i v}$ , respectively. For any hypothesis $h$ and $h_{d i v}$ in $\\mathcal{H}$ , and $0<\\delta<1$ , with a probability of at least $1-\\delta$ , the following inequality holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{O}(G E r r o r(h_{d i v}))\\leq\\mathcal{O}(G E r r o r(h)).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The detailed proof proceeds as follows: ", "page_idx": 16}, {"type": "text", "text": "Proof. At first, we prove that diverse outliers correspond to a smaller distribution shift error than vanilla outliers. Because $\\mathcal{X}_{a u x}\\subset\\mathcal{X}_{d i v}$ holds, the hypotheses performing well on $\\mathcal{P}_{\\mathcal{X}_{d i v}}$ also perform well on $\\mathcal{P}_{\\mathcal{X}_{a u x}}$ , giving rise to $\\mathcal{H}_{d i v}^{*}\\subset\\mathcal{H}_{a u x}^{*}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{h\\in\\mathcal{H}_{d i v}^{*}}{s u p}\\ \\epsilon_{x\\sim\\mathcal{P}_{X}}(h,h_{o o d}^{*})\\leq\\operatorname*{max}\\{\\underset{h\\in\\mathcal{H}_{d i v}^{*}}{s u p}\\ \\epsilon_{x\\sim\\mathcal{P}_{X}}(h,h_{o o d}^{*}),\\underset{h\\in\\mathcal{H}_{a u x}^{*}-\\mathcal{H}_{d i v}^{*}}{s u p}\\ \\epsilon_{x\\sim\\mathcal{P}_{X}}(h,h_{o o d}^{*})\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "note that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}\\{\\phantom{}_{h\\in\\mathcal{H}_{d v}^{\\ast}}\\phantom{}_{\\epsilon_{x}\\sim\\mathcal{P}_{x}}(h,h_{o o d}^{\\ast}),\\phantom{\\epsilon_{x}\\sim}s u p\\phantom{}_{h\\in\\mathcal{H}_{a v}^{\\ast}-\\mathcal{H}_{d v}^{\\ast}}\\epsilon_{x\\sim\\mathcal{P}_{x}}(h,h_{o o d}^{\\ast})\\}=\\underset{h\\in\\mathcal{H}_{a v}^{\\ast}}{s u p}\\phantom{}_{\\epsilon_{x}\\sim\\mathcal{P}_{x}}(h,h_{o o d}^{\\ast}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consequently, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{h\\in\\mathcal{H}_{d i v}^{*}}{s u p}\\epsilon_{x\\sim\\mathcal{P}x}(h,h_{o o d}^{*})\\le\\underset{h\\in\\mathcal{H}_{a u x}^{*}}{s u p}\\epsilon_{x\\sim\\mathcal{P}x}(h,h_{o o d}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, model effective training leads to small empirical error and small reducible error, if we continue to use the same model architecture, the intrinsic complexity of the model $\\mathcal{R}_{m}(\\mathcal{H})$ remains invariant, consider that $\\beta$ is a small constant value, therefore, it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{O}(\\mathrm{GError}(h_{d i v}))\\leq\\mathcal{O}(\\mathrm{GError}(h)),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with a probability of at least $1-\\delta$ . ", "page_idx": 16}, {"type": "text", "text": "A.4 Proof of Lemma 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we give the proof of the Lemma 1, which can be formalized as follow: ", "page_idx": 17}, {"type": "text", "text": "(Diversity Enhancement with Mixup). For a group of mixup transforms4 $\\mathcal{G}$ acting on the input space $\\chi_{a u x}$ to generate an augmented input space $\\mathcal{G X}_{a u x}$ , defined as $\\mathcal G\\mathcal X_{a u x}=\\{\\hat{x}|\\hat{x}=\\lambda x_{1}+(1-$ $\\bar{\\lambda})x_{2};x_{1},x_{2}\\in\\bar{\\mathcal{X}}_{a u x},\\lambda\\in[0,\\bar{1}]\\}$ , the following relation holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{X}_{a u x}\\subset\\mathcal{G}\\mathcal{X}_{a u x}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. $\\mathcal{X}_{a u x}\\,=\\,\\mathcal{X}_{a u x}^{y_{1}}\\cup\\hdots\\cup\\mathcal{X}_{a u x}^{y_{i}}\\cup\\hdots\\cup\\mathcal{X}_{a u x}^{y_{j}}\\cup\\hdots\\cup\\mathcal{X}_{a u x}^{y_{n}}$ . Conysider performing mixup to obtain a mixed outlier $\\hat{x}=\\lambda x_{i}+(1-\\lambda)x_{j}$ , where $x_{i}\\in\\mathcal{X}_{a u x}^{y_{i}}$ , $\\overline{{x}}_{j}\\in\\mathcal X_{a u x}^{y_{j}}$ and $y_{i}\\neq y_{j}$ . According to assumption 1, there exists $\\lambda$ such that $\\hat{x}$ exhibits different semantics from the original, i.e., $\\hat{x}\\notin\\mathcal X_{a u x}^{y_{i}}$ and $\\hat{x}\\notin\\mathcal{X}_{a u x}^{y_{j}}$ . Clearly, the semantic of $\\hat{x}$ is also inconsistent with other outliers in $\\chi_{a u x}$ . Therefore, $\\hat{x}\\notin\\mathcal X_{a u x}$ . We define $\\mathcal{X}_{m i x}=\\{\\hat{x}\\mid\\hat{x}\\notin\\mathcal{X}_{a u x},\\hat{x}=\\lambda x_{i}+(1-\\lambda)x_{j},x_{i},x_{j}\\in\\mathcal{X}_{a u x}\\}$ to represents the input space of mixed outliers with distinct semantic to the original. Consequently, $\\mathcal G\\mathcal X_{a u x}=\\mathcal X_{a u x}\\cup\\mathcal X_{m i x}$ , leading to $\\mathcal{G}\\mathcal{X}_{a u x}\\supset\\mathcal{X}_{a u x}$ . ", "page_idx": 17}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Details of Dataset ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Auxiliary OOD datasets. $\\circ$ For CIFAR experiments, we employ the downsampled ImageNet dataset (ImageNet $64\\times64$ ) as a variant of the original ImageNet dataset, comprising 1,281,167 images with dimensions of $64\\!\\times\\!64$ pixels and organized into 1000 distinct classes. Notably, there is overlap between some of these classes and those present in CIFAR-10 and CIFAR-100 datasets. It is important to emphasize that we abstain from utilizing any label information from this dataset, thereby regarding it as an unlabeled auxiliary OOD dataset. To augment the dataset, we apply a random cropping procedure to the $64\\!\\times\\!64$ images, resulting in $32\\!\\times\\!32$ pixel images with a 4-pixel padding. This operation performed with a high probability ensures that the resulting images are unlikely to contain objects corresponding to the ID classes, even if the original images featured such objects. Consequently, we retain a substantial quantity of OOD data for training purposes, yielding a low proportion of ID data within the auxiliary outliers. For conciseness and clarity, we refer to this dataset as ImageNet-RC.\u25e6For ImageNet experiments, we selected a subset from ImageNet-1K, which includes 200 classes, to serve as the ID data. Images from the other 800 classes are used as auxiliary datasets, following the setting of [50]. The resolution for both ID and auxiliary images are $224\\times224$ . ", "page_idx": 17}, {"type": "text", "text": "Test OOD datasets. For CIFAR experiments, we follow the setting in [6, 35]. Specifically, we employ six different natural image datasets as our OOD test datasets, while CIFAR-10 and CIFAR-100 serve as our ID test datasets. These six datasets are SVHN [38], Textures [8], Places365 [57], LSUN (crop), LSUN (resize) [53], and iSUN [49]. Below, we provide detailed information about these OOD test datasets, all of which consist of $32\\times32$ pixel images. \u25e6SVHN. The SVHN dataset [38] comprises color images of house numbers, encompassing ten different digit classes from 0 to 9. For our evaluation, we randomly select 1,000 test images from each digit class, creating a new test dataset with 10,000 images. $\\circ$ Textures. The Describable Textures Dataset [8] consists of textural images in the wild. We include the entire collection of 5,640 images for evaluation. \u25e6Places365. The Places365 dataset [57] comprises a large-scale photographs depicting scenes classified into 365 scene categories. In the test set, there are 900 images per category. We randomly sample 10,000 images from the test set for our evaluation. \u25e6LSUN (crop) and LSUN (resize). The Large-scale Scene Understanding dataset (LSUN) [53] offers a testing set containing 10,000 images from 10 different scenes. We create two variants of this dataset, namely LSUN (crop) and LSUN (resize). LSUN (crop) is generated by randomly cropping image patches to the size of $32\\times32$ pixels, while LSUN (resize) involves downsampling each image to the same size. $\\circ$ iSUN. The iSUN dataset [49] is a subset of SUN images. We incorporate the entire collection of 8,925 images from iSUN for our evaluation. ", "page_idx": 17}, {"type": "text", "text": "In ImageNet experiments, we follow the settings of [50], where OpenImage- $o$ [44], SSB-hard [43], Textures [8], iNaturalist [21] and NINCO [5] are selected as OOD test datasets. We include SSB-hard and NINCO in the near-OOD group, while the far-OOD group considers iNaturalist, Textures, and OpenImage-O. \u25e6OpenImage-O contains 17632 manually filtered images and is $7.8~\\times$ larger than the ImageNet- $o$ dataset. $\\circ$ SSB-hard is selected from ImageNet-21K. It consists of 49K images and covers 980 categories. $\\circ$ iNaturalist consists of 859000 images from over 5000 different species of plants and animals. $\\circ$ NINCO consists with a total of 5879 samples of 64 classes which are non-overlapped with ImageNet- $I K$ . ", "page_idx": 17}, {"type": "table", "img_path": "C1hiRbzEH9/tmp/d4974789eb26ba5fe095b6b0d352411f33d8f521e71749ad6cadccf2489571bf.jpg", "table_caption": ["Table 4: Main results with standard deviation. Comparison with competitive OOD detection methods trained with the same DenseNet backbone. The performance metrics are averaged $(\\%)$ over six OOD test datasets from Section 5.1. Some baseline results are sourced from [35]. The best results are in bold. diverseMix not only demonstrates state-of-the-art OOD detection performance on the CIFAR benchmark but also maintains high accuracy in ID classification. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "B.2 Training Details. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "$\\circ$ CIFAR experiments. We use DenseNet-101 [22] as the backbone for all methods, employing stochastic gradient descent with Nesterov momentum (momentum $=0.9$ ) over 100 epochs. The initial learning rate of 0.1 decreases by a factor of 0.1 at 50, 75, and 90 epochs. Batch sizes are 128 for both ID data and OOD data. For DiverseMix, we set $\\alpha=4$ , $T=10$ . Experiments are run over five times to report the means and standard deviations. \u25e6ImageNet experiments. We use ResNet18 [18] as the backbone network. We use SGD optimizer to train all the models. The momentum is set to 0.9. Model is obtained by training ResNet18 for 100 epochs with an initial learning rate of 0.1, utilizing a cosine annealing strategy to adjust the learning rate. The weight decay is set to 0.0005. Batch size is set to 256 both ID data and OOD data. For DiverseMix, we set $\\alpha=8$ , $T=0.1$ . We use OE loss as regularization loss. Experiments are run over five times to report the means and standard deviations. ", "page_idx": 18}, {"type": "text", "text": "B.3 Details of OOD Regularization Method. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In addition to the Energy loss mentioned in Section 4.2, our method can be extended to different OOD regularization methods, such as OE [20] and $K{+}1$ [6]. The details are as follows: ", "page_idx": 18}, {"type": "text", "text": "Outlier Exposure (OE). OE introduces a promising approach towards OOD detection by utilizing outliers to force apart the distributions of ID and OOD. Its scoring function and corresponding regular function can be expressed as: ", "page_idx": 18}, {"type": "equation", "text": "$$\nS(x,\\theta)=\\operatorname*{max}{\\operatorname{sof}\\!\\operatorname{tmax}(F(x,\\theta))},\\,\\,\\mathcal{L}_{a u x}=\\mathbb{E}_{x\\sim\\mathcal{D}_{a u x}}[\\mathcal{L}_{C E}(F(x,\\theta),\\mathcal{U})],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\boldsymbol{\\mathcal{U}}$ is the uniform distribution over $K$ classes. ", "page_idx": 18}, {"type": "text", "text": "$(\\mathbf{K}{+}1)$ -way regularization method. Considering a $(\\mathsf{K}{+}1)$ -way classifier network $F$ , where the $(\\mathsf{K}{+}1)$ -th label indicates OOD class. Its scoring function and regular function can be expressed as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S(x,\\theta)=-\\mathrm{softmax}_{\\mathrm{K+1}}(F(x,\\theta)),\\ \\mathcal{L}_{a u x}=\\mathbb{E}_{x\\sim\\mathcal{D}_{a u x}}[\\mathcal{L}_{C E}(F(x,\\theta),K+1)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathrm{softmax}_{\\mathrm{K+1}}(\\cdot)$ represents the softmax output in the $K{+}1$ dimension. ", "page_idx": 18}, {"type": "text", "text": "B.4 Details of Main Experiment. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Full Results with Standard Deviation. In Tab. 4 and Tab. 5, we present the experimental results for all evaluation metrics along with the corresponding standard deviations. From the experimental results we can draw similar conclusions as those in Sec. 5. ", "page_idx": 18}, {"type": "text", "text": "Results on Individual OOD Dataset. We also provide the performance of our method on individual OOD dataset in tabel 6. ", "page_idx": 18}, {"type": "text", "text": "Table 5: Main results on large-scale datasets. Comparison with competitive OOD detection methods trained with the same ResNet backbone. We divide the OOD test set into two distinct groups: near-OOD and far-OOD. For each group, we report the average performance metrics with standard deviation. The best and second-best results are in bold and underline respectively. Echoing the findings from our CIFAR experiments, diverseMix demonstrates strong OOD detection capabilities for both near-OOD and far-OOD test sets, achieving state-of-the-art OOD detection performance. ", "page_idx": 19}, {"type": "table", "img_path": "C1hiRbzEH9/tmp/ac11f4f99547965a20d6ce78b211f1824df22ba8950b3b288dd31ddd806585d1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "C1hiRbzEH9/tmp/dffbebfdadc4359955da12c0c62e096525c1b59ad082684fe1712abad475d229.jpg", "table_caption": ["Table 6: main results on individual OOD dataset. We provide the results of diverseMix on each individual OOD dataset from Section 5.1. The reported performance of our method is based on five independent training runs using different random seeds. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.5 Details of Figure 2. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this experiment, we aim to explore the effect of outlier quality on OOD detection performance. We analyzed this from two perspectives: the sample size of outliers and the diversity of outliers. Specifically, we constructed a series of subsets from the Imagenet-RC dataset to generate low-quality auxiliary outliers datasets with different sample size and diversity. Afterwards, we used these constructed low-quality subsets as the auxiliary outliers dataset to train the model. All experimental results are run over three times and averaged. The experimental details are as follows: ", "page_idx": 19}, {"type": "text", "text": "Decreasing the sample size of auxiliary outliers. To explore the impact of sample size on our experimental results, we keep the number of classes constant and decrease the size of the auxiliary outliers dataset. This is achieved by applying downsampling techniques, resulting in subsets with the same classes as the original Imagenet- $R C$ dataset but with sizes of $\\{1\\bar{0}0\\%$ , $85\\%$ , $70\\%$ , $55\\%$ , $40\\%$ , $25\\%$ , $10\\%\\}$ compared to the original auxiliary outliers dataset. ", "page_idx": 19}, {"type": "text", "text": "Decreasing the diversity of auxiliary outliers. To investigate the effect of outlier diversity on OOD detection performance, we further reduce the number of classes included in the subset. Specifically, we keep the sample size of the subset at $10\\%$ of the original outliers dataset, but gradually decrease the number of classes included (as the number of classes decreases, the number of samples per class increases, ensuring a consistent overall sample size). We constructed a series of subsets with $\\{1000,850,700,550,4{\\bar{0}}0,250,100\\}$ classes to serve as auxiliary outliers for experimental evaluation. ", "page_idx": 19}, {"type": "text", "text": "B.6 Details of Q5 Ablation Study. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we conduct an ablation study from two perspectives. Firstly, we compare our method with traditional data augmentation techniques (semantic-preserving) to demonstrate that our method effectively enhances the diversity of outliers by altering their semantics. Secondly, considering that our method is an improved variant of mixup, we investigate different mixup strategies to explore what factors contribute to the performance gains. Detailed experimental results are shown in Table 7. ", "page_idx": 19}, {"type": "text", "text": "Ablation study (I): Ablation study with different data augmentation method. To investigate if diverseMix offers unique advantages over other data augmentation techniques in enhancing the diversity of outliers, we select different data augmentation methods to process the auxiliary outliers and validate their impact on performance. Specifically, we choose semantic-invariant data augmentation methods: Gaussian noise [39], cutout [11], and color jitter for comparison with our method. ", "page_idx": 19}, {"type": "table", "img_path": "C1hiRbzEH9/tmp/5bcbc3961d5edabe6a7f6e8cb7f8fa99d125b35337fc25de811ddeee3b53dad6.jpg", "table_caption": ["Table 7: Ablation study on different data augmentation methods. Performance are averaged $(\\%)$ over six OOD test datasets from Section 5.1. The best results are in bold. The reported OOD detection performance is based on five independent training runs using different random seeds. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Gaussian noise. Here, we introduce an appropriate level of noise to the training data to augment its diversity and quantity. We incorporate Gaussian noise with a mean of 0 and a variance of 0.1. To effectively mitigate the risk of model overfitting to Gaussian noise, wherein the model incorrectly classifies any image with Gaussian noise as an OOD input and any noise-free image as an ID sample, this type of noise is applied to only half of the outlier samples during the model training phase. ", "page_idx": 20}, {"type": "text", "text": "Cutout. Cutout is a data augmentation technique that introduces random masking of small regions in input images, preventing the model from relying on specific features. In our study, we apply the cutout augmentation to half of the auxiliary outlier samples. This involves randomly masking out small regions within these outlier images by setting all pixel values in the masked regions to zero. ", "page_idx": 20}, {"type": "text", "text": "Color jittering. Color jittering is a widely adopted data augmentation technique in image processing. It introduces random variations to the brightness, contrast, saturation, and hue of an image, simulating the diverse conditions encountered in real-world scenarios, such as different lighting environments or camera settings. Specifically, for each auxiliary outlier image, we randomly adjust its brightness within a range of $\\pm0.4$ , its contrast within a range of $\\pm0.4$ and its saturation within a range of $\\pm0.4$ , while rotating the hue by $\\pm0.1$ radians. This data augmentation strategy preserves the semantic content of the original outlier image while introducing controlled variations in color properties. ", "page_idx": 20}, {"type": "text", "text": "Ablation study $(\\mathbf{II})$ : Ablation study with different semantic interpolation method. To explore how diverseMix differs from other mixup-based methods, we compared the performance to vanilla mixup and cutmix. We set the hyperparameter $\\alpha=4$ , consistent with our method diverseMix. ", "page_idx": 20}, {"type": "text", "text": "Vanilla mixup. Vanilla mixup involves generating virtual training examples (referred to as mixed samples) through linear interpolations between data points and corresponding labels, given by: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{x}=\\lambda x_{i}+(1-\\lambda)x_{j},\\quad\\hat{y}=\\lambda y_{i}+(1-\\lambda)y_{j},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(x_{i},y_{i})$ and $(x_{j},y_{j})$ are two samples drawn randomly from the empirical training distribution, and $\\lambda\\in[0,1]$ is usually sampled from a Beta distribution with parameter $\\alpha$ denoted as $B e t a(\\alpha,\\alpha)$ . ", "page_idx": 20}, {"type": "text", "text": "Cutmix. Cutmix is a data augmentation method that constructs virtual training examples by performing cutting and replacing the cutted region with the corresponding region from the other image: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{x}=M(\\lambda)\\odot x_{i}+(1-M(\\lambda))\\odot x_{j},\\quad\\hat{y}=\\lambda y_{i}+(1-\\lambda)y_{j},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $M(\\lambda)$ is a binary mask randomly chosen covering $\\lambda$ proportion of the input, and $\\odot$ represents the element-wise product. Here, $\\lambda$ is usually sampled from a preset beta distribution $B e t a(\\alpha,\\alpha)$ . ", "page_idx": 20}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Hyperparameter Analysis. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we analyze the main hyperparameters involved in our method. The experimental results are shown in the table 8. From the experimental results, we find that diverseMix is more effective with larger values of $\\alpha$ . A larger $\\alpha$ means that the model will adopt a more aggressive interpolation strategy, generating mixed outliers that deviates further from the original samples. This aligns with our expectations. The temperature $T$ controls diverseMix\u2019s sensitivity to the samples, an appropriate $T$ allows diverseMix to accurately perceive the model\u2019s familiarity with the samples. $\\omega$ controls the strength of regularization, an excessively large $\\omega$ may impair the classification performance. In addition, we provide our strategy for hyperparameter adjustment in practice as follows: ", "page_idx": 20}, {"type": "text", "text": "hyper-parameter tuning. We can first determine the largest possible value of $\\omega$ for the original baseline model while maintaining the ID classification accuracy. Then, we can select more suitable parameters for $\\alpha$ and $T$ , with adjustments made using an OOD validation set distinct from the testing OOD dataset. For example, a subset from the auxiliary outliers could serve as an OOD validation set. ", "page_idx": 20}, {"type": "table", "img_path": "C1hiRbzEH9/tmp/e62b979393e07f12cf042ee20e0e75201a187bee46807317a0032cf63b91c54e.jpg", "table_caption": ["Table 8: Hyperparameter analysis. Performance averaged $(\\%)$ over six OOD test datasets from Section 5.1. The performance reported are averaged over different random seeds. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "C1hiRbzEH9/tmp/4402a28cfbbe955ab533407bd7d9dc070b16141a874837342eb70ea138166ad2.jpg", "table_caption": ["Table 9: Experimental Results on NLP OOD detection task. The best results are in bold. The same network architecture is used for all three detectors. All results are represented in percentages. Our method diverseMix also achieves good performance in the field of natural language proceeding. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "C1hiRbzEH9/tmp/da334954f41271ae8af49c7fc8d70243f8808556239ec07882ceeae2d6e5610a.jpg", "table_caption": ["Table 10: Time and memory cost of different methods. We compare the computational overhead of DiverseMix and other methods on CIFAR-100 under the same setting. Best results are in bold. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "C.2 DiverseMix for OOD Detection in Natural Language Processing. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To further validate the applicability of our method in non-image domains, we explore the use of diverseMix in the task of Natural Language Processing, following the setting of OE [20]. ", "page_idx": 21}, {"type": "text", "text": "Experimental Setting. We use the SST dataset as the ID data, while utilizing the WikiText-2 dataset as auxiliary outlier data. We employ the SNLI, Multi30K, WMT16, and Yelp Reviews datasets as OOD test set. We use QRNN [34] language models as baseline OOD detectors. Initially, we train vanilla models for 50 epochs and subsequently fine-tune them on the WikiText-2 dataset using $O E$ or DiverseMix for an additional 5 epochs. Outlier Exposure is implemented by adding the cross entropy to the uniform distribution on tokens from sequences in $\\mathcal{D}_{a u x}$ as an additional loss term. For DiverseMix, we apply mixup strategy at embedding level, and the loss function is consistent with $O E$ . ", "page_idx": 21}, {"type": "text", "text": "Experimental Results. The results presented in table 9 highlight that: 1) The incorporation of auxiliary outliers enhances OOD detection performance in non-image domains. 2) Our method increases the diversity of auxiliary outliers, further enhancing the model\u2019s OOD detection performance. ", "page_idx": 21}, {"type": "text", "text": "C.3 Experiments on Computational Cost. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To better understand the computational budget, we summarize the time and memory cost results in Table 10, which shows that diverseMix can achieve better performance with relatively low time and memory overhead compared with other OOD detection methods that train with auxiliary outliers. ", "page_idx": 21}, {"type": "text", "text": "C.4 Impact Statements ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our work focuses on enhancing AI safety and trustworthiness by improving the robust performance of machine learning models on OOD data, which is crucial for high-stakes tasks in real-world scenarios. ", "page_idx": 21}, {"type": "text", "text": "However, biases in benchmark OOD detection data, such as ImageNet, necessitate careful auxiliary outlier selection for safety-critical applications to ensure the proposed method\u2019s reliability and safety. ", "page_idx": 22}, {"type": "text", "text": "D Hardware and Software ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We run all the experiments on NVIDIA GeForce RTX 3090 GPU. Our implementations are based on Ubuntu Linux 18.04 with Python 3.8. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The paper has discussed the limitations of the work performed by the authors. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper has provided the full set of assumptions and a complete (and correct) proof. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and conclusions of the paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The paper specify all the training and test details necessary to understand the results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have discussed both potential positive societal impacts and negative societal impacts of the work performed. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have not released data or models that have a high risk for misuse. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited and the license and terms of use explicitly are mentioned and properly respected. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There is no new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There is no crowdsourcing experiments and research. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There is no research with Human Subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]