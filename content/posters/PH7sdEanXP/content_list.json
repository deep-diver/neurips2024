[{"type": "text", "text": "Scaling Laws in Linear Regression: Compute, Parameters, and Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Licong Lin Jingfeng Wu Sham M. Kakade UC Berkeley UC Berkeley Harvard University liconglin@berkeley.edu uuujf@berkeley.edu sham@seas.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Peter L. Bartlett Jason D. Lee UC Berkeley and Google DeepMind Princeton University peter@berkeley.edu jasonlee@princeton.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow. However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size. This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance. ", "page_idx": 0}, {"type": "text", "text": "We study the theory of scaling laws in an infinite dimensional linear regression setup. Specifically, we consider a model with $M$ parameters as a linear function of sketched covariates. The model is trained by one-pass stochastic gradient descent (SGD) using $N$ data. Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a>1$ , we show that the reducible part of the test error is $\\Theta(M^{-(\\stackrel{\\cdot}{a}-1)}+N^{-(a-1)/a})$ . The variance error, which increases with $M$ , is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound. Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning models, particularly those on a large scale, are pivotal in advancing the state-of-theart across various fields. Recent empirical studies have shed light on the so-called neural scaling laws [see 26, 21, for example], which suggest that the generalization performance of these models improves polynomially as both model size, denoted by $M$ , and data size, denoted by $N$ , increase. The neural scaling law quantitatively describes the population risk as: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathcal{R}(M,N)\\approx\\mathcal{R}^{*}+\\frac{c_{1}}{M^{a_{1}}}+\\frac{c_{2}}{N^{a_{2}}},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\mathcal{R}^{*}$ is a positive irreducible risk and $c_{1},c_{2},a_{1},a_{2}$ are positive constants independent of $M$ and $N$ . For instance, by fitting the above formula with empirical measurements in standard large-scale language benchmarks, Hoffmann et al. [21] estimated $a_{1}\\approx0.34$ and $a_{2}\\approx0.28$ , while Besiroglu et al. [7] estimated that $a_{1}\\approx0.35$ and $a_{2}\\approx0.37$ . Though the exact exponents depend on the tasks, neural scaling laws in (1) are observed consistently in practice and are used as principled guidance to build state-of-the-art models, especially under a compute budget [21]. ", "page_idx": 0}, {"type": "text", "text": "From the perspective of statistical learning theory, (1) is rather intriguing. Standard statistical learning bounds [see 30, 41, for example] often decompose the population risk into the sum of irreducible error, approximation error, bias error, and variance error (some theory replaces bias and variance errors by optimization and generalization errors, respectively) as in the form of ", "page_idx": 0}, {"type": "image", "img_path": "PH7sdEanXP/tmp/3c23f0d3ce45b0e19ab8fb9365c7d27952ba9c063179cea150815fd14affc371.jpg", "img_caption": ["Figure 1: The expected risk (Risk) of the last iterate of (SGD) versus the effective sample size $N_{\\mathsf{e f f}}$ and the model size $M$ for different power-law degrees $a$ . The expected risk is computed by averaging over 1000 independent samples of $(\\mathbf{w}^{*},\\mathbf{S})$ . We fit the expected risk using the formula Risk $\\sim\\sigma^{2^{*}}\\!+c_{1}/{\\bar{M}^{a_{1}^{*}}}+c_{2}/N^{a_{2}}$ via minimizing the Huber loss as in [21]. Parameters: $\\sigma=1,\\gamma=0.1$ . Left: For $a=1.5$ , $d=20000$ , the ftited exponents are $(a_{1},a_{2})=(0.54,0.34)\\approx(0.5,0.33)$ . Right: For $a=2$ , $d=2000$ , the fitted exponents are $(a_{1}^{-},a_{2})=(1.07,0.49)\\approx(1.0,0.5)$ . Note that the values of $\\left(a_{1},a_{2}\\right)$ are close to our theoretical predictions $(a\\!-\\!1,1\\!-\\!1/a)$ in both cases, verifying the sharpness of our risk bounds. More details can be found in Sections 4 and 5. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{R}(M,N)=\\mathcal{R}^{*}+\\underbrace{\\mathcal{O}\\biggl(\\frac{1}{M^{a_{1}}}\\biggr)}_{\\mathrm{approximation}}+\\underbrace{\\mathcal{O}\\biggl(\\frac{1}{N^{a_{2}}}\\biggr)}_{\\mathrm{bias}}+\\underbrace{\\mathcal{O}\\biggl(\\frac{c(M)}{N^{a_{3}}}\\biggr)}_{\\mathrm{variance}},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $a_{1},a_{2},a_{3}$ are positive constants and $c(M)$ is a measure of model complexity that typically increases with the model size $M$ . In (2), the approximation error is induced by the mismatch of the best-in-class predictor and the best possible predictor, hence decreasing with the model size $M$ . The bias error is induced by the mismatch of the expected algorithm output and the best-in-class predictor, hence decreasing with the data size $N$ . The variance error measures the uncertainty of the algorithm output, which decreases with the data size $N$ but increases with the model size $M$ (since the model complexity $c(M)$ increases). ", "page_idx": 1}, {"type": "text", "text": "A mystery. The empirical neural scaling law (1) is incompatible with the typical statistical learning theory bound (2). While the two error terms in the neural scaling law (1) can be explained by the approximation and bias errors in the theoretical bound (2) respectively, it is not clear why the variance error is unobservable when ftiting the neural scaling law empirically. This difference must be reconciled, otherwise, the statistical learning theory and the empirical scaling law make conflict predictions: as the model size $M$ increases, the theoretical bound (2) predicts an increase of variance error that eventually causes an increase of the population risk, but the neural scaling law (1) predicts a decrease of the population risk. In other words, it remains unclear when to follow the prediction of the empirical scaling law (1) and when to follow that of the statistical learning bound (2). ", "page_idx": 1}, {"type": "text", "text": "Certain prior works provided risk upper bounds that do not grow with model size [see for example 36, 12]. Still, their results are insufficient for studying scaling law as those bounds require a large model size such that the approximation error is ignorable. Moreover, they do not provide instancewise matching lower bounds to verify the tightness of the upper bounds. See a detailed discussion in Section 2. ", "page_idx": 1}, {"type": "text", "text": "Our explanation. We investigate this issue in an infinite dimensional linear regression setup. We only assume access to $M$ -dimensional sketched covariates given by a fixed Gaussian sketch and their responses. We consider a linear predictor with $M$ trainable parameters, which is trained by one-pass stochastic gradient descent (SGD) with geometrically decaying stepsizes using $N$ sketched data. Assuming that the spectrum of the data covariance matrix satisfies a power-law of degree $a>1$ and that the optimal model parameters satisfy a Gaussian prior, we derive matching upper and lower bounds on the population risk achieved by the SGD output (see Theorem 4.1). Specifically, we show ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{R}(M,N)=\\mathcal{R}^{*}+\\;\\Theta\\bigg(\\frac{1}{M^{a-1}}\\bigg)+\\tilde{\\Theta}\\bigg(\\frac{1}{(N\\gamma)^{(a-1)/a}}\\bigg)\\,,\\quad\\mathsf{V a r}=\\tilde{\\Theta}\\bigg(\\frac{\\operatorname*{min}\\{M,(N\\gamma)^{1/a}\\}}{N}\\bigg),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\gamma=\\mathcal{O}(1)$ is the initial stepsize used in SGD and $\\tilde{\\Theta}(\\cdot)$ hides $\\log(N)$ factors. In our bound, the sum of the approximation and bias errors determines the order of the excess risk, while the variance error is of a strictly higher order and is therefore nearly unobservable when fitting $\\mathcal{R}(M,N)$ as a function of $M$ and $N$ empirically. In addition, our analysis reveals that the small variance error is due to the implicit regularization effect of one-pass SGD [47]. Our theory suggests that the empirical neural scaling law (1) is a simplification of the statistical learning bound (2) in a special regime when strong regularization (either implicit or explicit) is employed. ", "page_idx": 2}, {"type": "text", "text": "Moreover, we generalize the above scaling law to (1) constant stepsize SGD with iterate average (see Theorem F.6), (2) cases where the optimal model parameter satisfies an anisotropic prior (see Theorem 4.2), and (3) where the spectrum of the data covariance matrix satisfies a logarithmic power law (see Theorem 4.3). ", "page_idx": 2}, {"type": "text", "text": "Emprical evidence. Based on our theoretical results, we conjecture that the clean neural scaling law (1) observed in practice is due to the disappearance of variance error caused by strong regularization. Two pieces of empirical evidence to support our understanding. First, large language models that follow the scaling law (1) are often underfitted, as the models are trained over a single pass or a few passes over the data [27, 31, 9, 39]. When models are underfitted, the variance error tends to be smaller. Second, when language models are trained with multiple passes (up to 7 passes), Muennighoff et al. [31] found that the clean scaling law in (1) no longer holds and they proposed a more sophisticated scaling law to explain their data. This can be explained by a relatively large variance error caused by multiple passes. ", "page_idx": 2}, {"type": "text", "text": "Notation. For two positive-valued functions $f(x)$ and $g(x)$ , we write $f\\!\\left(x\\right)\\lesssim g\\!\\left(x\\right)$ (and $f(x)=$ ${\\mathcal{O}}(g(x)))$ or $f(x)\\,\\gtrsim\\,g(x)$ (and $f(x)\\;=\\;\\Omega(g(x)))$ if $f(x)\\;\\leq\\;c g(x)$ or $f(x)\\;\\geq\\;c g(x)$ holds for some absolute (if not otherwise specified) constant $c>0$ respectively. We write $f(x)\\stackrel{!}{\\sim}g(x)$ (and $f(x)=\\Theta(g(x)))$ if $f(x)\\lesssim g(x)\\stackrel{\\cdot}{\\sim}f(x)$ . For two vectors $\\mathbf{u}$ and $\\mathbf{v}$ in a Hilbert space, we denote their inner product by $\\langle\\mathbf{u},\\mathbf{v}\\rangle$ or $\\mathbf{u}^{\\top}\\mathbf{v}$ . For two matrices $\\mathbf{A}$ and $\\mathbf{B}$ of appropriate dimensions, we define their inner product b $\\operatorname{\\langleA,B\\rangle}:=\\operatorname{tr}(\\mathbf{A}^{\\top}\\mathbf{B})$ . We use $\\|\\cdot\\|$ to denote the operator norm for matrices and $\\ell_{2}$ -norm for vectors. For a positive semi-definite (PSD) matrix A and a vector $\\mathbf{v}$ of appropriate dimension, we write $\\|\\mathbf{v}\\|_{\\mathbf{A}}^{2}:=\\mathbf{\\dot{v}}^{\\top}\\mathbf{A}\\mathbf{v}$ . For a symmetric matrix A, we use $\\mu_{j}(\\mathbf{A})$ to refer to the $j$ -th eigenvalue of $\\mathbf{A}$ and $r(\\mathbf{A})$ to refer to its rank. Finally, $\\log(\\cdot)$ refers to logarithm base 2. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Empirical scaling laws. In recent years, the scaling laws of deep neural networks in compute, sample size, and model size have been widely studied across different models and domains [20, 35, 26, 19, 21, 46, 31]. The early work by Kaplan et al. [26] first proposed the neural scaling laws of transformer-based models. They observed that the test loss exhibits a power-law decay in quantities including the amount of compute, sample size, and model size, and provided joint formulas in these quantities to predict the test loss. The proposed formulas were later generalized and refined in subsequent works [19, 21, 1, 10, 31]. Notably, Hoffmann et al. [21] proposed the Chinchilla law, that is, (1) with $a_{1}\\approx0.34$ and $a_{2}\\approx0.28$ . The empirical observation guided them to allocate data and model size under a given compute budget. The Chinchilla law is further revised by Besiroglu et al. [7]. Motivated by the Chinchilla law, Muennighoff et al. [31] considered the effect of multiple passes over training data and empirically fitted a more sophisticated scaling law that takes account of the effect of data reusing. ", "page_idx": 2}, {"type": "text", "text": "Theory of scaling laws. Although neural scaling laws have been empirically observed over a broad spectrum of problems, there is a relatively limited literature on understanding these scaling laws from a theoretical perspective [37, 4, 28, 22, 42, 29, 23, 8, 2, 32, 17]. Among these works, [37] showed that the test loss scales as $N^{4/d}$ for regression on data with intrinsic dimension $d$ . Hutter [22] studied a toy problem under which a non-trivial power of $N$ arises in the test loss. Jain et al. [23] considered scaling laws in data selection. Bahri et al. [4] considered a linear teacher-student model under a power-law spectrum assumption on the covariates, and they showed that the test loss of the ordinary least square estimator decreases following a power law in sample size $N$ (resp. model size $M_{\\sun}$ ) when the model size $M$ (resp. sample size $N$ ) is infinite. Bordelon et al. [8] considered a linear random feature model and analyzed the test loss of the solution found by (batch) gradient flow. They focused on the bottleneck regimes where two of the quantities $N,M,T$ (training steps) are infinite and showed that the risk has a power-law decay in the remaining quantity. The problem in Bahri et al. [4], Bordelon et al. [8] can be viewed as a sketched linear regression model similar to ours. It should be noted that both Bahri et al. [4] and Bordelon et al. [8] only derived the dependence of population risk on one of the data size, model size, or training steps in the asymptotic regime where the remaining quantities go to infinity, and their derivations are based on statistical physics heuristics. In comparison, we prove matching (ignoring constant factors) upper and lower risk bounds jointly depending on the finite model size $M$ and data size $N$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Implicit regularization of SGD. One-pass SGD in linear regression has been extensively studied in both the classical finite-dimensional setting [34, 3, 14, 16, 25, 24, 18] and the modern highdimensional setting [15, 6, 48, 47, 44, 45, 40]. In particular, Zou et al. [47] showed that SGD induces an implicit regularization effect that is comparable to, and in certain cases even more preferable than, the explicit regularization effect induced by ridge regression. This is one of the key motivations of our scaling law interpretation. From a technical perspective, we utilize the sharp finite-sample and dimension-free analysis of SGD developed by Zou et al. [48], Wu et al. [44, 45]. Different from them, we consider a sequence of linear regression models with an increasing number of trainable parameters given by data sketch. Our main technical innovation is to sharply control the effect of data sketch. Some of our intermediate results, for example, tight bounds on the spectrum of the sketched data covariance under the power law (see Lemma 6.2), might be of independent interest. ", "page_idx": 3}, {"type": "text", "text": "Prior works investigated linear regression with random features [36, 12], which can be viewed as a kind of sketched features via random coordinate selection. They mainly focused on the small approximation error regime, where the model size (or the number of features) is much larger than the data size. In comparison, we treat both model size and data size as free variables. Moreover, we provide matching upper and lower bounds while prior works mainly focused on upper bounds. These two innovations are crucial for studying scaling laws that predict test error as a function of both model size and data size. Finally, in the comparable regimes with small or zero approximation error, our excess risk bounds recover the bounds in prior works [36, 12, 33, 15, 13]. ", "page_idx": 3}, {"type": "text", "text": "3 Setup", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use $\\mathbf{x}\\in\\mathbb{H}$ to denote a feature vector, where $\\mathbb{H}$ is a finite $d$ -dimensional or countably infinite dimensional Hilbert space, and $y\\,\\in\\,\\mathbb{R}$ to denote its label. In linear regression, we measure the population risk of a parameter $\\mathbf{w}\\in\\mathbb{H}$ by the mean squared error, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(\\mathbf{w}):=\\mathbb{E}\\big(\\langle\\mathbf{x},\\mathbf{w}\\rangle-y\\big)^{2},\\quad\\mathbf{w}\\in\\mathbb{H},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the expectation is over $(\\mathbf{x},y)\\sim P$ for some distribution $P$ on $\\mathbb{H}\\times\\mathbb{R}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Data covariance and optimal parameter). Let $\\mathbf{H}:=\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$ be the data covariance. Assume that $\\operatorname{tr}(\\mathbf{H})$ and all entries of $\\mathbf{H}$ are finite. Let $(\\lambda_{i})_{i\\geq0}$ be the eigenvalues of $\\mathbf{H}$ sorted in non-increasing order. Let $\\mathbf{w^{\\ast}}\\in$ arg minw $\\mathcal{R}(\\mathbf{w})$ be the optimal model parameter1. Assume that $\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2}:=\\bar{(\\mathbf{w}^{*})^{\\top}}\\mathbf{H}\\mathbf{w}^{*}$ is finite. ", "page_idx": 3}, {"type": "text", "text": "We only assume access to $M$ -dimensional sketched covariates and their responses, that is, $(\\mathbf{Sx},y)$ , where $\\mathbf{\\dot{S}}\\in\\mathbb{R}^{M}\\times\\mathbb{H}$ is a fixed sketch matrix. We focus on the Gaussian sketch matrix2, that is, entries of S are independently sampled from ${\\mathcal{N}}(0,1/M)$ . We then consider linear predictors with $M$ trainable parameters given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\mathbf{v}}:\\mathbb{H}\\to\\mathbb{R},\\quad\\mathbf{x}\\mapsto\\langle\\mathbf{v},\\mathbf{S}\\mathbf{x}\\rangle,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{v}\\in\\mathbb{R}^{M}$ are the trainable parameters. Varying $M$ should be viewed as a linear analog of varying the neural network model size. Our sketched linear regression setting is comparable to the teacher-student setting considered by Bahri et al. [4], Bordelon et al. [8]. ", "page_idx": 3}, {"type": "text", "text": "We consider the training of $f_{\\mathbf{v}}$ via one-pass stochastic gradient descent (SGD), that is, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf v_{t}:=\\mathbf v_{t-1}-\\gamma_{t}\\bigl(f_{\\mathbf v_{t-1}}(\\mathbf x_{t})-y_{t}\\bigr)\\nabla_{\\mathbf v}f_{\\mathbf v_{t-1}}(\\mathbf x_{t})}\\\\ &{\\qquad:=\\mathbf v_{t-1}-\\gamma_{t}\\bigl(\\mathbf x_{t}^{\\top}\\mathbf S^{\\top}\\mathbf v_{t-1}-y_{t}\\bigr)\\mathbf S\\mathbf x_{t},\\qquad\\qquad t=1,\\dots,N,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(\\mathbf{x}_{t},y_{t})_{t=1}^{N}$ are independent samples from $P$ and $(\\gamma_{t})_{t=1}^{N}$ are the stepsizes. We consider a popular geometric decaying stepsize scheduler [18, 44], ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{for}\\,t=1,\\dots,N,\\,\\gamma_{t}:=\\gamma/2^{\\ell},\\ \\mathrm{where}\\ \\ell=\\lfloor t/(N/\\log(N))\\rfloor.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, the initial stepsize $\\gamma$ is a hyperparameter for the SGD algorithm. Without loss of generality, we assume the initial parameter is $\\mathbf{v}_{0}=0$ . The output of the SGD algorithm is the last iterate $\\mathbf{v}_{N}$ . Our proof techniques apply to other stepsize schedulers (e.g., polynomial decay) as well, but we focus on geometric decay as it is known to achieve near minimax-optimal excess risk for the last iterate of SGD [18]. ", "page_idx": 4}, {"type": "text", "text": "Conditioning on a sketch matrix $\\mathbf{S}\\in\\mathbb{R}^{M}\\times\\mathbb{H}$ , each parameter $\\mathbf{v}\\in\\mathbb{R}^{M}$ induces a sketched predictor through $\\mathbf{x}\\mapsto\\langle\\mathbf{S}^{\\top}\\mathbf{v},\\mathbf{x}\\rangle$ , and we denote its risk by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}_{M}(\\mathbf{v}):=\\mathcal{R}(\\mathbf{S}^{\\top}\\mathbf{v})=\\mathbb{E}\\big(\\langle\\mathbf{S}\\mathbf{x},\\mathbf{v}\\rangle-y\\big)^{2},\\quad\\mathbf{v}\\in\\mathbb{R}^{M}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By increasing $M$ and $N$ , we have a sequence of datasets and trainable parameters of increasing sizes, respectively. This prepares us to study the scaling law (1) in the sketched linear regression problem, that is, to understand $\\mathcal{R}_{M}(\\mathbf{v}_{N})$ as a function of both $M$ and $N$ . ", "page_idx": 4}, {"type": "text", "text": "Risk decomposition. In a standard way, we decompose the risk achieved by $\\mathbf{v}_{N}$ , the last iterate of (SGD), to the sum of irreducible risk, approximization error, and excess risk as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}_{M}(\\mathbf{v}_{N})=\\underbrace{\\operatorname*{min}_{\\mathrm{Ireducible}}}_{\\mathrm{Irreducible}}+\\underbrace{\\operatorname*{min}\\mathcal{R}_{M}(\\cdot)-\\operatorname*{min}\\mathcal{R}(\\cdot)}_{\\mathsf{A p p r o x}}+\\underbrace{\\mathcal{R}_{M}(\\mathbf{v}_{N})-\\operatorname*{min}\\mathcal{R}_{M}(\\cdot)}_{\\mathsf{E x c e s s}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We emphasize that the irreducible risk is independent of $M$ and $N$ and thus can be viewed as a constant; the approximation error is determined by the sketch matrix S, thus depends on $M$ but is independent of $N$ ; the excess risk depends on both $M$ and $N$ as it is determined by the algorithm. ", "page_idx": 4}, {"type": "text", "text": "4 Scaling laws ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first demonstrate a scaling-law behavior when the data spectrum satisfies a power law. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 (Distributional conditions). Assume the following about the data distribution. ", "page_idx": 4}, {"type": "text", "text": "A. Gaussian design. Assume that $\\mathbf{x}\\sim\\mathcal{N}(0,\\mathbf{H})$ . ", "page_idx": 4}, {"type": "text", "text": "B. Well-specified model. Assume that $\\mathbb{E}[y|\\mathbf{x}]=\\mathbf{x}^{\\top}\\mathbf{w}^{*}$ . Define $\\sigma^{2}:=\\mathbb{E}(y-\\mathbf{x}^{\\top}\\mathbf{w}^{*})^{2}$ . ", "page_idx": 4}, {"type": "text", "text": "C. Parameter prior. Assume that $\\mathbf{w}^{*}$ satisfies a prior such that $\\mathbb{E}(\\mathbf{w}^{*})^{\\otimes2}=\\mathbf{I}.$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Power-law spectrum). There exists $a\\,>\\,1$ such that the eigenvalues of $\\mathbf{H}$ satisfy $\\lambda_{i}\\asymp i^{-a}$ , $i>0$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 (Scaling law). Suppose that Assumptions $I$ and 2 hold. Consider an $M$ -dimensional sketched predictor trained by (SGD) with $N$ samples. Let $N_{\\mathrm{eff}}:=N/\\log(N)$ and recall the risk decomposition in (4). Then there exists some $a$ -dependent constant $c>0$ such that when the initial stepsize $\\gamma\\leq c$ , with probability at least $1-e^{-\\Omega(\\mathring{M})}$ over the randomness of the sketch matrix S, we have ", "page_idx": 4}, {"type": "text", "text": "2. $\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\approx M^{1-a}$ ", "page_idx": 4}, {"type": "text", "text": "3. Suppose in addition $\\sigma^{2}\\gtrsim1$ . The expected excess risk (Excess) can be decomposed into a bias error (Bias) and a variance error (Var), namely, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\mathsf{E x c e s s}\\approx\\mathsf{B i a s}+\\sigma^{2}\\mathsf{V a r},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the expectation is over the randomness of $\\mathbf{w}^{*}$ and $(\\mathbf{x}_{i},y_{i})_{i=1}^{N}$ . Moreover, Bias and Var satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{B i a s}\\lesssim\\operatorname*{max}\\big\\{M^{1-a},\\ (N_{\\mathrm{eff}}\\gamma)^{1/a-1}\\big\\},}\\\\ &{\\mathsf{B i a s}\\gtrsim(N_{\\mathrm{eff}}\\gamma)^{1/a-1}\\ w h e n\\,\\big(N_{\\mathrm{eff}}\\gamma\\big)^{1/a}\\leq M/c\\ \\ f o r\\ s o m e\\ c o n s t a n t\\ c>0,}\\\\ &{\\mathsf{V a r}\\approx\\operatorname*{min}\\big\\{M,\\ (N_{\\mathrm{eff}}\\gamma)^{1/a}\\big\\}/N_{\\mathrm{eff}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In all results, the hidden constants only depend on the power-law degree a. As a direct consequence, when $\\sigma^{2}\\approx1$ , it holds with probability at least $1-e^{-\\hat{\\Omega}(M)}$ over the randomness of the sketch matrix S that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\mathcal{R}_{M}(\\mathbf{v}_{N})=\\sigma^{2}+\\Theta\\bigg(\\frac{1}{M^{a-1}}\\bigg)+\\Theta\\bigg(\\frac{1}{(N_{\\mathrm{eff}}\\gamma)^{(a-1)/a}}\\bigg),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the expectation is over the randomness of $\\mathbf{w}^{*}$ and $(\\mathbf{x}_{i},y_{i})_{i=1}^{N}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 shows a sharp (up to constant factors) scaling law risk bound under an isotroptic prior assumption and the power-law spectrum assumption. We emphasize that the scaling law bound in Theorem 4.1 holds for every $M,N\\ge1$ . We also remark that the sum of approximization and bias errors dominates $\\mathbb{E}\\mathcal{R}_{M}(\\mathbf{v}_{N})-\\sigma^{2}$ , whereas the variance error is of strict higher order in terms of both $M$ and $N$ , and is thus disappeared in the population risk bound. ", "page_idx": 5}, {"type": "text", "text": "Optimal stepsize. Based on the tight scaling law in Theorem 4.1, we can calculate the optimal stepsize that minimizes the risk. Specifically, the optimal stepsize is $\\gamma\\gtrsim1$ when $N_{\\mathsf{e f f}}\\lesssim M^{a}$ and can be anything such that $M^{a}/N_{\\mathrm{eff}}\\lesssim\\gamma\\lesssim1$ when $N_{\\mathsf{e f f}}\\gtrsim M^{a}$ . In both cases, choosing $\\gamma\\gtrsim1$ is optimal. When the sample size is large such that $N_{\\mathsf{e f f}}\\gtrsim M^{a}$ , the optimal stepsize is relatively robust and can be chosen from a range. ", "page_idx": 5}, {"type": "text", "text": "Allocation of data and model sizes. Following Hoffmann et al. [21], we measure the compute complexity by $M N$ as (SGD) queries $M$ -dimensional gradients for $N$ times. Given a total compute budget of $M N=C$ , from Theorem 6.1 and $N_{\\mathrm{eff}}:=N/\\log(N)$ , we see that the best population risk is achieved by setting $\\gamma=\\Theta(1)$ , $M=\\tilde{\\Theta}(C^{1/(a+1)})$ , and $\\begin{array}{r}{N=\\tilde{\\Theta}(C^{a/(a+1)})}\\end{array}$ . Our theory suggests setting a data size slightly larger than the model size when the compute budget is the bottleneck. ", "page_idx": 5}, {"type": "text", "text": "Comparison with [8]. The work by Bordelon et al. [8] considered the scaling law of batch gradient descent (or gradient flow) on a teacher-student model (see their equation (14)). Their teacher-student model can be viewed as our sketched linear regression model. However, we consider one-pass SGD, therefore in our setting the number of gradient steps is equivalent to the data size. When we equalize the number of gradient steps and the data size in their equation (14) and set the parameter prior as Assumption 1C, their prediction is consistent with ours. However, our analysis shows the computational advantage of SGD over batch GD since each iteration requires only $1/N$ the compute. Bordelon et al. [8] obtained the limit of the population risk as two out of the data size, model size, and the number of gradient steps go to infinity based on statistical physics heuristics. In comparison, we obtain upper and lower risk bounds that hold for any finite $M$ and $N$ and match ignoring a constant factor depending only on the spectrum power-law degree $a$ . ", "page_idx": 5}, {"type": "text", "text": "Average of the SGD iterates Results similar to Theorem 4.1 can also be established for the average of the iterates of online SGD with constant stepsize [34, 16, 25, 24, 48]. All results will be the same once replacing the effective sample size $N_{\\mathsf{e f f}}$ in Theorem 4.1 to the sample size $N$ . For more details see Theorem F.6 in Appendix F. ", "page_idx": 5}, {"type": "text", "text": "4.1 Scaling law under source condition ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The isotropic parameter prior condition (Assumption 1C) in Theorem 4.1 can be generalized to the following anisotropic version [11]. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 (Source condition). Let $(\\lambda_{i},\\mathbf{v}_{i})_{i>0}$ be the eigenvalues and eigenvectors of $\\mathbf{H}$ with $(\\lambda_{i})_{i>0}$ in non-increasing order. Assume $\\mathbf{w}^{*}$ satisfies a prior such that ", "page_idx": 5}, {"type": "equation", "text": "$$\nf o r\\;i\\neq j,\\;\\;\\mathbb{E}\\langle\\mathbf{v}_{i},\\mathbf{w}^{*}\\rangle\\langle\\mathbf{v}_{j},\\mathbf{w}^{*}\\rangle=0;\\;\\;a n d f o r\\;i>0,\\;\\;\\mathbb{E}\\lambda_{i}\\langle\\mathbf{v}_{i},\\mathbf{w}^{*}\\rangle^{2}\\approx i^{-b},\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A larger exponent $b$ implies a faster decay of signal $\\mathbf{w}^{*}$ and thus corresponds to a simpler task [11].   \nNote that Assumption 1C satisfies Assumption 3 with $b=a$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2 (Scaling law under source condition). In Theorem 4.1, suppose Assumption $I C$ is replaced by Assumption $3$ with $1<b<a+1$ . Then there exists some $a$ -dependent constant $c>0$ such that when $\\gamma\\leq c$ , with probability at least $1-e^{-\\Omega(M)}$ over the randomness of the sketch matrix S, we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\mathcal{R}_{M}(\\mathbf{v}_{N})=\\sigma^{2}+\\underbrace{\\Theta\\biggl(\\frac{1}{M^{b-1}}\\biggr)+\\Theta\\biggl(\\frac{1}{(N_{\\mathrm{eff}}\\,\\gamma)^{(b-1)/a}}\\biggr)}_{\\mathrm{Approx+Bias}}+\\underbrace{\\Theta\\biggl(\\frac{\\operatorname*{min}\\big\\{M,\\,(N_{\\mathrm{eff}}\\,\\gamma)^{1/a}\\bigr\\}}{N_{\\mathrm{eff}}}\\biggr)}_{\\mathrm{Vart}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the expectation is over the randomness of $\\mathbf{w}^{*}$ and $(\\mathbf{x}_{i},y_{i})_{i=1}^{N}$ , and $\\Theta(\\cdot)$ hides constants that may depend on $(a,b)$ . ", "page_idx": 6}, {"type": "text", "text": "When $1<b\\le a$ , the tasks are relatively hard (compared to when $b=a$ ), and the variance error is dominated by the sum of approximation and bias errors for all choices of $M$ , $N$ , and $\\gamma\\lesssim1$ . In this case, Theorem 4.2 gives the same prediction about optimal stepsize and optimal allocation of data and model sizes under compute budget as Theorem 4.1. ", "page_idx": 6}, {"type": "text", "text": "When $a<b<a+1$ , the tasks are relatively easy (compared to when $b=a$ ), and variance remains dominated by the sum of approximation and bias error if the stepsize is optimally tuned. Recall that $\\gamma\\lesssim1$ , thus we can rewrite the risk bound in Theorem 4.2 as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\mathcal{R}_{M}(\\mathbf{v}_{N})-\\sigma^{2}\\approx\\frac{1}{\\operatorname*{min}\\big\\{M,~(N_{\\mathrm{eff}}\\gamma)^{1/a}\\big\\}^{b-1}}+\\frac{\\operatorname*{min}\\big\\{M,~(N_{\\mathrm{eff}}\\gamma)^{1/a}\\big\\}}{N_{\\mathrm{eff}}}}\\\\ &{\\qquad\\qquad\\qquad\\approx\\left\\{\\operatorname*{min}\\big\\{M,~(N_{\\mathrm{eff}}\\gamma)^{1/a}\\big\\}/N_{\\mathrm{eff}}\\quad M\\gtrsim N_{\\mathrm{eff}}^{1/b}\\mathrm{~and~}N_{\\mathrm{eff}}^{a/b-1}\\lesssim\\gamma\\lesssim1,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad M\\lesssim N_{\\mathrm{eff}}^{1/b}\\,\\mathrm{orf}\\,\\,\\gamma\\lesssim N_{\\mathrm{eff}}^{a/b-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Therefore the optimal stepsize and the risk under the optimal stepsize is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\gamma\\asymp N_{\\mathrm{eff}}^{a/b-1}\\,\\mathrm{if}\\;M\\gtrsim N_{\\mathrm{eff}}^{1/b},\\mathrm{~\\:~\\:~\\and~\\:~}M^{a}/N_{\\mathrm{eff}}\\lesssim\\gamma\\lesssim1\\,\\mathrm{if}\\;M\\lesssim N_{\\mathrm{eff}}^{1/b}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Under the optimally tuned stepsize, the population risk is in the form of ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\gamma}\\mathbb{E}\\mathcal{R}_{M}(\\mathbf{v}_{N})=\\sigma^{2}+\\Theta(N_{\\mathrm{eff}}^{(1-b)/b})+\\Theta(M^{1-b}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which is again in the scaling law form (1). This is expected since an optimally tuned stepsize controls the variance error by adjusting the strength of the implicit bias of SGD. Under a fixed compute budget $C=M N$ , our theory suggests to assign $M=\\tilde{\\Theta}(C^{1/(b+1)})$ and $N=\\tilde{\\Theta}(C^{b/(b+1)})$ , and set the stepsize to $\\gamma\\approx\\tilde{\\Theta}(C^{(\\stackrel{.}{a}-b)/(b+1)})$ . ", "page_idx": 6}, {"type": "text", "text": "When $b\\geq a+1$ , the tasks are even simpler. We provide upper and lower bounds in Appendix D.3.   \nHowever, there exists a gap between the bounds, fixing which is left for future work. ", "page_idx": 6}, {"type": "text", "text": "Moreover, we note that in the comparable regimes where $M$ is large, the results in Theorem 4.2 match existing bounds on the risk of SGD iterates and ridge estimators [33, 36]. ", "page_idx": 6}, {"type": "text", "text": "4.2 Scaling law under logarithmic power law ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We also derive the risk formula when the data covariance has a logarithmic power-law spectrum [5]. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4 (Logarithmic power-law spectrum). There exists $a>1$ such that the eigenvalues of $\\mathbf{H}$ satisfy $\\lambda_{i}\\approx i^{-1}\\log^{-a}(i+1),\\,i>0$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3 (Scaling law under logarithmic power spectrum). In Theorem 4.1, suppose Assumption 2 is replaced by Assumption 4. Then with probability at least $1-e^{-\\Omega(M)}$ over the randomness of the sketch matrix S, we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\mathcal{R}_{M}(\\mathbf{v}_{N})=\\sigma^{2}+\\Theta\\Big(\\frac{1}{\\log^{a-1}(M)}\\Big)+\\Theta\\Big(\\frac{1}{\\log^{a-1}(N_{\\mathrm{eff}}\\,\\gamma)}\\Big),\\quad\\forall\\mathsf{a r}\\approx\\frac{\\operatorname*{min}\\big\\{M,\\,\\frac{N_{\\mathrm{eff}}\\,\\gamma}{\\log^{a}(N_{\\mathrm{eff}}\\,\\gamma)}\\big\\}}{N_{\\mathrm{eff}}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the expectation is over the randomness of $\\mathbf{w}^{*}$ and $(\\mathbf{x}_{i},y_{i})_{i=1}^{N}$ . ", "page_idx": 6}, {"type": "image", "img_path": "PH7sdEanXP/tmp/081ae5f4e58e8cbfa37e545807fdd2fcfb32d4f761fd057d00efe43bfb11dae9.jpg", "img_caption": ["Figure 2: The expected risk of the last iterate of (SGD) minus the irreducible risk versus the effective sample size and model size. Parameters $\\sigma=1,\\gamma=0.1.$ . (a), (b): $a=1.5$ , $d=10000$ ; (c), (d): $a=2$ , $d=1000$ . The error bars denote the $\\pm1$ standard deviation of estimating the expected risk using 100 independent samples of $(\\mathbf{w}^{*},\\mathbf{S})$ . We use linear functions to fti the expected risk under the log-log scale and report the slope of the ftited lines (denoted by $k$ ). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Theorem 4.3 provides a scaling law under the logarithmic power-law spectrum. Similar to Theorem 4.1, the variance error is dominated by the approximation and bias errors for all choices of $M$ , $N$ , and $\\gamma$ , and thus disappeared from the risk bound. Different from Theorem 4.1, here the population risk is a polynomial of $\\bar{\\log}(M)$ and $\\log(N_{\\tt e f f}\\gamma)$ . ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we examine the relation between the expected risk of the (SGD) output, the data size $N$ , and the model size $M$ when the covariates satisfy a power-law covariance spectrum. Although our results in Section 4 hold with high probability over S, for simplicity, we assume the expectation of the risk is taken over both $\\mathbf{w}^{*}$ and S in our simulations. We adopt the model in Section 3 and train it using one-pass (SGD) with geometric decaying stepsize (3). We choose the dimension $d$ sufficiently large to approximate the infinite-dimensional case, and the data are generated so that Assumption 1 is satisfied. Moreover, we choose the covariance $\\mathbf{H}\\,\\in\\,\\mathbb{R}^{d\\times d}$ to be diagonal with $\\mathbf{H}_{i i}\\propto i^{a}$ and $\\operatorname{tr}(\\mathbf{H})=1$ for some $a>1$ . From Figure 1, we observe that the risk indeed follows a power-law formula jointly in the number of samples and the number of parameters. In addition, the ftited exponents are aligned with our theoretical predictions $(a-1,1-1/{a})$ in Theorem 4.1. Figure 2 shows the scaling of the expected risk in data size (or model size) when the model size (or data size) is relatively large. We see that the expected risk also satisfies a power-law decay with exponents matching our predictions. It is noteworthy that our simulations demonstrate stronger observations than the theoretical results in Theorem 4.1, which only establishes matching upper and lower bounds up to a constant factor. Additional simulation results on the risk of the average of (SGD) iterates can be found in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "6 Risk bounds under a general spectrum ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present some general results on the upper and lower bounds of the risk of the output of (SGD). Due to the rotational invariance of the sketched matrix S, without loss of generality, we assume the covariance $\\mathbf{H}$ is diagonal with non-increasing diagonal entries. Our main results in Section 4 are directly built on the general bounds introduced here. ", "page_idx": 7}, {"type": "text", "text": "Assumption 5 (General distributional conditions). Assume the following about the data distribution. ", "page_idx": 7}, {"type": "text", "text": "A. Hypercontractivity. There exists $\\alpha\\geq1$ such that for every PSD matrix A it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{\\mathbb{E}}\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}\\mathbf{x}^{\\top}\\preceq\\alpha\\operatorname{tr}(\\mathbf{H}\\mathbf{A})\\mathbf{H}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "B. Misspecified model. There exists $\\sigma^{2}>0$ such that $\\mathbb{E}(y-\\mathbf{x}^{\\top}\\mathbf{w}^{*})^{2}\\mathbf{x}\\mathbf{x}^{\\top}\\preceq\\sigma^{2}\\mathbf{H}.$ ", "page_idx": 7}, {"type": "text", "text": "It is clear that Assumption 1 implies Assumption 5 with $\\alpha=3$ . ", "page_idx": 7}, {"type": "text", "text": "Excess risk decomposition. Conditioning on the sketch matrix S, the training of the sketched linear predictor can be viewed as an $M$ -dimensional linear regression problem. We can therefore ", "page_idx": 7}, {"type": "text", "text": "invoke existing SGD analysis [44, 45] to sharply control the excess risk by controlling the bias and variance errors. Specifically, let us define the ( $\\mathbf{\\DeltaW}^{*}$ -dependent) bias error as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{Bias}(\\mathbf{w}^{*}):=\\bigg\\|\\prod_{t=1}^{N}\\big(\\mathbf{I}-\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)\\mathbf{v}^{*}\\bigg\\|_{\\mathrm{SHS}^{\\top}}^{2},\\quad\\mathrm{where~}\\mathbf{v}^{*}:=(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and the variance error as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathsf{V a r}:=\\frac{\\#\\{\\tilde{\\lambda}_{j}\\geq1/(N_{\\mathrm{eff}}\\gamma)\\}+(N_{\\mathrm{eff}}\\gamma)^{2}\\sum_{\\tilde{\\lambda}_{j}<1/(N_{\\mathrm{eff}}\\gamma)}\\tilde{\\lambda}_{j}^{2}}{N_{\\mathrm{eff}}},\\quad N_{\\mathrm{eff}}:=N/\\log(N),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\left(\\tilde{\\lambda}_{j}\\right)_{j=1}^{M}$ e.igenvalues of $\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}$ . We also let $\\mathsf{B i a s}:=\\mathbb{E B i a s}(\\mathbf{w}^{*})$ where  [t4h4e  e4x5p]ectation is $\\mathbf{w}^{*}$ $\\mathrm{Wu}$ that the excess risk in (4) can be exactly decomposed as the sum of bias and variance errors under weak conditions. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6.1 (Excess risk decomposition). Conditioning on the sketch matrix S, consider the excess risk in (4) induced by the output of (SGD). Assume $\\mathbf{v}_{0}=0$ . Then for any $\\mathbf{w}^{*}\\in\\mathbb{H}$ , ", "page_idx": 8}, {"type": "text", "text": "1. Under Assumptions $5A$ and $5B$ and suppose $\\gamma\\leq1/\\big(c\\alpha\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\big)$ for some constant $c>0$ , we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}{\\sf E x c e s s}\\lesssim{\\sf B i a s}({\\bf w}^{*})+(\\alpha\\|{\\bf w}^{*}\\|_{\\bf H}^{2}+\\sigma^{2}){\\sf V a r}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "2. Under the stronger Assumptions 1A and $I B$ and suppose $\\gamma\\leq1/\\big(c\\alpha\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\big)$ for some constant $c>0$ , we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\mathsf{E x c e s s}\\gtrsim\\mathsf{B i a s}(\\mathbf{w}^{*})+\\sigma^{2}\\mathsf{V a r}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In both results, the expectations of Excess are taken over $(\\mathbf{x}_{t},y_{t})_{t=1}^{N}$ ", "page_idx": 8}, {"type": "text", "text": "Assuming that the signal-to-noise ratio is upper bounded, that is, $\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2}/\\sigma^{2}\\lesssim1$ , then the biasvariance decomposition of the excess risk is sharp up to constant factors. ", "page_idx": 8}, {"type": "text", "text": "The variance error is in a nice form and can be computed using the following important lemma on the spectrum of $\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}$ . Similar results for logarithmic power-law are also established in Lemma G.6 in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "Lemma 6.2 (Power law). Under Assumption 2, it holds with probability at least $1-e^{-\\Omega(M)}$ that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mu_{j}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\approx\\mu_{j}(\\mathbf{H})\\approx j^{-a},\\quad j=1,\\ldots,M.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "For any $0\\leq k^{*}\\leq k^{\\dagger}\\leq\\infty$ , let $\\mathbf{S}_{k^{*}:k^{\\dagger}}\\in\\mathbb{R}^{M\\times(k^{\\dagger}-k^{*})}$ denote the matrix formed by the $k^{*}+1-k^{\\dagger}$ -th columns of S. We also abuse the notation $k^{\\dagger}:\\infty$ for $k^{\\dagger}:d$ when $d$ is finite. We let $\\mathbf{H}_{k^{*}:k^{\\dagger}}\\in$ $\\mathbb{R}^{(k^{\\dagger}-k^{*})\\times(k^{\\dagger}-k^{*})}$ be the submatrix of $\\mathbf{H}$ formed by the $k^{*}+1-k^{\\dagger}$ -th eigenvalues. For the approximation and bias error, we use the following upper and lower bounds to compute their values. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6.3 (A general upper bound). Suppose Assumption $5$ holds. Assume $\\mathbf{v}_{0}=0$ , $r(\\mathbf{H})\\geq2M$ and the initial stepsize satisfies $\\gamma\\,<\\,1/(c\\alpha\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}))$ for some constant $c\\,>\\,0$ . Then for any $k_{1},k_{2}\\leq M/3$ , with probability at least $1-e^{-\\Omega(M)}$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{A p p r o x}\\lesssim\\|\\mathbf{w}_{k_{1}:\\infty}^{*}\\|_{\\mathbf{H}_{k_{1}:\\infty}}^{2}+\\left(\\frac{\\sum_{i>k_{1}}\\lambda_{i}}{M}+\\lambda_{k_{1}+1}+\\sqrt{\\frac{\\sum_{i>k_{1}}\\lambda_{i}^{2}}{M}}\\right)\\|\\mathbf{w}_{0:k_{1}}^{*}\\|^{2},}\\\\ &{\\mathsf{B i a s}(\\mathbf{w}^{*})\\lesssim\\frac{\\|\\mathbf{w}_{0:k_{2}}^{*}\\|_{2}^{2}}{N_{\\mathrm{eff}}\\gamma}\\cdot\\left[\\frac{\\mu_{M/2}\\left(\\mathbf{S}_{k_{2}:\\infty}\\mathbf{H}_{k_{2}:\\infty}\\mathbf{S}_{k_{2}:\\infty}^{\\top}\\right)}{\\mu_{M}\\left(\\mathbf{S}_{k_{2}:\\infty}\\mathbf{H}_{k_{2}:\\infty}\\mathbf{S}_{k_{2}:\\infty}^{\\top}\\right)}\\right]^{2}+\\|\\mathbf{w}_{k_{2}:\\infty}^{*}\\|_{\\mathbf{H}_{k_{2}:\\infty}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Theorem 6.4 (A general lower bound). Suppose Assumption $I$ holds. Assume $\\mathbf{v}_{0}=0$ , $r(\\mathbf{H})\\geq M$ and the initial stepsize $\\gamma<1/\\bigl(c\\,\\mathrm{tr}\\bigl(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\bigr)\\bigr)$ for some constant $c>0$ . Then ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\gtrsim\\sum_{i=M}^{d}\\lambda_{i},\\quad\\;\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{B i a s}(\\mathbf{w}^{*})\\gtrsim\\sum_{i:\\tilde{\\lambda}_{i}<1/(N_{\\mathrm{eff}}\\,\\gamma)}\\frac{\\mu_{i}(\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top})}{\\mu_{i}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "almost surely, where $(\\lambda_{i})_{i=1}^{d}$ are eigenvalues of $\\mathbf{H}$ in non-increasing order, $(\\widetilde{\\lambda}_{i})_{i=1}^{d}$ are eigenvalues of $\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}$ in non-increasing order. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We analyze neural scaling laws in infinite-dimensional linear regression. We consider a linear predictor with $M$ trainable parameters on the sketched covariates, which is trained by one-pass stochastic gradient descent with $N$ data. Under a Gaussian prior assumption on the optimal model parameter and a power law (of degree $a>1$ ) assumption on the spectrum of the data covariance, we derive matching upper and lower bounds on the population risk minus the irreducible error, that is, $\\Theta(M^{-(a-1)}+{\\bar{N}}^{{\\dot{-}}{\\dot{(a-1)}}/a})$ . In particular, we show that the variance error, which increases with $M$ , is of strictly higher order compared to the other errors, thus disappearing from the risk bound. We attribute the nice empirical formula of the neural scaling law to the non-domination of the variance error, which ultimately is an effect of the implicit regularization of SGD. ", "page_idx": 9}, {"type": "text", "text": "Many directions remain open for future study. First, our work is limited to the linear model; it would be interesting to see whether similar scaling laws can be derived in more complex models, such as random feature models or two-layer networks. Second, we focus on one-pass SGD training, and it is unclear if similar results hold for other optimization methods like accelerated SGD or Adam. Additionally, from a technical perspective, many results in our work depend on the Gaussian assumption and the source condition of the data. Investigating how these assumptions can be relaxed would also be valuable. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We gratefully acknowledge the support of the NSF for FODSI through grant DMS-2023505, of the NSF and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning through awards DMS-2031883 and #814639, and of the ONR through MURI award N000142112431. JDL acknowledges support of the NSF CCF 2002272, NSF IIS 2107304, and NSF CAREER Award 2144994. SMK acknowledges a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence; support from ONR under award N000142212377, and NSF under award IIS 2229881. ", "page_idx": 9}, {"type": "text", "text": "Bibliography ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. Advances in Neural Information Processing Systems, 35:22300\u2013 22312, 2022.   \n[2] Alexander B Atanasov, Jacob A Zavatone-Veth, and Cengiz Pehlevan. Scaling and renormalization in high-dimensional regression. arXiv preprint arXiv:2405.00592, 2024.   \n[3] Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate $o(1/n)$ . Advances in neural information processing systems, 26:773\u2013781, 2013.   \n[4] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. arXiv preprint arXiv:2102.06701, 2021.   \n[5] Peter L Bartlett, Philip M Long, G\u00e1bor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 2020.   \n[6] Rapha\u00ebl Berthier, Francis Bach, and Pierre Gaillard. Tight nonparametric convergence rates for stochastic gradient descent under the noiseless linear model. Advances in Neural Information Processing Systems, 33:2576\u20132586, 2020.   \n[7] Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You. Chinchilla scaling: A replication attempt. arXiv preprint arXiv:2404.10102, 2024.   \n[8] Blake Bordelon, Alexander Atanasov, and Cengiz Pehlevan. A dynamical model of neural scaling laws. arXiv preprint arXiv:2402.01092, 2024. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[10] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. arXiv preprint arXiv:2210.14891, 2022.   \n[11] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7:331\u2013368, 2007.   \n[12] Luigi Carratino, Alessandro Rudi, and Lorenzo Rosasco. Learning with sgd and random features. Advances in neural information processing systems, 31, 2018.   \n[13] Leonardo Defliippis, Bruno Loureiro, and Theodor Misiakiewicz. Dimension-free deterministic equivalents for random feature regression. arXiv preprint arXiv:2405.15699, 2024.   \n[14] Alexandre D\u00e9fossez and Francis Bach. Averaged least-mean-squares: Bias-variance trade-offs and optimal sampling distributions. In Artificial Intelligence and Statistics, pages 205\u2013213, 2015.   \n[15] Aymeric Dieuleveut and Francis R. Bach. Non-parametric stochastic approximation with large step sizes. The Annals of Statistics, 2015.   \n[16] Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger convergence rates for least-squares regression. The Journal of Machine Learning Research, 18 (1):3520\u20133570, 2017.   \n[17] Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, and Julia Kempe. A tale of tails: Model collapse as a change of scaling laws. arXiv preprint arXiv:2402.07043, 2024.   \n[18] Rong Ge, Sham M Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A near optimal, geometrically decaying learning rate procedure for least squares. Advances in neural information processing systems, 32, 2019.   \n[19] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.   \n[20] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.   \n[21] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \n[22] Marcus Hutter. Learning curve theory. arXiv preprint arXiv:2102.04074, 2021.   \n[23] Ayush Jain, Andrea Montanari, and Eren Sasoglu. Scaling laws for learning with real and surrogate data. arXiv preprint arXiv:2402.04376, 2024.   \n[24] Prateek Jain, Praneeth Netrapalli, Sham M Kakade, Rahul Kidambi, and Aaron Sidford. Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification. The Journal of Machine Learning Research, 18(1):8258\u20138299, 2017.   \n[25] Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, Venkata Krishna Pillutla, and Aaron Sidford. A Markov Chain Theory Approach to Characterizing the Minimax Optimality of Stochastic Gradient Descent (for Least Squares). In 37th IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS 2017), 2018.   \n[26] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[27] Aran Komatsuzaki. One epoch is all you need. arXiv preprint arXiv:1906.06669, 2019.   \n[28] Alexander Maloney, Daniel A Roberts, and James Sully. A solvable model of neural scaling laws. arXiv preprint arXiv:2210.16859, 2022.   \n[29] Eric Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural scaling. Advances in Neural Information Processing Systems, 36, 2024.   \n[30] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018.   \n[31] Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. arXiv preprint arXiv:2305.16264, 2023.   \n[32] Yoonsoo Nam, Nayara Fonseca, Seok Hyeong Lee, and Ard Louis. An exactly solvable model for emergence and scaling laws. arXiv preprint arXiv:2404.17563, 2024.   \n[33] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. Advances in Neural Information Processing Systems, 31, 2018.   \n[34] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838\u2013855, 1992.   \n[35] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. arXiv preprint arXiv:1909.12673, 2019.   \n[36] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. Advances in neural information processing systems, 30, 2017.   \n[37] Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data manifold. arXiv preprint arXiv:2004.10802, 2020.   \n[38] William Swartworth and David P Woodruff. Optimal eigenvalue approximation via sketching. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 145\u2013155, 2023.   \n[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[40] Aditya Vardhan Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Last iterate convergence of SGD for least-squares in the interpolation regime. In Advances in Neural Information Processing Systems, 2021.   \n[41] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press, 2019.   \n[42] Alexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how real-world neural representations generalize. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, 2022.   \n[43] David P Woodruff et al. Sketching as a tool for numerical linear algebra. Foundations and Trends\u00ae in Theoretical Computer Science, 10(1\u20132):1\u2013157, 2014.   \n[44] Jingfeng Wu, Difan Zou, Vladimir Braverman, Quanquan Gu, and Sham M. Kakade. Last iterate risk bounds of sgd with decaying stepsize for overparameterized linear regression. The 39th International Conference on Machine Learning, 2022.   \n[45] Jingfeng Wu, Difan Zou, Vladimir Braverman, Quanquan Gu, and Sham M. Kakade. The power and limitation of pretraining-finetuning for linear regression under covariate shift. The 36th Conference on Neural Information Processing Systems, 2022.   \n[46] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12104\u201312113, 2022.   \n[47] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, Dean P Foster, and Sham Kakade. The beneftis of implicit regularization from sgd in least squares problems. Advances in Neural Information Processing Systems, 34:5456\u20135468, 2021.   \n[48] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Benign overftiting of constant-stepsize sgd for linear regression. Journal of Machine Learning Research, 24(326):1\u201358, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Preliminary 14   \nA.1 Additional notations and comments on data assumptions 14   \nA.2 Approximation error . 15   \nA.3 Bias-variance decomposition 16   \nA.4 Proof of Theorem 6.1 17   \nA.5 Proofs of Lemma 6.2, Theorem 6.3 and 6.4 . 18   \nB Proofs in Section 4 18   \nB.1 Proof of Theorem 4.1 18   \nB.2 Proof of Theorem 4.2 19   \nB.3 Proof of Theorem 4.3 19   \nApproximation error 19   \nC.1 An upper bound 20   \nC.2 A lower bound 22   \nC.3 A lower bound under Assumption 3 23   \nC.4 Examples on matching bounds for Approx 24   \nD Bias error 26   \nD.1 An upper bound 26   \nD.2 A lower bound 28   \nD.3 Examples on matching bounds for $\\mathsf{B i a s}(\\mathbf{w}^{*})$ 29 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "E Variance error 32 ", "page_idx": 13}, {"type": "text", "text": "F Expected risk of the average of (SGD) iterates 32   \nF.1 Matching bounds for the average of (SGD) iterates under power-law spectrum . . 34   \nF.2 Proofs 34   \nG Concentration lemmas 38   \nG.1 General concentration results 38   \nG.2 Concentration results under power-law spectrum 42   \nG.3 Concentration results under logarithmic power-law spectrum 43 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Preliminary ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide some preliminary discussions and a proof of Theorem 6.1. Concretely, in Section A.1 we discuss our data assumptions and introduce additional notations. In Section A.2, A.3 we derive intermediate results that contribute to the proof of Theorem 6.1. Finally, a complete proof of Theorem 6.1 is contained in Section A.4. ", "page_idx": 13}, {"type": "text", "text": "A.1 Additional notations and comments on data assumptions ", "page_idx": 13}, {"type": "text", "text": "Tensors. For matrices A, B, C, D, and $\\mathbf{X}$ of appropriate shape, it holds that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\circ\\mathbf{X}=\\mathbf{A}\\mathbf{X}\\mathbf{B},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and that ", "page_idx": 13}, {"type": "equation", "text": "$$\n(\\mathbf{D}^{\\top}\\otimes\\mathbf{C})\\circ(\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\circ\\mathbf{X}=\\left((\\mathbf{D}^{\\top}\\mathbf{B}^{\\top})\\otimes(\\mathbf{C}\\mathbf{A})\\right)\\circ\\mathbf{X}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For simplicity, we denote ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{A}^{\\otimes2}:=\\mathbf{A}\\otimes\\mathbf{A}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Comments on Assumption 2, 3 and 4 Due to the rotational invariance of the Gaussian sketched matrix S, throughout the appendix, we assume w.l.o.g. that the covariance of the input covariates $\\mathbf{H}$ is diagonal with the $(i,i)$ -th entry being the $i$ -th eigenvalue. Specifically, Assumption 3 can be rewritten as ", "page_idx": 14}, {"type": "text", "text": "Assumption 6 (Source condition). Assume $\\mathbf{H}\\,=\\,(h_{i j})_{i,j\\ge1}$ is a diagonal matrix with diagonal entries in non-increasing order, and $\\mathbf{w}^{*}$ satisfies a prior such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathit{f o r}\\ i\\neq j,\\ \\mathbb{E}\\mathbf{w}_{i}^{*}\\mathbf{w}_{j}^{*}=0;\\ a n d\\mathit{f o r}\\ i>0,\\ \\mathbb{E}\\lambda_{i}\\mathbf{w}_{i}^{*2}\\approx i^{-b},\\ \\mathit{f o r}\\ s o m e\\ b>1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now that we assume $\\mathbf{H}$ is diagonal. We make the following notations. Define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{H}_{k^{*}:k^{\\dagger}}:=\\mathrm{diag}(\\lambda_{k^{*}+1},\\ldots,\\lambda_{k^{\\dagger}})\\in\\mathbb{R}^{(k^{\\dagger}-k^{*})^{2}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $0\\leq k^{*}\\leq k^{\\dagger}$ are two integers, and we allow $k^{\\dagger}=\\infty$ . For example, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{H}_{0:k}=\\mathrm{diag}(\\lambda_{1},\\dots,\\lambda_{i}),\\quad\\mathbf{H}_{k:\\infty}=\\mathrm{diag}(\\lambda_{k+1},\\dots).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, for a vector $\\mathbf{w}\\in\\mathbb{H}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{w}_{k^{*}:k^{\\dagger}}:=\\big(\\mathbf{w}_{k^{*}+1},\\ldots,\\mathbf{w}_{k^{\\dagger}}^{*}\\big)^{\\top}\\in\\mathbb{R}^{k^{\\dagger}-k^{*}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 Approximation error ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recall the risk decomposition in (4), ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{R}_{M}(\\mathbf{v}_{N})=\\underbrace{\\operatorname*{min}_{\\mathbf{\\mathcal{R}}(\\cdot)}+\\operatorname*{min}_{\\mathbf{\\mathcal{R}}_{M}(\\cdot)}-\\operatorname*{min}_{\\mathbf{\\mathcal{R}}(\\cdot)}}_{\\mathrm{lreducible}}+\\underbrace{\\operatorname*{min}_{\\mathbf{\\mathcal{R}}_{M}(\\cdot)}-\\operatorname*{min}_{\\mathbf{\\mathcal{R}}(\\cdot)}}_{\\mathrm{Approx}}+\\underbrace{\\mathcal{R}_{M}(\\mathbf{v}_{N})-\\operatorname*{min}_{\\mathbf{\\mathcal{R}}_{M}(\\cdot)}}_{\\mathrm{Excess}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma A.1 (Approximization error). Conditional on the sketch matrix S, the minimizer of ${\\mathcal{R}}_{M}({\\bf{v}})$ is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{v}^{*}:=(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the approximation error in (4) is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{A p p r o x}:=\\operatorname*{min}\\mathcal{R}_{M}(\\cdot)-\\operatorname*{min}\\mathcal{R}(\\cdot)}\\\\ &{\\qquad\\quad=\\Big\\|\\Big(\\mathbf{I}-\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}\\big(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)^{-1}\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\Big)\\mathbf{H}^{\\frac{1}{2}}\\mathbf{w}^{*}\\Big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, Approx $\\leq\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2}$ almost surely over the randomness of S. ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma A.1. Recall that the risk ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\mathbf{w}):=\\mathbb{E}\\!\\left(\\langle\\mathbf{x},\\mathbf{w}\\rangle-y\\right)^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "is a quadratic function and that $\\mathbf{w}^{*}$ is the minimizer of $\\mathcal{R}(\\cdot)$ , so we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}\\mathbf{x}^{\\otimes2}\\right)\\mathbf{w}^{*}=\\mathbb{E}\\mathbf{x}y\\Leftrightarrow\\mathbf{H}\\mathbf{w}^{*}=\\mathbb{E}\\mathbf{x}y,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal{R}}(\\mathbf{w})=\\mathbb{E}\\big(\\langle\\mathbf{x},\\mathbf{w}\\rangle-\\langle\\mathbf{x},\\mathbf{w}^{*}\\rangle\\big)^{2}+{\\mathcal{R}}(\\mathbf{w}^{*})}\\\\ &{\\qquad\\quad=\\|\\mathbf{H}^{\\frac{1}{2}}(\\mathbf{w}-\\mathbf{w}^{*})\\|^{2}+{\\mathcal{R}}(\\mathbf{w}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall that the risk in a restricted subspace ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{R}_{M}(\\mathbf{v}):=\\mathcal{R}(\\mathbf{S}^{\\top}\\mathbf{v})=\\mathbb{E}\\big(\\langle\\mathbf{S}\\mathbf{x},\\mathbf{v}\\rangle-y\\big)^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "is also a quadratic function, so its minimizer is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{v}^{\\ast}=\\left(\\mathbb{E}(\\mathbf{S}\\mathbf{x})^{\\otimes2}\\right)^{-1}\\mathbb{E}\\mathbf{S}\\mathbf{x}y\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{\\Xi}=\\big(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, the approximation error is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathsf{A p p r o x}}:=\\mathcal{R}_{M}(\\mathbf{v}^{*})-\\mathcal{R}(\\mathbf{w}^{*})}\\\\ &{\\qquad\\quad=\\mathcal{R}(\\mathbf{S}^{\\top}\\mathbf{v}^{*})-\\mathcal{R}(\\mathbf{w}^{*})}\\\\ &{\\qquad\\quad=\\|\\mathbf{H}^{\\frac{1}{2}}(\\mathbf{S}^{\\top}\\mathbf{v}^{*}-\\mathbf{w}^{*})\\|^{2}}\\\\ &{\\qquad\\quad=\\left\\|\\mathbf{H}^{\\frac{1}{2}}(\\mathbf{S}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}-\\mathbf{w}^{*})\\right\\|^{2}}\\\\ &{\\qquad\\quad=\\left\\|\\left(\\mathbf{I}-\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}\\big(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)^{-1}\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\right)\\mathbf{H}^{\\frac{1}{2}}\\mathbf{w}^{*}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, since ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\mathbf{I}-\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}\\left(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\right)^{-1}\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\right)^{2}=\\mathbf{I}-\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}\\left(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\right)^{-1}\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\preceq\\mathbf{I},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "it follows that $\\mathsf{A p p r o x}\\leq\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2}$ . ", "page_idx": 15}, {"type": "text", "text": "A.3 Bias-variance decomposition ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The excess risk in (4) can be viewed as the SGD excess risk in an $M$ -dimensional (misspecified) linear regression problem. We will utilize Corollary 3.4 in [45] to get a bias-variance decomposition of the excess risk. The following two lemmas check the related assumptions for Corollary 3.4 in [45] in our setup. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.2 (Hypercontractivity and the misspecified noise under sketched feature). Suppose that Assumptions $5A$ and $5B$ hold. Conditioning on the sketch matrix S, for every PSD matrix $\\mathbf{A}\\in\\mathbb{R}^{M\\times M}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}(\\mathbf{S}\\mathbf{x})^{\\otimes2}\\mathbf{A}(\\mathbf{S}\\mathbf{x})^{\\otimes2}\\preceq\\alpha\\operatorname{tr}\\big(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\mathbf{A}\\big)\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, for the minimizer of ${\\mathcal{R}}_{M}({\\bf{v}})$ , that is, $\\mathbf{v}^{*}$ defined in Lemma A.1, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big(y-\\langle\\mathbf{v}^{*},\\mathbf{S}\\mathbf{x}\\rangle\\big)^{2}(\\mathbf{S}\\mathbf{x})^{\\otimes2}\\preceq2(\\sigma^{2}+\\alpha\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2})\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The expectation in the above is over $(\\mathbf{x},y)$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma A.2. The first part is a direct application of Assumption 5A: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}(\\mathbf{S}\\mathbf{x})^{\\otimes2}\\mathbf{A}(\\mathbf{S}\\mathbf{x})^{\\otimes2}=\\mathbf{S}\\big(\\mathbb{E}\\mathbf{x}\\mathbf{x}^{\\top}(\\mathbf{S}^{\\top}\\mathbf{A}\\mathbf{S})\\mathbf{x}\\mathbf{x}^{\\top}\\big)\\mathbf{S}^{\\top}}\\\\ &{\\qquad\\qquad\\qquad\\preceq\\mathbf{S}\\big(\\alpha\\operatorname{tr}\\big(\\mathbf{H}\\mathbf{S}^{\\top}\\mathbf{A}\\mathbf{S}\\big)\\mathbf{H}\\big)\\mathbf{S}^{\\top}}\\\\ &{\\qquad\\qquad\\qquad=\\alpha\\operatorname{tr}\\big(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\mathbf{A}\\big)\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the second part, we first show that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big(y-\\langle\\mathbf{v^{*}},\\mathbf{S}\\mathbf{x}\\rangle\\big)^{2}\\mathbf{x}^{\\otimes2}\\preceq2\\mathbb{E}\\big(y-\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle\\big)^{2}\\mathbf{x}^{\\otimes2}+2\\mathbb{E}\\langle\\mathbf{w}^{*}-\\mathbf{S}^{\\top}\\mathbf{v}^{*},\\mathbf{x}\\rangle^{2}\\mathbf{x}^{\\otimes2}}\\\\ &{\\qquad\\qquad\\qquad\\preceq2\\sigma^{2}\\mathbf{H}+2\\alpha\\langle\\mathbf{H},(\\mathbf{w}^{*}-\\mathbf{S}^{\\top}\\mathbf{v}^{*})^{\\otimes2}\\rangle\\mathbf{H},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality is by Assumptions 5A and 5B. From the proof of Lemma A.1, we know that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\langle\\mathbf{H},(\\mathbf{w}^{*}-\\mathbf{S}^{\\top}\\mathbf{v}^{*})^{\\otimes2}\\rangle=\\mathsf{A p p r o x}\\leq\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big(\\boldsymbol{y}-\\langle\\mathbf{v}^{*},\\mathbf{S}\\mathbf{x}\\rangle\\big)^{2}\\mathbf{x}^{\\otimes2}\\preceq2(\\sigma^{2}+\\alpha\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2})\\mathbf{H}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Left and right multiplying both sides with $\\mathbf{S}$ and $\\mathbf{S}^{\\top}$ , we obtain the second claim. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3 (Gaussianity and well-specified noise under sketched features). Suppose that Assumptions 1A and $I B$ hold. Conditional on the sketch matrix $\\mathbf{S}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{S}\\mathbf{x}\\sim\\mathcal{N}(0,\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, for the minimizer of ${\\mathcal{R}}_{M}({\\bf{v}})$ , that is, $\\mathbf{v}^{*}$ defined in Lemma A.1, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[y|\\mathbf{S}\\mathbf{x}]=\\langle\\mathbf{S}\\mathbf{x},\\mathbf{v}^{*}\\rangle,\\quad\\mathbb{E}(y-\\langle\\mathbf{S}\\mathbf{x},\\mathbf{v}^{*}\\rangle)^{2}=\\sigma^{2}+\\mathsf{A}\\mathsf{p}\\mathsf{p}\\mathsf{r o}\\mathbf{x}\\geq\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma A.3. The first claim is a direct consequence of Assumption 1A. ", "page_idx": 16}, {"type": "text", "text": "For the second claim, by Assumption 1A and Lemma A.1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[y|\\mathbf{x}]=\\langle\\mathbf{x},\\mathbf{w}^{*}\\rangle}\\\\ &{\\quad\\quad=\\langle\\mathbf{x},\\mathbf{S}^{\\top}\\mathbf{v}^{*}\\rangle+\\langle\\mathbf{x},\\mathbf{w}^{*}-\\mathbf{S}^{\\top}\\mathbf{v}^{*}\\rangle}\\\\ &{\\quad\\quad=\\langle\\mathbf{x},\\mathbf{S}^{\\top}\\mathbf{v}^{*}\\rangle+\\langle\\mathbf{x},\\left[\\mathbf{I}-(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\right]\\mathbf{w}^{*}\\rangle}\\\\ &{\\quad\\quad=\\langle\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{x},\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}\\mathbf{v}^{*}\\rangle+\\langle\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{x},\\left[\\mathbf{I}-\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\right]\\mathbf{H}^{\\frac{1}{2}}\\mathbf{w}^{*}\\rangle}\\\\ &{\\quad\\quad=\\langle\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{x},\\mathbf{v}^{*}\\rangle+\\langle\\left[\\mathbf{I}-\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\right]\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{x},\\mathbf{H}^{\\frac{1}{2}}\\mathbf{w}^{*}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Notice that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{x}\\sim\\mathcal{N}(0,\\mathbf{I}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "by Assumption 1A and that ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\bf S}{\\bf H}^{\\frac{1}{2}}\\big[{\\bf I}-{\\bf H}^{\\frac{1}{2}}{\\bf S}^{\\top}({\\bf S}{\\bf H}{\\bf S}^{\\top})^{-1}{\\bf S}{\\bf H}^{\\frac{1}{2}}\\big]=0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "therefore ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{S}\\mathbf{x}=\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{x}\\mathrm{~is~independent~of~}\\big[\\mathbf{I}-\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\big]\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{x}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking expectation over the second random vector in (8), we find ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[y|\\mathbf{S}\\mathbf{x}]=\\mathbb{E}\\mathbb{E}[y|\\mathbf{x}]=\\langle\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{x},\\mathbf{v}^{*}\\rangle=\\langle\\mathbf{S}\\mathbf{x},\\mathbf{v}^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It remains to show ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}(y-\\langle\\mathbf{S}\\mathbf{x},\\mathbf{v}^{*}\\rangle)^{2}=\\sigma^{2}+\\mathsf{A p p r o x}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This follows from the proof of Lemma A.1. Specifically, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}(y-\\langle\\mathbf{S}\\mathbf{x},\\mathbf{v}^{*}\\rangle)^{2}=\\mathcal{R}(\\mathbf{S}^{\\top}\\mathbf{v}^{*})}\\\\ &{\\phantom{=}=\\mathsf{A p p r o x}+\\mathcal{R}(\\mathbf{w}^{*})}\\\\ &{\\phantom{=}=\\mathsf{A p p r o x}+\\sigma^{2}}\\\\ &{\\phantom{=}\\ge\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second equality is by the definition of Approx and the third equality is by Assumption 1B. We have completed the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.4 Proof of Theorem 6.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We now use the results in [44, 45] for SGD to obtain the following bias-variance decomposition on the excess risk. ", "page_idx": 16}, {"type": "text", "text": "Theorem A.4 (Excess risk bounds). Consider the excess risk in (4) induced by the output of (SGD). Let ", "page_idx": 16}, {"type": "equation", "text": "$$\nN_{\\mathrm{eff}}:=N/\\log(N),\\quad\\mathrm{SNR}:=(\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2}+\\|\\mathbf{v}_{0}\\|_{\\mathbf{SHS}^{\\top}}^{2})/\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then conditioning on the sketch matrix S, for any $\\mathbf{w}^{*}\\in\\mathbb{H}$ ", "page_idx": 16}, {"type": "text", "text": "1. Under Assumptions $5A$ and $5B$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\mathsf{E x c e s s}\\lesssim\\bigg\\|\\prod_{t=1}^{N}\\big(\\mathbf{I}-\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)(\\mathbf{v}_{0}-\\mathbf{v}^{*})\\bigg\\|_{\\mathbf{SHS}^{\\top}}^{2}+\\big(1+\\alpha\\mathbf{S}\\mathtt{N R}\\big)\\sigma^{2}\\cdot\\frac{D_{\\mathrm{eff}}}{N_{\\mathrm{eff}}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "when $\\begin{array}{r}{\\gamma\\lesssim\\frac{1}{c\\alpha\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})}}\\end{array}$ for some constant $c>0$ . ", "page_idx": 16}, {"type": "text", "text": "2. Under Assumptions $1A$ and $I B$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\mathsf{E x c e s s}\\gtrsim\\bigg\\|\\prod_{t=1}^{N}\\big(\\mathbf{I}-\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)\\big(\\mathbf{v}_{0}-\\mathbf{v}^{*}\\big)\\bigg\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}+\\sigma^{2}\\cdot\\frac{D_{\\mathrm{eff}}}{N_{\\mathrm{eff}}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "when $\\begin{array}{r}{\\gamma\\lesssim\\frac{1}{c\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})}}\\end{array}$ for some constant $c>0$ ", "page_idx": 16}, {"type": "text", "text": "In both results, the expectation is over $(\\mathbf{x}_{t},y_{t})_{t=1}^{N}$ , and ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{\\mathsf{e f f}}:=\\#\\{\\tilde{\\lambda}_{j}\\geq1/(N_{\\mathsf{e f f}}\\,\\gamma)\\}+(N_{\\mathsf{e f f}}\\,\\gamma)^{2}\\sum_{\\tilde{\\lambda}_{j}<1/(N_{\\mathsf{e f f}}\\,\\gamma)}\\tilde{\\lambda}_{j}^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(\\tilde{\\lambda}_{j})_{j=1}^{M}$ are eigenvalue of SHS\u22a4. ", "page_idx": 17}, {"type": "text", "text": "Theorem 6.1 follows immediately by Lemma A.1 and by setting $\\mathbf{v}_{0}=0$ and plugging the definition of $\\mathsf{B i a s}(\\mathbf{w}^{*})$ and Var into Theorem A.4. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem A.4. This follows from Corollary 3.4 in [45] for a linear regression problem with population data given by $(\\mathbf{Sx},y)$ . Note that the data covariance becomes $\\bar{\\bf S H S}^{\\top}$ and the optimal model parameter becomes $\\mathbf{v}^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "For the upper bound, Lemma A.2 verifies Assumptions 1A and 2 in [45], with the noise level being ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\sigma}^{2}=2(\\sigma^{2}+\\alpha\\lVert\\mathbf{w}^{*}\\rVert_{\\mathbf{H}}^{2}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then we can apply the upper bound in Corollary 3.4 in [45] (setting their index set $\\mathbb{K}=\\emptyset$ ) to get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\mathsf{E x c e s s}\\lesssim\\left\\|\\prod_{t=1}^{N}\\big(\\mathbf{I}-\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)(\\mathbf{v}_{0}-\\mathbf{v}^{*})\\right\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}+(\\|\\mathbf{v}^{*}-\\mathbf{v}_{0}\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}+\\tilde{\\sigma}^{2})\\frac{D_{\\mathrm{eff}}}{N_{\\mathrm{eff}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We verify that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{v}^{*}-\\mathbf{v}_{0}\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}\\leq2\\|\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}\\mathbf{v}^{*}\\|^{2}+2\\|\\mathbf{v}_{0}\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}}\\\\ &{=2\\|\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}\\|^{2}+2\\|\\mathbf{v}_{0}\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}}\\\\ &{\\leq2\\|\\mathbf{H}^{\\frac{1}{2}}\\mathbf{w}^{*}\\|^{2}+2\\|\\mathbf{v}_{0}\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}}\\\\ &{=2\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2}+2\\|\\mathbf{v}_{0}\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\|\\mathbf{v}^{*}-\\mathbf{v}_{0}\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}+\\tilde{\\sigma}^{2})\\leq2\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2}+2\\|\\mathbf{v}_{0}\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}+2(\\sigma^{2}+\\alpha\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2})}\\\\ {\\lesssim(1+\\alpha\\mathrm{SNR})\\sigma^{2}.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Substituting, we get the upper bound. ", "page_idx": 17}, {"type": "text", "text": "For the lower bound, Lemma A.3 shows $\\mathbf{Sx}$ is Gaussian, therefore it satisfies Assumption 1B in $\\mathrm{Wu}$ et al. [45] with $\\beta=1$ . Besides, Lemma A.3 shows that the linear regression problem is well-specified, with the noise level being ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\sigma}^{2}=\\sigma^{2}+\\mathsf{A p p r o x}\\geq\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Although the lower bound in Corollary 3.4 in Wu et al. [45] is stated for Gaussian additive noise (see their Assumption 2\u2019), it is easy to check that the lower bound holds for any well-specified noise as described by Lemma A.3. Using the lower bound in Corollary 3.4 in $\\mathrm{W}\\mathbf{u}$ et al. [45], we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\mathsf{E x c e s s}\\gtrsim\\bigg\\|\\prod_{t=1}^{N}\\big(\\mathbf{I}-\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)\\big(\\mathbf{v}_{0}-\\mathbf{v}^{*}\\big)\\bigg\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}+\\tilde{\\sigma}^{2}\\frac{D_{\\mathrm{eff}}}{N_{\\mathrm{eff}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Plugging in $\\tilde{\\sigma}^{2}\\geq\\sigma^{2}$ gives the desired lower bound. ", "page_idx": 17}, {"type": "text", "text": "A.5 Proofs of Lemma 6.2, Theorem 6.3 and 6.4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 6.2 is proved in Lemma G.4. Theorem 6.3 follows from Lemma C.1 and D.1. Theorem 6.4 follows from Lemma C.2 and D.2. ", "page_idx": 17}, {"type": "text", "text": "B Proofs in Section 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of part 1. By Assumption 1B and the definition of $\\mathcal{R}(\\cdot)$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal R}(\\mathbf{w})={\\mathbb E}(\\langle\\mathbf{x},\\mathbf{w}\\rangle-y)^{2}={\\mathbb E}(\\langle\\mathbf{x},\\mathbf{w}\\rangle-{\\mathbb E}[y\\mid\\mathbf{x}])^{2}+{\\mathbb E}(y-{\\mathbb E}[y\\mid\\mathbf{x}])^{2}}\\\\ &{\\qquad={\\mathbb E}(\\langle\\mathbf{x},\\mathbf{w}\\rangle-\\langle\\mathbf{x},\\mathbf{w}^{*}\\rangle)^{2}+\\sigma^{2}\\geq\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that the equality holds if and only if $\\mathbf{w}=\\mathbf{w}^{*}$ . Therefore we have $\\operatorname*{min}\\mathcal{R}(\\cdot)=\\mathcal{R}(\\mathbf{w}^{*})=\\sigma^{2}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of part 3. We choose $\\mathsf{B i a s}(\\mathbf{w}^{*})$ , Var as defined in Eq. (5) and (6) and let Bias : $:=$ $\\mathbb{E}_{\\mathbf{w}^{*}}\\operatorname{Bias}(\\mathbf{w}^{*})$ . Part 3 of Theorem 4.1 follows directly from the decomposition of the excess risk in Theorem 6.1 (note that $\\mathbb{E}\\Vert\\mathbf{w}^{*}\\Vert_{\\mathbf{H}}^{2}/\\sigma^{2}\\lesssim1)$ , and the matching bounds in Lemma D.3 and E.1. ", "page_idx": 18}, {"type": "text", "text": "It remains to verify the stepsize assumption required in Lemma D.3. Since we have from Lemma G.4 that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})}=\\frac{1}{\\sum_{i=1}^{M}\\tilde{\\lambda}_{i}}\\geq\\frac{c_{1}}{\\sum_{i=1}^{M}\\lambda_{i}}\\geq\\frac{c_{2}}{\\sum_{i=1}^{M}i^{-a}}\\geq c_{3}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some $a$ -dependent constants $c_{1},c_{2},c_{3}>0$ with probability at least $1-e^{-\\Omega(M)}$ , it follows that for any constant $c>1$ , we can choose $\\gamma\\le c_{0}$ for some $a$ -dependent $c_{\\mathrm{0}}$ such that $\\begin{array}{r}{\\gamma\\leq\\frac{1}{c\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})}}\\end{array}$ Therefore, we have verified the stepsize assumption. ", "page_idx": 18}, {"type": "text", "text": "Finally, the last claim in Theorem 4.1 follows directly from combining the previous three parts and Theorem 6.1, noting $\\sigma^{2}\\lesssim1$ . and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{V a r}\\approx\\frac{\\operatorname*{min}\\{M,\\,(N_{\\mathrm{eff}}\\,\\gamma)^{1/a}\\}}{N_{\\mathrm{eff}}}\\lesssim\\frac{(N_{\\mathrm{eff}}\\,\\gamma)^{1/a}}{N_{\\mathrm{eff}}}\\lesssim(N_{\\mathrm{eff}}\\,\\gamma)^{1/a-1}\\lesssim\\mathsf{B i a s}+\\mathsf{A p p r o x}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "under the stepsize assumption $\\gamma\\lesssim1$ . Here the hidden constants may depend on $a$ . ", "page_idx": 18}, {"type": "text", "text": "B.2 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Similar to the proof of Theorem 4.1, we have $\\operatorname*{min}\\mathcal{R}(\\cdot)=\\sigma^{2}$ under Assumption 1B. Moreover, by Lemma C.5, D.4 and E.1, we have with probability at least $1-e^{-\\Omega(M)}$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\simeq M^{1-b},}\\\\ &{\\qquad\\quad\\mathsf{B i a s}\\lesssim\\operatorname*{max}\\big\\{M^{1-b},\\ (N_{\\mathsf{e f f}}\\gamma)^{(1-b)/a}\\big\\},}\\\\ &{\\qquad\\quad\\mathsf{B i a s}\\gtrsim(N_{\\mathsf{e f f}}\\gamma)^{(1-b)/a}\\mathrm{~when~}(N_{\\mathsf{e f f}}\\gamma)^{1/a}\\leq M/3,}\\\\ &{\\qquad\\quad\\mathsf{V a r}\\approx\\operatorname*{min}\\big\\{M,\\ (N_{\\mathsf{e f f}}\\gamma)^{1/a}\\big\\}/N_{\\mathsf{e f f}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "when the stepsize $\\gamma\\leq c$ for some $a$ -dependent constant $c>0$ . Here the hidden constants in the bounds may depend only on $(a,b)$ . Combining the bounds on Approx, Bias, Var and noting ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{V a r}\\approx\\frac{\\operatorname*{min}\\{M,\\ (N_{\\mathrm{eff}}\\,\\gamma)^{1/a}\\}}{N_{\\mathrm{eff}}}\\lesssim\\frac{(N_{\\mathrm{eff}}\\,\\gamma)^{1/a}}{N_{\\mathrm{eff}}}\\lesssim(N_{\\mathrm{eff}}\\,\\gamma)^{(1-b)/a}\\lesssim\\mathsf{B i a s}+\\mathsf{A p p r o x}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "yields Theorem 4.2. Here in the second inequality, we use the assumption $b\\leq a$ . ", "page_idx": 18}, {"type": "text", "text": "B.3 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Similar to the proof of Theorem 4.1, we have mi $\\mathrm{}_{1}\\mathscr{R}(\\cdot)=\\sigma^{2}$ under Assumption 1B. Notice that we have $\\gamma\\lesssim1$ implies $\\gamma\\lesssim1/(\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}))$ with probability at least $1-e^{-\\Omega(M)}$ by Lemma G.6. It follows from Lemma C.6, D.5 and E.2 that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cdot\\mathsf{A p p r o x}\\approx\\log^{1-a}M,}\\\\ &{\\quad\\mathsf{B i a s}\\lesssim\\operatorname*{max}\\big\\{\\log^{1-a}M,\\,\\log^{1-a}(N_{\\mathrm{eff}}\\,\\gamma)\\big\\},}\\\\ &{\\quad\\mathsf{B i a s}\\gtrsim\\log^{1-a}(N_{\\mathrm{eff}}\\,\\gamma)\\,\\,\\mathrm{when}\\,\\,(N_{\\mathrm{eff}}\\,\\gamma)^{1/a}\\leq M^{c}\\,\\,\\mathrm{for~some~small~constant}\\,c>0,}\\\\ &{\\quad\\mathsf{V a r}\\sim\\frac{\\operatorname*{min}\\{M,\\,(N_{\\mathrm{eff}}\\,\\gamma)/\\log^{a}(N_{\\mathrm{eff}}\\,\\gamma)\\}}{N_{\\mathrm{eff}}}\\lesssim\\frac{(N_{\\mathrm{eff}}\\,\\gamma)/\\log^{a}(N_{\\mathrm{eff}}\\,\\gamma)}{N_{\\mathrm{eff}}\\,\\gamma}=\\log^{-a}(N_{\\mathrm{eff}}\\,\\gamma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ when the stepsize $\\gamma\\leq c$ for some $a$ -dependent constant $c>0$ . Since Var $\\lesssim$ Bias and $\\log^{1-a}(N_{\\mathrm{eff}}\\,\\gamma)\\lesssim\\log^{1-a}M$ when $(N_{\\mathbf{eff}}\\gamma)^{1/a}\\gtrsim M^{c}$ , putting the bounds together gives Theorem 4.3. ", "page_idx": 18}, {"type": "text", "text": "C Approximation error ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we derive upper and lower bounds for the approximation error in (4) (and 7). We will also show that the upper and lower bounds match up to constant factors in several examples. ", "page_idx": 18}, {"type": "text", "text": "C.1 An upper bound ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma C.1 (An upper bound on the approximation error). Given any $k\\leq d$ such that $r(\\mathbf{H})\\geq k{+}M$ , the approximation error in (4) (and 7) satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{A p p r o x}\\lesssim\\lVert\\mathbf{w}_{k:\\infty}^{*}\\rVert_{\\mathbf{H}_{k:\\infty}}^{2}+\\langle[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1},\\mathbf{w}_{0:k}^{*}\\mathbf{w}_{0:k}^{*}\\mathbf{\\Delta}^{\\top}\\rangle\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "almost surely, where $\\mathbf{A}_{k}\\ :=\\ \\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}$ . If in addition $k\\ \\leq\\ M/2$ , then with probability 1 \u2212e\u2212\u2126(M) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{A p p r o x}\\lesssim\\|\\mathbf{w}_{k:\\infty}^{*}\\|_{\\mathbf{H}_{k:\\infty}}^{2}+\\Big(\\frac{\\sum_{i>k}\\lambda_{i}}{M}+\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\Big)\\|\\mathbf{w}_{0:k}^{*}\\|^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(\\lambda_{i})_{i=1}^{p}$ are eigenvalues of $\\mathbf{H}$ in non-increasing order. ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma C.1. Write the singular value decomposition $\\textbf{H}=\\textbf{U}\\mathbf{A}\\mathbf{U}^{\\top}$ , where $\\Lambda\\ :=$ $\\mathrm{diag}\\{\\lambda_{1},\\lambda_{2},...\\}$ with $\\lambda_{1}\\,\\geq\\,\\lambda_{2}\\,\\geq\\,.\\,.\\,\\geq\\,0$ and $\\mathbf{U}\\mathbf{U}^{\\top}=\\mathbf{I}.$ . Define $\\tilde{\\mathbf{S}}\\,:=\\,\\mathbf{S}\\mathbf{U},\\tilde{\\mathbf{w}}^{*}\\,:=\\,\\mathbf{U}^{\\top}\\mathbf{w}^{*}$ . Then by Lemma A.1 the approximation error $\\mathsf{A p p r o x}=\\mathsf{A p p r o x}(\\mathbf{S},\\mathbf{H},\\mathbf{w}^{*})$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{A p p r o x}(\\mathbf{S},\\mathbf{H},\\mathbf{w}^{*})=\\left\\|\\left(\\mathbf{I}-\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}\\big(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)^{-1}\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\right)\\mathbf{H}^{\\frac{1}{2}}\\mathbf{w}^{*}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\left\\|\\left(\\mathbf{I}-\\mathbf{U}\\mathbf{A}^{\\frac{1}{2}}\\tilde{\\mathbf{S}}^{\\top}\\big(\\tilde{\\mathbf{S}}\\mathbf{A}\\tilde{\\mathbf{S}}^{\\top}\\big)^{-1}\\mathbf{S}\\mathbf{A}^{\\frac{1}{2}}\\mathbf{U}^{\\top}\\right)\\mathbf{U}\\mathbf{A}^{\\frac{1}{2}}\\mathbf{U}^{\\top}\\mathbf{w}^{*}\\right\\|^{2}}\\\\ &{\\qquad\\qquad=\\left\\|\\mathbf{U}\\Big(\\mathbf{I}-\\mathbf{A}^{\\frac{1}{2}}\\tilde{\\mathbf{S}}^{\\top}\\big(\\tilde{\\mathbf{S}}\\mathbf{A}\\tilde{\\mathbf{S}}^{\\top}\\big)^{-1}\\mathbf{S}\\mathbf{A}^{\\frac{1}{2}}\\Big)\\mathbf{A}^{\\frac{1}{2}}\\mathbf{U}^{\\top}\\mathbf{w}^{*}\\right\\|^{2}}\\\\ &{\\qquad\\qquad=\\left\\|\\left(\\mathbf{I}-\\mathbf{A}^{\\frac{1}{2}}\\tilde{\\mathbf{S}}^{\\top}\\big(\\tilde{\\mathbf{S}}\\mathbf{A}\\tilde{\\mathbf{S}}^{\\top}\\big)^{-1}\\mathbf{S}\\mathbf{A}^{\\frac{1}{2}}\\right)\\mathbf{A}^{\\frac{1}{2}}\\tilde{\\mathbf{w}}^{*}\\right\\|^{2}=\\mathsf{A p p r o x}(\\tilde{\\mathbf{S}},\\mathbf{A},\\tilde{\\mathbf{w}}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\tilde{\\mathbf{S}}\\overset{d}{=}\\mathbf{S}$ by rotational invariance of standard gaussian variables, it suffices to analyze the case where $\\mathbf{H}=\\mathbf{A}$ is a diagonal matrix, as the results may transfer to general $\\mathbf{H}$ by replacing $\\tilde{\\mathbf{w}}^{*}$ with $\\mathbf{w}^{*}$ . ", "page_idx": 19}, {"type": "text", "text": "Therefore, from now on we assume w.l.o.g. that $\\mathbf{H}$ is a diagonal matrix with non-increasing diagonal entries. Define $\\mathbf{A}:=\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}$ . ", "page_idx": 19}, {"type": "text", "text": "By definition of Approx, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{A p p r o x}=\\left\\|\\left(\\mathbf{I}-\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}\\big(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)^{-1}\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\right)\\mathbf{H}^{\\frac{1}{2}}\\mathbf{w}^{*}\\right\\|^{2}}\\\\ &{\\qquad\\qquad=\\Big\\langle[\\mathbf{H}^{1/2}\\mathbf{S}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}\\mathbf{H}^{1/2}-\\mathbf{I}_{p}]^{\\otimes2},\\mathbf{H}^{1/2}\\mathbf{w}^{*}\\mathbf{w}^{*}^{\\top}\\mathbf{H}^{1/2}\\Big\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, for any $k\\in[p]$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\mathbb{I}}^{1/2}\\mathbf{S}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}\\mathbf{H}^{1/2}-\\mathbf{I}_{p}=\\left(\\mathbf{H}_{b:\\infty}^{1/2}\\mathbf{S}_{0:k}^{\\top}\\right)\\mathbf{A}^{-1}\\left(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}\\right.\\}_{k:\\infty}\\left.\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}\\right)-\\mathbf{I}_{p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.=\\left(\\mathbf{H}_{0:k}^{1/2}\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}^{1/2}-\\mathbf{I}_{k}\\mathbf{\\Lambda}\\mathbf{H}_{0:k}^{1/2}\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\left.\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}^{1/2}\\right.\\right.\\ \\ \\ \\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}-\\mathbf{I}_{d-k}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.\\left.=:\\left(\\mathbf{\\mathbb{V}}^{\\top}\\mathbf{\\Lambda}\\mathbf{\\Psi}\\right)\\mathbf{\\Lambda}\\mathbf{W}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{H}^{1/2}\\mathbf{S}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}\\mathbf{H}^{1/2}-\\mathbf{I}_{p}|^{\\otimes2}=\\left(\\mathbf{V}^{7}\\mathbf{U}+\\mathbf{V}\\mathbf{V}^{\\top}\\mathbf{\\Lambda}\\ \\ \\mathbf{U}\\mathbf{V}+\\mathbf{V}\\mathbf{W}\\right)\\preceq2\\left(\\mathbf{U}^{2}+\\mathbf{V}\\mathbf{V}^{\\top}\\mathbf{\\Lambda}\\ \\ \\mathbf{U}\\mathbf{\\Lambda}^{0}\\right)\\mathbf{W}^{2}+\\mathbf{V}^{\\top}\\mathbf{V}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and hence ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{A p p r o x}\\leq2\\Bigg\\langle\\left(\\!\\!\\begin{array}{c c}{\\mathsf{U}^{2}+\\mathsf{V V}^{\\top}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathsf{W}^{2}+\\mathsf{V}^{\\top}\\mathsf{V}}\\end{array}\\!\\!\\right),\\mathbf{H}^{1/2}\\mathbf{w}^{*}\\mathbf{w}^{*}^{\\top}\\mathbf{H}^{1/2}\\Bigg\\rangle\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\mathrm{~\\boldmath~\\Gamma~}=2\\big\\langle{{\\bf{U}}^{2}+\\nabla{\\bf{V}}^{\\top},{\\bf{H}}_{0:k}^{1/2}{\\bf{w}}_{*,0:k}{\\bf{w}}_{*,0:k}^{\\top}{\\bf{H}}_{0:k}^{1/2}}\\big\\rangle+2\\big\\langle{{\\bf{W}}^{2}+\\nabla^{\\top}\\nabla,{\\bf{H}}_{k:\\infty}^{1/2}{\\bf{w}}_{*,k:\\infty}{\\bf{w}}_{*,k:\\infty}^{\\top}{\\bf{H}}_{k:\\infty}^{1/2}}\\big\\rangle.$ We claim the following results which we will prove at the end of the proof. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big\\langle\\mathsf{W}^{2}+\\mathsf{V}^{\\top}\\mathsf{V},\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{w}_{*,k:\\infty}\\mathbf{w}_{*,k:\\infty}^{\\top}\\mathbf{H}_{k:\\infty}^{1/2}\\big\\rangle\\leq\\|\\mathbf{w}_{k:\\infty}^{*}\\|_{\\mathbf{H}_{k:\\infty}}^{2},}\\\\ &{\\qquad\\big\\langle\\mathsf{U}^{2}+\\mathsf{V}\\mathsf{V}^{\\top},\\mathbf{H}_{0:k}^{1/2}\\mathbf{w}_{*,0:k}\\mathbf{w}_{*,0:k}^{\\top}\\mathbf{H}_{0:k}^{1/2}\\big\\rangle=\\big\\langle\\big[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}\\big]^{-1},\\mathbf{w}_{0:k}^{*}\\mathbf{w}_{0:k}^{*}\\big\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that in claim (11) the inverse ${\\bf A}_{k}^{-1}$ exists almost surely since $r(\\mathbf{H}_{k:\\infty})\\,\\geq\\,r(\\mathbf{H})\\,-\\,k\\,\\geq\\,M$ by our assumption and $\\mathbf{S}_{k:\\infty}\\in\\mathbb{R}^{M\\times(d-k)}$ is a random gaussian projection onto $\\mathbb{R}^{M}$ . First part of Lemma C.1 follows immediately from combining claim (10) and (11). ", "page_idx": 20}, {"type": "text", "text": "To prove the second part of Lemma C.1, first note that with probability $1-e^{-\\Omega(M)}$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mu_{\\mathrm{min}}(\\mathbf{A}_{k}^{-1})=\\|\\mathbf{A}_{k}\\|^{-1}\\geq c/\\Big(\\frac{\\sum_{i>k}\\lambda_{i}}{M}+\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\Big)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "forc some constant $c>0$ by Lemma G.2. Moreover, by the concentration of the Gaussian variance matrix (see e.g., Theorem 6.1 in [41]), we have $\\mathbf{S}_{0:k}^{\\top}\\mathbf{S}_{0:k}\\succeq\\mathbf{I}_{k}/5$ with probability $1-e^{-\\Omega(M)}$ when $M/k\\ge2$ . Combining the last two arguments, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}\\succeq c\\mathbf{S}_{0:k}^{\\top}\\mathbf{S}_{0:k}/\\bigg(\\frac{\\sum_{i>k}\\lambda_{i}}{M}+\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\gtrsim\\mathbf{I}_{k}/\\bigg(\\frac{\\sum_{i>k}\\lambda_{i}}{M}+\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and therefore ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1},\\mathbf{w}_{0:k}^{*}{\\mathbf{w}_{0:k}^{*}}^{\\top}\\rangle\\le\\langle[\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1},\\mathbf{w}_{0:k}^{*}{\\mathbf{w}_{0:k}^{*}}^{\\top}\\rangle}&{}\\\\ &{\\phantom{\\le}\\|[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\|\\|\\mathbf{w}_{0:k}^{*}\\|^{2}}\\\\ &{\\phantom{\\le}\\lesssim\\Big(\\frac{\\sum_{i>k}\\lambda_{i}}{M}+\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\Big)\\|\\mathbf{w}_{0:k}^{*}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with probability $1-e^{-\\Omega(M)}$ . Combining Eq. (12) with the first part of Lemma C.1 completes the proof. ", "page_idx": 20}, {"type": "text", "text": "Proof of claim (10) Note that ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\mathbf{I}_{d-k}\\preceq\\mathsf{W}=\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}-\\mathbf{I}_{d-k}}\\\\ &{\\phantom{=}\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}+\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}-\\mathbf{I}_{d-k}}\\\\ &{\\phantom{=}\\preceq\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}-\\mathbf{I}_{d-k}\\preceq\\mathsf{0}_{d-k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality uses the fact that the norm of projection matrices is no greater than one. Therefore, we have $\\|\\mathsf{W}\\|_{2}\\leq1$ . Now, it remains to show ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathsf{W}^{2}+\\mathsf{V}^{\\top}\\mathsf{V}=-\\mathsf{W},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as claim (10) is a direct consequence of Eq. (13) and the fact that $\\|\\mathsf{W}\\|\\leq1$ . ", "page_idx": 20}, {"type": "text", "text": "By definition of $\\mathsf W$ in Eq. (9), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{W}^{2}=(\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}-\\mathbf{I}_{d-k})^{2}}\\\\ &{\\phantom{=}\\mathbf{I}_{d-k}-2\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}+\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}}\\\\ &{\\phantom{=}=\\mathbf{I}_{d-k}-2\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}+\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{A}_{k}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By definition of $\\vee$ in Eq. (9), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathsf{V}^{\\top}\\mathsf{V}=\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top})\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}+\\mathbf{A}_{k}=\\mathbf{A}$ , it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{W}^{2}+\\mathsf{V}^{\\top}\\mathsf{V}=\\mathbf{I}_{d-k}-2\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}+\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}}\\\\ &{\\qquad\\qquad=\\mathbf{I}_{d-k}-\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}=-\\mathsf{W}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of claim (11) It suffices to show $\\mathsf{U}^{2}\\!+\\!\\mathsf{V}^{\\top}\\mathsf{V}=[\\mathbf{H}_{0:k}^{-1}\\!+\\!\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}$ . Using the definition of U in Eq. (9), we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{J}=\\mathbf{H}_{0;k}^{1/2}\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{0;k}\\mathbf{H}_{0;k}^{1/2}-\\mathbf{I}_{k}}\\\\ &{\\quad=\\mathbf{H}_{0;k}^{1/2}\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0;k}\\mathbf{H}_{0;k}^{1/2}-\\mathbf{H}_{0;k}^{1/2}\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0;k}\\big[\\mathbf{H}_{0;k}^{-1}+\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0;k}\\big]^{-1}\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0;k}\\mathbf{H}_{0;k}^{1/2}-\\mathbf{I}_{k}}\\\\ &{\\quad=\\mathbf{H}_{0;k}^{1/2}\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0;k}\\big[\\mathbf{H}_{0;k}^{-1}+\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0;k}\\big]^{-1}\\mathbf{H}_{0;k}^{-1}\\mathbf{H}_{0;k}^{1/2}-\\mathbf{I}_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second line uses Woodbury\u2019s matrix identity, namely ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{A}^{-1}=[\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}+\\mathbf{A}_{k}]^{-1}=\\mathbf{A}_{k}^{-1}-\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Continuing the calculation of U, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{U}=\\mathbf{H}_{0:k}^{1/2}\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\mathbf{H}_{0:k}^{-1/2}-\\mathbf{I}_{k}}\\\\ &{\\quad=\\mathbf{H}_{0:k}^{1/2}(\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}-\\mathbf{I}_{k})\\mathbf{H}_{0:k}^{-1/2}}\\\\ &{\\quad=-\\mathbf{H}_{0:k}^{-1/2}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\mathbf{H}_{0:k}^{-1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{U}^{2}=\\mathbf{H}_{0:k}^{-1/2}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\mathbf{H}_{0:k}^{-1}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\mathbf{H}_{0:k}^{-1/2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}_{0:k}^{1/2}\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}^{-1}=\\mathbf{H}_{0:k}^{1/2}\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}-\\mathbf{H}_{0:k}^{1/2}\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{H}_{0:k}^{-1/2}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "by Woodbury\u2019s matrix indentity, it follows from the definition of $\\vee$ in Eq. (9) that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle N^{\\top}=\\mathbf{H}_{0;k}^{1/2}\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k;\\infty}\\mathbf{H}_{k;\\infty}\\mathbf{S}_{k;\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{0;k}\\mathbf{H}_{0;k}^{1/2}}\\\\ &{\\qquad=\\mathbf{H}_{0;k}^{-1/2}\\big[\\mathbf{H}_{0;k}^{-1}+\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0;k}\\big]^{-1}\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}_{k}^{-1}\\big(\\mathbf{S}_{k;\\infty}\\mathbf{H}_{k;\\infty}\\mathbf{S}_{k;\\infty}^{\\top}\\big)\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0;k}\\big[\\mathbf{H}_{0;k}^{-1}+\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0;k}\\big]}\\\\ &{\\qquad=\\mathbf{H}_{0;k}^{-1/2}\\big[\\mathbf{H}_{0;k}^{-1}+\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0;k}\\big]^{-1}\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0;k}\\big[\\mathbf{H}_{0;k}^{-1}+\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0;k}\\big]^{-1}\\mathbf{H}_{0;k}^{-1/2}.\\qquad\\qquad(15)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining Eq. (14) and (15) yields ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{U}^{2}+\\mathsf{W}\\mathsf{V}^{\\top}=\\mathbf{H}_{0:k}^{-1/2}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\mathbf{H}_{0:k}^{-1/2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\big\\langle\\mathsf{U}^{2}+\\mathsf{V}\\mathsf{V}^{\\top},\\mathbf{H}_{0:k}^{1/2}\\mathbf{w}_{*,0:k}\\mathbf{w}_{*,0:k}^{\\top}\\mathbf{H}_{0:k}^{1/2}\\big\\rangle=\\big\\langle\\big[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}\\big]^{-1},\\mathbf{w}_{0:k}^{*}\\mathbf{w}_{0:k}^{*}\\,\\big^{\\top}\\big\\rangle.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.2 A lower bound ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For the approximation error Approx, we have the following result. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.2 (Lower bound on the approximation error). When $r(\\mathbf{H})\\geq M$ , under Assumption $I C$ , the approximation error in (4) (and 7) satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\gtrsim\\sum_{i=M}^{d}\\lambda_{i},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(\\lambda_{i})_{i=1}^{d}$ are eigenvalues of $\\mathbf{H}$ in non-increasing order. ", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma C.2. For any $k\\leq d$ , following the proof of Lemma C.1, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{A p p r o x}=\\left\\langle[\\mathbf{H}^{1/2}\\mathbf{S}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}\\mathbf{H}^{1/2}-\\mathbf{I}_{d}]^{\\otimes2},\\mathbf{H}^{1/2}\\mathbf{w}^{*}(\\mathbf{w}^{*})^{\\top}\\mathbf{H}^{1/2}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{I}^{1/2}\\mathbf{S}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}\\mathbf{H}^{1/2}-\\mathbf{I}_{d}=\\left(\\mathbf{H}_{0;k}^{1/2}\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{0;k}\\mathbf{H}_{0;k}^{1/2}-\\mathbf{I}_{k}\\qquad\\mathbf{H}_{0;k}^{1/2}\\mathbf{S}_{0;k}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k;\\infty}\\mathbf{H}_{k;\\infty}^{1/2}\\right)}\\\\ {\\mathbf{H}_{k;\\infty}^{1/2}\\mathbf{S}_{k;\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{0;k}\\mathbf{H}_{0;k}^{1/2}\\;\\;\\;\\;\\;\\mathbf{H}_{k;\\infty}^{1/2}\\mathbf{S}_{k;\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k;\\infty}\\mathbf{H}_{k;\\infty}^{1/2}-\\mathbf{I}_{d-k}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{\\Sigma}=:\\left(\\overset{\\cup}{\\lor}\\begin{array}{l l}{\\lor}&{\\lor}\\\\ {\\lor}&{\\mathsf{W}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\n[\\mathbf{H}^{1/2}\\mathbf{S}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}\\mathbf{H}^{1/2}-\\mathbf{I}_{d}]^{\\otimes2}=\\left(\\mathsf{U}^{2}+\\mathsf{V}\\mathsf{V}^{\\top}\\right.\\quad\\mathsf{U}\\mathsf{V}+\\mathsf{V}\\mathsf{W}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{z}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}=\\mathbb{E}_{\\mathbf{w}^{*}}\\big\\langle\\mathsf{U}^{2}+\\mathsf{V V}^{\\top},\\mathsf{H}_{0;k}^{1/2}\\mathbf{w}_{0;k}^{*}\\mathbf{w}_{0;k}^{*}\\mathbf{H}_{0;k}^{1/2}\\big\\rangle+\\mathbb{E}_{\\mathbf{w}^{*}}\\big\\langle\\mathsf{W}^{2}+\\mathsf{V}^{\\top}\\mathsf{V},\\mathbf{H}_{0;k}^{1/2}\\mathbf{w}_{k;\\infty}^{*}\\mathbf{w}_{k;\\infty}^{*}\\mathbf{H}_{k;\\infty}^{1/2}\\big\\rangle}\\\\ &{\\phantom{=}+2\\mathbb{E}_{\\mathbf{w}^{*}}\\big\\langle\\mathsf{U}\\mathsf{V}+\\mathsf{V W},\\mathbf{H}_{0;k}^{1/2}\\mathbf{w}_{0;k}^{*}\\mathbf{w}_{k;\\infty}^{*}\\mathbf{H}_{k;\\infty}^{1/2}\\big\\rangle}\\\\ &{\\phantom{=}=\\mathrm{tr}\\big((\\mathsf{U}^{2}+\\mathsf{V}^{\\top})\\mathbf{H}_{0;k}\\big)+\\mathrm{tr}\\big((\\mathsf{W}^{2}+\\mathsf{V}^{\\top}\\mathsf{V})\\mathbf{H}_{k;\\infty}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last line uses the fact that $\\mathbb{E}_{\\mathbf{w}^{*}}(\\mathbf{w}^{*})^{\\otimes2}\\,=\\,\\mathbf{I}_{d}$ . Using Eq. (13) and (16) in the proof of Lemma C.1, we further obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}=\\mathrm{tr}(\\mathbf{H}_{0:k}^{-1/2}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\mathbf{H}_{0:k}^{-1/2}\\mathbf{H}_{0:k})-\\mathrm{tr}(\\mathsf{W}\\mathbf{H}_{k:\\infty})}\\\\ &{\\phantom{=}=\\mathrm{tr}([\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1})-\\mathrm{tr}(\\mathsf{W}\\mathbf{H}_{k:\\infty})}\\\\ &{\\phantom{=}\\ge-\\mathrm{tr}(\\mathsf{W}\\mathbf{H}_{k:\\infty})=:T_{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathbf{A}_{k}:=\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}$ . For $T_{3}$ , we further have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{3}=\\mathrm{tr}(\\mathbf{H}_{k:\\infty}^{1/2}[\\mathbf{I}_{d-k}-\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}]\\mathbf{H}_{k:\\infty}^{1/2})}\\\\ &{\\quad\\geq\\mathrm{tr}(\\mathbf{H}_{k:\\infty}^{1/2}[\\mathbf{I}_{d-k}-\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}]\\mathbf{H}_{k:\\infty}^{1/2})}\\\\ &{\\quad\\geq\\displaystyle\\sum_{i=1}^{d-k}\\mu_{i}(\\mathbf{I}_{d-k}-\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2})\\cdot\\mu_{d+1-k-i}(\\mathbf{H}_{k:\\infty}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second line is due to ${\\bf A}\\succeq{\\bf A}_{k}$ (and hence $-\\mathbf{A}^{-1}\\succeq-\\mathbf{A}_{k}^{-1}$ ), the third line follows from Von-Neuman \u2019s inequality. Since $\\mathbf{M}:=\\mathbf{I}_{d-k}-\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}$ is a projection matrix such that and $\\operatorname{tr}(\\mathbf{I}_{d-k}-\\mathbf{M})=M$ , it follows that M has $M$ eigenvalues 0 and eigenvalues 1. Therefore, we further have ", "page_idx": 22}, {"type": "equation", "text": "$$\nT_{3}\\geq\\sum_{i=1}^{d-k}\\mu_{i}(\\mathbf{M})\\cdot\\mu_{d+1-k-i}(\\mathbf{H}_{k:\\infty})\\geq\\sum_{i=k+M}^{d}\\lambda_{i}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any $k\\leq d$ . Letting $k=0$ maximizes the lower bound and concludes the proof. ", "page_idx": 22}, {"type": "text", "text": "C.3 A lower bound under Assumption 3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma C.3 (Lower bound on the approximation error under Assumption 3). Under Assumption 3, the approximation error in (4) (and 7) satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\gtrsim\\sum_{i=M}^{d}\\lambda_{i}i^{a-b},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $(\\lambda_{i})_{i=1}^{d}$ are eigenvalues of $\\mathbf{H}$ in non-increasing order and the inequality hides some $(a,b)$ - dependent constant. ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma C.3. The proof is essentially the same as the proof of Lemma C.2 but we include it here for completeness. Let $\\mathbf{H}^{\\mathbf{w}}:=\\mathbb{E}[\\mathbf{w}^{*}\\mathbf{w}^{*\\top}]$ be the covariance of the prior on $\\mathbf{w}^{*}$ . Following the proof of Lemma C.2, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{z}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}=\\mathbb{E}_{\\mathbf{w}^{*}}\\big\\langle\\mathsf{U}^{2}+\\mathsf{V V}^{\\top},\\mathsf{H}_{0;k}^{1/2}\\mathbf{w}_{0;k}^{*}\\mathbf{w}_{0;k}^{*}\\mathbf{H}_{0;k}^{1/2}\\big\\rangle+\\mathbb{E}_{\\mathbf{w}^{*}}\\big\\langle\\mathsf{W}^{2}+\\mathsf{V}^{\\top}\\mathsf{V},\\mathbf{H}_{0;k}^{1/2}\\mathbf{w}_{k;\\infty}^{*}\\mathbf{w}_{k;\\infty}^{*}\\mathbf{H}_{k;\\infty}^{1/2}\\big\\rangle}\\\\ &{\\phantom{=}+2\\mathbb{E}_{\\mathbf{w}^{*}}\\big\\langle\\mathsf{U}\\mathsf{V}+\\mathsf{V W},\\mathsf{H}_{0;k}^{1/2}\\mathbf{w}_{0;k}^{*}\\mathbf{w}_{k;\\infty}^{*}\\mathbf{H}_{k;\\infty}^{1/2}\\big\\rangle}\\\\ &{\\phantom{=}=\\mathrm{tr}\\big((\\mathsf{U}^{2}+\\mathsf{V}^{\\top})\\mathbf{H}_{0;k}\\mathbf{H}_{0;k}^{\\mathrm{w}}\\big)+\\mathrm{tr}\\big((\\mathsf{W}^{2}+\\mathsf{V}^{\\top}\\mathsf{V})\\mathbf{H}_{k;\\infty}\\mathbf{H}_{k;\\infty}^{\\mathrm{w}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last line uses Assumption 3 and notice that $\\mathbf{H},\\mathbf{H}^{\\mathbf{w}}$ are both diagonal. Next, similar to the proof of Lemma C.2, using Eq. (13) and (16), we derive ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}=\\mathrm{tr}(\\mathbf{H}_{0:k}^{-1/2}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\mathbf{H}_{0:k}^{-1/2}\\mathbf{H}_{0:k}\\mathbf{H}_{0:k}^{\\mathbf{w}})-\\mathrm{tr}(\\mathsf{W}\\mathbf{H}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{\\mathbf{w}})}\\\\ &{\\phantom{=}\\mathrm{tr}([\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\mathbf{H}_{0:k}^{\\mathbf{w}})-\\mathrm{tr}(\\mathsf{W}\\mathbf{H}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{\\mathbf{w}})}\\\\ &{\\phantom{=}\\ge-\\mathrm{tr}(\\mathsf{W}\\mathbf{H}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{\\mathbf{w}})=:\\overleftarrow{T}_{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathbf{A}_{k}\\ :=\\ \\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}$ . For $\\tilde{T}_{3}$ , following the same argument for $T_{3}$ in the proof of Lemma C.2, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{T}_{3}\\geq\\displaystyle\\sum_{i=1}^{d-k}\\mu_{i}(\\mathbf{I}_{d-k}-\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2})\\cdot\\mu_{d+1-k-i}(\\mathbf{H}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{\\mathbf{w}})}\\\\ &{\\quad\\geq\\displaystyle\\sum_{i=k+M}^{d}\\mu_{i}(\\mathbf{H}\\mathbf{H}^{\\mathbf{w}})\\gtrsim\\displaystyle\\sum_{i=k+M}^{d}i^{a-b}\\lambda_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for any $k\\leq d$ where the last inequality uses Assumption 3. Setting $k=0$ maximizes the lower bound and concludes the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C.4 Examples on matching bounds for Approx ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we derive matching upper and lower bounds for Approx (defined in Eq. 4 and 7) in three concrete examples: power-law spectrum (Lemma C.4), power-law spectrum with source condition (Lemma C.5) and logarithmic power-law spectrum (Lemma D.5). ", "page_idx": 23}, {"type": "text", "text": "Lemma C.4 (Bounds on Approx under the power-law spectrum). Suppose Assumption $I C$ and 2 hold. Then with probability at least $1-e^{-\\Omega(M)}$ over the randomness of S ", "page_idx": 23}, {"type": "equation", "text": "$$\nM^{1-a}\\lesssim\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\lesssim M^{1-a}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, the hidden constants only depend on the power-law degree $a$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma C.4. For the upper bound, by Lemma C.1 and noting $\\mathbb{E}\\mathbf{w}_{i}^{*2}=1$ for all $i$ , we have with probability at least $1-e^{-\\Omega(M)}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\lesssim\\displaystyle\\sum_{k>k_{1}}\\lambda_{i}+\\bigg(\\frac{\\sum_{i>k_{1}}\\lambda_{i}}{M}+\\lambda_{k_{1}+1}+\\sqrt{\\frac{\\sum_{i>k_{1}}\\lambda_{i}^{2}}{M}}\\bigg)\\cdot k_{1}}&{}\\\\ {\\lesssim k_{1}^{1-a}+\\bigg(\\frac{k_{1}^{1-a}}{M}+k_{1}^{-a}+\\sqrt{\\frac{k_{1}^{1-2a}}{M}}\\bigg)k_{1}}&{}\\\\ {\\lesssim\\bigg(\\frac{k_{1}}{M}+1\\bigg)k_{1}^{1-a}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for any given $k_{1}\\leq M/2$ . Here the hidden constants depend on $a$ . Therefore, letting $k_{1}=M/2$ in the upper bound yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\lesssim M^{1-a}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . ", "page_idx": 23}, {"type": "text", "text": "For the lower bound, we have from Lemma C.2 that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{\\ast}}\\mathsf{A p p r o x}\\gtrsim\\sum_{i=M}^{\\infty}i^{-a}\\gtrsim M^{1-a}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This completes the proof. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.5 (Bounds on Approx under the source condition). Suppose Assumption 3 hold. Then with probability at least $1-e^{-\\dot{\\Omega}(M)}$ over the randomness of S ", "page_idx": 23}, {"type": "equation", "text": "$$\nM^{1-b}\\lesssim\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\lesssim M^{1-b}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, the hidden constants only depend on the power-law degrees $a,b$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma C.5. For the upper bound, by Lemma C.1 and noting $\\mathbb{E}\\mathbf{w}_{i}^{*2}\\approx i^{a-b}$ for all $i$ , we have with probability at least $1-e^{-\\Omega(M)}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{A p p r o x}\\lesssim\\displaystyle\\sum_{k>k_{1}}\\lambda_{i}i^{a-b}+\\bigg(\\frac{\\sum_{i>k_{1}}\\lambda_{i}}{M}+\\lambda_{k_{1}+1}+\\sqrt{\\frac{\\sum_{i>k_{1}}\\lambda_{i}^{2}}{M}}\\bigg)\\cdot k_{1}^{1+a-b}}\\\\ &{\\qquad\\lesssim k_{1}^{1-b}+\\bigg(\\frac{k_{1}^{1-a}}{M}+k_{1}^{-a}+\\sqrt{\\frac{k_{1}^{1-2a}}{M}}\\bigg)k_{1}^{1+a-b}}\\\\ &{\\qquad\\lesssim\\bigg(\\frac{k_{1}}{M}+1\\bigg)k_{1}^{1-b}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for any given $k_{1}\\leq M/2$ . Here the hidden constants depend on $a,b$ . Moreover, choosing $k_{1}=M/2$ in the upper bound gives ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\lesssim M^{1-b}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . ", "page_idx": 24}, {"type": "text", "text": "For the lower bound, we have from Lemma C.3 that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\gtrsim\\sum_{i=M}^{\\infty}i^{-a}\\cdot i^{a-b}\\gtrsim M^{1-b}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This completes the proof. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.6 (Bounds on Approx under the logarithmic power-law spectrum). Suppose Assumption $^{4}$ hold. Then with probability at least $1-e^{-\\Omega\\bar{(M)}}$ over the randomness of S ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\log^{1-a}M\\lesssim\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\lesssim\\log^{1-a}M.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, the hidden constants only depend on the power-law degree $a$ . ", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma C.6. For the upper bound, by Lemma C.1 and noting $\\mathbb{E}\\mathbf{w}_{i}^{*2}=1$ for all $i$ , we have with probability at least $1-e^{-\\Omega(M)}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{A p p r o x}\\lesssim\\displaystyle\\sum_{k>k_{1}}\\lambda_{i}+\\bigg(\\frac{\\sum_{i>k_{1}}\\lambda_{i}}{M}+\\lambda_{k_{1}+1}+\\sqrt{\\frac{\\sum_{i>k_{1}}\\lambda_{i}^{2}}{M}}\\bigg)k_{1}}\\\\ &{\\quad\\quad\\lesssim\\log^{1-a}k_{1}+\\bigg(\\frac{\\log^{1-a}k_{1}}{M}+k_{1}^{-1}\\log^{-a}k_{1}+\\sqrt{\\frac{k_{1}^{1-2a}}{M}}\\bigg)k_{1}}\\\\ &{\\quad\\quad\\lesssim\\bigg(1+\\frac{k_{1}}{M}+\\frac{1}{\\log k_{1}}+\\frac{1}{\\log k_{1}}\\sqrt{\\frac{k_{1}}{M}}\\bigg)\\log^{1-a}k_{1}}\\\\ &{\\quad\\quad\\lesssim\\log^{1-a}k_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for any given $k_{1}\\leq M/2$ , where the third line uses $\\begin{array}{r}{\\sum_{i>k_{1}}\\lambda_{i}^{2}\\lesssim1/(k_{1}\\log^{2a}k_{1})}\\end{array}$ . Choosing $k_{1}=$ $M/2$ , we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\lesssim\\log^{1-a}M\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . Here the hidden constants depend on $a,b$ . ", "page_idx": 24}, {"type": "text", "text": "For the lower bound, we have from Lemma C.2 that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}\\gtrsim\\sum_{i=M}^{\\infty}\\lambda_{i}\\gtrsim\\sum_{i=M}^{\\infty}i^{-1}\\log^{-a}i\\gtrsim\\log^{1-a}M.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we have established matching upper and lower bounds for Approx. ", "page_idx": 24}, {"type": "text", "text": "D Bias error ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we derive upper and lower bounds for $\\mathsf{B i a s}(\\mathbf{w}^{*})$ defined in Eq. (5). Moreover, we show that the upper and lower bounds match up to constant factors in concrete examples. ", "page_idx": 25}, {"type": "text", "text": "D.1 An upper bound ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma D.1 (Upper bound on the bias term). Suppose the initial stepsize $\\begin{array}{r}{\\gamma\\leq\\frac{1}{c\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})}}\\end{array}$ for some constant $c>1$ . Then for any $\\mathbf{w}^{*}\\in\\mathbb{H}$ and $k\\,\\in\\,[d]$ such that $r(\\mathbf{H})\\geq k+M$ , the bias term in (5) satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathsf{B i a s}(\\mathbf{w}^{*})\\lesssim\\frac{1}{N_{\\mathbf{eff}}\\gamma}\\|\\mathbf{v}^{*}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, for any $k\\le M/3$ such that $r(\\mathbf{H})\\geq k+M,$ , the bias term satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Bias}(\\mathbf{w}^{*})\\lesssim\\frac{\\|\\mathbf{w}_{0:k}^{*}\\|_{2}^{2}}{N_{\\mathrm{eff}}\\gamma}\\cdot\\left[\\frac{\\mu_{M/2}(\\mathbf{A}_{k})}{\\mu_{M}(\\mathbf{A}_{k})}\\right]^{2}+\\|\\mathbf{w}_{k:\\infty}^{*}\\|_{\\mathbf{H}_{k:\\infty}}^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with probability $1-e^{-\\Omega(M)}$ , where $\\mathbf{A}_{k}:=\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}$ , $\\{\\mu_{i}(\\mathbf{A}_{k})\\}_{i=1}^{M}$ denote the eigenvalues of ${\\bf A}_{k}$ in non-increasing order for some constant $c>1$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma $D.I$ . Similar to the proof of Lemma C.1, we can without loss of generality assume the covariance matrix $\\mathbf{H}=\\operatorname{diag}\\{\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{d}\\}$ where $\\lambda_{i}\\geq\\lambda_{j}$ for any $i\\geq j$ . Let $\\dot{\\bf S H}^{1/2}\\,=$ $\\tilde{\\mathbf{U}}\\left(\\tilde{\\mathbf{A}}^{1/2}\\quad\\mathbf{0}\\right)\\tilde{\\mathbf{V}}^{\\top}$ be the singular value decomposition of $\\mathbf{SHS}^{\\top}$ , where $\\tilde{\\mathbf{A}}:=\\mathrm{diag}\\{\\tilde{\\lambda}_{1},\\tilde{\\lambda}_{2},\\dots,\\tilde{\\lambda}_{d}\\}$ is a diagonal matrix diagonal entries in non-increasing order. Define $\\mathbf{A}_{k}:=\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}$ . Then it follows from similar arguments as in Lemma C.1 that ${\\bf A}_{k}$ is invertible. ", "page_idx": 25}, {"type": "text", "text": "Since ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\|_{2}=\\gamma_{t}\\tilde{\\lambda}_{1}\\leq\\gamma\\tilde{\\lambda}_{1}\\leq\\frac{\\tilde{\\lambda}_{1}}{c\\sum_{i=1}^{M}\\tilde{\\lambda}_{i}}\\leq1\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for some constant $c>1$ by the stepsize assumption, it follows that $\\mathbf{I}_{M}-\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\succ\\mathbf{0}_{M}$ for all $t\\in[N]$ . Therefore, it can be verified that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\prod_{n=1}^{N}(\\mathbf{I}_{M}-\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\prod_{t=1}^{N}(\\mathbf{I}_{M}-\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\preceq\\left(\\mathbf{I}_{M}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\right)^{N_{e t}}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\left(\\mathbf{I}_{M}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\right)^{N_{e t}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and by definition of $\\mathsf{B i a s}(\\mathbf{w}^{*})$ in Eq. (5), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Bias}(\\mathbf{w}^{*})\\approx\\bigg\\|\\prod_{t=1}^{N}\\big(\\mathbf{I}-\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)\\mathbf{v}^{*}\\bigg\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}\\leq\\bigg\\|\\big(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)^{N_{\\mathrm{eff}}}\\mathbf{v}^{*}\\bigg\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that the eigenvalues of $\\mathsf{M}$ are $\\{\\tilde{\\lambda}_{i}(1-\\gamma\\tilde{\\lambda}_{i})^{2N_{\\mathrm{eff}}}\\}_{i=1}^{M}$ . Since the function $f(x)=x(1-\\gamma x)^{2N_{\\mathrm{eff}}}$ is maximized at $x_{0}=1/[(2N_{\\mathrm{eff}}+1)\\dot{\\gamma}]$ for $x\\in[0,1/\\gamma]$ with $f(x_{0})\\lesssim1/(\\bar{N}_{\\mathrm{eff}}\\acute{\\gamma})$ , it follows that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\mathsf{M}\\|_{2}\\leq c/(N_{\\mathrm{eff}}\\gamma)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for some constant $c>0$ . The first part of Lemma D.1 follows immediately. ", "page_idx": 25}, {"type": "text", "text": "Now we prove the second part of Lemma D.1. Recall that $\\mathbf{v}^{*}=(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}$ . Substituting $\\mathbf{SH}=(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}$ $\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty})$ into $\\mathbf{v}^{*}$ , we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\mathbf{M},\\mathbf{v}^{*\\otimes2}\\rangle=\\langle\\mathbf{M},((\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*})^{\\otimes2}\\rangle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {=\\mathbf{w}^{*\\,\\top}\\mathbf{H}\\mathbf{S}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathsf{M}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}}\\\\ {\\quad\\leq2T_{1}+2T_{2},\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{1}:=(\\mathbf{w}_{0:k}^{*})^{\\top}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbb{M}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{w}_{0:k}^{*},}\\\\ &{T_{2}:=(\\mathbf{w}_{k:\\infty}^{*})^{\\top}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbb{M}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{w}_{k:\\infty}^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We claim the following results which we prove later. With probability $1-e^{-\\Omega(M)}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nT_{1}\\leq\\frac{c\\|\\mathbf{w}_{0:k}^{*}\\|_{2}^{2}}{N_{\\mathbf{eff}}\\gamma}\\cdot\\left[\\frac{\\mu_{M/2}(\\mathbf{A}_{k})}{\\mu_{M}(\\mathbf{A}_{k})}\\right]^{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for some constant $c>0$ . ", "page_idx": 26}, {"type": "equation", "text": "$$\nT_{2}\\leq\\|\\mathbf{w}_{k:\\infty}^{*}\\|_{\\mathbf{H}_{k:\\infty}}^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining Eq. (22a), (22b) gives the second part of Lemma D.1. ", "page_idx": 26}, {"type": "text", "text": "Proof of claim (22a) By definition of $T_{1}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nT_{1}\\leq\\|\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbb{M}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\|_{2}\\cdot\\|\\mathbf{w}_{0:k}^{*}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Moreover, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\|\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{M}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\|_{2}}\\\\ &{\\leq\\|\\mathbf{M}\\|_{2}\\cdot\\|(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\|_{2}^{2}}\\\\ &{\\leq\\frac{c}{N_{\\mathbf{eff}}\\gamma}\\|(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for some constant $c>0$ , where the last line uses Eq. (19). ", "page_idx": 26}, {"type": "text", "text": "It remains to show ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\|_{2}\\leq c\\cdot\\frac{\\mu_{M/2}(\\mathbf{A}_{k})}{\\mu_{M}(\\mathbf{A}_{k})}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\quad}&{({\\bf S H S}^{\\top})^{-1}{\\bf S}_{0:k}{\\bf H}_{0:k}=({\\bf A}_{k}^{-1}-{\\bf A}_{k}^{-1}{\\bf S}_{0:k}[{\\bf H}_{0:k}^{-1}+{\\bf S}_{0:k}^{\\top}{\\bf A}_{k}^{-1}{\\bf S}_{0:k}]^{-1}{\\bf S}_{0:k}^{\\top}{\\bf A}_{k}^{-1}){\\bf S}_{0:k}{\\bf H}_{0:k}}&{\\quad}\\\\ &{}&{\\quad={\\bf A}_{k}^{-1}{\\bf S}_{0:k}{\\bf H}_{0:k}-{\\bf A}_{k}^{-1}{\\bf S}_{0:k}[{\\bf H}_{0:k}^{-1}+{\\bf S}_{0:k}^{\\top}{\\bf A}_{k}^{-1}{\\bf S}_{0:k}]^{-1}{\\bf S}_{0:k}^{\\top}{\\bf A}_{k}^{-1}{\\bf S}_{0:k}{\\bf H}_{0:k}}&{\\quad}\\\\ &{}&{\\quad={\\bf A}_{k}^{-1}{\\bf S}_{0:k}[{\\bf H}_{0:k}^{-1}+{\\bf S}_{0:k}^{\\top}{\\bf A}_{k}^{-1}{\\bf S}_{0:k}]^{-1}{\\bf H}_{0:k}^{-1}{\\bf H}_{0:k}}&{\\quad}\\\\ &{}&{\\quad={\\bf A}_{k}^{-1}{\\bf S}_{0:k}[{\\bf H}_{0:k}^{-1}+{\\bf S}_{0:k}^{\\top}{\\bf A}_{k}^{-1}{\\bf S}_{0:k}]^{-1},}&{\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second line uses Woodbury\u2019s identity. Since ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}\\succeq\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "it follows that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\|_{2}\\leq\\|[\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, with probability at least $1-e^{-\\Omega(M)}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\|_{2}\\le\\|\\mathbf{A}_{k}^{-1}\\|_{2}\\cdot\\|\\mathbf{S}_{0:k}\\|_{2}\\cdot\\|[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\|_{2}}&{}\\\\ {\\le\\|\\mathbf{A}_{k}^{-1}\\|_{2}\\cdot\\|\\mathbf{S}_{0:k}\\|_{2}\\cdot\\|[\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\|_{2}}&{}\\\\ {\\le\\frac{\\|\\mathbf{A}_{k}^{-1}\\|_{2}\\cdot\\|\\mathbf{S}_{0:k}\\|_{2}}{\\mu_{\\operatorname*{min}}(\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k})}\\lesssim\\frac{\\|\\mathbf{A}_{k}^{-1}\\|_{2}}{\\mu_{\\operatorname*{min}}(\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k})}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality follows from the fact that $\\|\\mathbf{S}_{0:k}\\|_{2}=\\sqrt{\\|\\mathbf{S}_{0:k}^{\\top}\\mathbf{S}_{0:k}\\|_{2}}\\le c$ for some constant $c>0$ when $k\\leq M/2$ with probability at least $1-e^{-\\Omega(M)}$ . Since $\\mathbf{S}_{0:k}$ is independent of ${\\bf A}_{k}$ and the distribution of S0:k is rotationally invariant, we may write S0\u22a4:kAk\u2212 1S0:k = iM=1\u03bb\u02c61 \u02dcs ", "page_idx": 26}, {"type": "text", "text": "where $\\tilde{\\mathbf{s}}_{i}\\overset{\\mathrm{iid}}{\\sim}\\mathcal{N}(0,\\mathbf{I}_{k}/M)$ and $(\\hat{\\lambda}_{i})_{i=1}^{M}$ are eigenvalues of ${\\bf A}_{k}$ in non-increasing order. Therefore, for $k\\le M/3$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}=\\sum_{i=1}^{M}\\frac{1}{\\hat{\\lambda}_{M-i}}\\tilde{\\mathbf{s}}_{i}\\tilde{\\mathbf{s}}_{i}^{\\top}\\succeq\\sum_{i=1}^{M/2}\\frac{1}{\\hat{\\lambda}_{M-i}}\\tilde{\\mathbf{s}}_{i}\\tilde{\\mathbf{s}}_{i}^{\\top}\\succeq\\frac{1}{\\hat{\\lambda}_{M/2}}\\sum_{i=1}^{M/2}\\tilde{\\mathbf{s}}_{i}\\tilde{\\mathbf{s}}_{i}^{\\top}\\succeq\\frac{\\mathbf{c}\\mathbf{I}_{k}}{\\hat{\\lambda}_{M/2}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for some constant $c>0$ with probability at least $1-e^{-\\Omega(M)}$ , where in the last line we again use the concentration properties of gaussian covariance matrices (see e.g., Theorem 6.1 in [41]). As a direct consequence, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}[\\mathbf{H}_{0:k}^{-1}+\\mathbf{S}_{0:k}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{0:k}]^{-1}\\|_{2}\\leq c\\cdot\\frac{\\mu_{M/2}(\\mathbf{A}_{k})}{\\mu_{M}(\\mathbf{A}_{k})}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ for some constant $c>0$ . This concludes the proof. ", "page_idx": 27}, {"type": "text", "text": "Proof of claim (22b) By definition of $T_{2}$ in Eq. (21), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{2}=\\mathbf{w}_{k:\\infty}^{*}{^{\\top}\\mathbf{H}_{k:\\infty}}\\mathbf{S}_{k:\\infty}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1/2}(\\mathbf{I}_{M}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{2N_{\\mathrm{eff}}}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1/2}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{w}_{k:\\infty}^{*}}\\\\ &{\\quad\\leq\\mathbf{w}_{k:\\infty}^{*}{^{\\top}\\mathbf{H}_{k:\\infty}}\\mathbf{S}_{k:\\infty}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{w}_{k:\\infty}^{*}}\\\\ &{\\quad\\leq\\|\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}\\|\\cdot\\|\\mathbf{w}_{k:\\infty}^{*}\\|_{\\mathbf{H}_{k:\\infty}}^{2}}\\\\ &{\\quad\\leq\\|\\mathbf{w}_{k:\\infty}^{*}\\|_{\\mathbf{H}_{k:\\infty}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last line follows from ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}\\|_{2}=\\|\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}+\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\|\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{A}_{k}^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}\\|_{2}\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "D.2 A lower bound ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma D.2 (Lower bound on the bias term). Suppose w\u2217follows some prior distribution and the initial stepsize $\\begin{array}{r}{\\gamma\\leq\\frac{1}{c\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})}}\\end{array}$ for some constant $c>2$ . Let $\\mathbf{H^{w}}:=\\mathbb{E}\\mathbf{w}^{*}\\mathbf{w}^{*\\top}$ . Then the bias term in Eq. (5) satisfies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathrm{Bias}(\\mathbf{w}^{*})\\gtrsim\\sum_{i:\\tilde{\\lambda}_{i}<1/(\\gamma N_{\\mathrm{eff}})}\\frac{\\mu_{i}(\\mathbf{SHH^{w}H S^{T}})}{\\mu_{i}(\\mathbf{SHS^{T}})}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "almost surely, where MN := SHS\u22a4(I \u22122\u03b3SHS\u22a4 2Neff. ", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma $D.2$ . Adopt the notations in the proof of Lemma D.1. By definition of the bias term, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Bias}(\\mathbf{w}^{*})\\approx\\displaystyle\\left\\|\\prod_{t=1}^{N}\\left(\\mathbf{I}-\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\right)\\mathbf{v}^{*}\\right\\|_{\\mathbf{SHS}^{\\top}}^{2}}\\\\ &{\\quad\\quad\\quad\\quad=\\langle\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\displaystyle\\prod_{t=1}^{N}(\\mathbf{I}-\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{2N_{e t f}},\\mathbf{v}^{*\\otimes2}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\geq\\langle\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}(\\mathbf{I}-\\displaystyle\\sum_{t=1}^{N}\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{2N_{e t f}},\\mathbf{v}^{*\\otimes2}\\rangle}\\\\ &{\\quad\\quad\\quad\\geq\\langle\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}(\\mathbf{I}-2\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{2N_{e t f}},\\mathbf{v}^{*\\otimes2}\\rangle=:\\langle\\mathbf{M}_{\\mathsf{N}},\\mathbf{v}^{*\\otimes2}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the third line uses $\\mathbf{I}_{M}\\!-\\!2\\gamma_{t}\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\succ\\mathbf{0}_{M}$ for all $t\\in[N]$ established in the proof of Lemma D.1, $\\begin{array}{r}{\\sum_{i=1}^{N}\\gamma_{i}\\le2\\gamma N_{\\mathrm{eff}}}\\end{array}$ , and the fact that $(1-w)(1-v)\\geq1-w-v$ for $w,v>0$ . Substituting the definition of $\\mathbf{v}^{*}$ in Eq. (5) into the expression, we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{\\ast}}\\mathbf{B}\\mathbf{ias}(\\mathbf{w}^{\\ast})\\gtrsim\\mathbb{E}_{\\mathbf{w}^{\\ast}}\\langle\\mathsf{M}_{\\mathsf{N}},\\mathbf{v}^{\\ast\\otimes2}\\rangle=\\mathbb{E}_{\\mathbf{w}^{\\ast}}\\langle\\mathsf{M}_{\\mathsf{N}},((\\mathbf{SHS}^{\\top})^{-1}\\mathbf{SHw}^{\\ast})^{\\otimes2}\\rangle\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathrm{tr}(\\mathbf{HS}^{\\top}(\\mathbf{S}\\mathbf{HS}^{\\top})^{-1}\\mathsf{M}_{\\mathsf{N}}(\\mathbf{S}\\mathbf{HS}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{H}^{\\mathbf{w}})}\\\\ &{=\\mathrm{tr}((\\mathbf{S}\\mathbf{HS}^{\\top})^{-1}\\mathsf{M}_{\\mathsf{N}}(\\mathbf{S}\\mathbf{HS}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{H}^{\\mathbf{w}}\\mathbf{HS}^{\\top})}\\\\ &{\\ge\\displaystyle\\sum_{i=1}^{M}\\mu_{M-i+1}((\\mathbf{S}\\mathbf{HS}^{\\top})^{-1}\\mathsf{M}_{\\mathsf{N}}(\\mathbf{S}\\mathbf{HS}^{\\top})^{-1})\\cdot\\mu_{i}(\\mathbf{S}\\mathbf{H}\\mathbf{H}^{\\mathbf{w}}\\mathbf{HS}^{\\top}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last line uses Von Neumann\u2019s trace inequality. Continuing the calculation, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbf{w}^{*}}\\mathrm{Bias}(\\mathbf{w}^{*})\\gtrsim\\displaystyle\\sum_{i=1}^{M}\\frac{\\mu_{i}(\\mathbf{SHH^{w}H S^{T}})}{\\mu_{i}\\big((\\mathbf{SHS^{T}})^{2}\\mathbf{M}_{\\mathrm{N}}^{-1}\\big)}}&{}\\\\ {=\\displaystyle\\sum_{i=1}^{M}\\frac{\\mu_{i}(\\mathbf{SHH^{w}H S^{T}})}{\\mu_{i}\\Big((\\mathbf{SHS^{T}})\\big(\\mathbf{I}-2\\gamma\\mathbf{SHS^{T}}\\big)^{-2N_{\\mathrm{eff}}}\\Big)}}&{}\\\\ {\\gtrsim\\displaystyle\\sum_{i;\\tilde{\\lambda}_{i}<1/(\\gamma N_{\\mathrm{eff}})}\\frac{\\mu_{i}\\big(\\mathbf{SHH^{w}H S^{T}}\\big)}{\\mu_{i}\\big(\\mathbf{SHS^{T}}\\big)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the first inequality uses $\\mu_{M+i-1}(A)=\\mu_{i}^{-1}(A^{-1})$ for any positive definite matrix $A\\in\\mathbb{R}^{M\\times M}$ , and the second line follows from the definition of $\\mathsf{M}_{\\mathsf{N}}$ and the fact that $(1-\\lambda\\gamma N_{\\mathrm{eff}})^{-2N_{\\mathrm{eff}}}\\,\\lesssim\\,1$ when $\\lambda<1/(\\gamma N_{\\mathrm{eff}})$ . \u53e3 ", "page_idx": 28}, {"type": "text", "text": "D.3 Examples on matching bounds for $\\mathsf{B i a s}(\\mathbf{w}^{*})$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we derive matching upper and lower bounds for $\\mathsf{B i a s}(\\mathbf{w}^{*})$ in (5) in three scenarios: power-law spectrum (Lemma D.3), power-law spectrum with source condition (Lemma D.4) and logarithmic power-law spectrum (Lemma D.5). Recall that we define $\\mathsf{B i a s}:=\\mathbb{E}_{\\mathbf{w}^{\\ast}}\\mathsf{B i a s}(\\mathbf{w}^{\\ast})$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma D.3 (Bounds on Bias under the power-law spectrum). Suppose Assumption $I C$ and 2 hold and the initial stepsize $\\begin{array}{r}{\\gamma\\,\\leq\\,\\frac{1}{c\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})}}\\end{array}$ for some constant $c>2$ . Then with probability at least $1-e^{-\\Omega(M)}$ over the randomness of S ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{B i a s}(\\mathbf{w}^{*})\\lesssim\\operatorname*{max}\\big\\{(N_{\\mathbf{eff}}\\gamma)^{1/a-1},\\ M^{1-a}\\big\\},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathrm{Bias}(\\mathbf{w}^{*})\\gtrsim(N_{\\mathbf{eff}}\\gamma)^{1/a-1}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "when $(N_{\\mathrm{eff}}\\gamma)^{1/a}\\le M/c$ for some constant $c>0,$ . Here, all the (hidden) constants depend only on the power-law degree $a$ . ", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma $D.3$ . For the upper bound, using Lemma G.5, D.1 and the assumption that $\\mathbb{E}\\mathbf{w}_{i}^{*2}=1$ for all $i>0$ , with probability at least $1-e^{-\\Omega(M)}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{w}^{*}}\\mathrm{Bias}(\\mathbf{w}^{*})\\lesssim\\mathbb{E}_{\\mathbf{w}^{*}}\\left[\\frac{\\|\\mathbf{w}_{0:k_{2}}^{*}\\|_{2}^{2}}{N_{\\mathrm{eff}}\\gamma}+\\|\\mathbf{w}_{k_{2}:\\infty}^{*}\\|_{\\mathbf{H}_{k_{2}:\\infty}}^{2}\\right]}\\\\ &{\\phantom{\\leq}\\lesssim\\frac{k_{2}}{N_{\\mathrm{eff}}\\gamma}+\\displaystyle\\sum_{k>k_{2}}\\lambda_{i}}\\\\ &{\\phantom{\\leq}\\tilde{N}_{\\mathrm{eff}}\\gamma+k_{2}^{1-a}}\\\\ &{\\lesssim\\operatorname*{max}\\big\\{\\big(N_{\\mathrm{eff}}\\gamma\\big)^{1/a-1},\\ M^{1-a}\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where in the last inequality, we choose $k_{2}=[M/3]\\wedge(N_{\\mathrm{eff}}\\gamma)^{1/a}$ to minimize the upper bound. When $(N_{\\mathrm{eff}}\\gamma)^{1/a}\\le M/3$ , combining Lemma D.2 and G.4 gives the lower bound ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathbf{B}\\mathbf{ias}(\\mathbf{w}^{*})\\gtrsim\\sum_{\\substack{i:\\,\\tilde{\\lambda}_{i}<1/(N_{\\mathrm{eff}}\\,\\gamma)}}\\frac{\\mu_{i}(\\mathbf{SHH^{w}H S^{\\top}})}{\\mu_{i}(\\mathbf{SHS^{\\top}})}=\\sum_{\\substack{i:\\,\\tilde{\\lambda}_{i}<1/(N_{\\mathrm{eff}}\\,\\gamma)}}\\frac{\\mu_{i}(\\mathbf{SH^{2}S^{\\top}})}{\\mu_{i}(\\mathbf{SHS^{\\top}})},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\gtrsim\\sum_{\\tilde{\\lambda}_{i}<1/(N_{\\mathrm{eff}}\\gamma),i\\leq M}\\frac{i^{-2a}}{i^{-a}}=\\sum_{\\lambda_{i}<1/(N_{\\mathrm{eff}}\\gamma),i\\leq M}i^{-a}\\gtrsim(N_{\\mathrm{eff}}\\gamma)^{1/a-1}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . Here, the hidden constants depend only on $a$ . ", "page_idx": 29}, {"type": "text", "text": "Lemma D.4 (Bounds on Bias under the source condition). Suppose Assumption $3$ hold and the initial stepsize c tr(S1HS\u22a4) for some constant c > 2. Then with probability at least 1 \u2212e\u2212\u2126(M) over the randomness of S ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{B i a s}(\\mathbf{w}^{*})\\lesssim\\operatorname*{max}\\big\\{(N_{\\mathsf{e f f}}\\,\\gamma)^{(1-b)/a},\\ M^{1-b}\\big\\},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathrm{Bias}(\\mathbf{w}^{*})\\gtrsim(N_{\\mathbf{eff}}\\gamma)^{(1-b)/a}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "when $(N_{\\mathrm{eff}}\\gamma)^{1/a}\\le M/c$ for some constant $c>0$ . In both results, the hidden constants depend only on $a,b$ . ", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma $D.4$ . For the upper bound, using Lemma G.5, D.1 and the assumption that (w.l.o.g.) $\\mathbb{E}\\mathbf{w}_{i}^{*2}\\approx i^{a-b}$ for all $i>0$ , with probability at least $1-e^{-\\Omega(M)}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{w}^{*}}\\mathrm{Bias}(\\mathbf{w}^{*})\\lesssim\\mathbb{E}_{\\mathbf{w}^{*}}\\Bigg[\\frac{\\|\\mathbf{w}_{0:k_{2}}^{*}\\|_{2}^{2}}{N_{\\mathrm{eff}}\\gamma}+\\|\\mathbf{w}_{k_{2}:\\infty}^{*}\\|_{\\mathbf{H}_{k_{2}:\\infty}}^{2}\\Bigg]}\\\\ &{\\lesssim\\frac{k_{2}^{1+a-b}}{N_{\\mathrm{eff}}\\gamma}+\\displaystyle\\sum_{k>k_{2}}\\lambda_{i}\\cdot i^{a-b}}\\\\ &{\\lesssim\\frac{k_{2}^{1+a-b}}{N_{\\mathrm{eff}}\\gamma}+k_{2}^{1-b}}\\\\ &{\\lesssim\\operatorname*{max}\\left\\{(N_{\\mathrm{eff}}\\gamma)^{(1-b)/a},\\ M^{1-b}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "when $b<a+1$ , where in the last inequality, we choose $k_{2}=[M/3]\\wedge(N_{\\mathrm{eff}}\\gamma)^{1/a}$ to minimize the upper bound. ", "page_idx": 29}, {"type": "text", "text": "When $(N_{\\mathrm{eff}}\\gamma)^{1/a}\\le M/c$ for some large constant $c>0$ , combining Lemma D.2 and G.4 yields the lower bound ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{w}^{*}}\\mathrm{Bias}(\\mathbf{w}^{*})\\gtrsim\\underset{i:\\tilde{\\lambda}_{i}<1/(N_{\\mathrm{eff}}\\gamma)}{\\sum}\\frac{\\mu_{i}(\\mathbf{SHH}\\mathbf{H}^{\\mathbf{w}}\\mathbf{H}\\mathbf{S}^{\\top})}{\\mu_{i}(\\mathbf{SHS}^{\\top})}\\approx\\underset{i:\\tilde{\\lambda}_{i}<1/(N_{\\mathrm{eff}}\\gamma)}{\\sum}\\frac{\\mu_{i}(\\mathbf{SH}^{(a+b)/a}\\mathbf{S}^{\\top})}{\\mu_{i}(\\mathbf{SHS}^{\\top})},}\\\\ {\\gtrsim\\underset{\\tilde{\\lambda}_{i}<1/(N_{\\mathrm{eff}}\\gamma),i\\leq M}{\\sum}\\frac{i^{-(a+b)}}{i^{-a}}=\\underset{\\lambda_{i}<1/(N_{\\mathrm{eff}}\\gamma),i\\leq M}{\\sum}i^{-b}\\gtrsim(N_{\\mathrm{eff}}\\gamma)^{(1-b)/a}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . Here, the hidden constants depend only on $a,b$ . ", "page_idx": 29}, {"type": "text", "text": "Upper bound when $b\\geq a+1$ . Following the previous derivations, when $b=a+1$ , we have with probability at least $1-e^{-\\Omega(M)}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{w}^{*}}\\mathrm{Bias}(\\mathbf{w}^{*})\\lesssim\\mathbb{E}_{\\mathbf{w}^{*}}\\left[\\frac{\\|\\mathbf{w}_{0:k_{2}}^{*}\\|_{2}^{2}}{N_{\\mathrm{eff}}\\gamma}+\\|\\mathbf{w}_{k_{2}:\\infty}^{*}\\|_{\\mathbf{H}_{k_{2};\\infty}}^{2}\\right]}\\\\ &{\\lesssim\\frac{\\log k_{2}}{N_{\\mathrm{eff}}\\gamma}+k_{2}^{1-b}}\\\\ &{\\lesssim\\frac{\\log(N_{\\mathrm{eff}}\\gamma)}{N_{\\mathrm{eff}}\\gamma}+M^{1-b}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last line follows by setting $k_{2}=[M/3]\\wedge(N_{\\mathrm{eff}}\\gamma)^{1/a}$ . When $b>a+1$ , we have with probability at least 1 \u2212e\u2212\u2126(M) ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathrm{Bias}(\\mathbf{w}^{*})\\lesssim\\mathbb{E}_{\\mathbf{w}^{*}}\\left[\\frac{\\|\\mathbf{w}_{0:k_{2}}^{*}\\|_{2}^{2}}{N_{\\mathbf{eff}}\\gamma}+\\|\\mathbf{w}_{k_{2}:\\infty}^{*}\\|_{\\mathbf{H}_{k_{2}:\\infty}}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\lesssim\\frac{1}{N_{\\mathrm{eff}}\\gamma}+k_{2}^{1-b}}\\\\ {\\displaystyle\\lesssim\\frac{1}{N_{\\mathrm{eff}}\\gamma}+M^{1-b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last follows by chooing $k_{2}=M/3$ to minimize the upper bound. ", "page_idx": 30}, {"type": "text", "text": "Note that there exist non-constant gaps between the upper and lower bounds on the bias term in the simple regime where $b\\geq a+1$ . We leave a more precise analysis of the bias term for future work. ", "page_idx": 30}, {"type": "text", "text": "Lemma D.5 (Bounds on Bias under the logarithmic power-law spectrum). Suppose Assumption $^{4}$ hold and the initial stepsize $\\begin{array}{r}{\\gamma\\leq\\frac{1}{c\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})}}\\end{array}$ for some constant $c>2$ . Let $\\bar{\\mathsf{k}}:=\\operatorname*{inf}\\{k:k\\log^{a}\\dot{k}\\geq N_{\\mathsf{e f f}}\\gamma\\}$ . Then with probability at least $\\mathrm{1}-e^{-\\Omega(M)}$ over the randomness of S ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{B i a s}(\\mathbf{w}^{*})\\lesssim\\operatorname*{max}\\big\\{\\log^{1-a}(N_{\\mathsf{e f f}}\\gamma),\\log^{1-a}M\\big\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathrm{Bias}(\\mathbf{w}^{*})\\gtrsim\\log^{1-a}(N_{\\mathrm{eff}}\\gamma)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "when $(N_{\\tt e f f}\\gamma)\\leq M^{c}$ for some sufficiently small constant $c>0$ . Here, all constants depend only on the power-law degree $a$ . ", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma $D.5$ . For the upper bound, using Lemma G.7, D.1 and the assumption that $\\mathbb{E}\\mathbf{w}_{i}^{*2}=1$ for all $i>0$ , with probability at least $1-e^{-\\Omega(M)}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{B i a s}(\\mathbf{w}^{*})\\lesssim\\mathbb{E}_{\\mathbf{w}^{*}}\\Bigg[\\frac{\\|\\mathbf{w}_{0:k_{2}}^{*}\\|_{2}^{2}}{N_{\\mathrm{eff}}\\gamma}+\\|\\mathbf{w}_{k_{2}:\\infty}^{*}\\|_{\\mathbf{H}_{k_{2}:\\infty}}^{2}\\Bigg]}\\\\ &{\\phantom{\\leq}\\lesssim\\frac{k_{2}}{N_{\\mathrm{eff}}\\gamma}+\\displaystyle\\sum_{k>k_{2}}\\lambda_{i}}\\\\ &{\\phantom{\\leq}\\frac{k_{2}}{N_{\\mathrm{eff}}\\gamma}+\\log^{1-a}k_{2}}\\\\ &{\\lesssim\\operatorname*{max}\\big\\{\\log^{1-a}(N_{\\mathrm{eff}}\\gamma),\\log^{1-a}M\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where in the last inequality, we choose $k_{2}=[M/3]\\wedge\\left[(N_{\\sf e f f}\\,\\gamma)/\\log^{a}(N_{\\sf e f f}\\,\\gamma)\\right]$ to minimize the upper bound. ", "page_idx": 30}, {"type": "text", "text": "Recall $\\mathsf{k}^{*}\\approx M/\\log M$ (for example we may define $\\mathsf{k}^{*}=\\operatorname*{inf}\\{k:k\\log k\\geq M\\},$ ) in Lemma G.6. Combining Lemma D.2 and G.6 gives the lower bound ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{w}^{*}}\\mathrm{Bias}(\\mathbf{w}^{*})\\gtrsim\\underset{i\\lambda_{i}<1/(N_{\\mathrm{eff}}\\gamma)}{\\sum}\\frac{\\mu_{i}\\big(\\mathbf{S}\\mathbf{H}\\mathbf{H}^{\\mathbf{w}}\\mathbf{H}^{\\mathbf{v}}\\mathbf{H}\\mathbf{S}^{\\top}\\big)}{\\mu_{i}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})}\\simeq\\underset{i\\lambda_{i}<1/(N_{\\mathrm{eff}}\\gamma)}{\\sum}\\frac{\\mu_{i}(\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top})}{\\mu_{i}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})},}\\\\ &{\\gtrsim\\underset{\\bar{\\lambda}_{i}<1/(N_{\\mathrm{eff}}\\gamma),i\\leq\\mathbf{k}^{*}}{\\sum}\\frac{i^{-2}\\log^{-2a}i}{i^{-1}\\log^{-a}i}=\\underset{\\lambda_{i}<1/(N_{\\mathrm{eff}}\\gamma),i\\leq\\mathbf{k}^{*}}{\\sum}i^{-1}\\log^{-a}i}\\\\ &{\\gtrsim\\underset{i=N_{\\mathrm{eff}}\\gamma}{\\sum}^{i^{-1}}1\\log^{-a}i}\\\\ &{\\gtrsim\\log^{1-a}(N_{\\mathrm{eff}}\\gamma)-\\log^{1-a}(\\mathbf{k}^{*})}\\\\ &{\\gtrsim\\log^{1-a}(N_{\\mathrm{eff}}\\gamma)-\\log^{1-a}(\\mathbf{k}^{*})}\\\\ &{\\gtrsim\\log^{1-a}(N_{\\mathrm{eff}}\\gamma)-c_{1}\\log^{1-a}(M)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ for some constant $c_{1}>0$ . Here, the (hidden) constants depend only on $a$ . Therefore, when $(N_{\\mathbf{eff}}\\gamma)^{1/a}\\le M^{c}$ for some sufficiently small constant $c>0$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{B i a s}(\\mathbf{w}^{*})\\gtrsim\\log^{1-a}(N_{\\mathrm{eff}}\\gamma)-c_{1}\\log^{1-a}(M)\\gtrsim\\log^{1-a}(N_{\\mathrm{eff}}\\gamma).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . ", "page_idx": 30}, {"type": "text", "text": "E Variance error ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we present matching upper and lower bounds on the variance term Var defined in (6) under the power-law or logarithmic power-law spectrum. ", "page_idx": 31}, {"type": "text", "text": "Lemma E.1 (Matching bounds on Var under power-law spectrum). Under Assumption 2, Var defined in Eq. (6) satisfies ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathsf{V a r}\\approx\\frac{\\operatorname*{min}\\{M,\\;(N_{\\mathsf{e f f}}\\gamma)^{1/a}\\}}{N_{\\mathsf{e f f}}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ over the randomness of S. Here, the hidden constants only depend on $a$ . ", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma $E.l$ . By the definition of Var in Eq. (6) and Lemma G.4, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathsf{V a r}=\\frac{\\#\\{\\tilde{\\lambda}_{j}\\geq1/(N_{\\mathrm{eff}}\\,\\gamma)\\}+\\left(N_{\\mathrm{eff}}\\,\\gamma\\right)^{2}\\sum_{\\tilde{\\lambda}_{j}<1/(N_{\\mathrm{eff}}\\,\\gamma)}\\tilde{\\lambda}_{j}^{2}}{N_{\\mathrm{eff}}}}\\\\ &{\\approx\\frac{\\operatorname*{min}\\big\\{M,\\,(N_{\\mathrm{eff}}\\,\\gamma)^{1/a}+(N_{\\mathrm{eff}}\\,\\gamma)^{2}\\cdot(N_{\\mathrm{eff}}\\,\\gamma)^{(1-2a)/a}\\big\\}}{N_{\\mathrm{eff}}}}\\\\ &{\\approx\\frac{\\operatorname*{min}\\{M,\\,(N_{\\mathrm{eff}}\\,\\gamma)^{1/a}\\}}{N_{\\mathrm{eff}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ over the randomness of S. Here the hidden constants may depend on $a$ . \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Lemma E.2 (Matching bounds on Var under logarithmic power-law spectrum). Under Assumption 4, Var defined in Eq. (6) satisfies ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathsf{V a r}\\approx\\frac{\\operatorname*{min}\\{M,\\,\\bar{\\mathsf{k}}\\}}{N_{\\mathsf{e f f}}}\\approx\\frac{\\operatorname*{min}\\{M,\\,(N_{\\mathsf{e f f}}\\gamma)/\\log^{a}(N_{\\mathsf{e f f}}\\gamma)\\}}{N_{\\mathsf{e f f}}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ over the randomness of S, where $\\bar{\\mathsf{k}}:=\\operatorname*{inf}\\left\\{k:k\\log^{a}k\\ \\geq\\right.$ $\\left(N_{\\mathsf{e f f}}\\gamma\\right)\\}$ and $\\eqsim$ hides constants that only depend on $a$ . ", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma $E.2$ . Define $\\mathsf{k}^{*}\\,=\\,\\operatorname*{inf}\\left\\{k\\,:\\,k\\log k\\,\\geq\\,M\\right\\}$ and let $\\tilde{D}:=\\#\\{\\tilde{\\lambda}_{j}\\,\\geq\\,1/(N_{\\mathrm{eff}}\\,\\gamma)\\}\\,+$ $(N_{\\mathrm{eff}}\\gamma)^{2}\\sum\\tilde{\\lambda}_{j}{<}1/(N_{\\mathrm{eff}}\\gamma)\\stackrel{\\sim}{\\lambda}_{j}^{2}$ . By the definition of Var in Eq. (6) and Lemma G.6, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{V a r}=\\frac{\\#\\{\\tilde{\\lambda}_{j}\\geq1/(N_{\\mathsf{e f f}}\\,\\gamma)\\}+(N_{\\mathsf{e f f}}\\,\\gamma)^{2}\\sum_{\\tilde{\\lambda}_{j}<1/(N_{\\mathsf{e f f}}\\,\\gamma)}\\tilde{\\lambda}_{j}^{2}}{N_{\\mathsf{e f f}}}}\\\\ &{\\quad=\\frac{\\tilde{D}}{N_{\\mathsf{e f f}}}\\approx\\frac{\\operatorname*{min}\\{M,\\,\\bar{\\mathsf{k}}\\}}{N_{\\mathsf{e f f}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ over the randomness of $\\mathbf{S}$ , where the second line follows from ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{D}\\gtrsim\\#\\{\\tilde{\\lambda}_{j}\\geq1/(N_{\\mathrm{eff}}\\,\\gamma)\\}\\approx\\frac{N_{\\mathrm{eff}}\\,\\gamma}{\\log^{a}(N_{\\mathrm{eff}}\\,\\gamma)},\\qquad\\qquad\\mathrm{and}}\\\\ &{\\tilde{D}\\lesssim\\frac{N_{\\mathrm{eff}}\\,\\gamma}{\\log^{a}(N_{\\mathrm{eff}}\\,\\gamma)}+\\frac{(N_{\\mathrm{eff}}\\,\\gamma)^{2}}{\\log^{2a}(N_{\\mathrm{eff}}\\,\\gamma)}\\cdot\\sum_{j:\\tilde{\\lambda}_{j}<1/(N_{\\mathrm{eff}}\\,\\gamma)}\\frac{1}{j^{2}}\\lesssim\\frac{N_{\\mathrm{eff}}\\,\\gamma}{\\log^{a}(N_{\\mathrm{eff}}\\,\\gamma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "when $\\overline{{\\mathsf{k}}}\\lesssim M$ . ", "page_idx": 31}, {"type": "text", "text": "F Expected risk of the average of (SGD) iterates ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we study the expected risk of the average of (SGD) iterates. Namely, we consider a fixed stepsize (SGD) procedure where $\\gamma_{t}=\\gamma$ and define $\\begin{array}{r}{\\bar{\\mathbf{v}}_{N}:=\\sum_{i=0}^{N-1}\\mathbf{v}_{i}/N}\\end{array}$ . Our goal is to derive matching upper and lower bounds $\\mathcal{R}(\\bar{\\bf v}_{N})$ in terms of the sample size $N$ and model size $M$ . ", "page_idx": 31}, {"type": "text", "text": "Compared with the last iterate of (SGD) with geometrically decaying stepsizes, we show that the average of (SGD) iterates with a fixed stepsize achieves a better risk, in the sense that the effective sample size $N_{\\mathsf{e f f}}$ is replaced by $N$ in the bounds (c.f. Theorem 4.1). This may give improvement up to logarithmic factors. ", "page_idx": 32}, {"type": "text", "text": "We start with invoking the following result in [48]. ", "page_idx": 32}, {"type": "text", "text": "Theorem F.1 (A variant of Theorem 2.1 and 2.2 in [48]). Suppose Assumption 1 hold. Consider an $M$ -dimensional sketched predictor trained by fixed stepsize (SGD) with $N$ samples. Let $\\bar{\\mathbf{v}}_{N}:=$ $\\sum_{i=0}^{N-1}\\mathbf{v}_{i}/N$ be the average of the iterations of SGD. Assume $\\mathbf{v}_{0}=\\mathbf{0}$ and $\\sigma^{2}\\gtrsim1$ . Conditional on S and suppose the stepsize $\\gamma\\,<\\,1/(c\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}))$ for some constant $c\\,>\\,0$ , then there exist Approx, Bias, Var such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\mathcal{R}_{M}(\\bar{\\mathbf{v}}_{N})-\\sigma^{2}\\approx\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{A p p r o x}+\\mathsf{B i a s}+\\sigma^{2}\\mathsf{V a r},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the expectation of ${\\mathcal{R}}_{M}$ is over $\\mathbf{w}^{*}$ and $(\\mathbf{x}_{i},y_{i})_{i=1}^{N}$ and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{A p p r o x}:=\\mathbb{E}\\xi^{2}}\\\\ &{\\qquad\\qquad=\\Big\\|\\Big(\\mathbf{I}-\\mathbf{H}^{\\frac{1}{2}}\\mathbf{S}^{\\top}\\big(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\big)^{-1}\\mathbf{S}\\mathbf{H}^{\\frac{1}{2}}\\Big)\\mathbf{H}^{\\frac{1}{2}}\\mathbf{w}^{*}\\Big\\|^{2},}\\\\ &{\\qquad\\quad\\mathbb{E}_{\\mathbf{w}^{*}}\\big(T_{1}+T_{3}\\big)\\lesssim\\mathsf{B i a s}\\lesssim\\mathbb{E}_{\\mathbf{w}^{*}}(T_{2}+T_{4}),}\\\\ &{\\qquad\\qquad\\qquad\\quad\\mathsf{V a r}\\stackrel{}{\\sim}\\frac{D_{\\mathrm{eff},N}}{N},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{1}:=\\frac{1}{\\gamma^{2}N^{2}}\\operatorname{tr}\\left(\\left(\\mathbf{I}-(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{N/4}\\right)^{2}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{B}_{0}\\right),}\\\\ &{T_{2}:=\\frac{1}{\\gamma^{2}N^{2}}\\operatorname{tr}\\left(\\left(\\mathbf{I}-(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{N}\\right)^{2}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{B}_{0}\\right),}\\\\ &{T_{3}:=\\frac{1}{\\gamma N^{2}}\\operatorname{tr}\\left(\\left(\\mathbf{I}-(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{N/4}\\right)\\mathbf{B}_{0}\\right)\\cdot\\operatorname{tr}\\left(\\left(\\mathbf{I}-(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{N/4}\\right)^{2}\\right),}\\\\ &{T_{4}:=\\frac{1}{\\gamma N}\\operatorname{tr}\\left(\\mathbf{B}_{0}-(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{N}\\mathbf{B}_{0}(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{N}\\right)\\cdot\\frac{D_{\\mathrm{eff,N}}}{N},}\\\\ &{\\mathbf{B}_{0}:=\\mathbf{v}^{\\ast}\\mathbf{v}^{\\ast},}\\\\ &{D_{\\mathrm{eff,N}}:=\\#\\{\\hat{\\lambda}_{j}\\geq1/(N\\gamma)\\}+(N\\gamma)^{2}\\underset{\\Tilde{\\lambda}_{j}\\in\\mathcal{N}^{2}}{\\sum}\\underset{j}{\\sum_{j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $(\\tilde{\\lambda}_{j})_{j=1}^{M}$ are eigenvalue of SHS\u22a4. ", "page_idx": 32}, {"type": "text", "text": "See Section F.2.1 for the proof. ", "page_idx": 32}, {"type": "text", "text": "For $T_{i}(i=1,2,3,4)$ , we also have the following upper (and lower) bounds. ", "page_idx": 32}, {"type": "text", "text": "Lemma F.2 (Lower bound on $T_{1}$ ). Under the assumptions and notations in Theorem F.1, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}T_{1}\\gtrsim\\sum_{i:\\tilde{\\lambda}_{i}<1/(\\gamma N)}\\frac{\\mu_{i}(\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top})}{\\mu_{i}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "almost surely, where $(\\tilde{\\lambda}_{i})_{i=1}^{N}$ are eigenvalues of $\\mathbf{SHS}^{\\top}$ in non-increasing order. ", "page_idx": 32}, {"type": "text", "text": "See the proof in Section F.2.2. ", "page_idx": 32}, {"type": "text", "text": "Lemma F.3 (Upper bound on $T_{2}$ ). Under the assumptions and notations in Theorem F.1, for any $k\\le M/3$ such that $r(\\mathbf{H})\\geq k+M$ , we have with probability at least $1-e^{-\\Omega(M)}$ that ", "page_idx": 32}, {"type": "equation", "text": "$$\nT_{2}\\lesssim\\frac{1}{N\\gamma}\\Big[\\frac{\\mu_{M/2}(\\mathbf{A}_{k})}{\\mu_{M}(\\mathbf{A}_{k})}\\Big]^{2}\\cdot\\|\\mathbf{w}_{0:k}^{*}\\|^{2}+\\|\\mathbf{w}_{k:\\infty}^{*}\\|_{\\mathbf{H}_{k:\\infty}}^{2},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\mathbf{A}_{k}:=\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}$ ", "page_idx": 32}, {"type": "text", "text": "See the proof in Section F.2.3. ", "page_idx": 33}, {"type": "text", "text": "Lemma F.4 (Lower bound on $T_{3}$ ). Under the assumptions and notations in Theorem $F.l$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}T_{3}\\gtrsim\\frac{D_{\\mathrm{eff},N}}{N}\\cdot\\sum_{i:\\tilde{\\lambda}_{i}<1/(\\gamma N)}\\frac{\\mu_{i}(\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top})}{\\mu_{i}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "almost surely, where $(\\tilde{\\lambda}_{i})_{i=1}^{M}$ are eigenvalues of SHS\u22a4in non-increasing order. ", "page_idx": 33}, {"type": "text", "text": "See the proof in Section F.2.4. ", "page_idx": 33}, {"type": "text", "text": "Lemma F.5 (Upper bound on $T_{4}$ ). Under the assumptions and notations in Theorem F.1 and assume $r(\\mathbf{H})\\geq M$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\nT_{4}\\lesssim\\lvert|\\mathbf{w}^{*}\\rvert|_{\\mathbf{H}}^{2}\\cdot\\frac{D_{\\mathrm{eff},N}}{N}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "almost surely, where $\\mathbf{A}_{k}:=\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}$ . ", "page_idx": 33}, {"type": "text", "text": "See the proof in Section F.2.5. ", "page_idx": 33}, {"type": "text", "text": "With these results at hand, we are ready to derive upper and lower bounds for the risk of the average of (SGD) iterates. ", "page_idx": 33}, {"type": "text", "text": "F.1 Matching bounds for the average of (SGD) iterates under power-law spectrum ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we derive upper and lower bounds for the expected risk under the power-law spectrum. Our main result (Theorem F.6) follows directly from Theorem F.1 and the bounds on $T_{i}(i=\\bar{1},2,3,4)$ in Lemmas F.2 to F.5. ", "page_idx": 33}, {"type": "text", "text": "Theorem F.6 (Scaling law for average iterates of SGD). Suppose Assumption $^{l}$ and 2 hold and $\\sigma^{2}\\lesssim1$ . Then there exists some $a$ -dependent constant $c>0$ such that when $\\gamma\\leq c,$ , with probability at least $1-e^{-\\Omega(M)}$ over the randomness of the sketch matrix S, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\mathcal{R}_{M}(\\bar{\\mathbf{v}}_{N})=\\sigma^{2}+\\Theta\\big(M^{1-a}\\big)+\\Theta\\big((N\\gamma)^{1/a-1}\\big),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the expectation is over the randomness of $\\mathbf{w}^{*}$ and $(\\mathbf{x}_{i},y_{i})_{i=1}^{N}$ , and $\\Theta(\\cdot)$ hides constants that may depend on $a$ . ", "page_idx": 33}, {"type": "text", "text": "See the proof in Section F.2.6. ", "page_idx": 33}, {"type": "text", "text": "Compared with Theorem 4.1, Theorem F.6 suggests that the average of (SGD) achieves a smaller risk in the sketched linear model\u2014the $(N_{\\mathsf{e f f}}\\gamma)^{\\bar{1/}a}$ is replaced by $(\\bar{N}\\gamma)^{1/a}$ in the bound for the bias term. This is intuitive since the sum of stepsizes $\\begin{array}{r}{\\sum_{t}\\gamma_{t}\\approx N_{\\mathrm{eff}}\\gamma}\\end{array}$ for the geometrically decaying stepsize scheduler while $\\textstyle\\sum_{t}\\gamma_{t}\\approx N\\gamma$ for the fixed stepsize scheduler. ", "page_idx": 33}, {"type": "text", "text": "We also verify the observations in Theorem F.6 via simulations. We adopt the same model and setup as in Section 5 but use the average of iterates of fixed stepsize (SGD) (denoted by $\\bar{\\mathbf{v}}_{N}$ ) as the predictor. From Figure 3 and 4 we see that the expected risk $\\mathbb{E}\\mathcal{R}(\\bar{\\mathbf{v}}_{N})$ also scales following a power-law relation in both sample size $N$ and model size $M$ . Moreover, the fitted exponents match our theoretical predictions in Theorem F.6. ", "page_idx": 33}, {"type": "text", "text": "F.2 Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "F.2.1 Proof of Theorem F.1 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Similar to the proof of Theorem A.4, we have the decomposition ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\bar{\\mathbf{v}}_{N})=\\sigma^{2}+\\mathsf{A p p r o x}+\\|\\bar{\\mathbf{v}}_{N}-\\mathbf{v}^{*}\\|_{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}}^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that $(\\mathbf{v}_{t})_{t=1}^{N}$ can also be viewed as the SGD iterates on the model $y=\\langle\\mathbf{S}\\mathbf{x},\\mathbf{v}^{*}\\rangle+\\xi+\\epsilon$ , where the noise satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}(\\xi+\\epsilon)^{2}=\\mathcal{R}(\\mathbf{v}^{*})=\\mathbb{E}\\xi^{2}+\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, the upper and lower bounds on Bias, Var follow directly from the proof of Theorem 2.1, 2.2 and related lemmas (Lemma B.6, B.11, C.3, C.5) in [48]. ", "page_idx": 33}, {"type": "image", "img_path": "PH7sdEanXP/tmp/84befa78c56c5b566ae94c7d40ca35c810b770d32d54c68a43a2b0e9fe1f0fbb.jpg", "img_caption": ["Figure 3: The expected risk (Risk) of the average of iterates of (SGD) versus the sample size $N$ and the model size $M$ for different power-law degrees $a$ . The expected risk is computed by averaging over 1000 independent samples of $(\\mathbf{w}^{*},\\mathbf{S})$ . We fit the expected risk using the formula Risk $\\stackrel{>}{\\sim}\\bar{\\sigma^{2}}\\!+\\!c_{1}/M^{a_{1}}\\!+\\!c_{2}\\bar{/}N^{a_{2}}$ via minimizing the Huber loss as in [21]. Parameters: $\\sigma=1,\\gamma=0.1$ . Left: For $a=1.5$ , $d=20000$ , the fitted exponents are $(a_{1},a_{2})=(0.59,0.33)\\approx(0.5,0.33)$ . Right: For $a=2$ , $d=2000$ , the fitted exponents are $(a_{1},a_{2})=(1.09,0.49)\\approx(1.0,0.5)$ . Note that the values of $(a_{1},a_{2})$ are close to our theoretical predictions $(a-1,1-1/a)$ in both cases. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "PH7sdEanXP/tmp/110893056ce3dc90f9b2f40e3af0cf423c691df964cf7b5207f2a5d0373a1760.jpg", "img_caption": ["Figure 4: The expected risk of the average of iterates of (SGD) minus the irreducible risk versus the effective sample size and model size. Parameters $\\sigma=1,\\gamma=0.1$ . (a), (b): $a=1.5$ , $d=10000$ ; (c), (d): $a=2$ , $d=1000$ . The errobars denotes the $\\pm1$ standard deviation of estimating the expected risk using 100 independent samples of $\\left(\\mathbf{w}^{*},\\mathbf{S}\\right)$ . We use linear functions to fti the expected risk under log-log scale and report the slope of the fitted lines (denoted by $k$ ). "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "F.2.2 Proof of Lemma F.2 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Let $f_{1}(A)\\;:=\\;(\\mathbf{I}\\mathrm{~-~}(\\mathbf{I}\\mathrm{~-~}\\gamma A)^{N/4})^{2}A^{-1}/\\gamma^{2}/N^{2}$ for any positive definite matrix $A~\\in~\\mathbb{R}^{M\\times M}$ . Since $\\gamma\\,\\leq\\,1/(c\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}))$ , we have $f_{1}(\\mathbf{SHS}^{\\top})\\succeq\\mathbf{0}$ . By definition of $T_{1}$ and recalling $\\mathbf{v}^{*}=$ $(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}$ , we have with probability at least $1-e^{-\\Omega(M)}$ that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{w}^{*}}T_{1}=\\mathbb{E}_{\\mathbf{w}^{*}}[\\mathbf{w}^{*\\top}\\mathbf{H}\\mathbf{S}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}f_{1}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}]}\\\\ &{\\quad\\quad\\quad=\\mathrm{tr}\\left([(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}f_{1}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}](\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Following the proof of Lemma D.2 (by Von Neumann\u2019s trace inequality), we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{\\mathbf{w}^{*}}T_{1}\\geq\\sum_{i=1}^{M}\\frac{\\mu_{i}\\left(\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top}\\right)}{\\mu_{i}\\left((\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{2}f_{1}\\left(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\right)^{-1}\\right)}}\\quad}&{}&\\\\ &{\\geq\\sum_{i:\\bar{\\lambda}_{i}<1/(\\gamma_{N})}\\frac{\\mu_{i}\\left(\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top}\\right)}{\\mu_{i}\\left((\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{2}f_{1}\\left(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\right)^{-1}\\right)}}\\\\ &{\\gtrsim\\sum_{i:\\bar{\\lambda}_{i}<1/(\\gamma_{N})}\\frac{\\mu_{i}\\left(\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top}\\right)}{\\mu_{i}\\left(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the third inequality is due to ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\lambda/f_{1}(\\lambda)\\lesssim\\frac{\\lambda^{2}\\gamma^{2}N^{2}}{(1-(1-\\gamma\\lambda)^{N/4})^{2}}\\lesssim\\frac{N^{2}}{(\\sum_{i=0}^{N/4-1}(1-\\gamma\\lambda)^{i})^{2}}\\lesssim\\frac{1}{(1-\\gamma\\lambda)^{2N}}\\lesssim1\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "when $\\lambda<1/(N\\gamma)$ . ", "page_idx": 35}, {"type": "text", "text": "F.2.3 Proof of Lemma F.3 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "By definition of $T_{2}$ , the fact that $\\begin{array}{r}{1-x^{N}=(1-x)\\sum_{i=0}^{N-1}x^{i}}\\end{array}$ , and recalling $\\mathbf{v}^{*}=(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{\\Xi}}_{2}^{r}=\\mathbf{w}^{*}\\bar{\\mathbf{\\Xi}}^{\\top}\\mathbf{H}\\mathbf{S}^{\\top}f_{2}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*},}\\\\ &{\\quad\\leq2\\bigl[\\mathbf{w}_{0:k}^{*}\\bar{\\mathbf{\\Xi}}^{\\top}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}f_{2}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{w}_{0:k}^{*}+\\underbrace{\\mathbf{w}_{k:\\infty}^{*}\\mathbf{\\Xi}^{\\top}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}f_{2}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{w}_{k:\\infty}^{*}}_{T_{22}}\\bigr],}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\begin{array}{r}{f_{2}(A):=[\\sum_{i=0}^{N-1}(\\mathbf I-\\gamma A)^{i}]^{2}/A/N^{2}}\\end{array}$ for any symmetric matrix $A\\in\\mathbb{R}^{M\\times M}$ . Moreover, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{21}=\\mathbf{w}_{0:k}^{*}\\mathbf{\\Gamma}^{\\top}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}f_{2}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{w}_{0:k}^{*}}\\\\ {\\leq\\lVert f_{2}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{2}\\rVert\\cdot\\lVert(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{w}_{0:k}^{*}\\rVert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using the assumption on the stepsize that $\\gamma\\leq1/(c\\operatorname{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}))$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|f_{2}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{2}\\|\\leq\\displaystyle\\operatorname*{max}_{\\lambda\\in[0,1/\\gamma]}\\displaystyle\\frac{1}{N^{2}}\\Big[\\displaystyle\\sum_{i=0}^{N-1}(1-\\gamma\\lambda)^{i}\\Big]^{2}\\lambda}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\operatorname*{max}_{\\lambda\\in[0,1/\\gamma]}\\displaystyle\\frac{1}{N^{2}\\gamma}\\Big[\\displaystyle\\sum_{i=0}^{N-1}(1-\\gamma\\lambda)^{i}\\Big]\\cdot(1-(1-\\gamma\\lambda)^{N})}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{1}{N^{2}\\gamma}\\cdot N\\cdot1=\\displaystyle\\frac{1}{N\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Combining Eq. (28) with Eq. (23) in the proof of Lemma D.1 (note that we assume $k\\leq M/3)$ , we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\nT_{21}\\leq c\\frac{1}{N\\gamma}\\Big[\\frac{\\mu_{M/2}(\\mathbf{A}_{k})}{\\mu_{M}(\\mathbf{A}_{k})}\\Big]^{2}\\cdot\\|\\mathbf{w}_{0:k}^{*}\\|^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for some constant $c>0$ with probability at least $1-e^{-\\Omega(M)}$ . For $T_{22}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{22}=\\mathbf{w}_{k:\\infty}^{*}{^{\\top}\\mathbf{H}_{k:\\infty}}\\mathbf{S}_{k:\\infty}^{\\top}f_{2}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{w}_{k:\\infty}^{*}}\\\\ &{\\quad\\quad\\leq\\|f_{2}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\|\\cdot\\|(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1/2}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2})\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{w}_{k:\\infty}^{*}\\|^{2}}\\\\ &{\\quad\\quad\\leq\\|f_{2}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\|\\cdot\\|(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1/2}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}\\|^{2}\\cdot\\|\\mathbf{w}_{k:\\infty}^{*}\\|_{\\mathbf{H}_{k:\\infty}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since $\\begin{array}{r l r}{\\|f_{2}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\|}&{\\!=\\!}&{\\|[\\sum_{i=0}^{N-1}(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{i}]^{2}/N^{2}\\|\\mathbf{\\Omega}\\leq\\mathbf{\\Omega}1}\\end{array}$ by the assumption $\\gamma\\ \\leq$ $1/(c\\,\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}))$ , and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1/2}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}\\|^{2}=\\|\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\|\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}+\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\|\\mathbf{H}_{k:\\infty}^{1/2}\\mathbf{S}_{k:\\infty}^{\\top}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})^{-1}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{1/2}\\|=1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "it follows that $T_{22}\\leq\\|\\mathbf{w}_{k:\\infty}^{*}\\|_{\\mathbf{H}_{k:\\infty}}^{2}$ . Combining the bounds on $T_{21},T_{22}$ completes the proof. ", "page_idx": 35}, {"type": "text", "text": "F.2.4 Proof of Lemma F.4 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Let $f_{3}(A):=(\\mathbf{I}-(\\mathbf{I}-\\gamma\\mathbf{A})^{N/4})/\\gamma/N^{2}$ for any positive definite matrix $A\\in\\mathbb{R}^{M\\times M}$ . Following the same arguments as in the proof of Lemma F.2, we have $f_{3}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\succeq\\mathbf{0}$ and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\Big[\\frac{1}{\\gamma N^{2}}\\,\\mathrm{tr}\\left(\\Big(\\mathbf{I}-(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{N/4}\\Big)\\mathbf{B}_{0}\\right)\\Big]\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{\\mathbf{w}^{*}}[{\\mathbf{w}^{*}}^{\\top}\\mathbf{H}\\mathbf{S}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}f_{3}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}]}\\\\ &{=\\mathrm{tr}((\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}f_{3}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Moreover, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{\\mathbf{w}^{*}}T_{1}\\geq\\sum_{i=1}^{M}\\frac{\\mu_{i}\\left(\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top}\\right)}{\\mu_{i}\\left((\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{2}f_{3}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\right)}}\\quad}&{}&\\\\ &{\\geq\\sum_{i:\\tilde{\\lambda}_{i}<1/(\\gamma_{N})}\\frac{\\mu_{i}\\left(\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top}\\right)}{\\mu_{i}\\left((\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{2}f_{3}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\right)}}\\\\ &{}&{\\gtrsim\\frac{1}{N}\\sum_{i:\\tilde{\\lambda}_{i}<1/(\\gamma_{N})}\\frac{\\mu_{i}\\left(\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top}\\right)}{\\mu_{i}\\left(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the third inequality is due to ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\lambda/f_{3}(\\lambda)\\lesssim\\frac{\\lambda\\gamma N^{2}}{1-(1-\\gamma\\lambda)^{N/4}}\\lesssim\\frac{N^{2}}{\\sum_{i=0}^{N/4-1}(1-\\gamma\\lambda)^{i}}\\lesssim\\frac{N}{(1-\\gamma\\lambda)^{N}}\\lesssim N\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "when $\\lambda<1/(N\\gamma)$ . Note that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{1-\\big(1-\\gamma\\tilde{\\lambda}_{i}\\big)^{\\frac{N}{4}}\\geq\\left\\{1-\\big(1-\\frac{1}{N}\\big)^{\\frac{N}{4}}\\geq1-e^{-\\frac{1}{4}}\\geq\\frac{1}{5},\\qquad}&{\\tilde{\\lambda}_{i}\\geq\\frac{1}{\\gamma N},}\\\\ {\\frac{N}{4}\\cdot\\gamma\\tilde{\\lambda}_{i}-\\frac{N(N-4)}{32}\\cdot\\gamma^{2}\\tilde{\\lambda}_{i}^{2}\\geq\\frac{N}{5}\\cdot\\gamma\\tilde{\\lambda}_{i},}&{\\tilde{\\lambda}_{i}<\\frac{1}{\\gamma N},}\\end{array}\\right.}&{{}\\geq\\frac{1}{5}\\operatorname*{min}\\{N\\gamma\\tilde{\\lambda}_{i},1\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We thus have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{tr}\\left(\\big({\\bf I}-({\\bf I}-\\gamma{\\bf S}{\\bf H}{\\bf S}^{\\top})^{N/4}\\big)^{2}\\right)=\\displaystyle\\sum_{i=1}^{M}[1-(1-\\gamma\\tilde{\\lambda}_{i})^{\\frac{N}{4}}]^{2}\\gtrsim\\displaystyle\\sum_{i=1}^{M}\\operatorname*{min}\\{(N\\gamma\\tilde{\\lambda}_{i})^{2},1\\}}\\\\ {=\\#\\{\\tilde{\\lambda}_{i}\\geq\\displaystyle\\frac{1}{N\\gamma}\\}+N^{2}\\gamma^{2}\\sum_{\\tilde{\\lambda}_{i}<1/(N\\gamma)}\\tilde{\\lambda}_{i}^{2}=D_{\\mathrm{eff},N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining Eq. (31) and (29) completes the proof. ", "page_idx": 36}, {"type": "text", "text": "F.2.5 Proof of Lemma F.5 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Substituting $\\mathbf{v}^{*}=(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}$ in the expression of $T_{4}$ and noting $\\mathbf{v}_{0}=\\mathbf{0}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{4}=\\cfrac{1}{\\gamma N}\\operatorname{tr}\\big(\\mathbf{B}_{0}-(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{N}\\mathbf{B}_{0}(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{N}\\big)\\cdot\\cfrac{D_{\\mathrm{eff},N}}{N}}\\\\ &{\\quad=\\cfrac{1}{\\gamma N}\\operatorname{tr}\\big(\\mathbf{w}^{*\\,\\top}\\mathbf{H}\\mathbf{S}^{\\top}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\big[\\mathbf{I}-(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{2N}\\big](\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1}\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}\\big)\\cdot\\cfrac{D_{\\mathrm{eff},N}}{N}}\\\\ &{\\quad=:\\operatorname{tr}\\big(\\mathbf{w}^{*\\,\\top}\\mathbf{H}\\mathbf{S}^{\\top}f_{4}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}\\big)\\cdot\\cfrac{D_{\\mathrm{eff},N}}{N},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $f_{4}(A):=A^{-1}\\big[\\mathbf{I}\\!-(\\mathbf{I}\\!-\\!\\gamma A)^{2N}\\big]A^{-1}/(N\\gamma)$ for any symmetric matrix $A\\in\\mathbb{R}^{M\\times M}$ . Moreover, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{tr}\\left(\\mathbf{w}^{*\\top}\\mathbf{H}\\mathbf{S}^{\\top}f_{4}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}\\mathbf{H}\\mathbf{w}^{*}\\right)}\\\\ &{\\leq\\lVert f_{4}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\rVert\\cdot\\lVert(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{-1/2}\\mathbf{S}\\mathbf{H}^{1/2}\\rVert^{2}\\cdot\\lVert\\mathbf{w}_{*}\\rVert_{\\mathbf{H}}^{2}}\\\\ &{\\leq\\lVert f_{4}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\rVert\\cdot\\lVert\\mathbf{w}_{*}\\rVert_{\\mathbf{H}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|f_{4}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\|=\\frac{1}{N}\\|\\sum_{i=0}^{2N-1}(\\mathbf{I}-\\gamma\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})^{i}\\|\\leq2\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "by our assumption on the stepsize, it follows that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left(\\mathbf{w^{\\ast}}^{\\top}\\mathbf{H}\\mathbf{S}^{\\top}f_{4}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\mathbf{S}\\mathbf{H}\\mathbf{w^{\\ast}}\\right)\\lesssim\\|\\mathbf{w}_{*}\\|_{\\mathbf{H}}^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining Eq. (32) and (33) we find ", "page_idx": 36}, {"type": "equation", "text": "$$\nT_{4}\\lesssim\\lvert\\lvert\\mathbf{w}_{*}\\rvert\\rvert_{\\mathbf{H}}^{2}\\cdot\\frac{D_{\\mathrm{eff},N}}{N}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "F.2.6 Proof of Theorem F.6 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "First, by Lemma G.4 we have $1/\\operatorname{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\gtrsim c_{1}$ for some $a$ -dependent $c_{1}>0$ with probability at least $1-e^{-\\Omega(M)}$ . Therefore we may choose $c$ sufficiently small so that $\\gamma\\leq c$ implies $\\gamma\\lesssim$ $1/\\operatorname{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})$ with probability at least $\\mathrm{i}-e^{-\\Omega(M)}$ . Now, suppose we have $\\gamma\\lesssim1/\\mathrm{tr}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})$ . Following the notations in Theorem F.1, we claim the following bounds on Approx, Bias, Var: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E A}\\mathsf{p p r o x}\\gtrsim M^{1-a}}\\\\ &{\\qquad\\mathsf{V a r}\\approx\\operatorname*{min}\\big\\{M,\\,(N\\gamma)^{1/a}\\big\\}/N.}\\\\ &{\\qquad\\mathsf{B i a s}\\lesssim\\operatorname*{max}\\big\\{M^{1-a},\\,(N\\gamma)^{1/a-1}\\big\\},}\\\\ &{\\qquad\\mathsf{B i a s}\\gtrsim(N\\gamma)^{1/a-1}\\,\\mathrm{when}\\,\\,(N\\gamma)^{1/a}\\leq}\\end{array}\n$$$(N\\gamma)^{1/a}\\leq M/c$ ", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . Putting the bounds together yields Theorem F.6. ", "page_idx": 37}, {"type": "text", "text": "Proof of claim (34a) Note that our definition of Approx in Thereom F.1 is the same as that in Eq. (4) (and 7). Therefore the claim follows immediately from Lemma C.4. ", "page_idx": 37}, {"type": "text", "text": "Proof of claim (34b) This follows from the proof of Lemma E.1 with $N_{\\mathsf{e f f}}$ replaced by $N$ . ", "page_idx": 37}, {"type": "text", "text": "Proof of claim (34c) By Theorem F.1, Lemma F.3 and F.5, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathtt{B i a s}\\lesssim\\mathbb{E}_{\\mathbf{w}^{*}}\\frac{\\|\\mathbf{w}_{0:k_{2}}^{*}\\|_{2}^{2}}{N\\gamma}\\cdot\\left[\\frac{\\mu_{M/2}\\left(\\mathbf{S}_{k_{2}:\\infty}\\mathbf{H}_{k_{2}:\\infty}\\mathbf{S}_{k_{2}:\\infty}^{\\top}\\right)}{\\mu_{M}\\left(\\mathbf{S}_{k_{2}:\\infty}\\mathbf{H}_{k_{2}:\\infty}\\mathbf{S}_{k_{2}:\\infty}^{\\top}\\right)}\\right]^{2}+\\mathbb{E}_{\\mathbf{w}^{*}}\\|\\mathbf{w}_{k_{2}:\\infty}^{*}\\|_{\\mathbf{H}_{k_{2}:\\infty}}^{2}+\\sigma^{2}\\frac{D_{\\mathrm{eff},N}}{N},}\\\\ &{\\qquad\\lesssim\\frac{k_{2}}{N\\gamma}\\Bigg[\\frac{\\mu_{M/2}\\left(\\mathbf{S}_{k_{2}:\\infty}\\mathbf{H}_{k_{2}:\\infty}\\mathbf{S}_{k_{2}:\\infty}^{\\top}\\right)}{\\mu_{M}\\left(\\mathbf{S}_{k_{2}:\\infty}\\mathbf{H}_{k_{2}:\\infty}\\mathbf{S}_{k_{2}:\\infty}^{\\top}\\right)}\\Bigg]^{2}+k_{2}^{1-a}+\\frac{D_{\\mathrm{eff},N}}{N}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ for any $k_{2}\\leq M/3$ . Choosing $k_{2}=\\operatorname*{min}\\{M/3,(N\\gamma)^{1/a}\\}$ and using Lemma G.4 and claim (34b), we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{l i s}\\lesssim\\operatorname*{max}\\left\\{M^{1-a},\\,(N\\gamma)^{1/a-1}\\right\\}+\\frac{\\operatorname*{min}\\left\\{M,\\,(N\\gamma)^{1/a}\\right\\}}{N}\\lesssim\\operatorname*{max}\\left\\{M^{1-a},\\,(N\\gamma)^{1/a-1}\\right\\}+(N\\gamma)^{1/a-1}}\\\\ &{\\quad\\lesssim\\operatorname*{max}\\left\\{M^{1-a},\\,(N\\gamma)^{1/a-1}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . ", "page_idx": 37}, {"type": "text", "text": "Proof of claim (34d) By Theorem F.1 and Lemma F.2, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{B i a s}\\gtrsim\\sum_{i:\\tilde{\\lambda}_{i}<1/(\\gamma N)}\\frac{\\mu_{i}(\\mathbf{SH}^{2}\\mathbf{S}^{\\top})}{\\mu_{i}(\\mathbf{SHS}^{\\top})}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "When $(N\\gamma)^{1/a}\\leq M/c$ for some large constant $c>0$ , we have from Lemma G.4 that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}^{*}}\\mathsf{B i a s}\\gtrsim\\sum_{\\substack{i:\\tilde{\\lambda}_{i}<1/(\\gamma N)}}\\frac{i^{-2a}}{i^{-a}}=\\sum_{\\substack{i:\\tilde{\\lambda}_{i}<1/(\\gamma N)}}i^{-a}\\gtrsim[(N\\gamma)^{1/a}]^{1-a}=(N\\gamma)^{1/a-1}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . ", "page_idx": 37}, {"type": "text", "text": "G Concentration lemmas ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "G.1 General concentration results ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Lemma G.1. Suppose that $\\mathbf{S}\\in\\mathbb{R}^{M\\times d}$ is such that 3 ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbf{S}_{i j}\\sim\\mathcal{N}(0,1/M).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let $(\\lambda_{i})_{i\\geq1}$ be the eigenvalues of $\\mathbf{H}$ in non-increasing order. Let $(\\tilde{\\lambda}_{i})_{i=1}^{M}$ be the eigenvalues of SHS\u22a4 in non-increasing order. Then there exists a constant $c>1$ such that for every $M\\geq0$ and every $0\\leq k\\leq M$ , with probability $\\geq1-e^{-\\Omega(M)}$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nf o r\\,e v e r y\\,j\\le M,\\quad\\left|\\tilde{\\lambda}_{j}-\\left(\\lambda_{j}+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\right)\\right|\\le c\\cdot\\left(\\sqrt{\\frac{k}{M}}\\cdot\\lambda_{j}+\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "As a direct consequence, for $k\\leq M/c^{2}$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nf o r\\;e v e r y\\;j\\leq M,\\quad\\left|\\tilde{\\lambda}_{j}-\\left(\\lambda_{j}+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\right)\\right|\\leq\\frac{1}{2}\\cdot\\left(\\lambda_{j}+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\right)+c_{1}\\cdot\\lambda_{k+1},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $c_{1}=c+2c^{2}$ . ", "page_idx": 38}, {"type": "text", "text": "Proof of Lemma G.1. We have the following decomposition motivated by Swartworth and Woodruff [38] (see their Section 3.4, Proof of Theorem 1). ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}=\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}+\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}}\\\\ &{\\qquad\\quad=\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}+\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}-\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We remark that this decomposition idea has been implicitly used in Bartlett et al. [5] to control the eigenvalues of a Gram matrix. In fact, we will use techniques from Bartlett et al. [5] to obtain a sharper bound than that presented in Swartworth and Woodruff [38]. ", "page_idx": 38}, {"type": "text", "text": "For the upper bound, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{j}\\left(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\right)\\leq\\mu_{j}\\left(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}\\right)+\\left\\|\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}-\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}\\right\\|_{2}}\\\\ &{\\qquad\\qquad=\\mu_{j}\\left(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}\\right)+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}+\\left\\|\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}-\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\mu_{j}\\left(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}\\right)+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}+c_{1}\\cdot\\left(\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last inequality is by Lemma G.2. For $j\\leq k$ , using Lemma G.3, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mu_{j}\\bigl(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}\\bigr)\\leq\\lambda_{j}+c_{2}\\cdot\\sqrt{\\frac{k}{M}}\\cdot\\lambda_{j}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For $k<j\\le M$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mu_{j}\\bigl(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}\\bigr)=0\\leq\\lambda_{j}+c_{2}\\cdot\\sqrt{\\frac{k}{M}}\\cdot\\lambda_{j}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Putting these together, we have the following for every $j=1,\\dots,M$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{j}\\bigl(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\bigr)\\leq\\mu_{j}\\bigl(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}\\bigr)+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}+c_{1}\\cdot\\Biggl(\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\Biggr)}\\\\ &{\\qquad\\qquad\\leq\\lambda_{j}+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}+c\\cdot\\Biggl(\\sqrt{\\frac{k}{M}}\\cdot\\lambda_{j}+\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\Biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Similarly, we can show the lower bound. By the decomposition, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{j}\\left(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\right)\\geq\\mu_{j}\\left(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}\\right)-\\left\\|\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}-\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}\\right\\|}\\\\ {=\\mu_{j}\\left(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}\\right)+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}-\\left\\|\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}-\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\geq\\mu_{j}\\big(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}\\big)+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}-c_{1}\\cdot\\left(\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the last inequality is by Lemma G.2. For $j\\leq k$ , using Lemma G.3, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mu_{j}\\bigl(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}\\bigr)\\geq\\lambda_{j}-c_{2}\\cdot\\sqrt{\\frac{k}{M}}\\cdot\\lambda_{j}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For $k<j\\le M$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mu_{j}\\bigl(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}\\bigr)=0\\geq\\lambda_{j}-\\lambda_{k+1}-c_{2}\\cdot\\sqrt{\\frac{k}{M}}\\cdot\\lambda_{j},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the last inequality is due to $\\lambda_{j}\\le\\lambda_{k}$ for $j\\geq k$ . Putting these together, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{j}\\bigl(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}\\bigr)\\geq\\mu_{j}\\bigl(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}\\bigr)+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}-c_{1}\\cdot\\Biggl(\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\Biggr)}\\\\ &{\\qquad\\qquad\\geq\\lambda_{j}+\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}-c\\cdot\\Biggl(\\sqrt{\\frac{k}{M}}\\cdot\\lambda_{j}+\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\Biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "So far, we have proved the first claim. To show the second claim, we simply apply ", "page_idx": 39}, {"type": "equation", "text": "$$\nc\\cdot{\\sqrt{\\frac{k}{M}}}\\leq{\\frac{1}{2}}\\quad{\\mathrm{for}}\\;k\\leq M/c^{2},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{c\\cdot\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}\\le c\\cdot\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}}{M}}\\cdot\\lambda_{k+1}}}\\\\ &{}&{\\le\\frac{1}{2}\\cdot\\frac{\\sum_{i>k}\\lambda_{i}}{M}+2c^{2}\\cdot\\lambda_{k+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "in the first claim. ", "page_idx": 39}, {"type": "text", "text": "Lemma G.2 (Tail concentration, Lemma 26 in Bartlett et al. [5]). For any $k\\geq1$ , with probability at least $1-e^{-\\dot{\\Omega}(M)}$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\biggl\\|\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}-\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}\\biggr\\|_{2}\\lesssim\\lambda_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Moreover, the minimum eigenvalue of $\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}$ satisfies ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mu_{\\mathrm{min}}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})\\gtrsim\\lambda_{k+2M}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . ", "page_idx": 39}, {"type": "text", "text": "Proof of Lemma G.2. The first part of Lemma G.2 is a version of Lemma 26 in [5] (see their proof).   \nWe provide proof here for completeness. ", "page_idx": 39}, {"type": "text", "text": "We write $\\mathbf{S}\\in\\mathbb{R}^{M\\times p}$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbf{S}=(\\mathbf{s}_{1}\\quad\\dots\\quad\\mathbf{s}_{p})\\,,\\quad\\mathbf{s}_{i}\\sim{\\mathcal{N}}\\biggl(0,\\,\\frac{1}{M}\\cdot\\mathbf{I}_{M}\\biggr),\\quad i\\geq1.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since Gaussian distribution is rotational invariance, without loss of generality, we may assume ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbf{H}=\\mathrm{diag}\\{\\lambda_{1},\\ldots,\\lambda_{p}\\}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}=\\sum_{i>k}\\lambda_{i}\\mathbf{s}_{i}\\mathbf{s}_{i}^{\\top}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Fixing a unit vector $\\mathbf{v}\\in\\mathbb{R}^{M}$ , then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbf{v}^{\\top}\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}\\mathbf{v}=\\sum_{i>k}\\lambda_{i}\\big(\\mathbf{s}_{i}^{\\top}\\mathbf{v}\\big)^{2},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where each $\\mathbf{s}_{i}^{\\top}\\mathbf{v}$ is $(1/M)$ -subGaussian. By Bernstein\u2019s inequality, we have, with probability $\\geq1-\\delta$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\bigg|\\sum_{i>k}\\boldsymbol{\\lambda}_{i}\\big(\\mathbf{s}_{i}^{\\top}\\mathbf{v}\\big)^{2}-\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\bigg|\\lesssim\\frac{1}{M}\\cdot\\bigg(\\boldsymbol{\\lambda}_{k+1}\\cdot\\log\\frac{1}{\\delta}+\\sqrt{\\sum_{i>k}\\boldsymbol{\\lambda}_{i}^{2}\\cdot\\log\\frac{1}{\\delta}}\\bigg).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By a union bound and net argument on $S^{M-1}$ , we have, with probability $\\geq1-\\delta$ , for every unit vector v \u2208RM, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\bigg|\\sum_{i>k}\\lambda_{i}\\big(\\mathbf{s}_{i}^{\\top}\\mathbf{v}\\big)^{2}-\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\bigg|\\lesssim\\frac{1}{M}\\cdot\\bigg(\\lambda_{k+1}\\cdot\\bigg(M+\\log\\frac{1}{\\delta}\\bigg)+\\sqrt{\\sum_{i>k}\\lambda_{i}^{2}\\cdot\\bigg(M+\\log\\frac{1}{\\delta}\\bigg)}\\bigg).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "So with probability at least $1-e^{-\\Omega(M)}$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|{\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}-\\frac{\\sum_{i>k}\\lambda_{i}}{M}\\cdot\\mathbf{I}_{M}}\\right\\|_{2}\\lesssim\\frac{1}{M}\\cdot\\left(\\lambda_{k+1}\\cdot M+\\sqrt{\\sum_{i>k}\\lambda_{i}^{2}\\cdot M}\\right)}&{}\\\\ {\\ }&{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\breve{\\lambda}_{k+1}+\\sqrt{\\frac{\\sum_{i>k}\\lambda_{i}^{2}}{M}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which completes the proof of the first part of Lemma G.2. ", "page_idx": 40}, {"type": "text", "text": "To prove the second part of Lemma G.2, it suffices to note that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}\\succeq\\sum_{i=k+1}^{2M+k}\\lambda_{i}\\mathbf{s}_{i}\\mathbf{s}_{i}^{\\top}\\succeq\\lambda_{2M+k}\\cdot\\sum_{i=k+1}^{2M+k}\\mathbf{s}_{i}\\mathbf{s}_{i}^{\\top}\\succeq c\\lambda_{2M+k}\\cdot\\mathbf{I}_{M}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for some constant $c>1$ with probability at least $1-e^{-\\Omega(M)}$ , where the last line follows from concentration properties of Gaussian covariance matrices (see e.g., Thereom 6.1 [41]). \u53e3 ", "page_idx": 40}, {"type": "text", "text": "Lemma G.3 (Head concentration). With probability at least $1-e^{-\\Omega(M)}$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\nf o r\\,e\\nu e r y\\;j\\leq k,\\quad|\\mu_{j}(\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top})-\\lambda_{j}|\\lesssim\\sqrt{\\frac{k}{M}}\\cdot\\lambda_{j}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof of Lemma G.3. Note that the spectrum of $\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}\\mathbf{S}_{0:k}^{\\top}$ is indentical to the spectrum of $\\mathbf{H}_{0:k}^{1/2}\\mathbf{S}_{0:k}^{\\top}\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}^{1/2}$ . \u22a4We wilkl \u00d7bMou nd the latter. We start with bounding the spectrum of $\\mathbf{S}_{0:k}\\mathbf{S}_{0:k}^{\\top}$ . To this end, we write as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbf{S}_{0:k}^{\\top}=\\left(\\mathbf{s}_{1}\\quad\\cdot\\,\\cdot\\quad\\mathbf{s}_{M}\\right),\\quad\\mathbf{s}_{i}\\sim\\mathcal{N}\\Bigg(0,\\ \\frac{1}{M}\\cdot\\mathbf{I}_{k}\\Bigg),\\quad i=1,\\ldots,M.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then repeating the argument in Lemma G.2, we have, with probability $\\geq1-\\delta$ , for every unit vector ${\\bf v}\\in\\mathbb{R}^{k}$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbf{v}^{\\top}\\mathbf{S}_{0:k}^{\\top}\\mathbf{S}_{0:k}\\mathbf{v}-1\\right|=\\left|\\displaystyle\\sum_{i=1}^{M}\\left(\\mathbf{s}_{i}^{\\top}\\mathbf{v}\\right)^{2}-1\\right|}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\displaystyle\\frac{1}{M}\\cdot\\left(1\\cdot\\left(k+\\log\\frac{1}{\\delta}\\right)+\\sqrt{M\\cdot\\left(k+\\log\\frac{1}{\\delta}\\right)}\\right)}\\\\ &{\\qquad\\qquad\\lesssim\\sqrt{\\frac{k+\\log(1/\\delta))}{M}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "So we have, with probability $\\geq1-e^{-\\Omega(M)}$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{S}_{0:k}^{\\top}\\mathbf{S}_{0:k}-\\mathbf{I}_{k}\\right\\|_{2}\\lesssim\\sqrt{\\frac{k}{M}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This implies that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{j}\\bigl(\\mathbf{H}_{0:k}^{1/2}\\mathbf{S}_{0:k}^{\\top}\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}^{1/2}\\bigr)\\leq\\mu_{j}\\bigl(\\mathbf{H}_{0:k}^{1/2}\\mathbf{H}_{0:k}^{1/2}\\bigr)+c_{1}\\cdot\\sqrt{\\frac{k}{M}}\\cdot\\mu_{j}\\bigl(\\mathbf{H}_{0:k}^{1/2}\\mathbf{H}_{0:k}^{1/2}\\bigr)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\lambda_{j}+c_{1}\\cdot\\sqrt{\\frac{k}{M}}\\cdot\\lambda_{j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{j}\\left(\\mathbf{H}_{0:k}^{1/2}\\mathbf{S}_{0:k}^{\\top}\\mathbf{S}_{0:k}\\mathbf{H}_{0:k}^{1/2}\\right)\\geq\\mu_{j}\\left(\\mathbf{H}_{0:k}^{1/2}\\mathbf{H}_{0:k}^{1/2}\\right)-c_{1}\\cdot\\sqrt{\\frac{k}{M}}\\cdot\\mu_{j}\\left(\\mathbf{H}_{0:k}^{1/2}\\mathbf{H}_{0:k}^{1/2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\lambda_{j}-c_{1}\\cdot\\sqrt{\\frac{k}{M}}\\cdot\\lambda_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We have completed the proof. ", "page_idx": 41}, {"type": "text", "text": "G.2 Concentration results under power-law spectrum ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Lemma G.4 (Eigenvalues of $\\mathbf{SHS}^{\\top}$ under power-law spectrum). Suppose Assumption 2 hold. There exist $a$ -dependent constants $c_{2}>c_{1}>0$ such that ", "page_idx": 41}, {"type": "equation", "text": "$$\nc_{1}j^{-a}\\leq\\mu_{j}(\\mathbf{SHS}^{\\top})\\leq c_{2}j^{-a}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . ", "page_idx": 41}, {"type": "text", "text": "Proof of Lemma G.4. Let $(\\tilde{\\lambda}_{i})_{i=1}^{M}$ denote the eigenvalues of $\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top}$ in an non-increasing order. Using Lemma G.1 with $k=M/c$ for some sufficiently large constant $c$ and noting that $\\begin{array}{r}{\\sum_{i>k}i^{-a}\\approx k^{1-\\tilde{a}}}\\end{array}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\cdot(j^{-a}+\\tilde{c}_{1}M^{-a})-\\tilde{c}_{2}\\cdot M^{-a}\\leq\\tilde{\\lambda}_{j}\\leq\\frac{3}{2}\\cdot(j^{-a}+\\tilde{c}_{1}M^{-a})+\\tilde{c}_{2}\\cdot M^{-a}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for every $j\\in[M]$ for some constants $\\tilde{c}_{i},i\\in[2]$ with probability at least $1-e^{-\\Omega(M)}$ . Therefore, for all $j\\le M/\\tilde{c}$ for some sufficiently large constant $\\tilde{c}>1$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{j}\\in[\\tilde{c}_{3}j^{-a},\\tilde{c}_{4}j^{-a}]\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ for some constants $\\tilde{c}_{3},\\tilde{c}_{4}>0$ . For $j\\in[M/\\tilde{c},M]$ , by monotonicity of the eigenvalues, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{j}\\leq\\tilde{\\lambda}_{\\lfloor M/\\tilde{c}\\rfloor}\\leq\\tilde{c}_{4}\\Big(\\Big\\lfloor\\frac{M}{\\tilde{c}}\\Big\\rfloor\\Big)^{-a}\\leq\\tilde{c}_{5}M^{-a}\\leq\\tilde{c}_{5}j^{-a}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for some sufficiently large constant $\\tilde{c}_{5}>\\tilde{c}_{4}$ with probability at least $1-e^{-\\Omega(M)}$ . Moreover, using Lemma G.2 with $k=0$ , we obtain ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{j}\\geq\\tilde{\\lambda}_{M}\\geq\\mu_{\\operatorname*{min}}\\big(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}\\big)\\geq\\tilde{c}_{6}\\tilde{\\lambda}_{2M}\\geq\\tilde{c}_{7}(M/\\tilde{c})^{-a}\\geq\\tilde{c}_{8}j^{-a}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ for some constants $\\tilde{c}_{6},\\tilde{c}_{7},\\tilde{c}_{8}>0$ . Combining the bounds for $j\\le M/\\tilde{c}$ and $j\\in[M/\\tilde{c},M]$ completes the proof. \u53e3 ", "page_idx": 41}, {"type": "text", "text": "Lemma G.5 (Ratio of eigenvalues of $\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}$ under power-law spectrum). Suppose Assumption 2 hold. There exists some $a$ -dependent constant $c>0$ such that for any $k\\geq1$ , the ratio between the $M/2$ -th and $M$ -th eigenvalues ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{\\mu_{M/2}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})}{\\mu_{M}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})}\\leq c\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . ", "page_idx": 41}, {"type": "text", "text": "Proof of Lemma G.5. We prove the lemma under two scenarios where $k$ is relatively small (or large) compared with $M$ . ", "page_idx": 42}, {"type": "text", "text": "Let $c>0$ be some sufficiently large constant. Applying Lemma G.1 with $\\mathbf{H}_{k:\\infty}$ replacing $\\mathbf{H}$ , for $k_{0}=M/c$ , we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{M/2}(\\displaystyle\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})\\leq\\frac{3}{2}\\cdot\\bigg(\\lambda_{M/2+k}+\\displaystyle\\frac{\\sum_{i>k_{0}}\\lambda_{i+k}}{M}\\bigg)+c_{1}\\cdot\\lambda_{k_{0}+1+k},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim\\big(\\displaystyle\\frac{M}{2}+k\\big)^{-a}+\\frac{(k_{0}+k)^{1-a}}{M}+(k_{0}+1+k)^{-a}}\\\\ &{\\qquad\\qquad\\lesssim(k\\vee M)^{-a}+(k\\vee M)^{-a}\\big(1\\vee\\displaystyle\\frac{k}{M}\\big)+(k\\vee M)^{-a}}\\\\ &{\\qquad\\qquad\\lesssim(k\\vee M)^{-a}\\big(1\\vee\\displaystyle\\frac{k}{M}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ for some constant $c_{1}>0$ . ", "page_idx": 42}, {"type": "text", "text": "Case 1: $k\\lesssim M$ From Lemma G.2, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mu_{\\mathrm{min}}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})\\gtrsim\\lambda_{k+2M}\\gtrsim(k\\vee M)^{-a}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . Therefore ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{\\mu_{M/2}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})}{\\mu_{M}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})}\\lesssim1\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ when $k/M\\lesssim1$ . ", "page_idx": 42}, {"type": "text", "text": "Case 2: $k\\gtrsim M$ On the other hand, when $k$ is relatively large, using Lemma G.1 with $\\mathbf{H}_{k:\\infty}$ replacing $\\mathbf{H}$ again, we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{M}\\bigl(\\mathbf S_{k:\\infty}\\mathbf H_{k:\\infty}\\mathbf S_{k:\\infty}^{\\top}\\bigr)\\geq\\frac12\\cdot\\left(\\lambda_{M+k}+\\frac{\\sum_{i>k_{0}}\\lambda_{i+k}}{M}\\right)-c_{1}\\cdot\\lambda_{k_{0}+1+k},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq c_{2}\\Bigl[\\left(M+k\\right)^{-a}+\\frac{(k_{0}+k)^{1-a}}{M}\\Bigr]-c_{3}\\cdot(k_{0}+1+k)^{-a}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ , where $c_{1},c_{2},c_{3}>0$ are some universal constants. Choosing $\\bar{k}_{0}=\\bar{M}/c^{2}$ for some sufficiently large constant $c>0$ , we further obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{M}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})\\geq c_{4}\\big(M+k\\big)^{-a}\\Big[1+\\displaystyle\\frac{k}{M}\\Big]-c_{5}(M+k)^{-a}}\\\\ {\\geq c_{6}\\big(M\\vee k\\big)^{-a}\\Big[1\\vee\\displaystyle\\frac{k}{M}\\Big]-c_{7}(M\\vee k)^{-a}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ , where $(c_{i})_{i=4}^{7}$ are $a$ -dependent constants. Since ", "page_idx": 42}, {"type": "equation", "text": "$$\nc_{6}\\big(M\\vee k\\big)^{-a}\\Big[1\\vee\\frac{k}{M}\\Big]-c_{7}(M\\vee k)^{-a}\\ge\\frac{c_{6}}{2}(k\\vee M)^{-a}\\big(1\\vee\\frac{k}{M}\\big)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "when $k$ is large, i.e., $k/M>\\tilde{c}$ for some sufficiently large $a$ -dependent constant $\\tilde{c}>0$ that may depend on $(c_{i}\\bar{)}_{i=1}^{7}$ , we have from Eq. (35) and (36) that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{\\mu_{M/2}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})}{\\mu_{M}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})}\\lesssim\\frac{(k\\vee M)^{-a}\\big(1\\vee\\frac{k}{M}\\big)}{(k\\vee M)^{-a}\\big(1\\vee\\frac{k}{M}\\big)}\\lesssim1\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . ", "page_idx": 42}, {"type": "text", "text": "G.3 Concentration results under logarithmic power-law spectrum ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Lemma G.6 (Proof of Theorem 6 in [5]). Suppose Assumption $^{4}$ hold. Then there exist some $a$ -dependent constants $c,\\tilde{c}>0$ such that, with probability at least $1-e^{-\\Omega(M)}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mu_{j}(\\mathbf{S}\\mathbf{H}\\mathbf{S}^{\\top})\\in\\left\\{\\!\\!\\left[c\\cdot j^{-1}\\log^{-a}(j+1),\\tilde{c}\\cdot j^{-1}\\log^{-a}(j+1)\\right]\\!\\right.\\quad j\\leq\\mathsf{k}^{*},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\mathsf{k}^{*}\\approx M/\\log(M)$ . Also, there exists some $a$ -dependent constants $c_{1},c_{2}>0$ such that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{c_{1}}{j\\log^{2a}(j+1)}\\leq\\mu_{j}(\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top})\\leq\\frac{c_{2}}{j\\log^{2a}(j+1)}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . ", "page_idx": 43}, {"type": "text", "text": "Proof of Lemma G.6. The proof is adapted from the proof of Theorem 6 in [5]. We include it here for completeness. ", "page_idx": 43}, {"type": "text", "text": "First part of Lemma G.6. In Lemma G.1, for some constant $c>1$ , choose ", "page_idx": 43}, {"type": "equation", "text": "$$\nk^{\\ast}:=\\operatorname*{min}\\bigg\\{k\\ge0:\\sum_{i>k}\\lambda_{i}\\ge c\\cdot M\\cdot\\lambda_{k+1}\\bigg\\}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then with probability $\\geq1-e^{-\\Omega(M)}$ , we have: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathrm{for\\;every\\;}1\\leq j\\leq M,\\quad\\frac{1}{c_{1}}\\cdot\\bigg(\\lambda_{j}+\\frac{\\sum_{i>\\mathsf{k}^{*}}\\lambda_{i}}{M}\\bigg)\\leq\\widetilde{\\lambda}_{j}\\leq c_{1}\\cdot\\bigg(\\lambda_{j}+\\frac{\\sum_{i>\\mathsf{k}^{*}}\\lambda_{i}}{M}\\bigg),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $c_{1}>1$ is a constant. ", "page_idx": 43}, {"type": "text", "text": "When $\\lambda_{j}\\approx j^{-1}\\log^{-a}(j+1)$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathsf{k}^{*}\\approx M/\\log(M),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{i>\\mathsf{k}^{*}}\\lambda_{i}\\approx\\log^{1-a}(\\mathsf{k}^{*})\\approx\\log^{1-a}(M).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Therefore, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\lambda}_{j}\\approx\\lambda_{j}+\\frac{\\sum_{i>k^{*}}\\lambda_{i}}{M}}\\\\ &{\\quad\\approx\\left\\{\\begin{array}{l l}{j^{-1}\\log^{-a}(j+1)}&{j\\le k^{*},}\\\\ {M^{-1}\\log^{1-a}(M)}&{k^{*}<j\\le M,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\mathsf{k}^{*}\\approx M/\\log(M)$ . ", "page_idx": 43}, {"type": "text", "text": "Second part of Lemma G.6. Let $\\bar{\\lambda}_{i}$ denote the $i$ -th eigenvalue of $\\mathbf{S}\\mathbf{H}^{2}\\mathbf{S}^{\\top}$ for $i\\in[M]$ . Using Lemma G.1 with $k\\,=\\,M/c$ for some sufficiently large constant $c_{\\mathrm{0}}$ and noting that $\\bar{\\sum_{i>k}}\\,\\lambda_{i}^{2}\\;\\bar{\\tilde{\\sim}}$ $\\begin{array}{r}{\\sum_{i>k}i^{-2}\\log^{-2a}(i+1)\\lesssim k^{-1}\\log^{-2a}k}\\end{array}$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{2}\\cdot j^{-2}\\log^{-2a}(j+1)-\\tilde{c}_{2}\\cdot M^{-2}\\log^{-2a}M}}\\\\ &{\\leq\\bar{\\lambda}_{j}\\leq\\frac{3}{2}\\cdot(j^{-2}\\log^{-2a}(j+1)+\\tilde{c}_{1}M^{-2}\\log^{-2a}M)+\\tilde{c}_{2}\\cdot M^{-2}\\log^{-2a}M}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for every $j\\in[M]$ for some constants $\\tilde{c}_{i},i\\in[2]$ with probability at least $1-e^{-\\Omega(M)}$ . Therefore, for all $j\\le M/\\tilde{c}$ for some sufficiently large constant $\\tilde{c}>1$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\bar{\\lambda}_{j}\\in[\\tilde{c}_{3}\\cdot j^{-2}\\log^{-2a}(j+1),\\tilde{c}_{4}\\cdot j^{-2}\\log^{-2a}(j+1)]\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ for some constants $\\tilde{c}_{3},\\tilde{c}_{4}>0$ . For $j\\in[M/\\tilde{c},M]$ , by monotonicity of the eigenvalues, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\bar{\\lambda}_{j}\\leq\\bar{\\lambda}_{\\lfloor M/\\tilde{c}\\rfloor}\\leq\\tilde{c}_{4}\\Bigl(\\Bigl\\lfloor\\frac{M}{\\tilde{c}}\\Bigr\\rfloor\\Bigr)^{-2}\\log^{-2a}\\left(\\Bigl\\lfloor\\frac{M}{\\tilde{c}}\\Bigr\\rfloor\\right)\\leq\\tilde{c}_{5}M^{-2}\\log^{-2a}M\\leq\\tilde{c}_{6}\\cdot j^{-2}\\log^{-2a}(j+1)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for some constants $c_{5},c_{6}>0$ with probability at least $1-e^{-\\Omega(M)}$ . Moreover, using Lemma G.2 with $k=0$ , we obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\bar{\\lambda}_{j}\\geq\\bar{\\lambda}_{M}\\geq\\mu_{\\operatorname*{min}}\\big(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}^{2}\\mathbf{S}_{k:\\infty}^{\\top}\\big)\\geq\\tilde{c}_{7}\\bar{\\lambda}_{2M}\\geq\\tilde{c}_{8}\\cdot j^{-2}\\log^{-2a}(j+1)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ for some constants $\\tilde{c}_{7},\\tilde{c}_{8}>0$ when $j\\in[M/\\tilde{c},M]$ . Combining the bounds for $j\\le M/\\tilde{c}$ and $j\\in[M/\\tilde{c},M]$ completes the proof. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "Lemma G.7 (Ratio of eigenvalues of $\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top}$ under logarithmic power-law spectrum). Suppose Assumption hold. There exists some $a$ -dependent constant $c>0$ such that for any $k\\geq1$ , the ratio between the $M/2$ -th and $M$ -th eigenvalues ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{\\mu_{M/2}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})}{\\mu_{M}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})}\\leq c\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . ", "page_idx": 44}, {"type": "text", "text": "Proof of Lemma G.7. Similar to the proof of Lemma G.5, we prove the lemma under two scenarios where $k$ is relatively small (or large) compared with $M$ . ", "page_idx": 44}, {"type": "text", "text": "Let $c>0$ be some sufficiently large constant. Applying Lemma G.1 with $\\mathbf{H}_{k:\\infty}$ replacing $\\mathbf{H}$ , for $k_{0}=M/c$ , we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{M/2}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})\\leq\\frac{3}{2}\\cdot\\bigg(\\lambda_{M/2+k}+\\frac{\\sum_{i>k_{0}}\\lambda_{i+k}}{M}\\bigg)+c_{1}\\cdot\\lambda_{k_{0}+1+k},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim\\big(\\frac{M}{2}+k\\big)^{-1}\\log^{-a}\\big(\\frac{M}{2}+k\\big)+\\frac{\\log^{1-a}(k_{0}+k)}{M}+\\frac{\\log^{-a}(k_{0}+1+k)}{k_{0}+1+k}}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\frac{\\log^{-a}(M+k)}{(M+k)}+\\frac{\\log^{1-a}(M+k)}{M}\\lesssim\\frac{\\log^{1-a}(M+k)}{M}\\qquad\\qquad\\qquad(37)}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ for some constant $c_{1}>0$ . ", "page_idx": 44}, {"type": "text", "text": "Case 1: $k\\lesssim M$ . Applying Lemma G.1 with $\\mathbf{H}_{k:\\infty}$ replacing $\\mathbf{H}$ , for $k_{0}=M/c$ , we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{M}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})\\gtrsim\\frac{1}{2}\\cdot\\left(\\lambda_{M+k}+\\frac{\\sum_{i>k_{0}}\\lambda_{i+k}}{M}\\right)-c_{1}\\cdot\\lambda_{k_{0}+1+k},}\\\\ &{\\qquad\\qquad\\qquad\\approx\\left(M+k\\right)^{-1}\\log^{-a}\\left(M+k\\right)+\\frac{\\log^{1-a}\\left(k_{0}+k\\right)}{M}-c\\frac{\\log^{-a}\\left(k_{0}+1+k\\right)}{k_{0}+1+k}}\\\\ &{\\qquad\\qquad\\qquad\\gtrsim\\frac{\\log^{-a}\\left(M+k\\right)}{\\left(M+k\\right)}+\\frac{\\log^{1-a}\\left(M+k\\right)}{M}-\\bar{c}\\frac{\\log^{-a}\\left(M\\right)}{M}}\\\\ &{\\qquad\\qquad\\qquad\\gtrsim\\frac{\\log^{1-a}M}{M}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ . Therefore, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{\\mu_{M/2}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})}{\\mu_{M}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})}\\lesssim\\Bigl[\\frac{\\log^{1-a}(M+k)}{M}\\Bigr]\\Bigl/\\Bigl[\\frac{\\log^{1-a}M}{M}\\Bigr]\\lesssim1\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ when $k/M\\lesssim1$ . ", "page_idx": 44}, {"type": "text", "text": "Case 2: $k\\gtrsim M$ . On the other hand, when $k$ is relatively large, using Lemma G.1 with $\\mathbf{H}_{k:\\infty}$ replacing $\\mathbf{H}$ and $k_{0}=M/c$ again, we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{M}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})\\geq\\frac{1}{2}\\cdot\\left(\\lambda_{M+k}+\\frac{\\sum_{i>k_{0}}\\lambda_{i+k}}{M}\\right)-c_{1}\\cdot\\lambda_{k_{0}+1+k},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\gtrsim\\left(M+k\\right)^{-1}\\log^{-a}\\left(M+k\\right)+\\frac{\\log^{1-a}(k_{0}+k)}{M}-c_{2}\\frac{\\log^{-a}(k_{0}+1+k)}{k_{0}+1+k}}\\\\ &{\\qquad\\qquad\\qquad\\gtrsim k^{-1}\\log^{-a}\\left(k\\right)+\\frac{\\log^{1-a}(M+k)}{M}-c_{3}\\frac{\\log^{-a}(M+k)}{M+k}}\\\\ &{\\qquad\\qquad\\qquad\\gtrsim\\frac{\\log^{1-a}(M+k)}{M}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ , where $c_{1},c_{2},c_{3}>0$ are some $a$ -dependent constants. Therefore, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{\\mu_{M/2}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})}{\\mu_{M}(\\mathbf{S}_{k:\\infty}\\mathbf{H}_{k:\\infty}\\mathbf{S}_{k:\\infty}^{\\top})}\\lesssim\\Bigl[\\frac{\\log^{1-a}(M+k)}{M}\\Bigr]\\Bigl/\\Bigl[\\frac{\\log^{1-a}(M+k)}{M}\\Bigr]\\lesssim1\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "with probability at least $1-e^{-\\Omega(M)}$ when $k/M\\gtrsim1$ . ", "page_idx": 44}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: The claims are accurate and reflect the contributions and scope of the paper. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 45}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: We discuss the limitations of our results. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 45}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We provide all the details for our theorems. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We provide the details for our experiments. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [No] ", "page_idx": 47}, {"type": "text", "text": "Justification: Our experiments are numerical simulations and easy to reproduce. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 47}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide the details for our experiments. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 47}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: We provide error bars. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: Our experiments are numerical simulations and can run with light compute resources. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 48}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We conform with the NeurIPS Code of Ethics. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 48}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: Our paper is theoretical and we do not expect direct societal impacts. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: Our paper is theoretical and we do not expect risk of such. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 49}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: Our paper is theoretical and does not use existing assets. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 49}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 50}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: Our paper is theoretical and does not create new assets. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 50}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: Our paper is theoretical and does not involve crowdsourcing nor research with human subjects. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 50}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: Our paper is theoretical and does not involve crowdsourcing nor research with human subjects. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 50}]