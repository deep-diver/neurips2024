[{"figure_path": "wgpmDyJgsg/figures/figures_0_1.jpg", "caption": "Figure 1: Given a set of unposed input images, SparseAGS jointly infers the corresponding camera poses and underlying 3D, allowing high-fidelity 3D inference in the wild.", "description": "This figure shows the input images of three different objects (shoes, a robot arm, and a teddy bear) in different viewpoints.  The middle row displays the inferred camera poses and a 3D representation of the objects. The bottom row shows the novel views generated by the model based on the inferred 3D model and poses. This illustrates the key capability of SparseAGS: accurately estimating 3D models and camera poses from a sparse set of input images.", "section": "Introduction"}, {"figure_path": "wgpmDyJgsg/figures/figures_3_1.jpg", "caption": "Figure 2: (a) Overview of SparseAGS: Given estimated camera poses from off-the-shelf models, our method iteratively reconstructs 3D and optimizes poses leveraging diffusion priors. (b) Detailed View of Each Component: We use rendering loss and multi-view SDS loss for 3D reconstruction while the rendering loss is propagated back to refine camera poses. At the end of each reconstruction iteration, we identify outliers by checking if their involvement in 3D inference yields larger errors in other views, implying the inconsistency of their poses with others.", "description": "This figure provides a detailed overview of the SparseAGS framework.  Part (a) shows the overall pipeline, starting with input images and off-the-shelf camera pose estimations and ending with a refined 3D model and poses. Part (b) breaks down the individual components, highlighting the iterative process of 3D reconstruction and pose refinement, the use of rendering loss and multi-view score distillation sampling (SDS) loss, and how outlier views are identified and handled. This iterative process aims to improve the accuracy of camera pose and 3D reconstruction by iteratively refining both.", "section": "3 Method"}, {"figure_path": "wgpmDyJgsg/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative Comparison on Camera Pose Accuracy. Given initial poses from off-the-shelf methods (top to bottom: DUSt3R [41], Ray Diff. [47] and RelPose++ [18]), the refined poses from SPARF [38] are compared with the output of SparseAGS. The estimated cameras are aligned with ground truth (in black) with an optimal similarity transform. More results are available in Fig. 8.", "description": "This figure compares the camera pose accuracy of three different methods: SPARF, SparseAGS, and the initial poses from three different off-the-shelf pose estimation methods (DUSt3R, Ray Diff, and RelPose++).  The results show that SparseAGS significantly improves upon the initial poses and outperforms SPARF in terms of aligning estimated camera positions to ground truth.", "section": "4 Experiments"}, {"figure_path": "wgpmDyJgsg/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative Comparison with LEAP [12] on Novel View Synthesis. We use two pose estimation baselines (Ray Diffusion [47] and DUSt3R [41]). SparseAGS better preserves details from the input images and shows enhanced performance with more accurate initial camera poses. More results are available in Fig. 9 of the appendix.", "description": "This figure compares the novel view synthesis capabilities of SparseAGS against LEAP, using two different pose estimation baselines (Ray Diffusion and DUSt3R). SparseAGS demonstrates superior performance in preserving details and achieving higher fidelity in novel view generation, particularly when initialized with more accurate camera poses. The zoomed-in sections highlight these differences.", "section": "4.2 Evaluation"}, {"figure_path": "wgpmDyJgsg/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative Comparison with LEAP [12] on Novel View Synthesis. We use two pose estimation baselines (Ray Diffusion [47] and DUSt3R [41]). SparseAGS better preserves details from the input images and shows enhanced performance with more accurate initial camera poses.", "description": "This figure shows a qualitative comparison of novel view synthesis results between SparseAGS and the baseline method LEAP, using two different pose estimation methods (Ray Diffusion and DUSt3R) for initialization.  SparseAGS demonstrates better preservation of details from the input images and improved performance when initialized with more accurate camera poses.  The comparison is presented for multiple objects (a toy chicken racer, a toy bull, and others), visually showcasing the quality differences.", "section": "4.2 Evaluation"}, {"figure_path": "wgpmDyJgsg/figures/figures_9_1.jpg", "caption": "Figure 6: Qualitative Comparison with No-Generative-Prior Setup (N = 8). Novel Views (NV) & Normal: From left to right \u2013 GT, NV w/o SDS, NV w/ SDS, Normal w/o SDS, Normal w/ SDS. Leveraging generative priors in the form of SDS contributes to a consistent 3D representation.", "description": "This figure compares the 3D reconstruction results with and without using Score Distillation Sampling (SDS) for generative priors.  It shows that including SDS leads to more consistent and coherent 3D models, especially when generating novel views. The comparison is performed on eight input images (N=8). The results are presented for both novel views and normal views, demonstrating the benefit of SDS across different viewing conditions.", "section": "4.2 Evaluation"}, {"figure_path": "wgpmDyJgsg/figures/figures_14_1.jpg", "caption": "Figure 7: SPARF Fails When Incorrect Correspondence is Leveraged. Rightmost Section: From left to right \u2013 Source, Target, Warped Source to Target Based on Estimated Correspondence, Confidence Map (yellow indicates high confidence). The estimated false correspondence due to symmetric patterns causes pose optimization to fail, leading to degraded novel views in SPARF.", "description": "This figure demonstrates a failure case of the SPARF method, where incorrect correspondences between images lead to inaccurate pose estimation and poor novel view synthesis. The figure shows a comparison between the ground truth, SparseAGS results, and SPARF results for novel view synthesis.  The rightmost section highlights the false correspondences identified by SPARF, which are marked in yellow on the confidence map.  These false correspondences are caused by symmetric patterns in the input images, leading to errors in the pose estimation and ultimately degrading the quality of the novel views generated by SPARF.", "section": "4.2 Evaluation"}, {"figure_path": "wgpmDyJgsg/figures/figures_16_1.jpg", "caption": "Figure 3: Qualitative Comparison on Camera Pose Accuracy. Given initial poses from off-the-shelf methods (top to bottom: DUSt3R [41], Ray Diff. [47] and RelPose++ [18]), the refined poses from SPARF [38] are compared with the output of SparseAGS. The estimated cameras are aligned with ground truth (in black) with an optimal similarity transform. More results are available in Fig. 8.", "description": "This figure compares the camera pose accuracy of SparseAGS against three baselines (DUSt3R, Ray Diffusion, and RelPose++) and SPARF.  For each method, initial camera poses are estimated using the baselines.  Then, the poses are further refined either by SPARF or SparseAGS. The refined poses are visually compared to ground truth to show improvements. The alignment is done by using an optimal similarity transform.  The black dots represent ground truth camera locations.", "section": "4 Experiments"}, {"figure_path": "wgpmDyJgsg/figures/figures_17_1.jpg", "caption": "Figure 4: Qualitative Comparison with LEAP [12] on Novel View Synthesis. We use two pose estimation baselines (Ray Diffusion [47] and DUSt3R [41]). SparseAGS better preserves details from the input images and shows enhanced performance with more accurate initial camera poses. More results are available in Fig. 9 of the appendix.", "description": "This figure compares the novel view synthesis results of SparseAGS against LEAP, using two different pose estimation baselines.  It demonstrates that SparseAGS better maintains details from the original input images and achieves improved performance when initialized with more accurate camera poses.", "section": "4.2 Evaluation"}]