[{"type": "text", "text": "Collision Cross-entropy for Soft Class Labels and Entropy-based Clustering ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We propose \u201ccollision cross-entropy\u201d as a robust alternative to Shannon\u2019s cross  \n2 entropy (CE) loss when class labels are represented by soft categorical distributions   \n3 y. In general, soft labels can naturally represent ambiguous targets in classification.   \n4 They are particularly relevant for self-labeled clustering methods, where latent   \n5 pseudo-labels $y$ are jointly estimated with the model parameters and uncertainty is   \n6 prevalent. In case of soft labels $y$ , Shannon\u2019s CE teaches the model predictions $\\sigma$   \n7 to reproduce the uncertainty in each training example, which inhibits the model\u2019s   \n8 ability to learn and generalize from these examples. As an alternative loss, we   \n9 propose the negative log of \u201ccollision probability\u201d that maximizes the chance of   \n10 equality between two random variables, predicted class and unknown true class,   \n11 whose distributions are $\\sigma$ and $y$ . We show that it has the properties of a generalized   \n12 CE. The proposed collision CE agrees with Shannon\u2019s CE for one-hot labels $y$ , but   \n13 the training from soft labels differs. For example, unlike Shannon\u2019s CE, data points   \n14 where $y$ is a uniform distribution have zero contribution to the training. Collision   \n15 CE significantly improves classification supervised by soft uncertain targets. Unlike   \n16 Shannon\u2019s, collision CE is symmetric for $y$ and $\\sigma$ , which is particularly relevant   \n17 when both distributions are estimated in the context of self-labeled clustering.   \n18 Focusing on discriminative deep clustering where self-labeling and entropy-based   \n19 losses are dominant, we show that the use of collision CE improves the state-of  \n20 the-art. We also derive an efficient EM algorithm that significantly speeds up the   \n21 pseudo-label estimation with collision CE. ", "page_idx": 0}, {"type": "text", "text": "22 1 Introduction and Motivation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 Shannon\u2019s cross-entropy $H(y,\\sigma)$ is the most common loss for training network predictions $\\sigma$ from   \n24 ground truth labels $y$ in the context of classification, semantic segmentation, etc. However, this   \n25 loss may not be ideal for applications where the targets $y$ are soft distributions representing various   \n26 forms of uncertainty. For example, this paper is focused on self-labeled classification [17, 1, 15, 16]   \n27 where the ground truth is not available and the network training is done jointly with estimating   \n28 latent pseudo-labels $y$ . In this case soft $y$ can represent the distribution of label uncertainty. Similar   \n29 uncertainty of class labels is also natural for supervised problems where the ground truth has errors   \n30 [26, 41]. In any cases of label uncertainty, if soft distribution $y$ is used as a target in $H(y,\\sigma)$ , the   \n31 network is trained to reproduce the uncertainty, see the dashed curves in Fig.1.   \n32 Our work is inspired by generalized entropy measures [33, 18]. Besides mathematical gen  \n33 erality, the need for such measures \u201cstems from practical aspects when modelling real world   \n34 phenomena though entropy optimization algorithms\u201d [30]. Similarly to $L_{p}$ norms, parametric   \n35 families of generalized entropy measures offer a wide spectrum of options. The Shannon\u2019s   \n36 entropy is just one of them. Other measures could be more \u201cnatual\u201d for any given problem.   \n37 A simple experiment in Figure 2 shows that   \n38 Shannon\u2019s cross-entropy produces deficient so  \n39 lutions for soft labels $y$ compared to the pro  \n40 posed collision cross-entropy. The limitation   \n41 of the standard cross-entropy is that it encour  \n42 ages the distributions $\\sigma$ and $y$ to be equal, see   \n43 the dashed curves in Fig.1. For example, the   \n44 model predictions $\\sigma$ are trained to copy the un  \n45 certainty of the label distribution $y$ , even when   \n46 $y$ is an uninformative uniform distribution. In   \n47 contrast, our collision cross-entropy (the solid   \n48 curves) gradually weakens the training as $y$   \n49 gets less certain. This numerical property of   \n50 our cross-entropy follows from its definition   \n51 (9) - it maximizes the probability of \u201ccolli  \n52 sion\u201d, which is an event when two random   \n53 variables sampled from the distributions $\\sigma$ and   \n54 $y$ are equal. This means that the predicted class   \n55 value is equal to the latent label. This is signif  \n56 icantly different from the $\\sigma=y$ encouraged   \n57 by the Shannon\u2019s cross-entropy. For example,   \n58 if $y$ is uniform then it does not matter what the   \n59 model predicts as the probability of collision   \n60 $\\textstyle{\\frac{1}{K}}$ would not change.   \n61 Organization of the paper: After the summary of our contributions below, Section 2 reviews the   \n62 relevant background on self-labeling models/losses and generalized information measures for entropy,   \n63 divergence, and cross-entropy. Then, Section 3 introduces our collision cross entropy measure,   \n64 discusses its properties, related formulations of R\u00e9nyi cross-entropy, and relation to noisy labels in   \n65 fully-supervised settings. Section 4 formulates our self-labeling loss by replacing the Shannon\u2019s cross   \n66 entropy term in a representative state-of-the-art formulation using soft pseudo-labels [16] with our   \n67 collision-cross-entropy. The obtained loss function is convex w.r.t. pseudo-labels $y$ , which makes   \n68 estimation of $y$ amenable to generic projected gradient descent. However, Section 4 derives a much   \n69 faster EM algorithm for estimating $y$ . As common for self-labeling, optimization of the total loss   \n70 w.r.t. network parameters is done via backpropagation. Section 5 presents our experiments, followed   \n71 by conclusions.   \n72 Summary of Contributions: We propose the collision cross-entropy as an alternative to the standard   \n73 Shannon\u2019s cross-entropy mainly in the context of self-labeled classification with soft pseudo-labels.   \n74 The main practical advantage is its robustness to uncertainty in the labels, which could also be   \n75 useful in other applications. The definition of our cross-entropy has an intuitive probabilistic   \n76 interpretation that agrees with the numerical and empirical properties. Unlike the Shannon\u2019s cross  \n77 entropy, our formulation is symmetric w.r.t. predictions $\\sigma$ and pseudo-labels $y$ . This is a conceptual   \n78 advantage since both $\\sigma$ and $y$ are estimated/optimized distributions. Our cross-entropy allows efficient   \n79 optimization of pseudo-labels by a proposed EM algorithm, that significantly accelerates a generic   \n80 projected gradient descent. Our experiments show consistent improvement over multiple examples of   \n81 unsupervised and semi-supervised clustering, and several standard network architectures. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "dQmEIwRw16/tmp/5bc9f0dfc4bcd74c48dc27a6f0a531213730be32cda2509635f5c57cfed19502.jpg", "img_caption": ["Figure 1: Collision cross-entropy $H_{2}(y,\\sigma)$ in (9) for fixed soft labels $y$ (red, green, and blue). Assuming binary classification, all possible predictions $\\sigma=(x,1-x)\\in$ $\\Delta_{2}$ are represented by points $x\\in[0,1]$ on the horizontal axis. For comparison, thin dashed curves show Shannon\u2019s cross-entropy $H(y,\\sigma)$ in (8). Note that $H$ converges to infinity at both endpoints of the interval. In contrast, $H_{2}$ is bounded for any non-hot $y$ . Such boundedness suggests robustness to target errors represented by soft labels $y$ . Also, collision cross-entropy $H_{2}$ gradually turns off the training (sets zero-gradients) as soft labels become highly uncertain (solid blue). In contrast, $H(y,\\sigma)$ trains the network to copy this uncertainty, e.g. observe the optimum $\\sigma$ for all dashed curves. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "82 2 Background Review ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "83 We study a new generalized cross-entropy measure in the context of deep clustering. The models are   \n84 trained on unlabeled data, but applications with partially labeled data are also relevant. Self-labeled   \n85 deep clustering is a popular area of research [5, 31]. More recently, the-state-of-the-art is achieved by   \n86 discriminative clustering methods based on maximizing the mutual information between the input and   \n87 the output of the deep model [3]. There is a large group of relevant methods [22, 10, 15, 17, 1, 16]   \n88 and we review the most important loss functions, all of which use standard information-theoretic   \n89 measures such as Shannon\u2019s entropy. In the second part of this section, we overview the necessary   \n90 mathematical background on the generalized entropy measures, which are central to our work.   \n92 The work of Bridle, Heading, and MacKay from 1991 [3] formulated mutual information (MI) loss for   \n93 unsupervised discriminative training of neural networks using probability-type outputs, e.g. softmax   \n94 $\\sigma:\\mathring{\\mathcal{R}}^{K}\\to\\Delta^{K}$ mapping $K$ logits $l_{k}\\in\\mathcal{R}$ to a point in the probability simplex $\\dot{\\Delta}^{K}$ . Such output   \n95 $\\sigma=(\\sigma_{1},\\dots,\\sigma_{K})$ is often interpreted as a posterior over $K$ classes, where $\\begin{array}{r}{\\sigma_{k}=\\frac{\\exp{l_{k}}}{\\sum_{i}\\exp{l_{i}}}}\\end{array}$ is a scalar   \n96 prediction for each class $k$ .   \n97 The unsupervised loss proposed in [3] trains the model predictions to keep as much information about   \n98 the input as possible. They derived an estimate of MI as the difference between the average entropy   \n99 of the output and the entropy of the average output ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{m i}\\;\\;:=\\;\\;-M I(c,X)\\;\\;\\;\\;\\approx\\;\\;\\;\\;\\overline{{{H(\\sigma)}}}\\;-\\;H(\\overline{{{\\sigma}}})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "100 where $c$ is a random variable representing class prediction, $X$ represents the input, and the av  \n101 eraging is done over all input samples $\\{\\breve{X}_{i}\\}_{i=1}^{M}$ , i.e. over $M$ training examples. The derivation   \n102 in [3] assumes that softmax represents the distribution $\\operatorname*{Pr}(c|X)$ . However, since softmax is not   \n103 a true posterior, the right hand side in (1) can be seen only as an MI loss. In any case, (1)   \n104 has a clear discriminative interpretation that stands on its own: $H(\\overline{{\\sigma}})$ encourages \u201cfair\u201d predic  \n105 tions with a balanced support of all categories across the whole training data set, while $\\overline{{H(\\sigma)}}$   \n106 encourages confident or \u201cdecisive\u201d prediction at each data point implying that decision bound  \n107 aries are away from the training examples [11]. Generally, we call clustering losses for soft  \n108 max models \u201cinformation-based\u201d if they use measures from the information theory, e.g. entropy.   \n109   \n110 Discriminative clustering loss (1) can be ap  \n111 plied to deep or shallow models. For clarity,   \n112 this paper distinguishes parameters w of the   \n113 representation layers of the network comput  \n114 ing features $f_{\\mathbf{w}}(\\mathbf{\\dot{\\boldsymbol{X}}})\\,\\in\\,\\mathcal{R}^{N}$ for any input $X$   \n115 and the linear classifier parameters $\\mathbf{v}$ of the   \n116 output layer computing $K$ -logit vector $\\mathbf{v}^{\\top}f$   \n117 for any feature $f\\stackrel{\\star}{\\in}\\mathcal{R}^{N}$ . The overall network   \n118 model is defined as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\sigma(\\mathbf{v}^{\\top}f_{\\mathbf{w}}(X)).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "119 A special \u201cshallow\u201d case in (2) is a basic linear   \n120 discriminator ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\sigma({\\bf v}^{\\top}X)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "121 directly operating on low-level input features   \n122 $f=X$ . Optimization of the loss (1) for the   \n123 shallow model (3) is done only over linear clas  \n124 sifier parameters $\\mathbf{v}$ , but the deeper network   \n125 model (2) is optimized over all network pa  \n126 rameters $[\\mathbf{v},\\mathbf{w}]$ . Typically, this is done via   \n127 gradient descent or backpropagation [35, 3].   \n128 Optimization of MI losses (1) during network   \n129 training is mostly done with standard gradi  \n130 ent descent or backpropagation [3, 22, 15].   \n131 However, due to the entropy term represent  \n132 ing the decisiveness, such loss functions are   \n133 non-convex and present challenges to the gradient descent. This motivates alternative formulations   \n134 and optimization approaches. For example, it is common to incorporate into the loss auxiliary   \n135 variables $y$ representing pseudo-labels for unlabeled data points $X$ and to estimate them jointly   \n136 with optimization of the network parameters [10, 1, 16]. Typically, such self-labeling approaches   \n137 to unsupervised network training iterate optimization of the loss over pseudo-labels and network   \n138 parameters, similarly to the Lloyd\u2019s algorithm for $K$ -means [2]. While the network parameters are   \n139 still optimized via gradient descent, the pseudo-labels can be optimized via more powerful algorithms. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "dQmEIwRw16/tmp/2d096ba9eeee60063bc7c6d04ae16bf75a78c7d1e5e9821d444660127e0437f9.jpg", "img_caption": ["Figure 2: Robustness to label uncertainty: collision crossentropy (9) vs Shannon\u2019s cross-entropy (8). The test uses ResNet-18 architecture on fully-supervised Natural Scene dataset [27] where we corrupted some labels. The horizontal axis shows the percentage $\\eta$ of training images where the correct ground truth labels were replaced by a random label. Both losses trained the model using soft target distributions $\\hat{y}=\\eta\\!*u\\!+\\!(1\\!-\\!\\eta)\\!*y$ representing the mixture of one-hot distribution $y$ for the observed corrupt label and the uniform distribution $u$ , as recommended in [26]. The vertical axis shows the test accuracy. Training with the collision cross-entropy is robust to much higher levels of label uncertainty. As discussed in the last part of Sec.3, in the context of classification supervised by hard noisy labels, collision CE with soft labels can be related to the forward correction methods [28]. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "140 For example, self-labeling in [1] uses the following constrained optimization problem with discrete \\ pseudo-labels $y$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{{\\cal L}_{c e}}}&{{=}}&{{\\overline{{{{\\cal H}(y,\\sigma)}}}\\qquad s.t.\\quad y\\in\\Delta_{0,1}^{K}\\quad a n d\\quad\\bar{y}=u}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "142 where $\\Delta_{0,1}^{K}$ are one-hot distributions, i.e. corners of the probability simplex $\\Delta^{K}$ . Training the   \n143 network predictions $\\sigma$ is driven by the standard cross entropy loss $H(y,\\sigma)$ , which is convex assuming   \n144 fixed (pseudo) labels $y$ . With respect to variables $y$ , the cross entropy is linear. Without the balancing   \n145 constraint $\\bar{y}~=~u$ , the optimal $y$ corresponds to the hard arg $\\operatorname*{max}(\\sigma)$ . However, the balancing   \n146 constraint converts this into an integer programming problem that can be solved approximately via   \n147 optimal transport [9]. The cross-entropy in (4) encourages the predictions $\\sigma$ to approximate one-hot   \n148 pseudo-labels $y$ , which implies the decisiveness.   \n149 Self-labeling methods for unsupervised clustering can also use soft pseudo-labels $y\\in\\Delta^{K}$ as target   \n150 distributions in cross-entropy $H(y,\\sigma)$ . In general, soft targets $y$ are common in $H(y,\\sigma)$ , e.g. in the   \n151 context of noisy labels [41, 38]. Softened targets $y$ can also assist network calibration [12, 26] and   \n152 improve generalization by reducing over-confidence [29]. In the context of unsupervised clustering,   \n153 cross-entropy $H(y,\\sigma)$ with soft pseudo-labels $y$ approximates the decisiveness since it encourages   \n154 $\\sigma\\approx y$ implying $H(y,\\sigma)\\approx H(y)\\approx H(\\sigma)$ where the latter is the first term in (1). Instead of the   \n155 hard constraint ${\\bar{y}}=u$ used in (4), the soft fairness constraint can be represented by KL divergence   \n\\ $K L(\\bar{y}\\parallel u)$ , as in [10, 16]. In particular, [16] formulates the following self-labeled clustering loss ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l}{L_{c e+k l}}&{=}&{}&{\\overline{{H(y,\\sigma)}}}&{}&{+\\ \\ K L(\\bar{y}\\parallel u)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "157 encouraging decisiveness and fairness as discussed. Similarly to (4), the network parameters in   \n158 loss (5) are trained by the standard cross-entropy term, but optimization over relaxed pseudo-labels   \n159 $y\\in\\Delta^{K}$ is relatively easy due to convexity. While there is no closed-form solution, the authors offer   \n160 an efficient approximate solver for $y$ . Iterating steps that estimate pseudo-labels $y$ and optimize the   \n161 model parameters resembles the Lloyd\u2019s algorithm for $\\mathbf{K}$ -means. The results in [16] also establish a   \n162 formal relation between the loss (5) and the $K$ -means objective. ", "page_idx": 3}, {"type": "text", "text": "163 2.2 Generalized Entropy Measures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Below, we review relevant generalized formulations of the information-theoretic concepts: entropy, divergence, and cross-entropy. R\u00e9nyi [33] introduced the entropy of order $\\alpha>0$ for any probability distribution $p$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nH_{\\alpha}(p)\\;:=\\;{\\frac{1}{1-\\alpha}}\\ln\\sum_{k}p_{k}^{\\alpha}\\qquad(\\alpha\\neq1)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "derived as the most general measure of uncertainty in $p$ satisfying four intuitively evident postulates. The entropy measures the average information and the order parameter $\\alpha$ relates to the power of the corresponding mean statistic [44]. The general formula above includes the Shannon\u2019s entropy ", "page_idx": 3}, {"type": "equation", "text": "$$\nH(p)\\;=\\;-\\sum_{k}p_{k}\\ln p_{k}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "164 as a special case when $\\alpha\\rightarrow1$ . The quadratic or second-order R\u00e9nyi entropy ", "page_idx": 3}, {"type": "equation", "text": "$$\nH_{2}(p)\\;:=\\;-\\ln\\sum_{k}p_{k}^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "165 is also known as a collision entropy since it is a negative log-likelihood of a \u201ccollision\u201d or \u201crolling   \n166 double\u201d when two i.i.d. samples from distribution $p$ have equal values. ", "page_idx": 3}, {"type": "text", "text": "Basic characterization postulates in [33] also lead to the general R\u00e9nyi formulation of the divergence, also known as the relative entropy, of order $\\alpha>0$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{\\alpha}(p\\,|\\,q)\\;:=\\;\\frac{1}{\\alpha-1}\\ln\\sum_{k}p_{k}^{\\alpha}\\,q_{k}^{1-\\alpha}\\qquad(\\alpha\\neq1)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "167 defined for any pair of distributions $p$ and $q$ . This reduces to the standard $\\mathrm{KL}$ divergence when $\\alpha\\rightarrow1$   \n168 ", "page_idx": 3}, {"type": "equation", "text": "$$\nD(p,q)=\\sum_{k}p_{k}\\ln\\frac{p_{k}}{q_{k}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "169 and to the Bhattacharyya distance for $\\alpha={\\textstyle{\\frac{1}{2}}}$ . ", "page_idx": 4}, {"type": "text", "text": "170 Optimization of entropy and divergence [24] is fundamental to many machine learning problems   \n171 [37, 20, 19, 30], including pattern classification and cluster analysis [36]. However, the entropy  \n172 related terminology is often mixed-up. For example, when discussing the cross-entropy minimization   \n173 principle (MinxEnt), many of the references cited earlier in this paragraph define cross-entropy using   \n174 the expression for KL-divergence (7). Nowadays, it is standard to define the Shannon\u2019s cross-entropy   \n175 as ", "page_idx": 4}, {"type": "equation", "text": "$$\nH(p,q)\\;=\\;-\\sum_{k}p_{k}\\ln q_{k}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "176 One simple explanation for the confusion is that KL-divergence $D(p,q)$ and cross-entropy $H(p,q)$   \n177 as functions of $q$ only differ by a constant if $p$ is a fixed known target, which is often the case. ", "page_idx": 4}, {"type": "text", "text": "178 3 Collision Cross-Entropy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Minimizing divergence enforces proximity between two distributions, which may work as a loss for training model predictions $\\sigma$ with labels $y$ , for example, if $y$ are ground truth one-hot labels. However, if $y$ are pseudo-labels that are estimated jointly with $\\sigma$ , proximity between $y$ and $\\sigma$ is not a good criterion for the loss. For example, highly uncertain model predictions $\\sigma$ in combination with uniformly distributed pseudo-labels $y$ correspond to the optimal zero divergence, but this is not a very useful result for self-labeling. Instead, all existing self-labeling losses for deep clustering minimize Shannon\u2019s cross-entropy (8) that reduces the divergence and uncertainty at the same time ", "page_idx": 4}, {"type": "equation", "text": "$$\nH(y,\\sigma)\\;\\equiv\\;D(y,\\sigma)+H(y).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "179 The entropy term corresponds to the \u201cdecisiveness\u201d constraint in unsupervised discriminative clus  \n180 tering [3, 17, 1, 15, 16]. In general, it is recommended as a regularizer for unsupervised and   \n181 semi-supervised network training [11] to encourage decision boundaries away from the data points   \n182 implicitly increasing the decision margins. ", "page_idx": 4}, {"type": "text", "text": "183 We propose a new form of cross-entropy ", "page_idx": 4}, {"type": "equation", "text": "$$\nH_{2}(p,q)\\;:=\\;-\\ln\\sum_{k}p_{k}\\:q_{k}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "184 that we call collision cross-entropy since it extends the collision entropy in (6). Indeed, (9) is the   \n185 negative log-probability of an event that two random variables with (different) distributions $p$ and $q$   \n186 are equal. When training softmax $\\sigma$ with pseudo-label distribution $y$ , the collision event is the exact   \n187 equality of the predicted class and the pseudo-label, where these are interpreted as specific outcomes   \n188 for random variables with distributions $\\sigma$ and $y$ . Note that the collision event, i.e. the equality of   \n189 two random variables, has very little to do with the equality of distributions $\\sigma=y$ . The collision   \n190 may happen when $\\sigma\\neq y$ , as long as $\\sigma\\cdot y>0$ . Vice versa, this event is not guaranteed even when   \n191 $\\sigma=y$ . It will happen almost surely only if the two distributions are the same one-hot. However, if   \n192 the distributions are both uniform, the collision probability is only $1/K$ . ", "page_idx": 4}, {"type": "text", "text": "As easy to check, the collision cross-entropy (9) can be equivalently represented as ", "page_idx": 4}, {"type": "equation", "text": "$$\nH_{2}(p,q)\\;\\equiv\\;-\\ln c o s(p,q)\\;+\\;\\frac{H_{2}(p)+H_{2}(q)}{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "193 where $c o s(p,q)$ is the cosine of the angle between $p$ and $q$ as vectors in $\\mathcal{R}^{K}$ and $H_{2}$ is the collision   \n194 entropy (6). The first term corresponds to a \u201cdistance\u201d between the two distributions: it is non  \n195 negative, equals 0 iff $p=q$ , and $-\\ln c o s(\\cdot)$ is a convex function of an angle, which can be interpreted   \n196 as a spherical metric. Thus, analogously to the Shannon\u2019s cross-entropy, $H_{2}$ is the sum of divergence   \n197 and entropy.   \n198 The formula (9) can be found as a definition of quadratic R\u00e9nyi cross-entropy [30, 32, 46]. However,   \n199 we could not identify information-theoretic axioms characterizing a generalized cross-entropy. R\u00e9nyi   \n200 himself did not discuss the concept of cross-entropy in his seminal work [33]. Also, two different   \n201 formulations of \u201cnatural\u201d and \u201cshifted\u201d R\u00e9nyi cross-entropy of arbitrary order could be found in   \n202 [44, 42]. In particular, the shifted version of order 2 agrees with our formulation of collision cross  \n203 entropy (9). However, lack of postulates or characterization for the cross-entropy, and the existence of   \n204 multiple non-equivalent formulations did not give us the confidence to use the name R\u00e9nyi. Instead,   \n205 we use \u201ccollision\u201d due to its clear intuitive interpretation of the loss (9). But, the term \u201ccross-entropy\u201d   \n206 is used only informally.   \n207 The numerical and empirical properties of the collision cross-entropy (9) are sufficiently different   \n208 from the Shannons cross-entropy (8). Figure 1 illustrates $H_{2}(y,\\sigma)$ as a function of $\\sigma$ for different   \n209 label distributions $y$ . For confident $y$ it behaves the same way as the standard cross entropy $H(y,\\sigma)$ ,   \n210 but softer low-confident labels $y$ naturally have little influence on the training. In contrast, the   \n211 standard cross entropy encourages prediction $\\sigma$ to be the exact copy of uncertainty in distribution   \n212 $y$ . Self-labeling methods based on $\\bar{H}(y,\\sigma)$ often \u201cprune out\u201d uncertain pseudo-labels [4]. Collision   \n213 cross entropy $H_{2}(y,\\sigma)$ makes such heuristics redundant. We also demonstrate the \u201crobustness to   \n214 label uncertainty\u201d on an example where the ground truth labels are corrupted by noise, see Fig.2.   \n215 This artificial fully-supervised test is used only to compare the robustness of (9) and (8) in complete   \n216 isolation from other terms in the self-labeled clustering losses, which are the focus of this work.   \n217 Due to the symmetry of the arguments in (9), such robustness of $H_{2}(y,\\sigma)$ also works the other way   \n218 around. Indeed, self-labeling losses are often used for both training $\\sigma$ and estimating $y$ : the loss is   \n219 iteratively optimized over predictions $\\sigma$ (i.e. model parameters responsible for it) and over pseudo  \n220 label distribution $y$ . Thus, it helps if $y$ also demonstrates \u201crobustness to prediction uncertainty\u201d. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Soft labels vs noisy labels: Our collision CE for soft labels, represented by distributions $y$ , can be related to loss functions used for supervised classification with noisy labels [40, 28, 38], which assume some observed hard target labels $l$ that may not be true due to corruption or \u201cnoise\u201d. Instead of our probability of collision ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(C=T)=\\sum_{k}\\operatorname*{Pr}(C=k,T=k)=\\sum_{k}\\sigma_{k}y_{k}\\equiv y^{\\top}\\sigma\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "221 between the predicted class $C$ and unknown true class $T$ , whose distributions are prediction $\\sigma$ and   \n222 soft target $y$ , they maximize the probability that a random variable $L$ representing a corrupted target   \n223 equals the observed value $l$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(L=l)=\\sum_{k}\\operatorname*{Pr}(L=l|T=k)\\operatorname*{Pr}(T=k)\\approx\\sum_{k}\\operatorname*{Pr}(L=l|T=k)\\;\\sigma^{k}\\;\\equiv\\;Q_{l}\\,\\sigma\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "224 where the approximation uses the model predictions $\\sigma^{k}$ instead of true class probabilities $\\mathrm{Pr}(T=k)$ ,   \n225 which is a significant assumption. Vector $Q_{l}$ is the $l$ -th row of the transition matrix $Q$ , such that   \n226 $Q_{l k}=\\mathrm{Pr}(L=l|T=k)$ , that has to be obtained in addition to hard noisy labels $l$ .   \n227 Our approach maximizing the collision probability based on soft labels $y$ is a generalization of the   \n228 methods for hard noisy labels. Their transitional matrix $Q$ can be interpreted as an operator for   \n229 converting any hard label $l$ into a soft label $y=Q^{\\top}\\mathbf{1}_{l}=Q_{l}$ . Then, the two methods are numerically   \n230 equivalent, though our statistical motivation is significantly different. Moreover, our approach is more   \n231 general since it applies to a wider set of problems where the class target $T$ can be directly specified   \n232 by a distribution, a soft label $y$ , representing the target uncertainty. For example, in fully supervised   \n233 classification or segmentation the human annotator can directly indicate uncertainty (odds) for classes   \n234 present in the image or at a specific pixel. In fact, class ambiguity is common in many data sets,   \n235 though for efficiency, the annotators are typically forced to provide one hard label. Moreover, in the   \n236 context of self-supervised clustering, it is natural to estimate pseudo-labels as soft distributions $y$ .   \n237 Such methods directly benefit from our collision CE, as this paper shows. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "238 4 Our Self-labeling Loss and EM ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "239 Based on prior work (5), we replace the standard cross-entropy with our collision cross-entropy to   \n240 formulate our self-labeling loss as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{L_{C C E}}&{:=}&{\\overline{{H_{2}(y,\\sigma)}}+\\lambda\\,K L(\\bar{y}\\|u)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "241 To optimize such loss, we iterate between two alternating steps for $\\sigma$ and $y$ . For $\\sigma$ , we use the standard   \n242 stochastic gradient descent algorithms[34]. For $y$ , we use the projected gradient descent (PGD) [7].   \n243 However, the speed of PGD is slow as shown in Table 1 especially when there are more classes. This   \n244 motivates us to find more efficient algorithms for optimizing $y$ . To derive such an algorithm, we made   \n245 a minor change to (10) by switching the order of variables in the divergence term: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{C C E+}\\;\\;:=\\;\\;\\overline{{{H_{2}(y,\\sigma)}}}+\\lambda\\,K L(u\\|\\bar{y})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "246 Such change allows us to use the Jensen\u2019s inequality on the divergence term to derive an efficient EM   \n247 algorithm while the quality of the self-labeled classification results is almost the same as shown in   \n248 the Appendix D.   \n249 EM algorithm for optimizing $y$ We derive the EM algorithm introducing latent variables, $K$   \n250 distributions $S^{k}\\in\\Delta^{\\bar{M}}$ representing normalized support for each cluster over $M$ data points. We   \n251 refer to each vector $S^{k}$ as a normalized cluster $k$ . Note the difference with distributions represented   \n252 by pseudo-labels $y\\in\\Delta^{K}$ showing support for each class at a given data point. Since we explicitly   \n253 use individual data points below, we will start to carefully index them by $i\\in\\{1,\\ldots,M\\}$ . Thus, we   \n254 will use $y_{i}\\in\\Delta^{K}$ and $\\sigma_{i}\\in\\Delta^{K}$ . Individual components of distribution $S^{k}\\in\\Delta^{M}$ corresponding to   \n255 data point $i$ will be denoted by scalar $S_{i}^{k}$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "\\ First, we expand (11) introducing the latent variables $S^{k}\\in\\Delta^{M}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c l l}{{{\\cal L}_{C C E+}}}&{{\\displaystyle\\stackrel{c}{=}}}&{{\\displaystyle{\\cal H}_{2}(y,\\sigma)+\\lambda\\,{\\cal H}(u,\\bar{y})}}\\\\ {{}}&{{=}}&{{\\displaystyle\\overline{{{{\\cal H}_{2}(y,\\sigma)}}}-\\lambda\\,\\sum_{k}u^{k}\\ln\\sum_{i}S_{i}^{k}\\,\\frac{y_{i}^{k}}{S_{i}^{k}M}\\le\\,\\,\\displaystyle\\overline{{{{\\cal H}_{2}(y,\\sigma)}}}-\\lambda\\,\\sum_{k}\\sum_{i}u^{k}S_{i}^{k}\\ln\\frac{y_{i}^{k}}{S_{i}^{k}M}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "257 Due to the convexity of negative log, we apply the Jensen\u2019s inequality to derive an upper bound, i.e.   \n258 (13), to $L_{C C E+}$ . Such bound becomes tight when: ", "page_idx": 6}, {"type": "equation", "text": "$$\nS_{i}^{k}=\\frac{y_{i}^{k}}{\\sum_{j}{y_{j}^{k}}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "59 Next, we derive the M step. Introducing the hidden variable $S$ breaks the   \n60 fairness term into the sum of independent terms for pseudo-labels $y_{i}$ $\\in$ \u2206K   \n61 at each data point $i$ . The solution for $S$ does not change (E step). Lets   \n262 focus on the loss with respect to $y$ . The col  \n263 lision cross-entropy (CCE) also breaks into   \n264 the sum of independent parts for each $y_{i}$ . For   \n265 simplicity, we will drop all indices $i$ in vari  \n266 ables $y_{i}^{k},\\,S_{i}^{k},\\,\\sigma_{i}^{k}$ \u03c3k . Then, the combination of   \n267 CCE loss with the corresponding part of the   \n268 fairness constraint can be written for each   \n269 $y=\\{y_{k}\\}\\in\\Delta_{K}$ as ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n-\\ln\\sum_{k}\\sigma_{k}y_{k}\\;\\;-\\;\\;\\lambda\\sum_{k}u_{k}S_{k}\\ln y_{k}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "table", "img_path": "dQmEIwRw16/tmp/24da114ca4f24e6e2a0f223f3137fb61138e45ab30f45b3a202baebee9e26b83.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: Comparison of our EM algorithm to Projected Gradient Descent (PGD). $\\eta$ is the step size. For $K=2$ , $\\eta_{1}\\sim\\eta_{3}$ are 1, 10 and 20 respectively. For $K=20$ and $K=200$ , $\\eta_{1}\\sim\\eta_{3}$ are 0.1, 1 and 5 respectively. Higher step size leads to divergence of PGD. ", "page_idx": 6}, {"type": "text", "text": "270 First, observe that this loss must achieve its global optimum in the interior of the simplex if $S_{k}>0$   \n271 and $u_{k}\\,>\\,0$ for all $k$ . Indeed, the second term enforces the \u201clog-barier\u201d at the boundary of the   \n272 simplex. Thus, we do not need to worry about KKT conditions in this case. Note that $S_{k}$ might be   \n273 zero, in which case we need to consider the full KKT conditions. However, the Property 1 that will   \n274 be mentioned later eliminates such concern if we use positive initialization. For completeness, we   \n275 also give the detailed derivation for such case and it can be found in the Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Adding the Lagrange multiplier $\\gamma$ for the simplex constraint, we get an unconstrained loss ", "page_idx": 6}, {"type": "equation", "text": "$$\n-\\ln\\sum_{k}\\sigma_{k}y_{k}\\;\\;-\\;\\;\\lambda\\sum_{k}u_{k}S_{k}\\ln y_{k}\\;\\;+\\;\\;\\gamma\\left(\\sum_{k}y_{k}-1\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "276 that must have a stationary point inside the simplex. The following theorem indicates the way to   \n277 solve the problem above. All the missing proofs can be found in Appendix A.   \n278 Theorem 1. [M-step solution]: The sum $\\sum_{k}y_{k}$ as in (16) is positive, continuous, convex, and   \n279 monotonically decreasing function of $x$ on the specified interval. Moreover, there exists a unique   \n280 solution $\\{y_{k}\\}\\in\\Delta_{k}$ and $x$ such that ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\sum_{k}y_{k}}&{\\equiv}&{\\sum_{k}\\frac{\\lambda u_{k}S_{k}}{\\lambda u^{\\top}S+1-\\frac{\\sigma_{k}}{x}}\\ \\ =\\ \\ 1\\quad a n d\\quad x\\in\\left(\\frac{\\sigma_{m a x}}{1+\\lambda u^{\\top}S}\\,,\\,\\sigma_{m a x}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "281 The monotonicity and convexity of $\\sum_{k}y_{k}$ with respect to $x$ suggest that the problem (16) formulated   \n282 in Theorem 1 allows efficient algor ithms for finding the corresponding unique solution. For example,   \n283 one can use the iterative Newton\u2019s updates to search for $x$ in the specified interval. The following   \n284 Lemma gives us a proper starting point ", "page_idx": 7}, {"type": "text", "text": "Lemma 1. Assuming $u_{k}S_{k}$ is positive for each $k$ , then the reachable left end point in Theorem 1 can be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\nl:=\\operatorname*{max}_{k}\\frac{\\sigma_{k}}{1+\\lambda u^{\\top}S-\\lambda u_{k}S_{k}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "285 for Newton\u2019s method. The algorithm for M-step solution is summarized in Algorithm 1 in Appendix   \n286 C. Note that we present the algorithm for only one data point, and we can easily and efficiently scale   \n287 up for more data in a batch by using the Numba compiler. In the following, we give the property   \n288 about the positivity of the solution. This property implies that if our EM algorithm has only (strictly)   \n289 positive variables $S_{k}$ or $y_{k}$ at initialization, these variables will remain positive during all iterations.   \n290 Property 1. For any category $k$ such that $u_{k}>0$ , the set of strictly positive variables $y_{k}$ or $S_{k}$ can   \n291 only grow during iterations of our EM algorithm for the loss (15) based on the collision cross-entropy.   \n292 Note that Property 1 does not rule out the possibility that $y_{k}$ may become arbitrarily close to zero   \n293 during EM iterations. Empirically, we did not observe any numerical issues. The complete algorithm   \n294 is given in Appendix C. Inspired by [39, 15], we also update our $y$ in each batch. Intuitively, updating   \n295 $y$ on the fly can prevent the network from being easily trapped in some local minima created by the   \n296 incorrect pseudo-labels. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "297 5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "298 We apply our new loss to self-labeled classification problems in both shallow and deep settings, as   \n299 well as semi-supervised modes. All the results are reproduced using either public codes or our own   \n300 implementation under the same experimental settings for fair comparison. Our approach consistently   \n301 achieves either the best or highly competitive results across all the datasets and is therefore more   \n302 robust. All the missing details in the experiments can be found in Appendix E.   \n303 Dataset We use four standard datasets: MNIST [25], CIFAR10/100 [43] and STL10 [8]. The   \n304 training and test data are the same unless otherwise specified.   \n305 Evaluation As for the evaluation of self-labeled classification, we set the number of clusters to   \n306 the number of ground-truth categories. To calculate the accuracy, we use the standard Hungarian   \n307 algorithm [23] to find the best one-to-one mapping between clusters and labels. We don\u2019t need this   \n308 matching step if we use other metrics, i.e. NMI, ARI. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "309 5.1 Clustering with Fixed Features ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "310 In this section, we test our loss as a proper clus  \n311 tering loss and compare it to the widely used   \n312 Kmeans (generative) and other closely related   \n313 losses (entropy-based and discriminative). We   \n314 use the pretrained (ImageNet) Resnet-50 [14]   \n315 to extract the features. For Kmeans, the model   \n316 is parameterized by K cluster centers. Com  \n317 parably, we use a one-layer linear classifier   \n318 followed by softmax for all other losses includ  \n319 ing ours. Kmeans results were obtained using   \n320 scikit-learn package in Python. To optimize   \n321 the model parameters for other losses, we use ", "page_idx": 7}, {"type": "table", "img_path": "dQmEIwRw16/tmp/b3230e1bf2287016370122d24a3e587f5b9542f469981d302b2971ce0e5cf347.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Comparison of different methods on clustering with fixed features extracted from Resnet-50. The numbers are the average accuracy and the standard deviation over trials. We use the 20 coarse categories for CIFAR100 similarly to others. ", "page_idx": 7}, {"type": "text", "text": "322 stochastic gradient descent. Here we report the average accuracy and standard deviation over 6   \n323 randomly initialized trials in Table 2.   \n325 In this section, we train a deep network to   \n326 jointly learn the features and cluster the data.   \n327 We test our method on both a small architec  \n328 ture (VGG4) and a large one (ResNet-18). The   \n329 only extra standard technique we add here is   \n330 self-augmentation following [15, 1, 6].   \n331 To train the VGG4, we use random initial  \n332 ization for network parameters. From Ta  \n333 ble 3, it can be seen that our approach con  \n334 sistently achieves the most competitive re", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "dQmEIwRw16/tmp/1fa997927614e6d8bccfc6396d3ae0ac715a6cee70a1770d81c683c2c946d006.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Quantitative comparison of discriminative clustering-based classification methods with simultaneous feature training from the scratch. The network architecture is VGG-4. We reuse the code published by [17, 1, 15] and use our improved implementation of [16] (also for other tables). ", "page_idx": 8}, {"type": "text", "text": "335 sults in terms of accuracy (ACC). Most of the methods we compared in our work (including   \n336 our method) are general concepts applicable to single-stage end-to-end training. To be fair,   \n337 we tested all of them on the same simple architecture. But, these general methods can be   \n338 easily integrated into other more complex systems with larger architecture such as ResNet-18.   \n339 In Table 4, we show the results using the   \n340 pretext-trained network from SCAN [45] as   \n341 initialization for our clustering loss as well as   \n342 IMSAT and MIADM. We use only the cluster  \n343 ing loss together with the self-augmentation   \n344 (one augmentation per image). As shown in   \n345 the table below, our method reaches a higher   \n346 number with more robustness almost for every   \n347 metric on all datasets compared to the SOTA   \n348 method SCAN. More importantly, we consis", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "dQmEIwRw16/tmp/372db3ce75b8d439f540accc03fe6bc4773e758e1770cc5bbe3fb487809b54f6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 4: Quantitative comparison using network ResNet18. The most related work MIADM (5) is also highlighted in all tables. ", "page_idx": 8}, {"type": "text", "text": "349 tently improve over the most related method, MIADM, by a large margin, which clearly demonstrates   \n350 the effectiveness of our proposed loss together with the optimization algorithm. ", "page_idx": 8}, {"type": "text", "text": "351 5.3 Semi-supervised Classification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "352 Although our paper is focused on self-labeled classification, we find it also interesting and natural to   \n353 test our loss under semi-supervised settings where partial data is provided with ground-truth labels.   \n354 We use the standard cross-entropy loss for labeled data and directly add it to the self-labeled loss to   \n355 train the network initialized by the pretext-trained network following [45]. ", "page_idx": 8}, {"type": "text", "text": "356 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "357 We propose a new collision cross-entropy loss.   \n358 Such loss is naturally interpreted as measur  \n359 ing the probability of the equality between two   \n360 random variables represented by the two distri  \n361 butions $\\sigma$ and $y$ , which perfectly ftis the goal of   \n362 self-labeled classification. It is symmetric w.r.t.   \n363 the two distributions instead of treating one   \n364 as the target, like the standard cross-entropy. ", "page_idx": 8}, {"type": "table", "img_path": "dQmEIwRw16/tmp/991857f961ba68cdae975ccf1662b6b8db1254593d034d3704ba982e64744086.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 5: Quantitative results for semi-supervised classification on STL10 and CIFAR10 using ResNet18. The numbers 0.1, 0.05 and 0.01 correspond to different ratio of labels used for supervision. \u201cOnly seeds\u201d means we only use standard cross-entropy loss on seeds for training. ", "page_idx": 8}, {"type": "text", "text": "365 While the latter makes the network copy the uncertainty in estimated pseudo-labels, our cross-entropy   \n366 naturally weakens the training on data points where pseudo labels are more uncertain. This makes   \n367 our cross-entropy robust to labeling errors. In fact, the robustness works both for prediction and for   \n368 pseudo-labels due to the symmetry. We also developed an efficient EM algorithm for optimizing the   \n369 pseudo-labels. Such EM algorithm takes much less time compared to the standard projected gradient   \n370 descent. Experimental results show that our method consistently produces top or near-top results on   \n371 all tested clustering and semi-supervised benchmarks. ", "page_idx": 8}, {"type": "text", "text": "372 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "373 [1] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous   \n374 clustering and representation learning. In International Conference on Learning Representations,   \n375 2020.   \n376 [2] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.   \n377 [3] John S. Bridle, Anthony J. R. Heading, and David J. C. MacKay. Unsupervised classifiers,   \n378 mutual information and \u2019phantom targets\u2019. In NIPS, pages 1096\u20131101, 1991.   \n379 [4] Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. Deep   \n380 adaptive image clustering. In International Conference on Computer Vision (ICCV), pages   \n381 5879\u20135887, 2017.   \n382 [5] Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. Deep   \n383 adaptive image clustering. In Proceedings of the IEEE international conference on computer   \n384 vision, pages 5879\u20135887, 2017.   \n385 [6] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings   \n386 of the IEEE/CVF conference on computer vision and pattern recognition, pages 15750\u201315758,   \n387 2021.   \n388 [7] Yunmei Chen and Xiaojing Ye. Projection onto a simplex, 2011.   \n389 [8] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsuper  \n390 vised feature learning. In Proceedings of the fourteenth international conference on artificial   \n391 intelligence and statistics, pages 215\u2013223. JMLR Workshop and Conference Proceedings, 2011.   \n392 [9] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in   \n393 neural information processing systems, 26, 2013.   \n394 [10] Kamran Ghasedi Dizaji, Amirhossein Herandi, Cheng Deng, Weidong Cai, and Heng Huang.   \n395 Deep clustering via joint convolutional autoencoder embedding and relative entropy minimiza  \n396 tion. In Proceedings of the IEEE international conference on computer vision, pages 5736\u20135745,   \n397 2017.   \n398 [11] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization.   \n399 Advances in neural information processing systems, 17, 2004.   \n400 [12] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural   \n401 networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.   \n402 [13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:   \n403 Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE   \n404 international conference on computer vision, pages 1026\u20131034, 2015.   \n405 [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image   \n406 recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,   \n407 pages 770\u2013778, 2016.   \n408 [15] Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning   \n409 discrete representations via information maximizing self-augmented training. In International   \n410 conference on machine learning, pages 1558\u20131567. PMLR, 2017.   \n411 [16] Mohammed Jabi, Marco Pedersoli, Amar Mitiche, and Ismail Ben Ayed. Deep clustering: On   \n412 the link between discriminative models and k-means. IEEE Transactions on Pattern Analysis   \n413 and Machine Intelligence, 43(6):1887\u20131896, 2021.   \n414 [17] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsuper  \n415 vised image classification and segmentation. In Proceedings of the IEEE/CVF International   \n416 Conference on Computer Vision, pages 9865\u20139874, 2019.   \n417 [18] Jagat N. Kapur. Measures of Information and Their Applications. John Wiley and Sons, 1994.   \n418 [19] Jagat N. Kapur and Hiremagalur K. Kesavan. Entropy Optimization Principles and Applications.   \n419 Springer, 1992.   \n420 [20] Hiremagalur K. Kesavan and Jagat N. Kapur. Maximum Entropy and Minimum Cross-Entropy   \n421 Principles: Need for a Broader Perspective, pages 419\u2013432. Springer, 1990.   \n422 [21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR   \n423 (Poster), 2015.   \n424 [22] Andreas Krause, Pietro Perona, and Ryan Gomes. Discriminative clustering by regularized   \n425 information maximization. Advances in neural information processing systems, 23, 2010.   \n426 [23] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics   \n427 quarterly, 2(1-2):83\u201397, 1955.   \n428 [24] Solomon Kullback. Information Theory and Statistics. Wiley, New York, 1959.   \n429 [25] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document   \n430 recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n431 [26] Rafael M\u00fcller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help?   \n432 Advances in neural information processing systems, 32, 2019.   \n433 [27] NSD. Natural Scenes Dataset [NSD]. https://www.kaggle.com/datasets/   \n434 nitishabharathi/scene-classification, 2020.   \n435 [28] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu.   \n436 Making deep neural networks robust to label noise: A loss correction approach. In Proceedings   \n437 of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), pages 1944\u20131952,   \n438 2017.   \n439 [29] Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regular  \n440 izing neural networks by penalizing confident output distributions. 2017.   \n441 [30] Jose C. Principe, Dongxin Xu, and John W. Fisher III. Information-theoretic learning. Advances   \n442 in unsupervised adaptive filtering, 2000.   \n443 [31] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with   \n444 deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.   \n445 [32] Sudhir Rao, Allan de Medeiros Martins, and Jos\u00e9 C. Pr\u00edncipe. Mean shift: An information   \n446 theoretic perspective. Pattern Recognition Letters, 30:222\u2013230, 2009.   \n447 [33] Alfr\u00e9d R\u00e9nyi. On measures of entropy and information. Fourth Berkeley Symp. Math. Stat.   \n448 Probab., 1:547\u2013561, 1961.   \n449 [34] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint   \n450 arXiv:1609.04747, 2016.   \n451 [35] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by   \n452 back-propagating errors. Nature, 323(6088):533\u2013536, 1986.   \n453 [36] John E. Shore and Robert M. Gray. Minimum cross-entropy pattern classification and cluster   \n454 analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 11\u201317, 1982.   \n455 [37] John E. Shore and Rodney W. Johnson. Axiomatic derivation of the principle of maximum   \n456 entropy and the principle of minimum cross-entropy. IEEE Transactions on Information Theory,   \n457 26(1):547\u2013561, 1980.   \n458 [38] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from   \n459 noisy labels with deep neural networks: A survey. IEEE Transactions on Neural Networks and   \n460 Learning Systems, 2022.   \n461 [39] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical genera  \n462 tive adversarial networks. In International Conference on Learning Representations, 2015.   \n463 [40] Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training   \n464 convolutional networks with noisy labels. ICLR workshop, 2015.   \n465 [41] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization   \n466 framework for learning with noisy labels. In Proceedings of the IEEE conference on computer   \n467 vision and pattern recognition, pages 5552\u20135560, 2018.   \n468 [42] Ferenc C. Thierrin, Fady Alajaji, and Tam\u00e1s Linder. R\u00e9nyi cross-entropy measures for common   \n469 distributions and processes with memory. Entropy, 24(10), 2022.   \n470 [43] Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data   \n471 set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and   \n472 machine intelligence, 30(11):1958\u20131970, 2008.   \n473 [44] Francisco J. Valverde-Albacete and Carmen Pel\u00e1ez-Moreno. The case for shifting the R\u00e9nyi   \n474 entropy. Entropy, 21(1), 2019.   \n475 [45] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc   \n476 Van Gool. Scan: Learning to classify images without labels. In Computer Vision\u2013ECCV 2020:   \n477 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part X, pages   \n478 268\u2013285. Springer, 2020.   \n479 [46] Xiao-Tong Yuan and Bao-Gang Hu. Robust feature extraction via information theoretic learning.   \n480 In International Conference on Machine Learning, (ICML), page 1193\u20131200, 2009. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "481 A Missing proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "482 Theorem 2. [M-step solution]: The sum $\\sum_{k}y_{k}$ as in (17) is positive, continuous, convex, and   \n483 monotonically decreasing function of $x$ on the specified interval. Moreover, there exists a unique   \n484 solution $\\{y_{k}\\}\\in\\Delta_{k}$ and $x$ such that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\sum_{k}y_{k}}&{\\equiv}&{\\sum_{k}\\frac{\\lambda u_{k}S_{k}}{\\lambda u^{\\top}S+1-\\frac{\\sigma_{k}}{x}}\\ \\ =\\ \\ 1\\quad a n d\\quad x\\in\\left(\\frac{\\sigma_{m a x}}{1+\\lambda u^{\\top}S}\\,,\\,\\sigma_{m a x}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "485 Proof. All $y_{k}$ in (17) are positive, continuous, convex, and monotonically decreasing functions of $x$   \n486 on the specified interval. Thus, $\\sum y_{k}$ behaves similarly. Assuming that max is the index of prediction   \n487 $\\sigma_{m a x}$ , we have $y_{m a x}\\rightarrow+\\infty$ when approaching the interval\u2019s left endpoint $\\begin{array}{r}{x\\rightarrow\\frac{\\sigma_{m a x}}{1+\\lambda u^{\\top}S}}\\end{array}$ . Thus,   \n488 $\\sum y_{k}>1$ for smaller values of $x$ . At the right endpoint $x=\\sigma_{m a x}$ we have $\\begin{array}{r}{y_{k}\\,\\le\\,\\frac{\\lambda u_{k}S_{k}}{\\lambda u^{\\top}S}}\\end{array}$ for all $k$   \n489 implying $\\sum y_{k}\\leq1$ . Monotonicity and continuity of $\\sum y_{k}$ w.r.t. $x$ imply the theorem. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Lemma 2. Assuming $u_{k}S_{k}$ is positive for each $k_{z}$ , then the reachable left end point in Theorem 1 can be written as ", "page_idx": 12}, {"type": "equation", "text": "$$\nl:=\\operatorname*{max}_{k}\\frac{\\sigma_{k}}{1+\\lambda u^{\\top}S-\\lambda u_{k}S_{k}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "490 Proof. Firstly, we prove that $l$ is (strictly) inside the interior of the interval in Theorem 1. For the left \\n end point, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l:=\\underset{k}{\\operatorname*{max}}\\,\\frac{\\sigma_{k}}{1+\\lambda u^{\\top}S-\\lambda u_{k}S_{k}}}\\\\ &{\\geq\\quad\\frac{\\sigma_{m a x}}{1+\\lambda u^{\\top}S-\\lambda u_{m a x}S_{m a x}}}\\\\ &{>\\quad\\frac{\\sigma_{m a x}}{1+\\lambda u^{\\top}S}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "\\n For the right end point, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l:=\\underset{k}{\\operatorname*{max}}\\,\\frac{\\sigma_{k}}{1+\\lambda\\boldsymbol{u}^{\\top}S-\\lambda\\boldsymbol{u}_{k}S_{k}}}\\\\ &{<\\quad\\underset{k}{\\operatorname*{max}}\\,\\sigma_{k}}\\\\ &{=\\quad\\sigma_{m a x}}\\end{array}\\qquad\\qquad\\qquad1+\\lambda\\boldsymbol{u}^{\\top}S-\\lambda\\boldsymbol{u}_{k}S_{k}>1\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, $l$ is a reachable point. Moreover, any $\\begin{array}{r}{\\frac{\\sigma_{m a x}}{1+\\lambda u^{\\top}S}<x<l}\\end{array}$ will still induce positive $y_{k}$ for any and we will also use this to prove that $x$ should not be smaller than . Let ", "page_idx": 12}, {"type": "equation", "text": "$$\nc:=\\arg\\operatorname*{max}_{k}\\frac{\\sigma_{k}}{1+\\lambda u^{\\top}S-\\lambda u_{k}S_{k}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "493 then we can substitute $l$ into the $x$ of $y_{c}$ . It can be easily verified that $y_{c}=1$ at such $l$ . Since $y_{c}$ is   \n494 monotonically decreasing in terms of $x$ , any $x$ smaller than $l$ will cause $y_{c}$ to be greater than 1. At   \n495 the same time, other $y_{k}$ is still positive as mentioned just above, so the $\\sum_{k}y_{k}$ will be greater than 1.   \n496 Thus, $l$ is a reachable left end point. \u53e3   \n497 Property 2. For any category $k$ such that $u_{k}>0$ , the set of strictly positive variables $y_{k}$ or $S_{k}$ can   \n498 only grow during iterations of our EM algorithm for the loss (d) based on the collision cross-entropy.   \n499 Proof. As obvious from the E-step (14), it is sufficient to prove this for variables $y_{k}$ . If $y_{k}=0$ , then   \n500 the E-step (14) gives $S_{k}=0$ . According to the M-step for the case of collision cross-entropy, variable   \n501 $y_{k}$ may become (strictly) positive at the next iteration if $\\sigma_{k}=\\sigma_{m a x}$ . Once $y_{k}$ becomes positive, the   \n502 following E-step (14) produces $S_{k}>0$ . Then, the fairness term effectively enforces the log-barrier   \n503 from the corresponding simplex boundary making M-step solution $y_{k}=0$ prohibitively expensive.   \n504 Thus, $y_{k}$ will remain strictly positive at all later iterations. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "505 B Complete Solutions for M step ", "text_level": 1, "page_idx": 13}, {"type": "equation", "text": "$$\n-\\ln\\sum_{k}\\sigma_{k}y_{k}\\;\\;-\\;\\;\\lambda\\sum_{k}u_{k}S_{k}\\ln y_{k}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The main case when $u_{k}S_{k}>0$ for all $k$ is presented in the main paper. Here we derive the case when there exist some $k$ such that $u_{k}S_{k}=0$ . Assume a non-empty subset of categories/classes ", "page_idx": 13}, {"type": "equation", "text": "$$\nK_{o}:=\\{k\\:|\\:u_{k}S_{k}=0\\}\\quad\\neq\\quad\\emptyset\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and its non-empty complement ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bar{K}_{o}:=\\left\\{k\\:\\vert\\:u_{k}S_{k}>0\\right\\}\\quad\\neq\\quad\\emptyset.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In this case the second term (fairness) in our loss (d) does not depend on variables $y_{k}$ for $k\\in K_{o}$ . Also, note that the first term ( collision cross-entropy) in (d) depends on these variables only via their linear combination $\\sum_{k\\in K_{o}}\\sigma_{k}y_{k}$ . It is easy to see that for any given confidences $y_{k}$ for $\\boldsymbol{k}\\in\\bar{K}_{o}$ it is optimal to put all the remaining confidence $1-\\textstyle\\sum_{k\\in\\bar{K}_{o}}y_{k}$ into one class $c\\in K_{o}$ corresponding to the larges prediction among the classes in $K_{o}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\nc:=\\underset{k\\in K_{o}}{\\arg\\operatorname*{max}}\\,\\sigma_{k}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "so that ", "page_idx": 13}, {"type": "equation", "text": "$$\ny_{c}\\;=\\;1-\\sum_{k\\in\\bar{K}_{o}}y_{k}\\quad\\quad\\quad\\mathrm{and}\\quad\\quad\\quad y_{k}=0,\\;\\;\\forall k\\in K_{o}\\setminus c.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "506 Then, our loss function (d) can be written as ", "page_idx": 13}, {"type": "equation", "text": "$$\n-\\ln\\sum_{k\\in\\bar{K}_{o}\\cup\\{c\\}}\\sigma_{k}y_{k}\\;\\;-\\;\\;\\lambda\\sum_{k\\in\\bar{K}_{o}}u_{k}S_{k}\\ln y_{k}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "507 that gives the Lagrangian function incorporating the probability simplex constraint ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r}{-\\ln\\sum_{k\\in\\bar{K}_{o}\\cup\\{c\\}}\\sigma_{k}y_{k}}&{-}&{\\lambda\\sum_{k\\in\\bar{K}_{o}}u_{k}S_{k}\\ln y_{k}}&{+}&{\\gamma\\left(\\sum_{k\\in\\bar{K}_{o}\\cup\\{c\\}}y_{k}-1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "508 The stationary point for this Lagrangian function should satisfy equations ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l r}{-\\frac{\\sigma_{k}}{\\sigma^{\\top}y}-\\lambda u_{k}S_{k}\\frac{1}{y_{k}}+\\gamma}&{=}&{0,}&{\\forall k\\in\\bar{K}_{o}}&{\\quad}&{\\mathrm{and}}&{\\quad}&{-\\frac{\\sigma_{c}}{\\sigma^{\\top}y}+\\gamma}&{=}&{0}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "509 which could be easily written as a linear system w.r.t variables $y_{k}$ for $k\\in\\bar{K}_{o}\\cup\\{c\\}$ . ", "page_idx": 13}, {"type": "text", "text": "510 We derive a closed-form solution for the stationary point as follows. Substituting $\\gamma$ from the right   \n511 equation into the left equation, we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\sigma_{c}-\\sigma_{k}}{\\sigma^{\\top}y}\\,y_{k}\\;\\;=\\;\\;\\lambda u_{k}S_{k},\\qquad\\forall k\\in\\bar{K}_{o}\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "512 Summing over $k\\in\\bar{K}_{o}$ we further obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r}{\\frac{\\eta_{c}(1-y_{c})-\\sum_{k\\in\\bar{K}_{o}}\\sigma_{k}y_{k}}{\\sigma^{\\top}y}\\ =\\ \\lambda u^{\\top}S}&{\\quad}&{\\Rightarrow\\quad}&{\\frac{\\sigma_{c}-\\sigma^{\\top}y}{\\sigma^{\\top}y}}&{=\\ \\lambda u^{\\top}S}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "giving a closed-form solution for $\\sigma^{\\top}y$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sigma^{\\top}y\\;\\;=\\;\\;\\frac{\\sigma_{c}}{1+\\lambda u^{\\top}S}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Substituting this back into (f) we get closed-form solutions for $y_{k}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\ny_{k}\\;\\;=\\;\\;\\frac{\\lambda u_{k}S_{k}}{(1+\\lambda u^{\\top}S)(1-\\frac{\\sigma_{k}}{\\sigma_{c}})}\\,,\\qquad\\forall k\\in\\bar{K}_{o}\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that positivity and boundedness of $y_{k}$ requires $\\sigma_{c}>\\sigma_{k}$ for all $k\\in\\bar{K}_{o}$ . In particular, this means $\\sigma_{c}=\\sigma_{m a x}$ , but it also requires that all $\\sigma_{k}$ for $k\\in\\bar{K}_{o}$ are strictly smaller than $\\sigma_{m a x}$ . We can also write the corresponding closed-form solution for $y_{c}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\ny_{c}\\ =\\ 1-\\sum_{k\\in\\bar{K}_{o}}y_{k}\\ =\\ 1\\ -\\ \\frac{\\sigma_{c}}{1+\\lambda u^{\\top}S}\\sum_{k\\in\\bar{K}_{o}}\\frac{\\lambda u_{k}S_{k}}{\\sigma_{c}-\\sigma_{k}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "513 Note that this solution should be positive $y_{c}>0$ as well. ", "page_idx": 14}, {"type": "text", "text": "514 In case any of the mentioned constraints $(\\sigma_{c}\\;>\\;\\sigma_{k},\\forall k\\;\\in\\;\\bar{K}_{o}$ and $y_{c}\\;>\\;0_{\\l}$ ) is not satisfied, the   \n515 complimentary slackness (KKT) can be used to formally prove that the optimal solution is $y_{c}=0$ .   \n516 That is, $y_{k}=0$ for all $k\\in K_{o}$ . This reduces the optimization problem to the earlier case focusing   \n517 on resolving $y_{k}$ for $k\\in\\bar{K}_{o}$ . This case is guaranteed to find a unique solution in the interior of the   \n518 simplex $\\Delta_{\\bar{K}_{o}}$ . Indeed, since inequality $u_{k}S_{k}>0$ holds for all $k\\in\\bar{K}_{o}$ , the strong fairness enforces a   \n519 log-barrier for all the boundaries of this simplex. ", "page_idx": 14}, {"type": "text", "text": "520 C Optimization algorithms ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "dQmEIwRw16/tmp/120c793590ab14d1cd48edea842415d2125bec7c129ec015f044116067e7f7d3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Algorithm 2: Optimization for (11)   \nInput :network parameters and dataset   \nOutput :network parameters   \nfor each epoch do for each iteration do Initialize $y$ by the network output at current stage as a warm start; while not convergent do E step: $\\begin{array}{r}{S_{i}^{k}=\\frac{y_{i}^{k}}{\\sum_{j}y_{j}^{k}}}\\end{array}$ ; M step: find $y_{i}^{k}$ using Newton\u2019s method; end Update network using loss $\\overline{{H_{2}(y,\\sigma)}}$ via stochastic gradient descent end   \nend ", "page_idx": 14}, {"type": "text", "text": "521 D Self-supervision Loss Comparison ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{L_{C C E}}&{:=}&{\\overline{{H_{2}(y,\\sigma)}}+\\lambda\\,K L(\\bar{y}\\|u)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{C C E+}\\;\\;:=\\;\\;\\overline{{{H_{2}(y,\\sigma)}}}+\\lambda\\,K L(u\\|\\bar{y})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "table", "img_path": "dQmEIwRw16/tmp/23d1332518fe5ed0463589a2b4c7fd73f30028894af7f43a749d2066e4001ca7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "dQmEIwRw16/tmp/eb69eecb3438f455b40f4361f8eead3fea2533c9c19b018b6f3f5ab5035d2dbd.jpg", "table_caption": ["Table 7: With simultaneous feature training from the scratch. The network architecture is VGG-4. ", "Table 6: Using fixed features extracted from Resnet-50. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "522 E Experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "523 E.1 Network Architecture ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "524 The network structure of VGG4 is adapted from [17]. We used standard ResNet-18 from the PyTorch   \n525 library as the backbone architecture for Figure 2. As for the ResNet-18 used for Table 4, we used the   \n526 code from this repository 1. ", "page_idx": 15}, {"type": "table", "img_path": "dQmEIwRw16/tmp/9b6566dbd5581bbe027ef505da3887c8321e883f21f24c3e03f8289ccbfb08f6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 8: Network architecture summary. s: stride; p: padding; K: number of clusters. The first column is used on MNIST [25]; the second one is used on CIFAR10/100 [43]; the third one is used on STL10 [8]. Batch normalization is also applied after each Conv layer. ReLu is adopted for non-linear activation function. ", "page_idx": 15}, {"type": "text", "text": "527 E.2 Experimental Settings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "528 Here we present the missing details of experimental settings for Table 2 - 5. As for Table 2, the   \n529 weight of the linear classifier is initialized by using Kaiming initialization [13] and the bias is all set   \n530 to zero at the beginning. We use the $l_{2}$ -norm weight decay and set the coefficient of this term to 0.001,   \n531 0.02, 0.009, and 0.02 for MNIST, CIFAR10, CIFAR100 and STL10 respectively. The optimizer is   \n532 stochastic gradient descent with a learning rate set to 0.1. The batch size is set to 250. The number of   \n533 epochs is 10. We set $\\lambda$ in our loss to 100 and separately tuned the hyperparameters for other methods.   \n534 For Table 3, we use Adam [21] with learning rate $1e^{-4}$ for optimizing the network parameters. We   \n535 set batch size to 250 for CIFAR10, CIFAR100 and MNIST and we use 160 for STL10. We report the   \n536 mean accuracy and Std from 6 runs with random initializations. We use 50 epochs for each run and   \n537 all methods reach convergence within 50 epochs. The weight decay coefficient is set to 0.01.   \n538 As for the training of ResNet-18 in Table 4, we still use the Adam optimizer, and the learning rate is   \n539 set to $5e^{-2}$ for the linear classifier and $1e^{-5}$ for the backbone. The weight decay coefficient is set to   \n540 $1e^{-4}$ . The batch size is 200 and the number of total epochs is 50. The $\\lambda$ is still set to 100. We only   \n541 use one augmentation per image, and the coefficient for the augmentation term is set to 0.5, 0.2, and   \n542 0.4 respectively for STL10, CIFAR10, and CIFAR100 (20).   \n543 As for the semi-supervised settings, we made two changes compared to the above. First, we added   \n544 the cross-entropy loss on the labeled images and set the weight to 2, and separately tuned the   \n545 hyperparameters for other methods. Second, the pseudo-labels on the labeled images are constrained   \n546 to be the ground truth during the optimization.   \n548 The checklist is designed to encourage best practices for responsible machine learning research,   \n549 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n550 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n551 follow the references and follow the (optional) supplemental material. The checklist does NOT count   \n552 towards the page limit.   \n553 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n554 each question in the checklist:   \n555 \u2022 You should answer [Yes] , [No] , or [NA] .   \n556 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n557 relevant information is Not Available.   \n558 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n559 The checklist answers are an integral part of your paper submission. They are visible to the   \n560 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n561 (after eventual revisions) with the final version of your paper, and its final version will be published   \n562 with the paper.   \n563 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n564 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n565 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n566 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n567 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n568 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n569 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n570 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n571 please point to the section(s) where related material for the question can be found. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "572 IMPORTANT, please: ", "page_idx": 16}, {"type": "text", "text": "573 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n574 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n575 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n576 1. Claims   \n577 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n578 paper\u2019s contributions and scope?   \n579 Answer: [Yes]   \n580 Justification: This can be justified from reading the paper.   \n581 Guidelines:   \n582 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n583 made in the paper.   \n584 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n585 contributions made in the paper and important assumptions and limitations. A No or   \n586 NA answer to this question will not be perceived well by the reviewers.   \n587 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n588 much the results can be expected to generalize to other settings.   \n589 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n590 are not attained by the paper.   \n591 2. Limitations   \n592 Question: Does the paper discuss the limitations of the work performed by the authors?   \n593 Answer: [NA]   \n594 Justification:   \n595 Guidelines:   \n596 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n597 the paper has limitations, but those are not discussed in the paper.   \n598 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n599 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n600 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n601 model well-specification, asymptotic approximations only holding locally). The authors   \n602 should reflect on how these assumptions might be violated in practice and what the   \n603 implications would be.   \n604 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n605 only tested on a few datasets or with a few runs. In general, empirical results often   \n606 depend on implicit assumptions, which should be articulated.   \n607 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n608 For example, a facial recognition algorithm may perform poorly when image resolution   \n609 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n610 used reliably to provide closed captions for online lectures because it fails to handle   \n611 technical jargon.   \n612 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n613 and how they scale with dataset size.   \n614 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n615 address problems of privacy and fairness.   \n616 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n617 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n618 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n619 judgment and recognize that individual actions in favor of transparency play an impor  \n620 tant role in developing norms that preserve the integrity of the community. Reviewers   \n621 will be specifically instructed to not penalize honesty concerning limitations.   \n622 3. Theory Assumptions and Proofs   \n623 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n624 a complete (and correct) proof?   \n625 Answer: [Yes]   \n626 Justification: Assumptions are clearly stated and missing proofs can be found in the ap  \n627 pendix.   \n628 Guidelines:   \n629 \u2022 The answer NA means that the paper does not include theoretical results.   \n630 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n631 referenced.   \n632 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n633 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n634 they appear in the supplemental material, the authors are encouraged to provide a short   \n635 proof sketch to provide intuition.   \n636 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n637 by formal proofs provided in appendix or supplemental material.   \n638 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n639 4. Experimental Result Reproducibility   \n640 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n641 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n642 of the paper (regardless of whether the code and data are provided or not)?   \n643 Answer: [Yes]   \n644 Justification: All the details can be found in the appendix.   \n645 Guidelines:   \n646 \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceive well by the reviewers: Making the paper reproducible is important, regardless o whether the code and data are provided or not. \u2022 If the contribution is a dataset and/or model, the authors should describe the steps take to make their results reproducible or verifiable. \u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same   \n6 dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detaile instructions for how to replicate the results, access to a hosted model (e.g., in the cas of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. \u2022 While NeurIPS does not require releasing code, the conference does require all submis sions to provide some reasonable avenue for reproducibility, which may depend on th nature of the contribution. For example   \n4 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n5 to reproduce that algorithm.   \n6 (b) If the contribution is primarily a new model architecture, the paper should describ the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there shoul either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construc the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which cas authors are welcome to describe the particular way they provide for reproducibility In the case of closed-source models, it may be that access to the model is limited i some way (e.g., to registered users), but it should be possible for other researcher to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc   \n9 tions to faithfully reproduce the main experimental results, as described in supplementa material? Answer: [No] Justification: Code is released upon acceptance.   \n3 Guidelines:   \n4 \u2022 The answer NA means that paper does not include experiments requiring code.   \n5 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc   \n6 public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not b possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for no including code, unless this is central to the contribution (e.g., for a new open-sourc benchmark). \u2022 The instructions should contain the exact command and environment needed to run t   \n2 reproduce the results. See the NeurIPS code and data submission guidelines (https //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 18}, {"type": "text", "text": "701 \u2022 Providing as much information as possible in supplemental material (appended to the   \n702 paper) is recommended, but including URLs to data and code is permitted.   \n703 6. Experimental Setting/Details   \n704 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n705 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n706 results?   \n707 Answer: [Yes]   \n708 Justification: See appendix.   \n709 Guidelines:   \n710 \u2022 The answer NA means that the paper does not include experiments.   \n711 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n712 that is necessary to appreciate the results and make sense of them.   \n713 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n714 material.   \n715 7. Experiment Statistical Significance   \n716 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n717 information about the statistical significance of the experiments?   \n718 Answer: [Yes]   \n719 Justification: Mean and standard deviation are provided for most of the experiments.   \n720 Guidelines:   \n721 \u2022 The answer NA means that the paper does not include experiments.   \n722 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n723 dence intervals, or statistical significance tests, at least for the experiments that support   \n724 the main claims of the paper.   \n725 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n726 example, train/test split, initialization, random drawing of some parameter, or overall   \n727 run with given experimental conditions).   \n728 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n729 call to a library function, bootstrap, etc.)   \n730 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n731 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n732 of the mean.   \n733 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n734 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n735 of Normality of errors is not verified.   \n736 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n737 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n738 error rates).   \n739 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n740 they were calculated and reference the corresponding figures or tables in the text.   \n741 8. Experiments Compute Resources   \n742 Question: For each experiment, does the paper provide sufficient information on the com  \n743 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n744 the experiments?   \n745 Answer: [No]   \n746 Justification: The datasets are not large. We used single P100 GPU card.   \n747 Guidelines:   \n748 \u2022 The answer NA means that the paper does not include experiments.   \n749 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n750 or cloud provider, including relevant memory and storage. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "760 Justification:   \n761 Guidelines:   \n762 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n763 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n764 deviation from the Code of Ethics.   \n765 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n766 eration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "767 10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "68 Question: Does the paper discuss both potential positive societal impacts and negative   \n69 societal impacts of the work performed?   \n770 Answer: [NA]   \n771 Justification:   \n72 Guidelines:   \n73 \u2022 The answer NA means that there is no societal impact of the work performed.   \n774 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n75 impact or why the paper does not address societal impact.   \n776 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n77 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n78 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n79 groups), privacy considerations, and security considerations.   \n80 \u2022 The conference expects that many papers will be foundational research and not tied   \n81 to particular applications, let alone deployments. However, if there is a direct path to   \n782 any negative applications, the authors should point it out. For example, it is legitimate   \n83 to point out that an improvement in the quality of generative models could be used to   \n84 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n85 that a generic algorithm for optimizing neural networks could enable people to train   \n786 models that generate Deepfakes faster.   \n87 \u2022 The authors should consider possible harms that could arise when the technology is   \n88 being used as intended and functioning correctly, harms that could arise when the   \n89 technology is being used as intended but gives incorrect results, and harms following   \n790 from (intentional or unintentional) misuse of the technology.   \n91 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n792 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n793 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n794 feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "03 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n04 necessary safeguards to allow for controlled use of the model, for example by requiring   \n05 that users adhere to usage guidelines or restrictions to access the model or implementing   \n06 safety filters.   \n07 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n08 should describe how they avoided releasing unsafe images.   \n09 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n10 not require this, but we encourage authors to take this into account and make a best   \n11 faith effort. ", "page_idx": 21}, {"type": "text", "text": "812 12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "813 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n814 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n815 properly respected? ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "816 Answer: [Yes]   \n817 Justification: We cite them and put the links as well.   \n818 Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "834 13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "848 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "849 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n850 include the full text of instructions given to participants and screenshots, if applicable, as   \n851 well as details about compensation (if any)?   \n852 Answer: [NA]   \n853 Justification: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "854   \n855   \n856   \n857   \n858   \n859   \n860   \n861   \n862   \n863   \n864   \n865   \n866   \n67   \n868   \n869   \n870   \n871   \n872   \n873   \n874   \n875   \n876   \n877   \n878   \n879   \n880   \n881 ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]