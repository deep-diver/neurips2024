[{"figure_path": "dQmEIwRw16/figures/figures_1_1.jpg", "caption": "Figure 1: Collision cross-entropy H2(y, \u03c3) in (9) for fixed soft labels y (red, green, and blue). Assuming binary classification, all possible predictions \u03c3 = (x, 1 \u2212 x) \u2208 \u22062 are represented by points x \u2208 [0, 1] on the horizontal axis. For comparison, thin dashed curves show Shannon\u2019s cross-entropy H(y, \u03c3) in (8). Note that H converges to infinity at both endpoints of the interval. In contrast, H2 is bounded for any non-hot y. Such boundedness suggests robustness to target errors represented by soft labels y. Also, collision cross-entropy H2 gradually turns off the training (sets zero-gradients) as soft labels become highly uncertain (solid blue). In contrast, H(y, \u03c3) trains the network to copy this uncertainty, e.g. observe the optimum \u03c3 for all dashed curves.", "description": "The figure compares Shannon\u2019s cross-entropy and collision cross-entropy for different soft labels in a binary classification setting.  It shows how the collision cross-entropy is bounded and robust to uncertain labels, unlike Shannon\u2019s cross-entropy, which tends to copy label uncertainty. ", "section": "3 Collision Cross-Entropy"}, {"figure_path": "dQmEIwRw16/figures/figures_2_1.jpg", "caption": "Figure 2: Robustness to label uncertainty: collision cross-entropy (9) vs Shannon\u2019s cross-entropy (8). The test uses ResNet-18 architecture on fully-supervised Natural Scene dataset [27] where we corrupted some labels. The horizontal axis shows the percentage \u03b7 of training images where the correct ground truth labels were replaced by a random label. Both losses trained the model using soft target distributions \u0177 = \u03b7*u + (1-\u03b7)*y representing the mixture of one-hot distribution y for the observed corrupt label and the uniform distribution u, as recommended in [26]. The vertical axis shows the test accuracy. Training with the collision cross-entropy is robust to much higher levels of label uncertainty. As discussed in the last part of Sec.3, in the context of classification supervised by hard noisy labels, collision CE with soft labels can be related to the forward correction methods [28].", "description": "This figure compares the robustness of the collision cross-entropy and Shannon\u2019s cross-entropy to label uncertainty in a classification task. The x-axis represents the corruption level (percentage of labels replaced by random labels), and the y-axis represents the test accuracy. The results show that the collision cross-entropy is significantly more robust to label noise than Shannon\u2019s cross-entropy.", "section": "3 Collision Cross-Entropy"}]