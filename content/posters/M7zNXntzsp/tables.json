[{"figure_path": "M7zNXntzsp/tables/tables_8_1.jpg", "caption": "Table 1: Results on the Fashion MNIST data set. We display the frequency of returning the correct label as a singleton, Bcorrect-single and the average size, Bset-size, both of which are defined in (6). Standard errors for these averages are shown in parentheses. The final column shows the worst-case instability over the test set, Bmax-instability, defined in (5). For each metric, the symbol // indicates that higher values are desirable, while indicates that lower values are desirable.", "description": "This table presents results on the Fashion MNIST data set, comparing four different methods for multiclass classification: argmax, inflated argmax, bagged argmax, and bagged inflated argmax.  The table shows the frequency of each method returning the correct label as a singleton, the average size of the returned sets of labels, and a measure of the worst-case instability of the method.", "section": "4 Experiments"}, {"figure_path": "M7zNXntzsp/tables/tables_24_1.jpg", "caption": "Table 2: Results on the Fashion MNIST data set. The table displays the frequency of returning the correct label as a singleton, Bcorrect-single, average size, Bset-size, utility-discounted predictive accuracies 465 and 480, and the superfluous inflation, Bsup. infl.. For each metric, the symbol / indicates that higher values are desirable, while indicates that lower values are desirable. Results for the base algorithm are in white, and results for the subbagged algorithm are in gray. Standard errors are in parentheses.", "description": "This table presents a comparison of several selection rules for a multiclass classification task on the Fashion MNIST dataset.  Metrics include the frequency of correctly identifying the class as a singleton, average set size, accuracy considering utility functions (465 and 480), and a measure of superfluous inflation.  Results are shown for both a base learning algorithm and a bagged version.", "section": "Additional experimental results"}]