[{"type": "text", "text": "Building a stable classifier with the inflated argmax ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jake A. Soloff   \nDepartment of Statistics University of Chicago Chicago, IL 60637   \nsoloff@uchicago.edu ", "page_idx": 0}, {"type": "text", "text": "Rina Foygel Barber Department of Statistics University of Chicago Chicago, IL 60637 rina@uchicago.edu ", "page_idx": 0}, {"type": "text", "text": "Rebecca Willett Departments of Statistics and Computer Science NSF-Simons National Institute for Theory and Mathematics in Biology University of Chicago Chicago, IL 60637 willett@uchicago.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose a new framework for algorithmic stability in the context of multiclass classification. In practice, classification algorithms often operate by first assigning a continuous score (for instance, an estimated probability) to each possible label, then taking the maximizer\u2014i.e., selecting the class that has the highest score. A drawback of this type of approach is that it is inherently unstable, meaning that it is very sensitive to slight perturbations of the training data, since taking the maximizer is discontinuous. Motivated by this challenge, we propose a pipeline for constructing stable classifiers from data, using bagging (i.e., resampling and averaging) to produce stable continuous scores, and then using a stable relaxation of argmax, which we call the \u201cinflated argmax\u201d, to convert these scores to a set of candidate labels. The resulting stability guarantee places no distributional assumptions on the data, does not depend on the number of classes or dimensionality of the covariates, and holds for any base classifier. Using a common benchmark data set, we demonstrate that the inflated argmax provides necessary protection against unstable classifiers, without loss of accuracy. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "An algorithm that learns from data is considered to be stable if small perturbations of the training data do not lead to large changes in its output. Algorithmic stability is an important consideration in many statistical applications. Within the fairness literature, for instance, stability is one aspect of reliable decision-making systems [FSVC19; HV19]. In interpretable machine learning, it similarly serves as a form of reproducibility [MSKA19; YK20; YB24]. [CMX11] relates stability to robustness, where a machine learning algorithm is robust if two samples with similar feature vectors have similar test error. In the context of generalization bounds, [BE02] and subsequent authors study stability of an algorithm\u2019s real-valued output\u2014for instance, values of a regression function. In the setting of a multiclass classification problem, where the data consists of features $X\\in\\mathcal{X}$ and labels $Y\\in[L]=\\{1,\\dots,L\\}$ , the results of this literature can thus be applied to analyzing a classifier\u2019s predicted probabilities $\\hat{p}_{\\ell}(X)\\in[0,1].$ \u2014i.e., our learned estimates of the conditional probabilities $\\operatorname*{ly}\\{Y=\\bar{\\ell}\\,|\\,X\\}$ , for each $\\ell\\in[L]$ . However, as we will see, stability of these predicted probabilities by no means implies stability of the predicted label itself, $\\hat{y}=\\operatorname{argmax}_{\\ell\\in[L]}\\hat{p}_{\\ell}(x).$ \u2014an arbitrarily small perturbation in $\\hat{p}_{\\ell}(x)$ might completely change the predicted label $\\hat{y}$ . The distinction matters: for system trustworthiness, we care about the model\u2019s final output, on which users base their decisions. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a new framework for algorithmic stability in the context of multiclass classification, to define\u2014and achieve\u2014a meaningful notion of stability when the output of the algorithm consists of predicted labels, rather than predicted probabilities. Our work connects to other approaches to allowing for ambiguous classification, including set-valued classification [Gry93; DDB09; SLW19; CDHL21] and conformal inference [PPVG02; VGS05; Lei14; AB23] (we will discuss related work further, below). ", "page_idx": 1}, {"type": "text", "text": "1.1 Problem setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In supervised classification, we take a data set ${\\mathcal{D}}=\\left((X_{i},Y_{i})\\right)_{i\\in[n]}$ consisting of $n$ observations, and train a classifier that maps from the feature space $\\mathcal{X}$ to the set $[L]$ of possible labels.1 Typically, this map is constructed in two stages. First we run some form of regression to learn a map $\\hat{p}:\\mathcal{X}\\to\\Delta_{L-1}$ from features to predicted probabilities, with ${\\hat{p}}_{\\ell}(x)$ denoting our learned estimate of the conditional label probability, $\\operatorname*{Pr}\\{Y=\\ell\\mid X=x\\}$ (here $\\begin{array}{r}{\\dot{\\Delta_{L-1}}=\\{\\bar{w}\\in\\mathbb{R}^{L}:w_{i}\\geq0,\\sum_{i}w_{i}=1\\}}\\end{array}$ denotes the probability simplex in $L$ dimensions). We will write $\\hat{p}=A(\\mathcal{D})$ , where $\\boldsymbol{\\mathcal{A}}$ denotes the learning algorithm mapping a data set $\\mathcal{D}$ (of any size) to the map $\\hat{p}$ . Then the second step is to convert the predicted probabilities ${\\hat{p}}_{\\ell}(x)$ to a predicted label, which is typically done by taking the argmax, i.e., $\\hat{y}\\,=\\,\\mathrm{argmax}_{\\ell}\\,\\hat{p}_{\\ell}(x)$ (with some mechanism for breaking ties). This two-stage procedure can be represented in the following diagram: ", "page_idx": 1}, {"type": "image", "img_path": "M7zNXntzsp/tmp/34a6a66b1c8cf0f4a16c8a11e6b9831a8cd468dffe54e9fb901e0ba961e1341f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "When predictions are ambiguous, meaning two or more classes nearly achieve the maximum predicted probability, the selected label becomes unstable and can change based on seemingly inconsequential perturbations to the training data. In other words, the above workflow is fundamentally incompatible with the goal of algorithmic stability. Consequently, in this paper we instead work with set-valued classifiers, which return a set of candidate labels\u2014in practice, this typically leads to a singleton set for examples where we have high confidence in the label, but allows for a larger set in the case of ambiguous examples. While the idea of returning a set of candidate labels is not itself new, we will see that the novelty of our work lies in finding a construction that offers provable stability guarantees. ", "page_idx": 1}, {"type": "text", "text": "In this framework, a feature vector $x\\in\\mathscr{X}$ is now mapped to a set of candidate labels $\\hat{S}\\subseteq[L]$ (rather than a single label $\\hat{y}$ ), via a selection rule $s$ , as illustrated here: ", "page_idx": 1}, {"type": "image", "img_path": "M7zNXntzsp/tmp/cae523685329511aca788aec31819eb3e20a01a3afc7198c2188d7bd7cb9ccfc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Formally, given a test point $x\\in\\mathscr{X}$ , this two-stage procedure returns $\\hat{S}\\,=\\,s(\\hat{p}(x))\\,\\subseteq\\,[L]$ , where $\\hat{p}=A(\\mathcal{D})$ is the output of the regression algorithm $\\boldsymbol{\\mathcal{A}}$ trained on data $\\mathcal{D}$ . Here $s:\\tilde{\\Delta}_{L-1}\\to\\wp([L])$ denotes a selection rule, mapping a vector of estimated probabilities to the set of candidate labels, and $\\mathcal{S}([L])$ denotes the set of subsets of $[L]$ . Of course, the earlier setting\u2014where the procedure returns a single label $\\hat{y}=\\mathrm{argmax}_{\\ell}\\ \\hat{p}_{\\ell}(x)$ , rather than a subset\u2014can be viewed as a special case by simply taking $s$ to be the argmax operator. ", "page_idx": 1}, {"type": "text", "text": "If we instead allow for $\\hat{S}$ to contain multiple candidate labels, a commonly used version of this framework is given by a top-5 procedure (or top- $k$ , for any fixed $k$ ). That is, after running a learning algorithm $\\boldsymbol{\\mathcal{A}}$ to estimate probabilities ${\\hat{p}}(x)$ , the selection rule $s$ then returns the top 5 labels\u2014the labels $\\ell_{1},\\ldots,\\ell_{5}\\in[L]$ corresponding to the highest 5 values of ${\\hat{p}}_{\\ell}(x)$ . This approach is more stable than a standard argmax. However, the choice of 5 in this setting is somewhat arbitrary, and the set $\\hat{S}$ always contains 5 candidate labels, regardless of the difficulty of the test sample\u2014while intuitively, we might expect that it should be possible to return a smaller $\\hat{S}$ for test points $x$ that are easier to classify (and a larger $\\hat{S}$ if $x$ is more ambiguous). In contrast, in our work we seek a more principled approach, where we provide provable stability guarantees while also aiming for the set $\\hat{S}$ to be as small as possible. ", "page_idx": 2}, {"type": "text", "text": "1.2 Overview of main results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we introduce selection stability, a new definition of algorithmic stability in the context of classification, which focuses on the stability of the predicted label. We reduce the problem of stabilizing a classifier to separately stabilizing the learning and selection stages, described above. For the selection rule $s$ of the two-stage procedure, we propose the inflated argmax operation: ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Inflated argmax). For any $w\\in\\mathbb{R}^{L}$ , define the inflated argmax as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{argmax}^{\\varepsilon}(w):=\\left\\{j\\in[L]:\\mathrm{dist}(w,R_{j}^{\\varepsilon})<\\varepsilon\\right\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\operatorname{dist}(w,R_{j}^{\\varepsilon})=\\operatorname*{inf}_{v\\in R_{j}^{\\varepsilon}}\\|w-v\\|$ , and where ", "page_idx": 2}, {"type": "equation", "text": "$$\nR_{j}^{\\varepsilon}=\\left\\{w\\in\\mathbb{R}^{L}:w_{j}\\geq\\operatorname*{max}_{\\ell\\neq j}w_{\\ell}+\\varepsilon/\\sqrt{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Our procedure will then return the set $\\hat{S}=\\operatorname{argmax}^{\\varepsilon}(\\hat{p}(x))$ of candidate labels\u2014intuitively, $j\\in$ argmax ${}^{\\varepsilon}(\\hat{p}(x))$ indicates that a small perturbation of the predicted probabilities, ${\\hat{p}}(x)$ , would lead to the $j$ th label\u2019s predicted probability being largest by some margin. In particular, by construction, the inflated argmax will always include any maximal index\u2014that is, if $\\hat{p}(\\bar{x_{}\\rangle_{j}}$ is the (possibly non-unique) largest estimated probability, then we must have $j\\,\\in\\,\\hat{S}\\,=\\,\\mathrm{argmax}^{\\bar{\\varepsilon}}(\\hat{p}(x))$ (this fact, and other properties of the inflated argmax, will be established formally in Proposition 10 below). ", "page_idx": 2}, {"type": "text", "text": "In this work, we derive the stabilizing properties of the inflated argmax, and give an algorithm to compute it efficiently. We prove that combining this operation with bagging at the learning step will provably stabilize any classifier. In particular, our guarantee holds with no assumptions on the data, and no constraints on the dimensionality of the covariates nor on the number of classes. ", "page_idx": 2}, {"type": "text", "text": "2 Framework: stable classification ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we propose a definition of algorithmic stability in the setting of multiclass classification. To begin, we formally define a classification algorithm as a map2 ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathscr{C}:\\cup_{n\\geq0}(\\mathcal{X}\\times[L])^{n}\\,\\times\\,\\mathcal{X}\\,\\longrightarrow\\,\\wp([L]),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which maps a training data set $\\mathcal{D}$ of any size $n$ , together with a test feature vector $x\\,\\in\\,{\\mathcal{X}}$ , to a candidate set of labels $\\hat{S}=\\mathcal{C}(\\mathcal{D},x)$ . To relate this notation to our earlier terminology, the two-stage selection procedure described in Section 1.1 corresponds to the classification algorithm ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathcal{C}}({\\mathcal{D}},x)=s({\\hat{p}}(x)){\\mathrm{~where~}}{\\hat{p}}=A({\\mathcal{D}}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Abusing notation, we will write this as $\\mathcal{C}=\\mathfrak{s}\\circ\\mathcal{A}$ , indicating that $\\mathcal{C}$ is obtained by applying the selection rule $s$ to the output of the learning algorithm $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 2}, {"type": "text", "text": "We now present our definition of algorithmic stability for a classification algorithm $\\mathcal{C}$ . As is common in the algorithmic stability literature, we focus on the stability of the algorithm\u2019s output with respect to randomly dropping a data point: does the output of $\\mathcal{C}$ on a test point $x$ change substantially if we drop a single data point from the training data $\\mathcal{D}?$ If the algorithm\u2019s output were a real-valued prediction $\\hat{y}\\in\\mathbb{R}$ , we could assess this by measuring the real-valued change in $\\hat{y}$ when a single data point is dropped. For classification, however, we will need to take a different approach: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Selection stability). We say a classification algorithm $\\mathcal{C}$ has selection stability $\\delta$ at sample size n if, for all datasets $\\ensuremath{\\mathcal{D}}\\in(\\ensuremath{\\mathcal{X}}\\times[L])^{n}$ and all test features $x\\in\\mathscr{X}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\left\\{\\hat{S}\\cap\\hat{S}^{\\backslash i}=\\emptyset\\right\\}\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{S}=\\mathcal{C}(\\mathcal{D},x)$ and where $\\hat{S}^{\\backslash i}=\\mathcal{C}(\\mathcal{D}^{\\backslash i},x).$ , for each $i\\in[n]$ . ", "page_idx": 3}, {"type": "text", "text": "Here the notation $\\mathcal{D}^{\\backslash i}$ denotes the data set $\\mathcal{D}$ with $i$ th data point removed\u2014that is, for a data set $\\boldsymbol{\\mathscr{D}}=\\left((\\boldsymbol{\\mathscr{X}}_{j},\\boldsymbol{Y}_{j})\\right)_{j\\in[n]}$ , the leave-one-out data set is given by $D^{\\backslash i}=\\left((X_{j},Y_{j})\\right)_{j\\in[n]\\backslash i}.$ ", "page_idx": 3}, {"type": "text", "text": "2.1 Interpreting the definition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Intuitively, selection stability controls the probability that our algorithm makes wholly inconsistent claims when dropping a single data point at random from the training set. If we interpret $\\hat{S}$ as making the claim that the true label $Y$ is equal to one of the labels in the candidate set $\\hat{S}$ , and similarly for $\\hat{S}^{\\backslash i}$ , then as soon as there is even one single value that lies in both $\\hat{S}$ and $\\hat{S}^{\\backslash i}$ , this means that the two claims are not contradictory\u2014even if the sets are large and are mostly nonoverlapping. ", "page_idx": 3}, {"type": "text", "text": "At first glance, this condition appears to be too mild, in the sense that we require the two prediction sets only to have some way of being compatible, and allows for substantial difference between the sets $\\hat{S}$ and $\\hat{S}^{\\backslash i}$ . However, since standard classification algorithms always output a single label, they often cannot be said to be stable even in this basic sense. Thus, we can view this definition as providing a minimal notion of stability that we should require any interpretable method to satisfy. ", "page_idx": 3}, {"type": "text", "text": "2.2 Connection to classical algorithmic stability ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Most prior work on algorithmic stability concerns algorithms with continuous outputs\u2014for example, in our notation above, the algorithm $\\boldsymbol{\\mathcal{A}}$ that returns estimated probabilities ${\\hat{p}}(x)$ . With such algorithms, there are more standard tools at our disposal for quantifying and ensuring stability. In this section, we connect classical algorithmic stability to our notion of selection stability (Definition 2). We first recall the following definition. ", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Tail stability). [SBW24b] A learning algorithm $\\boldsymbol{\\mathcal{A}}$ has tail stability $(\\varepsilon,\\delta)$ at sample size n if, for all datasets $\\mathcal{D}$ of size n and all test features $x\\in\\mathscr{X}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n}\\mathbf{1}\\left\\{\\|{\\hat{p}}(x)-{\\hat{p}}^{\\backslash i}(x)\\|\\geq\\varepsilon\\right\\}\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{p}=\\mathcal{A}(\\mathcal{D})$ and $\\hat{p}^{\\backslash i}=\\mathcal{A}(\\mathcal{D}^{\\backslash i})$ , and where $\\Vert\\cdot\\Vert$ denotes the usual Euclidean norm on $\\mathbb{R}^{L}$ . ", "page_idx": 3}, {"type": "text", "text": "This intuitive notion of stability is achieved by many well-known algorithms $\\boldsymbol{\\mathcal{A}}$ , such as nearestneighbor type methods or methods based on bagging or ensembling (as established by [SBW24a; SBW24b]\u2014see Section 3.1 below for details). ", "page_idx": 3}, {"type": "text", "text": "2.3 From stability to selection stability ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To construct a classification algorithm using the two-stage procedure outlined in Section 1.1, we need to apply a selection rule $s$ to the output of our learning algorithm $\\boldsymbol{\\mathcal{A}}$ . We might expect that choosing a stable $\\boldsymbol{\\mathcal{A}}$ will lead to selection stability in the resulting classification algorithm\u2014but in fact, this is not the case: even if the learning algorithm $\\boldsymbol{\\mathcal{A}}$ is itself extremely stable in the sense of Definition 3, the classification rule obtained by applying argmax to the output of $\\boldsymbol{\\mathcal{A}}$ can still be extremely unstable. The underlying issue is that argmax is extremely discontinuous\u2014the perturbation $\\lVert\\hat{p}(x)-\\hat{p}^{\\backslash i}(x)\\rVert$ in the predicted probabilities can be arbitrarily small but still lead to different predicted labels, i.e., $\\mathrm{argmax}_{\\ell}\\,\\hat{p}_{\\ell}(x)\\neq\\mathrm{argmax}_{\\ell}\\,\\hat{p}_{\\ell}^{\\setminus i}(x)$ . ", "page_idx": 3}, {"type": "text", "text": "Since combining argmax with a stable learning algorithm $\\boldsymbol{\\mathcal{A}}$ will not suffice, we instead seek a different selection rule $s$ \u2014one that will ensure selection stability (when paired with a stable learning algorithm $\\mathcal{A}$ ). To formalize this aim, we introduce another definition: ", "page_idx": 4}, {"type": "text", "text": "Definition 4 ( $\\varepsilon$ -compatibility). A selection rule $s:\\Delta_{L-1}\\to\\wp([L])$ is $\\varepsilon$ -compatible if, for any $v,w\\in\\Delta_{L-1}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|v-w\\|<\\varepsilon\\implies s(v)\\cap s(w)\\neq\\emptyset.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This notion of $\\varepsilon$ -compatibility allows us to construct classification algorithms with selection stability. Combining the above definitions leads immediately to the following result: ", "page_idx": 4}, {"type": "text", "text": "Proposition 5. Let $\\boldsymbol{\\mathcal{A}}$ be a learning algorithm with tail stability $(\\varepsilon,\\delta)$ at sample size $n_{i}$ , and let s be a selection rule satisfying $\\varepsilon$ -compatibility. Then the classification algorithm $\\mathcal{C}=s\\circ A$ has selection stability $\\delta$ at sample size $n$ . ", "page_idx": 4}, {"type": "text", "text": "Therefore, by pairing a stable learning algorithm $\\boldsymbol{\\mathcal{A}}$ with a compatible selection rule $s$ , we will automatically ensure selection stability of the resulting classification algorithm $\\mathcal{C}=s\\circ A$ . ", "page_idx": 4}, {"type": "text", "text": "Of course, $\\varepsilon$ -compatibility of the selection rule $s$ might be achieved in a trivial way\u2014for instance, $s$ returns the entire set $[L]$ for any input. As is common in statistical settings (e.g., a tradeoff between Type I error and power, in hypothesis testing problems), our goal is to ensure selection stability while returning an output S\u02c6 that is as informative as possible. In particular, later we will consider the specific aim of constructing $\\hat{S}$ to be a singleton set as often as possible. ", "page_idx": 4}, {"type": "text", "text": "3 Methodology: assumption-free stable classification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we formally define our pipeline for building a stable classification procedure using any base learning algorithm $\\boldsymbol{\\mathcal{A}}$ . At a high level, we leverage Proposition 5 and separately address the learning and selection stages described in Section 2. ", "page_idx": 4}, {"type": "text", "text": "1. In Section 3.1, we construct a bagged (i.e., ensembled) version of the base learning algorithm $\\boldsymbol{\\mathcal{A}}$ . The recent work of [SBW24b] ensures that the resulting bagged algorithm has tail stability $(\\varepsilon,\\delta)$ , with $\\begin{array}{r}{\\varepsilon\\asymp\\frac{1}{\\sqrt{n\\delta}}}\\end{array}$ . 2. In Section 3.2, we introduce a new selection rule, the inflated argmax, and establish its $\\varepsilon$ -compatibility. Combined with the bagged algorithm, then, the resulting classification algorithm will be guaranteed to have selection stability $\\delta$ (by Proposition 5). ", "page_idx": 4}, {"type": "text", "text": "Before describing these two steps in detail, we first present our main theorem that characterizes the selection stability guarantee of the resulting procedure. Given sample size $n$ for the training data set $\\mathcal{D}$ , the notation $\\widetilde{A}_{m}$ will denote a bagged version of the base algorithm $\\boldsymbol{\\mathcal{A}}$ , obtained by averaging over subsamp le s of $\\mathcal{D}$ comprised of $m$ data points sampled either with replacement (\u201cbootstrapping\u201d, commonly run with $m=n$ ) or without replacement (\u201csubbagging\u201d, commonly run with $m=n/2$ )\u2014see below for details. ", "page_idx": 4}, {"type": "text", "text": "Theorem 6. Fix any sample size $n$ , any bag size $m$ , and any inflation parameter $\\varepsilon>0$ . For any base learning algorithm $\\mathcal{A}$ , the classification algorithm $\\mathcal{C}=\\mathrm{argmax}^{\\varepsilon}\\circ\\widetilde{A}_{m},$ , obtained by combining the bagged version of $\\boldsymbol{\\mathcal{A}}$ together with the inflated argmax, satisfies selec tion stability $\\delta$ where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\delta=\\varepsilon^{-2}\\cdot\\frac{1-1/L}{n-1}\\cdot\\frac{p_{n,m}}{1-p_{n,m}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{p_{n,m}=1-(1-\\frac{1}{n})^{m}}\\end{array}$ for bootstrapping, and $\\begin{array}{r}{p_{n,m}=\\frac{m}{n}}\\end{array}$ for subbagging. ", "page_idx": 4}, {"type": "text", "text": "The guarantee in Theorem 6 holds for any base learning algorithm $\\boldsymbol{\\mathcal{A}}$ , and applies regardless of the complexity of the feature space $\\mathcal{X}$ , and allows the test feature $x\\in\\mathscr{X}$ to be chosen adversarially. The dependence on the number of classes $L$ is mild\u2014in fact, the tail stability parameter $\\delta$ in (2) differs only by a factor of two for $L=2$ versus $L=\\infty$ . Of course, the guarantee does depend on the choice of the bag size $m$ . In general, a smaller value of $m$ leads to a stronger stability guarantee (since $p_{n,m}$ increases with $m$ ), but this comes at the expense of accuracy since we are training the base algorithm $\\boldsymbol{\\mathcal{A}}$ on subsampled data sets $\\mathcal{D}^{r}$ of size $m$ (rather than $n$ ). For the common choices of $m=n$ for bootstrap or m = n/2 for subbagging, we have1\u2212pnp,nm,m $\\begin{array}{r}{\\frac{p_{n,m}}{1-p_{n,m}}=\\mathcal{O}(1)}\\end{array}$ for each. ", "page_idx": 4}, {"type": "text", "text": "3.1 Bagging classifier weights ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we formally define the construction of a bagged algorithm to recap the tail stability result that our framework leverages. We consider the two most common variants of this ensembling method: bagging (based on bootstrapping training points) and subbagging (based on subsampling). For any data set $\\mathcal{D}\\in(\\mathcal{X}\\times[L])^{n}$ , we can define a subsampled data set of size $m$ as follows: for a sequence $r=(i_{1},\\ldots,i_{m})\\in[n]^{m}$ (which is called a bag), we define the corresponding subsampled data set $\\mathcal{D}^{r}=\\left((X_{i},Y_{i})\\right)_{i\\in r}\\in(\\mathcal{X}\\times[L])^{m}$ . Note that if the bag $r$ contains repeated indices (i.e., $i_{k}=i_{\\ell}$ for some $k\\neq\\ell$ ), then the same data point from the original data set $\\mathcal{D}$ will appear multiple times in $\\mathcal{D}^{r}$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 7 (Bootstrapping or subbagging a base learning algorithm $\\boldsymbol{\\mathcal{A}}$ ). Given a data set $\\mathcal{D}\\in$ $(\\mathcal{X}\\times[L])^{n}$ and some $x\\in\\mathscr{X}$ , return the output $\\begin{array}{r}{\\tilde{\\mathcal{A}}_{m}(\\mathcal{D})(x)=\\mathrm{\\dot{\\mathbb{E}}}_{r}[A(\\mathcal{D}^{r})(x)],}\\end{array}$ , where the expected value is taken with respect to a bag $r$ that is sam pl ed as follows: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Bootstrapping (sometimes simply referred to as bagging) [Bre96a; Bre96b] constructs each bag r by sampling m indices $r=(i_{1},\\ldots,i_{m})$ uniformly with replacement from $[n]$ . \u2022 Subbagging [AEEP02] constructs each bag $r$ by sampling $m\\leq n$ indices $r=(i_{1},\\ldots,i_{m})$ uniformly without replacement from $[n]$ . ", "page_idx": 5}, {"type": "text", "text": "The following result [SBW24b] ensures tail stability for any bootstrapped or subbagged algorithm: ", "page_idx": 5}, {"type": "text", "text": "Theorem 8 ([SBW24b]). For any base learning algorithm $\\boldsymbol{\\mathcal{A}}$ returning outputs in $\\Delta_{L-1}$ , the bagged algorithm $\\widetilde{A}_{m}$ has tail stability $(\\varepsilon,\\delta)$ for any $\\varepsilon,\\delta>0$ satisfying (2). ", "page_idx": 5}, {"type": "text", "text": "Computational cost of bagging. In this section, we have worked with the ideal, derandomized version of bagging for simplicity\u2014that is, we assume that the expected value $\\mathbb{E}_{r}[\\mathcal{A}(\\mathcal{D}^{r})(x)]$ is calculated exactly with respect to the distribution of $r$ . In practice, of course, this is computationally infeasible (since for bootstrapping, say, there are $n^{m}$ possible bags $r$ ), and so we typically resort to Monte Carlo sampling to approximate this expected value, defining $\\begin{array}{r}{\\widetilde{A}_{m}(\\mathcal{D})(x)=\\frac{\\widetilde{1}}{B}\\sum_{b=1}^{B}A(\\mathcal{D}^{r_{b}})(x)}\\end{array}$ , for i.i.d. bags $r_{1},\\dots,r_{B}$ sampled via bootstrapping or subbag g ing. See Appendix C for extensions of our main result, Theorem 6, to the finite- $B$ version of the method. ", "page_idx": 5}, {"type": "text", "text": "3.2 The inflated argmax ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now return to the inflated argmax operator, as defined in Definition 1. This operator, in place of the standard argmax, allows for stability in classification. ", "page_idx": 5}, {"type": "text", "text": "While argmax\u03b5 is defined as an operator on $\\mathbb{R}^{L}$ , in the context of classification algorithms, we only apply it to vectors $w$ lying in the simplex $\\Delta_{L-1}$ (in particular, in the context of a two-stage classification procedure as in Section 1.1, we will apply the inflated argmax to the vector $w=\\hat{p}(x)$ of estimated probabilities). In Figure 1, we visualize the inflated argmax applied to the simplex in a setting with three possible labels, $L=3$ . Each different shaded area corresponds to the region of the simplex $\\Delta_{L-1}$ where argmax ${\\varepsilon}_{}(w)$ returns a particular subset. ", "page_idx": 5}, {"type": "text", "text": "3.2.1 Inflated argmax leads to selection stability ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Unlike the standard argmax, the inflated argmax allows us to achieve stability in the context of classification. This is due to the following theorem, which shows that the inflated argmax is $\\varepsilon$ - compatible, as introduced in Definition 4. (The proofs of this result and all properties of the inflated argmax are deferred to Appendix A.) ", "page_idx": 5}, {"type": "text", "text": "Theorem 9. For any $\\varepsilon>0$ and any $v,w\\in\\mathbb{R}^{L}$ , $i f\\|v\\!-\\!w\\|<\\varepsilon$ then arg $\\operatorname{1ax}^{\\varepsilon}(v)\\cap\\operatorname{argmax}^{\\varepsilon}(w)\\neq\\varnothing$ .   \nIn particular, viewing argmax\u03b5 as a map on the simplex $\\Delta_{L-1}$ , argmax\u03b5 is $\\varepsilon$ -compatible. ", "page_idx": 5}, {"type": "text", "text": "To gain a more concrete understanding of this theorem, we can look again at Figure 1 to examine the case $L=3$ . Theorem 9 ensures that any two regions corresponding to disjoint subsets\u2014e.g., the top corner region corresonding to $\\{2\\}$ , and the bottom center region corresponding to $\\{1,3\\}$ \u2014are distance at least $\\varepsilon$ apart. In particular, the region in the center, corresponding to vectors $w$ that map to the full set $\\{1,2,3\\}$ , is a curved triangle of constant width $\\varepsilon$ , also known as a Reuleaux triangle [Reu76; Wei05]. ", "page_idx": 5}, {"type": "image", "img_path": "M7zNXntzsp/tmp/88b8eaaa91ed3766cb5405d867e24a6b9b4adfb6fd1de136ace1350d8cc2e279.jpg", "img_caption": ["Figure 1: The left plot illustrates the inflated argmax (1) over the simplex $\\Delta_{L-1}$ when $L=3$ . The numbers in brackets correspond to the output of the inflated argmax, argmax $\\varepsilon_{(w)}$ , for various points $w$ in the simplex. The right plot shows the same but for the standard argmax, which corresponds to the limit of $\\operatorname{argmax}^{\\varepsilon}(w)$ as $\\varepsilon\\rightarrow0$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Proof of Theorem 6. With the above results in place, we can now see that the inflated argmax allows us to achieve selection stability, when combined with a bagged algorithm. In particular, combining Theorem 8, Theorem 9, and Proposition 5 immediately implies our main result, Theorem 6. ", "page_idx": 6}, {"type": "text", "text": "3.2.2 Additional properties of the inflated argmax ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The following result establishes some natural properties obeyed by the inflated argmax, and also compares to the standard argmax. ", "page_idx": 6}, {"type": "text", "text": "Proposition 10 (Basic properties of the inflated argmax). Fix any $\\varepsilon\\,>\\,0$ . The inflated argmax operator satisfies the following properties: ", "page_idx": 6}, {"type": "text", "text": "1. (Including the argmax.) For any $w~\\in~\\mathbb{R}^{L}$ , ${\\mathrm{argmax}}(w)\\ \\subseteq\\ {\\mathrm{argmax}}^{\\varepsilon}(w).$ .3 Moreover, $\\cap_{\\varepsilon>0}$ argmax ${\\mathfrak{s}}_{\\mathbf{\\ell}}(w)={\\mathrm{argmax}}(w)$ .   \n2. (Monotonicity in $\\varepsilon.$ .) For any $w\\in\\mathbb{R}^{L}$ and any $\\varepsilon<\\varepsilon^{\\prime}$ , argmax\u03b5(w) \u2286argmax\u03b5\u2032(w).   \n3. (Monotonicity in $w$ .) For any $w\\ \\in\\ \\mathbb{R}^{L}$ , if $w_{j}~\\leq~w_{k}$ , then $j\\;\\in\\;\\mathrm{argmax}^{\\varepsilon}(w)\\;\\Rightarrow\\;k\\;\\in$ argmax $^\\varepsilon(w)$ .   \n4. (Permutation invariance.) For any $v,w\\in\\mathbb{R}^{L}$ , $\\mathbf{\\chi};f v=\\left(w_{\\sigma(1)},\\dots,w_{\\sigma(L)}\\right)$ for some permutation $\\sigma$ on $[L]$ , then $j\\in\\mathrm{argmax}^{\\varepsilon}(v)\\Leftrightarrow\\sigma(j)\\in\\mathrm{argmax}^{\\varepsilon}(\\dot{w}).$ . ", "page_idx": 6}, {"type": "text", "text": "Next, while we have established that inflated argmax offers favorable stability properties, we have not yet asked whether it can be efficiently computed\u2014in particular, it is not immediately clear how to verify the condition $\\mathrm{dist}(w,R_{j}^{\\varepsilon})<\\varepsilon$ in (1), in order to determine whether $j\\in\\mathrm{argmax}^{\\varepsilon}(w)$ . The following result offers an efficient algorithm for computing the inflated argmax set. ", "page_idx": 6}, {"type": "text", "text": "Proposition 11 (Computing the inflated argmax). Fix any $w\\in\\mathbb{R}^{L}$ and $\\varepsilon>0$ . Let $w_{[1]}\\geq\\cdots\\geq w_{[L]}$ denote the order statistics of $w$ , and define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{k}(w)=\\operatorname*{max}\\left\\{k\\in[L]:\\Big(\\sum_{\\ell=1}^{k}(w_{[\\ell]}-w_{[k]})\\Big)^{2}+\\sum_{\\ell=1}^{k}(w_{[\\ell]}-w_{[k]})^{2}\\leq\\varepsilon^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{argmax}^{\\varepsilon}(w)=\\left\\{j\\in[L]:w_{j}>\\frac{\\varepsilon}{\\sqrt{2}}+\\hat{A}_{1}(w)-\\sqrt{\\hat{k}(w)+1}\\sqrt{\\frac{\\varepsilon^{2}}{\\hat{k}(w)}+(\\hat{A}_{1}(w))^{2}-\\hat{A}_{2}(w)}\\right\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "3In this result, $\\operatorname{argmax}(w)$ should be interpreted as the set of $a l l$ maximizing entries of $w$ \u2014i.e., in the case of ties, argmax $(w)$ may not be a singleton set. ", "page_idx": 6}, {"type": "image", "img_path": "M7zNXntzsp/tmp/d6c9d967af71e8466249d9ad30428201fffd126dd3ce8d7ceba13b3a4599c3b7.jpg", "img_caption": ["Figure 2: Results on the Fashion MNIST data set. The figure shows the instability $\\delta_{j}$ (defined in (4)) over all test points $j=1,\\ldots,N$ . The curves display the fraction of $\\delta_{j}$ \u2019s that exceed $\\delta$ , for each value $\\delta\\in[0,1]$ . The vertical axis is on a log scale. See Section 4 for details. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Finally, thus far we have established that argmax\u03b5 enables us to achieve stable classification with a computationally efficient selection rule\u2014but we do not yet know whether argmax\u03b5 is optimal, or whether some other choice of $s$ might lead to smaller output sets $\\hat{S}$ (while still offering assumptionfree stability). For instance, we might consider a fixed-margin rule, ", "page_idx": 7}, {"type": "equation", "text": "$$\ns_{\\mathrm{margin}}^{\\varepsilon}(w)=\\{j:w_{j}>\\operatorname*{max}_{\\ell}w_{\\ell}-\\varepsilon/\\sqrt{2}\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for which $\\varepsilon$ -compatibility also holds\u2014might this simpler rule be better than the inflated argmax? Our next result establishes that\u2014under some natural constraints\u2014argmax\u03b5 is in fact the optimal choice of selection rule, in the sense of returning a singleton set (i.e., $|\\bar{S}|=1\\$ ) as often as possible. ", "page_idx": 7}, {"type": "text", "text": "Proposition 12 (Optimality of the inflated argmax). Let $s:\\Delta_{L-1}\\to\\wp([L])$ be any selection rule. Suppose s is $\\varepsilon$ -compatible (Definition 4), permutation invariant (in the sense of Proposition $I O$ ), and contains the argmax. Then for any $w\\in\\Delta_{L-1}$ and any $j\\in[L]$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\ns(w)=\\{j\\}\\implies\\mathrm{argmax}^{\\varepsilon}(w)=\\{j\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In other words, for any selection rule $s$ satisfying the assumptions of the proposition (which includes the fixed-margin rule, $s_{\\mathrm{margin}}^{\\varepsilon})$ , if $s(w)$ is a singleton set then so is argmax $^\\varepsilon(w)$ . (See also Appendix D for a closer comparison between the inflated argmax and the fixed-margin selection rule given in (3).) ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate our proposed pipeline, combining subbagging with the inflated argmax, with deep learning models and on a common benchmark data set.4 ", "page_idx": 7}, {"type": "text", "text": "Data and models. We use Fashion-MNIST [XRV17], which consists of $n=60,000$ training pairs $(X_{i},Y_{i})$ , $N=10,000$ test pairs $(\\tilde{X}_{j},\\tilde{Y}_{j})$ , and $L=10$ classes. For each data point $(X,Y)$ , $X$ is a $28\\times28$ grayscale image that pictures a clothing item, and $Y\\in[L]$ indicates the type of item, e.g., a dress, a coat, etc. The base model we use is a variant of LeNet-5, implemented in PyTorch [PGML19] tutorials as GarmentClassifier(). The base algorithm $\\boldsymbol{\\mathcal{A}}$ trains this classifier using 5 epochs of stochastic gradient descent. ", "page_idx": 7}, {"type": "text", "text": "Methods and evaluation. We compare four methods: ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1. The argmax of the base learning algorithm $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 7}, {"type": "table", "img_path": "M7zNXntzsp/tmp/ac63830eaeba6271f0872b6363ea15847fe44e7c443d723084fe5567f4523103.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 1: Results on the Fashion MNIST data set. We display the frequency of returning the correct label as a singleton, $\\beta_{\\mathrm{correct-single}}$ and the average size, $\\beta_{\\mathrm{set-size}}$ , both of which are defined in (6). Standard errors for these averages are shown in parentheses. The final column shows the worst-case instability over the test set, $\\beta_{\\mathrm{max}}$ -instability, defined in (5). For each metric, the symbol $\\nearrow$ indicates that higher values are desirable, while $\\searrow$ indicates that lower values are desirable. ", "page_idx": 8}, {"type": "text", "text": "2. The $\\varepsilon$ -inflated argmax of the base learning algorithm $\\boldsymbol{\\mathcal{A}}$ with tolerance $\\varepsilon=.05$ . 3. The argmax of the subbagged algorithm $\\widetilde{A}_{m}$ , with $B=1,000$ bags of size $m=n/2$ . 4. The $\\varepsilon$ -inflated argmax of the subbagged algorithm $\\widetilde{A}_{m}$ , with $B\\:=\\:1,000$ bags of size $m=n/2$ and tolerance $\\varepsilon=.05$ . ", "page_idx": 8}, {"type": "text", "text": "We evaluate each method based on several metrics. First, to assess selection stability, for each test point $j=1,\\ldots,N$ we compute its instability ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\delta_{j}:=\\frac{1}{500}\\sum_{k=1}^{500}{\\mathbf1}\\left\\{s(\\hat{p}(\\tilde{X}_{j}))\\cap s(\\hat{p}^{\\backslash i_{k}}(\\tilde{X}_{j}))=\\emptyset\\right\\},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $i_{1},\\ldots,i_{500}$ are sampled uniformly without replacement from $[n]$ (i.e., we are sampling 500 leave-one-out models $\\hat{p}^{\\backslash i}$ to compare to the model $\\hat{p}$ trained on the full training set). Since our theory concerns worst-case instability over all possible test points, we evaluate the maximum instability ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\beta_{\\mathrm{max-instability}}:=\\operatorname*{max}_{j\\in[N]}\\delta_{j}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Second, to evaluate how accurate each method is, we compute how often the method returns the correct label as a singleton $\\beta_{\\mathrm{correct-single}}$ and set size $\\beta_{\\mathrm{set-size}}$ (the number of labels in the candidate set): ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\beta_{\\mathrm{correct-single}}:=\\frac{1}{N}\\sum_{j=1}^{N}\\mathbf{1}\\left\\{s(\\hat{p}(\\tilde{X}_{j}))=\\{\\widetilde{Y}_{j}\\}\\right\\},\\quad\\beta_{\\mathrm{set\\cdotsize}}:=\\frac{1}{N}\\sum_{j=1}^{N}\\left|s(\\hat{p}(\\tilde{X}_{j}))\\right|.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Ideally we would want a method to return the correct singleton as frequently as possible (a large value of $\\beta_{\\mathrm{correct-single}}\\in[0,1]$ that is close to 1) and small set size (a value of $\\beta_{\\mathrm{set-size}}\\geq1$ that is close to 1). ", "page_idx": 8}, {"type": "text", "text": "Results. In Figure 2, we present results for the instability of each method, plotting the survival function of the instability for all test points $(\\delta_{j})_{j\\in[N]}$ . The standard argmax applied to the base algorithm, argmax $\\circ A$ , has the longest tail, meaning $\\delta_{j}$ is large for many test points $j$ . The inflated argmax applied to the base algorithm, argmax\u03b5 $\\circ A$ , offers only a very small improvement on the stability. By contrast, applying the standard argmax to the subbagged algorithm, argmax $\\circ\\widetilde{A}_{m}$ , offers a much more substantial improvement, since the red dashed curve is much smaller tha n both of the solid curves. Still, some of the largest $\\delta_{j}$ for this method are near 1, meaning for these test points the returned set is sensitive to dropping a single training instance. Combining the inflated argmax with subbagging, $\\mathrm{argmax}^{\\varepsilon}\\circ\\widetilde{A}_{m}$ , offers a dramatic improvement: in this case $\\delta_{j}=0$ for every $j=1,\\ldots,N$ . ", "page_idx": 8}, {"type": "text", "text": "In Table 1, we present the average measures, $\\beta_{\\mathrm{correct-single}}$ and $\\beta_{\\mathrm{set-size}}$ and the worst-case measure $\\beta_{\\mathrm{max}}$ -instability. For both the base algorithm and the subbagged algorithm, applying the inflated argmax incurs a small cost both in terms of the frequency of returning the correct label as a singleton and the average size. The inflated argmax increases set size only very slightly, but when combined with subbagging, we improve upon the $\\beta_{\\mathrm{correct-single}}$ of the original method and achieve an extremely high level of stability $/\\beta_{\\mathrm{max-instability}}=0)$ ), while returning a singleton set in the vast majority of cases. ", "page_idx": 8}, {"type": "text", "text": "We extend our experiment in Appendix E. In particular, we compare the inflated argmax to some existing methods for set-valued classification, including simple thresholding and top- $K$ rules as well as more involved methods, all of which are Bayes rules for various utility functions [MWDH21]. We also evaluate each method using some additional metrics from set-valued classification, including utility-discounted predictive accuracy [ZCM12]. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Related work. There is an enormous literature on set-valued classification, so we only reference some of the most relevant works. Much prior work has considered the possibility of a more relaxed argmax to allow for uncertainty [Bri90; BV16; MA16; BMN20; ZLYW23]. The more recent papers in this line of work have focused on producing a sparse set of weights, but none of these works offer a formal stability guarantee for the support of the weights. Our work is the first to propose a version of the argmax that can provably control a notion of stability for the classification setting. ", "page_idx": 9}, {"type": "text", "text": "Our definition of selection stability relies on a notion of consistency between sets\u2014two sets of candidate labels are consistent if they have at least one common element. This is similar in flavor to conformal classification [Lei14; SLW19], where a set of candidate values is valid if it contains the correct label; this method does not rely on any distributional assumptions, and consequently has been applied to complex and high-dimensional applications such as generative language models [QFSY23]. These frameworks share the motivation of avoiding \u2018overconfidence in the assignment of a definite label\u2019 [HPW18]. ", "page_idx": 9}, {"type": "text", "text": "Overview, limitations and open questions. We prove a guarantee on the selection stability based on our methodology combining bagging with the inflated argmax. Theorem 6 does not place any assumptions on the learning algorithm nor on the data, including any distributional assumptions. In fact, the training data and test point may be chosen adversarially based on the algorithm, and the output will still be stable. Moreover, we do not assume that the sample size is large: the guarantee holds for any fixed training set size $n$ and improves as $n$ increases. Furthermore, the inflated argmax selection rule ensures that the returned sets of candidate labels are as small as possible (i.e., that there is as little ambiguity as possible about the predicted label for any given test point $x$ ). ", "page_idx": 9}, {"type": "text", "text": "While our theorem does not require assumptions, our method does require bagging the base learning algorithm. The main limitation of our work is that bagging is computationally intensive. However, training different bags is easily parallelizable, which is what allowed us to easily train a convolutional neural network on $B\\:=\\:1,000$ total subsets of the training data. Moreover, while the original definition of bagging uses the conventional bootstrap, where each bag contains as many samples as the original data set, i.e. $m=n$ , in our framework we allow for arbitrary bag size $m$ , which could be much smaller than the sample size $n$ . Massively subsampling the data $(m\\ll n)$ ) can actually help scale learning algorithms to large data sets [KTSJ14]. Moreover, bagging with $m=n$ can be expensive, but there are still many areas of machine learning where it is used, notably in Random Forests. Finally, our experiments also show that a modest number of bags $(B=1,000)$ is all that we really need to start seeing major gains in selection stability. ", "page_idx": 9}, {"type": "text", "text": "We leave open several avenues for future work. Practitioners may be interested in other forms of discrete stability, which are more strict than requiring one common element between $\\hat{S}$ and $\\hat{S}^{\\backslash i}$ . For instance, one popular way to measure set similarity is the Jaccard index, given by | S\u02c6\u2229S\u02c6\\i|. Another open problem is to extend the stability framework and methods studied here to more structured discrete outputs, such as in clustering and variable selection, where nontrivial metrics on the output are more common. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We gratefully acknowledge the support of the National Science Foundation via grant DMS-2023109 DMS-2023109. R.F.B. and J.A.S. gratefully acknowledge the support of the Office of Naval Research via grant N00014-20-1-2337. R.M.W. gratefully acknowledges the support of the NSF-Simons National Institute for Theory and Mathematics in Biology (NSF DMS-2235451, Simons Foundation MP-TMPS-00005320), and AFOSR FA9550-18-1-0166. The authors thank Melissa Adrian for help with running the experiments. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[AB23] Anastasios N Angelopoulos, Stephen Bates, et al. \u201cConformal prediction: A gentle introduction\u201d. In: Foundations and Trends\u00ae in Machine Learning 16.4 (2023), pp. 494\u2013 591.   \n[AEEP02] Savina Andonova et al. \u201cA simple algorithm for learning stable machines\u201d. In: European Conference on Artificial Intelligence (ECAI). 2002, pp. 513\u2013517.   \n[BE02] Olivier Bousquet and Andr\u00e9 Elisseeff. \u201cStability and generalization\u201d. In: The Journal of Machine Learning Research 2 (2002), pp. 499\u2013526.   \n[BMN20] Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. \u201cLearning with FenchelYoung losses\u201d. In: J. Mach. Learn. Res. 21 (2020), Paper No. 35, 69. ISSN: 1532- 4435,1533-7928.   \n[Bre96a] Leo Breiman. \u201cBagging predictors\u201d. In: Machine learning 24.2 (1996), pp. 123\u2013140.   \n[Bre96b] Leo Breiman. \u201cHeuristics of instability and stabilization in model selection\u201d. In: The Annals of Statistics 24.6 (1996), pp. 2350\u20132383.   \n[Bri90] John S. Bridle. \u201cProbabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition\u201d. In: Neurocomputing (Les Arcs, 1989). Vol. 68. NATO Adv. Sci. Inst. Ser. F: Comput. Systems Sci. Springer, Berlin, 1990, pp. 227\u2013236. ISBN: 3-540-53278-1.   \n[BV16] Alexandre de Br\u00e9bisson and Pascal Vincent. \u201cAn exploration of softmax alternatives belonging to the spherical loss family\u201d. In: International Conference on Learning Representations (ICLR). 2016.   \n[CDHL21] Evgenii Chzhen et al. \u201cSet-valued classification\u2013overview via a unified framework\u201d. In: arXiv preprint arXiv:2102.12318 (2021).   \n[CMX11] Constantine Caramanis, Shie Mannor, and Huan Xu. \u201cRobust optimization in machine learning\u201d. In: (2011).   \n[DDB09] Juan Jos\u00e9 Del Coz, Jorge D\u00edez, and Antonio Bahamonde. \u201cLearning Nondeterministic Classifiers.\u201d In: J. Mach. Learn. Res. 10.10 (2009).   \n[FSVC19] Sorelle A Friedler et al. \u201cA comparative study of fairness-enhancing interventions in machine learning\u201d. In: Proceedings of the conference on fairness, accountability, and transparency. 2019, pp. 329\u2013338.   \n[Gry93] Eugen Grycko. \u201cClassification with set-valued decision functions\u201d. In: Information and Classification: Concepts, Methods and Applications Proceedings of the 16th Annual Conference of the \u201cGesellschaft f\u00fcr Klassifikation eV\u201d University of Dortmund, April 1\u20133, 1992. Springer. 1993, pp. 218\u2013224.   \n[HPW18] Yotam Hechtlinger, Barnab\u00e1s P\u00f3czos, and Larry Wasserman. \u201cCautious deep learning\u201d. In: arXiv preprint arXiv:1805.09460 (2018).   \n[HV19] Lingxiao Huang and Nisheeth Vishnoi. \u201cStable and fair classification\u201d. In: International Conference on Machine Learning. PMLR. 2019, pp. 2879\u20132890.   \n[KTSJ14] Ariel Kleiner et al. \u201cA scalable bootstrap for massive data\u201d. In: Journal of the Royal Statistical Society: Series B: Statistical Methodology (2014), pp. 795\u2013816.   \n[Lei14] Jing Lei. \u201cClassification with confidence\u201d. In: Biometrika 101.4 (2014), pp. 755\u2013 769. ISSN: 0006-3444,1464-3510. DOI: 10.1093/biomet/asu038. URL: https: //doi.org/10.1093/biomet/asu038.   \n[MA16] Andre Martins and Ramon Astudillo. \u201cFrom softmax to sparsemax: A sparse model of attention and multi-label classification\u201d. In: International conference on machine learning. PMLR. 2016, pp. 1614\u20131623.   \n[MSKA19] W James Murdoch et al. \u201cDefinitions, methods, and applications in interpretable machine learning\u201d. In: Proceedings of the National Academy of Sciences 116.44 (2019), pp. 22071\u201322080.   \n[MWDH21] Thomas Mortier et al. \u201cEfficient set-valued prediction in multi-class classification\u201d. In: Data Mining and Knowledge Discovery 35.4 (2021), pp. 1435\u20131469.   \n[NDMH18] VL Nguyen et al. \u201cReliable multi-class classification based on pairwise epistemic and aleatoric uncertainty\u201d. In: 27th international joint conference on artificial intelligence (IJCAI). 2018, pp. 5089\u20135095.   \n[PGML19] Adam Paszke et al. \u201cPyTorch: An imperative style, high-performance deep learning library\u201d. In: Advances in neural information processing systems 32 (2019).   \n[PPVG02] Harris Papadopoulos et al. \u201cInductive confidence machines for regression\u201d. In: Machine learning: ECML 2002: 13th European conference on machine learning Helsinki, Finland, August 19\u201323, 2002 proceedings 13. Springer. 2002, pp. 345\u2013356.   \n[QFSY23] Victor Quach et al. \u201cConformal language modeling\u201d. In: arXiv preprint arXiv:2306.10193 (2023).   \n[Reu76] F Reuleaux. Kinematics of machinery: outlines of a theory of machines. Reprinted as \u201cThe Kinematics of Machinery\u201d. New York: Dover, 1876.   \n[SBW24a] Jake A. Soloff, Rina Foygel Barber, and Rebecca Willett. \u201cBagging Provides Assumption-free Stability\u201d. In: Journal of Machine Learning Research 25.131 (2024), pp. 1\u201335. URL: http://jmlr.org/papers/v25/23-0536.html.   \n[SBW24b] Jake A. Soloff, Rina Foygel Barber, and Rebecca Willett. \u201cStability via resampling: statistical problems beyond the real line\u201d. In: arXiv preprint arXiv:2405.09511 (2024).   \n[SLW19] Mauricio Sadinle, Jing Lei, and Larry Wasserman. \u201cLeast ambiguous set-valued classifiers with bounded error levels\u201d. In: J. Amer. Statist. Assoc. 114.525 (2019), pp. 223\u2013 234. ISSN: 0162-1459,1537-274X. DOI: 10.1080/01621459.2017.1395341. URL: https://doi.org/10.1080/01621459.2017.1395341.   \n[VGS05] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world. Springer, New York, 2005, pp. xv $\\pm324$ .   \n[Wei05] Eric W Weisstein. \u201cReuleaux Triangle\u201d. In: MathWorld\u2013A Wolfram Web Resource (2005). https://mathworld.wolfram.com/ReuleauxTriangle.html.   \n[XRV17] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. 2017. arXiv: 1708.07747 [cs.LG].   \n[YB24] Bin Yu and Rebecca Barter. Veridical Data Science: The Practice of Responsible Data Analysis and Decision Making. MIT Press, 2024.   \n[YK20] Bin Yu and Karl Kumbier. \u201cVeridical data science\u201d. In: Proceedings of the National Academy of Sciences 117.8 (2020), pp. 3920\u20133929.   \n[ZCM12] Marco Zaffalon, Giorgio Corani, and Denis Mau\u00e1. \u201cEvaluating credal classifiers by utility-discounted predictive accuracy\u201d. In: Internat. J. Approx. Reason. 53.8 (2012), pp. 1282\u20131301. ISSN: 0888-613X,1873-4731. DOI: 10.1016/j.ijar.2012.06.022. URL: https://doi.org/10.1016/j.ijar.2012.06.022.   \n[ZLYW23] Shuai Zhao et al. \u201cFrom softmax to nucleusmax: A novel sparse language model for chinese radiology report summarization\u201d. In: ACM Transactions on Asian and Low-Resource Language Information Processing 22.6 (2023), pp. 1\u201321. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs of properties of the inflated argmax ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we prove all the results of Section 3.2, establishing the various properties of the inflated argmax. In fact, all the proofs will rely on the following theorem, which gives an alternative characterization of the set argmax $\\varepsilon_{}(w)$ . ", "page_idx": 12}, {"type": "text", "text": "Theorem 13. Fix $\\varepsilon>0,$ . Define the map $c_{\\varepsilon}:\\mathbb{R}^{L}\\to\\mathbb{R}$ as ", "page_idx": 12}, {"type": "equation", "text": "$$\nc_{\\varepsilon}(w){\\mathrm{~is~the~unique~solution~to~}}\\left(\\sum_{j\\in[L]}(w_{j}-c)_{+}\\right)^{2}+\\sum_{j\\in[L]}(w_{j}-c)_{+}^{2}=\\varepsilon^{2},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and define the map $t_{\\varepsilon}:\\mathbb{R}^{L}\\to\\mathbb{R}$ as ", "page_idx": 12}, {"type": "equation", "text": "$$\nt_{\\varepsilon}(w)=c_{\\varepsilon}(w)+\\varepsilon/\\sqrt{2}-\\sum_{j\\in[L]}(w_{j}-c_{\\varepsilon}(w))_{+}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then for any $w\\in\\mathbb{R}^{L}$ , it holds that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname{argmax}^{\\varepsilon}(w)=\\{j\\in[L]:w_{j}>t_{\\varepsilon}(w)\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Moreover, $t_{\\varepsilon}(w)\\leq c_{\\varepsilon}(w)$ for all $w$ and all $\\varepsilon$ , and $t_{\\varepsilon}(w)$ and $c_{\\varepsilon}(w)$ are both nondecreasing functions of $\\varepsilon$ , and both nondecreasing functions of $w$ (i.e., if $w\\le w^{\\prime}$ holds coordinatewise, then $c_{\\varepsilon}(w)\\leq$ $c_{\\varepsilon}(w^{\\prime})$ and $t_{\\varepsilon}(w)\\leq t_{\\varepsilon}(w^{\\prime})\\}$ ). ", "page_idx": 12}, {"type": "text", "text": "With this key result in place, we are now ready to turn to the proofs of the results stated in Section 3.2. Throughout all the proofs below, the functions $c_{\\varepsilon}(w)$ and $t_{\\varepsilon}(w)$ are defined as in the statement of Theorem 13. ", "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Theorem 9 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Let $w,w^{\\prime}\\in\\mathbb{R}^{L}$ be any vectors with arg $\\operatorname{\\mathrm{nax}}^{\\varepsilon}(w)\\cap\\operatorname{argmax}^{\\varepsilon}(w^{\\prime})=\\varnothing$ . Define ", "page_idx": 12}, {"type": "equation", "text": "$$\nB=\\{j\\in[L]:w_{j}>c_{\\varepsilon}(w)\\},\\quad B^{\\prime}=\\{j\\in[L]:w_{j}^{\\prime}>c_{\\varepsilon}(w^{\\prime})\\},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and note that we must have $B\\subseteq\\operatorname{argmax}^{\\varepsilon}(w)$ since for $j\\not\\in\\mathrm{argmax}^{\\varepsilon}(w)$ , $w_{j}\\leq t_{\\varepsilon}(w)\\leq c_{\\varepsilon}(w)$ by Theorem 13. Similarly, $B^{\\prime}\\subseteq\\operatorname{argmax}^{\\varepsilon}(w^{\\prime})$ . We also must have $B$ (and similarly $B^{\\prime}$ ) nonempty by definition of $c_{\\varepsilon}(w)$ . ", "page_idx": 12}, {"type": "text", "text": "For convenience, define $c=c_{\\varepsilon}(w),c^{\\prime}=c_{\\varepsilon}(w^{\\prime}),t=t_{\\varepsilon}(w),t^{\\prime}=t_{\\varepsilon}(w^{\\prime})$ . We then have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|w-w^{\\prime}\\|^{2}\\geq\\|w_{B}-w_{B}^{\\prime}\\|^{2}+\\|w_{B^{\\prime}}^{\\prime}-w_{B^{\\prime}}\\|^{2}}\\\\ &{=\\|\\big(w_{B}-c\\mathbf{1}_{B}\\big)+\\big(t^{\\prime}\\mathbf{1}_{B}-w_{B}^{\\prime}\\big)+(c-t^{\\prime})\\mathbf{1}_{B}\\|^{2}+\\|\\big(w_{B^{\\prime}}^{\\prime}-c^{\\prime}\\mathbf{1}_{B^{\\prime}}\\big)+\\big(t\\mathbf{1}_{B^{\\prime}}-w_{B^{\\prime}}\\big)+\\big(c^{\\prime}-t\\big)\\mathbf{1}_{B^{\\prime}}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the first step holds since $B\\cap B^{\\prime}=\\mathcal{O}$ due to argma $\\mathrm{s}^{\\varepsilon}(w)\\cap\\mathrm{argmax}^{\\varepsilon}(w^{\\prime})=\\emptyset$ . Next we will need a technical lemma. ", "page_idx": 12}, {"type": "text", "text": "Lemma 14. Let $k,k^{\\prime}\\geq1$ be integers and let $\\varepsilon>0$ . Then for any $x,y\\in\\mathbb{R}_{\\ge0}^{k}$ , $x^{\\prime},y^{\\prime}\\in\\mathbb{R}_{\\geq0}^{k^{\\prime}}$ R 0 and , $r,r^{\\prime}\\in\\mathbb{R},$ if ", "page_idx": 12}, {"type": "equation", "text": "$$\n(x^{\\top}\\mathbf{1}_{k})^{2}+\\|x\\|^{2}=(x^{\\prime\\top}\\mathbf{1}_{k^{\\prime}})^{2}+\\|x^{\\prime}\\|^{2}=\\varepsilon^{2}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\boldsymbol{x}^{\\top}\\mathbf{1}_{k}+\\boldsymbol{x}^{\\prime\\top}\\mathbf{1}_{k^{\\prime}}=\\boldsymbol{r}+\\boldsymbol{r}^{\\prime}+\\varepsilon\\sqrt{2},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|x+y+r\\mathbf{1}_{k}\\|^{2}+\\|x^{\\prime}+y^{\\prime}+r^{\\prime}\\mathbf{1}_{k^{\\prime}}\\|^{2}\\geq\\varepsilon^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "To apply this lemma, first we let $k=|B|\\geq1$ and $k^{\\prime}=|B^{\\prime}|\\geq1$ , and define ", "page_idx": 12}, {"type": "equation", "text": "$$\nx=w_{B}-c{\\bf1}_{B},\\ y=t^{\\prime}{\\bf1}_{B}-w_{B}^{\\prime},\\ x^{\\prime}=w_{B^{\\prime}}^{\\prime}-c^{\\prime}{\\bf1}_{B^{\\prime}},\\ y^{\\prime}=t{\\bf1}_{B^{\\prime}}-w_{B^{\\prime}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and note that all these vectors have nonnegative entries (specifically, $x$ and $x^{\\prime}$ are nonnegative by definition of $B$ and $B^{\\prime}$ , while $y$ and $y^{\\prime}$ are nonnegative by Theorem 13, using the fact that ", "page_idx": 12}, {"type": "text", "text": "$B\\cap s(w^{\\prime})=B^{\\prime}\\cap s(w)=\\emptyset$ . Then the condition (8) is satisfied by definition of $c=c_{\\varepsilon}(w)$ and $c^{\\prime}=c_{\\varepsilon}(w^{\\prime})$ . Define also ", "page_idx": 13}, {"type": "equation", "text": "$$\nr=c-t^{\\prime},\\ r^{\\prime}=c^{\\prime}-t.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and note that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r+r^{\\prime}=(c-t)+(c^{\\prime}-t^{\\prime})=\\displaystyle\\sum_{j\\in[L]}(w_{j}-c)_{+}-\\varepsilon/\\sqrt{2}+\\displaystyle\\sum_{j\\in[L]}(w_{j}^{\\prime}-c^{\\prime})_{+}-\\varepsilon/\\sqrt{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=x^{\\top}{\\bf1}_{k}+x^{\\prime^{\\top}}{\\bf1}_{k^{\\prime}}-\\varepsilon\\sqrt{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the second step holds by definition of $t\\,=\\,t_{\\varepsilon}(w)$ and $t^{\\prime}=\\,t_{\\varepsilon}(w^{\\prime})$ . Therefore, (9) is also satisfied. Returning to (7), Lemma 14 then implies that $\\|\\boldsymbol{w}-\\boldsymbol{w}^{\\prime}\\|^{2}\\geq\\varepsilon^{2}$ , which completes the proof of the theorem. ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Proposition 10 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "First we verify that ${\\mathrm{argmax}}(w)\\,\\subseteq\\,{\\mathrm{argmax}}^{\\varepsilon}(w)$ . Let $j\\,\\in\\,\\mathrm{argmax}(w)$ , i.e., $w_{j}\\,=\\,\\operatorname*{max}_{\\ell}\\,w_{\\ell}$ . Let $v=w+\\mathbf{e}_{j}\\cdot\\varepsilon/\\sqrt{2}$ , where ${\\bf e}_{j}=(0,\\dots,0,1,0,\\dots,0)$ is the $j$ th canonical basis vector. Then $v\\in R_{j}^{\\varepsilon}$ , and $\\|w-v\\|=\\varepsilon/{\\sqrt{2}}<\\varepsilon$ , so we have $\\mathrm{dist}(w,R_{j}^{\\varepsilon})<\\varepsilon$ and therefore $j\\in\\mathrm{argmax}^{\\varepsilon}(w)$ . We also need to verify that $\\cap_{\\varepsilon>0}\\arg\\!\\operatorname*{max}^{\\varepsilon}(w)=\\arg\\!\\operatorname*{max}({\\bar{w}})$ , i.e., for $j\\not\\in\\mathrm{argmax}(w)$ , there is some $\\varepsilon>0$ with $j\\not\\in\\mathrm{argmax}^{\\varepsilon}(w)$ . Let $k\\in[L]$ be an index with $w_{k}>w_{j}$ , and let $\\varepsilon=\\sqrt{2}(w_{k}-w_{j})$ . Then for any $v\\in R_{j}^{\\varepsilon}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|w-v\\|^{2}\\geq(w_{j}-v_{j})^{2}+(w_{k}-v_{k})^{2}=(w_{k}-\\varepsilon/\\sqrt{2}-v_{j})^{2}+(w_{k}-v_{k})^{2}}\\\\ {\\geq\\displaystyle\\operatorname*{inf}_{t\\in\\mathbb{R}}\\bigg\\{\\bigg(t-(\\varepsilon/\\sqrt{2}+(v_{j}-v_{k}))\\bigg)^{2}+t^{2}\\bigg\\}=\\frac{1}{2}(\\varepsilon/\\sqrt{2}+(v_{j}-v_{k}))^{2}\\geq\\varepsilon^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last step holds since $v_{j}\\,-\\,v_{k}\\,\\geq\\,\\varepsilon/{\\sqrt{2}}$ . This means that $\\mathrm{dist}(w,R_{j}^{\\varepsilon})\\,\\geq\\,\\varepsilon$ and so $j\\not\\in$ argmax $\\varepsilon_{(w)}$ at this value of $\\varepsilon>0$ . ", "page_idx": 13}, {"type": "text", "text": "Next we check monotonicity in $\\varepsilon$ . Fix $\\varepsilon<\\varepsilon^{\\prime}$ . Then for any $j\\in[L]$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\nj\\in\\mathop{\\mathrm{argmax}}^{\\varepsilon}(\\varepsilon)\\iff w_{j}>t_{\\varepsilon}(w)\\implies w_{j}>t_{\\varepsilon^{\\prime}}(w)\\iff j\\in\\mathop{\\mathrm{argmax}}^{\\varepsilon^{\\prime}}(w),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where each step holds by Theorem 13. ", "page_idx": 13}, {"type": "text", "text": "Next we verify monotonicity in $w$ . If $w_{j}\\leq w_{k}$ , then by Theorem 13 ", "page_idx": 13}, {"type": "equation", "text": "$$\nj\\in\\mathop{\\mathrm{argmax}}^{\\varepsilon}(w)\\iff w_{j}>t_{\\varepsilon}(w)\\implies w_{k}>t_{\\varepsilon}(w)\\iff k\\in\\mathop{\\mathrm{argmax}}^{\\varepsilon}(w).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally we check permutation invariance. Suppose $v=(w_{\\sigma(1)},\\ldots,w_{\\sigma(L)})$ is a permutation of $w$ . Then by construction, we have $c_{\\varepsilon}(v)=c_{\\varepsilon}(w)$ and $t_{\\varepsilon}(v)=t_{\\varepsilon}(w)$ . In particular, ", "page_idx": 13}, {"type": "equation", "text": "$$\nj\\in\\mathrm{argmax}^{\\varepsilon}(v)\\iff v_{j}>t_{\\varepsilon}(v)\\iff w_{\\sigma(j)}>t_{\\varepsilon}(w)\\iff\\sigma(j)\\in\\mathrm{argmax}^{\\varepsilon}(w).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.3 Proof of Proposition 11 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Define a function $f_{w}$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\nf_{w}(c)=\\left(\\sum_{j\\in[L]}(w_{j}-c)_{+}\\right)^{2}+\\sum_{j\\in[L]}(w_{j}-c)_{+}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then $\\hat{k}(w)$ can equivalently be defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{k}(w)=\\operatorname*{max}\\{k\\in[L]:f_{w}(w_{[k]})\\leq\\varepsilon^{2}\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(Note that this maximum is well defined, since $f_{w}(w_{[1]})\\,=\\,0$ and so the set is nonempty.) Since $c\\mapsto f_{w}(c)$ is strictly decreasing over $c\\leq\\,w_{[1]}$ , we see that the solution $c_{\\varepsilon}(w)$ to the equation ", "page_idx": 13}, {"type": "text", "text": "$f_{w}(c)=\\varepsilon^{2}$ must satisfy $c_{\\varepsilon}(w)\\leq w_{[\\hat{k}(w)]}$ , and (if $\\hat{k}(w)<L)$ also $c_{\\varepsilon}(w)>w_{[\\hat{k}(w)+1]}.$ . In particular, this implies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\varepsilon^{2}=\\left(\\displaystyle\\sum_{j\\in[L]}(w_{j}-c_{\\varepsilon}(w))_{+}\\right)^{2}+\\displaystyle\\sum_{j\\in[L]}(w_{j}-c_{\\varepsilon}(w))_{+}^{2}}\\\\ {=\\left(\\displaystyle\\sum_{j=1}^{k(w)}(w_{j}-c_{\\varepsilon}(w))\\right)^{2}+\\displaystyle\\sum_{j=1}^{k(w)}(w_{j}-c_{\\varepsilon}(w))^{2}}\\\\ {=\\left(\\hat{k}(w)\\cdot(\\hat{A_{1}}(w)-c_{\\varepsilon}(w))\\right)^{2}+\\left(\\hat{k}(w)\\hat{A_{2}}(w)-2\\hat{k}(w)\\hat{A_{1}}(w)c_{\\varepsilon}(w)+\\hat{k}(w)c_{\\varepsilon}(w)^{2}\\right)}\\\\ {=\\hat{k}(w)(\\hat{k}(w)+1)\\left(c_{\\varepsilon}(w)^{2}-2c_{\\varepsilon}(w)\\hat{A_{1}}(w)+\\displaystyle\\frac{\\hat{k}(w)(\\hat{A_{1}}(w))^{2}+\\hat{A_{2}}(w)}{\\hat{k}(w)+1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This is a quadratic function of $c_{\\varepsilon}(w)$ , and is solved by ", "page_idx": 14}, {"type": "equation", "text": "$$\nc_{\\varepsilon}(w)=\\hat{A}_{1}-\\sqrt{\\frac{(\\hat{A}_{1}(w))^{2}-\\hat{A}_{2}(w)+\\frac{\\varepsilon^{2}}{\\hat{k}(w)}}{\\hat{k}(w)+1}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We also have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle t_{\\varepsilon}(w)=c_{\\varepsilon}(w)+\\varepsilon/\\sqrt{2}-\\sum_{j\\in[L]}(w_{j}-c_{\\varepsilon}(w))_{+}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}=c_{\\varepsilon}(w)+\\varepsilon/\\sqrt{2}-\\sum_{j=1}^{\\hat{k}(w)}(w_{[j]}-c_{\\varepsilon}(w))}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}=(\\hat{k}(w)+1)c_{\\varepsilon}(w)+\\frac{\\varepsilon}{\\sqrt{2}}-\\hat{k}(w)\\hat{A}_{1}(w).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Plugging in our above expression for $c_{\\varepsilon}(w)$ , then, ", "page_idx": 14}, {"type": "equation", "text": "$$\nt_{\\varepsilon}(w)=\\frac{\\varepsilon}{\\sqrt{2}}+\\hat{A}_{1}(w)-\\sqrt{\\hat{k}(w)+1}\\sqrt{(\\hat{A}_{1}(w))^{2}-\\hat{A}_{2}(w)+\\frac{\\varepsilon^{2}}{\\hat{k}(w)}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since argmax ${\\v{e}}\\left(w\\right)=\\left\\{j:w_{j}>t_{\\varepsilon}(w)\\right\\}$ by Theorem 13, this completes the proof. ", "page_idx": 14}, {"type": "text", "text": "A.4 Proof of Proposition 12 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "First we will need a lemma: ", "page_idx": 14}, {"type": "text", "text": "Lemma 15. Fix $w\\in\\mathbb{R}^{L}$ . Then $w\\in R_{j}^{\\varepsilon}$ if and only $i f\\mathrm{argmax}^{\\varepsilon}(w)=\\{j\\}$ . ", "page_idx": 14}, {"type": "text", "text": "For intuition on this result, we can look back at Figure 1\u2014for instance, the blue region marked by $\\{1\\}$ illustrates the set of vectors $w\\in\\Delta_{L-1}$ such that argmax ${\\varepsilon}(w)\\,=\\,\\{1\\}$ , and according to this lemma, this is equal to the region $R_{1}^{\\varepsilon}$ . ", "page_idx": 14}, {"type": "text", "text": "Using this lemma, we now need to show that, for any $\\varepsilon$ -compatible and permutation invariant selection rule $s$ , if $s(w)\\,=\\,\\{j\\}$ for some $w\\in\\Delta_{L-1}$ and $j\\in[L]$ , then $w\\in R_{j}^{\\varepsilon}$ for some $j$ . First, we must have $j=\\operatorname{argmax}(w)$ , since $s$ is assumed to contain the argmax. Fix any $k\\neq j$ , and define a vector $v\\in\\Delta_{L-1}$ by permuting $w_{j}$ and $w_{k}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nv_{\\ell}=\\left\\{{\\psi}_{k},\\quad\\ell=j,\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then by permutation invariance we have $s(v)=\\{k\\}$ . Since $s$ is $\\varepsilon$ -compatible, we therefore have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\varepsilon\\leq\\|w-v\\|=\\sqrt{(w_{j}-v_{j})^{2}+(w_{k}-v_{k})^{2}+\\sum_{\\ell\\neq j,k}(w_{\\ell}-v_{\\ell})^{2}}=\\sqrt{2(w_{j}-w_{k})^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore we have $w_{j}\\,-\\,w_{k}\\ \\geq\\ \\varepsilon/\\sqrt{2}$ for all $k~\\ne~j$ , which proves $w~\\in~R_{j}^{\\varepsilon}$ and therefore argmax $\\varepsilon(w)=\\{j\\}$ . ", "page_idx": 14}, {"type": "text", "text": "A.5 Proof of Theorem 13 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Step 1: verifying that $c_{\\varepsilon}(w)$ is well defined. First we check that the solution $c_{\\varepsilon}(w)$ exists and is unique. Fix $w$ and define the function $f_{w}$ as in (10). Note that $f_{w}(c)\\equiv0$ for $c\\geq\\operatorname*{max}_{i}w_{i}$ , and $c\\mapsto f_{w}(c)$ is strictly decreasing over $c\\leq\\operatorname*{max}_{i}w_{i}$ , with $\\mathrm{lim}_{c\\rightarrow-\\infty}\\,f_{w}(c)=\\infty$ ; therefore, a unique solution $c_{\\varepsilon}(w)$ to the equation $f_{w}(c)=\\varepsilon^{2}$ must exist. ", "page_idx": 15}, {"type": "text", "text": "Step 2: verifying $t_{\\varepsilon}(w)\\leq c_{\\varepsilon}(w)$ . Next we check that the claim $t_{\\varepsilon}(w)\\leq c_{\\varepsilon}(w)$ must hold. First, by definition of $c_{\\varepsilon}(w)$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\varepsilon^{2}=\\left(\\sum_{j\\in[L]}\\left(w_{j}-c_{\\varepsilon}(w)\\right)_{+}\\right)^{2}+\\sum_{j\\in[L]}\\left(w_{j}-c_{\\varepsilon}(w)\\right)_{+}^{2}\\leq2\\left(\\sum_{j\\in[L]}\\left(w_{j}-c_{\\varepsilon}(w)\\right)_{+}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the inequality holds by the properties of the $\\ell_{1}$ and $\\ell_{2}$ norms. Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{j\\in[L]}(w_{j}-c)_{+}\\geq\\varepsilon/\\sqrt{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which verifies that ", "page_idx": 15}, {"type": "equation", "text": "$$\nt_{\\varepsilon}(w)=c_{\\varepsilon}(w)+\\varepsilon/\\sqrt{2}-\\sum_{j\\in[L]}(w_{j}-c_{\\varepsilon}(w))_{+}\\leq c_{\\varepsilon}(w).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Step 3: checking monotonicity. Now we turn to verifying the monotonicity properties of $t_{\\varepsilon}(w)$ and $c_{\\varepsilon}(w)$ . First we check that these functions are nonincreasing in $\\varepsilon$ . First, since $f_{w}(c)$ is nonincreasing in $c$ , and $c_{\\varepsilon}(w)$ is the solution to $f_{w}(c)=\\varepsilon^{2}$ , this immediately implies that $c_{\\varepsilon}(w)$ is nonincreasing in $\\varepsilon$ . Next we turn to $t_{\\varepsilon}(w)$ . Define ", "page_idx": 15}, {"type": "equation", "text": "$$\nt^{\\prime}(c)=c+\\frac{1}{\\sqrt{2}}\\sqrt{\\left(\\sum_{j\\in[L]}(w_{j}-c)_{+}\\right)^{2}+\\sum_{j\\in[L]}(w_{j}-c)_{+}^{2}}-\\sum_{j\\in[L]}(w_{j}-c)_{+},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "so that we have $t_{\\varepsilon}(w)=t^{\\prime}(c_{\\varepsilon}(w))$ by construction. We can verify that $c\\mapsto t^{\\prime}(c)$ is nondecreasing, and therefore, $t_{\\varepsilon}(w)=t^{\\prime}(c_{\\varepsilon}(w))\\leq\\dot{t}^{\\prime}(c_{\\varepsilon^{\\prime}}(w))=t_{\\varepsilon^{\\prime}}(w)$ , where the inequality holds since $c_{\\varepsilon}(w)\\leq$ $c_{\\varepsilon^{\\prime}}(w)$ . ", "page_idx": 15}, {"type": "text", "text": "Next we check that $t_{\\varepsilon}(w)$ and $c_{\\varepsilon}(w)$ are nondecreasing functions of $w$ . Fix any $w\\le w^{\\prime}$ (where the inequality is coordinatewise, i.e., $\\dot{w}_{j}\\le w_{j}^{\\prime}$ for all $j\\in[L])$ . Since $w\\mapsto f_{w}(c)$ is a nondecreasing function, we have $f_{w}(c)\\leq f_{w^{\\prime}}(c)$ for all $c$ . We therefore have $f_{w^{\\prime}}(c_{\\varepsilon}(w))\\geq f_{w}(c_{\\varepsilon}(w))=\\varepsilon^{2}=$ $f_{w^{\\prime}}(c_{\\varepsilon}(w^{\\prime}))$ . Since $c\\mapsto f_{w^{\\prime}}(c)$ is nonincreasing, therefore, $c_{\\varepsilon}(w^{\\prime})\\geq c_{\\varepsilon}(w)$ . Next we consider $t_{\\varepsilon}$ . Let $\\begin{array}{r}{t_{\\varepsilon}^{\\prime\\prime}(c)=c+\\varepsilon/\\sqrt{2}-\\sum_{j\\in[L]}(w_{j}-c)_{+}}\\end{array}$ , which is a nondecreasing function of $c$ . Then we have $t_{\\varepsilon}(w)=t_{\\varepsilon}^{\\prime\\prime}(c_{\\varepsilon}(w))\\leq t_{\\varepsilon}^{\\prime\\prime}(c_{\\varepsilon}(w^{\\prime}))=t_{\\varepsilon}(w^{\\prime})$ . ", "page_idx": 15}, {"type": "text", "text": "Step 4: returning to the inflated argmax. Finally, fixing any $w\\ \\in\\ \\mathbb{R}^{L}$ , we will prove that argm $\\mathrm{ax}^{\\varepsilon}(w)=\\{\\bar{j}:w_{j}>t_{\\varepsilon}(w)\\}$ . First choose any $j\\in[L]$ with $w_{j}>t_{\\varepsilon}(w)$ . We need to verify that $j\\in\\mathrm{argmax}^{\\varepsilon}(w)$ . Define $\\boldsymbol{v}\\in\\mathbb{R}^{L}$ with entries ", "page_idx": 15}, {"type": "equation", "text": "$$\nv_{k}=\\{\\operatorname*{max}\\{c_{\\varepsilon}(w)+\\varepsilon/\\sqrt{2},w_{j}\\},\\:\\:\\:k=j,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By construction, we have $v\\in R_{j}^{\\varepsilon}$ . We calculate ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{dist}(w,R_{j}^{\\varepsilon})^{2}\\leq\\|w-v\\|^{2}}\\\\ &{\\qquad\\qquad=\\Big(w_{j}-\\operatorname*{max}\\{c_{\\varepsilon}(w)+\\varepsilon/\\sqrt{2},w_{j}\\}\\Big)^{2}+\\displaystyle\\sum_{k\\neq j}\\big(w_{k}-\\operatorname*{min}\\{c_{\\varepsilon}(w),w_{k}\\}\\big)^{2}}\\\\ &{\\qquad=\\Big(c_{\\varepsilon}(w)+\\varepsilon/\\sqrt{2}-w_{j}\\Big)_{+}^{2}+\\displaystyle\\sum_{k\\neq j}(w_{k}-c_{\\varepsilon}(w))_{+}^{2}}\\\\ &{\\qquad<\\Big(c_{\\varepsilon}(w)+\\varepsilon/\\sqrt{2}-t_{\\varepsilon}(w)\\Big)^{2}+\\displaystyle\\sum_{k\\neq j}(w_{k}-c_{\\varepsilon}(w))_{+}^{2}}\\\\ &{\\qquad\\leq\\Bigg(\\displaystyle\\sum_{k\\in\\{L\\}}\\big(w_{k}-c_{\\varepsilon}(w)\\big)_{+}\\Bigg)^{2}+\\displaystyle\\sum_{k\\in\\{L\\}}(w_{k}-c_{\\varepsilon}(w))_{+}^{2}=\\varepsilon^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last two steps hold by definition of $t_{\\varepsilon}(w)$ and $c_{\\varepsilon}(w)$ , while the strict inequality holds since $t_{\\varepsilon}(w)\\leq c_{\\varepsilon}(w)<c_{\\varepsilon}(w)+\\varepsilon/\\sqrt{2}$ as established above, while $w_{j}>t_{\\varepsilon}(w)$ by assumption. Therefore $j\\in\\mathrm{argmax}^{\\varepsilon}(w)$ . ", "page_idx": 16}, {"type": "text", "text": "Now we check the converse. For this last step, we will need a lemma. The following result characterizes the projection of $w$ to the set $R_{j}^{\\varepsilon}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 16. Fix any $w\\in\\mathbb{R}^{L}$ and any $j\\in[L]$ . Then there is a unique $a\\in\\mathbb R$ satisfying ", "page_idx": 16}, {"type": "equation", "text": "$$\nw_{j}=a+\\varepsilon/\\sqrt{2}-\\sum_{k\\neq j}(w_{k}-a)_{+}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, defining the vector $\\boldsymbol{v}\\in\\mathbb{R}^{L}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\nv_{k}={\\binom{a+\\varepsilon/{\\sqrt{2}},\\quad k=j,}{a\\wedge w_{k}}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "it holds that $v=\\operatorname{argmin}_{u\\in R_{j}^{\\varepsilon}}\\|w-u\\|$ , i.e., $v$ is the projection of $w$ to the set $R_{j}^{\\varepsilon}$ . ", "page_idx": 16}, {"type": "text", "text": "Next suppose $w_{j}\\leq t_{\\varepsilon}(w)$ . We need to verify that $j\\not\\in\\mathrm{argmax}^{\\varepsilon}(w)$ . Let $a$ and $v$ be defined as in Lemma 16 above. We can compare the equation (11) to the definition of $t_{\\varepsilon}(w)$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nt_{\\varepsilon}(w)=c_{\\varepsilon}(w)+\\varepsilon/\\sqrt{2}-\\sum_{k\\in[L]}\\left(w_{k}-c_{\\varepsilon}(w)\\right)_{+}=c_{\\varepsilon}(w)+\\varepsilon/\\sqrt{2}-\\sum_{k\\neq j}(w_{k}-c_{\\varepsilon}(w))_{+},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last step holds since $w_{j}\\,\\leq\\,t_{\\varepsilon}(w)$ by assumption, and $t_{\\varepsilon}(w)\\,\\leq\\,c_{\\varepsilon}(w)$ as proved above. Since $c\\mapsto c+\\varepsilon/\\sqrt{2}-\\textstyle\\sum_{k\\neq j}(w_{k}-c)_{+}$ is an increasing function, and $w_{j}\\leq t_{\\varepsilon}(w)$ , this implies ", "page_idx": 16}, {"type": "text", "text": "$a\\leq c_{\\varepsilon}(w)$ . We then calculate ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{dist}(w,R_{f}^{2})^{2}=\\left[1\\!\\!\\begin{array}{l}{\\mathrm{2}}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{=\\left(w_{j}-\\left(a+\\varepsilon/\\sqrt{z}\\right)\\right)^{2}+\\sum_{k\\neq j}^{\\infty}\\left(w_{k}-a\\wedge w_{k}\\right)^{2}\\mathrm{by}\\left(1\\right)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{=\\left(w_{j}-\\left(a+\\varepsilon/\\sqrt{z}\\right)\\right)^{2}+\\sum_{k\\neq j}^{\\infty}(w_{k}-a)_{+}^{2}}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{=\\left(\\sum_{k\\neq j}\\left(w_{k}-a\\right)_{+}\\right)^{2}+\\sum_{k\\neq j}^{\\infty}(w_{k}-a)_{+}^{2}\\mathrm{by}\\left(1\\right)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\geq\\left(\\sum_{k\\neq j}\\left(w_{k}-c_{\\varepsilon}(w)\\right)_{+}\\right)^{2}+\\sum_{k\\neq j}^{\\infty}(w_{k}-c_{\\varepsilon}(w))_{+}^{2}\\mathrm{since}\\ a\\leq c_{\\varepsilon}(w)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{=\\left(\\sum_{k\\in[2]}\\left(w_{k}-c_{\\varepsilon}(w)\\right)_{+}\\right)^{2}+\\sum_{k\\in[2]}^{\\infty}(w_{k}-c_{\\varepsilon}(w))_{+}^{2}\\mathrm{since}\\ w_{j}\\leq t_{\\varepsilon}(w)\\leq c_{\\varepsilon}(w)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, $\\mathrm{dist}(w,R_{j}^{\\varepsilon})\\geq\\varepsilon$ , and so $j\\not\\in\\mathrm{argmax}^{\\varepsilon}(w)$ . ", "page_idx": 17}, {"type": "text", "text": "A.6 Proofs of technical lemmas ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.6.1 Proof of Lemma 14 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "First, since $y$ is constrained to have nonnegative entries, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|x+y+r\\mathbf{1}_{k}\\|^{2}\\geq\\|(x+r\\mathbf{1}_{k})_{+}\\|^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where for a vector $v=(v_{1},\\ldots,v_{L})\\in\\mathbb{R}^{L}$ , we write $(v)_{+}$ to denote the vector with $j\\mathrm{th}$ entry given by $(v_{j})_{+}=\\operatorname*{max}\\{v_{j},0\\}$ for each $j$ . The analogous bound holds for $\\|x^{\\prime}+y^{\\prime}+r^{\\prime}\\mathbf{1}_{k^{\\prime}}\\|^{2}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|{\\boldsymbol x}+{\\boldsymbol y}+r\\mathbf{1}_{k}\\|^{2}+\\|{\\boldsymbol x}^{\\prime}+{\\boldsymbol y}^{\\prime}+r^{\\prime}\\mathbf{1}_{k^{\\prime}}\\|^{2}\\geq\\|({\\boldsymbol x}+r\\mathbf{1}_{k})_{+}\\|^{2}+\\|({\\boldsymbol x}^{\\prime}+r^{\\prime}\\mathbf{1}_{k^{\\prime}})_{+}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now need to show that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|(x+r\\mathbf{1}_{k})_{+}\\|^{2}+\\|(x^{\\prime}+r^{\\prime}\\mathbf{1}_{k^{\\prime}})_{+}\\|^{2}\\geq\\varepsilon^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next let ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\bar{r}}={\\frac{r+r^{\\prime}}{2}}={\\frac{x^{\\top}\\mathbf{1}_{k}+{x^{\\prime}}^{\\top}\\mathbf{1}_{k^{\\prime}}-\\varepsilon{\\sqrt{2}}}{2}},\\quad\\Delta={\\frac{-r+r^{\\prime}}{2}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "so that we can write ", "page_idx": 17}, {"type": "equation", "text": "$$\nr=\\bar{r}-\\Delta,\\;r^{\\prime}=\\bar{r}+\\Delta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for some $\\Delta\\in\\mathbb R$ . We we therefore need to show $\\operatorname*{inf}_{\\Delta\\in\\mathbb{R}}f(\\Delta)\\geq\\varepsilon^{2}$ , where ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(\\Delta)=\\|(x+(\\bar{r}-\\Delta)\\mathbf{1}_{k})_{+}\\|^{2}+\\|(x^{\\prime}+(\\bar{r}+\\Delta)\\mathbf{1}_{k^{\\prime}})_{+}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "First, we observe that we can restrict the range of $\\Delta$ . Specifically, for any $\\Delta\\geq\\operatorname*{max}_{j\\in[L]}x_{j}+\\bar{r}$ , we have $f(\\Delta)=\\|(x^{\\prime}+(\\bar{r}+\\Delta)\\mathbf{1}_{k^{\\prime}})_{+}\\|^{2}$ , which is a nondecreasing function; therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\substack{\\Delta\\ge\\operatorname*{max}_{j\\in[L]}x_{j}+\\bar{r}}}f(\\Delta)=f\\left(\\operatorname*{max}_{j\\in[L]}x_{j}+\\bar{r}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "meaning that we do not need to consider values of $\\Delta$ beyond this upper bound. Applying a similar argument for a lower bound, we see that from this point on we only need to verify that ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(\\Delta)\\ge\\varepsilon^{2}\\:\\:\\mathrm{for}-\\operatorname*{max}_{j\\in[k^{\\prime}]}x_{j}^{\\prime}-\\bar{r}\\le\\Delta\\le\\operatorname*{max}_{j\\in[L]}x_{j}+\\bar{r}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, for any $\\Delta$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n(x_{j}+(\\bar{r}-\\Delta))^{2}=(x_{j}+(\\bar{r}-\\Delta))_{+}^{2}+(x_{j}+(\\bar{r}-\\Delta))_{-}^{2}\\leq(x_{j}+(\\bar{r}-\\Delta))_{+}^{2}+(\\bar{r}-\\Delta)^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $j$ , by nonnegativity of $x$ . Furthermore we must have ", "page_idx": 18}, {"type": "text", "text": "for at least one $j\\in[L]$ when $\\Delta\\leq\\operatorname*{max}_{j\\in[L]}x_{j}+\\bar{r}$ as specified above (i.e., because for $j$ maximizing the entry $x_{j}$ , the value $(x+(\\bar{r}-\\Delta)\\mathbf{1}_{k})_{j}$ is nonnegative). Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(x+(\\bar{r}-\\Delta)\\mathbf{1}_{k})_{+}\\|^{2}\\geq\\|x+(\\bar{r}-\\Delta)\\mathbf{1}_{k}\\|^{2}-(k-1)(\\bar{r}-\\Delta)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we can show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|(x^{\\prime}+(\\bar{r}+\\Delta)\\mathbf{1}_{k^{\\prime}})_{+}\\|^{2}\\geq\\|x^{\\prime}+(\\bar{r}+\\Delta)\\mathbf{1}_{k^{\\prime}}\\|^{2}-(k^{\\prime}-1)(\\bar{r}+\\Delta)^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We then calculate ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\Delta)=\\Vert(x+(\\bar{r}-\\Delta)\\mathbf{1}_{k})_{+}\\Vert^{2}+\\Vert(x^{\\prime}+(\\bar{r}+\\Delta)\\mathbf{1}_{k^{\\prime}})_{+}\\Vert^{2}}\\\\ &{\\qquad\\ge\\Vert x+(\\bar{r}-\\Delta)\\mathbf{1}_{k}\\Vert^{2}-(k-1)(\\bar{r}-\\Delta)^{2}+\\Vert x^{\\prime}+(\\bar{r}+\\Delta)\\mathbf{1}_{k^{\\prime}}\\Vert^{2}-(k^{\\prime}-1)(\\bar{r}+\\Delta)^{2}}\\\\ &{\\qquad=\\Vert x\\Vert^{2}+2(\\bar{r}-\\Delta)x^{\\top}\\mathbf{1}_{k}+(\\bar{r}-\\Delta)^{2}+\\Vert x^{\\prime}\\Vert^{2}+2(\\bar{r}+\\Delta)x^{\\prime\\top}\\mathbf{1}_{k^{\\prime}}+(\\bar{r}+\\Delta)^{2}}\\\\ &{\\qquad=2\\varepsilon^{2}-(x^{\\top}\\mathbf{1}_{k})^{2}-(x^{\\prime\\top}\\mathbf{1}_{k^{\\prime}})^{2}+2(\\bar{r}-\\Delta)x^{\\top}\\mathbf{1}_{k}+2(\\bar{r}+\\Delta)x^{\\prime\\top}\\mathbf{1}_{k^{\\prime}}+2\\bar{r}^{2}+2\\Delta^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last step holds by (8). ", "page_idx": 18}, {"type": "text", "text": "Writing $z=x^{\\top}{\\mathbf{1}}_{k}$ and $z^{\\prime}=x^{\\prime}{}^{\\top}{\\bf1}_{k^{\\prime}}$ for convenience, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\Delta)\\geq2\\varepsilon^{2}-z^{2}-z^{\\prime2}+2(\\bar{r}-\\Delta)z+2(\\bar{r}+\\Delta)z^{\\prime}+2\\bar{r}^{2}+2\\Delta^{2}}\\\\ &{\\qquad=2\\varepsilon^{2}-z^{2}-z^{\\prime2}+2\\left(\\frac{z+z^{\\prime}-\\varepsilon\\sqrt{2}}{2}-\\Delta\\right)z+2\\left(\\frac{z+z^{\\prime}-\\varepsilon\\sqrt{2}}{2}+\\Delta\\right)z^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,2\\left(\\frac{z+z^{\\prime}-\\varepsilon\\sqrt{2}}{2}\\right)^{2}+2\\Delta^{2}}\\\\ &{\\qquad=\\varepsilon^{2}+4\\left(z-\\varepsilon/\\sqrt{2}\\right)\\left(z^{\\prime}-\\varepsilon/\\sqrt{2}\\right)+2\\left(\\Delta-\\frac{z-z^{\\prime}}{2}\\right)^{2}}\\\\ &{\\qquad\\geq\\varepsilon^{2}+4\\left(z-\\varepsilon/\\sqrt{2}\\right)\\left(z^{\\prime}-\\varepsilon/\\sqrt{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second step holds because $\\begin{array}{r}{\\bar{r}=\\frac{r+r^{\\prime}}{2}=\\frac{z+z^{\\prime}-\\varepsilon\\sqrt{2}}{2}}\\end{array}$ z+z\u2032\u2212\u03b5\u221a2by (9). ", "page_idx": 18}, {"type": "text", "text": "Finally, we also have $x^{\\top}\\mathbf{1}_{k}\\,=\\,\\|x\\|_{1}\\,\\geq\\,\\|x\\|$ , where the first step holds since $x$ is nonnegative, and so by assumption (8) we have $z\\ =\\ x^{\\top}\\mathbf{1}_{k}\\ \\geq\\ \\varepsilon/\\sqrt{2}$ . Similarly, $z^{\\prime}\\ \\geq\\ \\varepsilon/{\\sqrt{2}}$ . Therefore, $4\\left(z-\\varepsilon/{\\sqrt{2}}\\right)\\left(z^{\\prime}-\\varepsilon/{\\sqrt{2}}\\right)\\geq0$ , and so we have shown that $f(\\Delta)\\geq\\varepsilon^{2}$ , as desired. ", "page_idx": 18}, {"type": "text", "text": "A.6.2 Proof of Lemma 15 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "First, suppose $w\\in R_{j}^{\\varepsilon}$ . Then $\\mathrm{argmax}(w)=\\{j\\}$ , and so $j\\in\\mathrm{argmax}^{\\varepsilon}(w)$ by Proposition 10. Next, let $c=w_{j}-\\varepsilon/\\sqrt{2}$ . Then $w_{\\ell}\\leq c$ for all $\\ell\\neq j$ , and so defining $f_{w}$ as in (10), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{w}(c)=(w_{j}-c)^{2}+(w_{j}-c)^{2}=\\varepsilon^{2}\\implies c_{\\varepsilon}(w)=c=w_{j}-\\varepsilon/\\sqrt{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then ", "page_idx": 18}, {"type": "equation", "text": "$$\nt_{\\varepsilon}(w)=c_{\\varepsilon}(w)+\\varepsilon/\\sqrt{2}-\\sum_{\\ell\\in[L]}(w_{\\ell}-c_{\\varepsilon}(w))_{+}=\\Big(w_{j}-\\varepsilon/\\sqrt{2}\\Big)+\\varepsilon/\\sqrt{2}-\\varepsilon/\\sqrt{2}=w_{j}-\\varepsilon/\\sqrt{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and so for all $\\ell\\neq j$ , we have $w_{\\ell}\\leq w_{j}-\\varepsilon/\\sqrt{2}=t_{\\varepsilon}(w)$ and thus $\\ell\\not\\in\\mathrm{argmax}^{\\varepsilon}(w)$ . This verifies that argmax $\\varepsilon(w)=\\{j\\}$ . ", "page_idx": 18}, {"type": "text", "text": "To prove the converse, suppose argmax ${\\varepsilon}(w)=\\{j\\}$ . By Proposition 10, we then have $\\operatorname{argmax}(w)=$ $\\{j\\}$ . Let $k\\in\\mathrm{argmax}_{\\ell\\neq j}\\,w_{\\ell}$ , then to show $w\\in R_{j}^{\\varepsilon}$ it suffices to show that $w_{j}\\geq w_{k}+\\varepsilon/\\sqrt{2}$ . Define $\\boldsymbol{v}\\in\\mathbb{R}^{L}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\nv_{\\ell}=\\left\\{{\\begin{array}{l l}{w_{k},}&{\\ell=j,}\\\\ {w_{k}+\\varepsilon/{\\sqrt{2}},}&{\\ell=k,}\\\\ {w_{\\ell},}&{\\ell\\neq j,k.}\\end{array}}\\right..\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then $v\\in R_{k}^{\\varepsilon}$ , and so we must have $\\left\\|w-v\\right\\|\\geq\\varepsilon$ since $k\\not\\in\\mathrm{argmax}^{\\varepsilon}(w)$ . We calculate ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|w-v\\|^{2}=(w_{j}-v_{j})^{2}+(w_{k}-v_{k})^{2}=(w_{j}-w_{k})^{2}+(\\varepsilon/\\sqrt{2})^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This implies that we must have $(w_{j}-w_{k})^{2}\\geq\\varepsilon^{2}/2$ , which completes the proof. ", "page_idx": 18}, {"type": "text", "text": "A.6.3 Proof of Lemma 16 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "First we verify existence and uniqueness of $a$ . Let ", "page_idx": 19}, {"type": "equation", "text": "$$\nf(a)=w_{j}-a+\\sum_{k\\neq j}(w_{k}-a)_{+}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a continuous and strictly decreasing bijection, so $f(a)=\\varepsilon/{\\sqrt{2}}$ must have a unique solution. ", "page_idx": 19}, {"type": "text", "text": "Next let $a$ be this unique solution and let $v$ be defined as in (12). Now we verify that $v$ is the unique solution to the optimization problem ", "page_idx": 19}, {"type": "equation", "text": "$$\nv=\\operatorname{argmin}_{u\\in\\mathbb{R}^{L}}\\left\\{\\frac{1}{2}\\|u-w\\|^{2}:u_{j}\\geq u_{k}+\\varepsilon/\\sqrt{2}\\;\\mathrm{for~all}\\;k\\neq j\\right\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which defines the projection of $w$ to $R_{j}^{\\varepsilon}$ . By the Karush\u2013Kuhn\u2013Tucker (KKT) conditions for first-order optimality, it is sufficient to verify that ", "page_idx": 19}, {"type": "equation", "text": "$$\nv-w-\\sum_{k\\neq j}\\lambda_{k}(\\mathbf{e}_{j}-\\mathbf{e}_{k})=0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\lambda_{k}\\geq0$ for all $k$ , and $\\lambda_{k}=0$ for any inactive constraints, i.e., any $k$ for which $v_{j}>v_{k}+\\varepsilon/\\sqrt{2}$ . Let $\\lambda_{k}=(w_{k}-a)_{+}$ for each $k\\neq j$ . Note that if $v_{j}>v_{k}+\\varepsilon/\\sqrt{2}$ (i.e., an inactive constraint), then we must have $w_{k}<a$ and so $\\lambda_{k}=0$ , by construction of $v$ . Then, for any $\\ell\\neq j$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(v-w-\\sum_{k\\neq j}\\lambda_{k}({\\bf e}_{j}-{\\bf e}_{k})\\right)_{\\ell}=v_{\\ell}-w_{\\ell}+\\lambda_{\\ell}=a\\wedge w_{\\ell}-w_{\\ell}+(w_{\\ell}-a)_{+}=0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{e}_{k}$ denotes the $k$ th canonical basis vector in $\\mathbb{R}^{L}$ . This proves that the \u2113th coordinate of the system of equations (13) holds for each $\\ell\\neq j$ . Now we verify the $j$ th coordinate. We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left(v-w-\\sum_{k\\neq j}\\lambda_{k}(\\mathbf{e}_{j}-\\mathbf{e}_{k})\\right)_{j}=v_{j}-w_{j}-\\sum_{k\\neq j}\\lambda_{k}}}\\\\ &{}&{=a+\\varepsilon/\\sqrt{2}-w_{j}-\\displaystyle\\sum_{k\\neq j}(w_{k}-a)_{+}=\\varepsilon/\\sqrt{2}-f(a)=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "by our choice of $a$ as the solution to $f(a)=\\varepsilon/{\\sqrt{2}}$ . This verifies the KKT conditions, and thus, $v$ is the projection of $w$ to $R_{j}^{\\varepsilon}$ as claimed. ", "page_idx": 19}, {"type": "text", "text": "B Extension to randomized algorithms ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As mentioned in Section 2, in many applications it is common to use randomization in the construction of a classification procedure, in the learning algorithm $\\boldsymbol{\\mathcal{A}}$ and/or in the selection rule $s$ . In this section we formalize this more general framework, and will see how our results apply. ", "page_idx": 19}, {"type": "text", "text": "First, in the non-random setting, a learning algorithm $\\boldsymbol{\\mathcal{A}}$ maps a data set $\\boldsymbol{D}\\;\\in\\;(\\boldsymbol{\\mathcal{X}}\\times[\\boldsymbol{L}])^{n}$ to a fitted probability estimate function $\\hat{p}\\;:\\;\\mathcal{X}\\;\\rightarrow\\;\\Delta_{L-1}\\ \u2013$ \u2014we write the regression procedure as $\\hat{p}=\\mathcal{A}(\\bar{\\mathcal{D}})$ . In the randomized setting, we also allow for external randomness, $\\hat{p}=\\mathcal{A}(\\mathcal{D};\\xi)$ , where $\\xi\\sim\\mathrm{Uniform}[0,1]$ is a random seed (e.g., we might use $\\xi$ to randomly shuffle the training data when running stochastic gradient descent). ", "page_idx": 19}, {"type": "text", "text": "Next, in the non-random setting, a selection rule $s$ is a map from $\\Delta_{L-1}$ to subsets of $[L]$ , resulting in a candidate set of labels ${\\hat{S}}=s({\\hat{p}}(x))$ . In the randomized setting (for instance, if $s$ is the argmax but with ties broken at random), we instead include a random seed $\\zeta\\sim\\mathrm{Uniform}[0,1]$ , and write $\\hat{S}=s(\\hat{p}(x);\\zeta)$ . Formally, then, we have $s:\\Delta_{L-1}\\times[0,1]\\to\\wp([L])$ . (Note that the selection rule proposed in this work\u2014the inflated argmax\u2014is itself not random.) ", "page_idx": 19}, {"type": "text", "text": "Combining the two stages of the procedure, then, the classification algorithm is given by $\\mathcal{C}=s\\circ A$ , where given a training set $\\mathcal{D}$ and a test point $x$ , along with i.i.d. random seeds $\\xi,\\zeta\\sim\\mathrm{Uniform}[0,1]$ , we return the candidate set of labels given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{C}(\\mathcal{D},x;\\xi,\\zeta)=s(\\hat{p}(x);\\zeta)\\mathrm{~where~}\\hat{p}=\\mathcal{A}(\\mathcal{D};\\xi).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Of course, we can derive the non-random setting as a special case, simply by designing $\\boldsymbol{\\mathcal{A}}$ and $s$ to not depend on the random seeds $\\xi$ and $\\zeta$ . ", "page_idx": 20}, {"type": "text", "text": "Now, how do the results of this paper extend to this randomized setting? First, we need to reconsider our definition of selection stability (Definition 2). We will now define it as ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n}\\operatorname*{Pr}\\left\\{{\\hat{S}}\\cap{\\hat{S}}^{\\backslash i}=\\emptyset\\right\\}\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\hat{S}\\,=\\,\\mathcal{C}(\\mathcal{D},x;\\xi,\\zeta)$ and $\\hat{S}^{\\backslash i}\\,=\\,\\mathcal{C}(\\mathcal{D}^{\\backslash i},x;\\xi,\\zeta)$ , and where the probabilities are taken with respect to the distribution of the random seeds. We can also consider a notion of algorithmic stability (Definition 3) that allows for randomization: we say that $\\boldsymbol{\\mathcal{A}}$ has tail stability $(\\varepsilon,\\delta)$ if ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname*{Pr}\\Big\\{\\|\\hat{p}(x)-\\hat{p}^{\\backslash i}(x)\\|\\geq\\varepsilon\\Big\\}\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\hat{p}=\\mathcal{A}(\\mathcal{D};\\xi)$ and $\\hat{p}^{\\backslash i}=\\mathcal{A}(\\mathcal{D}^{\\backslash i};\\xi)$ , and where again probability is taken with respect to the distribution of $\\xi$ . ", "page_idx": 20}, {"type": "text", "text": "With these definitions in place, we observe that the result of Proposition 5 still holds in this more general setting for the selection rule argmax\u03b5: if $\\boldsymbol{\\mathcal{A}}$ is a randomized algorithm with tail stability $(\\varepsilon,\\delta)$ (under the new definition given above), then the randomized classification algorithm given by $\\mathcal{C}=\\mathrm{argmax}^{\\varepsilon}\\circ A$ has selection stability $\\delta$ (again, under the new definition given above), simply due to the $\\varepsilon$ -compatibility property of argmax $\\varepsilon$ (Theorem 9). [SBW24b] proved Theorem 8 in this more general setting of randomized learning algorithms $\\boldsymbol{\\mathcal{A}}$ , so Theorem 6 holds for randomized $\\boldsymbol{\\mathcal{A}}$ under this more general definition of selection stability. ", "page_idx": 20}, {"type": "text", "text": "C Extension to finitely many bags ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Section 3.1, we discussed how in practice, the bagged algorithm $\\widetilde{A}_{m}$ would be constructed by taking an empirical average over some large number $B$ of sampled ba g s, rather than computing the exact expectation $\\mathbb{E}_{r}[\\mathcal{A}(\\bar{D^{r}})(x)]$ . Now that we have allowed for randomized learning algorithms (as in Appendix B), we can formalize this setting: for a random seed $\\xi\\sim\\mathrm{Uniform}[0,1]$ , we define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{A}}_{m}(\\mathcal{D},\\xi)(x)=\\frac{1}{B}\\sum_{b=1}^{B}A(\\mathcal{D}^{r_{b}})(x),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the random draw of $B$ many bags, $r_{1},\\dots,r_{B}$ , is generated using the random seed $\\xi$ . Recall that [SBW24b] showed tail stability for the bagged version of any algorithm (as restated in Theorem 8)\u2014 their work also gives a result for the finite- $B$ case, as follows: ", "page_idx": 20}, {"type": "text", "text": "Theorem 17 ([SBW24b]). For any base learning algorithm $\\boldsymbol{\\mathcal{A}}$ returning outputs in $\\Delta_{L-1}$ , the bagged algorithm $\\widetilde{A}_{m}$ (computed with a finite number of bags, $B$ ) has tail stability $(\\varepsilon,\\delta)$ for any $\\varepsilon,\\delta>0$ satisfying ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta=\\varepsilon^{-2}\\cdot(1-1/L)\\left(\\frac{1}{n-1}\\cdot\\frac{p_{n,m}}{1-p_{n,m}}+\\frac{16e^{2}}{B}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In particular, combined with our result on the $\\varepsilon$ -compatibility of the inflated argmax, we have the following generalization of our main result (Theorem 6): ", "page_idx": 20}, {"type": "text", "text": "Theorem 18. Fix any sample size $n$ , any bag size $m$ , any inflation parameter $\\varepsilon>0$ , and any number of bags $B\\geq1$ . For any base learning algorithm $\\mathcal{A}$ , the classification algorithm $\\mathcal{C}=\\mathrm{argmax}^{\\varepsilon}\\circ\\widetilde{A}_{m}$ , obtained by combining the bagged version of $\\boldsymbol{\\mathcal{A}}$ (with $B$ many bags) together with the inflated argm ax, satisfies selection stability $\\delta$ where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta=\\varepsilon^{-2}\\cdot(1-1/L)\\left(\\frac{1}{n-1}\\cdot\\frac{p_{n,m}}{1-p_{n,m}}+\\frac{16e^{2}}{B}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "image", "img_path": "M7zNXntzsp/tmp/1e795d3a8fd411d553aa20fcf0932790f7b45cd05a51ce82cf9190a01a383363.jpg", "img_caption": ["Figure 3: The fixed-margin selection rule (3), left, and the inflated argmax (1), right, when $L=3$ . "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Comparing to our main result for the derandomized case (Theorem 6, which can be interpreted as taking $B\\rightarrow\\infty$ ), we see that this finite- $\\cdot B$ result is essentially the same as Theorem 6 once $B\\gg n$ . ", "page_idx": 21}, {"type": "text", "text": "D Compare to a simpler selection rule: the fixed-margin rule ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we compare to the simpler fixed-margin selection rule, which was defined earlier in (3)\u2014for convenience, we define it again here: ", "page_idx": 21}, {"type": "equation", "text": "$$\ns_{\\mathrm{margin}}^{\\varepsilon}(w)=\\left\\{j\\in[L]:w_{j}>\\operatorname*{max}_{\\ell\\in[L]}w_{\\ell}-\\varepsilon/\\sqrt{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "the set of all indices $j$ that are within some margin of being the maximum. ", "page_idx": 21}, {"type": "text", "text": "In this section, we will see that this fixed-margin selection rule is $\\varepsilon$ -compatible. Since this rule clearly has the advantage of being much simpler than the inflated argmax (both in terms of its definition and interpretability, and in terms of its computation), we might ask whether this rule is perhaps preferable to the more complex inflated argmax. However, we will also see that the fixed-margin selection rule can be very inefficient compared to the inflated argmax: the inflated argmax can never return a larger set, and will often return a substantially smaller one. ", "page_idx": 21}, {"type": "text", "text": "D.1 Theoretical results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "First, we verify that this simple rule satisfies $\\varepsilon$ -compatibility (Definition 4). ", "page_idx": 21}, {"type": "text", "text": "Proposition 19. For any $\\varepsilon>0$ , the selection rule s\u03b5margin is \u03b5-compatible. ", "page_idx": 21}, {"type": "text", "text": "In particular, since $s_{\\mathrm{margin}}^{\\varepsilon}$ clearly contains the argmax (i.e., $\\operatorname{argmax}(w)\\subseteq s_{\\operatorname{margin}}^{\\varepsilon}(w))$ and satisfies permutation invariance (in the sense of Proposition 10), by Proposition 12 this immediately implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\ns_{\\mathrm{margin}}^{\\varepsilon}(w)=\\{j\\}\\implies\\mathrm{argmax}^{\\varepsilon}(w)=\\{j\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for all $w\\in\\mathbb{R}^{L}$ , i.e., argmax\u03b5 is at least as good as $s_{\\mathrm{margin}}^{\\varepsilon}$ at returning a singleton set. However, for this particular selection rule, we can state an even stronger result: ", "page_idx": 21}, {"type": "text", "text": "Proposition 20. For any $w\\in\\mathbb{R}^{L}$ and any $\\varepsilon>0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\ns_{\\mathrm{margin}}^{\\varepsilon}(w)\\supseteq\\mathrm{argmax}^{\\varepsilon}(w).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In particular, this theoretical result ensures that the set of candidate labels $\\hat{S}$ returned by the inflated argmax can never be larger than for the fixed-margin selection rule. In low dimensions, however, the improvement is small. Indeed, for $L=2$ , we actually have $s_{\\mathrm{margin}}^{\\varepsilon}(w)=\\mathrm{argmax}^{\\varepsilon}(w)$ for all $w$ . For $L\\,=\\,3$ , Figure 3 shows that the inflated argmax is strictly better (i.e., the set inclusion result of Proposition 20 can, for certain values of $w$ , be a strict inclusion), but the difference in this low-dimensional setting appears minor. Next, however, we will see empirically that as the dimension $L$ grows, the benefit of the inflated argmax can be substantial. ", "page_idx": 21}, {"type": "image", "img_path": "M7zNXntzsp/tmp/a7ad851e8c2192b7f818912658ff10006ef88318611b4ef3a715556372848993.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 4: Simulation to compare the selection rules argmax\u03b5 and $s_{\\mathrm{margin}}^{\\varepsilon}$ . The figure shows the average set size, $|\\operatorname{argmax}^{\\varepsilon}(w)|$ and $\\big|s_{\\mathrm{margin}}^{\\varepsilon}(w)\\big|$ , averaged over $1,000$ random draws of $w$ (with standard error bars shown). See Appendix D.2 for details. ", "page_idx": 22}, {"type": "text", "text": "D.2 Simulation to compare the selection rules ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we compare the fixed-margin rule to the inflated argmax using randomly generated probability weights. For our simulation, we consider the size of the two sets for various values of $L$ To sample the weights, we draw a standard Gaussian random vector $Z\\sim\\mathcal{N}(0,\\mathbf{I}_{L})$ and define $w$ as the softmax of $Z$ , i.e., ", "page_idx": 22}, {"type": "equation", "text": "$$\nw_{j}=\\frac{e^{Z_{j}}}{\\sum_{\\ell\\in[L]}e^{Z_{\\ell}}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for $j\\in[L]$ . We then compute argmax $^\\varepsilon(w)$ and $s_{\\mathrm{margin}}^{\\varepsilon}(w)$ , where $\\varepsilon=0.1$ . In Figure 4, we present results comparing the average size of the sets returned by each of the two methods. In this setting, we see that the inflated argmax has substantial gains over the fixed-margin selection rule when the number of classes $L$ is large. Even when $L=25$ , the ratio of the expected sizes is about $78\\%$ , so the inflated argmax has a nontrivial advantage in that setting. When $L=100$ , the ratio of expected sizes is $48\\%$ , meaning the inflated argmax is on average more than twice as small. ", "page_idx": 22}, {"type": "text", "text": "D.3 Proofs for the fixed-margin selection rule ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "D.3.1 Proof of Proposition 19 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Let $w,v\\in\\mathbb{R}^{L}$ with $s_{\\mathrm{margin}}^{\\varepsilon}(w)\\cap s_{\\mathrm{margin}}^{\\varepsilon}(v)=\\emptyset$ . Let $j\\in\\operatorname{argmax}_{\\ell}w_{\\ell}$ and $k\\in\\operatorname{argmax}_{\\ell}v_{\\ell}$ . Then clearly $j\\in s_{\\mathrm{margin}}^{\\varepsilon}(w)$ , so since $s_{\\mathrm{margin}}^{\\varepsilon}(w)\\cap s_{\\mathrm{margin}}^{\\varepsilon}(v)=\\emptyset$ , this implies $j\\not\\in s_{\\mathrm{margin}}^{\\varepsilon}(v)$ . By definition, then, $\\begin{array}{r}{v_{j}\\le\\operatorname*{max}_{\\ell}v_{\\ell}-\\varepsilon/\\sqrt{2}=v_{k}-\\varepsilon/\\sqrt{2}}\\end{array}$ . By an identical argument, we have $w_{k}\\leq w_{j}-\\varepsilon/\\sqrt{2}$ . Then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|w-v\\|^{2}\\geq(w_{j}-v_{j})^{2}+(w_{k}-v_{k})^{2}}\\\\ &{\\qquad\\qquad\\geq(w_{j}-v_{j})_{+}^{2}+(v_{k}-w_{k})_{+}^{2}}\\\\ &{\\qquad\\qquad\\geq\\Big(w_{j}-(v_{k}-\\varepsilon/\\sqrt{2})\\Big)_{+}^{2}+\\Big(v_{k}-(w_{j}-\\varepsilon/\\sqrt{2})\\Big)_{+}^{2}}\\\\ &{\\qquad\\qquad\\geq\\operatorname*{inf}_{t\\in\\mathbb{R}}\\Big\\{(\\varepsilon/\\sqrt{2}-t)_{+}^{2}+(\\varepsilon/\\sqrt{2}+t)_{+}^{2}\\Big\\}}\\\\ &{\\qquad\\qquad=2(\\varepsilon/\\sqrt{2})^{2}=\\varepsilon^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This proves $\\left\\|w-v\\right\\|\\geq\\varepsilon$ , and therefore, we have shown that $\\varepsilon$ -compatibility is satisfied. ", "page_idx": 22}, {"type": "text", "text": "D.3.2 Proof of Proposition 20 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Suppose $j\\in\\mathrm{argmax}^{\\varepsilon}(w)$ . Then $\\mathrm{dist}(w,R_{j}^{\\varepsilon})<\\varepsilon$ , so we can find some $v\\in R_{j}^{\\varepsilon}$ with $\\|w-v\\|<\\varepsilon$ . Then, for any $k\\neq j$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{j}-w_{k}=v_{j}-v_{k}+(w_{j}-v_{j})+(v_{k}-w_{k})}\\\\ &{\\qquad\\qquad\\geq\\varepsilon/\\sqrt{2}+(w_{j}-v_{j})+(v_{k}-w_{k})\\,\\mathrm{since}\\,v\\in R_{j}^{z}}\\\\ &{\\qquad\\qquad\\geq\\varepsilon/\\sqrt{2}-\\left(|w_{j}-v_{j}|+|w_{k}-v_{k}|\\right)}\\\\ &{\\qquad\\qquad\\geq\\varepsilon/\\sqrt{2}-\\sqrt{2}\\sqrt{(w_{j}-v_{j})^{2}+(w_{k}-v_{k})^{2}}}\\\\ &{\\qquad\\qquad\\geq\\varepsilon/\\sqrt{2}-\\sqrt{2}\\|w-v\\|}\\\\ &{\\qquad>\\varepsilon/\\sqrt{2}-\\sqrt{2}\\cdot\\varepsilon}\\\\ &{\\qquad=-\\varepsilon/\\sqrt{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since this holds for all $k\\neq j$ , then, ", "page_idx": 23}, {"type": "equation", "text": "$$\nw_{j}>\\operatorname*{max}_{k\\in[L]}w_{k}-\\varepsilon/\\sqrt{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which proves that $j\\in s_{\\mathrm{margin}}^{\\varepsilon}(w)$ . ", "page_idx": 23}, {"type": "text", "text": "E Additional experimental results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we extend our experiment from Section 4 to consider some additional existing methods as baselines and evaluate each method using some common metrics from set-valued classification. ", "page_idx": 23}, {"type": "text", "text": "Selection rules. We compare the following selection rules: ", "page_idx": 23}, {"type": "text", "text": "1. Standard argmax.   \n2. $\\varepsilon$ -inflated argmax with tolerance $\\varepsilon=.05$ .   \n3. Top- $K$ classification with $K=2$ .   \n4. Thresholding: including classes in the output set until the sum of probabilities becomes at   \nleast $\\tau=0.8$ , i.e., ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\Gamma_{\\tau}^{*}(w):=\\left\\{\\ell\\in[L]:w_{\\ell}\\geq w_{[\\hat{k}]}\\right\\},\\mathrm{~where~}}\\\\ &{}&{\\hat{k}=\\operatorname*{min}\\{k:w_{[1]}+\\cdot\\cdot\\cdot+w_{[k]}\\geq\\tau\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "5. Nondeterministic classification optimized for $F_{1}$ -score [DDB09]: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{NDC}_{F_{1}}(w):=\\left\\{\\ell\\in[L]:w_{\\ell}\\geq w_{[\\hat{k}]}\\right\\},\\;\\mathrm{where}\\;\\;\\;\\;}\\\\ {\\hat{k}=\\operatorname*{min}\\{k:w_{[1]}+\\cdot\\cdot\\cdot+w_{[k]}\\geq(k+1)w_{[k+1]}\\},\\;\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with the convention $w_{[L+1]}=0$ . ", "page_idx": 23}, {"type": "text", "text": "6. Set-valued Bayes-optimal prediction [MWDH21]: for any utility $u:[L]\\times2^{[L]}\\setminus\\{\\emptyset\\}\\to\\mathbb{R}_{+}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{SVBOP}_{u}(w):=\\operatorname*{argmax}_{S\\subseteq[L]}\\,\\sum_{\\ell\\in[L]}w_{\\ell}\\cdot u(\\ell,S).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We consider two utility functions based on $u_{65}$ and $u_{80}$ , defined below. ", "page_idx": 23}, {"type": "text", "text": "Evaluation metrics. We evaluate each method based on a variety of metrics. In addition to $\\beta_{\\mathrm{prec}}$ and $\\beta_{\\mathrm{set-size}}$ , defined in Section 4, we assess each method in terms of utility-discounted predictive accuracy [ZCM12]. For a set $S\\subseteq[L]$ and label $\\ell\\in[L]$ , define for some parameters $\\alpha,\\beta>0$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\nu(\\ell,S):={\\bf1}\\left\\{\\ell\\in S\\right\\}\\cdot\\left(\\frac{\\alpha}{|S|}-\\frac{\\beta}{|S|^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "table", "img_path": "M7zNXntzsp/tmp/8d9ef205777c1368e3252ff9a807aab676dc5e35b265ece46b361c78fb2fd76b.jpg", "table_caption": [], "table_footnote": ["Table 2: Results on the Fashion MNIST data set. The table displays the frequency of returning the correct label as a singleton $\\beta_{\\mathrm{correct-single}}$ , average size $\\beta_{\\mathrm{set-size}}$ , utility-discounted predictive accuracies $u_{65}$ and $u_{80}$ , and the superfluous inflation, $\\beta_{\\mathrm{sup}}$ . infl.. For each metric, the symbol $\\nearrow$ indicates that higher values are desirable, while $\\searrow$ indicates that lower values are desirable. Results for the base algorithm are in white, and results for the subbagged algorithm are in gray. Standard errors are in parentheses. "], "page_idx": 24}, {"type": "text", "text": "We use the measures $u_{65}$ with $(\\alpha,\\beta)=(1.6,0.6)$ and $u_{80}$ with $(\\alpha,\\beta)=(2.2,1.2)$ , which respectively give small and large rewards for being cautious [NDMH18]. ", "page_idx": 24}, {"type": "text", "text": "Finally, we directly assess the extent to which each selection rule resorts to returning a non-singleton set on difficult instances. Specifically, we define the superfluous inflation, which, for a selection rule $s$ , is the ratio of the accuracy of the standard argmax given $s$ returns at least two labels divided by the accuracy of $s$ given $s$ returns at least two labels: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\beta_{\\mathrm{sup.\\infl.}}:=\\frac{\\sum_{j=1}^{N}\\mathbf{1}\\left\\{\\tilde{Y}_{j}\\in\\mathrm{argmax}(\\hat{p}(\\tilde{X}_{j})),|s(\\hat{p}(\\tilde{X}_{j}))|\\geq2\\right\\}}{\\sum_{j=1}^{N}\\mathbf{1}\\left\\{\\tilde{Y}_{j}\\in s(\\hat{p}(\\tilde{X}_{j})),|s(\\hat{p}(\\tilde{X}_{j}))|\\geq2\\right\\}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "If this ratio is close to 1, it means the standard argmax is correct as often as $s$ (when the latter expresses uncertainty by returning multiple labels), so outputting multiple labels could be seen as overly conservative. Note that the argmax never returns more than a singleton, so its superfluous inflation is left blank in our results. ", "page_idx": 24}, {"type": "text", "text": "Results. This experiment shows that the inflated argmax combined with the subbagged algorithm $\\widetilde{A}_{m}$ has accuracy (according to a variety of metrics) comparable with several alternative metho d s, does not output overly large sets of candidate labels, and at the same time admits rigorous stability guarantees. That is, our algorithmic framework does not unduly harm empirical performance. ", "page_idx": 24}, {"type": "text", "text": "In Table 2, we present results for each selection rule applied to the base algorithm $\\boldsymbol{\\mathcal{A}}$ and the subbagged algorithm $\\widetilde{A}_{m}$ described in Section 4. The inflated argmax has significantly higher average precision and signifi ca ntly smaller set sizes than all of the alternative set-valued classifiers. These two measures are related, since a selection rule can only have high precision if it frequently returns the correct label as a singleton set. The inflated argmax also has the lowest superfluous inflation, meaning that it tends return multiple labels on difficult instances. The $u_{65}$ of the inflated argmax is also within two standard errors of the $u_{65}$ of $\\mathrm{SVBOP_{65}}$ , which seeks to optimize this utility. Our method does have a significantly lower $u_{80}$ than many of the competing methods, since this utility is more forgiving of returning multiple labels. ", "page_idx": 24}, {"type": "text", "text": "While this experiment considers many different perspectives on set-valued classification, we reiterate that our chief contribution is distribution-free stability guarantees. This means that, regardless of the dataset or base algorithm used, we can guarantee that our method will be stable. In the context of our experiment, Theorem 6 guarantees that $\\begin{array}{r}{\\delta_{j}\\leq\\delta^{*}=\\varepsilon^{-2}\\cdot\\frac{1-1/L}{n-1}\\cdot\\frac{p_{n,m}}{1-p_{n,m}}\\approx0.006}\\end{array}$ for every test point $j=1,\\ldots,N$ . Furthermore, our optimality result shows that the inflated argmax returns a singleton as often as possible among all $\\varepsilon$ -compatible selection rules. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The abstract and Section 1.2 in the introduction clearly state our contributions. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We discuss limitations of the work in Section 5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All the theorems are numbered and cross-referenced in the supplemental material, where the proofs appear. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide all necessary details to reproduce the analysis in Section 4. To reproduce the learning algorithm $\\boldsymbol{\\mathcal{A}}$ , we both provide a reference and release our own documented code using the same architecture. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We attach our code in our submission to OpenReview, and we will deanonymize the link to the Github repository after the review process. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The experimental setting is presented in the core of the paper with enough detail to appreciate the results. The full details are provided with the code. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Results in Table 1 are reported with standard errors. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: In Section 4 we provide details on the type and quantity of compute workers and total time for the experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have reviewed the Code of Ethics and our work conforms to it. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our research contribution is primarily theoretical and does not have immediately foreseeable impacts on society on a large scale. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: PyTorch and Fashion MNIST are properly credited. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]