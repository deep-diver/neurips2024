[{"figure_path": "5DwqmoCE1N/figures/figures_1_1.jpg", "caption": "Figure 1: Using only attention is sufficient for full performance. By copying the attention maps (top) from a MAE [20] pre-trained ViT-L [11], a ViT-L can reach a top-1 accuracy of 85.1 on ImageNet-1K [9] recovering 77.8% of the gap between no transfer (training from scratch, 83.0) and full transfer (fine-tuning all the weights, 85.7). Distilling attention maps (bottom) can even fully match MAE weight tuning while only transferring the inter-token flow.", "description": "This figure demonstrates the effectiveness of attention transfer in achieving comparable performance to full weight fine-tuning in Vision Transformers.  By simply copying or distilling the attention maps from a pre-trained teacher model, a student model can achieve high accuracy on the ImageNet-1K classification task, even when learning features from scratch. The top part shows that copying attention maps recovers 77.8% of the performance gap between training from scratch and fine-tuning.  The bottom part indicates that distilling attention maps fully matches the performance of fine-tuning, highlighting the significance of attention patterns in model training.", "section": "3 Main Results"}, {"figure_path": "5DwqmoCE1N/figures/figures_2_1.jpg", "caption": "Figure 2: Two types of Attention transfer for Vision Transformers. Attention Copy (left): We simply \"copy-and-paste\" the attention maps from a pre-trained teacher model to a randomly initialized student one. Other weights of the student are then trained via supervised learning. This fully decouples inter-token learning (from the teacher) and intra-token learning (in the student); but is less practical. Attention Distillation (right): The student computes its own attention maps, with an additional cross-entropy loss to distill patterns from the teacher during training. The teacher is no longer used during inference. H: number of heads; L: number of Transformer layers.", "description": "This figure illustrates the two methods for attention transfer proposed in the paper: Attention Copy and Attention Distillation.  Attention Copy directly copies the attention maps from a pre-trained teacher network to a randomly initialized student network. The student then learns its own intra-token features while the inter-token interactions are solely determined by the teacher's attention maps. This method is less practical due to the need for both networks during inference. Attention Distillation allows the student network to learn its own attention maps by distilling the attention patterns from the teacher network using a cross-entropy loss. This makes it more practical as only the student network is needed during inference.  The figure visually depicts the architecture and data flow for both methods, highlighting the key differences in how attention maps are handled.", "section": "2 Attention Transfer"}, {"figure_path": "5DwqmoCE1N/figures/figures_5_1.jpg", "caption": "Figure 3: Copy a subset of layers. By default, all 24 ViT-L layers are transferred. Here we only transfer a subset, and find: more layers always helps; and attention maps from top layers are more beneficial than those from bottom layers.", "description": "This figure shows the impact of transferring only a subset of layers when using attention transfer.  The results indicate that using more layers generally improves performance, and that layers closer to the output of the network (top layers) are more crucial than those at the beginning of the network (bottom layers).  It highlights the importance of the higher-level interactions learned during pre-training for downstream tasks.", "section": "4 Analysis"}, {"figure_path": "5DwqmoCE1N/figures/figures_5_2.jpg", "caption": "Figure 4: Copy a subset of heads. The pre-trained ViT-L has 16 heads in each MSA block. By default, all of them are transferred. Here we only transfer a subset, and find more heads helps in general, but performance saturates at 12 heads.", "description": "The figure shows the impact of the number of transferred heads on the accuracy of the model.  The x-axis represents the number of heads transferred, and the y-axis represents the accuracy. The results indicate that increasing the number of transferred heads generally improves accuracy, but the improvement plateaus after 12 heads. This suggests a point of diminishing returns in using more heads, implying that beyond a certain point, the additional information provided by extra heads does not significantly improve model performance.", "section": "4 Analysis"}, {"figure_path": "5DwqmoCE1N/figures/figures_5_3.jpg", "caption": "Figure 1: Using only attention is sufficient for full performance. By copying the attention maps (top) from a MAE [20] pre-trained ViT-L [11], a ViT-L can reach a top-1 accuracy of 85.1 on ImageNet-1K [9] recovering 77.8% of the gap between no transfer (training from scratch, 83.0) and full transfer (fine-tuning all the weights, 85.7). Distilling attention maps (bottom) can even fully match MAE weight tuning while only transferring the inter-token flow.", "description": "This figure demonstrates the effectiveness of attention transfer in Vision Transformers.  It shows that using only the attention patterns from a pre-trained model (rather than the full model weights) achieves comparable performance on downstream tasks like ImageNet classification.  The top part illustrates \"Attention Copy,\" where the attention maps are directly copied. The bottom part shows \"Attention Distillation,\" where the student model learns to mimic the teacher's attention maps.  Both methods significantly outperform training from scratch and nearly match the performance of fine-tuning the entire model.", "section": "3 Main Results"}, {"figure_path": "5DwqmoCE1N/figures/figures_6_1.jpg", "caption": "Figure 1: Using only attention is sufficient for full performance. By copying the attention maps (top) from a MAE [20] pre-trained ViT-L [11], a ViT-L can reach a top-1 accuracy of 85.1 on ImageNet-1K [9] recovering 77.8% of the gap between no transfer (training from scratch, 83.0) and full transfer (fine-tuning all the weights, 85.7). Distilling attention maps (bottom) can even fully match MAE weight tuning while only transferring the inter-token flow.", "description": "This figure demonstrates the effectiveness of attention transfer for Vision Transformers (ViTs). It shows that using only the attention patterns from a pre-trained teacher ViT is enough to achieve comparable downstream performance to fine-tuning the entire model. The top part illustrates Attention Copy where attention maps are directly copied, while the bottom shows Attention Distillation where the student learns to mimic the teacher's attention patterns.  Both methods significantly reduce the performance gap between training from scratch and full fine-tuning.", "section": "3 Main Results"}, {"figure_path": "5DwqmoCE1N/figures/figures_15_1.jpg", "caption": "Figure 7: Attention map similarity across methods. Each column corresponds to a different way of matching up attention heads between two models. The top row shows the Jensen-Shannon divergence (JSD) with respect to the MAE pre-trained teacher, whereas the bottom row shows the JSD with respect to the fine-tuned MAE model. These plots match our intuition on distillation or fine-tuning methods, but copy Q is consistently dissimilar from the PT and FT models. Note that this may be a limitation of this particular analysis, since there are many settings of Q, K, and V that lead to the same layer output.", "description": "This figure compares the similarity of attention maps from different methods (pre-trained, attention distillation, copy Q, copy K, fine-tuned) to those of the pre-trained and fine-tuned MAE models using Jensen-Shannon Divergence (JSD).  Four different head matching methods are shown to illustrate various aspects of the similarity. The results reveal that the copy Q method shows consistently low similarity to both pre-trained and fine-tuned models, despite its strong performance, suggesting potential limitations in the analysis due to the many possible ways to obtain the same layer output.", "section": "4.1 Variants of Attention Transfer"}, {"figure_path": "5DwqmoCE1N/figures/figures_16_1.jpg", "caption": "Figure 8: Visualization of attention maps for different methods. We show what the [CLS] token attends to at various layers within the network. Darker patches indicate more attention weight. Notably, the pre-trained MAE model's attention maps provide a significant prior over what the model should use, separating the object from potentially spurious cues like the branch or the background. In contrast, models right at random initialization (\"init\") start off attending uniformly over the image, which leads the scratch model to use more of the spurious patches. We show more attention map visualizations in Appendix D.", "description": "This figure visualizes the attention maps learned by different methods at various layers for a single image.  It highlights how the pre-trained model uses attention to focus on the main subject, while the randomly initialized and scratch models attend more to background details.  The attention maps from the fine-tuned and attention distillation methods show a combination of these approaches.", "section": "B Additional Analysis"}, {"figure_path": "5DwqmoCE1N/figures/figures_19_1.jpg", "caption": "Figure 11: Attention map visualizations on more examples", "description": "This figure visualizes attention maps for different methods (init, scratch, pre-train/copy, fine-tune, attn. distill) across various layers (1, 13, 23) for two different example images.  The visualization highlights how attention focuses differently based on the method used. It shows the attention patterns from the pre-trained model providing a significant prior over the model\u2019s attention, while the randomly initialized models start by attending uniformly over the image.", "section": "3 Main Results"}, {"figure_path": "5DwqmoCE1N/figures/figures_20_1.jpg", "caption": "Figure 11: Attention map visualizations on more examples", "description": "This figure visualizes attention maps for different methods (init, scratch, pre-train/copy, fine-tune, attn. distill) across various layers (Layer 1, Layer 13, Layer 23) for two example images.  It demonstrates how different training approaches affect the attention patterns learned by the model.  The pre-trained model (pre-train/copy) shows focused attention on relevant parts of the images, whereas the untrained model (init) and the model trained from scratch (scratch) exhibit more diffuse attention patterns. Fine-tuning and attention distillation show results closer to pre-trained model but still differing in their focus.", "section": "Additional Attention Map Visualizations"}]