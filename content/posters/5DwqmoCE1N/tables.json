[{"figure_path": "5DwqmoCE1N/tables/tables_4_1.jpg", "caption": "Table 1: Main results. We show that the pre-trained attention patterns are sufficient to match fine-tuning accuracy on ImageNet. Attention Copy closes most of the gap, and Attention Distillation achieves the same accuracy.", "description": "This table presents the main results of the paper, comparing the performance of different methods on ImageNet-1K classification.  It shows that using only the attention patterns from a pre-trained model (Attention Copy and Attention Distillation) is almost as effective as fine-tuning the entire model, demonstrating the sufficiency of attention maps for achieving high accuracy.  Ensemble methods combining fine-tuning with attention distillation are also shown to achieve even higher accuracy.", "section": "3 Main Results"}, {"figure_path": "5DwqmoCE1N/tables/tables_4_2.jpg", "caption": "Table 2: Transfer other attention activations. We test copying alternative attention activations other than the attention map \u2013 softmax(QKT). All alternatives do better than training from scratch, and transferring queries Q actually does better than transferring the attention map.", "description": "This table presents the results of experiments where different parts of the self-attention mechanism (queries Q, keys K, values V) are transferred from a pre-trained teacher model to a student model during training.  The goal is to determine the impact of each component on downstream task performance. The table shows that transferring only the queries Q achieves the best performance, exceeding that of transferring the attention map itself, demonstrating that the queries Q are the most important element in attention transfer for downstream tasks.", "section": "4 Analysis"}, {"figure_path": "5DwqmoCE1N/tables/tables_7_1.jpg", "caption": "Table 4: Different pre-training datasets. We pre-train MAE on more datasets, and then either fine-tune or do copy for ImageNet-1K classification. Attention transfer works well when the data distribution stays stable, but its effectiveness is more negatively affected by distribution shifts.", "description": "This table presents the ImageNet-1k classification accuracy results using different pre-training datasets (ImageNet, ImageNet-22K, COCO) and evaluation datasets (iNaturalist 2017, iNaturalist 2018, iNaturalist 2019).  It compares the performance of fine-tuning and attention transfer methods (copy and distill) against a baseline of training from scratch. The results highlight the impact of distribution shifts between pre-training and downstream tasks on the effectiveness of attention transfer. When the pre-training and evaluation datasets are consistent (e.g., both ImageNet), attention transfer achieves comparable accuracy to fine-tuning.  However, when there's a distribution shift (e.g., pre-training on ImageNet and evaluating on iNaturalist), the effectiveness of attention transfer decreases significantly.", "section": "5 Generalization and Limitations"}, {"figure_path": "5DwqmoCE1N/tables/tables_7_2.jpg", "caption": "Table 6: Out-of-distribution robustness. We take two models that achieve the same accuracy on ImageNet-1K (fine-tuned and distilled), and evaluate them on a suite of distribution shifts. Attention Distillation does well when the distribution is close (e.g., on ImageNet-V2), but loses the mild \"effective robustness\" that fine-tuned MAE has been found to have [13].", "description": "This table shows the out-of-distribution robustness of fine-tuned and distilled models.  The models were first trained to achieve the same accuracy on ImageNet-1K, and then tested on four datasets representing different distribution shifts: ImageNet-A, ImageNet-R, ImageNet-S, and ImageNet-V2.  The results show that while Attention Distillation performs well on similar distributions (ImageNet-V2), its performance degrades more significantly than the fine-tuned model's when facing greater distribution shifts.", "section": "5 Out-of-distribution robustness"}, {"figure_path": "5DwqmoCE1N/tables/tables_8_1.jpg", "caption": "Table 8: Different model size with MAE pre-trained on ImageNet-1K. Similar to weight tuning, the classification accuracy of attention transfer scales well as we vary the model size, while scratch training saturates.", "description": "This table presents the ImageNet-1K classification accuracy results using different sized Vision Transformers (ViT-B, ViT-L, ViT-H).  Three different training methods were used: fine-tuning, attention copy, and attention distillation.  The results show that attention transfer methods scale effectively with model size, achieving comparable accuracy to fine-tuning, unlike training from scratch which saturates at smaller model sizes.", "section": "Main Results"}, {"figure_path": "5DwqmoCE1N/tables/tables_8_2.jpg", "caption": "Table 9: Object detection results on COCO with a MAE ViT-B pre-trained on COCO. Attention transfer achieves a majority of the gains of pre-training in this setting as well.", "description": "This table presents the results of object detection experiments on the COCO dataset using a Vision Transformer (ViT-B) model pre-trained with Masked Autoencoding (MAE).  It compares the performance of three different training methods: training from scratch, fine-tuning the pre-trained model, and using attention transfer. The results are measured using two metrics: Average Precision (AP) for bounding boxes (bbox) and Average Precision (AP) for segmentation masks (mask).  The numbers in parentheses show the improvement over the scratch method for each metric. The table demonstrates that attention transfer achieves a substantial portion of the performance gains obtained by fine-tuning, showcasing its effectiveness even in a more complex task like object detection.", "section": "Main Results"}, {"figure_path": "5DwqmoCE1N/tables/tables_13_1.jpg", "caption": "Table 10: Number of parameters activations transferred per example for a ViT-L teacher.", "description": "This table presents two different ways to estimate the number of activations transferred during attention transfer in a ViT-L teacher model. The first method considers the dimensions of the query (Q), key (K), and attention map (QKT) to calculate the total number of parameters.  The second approach takes into account that QKT is low-rank, resulting in a reduced parameter count. Both methods show a significant number of transferred activations.", "section": "A Key Numbers"}, {"figure_path": "5DwqmoCE1N/tables/tables_13_2.jpg", "caption": "Table 11: Training cost of Attention Transfer.", "description": "This table compares the memory usage and time per iteration for different training methods using a ViT-L model and a batch size of 16 on a 16GB NVIDIA GP100.  It shows that weight fine-tuning is the most memory-efficient but also the fastest.  Knowledge distillation and attention transfer have similar memory requirements and iteration times, which are slightly higher than fine-tuning. ", "section": "2.2 Attention Copy"}, {"figure_path": "5DwqmoCE1N/tables/tables_14_1.jpg", "caption": "Table 12: Aggregated attention transfer. The full set of attention maps from the teacher has shape (examples, layers, heads, query tokens, key tokens). Here we try to average over each of these axes before the transfer. Performance drops the most when averaging over examples or query tokens, indicating that these are the most important aspects of the attention maps.", "description": "This table shows the results of experiments where different dimensions of the attention maps are averaged before transfer to the student.  Averaging across examples or query tokens significantly reduces performance, highlighting the importance of preserving the per-example, per-token detail in the attention maps.", "section": "4.1 Variants of Attention Transfer"}, {"figure_path": "5DwqmoCE1N/tables/tables_14_2.jpg", "caption": "Table 13: Comparison with knowledge distillation. We try knowledge distillation from the pre-trained teacher by adding an auxiliary MSE loss on the residual stream output. We find that this does much worse than Attention Distillation.", "description": "This table compares the performance of Attention Distillation with a standard knowledge distillation method where the residual stream features are distilled.  The results show that Attention Distillation significantly outperforms standard knowledge distillation, achieving an accuracy of 85.7 compared to 81.3.  This highlights the importance of transferring attention maps specifically for downstream tasks rather than relying on a general feature distillation approach.", "section": "4 Analysis"}, {"figure_path": "5DwqmoCE1N/tables/tables_17_1.jpg", "caption": "Table 1: Main results. We show that the pre-trained attention patterns are sufficient to match fine-tuning accuracy on ImageNet. Attention Copy closes most of the gap, and Attention Distillation achieves the same accuracy.", "description": "This table presents the main results of the paper, comparing the accuracy of different methods on the ImageNet dataset.  It demonstrates that using only the attention patterns from a pre-trained model (Attention Copy and Attention Distillation) achieves comparable accuracy to fine-tuning the entire model.  The results highlight the significance of attention mechanisms in Vision Transformers.", "section": "3 Main Results"}, {"figure_path": "5DwqmoCE1N/tables/tables_17_2.jpg", "caption": "Table 14: Decoupling attention maps from features. We try Attention Distillation with various pre-trained students.", "description": "This table presents the ImageNet-1k top-1 accuracy results of using Attention Distillation with different combinations of pre-trained teacher and student models.  The results demonstrate that the quality of pre-trained attention maps (from the teacher) significantly impacts performance, regardless of whether the student model is randomly initialized or pre-trained. The best performance is achieved when the teacher is pre-trained with FLIP and the student is randomly initialized, highlighting that the attention maps are the key factor that determines the overall performance.", "section": "4 Analysis"}, {"figure_path": "5DwqmoCE1N/tables/tables_18_1.jpg", "caption": "Table 15: Training recipe for Attention Copy on ViT-L.", "description": "This table presents the hyperparameters used for training the Vision Transformer (ViT-L) model using the Attention Copy method.  It details the optimizer, learning rate schedule, weight decay, momentum, batch size, data augmentation techniques (RandAug, Mixup, Cutmix), label smoothing, and other relevant settings.  The table provides a comprehensive overview of the training configuration used for this specific experiment in the paper.", "section": "C Implementation Details"}, {"figure_path": "5DwqmoCE1N/tables/tables_18_2.jpg", "caption": "Table 16: Training recipe for Attention Distillation on ViT-L.", "description": "This table details the hyperparameters used for training the Vision Transformer (ViT-L) model using the Attention Distillation method.  It includes settings for the optimizer (AdamW), learning rate, weight decay, momentum, batch size, learning rate schedule, warmup epochs, training epochs, data augmentation techniques (RandAug), label smoothing, mixup and cutmix regularization, dropout, exponential moving average (EMA), the number of layers where attention maps are copied, and the weight of the distillation loss.", "section": "Implementation Details"}]