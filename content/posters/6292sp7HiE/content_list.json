[{"type": "text", "text": "One-shot Federated Learning via Synthetic Distiller-Distillate Communication ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Junyuan Zhang1,2 Songhua Liu1 Xinchao Wang1\u2217 National University of Singapore1 Beihang University2 junyuanpk@gmail.com, songhua.liu@u.nus.edu, xinchao $@$ nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "One-shot Federated learning (FL) is a powerful technology facilitating collaborative training of machine learning models in a single round of communication. While its superiority lies in communication efficiency and privacy preservation compared to iterative FL, one-shot FL often compromises model performance. Prior research has primarily focused on employing data-free knowledge distillation to optimize data generators and ensemble models for better aggregating local knowledge into the server model. Prior research has primarily focused on employing datafree knowledge distillation to optimize data generators and ensemble models for better aggregating local knowledge into the server model. However, these methods typically struggle with data heterogeneity, where inconsistent local data distributions can cause teachers to provide misleading knowledge. Additionally, they may encounter scalability issues with complex datasets due to inherent twostep information loss: first, during local training (from data to model), and second, when transferring knowledge to the server model (from model to inversed data). In this paper, we propose FedSD2C, a novel and practical one-shot FL framework designed to address these challenges. FedSD2C introduces a distiller to synthesize informative distillates directly from local data to reduce information loss and proposes sharing synthetic distillates instead of inconsistent local models to tackle data heterogeneity. Our empirical results demonstrate that FedSD2C consistently outperforms other one-shot FL methods with more complex and real datasets, achieving up to $2.6~\\times$ the performance of the best baseline. Code: https:// github.com/Carkham/FedSD2C ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) has emerged as a cutting-edge technology that enables training a global model across multiple clients without sharing their raw data [1]. Original FL requires multiple communication rounds for exchanging information between clients and servers. While this paradigm yields a better global model by frequent communication, such high communication costs along with the risk of connection drop errors make it impractical and intolerable in real-world FL applications [2, 3, 4]. Moreover, frequent communication poses security risks such as man-in-the-middle attacks [5] and privacy concerns [6]. ", "page_idx": 0}, {"type": "text", "text": "To address these issues, one-shot FL [7] has been proposed, requiring only a single communication round, significantly reducing communication costs and concurrently diminishing vulnerability to malicious interception. Due to one communication round property, One-shot FL can also be easily scaled up to large-scale client scenarios, especially for cross-device settings [3]. Despite its sufficient benefits, the limitation of a single communication round makes one-shot FL fall short in accuracy compared to conventional multiple-round FL. ", "page_idx": 0}, {"type": "text", "text": "(1) $\\textcircled{1}\\textcircled{2}$ Two-tier information loss ", "text_level": 1, "page_idx": 1}, {"type": "image", "img_path": "6292sp7HiE/tmp/2e20a4ce953b8ddd69a1d43a7e0a966e547c41089d40c03effecabcc9effaacd.jpg", "img_caption": ["Figure 1: Illustration of issues in one-shot FL based on DFKD: (1) Information loss occurs during the transfer from local data to the model and from the model back to the inversed data. (2) t-SNE plots of feature distributions of data generated by DENSE(left $\\blacktriangle$ ), Co-Boosting(middle \u25a0), and our FedSD2C(right $\\bigstar$ ). We randomly select five different classes (indicated by different colors) of real and synthetic data from Tiny-ImageNet. Bad samples are data generated by the DFKD-based method that deviates from the distribution of local real data. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To facilitate effective knowledge transfer from client models to the server model within a single round, most previous one-shot FL methods [7, 8, 9, 10] have focused on knowledge distillation. Early approaches [11, 7] use knowledge distillation to transfer knowledge from an ensemble of client models [12] to the server model. While effective, these methods often require additional public datasets, which can be cumbersome or unfeasible in real-world scenarios [13]. Alternatively, data-free knowledge distillation (DFKD) is introduced to avoid the need for public datasets. For example, DENSE [8] employs Generative Adversarial Networks (GANs) [14] as data generators and an ensemble of client models as a discriminator to synthesize diverse data for knowledge transfer to server model in a data-free manner. Co-Boosting [10] extends DENSE by proposing a mutually reinforcing approach to enhance synthetic data and the ensemble model for server training. ", "page_idx": 1}, {"type": "text", "text": "Nevertheless, such a method of generating data implies two-tier information loss. First, due to model capacity limitations, client models may struggle to encapsulate all information about local data, affecting the quality of the generated data. Second, the generated data do not fully represent the information within the model, as they are produced from random noise without explicit guidance. Therefore, some classes of synthetic data cannot have similar feature distributions to the original real data, as depicted in Figure 1. Moreover, data heterogeneity [15] in FL can result in inconsistent and misleading predictions from local models [16], which has been shown to hinder knowledge distillation [17, 18]. Consequently, the server model trained on such noisy and information-lossy generated data typically suffers significant performance degradation, particularly on complex datasets [19]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose FedSD2C (One-shot Federated Learning via Synthetic Distiller-Distillate Communication), a novel and practical one-shot FL framework that introduces a pre-defined distiller for informative, privacy-enhanced, and communication-efficient distillate communication. In specific, FedSD2C first adopts a $\\nu$ -information [20] based Core-Set selection method to distill the local dataset into an informative Core-Set. By capturing the diversity and realism through $\\mathcal{V}$ -information, the distilled Core-Set fully encapsulates the information of the local data domain for training a robust server model. However, directly transmitting the Core-Set, which may include the original samples, poses potential privacy risks and incurs significant communication costs, especially for highresolution images. In this regard, FedSD2C employs two techniques to further distill the Core-Set into distillates, thereby enhancing privacy and reducing communication costs: 1) Utilizing Fourier transform perturbation to alter the amplitude components of the Core-Set samples for distillate initialization, enhancing privacy while retaining semantic content; 2) Employing a pre-trained Autoencoder [21] provided by the server as a distiller to distill the perturbed Core-Set into distillates and optimizing its $\\nu$ -information to be as close as possible to original Core-Set, thus minimizing information loss. Finally, clients transmit synthetic distillates to the server instead of inconsistent models for knowledge transfer. Compared to generating noisy knowledge from inconsistent client models with two-tier information loss, end-to-end distillate synthesis minimizes information loss and their aggregation mitigates the impact of data heterogeneity. Through extensive experiments over various real-world datasets , we show that our proposed method significantly surpasses the generated-based one-shot FL methods. The contributions of this paper are: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We propose a new one-shot FL framework named FedSD2C which proposes to share synthetic distillates instead of generating noisy data from inconsistent models for server-side training.   \n\u2022 To mitigate the potential of privacy leakage and reduce communication costs, we propose two techniques: distillate initialization with Fourier transform perturbation and distillate synthesis with a pre-trained Autoencoder.   \n\u2022 We conduct extensive experiments over various datasets and settings. The results demonstrate the effectiveness of the proposed method which achieves up to $2.7~\\times$ the performance of the best baseline. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 One-shot Federated Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "One-shot federated learning was first proposed by [22], which introduces a method to aggregate a server model by distilling knowledge from an ensemble of client models using public datasets. FedKT [11] propose a hierarchical knowledge transfer framework, enabling various types of classification models. While their approaches demonstrate promising results, the requirement of public datasets which is inaccessible for privacy or transmission reasons limits their practical applications. To address the limitations associated with public datasets, DENSE [8] introduces a DFKD process utilizing an additional data generator trained on the ensemble model. Considering the challenge of high statistical heterogeneity, FedCVAE [9] proposes replacing the local training task with training conditional Variation Autoencoders. Furthermore, Co-Boosting [10] aims to enhance the performance of both the data generator and the ensemble model through a two-tier process. Despite these advancements, generating data through DFKD involves a two-tier information loss. Simultaneously, the inconsistency among client models due to data heterogeneity [15, 23, 24, 25], further degrades the quality of generated data, introducing label noise and thus limiting the performance of the server model. In this work, we tackle these problems from the perspective of sharing synthetic distillates. By utilizing Core-Set selection and pre-trained Autoencoders as distillers, our proposed methods distill diverse and informative data for server training. ", "page_idx": 2}, {"type": "text", "text": "2.2 Dataset Distillation in Federated learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Dataset Distillation (DD) was first introduced by [26], aiming to distill the knowledge of datasets into synthetic data while preserving the performance of the model trained on it. Early dataset distillation methods are formulated as a bi-level optimization problems [27], where the outer loop optimize the synthetic data via gradient matching [28, 29, 30], distribution matching [31, 32] and performance matching [26], while the inner loop progressively trains a model on the synthetic data. Considering computational resources constraints, single-level optimization methods [33, 34] based on kernel ridge regression are proposed to decouple the bi-level optimization, thereby reducing training cost. These methods demonstrate comparable performance in non-complex datasets like CIFAR10 and are implemented in FL to tackle communication bottlenecks [35, 36], data heterogeneity [37, 38, 39] and one-shot FL [40, 36]. However, these methods require significant computational resources, making them impractical for edge devices with limited capability in FL. Additionally, they may struggle to effectively distill high-resolution datasets. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We proposed FedSD2C to alleviate the two-tier information loss inherent in one-shot FL methods based on DFKD and account for data heterogeneity by synthetic distiller-distillate communication. ", "page_idx": 2}, {"type": "image", "img_path": "6292sp7HiE/tmp/62340c17ab04f0266e8929cd24baec921bf57c997df8ce64abb2e3a34eea4b41.jpg", "img_caption": ["Figure 2: Framework of proposed FedSD2C "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "The details of FedSD2C are described in Figure 2 and Algorithm 1. In the preparation phase of one-shot FL, the server distributed a pre-trained Autoencoder [21] as the distiller to each client. Subsequently, clients synthesize informative, privacy-enhanced, and communication-efficient distillates for server-side training. Specifically, to ensure the distillates fully encompass local information, clients first utilize a $\\nu$ -information-based Core-Set selection method to extract diverse and informative Core-Set from their local data domains. Aiming to further reduce the communication costs and enhance the privacy of distillates, clients then perturb the Core-Set with Fourier transform for distillate initialization and employ the received pre-trained Autoencoder to optimize distillates in a compact latent space via $\\nu$ -information alignment with the Core-Set. Finally, clients transmit the distillates, and the server decodes them using the pre-trained Autoencoder for training. We will now delve into the details of each component. ", "page_idx": 3}, {"type": "text", "text": "3.2 $\\nu$ -information based Core-Set Selection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "$\\nu$ -information [20] was first proposed to measure the mutual information between $X$ and $Y$ constrained on predictive family $V$ which is denoted as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nI_{\\mathcal{V}}(X\\to Y)=H_{\\mathcal{V}}(Y|\\mathcal{D})-H_{\\mathcal{V}}(Y|X)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $H_{\\mathcal{V}}(Y|\\mathcal{O})$ and $H_{\\mathcal{V}}(Y|X)$ denote the predictive $\\mathcal{V}$ -entropy conditioned on $\\mathcal{Q}$ or $X$ ", "page_idx": 3}, {"type": "text", "text": "Core-Set selection is a type of dataset distillation method that focuses on preserving a subset of the original training dataset containing only valuable or representative samples. The objective is to enable models trained on this subset to achieve performance similar to those trained on the entire dataset. Typically, this is achieved by minimizing certain criteria, such as data distribution [41]. In our case, the emphasis lies on maximizing the diversity and information content of the subset. Therefore, in the context of $\\mathcal{V}$ -information, Core-Set selection can be reformulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n(X_{s},Y_{s})=\\operatorname*{arg\\,max}_{X,Y}I_{\\mathcal{V}}(X_{t}\\to Y_{t})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $X_{t}$ and $Y_{t}$ denotes the images and labels in the original datasets and $(X_{s},Y_{s})$ denotes the selected Core-Set. The intuition behind this equation is that Core-Set should include sufficient information and provide a concise representation corresponding to original datasets, constrained by observers $\\mathcal{V}$ . ", "page_idx": 3}, {"type": "text", "text": "Inspired by [42], we maximize the $\\nu$ -information of the Core-Set from two levels. First, we identify the most informative image segments within each image by evaluating patches extracted at various scales from each image. Second, we select the top-ipc with the highest $\\nu$ -information for each class to construct the final Core-Set. The algorithm description can be found at Appendix A. Since a model pre-trained on original datasets can serve as an optimal observer for approximating the $\\nu$ -information [42], we proposed using local pre-trained models as the observer models (predictive family $\\nu$ ) to conduct $\\mathcal{V}$ -information-based Core-Set selection on local datasets. Given that pre-trained local models are commonly present in one-shot FL [8, 7], their utilization does not violate the practicality of our approach. ", "page_idx": 3}, {"type": "text", "text": "3.3 Distilling Core-Set into Distillates with Pre-trained Autoencoders ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we will describe how to synthesize informative, privacy-enhanced, and communicationefficient distillates using pre-trained Autoencoders. Although Core-Set significantly reduces dataset size, communication costs remain a challenge for high-resolution data in real-world applications. Moreover, while sharing Core-Set provides consistent resistance to membership inference attacks [43], transmitting patches may still risk exposing sensitive information of original images. Therefore, further enhancing the privacy of shared data is essential. To alleviate these concerns, we propose two novel techniques: 1) distillate initialization with Fourier transform perturbation, which alters the amplitude components of the Core-Set samples to enhance privacy while retaining semantic content; and 2) distillate synthesis with pre-trained Autoencoders which act as the distillers. The pre-trained Autoencoder converts the perturbed samples into distillates by optimizing their $\\nu$ -information to be as close as possible to the original Core-Set samples, minimizing information loss. We will now discuss each component in detail below. The process is described in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Distillate initialization with Fourier transform perturbation. A typical privacy-enhanced technique for sharing synthetic data is adding noise [44]. Although this approach can blur the visual information of the synthetic data, it can also destroy important semantic information and seriously degrade model performance. Our approach leverages a well-known property of the Fourier transform: the phase component of the Fourier spectrum encodes high-level semantic information, whereas the amplitude component captures low-level details [45, 46]. Inspired by this, we propose a novel privacy-enhanced method that perturbs the amplitude components in Core-Set samples through the Fourier transform to reduce visual information while preserving semantic information. Given an image $x$ , its Fourier transform can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}(x)=\\mathcal{A}(x)\\times e^{-j\\times\\mathcal{P}(x)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\mathcal{A}}(x),{\\mathcal{P}}(x)$ depict the amplitude and phase components respectively. We then perturb the amplitude information via linearly interpolating: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{A}}(x)=(1-\\lambda)\\mathcal{A}(x)+\\lambda\\mathcal{A}(x^{*})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the $\\lambda$ is a scaling coefficient and $x^{*}$ can be other images or random noise. Then, we combine the perturbed amplitude spectrums with the original phase component to generate the perturbed Core-Set sample: ", "page_idx": 4}, {"type": "equation", "text": "$$\nx=\\mathcal{F}^{-1}(\\hat{\\mathcal{A}}(x)\\times e^{-j\\times\\mathcal{P}(x)})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\mathcal{F}}^{-1}(x)$ defines the inverse Fourier transform which can be calculated with the FFT algorithm [47] effectively. ", "page_idx": 4}, {"type": "text", "text": "Distillate synthesis with pre-trained Autoencoders. The Fourier transform perturbation serves to protect visual privacy but also compromises the realism of images, resulting in inconsistent $\\nu$ - information between the perturbed Core-Set and the original Core-Set. To address this issue, we propose optimizing the alignment of the $\\nu$ -information between the perturbed Core-Set and the original Core-Set to reconstruct key information. One straightforward and efficient approach is to optimize the perturbed Core-Set in pixel space [48, 49]. However, this method can be prone to overftiting into high-frequency patterns that only match the observer model [50]. Such overftiting is detrimental to training a global model due to inconsistent local models caused by data heterogeneity. Leveraging the powerful priors obtained from large-scale datasets, a pre-trained Autoencoder can decode latent representations into generalizable images. As a result, optimizing in the latent space acts as a regularization method that encourages synthetic data to be more generalizable, thereby making pre-trained Autoencoder an ideal distiller for local Core-Sets distillation. Furthermore, compact latent representations can reduce communication costs and mitigate privacy leakage if the latent is intercepted by attackers. Consequently, we employ a pre-trained Autoencoder on the client to distill the Core-Set into informative, privacy-enhanced, and communication-efficient distillates, and transmit them to the server for model training. On the server side, they are decoded by the decoder and are expected to maintain similar $\\nu$ -information to the original Core-Set from the perspective of the observer model while remaining visually unidentifiable. ", "page_idx": 4}, {"type": "text", "text": "In specific, in the preparation phase of one-shot FL, the server first distributes a pre-trained Autoencoder to $n$ clients, denoted as $\\mathcal{E}$ and $\\mathcal{D}$ for encoder and decoder, respectively. Each client $i$ then conducts $\\nu$ -information-based selection to construct a Core-Set $(X_{s}^{i},\\bar{Y}_{s}^{i}),i=\\bar{1},2\\cdots\\,,n$ with diverse information regarding the original local datasets. Subsequently, client $i$ learns a latent set $\\mathcal{Z}^{i}=\\{z_{j}\\}_{j=1}^{|\\mathcal{Z}^{i}|}$ initiialized by $\\{\\mathcal{E}(x_{j}^{i})\\}_{j=1}^{|X_{s}^{i}|},x_{j}^{i}\\in X_{s}^{i}$ which is perturbed with Fourier transform, such that the $\\{\\mathcal{D}(z_{j}^{i})\\}_{j=1}^{|\\mathcal{Z}^{i}|}$ is as close as possible to the corresponding data in the Core-Set: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\small\\arg\\operatorname*{min}_{\\mathcal{Z}^{i}}\\left\\|I_{\\mathcal{V}^{i}}(X_{s}^{i}\\rightarrow Y_{s}^{i})-I_{\\mathcal{V}^{i}}(\\{\\mathcal{D}(z_{j}^{i})\\}_{j=1}^{|\\mathcal{Z}^{i}|}\\rightarrow Y_{s}^{i})\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Require: Client local model $f^{i}(h^{i}(\\cdot))$ , pre-trained VAE $(\\mathcal D,\\mathcal E)$ , Server model parameter $\\theta$ , number   \nof clients $n$ , training iterations of local synthesis $T_{s y n}$ , training iterations of server model $T_{t r n}$ ,   \nlearning rate $\\eta_{s y n}$ and $\\eta_{t r n}$   \n1: Server distributes pre-trained VAE $(\\mathcal D,\\mathcal E)$ to $n$ clients.   \n2: for each client $i=1,\\cdots,n$ do   \n3: $(X_{s}^{i},Y_{s}^{i})\\gets\\mathrm{CoresetSelection}(i)$ \u25b7See Algorithm 2 in Appendix   \n4: for each $x_{j}^{i}\\in X_{s}^{i}$ do   \n5: Perturb $\\boldsymbol{x}_{j}^{i}$ via Equations (3), (4) and (5)   \n6: end for   \n7: Initialize latent set $\\mathcal{Z}^{i}=\\{\\mathcal{E}(x_{j}^{i})\\}_{j=1}^{|X_{s}^{i}|}$ using pre-trained VAE decoder $\\mathcal{E}$   \n8: for $t=1,\\cdot\\cdot\\cdot T_{s y n}$ do   \n9: for mini-batch $(z^{i},x_{s}^{i})\\in(\\mathcal{Z}^{i},X_{s}^{i})$ do   \n10: Compute synthetic loss $\\mathcal{L}_{s y n}$ based on Equation (7)   \n11: zi \u2190zi \u2212\u03b7syn\u2207ziLsyn   \n12: end for   \n13: end for   \n14: Generate soft label for each synthetic latents $Y_{s}^{i}=\\{f^{i}(h^{i}({\\mathcal{D}}(z_{j}^{i})))\\}_{j=1}^{|{\\mathcal{Z}}^{i}|}$   \n15: Transmit $S^{i}=(\\mathcal{Z}^{i},Y_{s}^{i})$ to the server   \n16: end for   \n17: Combine client synthetic data into $S=(\\mathcal{Z},Y_{s})=(\\mathcal{Z}^{1}\\cup\\cdot\\cdot\\cdot\\cup\\mathcal{Z}^{n},Y_{s}^{1}\\cup\\cdot\\cdot\\cdot\\cup Y_{s}^{n})$   \n18: for $t=1,\\cdot\\cdot\\cdot\\,,T_{t r n}$ do   \n19: for mini-batch $(z,y)\\in(\\mathcal{Z},Y_{s})$ do   \n20: Compute $\\mathcal{L}_{t r n}$ based on Equation (8)   \n21: $\\theta\\leftarrow\\theta-\\eta_{t r n}\\nabla_{\\theta}\\mathcal{L}_{t r r}$ 2   \n22: end for   \n23: end for ", "page_idx": 5}, {"type": "text", "text": "As Core-Set selection employs the local pre-trained models as the observer models $\\nu^{i}$ , the reformulated Equation (6) and objective function can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s y n}=\\left\\|\\frac{{\\underset{s^{i}}{\\operatorname{arg\\,min}}}\\left\\|h_{i}(\\mathcal{D}(z^{i}))-h_{i}(x^{i})\\right\\|^{2}}{2^{i}}\\right\\|\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $h_{i}(\\cdot)$ denotes the feature extractor of pre-trained local model of client $i$ , and $\\boldsymbol{x}_{j}^{i}$ and $z_{j}^{i}$ are paired. By minimizing $\\mathcal{L}_{s y n}$ , we synthesize a set of latent variables $\\mathcal{Z}^{i}=\\{z_{j}\\}_{j=1}^{|\\mathcal{Z}^{i}|}$ that contains diverse information of local data domains. ", "page_idx": 5}, {"type": "text", "text": "Finally, clients transmit the latent set $\\mathcal{Z}^{i}$ along with the corresponding soft label $Y_{s}^{i}$ predicted by local models to the server. The server combines the synthetic local distillates from each client $S=(\\mathcal{Z},Y_{s})$ . It then uses decoder $\\mathcal{D}$ to reconstruct the images from data $(z,y)\\in(\\mathcal{Z},Y_{s})$ and distills the knowledge by minimizing the following objective function: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t r n}=\\sum_{(z,y)\\in(\\mathcal{Z},Y_{s})}K L(f(h(\\mathscr{D}(z))),y)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f(h(\\cdot))$ denotes the server model. By minimizing the KL loss, we can transfer the local knowledge in the distillate to the server model. ", "page_idx": 5}, {"type": "text", "text": "Discussion on privacy. We first consider whether an attacker can train a performant model with the intercepted distilled data and labels during transmission [40, 9]. Because the attacker cannot know that the distilled data is encoded by VAE, nor can the attacker access the pre-trained VAE encoder, which can be easily achieved by being predefined offilne or via encryption, it is hard for the attacker to reproduce an effective model. For model inversion and membership inference attacks, according to [51], there has been no prior research has performed these attacks solely using distilled data and labels and previous works [52, 36, 35, 43] have also revealed the advantage of dataset distillation in this regard. Therefore, we employ the synthetic data as the reconstructed samples for the evaluation of model inversion attacks. Furthermore, we compare our proposed Fourier transform perturbation with other privacy-enhanced techniques, including adding random noise to synthetic samples and data augment [53]. Experimental results can be found in Section 4.3.1. ", "page_idx": 5}, {"type": "table", "img_path": "6292sp7HiE/tmp/bd960eb87ba5c1acc781cf9de0157a8fb95d9cb6a310d91d11672c8a564e81c7.jpg", "table_caption": ["Table 1: Accuracy of different one-shot FL methods over three datasets with ConvNet and ResNet-18. We vary the $\\alpha=\\{0.1,0.3,0.5\\}$ to simulate different levels of data heterogeneity for Tiny-ImageNet and Imagenette and use pre-defined splits for OpenImage. \"Central\" means that clients send all their local data to the server for centralized training, representing the upper bound of model performance. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets and partitions. We conduct experiments on three real-world image datasets with different ranges of resolution including Tiny-ImageNet [54], ImageNette [55], and OpenImage [56]. TinyImageNet contains 10000 images of $64\\!\\times\\!64$ resolution across 200 classes. ImageNette is a widely used subset of 10 classes from ImageNet-1K [57] with 9469 color images, resized to $128\\!\\times\\!128$ . OpenImage is a large-scale real-world vision dataset with over 9 million images of $256\\!\\times\\!256$ resolution. To simulate data heterogeneity in real-world applications of one-shot FL, we use Dirichlet distribution to generate non-IID data to generate non-IID local data, as in [58] for Tiny-ImageNet and ImageNette. Specifically, for client $i$ , we sample $p_{k}^{i}\\ D i r(\\alpha)$ to allocate a $p_{k}^{i}$ proportion of class $k$ to client $i$ . The parameter $\\alpha$ controls the degree of data heterogeneity, with smaller $\\alpha$ indicating severe data heterogeneity. The $\\alpha$ is set to 0.1 by default unless otherwise stated. For OpenImage, we randomly choose $n$ real-world clients from FedScale [59] and use their corresponding test sets to form global sets. We set the default number of clients $n$ to 10, unless otherwise specified. ", "page_idx": 6}, {"type": "text", "text": "Baseline methods and Configurations. We compare our proposed FedSD2C with existing methods: FedAVG [1], DENSE [8] and Co-Boosting [10]. Following [8, 10], we also introduce DAFL [60] with one-shot FL settings, denoted as F-DAFL. We use two different model architectures: ConvNet [28] and ResNet-18 [61] for all methods. In FedSD2C, the image per class ipc is set to 50 for TinyImageNet and ImageNette, and 10 for OpenImage. We set the Fourier transform coefficient $\\lambda=0.8$ and use a public pre-trained Autoencoder from Stable Diffussion [62] by default for all tasks. For distillate synthesis, we set $T_{s y n}=50$ , $\\eta_{s y n}=0.1$ by default. $\\eta_{t r n}$ is set to 0.2 for Tiny-ImageNet and 0.02 for ImageNette and OpenImage. More experimental details can be found in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "4.2 Evaluation Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To evaluate the effectiveness of our method, we conduct experiments under various non-IID settings with $\\alpha\\,=\\,\\{0.1,0.3,0.5\\}$ for Tiny-ImageNet and Imagenette and pre-defined splits [59] for OpenImage. As illustrated in Table 1, our proposed FedSD2C surpasses all other methods in most settings. In particular, under extreme data heterogeneity $\\alpha=0.1$ ), FedSD2C achieves up to $1.3\\times$ , $2.6\\,\\times$ , and $1.8\\times$ the accuracy of the best baseline on ImageNette, Tiny-ImageNet and OpenImage, respectively. This superior performance is attributed to FedSD2C\u2019s approach of sharing synthetic distillates rather than inconsistent local models, thereby mitigating the impact of data heterogeneity. Moreover, FedSD2C demonstrates the independence from model structures. In contrast, other methods struggle to adapt to different model structures and complex datasets. For instance, at $\\alpha=0.5$ , Co-Boosting with ResNet-18 achieves only half the accuracy of ConvNet on ImageNette, whereas FedSD2C maintains consistent performance. This discrepancy arises because differences in model capacity affect their ability to condense local knowledge and the two-tier information loss during data generation increases the difficulty of transferring local knowledge to the server model, resulting in poor robustness to complex datasets and varied networks. In contrast, the shared distillates in FedSD2C are synthesized through end-to-end local distillation, mitigating information loss during knowledge transfer. ", "page_idx": 6}, {"type": "table", "img_path": "6292sp7HiE/tmp/8d45744eb09c78607bf2f0eab4fe3da7e509de1a80b28e5a587557ac59e5d0ff.jpg", "table_caption": ["Table 2: Accuracy, PSNR and SSIM of FedSD2C combining different privacy-enhanced techniques. Laplace and Gaussian indicate adding corresponding noise into synthetic distillates without Fourier transform initialization. FedMix denotes averaging two real samples from Core-Set to synthesize data. \"-\" indicates no privacy-enhanced technique is combined. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Analysis of Our Method ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.3.1 Privacy Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For privacy evaluation, we consider an honest-but-curious server attempting to reconstruct client data from distillates. We compare our proposed FedSD2C with other privacy-enhanced techniques for sharing synthetic data, including adding random noise [44, 63] and FedMix [53]. In the random noise approach, we incorporate it into FedSD2C by removing the Fourier transform perturbation and instead directly using Core-Set samples for initialization. We then add random noise to the synthetic distillates before transmitting them to the server, following the methods in [44, 63]. Specifically, Given latent $z$ , a perturbation coefficient $p$ , randomly generated noise $e$ and its scale parameter $s$ , the data to be shared is formulated as $\\begin{array}{r}{z=(1-p)z+e\\times s}\\end{array}$ . FedMix [53] proposes using linear interpolation of real samples to preserve privacy. In this approach, we synthesize data by averaging each two real samples from the Core-Set. For our proposed Fourier transform perturbation, we vary the $\\lambda=\\{0.1,\\bar{0}.5,0.8\\}$ and observe the variations in performance and privacy protection. To quantitatively evaluate the privacy protection of the synthetic data, we employ the Peak Signal-toNoise Ratio (PSNR) and Structure Similarity Index Measure (SSIM). A higher PSNR or SSIM value indicates greater similarity between the synthetic samples and the original samples, which implies more severe privacy leakage. We calculate the average PSNR and SSIM values of all the synthetic samples. ", "page_idx": 7}, {"type": "text", "text": "As depicted in Table 2, although FedMix provides better privacy protection, as evidenced by lower PSNR and SSIM values, it comes at the expense of significant performance degradation. The application of random noise requires a delicate balance between performance and privacy protection. For example, with a perturbation coefficient $p\\,=\\,0.2$ , it offers similar privacy protection to that of FedMix, but the performance drops approximately $10\\%$ compared to $p\\,=\\,0.1$ . However, the $p~=~0.1$ setting increases the risk of privacy leakage. In comparison, the synthetic distillates generated by our proposed FedSD2C achieve comparable PSNR values with them. This suggests that our proposed Fourier transform perturbation offers effective privacy protection for the real data sample. Furthermore, FedSD2C consistently outperforms other methods in terms of accuracy while maintaining a minimal performance degradation compared to no privacy protection techniques. This indicates that FedSD2C strikes a balance between privacy preservation and performance. We also perform membership inference attacks on FedSD2C and other methods, please refer to Appendix C.2 for more details. ", "page_idx": 7}, {"type": "table", "img_path": "6292sp7HiE/tmp/bde031de180660603ef411b527330bfab2754bf304254a6e9d25ac00c3ed0e4f.jpg", "table_caption": ["Table 3: Comparison of communication costs and accuracy at $\\alpha=0.1$ with ResNet-18. Results highlighted in bold represent outcomes with default ipc settings. Acc. and Comm. denote accuracy and communication costs, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3.2 Scalability of Communication Efficiency ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "By employing Core-Set selection and communication-efficient distillate communication, our FedSD2C condenses local data to mere MBs, while the model trained on these condensed data exhibits comparable performance, as shown in Table 3. Specifically, the communication costs of FedSD2C for sharing synthetic distillate is at most $4\\%$ of that of sharing model. Notably, we exclude the communication costs of sending and receiving pre-trained Autoencoder, as this can be pre-defined offline, allowing for the reuse of multiple one-shot FL tasks. Given the considerable capacity for communication costs, we further investigate the scalability of communication efficiency and performance. Our experimental results demonstrate that increased data transmission enhances the diversity of compressed data, leading to further improvements in performance. By increasing ipc from 20 to 80, the accuracy boosts by $13.03\\%$ and $4.48\\%$ on ImageNette and Tiny-ImageNet respectively. Additionally, we compare FedSD2C without Autoencoder at equivalent communication costs, where the absence of synthesized images limits performance to at best half of the default setting. The communication-efficiency of FedSD2C highlights its practicality in real-world applications. ", "page_idx": 8}, {"type": "text", "text": "4.3.3 Impact of Pre-trained Autoencoder ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Pre-trained Autoencoders are typically trained on natural data domains [64], while practical applications of federated learning often involve a broader range of domains, such as medical images. This raises the question of whether pre-trained Autoencoders remain effective when applied to a different domain and whether our proposed FedSD2C can adapt to these differences. To investigate this, we evaluate performance using a medical dataset COVID-FL [65]. ", "page_idx": 8}, {"type": "image", "img_path": "6292sp7HiE/tmp/a809964e78e1fac516304f0a8f6145e910ef131de178ce848974762c87537b10.jpg", "img_caption": ["(a) COVID-FL "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "6292sp7HiE/tmp/f377971a22bf03d12f66775c17038a05554f37134610ac8a083e6bf5eaae88e0.jpg", "img_caption": ["(b) ImageNette "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: (a) Experiments on the medical image data domain. Adopting pre-trained Autoencoders on other data domains can reduce performance. However, this can be mitigated by increasing $T_{s y n}$ . (b) Experiments of FedSD2C with randomly initialized downsampling and upsampling modules (blue line) compared to pre-trained Autoencoders (orange line) on ImageNette. Without pre-trained knowledge, FedSD2C requires a higher $T_{s y n}$ for distillate synthesis but can still achieve comparable results. ResNet-18 is used for both experiments. ", "page_idx": 8}, {"type": "text", "text": "As shown in Figure 3a, using the default synthesis iteration of $T_{s y n}~=~50$ yields suboptimal results. By increasing $T_{s y n}$ to ", "page_idx": 8}, {"type": "text", "text": "1000, the performance improves and then stabilizes. This observation suggests that the pre-trained knowledge of Autoencoders may influence the speed of distillate synthesis convergence. To further validate this, we replace the encoder and decoder of pre-trained Autoencoders with randomly initialized downsampling and upsampling modules. We vary $T_{s y n}$ from 50 to 1000 and compare its performance with employing pre-trained Autoencoders on ImageNette, setting $i p c\\,=\\,80$ for better illustration. Figure 3b demonstrates that as $T_{s y n}$ increases, the performance of FedSD2C with randomly initialized modules improves progressively, eventually matching the performance of FedSD2C with pre-trained Autoencoders. In summary, while pre-trained knowledge can enhance convergence rate, FedSD2C can achieve comparable performance by adjusting $T_{s y n}$ , demonstrating its adaptability across domains. ", "page_idx": 8}, {"type": "table", "img_path": "6292sp7HiE/tmp/1c15456697b6ca323a647a5f3ce2eb2295e3f3cd91198717fc4032752672f66c.jpg", "table_caption": ["Table 4: Accuracy on Tiny-ImageNet with different client amounts. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.3.4 Impact of Client Scales ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As practical FL deployments often involve participating clients [59], we evaluate our FedSD2C with various numbers of clients $n=\\{20,50,\\bar{100}\\}$ and maintain consistent communication budget by setting $i p c=\\{40,20,10\\}$ , respectively. We compare these methods under on Tiny-ImageNet with data heterogeneity $\\alpha=\\{0.1,0.3,0.5\\}$ for partitions and employ ConvNet. As depicted in Table 4, FedSD2C consistently achieves the highest accuracy as the number of clients increases. Moreover, FedSD2C demonstrates greater robustness to the number of participants. Specifically, as the number of participants changes, the accuracy of FedSD2C fluctuates within only $1\\%$ . In contrast, the accuracy of F-DAFL, DENSE, and Co-Boosting dropped by up to $4.71\\%$ , $3.49\\%$ , and $3.10\\%$ , respectively under different settings. This further validates the utility of sharing synthesized distillates in real-world one-shot FL applications. ", "page_idx": 9}, {"type": "text", "text": "5 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The local distillation process introduces additional computational overhead. While Core-Set selection requires no training and the distillate synthesis process only requires 50 iterations with a speed of 0.4s/per image on RTX3090, it still imposes higher resource requirements on the local device compared to the method of sharing model. One direction worth exploring is to integrate with the model market [66] to enable clients to synthesize distillates once for permanent use. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a new one-shot FL framework driven by distiller-distillate communication, denoted as FedSD2C, to alleviate the information loss of knowledge transfer and impacts of data heterogeneity. FedSD2C compresses the local data into Core-Set with $\\mathcal{V}$ -information and employs a pre-trained Autoencoder as the distiller to distill informative, communication-efficient, and privacy-enhanced distillates from Core-Set. Moreover, We discuss FedSD2C\u2019s resistance to attackers intercepting distillate communications and attacks from honest-but-curious servers and introduce Fourier transform perturbation to further minimize the risk of privacy leakage. Empirical results validate the effectiveness of FedSD2C in transferring local knowledge to the server in one-shot FL while balancing communication efficiency and privacy protection. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This project is supported by the National Research Foundation, Singapore, under its AI Singapore Programme (AISG Award No: AISG2-RP-2021-023). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Brendan Mcmahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera Y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273\u20131282. PMLR, 2017.   \n[2] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. IEEE Signal Processing Magazine, 37(3):50\u201360, 2020.   \n[3] Peter Kairouz, H Brendan Mcmahan, Brendan Avent, Aur\u00c9Lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, and Others. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14(1\u20132):1\u2013210, 2021.   \n[4] Junyuan Zhang, Shuang Zeng, Miao Zhang, Runxi Wang, Feifei Wang, Yuyin Zhou, Paul Pu Liang, and Liangqiong Qu. Flhetbench: Benchmarking device and state heterogeneity in federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12098\u201312108, June 2024.   \n[5] Derui Wang, Chaoran Li, Sheng Wen, Surya Nepal, and Yang Xiang. Man-in-the-middle attacks against machine learning classifiers via malicious generative models. IEEE Transactions on Dependable and Secure Computing, 18(5):2074\u20132087, 2020.   \n[6] Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M Alvarez, Jan Kautz, and Pavlo Molchanov. See through gradients: Image batch recovery via gradinversion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16337\u201316346, 2021.   \n[7] Neel Guha, Ameet Talwalkar, and Virginia Smith. One-shot federated learning. arxiv preprint arxiv:1902.11175, 2019.   \n[8] Jie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Shouhong Ding, Chunhua Shen, and Chao Wu. Dense: Data-free one-shot federated learning. Advances in Neural Information Processing Systems, 35:21414\u201321428, 2022.   \n[9] Clare Elizabeth Heinbaugh, Emilio Luz-Ricca, and Huajie Shao. Data-free one-shot federated learning under very high statistical heterogeneity. In ICLR, 2022.   \n[10] Rong Dai, Yonggang Zhang, Ang Li, Tongliang Liu, Xun Yang, and Bo Han. Enhancing one-shot federated learning through data and ensemble co-boosting. arxiv preprint arxiv:2402.15070, 2024.   \n[11] Qinbin Li, Bingsheng He, and Dawn Song. Practical one-shot federated learning for cross-silo setting. arxiv preprint arxiv:2010.01017, 2020.   \n[12] Xingyi Yang, Daquan Zhou, Songhua Liu, Jingwen Ye, and Xinchao Wang. Deep model reassembly. Advances in neural information processing systems, 35:25739\u201325753, 2022.   \n[13] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learning. In International Conference on Machine Learning, pages 12878\u201312889. PMLR, 2021.   \n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural Information Processing Systems, 27, 2014.   \n[15] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.   \n[16] Jiawei Shao, Fangzhao Wu, and Jun Zhang. Selective knowledge sharing for privacy-preserving federated distillation without a good teacher. Nature Communications, 15(1):349, 2024.   \n[17] Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. Advances in Neural Information Processing Systems, 33:22045\u201322055, 2020.   \n[18] Xingyi Yang, Jingwen Ye, and Xinchao Wang. Factorizing knowledge in neural networks. In European Conference on Computer Vision, pages 73\u201391. Springer, 2022.   \n[19] Shikang Yu, Jiachen Chen, Hu Han, and Shuqiang Jiang. Data-free knowledge distillation via feature exchange and activation region constraint. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 24266\u201324275, 2023.   \n[20] Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of usable information under computational constraints. arxiv preprint arxiv:2002.10689, 2020.   \n[21] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arxiv preprint arxiv:1312.6114, 2013.   \n[22] Neel Guha, Ameet Talwalkar, and Virginia Smith. One-shot federated learning. arxiv preprint arxiv:1902.11175, 2019.   \n[23] Liangqiong Qu, Yuyin Zhou, Paul Pu Liang, Yingda Xia, Feifei Wang, Ehsan Adeli, Li Fei-Fei, and Daniel Rubin. Rethinking architecture design for tackling data heterogeneity in federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10061\u201310071, 2022.   \n[24] Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin. Federated learning on non-iid data: A survey. Neurocomputing, 465:371\u2013390, 2021.   \n[25] Shuang Zeng, Pengxin Guo, Shuai Wang, Jianbo Wang, Yuyin Zhou, and Liangqiong Qu. Tackling data heterogeneity in federated learning via loss decomposition. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 707\u2013717. Springer, 2024.   \n[26] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arxiv preprint arxiv:1811.10959, 2018.   \n[27] Ruonan Yu, Songhua Liu, and Xinchao Wang. Dataset distillation: A comprehensive review. PAMI, 2023.   \n[28] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. ICLR, 2021.   \n[29] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4750\u20134759, 2022.   \n[30] Ruonan Yu, Songhua Liu, Jingwen Ye, and Xinchao Wang. Teddy: Efficient large-scale dataset distillation via taylor-approximated matching. In European Conference on Computer Vision, pages 1\u201317. Springer, 2025.   \n[31] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In WACV, pages 6514\u20136523, 2023.   \n[32] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and Yang You. Cafe: Learning to condense dataset by aligning features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12196\u201312205, 2022.   \n[33] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset meta-learning from kernel ridge-regression. ICLR, 2021.   \n[34] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely wide convolutional networks. Advances in Neural Information Processing Systems, 34:5186\u20135198, 2021.   \n[35] Yuanhao Xiong, Ruochen Wang, Minhao Cheng, Felix Yu, and Cho-Jui Hsieh. Feddm: Iterative distribution matching for communication-efficient federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16323\u201316332, 2023.   \n[36] Rui Song, Dai Liu, Dave Zhenyu Chen, Andreas Festag, Carsten Trinitis, Martin Schulz, and Alois Knoll. Federated learning via decentralized dataset distillation in resource-constrained edge environments. In IJCNN, pages 1\u201310. IEEE, 2023.   \n[37] Renjie Pi, Weizhong Zhang, Yueqi Xie, Jiahui Gao, Xiaoyu Wang, Sunghun Kim, and Qifeng Chen. Dynafed: Tackling client data heterogeneity with global dynamics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12177\u201312186, 2023.   \n[38] Ping Liu, Xin Yu, and Joey Tianyi Zhou. Meta knowledge condensation for federated learning. ICLR, 2023.   \n[39] Jie Zhang, Xiaohua Qi, and Bo Zhao. Federated generative learning with foundation models. arXiv preprint arXiv:2306.16064, 2023.   \n[40] Yanlin Zhou, George Pu, Xiyao Ma, Xiaolin Li, and Dapeng Wu. Distilled one-shot federated learning. arxiv preprint arxiv:2009.07999, 2020.   \n[41] Max Welling. Herding dynamical weights to learn. In International Conference on Machine Learning, pages 1121\u20131128, 2009.   \n[42] Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin. on the diversity and realism of distilled dataset: An efficient dataset distillation paradigm. arxiv preprint arxiv:2312.03526, 2023.   \n[43] Hanlin Lu, Changchang Liu, Ting He, Shiqiang Wang, and Kevin S Chan. Sharing models or coresets: A study based on membership inference attack. arxiv preprint arxiv:2007.02977, 2020.   \n[44] Yue Tan, Guodong Long, Jie Ma, Lu Liu, Tianyi Zhou, and Jing Jiang. Federated learning from pretrained models: A contrastive learning approach. Advances in Neural Information Processing Systems, 35:19332\u201319344, 2022.   \n[45] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14383\u201314392, 2021.   \n[46] Leon N Piotrowski and Fergus W Campbell. A demonstration of the visual importance and flexibility of spatial-frequency amplitude and phase. Perception, 11(3):337\u2013346, 1982.   \n[47] Henri J Nussbaumer and Henri J Nussbaumer. the Fast Fourier Transform. Springer, 1982.   \n[48] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8715\u20138724, 2020.   \n[49] Zeyuan Yin, Eric Xing, and Zhiqiang Shen. Squeeze, recover and relabel: Dataset condensation at imagenet scale from a new perspective. Advances in Neural Information Processing Systems, 36, 2024.   \n[50] Jonas Geiping, Hartmut Bauermeister, Hannah Dr\u00d6Ge, and Michael Moeller. Inverting gradients-how easy is it to break privacy in federated learning? Advances in Neural Information Processing Systems, 33:16937\u201316947, 2020.   \n[51] Jiawei Shao, Zijian Li, Wenqiang Sun, Tailin Zhou, Yuchang Sun, Lumin Liu, Zehong Lin, and Jun Zhang. A survey of what to share in federated learning: Perspectives on model utility, privacy leakage, and communication efficiency. arxiv preprint arxiv:2307.10655, 2023.   \n[52] Tian Dong, Bo Zhao, and Lingjuan Lyu. Privacy for free: How does dataset condensation help privacy? In International Conference on Machine Learning, pages 5378\u20135396. PMLR, 2022.   \n[53] Tehrim Yoon, Sumin Shin, Sung Ju Hwang, and Eunho Yang. Fedmix: Approximation of mixup under mean augmented federated learning. arxiv preprint arxiv:2107.00233, 2021.   \n[54] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. Cs 231n, 7(7):3, 2015.   \n[55] Imagenette. https://github.com/fastai/imagenette.   \n[56] Google Open Images Dataset. https://storage.googleapis.com/openimages/web/index.html.   \n[57] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 248\u2013255. IEEE, 2009.   \n[58] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arxiv preprint arxiv:1909.06335, 2019.   \n[59] Fan Lai, Yinwei Dai, Sanjay Singapuram, Jiachen Liu, Xiangfeng Zhu, Harsha Madhyastha, and Mosharaf Chowdhury. Fedscale: Benchmarking model and system performance of federated learning at scale. In International Conference on Machine Learning, pages 11814\u201311827. PMLR, 2022.   \n[60] Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, and Qi Tian. Data-free learning of student networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 3514\u20133522, 2019.   \n[61] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2016.   \n[62] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00d6Rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, 2022.   \n[63] Jianqing Zhang, Yang Liu, Yang Hua, and Jian Cao. An upload-efficient scheme for transferring knowledge from a server-side pre-trained generator to clients in heterogeneous federated learning. arxiv preprint arxiv:2403.15760, 2024.   \n[64] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, and Others. Laion-5b: An open largescale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \n[65] Rui Yan, Liangqiong Qu, Qingyue Wei, Shih-Cheng Huang, Liyue Shen, Daniel Rubin, Lei Xing, and Yuyin Zhou. Label-efficient self-supervised federated learning for tackling data heterogeneity in medical imaging. IEEE Transactions on Medical Imaging, 2023.   \n[66] Manasi Vartak, Harihar Subramanyam, Wei-En Lee, Srinidhi Viswanathan, Saadiyah Husnoo, Samuel Madden, and Matei Zaharia. Modeldb: A system for machine learning model management. In Proceedings of the Workshop on Human-in-the-Loop Data Analytics, pages 1\u20133, 2016.   \n[67] Alex Krizhevsky, Geoffrey Hinton, and Others. Learning multiple layers of features from tiny images. 2009.   \n[68] Michael Aerni, Jie Zhang, and Florian Tram\u00c8R. Evaluations of machine learning privacy defenses are misleading. arxiv preprint arxiv:2404.17399, 2024.   \n[69] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages 308\u2013318, 2016.   \nAlgorithm S1 $\\nu$ -information-based Core-Set Selection   \nRequire: Client local dataset $(X_{t},Y_{t})$ Client local feature extractor $h$ , image per class ipc $\\overline{{X_{T}^{\\prime}\\leftarrow\\emptyset}}$   \n2: Level 1: identifying the most informative image segments for each $x,y\\in\\left(X_{t},Y_{t}\\right)$ do   \n4: $\\{x^{k}\\}_{k=1}^{K}\\gets$ tract $K$ patches with multiple scales from $x$ . for $k=1$ to $K$   \n6: Calculate $\\mathcal{V}$ -information score $s^{k}\\leftarrow-\\mathcal{L}(h(x^{k}),y)$ end for   \n8: select $x^{\\prime}$ with highest score $s^{\\prime}$ $X_{T}^{\\prime}\\bigcup\\{(x^{\\prime},y,s^{\\prime})\\}$   \n10: end for Level 2: select the top- $\\cdot i p c$ patches with highest $\\mathcal{V}$ -information   \n12: $X_{s},Y_{s}\\gets\\emptyset$ for each class $c\\in Y_{t}$ do   \n14: if size of $\\{(x_{j}^{\\prime},y_{j},s_{j}^{\\prime})\\},y_{j}=c\\ge i p c$ then select top-ipc $\\{(x_{j}^{\\prime},y_{j})\\}_{j=1}^{i p c}$ via $s_{j}^{\\prime}$   \n16: $X_{s}\\bigcup\\{x_{j}^{\\prime}\\}_{j=1}^{i p c},Y_{s}\\bigcup\\{y_{j}\\}_{j=1}^{i p c}$ end if   \n18: end for return $(X_{s},Y_{s})$ ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B More Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We use the SGD optimizer with momentum $=\\!0.9$ , learning rate $=\\!0.01$ and weight decay ${\\it=}0.0001$ for clients\u2019 local training. The batch size is set to 128 and local epoch is 200. For all generation based methods, we set the resolution of the generated images to $64\\times64$ , $128\\times128$ and $256\\times256$ for Tiny-ImageNet, ImageNette and OpenImage, the number of generated images in each batch is 128, and the learning rate of the generator is 0.001, the latent dimension is 256, iteration for training generator is 30, using Adam for optimization. The server model is optimized with SGD with momentum 0.9, the learning rate is 0.01, and the training epochs are 200. The synthesized batch size and server model training batch size is both 128. In DENSE, we set $\\lambda_{1}=1$ for BN loss and $\\lambda_{2}=0.5$ for diversity loss. In Co-Boosting, the perturbation strength is set to $\\epsilon=8/255$ and the step size $\\mu=0.1/n$ . In Core-Set selection stage of FedSD2C, for each image $x_{i}$ , we employ the torchvision.transform.RandomResizeCrop $K$ times to generate a collection of patches. For patch size, we set the scale=(0.08, 1.0), which is to collect diverse image patches. Following [42], we employ ConvNet-4 for Tiny-ImageNet, ConvNet-5 for ImageNette and ConvNet-6 for OpenImage. All methods are implemented with Pytorch and conducted on GeForce RTX 3090. ", "page_idx": 14}, {"type": "text", "text": "Details of t-SNE plots in Figure 1 We randomly select a client and five classes from its local dataset (Tiny-ImageNet) and employs its local model (ResNet-18) to extract features. The feature is extracted from the final layer (before the classifier). We then use t-SNE plots to illustrate the feature distribution. ", "page_idx": 14}, {"type": "text", "text": "C Additional Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Additional datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "CIFAR-10. We set the resolution of generated images to $32\\!\\times\\!32$ for CIFAR-10 [67] and keep all the other settings the same. As shown in Table S1, there is an initial performance discrepancy at the standard setting of $i p c=50$ . This occurs because our method prioritizes efficiency with large datasets rather than low-resolution ones. However, upon increasing the amount of synthetic data $(i p c=500)$ ), our method can still achieve comparable results. ", "page_idx": 14}, {"type": "table", "img_path": "6292sp7HiE/tmp/f7c04bf567eaeece42664f50a05648f6eaf392bb10d0118b424fb80164c69807.jpg", "table_caption": ["Table S1: Performance on CIFAR-10 with ResNet-18. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "COVID-FL. We crop the images of COVID-FL [65] into $128\\!\\times\\!128$ , set the $T_{s y n}=1000$ as in Section 4.3.3 and keep all the other settings the same. The results in Table S2 indicate that our FedSD2C still acheive better results compared to DENSE and Co-Boosting. ", "page_idx": 15}, {"type": "table", "img_path": "6292sp7HiE/tmp/8ce0e50d7a68971556738367b3f2b67ae53626f8d7d4949056f404e39a5623c4.jpg", "table_caption": ["Table S2: Performance on COVID-FL datasets with ResNet-18. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 Membership Inference Attack ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To further validate the effectiveness of our method, we employ an improved version of LiRA [68] to conduct Membership Inference Attacks on our methods. When attacking each client, for FedSD2C, we use the distillates uploaded by the client to train a new model and conduct membership inference attacks on that model. For the sharing model-based methods, we perform membership inference attacks on the models uploaded by the clients. The client model is ResNet-18 with $\\alpha=0.1,i p c=50$ . We set the raw images of Core-Set as the canary (target data $x$ ), as this is the most serious case of our methods. The results confirm that our approach does not introduce more privacy risk than the sharing model-based approach, even for the most vulnerable targets. ", "page_idx": 15}, {"type": "table", "img_path": "6292sp7HiE/tmp/f7a70b38eddf59c72b92f1bc005fd4ed7d6ee8b1efb5bc4ef76a25153bc1a087.jpg", "table_caption": ["Table S3: Membership Inference Attack. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.3 Wavelet transform perturbation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We explore the use of wavelet transforms to replace the Fourier transforms during Fourier transform perturbations. We use ResNet-18 on Tiny-ImageNet with $\\alpha=0.1,i p c=50$ . The results indicate that Wavelet Transform offers greater scalability in privacy protection. By increasing $\\lambda$ , the PSNR/SSIM can be reduced to as low as 12.90/15.30. When the accuracy is comparable to that of Fourier transform (Wavelet $\\lambda=0.5$ vs. Fourier $\\lambda=0.8)$ ), the PSNR and SSIM of Wavelet transform is lower. ", "page_idx": 15}, {"type": "table", "img_path": "6292sp7HiE/tmp/d5d35c20c83e27ae2de5df8091e08225b5e4aab76bc2c0331ad6e6d08e44e21b.jpg", "table_caption": ["Table S4: Comparison between Wavelet transform and Fourier transform. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.4 Ablation study on Core-Set selection ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we perform ablation experiments to explore the significance of $\\nu$ -information Core-Set selection. We use ResNet-18 with $\\alpha\\,=\\,0.1,i p c\\,=\\,50$ . Compared with $\\nu$ -information Core-Set selection, the accuracy of random selection decreased by 3.5 and 5.46 on Tiny-ImageNet and ImageNette, respectively. We also report the performance of uploading Core-Set directly, which achieve the best performance. However, without distillate synthesis, it will increase the cost of communication and the risk of privacy leakage. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "6292sp7HiE/tmp/29c3c16b3cdc8ed9da766e69465591e67ebd6cf282e39c1f4eb6b7997c37738d.jpg", "table_caption": ["Table S5: Performance of different selection strategy. Core-Set denotes that clients directly upload their local Core-Set, which leads to privacy issue. FedSD2C w/ random selection denotes replacing $\\nu$ -information-based Core-Set selection with random selection. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.5 Integrating with Differential Privacy ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "According to [35], introducing DP-SGD [69] during the distillate synthesis stage can provide theoretical privacy guarantees for our method. We perform experiments of integrating DP-SGD in our method on Tiny-ImageNet with ResNet-18 $\\alpha=0.1$ , $i p c=50]$ ) to provide a clear view of the trade-offs involved. ", "page_idx": 16}, {"type": "table", "img_path": "6292sp7HiE/tmp/315a9fe30ceea625fb71b4a8c24b58e5044a6baf035087883f3818204c838da8.jpg", "table_caption": ["Table S6: Performance of integrating DP-SGD "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D Visualization ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "6292sp7HiE/tmp/3f83ac274242afe7324385e10938f8baedfa83c29fee3983055fc070f8f73dac.jpg", "img_caption": ["Figure S1: Visualization of synthetic distillate reconstructed by the pre-trained Autoencoder compared to the original sample on Tiny-ImageNet. The image style is similar, but with enhanced privacy protection. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The contributions are empirically validated in the main body. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We discuss the limitations in Section 5 ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: the paper does not include theoretical results. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We provide a detailed description of FedSD2C and the parameters for implementing the baseline in Section 4 and the Appendix. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our code is attached to the supplementary material and is released at: https: //github.com/Carkham/FedSD2C. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide a detailed description of FedSD2C and the parameters for implementing the baseline in Section 4 and the Appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We report the standard deviation in Table 1. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We describe the computational resources used in the Appendix ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We do not violate Code of Ethics ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We discuss the potential impacts about privacy issues. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We cite all sources used and comply with their licenses. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not involve crowdsourcing nor research with human subjects. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We do not involve crowdsourcing nor research with human subjects. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]