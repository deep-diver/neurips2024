[{"figure_path": "ZxVrkm7Bjl/figures/figures_3_1.jpg", "caption": "Figure 1: Layer grouping: 8 layers with group size of 2.", "description": "This figure illustrates the layer grouping concept used in the MoEUT architecture.  It shows 8 layers stacked recurrently.  The layers are grouped into pairs (G=2), where the layers within each group have different weights, but the groups themselves share parameters.  This strategy balances the benefits of parameter sharing across layers for efficient computation, with the expressiveness gained from having multiple non-shared layers within a group, to mitigate limitations of shared-layer Transformer architecture. The diagram visually separates layers A and B within the groups to emphasize the weight sharing.", "section": "2 The MoEUT Architecture"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_4_1.jpg", "caption": "Figure 4: Scaling of different models on C4 (with perplexity measured on a held-out subset of C4). (a) MOEUT slightly outperforms parameter-matched models with no layer sharing. The gap grows with scale. (b) Given equal amounts of compute, MoEUT outperforms other models by a large margin.", "description": "This figure compares the performance of MoEUT against standard Transformers and \u03c3-MoE models on the C4 dataset.  Two subfigures illustrate scaling performance: (a) shows that MoEUT slightly outperforms models with a similar number of parameters but without layer sharing, and this difference increases with model size. (b) demonstrates that for a given compute budget (measured in Multiply-Accumulate operations during training), MoEUT significantly surpasses other models in terms of perplexity.", "section": "Main Experimental Results"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_4_2.jpg", "caption": "Figure 4: Scaling of different models on C4 (with perplexity measured on a held-out subset of C4). (a) MOEUT slightly outperforms parameter-matched models with no layer sharing. The gap grows with scale. (b) Given equal amounts of compute, MoEUT outperforms other models by a large margin.", "description": "This figure compares the performance of different Transformer models on the C4 dataset in terms of perplexity.  Panel (a) shows that the MoEUT model slightly outperforms dense models with an equivalent number of parameters. The performance difference is larger with bigger models. Panel (b) demonstrates that given equal computational resources (measured in multiply-accumulate operations, or MACs), MoEUT significantly surpasses other models.", "section": "Main Experimental Results"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_5_1.jpg", "caption": "Figure 4: Scaling of different models on C4 (with perplexity measured on a held-out subset of C4). (a) MOEUT slightly outperforms parameter-matched models with no layer sharing. The gap grows with scale. (b) Given equal amounts of compute, MoEUT outperforms other models by a large margin.", "description": "This figure demonstrates the scaling properties of different models on the C4 dataset.  It shows perplexity on a held-out subset of C4, comparing MoEUT against standard Transformers and a \u03c3-MoE model.  Panel (a) compares the models with similar numbers of parameters, showing MoEUT performs slightly better, with the difference growing as the parameter count increases. Panel (b) compares models using equivalent compute resources (measured in Multiply-Accumulate operations), showing that MoEUT significantly outperforms the others.", "section": "Main Experimental Results"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_5_2.jpg", "caption": "Figure 6: Perplexity of 244M MoEUT models with different layer grouping options. A small group size of G = 2 works the best, showing the advantage of layer sharing.", "description": "This figure shows the perplexity achieved by 244M parameter MoEUT models trained on the C4 dataset,  with varying group sizes (G).  The group size, G, represents the number of layers stacked together before parameter sharing is reintroduced. The results demonstrate that a small group size of G=2 yields the lowest perplexity, outperforming models with larger group sizes or no layer grouping. This finding highlights the benefit of carefully balancing the extent of parameter sharing in the MoEUT architecture to optimize performance.", "section": "Main Experimental Results"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_7_1.jpg", "caption": "Figure 4: Scaling of different models on C4 (with perplexity measured on a held-out subset of C4). (a) MOEUT slightly outperforms parameter-matched models with no layer sharing. The gap grows with scale. (b) Given equal amounts of compute, MoEUT outperforms other models by a large margin.", "description": "This figure compares the performance of MoEUT against standard Transformers and \u03c3-MoE models on the C4 dataset.  Subfigure (a) shows that MOEUT performs slightly better than parameter-matched models without layer sharing, with the performance gap widening as model size increases. Subfigure (b) demonstrates MOEUT's significant computational advantage, showcasing its superior performance when compared to other models given equivalent compute resources.", "section": "Main Experimental Results"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_7_2.jpg", "caption": "Figure 8: Comparing layernorm variants. Peri-layernorm outperforms both pre- and post-layernorm.", "description": "This figure compares the performance of three different layernorm schemes (pre-layernorm, post-layernorm, and peri-layernorm) on two different sized models (44M and 244M parameters).  The y-axis represents perplexity, a measure of the model's performance on a language modeling task.  Peri-layernorm consistently achieves lower perplexity, indicating superior performance compared to the other methods. This suggests that the peri-layernorm scheme effectively balances gradient flow and residual signal propagation, leading to improved model training and generalization. The improvements are more significant for the smaller model (44M parameters), and the difference narrows as the model size grows.", "section": "2.4 Novel LayerNorm Scheme for Improved Signal Propagation in Universal Transformers"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_8_1.jpg", "caption": "Figure 9: Layer preference of different experts. Most experts are used in multiple layers, while some of them (see, bottom right) specialize to certain layers, showing the flexibility of our model.", "description": "This figure visualizes the distribution of layers where each expert in the MLP layers is activated.  The x-axis represents the expert ID, and the y-axis represents the layer number. The color intensity represents the frequency of expert activation in each layer. The figure demonstrates that while some experts are primarily activated in specific layers, most experts are used across multiple layers, highlighting the model's flexibility in assigning experts to layers dynamically, based on the task's needs.", "section": "Analysis"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_8_2.jpg", "caption": "Figure 10: No. of unique experts used in different layers. Tokens are routed to many different experts (depending on the context), especially in the middle layers.", "description": "This figure shows the number of unique experts used for different tokens across various layers of the model.  The x-axis represents tokens ordered by the number of experts used in layer 1. The y-axis shows the total number of unique experts used.  The plot demonstrates that the number of unique experts used per token increases significantly in the middle layers (around layer 9), indicating that contextual information significantly influences expert selection.  The diversity of experts used then decreases slightly towards the output layers.", "section": "4 Analysis"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_8_3.jpg", "caption": "Figure 11: Instance-level average expert selection similarity between layers. Individual tokens are routed to a diverse set of experts across the layers.", "description": "This figure visualizes the similarity of expert selection between different layers for individual input tokens.  The heatmap shows the Jaccard similarity (intersection over union) of the sets of experts used for each token across various layers. High values (yellow) indicate high similarity, meaning that the same or very similar sets of experts are selected for that token across multiple layers. Lower values (darker colors) suggest more variation in expert selection across layers.  The figure illustrates the dynamic nature of expert selection in MoEUT, where expert use varies depending on the context and layer.", "section": "Analysis"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_14_1.jpg", "caption": "Figure 12: The update magnitude of different layers in a 44M parameter Transformer on C4. The norm of the updates grows throughout the layers to compensate for the residual growth (see Sec. 2.3 for more details).", "description": "This figure shows the L2 norm of the difference between the residual before and after applying a standard Transformer layer (both attention and MLP block) in different layers of a 44M parameter Transformer trained on C4 dataset.  The y-axis represents the update magnitude, and the x-axis represents the layer number. The figure demonstrates that the norm of these updates increases as the layer number increases, illustrating the phenomenon of growing residual norm in standard Transformers. This growth necessitates a compensatory mechanism, which is addressed in Section 2.3 of the paper.", "section": "A.2 Growing Residual Norm In Standard Transformers"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_16_1.jpg", "caption": "Figure 13: Performance of our 244M MoEUT on a held-out subset of C4 with expert sizes (dexpert) in the MLP layer. The smallest expert size performs the best.", "description": "This figure shows the results of an experiment that tested the performance of the 244M MoEUT model on a held-out subset of the C4 dataset.  The experiment varied the size of the experts (dexpert) in the Multi-Layer Perceptron (MLP) layer of the model. The results show that the model performs best when the expert size is smallest (128).", "section": "A.5 The Effects of dexpert and K"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_16_2.jpg", "caption": "Figure 14: Performance of our 244M MoEUT on a held-out subset of C4 with different number of active experts for the \u03c3-MoE layer. Increasing the number of experts always helps, but the returns diminish.", "description": "The figure shows the perplexity on a held-out subset of the C4 dataset for a 244M parameter MoEUT model with different numbers of active experts (K) in the \u03c3-MoE layer.  As the number of active experts increases, the perplexity decreases, indicating improved performance. However, the rate of improvement diminishes as K increases, suggesting diminishing returns.", "section": "A.5 The Effects of  dexpert and K"}, {"figure_path": "ZxVrkm7Bjl/figures/figures_16_3.jpg", "caption": "Figure 9: Layer preference of different experts. Most experts are used in multiple layers, while some of them (see, bottom right) specialize to certain layers, showing the flexibility of our model.", "description": "This figure visualizes the layer preference of different experts in the MoEUT model.  The heatmap shows the frequency with which each expert is activated in each layer. Most experts are used across multiple layers, indicating versatility. However, some experts show a strong preference for specific layers, demonstrating the model's capacity for both shared and specialized computations.", "section": "Analysis"}]