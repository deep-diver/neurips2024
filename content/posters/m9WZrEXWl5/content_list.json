[{"type": "text", "text": "Directional Smoothness and Gradient Methods: Convergence and Adaptivity ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aaron Mishkin\u2217 Ahmed Khaled\u2217 Stanford University Princeton University amishkin@cs.stanford.edu ahmed.khaled@princeton.edu ", "page_idx": 0}, {"type": "text", "text": "Yuanhao Wang Aaron Defazio Robert M. Gower Princeton University FAIR, Meta AI CCM, Flatiron Institute yuanhaoa@princeton.edu adefazio@meta.com gowerrobert@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on $L$ -smoothness. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gradient methods for differentiable functions are typically analyzed under the assumption that $f$ is $L$ -smooth, meaning $\\nabla f$ is $L$ -Lipschitz continuous. This condition implies $f$ is upper-bounded by a quadratic and guarantees that gradient descent (GD) with step-size $\\eta<2/L$ decreases the optimality gap at each iteration (Bertsekas, 1997). However, experience shows that GD can still decrease the objective when $f$ is not $L$ -smooth, particularly for deep neural networks (Bengio, 2012; Z. Li et al., 2020; J. Cohen et al., 2021). Even for functions verifying smoothness, convergence rates are often pessimistic and fail to predict optimization speed in practice (Paquette et al., 2023). ", "page_idx": 0}, {"type": "text", "text": "One alternative to global smoothness is local Lipschitz continuity of the gradient (\u201clocal smoothness\u201d). Local smoothness assumes different Lipschitz constants hold for different neighbourhoods, which avoids global assumptions and improves rates. However, such analyses typically rely on boundedness of the iterates and then use local smoothness to obtain $L$ -smoothness over a compact set (Malitsky and Mishchenko, 2020). Boundedness is guaranteed in several ways: Junyu Zhang and Hong (2020) break optimization into stages, Patel and Berahas (2022) use stopping-times, and Lu and S. Mei (2023) employ a line-search. Unfortunately, these approaches modify the underlying optimization algorithm, require local smoothness oracles (Park et al., 2021), or rely on highly complex arguments. ", "page_idx": 0}, {"type": "text", "text": "In contrast, we prove simple rates for GD without global smoothness by deriving bounds of the form, ", "page_idx": 0}, {"type": "equation", "text": "$$\nf(x_{k+1})\\leq f(x_{k})+\\langle\\nabla f(x_{k}),x_{k+1}-x_{k}\\rangle+\\frac{M(x_{k+1},x_{k})}{2}\\|x_{k+1}-x_{k}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "image", "img_path": "m9WZrEXWl5/tmp/3aad1c80c904822134572a5bc2a4befb8c7f425a3a803b362b2c5acce801e1a5.jpg", "img_caption": ["$=1/M(x_{k+1},x_{k})\\ \\widetilde{\\texttt{\\small P o l y a k}}$ k Bound (L-Smooth) Bound $(1/M(x_{k+1},x_{k}))$ ) Bound (Polyak) "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparison of actual (solid lines) and theoretical (dashed lines) convergence rates for GD with (i) step-sizes strongly adapted to the directional smoothness $(\\eta_{k}=1/M(x_{k+1},x_{k}))$ and (ii) the Polyak step-size. Both problems are logistic regressions on UCI repository datasets (Asuncion and Newman, 2007). Our bounds using directional smoothness are tighter than those based on global $L$ - smoothness of $f$ and adapt to the optimization path. For example, on mammographic our theoretical rate for the Polyak step-size concentrates rapidly exactly when the optimizer shows fast convergence. ", "page_idx": 1}, {"type": "text", "text": "where the directional smoothness function $M(x_{k+1},x_{k})$ depends only on properties of $f$ along the chord between $x_{k}$ and $x_{k+1}$ . Our sub-optimality bounds provide a path-dependent perspective on GD and are tighter than conventional analyses when the step-size sequence is adapted to the directional smoothness, meaning $\\eta_{k}<2/M(x_{k+1},x_{k})$ . See Figure 1 for two real-data examples highlighting our improvement over classical rates. We summarize all our contributions as follows. ", "page_idx": 1}, {"type": "text", "text": "Directional Smoothness. We introduce three constructive directional smoothness functions $M(x,y)$ . The first, point-wise smoothness, depends only on the end-points $x,y$ and is easily computed, while the second, path-wise smoothness, yields a tighter bound, but depends on the chord ${\\mathcal{C}}\\,=$ $\\{\\alpha x+(1-\\alpha)y:\\bar{\\alpha}\\in[0,1]\\}$ . The last function, which we call the optimal point-wise smoothness, is both easy-to-evaluate and provides the tightest possible quadratic upper bound. ", "page_idx": 1}, {"type": "text", "text": "Sub-optimality bounds. We leverage directional smoothness functions to prove new sub-optimality bounds for GD on convex functions. Our bounds are localized to the GD trajectory, hold for any step-size sequence, and are tighter than the classic analysis using $L$ -smoothness. They are also more general since we do not need to assume that $f$ is globally $L$ -smooth to show progress; all we require is a sequence of step-sizes adapted to the directional smoothness function. Furthermore, our approach extends naturally to acceleration, allowing us to prove optimal rates for (strongly)-convex functions. ", "page_idx": 1}, {"type": "text", "text": "Adaptive Step-Sizes in the Quadratic Case. In the general setting, computing step-sizes which are adapted to the directional smoothness requires solving a challenging non-linear root-finding problem. For quadratic problems, we show that the ideal step-size that satisfies $\\eta_{k}=1/M(x_{k+1},\\Bar{x_{k}})$ is the Rayleigh quotient and is connected to the hedging algorithm (Altschuler and Parrilo, 2023). ", "page_idx": 1}, {"type": "text", "text": "Exponential Search. Moving beyond quadratics, we prove that the equation $\\eta_{k}=1/M(x_{k+1},x_{k})$ admits a solution under mild conditions, meaning ideal step-sizes can be computed using Newton\u2019s method. Since computing these step-sizes is typically impractical, we adapt exponential search (Carmon and Hinder, 2022) to obtain similar path-dependent complexities up to a log-log penalty. ", "page_idx": 1}, {"type": "text", "text": "Polyak and Normalized GD. More importantly, we show that the Polyak step-size (Polyak, 1987) and normalized GD achieve fast, path-dependent rates without knowledge of the directional smoothness. Our analysis reveals that the Polyak step-size adapts to any directional smoothness to obtain the tightest possible convergence rate. This property is not shared by constant step-size GD and may explain the superiority of the Polyak step-size in many practical settings. ", "page_idx": 1}, {"type": "text", "text": "1.1 Additional Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Directional smoothness is a relaxation of non-uniform smoothness (J. Mei et al., 2021), which restricts the smoothness function $M$ to depend only on $x$ , the origin point. J. Mei et al. (2021) leverage non-uniform smoothness and a non-uniform \u0141ojasiewicz inequality to break lower-bounds for first-order optimization. Similarly, Berahas et al. (2023) show that a weak local smoothness oracle can break lower bounds for gradient methods. A major advantage of our work over such oracle-based approaches is that we construct explicit directional smoothness functions that are easy to evaluate. ", "page_idx": 1}, {"type": "text", "text": "Similar to non-uniform smoothness, Grimmer (2019) and Orabona (2023) consider H\u00f6lder-type growth conditions with constants that depend on a neighbourhood of $x$ . Since directional smoothness is stronger than and implies these H\u00f6lder error bounds, our $M$ functions can be leveraged to make their results fully explicit (the H\u00f6lder bounds are non-constructive). Finally, while they also analyze normalized GD, our rates are anytime and do not use online-to-batch reductions like Orabona (2023). ", "page_idx": 2}, {"type": "text", "text": "Directional smoothness is also related to $(L_{0},L_{1})$ -smoothness (Jingzhao Zhang et al., 2020; B. Zhang et al., 2020), which can be interpreted as a directional smoothness function with exponential dependence on the distance between $x$ and $y$ . The extension of $\\left(L_{0},L_{1}\\right)$ -smoothness to $(r,l)$ -smoothness by H. Li et al. (2023) shows how to bound sequences of such directional smoothness functions, even for accelerated methods. These approaches are complementary to ours and showcase a setting where directional smoothness leads to concrete convergence rates. ", "page_idx": 2}, {"type": "text", "text": "Our work is most closely connected to that by Malitsky and Mishchenko (2020), who use a smoothed version of $M(x,y)$ to set the step-size. Vladarean et al. (2021) apply a similar smoothed step-size scheme to primal-dual hybrid gradient methods, while Zhao and Huang (2024) relate directional smoothness to Barzilai-Borwein updates (Barzilai and Borwein, 1988) and Vainsencher et al. (2015) use local smoothness over neighbourhoods of the global minimizer to set the step-size for SVRG. ", "page_idx": 2}, {"type": "text", "text": "Finally, we note that adaptivity to directional smoothness is different from adaptivity to the sequence of observed gradients obtained by methods such as Adagrad (Duchi et al., 2010; Streeter and McMahan, 2010). Adagrad and its variants are most useful when the gradients are bounded, such as in Lipschitz optimization, although they can also be used to obtain rates for smooth functions (Levy, 2017). We do not address adaptivity to gradients in this work. ", "page_idx": 2}, {"type": "text", "text": "2 Directional Smoothness ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We say that a convex function $f$ is $L$ -smooth if for all $x,y\\in\\mathbb{R}^{d}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(y)\\leq f(x)+\\langle\\nabla f(x),y-x\\rangle+\\frac{L}{2}\\|y-x\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Minimizing this quadratic upper bound in $y$ gives the classical GD update with step-size $\\eta_{k}=1/L$ . However, this viewpoint leads to rates which depend on the global, worst-case growth of $f$ . This is both counter-intuitive and undesirable because the iterates of GD, $x_{k+1}=x_{k}-\\eta_{k}\\nabla f(x_{k})$ , depend only on local properties of $f$ . Ideally, the analysis should also depend only on the local conditioning along the path $\\{x_{1},x_{2},\\ldots\\}$ . Towards this end, we generalize the smoothness upper-bound as follows. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1. We call $M:\\mathbb{R}^{d,d}\\rightarrow\\mathbb{R}_{+}$ a directional smoothness function for $f$ if for all $x,y\\in\\mathbb{R}^{d}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(y)\\leq f(x)\\!+\\!\\langle\\nabla f(x),y\\!-\\!x\\rangle+\\frac{M(x,y)}{2}\\|y\\!-\\!x\\|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If a function is $L$ -smooth, then $M(x,y)=L$ is a trivial choice of directional smoothness function. In the rest of this section, we construct different $M$ functions that provide tighter bounds on $f$ while still being possible to evaluate. The first is the point-wise directional smoothness, ", "page_idx": 2}, {"type": "equation", "text": "$$\nD(x,y):=\\frac{2\\|\\nabla f(y)-\\nabla f(x)\\|_{2}}{\\|y-x\\|_{2}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Point-wise smoothness is a directional estimate of $L$ and satisfies $D(x,y)\\leq2L$ . Indeed, $L$ can be equivalently defined as the supremum of $D(x,y)/2$ over the domain of $f$ (Beck, 2017). If $f$ is convex and differentiable, then $D(x,y)$ is a directional smoothness function according to Definition 2.1. ", "page_idx": 2}, {"type": "text", "text": "Lemma 2.2. If $f$ is convex and differentiable, then the point-wise directional smoothness satisfies, ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(y)\\leq f(x)+\\langle\\nabla f(x),y-x\\rangle+\\frac{D(x,y)}{2}\\|y-x\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "See Appendix A (we defer all proofs to the relevant appendices). In the worst-case, the point-wise directional smoothness $D$ is weaker than the standard upper-bound $M(x,y)=L$ by a factor of two. This is not an artifact of the analysis and is generally unavoidable, as the next proposition shows. ", "page_idx": 2}, {"type": "text", "text": "Proposition 2.3. There exists a convex, differentiable $f$ and $x,y\\in\\mathbb{R}^{d}$ such that if $t<2$ , then ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(y)>f(x)+\\langle\\nabla f(x),y-x\\rangle+\\frac{t\\|\\nabla f(x)-\\nabla f(y)\\|}{2\\|y-x\\|_{2}}\\|y-x\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "m9WZrEXWl5/tmp/f7837f3d1f9a01dbe03ac3c098cccb4b82e7d1df46c27db64c2c39da0ed3f124.jpg", "img_caption": ["Figure 2: Illustration of GD with $\\eta_{k}\\,=\\,1/L$ . Even though this step-size exactly minimizes the upper-bound from $L$ -smoothness, $M_{k}$ directional smoothness better predicts the progress of the gradient step because $M_{k}\\ll L$ . Our rates improve on $L$ -smoothness because of this tighter bound. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "While the point-wise smoothness is easy to compute, this additional factor of two can make Equation (5) looser than $L$ -smoothness \u2014 on isotropic quadratics, for example. As an alternative, we define the path-wise directional smoothness, ", "page_idx": 3}, {"type": "equation", "text": "$$\nA(x,y):=\\operatorname*{sup}_{t\\in[0,1]}{\\frac{\\langle\\nabla f(x\\!+\\!t(y\\!-\\!x))\\!-\\!\\nabla f(x),y\\!-\\!x\\rangle}{t\\|y-x\\|^{2}}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and show it verifies the quadratic upper-bound and satisfies Definition 2.1 even without convexity. Lemma 2.4. For any differentiable function $f$ , the path-wise smoothness (7) satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(y)\\leq f(x)+\\langle\\nabla f(x),y-x\\rangle+\\frac{A(x,y)}{2}\\|y-x\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Path smoothness is tighter than point-wise smoothness since $A(x,y)\\leq D(x,y)$ , but hard to compute because it depends on the chord between $x$ and $y$ . That is, it depends on the properties of $f$ on the line $\\{t x+(1-\\bar{t})y:t\\in[0,1]\\}$ rather than solely on the points $x$ and $y$ like the point-wise smoothness. ", "page_idx": 3}, {"type": "text", "text": "Point-wise and path-wise smoothness are constructive, but they may not yield the tightest bounds in all situations. The tightest directional smoothness function, which we call the optimal point-wise smoothness, is the smallest number for which the quadratic upper bound holds, ", "page_idx": 3}, {"type": "equation", "text": "$$\nH(x,y)={\\frac{|f(y)-f(x)-\\left\\langle\\nabla f(x),y-x\\right\\rangle|}{{\\frac{1}{2}}\\|y-x\\|^{2}}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By definition, $H$ is the tightest possible directional smoothness function; it lower bounds any constant $C$ that satisfies the quadratic bound (2). Thus, $H(x,y)\\leq M(x,y)$ for any smoothness function $M$ . ", "page_idx": 3}, {"type": "text", "text": "The directional smoothness functions introduced in this section represent different trade-offs between computability and tightness. The optimal point-wise smoothness $H(x,y)$ requires access to both the function and gradient values, whereas the point-wise directional-smoothness $D(x,y)$ requires only access to the gradients and convexity. In contrast, the path-wise direction smoothness $A(x,y)$ satisfies Lemma 2.4 with or without convexity, but may be hard to evaluate. ", "page_idx": 3}, {"type": "text", "text": "3 Path-Dependent Sub-Optimality Bounds ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Using directional smoothness, we obtain a descent lemma which depends only on local geometry, ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(x_{k+1})\\leq f(x_{k})-\\left(\\eta_{k}-\\frac{\\eta_{k}^{2}M(x_{k},x_{k+1})}{2}\\right)\\|\\nabla f(x_{k})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "See Lemma A.1. If $\\eta_{k}<2/M(x_{k},x_{k+1})$ , then GD is guaranteed to decrease the function value and we call $\\eta_{k}$ adapted to $M(x_{k},x_{k+1})$ . However, computing adapted step-sizes is not always straightforward. For instance, finding $\\eta_{k}=1/M(x_{k},x_{k+1}(\\eta_{k}))$ requires solving a non-linear equation. ", "page_idx": 3}, {"type": "text", "text": "The rest of this section leverages directional smoothness to derive new guarantees for GD with arbitrary step-sizes. We emphasize that these results are sub-optimality bounds, rather than convergence rates; a sequence of adapted step-sizes is required to convert our propositions into a convergence theory. As a trade-off, our bounds reflect the locality of GD, rather than treating it as a global method. ", "page_idx": 3}, {"type": "text", "text": "We start with the case when $f$ has lower curvature. Instead of using strong convexity or the PLcondition (Karimi et al., 2016), we propose the directional strong convexity constant, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mu(x,y)\\!=\\!\\operatorname*{inf}_{t\\in[0,1]}{\\frac{\\langle\\nabla f(x\\!+\\!t(y\\!-\\!x))\\!-\\!\\nabla f(x),y\\!-\\!x\\rangle}{t\\|y-x\\|_{2}^{2}}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If $f$ is convex, then $\\mu(x,y)\\geq0$ and it verifies the standard lower-bound from strong convexity, ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(y)\\geq f(x)+\\langle\\nabla f(x),y-x\\rangle+\\frac{\\mu(x,y)}{2}\\|y-x\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, we have $\\mu(x,y)\\geq\\mu$ when $f$ is $\\mu$ \u2013strongly convex. We prove two bounds for convex functions using directional strong convexity. For brevity, we denote $M_{i}\\,:=\\,M(x_{i},x_{i+1})$ , $\\mu_{i}~:=$ $\\mu_{i}(x_{i},x^{*})$ , $\\delta_{i}\\bar{=}f(x_{i})-f(x^{*})$ , and $\\Delta_{i}=\\|\\dot{{\\boldsymbol{x}}}_{i}-{\\boldsymbol{x}}^{*}\\|_{2}^{2}$ , where $x^{*}$ is a minimizer of $f$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1. If $f$ is convex and differentiable, then $G D$ with step-size sequence $\\{\\eta_{k}\\}$ satisfies, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\delta_{k}\\leq\\left[\\prod_{i\\in\\mathcal{G}}\\left(1+\\eta_{i}\\lambda_{i}\\mu_{i}\\right)\\right]\\delta_{0}+\\sum_{i\\in\\mathcal{B}}\\left[\\prod_{\\substack{j>i,j\\in\\mathcal{G}}}\\left(1+\\eta_{j}\\lambda_{j}\\mu_{j}\\right)\\right]\\frac{\\eta_{i}\\lambda_{i}}{2}\\|\\nabla f(x_{i})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{i}\\,{=}\\,\\eta_{i}M_{i}\\,{-}\\,2$ , $\\mathcal{G}=\\{i:\\eta_{i}\\!<\\!2/M_{i}\\}$ , and $B=[k]\\backslash{\\mathcal{G}}$ . ", "page_idx": 4}, {"type": "text", "text": "The analysis splits iterations into good steps $\\mathcal{G}$ , where $\\eta_{k}$ is adapted to the directional smoothness, and bad steps $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , where the step-size is too large and GD may increase the optimality gap. When $f$ is $L$ -smooth and $\\mu$ -strongly convex, using the step-size sequence $\\eta_{k}=1/L$ gives ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(x_{k+1})-f(x^{*})\\leq\\left[\\prod_{i=0}^{k}\\left(1-{\\frac{\\mu_{i}\\left(2-M_{i}/L\\right)}{L}}\\right)\\right]\\left(f(x_{0})-f(x^{*})\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mu_{i}\\left(2-M_{i}/L\\right)\\geq\\mu$ . Thus, Equation (13) gives at least as tight a rate as standard assumptions by localizing to the convergence path using any directional smoothness $M$ . When $M_{i}<L$ , the gap in constants yields a strictly improved rate (see Figure 2). We also prove a more elegant bound. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.2. If $f$ is convex and differentiable, then $G D$ with step-size sequence $\\{\\eta_{k}\\}$ satisfies, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta_{k}\\leq\\left[\\prod_{i=0}^{k}\\frac{|1-\\mu_{i}\\eta_{i}|}{1+\\mu_{i+1}\\eta_{i}}\\right]\\Delta_{0}+\\sum_{i=0}^{k}\\left[\\prod_{j>i}\\frac{|1-\\mu_{j}\\eta_{j}|}{1+\\mu_{j+1}\\eta_{j}}\\right]\\frac{\\left(M_{i}\\eta_{i}^{3}-\\eta_{i}^{2}\\right)}{1+\\mu_{i+1}\\eta_{i}}\\|\\nabla f(x_{i})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Unlike Proposition 3.1, this analysis shows linear progress at each iteration and does not divide $k$ into good steps and bad steps. In exchange, the second term in Equation (15) reflects how much convergence is degraded when $\\eta_{k}$ is not adapted to the directional smoothness function $M$ . We conclude this section with a bound for when there is no lower curvature, meaning $\\mu_{i}=0$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.3. Let $\\begin{array}{r}{\\overline{{x}}_{k}\\!=\\!\\sum_{i=0}^{k}\\eta_{i}x_{i+1}/\\sum_{i=0}^{k}\\eta_{i}.\\:I\\!f\\:f}\\end{array}$ is convex and differentiable, then GD satisfies, ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(\\overline{{x}}_{k})-f(x^{*})\\leq\\frac{\\|x_{0}-x^{*}\\|_{2}^{2}}{2\\sum_{i=0}^{k}\\eta_{i}}+\\frac{\\sum_{i=0}^{k}\\eta_{i}^{2}(\\eta_{i}M_{i}-1)\\|\\nabla f(x_{i})\\|_{2}^{2}}{2\\sum_{i=0}^{k}\\eta_{i}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Eq. (16) is faster than standard analyses whenever $M_{i}<L$ ; it will be a key tool in the next sections. ", "page_idx": 4}, {"type": "text", "text": "3.1 Path-Dependent Acceleration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now we show that directional smoothness can also be used to derive path-dependent sub-optimality bounds for accelerated algorithms \u2014 that is, methods obtaining optimal rates for smooth, convex optimization. In particular, we study Nesterov\u2019s accelerated gradient descent (AGD) (Nesterov, 1983) and prove that directional smoothness leads to tighter rates given adapted step-sizes. Throughout this section we assume that $f$ is $\\mu$ -strongly convex with $\\mu=0$ when $f$ is merely convex. ", "page_idx": 4}, {"type": "text", "text": "Although our analysis uses estimating sequences (Nesterov et al., 2018), we state AGD in the following \u201cmomentum\u201d formulation, where $y_{k}$ is the momentum and $\\alpha_{k}$ the momentum parameter, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{k+1}=y_{k}-\\eta_{k}\\nabla f(y_{k})}\\\\ &{\\alpha_{k+1}^{2}=(1-\\alpha_{k+1})\\alpha_{k}^{2}\\cfrac{\\eta_{k+1}}{\\eta_{k}}+\\eta_{k+1}\\alpha_{k+1}\\mu}\\\\ &{y_{k+1}=x_{k+1}+\\cfrac{\\alpha_{k}(1-\\alpha_{k})}{\\alpha_{k}^{2}+\\alpha_{k+1}}\\left(x_{k+1}-x_{k}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If $\\eta_{k}\\leq1/M(x_{k},x_{k+1})$ , then Equation (10) combined with $1-\\eta_{k}M(x_{k},x_{k+1})/2\\le1/2$ implies, ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(x_{k+1})\\leq f(x_{k})-{\\frac{\\eta_{k}}{2}}\\|\\nabla f(y_{k})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Our analysis leverages the fact that this descent condition for $x_{k+1}$ is the only connection between the smoothness of $f$ and the convergence rate of AGD. Since Equation (18) depends only on the step-size $\\eta_{k}$ , we can replace $L$ within the analysis of AGD with a sequence of adapted step-sizes. The following theorem controls the effect of these step-sizes to obtain path-dependent bounds. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4. Suppose $f$ is differentiable, $\\mu_{-}$ \u2013strongly convex and AGD is run with adapted step-sizes $\\eta_{k}\\leq1/M_{k}$ . If $\\mu>0$ and $\\alpha_{0}=\\sqrt{\\eta_{0}\\mu},$ , then AGD obtains the following accelerated rate: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(x_{k+1})-f(x^{*})\\leq\\prod_{i=0}^{k}\\left(1-{\\sqrt{\\mu\\eta_{i}}}\\right)\\left[f(x_{0})-f(x^{*})+{\\frac{\\mu}{2}}\\|x_{0}-x^{*}\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let $\\begin{array}{r}{\\eta_{m i n}=\\operatorname*{min}_{i\\in[k]}\\eta_{i}}\\end{array}$ . If $\\mu\\geq0$ and $\\alpha_{0}\\in(\\sqrt{\\mu\\eta_{0}},c)$ , where $c$ is the maximum value of $\\alpha_{0}$ for which \u03b1\u03b7200(\u22121\u03b7\u22120\u03b1\u03b100)\u00b5 satisfies \u03b30 < 3/\u03b7min + \u00b5, then AGD obtains the following rate: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(x_{k+1})-f(x^{*})\\leq\\frac{4}{\\eta_{m i n}(\\gamma_{0}-\\mu)(k+1)^{2}}\\left[f(x_{0})-f(x^{*})+\\frac{\\gamma_{0}}{2}\\|x_{0}-x^{*}\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If $\\eta_{k}=1/M_{k}>1/L$ , then these rates are strictly faster than those obtained under $L$ -smoothness and Theorem 3.4 shows that AGD provably beneftis from taking the largest possible steps given the local geometry of $f$ . However, obtaining accelerated rates when $\\mu=0$ requires prior knowledge of the minimum step-size; while this is straightforward for $L$ -smooth functions, it is not clear how to extend such result to non-strongly convex acceleration with locally Lipschitz gradients. For example, while H. Li et al. (2023) show that the $(r,l)$ -smoothness (a valid directional smoothness function) is bounded over the iterate trajectory, their rate does not adapt to the optimization path. ", "page_idx": 5}, {"type": "text", "text": "4 Adaptive Learning Rates ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Converting our sub-optimality bounds into convergence rates requires adapted step-sizes satisfying $\\eta_{k}\\,<\\,2/\\bar{M}(x_{k},x_{k+1})$ . Given an adapted step-size, the directional descent lemma (Equation (10)) implies GD decreases $f$ and we can obtain fast rates if the step-sizes are bounded below. However, $x_{k+1}$ is itself a function of $\\eta_{k}$ , meaning adapted step-sizes are not straightforward to compute. ", "page_idx": 5}, {"type": "text", "text": "For $L$ -smooth $f$ , the different directional smoothness functions $M$ introduced in Section 2 satisfy $M(x_{k},x_{k+1})\\leq2L.$ . This implies $\\begin{array}{r}{\\eta_{k}<\\frac{1}{L}}\\end{array}$ is trivially adapted. As such step-sizes don\u2019t capture local properties of $f$ , we introduce the notion of strongly adapted step-sizes, which satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta_{k}=1/M(x_{k+1}(\\eta_{k}),x_{k}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Equation (10) implies GD with a strongly adapted step-size makes guaranteed progress as, ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(x_{k+1})\\leq f(x_{k})-[2M(x_{k+1},x_{k})]^{-1}\\,{\\|\\nabla f(x_{k})\\|_{2}^{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This progress is greater than that guaranteed by $L$ -smoothness when $M(x_{k},x_{k+1})<L$ and holds even when $f$ is not $L$ -smooth. However, it is not clear a priori if (i) strongly adapted step-sizes exist or if (ii) any iterative method achieves the progress in Eq. (21). Surprisingly, we provide a positive answer to both questions. Strongly adapted $\\eta_{k}$ are computable and we also prove GD with the Polyak step-size adapts to any choice of directional smoothness, including the optimal point-wise smoothness. Before presenting this strong result, we consider the illustrative case of quadratic minimization. ", "page_idx": 5}, {"type": "text", "text": "4.1 Adaptivity in Quadratics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Now we show that step-sizes adapted to both the point-wise smoothness $M$ and the path-wise smoothness $A$ exist when $f$ is quadratic. Let $f(\\boldsymbol{x})\\stackrel{\\bullet}{=}x^{\\top}B\\boldsymbol{x}/2-c^{\\top}x$ , where $B$ is positive semidefinite. Assuming $\\{\\eta_{k}\\}$ is strongly adapted to the directional smoothness, Equation (16) implies ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(\\overline{{\\boldsymbol{x}}}_{k})-f(\\boldsymbol{x}^{*})\\leq\\frac{\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}^{*}\\|_{2}^{2}}{2\\sum_{i=0}^{k}\\eta_{i}}=\\frac{\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}^{*}\\|_{2}^{2}}{2\\sum_{i=0}^{k}\\frac{1}{M(\\boldsymbol{x}_{i},\\boldsymbol{x}_{i+1})}}\\leq\\frac{\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}^{*}\\|_{2}^{2}}{2(k+1)}\\frac{\\sum_{i=0}^{k}M(\\boldsymbol{x}_{i},\\boldsymbol{x}_{i+1})}{k+1},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "m9WZrEXWl5/tmp/71038acc217d6261169294cbde3f4a3bd127405b836c369089377fc22eaa9d2e.jpg", "img_caption": ["Figure 3: Performance of GD with different step-size rules for a synthetic quadratic problem. We run GD for 20,000 steps on 20 random quadratic problems with $L=1000$ and Hessian skew. Leftto-right, the first plot shows the optimality gap $\\bar{f}(x_{k})-f(x^{*})$ , the second shows the point-wise directional smoothness $D(x_{k},x_{k+1})$ , and the third shows step-sizes used by the different methods. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "where we used $\\eta_{i}M_{i}=1$ as well as Jensen\u2019s inequality. This guarantee depends solely on the average directional smoothness along the optimization trajectory $\\{x_{0},x_{1},\\ldots\\}$ . When $f$ is quadratic, we can exactly compute these smoothness constants. In particular, the point-wise directional smoothness is, ", "page_idx": 6}, {"type": "equation", "text": "$$\nD(x_{i},x_{i+1})=2\\|B\\nabla f(x_{i})\\|_{2}/\\|\\nabla f(x_{i})\\|_{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Notably, $D(x_{i},x_{i+1})$ has no dependence on $x_{i+1}$ and the corresponding strongly adapted step-size is given by $\\eta_{i}=\\|\\nabla\\dot{f}(\\boldsymbol{x}_{i})\\|_{2}/(2\\|\\dot{B}\\nabla f(\\boldsymbol{x}_{i})\\|_{2})$ \u2014 see Lemma C.1. Remarkably, this expression recovers the step-size proposed by Dai and Yang (2006), who show it approximates the Cauchy step-size and converges to the \u201cedge-of-stability\u201d (J. Cohen et al., 2021) at $2/L$ as $k\\rightarrow\\infty$ . Combining this simple expression with Equation (23) gives a fast, non-asymptotic convergence rate for GD and new theoretical justification for their work. ", "page_idx": 6}, {"type": "text", "text": "We can also compute the path-wise directional smoothness in closed form. As Lemma C.2 shows, ", "page_idx": 6}, {"type": "equation", "text": "$$\nA(x_{i},x_{i+1})=\\nabla f(x_{i})^{\\top}B\\nabla f(x_{i})/\\nabla f(x)^{\\top}\\nabla f(x),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and $\\eta_{i}\\,=\\,\\nabla f(x_{i})^{\\top}\\nabla f(x_{i})/[\\nabla f(x_{i})^{\\top}B\\nabla f(x_{i})]$ is the well-known Cauchy step-size. Path-wise directional smoothness thus provides another interpretation (and convergence guarantee) for the Cauchy step-size, which is traditionally derived by minimizing $f(x-\\eta\\nabla\\bar{f}(x))$ in $\\eta$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Adaptivity for Convex Functions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the last subsection, we proved that strongly adapted step-sizes for the point-wise and path-wise directional smoothness functions have closed-form expressions when $f$ is quadratic. Moreover, these step-sizes recover two classical schemes from the optimization literature, giving them new justification and fast convergence rates. Now we consider the existence of strongly adapted step-sizes for general convex functions. Our first result gives simple conditions for Equation (21) to have at least one solution when $M$ is the point-wise directional smoothness. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.1. If $f$ is convex and continuously differentiable, then either $(i)~f$ is minimized along the ray $x(\\eta)=x-\\eta\\nabla f(x)$ or $(i i)$ there exists $\\eta>0$ satisfying $\\eta=1/D(x,x-\\eta\\nabla f(x))$ . ", "page_idx": 6}, {"type": "text", "text": "The next proposition uses a similar argument with slightly stronger conditions to show existence of strongly adapted step-sizes for the path-wise smoothness. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.2. If $f$ is convex and twice continuously differentiable, then either $(i)~f$ is minimized along the ray $x(\\eta)=x-\\eta\\nabla f(x)$ or (ii) there exists $\\eta>0$ satisfying $\\eta=1/A(x,x-\\eta\\nabla f(x))$ . ", "page_idx": 6}, {"type": "text", "text": "Propositions 4.1 and 4.2 do not assume the global smoothness; although neither proof is constructive, it is possible to compute strongly adapted step-sizes for the point-wise directional smoothness using root-finding methods. We show in Section 5 that if $f$ is twice differentiable, then strongly adapted step-sizes can be found via Newton\u2019s method using only Hessian-vector products, $\\nabla^{2}f(x)\\nabla f(x)$ . ", "page_idx": 6}, {"type": "text", "text": "4.2.1 Exponential Search ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now we show that the exponential search algorithm developed by Carmon and Hinder (2022) can be used to find step-sizes that adapt on average to the directional smoothness. Consider a fixed ", "page_idx": 6}, {"type": "text", "text": "optimization horizon $k$ and denote by $x_{i}(\\eta)$ the sequence of iterates obtained by running GD from $x_{0}$ using a fixed step-size $\\eta$ . Define the criterion function, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\psi(\\eta)=\\frac{\\sum_{i=0}^{k}||\\nabla f(x_{i}(\\eta))||_{2}^{2}}{\\sum_{i=0}^{k}M(x_{i}(\\eta),x_{i+1}(\\eta))||\\nabla f(x_{i}(\\eta))||_{2}^{2}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and suppose that we have a step-size $\\eta$ that satisfies $\\psi(\\eta)/2\\,\\le\\,\\eta\\le\\psi(\\eta)$ . Using these bounds in Proposition 3.3 yields the following convergence rate, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(\\overline{{x}}_{k})-f_{*}\\leq\\left[\\frac{k\\sum_{i=0}^{k}M(x_{i},x_{i+1})\\|\\nabla f(x_{i})\\|_{2}^{2}}{\\sum_{i=0}^{k}\\|\\nabla f(x_{i})\\|_{2}^{2}}\\right]\\|x_{0}-x^{*}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "While $\\eta$ does not adapt to each directional smoothness $M(x_{i},x_{i+1})$ along the path, it adapts to a weighted average of the directional smoothness constants, where the weights are the observed squared gradient norms. This is always smaller than the maximum directional smoothness along the trajectory and can be much smaller than the global smoothness. Furthermore, we have reduced our problem to finding $\\eta\\in[\\psi(\\eta)/2,\\psi(\\eta)]$ , which is similar to the problem Carmon and Hinder (2022) solve with exponential search. We adopt their approach as Algorithm 1 and give a convergence guarantee. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.3. Assume $f$ is convex and $L$ -smooth. Then Algorithm $^{\\,l}$ with $\\eta_{0}>0$ requires at most $2K(\\log\\log(2\\eta_{0}/L)\\vee1)$ iterations of $G D$ and in the last run it outputs a step-size $\\eta$ and point $\\begin{array}{r}{\\overline{{\\boldsymbol{x}}}_{K}=\\frac{1}{K}\\sum_{i=0}^{K-1}\\boldsymbol{x}_{i}(\\eta)}\\end{array}$ such that exactly one of the following holds: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\displaystyle C a s e\\ l!}&{\\eta=\\eta_{0}}&{a n d}&{f(\\overline{{\\boldsymbol{x}}}_{K})-f(\\boldsymbol{x}^{*})\\leq\\frac{\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}^{*}\\|_{2}^{2}}{2K\\eta_{0}}}\\\\ {\\displaystyle C a s e\\ 2!\\cdot}&{\\eta\\neq\\eta_{0}}&{a n d}&{f(\\overline{{\\boldsymbol{x}}}_{K})-f^{*}\\leq\\frac{\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}^{*}\\|_{2}^{2}}{2K}\\left[\\frac{\\sum_{i=0}^{k}M_{i}\\|\\nabla f(\\boldsymbol{x}_{i}^{\\prime})\\|_{2}^{2}}{\\sum_{i=0}^{k}\\|\\nabla f(\\boldsymbol{x}_{i}^{\\prime})\\|_{2}^{2}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $M_{i}\\overset{d e f}{=}M(x_{i}^{\\prime},x_{i+1}^{\\prime})$ and $\\ensuremath{\\boldsymbol{{x}}}_{i}^{\\prime}$ are the iterates generated by $G D$ with step-size $\\eta^{\\prime}\\in[\\eta,2\\eta]$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.3 requires $f$ to be $L$ -smooth, but has only a log log dependence on the global smoothness constant. Moreover, the rate scales with the weighted average of smoothness constants along a very close trajectory $\\{x_{1}^{\\prime},x_{2}^{\\prime},\\ldots\\}$ . In the next section, we give convergence bounds that depend on the unweighted average of the directional smoothness constants along the actual optimization trajectory. ", "page_idx": 7}, {"type": "text", "text": "4.2.2 Polyak\u2019s Step-Size Rule ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our theory so-far suggests using strongly adapted step-sizes, but neither root-finding nor exponential search are practical methods for large-scale optimization. Thus, we now consider other step-size selection rules which may leverage directional smoothness. In particular, the Polyak step-size sets, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\eta_{k}=\\gamma\\left(f(x_{k})-f(x^{*})\\right)/\\|\\nabla f(x_{k})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for some $\\gamma>0$ , which is optimal for smooth and non-smooth optimization (Hazan and Kakade, 2019) given knowledge of $f(w^{*})$ . Surprisingly, we show that GD with the Polyak step-size also achieves the same guarantee as strongly adapted step-sizes without knowledge of the directional smoothness. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.4. Suppose that $f$ is convex and differentiable and let $M$ be any directional smoothness function for $f$ . Let $\\Delta_{0}:=\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}^{*}\\|_{2}^{2}$ . Then $G D$ with the Polyak step-size and $\\gamma\\in\\left(1,2\\right)$ satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\nf(\\overline{{x}}_{k})-f(x^{*})\\leq\\frac{c(\\gamma)\\Delta_{0}}{2\\sum_{i=0}^{k-1}M(x_{i},x_{i+1})^{-1}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 4.4 measures sub-optimality at an average iterate obtained using the directional smoothness. However, it also holds for the best iterate, $\\hat{x}_{k}=\\arg\\operatorname*{min}_{i\\in[k]}f(x_{i})$ , meaning no knowledge of the directional smoothness is required to obtain the guarantee. We prove an alternative guarantee for the Polyak step-size in Theorem D.2, where the progress depends on the sum of step-sizes rather than on the average directional smoothness. This shows that the step-size in Equation (26) can itself be viewed as a measure of local smoothness, albeit without formal justification. ", "page_idx": 7}, {"type": "text", "text": "Compared with the standard guarantee for the Polyak step-size under $L$ -smoothness, $f(\\overline{{x}}_{k})-f(x^{*})\\leq$ $2L\\bar{\\Delta_{0}}/k$ (Hazan and Kakade, 2019), our analysis in Theorem 4.4 with the choice $\\gamma=1.5$ yields ", "page_idx": 8}, {"type": "equation", "text": "$$\nf(\\overline{{x}}_{k})-f(x^{*})\\leq\\frac{3\\Delta_{0}}{\\sum_{i=0}^{k-1}M(x_{i},x_{i+1})^{-1}}\\leq\\frac{3\\Delta_{0}}{k}\\frac{\\sum_{k=0}^{k-1}M(x_{i},x_{i+1})}{k},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the second bound follows from Jensen\u2019s inequality and shows that the convergence depends on the average directional smoothness along the trajectory, rather than on $L$ . If $f$ is $L$ -smooth, then $M(x_{k},x_{k+1})\\leq L$ immediately recovers the classic rate for Polyak\u2019s method up to a $3/2$ constant factor. If $f$ is not $L$ -smooth, but $M(x_{k},x_{k+1})$ is bounded, then Equation (27) generalizes the $O(1/k)$ rate proved concurrently by Takezawa et al. (2024), but for any choice of directional smoothness (of which $\\left(L_{0},L_{1}\\right)$ -smoothness (Jingzhao Zhang et al., 2020) is but one). ", "page_idx": 8}, {"type": "text", "text": "Comparison with strongly adapted step-sizes. As we saw for quadratics, strongly adapted step-sizes for any directional smoothness function allow us to obtain the following convergence rate, ", "page_idx": 8}, {"type": "equation", "text": "$$\nf(\\overline{{x}}_{k})-f(x^{*})\\leq\\frac{\\|x_{0}-x^{*}\\|_{2}^{2}}{2\\sum_{i=0}^{k-1}M(x_{i},x_{i+1})^{-1}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This is matches the guarantee given by Equation (27) up to constant factors. As a result, we give a positive answer to the question posed earlier in this section: GD with the Polyak step-size achieves the same convergence for any smoothness function $M$ as GD with step-sizes strongly adapted to $M$ . ", "page_idx": 8}, {"type": "text", "text": "Application to the optimal directional smoothness. Theorem 4.4 holds for every directional smoothness function $M$ . Therefore we can specialize Equation (27) with the optimal point-wise directional smoothness $H$ (as defined in Equation (4)) and $\\gamma=1.5$ to get the guarantee, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{i\\in[k-1]}\\left[f(x_{i})-f(x^{*})\\right]\\leq{\\frac{3\\|x_{0}-x^{*}\\|_{2}^{2}}{\\sum_{i=0}^{k-1}H(x_{i},x_{i+1})^{-1}}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This rate requires computing the iterate with the minimum function value, but that is easy to track during optimization. Unlike our previous results, Equation (28) requires no access to the optimal point-wise smoothness, yet obtains a dependence on the tightest constant possible. ", "page_idx": 8}, {"type": "text", "text": "4.3 Normalized Gradient Descent ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Now we change directions slightly and study normalized GD, whose convergence also depends on the directional smoothness. Normalized GD uses step-sizes which are divided by the gradient magnitude, ", "page_idx": 8}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\frac{\\eta_{k}}{\\|\\nabla f(x_{k})\\|_{2}}\\nabla f(x_{k}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Our next theorem shows that normalized GD obtains a guarantee which depends solely on the average of the point-wise directional smoothness $D_{k}:=D(x_{k}^{\\bar{}},x_{k+1})$ despite no explicit knowledge of $D_{k}$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.5. Suppose that $f$ is convex and differentiable. Let $D$ be the point-wise directional smoothness defined by Equation (4) and $\\Delta_{0}:=\\|\\ddot{x_{0}}-x^{*}\\|_{2}^{2}$ . Then normalized $G D$ with a sequence of non-increasing step-sizes $\\eta_{k}$ satisfies ", "page_idx": 8}, {"type": "equation", "text": "$$\nf(\\hat{x}_{k})-f(x^{*})\\leq\\frac{\\Delta_{0}+\\sum_{i=0}^{k-1}\\eta_{i}^{2}}{2k^{2}}\\left(\\frac{f(x_{0})}{\\eta_{0}^{2}}-\\frac{f(x^{*})}{\\eta_{k-1}^{2}}\\right)+\\frac{\\Delta_{0}+\\sum_{i=0}^{k-1}\\eta_{i}^{2}}{2k}\\sum_{i=0}^{k-1}\\frac{M(x_{i},x_{i+1})}{k},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where ${\\hat{x}}_{k}\\;=\\;\\arg\\operatorname*{min}_{i\\in[k-1]}f(x_{i})$ . If $\\cdot\\operatorname*{max}_{i\\in[k-1]}M(x_{i},x_{i+1})$ is bounded for all $k$ (i.e. $f$ is $L$ - smooth), then for $\\eta_{i}\\,=\\,1/{\\sqrt{i}}$ we have $f(\\hat{x}_{k})\\,-\\,f(x^{*})\\,\\in\\,\\mathcal{O}(1/k)$ and for $\\eta_{i}\\,=\\,1/{\\sqrt{i}}$ we get the anytime result $f({\\hat{x}}_{k})-f(x^{*})\\in{\\mathcal{O}}(\\log(k)/k)$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.5 gives a rate for normalized GD which is valid for any convex $f$ without any dependence on global smoothness. However, does not adapt to any smoothness function like the Polyak step-size. ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluate the practical improvement of our convergence rates over those using $L$ -smoothness on two logistic regression problems taken from the UCI repository (Asuncion and Newman, 2007). ", "page_idx": 8}, {"type": "image", "img_path": "m9WZrEXWl5/tmp/fe9589c52afaa7c25df1da1d1a5deb22ae9b6286f3743c17c4869a19b6b52526.jpg", "img_caption": ["Figure 4: Comparison of GD with $\\eta_{k}=1/L$ , step-sizes strongly adapted to the point-wise smoothness $(\\bar{\\eta_{k}}=1/D(x_{k}^{\\bar{}},x_{k+1}))$ , and the Polyak step-size against normalized GD (Norm. GD) and the AdGD method on three logistic regression problems. AdGD uses a smoothed version of the point-wise directional smoothness from the previous iteration to set $\\eta_{k}$ . We find that GD methods with adaptive step-sizes consistently outperform GD with $\\eta_{k}=1/L$ and even obtain a linear rate on horse-colic. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 1 compares GD with strongly adapted step-sizes $\\eta=1/M_{k}$ , where $M_{k}$ is the point-wise smoothness, against GD with the Polyak step-size. We also plot the exact convergence rates for each method, Equation (16) and Equation (27), respectively, and compare against the classical guarantee for both methods. Our convergence rates are an order of magnitude tighter on the ionosphere dataset and display a remarkable ability to adapt to the path of optimization on mammographic. ", "page_idx": 9}, {"type": "text", "text": "Figure 3 compares the performance of GD with strongly adapted step-sizes and with the fixed stepsize $\\eta_{k}=1/L$ for a synthetic quadratic with Hessian skew (R. Pan et al., 2022). Results are averaged over twenty random problems. We find that strongly adapted step-sizes lead to significantly faster convergence. Since $A_{k},D_{k}\\ll L$ , the adapted step-sizes are larger than $2/L$ , especially at the start of training; they eventually converge to $2/L$ , indicating these methods operate at the edge-of-stability (J. Cohen et al., 2021; J. M. Cohen et al., 2022). This is consistent with Ahn et al. (2022) and Y. Pan and Y. Li (2023), who show local smoothness is correlated with edge-of-stability behavior. ", "page_idx": 9}, {"type": "text", "text": "We conclude with a comparison of empirical convergence rates on three additional logistic regression problems from the UCI repository. We compare GD with $\\eta_{k}=1/L$ , GD with step-sizes strongly adapted to the point-wise smoothness $(\\eta_{k}\\doteq1/D_{k})$ , GD with the Polyak step-size (Polyak), and normalized GD (Norm. GD) against the AdGD method (Malitsky and Mishchenko, 2020). The Polyak step-size performs best on every dataset but ozone, where GD with $\\eta_{k}=1/D_{k}$ solves the problem to high accuracy in just a few iterations. Thus, although Polyak step-sizes have the optimal dependence on directional smoothness, computing strongly adapted step-sizes can still be advantageous. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present new sub-optimality bounds for GD under novel measures of local gradient variation which we call directional smoothness functions. Our results hold for any step-sizes, improve over standard analyses when $\\eta_{k}$ is adapted to the choice of directional smoothness, and depend only on properties of $f$ local to the optimization path. For convex quadratics, we show that computing step-sizes strongly adapted to directional smoothness functions is straightforward and recovers two well-known step-size schemes, including the Cauchy step-size. In the general case, we prove that an algorithm based on exponential search gives a weighted-version of the path-dependent convergence rate with no need for adapted step-sizes. We also show that GD with the Polyak step-size and normalized GD both obtain fast rates with no dependence on the global smoothness parameter. Crucially, the Polyak step-size adapts to any choice of directional smoothness, including the tightest possible parameter. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Aaron Mishkin was supported by NSF Grant DGE-1656518, by NSERC Grant PGSD3-547242-2020, and by an internship at the Center for Computational Mathematics, Flatiron Institute. We thank Si Yi Meng for insightful discussions during the preparation of this work and Fabian Schaipp for use of the step-back code. We also thank the anonymous reviewers for comments leading to improvements in Proposition 3.2 and the addition of Theorem D.2. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Ahn, Kwangjun, Jingzhao Zhang, and Suvrit Sra (2022). \u201cUnderstanding the unstable convergence of gradient descent\u201d. In: International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA. Vol. 162. Proceedings of Machine Learning Research, pp. 247\u2013257.   \nAltschuler, Jason M. and Pablo A. Parrilo (2023). \u201cAcceleration by Stepsize Hedging I: Multi-Step Descent and the Silver Stepsize Schedule\u201d. In: CoRR abs/2309.07879.   \nAsuncion, Arthur and David Newman (2007). UCI machine learning repository.   \nBarzilai, Jonathan and Jonathan M Borwein (1988). \u201cTwo-point step size gradient methods\u201d. In: IMA journal of numerical analysis 8.1, pp. 141\u2013148.   \nBeck, Amir (2017). First-order methods in optimization. MOS-SIAM series on optimization. Philadelphia : Philadelphia: Society for Industrial and Applied Mathematics ; Mathematical Optimization Society. ISBN: 978-161-197-4-9-9-7.   \nBengio, Yoshua (2012). \u201cPractical Recommendations for Gradient-Based Training of Deep Architectures\u201d. In: Neural Networks: Tricks of the Trade - Second Edition. Vol. 7700. Lecture Notes in Computer Science, pp. 437\u2013478.   \nBerahas, Albert S., Lindon Roberts, and Fred Roosta (2023). \u201cNon-Uniform Smoothness for Gradient Descent\u201d. In: arXiv preprint arXiv:2311.08615 abs/2311.08615.   \nBertsekas, Dimitri P (1997). \u201cNonlinear programming\u201d. In: Journal of the Operational Research Society 48.3, pp. 334\u2013334.   \nBubeck, S\u00e9bastien et al. (2015). \u201cConvex optimization: Algorithms and complexity\u201d. In: Foundations and Trends\u00ae in Machine Learning 8.3-4, pp. 231\u2013357.   \nCarmon, Yair and Oliver Hinder (2022). \u201cMaking SGD Parameter-Free\u201d. In: Conference on Learning Theory, 2-5 July 2022, London, UK. Vol. 178. Proceedings of Machine Learning Research, pp. 2360\u20132389.   \nCohen, Jeremy et al. (2021). \u201cGradient Descent on Neural Networks Typically Occurs at the Edge of Stability\u201d. In: 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.   \nCohen, Jeremy M. et al. (2022). \u201cAdaptive Gradient Methods At the Edge of Stability\u201d. In: arXiv preprint arXiv:2207.14484 abs/2207.14484.   \nDai, Y. H. and X. Q. Yang (2006). \u201cA New Gradient Method with an Optimal Stepsize Property\u201d. In: Computational Optimization and Applications 33.1, pp. 73\u201388.   \nDuchi, John C., Elad Hazan, and Yoram Singer (2010). \u201cAdaptive Subgradient Methods for Online Learning and Stochastic Optimization\u201d. In: COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pp. 257\u2013269.   \nFern\u00e1ndez-Delgado, Manuel et al. (2014). \u201cDo we need hundreds of classifiers to solve real world classification problems?\u201d In: The journal of machine learning research 15.1, pp. 3133\u20133181.   \nGrimmer, Benjamin (2019). \u201cConvergence Rates for Deterministic and Stochastic Subgradient Methods without Lipschitz Continuity\u201d. In: SIAM J. Optim. 29.2, pp. 1350\u20131365.   \nHazan, Elad and Sham Kakade (2019). \u201cRevisiting the Polyak step size\u201d. In: arXiv preprint arXiv:1905.00313.   \nHe, Kaiming et al. (2015). \u201cDelving deep into rectifiers: Surpassing human-level performance on imagenet classification\u201d. In: Proceedings of the IEEE international conference on computer vision, pp. 1026\u20131034.   \nHogan, William W (1973). \u201cPoint-to-set maps in mathematical programming\u201d. In: SIAM review 15.3, pp. 591\u2013603.   \nKarimi, Hamed, Julie Nutini, and Mark Schmidt (2016). \u201cLinear convergence of gradient and proximal-gradient methods under the polyak-\u0142ojasiewicz condition\u201d. In: Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16. Springer, pp. 795\u2013811.   \nLevy, Kfir Y. (2017). \u201cOnline to Offilne Conversions, Universality and Adaptive Minibatch Sizes\u201d. In: Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1613\u20131622.   \nLi, Haochuan et al. (2023). \u201cConvex and Non-convex Optimization Under Generalized Smoothness\u201d. In: Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.   \nLi, Zhiyuan, Kaifeng Lyu, and Sanjeev Arora (2020). \u201cReconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate\u201d. In: Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.   \nLiu, Dong C and Jorge Nocedal (1989). \u201cOn the limited memory BFGS method for large scale optimization\u201d. In: Mathematical programming 45.1-3, pp. 503\u2013528.   \nLu, Zhaosong and Sanyou Mei (2023). \u201cAccelerated first-order methods for convex optimization with locally Lipschitz continuous gradient\u201d. In: SIAM Journal on Optimization 33.3, pp. 2275\u20132310.   \nMalitsky, Yura and Konstantin Mishchenko (2020). \u201cAdaptive Gradient Descent without Descent\u201d. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event. Vol. 119. Proceedings of Machine Learning Research, pp. 6702\u20136712.   \nMei, Jincheng et al. (2021). \u201cLeveraging Non-uniformity in First-order Non-convex Optimization\u201d. In: Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event. Vol. 139. Proceedings of Machine Learning Research, pp. 7555\u20137564.   \nMishkin, Aaron, Mert Pilanci, and Mark Schmidt (2024). \u201cFaster Convergence of Stochastic Accelerated Gradient Descent under Interpolation\u201d. In: arXiv preprint arXiv:2404.02378.   \nNesterov, Yurii (1983). \u201cA method for solving the convex programming problem with convergence rate O (1/k2)\u201d. In: Dokl akad nauk Sssr. Vol. 269, p. 543.   \nNesterov, Yurii et al. (2018). Lectures on convex optimization. Vol. 137. Springer.   \nOrabona, Francesco (2023). \u201cNormalized Gradients for All\u201d. In: arXiv preprint arXiv:2308.05621 abs/2308.05621.   \nPan, Rui, Haishan Ye, and Tong Zhang (2022). \u201cEigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums\u201d. In: ICLR.   \nPan, Yan and Yuanzhi Li (2023). \u201cToward understanding why adam converges faster than sgd for transformers\u201d. In: arXiv preprint arXiv:2306.00204.   \nPaquette, Courtney et al. (2023). \u201cHalting time is predictable for large models: A universality property and average-case analysis\u201d. In: Foundations of Computational Mathematics 23.2, pp. 597\u2013673.   \nPark, Jea-Hyun, Abner J Salgado, and Steven M Wise (2021). \u201cPreconditioned accelerated gradient descent methods for locally Lipschitz smooth objectives with applications to the solution of nonlinear PDEs\u201d. In: Journal of Scientific Computing 89.1, p. 17.   \nPaszke, Adam et al. (2019). \u201cPytorch: An imperative style, high-performance deep learning library\u201d. In: Advances in neural information processing systems 32.   \nPatel, Vivak and Albert S. Berahas (2022). \u201cGradient descent in the absence of global Lipschitz continuity of the gradients: Convergence, divergence and limitations of its continuous approximation\u201d. In: arXiv preprint arXiv:2210.02418.   \nPolyak, Boris T (1987). \u201cIntroduction to optimization\u201d. In.   \nStreeter, Matthew and H. Brendan McMahan (2010). \u201cLess Regret Via Online Conditioning\u201d. In: arXiv preprint arXiv:1002.4862.   \nTakezawa, Yuki et al. (2024). \u201cPolyak Meets Parameter-free Clipped Gradient Descent\u201d. In: CoRR abs/2405.15010. DOI: 10.48550/ARXIV.2405.15010. arXiv: 2405.15010. URL: https: //doi.org/10.48550/arXiv.2405.15010.   \nVainsencher, Daniel, Han Liu, and Tong Zhang (2015). \u201cLocal Smoothness in Variance Reduced Optimization\u201d. In: Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 2179\u20132187.   \nVirtanen, Pauli et al. (2020). \u201cSciPy 1.0: Fundamental Algorithms for Scientific Computing in Python\u201d. In: Nature Methods 17, pp. 261\u2013272.   \nVladarean, Maria-Luiza, Yura Malitsky, and Volkan Cevher (2021). \u201cA first-order primal-dual method with adaptivity to local smoothness\u201d. In: Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 6171\u20136182.   \nZhang, Bohang et al. (2020). \u201cImproved Analysis of Clipping Algorithms for Non-convex Optimization\u201d. In: Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.   \nZhang, Jingzhao et al. (2020). \u201cWhy Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity\u201d. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.   \nZhang, Junyu and Mingyi Hong (2020). \u201cFirst-order algorithms without Lipschitz gradient: A sequential local optimization approach\u201d. In: arXiv preprint arXiv:2010.03194.   \nZhao, Weijing and He Huang (2024). \u201cAdaptive stepsize estimation based accelerated gradient descent algorithm for fully complex-valued neural networks\u201d. In: Expert Systems with Applications 236, p. 121166. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proofs for Section 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 2.2. If $f$ is convex and differentiable, then the point-wise directional smoothness satisfies, ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(y)\\leq f(x)+\\langle\\nabla f(x),y-x\\rangle+\\frac{D(x,y)}{2}\\|y-x\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. By the convexity of $f$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(x)+\\langle\\nabla f(x),y-x\\rangle\\leq f(y).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Rearranging and then using Cauchy-Schwarz we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x)\\leq f(y)+\\langle\\nabla f(x),x-y\\rangle}\\\\ &{\\qquad=f(y)+\\langle\\nabla f(y),x-y\\rangle+\\langle\\nabla f(x)-\\nabla f(y),x-y\\rangle}\\\\ &{\\qquad\\leq f(y)+\\langle\\nabla f(y),x-y\\rangle+\\|\\nabla f(x)-\\nabla f(y)\\|\\|x-y\\|}\\\\ &{\\qquad=f(y)+\\langle\\nabla f(y),x-y\\rangle+\\displaystyle\\frac{D(x,y)}{2}\\|x-y\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 2.4. For any differentiable function $f$ , the path-wise smoothness (7) satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(y)\\leq f(x)+\\langle\\nabla f(x),y-x\\rangle+\\frac{A(x,y)}{2}\\|y-x\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Starting from the fundamental theorem of calculus, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f(y)-f(x)-\\langle\\nabla f(x),y-x\\rangle=\\displaystyle\\int_{0}^{1}\\langle\\nabla f(x+t(y-x))-\\nabla f(x),y-x\\rangle\\,d t}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\int_{0}^{1}A(x,y)t\\|x-y\\|_{2}^{2}d t}\\\\ &{\\qquad\\qquad=\\frac{A(x,y)}{2}\\|y-x\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which completes the proof. ", "page_idx": 13}, {"type": "text", "text": "Proposition 2.3. There exists a convex, differentiable $f$ and $x,y\\in\\mathbb{R}^{d}$ such that if $t<2$ , then ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(y)>f(x)+\\langle\\nabla f(x),y-x\\rangle+\\frac{t\\|\\nabla f(x)-\\nabla f(y)\\|}{2\\|y-x\\|_{2}}\\|y-x\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Let $H_{f}$ denote the optimal pointwise directional smoothness associated with some convex and differentiable function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ (as defined in Equation (4)), and $D_{f}$ denote the pointwise directional smoothness associated with $f$ (as defined in Equation (4)). For any $t$ , the statement of (6) is equivalent to saying $\\begin{array}{r}{H_{f}>t\\frac{\\parallel\\nabla f(x)-\\dot{\\nabla}f(y)\\parallel}{\\parallel x-y\\parallel}}\\end{array}$ for all $x,y\\in\\mathbb{R}^{d}$ and convex, differentiable $f$ . Observe that Lemma 2.2 already shows that for all convex and differentiable functions $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\nH_{f}(x,y)\\leq D_{f}(x,y)=2{\\frac{\\|\\nabla f(x)-\\nabla f(y)\\|}{\\|x-y\\|}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for all $x,y\\in\\mathbb{R}^{d}$ . In order to show that this is tight, we suppose by the way of contradiction that there exists some $2>t\\geq0$ such that for all convex and differentiable functions $f:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\nH_{f}(x,y)\\leq t\\cdot{\\frac{\\|\\nabla f(x)-\\nabla f(y)\\|}{\\|x-y\\|}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for all $x,y\\in\\mathbb{R}$ . We shall show that no such $t$ exists by showing for each such $t$ there exists a function $f_{t}$ such that Equation (31) does not hold. ", "page_idx": 13}, {"type": "text", "text": "Consider $f_{\\epsilon}(x)=\\sqrt{x^{2}+\\epsilon^{2}}$ for $\\epsilon\\leq1$ . The function $f$ is differentiable. Moreover ", "page_idx": 13}, {"type": "equation", "text": "$$\nf_{\\epsilon}^{\\prime}(x)=\\frac{x}{\\sqrt{x^{2}+\\epsilon^{2}}},\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ f_{\\epsilon}^{\\prime\\prime}(x)=\\frac{\\epsilon^{2}}{(\\epsilon^{2}+x^{2})^{\\frac{3}{2}}}\\geq0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore $f$ is convex. Let $g(x)=|x|$ . Fix $x=1$ and $y=0$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|g(x)-g(y)-\\mathrm{sign}(y)\\cdot(x-y)|\\leq|f_{\\epsilon}(x)-f_{\\epsilon}(y)-f_{\\epsilon}^{\\prime}(y)\\cdot(x-y)|+|g(x)-f_{\\epsilon}(x)|~~~}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+|g(y)-f_{\\epsilon}(y)|+|(f_{\\epsilon}^{\\prime}(y)-\\mathrm{sign}(y))\\cdot(x-y)|~~}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=|f_{\\epsilon}(x)-f_{\\epsilon}(y)-f_{\\epsilon}^{\\prime}(y)\\cdot(x-y)|+\\Big|1-\\sqrt{1+\\epsilon^{2}}\\Big|}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+\\left|0-\\sqrt{\\epsilon^{2}}\\right|+|(0-0)\\cdot(1-0)|~~}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\leq|f_{\\epsilon}(x)-f_{\\epsilon}(y)-f_{\\epsilon}^{\\prime}(y)\\cdot(x-y)|+2\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now observe that ", "page_idx": 14}, {"type": "equation", "text": "$$\ng(x)-g(y)-\\operatorname{sign}(y)\\cdot(x-y)=|1|-|0|-0\\cdot(1-0)=1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore ", "page_idx": 14}, {"type": "equation", "text": "$$\n|f_{\\epsilon}(x)-f_{\\epsilon}(y)-f_{\\epsilon}^{\\prime}(y)\\cdot(x-y)|\\geq1-2\\epsilon.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By definition we have $\\begin{array}{r}{\\frac{1}{2}\\left\\|x-y\\right\\|^{2}=\\frac{1}{2}}\\end{array}$ , therefore ", "page_idx": 14}, {"type": "equation", "text": "$$\nH_{f}(x,y)\\ =\\ {\\frac{\\left|f_{\\epsilon}(x)-f_{\\epsilon}(y)-f_{\\epsilon}^{\\prime}(y)\\cdot(x-y)\\right|}{{\\frac{1}{2}}{\\left\\|x-y\\right\\|}^{2}}}\\geq2-4\\epsilon.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "But by our starting assumption we have that there exists some $t\\ <\\ 2$ such that $H_{f}(x,y)\\ \\leq$ t \u2225f (|xx)\u2212\u2212fy |(y)\u2225for all differentiable and convex functions f. Applying this to f = f\u03f5 we get ", "page_idx": 14}, {"type": "equation", "text": "$$\nH_{f_{\\epsilon}}(1,0)\\leq t\\frac{|f_{\\epsilon}^{\\prime}(1)-f_{\\epsilon}^{\\prime}(0)|}{|1|}=t\\cdot\\frac{1}{\\sqrt{1+\\epsilon^{2}}}\\leq t.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining Equations (33) and (34) we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n2-4\\epsilon\\le H_{f_{\\epsilon}}(1,0)\\le t\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Rearranging we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n2-t\\leq4\\epsilon\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Choosing $\\begin{array}{r}{\\epsilon=\\frac{2-t}{8}>0}\\end{array}$ we get a contradiction. It follows that the minimal $t$ such that $H(x,y)\\leq$ t|f \u2032(|xx)\u2212\u2212fy |\u2032(y)| for all convex and differentiable f is t = 2. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1. One step of gradient descent with step-size $\\eta_{k}>0$ makes progress as ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(x_{k+1})\\leq f(x_{k})-\\eta_{k}\\left(1-\\frac{\\eta_{k}M(x_{k},x_{k+1})}{2}\\right)\\|\\nabla f(x_{k})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Starting from Equation (4), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x_{k+1})\\leq f(x_{k})+\\langle\\nabla f(x_{k}),x_{k+1}-x_{k}\\rangle+\\displaystyle\\frac{M(x_{k},x_{k+1})}{2}\\|x_{k+1}-x_{k}\\|_{2}^{2}}\\\\ &{\\quad\\quad=f(x_{k})-\\eta_{k}\\|\\nabla f(x_{k})\\|_{2}^{2}+\\displaystyle\\frac{\\eta_{k}^{2}M(x_{k},x_{k+1})}{2}\\|\\nabla f(x_{k})\\|_{2}^{2}}\\\\ &{\\quad\\quad=f(x_{k})-\\eta_{k}\\left(\\displaystyle\\frac{1-\\eta_{k}M(x_{k},x_{k+1})}{2}\\right)\\|\\nabla f(x_{k})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Proofs for Section 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma B.1. If $f$ is convex, then for any $x,y\\in\\mathbb{R}^{d}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(y)\\geq f(x)+\\langle\\nabla f(x),y-x\\rangle+\\frac{\\mu(x,y)}{2}\\|y-x\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If $f$ is $\\mu$ strongly convex, then $\\mu(x,y)\\geq\\mu$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. The fundamental theorem of calculus implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f(x)-\\langle\\nabla f(x),y-x\\rangle=\\displaystyle\\int_{0}^{1}\\left<\\nabla f(x+t(y-x))-\\nabla f(x),y-x\\right>d t}&{}\\\\ {\\displaystyle\\geq\\displaystyle\\int_{0}^{1}\\mu(x,y)t\\|x-y\\|_{2}^{2}d t}&{}\\\\ {\\displaystyle=\\frac{\\mu(x,y)}{2}\\|y-x\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that we have implicitly used convexity to verify the inequality in the second line in the case where $\\mu(x,y)=0.$ . Now assume that $f$ is $\\mu$ strongly convex. As a standard consequence of strong-convexity, we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\langle\\nabla f(x+t(y-x))-\\nabla f(x),y-x\\rangle}{t\\|x-y\\|_{2}^{2}}=\\frac{\\langle\\nabla f(x+t(y-x))-\\nabla f(x),x+t(y-x)-x\\rangle}{t^{2}\\|x-y\\|_{2}^{2}}}\\\\ &{\\qquad\\qquad t\\|x-z\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\mu\\frac{\\|x-t(y-x)-x\\|_{2}^{2}}{t^{2}\\|y-x\\|_{2}^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mu.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proposition 3.1. If $f$ is convex and differentiable, then GD with step-size sequence $\\{\\eta_{k}\\}$ satisfies, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\delta_{k}\\leq\\left[\\prod_{i\\in\\mathcal{G}}\\left(1+\\eta_{i}\\lambda_{i}\\mu_{i}\\right)\\right]\\delta_{0}+\\sum_{i\\in\\mathcal{B}}\\left[\\prod_{j>i,j\\in\\mathcal{G}}\\left(1+\\eta_{j}\\lambda_{j}\\mu_{j}\\right)\\right]\\frac{\\eta_{i}\\lambda_{i}}{2}\\|\\nabla f(x_{i})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\lambda_{i}\\!=\\!\\eta_{i}M_{i}\\!-\\!2$ , $\\mathcal{G}=\\{i:\\eta_{i}\\!<\\!2/M_{i}\\}$ , and $B=[k]\\backslash{\\mathcal{G}}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. First note that $\\lambda_{i}<0$ for $i\\in\\mathcal G$ and $\\lambda_{i}\\geq0$ for $i\\in{\\cal B}$ . We start from Equation (10), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x_{k+1})\\leq f(x_{k})+\\eta_{k}\\left(\\frac{\\eta_{k}M\\left(x_{k},x_{k+1}\\right)}{2}-1\\right)\\|{\\nabla f(x_{k})}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=f(x_{k})+\\mathbb{1}_{k\\in\\mathcal{G}}\\cdot\\left[\\frac{\\eta_{k}\\lambda_{k}}{2}\\|{\\nabla f(x_{k})}\\|_{2}^{2}\\right]+\\mathbb{1}_{k\\in\\mathcal{B}}\\cdot\\left[\\frac{\\eta_{k}\\lambda_{k}}{2}\\|{\\nabla f(x_{k})}\\|_{2}^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq f(x_{k})+\\mathbb{1}_{k\\in\\mathcal{G}}\\cdot[\\eta_{k}\\lambda_{k}\\mu_{k}\\left(f(x_{k})-f(x^{*})\\right)]+\\mathbb{1}_{k\\in\\mathcal{B}}\\cdot\\left[\\frac{\\eta_{k}\\lambda_{k}}{2}\\|{\\nabla f(x_{k})}\\|_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used that directional strong convexity gives ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\nabla f(x_{k})\\|_{2}^{2}\\geq2\\mu_{k}\\left(f(x_{k})-f(x^{*})\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Subtracting $f(x^{*})$ from both sides and then recursively applying the inequality gives the result. Proposition 3.2. If $f$ is convex and differentiable, then $G D$ with step-size sequence $\\{\\eta_{k}\\}$ satisfies, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta_{k}\\leq\\left[\\prod_{i=0}^{k}\\frac{|1-\\mu_{i}\\eta_{i}|}{1+\\mu_{i+1}\\eta_{i}}\\right]\\Delta_{0}+\\sum_{i=0}^{k}\\left[\\prod_{j>i}\\frac{|1-\\mu_{j}\\eta_{j}|}{1+\\mu_{j+1}\\eta_{j}}\\right]\\frac{\\left(M_{i}\\eta_{i}^{3}-\\eta_{i}^{2}\\right)}{1+\\mu_{i+1}\\eta_{i}}\\|\\nabla f(x_{i})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\Delta_{k}=\\|x_{k}-x^{*}\\|_{2}^{2}$ and observe ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{k}=\\|x_{k}-x_{k+1}+x_{k+1}-x^{*}\\|_{2}^{2}=\\Delta_{k+1}+\\|x_{k}-x_{k+1}\\|_{2}^{2}+2\\left\\langle x_{k}-x_{k+1},x_{k+1}-x^{*}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using this expansion in $\\Delta_{k+1}-\\Delta_{k}$ , we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{k+1}-\\Delta_{k}=-\\|x_{k}-x_{k+1}\\|_{2}^{2}-2\\left\\langle x_{k}-x_{k+1},x_{k+1}-x^{*}\\right\\rangle}\\\\ &{\\qquad\\qquad=-\\eta_{k}^{2}\\|\\nabla f(x_{k})\\|_{2}^{2}-2\\eta_{k}\\left\\langle\\nabla f(x_{k}),x_{k+1}-x^{*}\\right\\rangle}\\\\ &{\\qquad\\qquad=-\\eta_{k}^{2}\\|\\nabla f(x_{k})\\|_{2}^{2}-2\\eta_{k}\\left\\langle\\nabla f(x_{k}),x_{k+1}-x_{k}\\right\\rangle-2\\eta_{k}\\left\\langle\\nabla f(x_{k}),x_{k}-x^{*}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we control the inner-products with directional strong convexity and directional smoothness. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq-\\eta_{k}^{2}\\|\\nabla f(x_{k})\\|_{2}^{2}-2\\eta_{k}\\left\\langle\\nabla f(x_{k}),x_{k+1}-x_{k}\\right\\rangle+2\\eta_{k}\\left[f(x^{*})-f(x_{k})-\\frac{\\mu_{k}}{2}\\Delta_{k}\\right]}\\\\ &{\\leq-\\eta_{k}^{2}\\|\\nabla f(x_{k})\\|_{2}^{2}+2\\eta_{k}\\left[f(x_{k})-f(x_{k+1})+\\frac{M\\left(x_{k},x_{k+1}\\right)\\eta_{k}^{2}}{2}\\|\\nabla f(x_{k})\\|_{2}^{2}\\right]}\\\\ &{\\qquad+2\\eta_{k}\\left[f(x^{*})-f(x_{k})-\\frac{\\mu_{k}}{2}\\Delta_{k}\\right]}\\\\ &{=\\eta_{k}^{2}\\left(M(x_{k},x_{k+1})\\eta_{k}-1\\right)\\|\\nabla f(x_{k})\\|_{2}^{2}+2\\eta_{k}\\left[f(x^{*})-f(x_{k+1})\\right]-\\mu_{k}\\eta_{k}\\Delta_{k}}\\\\ &{\\leq\\eta_{k}^{2}\\left(M(x_{k},x_{k+1})\\eta_{k}-1\\right)\\|\\nabla f(x_{k})\\|_{2}^{2}-\\eta_{k}\\mu_{k+1}\\Delta_{k+1}-\\mu_{k}\\eta_{k}\\Delta_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality follows from $\\mu_{k+1}$ strong convexity between $x_{k+1}$ and $x^{*}$ . Re-arranging this expression allows us to deduce a rate with error terms depending on the local smoothness, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\implies(1+\\mu_{k+1}\\eta_{k})\\Delta_{k+1}\\leq(1-\\mu_{k}\\eta_{k})\\,\\Delta_{k}+\\eta_{k}^{2}\\,(M(x_{k},x_{k+1})\\eta-1)\\,\\|\\nabla f(x_{k})\\|_{2}^{2}}}\\\\ &{}&{\\leq|1-\\mu_{k}\\eta_{k}|\\,\\Delta_{k}+\\eta_{k}^{2}\\,(M(x_{k},x_{k+1})\\eta-1)\\,\\|\\nabla f(x_{k})\\|_{2}^{2}}\\\\ &{}&{\\implies\\Delta_{k+1}\\leq\\frac{|1-\\mu_{k}\\eta_{k}|}{1+\\mu_{k+1}\\eta_{k}}\\Delta_{k}+\\frac{\\eta_{k}^{2}\\,\\left(M(x_{k},x_{k+1})\\eta-1\\right)}{1+\\mu_{k+1}\\eta_{k}}\\|\\nabla f(x_{k})\\|_{2}^{2}}\\\\ &{}&{\\leq\\left[\\displaystyle\\prod_{i=0}^{k}\\frac{|1-\\mu_{i}\\eta_{i}|}{1+\\mu_{i+1}\\eta_{i}}\\right]\\Delta_{0}}\\\\ &{}&{\\qquad+\\sum_{i=0}^{k}\\left[\\displaystyle\\prod_{j=i+1}^{k}\\frac{|1-\\mu_{j}\\eta_{j}|}{1+\\mu_{j+1}\\eta_{j}}\\right]\\frac{\\eta_{k}^{2}\\,(M(x_{i},x_{i+1})\\eta_{i}-1)}{1+\\mu_{i+1}\\eta_{i}}\\|\\nabla f(x_{i})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proposition 3.3. Let $\\scriptstyle{\\overline{{x}}}_{k}\\,=\\,\\sum_{i=0}^{k}\\eta_{i}x_{i+1}/\\sum_{i=0}^{k}\\eta_{i}$ . If $f$ is convex and differentiable, then GD satisfies, ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\overline{{x}}_{k})-f(x^{*})\\leq\\frac{\\|x_{0}-x^{*}\\|_{2}^{2}}{2\\sum_{i=0}^{k}\\eta_{i}}+\\frac{\\sum_{i=0}^{k}\\eta_{i}^{2}(\\eta_{i}M_{i}-1)\\|\\nabla f(x_{i})\\|_{2}^{2}}{2\\sum_{i=0}^{k}\\eta_{i}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Let $\\Delta_{k}=\\|x_{k}-x^{*}\\|_{2}^{2}$ and observe ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{k}=\\|x_{k}-x_{k+1}+x_{k+1}-x^{*}\\|_{2}^{2}=\\Delta_{k+1}+\\|x_{k}-x_{k+1}\\|_{2}^{2}+2\\left\\langle x_{k}-x_{k+1},x_{k+1}-x^{*}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using this expansion in $\\Delta_{k+1}-\\Delta_{k}$ , we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{k+1}-\\Delta_{k}=-\\|x_{k}-x_{k+1}\\|_{2}^{2}-2\\left\\langle x_{k}-x_{k+1},x_{k+1}-x^{*}\\right\\rangle}\\\\ &{\\qquad\\qquad=-\\eta_{k}^{2}\\|\\nabla f(x_{k})\\|_{2}^{2}-2\\eta_{k}\\left\\langle\\nabla f(x_{k}),x_{k+1}-x^{*}\\right\\rangle}\\\\ &{\\qquad\\qquad=-\\eta_{k}^{2}\\|\\nabla f(x_{k})\\|_{2}^{2}-2\\eta_{k}\\left\\langle\\nabla f(x_{k}),x_{k+1}-x_{k}\\right\\rangle-2\\eta_{k}\\left\\langle\\nabla f(x_{k}),x_{k}-x^{*}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we use convexity and directional smoothness to control the two inner-products as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{k+1}-\\Delta_{k}\\leq-\\eta_{k}^{2}\\|\\nabla f(x_{k})\\|_{2}^{2}-2\\eta_{k}\\left(f(x_{k})-f(x^{*})\\right)-2\\eta_{k}\\left\\langle\\nabla f(x_{k}),x_{k+1}-x_{k}\\right\\rangle}\\\\ &{\\qquad\\qquad\\leq-\\eta_{k}^{2}\\|\\nabla f(x_{k})\\|_{2}^{2}-2\\eta_{k}\\left(f(x_{k})-f(x^{*})\\right)+2\\eta_{k}(f(x_{k})-f(x_{k+1})\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\eta_{k}^{3}M(x_{k},x_{k+1})\\|\\nabla f(x_{k})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=\\eta_{k}^{2}(\\eta_{k}M(x_{k},x_{k+1})-1)\\|\\nabla f(x_{k})\\|_{2}^{2}-2\\eta_{k}\\left(f(x_{k+1})-f(x^{*})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Re-arranging this equation and summing over iterations implies the following sub-optimality bound: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{k}\\frac{\\eta_{i}}{\\sum_{i=0}^{k}\\eta_{i}}\\left(f(x_{i+1})-f(x^{*})\\right)\\leq\\frac{\\Delta_{0}+\\sum_{i=0}^{k}\\eta_{i}^{2}\\big(\\eta_{i}M(x_{i},x_{i+1})-1\\big)\\|\\nabla f(x_{i})\\|_{2}^{2}}{2\\sum_{i=0}^{k}\\eta_{i}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Convexity of $f$ and Jensen\u2019s inequality now imply the final result, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\implies f(\\overline{{x}}_{k})-f(x^{*})\\leq\\frac{\\Delta_{0}+\\sum_{i=0}^{k}\\eta_{i}^{2}(\\eta_{i}M(x_{i},x_{i+1})-1)\\|\\nabla f(x_{i})\\|_{2}^{2}}{2\\sum_{i=0}^{k}\\eta_{i}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.1 Path-Dependent Acceleration: Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This section proves Theorem 3.4 using estimating sequences. Throughout this section, we assume $\\mu\\geq0$ is the global strong convexity parameter, where $\\mu=0$ covers the non-strongly convex case. We start from the estimating sequences version of Equation (17), which is given as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\alpha_{k}^{2}=\\eta_{k}(1-\\alpha_{k})\\gamma_{k}+\\eta_{k}\\alpha_{k}\\mu}\\\\ &{\\gamma_{k+1}=(1-\\alpha_{k})\\gamma_{k}+\\alpha_{k}\\mu}\\\\ &{\\quad y_{k}=\\frac{1}{\\gamma_{k}+\\alpha_{k}\\mu}\\left[\\alpha_{k}\\gamma_{k}v_{k}+\\gamma_{k+1}x_{k}\\right]}\\\\ &{x_{k+1}=y_{k}-\\eta_{k}\\nabla f(y_{k})}\\\\ &{v_{k+1}=\\frac{1}{\\gamma_{k+1}}\\left[(1-\\alpha_{k})\\gamma_{k}v_{k}+\\alpha_{k}\\mu y_{k}-\\alpha_{k}\\nabla f(y_{k})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The algorithm is initialized with $x_{0}=v_{0}$ and some $\\gamma_{0}>0$ . Note that $y_{0}=x_{0}=v_{0}$ since $y_{k}$ is a convex combination of $x_{k}$ and $v_{k}$ . First we prove that this scheme is equivalent to the one given in Equation (17). ", "page_idx": 17}, {"type": "text", "text": "Lemma B.2. Equation (36) and Equation (17) lead to equivalent updates for the $y_{k},\\,x_{k}$ , and $\\alpha_{k}$ sequences. Moreover, given initialization $\\gamma_{0}>0$ , the corresponding initialization for $\\alpha_{0}$ is, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\alpha_{0}=\\frac{\\eta_{0}}{2}\\left((\\mu-\\gamma_{0})+\\sqrt{(\\gamma_{0}-\\mu)^{2}+4\\gamma_{0}/\\eta_{0}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The proof follows Nesterov et al. (2018, Theorem 2.2.3). Expanding the definition of $v_{k+1}$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{v_{k+1}}=\\displaystyle\\frac{1}{\\gamma_{k+1}}\\left[\\frac{\\left(1-\\alpha_{k}\\right)}{\\alpha_{k}}\\left[(\\gamma_{k}+\\alpha_{k}\\mu)y_{k}-\\gamma_{k+1}x_{k}\\right]+\\alpha_{k}\\mu y_{k}-\\alpha_{k}\\nabla f(y_{k})\\right]}\\\\ {\\displaystyle=\\frac{1}{\\gamma_{k+1}}\\left[\\frac{\\left(1-\\alpha_{k}\\right)\\gamma_{k}}{\\alpha_{k}}y_{k}+\\mu y_{k}\\right]-\\frac{\\left(1-\\alpha_{k}\\right)}{\\alpha_{k}}x_{k}-\\frac{\\alpha_{k}}{\\gamma_{k+1}}\\nabla f(y_{k})}\\\\ {\\displaystyle=x_{k}-\\frac{\\eta_{k}}{\\alpha_{k}}\\nabla f(y_{k})+\\frac{1}{\\alpha_{k}}(y_{k}-x_{k})}\\\\ {\\displaystyle=x_{k}+\\frac{1}{\\alpha_{k}}\\left(x_{k+1}-x_{k}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Plugging this back into the expression for $y k{+}1$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{y_{k+1}={\\frac{1}{\\gamma_{k+1}+\\alpha_{k+1}\\mu}}[\\alpha_{k+1}\\gamma_{k+1}v_{k+1}+\\gamma_{k+2}x_{k+1}]}\\\\ {={\\frac{1}{\\gamma_{k+1}+\\alpha_{k+1}\\mu}}{\\Big[}\\alpha_{k+1}\\gamma_{k+1}(x_{k}+{\\frac{1}{\\alpha_{k}}}(x_{k+1}-x_{k}))+\\gamma_{k+2}x_{k+1}{\\Big]}}\\\\ {={\\frac{\\alpha_{k+1}\\gamma_{k+1}+\\alpha_{k}(1-\\alpha_{k+1})\\gamma_{k+1}+\\alpha_{k}\\alpha_{k+1}\\mu}{\\alpha_{k}\\left(\\gamma_{k+1}+\\alpha_{k+1}\\mu\\right)}}x_{k+1}-{\\frac{\\alpha_{k+1}\\gamma_{k+1}(1-\\alpha_{k})}{\\alpha_{k}\\left(\\gamma_{k+1}+\\alpha_{k+1}\\mu\\right)^{x_{k}}}}}\\\\ {=x_{k+1}+{\\frac{\\alpha_{k+1}\\gamma_{k+1}(1-\\alpha_{k})}{\\alpha_{k}\\left(\\gamma_{k+1}+\\alpha_{k+1}\\mu\\right)}}(x_{k+1}-x_{k})}\\\\ {=x_{k+1}+{\\frac{\\alpha_{k+1}\\gamma_{k+1}(1-\\alpha_{k})}{\\alpha_{k}\\left(\\gamma_{k+1}+\\alpha_{k+1}+\\eta_{k}-\\left(1-\\alpha_{k+1}\\right)\\gamma_{k+1}\\right)}}(x_{k+1}-x_{k})}\\\\ {=x_{k+1}+{\\frac{\\alpha_{k}}{\\left(\\alpha_{k+1}+\\alpha_{k}\\right)}}(x_{k+1}-(1-\\alpha_{k+1})\\gamma_{k+1})}\\\\ {=x_{k+1}+{\\frac{\\alpha_{k}}{\\left(\\alpha_{k+1}+\\alpha_{k}\\right)}}(x_{k+1}-x_{k}).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that this update is consistent with Equation (17). Since $\\gamma_{k}=\\alpha_{k}^{2}/\\eta_{k}$ , we can write, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{k+1}^{2}=\\eta_{k+1}(1-\\alpha_{k+1})\\gamma_{k}+\\eta_{k+1}\\alpha_{k+1}\\mu}\\\\ &{\\qquad=\\displaystyle\\frac{\\eta_{k+1}}{\\eta_{k}}(1-\\alpha_{k+1})\\alpha_{k}^{2}+\\eta_{k+1}\\alpha_{k+1}\\mu,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is also consistent with Equation (17). Finally, the initialization for $\\alpha_{0}$ is determined by $\\gamma_{0}$ in Equation (36) as, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{0}^{2}=\\eta_{0}(1-\\alpha_{0})\\gamma_{0}+\\eta_{0}\\alpha_{0}\\mu.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The quadratic formula now implies, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{0}=\\frac{\\eta_{0}}{2}\\left((\\mu-\\gamma_{0})+\\sqrt{(\\gamma_{0}-\\mu)^{2}+4\\gamma_{0}/\\eta_{0}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This completes the proof. ", "page_idx": 18}, {"type": "text", "text": "As mentioned, our proof uses the concept of estimating sequences. ", "page_idx": 18}, {"type": "text", "text": "Definition B.3. Two sequences $\\lambda_{k},\\,\\phi_{k}$ are estimating sequences for $f$ if $\\lambda_{l}~\\ge~0$ for all $k\\,\\in\\,\\mathbb{N}$ , $\\dim_{k\\to\\infty}\\lambda_{k}=0$ , and, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi_{k}(x)\\leq(1-\\lambda_{k})f(x)+\\lambda_{k}\\phi_{0}(x),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $x\\in\\mathbb{R}^{d}$ . ", "page_idx": 18}, {"type": "text", "text": "We use the same estimating sequences as developed by Nesterov et al. (2018). Let $\\lambda_{0}=1$ , $\\phi_{0}(x)=$ $\\begin{array}{r}{f(x_{0})+\\frac{\\gamma_{0}}{2}\\|x-x_{0}\\|_{2}^{2}}\\end{array}$ , and define the updates, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\lambda_{k+1}=(1-\\alpha_{k})\\lambda_{k}}\\\\ {\\phi_{k+1}(x)=(1-\\alpha_{k})\\phi_{k}(x)+\\alpha_{k}\\left(f(y_{k})+\\langle\\nabla f(y_{k}),x-y_{k}\\rangle+\\frac{\\mu}{2}\\|x-y_{k}\\|_{2}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mu\\geq0$ is the strong convexity parameter, with $\\mu=0$ when $f$ is merely convex. It is straightforward to differentiate $\\phi_{k+1}$ to see that $v_{k+1}$ of Equation (36) is the minimizer. Indeed, Nesterov et al. (2018, Lemma 2.2.3) shows that this choice for the estimating sequences obeys the following canonical form: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi_{k+1}(x)=\\operatorname*{min}_{z}\\phi_{k+1}(z)+\\frac{\\gamma_{k+1}}{2}\\|x-v_{k+1}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\gamma_{k+1}$ and $v_{k+1}$ are given by Equation (36) and the minimum value is, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{z}\\phi_{k+1}(z)=(1-\\alpha_{k})\\operatorname*{min}_{z}\\phi_{k}(z)+\\alpha_{k}f(y_{k})-\\frac{\\alpha_{k}^{2}}{2\\gamma_{k+1}}\\|\\nabla f(y_{k})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\,\\frac{\\alpha_{k}(1-\\alpha_{k})\\gamma_{k}}{\\gamma_{k+1}}\\left(\\frac{\\mu}{2}\\|y_{k}-v_{k}\\|_{2}^{2}+\\langle\\nabla f(y_{k}),v_{k}-y_{k}\\rangle\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Before we can prove our main theorem, we must show that these choices for $\\lambda_{k}$ and $\\phi_{k}$ yield a valid estimating sequence. The following proofs build on (Nesterov et al., 2018) and (Mishkin et al., 2024). ", "page_idx": 18}, {"type": "text", "text": "Lemma B.4. Assume $\\alpha_{k}\\in(0,1]$ for all $k\\in\\mathbb{N}$ . If $\\mu>0$ and $\\gamma_{0}=\\mu_{:}$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda_{k}=\\prod_{i=0}^{k-1}(1-\\sqrt{\\eta_{k}\\mu}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $\\mu\\geq0$ and $\\gamma_{0}\\in(\\mu,\\mu+3/\\eta_{m i n}).$ , then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda_{k}\\leq{\\frac{4}{\\eta_{m i n}(\\gamma_{0}-\\mu)(k+1)^{2}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Assume $\\gamma_{0}=\\mu>0$ . Then $\\gamma_{k}=\\mu$ for all $k$ and, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{k}^{2}=(1-\\alpha_{k})\\eta_{k}\\mu+\\alpha_{k}\\eta_{k}\\mu}\\\\ {=\\eta_{k}\\mu.\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As a consequence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda_{k}=\\prod_{i=0}^{k-1}(1-\\sqrt{\\eta_{k}\\mu}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as claimed. ", "page_idx": 18}, {"type": "text", "text": "Now assume $\\gamma_{0}\\in\\left(\\mu,3L+\\mu\\right)$ . Modifying the proof by Nesterov et al. (2018, Lemma 2.2.4), we compute as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\gamma_{k+1}-\\mu=(1-\\alpha_{k})\\gamma_{k}+(\\alpha_{k}-1)\\mu=(1-\\alpha_{k})(\\gamma_{k}-\\mu)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recursing on this equality implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\gamma_{k+1}=(\\gamma_{0}-\\mu)\\prod_{i=0}^{k}(1-\\alpha_{k})=\\lambda_{k+1}(\\gamma_{0}-\\mu).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If $\\alpha_{k}=1$ or $\\lambda_{k}=0$ , then using $\\lambda_{k+1}=(1-\\alpha_{k})\\lambda_{k}$ implies $\\lambda_{k+1}=0$ and the result trivially holds. Otherwise, recall $\\alpha_{k}^{2}/\\gamma_{k+1}=\\eta_{k}$ to obtain, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{1-\\displaystyle\\frac{\\lambda_{k+1}}{\\lambda_{k}}=\\alpha_{k}=(\\gamma_{k+1}\\eta_{k})^{1/2}}}\\\\ {{=(\\eta_{k}\\mu+\\eta_{k}\\lambda_{k+1}(\\gamma_{0}-\\mu))^{1/2}}}\\\\ {{\\Longrightarrow{\\displaystyle\\frac{1}{\\lambda_{k+1}}-\\frac{1}{\\lambda_{k}}=\\displaystyle\\frac{1}{\\lambda_{k+1}^{1/2}}\\left[\\frac{\\eta_{k}\\mu}{\\lambda_{k+1}}+\\eta_{k}(\\gamma_{0}-\\mu)\\right]^{1/2}}}}\\\\ {{\\geq\\displaystyle\\frac{1}{\\lambda_{k+1}^{1/2}}\\left[\\frac{\\eta_{\\mathrm{min}}\\mu}{\\lambda_{k+1}}+\\eta_{\\mathrm{min}}(\\gamma_{0}-\\mu)\\right]^{1/2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, this implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{2}{\\lambda_{k+1}^{1/2}}\\left(\\frac{1}{\\lambda_{k+1}^{1/2}}-\\frac{1}{\\lambda_{k}^{1/2}}\\right)\\geq\\left(\\frac{1}{\\lambda_{k+1}^{1/2}}-\\frac{1}{\\lambda_{k}^{1/2}}\\right)\\left(\\frac{1}{\\lambda_{k+1}^{1/2}}+\\frac{1}{\\lambda_{k}^{1/2}}\\right)}\\\\ {\\geq\\frac{1}{\\lambda_{k+1}^{1/2}}\\left[\\frac{\\eta_{\\mathrm{min}}\\mu}{\\lambda_{k+1}}+\\eta_{\\mathrm{min}}(\\gamma_{0}-\\mu)\\right]^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, this bound holds uniformly for all $k\\in\\mathbb{N}$ . We have now exactly reached Eq. 2.2.11 of Nesterov et al. (2018, Lemma 2.2.4) with $L$ replaced by $1/\\eta_{\\mathrm{min}}$ . Applying that Lemma with this modification, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{k}\\leq\\frac{4}{\\eta_{\\mathrm{min}}(\\gamma_{0}-\\mu)(k+1)^{2}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which completes the proof. ", "page_idx": 19}, {"type": "text", "text": "Lemma B.5. If $f$ is strongly convex with parameter $\\mu\\geq0$ and $\\eta_{k}\\leq1/\\mu$ for all $k\\in\\mathbb{N},$ , then $\\lambda_{k}$ and $\\phi_{k}$ are estimating sequences. ", "page_idx": 19}, {"type": "text", "text": "Proof. Using the quadratic formula, we find ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{k}=\\frac{\\mu-\\gamma_{k}\\pm\\sqrt{(\\mu-\\gamma_{k})^{2}+4\\hat{\\gamma}_{k}/\\eta_{k}}}{2/\\eta_{k}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n(\\mu-\\gamma_{k})+\\left((\\mu-\\gamma_{k})^{2}+4\\hat{\\gamma}_{k}/\\eta_{k}\\right)^{1/2}>0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is sufficient for $\\alpha_{k}>0$ . This holds if $\\mu\\geq\\gamma_{k}$ . Otherwise, we require, ", "page_idx": 19}, {"type": "equation", "text": "$$\n(\\mu-\\gamma_{k})^{2}+4\\hat{\\gamma}_{k}/\\eta_{k}>(\\mu-\\gamma_{k})^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which holds if and only if $\\eta_{k},\\gamma_{k}>0$ . On the other hand, we also need $\\alpha_{k}\\leq1$ , which is satisfied when, ", "page_idx": 19}, {"type": "equation", "text": "$$\n4+4\\eta_{k}(\\gamma_{k}-\\mu)+\\eta_{k}^{2}(\\mu-\\gamma_{k})^{2}\\le\\eta_{k}^{2}(\\mu-\\gamma_{k})^{2}+4\\eta_{k}\\gamma_{k}\\iff\\eta_{k}\\leq\\frac{1}{\\mu},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as claimed. ", "page_idx": 19}, {"type": "text", "text": "Recall $\\lambda_{0}=1$ and $\\lambda_{k+1}=(1-\\alpha_{k})\\lambda_{k}$ . Since $\\alpha_{k}\\in(0,1]$ , $\\lambda_{k}\\geq0$ holds by induction. It remains to show that $\\lambda_{k}$ tends to zero, which holds by Lemma B.4 since we have shown $\\alpha_{k}\\in(0,1]$ for all $k$ . Now we establish the last piece, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\phi_{k}(x)\\leq(1-\\lambda_{k})f(x)+\\lambda_{k}\\phi_{0}(x).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "But this follows immediately by Nesterov et al. (2018, Lemma 2.2.2). ", "page_idx": 19}, {"type": "text", "text": "Now we can prove the last major lemma before our convergence result. ", "page_idx": 20}, {"type": "text", "text": "Lemma B.6. Suppose $f$ is strongly convex with parameter $\\mu\\geq0$ and $\\eta_{k}$ is a sequence of adapted step-sizes, meaning $\\eta_{k}\\leq1/M(x_{k},x_{k+1})$ . Then for every $k\\in\\mathbb{N}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{z}\\phi_{k}(z)\\geq f(x_{k}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We use an inductive proof again. The inductive assumption is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{z}\\phi_{k}(z)\\geq f(x_{k}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It is easy to see this holds at $k=0$ since, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\phi_{0}(x)=f(x_{0})+\\frac{\\gamma_{0}}{2}\\|x-v_{0}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "implies $\\mathrm{min}_{z}\\,\\phi_{0}(z)=f(x_{0})$ . Using Equation (41), we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{min}_{z}\\phi_{k+1}(z)=(1-\\alpha_{k})\\operatorname*{min}_{z}\\phi_{k}(z)+\\alpha_{k}f(y_{k})-\\frac{\\alpha_{k}^{2}}{2\\gamma_{k+1}}\\|\\nabla f(y_{k})\\|^{2}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad+\\,\\frac{\\alpha_{k}\\bigl(1-\\alpha_{k}\\bigr)\\gamma_{k}}{\\gamma_{k+1}}\\,\\Bigl(\\frac{\\mu}{2}\\|y_{k}-v_{k}\\|^{2}+\\langle\\nabla f(y_{k},z_{k}),v_{k}-y_{k}\\rangle\\Bigr)}\\\\ {\\displaystyle\\qquad\\ge\\,(1-\\alpha_{k})f(x_{k})+\\alpha_{k}f(y_{k})-\\frac{\\alpha_{k}^{2}}{2\\gamma_{k+1}}\\|\\nabla f(y_{k})\\|^{2}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad+\\,\\frac{\\alpha_{k}\\bigl(1-\\alpha_{k}\\bigr)\\gamma_{k}}{\\gamma_{k+1}}\\,\\Bigl(\\frac{\\mu}{2}\\|y_{k}-v_{k}\\|^{2}+\\langle\\nabla f(y_{k}),v_{k}-y_{k}\\rangle\\Bigr)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the inequality holds by the inductive assumption. Using convexity of $f$ and recalling $\\begin{array}{r}{\\frac{\\alpha_{k}^{2}}{\\gamma_{k+1}}=\\eta_{k}}\\end{array}$ from the definition of the update (Equation (36)), ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{z}{\\operatorname*{min}}\\,\\phi_{k+1}(z)\\geq(1-\\alpha_{k})\\left(f(y_{k})+\\langle\\nabla f(y_{k}),x_{k}-y_{k}\\rangle\\right)+\\alpha_{k}f(y_{k})-\\frac{\\eta_{k}}{2}\\|\\nabla f(y_{k})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{\\alpha_{k}\\left(1-\\alpha_{k}\\right)\\gamma_{k}}{\\gamma_{k+1}}\\left(\\frac{\\mu}{2}\\|y_{k}-v_{k}\\|^{2}+\\langle\\nabla f(y_{k}),v_{k}-y_{k}\\rangle\\right)}\\\\ &{\\qquad=f(y_{k})+(1-\\alpha_{k})\\left\\langle\\nabla f(y_{k}),x_{k}-y_{k}\\right\\rangle-\\frac{\\eta_{k}}{2}\\|\\nabla f(y_{k})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{\\alpha_{k}\\left(1-\\alpha_{k}\\right)\\gamma_{k}}{\\gamma_{k+1}}\\left(\\frac{\\mu}{2}\\|y_{k}-v_{k}\\|^{2}+\\langle\\nabla f(y_{k}),v_{k}-y_{k}\\rangle\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using the fact that the step-sizes are adapted and invoking the directional descent lemma (i.e. Equation (18)) now implies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{z}{\\operatorname*{min}}\\,\\phi_{k+1}(z)\\geq f(x_{k+1})+(1-\\alpha_{k})\\bigg(\\,\\langle\\nabla f(y_{k}),x_{k}-y_{k}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\frac{\\alpha_{k}\\gamma_{k}}{\\gamma_{k+1}}\\,\\Big(\\frac{\\mu}{2}\\|y_{k}-v_{k}\\|^{2}+\\langle\\nabla f(y_{k}),v_{k}-y_{k}\\rangle\\Big)\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The remainder of the proof is largely unchanged from the analysis in Nesterov et al. (2018). The definition of yk gives xk \u2212yk = $\\begin{array}{r}{\\bar{x}_{k}-y_{k}=\\frac{\\bar{\\alpha}_{k}\\bar{\\gamma_{k}}}{\\gamma_{k+1}}(y_{k}-\\bar{v_{k}})}\\end{array}$ \u03b1\u03b3kk+\u03b31k (yk \u2212vk), which we use to obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{z}{\\operatorname*{min}}\\,\\phi_{k+1}(z)\\geq f(x_{k+1})+(1-\\alpha_{k})\\bigg(\\frac{\\alpha_{k}\\gamma_{k}}{\\gamma_{k+1}}\\,\\langle\\nabla f(y_{k}),y_{k}-v_{k}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\frac{\\alpha_{k}\\gamma_{k}}{\\gamma_{k+1}}\\Big(\\frac{\\mu}{2}\\|y_{k}-v_{k}\\|^{2}+\\langle\\nabla f(y_{k}),v_{k}-y_{k}\\rangle\\Big)\\bigg)}\\\\ &{\\quad\\quad=f(x_{k+1})+\\frac{\\mu\\alpha_{k}\\big(1-\\alpha_{k}\\big)\\gamma_{k}}{2\\gamma_{k+1}}\\|y_{k}-v_{k}\\|^{2}}\\\\ &{\\quad\\quad\\geq f(x_{k+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "since $\\begin{array}{r}{\\frac{\\mu\\alpha_{k}\\left(1-\\alpha_{k}\\right)\\gamma_{k}}{2\\gamma_{k+1}}\\ge0}\\end{array}$ . We conclude the desired result by induction. ", "page_idx": 20}, {"type": "text", "text": "The main accelerated result now follows almost immediately. ", "page_idx": 21}, {"type": "text", "text": "Theorem 3.4. Suppose $f$ is differentiable, $\\mu_{-}$ \u2013strongly convex and AGD is run with adapted step-sizes $\\eta_{k}\\leq1/M_{k}$ . If $\\mu>0$ and $\\alpha_{0}=\\sqrt{\\eta_{0}\\mu},$ , then AGD obtains the following accelerated rate: ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(x_{k+1})-f(x^{*})\\leq\\prod_{i=0}^{k}\\left(1-{\\sqrt{\\mu\\eta_{i}}}\\right)\\left[f(x_{0})-f(x^{*})+{\\frac{\\mu}{2}}\\|x_{0}-x^{*}\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\begin{array}{r}{\\eta_{m i n}=\\operatorname*{min}_{i\\in[k]}\\eta_{i}}\\end{array}$ . If $\\mu\\geq0$ and $\\alpha_{0}\\in(\\sqrt{\\mu\\eta_{0}},c)$ , where $c$ is the maximum value of $\\alpha_{0}$ for which \u03b1\u03b7200(\u22121\u03b7\u22120\u03b1\u03b100)\u00b5 satisfies \u03b30 < 3/\u03b7min + \u00b5, then AGD obtains the following rate: ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(x_{k+1})-f(x^{*})\\leq\\frac{4}{\\eta_{m i n}(\\gamma_{0}-\\mu)(k+1)^{2}}\\left[f(x_{0})-f(x^{*})+\\frac{\\gamma_{0}}{2}\\|x_{0}-x^{*}\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We analyze the equivalent formulation given in Equation (36). See Lemma B.2 for a formal proof that these two schemes produce the same $x_{k},y_{k}$ , and $\\alpha_{k}$ iterates. Note that our proof follows Nesterov et al. (2018) and Mishkin et al. (2024) closely; while their results are very similar, we are not aware of pre-existing works which adapt them to our specific setting. ", "page_idx": 21}, {"type": "text", "text": "First, observe that $M(x_{k},x_{k+1})\\geq\\mu$ for all $k\\in\\mathbb N$ . Since the step-sizes $\\eta_{k}$ are assumed to satisfy $\\eta_{k}\\leq1/M(x_{k},x_{k+1})$ , we also have that $\\eta_{k}\\leq1/\\mu$ for every $k$ . ", "page_idx": 21}, {"type": "text", "text": "Thus, Lemma B.4 and Lemma B.5 apply. Using the definition of an estimating sequence and Lemma B.6, we obtain, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x_{k})\\leq\\underset{z}{\\operatorname*{min}}\\,\\phi_{k}(z)}\\\\ &{\\qquad\\leq\\underset{z}{\\operatorname*{min}}(1-\\lambda_{k})f(z)+\\lambda_{k}\\phi_{0}(z)}\\\\ &{\\qquad\\leq(1-\\lambda_{k})f(x^{*})+\\lambda_{k}\\phi_{0}(x^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Re-arranging this equation and expanding the definition $\\phi_{0}$ (Equation (39)), we deduce the following: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f(x_{k})-f(x^{*})\\leq\\lambda_{k}\\left(\\phi_{0}(x^{*})-f(x^{*})\\right)}\\\\ {\\qquad\\qquad\\qquad=\\lambda_{k}\\left(f(x_{0})-f(x^{*})+\\displaystyle\\frac{\\gamma_{0}}{2}\\|x_{0}-x^{*}\\|_{2}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We see that the rate of convergence of AGD is entirely controlled by the convergence of the sequence $\\lambda_{k}$ . If $\\mu>0$ and $\\gamma_{0}=\\mu$ , then Lemma B.4 implies ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(x_{k})-f(x^{*})\\leq\\prod_{i=0}^{k-1}\\left(1-\\sqrt{\\mu\\eta_{i}}\\right)\\left[f(x_{0})-f(x^{*})+\\frac{\\mu}{2}\\|x_{0}-x^{*}\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Lemma B.2, this initialization is equivalent to choosing $\\alpha_{0}=\\sqrt{\\eta_{0}\\mu}$ , which is the setting claimed in the theorem. ", "page_idx": 21}, {"type": "text", "text": "Alternatively, if $\\mu\\geq0$ and $\\gamma_{0}\\in(\\mu,\\mu+3/\\eta_{\\mathrm{min}})$ , then, ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(x_{k})-f(x^{*})\\leq{\\frac{4}{\\eta_{\\mathrm{min}}(\\gamma_{0}-\\mu)k^{2}}}\\left[f(x_{0})-f(x^{*})+{\\frac{\\gamma_{0}}{2}}\\|x_{0}-x^{*}\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the equality, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma_{0}=\\frac{\\alpha_{0}^{2}-\\eta_{0}\\alpha_{0}\\mu}{\\eta_{0}(1-\\alpha_{0})},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "holds by Lemma B.2. Since $\\alpha_{0}\\leq1$ for $\\eta_{0}\\,\\leq\\,1/\\mu,\\,\\eta_{0}$ is an increasing function of $\\gamma_{0}$ . Thus, an upper-bound $c$ on $\\alpha_{0}$ can be deduced from that on $\\gamma_{0}$ using the quadratic formula: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle c=-\\,\\frac{3\\eta_{0}}{2\\eta_{\\mathrm{min}}}+\\frac{\\eta_{0}}{2}\\left(\\frac{9}{(\\eta_{\\mathrm{min}})^{2}}+4\\frac{3\\eta_{\\mathrm{min}}+\\mu}{\\eta_{0}}\\right)^{1/2}\\,}\\\\ {\\displaystyle=\\frac{3\\eta_{0}}{2\\eta_{\\mathrm{min}}}\\left[\\left(1+4(\\eta_{\\mathrm{min}})^{2}\\frac{3\\eta_{\\mathrm{min}}+\\mu}{9\\eta_{0}}\\right)^{1/2}-1\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C Proofs for Section 4.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma C.1. Let $B$ be a positive semi-definite matrix and suppose that ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(x)={\\frac{1}{2}}x^{\\top}B x-c^{\\top}x.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $x_{i+1}=x_{i}-\\eta\\nabla f(x_{i})$ . Then for any $\\eta>0$ , the pointwise directional smoothness between the gradient descent iterates $x_{i},x_{i+1}$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{2}D(x_{i},x_{i+1})=\\frac{\\|B\\nabla f(x_{i})\\|_{2}}{\\|\\nabla f(x_{i})\\|_{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We have by straightforward algebra, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{2}D(x_{i},x_{i+1})=\\frac{\\|\\nabla f(x_{i+1})-\\nabla f(x_{i})\\|_{2}}{\\|x_{i+1}-x_{i}\\|_{2}}}\\\\ &{\\quad\\quad\\quad=\\frac{\\|\\left[B x_{i+1}-c\\right]-\\left[B x_{i}-c\\right]\\|_{2}}{\\|x_{i+1}-x_{i}\\|_{2}}}\\\\ &{\\quad\\quad\\quad=\\frac{\\|B\\left[x_{i+1}-x_{i}\\right]\\|_{2}}{\\|x_{i+1}-x_{i}\\|_{2}}}\\\\ &{\\quad\\quad\\quad=\\frac{\\|B\\left[\\-\\eta\\nabla f(x_{i})\\right]\\|_{2}}{\\|\\left[\\-\\eta\\nabla f(x_{i})\\right]\\|_{2}}}\\\\ &{\\quad\\quad\\quad=\\frac{\\|B\\nabla f(x_{i})\\|_{2}}{\\|\\nabla f(x_{i})\\|_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma C.2. Let $B$ be a positive semi-definite matrix and suppose that ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(x)={\\frac{1}{2}}x^{\\top}B x-c^{\\top}x.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $x_{i+1}=x_{i}-\\eta\\nabla f(x_{i})$ . Then for any $\\eta>0$ , the path-wise directional smoothness between the gradient descent iterates $x_{i},x_{i+1}$ is given by by ", "page_idx": 22}, {"type": "equation", "text": "$$\nA(x_{i},x_{i+1})={\\frac{\\nabla f(x_{i})^{\\top}B\\nabla f(x_{i})}{\\nabla f(x_{i})^{\\top}\\nabla f(x_{i})}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{A_{t}(x,y)=\\frac{\\langle\\nabla f(x+t(y-x))-\\nabla f(x),y-x\\rangle}{t\\|x-y\\|_{2}^{2}}}\\end{array}$ . We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{t}(x,y)=\\frac{\\langle\\nabla f(x+t(y-x))-\\nabla f(x),y-x\\rangle}{t\\|x-y\\|_{2}^{2}}}\\\\ &{\\phantom{A_{t}(x,y)=\\;}\\frac{\\langle(B(x+t(y-x)))-c-[B x-c],y-x\\rangle}{t\\|x-y\\|_{2}^{2}}}\\\\ &{\\phantom{A_{t}(x,y)=\\;}\\frac{\\langle t\\cdot B(y-x),y-x\\rangle}{t\\|x-y\\|_{2}^{2}}}\\\\ &{\\phantom{A_{t}(x,y)=\\;}\\frac{\\langle y-x\\rangle^{\\top}B(y-x)}{\\|x-y\\|_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The path-wise directional smoothness $A$ is therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A(x,y)=\\underset{t\\in[0,1]}{\\operatorname*{sup}}A_{t}(x,y)}\\\\ &{\\qquad=\\underset{t\\in[0,1]}{\\operatorname*{sup}}\\frac{(y-x)^{\\top}B(y-x)}{\\left\\|x-y\\right\\|_{2}^{2}}}\\\\ &{\\qquad=\\frac{\\left(y-x\\right)^{\\top}B(y-x)}{\\left\\|x-y\\right\\|_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plugging in $y=x-\\eta\\nabla f(x)=x-\\eta[B x-c]$ in the above gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A(x,x-\\eta\\nabla f(x))=\\frac{\\left(-\\eta\\left[B x-c\\right]\\right)B\\left(-\\eta\\right)\\left[B x-c\\right]}{\\left\\Vert\\eta\\left[B x-c\\right]\\right\\Vert_{2}^{2}}}\\\\ &{\\qquad=\\frac{\\left(B x-c\\right)^{\\top}B\\left(B x-c\\right)}{\\left\\Vert B x-c\\right\\Vert_{2}^{2}}}\\\\ &{\\qquad=\\frac{\\left(B x-c\\right)^{\\top}B\\left(B x-c\\right)}{\\left\\Vert B x-c\\right\\Vert_{2}^{2}}}\\\\ &{\\qquad=\\frac{\\left(B x-c\\right)^{\\top}B\\left(B x-c\\right)}{\\left\\Vert B x-c\\right\\Vert_{2}^{2}}}\\\\ &{\\qquad=\\frac{\\nabla f(x)^{\\top}B\\nabla f(x)}{\\nabla f(x)^{\\top}\\nabla f(x)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D Proofs for Section 4.2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proposition 4.1. If $f$ is convex and continuously differentiable, then either $(i)~f$ is minimized along the ray $x(\\eta)=x-\\eta\\nabla f(x)$ or $(i i)$ there exists $\\eta>0$ satisfying $\\eta=1/D(x,x-\\eta\\nabla f(x))$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. Let ${\\mathcal{Z}}=\\{\\eta:\\nabla f(x-\\eta\\nabla f(x))=\\nabla f(x)\\}$ . For every $\\eta\\in\\mathcal{T}$ , it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\left\\langle\\nabla f(x-\\eta\\nabla f(x)),\\nabla f(x)\\right\\rangle=-\\|\\nabla f(x)\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "However, since $f$ is convex, the directional derivative ", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\left\\langle\\nabla f(x-\\eta^{\\prime}\\nabla f(x)),\\nabla f(x)\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is monotone non-decreasing in $\\eta^{\\prime}$ . We deduce that $\\mathcal{T}$ must be an interval of form $[0,{\\overline{{\\eta}}}]$ . If $\\overline{{\\eta}}$ is not bounded, then $f$ is linear along $-\\nabla f(x)$ and is minimized by taking $\\eta\\rightarrow\\infty$ . Therefore, we may assume $\\overline{{\\eta}}$ is finite. ", "page_idx": 23}, {"type": "text", "text": "Let $\\eta>\\overline{{\\eta}}$ . Then we have the following: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x-\\eta\\nabla f(x)=x-\\frac{2\\|x-\\eta\\nabla f(x)-x\\|_{2}}{\\|\\nabla f(x-\\eta\\nabla f(x))-\\nabla f(x)\\|_{2}}\\nabla f(x)}\\\\ &{\\Longleftrightarrow\\ \\nabla f(x)=\\frac{2\\|\\nabla f(x)\\|_{2}}{\\|\\nabla f(x-\\eta\\nabla f(x))-\\nabla f(x)\\|_{2}}\\nabla f(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "from which we deduce ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\nabla f(x-\\eta\\nabla f(x))-\\nabla f(x)\\|_{2}=2\\|\\nabla f(x)\\|_{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is sufficient for the implicit equation to hold. Squaring both sides and multiplying by $1/2$ , we obtain the following alternative root-finding problem: ", "page_idx": 23}, {"type": "equation", "text": "$$\nh(\\eta):=\\frac{1}{2}\\|\\nabla f(x-\\eta\\nabla f(x))\\|_{2}^{2}-\\langle\\nabla f(x-\\eta\\nabla f(x)),\\nabla f(x)\\rangle-\\frac{1}{2}\\|\\nabla f(x)\\|_{2}^{2}=0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Because $f$ is $C^{1}$ , $h$ is a continuous function and it suffices to show that there exists an interval in which $h$ crosses 0. From the display above, we see ", "page_idx": 23}, {"type": "equation", "text": "$$\nh(\\overline{{\\eta}})=-\\|\\nabla f(x)\\|_{2}^{2}<0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Continuity now implies $\\exists\\eta^{\\prime}>\\overline{{\\eta}}$ such that $h(\\eta^{\\prime})<0$ . Now, suppose $h(\\eta)\\leq0$ for all $\\eta\\ge\\eta^{\\prime}$ . Working backwards, we see that this can only occur when ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{2\\|x-\\eta\\nabla f(x)-x\\|_{2}}{\\|\\nabla f(x-\\eta\\nabla f(x))-\\nabla f(x)\\|_{2}}=\\frac{1}{D(x(\\eta),x-\\eta\\nabla f(x))}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for all $\\eta\\ge\\eta^{\\prime}$ . The directional descent lemma (Equation (10)) now implies ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(x-\\eta\\nabla f(x))\\le f(x)-\\eta\\left(1-\\frac{\\eta D(x,x-\\eta\\nabla f(x))}{2}\\right)\\|\\nabla f(x)\\|_{2}^{2}\\leq f(x)-\\frac{\\eta}{2}\\|\\nabla f(x)\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking limits on both sides as $\\eta\\rightarrow\\infty$ implies $f(x-\\eta\\nabla f(x))$ is minimized along the ray $x(\\eta)=$ $\\boldsymbol{x}-\\eta\\nabla f(\\boldsymbol{x})$ . Thus, we deduce that either there exists $\\eta^{\\prime\\prime}>\\eta^{\\prime}$ such that $h(\\eta^{\\prime\\prime})>0$ exists, or $f$ is minimized along the gradient direction as claimed. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Proposition 4.2. If $f$ is convex and twice continuously differentiable, then either (i) $f$ is minimized along the ray $x(\\eta)=x-\\eta\\nabla f(x)$ or $(i i)$ there exists $\\eta>0$ satisfying $\\eta=1/A(x,x-\\eta\\nabla f(x))$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Let ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{I}=\\left\\{\\eta:\\left\\langle\\nabla f(x-\\eta\\nabla f(x)),\\nabla f(x)\\right\\rangle=\\|\\nabla f(x)\\|_{2}^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $f$ is convex, the directional derivative ", "page_idx": 24}, {"type": "equation", "text": "$$\n-\\left\\langle\\nabla f(x-\\eta^{\\prime}\\nabla f(x)),\\nabla f(x)\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is monotone non-decreasing in $\\eta^{\\prime}$ . We deduce that $\\mathcal{I}$ must be an interval of form $\\left[0,{\\overline{{\\eta}}}\\right]$ . If $\\overline{{\\eta}}$ is not bounded, then convexity implies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{lim}_{\\eta\\rightarrow\\infty}f(x-\\eta\\nabla f(x))\\leq\\displaystyle\\operatorname*{lim}_{\\eta\\rightarrow\\infty}f(x)-\\eta\\left\\langle\\nabla f(x-\\eta\\nabla f(x)),\\nabla f(x)\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=-\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "meaning $f$ is minimized along $-\\nabla f(x)$ . Therefore, we may assume $\\overline{{\\eta}}$ is finite. ", "page_idx": 24}, {"type": "text", "text": "We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x-\\eta\\nabla f(x)=x-\\cfrac{1}{A(x,x-\\eta\\nabla f(x))}\\nabla f(x)}\\\\ &{\\qquad\\Longleftrightarrow\\ \\eta=\\underset{t\\in[0,1]}{\\operatorname*{inf}}\\cfrac{t\\eta\\Vert\\nabla f(x)\\Vert_{2}^{2}}{\\langle\\nabla f(x)-\\nabla f(x-t\\eta\\nabla f(x)),\\nabla f(x)\\rangle}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, for $\\eta>\\overline{{\\eta}}$ , the equation we must solve reduces to ", "page_idx": 24}, {"type": "equation", "text": "$$\nh(\\eta):=\\eta-\\operatorname*{inf}_{t\\in[0,1]}\\frac{t\\eta\\|\\nabla f(x)\\|_{2}^{2}}{\\langle\\nabla f(x)-\\nabla f(x-t\\eta\\nabla f(x)),\\nabla f(x)\\rangle}=0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $f$ is $C^{2}$ , $h$ is continuous (see, e.g. Hogan (1973, Theorem 7)) and it suffices to show that there exists an interval over which $h$ crosses 0. ", "page_idx": 24}, {"type": "text", "text": "Using Taylor\u2019s theorem, we can re-write this expression as ", "page_idx": 24}, {"type": "equation", "text": "$$\nh(\\eta)=\\eta-\\operatorname*{inf}_{t\\in[0,1]}\\frac{\\|\\nabla f(x)\\|_{2}^{2}}{\\langle\\nabla f(x),\\nabla^{2}f(x-\\alpha(t\\eta)\\nabla f(x))\\nabla f(x)\\rangle},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where for some $\\alpha(t\\eta)\\in[0,t\\eta]$ . Examining the denominator, we find that, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{0}^{t}\\nabla f(x)^{\\top}\\nabla^{2}f(x-t\\bar{\\eta}\\nabla f(x))\\nabla f(x)d t=\\langle\\nabla f(x-\\bar{\\eta}\\nabla f(x))-\\nabla f(x),\\nabla f(x)\\rangle=0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which, since $f$ is convex, implies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nabla f(x)^{\\top}\\nabla^{2}f(x-\\alpha\\nabla f(x))\\nabla f(x)=0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for every $\\alpha\\,\\in\\,[0,\\overline{{\\eta}}]$ . By continuity of the Hessian, for every $\\epsilon>0$ , there exists $\\delta\\,>\\,0$ such that $\\eta^{\\prime}\\in[\\overline{{\\eta}},\\overline{{\\eta}}+\\delta]$ guarantees, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nabla f(x)^{\\top}\\nabla^{2}f(x-\\eta^{\\prime}\\nabla f(x))\\nabla f(x)<\\epsilon.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Substituting this into our expression for $h$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(\\eta^{\\prime})=\\eta^{\\prime}-\\operatorname*{inf}_{t\\in[0,1]}\\frac{\\|\\nabla f(x)\\|_{2}^{2}}{\\langle\\nabla f(x),\\nabla^{2}f(x-\\alpha(t\\eta^{\\prime})\\nabla f(x))\\nabla f(x)\\rangle}}\\\\ &{\\qquad<\\overline{{\\eta}}+\\delta-\\frac{\\|\\nabla f(x)\\|_{2}^{2}}{\\epsilon}}\\\\ &{\\qquad<0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $\\epsilon,\\delta$ sufficiently small. Thus, there exists $\\eta^{\\prime}>\\overline{{\\eta}}$ for which $h(\\eta^{\\prime})<0$ . ", "page_idx": 24}, {"type": "text", "text": "Now let us show that $h(\\eta^{\\prime\\prime})>0$ for some $\\eta^{\\prime\\prime}$ . For convenience, define ", "page_idx": 24}, {"type": "equation", "text": "$$\ng(\\eta)=\\operatorname*{inf}_{t\\in[0,1]}\\frac{t\\|\\nabla f(x)\\|_{2}^{2}}{\\langle\\nabla f(x)-\\nabla f(x-t\\eta\\nabla f(x)),\\nabla f(x)\\rangle},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Algorithm 1 Gradient Descent with Exponential Search ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1: Procedure ExponentialSearch $(x,\\eta_{0})$   \n2: for $k=1,2,3,\\ldots$ do   \n3: $\\eta_{\\mathrm{out}}\\leftarrow$ RootFindingBisection $\\left(x,2^{-2^{k}}\\eta_{0},\\eta_{0}\\right)$   \n4: if $\\eta_{\\mathrm{out}}<\\infty$ then   \n5: Return \u03b7out   \n6: end if   \n7: end for   \n8: End Procedure   \n9: Procedure RootFindingBisection $(x,\\eta_{\\mathrm{lo}},\\eta_{\\mathrm{hi}})$   \n10: Define $\\phi(\\eta)=\\eta-\\psi\\bar{(\\eta)}$ where $\\psi(\\eta)$ is given in (24) $\\backslash\\backslash$ One access to $\\phi$ requires $T$ descent   \nsteps.   \n11: if $\\phi(\\eta_{\\mathrm{hi}})\\leq0$ then   \n12: Return \u03b7hi   \n13: end if   \n14: if $\\phi(\\eta_{\\mathrm{lo}})>0$ then   \n15: Return $\\infty$   \n16: end if   \n17: while $\\eta_{\\mathrm{hi}}>2\\eta_{\\mathrm{lo}}$ do   \n18: $\\eta_{\\mathrm{mid}}=\\sqrt{\\eta_{\\mathrm{lo}}\\eta_{\\mathrm{hi}}}$   \n19: if $\\phi(\\eta_{\\mathrm{mid}})>0$ then   \n20: $\\eta_{\\mathrm{hi}}=\\eta_{\\mathrm{mid}}$   \n21: else   \n22: $\\eta_{\\mathrm{lo}}=\\eta_{\\mathrm{mid}}$   \n23: end if   \n\\\\ Invariant: $\\phi(\\eta_{\\mathrm{hi}})>0$ , and $\\phi(\\eta_{\\mathrm{lo}})\\leq0$ .   \n24: end while   \n25: Return $\\eta_{\\mathrm{lo}}$   \n26: End Procedure ", "page_idx": 25}, {"type": "text", "text": "which is a continuous and monotone non-increasing function. Take $\\eta\\rightarrow\\infty$ and let ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\eta\\to\\infty}g(\\eta)=c,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the limit exists, but may be $-\\infty$ . Indeed, it must hold that $c<\\infty$ since, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\eta\\to\\infty}g(\\eta)<g(\\eta^{\\prime})<\\infty.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If $c<0$ , then taking $\\eta^{\\prime\\prime}$ large enough that $g(\\eta^{\\prime\\prime})\\leq0$ suffices. Alternatively, if $c\\geq0$ , then there exists $\\tilde{\\eta}$ such that $g(\\eta)\\leq c+\\epsilon$ for every $\\eta\\geq\\tilde{\\eta}$ . Choosing $\\eta^{\\prime\\prime}>\\operatorname*{max}\\left\\{\\tilde{\\eta},c\\right\\}\\dot{+}\\,\\epsilon$ yields ", "page_idx": 25}, {"type": "equation", "text": "$$\nh(\\eta^{\\prime\\prime})=\\eta^{\\prime\\prime}-g(\\eta^{\\prime\\prime})>c+\\epsilon-c-\\epsilon=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This completes the proof. ", "page_idx": 25}, {"type": "text", "text": "Theorem 4.3. Assume $f$ is convex and $L$ -smooth. Then Algorithm $^{\\,l}$ with $\\eta_{0}>0$ requires at most $2K(\\log\\log(2\\eta_{0}/L)\\vee1)$ iterations of $G D$ and in the last run it outputs a step-size $\\eta$ and point $\\begin{array}{r}{\\overline{{x}}_{K}=\\frac{1}{K}\\sum_{i=0}^{K-1}x_{i}(\\eta)}\\end{array}$ such that exactly one of the following holds: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\displaystyle C a s e\\ l!}&{\\eta=\\eta_{0}}&{a n d}&{f(\\overline{{\\boldsymbol{x}}}_{K})-f(\\boldsymbol{x}^{*})\\leq\\frac{\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}^{*}\\|_{2}^{2}}{2K\\eta_{0}}}\\\\ {\\displaystyle C a s e\\ 2!\\cdot}&{\\eta\\neq\\eta_{0}}&{a n d}&{f(\\overline{{\\boldsymbol{x}}}_{K})-f^{*}\\leq\\frac{\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}^{*}\\|_{2}^{2}}{2K}\\left[\\frac{\\sum_{i=0}^{k}M_{i}\\|\\nabla f(\\boldsymbol{x}_{i}^{\\prime})\\|_{2}^{2}}{\\sum_{i=0}^{k}\\|\\nabla f(\\boldsymbol{x}_{i}^{\\prime})\\|_{2}^{2}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $M_{i}\\ {\\stackrel{d e f}{=}}\\ M(x_{i}^{\\prime},x_{i+1}^{\\prime})$ and $\\ensuremath{\\boldsymbol{{x}}}_{i}^{\\prime}$ are the iterates generated by $G D$ with step-size $\\eta^{\\prime}\\in[\\eta,2\\eta]$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem 4.3. This analysis follows (Carmon and Hinder, 2022). First, instantiate Equation (16) from Proposition 3.3 with $\\eta_{i}=\\eta$ for all $i$ to obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\nf(\\overline{{x}}_{k})-f^{*}\\leq\\frac{\\|x_{0}-x^{*}\\|^{2}}{2\\eta k}+\\frac{\\eta\\left[\\eta\\sum_{i=0}^{k}M(x_{i},x_{i+1})\\|\\nabla f(x_{i})\\|^{2}-\\sum_{i=0}^{k}\\|\\nabla f(x_{i})\\|^{2}\\right]}{2k}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now, observe that if we get a \u201cLucky strike\u201d and $\\phi(\\eta_{\\mathrm{hi}})=\\phi(\\eta_{0})\\leq0$ , then specializing Equation (45) for $\\eta=\\eta_{0}$ we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\overline{{x}}_{k})-f(x^{*})\\leq\\displaystyle\\frac{\\|x_{0}-x^{*}\\|_{2}^{2}}{2\\eta_{0}k}+\\frac{\\eta_{0}}{2k}\\left[\\eta_{0}\\sum_{i=0}^{k}M(x_{i},x_{i+1})\\|\\nabla f(x_{i})\\|_{2}^{2}-\\displaystyle\\sum_{i=0}^{k}\\|\\nabla f(x_{i})\\|_{2}^{2}\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{\\|x_{0}-x^{*}\\|_{2}^{2}}{2\\eta_{0}k}+\\frac{\\eta_{0}\\sum_{i=0}^{k}M(x_{i},x_{i+1})\\|\\nabla f(x_{i})\\|_{2}^{2}}{2k}\\cdot\\phi(\\eta_{0})}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{\\|x_{0}-x^{*}\\|_{2}^{2}}{2\\eta_{0}k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This covers the first case of Theorem 4.3. ", "page_idx": 26}, {"type": "text", "text": "With the first case out of the way, we may assume that $\\phi(\\eta_{\\mathrm{hi}})>0$ . This implies that $\\eta_{\\mathrm{hi}}>\\frac{1}{L}$ , since if $\\begin{array}{r}{\\eta\\le\\frac{1}{L}}\\end{array}$ we have $\\phi(\\eta)\\leq0$ . Now observe that when $\\begin{array}{r}{\\eta_{\\mathrm{lo}}=2^{2^{-k}}\\eta_{0}\\le\\frac{1}{L}.}\\end{array}$ , we have that $\\phi(\\eta_{\\mathrm{lo}})\\leq0$ , therefore it takes at most $\\begin{array}{r}{k\\,=\\,\\lceil\\log\\log\\frac{\\eta_{0}}{L^{-1}}\\rceil}\\end{array}$ to find such an $\\eta_{\\mathrm{lo}}$ . From here on, we suppose that $\\phi(\\eta_{\\mathrm{hi}})\\,>\\,0$ and $\\phi(\\eta_{\\mathrm{lo}})\\,\\leq\\,0$ . Now observe that the algorithm\u2019s main loop always maintains the imnvaakrei aant t $\\phi(\\eta_{\\mathrm{hi}})>0$ $\\phi(\\eta_{\\mathrm{lo}})\\leq0$ a, tiaonnds .e vTehrey  oituetrpautit osnt eopf- stihzee ps ahtailsvfeiess $\\log{\\frac{\\bar{\\eta}_{\\mathrm{hi}}}{\\eta_{\\mathrm{lo}}}}$ awnde $\\lceil\\log\\log\\eta_{0}L\\rceil$ $\\eta_{\\mathrm{lo}}$ $\\textstyle{\\frac{\\eta_{\\mathrm{hi}}}{2}}\\,\\leq\\,\\eta_{\\mathrm{lo}}\\,\\leq\\,\\eta_{h i}$ $\\phi(\\eta_{\\mathrm{lo}})\\leq0$ . Specializing Equation (45) for $\\eta=\\eta_{0}$ and using that $\\phi(\\eta_{\\mathrm{lo}})\\leq0$ we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\overline{{x}}_{k})-f(x^{*})\\leq\\frac{\\|x_{0}-x^{*}\\|_{2}^{2}}{2\\eta_{\\mathrm{lo}}k}+\\frac{\\eta_{\\mathrm{lo}}\\sum_{i=0}^{k}M(x_{i}(\\eta_{\\mathrm{lo}}),x_{i+1}(\\eta_{\\mathrm{lo}}))\\|\\nabla f(x_{i}(\\eta_{\\mathrm{lo}}))\\|_{2}^{2}}{2k}\\cdot\\phi(\\eta_{\\mathrm{lo}})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\|x_{0}-x^{*}\\|_{2}^{2}}{2\\eta_{\\mathrm{lo}}k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By the loop invariant $\\phi(\\eta_{\\mathrm{hi}})>0$ we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\phi(\\eta_{\\mathrm{hi}})>0\\Leftrightarrow\\eta_{\\mathrm{hi}}>\\frac{\\sum_{i=0}^{K}\\|\\nabla f(x_{i}(\\eta_{\\mathrm{hi}}))\\|_{2}^{2}}{\\sum_{i=0}^{K}\\|\\nabla f(x_{i}(\\eta_{\\mathrm{hi}}))\\|_{2}^{2}M(x_{i}(\\eta_{\\mathrm{hi}}),x_{i+1}(\\eta_{\\mathrm{hi}}))}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By the loop termination condition we have $\\eta_{\\mathrm{lo}}\\geq\\frac{\\eta_{\\mathrm{hi}}}{2}$ , combining this with the last equation we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\eta_{\\mathrm{lo}}\\ge\\frac{\\eta_{\\mathrm{hi}}}{2}\\ge\\frac{1}{2}\\frac{\\sum_{i=0}^{K}\\|\\nabla f(x_{i}(\\eta_{\\mathrm{hi}}))\\|_{2}^{2}}{\\sum_{i=0}^{K}\\|\\nabla f(x_{i}(\\eta_{\\mathrm{hi}}))\\|_{2}^{2}M(x_{i}(\\eta_{\\mathrm{hi}}),x_{i+1}(\\eta_{\\mathrm{hi}}))}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plugging this into Equation (46) we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\nf(\\overline{{x}}_{k})-f(x^{*})\\leq\\frac{\\|x_{0}-x^{*}\\|_{2}^{2}}{k}\\cdot\\frac{\\sum_{i=0}^{K}\\|\\nabla f(x_{i}(\\eta_{\\mathrm{hi}}))\\|_{2}^{2}M(x_{i}(\\eta_{\\mathrm{hi}}),x_{i+1}(\\eta_{\\mathrm{hi}}))}{\\sum_{i=0}^{K}\\|\\nabla f(x_{i}(\\eta_{\\mathrm{hi}}))\\|_{2}^{2}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It remains to notice that $\\eta_{\\mathrm{hi}}\\in[\\eta_{\\mathrm{lo}},2\\eta_{\\mathrm{lo}}]$ . ", "page_idx": 26}, {"type": "text", "text": "Theorem 4.4. Suppose that $f$ is convex and differentiable and let $M$ be any directional smoothness function for $f$ . Let $\\Delta_{0}:=\\|x_{0}-x^{*}\\|_{2}^{2}$ . Then $G D$ with the Polyak step-size and $\\gamma\\in\\left(1,2\\right)$ satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\nf(\\overline{{x}}_{k})-f(x^{*})\\leq\\frac{c(\\gamma)\\Delta_{0}}{2\\sum_{i=0}^{k-1}M(x_{i},x_{i+1})^{-1}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the proof of this theorem, we will need the following proposition: ", "page_idx": 26}, {"type": "text", "text": "Proposition D.1. Let $x\\,\\in\\,\\mathbb{R}^{d}$ . Define $\\begin{array}{r}{\\eta_{x}\\,=\\,\\gamma\\frac{f(x)-f(x^{*})}{\\|\\nabla f(x)\\|^{2}}}\\end{array}$ for some $\\gamma\\,\\in\\,(1,2)$ and let $\\tilde{x}\\,=\\,x\\,-$ $\\eta_{x}\\nabla f(x)$ . Then, ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(x)-f(x^{*})\\geq\\frac{\\gamma-1}{\\gamma^{2}}\\frac{2}{M(x,\\tilde{x})}\\|\\nabla f(x)\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Observe ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f(x)-f(x^{*})=f(x)-f(\\tilde{x})+f(\\tilde{x})-f(x^{*})}&{{}}\\\\ {\\geq f(x)-f(\\tilde{x}).}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By smoothness we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\widetilde{x})\\leq f(x)+\\langle\\nabla f(x),\\widetilde{x}-x\\rangle+\\displaystyle\\frac{M(x,\\widetilde{x})}{2}\\|\\widetilde{x}-x\\|^{2}}\\\\ &{\\qquad=f(x)-\\eta_{x}\\|\\nabla f(x)\\|_{2}^{2}+\\displaystyle\\frac{\\eta_{x}^{2}M(x,\\widetilde{x})}{2}\\|\\nabla f(x)\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Plugging back into Equation (47) we get ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(\\boldsymbol{x})-f(\\boldsymbol{x}^{*})\\ge\\eta_{x}\\|\\nabla f(\\boldsymbol{x})\\|_{2}^{2}-\\frac{\\eta_{x}^{2}M(\\boldsymbol{x},\\tilde{\\boldsymbol{x}})}{2}\\|\\nabla f(\\boldsymbol{x})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let us now use the definition of $\\begin{array}{r}{\\eta_{x}=\\gamma\\frac{f(x)-f(x^{*})}{||\\nabla f(x)||_{2}^{2}}}\\end{array}$ \u03b3 f(x)\u2212f(x\u2217)to get ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(x)-f(x^{*})\\geq\\gamma(f(x)-f(x^{*}))-{\\frac{\\gamma\\eta_{x}M(x,{\\tilde{x}})}{2}}(f(x)-f(x^{*})).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Assuming that $f(x)\\neq f(x^{*})$ then we get by cancellation ", "page_idx": 27}, {"type": "equation", "text": "$$\n1\\ge\\gamma-\\frac{\\gamma\\eta_{x}M(x,\\tilde{x})}{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using the definition of $\\eta_{x}$ again ", "page_idx": 27}, {"type": "equation", "text": "$$\n1-\\gamma\\geq-\\gamma^{2}\\frac{M(x,\\tilde{x})}{2}\\frac{f(x)-f(x^{*})}{\\left\\Vert\\nabla f(x)\\right\\Vert_{2}^{2}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Rearranging we get ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(x)-f(x^{*})\\geq\\frac{\\gamma-1}{\\gamma^{2}}\\frac{2}{M(x,\\tilde{x})}\\|\\nabla f(x)\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "If $f(x)\\;=\\;f(x^{*})$ then $\\|\\nabla f(x)\\|_{2}^{2}\\,=\\,0$ , both sides are identically zero and the statement holds trivially. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Now we can prove our theorem on the convergence of GD with Polyak step-sizes: ", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 4.4. We start by considering the distance to the optimum and expanding the square ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|{x}_{k+1}-{x}^{*}\\|_{2}^{2}=\\|{x}_{k}-{x}^{*}\\|_{2}^{2}+2\\left\\langle{x}_{k+1}-{x}_{k},{x}_{k}-{x}^{*}\\right\\rangle+\\|{x}_{k+1}-{x}_{k}\\|_{2}^{2}}\\\\ {=\\|{x}_{k}-{x}^{*}\\|_{2}^{2}-2\\eta_{k}\\left\\langle\\nabla f({x}_{k}),{x}_{k}-{x}^{*}\\right\\rangle+\\eta_{k}^{2}\\|\\nabla f({x}_{k})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $\\delta_{k}=f(x_{k})-f(x^{*})$ . By convexity we have $f(x^{*})\\geq f(x_{k})+\\langle\\nabla f(x_{k}),x^{*}-x_{k}\\rangle.$ Therefore we can upper bound Equation (48) as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x_{k+1}-x^{*}\\|_{2}^{2}\\leq\\|x_{k}-x^{*}\\|_{2}^{2}-2\\eta_{k}\\delta_{k}+\\eta_{k}^{2}\\|\\nabla f(x_{k})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\|x_{k}-x^{*}\\|_{2}^{2}-2\\eta_{k}\\delta_{k}+\\eta_{k}\\left(\\gamma\\frac{\\delta_{k}}{\\|\\nabla f(x_{k})\\|_{2}^{2}}\\right)\\|\\nabla f(x_{k})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=\\|x_{k}-x^{*}\\|_{2}^{2}-(2-\\gamma)\\eta_{k}\\delta_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where in the second line we used the definition of $\\eta_{k}$ . By Proposition D.1 we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\delta_{k}\\geq\\frac{\\gamma-1}{\\gamma}\\frac{2}{M(x_{k},x_{k+1})}\\|\\nabla f(x_{k})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Using this in Equation (49) gives ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x_{k+1}-x^{*}\\|_{2}^{2}\\leq\\|x_{k}-x^{*}\\|_{2}^{2}-(2-\\gamma)\\eta_{k}\\frac{\\gamma-1}{\\gamma^{2}}\\frac{2}{M(x_{k},x_{k+1})}\\|\\nabla f(x_{k})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=\\|x_{k}-x^{*}\\|_{2}^{2}-(2-\\gamma)\\frac{\\gamma-1}{\\gamma^{2}}\\frac{2}{M(x_{k},x_{k+1})}\\left(\\gamma\\frac{\\delta_{k}}{\\|\\nabla f(x_{k})\\|_{2}^{2}}\\right)\\|\\nabla f(x_{k})\\|_{2}^{2}}\\\\ &{\\qquad=\\|x_{k}-x^{*}\\|_{2}^{2}-\\frac{2(2-\\gamma)(\\gamma-1)}{\\gamma M(x_{k},x_{k+1})}\\delta_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Rearranging we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{2(2-\\gamma)(\\gamma-1)}{\\gamma M(x_{k},x_{k+1})}\\delta_{k}\\leq\\|x_{k}-x^{*}\\|_{2}^{2}-\\|x_{k+1}-x^{*}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Summing up and telescoping we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{k-1}\\frac{2(2-\\gamma)(\\gamma-1)}{\\gamma M(x_{i},x_{i+1})}\\delta_{i}\\leq\\|x_{0}-x^{*}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let xk = $\\begin{array}{r}{\\overline{{x}}_{k}\\;=\\;\\frac{1}{\\sum_{i=0}^{k-1}M(x_{i},x_{i+1})^{-1}}\\sum_{i=0}^{k-1}M(x_{i},x_{i+1})^{-1}x_{i}}\\end{array}$ , then by the convexity of $f$ and Jensen\u2019s inequality we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\overline{{x}}_{k})-f(x^{*})\\leq\\cfrac{1}{\\sum_{i=0}^{k-1}M(x_{i},x_{i+1})^{-1}}\\displaystyle\\sum_{i=0}^{k-1}M(x_{i},x_{i+1})^{-1}\\delta_{i}}\\\\ &{\\qquad\\qquad\\leq\\cfrac{\\gamma}{2(2-\\gamma)(\\gamma-1)}\\displaystyle\\frac{1}{\\sum_{i=0}^{k-1}M(x_{i},x_{i+1})^{-1}}\\|x_{0}-x^{*}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Theorem D.2. If $f$ is convex and differentiable, then $G D$ with the Polyak step-size and $\\gamma<2$ satisfies, ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(\\overline{{x}}_{k})-f(x^{*})\\leq\\frac{1}{(2-\\gamma)\\sum_{i=0}^{k}\\eta_{i}}\\|x_{0}-x^{*}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{x}}_{k}=\\sum_{i=0}^{k-1}\\eta_{i}x_{i}/\\left(\\sum_{i=0}^{k-1}\\eta_{i}\\right)}\\end{array}$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. The proof begins in the same manner as that for Theorem 4.4, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\Vert x_{k+1}-x^{*}\\right\\Vert_{2}^{2}=\\left\\Vert x_{k}-x^{*}\\right\\Vert_{2}^{2}+2\\left\\langle x_{k+1}-x_{k},x_{k}-x^{*}\\right\\rangle+\\left\\Vert x_{k+1}-x_{k}\\right\\Vert_{2}^{2}}&{}\\\\ {=\\left\\Vert x_{k}-x^{*}\\right\\Vert_{2}^{2}-2\\eta_{k}\\left\\langle\\nabla f(x_{k}),x_{k}-x^{*}\\right\\rangle+\\eta_{k}^{2}\\left\\Vert\\nabla f(x_{k})\\right\\Vert_{2}^{2}}&{}\\\\ {\\leq\\left\\Vert x_{k}-x^{*}\\right\\Vert_{2}^{2}-2\\eta_{k}\\delta_{k}+\\eta_{k}^{2}\\left\\Vert\\nabla f(x_{k})\\right\\Vert_{2}^{2}}&{}\\\\ {=\\left\\Vert x_{k}-x^{*}\\right\\Vert_{2}^{2}-2\\eta_{k}\\delta_{k}+\\eta_{k}\\left(\\gamma\\frac{\\delta_{k}}{\\left\\Vert\\nabla f(x_{k})\\right\\Vert_{2}^{2}}\\right)\\left\\Vert\\nabla f(x_{k})\\right\\Vert_{2}^{2}}&{}\\\\ {=\\left\\Vert x_{k}-x^{*}\\right\\Vert_{2}^{2}-(2-\\gamma)\\eta_{k}\\delta_{k}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Re-arranging, summing from $i=0$ to $k-1$ , and dividing by $\\textstyle\\sum_{i=0}^{k-1}\\eta_{i}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\implies\\sum_{i=0}^{k-1}\\frac{\\eta_{i}}{\\sum_{i=0}^{k}\\eta_{i}}\\left(f(x_{i})-f(w^{*})\\right)\\le\\frac{1}{\\left(2-\\gamma\\right)\\sum_{i=0}^{k}\\eta_{i}}\\left[\\|x_{0}-x^{*}\\|_{2}^{2}-\\|x_{k}-x^{*}\\|_{2}^{2}\\right]}\\\\ {\\displaystyle\\implies f(\\overline{{x}}_{k})-f(x^{*})\\le\\frac{1}{\\left(2-\\gamma\\right)\\sum_{i=0}^{k}\\eta_{i}}\\|x_{0}-x^{*}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which completes the proof. ", "page_idx": 28}, {"type": "text", "text": "Lemma D.3. Normalized GD with step-sizes $\\eta_{k}$ satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\n-\\frac{\\eta_{k}}{\\|\\nabla f(x_{k})\\|_{2}}\\left\\langle\\nabla f(x_{k}),\\nabla f(x_{k+1})\\right\\rangle\\leq\\eta_{k}^{2}M(x_{k},x_{k+1})-\\eta_{k}\\|\\nabla f(x_{k})\\|_{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. By convexity we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x_{k+1})\\leq f(x_{k})+\\langle x_{k+1}-x_{k},\\nabla f(x_{k+1})\\rangle}\\\\ &{\\quad\\quad\\quad=f(x_{k})-\\frac{\\eta_{k}}{\\|\\nabla f(x_{k})\\|_{2}}\\,\\langle\\nabla f(x_{k}),\\nabla f(x_{k+1})\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now note that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{-\\frac{\\eta_{k}}{\\|\\nabla f(x_{k})\\|_{2}}\\left\\langle\\nabla f(x_{k}),\\nabla f(x_{k+1})\\right\\rangle=\\frac{\\eta_{k}}{\\|\\nabla f(x_{k})\\|_{2}}\\left\\langle\\nabla f(x_{k}),\\nabla f(x_{k})-\\nabla f(x_{k+1})\\right\\rangle}}\\\\ &{}&{-\\eta_{k}\\|\\nabla f(x_{k})\\|_{2}}\\\\ &{}&{\\leq\\eta_{k}\\|\\nabla f(x_{k})-\\nabla f(x_{k+1})\\|-\\eta_{k}\\|\\nabla f(x_{k})\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we used Cauchy-Schwarz. Recalling the definition of directional smoothness ", "page_idx": 29}, {"type": "equation", "text": "$$\nM(x_{k},x_{k+1})\\stackrel{\\mathrm{def}}{=}\\frac{\\|\\nabla f(x_{k})-\\nabla f(x_{k+1})\\|}{\\|x_{k}-x_{k+1}\\|}=\\frac{\\|\\nabla f(x_{k})-\\nabla f(x_{k+1})\\|}{\\eta_{k}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "in Equation (55) gives ", "page_idx": 29}, {"type": "equation", "text": "$$\n-\\frac{\\eta_{k}}{\\|\\nabla f(x_{k})\\|_{2}}\\left\\langle\\nabla f(x_{k}),\\nabla f(x_{k+1})\\right\\rangle\\leq\\eta_{k}^{2}M(x_{k},x_{k+1})-\\eta_{k}\\|\\nabla f(x_{k})\\|_{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Theorem 4.5. Suppose that $f$ is convex and differentiable. Let $D$ be the point-wise directional smoothness defined by Equation (4) and $\\Delta_{0}:=\\|\\ddot{x_{0}}-x^{*}\\|_{2}^{2}$ . Then normalized $G D$ with a sequence of non-increasing step-sizes $\\eta_{k}$ satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\nf(\\hat{x}_{k})-f(x^{*})\\leq\\frac{\\Delta_{0}+\\sum_{i=0}^{k-1}\\eta_{i}^{2}}{2k^{2}}\\left(\\frac{f(x_{0})}{\\eta_{0}^{2}}-\\frac{f(x^{*})}{\\eta_{k-1}^{2}}\\right)+\\frac{\\Delta_{0}+\\sum_{i=0}^{k-1}\\eta_{i}^{2}}{2k}\\sum_{i=0}^{k-1}\\frac{M(x_{i},x_{i+1})}{k},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\hat{x}_{k}\\;=\\;\\arg\\operatorname*{min}_{i\\in[k-1]}f(x_{i}).\\;I f\\operatorname*{max}_{i\\in[k-1]}M(x_{i},x_{i+1})$ is bounded for all $k$ (i.e. $f$ is $L$ - smooth), then for $\\eta_{i}\\,=\\,1/{\\sqrt{i}}$ we have $f(\\hat{x}_{k})\\,-\\,f(x^{*})\\,\\in\\,\\mathcal{O}(1/k)$ and for $\\eta_{i}\\,=\\,1/{\\sqrt{i}}$ we get the anytime result $f({\\hat{x}}_{k})-f(x^{*})\\in{\\mathcal{O}}(\\log(k)/k)$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Here we will first establish that for any non-increasing sequence of step-sizes $\\eta_{k}>0$ we have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{i\\in[k-1]}f(x_{i})-f(x^{*})\\leq\\frac{1}{2}\\frac{\\Delta_{0}+\\sum_{i=0}^{k-1}\\eta_{i}^{2}}{k}\\left(\\frac{f(x_{0})}{k\\eta_{0}^{2}}-\\frac{f(x^{*})}{k\\eta_{k-1}^{2}}+\\sum_{i=0}^{k-1}\\frac{M(x_{i},x_{i+1})}{k}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The specialized results follow by assuming that  ik=\u221201M(xik,xi+1)i\u2217s bounded, which it is the case of \u2013Lipschitz gradients. In particular the result follows by plugging in $\\eta_{i}=1/\\sqrt{k}$ and using that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sum_{i=0}^{k-1}\\eta_{i}^{2}=\\sum_{i=0}^{k-1}\\frac{1}{k}=1}}\\\\ {{\\displaystyle\\frac{f(x_{0})}{k\\eta_{0}^{2}}-\\frac{f\\big(x^{*}\\big)}{k\\eta_{k-1}^{2}}=f(x_{0})-f(x^{*}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Alternatively we get $\\begin{array}{r}{\\operatorname*{min}_{i\\in[k-1]}f(x_{i})-f(x^{*})\\in{\\mathcal{O}}(\\log(T)/T)}\\end{array}$ by plugging in $\\eta_{i}=1/\\sqrt{i+1}$ and using that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\sum_{i=0}^{k-1}\\eta_{i}^{2}=\\sum_{i=0}^{k-1}\\frac{1}{i+1}\\le\\log(k)}\\\\ &{}&{\\displaystyle\\frac{f(x_{0})}{k\\eta_{0}^{2}}-\\frac{f(x^{*})}{k\\eta_{k-1}^{2}}=\\frac{f(x_{0})}{k}-f(x^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "With this in mind, let us prove Equation (56). ", "page_idx": 30}, {"type": "text", "text": "By convexity, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x_{k+1})\\leq f(x_{k})-\\frac{\\eta_{k}}{\\|\\nabla f(x_{k})\\|_{2}}\\nabla f(x_{k})^{\\top}\\nabla f(x_{k+1})}\\\\ &{\\qquad\\qquad\\leq f(x_{k})-\\eta_{k}\\|\\nabla f(x_{k})\\|_{2}+\\eta_{k}^{2}M(x_{k},x_{k+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Re-arranging, dividing through by $\\eta_{k}^{2}$ , and then summing over $i=0,\\cdot\\cdot\\cdot,k-1$ gives ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=0}^{k-1}\\frac{\\|\\nabla f(x_{i})\\|_{2}}{\\eta_{i}}\\leq\\frac{f(x_{0})}{\\eta_{0}^{2}}+\\sum_{i=1}^{k-2}f(x_{i})\\left(\\frac{1}{\\eta_{i}^{2}}-\\frac{1}{\\eta_{i-1}^{2}}\\right)-\\frac{f(x^{*})}{\\eta_{k-1}^{2}}+\\sum_{i=0}^{k-1}M(x_{i},x_{i+1})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{f(x_{0})}{\\eta_{0}^{2}}-\\frac{f(x^{*})}{\\eta_{k-1}^{2}}+\\sum_{i=0}^{k-1}M(x_{i},x_{i+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we used that $\\begin{array}{r}{\\eta_{i-1}\\leq\\eta_{i}\\implies\\frac{1}{\\eta_{i}^{2}}-\\frac{1}{\\eta_{i-1}^{2}}\\leq0.}\\end{array}$ . Using Jensen\u2019s inequality over the map $a\\mapsto1/a$ , which is convex for $a$ positive, gives ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{k-1}\\frac{\\eta_{i}}{\\|\\nabla f(x_{k})\\|_{2}}\\ge\\frac{k^{2}}{\\sum_{i=0}^{k-1}\\|\\nabla f(x_{k})\\|_{2}/\\eta_{i}}\\overset{(57)}{\\ge}\\frac{k^{2}}{\\frac{f(x_{0})}{\\eta_{0}^{2}}-\\frac{f(x^{*})}{\\eta_{k-1}^{2}}+\\sum_{i=0}^{k-1}M(x_{i},x_{i+1})}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Meanwhile, recall our notation $\\Delta_{i}=\\|x_{i}-x^{*}\\|_{2}^{2}$ . Expanding the squares and using that $f(x)$ is convex, we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{i+1}=\\Delta_{i}-2\\frac{\\eta_{i}}{\\left\\Vert\\nabla f\\left(x_{i}\\right)\\right\\Vert_{2}}\\nabla f(x_{i})^{\\top}(x_{i}-x^{*})+\\eta_{i}^{2}}\\\\ &{\\qquad\\leq\\Delta_{i}-2\\eta_{i}\\frac{f\\left(x_{i}\\right)-f\\left(x^{*}\\right)}{\\left\\Vert\\nabla f\\left(x_{k}\\right)\\right\\Vert_{2}}+\\eta_{i}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "As before, we use $\\delta_{i}\\;:=\\;f(x_{i})\\,-\\,f(x^{*})$ . Re-arranging, summing both sides of the above over $i=0,\\ldots,k-1$ and using telescopic cancellation gives ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{k-1}\\eta_{i}\\frac{\\delta_{i}}{\\|\\nabla f(x_{i})\\|}\\leq\\frac{\\Delta_{0}+\\sum_{i=0}^{t-1}\\eta_{i}^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Using the above along with (58) gives, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname*{min}_{i\\in[k-1]}\\delta_{i}\\leq\\frac{1}{\\sum_{i=0}^{k-1}\\frac{\\eta_{i}}{\\|\\nabla f(x_{i})\\|_{2}}}\\sum_{i=0}^{k-1}\\eta_{i}\\frac{\\delta_{i}}{\\|\\nabla f(x_{i})\\|_{2}}}}\\\\ &{\\leq\\frac{1}{2}\\frac{\\Delta_{0}+\\sum_{i=0}^{k-1}\\eta_{i}^{2}}{\\sum_{i=0}^{k-1}\\frac{\\eta_{i}}{\\|\\nabla f(x_{i})\\|}}}\\\\ &{\\leq\\frac{1}{2}\\frac{\\Delta_{0}+\\sum_{i=0}^{k-1}\\eta_{i}^{2}}{k}\\left(\\frac{f(x_{0})}{k\\eta_{0}^{2}}-\\frac{f(x^{*})}{k\\eta_{k-1}^{2}}+\\sum_{i=0}^{k-1}\\frac{M(x_{i},x_{i+1})}{k}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "E Experimental Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section we provide additional details necessary to reproduce our experiments. We run our logistic regression experiments using PyTorch (Paszke et al., 2019). For the UCI datasets, we use the pre-processed version of the data provided by Fern\u00e1ndez-Delgado et al. (2014), although we do not use their evaluation procedure as it is known have test-set leakage. Instead, we randomly perform an 80\u201320 train-test split and use the test set for validation. Unless otherwise stated, all methods are initialized using the Kaiming initialization (He et al., 2015), which is standard in PyTorch. ", "page_idx": 30}, {"type": "text", "text": "In order to compute the strongly adapted step-sizes, we run the SciPy (Virtanen et al., 2020) implementation of Newton method on Equation (44). In general, we find this procedure is surprisingly robust, although it can be slow. ", "page_idx": 31}, {"type": "text", "text": "Figure 1: We pick two datasets from the UCI repository to showcase different behaviors of the upper-bounds. We compute a tight-upper bound on $L$ as follows. Recall that for logistic regression problems the Hessian is given by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\nabla^{2}f(x)=A^{\\top}\\mathrm{Diag}\\left({\\frac{1}{\\sigma(-y\\cdot A x)+2+\\sigma(y\\cdot A x)}}\\right)A,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where A is the data matrix and \u03c3(z) =1+ex1p(z) is the sigmoid function. A short calculation shows that the diagonal matrix ", "page_idx": 31}, {"type": "equation", "text": "$$\n{\\mathrm{Diag}}\\left({\\frac{1}{\\sigma(-y\\cdot A x)+2+\\sigma(y\\cdot A x)}}\\right)\\preceq{\\frac{1}{4}}\\mathbf{I},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which is tight when $x=0$ . As a result, $L=\\lambda_{\\operatorname*{max}}(A^{\\top}A)/4$ . We compute this manually. We also compute the optimal value for the logistic regression problem using the SciPy implementation of BFGS (Liu and Nocedal, 1989). We use this value for $f(x^{*})$ to compute the Polyak step-size and when plotting sub-optimality. It turns out that the upper-bound based on $L$ -smoothness for both GD with the Polyak step-size (Hazan and Kakade, 2019) and standard GD (Bubeck et al., 2015) is ", "page_idx": 31}, {"type": "equation", "text": "$$\nf(x_{k})-f(x^{*})\\leq\\frac{2L\\|x_{0}-x^{*}\\|_{2}^{2}}{k}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Figure 3: We run these experiments using vanilla NumPy. As mentioned in the text, we generate a quadratic optimization problem ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\frac{1}{2}\\boldsymbol{x}^{\\top}\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}^{\\top}\\boldsymbol{x},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the eigenvalues of $A$ were generated to follow power law distribution with parameter $\\alpha=3$ .   \nWe scaled the eigenvalues to ensure $L=1000$ . The dimension of the problem we create is $d=300$ .   \nWe repeat the experiment for 20 random trials and plot the mean and standard deviations. ", "page_idx": 31}, {"type": "text", "text": "Figure 4: We pick three different datasets from the UCI repository to showcase the possible convergence behavior of the optimization methods. We compute $L$ and $f(w^{*})$ as described above for Figure 1. For normalized GD, we use the step-size schedule $\\eta_{k}\\,=\\,\\eta_{0}/\\sqrt{k}$ as suggested by our theory. To pick $\\eta_{0}$ , we run a grid search on the grid generated by np.logspace(-8, 1, 20). We implement AdGD from scratch and use a starting step-size of $\\eta_{0}=1\\bar{0}^{-3}$ . We use the same procedure to compute the strongly adapted step-sizes as described above. ", "page_idx": 31}, {"type": "text", "text": "F Computational Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The experiments in Figure 3 were run on a MacBook Pro (16 inch, 2019) with a 2.6 GHz 6-Core Intel i7 CPU and 16GB of memory. All other experiments were run on a Slurm cluster with several different node configurations. Our experiments on the cluster were run with nodes using (i) Nvidia A100 GPUs (80GB or 40GB memory) or Nvidia H100-80GB GPUs with Icelake CPUs, or (ii) Nvidia V100-32GB or V100-16GB GPUs with Skylake CPUs. All jobs were allocated a single GPU and 24GB of RAM. ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All claims in the abstract and introduction are justified with rigorous proofs and supported by experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The limitations of our theoretical results, including necessary assumptions, are addressed in the text. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our theoretical results are accompanied by their necessary assumptions and rigorous proofs are provided in the appendix. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All experimental procedures and hyper-parameter settings are provided in Appendix E. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: We will release the code to reproduce our experiments upon acceptance of the paper. All non-synthetic data used in this paper is open source and freely accessible online. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The details necessary to interpret our experimental results are provided in Section 5, while additional details necessary to reproduce the experiments are given in Appendix E. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our experiments consider only deterministic optimization methods. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We detail the compute resources used in our paper in Appendix F. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: All authors are familiar with the code of ethics and conform to its principles.   \nMoreover, our research is primarily theoretical and is of minor ethical concern. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our research is primarily theoretical and has no societal impact beyond the impact of general advances in the fields of machine learning and optimization. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No new data or models are released as part of this paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All libraries, models, and data sources are appropriately referenced. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: No new assets are introduced. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: We did not use any crowdsourcing for this work ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: IRB approval was not required for this paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}]