{"references": [{"fullname_first_author": "Ahn, Kwangjun", "paper_title": "Understanding the unstable convergence of gradient descent", "publication_date": "2022-07-17", "reason": "This paper provides crucial insights into the unstable convergence behavior of gradient descent, a phenomenon directly relevant to the study of directional smoothness and its impact on optimization."}, {"fullname_first_author": "Altschuler, Jason M.", "paper_title": "Acceleration by Stepsize Hedging I: Multi-Step Descent and the Silver Stepsize Schedule", "publication_date": "2023-09-07", "reason": "This work introduces the concept of stepsize hedging, which offers a novel approach to designing adaptive step sizes that address the challenges posed by non-uniform smoothness, aligning with the central theme of the current research."}, {"fullname_first_author": "Berahas, Albert S.", "paper_title": "Non-Uniform Smoothness for Gradient Descent", "publication_date": "2023-11-08", "reason": "This paper explores non-uniform smoothness, a concept closely related to directional smoothness, providing a theoretical foundation for understanding how local gradient variations impact the convergence of gradient descent methods."}, {"fullname_first_author": "Carmon, Yair", "paper_title": "Making SGD Parameter-Free", "publication_date": "2022-07-02", "reason": "This paper presents a parameter-free version of stochastic gradient descent (SGD) leveraging exponential search, which is directly relevant to adapting step sizes based on directional smoothness, improving upon the practical applicability of adaptive methods."}, {"fullname_first_author": "Dai, Y. H.", "paper_title": "A New Gradient Method with an Optimal Stepsize Property", "publication_date": "2006-01-01", "reason": "This paper introduces a novel gradient method with an optimal stepsize property, which serves as a foundational result for understanding adaptive step size methods and their convergence behavior in relation to the paper's findings."}]}