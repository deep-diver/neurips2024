[{"type": "text", "text": "FasMe: Fast and Sample-efficient Meta Estimator for Precision Matrix Learning in Small Sample Settings ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiao Tan1,3 Yiqin Wang1 Yangyang Shen1 Dian Shen1 Meng Wang2 Peibo Duan3 Beilun Wang1,\\* ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Engineering, Southeast University, Nanjing, China 2College of Design and Innovation, Tongji University, Shanghai, China 3Department of Data Science & AI, Monash University, Melbourne, Australia {xtan, 213200449, seu_syy, dshen, beilun}@seu.edu.cn mengwangtj@tongji.edu.cn peibo.duan@monash.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Precision matrix estimation is a ubiquitous task featuring numerous applications such as rare disease diagnosis and neural connectivity exploration. However, this task becomes challenging in small sample settings, where the number of samples is significantly less than the number of dimensions, leading to unreliable estimates. Previous approaches either fail to perform well in small sample settings or suffer from inefficient estimation processes, even when incorporating meta-learning techniques. To this end, we propose a novel approach FasMe for Fast and Sampleefficient Meta Precision Matrix Learning, which first extracts meta-knowledge through a multi-task learning diagram. Then, meta-knowledge constraints are applied using a maximum determinant matrix completion algorithm for the novel task. As a result, we reduce the sample size requirements to $O(\\log p/K)$ per meta-training task and $O(\\log|\\mathcal{G}|)$ for the meta-testing task. Moreover, the hereby proposed model only needs $O(p\\log\\epsilon^{-1})$ time and $O(p)$ memory for converging to an $\\epsilon$ -accurate solution. On multiple synthetic and biomedical datasets, FasMe is at least ten times faster than the four baselines while promoting prediction accuracy in small sample settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Precision matrix estimation aims to exploit conditional dependency relationships between random variables, which plays a vital role in high-dimensional statistical learning with a wide range of applications in areas such as genetics [1], neuroscience [2], and social networks. For instance, researchers are examining the gene network derived from the gene expression data of patients to investigate a rare disease. However, in cases involving rare diseases, the analysis is hindered by the high dimensionality of genetic datasets, where the number of features significantly exceeds the number of samples. For example, the Cholangiocarcinoma dataset in TCGA contains only 51 samples but thousands of genes. In this context, we define a \"small sample setting\" as a scenario in which the number of samples is less than one-tenth of the number of dimensions. Although inferring such relationship graphs in small sample settings is challenging, it can provide valuable insights into the genetic characteristics underlying this disease and its etiology. ", "page_idx": 0}, {"type": "text", "text": "Precision matrix estimator inherently has the capability to handle datasets where the number of dimensions $p$ is equal to or greater than the number of samples $n$ [3]. However, this capability has its limits, and maintaining effectiveness requires keeping the sample size at a certain level. [4] provides the lower bound of the sample size as $\\Omega(d^{2}\\log{p})$ for well-performed graph recovery under the sub-Gaussian assumption, where $d$ is the maximum node degree of the graph. Consequently, when the sample size is reduced to the aforementioned \"small sample\" levels, the sample size falls short of this theoretical lower bound with a relatively high probability due to the uncertainty of $d$ . This indicates insufficient data support for most methods, like GLasso [5] (detailed in Background), to recover the graph structure effectively, as shown in Figure 1. To address this issue, some methods use multi-task learning, integrating heterogeneous datasets to reduce the sample size bound. For example, the regularized methods [6] and [7] show that each task only needs $\\bar{O(\\log K+\\log p)}$ and $O(K^{\\stackrel{*}{}}+\\log p)$ samples, respectively, for proper estimation, and $K$ denotes the number of tasks. Although these methods are more capable of handling small sample problems, they concurrently introduce significant computational burdens. Specifically, integrating a new task necessitates repeated joint retraining, thereby resulting in substantial computational inefficiencies. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address these challenges, meta-learning emerges as a promising paradigm, as it equips models to efficiently learn a new task using minimal data through the application of meta-knowledge. Such methods are particularly attractive for precision matrix estimation in small sample settings, as they diminish the reliance on extensive data and reduce computational demands. For example, state-of-the-art approaches from the class of ModelAgnostic Meta-Learning (MAML) [8] have achieved favorable performance in many meta-learning tasks. The strength of MAML lies in its ability to quickly adapt to new tasks (metatesting dataset) with fewer samples by optimizing initial parameters using data from related tasks (meta-training dataset). ", "page_idx": 1}, {"type": "text", "text": "However, the design of MAML often emphasizes sample efficiency and fast adaptation, typically without a comprehensive and robust theoretical guarantee for its performance. While empirical improvements are sufficient for many neural network applications, they may lead to incorrect predictions when applied to precision matrix estimation, where precise and reliable results are crucial. Such empirical-only strategies typically rely on extensive training data. However, in precision matrix estimation, the available data is often significantly limited. For example, the dropout technique [9], which lacks a strict theoretical foundation, is frequently used in neural network training with millions of samples. In contrast, in precision matrix estimation, the volume of available data often falls short of the number of feature dimensions. Furthermore, MAML-based methods adapt to new tasks by performing gradient descent from meta-learned parameters, but as noted by [10], the computation and memory demands escalate dramatically as the number of features $p$ increases, presenting significant challenges. ", "page_idx": 1}, {"type": "image", "img_path": "wHFaAH3E8z/tmp/2efb241bf3fc04a33c4074fe0a71ab083c2a840690710ebdaab1446068af9121.jpg", "img_caption": ["Figure 1: Frobenius norm error $\\left(\\downarrow\\right)$ and Matthews Correlation Coefficient (MCC, $\\uparrow$ ) V.S. sample size using GLasso. We fix $p=200$ and vary $n$ from 10 to 400 with step 10. The model performance drops sharply around $n=20$ , indicating its inadequacy for small sample. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recent studies [11, 12] have made initial attempts to successfully integrate meta-learning with precision matrix estimation. Despite achieving sample-efficiency through meta-learning, their methods exhibit several limitations: Restrictive Data Assumptions, and Ineffective Adaption to New Tasks. Firstly, they assume that the edges of a new task must be subsets of existing edges (where existing edges can be seen as optimized initial parameters in MAML), thereby constraining the discovery of novel connections in real-world applications. Besides, these works state that they use the state-of-theart algorithm BigQuic, which speeds up the gradient calculation step with a block-coordinate descent Newton method, to solve a new task. The algorithm uses between $O(p)$ and $O(p^{3})$ time and $O(p^{2})$ and $O(p)$ memory per iteration. When the number of samples is extremely small, it takes a large number of iterations to converge to an accurate solution, leading to poor generalization performance. ", "page_idx": 1}, {"type": "text", "text": "Herein, we propose a novel approach for Fast and Sample-efficient Meta Precision Matrix Learning, FasMe. Our approach first extracts meta-knowledge as the shared pattern across auxiliary tasks through a multi-task learning diagram. We then adopt a maximum determinant matrix completion algorithm with meta-knowledge constraints to achieve fast adaptation in high-dimensional settings. In detail, our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 A novel meta-learning-based model: We propose a multi-task precision matrix estimator that extracts the meta-knowledge from the auxiliary tasks. We then utilize the maximum determinant matrix completion algorithm to learn a new precision matrix in the meta-testing. \u2022 Efficient adaptation: We design a recursive closed-form algorithm to speed up the adaptation in the novel task. Our model converges to an $\\epsilon$ -accurate solution in $\\bar{O}(p\\log\\epsilon^{-1})$ time and $O(p)$ memory. It significantly outperforms the state-of-the-art ones. \u2022 Theoretical guarantee: We also theoretically prove that for $p$ -dimensional multivariate sub-Gaussian random vectors and $K$ auxiliary tasks with meta-knowledge $\\theta$ , the sample complexity of our method is $O(\\log p/K)$ per auxiliary task for meta-knowledge and ${\\bar{O}}(\\log|{\\bar{\\mathcal{G}}}|)$ for the new task. It relaxes sample size requirements with the application of meta-learning, equipping FasMe with the ability to handle small sample settings. \u2022 Experimental evaluation: On multiple synthetic datasets, FasMe outperforms the other baselines. Specifically, FasMe is at least ten times faster than the four baselines while promoting prediction accuracy. In the real-world experiment, FasMe obtains the minimum log-determinant Bregman divergence and performs better than the other baselines. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Meta-Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The last years have witnessed a tremendous interest in methods for meta-learning, especially in a wide range of data-limited applications including medical image analysis [13, 14], language modeling [15, 16], and object detection [17, 18, 19]. Meta-learning is a machine learning technique that allows a model to learn from prior knowledge, or meta-knowledge, to improve its performance on new tasks and speed up the learning process [20, 21]. The process typically involves two stages: 1) Meta-training: In this stage, the model is trained on a set of related tasks, also known as auxiliary tasks [8]. The goal of this step is to extract meta-knowledge from these tasks, which can be used to improve the performance of the model on new tasks. This step helps the model to learn how to quickly and effectively adapt to new tasks by identifying the shared patterns or features across the auxiliary tasks [22]. 2) Meta-testing: In this stage, the model is tested on a new task, also known as the target task [23]. The goal of this step is to apply the meta-knowledge learned during the meta-training step to improve the model\u2019s performance on the target task. Meta-knowledge is used to quickly and effectively adapt the model to the new task, without the need for additional training data. Overall, meta-learning allows a model to better generalize to new tasks by leveraging prior knowledge, thereby making it more sample-efficient and fast. ", "page_idx": 2}, {"type": "text", "text": "2.2 Precision Matrix Estimation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Precision matrix estimation is the problem of finding the inverse of the covariance matrix of a multivariate random variable $\\Omega=\\mathbf{\\bar{\\Sigma}}\\Sigma^{-1}$ . It is particularly useful in scenarios where the number of samples is equal to or smaller than the number of variables since the covariance matrix is not invertible. The graphical lasso, namely GLasso [5], is a typical penalized maximum likelihood estimator for precision matrix $\\Omega$ inference under the Gaussian assumption on the dataset $\\mathbf{X}\\in\\mathbb{R}^{n\\times p}$ The popular estimator can be written as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\Omega}=\\mathop{\\arg\\operatorname*{min}}_{\\Omega\\succ0}~-\\log\\operatorname*{det}(\\Omega)+<\\Omega,\\Sigma>+\\lambda\\|\\Omega\\|_{1},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\lambda\\geq0$ , $\\begin{array}{r}{\\Sigma=\\frac{1}{n}\\sum_{i=1}^{n}(\\mathbf{X}_{i}-\\bar{\\mathbf{X}})(\\mathbf{X}_{i}-\\bar{\\mathbf{X}})^{\\top}}\\end{array}$ , $\\begin{array}{r}{\\bar{\\mathbf{X}}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{X}_{i}}\\end{array}$ , and $\\Omega\\succ0$ denotes that $\\Omega$ is positive definite and symmetric. $\\lVert\\boldsymbol{\\Omega}\\rVert_{1}$ is the $\\ell_{1}$ -norm of matrix $\\Omega$ . However, [24] proved that this estimator is sensible even for non-Gaussian $\\mathbf{X}$ , since it corresponds to minimizing an $\\ell_{1}$ -regularized log-determinant Bregman divergence. To solve the problem Eq. (1), we take the derivative of it $\\Omega^{\\bar{-}1}-\\Sigma=\\lambda\\partial\\|\\Omega\\|_{1}\\bar{/}\\partial\\Omega$ . Inspired by this derivative, [25] proposed the constrained $\\ell_{1}$ minimization method for inverse matrix estimation (CLIME) estimator. This estimator can be used to estimate the precision matrix $\\Omega$ via an $\\ell_{1}$ constrained optimization: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{\\Omega\\succ0}{\\arg\\operatorname*{min}}\\|\\Omega\\|_{1},\\quad\\mathrm{s.t.}\\ \\|\\Sigma\\Omega-I\\|_{\\infty}\\leq\\lambda.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "CLIME can be solved column-by-column. Compared with GLasso, CLIME has presented more favorable theoretical properties and can be solved by column-wise linear programming. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The section elaborates on how the proposed meta-learning framework works. We begin by providing a formal problem setting in Section 3.1, which covers the task and data assumptions we consider. Next, we formulate the two stages of our meta-learning framework in Section 3.2. ", "page_idx": 3}, {"type": "text", "text": "3.1 Problem Setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notations $\\{\\mathbf{X}^{(k)}\\}=\\left\\{\\mathbf{X}^{(1)},\\mathbf{X}^{(2)},\\ldots,\\mathbf{X}^{(K)}\\right\\}$ denotes $K$ datasets generated by $K$ auxiliary tasks. We use $\\mathbf{X}^{(k)}\\in\\mathbb{R}^{n_{k}\\times p}$ to represent the $k$ -th dataset with $n_{k}$ samples and $p$ features. $\\Omega^{(k)}\\in\\mathbb{R}^{p\\times p}$ represents the $k$ -th precision matrix corresponding to $\\mathbf{X}^{(k)}$ . $\\{\\Omega^{(k)}\\}=\\left\\{\\Omega^{(1)},\\ldots,\\Omega^{(K)}\\right\\}$ is the set of the precision matrices corresponding to the auxiliary data $\\{\\mathbf{X}^{(k)}\\}$ . $\\Omega_{\\mathrm{new}}$ denotes the precision matrix of a new task $\\mathbf{X}_{\\mathrm{new}}\\in\\mathbb{R}^{n\\times\\bar{p}}$ to be estimated. We use $\\theta$ to represent the meta-knowledge (i.e., the common substructure of $\\{\\Omega^{(k)}\\})$ and $\\theta_{r}^{(k)}$ to represent the rest of $\\Omega^{(k)}$ respectively. ", "page_idx": 3}, {"type": "text", "text": "Assumptions We consider a more general class of distributions than most previous studies, i.e., sub-Gaussian distributions, which cover Gaussian variables, bounded random variables, and so on. This relaxed data assumption significantly improves the flexibility and robustness of our method. We assume that the auxiliary tasks share some meta-knowledge $\\theta$ with the target task $\\mathbf{X}_{\\mathrm{new}}$ . Specifically, we assume that $\\mathrm{supp}(\\r\\r)\\subseteq\\mathrm{supp}(\\Omega_{\\mathrm{new}})$ . Here the meta-knowledge $\\theta$ takes the form of a common substructure among the tasks. The assumption has been widely adopted [26, 27, 28] and proved to be feasible and applicable in the biological and genetic domains [29]. ", "page_idx": 3}, {"type": "text", "text": "The paper aims to estimate the target precision matrix $\\Omega_{\\mathrm{new}}$ of sub-Gaussian distribution with sparse structure and relatively limited samples given sufficient samples from auxiliary tasks $\\{\\mathbf{X}^{(k)}\\}$ . The auxiliary datasets follow a family of multivariate sub-Gaussian distributions (see Definition 2 in Appendix F). To address the problem, we propose a novel meta-learning framework that leverages the meta-knowledge learned from auxiliary tasks to efficiently estimate $\\Omega_{\\mathrm{new}}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 A Meta-learning Framework for Small Sample Precision Matrix Estimator ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This part gives more details regarding the meta-learning framework we established. Figure 2 illustrates the pipeline of the framework in an intuitive way. ", "page_idx": 3}, {"type": "image", "img_path": "wHFaAH3E8z/tmp/93c985a422dae84d8aadd941ba46f95bdae82113c8a5ab19f0df4f15f53828a0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: The pipeline of the established framework. The entire pipeline can be divided into two stages: meta-knowledge extraction (meta-training) and efficient adaptation (meta-testing). In Stage 1, our meta-teacher extracts the meta-knowledge, namely the shared structure, from the related auxiliary tasks with sufficient samples. In Stage 2, we obtain a prior sparsity pattern from the meta-knowledge and target dataset with a few samples. Then our meta-student aims to recover the edge structure by solving a matrix completion problem rapidly. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Meta-teacher ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We extract the common substructure across the tasks as the meta-knowledge. The goal of pre-training is to extract meta-knowledge $\\theta$ from auxiliary tasks $\\{\\mathbf{X}^{(k)}\\}$ . ", "page_idx": 4}, {"type": "text", "text": "To obtain the meta-knowledge, every precision matrix is modeled as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Omega^{(k)}=\\theta+\\theta_{r}^{(k)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta\\in\\mathbb{R}^{p\\times p}$ is the common substructure among all graphs and $\\theta_{r}^{(k)}\\in\\mathbb{R}^{p\\times p}$ represents the rest for $k$ -th graph. ", "page_idx": 4}, {"type": "text", "text": "We adopt $\\ell_{1}$ -penalization for $\\theta$ and $\\theta_{r}^{(k)}$ since they are expected to be sparse for better interpretability. Therefore, we write Eq. (3) into $\\lVert\\theta\\rVert_{1}+\\rho\\lVert{\\theta}_{r}^{(k)}\\rVert_{1}$ , where $\\rho>0$ . The value of the hyper-parameter $\\rho$ depends on the properties of the graphs. This hyper-parameter controls the difference of sparsity level between $\\theta$ and $\\bar{\\theta}_{r}^{(k)}$ . Concretely, with a smaller $\\rho$ , the shared part gets denser and the rest gets sparser. ", "page_idx": 4}, {"type": "text", "text": "Then we apply the formulation to Eq. (2), thus obtaining the single-task recovering method: ", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\hat{\\theta},\\hat{\\theta}_{r}^{(k)})=\\operatorname*{arg\\,min}_{\\theta,\\theta_{r}}\\quad\\|\\theta\\|_{1}+\\rho\\|\\theta_{r}^{(k)}\\|_{1},\\quad\\quad\\mathrm{s.t.}\\quad\\|\\theta\\Sigma^{(k)}-(I_{p}-\\theta_{r}^{(k)}\\Sigma^{(k)})\\|_{\\infty}\\le\\lambda.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here $\\Sigma^{(k)}$ represents the covariance matrix corresponding to $\\mathbf{X}^{(k)}$ . This single-task method, however, ignores the inner relationship between the auxiliary tasks. To this end, we sum up all the single-task estimators and devise the optimization problem in the following form: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left(\\hat{\\theta},\\{\\hat{\\theta}_{r}^{(k)}\\}\\right)=\\arg\\operatorname*{min}_{\\theta,\\{\\theta_{r}^{(k)}\\}}K\\|\\theta\\|_{1}+\\rho\\sum_{k=1}^{K}\\|\\theta_{r}^{(k)}\\|_{1},\\qquad\\mathrm{s.t.~}\\|\\theta\\Sigma^{(k)}-(I_{p}-\\theta_{r}^{(k)}\\Sigma^{(k)})\\|_{\\infty}\\le\\lambda,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $k=1,2,\\ldots,K$ . ", "page_idx": 4}, {"type": "text", "text": "3.2.2 Meta-student ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To speed up the estimation of $\\hat{\\Omega}_{\\mathrm{new}}$ , we propose our efficient estimator, which is favorable even with a few samples of the new task available. Additionally, we obtain a sparsity pattern from the meta-knowledge $\\theta$ and target dataset with a few samples. ", "page_idx": 4}, {"type": "text", "text": "Different from most $\\ell_{1}$ -penalized methods, our estimator aims to estimate the precision matrix by solving a Maximum Determinant Matrix Completion (MDMC) problem. MDMC is an effective technique that aims to pick the unique maximum determinant completion (when it exists) for a partially observed matrix from all the possible matrices [30]. ", "page_idx": 4}, {"type": "text", "text": "Consider a maximum determinant matrix completion problem for a covariance matrix with some partially observed entries: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\Sigma}=\\operatorname*{arg\\,max}_{\\Sigma\\geq0}\\,\\log\\operatorname*{det}\\Sigma\\qquad\\mathrm{s.t.~}\\Sigma_{i j}=M_{i j},\\;\\forall(i,j)\\in\\mathcal{G},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $M$ represents the partially observed matrix, and the set $\\mathcal{G}$ contains all the observed entries. Thus, the Lagrangian dual of this problem can be derived as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\Omega}=\\underset{\\Omega\\succeq0}{\\arg\\operatorname*{min}}\\ -\\log\\operatorname*{det}\\Omega+\\langle\\Omega,M\\rangle+p\\qquad\\mathrm{s.t.}\\ \\Omega\\in\\mathbb{S}_{\\mathcal{G}}^{p},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with first-order optimality condition $|\\mathrm{sign}(M)|\\odot\\hat{\\Omega}^{-1}=M$ , where the operator $\\mathrm{{sign}(\\cdot)}$ computes the sign of the matrix and $\\odot$ denotes the Hadamard product. Herein, the set $\\mathbb{S}_{\\mathcal{G}}^{p}$ refers to the $p\\times p$ symmetric matrices with sparsity pattern $\\mathcal{G}$ . Strong duality indicates a straightforward relation back to the primal $\\hat{\\Sigma}=\\hat{\\Omega}^{-1}$ . Note that while $\\hat{\\Sigma}$ is (in general) a dense matrix, $\\hat{\\Omega}$ is always sparse. Instead of solving the primal problem for a dense matrix, we choose to solve the dual problem for a sparse matrix, which also satisfies the optimality condition. ", "page_idx": 4}, {"type": "text", "text": "To solve the precision matrix $\\hat{\\Omega}_{\\mathrm{new}}$ , the first step is to find a reliable sparsity pattern. Previous works [31] obtain the pattern by thresholding the covariance matrix. However, [31] also proved the invalidity of such a method in the case of a relatively small sample size, as the covariance matrix cannot be accurately estimated. To meet the challenge, our estimator combines the support set of the thresholded covariance matrix and meta-knowledge to obtain the sparsity pattern $\\mathcal{G}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{G}=\\mathrm{supp}(\\theta)\\cup\\mathrm{supp}(S_{\\eta}(\\Sigma_{\\mathrm{new}})).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $S_{\\eta}$ denotes the element-wise soft-thresholding operator with parameter $\\eta$ . Note that the problem (7) has a recursive closed-form solution whenever the graph is chordal, the embedding operation is conducted to satisfy the chordal property. To be specific, the chordal embedding of $\\mathcal{G}$ is represented as $\\tilde{\\mathcal{G}}$ and (7) is rewritten into the following formulation,: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\Omega}_{\\mathrm{new}}=\\underset{\\Omega>0}{\\mathrm{arg\\,min}}-\\log\\operatorname*{det}\\Omega+\\left\\langle\\Omega,\\Pi_{\\mathcal{G}}\\left(\\Sigma_{\\mathrm{new}}\\right)\\right\\rangle,\\qquad\\mathrm{s.t.~}\\Omega_{i j}=0,\\ (i,j)\\in\\tilde{\\mathcal{G}}\\backslash\\mathcal{G}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Pi_{\\mathcal{G}}$ is the projection operator from $\\mathbb{S}^{p}$ onto $\\mathbb{S}_{E}^{p}$ , i.e., by setting $\\Pi_{\\mathcal{G}}(A_{i j})=0$ if $(i,j)\\notin E$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 Optimization Algorithms ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The learning steps of meta-knowledge extraction (5) is solved efficiently through a formulation of multiple independent sub-problems of linear programming, which can be accelerated in a parallel form. With the assistance of meta-knowledge, we solve the optimization problem (9) using Newton Conjugate Gradient method. The key idea is to construct an inner conjugate gradients loop as a solution to the Newton subproblem of an outer Newton\u2019s method. The detailed description, complexity analysis, and pseudo-code (Algorithm 1) of the full algorithm are shown in Appendix $\\mathbf{C}$ . ", "page_idx": 5}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To present the analysis more concisely, we first define $\\Sigma_{\\mathrm{tot}}:=\\mathrm{diag}(\\Sigma^{(1)},\\dots,\\Sigma^{(K)})=(\\sigma_{i j})_{K p\\times K p}$ , $\\Theta\\;:=\\;\\mathrm{diag}(\\theta,\\ldots,\\theta),$ , $\\Theta_{r}\\;:=\\;\\mathrm{diag}(\\theta_{r}^{(1)},\\cdot\\cdot\\cdot,\\theta_{r}^{(K)})$ , $\\Omega_{\\mathrm{tot}}\\;:=\\;\\mathrm{diag}(\\Omega^{(1)},\\ldots,\\Omega^{(K)})\\;=\\;\\Theta+\\Theta_{r}$ , $\\mathbf{X}_{\\mathrm{tot}}:=\\mathrm{diag}(\\mathbf{X}^{(1)},\\ldots,\\mathbf{X}^{(K)})$ . We assume that the precision matrix $\\Omega$ belongs to the uniformity class of matrices, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{U}:=\\mathcal{U}\\left(q,s(K p)\\right)=\\Big\\{\\Omega:\\Omega\\succ0,\\|\\Omega\\|_{1}\\leq\\nu,\\|\\Omega\\|_{\\infty}\\leq\\phi,\\operatorname*{max}_{1\\leq i\\leq K p}\\sum_{j=1}^{K p}|\\omega_{i j}|^{q}\\leq s(K p)\\Big\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $q,\\nu,\\phi$ are some constants, $0\\leq q<1$ and $\\Omega:=(w_{i j})\\kappa_{p\\times K p}$ . $s(K p)$ represents the sparsity level of $\\Omega$ in the uniformity class. Note that the sparsity level is related to $p$ without an analytic form of relationships between them. ", "page_idx": 5}, {"type": "text", "text": "Then some important conditions and definitions are stated as the following. ", "page_idx": 5}, {"type": "text", "text": "Exponential Tail Condition Suppose there exists a constant $0\\leq\\gamma\\leq\\frac{1}{4}$ , so that $\\begin{array}{r}{\\frac{\\log p}{n_{k}}\\leq\\gamma}\\end{array}$ and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\exp(t(X_{i}-\\mu_{i})^{2})]\\leq C\\leq\\infty,\\forall|t|\\leq\\gamma,\\forall i\\in\\{1,\\ldots,p\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C$ is a constant. ", "page_idx": 5}, {"type": "text", "text": "Irrepresentable Condition [4] There exists some $\\alpha\\in(0,1]$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\||\\Gamma_{\\mathcal{G}^{c}\\mathcal{G}}^{*}(\\Gamma_{\\mathcal{G}\\mathcal{G}}^{*})^{-1}|\\|_{1}\\leq1-\\alpha,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Gamma^{\\ast}:=\\Omega^{\\ast}{}^{-1}\\otimes\\Omega^{\\ast}{}^{-1}$ ( $\\otimes$ represents the Kronecker matrix product) and $\\mathcal{G}^{c}=\\{1,\\dots,p\\}^{2}\\backslash\\mathcal{G}$ ", "page_idx": 5}, {"type": "text", "text": "We define $\\kappa_{\\Gamma^{*}}=\\||(\\Gamma_{\\mathcal{G}\\mathcal{G}}^{*})^{-1}\\||_{\\infty}.$ . Note that $\\Gamma_{\\mathcal{G G}}^{*}$ is a $(s+p)\\times(s+p)$ matrix indexed by vertex pairs, where $s=|\\mathcal{G}|$ . W e  define $\\omega_{\\mathrm{min}}^{\\mathrm{new}}=\\operatorname*{min}_{(i,j)\\in\\mathrm{supp}(\\Omega_{\\mathrm{new}}^{*})}|\\Omega_{\\mathrm{new},i j}^{*}|$ and $\\omega_{\\mathrm{min}}^{\\mathrm{tot}}=\\operatorname*{min}_{(i,j)\\in\\mathrm{supp}(\\Omega_{\\mathrm{tot}}^{*})}|\\Omega_{\\mathrm{tot},i j}^{*}|$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 1. $[32]$ Given a matrix $M\\in\\mathbb{S}^{p}$ , define $\\mathcal{G}_{M}=\\{(i,j):M_{i j}\\neq0\\}$ as its sparsity pattern. Then $M$ is called inverse-consistent if there exists a matrix $N\\in\\mathbb{S}^{p}$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\nM+N\\succeq0,\\quad\\forall(i,j)\\in\\mathcal{G}_{M}\\quad N_{i j}=0,\\quad(M+N)^{-1}\\in\\mathbb{S}_{\\mathcal{G}_{M}}^{p}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The matrix $N$ is called an inverse-consistent complement of $M$ and is denoted by $M^{(c)}$ . Furthermore, $M$ is called sign-consistent if for every $(i,j)\\in\\mathcal{G}_{M}$ , the $(i,j)$ -th elements of $M$ and $(M+M^{(c)})^{-1}$ have opposite signs. ", "page_idx": 5}, {"type": "text", "text": "Then we define the $\\beta(\\mathcal{G},\\alpha)$ function with respect to the sparsity pattern $\\mathcal{G}$ and scalar $\\alpha>0$ ", "page_idx": 6}, {"type": "text", "text": "$\\begin{array}{r l r}{\\beta(\\mathcal{G},\\alpha)=\\underset{M\\sim0}{\\operatorname*{max}}}&{{}\\|M^{(c)}\\|_{\\operatorname*{max}}}&{\\mathrm{s.t.}\\;M\\in\\mathbb{S}_{\\mathcal{G}}^{n}\\mathrm{~and~}\\|M\\|_{\\operatorname*{max}}\\leq\\alpha,M_{i,i}=1,M\\mathrm{~i~n~t~}}\\end{array}$ s inverse-consistent.   \nHere $i=1,2,\\ldots,n,\\,\\Vert\\cdot\\Vert_{\\mathrm{max}}$ denotes the $\\ell_{\\mathrm{max}}$ -norm, e.g., $\\|A\\|_{\\operatorname*{max}}=\\operatorname*{max}_{i\\neq j}|A_{i j}|$ . ", "page_idx": 6}, {"type": "text", "text": "The Appendix F provides all the detailed proofs of the lemmas, theorems, and corollaries. ", "page_idx": 6}, {"type": "text", "text": "4.1 Main Theorems ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we mainly study the theoretical properties of the meta-knowledge $\\hat{\\theta}$ in (5) and the precision matrix $\\hat{\\Omega}_{\\mathrm{new}}$ in (7). The first theorem specifies a probability lower bound of recovering the true meta-knowledge by our estimator in (5) for multiple random multivariate sub-Gaussian distributions. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Suppose that $\\Omega_{\\mathrm{tot}}^{*}\\in\\mathcal{U}$ and $\\begin{array}{r}{N=\\sum_{k=1}^{K}n_{k}}\\end{array}$ . When the variables are sub-Gaussian, the tail condition holds. Let \u03bb = C0\u03bd log(NK , where the constant $C_{0}=2\\gamma^{-2}(2+\\tau_{0}+\\gamma^{-1}e^{2}C^{2})^{2}$ and constant $\\tau_{0}>0$ . If $\\omega_{\\mathrm{min}}^{\\mathrm{tot}}>4\\tau_{n}$ , then we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I.\\ \\ \\|\\hat{\\theta}-\\theta^{*}\\|_{\\infty}\\leq8C_{0}\\nu^{2}\\sqrt{\\frac{\\log(K p)}{N}}+2\\phi;}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with a probability greater than $1-4p^{-\\tau_{0}}$ . Here $\\tilde{\\theta}$ is a threshold estimator with $\\tilde{\\theta}_{i j}=\\hat{\\theta}_{i j}I(|\\hat{\\theta}_{i j}|\\geq\\tau_{n})$ , where $\\tau_{n}\\geq2\\nu\\lambda$ is a tuning parameter. ", "page_idx": 6}, {"type": "text", "text": "sAucfcfoicrideinnt gf otro  tTheh eroerceomve r1y,  iof $n_{1}=n_{2}=\\cdots=n_{k}$ ,  Ian  sammosptl ec acsoesm, $O(\\frac{\\log K\\!+\\!\\log p}{K})$ assikn cies $\\begin{array}{r}{O(\\frac{\\log K+\\log p}{K})\\to O(\\frac{\\log p}{K})}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "The following theorem specifies a probability lower bound of recovering a correct precision matrix $\\hat{\\Omega}_{\\mathrm{new}}$ by our estimator in (7) for a multivariate sub-Gaussian distribution. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Suppose we have recovered the true sparsity pattern $\\mathcal{G}$ of a family of $p$ -dimensional random multivariate sub-Gaussian distribution. Then for a new task of multivariate sub-Gaussian distribution with the precision matrix $\\Omega_{\\mathrm{new}}^{\\ast}$ such that $\\mathrm{supp}(\\theta)\\subseteq\\mathrm{supp}(\\Omega_{\\mathrm{new}}^{*})$ and satisfying irrepresentable condition, consider the estimator $\\hat{\\Omega}_{\\mathrm{new}}$ with $\\begin{array}{r}{\\lambda=C_{3}\\nu\\sqrt{\\frac{\\log|\\mathcal{G}|}{n}}.}\\end{array}$ logn |G|. Then we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\hat{\\Omega}_{\\mathrm{new}}-\\Omega_{\\mathrm{new}}^{*}\\|_{\\infty}\\leq2C_{3}\\nu^{2}\\kappa_{\\Gamma^{*}}\\sqrt{\\frac{\\log|\\mathcal{G}|}{n}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with a probability $1-p^{-\\tau_{1}}$ , where $\\tau_{1}>0$ . ", "page_idx": 6}, {"type": "text", "text": "This theorem shows that $n\\in O(\\log|\\mathcal{G}|)$ is sufficient for estimating a correct precision matrix of the new task with our estimation. Then, efforts should be made to prove that the max-det matrix completion method guarantees the consistency of the support set of \u2126\u02c6new with $\\mathcal{G}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. $\\mathcal{G}$ coincides with the sparsity pattern of the optimal solution $\\Omega_{\\mathrm{new}}^{\\ast}$ if the normalized matrix $\\tilde{\\Sigma}_{\\mathrm{new}}=D^{-1/2}\\Pi_{\\mathcal{G}}(\\Sigma_{\\mathrm{new}})D^{-1/2}$ where $D=\\mathrm{diag}(\\Pi_{\\mathcal{G}}(\\Sigma_{\\mathrm{new}}))$ satisfies the following conditions: ", "page_idx": 6}, {"type": "text", "text": "1. $\\tilde{\\Sigma}_{\\mathrm{new}}$ is positive definite and sign-consistent, ", "page_idx": 6}, {"type": "text", "text": "2. We have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\beta\\left(\\mathcal{G},\\|\\tilde{\\Sigma}_{\\mathrm{new}}\\|_{\\mathrm{max}}\\right)\\leq\\operatorname*{min}_{(k,l)\\notin\\mathcal{G}}\\frac{\\eta-\\left|\\left(\\Pi_{\\mathcal{G}}\\left(\\Sigma_{\\mathrm{new}}\\right)\\right)_{k l}\\right|}{\\sqrt{\\left(\\Pi_{\\mathcal{G}}\\left(\\Sigma_{\\mathrm{new}}\\right)\\right)_{k k}\\cdot\\left(\\Pi_{\\mathcal{G}}\\left(\\Sigma_{\\mathrm{new}}\\right)\\right)_{l l}}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We set the diagonal elements of $\\tilde{\\Sigma}_{\\mathrm{new}}$ to 1 and its off-diagonal elements between $^-1$ and 1. The projection operation $\\Pi_{\\mathcal{G}}(\\cdot)$ leads to many zero elements in $\\tilde{\\Sigma}_{\\mathrm{new}}$ , resulting in $\\tilde{\\Sigma}_{\\mathrm{new}}$ being positive definite or even diagonally dominant in most cases. As shown by [32], when $\\mathcal{G}$ induces an acyclic structure, condition 2 is automatically satisfied by condition 1. More generally, [33] shows that $\\tilde{\\tilde{\\Sigma}}_{\\mathrm{new}}$ is sign-consistent if $(\\tilde{\\Sigma}_{\\mathrm{new}}+\\tilde{\\Sigma}_{\\mathrm{new}}^{(c)})^{-1}$ is close to its first order Taylor expansion. This assumption holds in practice due to the fact that the magnitude of the off-diagonal elements of $\\tilde{\\Sigma}_{\\mathrm{new}}+\\tilde{\\Sigma}_{\\mathrm{new}}^{(c)}$ is small. Moreover, [32] shows that the left side of (50) is upper bounded by $a\\cdot\\Vert\\tilde{\\Sigma}_{\\mathrm{new}}\\Vert_{\\mathrm{max}}^{2}$ for some $a>0$ . That implies that when $\\lVert\\tilde{\\Sigma}_{\\mathrm{new}}\\rVert_{\\mathrm{max}}$ is small, or equivalently $\\eta$ is large, condition 3 is automatically satisfied. If the conditions in Theorem 3 are satisfied, the support set of $\\hat{\\Omega}_{\\mathrm{new}}$ is consistent with $\\mathcal{G}$ according to (7). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "There have been several methods proposed to estimate precision matrices, among which, three stateof-the-art methods, QUIC [10], Neighborhood Selection [34], Meta-IE [11] and gRankLasso [35] are chosen as baselines in the experiments (See reasons in Appendix E). These approaches all consider high-dimensional settings which are similar to our work. QUIC: This approach utilizes a quadratic approximation of the negative log-likelihood function to estimate sparse inverse covariance matrices to accelerate computation speed, which reduces the computational cost from $O(p^{2})$ to $O(p)$ at each iteration. Neighborhood Selection: This method is a type of regularized maximum likelihood estimation method for estimating precision matrices. Given some i.i.d. observations, it estimates the conditional independence restrictions separately for each node in the graph. The complexity of the neighborhood selection for one node with the Lasso is $O(n p\\operatorname*{min}\\{\\bar{n_{}},p\\bar{\\})}$ using LARS algorithm [36]. Meta-IE (BigQuic): [11] first proposes a meta-learning-based method that is composed of two steps. First, Meta-IE takes the support union of all the precision matrices of the auxiliary tasks as prior knowledge. Then the method states that the testing step can be solved by an interior point method [37] in polynomial time $O(p^{3.5}\\log\\epsilon^{-1})$ or BigQuic [38] with a better time complexity around $\\bar{O(p^{2}\\log\\epsilon^{-1})}$ (See analysis in Appendix). gRankLasso: gRankLasso [35] is a method for sparse precision matrix estimation that realizes automatic parameter tuning, making it completely tuning-free. It achieves robustness and accuracy by integrating a group penalty with Lasso regularization, effectively estimating sparse structures in high-dimensional data. ", "page_idx": 7}, {"type": "text", "text": "6 Experiment ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Synthetic Experiment ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Baselines: We compare our method with the following baselines mentioned in Section 5: 1) The QUIC baseline, 2) the Neighborhood Selection (NS) baseline, 3) Meta-IE (BigQuic) baseline that is accelerated with 32 threads and 4) gRankLasso baseline. ", "page_idx": 7}, {"type": "text", "text": "Metric: We use the Frobenuis-norm $\\lVert\\hat{\\Omega}_{\\mathrm{new}}-\\Omega_{\\mathrm{new}}^{*}\\rVert_{F}$ and Mathews correlation coefficient (MCC) as metrics to evaluate the performance. MCC is widely used in machine learning as a measure of binary classifiers, defined as $\\mathbf{MCC}=(\\mathbf{TP}\\times\\mathbf{TN}-\\mathbf{FP}\\times\\mathbf{FN})/\\sqrt{(\\mathbf{TP}+\\mathbf{FP})(\\mathbf{TP}+\\mathbf{FN})(\\mathbf{TN}+\\mathbf{FP})(\\mathbf{TN}+\\mathbf{FN})}$ . Here the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) values indicate the number of true non-zero entries, true-zero entries, false non-zero entries, and false zero entries, respectively. It produces a high score if the classifier generates desirable estimations. ", "page_idx": 7}, {"type": "text", "text": "Simulated Datasets: Each graph model contains a new task and $K=4$ prior learning tasks. The sample matrices follow multivariate normal distributions where the corresponding precision matrices are generated using two sparse models: (1) Random graph and (2) Tree graph. More details about the data simulation are moved into Appendix B. ", "page_idx": 7}, {"type": "text", "text": "Time Comparison The time-complexity experiment on simulated datasets is conducted to observe the changes of all the estimators in computation time with the varying $p$ and $n$ . As shown in Fig. 3 and Table 1 &2, FasMe performs better than all the baselines. Using our method, we solve a sparse inverse covariance estimation problem containing 2500 variables, with time cost around 1 second. By comparing all the subfigures in Fig. 3, we find that all the methods except the neighborhood selection approach are insensitive to the change of $n$ . This is because the neighborhood selection deals with the sample matrix $\\mathbf{X}_{\\mathrm{new}}\\in\\mathbb{R}^{n\\times p}$ directly instead of the covariance matrix $\\Sigma_{\\mathrm{new}}\\in\\mathbb{R}^{p\\times p}$ . However, it remains computationally costly compared to FasMe when the sample size is much smaller than the number of features. Interestingly, despite Meta-IE (BigQuic) method can speed up by parallelizing multiple threads, it is less time-efficient because of the time assumption of the I/O reads and writes. Moreover, BigQuic cannot converge within the maximum iterations (maxiter $=100$ ), especially when only a few samples are provided $(n=p/20,p/10)$ . Regarding gRankLasso, its tuning mechanism requires a substantial amount of time, leading to significant time costs in highdimensional scenarios. In contrast, our method converges significantly faster within 20 iterations, regardless of small sample setting. ", "page_idx": 7}, {"type": "image", "img_path": "wHFaAH3E8z/tmp/98469d3b48341e0723aaec3a25182b29d94a97d9535efac2d67bf02c7b1a9060.jpg", "img_caption": ["Figure 3: Time cost of FasMe vs. baselines on simulated datasets with the feature dimension $p$ varying in $\\{500,1000,1500,2000,2500\\}$ . Subfigure (a)(b)(c)(d) records the time required for each method to be implemented on a series of datasets with different sample sizes $n=p/2\\bar{0},p/10,p/5,p,$ , respectively. Note that the missing points of the baselines mean the time cost is out of range. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "wHFaAH3E8z/tmp/4bcdd8461afa39d7a3923a218b253e309bc7189ebf4b576b80e90af696e1c9b7.jpg", "table_caption": ["Table 1: Comparison of estimation error (in terms of MCC and $\\mathrm{F-norm}=\\lVert\\hat{\\Omega}_{\\mathrm{new}}-\\Omega_{\\mathrm{new}}^{*}\\rVert_{F})$ and running time (seconds) on synthetic dataset. $\\Omega^{*}$ is generated from a Random graph. $^*$ means that the time exceeds 30 minutes. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "wHFaAH3E8z/tmp/bd16025023489f98de6eb66f7a03b1aeaf3202102ab0ecdcb3ab90ed48780897.jpg", "table_caption": ["Table 2: Comparison of estimation error (in terms of MCC and $\\mathrm{F-norm}=\\lVert\\hat{\\Omega}_{\\mathrm{new}}-\\Omega_{\\mathrm{new}}^{*}\\rVert_{F})$ and running time (seconds) on synthetic dataset. $\\Omega^{*}$ is generated from Tree graph. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Accuracy Comparison We conduct several experiments on the simulated datasets to compare the prediction power of all methods with four different small sample settings $n<p/10$ . Table 1&2 indicates that FasMe obtains lower Frobenius-norm and higher MCC under all the conditions. It suggests that FasMe outperforms the other baselines in high-dimensional settings. The comparison of the two tables shows that FasMe can effectively recover the special structure from the data. In addition, the MCC of Meta-IE becomes small as the feature dimension increases. This is because the predicted edges must be a subset of the support union. As $p$ increases, the estimation of the support union is not relatively accurate, thus significantly reducing the prediction accuracy of Meta-IE. The poor performance of gRankLasso can be attributed to the inefficacy of its automatic tuning mechanism in small sample settings, which degrades the model performance. We draw one subnetwork for the experimental result of every method and compare them with the ground truth in Fig. 4 (a)(b). They show that our method bears the highest similarity with the ground truth. The results are consistent with the MCC value of every method shown in Table 1 &2. ", "page_idx": 8}, {"type": "text", "text": "6.2 Real-world Experiment: Application to Gene and fMRI Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further evaluate our method for estimating precision matrices on two real-world datasets: ChIPSeq dataset (ENCODE project [39]) and fMRI dataset (OpenfMRI project [40], accession number: ds000002). Regarding the ChIP-Seq dataset, we aim to exploit the gene network of transcription factors (TFs). We randomly selected 300 gene features. 27 samples of H1-hESC (embryonic stem cells) and GM12878 (Blymphocyte) are chosen as two auxiliary tasks for meta-knowledge and 20 samples of K562 (leukemia) are chosen as a novel task. Regarding the fMRI dataset, we aim to exploit brain connectivity among different brain regions. We randomly selected 200 regions of interest (ROI) as features. 34 samples of PCL and DCL datasets are chosen as the auxiliary tasks for meta-knowledge and 20 samples of Mixed-Event dataset are chosen as a novel task. Additional experimental details including configuration choices and extended experiments in the economic domain are provided in Appendix B. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "wHFaAH3E8z/tmp/c1a3374cdae174b818decbdd6549bfe22105719cd3f6f2f4c9cf48384cbe4da7.jpg", "img_caption": ["Figure 4: Subfigure (a)(b) display graph recovery results on two synthetic datasets for different methods compared with the ground truth. Subfigure (c) shows the gene network predicted by our proposed method for 300 genes on ChIP-Seq dataset. Subfigure (d) shows the brain connectome recovered by our proposed method for 200 regions on fMRI dataset. Positive and negative correlations are represented by orange and green edges, respectively. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "In Subfigure (c)(d) of Fig. 4 , we visualize the experimental results on the two real-world datasets respectively. The predicted gene network exhibits scale-free and clustering behaviors, which is consistent with its biological properties. In another experiment, the majority of brain connections are found in the left ROIs. Besides, the left brain has positive connections, while the cross-hemisphere ones are negative. In this case, the subjects are asked to solve a classification problem, which mainly relies on their left brain\u2019s analysis function. The results align with our expectations. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced FasMe, a Fast and Sample-efficient Meta Estimator designed to address the challenges of precision matrix learning in small sample settings. Under a novel meta-learningbased framework, our approach first leverages a multi-task precision matrix estimator to extract shared meta-knowledge from auxiliary tasks, enabling efficient adaptation to new tasks with minimal data. FasMe then incorporates a maximum determinant matrix completion strategy to enhance precision matrix estimation, ensuring both computational efficiency and theoretical robustness. Our experiments on synthetic and real-world datasets demonstrate that FasMe significantly outperforms state-of-the-art baselines in terms of accuracy and speed. These results highlight the potential of FasMe to address real-world challenges in high-dimensional settings, such as genetics and neuroscience, where data is scarce. Moving forward, we plan to extend our work by relaxing the sub-Gaussian data assumption and further reducing the sample size requirements for training meta-knowledge. This will extend the framework to other domains and improve scalability for even larger datasets. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by National Natural Science Foundation of China (Grant Numbers 61906040, 61972085, 62276063, 62272101, 6509009710), the National Key Research and Development Program of China (Grant Number 2022YFF0712400), the Natural Science Foundation of Jiangsu Province (Grant Number BK20221457, BK20230083), the Fundamental Research Funds for the Central Universities (Grant Number 2242021R41177), and SIP Support-startup funding (Grant Number MSRI8001004). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] T. TONY CAI, HONGZHE LI, WEIDONG LIU, and JICHUN XIE. Covariate-adjusted precision matrix estimation with an application in genetical genomics. Biometrika, 100(1):139\u2013 156, 2013. [2] Azam Kheyri, Andriette Bekker, and Mohammad Arashi. High-dimensional precision matrix estimation through gsos with application in the foreign exchange market. Mathematics, 10(22), 2022.   \n[3] Jianqing Fan, Yuan Liao, and Han Liu. An overview of the estimation of large covariance and precision matrices. The Econometrics Journal, 19(1):C1\u2013C32, 2016. [4] Pradeep Ravikumar, Martin J Wainwright, and John D Lafferty. High-dimensional ising model selection using $\\ell_{1}$ -regularized logistic regression. The Annals of Statistics, 38(3):1287\u20131319, 2010.   \n[5] Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model. Biometrika, 94(1):19\u201335, 2007.   \n[6] Jean Honorio, Tommi Jaakkola, and Dimitris Samaras. On the statistical efficiency of $\\ell_{1,p}$ multi-task learning of gaussian graphical models. arXiv preprint arXiv:1207.4255, 2012.   \n[7] Jing Ma and George Michailidis. Joint structural estimation of multiple graphical models. The Journal of Machine Learning Research, 17(1):5777\u20135824, 2016.   \n[8] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017. [9] Federico Girosi, Michael Jones, and Tomaso Poggio. Regularization theory and neural networks architectures. Neural computation, 7(2):219\u2013269, 1995.   \n[10] Cho-Jui Hsieh, M\u00e1ty\u00e1s A Sustik, Inderjit S Dhillon, Pradeep Ravikumar, et al. Quic: quadratic approximation for sparse inverse covariance estimation. J. Mach. Learn. Res., 15(1):2911\u20132947, 2014.   \n[11] Qian Zhang, Yilin Zheng, and Jean Honorio. Meta learning for support recovery in highdimensional precision matrix estimation. In International Conference on Machine Learning, pages 12642\u201312652. PMLR, 2021.   \n[12] Huiming Xie and Jean Honorio. Meta learning for high-dimensional ising model selection using $\\ell_{1}$ -regularized logistic regression. arXiv preprint arXiv:2208.09539, 2022.   \n[13] Gabriel Maicas, Andrew P Bradley, Jacinto C Nascimento, Ian Reid, and Gustavo Carneiro. Training medical image analysis systems like radiologists. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 546\u2013554. Springer, 2018.   \n[14] Smart task design for meta learning medical image analysis systems: Unsupervised, weaklysupervised, and cross-domain design of meta learning tasks. In Hien Van Nguyen, Ronald Summers, and Rama Chellappa, editors, Meta Learning With Medical Imaging and Health Informatics Applications, pages 185\u2013209. Academic Press, 2023.   \n[15] Po-Sen Huang, Chenglong Wang, Rishabh Singh, Wen-tau Yih, and Xiaodong He. Natural language to structured query generation via meta-learning. arXiv preprint arXiv:1803.02400, 2018.   \n[16] Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-context tuning, 2021.   \n[17] Guangting Wang, Chong Luo, Xiaoyan Sun, Zhiwei Xiong, and Wenjun Zeng. Tracking by instance detection: A meta-learning approach. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6288\u20136297, 2020.   \n[18] Xiongwei Wu, Doyen Sahoo, and Steven C. H. Hoi. Meta-{rcnn}: Meta learning for few-shot object detection, 2020.   \n[19] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Meta-learning to detect rare objects. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9924\u20139933, 2019.   \n[20] Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artificial intelligence review, 18:77\u201395, 2002.   \n[21] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural networks: A survey. IEEE Transactions on Pattern Analysis & Machine Intelligence, 44(09):5149\u20135169, sep 2022.   \n[22] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):5149\u20135169, 2021.   \n[23] Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018.   \n[24] Pradeep Ravikumar, Martin J Wainwright, Garvesh Raskutti, and Bin Yu. High-dimensional covariance estimation by minimizing $\\ell_{1}$ -penalized log-determinant divergence. Electronic Journal of Statistics, 5:935\u2013980, 2011.   \n[25] Tony Cai, Weidong Liu, and Xi Luo. A constrained $\\ell_{1}$ minimization approach to sparse precision matrix estimation. Journal of the American Statistical Association, 106(494):594\u2013607, 2011.   \n[26] Wonyul Lee and Yufeng Liu. Joint estimation of multiple precision matrices with common structures. The Journal of Machine Learning Research, 16(1):1035\u20131062, 2015.   \n[27] Satoshi Hara and Takashi Washio. Learning a common substructure of multiple graphical gaussian models. Neural Networks, 38:23\u201338, 2013.   \n[28] Weidong Liu. Structural similarity and difference testing on multiple sparse gaussian graphical models. 2017.   \n[29] Data Coordinating Center Burton Robert 67 Jensen Mark A 53 Kahn Ari 53 Pihl Todd 53 Pot David 53 Wan Yunhu 53 and Tissue Source Site Levine Douglas A 68. The cancer genome atlas pan-cancer analysis project. Nature genetics, 45(10):1113\u20131120, 2013.   \n[30] Lieven Vandenberghe, Stephen Boyd, and Shao-Po Wu. Determinant maximization with linear matrix inequality constraints. SIAM journal on matrix analysis and applications, 19(2):499\u2013533, 1998.   \n[31] Peter J Bickel and Elizaveta Levina. Covariance regularization by thresholding. The Annals of statistics, 36(6):2577\u20132604, 2008.   \n[32] Salar Fattahi and Somayeh Sojoudi. Graphical lasso and thresholding: Equivalence and closedform solutions. Journal of machine learning research, 2019.   \n[33] Somayeh Sojoudi. Equivalence of graphical lasso and thresholding for sparse graphs. The Journal of Machine Learning Research, 17(1):3943\u20133963, 2016.   \n[34] Nicolai Meinshausen and Peter B\u00fchlmann. High-dimensional graphs and variable selection with the lasso. The annals of statistics, 34(3):1436\u20131462, 2006.   \n[35] Chau Tran and Guo Yu. A completely tuning-free and robust approach to sparse precision matrix estimation. In International Conference on Machine Learning, pages 21733\u201321750. PMLR, 2022.   \n[36] Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. The Annals of statistics, 32(2):407\u2013499, 2004.   \n[37] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[38] Cho-Jui Hsieh, M\u00e1ty\u00e1s A Sustik, Inderjit S Dhillon, Pradeep K Ravikumar, and Russell Poldrack. Big & quic: Sparse inverse covariance estimation for a million variables. Advances in neural information processing systems, 26, 2013.   \n[39] ENCODE Project Consortium et al. An integrated encyclopedia of dna elements in the human genome. Nature, 489(7414):57, 2012.   \n[40] Russell A Poldrack, Deanna M Barch, Jason P Mitchell, Tor D Wager, Anthony D Wagner, Joseph T Devlin, Chad Cumba, Oluwasanmi Koyejo, and Michael P Milham. Toward open sharing of task-based fmri data: the openfmri project. Frontiers in neuroinformatics, 7:12, 2013.   \n[41] Riten Mitra, Peter M\u00fcller, Shoudan Liang, Lu Yue, and Yuan Ji. A bayesian graphical model for chip-seq data on histone modifications. Journal of the American Statistical Association, 108(501):69\u201380, 2013.   \n[42] Scott M Lundberg, William B Tu, Brian Raught, Linda Z Penn, Michael M Hoffman, and Su-In Lee. Learning the human chromatin network from all encode chip-seq data. bioRxiv, page 023911, 2015.   \n[43] Felicia SL $\\mathrm{Ng}$ , David Ruau, Lorenz Wernisch, and Berthold G\u00f6ttgens. A graphical model approach visualizes regulatory relationships between genome-wide transcription factor binding profiles. Briefings in bioinformatics, 19(1):162\u2013173, 2018.   \n[44] Hantao Shu, Jingtian Zhou, Qiuyu Lian, Han Li, Dan Zhao, Jianyang Zeng, and Jianzhu Ma. Modeling gene regulatory networks using neural network architectures. Nature Computational Science, 1(7):491\u2013501, 2021.   \n[45] Michael VanInsberghe, Jeroen van den Berg, Amanda Andersson-Rolf, Hans Clevers, and Alexander van Oudenaarden. Single-cell ribo-seq reveals cell cycle-dependent translational pausing. Nature, 597(7877):561\u2013565, 2021.   \n[46] Yuanyuan Qu, Xiaohui Wu, Aihetaimujiang Anwaier, Jinwen Feng, Wenhao Xu, Xiaoru Pei, Yu Zhu, Yang Liu, Lin Bai, Guojian Yang, et al. Proteogenomic characterization of mit family translocation renal cell carcinoma. Nature Communications, 13(1):7494, 2022.   \n[47] Praneet Chaturvedi, Yaseswini Neelamraju, Waqar Arif, Auinash Kalsotra, and Sarath Chandra Janga. Uncovering rna binding proteins associated with age and gender during liver maturation. Scientific reports, 5(1):9512, 2015.   \n[48] Srikanth Ryali, Tianwen Chen, Kaustubh Supekar, and Vinod Menon. Estimation of functional connectivity in fmri data using stability selection-based sparse partial correlation with elastic net penalty. NeuroImage, 59(4):3852\u20133861, 2012.   \n[49] Xi Luo. A hierarchical graphical model for big inverse covariance estimation with an application to fmri. arXiv preprint arXiv:1403.4698, 2014.   \n[50] Thaddeus Robert Sulek. An application of graphical models to fMRI data using the lasso penalty. PhD thesis, University of Georgia, 2017.   \n[51] Jongik Chung, Brooke S Jackson, Jennifer E Mcdowell, and Cheolwoo Park. Joint estimation and regularized aggregation of brain network in fmri data. Journal of Neuroscience Methods, 364:109374, 2021.   \n[52] Martin S Andersen, Joachim Dahl, and Lieven Vandenberghe. Logarithmic barriers for sparse matrix cones. Optimization Methods and Software, 28(3):396\u2013423, 2013.   \n[53] Hossein Keshavarz, George Michaildiis, and Yves Atchad\u00e9. Sequential change-point detection in high-dimensional gaussian graphical models. Journal of machine learning research, 21(82):1\u2013 57, 2020.   \n[54] Mitchell Krock, William Kleiber, and Stephen Becker. Nonstationary modeling with sparsity for spatial data via the basis graphical lasso. Journal of Computational and Graphical Statistics, 30(2):375\u2013389, 2021.   \n[55] Aaron J Molstad, Wei Sun, and Li Hsu. A covariance-enhanced approach to multi-tissue joint eqtl mapping with application to transcriptome-wide association studies. The annals of applied statistics, 15(2):998, 2021.   \n[56] Jun Ho Yoon and Seyoung Kim. Eiglasso: Scalable estimation of cartesian product of sparse inverse covariance matrices. In Conference on Uncertainty in Artificial Intelligence, pages 1248\u20131257. PMLR, 2020.   \n[57] Seongoh Park, Xinlei Wang, and Johan Lim. Estimating high-dimensional covariance and precision matrices under general missing dependence. Electronic Journal of Statistics, 15(2):4868\u2013 4915, 2021.   \n[58] Tao Liu, Ziyuan Yang, Armando Marino, Gui Gao, and Jian Yang. Polsar ship detection based on neighborhood polarimetric covariance matrix. IEEE Transactions on Geoscience and Remote Sensing, 59(6):4874\u20134887, 2020.   \n[59] Zitong Wan, Rui Yang, Mengjie Huang, Weibo Liu, and Nianyin Zeng. Eeg fading data classification based on improved manifold learning with adaptive neighborhood selection. Neurocomputing, 482:186\u2013196, 2022.   \n[60] Boxin Zhao, Percy S Zhai, Y Samuel Wang, and Mladen Kolar. High-dimensional functional graphical model structure learning via neighborhood selection approach. arXiv preprint arXiv:2105.02487, 2021.   \n[61] Yan Zhu, Cangzhi Jia, Fuyi Li, and Jiangning Song. Inspector: a lysine succinylation predictor based on edited nearest-neighbor undersampling and adaptive synthetic oversampling. Analytical biochemistry, 593:113592, 2020.   \n[62] Joachim Dahl, Lieven Vandenberghe, and Vwani Roychowdhury. Covariance selection for nonchordal graphs via chordal embedding. Optimization Methods & Software, 23(4):501\u2013520, 2008.   \n[63] Martin S Andersen, Joachim Dahl, and Lieven Vandenberghe. Implementation of nonsymmetric interior-point methods for linear optimization over sparse matrix cones. Mathematical Programming Computation, 2(3-4):167\u2013201, 2010.   \n[64] Lieven Vandenberghe, Martin S Andersen, et al. Chordal graphs and semidefinite optimization. Foundations and Trends\u00ae in Optimization, 1(4):241\u2013433, 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We summarize the paper\u2019s contributions and scope in lines 85-99. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 14}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: Due to the space limit, we include the discussion about the limitation in Appendix C. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We provide the assumptions, theorems and the corresponding proofs in Appendix F. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Yes, we provide all the experimental details, including data generation methods, hyperparameter selection, and the full optimization algorithm. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have uploaded the relevant code on our GitHub (https://github.com/ MahjongGod-Saki/FasMe). ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Yes, we provide comprehensive documentation on all training and testing details, including data generation methods, hyperparameter selection, reasons for configuration choices, and the complete optimization algorithm. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 16}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We provide the error bound of our estimators in Theorem 2 and 3 of the main page. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We analyze computational complexity in Appendix C and compare the execution time of our method with other baselines in Section 6. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have no such risk. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We discuss them in Appendix A. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have no such risk. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 18}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We cite the corresponding source for all the real-world dataset we use. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have submitted our code in our GitHub. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 19}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have no such risk. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have no such risk. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "A Impact Statements ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This paper proposes a novel approach for fast and sample-efficient meta precision matrix learning, which can leverage prior knowledge from related tasks to improve the estimation of conditional dependency relationships between random variables. This approach has potential applications in various domains such as genetics, neuroscience, and social networks, where sparse and highdimensional data are common. The ethical aspects and future societal consequences of this work depend on the specific contexts and purposes of applying this approach, which are beyond the scope of this paper. However, we encourage researchers and practitioners to consider the possible beneftis and risks of using this approach, such as enhancing disease diagnosis or enabling authoritarian surveillance, and to adopt appropriate measures to ensure its responsible and beneficial use. ", "page_idx": 21}, {"type": "text", "text": "B More Experimental Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "All experiments are performed on a machine with an Intel Core i9-10910 ten-core 3.6 GHz CPU and 64 GB RAM. ", "page_idx": 21}, {"type": "text", "text": "B.1 Simulation Experiment ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For each prior task $k=1,2,\\ldots,K$ , we generate $n_{k}=p/2$ independently and identically distributed observations from a multivariate normal distribution with mean 0 and precision matrix $\\Omega^{(k)}$ . For the new task, we generate $n=p/10$ i.i.d. samples from a Gaussian distribution with mean 0 and precision matrix $\\Omega_{\\mathrm{new}}\\,=\\,\\Omega^{(k+1)}$ . This model assumes that the graph is composed of two parts, namely the common substructure $\\theta$ and the rest $\\theta_{r}^{(k)}$ . The entire precision matrix is parameterized as $\\Omega^{(k)}\\stackrel{\\cdot}{=}\\theta+\\theta_{r}^{(k)}$ . ", "page_idx": 21}, {"type": "text", "text": "Random Graph Each off-diagonal entry of $\\theta$ is generated independently and equals 0.5 with a probability $p_{0}=1/(p\\!-\\!1)$ and 0 with a probability $1\\!-\\!p_{0}$ . Each off-diagonal entry of $\\bar{\\theta}_{r}^{(k)}$ is generated independently and equals 0.5 with probability $0.2k p_{0}$ and 0 with probability $1-0.2k p_{0}$ . Then we add every diagonal element by a fixed value large enough to guarantee the positive definiteness of the precision matrix. ", "page_idx": 21}, {"type": "text", "text": "Tree Graph We generate $K+1$ networks each consisting of $c$ unconnected subnetworks with a tree structure. All the networks share one common subnetwork with $p/10$ dimensions. We randomly generate the dimension of the other $c-1$ subnetworks that range from 1 to $p/10$ . Every subnetwork follows a tree structure. Most subnetworks consist of only one node because we choose a relatively large value for $c$ . Each off-diagonal entry corresponding to the edge equals 0.5. Then we add every diagonal element by a fixed value large enough to guarantee the positive definiteness of the precision matrix. ", "page_idx": 21}, {"type": "text", "text": "B.2 Real-world Experiment ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "ChIP-Seq dataset ENCODE project is a widely used database for human genome research. For our experiment, we select the ChIP-Seq data for transcription factors (TFs). The ChIP-Seq dataset has been widely utilized as the benchmark dataset in other research papers in this domain [41, 42, 43, 44]. TFs are proteins that can control how genes work. The function of TFs is to regulate turn-on and off genes in order to make sure that they are expressed in the desired cells at the right time and in the right amount throughout the life of the cell and the organism. This experiment aims to exploit the gene network of transcription factors (TFs). ", "page_idx": 21}, {"type": "text", "text": "There are three kinds of human cells involved in the dataset: (1) H1-hESC(embryonic stem cells: primary tissue), (2) GM12878 (B-lymphocyte: normal tissue) and (3) K562 (leukemia: cancerous tissue). The dataset provides simultaneous binding measurements of TFs to thousands of gene targets, each flie contains 27 samples(TFs) and each sample has 25185 features(gene targets). We randomly selected 300 features for our experiment. The exact shape of the used data in our experiment is (27 TF samples, 300 TF features) for each kind of human cell. H1-hESC and GM12878 are chosen as two auxiliary tasks for meta- knowledge and 20 samples of K562 are chosen as a new task. ", "page_idx": 21}, {"type": "text", "text": "Note that the choice to utilize H1-hESC and GM12878 as auxiliary tasks, with K562 as the primary prediction task, was made with deliberate consideration of their biological significance. H1-hESC represents primary tissue, GM12878 represents normal tissue, and K562 represents cancer tissue. Given the heightened research focus on cancer cell gene networks, which are more likely to be the subject of predictive modeling and have rare cases, we opted for this configuration to use K562 as the test set. ", "page_idx": 22}, {"type": "image", "img_path": "wHFaAH3E8z/tmp/644a1b74584b8c89d23c81ae7980a7eb6ea0592e101bc38235b8015e7b6eb841.jpg", "img_caption": ["Figure 5: Gene network result of FasMe. Examples of discovered gene relationships that are consistent with other biological research: MYL6-GAA [45], HIF1A-ATP6V1G1 [46], WDR6-RBM4 [47]. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "fMRI dataset OpenfMRI is a task-based fMRI database, we chose a dataset about the task Classification learning (accession number: ds000002). This database has been widely used in other research papers in this domain [48, 49, 50, 51]. This dataset contains three different tasks, participants were instructed to perform probabilistic classification learning (PCL), deterministic classification learning (DCL), and mixed-up classification learning on Weather Prediction Task(WPT). WPT is commonly used to assess striatal or procedural learning capacities in different populations. This fMRI dataset effectively records neural responses to stimuli, delays, and negative and positive feedback components, and studying this dataset aims to address a fundamental question of whether and how the brain\u2019s memory systems interact. ", "page_idx": 22}, {"type": "text", "text": "In our experiment, the 4D fMRI brain image is divided into spatial regions of size $2\\,\\times\\,2\\,\\times\\,2$ , so the original scan data in spatial shape $64\\times64\\times30$ is divided into 15360 regions. And the activation of each region during the task (the temporal dimension) is added up to represent the average activation situation of corresponding regions. We randomly selected 200 active regions to lower the computational burden. By training on the tasks of PCL and DCL, the common key brain regions and the interaction between the tasks are obtained, that is, a common substructure. ", "page_idx": 22}, {"type": "text", "text": "Note that we employed PCL (Probabilistic Classification Learning) and DCL (Deterministic Classification Learning) as auxiliary datasets due to their representation of distinct learning paradigms (See Appendix A.2). PCL captures learning under uncertainty, while DCL reflects learning from clear-cut rules. This diversity aids in generalizing the model to novel scenarios. The Mixed-Event dataset, embodying elements from both PCL and DCL, mirrors the intricate and multifaceted nature of human cognition. Consequently, the Mixed-Event dataset emerged as the optimal choice for the test set, offering a realistic and challenging environment for the model to demonstrate its predictive prowess in brain connectivity, reflective of the complexity encountered in real-world cognitive processes. ", "page_idx": 22}, {"type": "text", "text": "By using the prior knowledge of common substructure, we effectively introduce the key areas of the brain on similar tasks and the interaction between these key areas, so that we can learn how the brain regions interact in other tasks efficiently. ", "page_idx": 22}, {"type": "text", "text": "In Table 3, we record the negative log-determinant Bregman divergence of our estimator for the new task and compare it with the other baselines. The experimental results show that our method generalizes better than all other baselines for the real-world dataset since it achieves the minimum log-determinant Bregman divergence. ", "page_idx": 22}, {"type": "text", "text": "US Industry Portfolios dataset This is a well-known dataset of US industry portfolios\u2019 monthly returns from 1926 to 2023. It covers various economic sectors such as agriculture, healthcare, energy, machinery, and electronics. We take each industry as a feature $\\textstyle p=49)$ ) and use the records of different financial metrics as the tasks. We use AVWR (Annual Voluntary Wage Reporting) dataset ", "page_idx": 22}, {"type": "text", "text": "Table 3: Negative log-determinant Bregman divergence of the estimated precision matrices of the new task in the real-world dataset using FasMe and other baselines. A larger value of negative log-determinant Bregman divergence indicates better performance. ", "page_idx": 23}, {"type": "table", "img_path": "wHFaAH3E8z/tmp/f6beaa00460b522b1d608f18d86aa1bc4e6251605643ca09756c042fe03f6481.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "and AEWR (Adverse Effect Wage Rate) dataset $(n_{1},n_{2}=50)$ ) for meta-training, and AFS (Annual Financial Statement) dataset for meta-testing $\\left[n=10\\right]$ ). With FasMe, we can extract meta-knowledge about industry interactions from records of different financial metrics, thus resulting in a better estimation of the hidden relationships of industries in the new task. The predicted connections, e.g., Steel (Steel Works)-Trans(Transportation) and ElcEq (Electrical Equipment)-Hardw(Computers), are consistent with reality. ", "page_idx": 23}, {"type": "image", "img_path": "wHFaAH3E8z/tmp/8f2e2db293bf72373ede2d179bcdf61d1b302496bfb36b639307c42de041e397.jpg", "img_caption": ["Figure 6: Visualization of the financial network predicted by FasMe. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "B.3 Time Complexity Analysis of Meta-IE (BigQuic) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "BigQuic [38], an \"big-data\" extension of QUIC, aims to estimate the precision matrix with thousands of variables. It solves the optimization problem using a block-coordinate descent Newton method. BigQuic divides the Newton direction into several blocks and utilizes the memory cache to accelerate the updated process. The computation complexity of BigQuic is mainly determined by $O\\,((p+|B|)h\\bar{T}T_{\\mathrm{outer}})$ , where $|B|$ is the number of boundary nodes, $h$ is the number of non-zero entries in the $t$ -th generation estimated solution ${\\hat{\\Omega}}^{(t)}$ , $T$ is the average number of Conjugate Gradient iterations and $T_{\\mathrm{outer}}$ is the number of computations within a block. Without loss of generality, $h$ is larger than $p$ in $t$ -th iteration. Hence, the time complexity of BigQuic is at least higher than $O(p^{2})$ time complexity. Furthermore, the performance of BigQuic depends largely on the choice of cluster scheme. A poor partition of variables leads to many \"cache misses\". The challenges of choosing a cluster schema make BigQuic harder to implement. ", "page_idx": 23}, {"type": "text", "text": "B.4 Difference between Meta-IE and FasMe ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Based on the meta-learning framework, FasMe and Meta-IE differ in each component of this framework, namely meta-teacher, meta-knowledge, and meta-student. ", "page_idx": 23}, {"type": "text", "text": "B.4.1 Meta-knowledge ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Meta-IE: Meta-IE supposes the support union $\\hat{\\Omega}$ as the meta-knowledge and assumes that the edges of the new task must be a subset of the edges of $\\hat{\\Omega}$ . ", "page_idx": 23}, {"type": "text", "text": "FasMe: Different from Meta-IE, we assume that the new task shares a common structure, i.e., meta-knowledge $\\hat{\\theta}$ , with the related auxiliary tasks. ", "page_idx": 23}, {"type": "text", "text": "As we stated in Section Introduction, it is impossible to make startling discoveries under this unrealistic assumption of Meta-IE. Our choice of meta-knowledge is more reasonable with biological research and existing works as evidence. ", "page_idx": 24}, {"type": "text", "text": "B.4.2 Meta-teacher ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Meta-IE: Meta-IE simply pools all the samples from $K$ auxiliary tasks to learn the meta-knowledge: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\Omega}=\\underset{\\Omega\\geq0}{\\arg\\operatorname*{min}}-\\log\\operatorname*{det}\\Omega+\\left\\langle\\sum_{k=1}^{K}\\frac{1}{K}\\Sigma^{(k)},\\Omega\\right\\rangle+\\lambda\\|\\Omega\\|_{1},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is data-inefficient. ", "page_idx": 24}, {"type": "text", "text": "FasMe: By taking the inherent heterogeneity among the auxiliary tasks into consideration, FasMe proposes a multitask-learning-based estimator to extract the common substructure \u03b8\u02c6 across the tasks. ", "page_idx": 24}, {"type": "text", "text": "B.4.3 Meta-student ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Meta-IE: Meta-IE uses BigQUIC method to solve a GLasso problem with the knowledge constraint $\\mathrm{supp}(\\Omega)\\subseteq\\mathrm{supp}(\\hat{\\Omega})$ . ", "page_idx": 24}, {"type": "text", "text": "FasMe: FasMe aims to estimate the precision matrix by solving the dual form of a MDMC problem with Newton Conjugate Gradient algorithm. We also would like to emphasize that the meta-student is not a simple application of standard MDMC. The formulation of a standard MDMC problem is: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{X}\\quad\\log\\operatorname*{det}X\\quad\\mathrm{~s.t.~}\\quad X_{i j}=M_{i j},\\forall i,j\\in E.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We improve the method from two aspects: 1) able to deal with the partially observed matrices without chordality by utilizing the chordal embedding 2) lower the time complexity by applying Newton CG algorithm ", "page_idx": 24}, {"type": "table", "img_path": "wHFaAH3E8z/tmp/54b6cd918975a1405754feba092d6791b2f61750b16c4b9b92c23d61930ce0fe.jpg", "table_caption": ["Table 4: Comparison of standard MDMC and our improved MDMC "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "In summary, Meta-IE has presented a preliminary idea for applying metalearning to precision matrix estimation with much potential for improvement. Compared to Meta-IE, FasMe exhibits better theoretical properties and superior performance. ", "page_idx": 24}, {"type": "text", "text": "B.5 Accuracy with Different Sparsity ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "wHFaAH3E8z/tmp/0c04949878280adadc0de4fb1b2f1bc585c6cd52431ceca71bd4488593d74d4b.jpg", "table_caption": ["Table 5: Random graph, $n=100,p=1000$ "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "wHFaAH3E8z/tmp/78cae5669000db7498a08fb6b9d5cbd478eb88790302e2d7f94dd2d52c43f82f.jpg", "table_caption": ["Table 6: Tree graph, $n=100,p=1000$ "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "B.6 Hyperarameter Selection ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "$K$ We actually have conducted a synthetic experiment with varying $K$ and a fixed data size $(n=100,p=1000)$ . We use MCC value as a metric to measure the performance of estimating meta-knowledge. A higher score represents better performance. By varying $K=2,3,\\ldots,6$ , Fig. 7 shows that the MCC value smoothly experiences a slight decline from 0.85 to 0.60. ", "page_idx": 25}, {"type": "image", "img_path": "wHFaAH3E8z/tmp/1a0efb72f3754f5d24378be0786172d3dff849db54deb5363196a8a9ba27bbf8.jpg", "img_caption": ["Figure 7: MCC values of our methods against varying $K$ "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "$\\rho$ This hyperparameter delineates the sparsity contrast between the shared and individual components of the model. As outlined in line 200, a lower $\\rho$ (chosen as 0.8 in our experiments) enriches the shared structure\u2019s density while sparing the individual parts. Our preliminary investigations, spanning $\\rho$ values from 0.1 to 0.9, indicated a stable predictive accuracy across the spectrum. We plan to enrich the appendix with these findings to demonstrate $\\rho$ \u2019s effect on model performance comprehensively. ", "page_idx": 25}, {"type": "text", "text": "$\\lambda$ This hyperparameter dictates the overall sparsity of both the shared and individual graph structures, with a higher value fostering sparser outcomes. Inspired by some previous researches[1, 2], we adopt \u03bb = \u03c9 $\\begin{array}{r}{\\lambda=\\omega\\sqrt{\\frac{\\log(K p)}{n_{\\mathrm{tot}}}}.}\\end{array}$ , where $\\omega$ varies across $0.05\\times i|i=1,2,\\ldots,30.$ . The Bayesian Information Criterion (BIC) assists in pinpointing the optimal $\\lambda$ for our model and is uniformly applied across baseline comparisons. ", "page_idx": 25}, {"type": "text", "text": "$\\eta$ Regarding $\\eta$ , functioning as the soft-thresholding parameter, $\\eta$ \u2019s selection leans on an empirical estimation of the graph sparsity by the user. It aims to adjust the number of nonzero entries in the model\u2019s sparsity pattern, correlating closely with those in the estimated precision matrix. A higher value of $\\eta$ corresponds to a more sparsely estimated graph structure. ", "page_idx": 25}, {"type": "text", "text": "B.7 Tables ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Table 7 & 8 & 9 & 10 present the detaild of Fig. 3. They correspond to Subfigure (a)(b)(c)(d) correspondingly, showing time cost (seconds) comparison between FasMe and other baselines on simulated datasets, with dimension $p$ changing from 500 to 2500. ", "page_idx": 25}, {"type": "table", "img_path": "wHFaAH3E8z/tmp/f59a41791f654b5bb8bbd5485c4a3898c90612e4c93ace597e983c7923d33fea.jpg", "table_caption": ["Table 7: The time comparisons of FasMe and the baselines on the simulated datasets varying $p$ and sample size $n=\\mathbf{p}/2\\mathbf{0}$ . "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "wHFaAH3E8z/tmp/02f87dde2b68c7516211dbae1cf3999034674774645bb539226ed223bffc23c8.jpg", "table_caption": ["Table 8: The time comparisons of FasMe and the baselines on the simulated datasets varying $p$ and sample size $n={\\bf p}/{\\bf10}$ . "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "wHFaAH3E8z/tmp/8f810bcd17095f313bba9f99e2d47a43262a7b2ec02705f8c4b12634200eb08b.jpg", "table_caption": ["Table 9: The time comparisons of FasMe and the baselines on the simulated datasets varying $p$ and sample size $n=\\mathbf{p}/\\mathbf{5}$ . "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "wHFaAH3E8z/tmp/def0730ec0bb6fd18f1552661f705bf8f2994ec330eb778e1fbfbde4876f4a20.jpg", "table_caption": ["Table 10: The time comparisons of FasMe and the baselines on the simulated datasets varying $p$ and sample size $n={\\bf p}$ . "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Algorithm 1 FasMe   \n1: Input: Auxiliary datasets $\\{\\mathbf{X}^{(k)}\\}$ , New task $\\mathbf{X}_{\\mathrm{new}}$ , hyper-parameter $\\lambda,\\;\\rho,\\;\\gamma,\\;\\eta,\\;c,\\;\\alpha,$ , the   \nmaximum iteration $T$ , two thresholds $\\epsilon_{1}$ and $\\epsilon_{2}$ , and a linear programming solver $\\operatorname{LP}(\\cdot)$ that   \nsolves (18).   \n2: for $k=1$ to $K$ do   \n3: Calculate the covariance matrices $\\begin{array}{r}{\\Sigma^{(k)}=\\frac{1}{n_{k}}\\sum_{i=1}^{n_{k}}({\\bf X}-\\bar{\\bf X})({\\bf X}-\\bar{\\bf X})^{\\top}}\\end{array}$   \n4: Initialize $\\theta=\\theta_{r}^{(k)}=\\mathbf{0}_{p\\times p}$   \n5: Initialize $\\mathbf{B}^{(k)}=\\left[\\mathbf{0},\\ldots,\\Sigma^{(k)},\\ldots,\\mathbf{0},{\\textstyle\\frac{1}{\\epsilon K}}\\Sigma^{(k)}\\right]$   \n6: end for   \n7: for $i=1$ to $p$ do   \n8: $\\mathbf{b}=\\mathbf{e}_{i}$ , $\\dot{\\beta}=\\mathrm{LP}({\\bf B}^{(k)},{\\bf b}),k=1,\\ldots,K$   \n190:: fo $k=1$ $K$   \n$\\theta_{r,i}^{(k)}=\\beta_{(k-1)p+1:k p}$   \n11: end for   \n12: \u03b8i = \u03b2(Kp+1):(K+1)p   \n13: end for   \n14: Calculate the sparsity pattern $\\mathcal{G}$ by (8) and its chordal embedding $\\tilde{\\mathcal{G}}$ by Cholesky factorization   \n15: Initialize $y_{1}=0$   \n16: for $t=1$ to $T$ do   \n17: $\\Delta y=-\\nabla^{2}l(y_{t})^{-1}\\nabla l(y_{t})$   \n18: $\\alpha=1$   \n19: if $l(y+\\alpha\\Delta y)>l(y)+\\gamma\\alpha\\Delta y^{\\top}\\nabla l(y)$ then   \n20: $\\alpha:=\\alpha\\cdot c$   \n21: end if   \n22: Calculate the Newton decrement $\\delta_{k}=|\\Delta y_{t}^{\\top}\\nabla l(y_{t})|$   \n23: if $\\delta_{t}<\\epsilon_{1}$ or $|\\Delta y_{t}|<\\epsilon_{2}$ then   \n24: Break   \n25: else   \n26: $y_{t+1}=y_{t}+\\alpha\\Delta y_{t}$   \n27: end if   \n28: end for   \n29: Output: Precision matrix $\\Omega_{\\mathrm{new}}$ . ", "page_idx": 27}, {"type": "text", "text": "Similar to CLIME, (5) can also be solved column by column: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg(\\hat{\\beta},\\{\\hat{\\beta}_{r}^{(k)}\\}\\bigg)=\\underset{\\beta,\\{\\beta_{r}^{(k)}\\}}{\\arg\\operatorname*{min}}}&{K\\|\\beta\\|_{1}+\\rho\\displaystyle\\sum_{k=1}^{K}\\|\\beta_{r}^{(k)}\\|_{1},}\\\\ {\\mathrm{s.t.}}&{\\|\\beta\\Sigma^{(k)}-(\\mathbf{e}_{j}-\\beta_{r}^{(k)}\\Sigma^{(k)})\\|_{\\infty}\\leq\\lambda,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $k=1,2,\\ldots,K$ . Here $\\beta$ represents the corresponding column vector in the meta-knowledge $\\theta$ and $\\beta_{r}^{(k)}$ represents the $k$ -th column vector in the rest $\\theta_{r}^{(k)}$ . ", "page_idx": 27}, {"type": "text", "text": "To solve (16), we can rewrite it as the following ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\hat{\\boldsymbol{\\beta}}=\\underset{\\boldsymbol{\\beta}}{\\arg\\operatorname*{min}}}&{\\|\\boldsymbol{\\beta}\\|_{1},}\\\\ {\\mathrm{s.t.}}&{\\|\\mathbf{B}^{(k)}\\boldsymbol{\\beta}-\\mathbf{b}\\|_{\\infty}\\leq\\lambda,k=1,\\ldots,K.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here, $\\begin{array}{r}{{\\bf B}^{(k)}=\\left[{\\bf0},\\ldots,\\Sigma^{(k)},\\ldots,{\\bf0},\\frac{1}{\\epsilon K}\\Sigma^{(k)}\\right],\\beta=\\left[(\\beta^{(1)})^{\\top},\\ldots,(\\beta^{(k)})^{\\top},\\ldots,(\\beta^{(K)})^{\\top},\\epsilon K(\\beta^{W})^{\\top}\\right]^{\\top}}\\end{array}$ and b = ej. ", "page_idx": 27}, {"type": "text", "text": "Through relaxation, we can convert (17) to the following linear programming formulation: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\hat{\\mathbf{a}}_{j}=\\underset{\\mathbf{a}_{j}}{\\arg\\operatorname*{min}}\\;}&{\\displaystyle\\sum_{j=1}^{(K+1)p}\\mathbf{a}_{j},}\\\\ {\\mathrm{s.t.}\\;}&{-\\;\\theta_{j}\\leq\\mathbf{a}_{j},\\;j=1,\\ldots,(K+1)p,}\\\\ &{\\theta_{j}\\leq\\mathbf{a}_{j},\\;j=1,\\ldots,(K+1)p,}\\\\ &{-\\;(\\mathbf{B}_{i}^{(k)})^{\\top}\\theta+\\mathbf{b}_{i}\\leq c,i=1,\\ldots,p;k=1,\\ldots,K,}\\\\ &{(\\mathbf{B}_{i}^{(k)})^{\\top}\\theta-\\mathbf{b}_{i}\\leq c,i=1,\\ldots,p;k=1,\\ldots,K.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here ${\\bf a}_{j}$ are the slack variables. $\\mathbf{B}_{i}^{(k)}$ represents the $i$ -th row of $\\mathbf{B}^{(k)}$ and ${\\bf b}_{i}$ is the $i$ -th entry of $\\mathbf{b}$ According to [25], we can apply the same symmetric operators on $\\Omega^{(k)}=\\theta+\\theta_{r}^{(k)}$ obtained from Algorithm 1. ", "page_idx": 28}, {"type": "text", "text": "Then we combined the support set of the thresholded covariance and meta-knowledge to obtain the sparsity pattern: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{G}=\\mathrm{supp}(\\theta)\\cup\\mathrm{supp}\\left(S_{\\eta}(\\Sigma_{\\mathrm{new}})\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To solve the optimization problem (9), we first need to obtain the chordal embedding $\\tilde{\\mathcal{G}}$ by Cholesky factorization. We compute the unique lower triangular Cholesky factor $L$ satisfying $\\Pi_{\\mathcal{G}}(\\Sigma_{\\mathrm{new}})=$ $L L^{\\top}$ . After ignoring perfect numerical cancellation, we have the adjacency matrix of the chordal embedding $L^{\\bar{+}\\,L^{\\top}}$ using symbolic Cholesky algorithm. ", "page_idx": 28}, {"type": "text", "text": "We define the cone of sparse positive semidefinite matrices $\\mathcal{P}$ , and the cone of sparse matrices with positive semidefinite completions $\\mathcal{P}_{*}$ as the following: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{P}=\\mathbb{S}_{+}^{p}\\cap\\mathbb{S}_{\\tilde{\\mathcal{G}}}^{p},\\quad\\mathcal{P}_{*}=\\{\\langle S,\\Omega\\rangle\\ge0:S\\in\\mathbb{S}_{\\tilde{\\mathcal{G}}}^{p}\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The primal and dual problem of Eq. (9) can be written as: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{\\Omega\\in\\mathcal{P}}\\langle\\Pi_{\\mathcal{G}}(\\Sigma),\\Omega\\rangle+g(\\Omega)\\quad\\mathrm{s.t.}\\;P^{\\top}(\\Omega)=0,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,max}_{S\\in\\mathcal{P},y\\in\\mathbb{R}^{m}}-g_{*}(S)\\quad\\mathrm{s.t.}\\ S=\\hat{\\Sigma}-P(y),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $P(\\cdot)\\,:\\,\\mathbb{R}^{m}\\,\\rightarrow\\,\\mathbb{S}_{\\tilde{\\mathcal{G}}\\backslash\\mathcal{G}}^{p}$ represents a linear map that converts a list of $m$ variables into the corresponding matrix in $\\tilde{\\mathcal{G}}\\backslash\\mathcal{G}$ . $g$ and $g_{*}$ are the \"log-det\" barrier functions on $\\mathcal{P}$ and $\\mathcal{P}_{*}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g(\\Omega)=-\\log\\operatorname*{det}\\Omega,\\quad g_{*}(S)=-\\operatorname*{min}_{\\Omega\\in\\mathcal{P}}\\langle S,\\Omega\\rangle-\\log\\operatorname*{det}\\Omega.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Under the sparsity and chordal assumption, the gradient evaluations and Hessian matrix-vector products can be efficiently evaluated in $O(p)$ time and $O(p)$ memory, using the numerical recipes described in [52]. ", "page_idx": 28}, {"type": "text", "text": "To solve the dual problem (21), we rewrite it into the following unconstrained optimization problem: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\hat{y}\\equiv\\operatorname*{arg\\,min}_{y\\in\\mathbb{R}^{m}}l(y)\\equiv g_{*}(\\Pi_{\\mathcal{G}}(\\Sigma_{\\mathrm{new}})-P(y)).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "After obtaining the solution $\\hat{y}$ , we can recover the optimal estimator for the primal problem through $\\hat{\\Omega}=-\\nabla g_{*}(\\Pi_{\\mathcal{G}}(\\Sigma_{\\mathrm{new}})-P(y))$ . The dual problem is easier to solve than the primal because the initial point $y=0$ tends to lie close to the solution $\\hat{y}$ . Starting from this point, we can use Newton\u2019s method to converge rapidly. ", "page_idx": 28}, {"type": "text", "text": "We then solve the Newton direction $\\Delta y$ via the $m\\times m$ system of equations ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\nabla^{2}l(y)\\Delta y=-\\nabla l(y).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Starting from the origin $y_{1}=0$ , the method converges to an $\\epsilon$ -accurate search direction $y_{1}$ satisfying ", "page_idx": 28}, {"type": "equation", "text": "$$\n(y_{1}-\\Delta y)^{\\top}\\nabla^{2}l(y)(y_{1}-\\Delta y)\\leq\\epsilon|\\Delta y^{\\top}\\nabla l(y)|\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "in at most $\\begin{array}{r}{\\sqrt{\\|\\nabla^{2}l(y)\\|\\|\\nabla^{2}l(y)^{-1}\\|}\\log(\\frac{2}{\\epsilon})}\\end{array}$ conjugate gradient iterations. ", "page_idx": 28}, {"type": "text", "text": "Computational Complexity In this section, we mainly analyze the time and memory complexity of our estimator in the meta-testing step. The time and memory cost of our estimator can be divided into two parts. The first part is to threshold the covariance matrix. This step is quadratic $O(p^{2})$ time and memory but embarrassingly parallelizable. The second part is to solve (7) using Newton Conjugate Gradient methods. If the prior sparsity pattern $\\mathcal{G}$ is sparse and chordal, then the second part can be performed using our algorithm in linear $O(p)$ time and memory. This significantly outperforms QUIC [10], Neighborhood Selection method [34] and Bigquic used in [11]. ", "page_idx": 29}, {"type": "text", "text": "D Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "As discussed earlier, the biological and genetic domains have adopted and validated the common structure assumption based on some biological evidence. For other domains, if the edges are sparse and the data are high-dimensional, it may be difficult to recover the common structure from the data without some prior evidence. In the worst case, the meta-teacher can learn the knowledge that there is no common edge among the tasks. The meta-student has to learn from scratch. The optimization problem in the meta-testing process can be seen as a graphical lasso problem (introduced in Section 2.2). The neighborhood selection baseline is one type of algorithm that solves the graphical lasso. Consequently, our proposed method remains at least as good as the baseline, if not stronger, in handling such challenges. ", "page_idx": 29}, {"type": "text", "text": "E Explanations about the Choices of the Baselines ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We chose QUIC and Neighborhood Selection since they continue to be widely utilized in current research and serve as critical benchmarks for new methodologies within the realm of state-of-the-art works. ", "page_idx": 29}, {"type": "text", "text": "We take QUIC as an example. [53] utilizes the QUIC method to detect structural changes in highdimensional Gaussian graphical models. QUIC\u2019s ability to perform fast and accurate estimation underpins the methodology for identifying change-points in the graphical model structure over time. Similarly, studies like those in recent works like [54, 55] also employ QUIC for various analytical tasks downstream. ", "page_idx": 29}, {"type": "text", "text": "Additionally, while both [56] and [57] utilize QUIC as a baseline for comparison, [56] relies on a structural assumption of Kronecker-sum-structured inverse covariance, and [57] operates under a general missing dependency assumption. These specific premises limit their applicability, rendering them less generalizable to our scenario. ", "page_idx": 29}, {"type": "text", "text": "Parallel to QUIC\u2019s applicability, Neighborhood Selection is similarly employed in contemporary studies, demonstrating its ongoing relevance and utility [58, 59, 60, 61]. ", "page_idx": 29}, {"type": "text", "text": "As mentioned above, we observed two main categories. The first category of recent works did not propose improvements to QUIC; instead, they applied it to more specific tasks. The second category, while outperforming QUIC as a baseline and demonstrating improvements, often relied on stricter structural assumptions not compatible with our problem context. Consequently, both categories were not suitable as baselines for our study. ", "page_idx": 29}, {"type": "text", "text": "We specifically chose Meta-IE method due to its closeness to our work. Meta-IE represents the latest advancement in meta-learning applied to sparse precision matrix estimation, making it an ideal candidate for direct comparison. ", "page_idx": 29}, {"type": "text", "text": "To this end, we have decided to incorporate an evaluation against gRankLasso [35], a recent development in graph learning, into our revised manuscript. With a completely tuning-free technique, gRankLasso shows better accuracy performance than CLIME, GLasso, and TIGER. However, its extensive time consumption for hyperparameter simulations and rank calculations poses a drawback, particularly underperforming with small datasets. ", "page_idx": 29}, {"type": "text", "text": "F Proof of Theorems ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Definition 2. Let X(1k ), $\\mathbf{X}_{1}^{(k)},\\ldots,\\mathbf{X}_{n_{k}}^{(k)}\\in\\mathbb{R}^{p}$ be i.i.d. random vectors for $1\\le k\\le K$ . Let $\\mathbf{X}_{i j}$ be the $j$ -th entry of $\\mathbf{X}_{i}^{(k)}$ for $1\\,\\le\\,j\\,\\le\\,p$ . We use $(\\Sigma^{(k)})^{*},(\\Omega^{(k)})^{*},\\theta^{*}$ to represent the real covariance matrix, precision matrix, and meta-knowledge, respectively. $\\{\\mathbf{X}^{(k)}\\}$ follows a family of random $p$ -dimensional multivariate sub-Gaussian distributions with parameter $\\sigma$ if ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "$(l)\\left\\{\\mathbf{X}_{i}^{(k)}\\right\\}_{1\\leq i\\leq n_{k},1\\leq k\\leq K}$ are conditionally independent given $\\{(\\Omega^{(k)})^{*}\\}_{k=1}^{K}$ ;   \n(2) $\\frac{\\mathbf{X}_{i j}^{(k)}}{\\sqrt{\\sigma_{j j}^{(k)}}}$ conditioned on $(\\Omega^{(k)})^{*}$ is sub-Gaussian with parameter $\\sigma$ for $1\\leq i\\leq n_{k},1\\leq j\\leq p,1\\leq$ $k\\leq K$ ;   \n(3) $\\begin{array}{r}{\\stackrel{\\cdots\\cdots}{\\mathrm{\\tiny~|\\mathbb{E}[}\\mathbf{X}_{i}^{(k)}|(\\Sigma^{(k)})^{*}]}=0,\\mathrm{Cov}\\left(\\mathbf{X}_{i}^{(k)}|(\\Sigma^{(k)})^{*}\\right)=\\left(\\Sigma^{(k)}\\right)^{*}=\\left((\\Omega^{(k)})^{*}\\right)^{-1}f o r\\,1\\le i\\le n_{k},1\\le k\\le1,}\\end{array}$ $K$ . ", "page_idx": 30}, {"type": "text", "text": "Then we remove the assumption on the support set of $\\theta_{r}^{(k)}$ , thus relaxing the data assumption of [11]: (4) $(\\Omega^{k})^{*}=\\theta^{*}+\\theta_{r}^{(k)}$ with $\\mathcal{I},\\theta_{r}^{(k)}\\in\\mathbb{R}^{p\\times p}$ . The meta-knowledge $\\theta$ is deterministic, and $\\theta_{r}^{(k)},1\\leq$ $k\\leq K$ , are i.i.d. random matrices. ", "page_idx": 30}, {"type": "text", "text": "Lemma 1. Given a simple optimization problem, $\\hat{x}$ and $\\hat{y}$ are the optimal solution. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\hat{x}+\\hat{y}=\\underset{x,y}{\\arg\\operatorname*{min}}|x|+\\rho|y|\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\rho>0$ and $C$ is a constant. Then we have ${\\hat{x}}\\cdot{\\hat{y}}\\geq0$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. If ${\\hat{x}}\\cdot{\\hat{y}}<0$ , let $x^{\\prime}=\\hat{x}+\\hat{y}$ and $y^{\\prime}=0$ . Without loss of generality, we assume that $|{\\hat{x}}|\\geq|{\\hat{y}}|$ . It follows that ", "page_idx": 30}, {"type": "equation", "text": "$$\n|x^{\\prime}|+\\rho|y^{\\prime}|=|\\hat{x}+\\hat{y}|<|\\hat{x}|<|\\hat{x}|+\\rho|\\hat{y}|,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which means that the pair $x^{\\prime},y^{\\prime}$ is a better solution. This contradicts the earlier stated premise that $\\hat{x},\\hat{y}$ are the optimal solution. The proof is completed by contradiction. ", "page_idx": 30}, {"type": "text", "text": "Lemma 2. If each $\\theta_{r}^{(k)}+\\theta$ satisfies (10), then $\\Omega_{\\mathrm{{tot}}}$ also satisfies (10). Thus $\\hat{\\Omega}_{\\mathrm{{tot}}}$ satisfies the condition $\\hat{\\Omega}_{\\mathrm{tot}}\\succ0$ , with a high probability. ", "page_idx": 30}, {"type": "text", "text": "Corollary 1. We assume that $\\hat{\\theta}$ is the optimal solution of Eq. (5) and ${\\hat{\\theta}}_{r}^{(k)}={\\hat{\\Omega}}^{(k)}-{\\hat{\\theta}}.$ . Based on Lemma $^{\\,l}$ , it follows that $\\hat{\\theta}_{r,i j}^{(k)}\\cdot\\hat{\\theta}_{i j}\\geq0$ . Then it is simple to see that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\Theta}_{r}\\|_{\\infty}\\leq\\|\\hat{\\Theta}_{r}\\|_{\\infty}+\\|\\hat{\\Theta}\\|_{\\infty}=\\|\\hat{\\Theta}_{r}+\\hat{\\Theta}\\|_{\\infty}=\\|\\hat{\\Omega}_{\\mathrm{tot}}\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Theorem 1. We use $\\hat{\\Theta},\\Theta^{*}$ to represent the estimated precision matrix and real precision matrix, respectively. Suppose that $\\Omega_{\\mathrm{tot}}^{*}\\in\\mathcal{U}$ . If $\\lambda\\geq\\nu(\\operatorname*{max}_{i j}\\left|\\hat{\\sigma}_{i j}-\\sigma_{i j}^{*}\\right|)$ , we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\hat{\\Theta}-\\Theta^{*}\\|_{\\infty}\\leq8\\nu\\lambda+2\\phi.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\nu=\\lVert\\Omega_{\\mathrm{tot}}^{*}\\rVert_{1}$ and $\\phi=\\|\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. According to the condition in Theorem 1, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\hat{\\Sigma}_{\\mathrm{tot}}-\\Sigma_{\\mathrm{tot}}^{*}\\|_{\\infty}\\leq\\frac{\\lambda}{\\|\\Omega_{\\mathrm{tot}}^{*}\\|_{1}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then we can show that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|I-\\hat{\\Sigma}_{\\mathrm{tot}}\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}=\\|(\\Sigma_{\\mathrm{tot}}^{*}-\\hat{\\Sigma}_{\\mathrm{tot}})\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}\\leq\\|\\Omega_{\\mathrm{tot}}^{*}\\|_{1}\\|{\\Sigma}_{\\mathrm{tot}}^{*}-\\hat{\\Sigma}_{\\mathrm{tot}}\\|_{\\infty}\\leq\\lambda,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $|\\mathbf{AB}|_{\\infty}\\leq|\\mathbf{A}|_{\\infty}|\\mathbf{B}|_{1}$ for any matrices $\\mathbf{A},\\mathbf{B}$ of appropriate sizes. ", "page_idx": 30}, {"type": "text", "text": "Then we can rewrite (5) as: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big(\\hat{\\Theta}_{r},\\hat{\\Omega}_{\\mathrm{tot}}\\Big)=\\underset{\\{\\theta_{r}^{(k)}\\},\\Omega_{\\mathrm{tot}}}{\\arg\\operatorname*{min}}}&{\\|\\Omega_{\\mathrm{tot}}\\|_{1}+(1-\\rho)\\|\\Theta_{r}\\|_{1}}\\\\ {\\mathrm{s.t.}}&{\\|\\Sigma_{\\mathrm{tot}}\\Omega_{\\mathrm{tot}}-I\\|_{\\infty}\\leq\\lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This is because $\\begin{array}{r}{\\|\\Omega_{\\mathrm{tot}}\\|_{1}+(1-\\rho)\\sum_{k=1}^{K}\\|\\theta_{r}^{(k)}\\|_{1}=\\sum_{k=1}^{K}\\|\\theta+\\theta_{r}^{(k)}\\|_{1}+(1-\\rho)\\sum_{k=1}^{K}\\|\\theta_{r}^{(k)}\\|_{1}}\\end{array}$ Notice that the constraint is not related to $\\Theta_{r}$ . Therefore $\\hat{\\Omega}_{\\mathrm{{tot}}}$ is also the solution of the following optimization problem: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\hat{\\Omega}_{\\mathrm{tot}}=\\arg\\operatorname*{min}_{\\Omega_{\\mathrm{tot}}}\\lvert|\\Omega_{\\mathrm{tot}}\\rvert|_{1}\\quad\\mathrm{s.t.}\\lVert\\Sigma_{\\mathrm{tot}}\\Omega_{\\mathrm{tot}}-I\\rVert_{\\infty}\\leq\\lambda.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\Omega_{\\mathrm{tot}}^{*}$ and $\\hat{\\Omega}_{\\mathrm{{tot}}}$ satisfy (32), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\lVert\\hat{\\Omega}_{\\mathrm{tot}}\\rVert_{1}\\leq\\lVert\\Omega_{\\mathrm{tot}}^{*}\\rVert_{1}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "It suggests that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\Sigma}_{\\mathrm{tot}}(\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*})\\|_{\\infty}\\leq\\|\\hat{\\Sigma}_{\\mathrm{tot}}\\hat{\\Omega}_{\\mathrm{tot}}-I\\|_{\\infty}+\\|I-\\hat{\\Sigma}_{\\mathrm{tot}}\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}\\leq2\\lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Consequently, it is straightforward to show that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Sigma_{\\mathrm{tot}}^{*}(\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*})\\|_{\\infty}\\leq\\|\\hat{\\Sigma}_{\\mathrm{tot}}(\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*})\\|_{1}+\\|(\\hat{\\Sigma}_{\\mathrm{tot}}-\\Sigma_{\\mathrm{tot}}^{*})(\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*})\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\lambda+\\|\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*}\\|_{1}\\|\\hat{\\Sigma}_{\\mathrm{tot}}-\\Sigma_{\\mathrm{tot}}^{*}\\|_{\\infty}\\leq4\\lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, it follows that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}\\leq\\|\\Omega_{\\mathrm{tot}}^{*}\\|_{1}\\|\\Sigma_{\\mathrm{tot}}^{*}(\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*})\\|_{\\infty}\\leq4\\|\\Omega_{\\mathrm{tot}}^{*}\\|_{1}\\lambda=4\\nu\\lambda,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\Vert\\Omega_{\\mathrm{tot}}^{*}\\Vert_{1}=\\nu$ . Based on the triangle inequality for norm, we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lVert\\hat{\\Theta}-\\Theta^{*}\\rVert_{\\infty}=\\lVert\\hat{\\Omega}_{\\mathrm{tot}}-\\hat{\\Theta}_{r}-(\\Omega_{\\mathrm{tot}}^{*}-\\Theta_{r}^{*})\\rVert_{\\infty}}&{}\\\\ {\\leq\\lVert\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*}\\rVert_{\\infty}+\\lVert\\hat{\\Theta}_{r}-\\Theta_{r}^{*}\\rVert_{\\infty}}&{}\\\\ {\\leq\\lVert\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*}\\rVert_{\\infty}+\\lVert\\hat{\\Theta}_{r}\\rVert_{\\infty}+\\lVert\\Theta_{r}^{*}\\rVert_{\\infty}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In terms of $\\Theta^{*},\\Theta_{r}^{*}$ , we assume $W_{\\mathrm{tot},i j}^{*}\\cdot\\Omega_{I,i j}^{*}=0$ and $W_{\\mathrm{tot},i j}^{*}+\\Omega_{I,i j}^{*}=0$ iff $W_{\\mathrm{tot},i j}^{*}=\\Omega_{I,i j}^{*}=0$ Thus we can derive that $\\lVert\\Theta^{*}\\rVert_{\\infty}\\leq\\lVert\\Omega_{\\mathrm{tot}}^{*}\\rVert_{\\infty}$ . We are now turning to the proof of Eq. (37). Combining Corollary 1 and Theorem 1, we can show that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{\\Theta}-\\Theta^{*}\\|_{\\infty}\\leq\\|\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}+\\|\\hat{\\Omega}_{\\mathrm{tot}}\\|_{\\infty}+\\|\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\|\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}+\\|\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}+2\\|\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\leq2\\|\\hat{\\Omega}_{\\mathrm{tot}}-\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}+2\\|\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\leq8\\|\\Omega_{\\mathrm{tot}}^{*}\\|_{1}\\lambda+2\\|\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}}\\\\ &{\\qquad\\qquad=8\\nu\\lambda+2\\phi,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\|\\Omega_{\\mathrm{tot}}^{*}\\|_{\\infty}=\\phi$ . The proof is completed. ", "page_idx": 31}, {"type": "text", "text": "Theorem 2. Suppose that $\\Omega_{\\mathrm{tot}}^{*}\\in\\mathcal{U}$ and $\\begin{array}{r}{N=\\sum_{k=1}^{K}n_{k}}\\end{array}$ . When the variables are sub-Gaussian, the tail condition holds. Let \u03bb = C0\u03bd log(NK , where the constant $C_{0}=2\\gamma^{-2}(2+\\tau_{0}+\\gamma^{-1}e^{2}C^{2})^{2}$ and constant $\\tau_{0}>0$ . If $\\omega_{\\mathrm{min}}^{\\mathrm{tot}}>2\\tau_{n}$ , then we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I.\\ \\ \\|\\hat{\\theta}-\\theta^{*}||_{\\infty}\\leq8C_{0}\\nu^{2}\\sqrt{\\frac{\\log(K p)}{N}}+2\\phi;}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$\\operatorname{supp}(\\tilde{\\theta})=\\operatorname{supp}(\\theta^{*}),$ ", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with a probability greater than $1-4p^{-\\tau_{0}}$ . Here $\\tilde{\\theta}$ is a threshold estimator with $\\tilde{\\theta}_{i j}=\\hat{\\theta}_{i j}I(|\\hat{\\theta}_{i j}|\\geq\\tau_{n})$ , where $\\tau_{n}\\geq4\\nu\\lambda$ is a tuning parameter. ", "page_idx": 31}, {"type": "text", "text": "Proof. To obtain the first conclusion of Theorem 2, we need to prove ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i j}|\\hat{\\sigma}_{i j}-\\sigma_{i j}^{*}|\\le C_{0}\\sqrt{\\frac{\\log K p}{N}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with a probability greater than $1\\mathrm{~-~}4p^{-\\tau_{0}}$ under the exponential tail condition. Without loss of generality, we assume that $\\mathbb{E}(\\mathbf{X}^{(k)})\\;=\\;0$ . Let $\\begin{array}{r}{\\Sigma_{\\mathrm{tot}}^{\\prime}\\;:=\\;N^{-1}\\sum_{k=1}^{N}\\mathbf{X}_{\\mathrm{tot},k}\\mathbf{X}_{\\mathrm{tot},k}^{\\top}}\\end{array}$ and ${\\bf Y}_{k i j}\\;=\\;$ $\\mathbf{X}_{\\mathrm{tot},k i}\\mathbf{X}_{\\mathrm{tot},k j}-\\mathbb{E}(\\mathbf{X}_{\\mathrm{tot},k i}\\mathbf{X}_{\\mathrm{tot},k j})$ . We then have $\\hat{\\Sigma}_{\\mathrm{tot}}=\\Sigma_{\\mathrm{tot}}^{\\prime}-\\bar{\\mathbf{X}}_{\\mathrm{tot}}\\bar{\\mathbf{X}}_{\\mathrm{tot}}^{\\top}$ . Let $t=\\gamma\\sqrt{\\log K p/N}$ . Using the inequality $|e^{s}-1-s|<s^{2}e^{\\operatorname*{max}(s,0)}$ for any $s\\in\\mathbb R$ and let $C_{1}=2+\\tau_{0}+\\gamma^{-1}C^{2}$ . Then by some basic calculations, we can get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\sum_{k=1}^{N}\\mathbf{Y}_{k i j}\\geq\\gamma^{-1}C_{1}\\sqrt{N\\log K p}\\right)\\leq\\exp(-C_{1}\\log K p)(\\mathbb{E}[\\exp(t\\mathbf{Y}_{k i j})])^{N}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\exp(-C_{1}\\log K p+N t^{2}\\mathbb{E}Y_{k i j}^{2}\\exp(t|\\mathbf{Y}_{k i j}|))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\exp(-C_{1}\\log K p+\\gamma^{-1}C^{2}\\log K d)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\exp(-(\\tau_{0}+2)\\log K p).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|\\Sigma_{\\mathrm{tot}}^{\\prime}-\\Sigma_{\\mathrm{tot}}^{*}\\|_{\\infty}\\geq\\gamma^{-1}C_{1}\\sqrt{\\frac{\\log K p}{N}}\\right)\\leq2p^{-\\tau_{0}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using the inequality $e^{s}\\leq e^{s^{2}+1}$ for $s>0$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}e^{t|X_{j}|}\\leq e C,\\;\\forall t\\leq\\sqrt{\\gamma}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let ", "page_idx": 32}, {"type": "equation", "text": "$$\nC_{2}=2+\\tau_{0}+\\gamma^{-1}e^{2}C^{2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and $a_{n}=C_{2}^{2}(\\log K p/N)^{\\frac{1}{2}}$ . As before, we can have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\|\\bar{\\mathbf{X}}_{\\mathrm{tot}}\\bar{\\mathbf{X}}_{\\mathrm{tot}}^{\\top}\\|_{\\infty}\\geq\\gamma^{-2}a_{n}\\sqrt{\\log K p/N}\\right)\\leq\\!p\\operatorname*{max}_{i}\\mathbb{P}\\left(\\displaystyle\\sum_{k=1}^{N}\\mathbf{X}_{k i}\\geq\\gamma^{-1}C_{2}\\sqrt{N\\log K p}\\right)\\quad}\\\\ {\\quad+\\,p\\operatorname*{max}_{i}\\mathbb{P}\\left(-\\displaystyle\\sum_{k=1}^{N}\\mathbf{X}_{k i}\\geq\\gamma^{-1}C_{2}\\sqrt{N\\log K p}\\right)\\quad}\\\\ {\\leq\\!2p^{-\\tau_{0}-1}\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By (41), (44), and the inequality $C>\\gamma^{-1}C_{1}+\\gamma^{-2}a_{n}$ , we see that (39) holds. The proof of the first inequality is completed. ", "page_idx": 32}, {"type": "text", "text": "To prove the sign-consistency of $\\hat{\\Theta}$ , we define a threshold estimator $\\tilde{\\Omega}_{\\mathrm{tot}}$ with $\\tilde{\\omega}_{i j}=\\hat{\\omega}_{i j}I\\{|\\hat{\\omega}_{i j}|\\geq\\tau_{n}\\}$ where $\\tau_{n}\\geq4\\nu\\lambda$ is a tuning parameter. We define $\\omega_{\\mathrm{min}}^{\\mathrm{tot}}=\\operatorname*{min}_{(i,j)\\in\\mathrm{supp}(\\Omega_{\\mathrm{tot}}^{*})}|\\bar{\\Omega}_{\\mathrm{tot,ij}}^{*}|$ and a threshold estimator $\\Tilde{\\Theta}$ with $\\tilde{\\Theta}_{i j}=\\hat{\\Theta}_{i j}I\\{|\\hat{\\Theta}_{i j}|\\geq\\tau_{n}\\}$ , where $\\tau\\geq4\\nu\\lambda$ . It is known that the resulting elements in $\\hat{\\Omega}_{\\mathrm{tot}}$ will exceed the threshold level if the corresponding element in $\\Omega_{\\mathrm{tot}}^{*}$ is large in magnitude. In contrast, the entries of $\\hat{\\Omega}_{\\mathrm{{tot}}}$ outside the support of $\\Omega_{\\mathrm{tot}}^{*}$ will remain below the threshold level with high probability. As a result, if $\\omega_{\\mathrm{min}}^{\\mathrm{tot}}>2\\tau_{n}$ , we have $\\mathrm{supp}(\\tilde{\\Omega}_{\\mathrm{tot}})=\\mathrm{supp}(\\Omega_{\\mathrm{tot}}^{*})$ . Since the nonzero entries in $\\theta$ correspond to the intersection of the nonzero entries in $\\{\\Omega^{(k)}\\}$ , it is straightforward to get that $\\mathrm{supp}(\\tilde{\\Theta})=\\mathrm{supp}(\\Theta^{\\ast})$ if $\\omega_{\\mathrm{min}}^{\\mathrm{tot}}>2\\tau_{n}$ . ", "page_idx": 32}, {"type": "text", "text": "Lemma 3. Consider a zero-mean random vector $(\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{p})$ with covariance $\\Sigma^{*}$ such that each $\\frac{\\mathbf{X}_{i}}{\\sqrt{\\Sigma_{i i}^{*}}}$ is sub-Gaussian with parameter $\\sigma$ . We define $W=\\hat{\\Sigma}_{\\mathrm{new}}-\\Sigma_{\\mathrm{new}}^{*}$ . Given n i.i.d. samples, the associated sample covariance $\\hat{\\Sigma}$ satisfies the tail bound i $\\begin{array}{r}{f\\left\\lVert\\sum_{\\mathrm{new}}^{*}\\right\\rVert_{\\infty}\\leq\\varphi.}\\end{array}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\Vert W\\Vert_{\\infty}>\\delta\\right]\\leq4|\\mathcal{G}|\\exp\\left\\{-\\frac{n\\delta^{2}}{128(1+4\\sigma^{2})^{2}\\varphi^{2}}\\right\\}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for all $\\delta\\in(0,8\\varphi(1+4\\sigma^{2}))$ . ", "page_idx": 32}, {"type": "text", "text": "Theorem 3. Suppose we have recovered the true sparsity pattern $\\mathcal{G}$ of a family of $d$ -dimensional random multivariate sub-Gaussian distribution. Then for a new task of multivariate sub-Gaussian distribution with the precision matrix $\\Omega_{\\mathrm{new}}^{\\ast}$ such that $\\mathrm{supp}(\\theta)\\subseteq\\mathrm{supp}(\\Omega_{\\mathrm{new}}^{*})$ and satisfying irrepresentable condition, consider the estimator $\\hat{\\Omega}_{\\mathrm{new}}$ with $\\begin{array}{r}{\\lambda=C_{3}\\nu\\sqrt{\\frac{\\log|\\mathcal{G}|}{n}}.}\\end{array}$ logn |G|. Then we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\hat{\\Omega}_{\\mathrm{new}}-\\Omega_{\\mathrm{new}}^{*}\\|_{\\infty}\\leq2C_{3}\\nu^{2}\\kappa_{\\Gamma^{*}}\\sqrt{\\frac{\\log|\\mathcal{G}|}{n}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with a probability $1-p^{-\\tau_{1}}$ , where $\\tau_{1}>0$ . ", "page_idx": 32}, {"type": "text", "text": "The proof is similar to Theorem 2 and will be omitted. ", "page_idx": 32}, {"type": "text", "text": "Definition 3. $[32]$ Given a matrix $M\\in\\mathbb{S}^{p}$ , define $\\mathcal{G}_{M}=\\{(i,j):M_{i j}\\neq0$ as its sparsity pattern. Then $M$ is called inverse-consistent if there exists a matrix $N\\in\\mathbb{S}^{p}$ such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{M+N\\succeq0}\\\\ {N=0\\quad\\forall(i,j)\\in\\mathcal{G}_{M}}\\\\ {(M+N)^{-1}\\in\\mathbb{S}_{\\mathcal{G}_{M}}^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The matrix $N$ is called an inverse-consistent complement of $M$ and is denoted by $M^{(c)}$ . Furthermore, $M$ is called sign-consistent if for every $(i,j)\\in\\mathcal{G}_{M}$ , the $(i,j)$ -th elements of $M$ and $(M+M^{(c)})^{-1}$ have opposite signs. ", "page_idx": 33}, {"type": "text", "text": "Then we define the $\\beta(\\mathcal{G},\\alpha)$ function defined with respect to the sparsity pattern $\\mathcal{G}$ and scalar $\\alpha>0$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\beta(\\mathcal{G},\\alpha)=\\underset{M\\succ0}{\\operatorname*{max}}}&{\\|M^{(c)}\\|_{\\operatorname*{max}}}\\\\ {\\mathrm{s.t.}}&{M\\in\\mathbb{S}_{\\mathcal{G}}^{p}\\mathrm{~and~}\\|M\\|_{\\operatorname*{max}}\\leq\\alpha}\\\\ &{M_{i,i}=1\\quad\\forall i\\in\\{1,\\ldots,p\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma 4. $[32]$ Any arbitrary matrix with positive-definite completion is inverse-consistent and has a unique inverse-consistent complement. ", "page_idx": 33}, {"type": "text", "text": "Lemma 5. $\\hat{\\Omega}$ is the optimal solution if and only if it satisfies the following conditions for every $(i,j)\\in\\{1,\\ldots,p\\}^{2}$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\Omega}_{i j}^{-1}=\\Sigma_{i j}\\quad i f\\,i=j}\\\\ &{\\bar{\\Omega}_{i j}^{-1}=\\Sigma_{i j}+\\eta\\times\\mathrm{sign}(\\bar{\\Omega}_{i j})\\quad i f\\,\\bar{\\Omega}_{i j}\\neq0}\\\\ &{\\Sigma_{i j}-\\eta\\leq\\bar{\\Omega}_{i j}^{-1}\\leq\\Sigma_{i j}+\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. The proof is straightforward and omitted for brevity. ", "page_idx": 33}, {"type": "text", "text": "Then, consider the following optimization problem: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Omega\\in\\mathbb{S}_{+}^{d}}-\\log\\operatorname*{det}(\\Omega)+\\langle\\tilde{\\Sigma},\\Omega\\rangle+\\sum_{(i,j)\\in\\mathcal{G}}\\tilde{\\eta}|\\Omega_{i j}|+2\\operatorname*{max}_{k}\\Sigma_{k k}\\sum_{(i,j)\\in\\mathcal{G}^{c}}|\\Omega_{i j}|\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\tilde{\\Sigma}_{i j}=\\frac{\\Sigma_{i j}}{\\sqrt{\\Sigma_{i i}\\times\\Sigma_{j j}}},\\;\\tilde{\\eta}=\\frac{\\eta}{\\sqrt{\\Sigma_{i i}\\times\\Sigma_{j j}}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We denote $\\tilde{\\Omega}$ as the optimal solution of (49) and define $D$ as a diagonal matrix with $D_{i i}=\\Sigma_{i i}$ for every $i=1,\\hdots,p$ . ", "page_idx": 33}, {"type": "text", "text": "Lemma 6. We have $\\bar{\\Omega}=D^{-\\frac{1}{2}}\\times\\tilde{\\Omega}\\times D^{-\\frac{1}{2}}$ . ", "page_idx": 33}, {"type": "text", "text": "Theorem 4. $\\mathcal{G}$ coincides with the sparsity pattern of the optimal solution $\\Omega_{\\mathrm{new}}^{\\ast}$ if the normalized matrix $\\tilde{\\Sigma}_{\\mathrm{new}}=D^{-1/2}\\Pi_{\\mathcal{G}}(\\Sigma_{\\mathrm{new}})D^{-1/2}$ where $D=\\mathrm{diag}(\\Pi_{\\mathcal{G}}(\\Sigma_{\\mathrm{new}}))$ satisfies the following conditions: ", "page_idx": 33}, {"type": "text", "text": "1. $\\tilde{\\Sigma}_{\\mathrm{new}}$ is positive definite,   \n2. $\\tilde{\\Sigma}_{\\mathrm{new}}$ is sign-consistent, ", "page_idx": 33}, {"type": "text", "text": "3. We have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta\\left(\\mathcal{G},\\Vert\\tilde{\\Sigma}_{\\mathrm{new}}\\Vert_{\\mathrm{max}}\\right)}\\\\ &{\\leq\\underset{(k,l)\\not\\in\\mathcal{G}}{\\operatorname*{min}}\\frac{\\eta-\\vert\\left(\\Pi_{\\mathcal{G}}\\left(\\Sigma_{\\mathrm{new}}\\right)\\right)_{k l}\\vert}{\\sqrt{\\left(\\Pi_{\\mathcal{G}}\\left(\\Sigma_{\\mathrm{new}}\\right)\\right)_{k k}\\cdot\\left(\\Pi_{\\mathcal{G}}\\left(\\Sigma_{\\mathrm{new}}\\right)\\right)_{l l}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. We define an intermediate optimization problem as the following: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Omega^{\\dagger}=\\underset{\\Omega\\in\\mathbb{S}_{+}^{p}}{\\mathrm{arg}\\,\\mathrm{min}}f(\\Omega)=-\\log\\operatorname*{det}\\Omega+\\langle\\Sigma,\\Omega\\rangle+\\sum_{(i,j)\\in\\mathcal{G}}\\lambda_{i j}|\\Omega_{i j}|+2\\operatorname*{max}_{k}\\sum_{k}\\sum_{(i,j)\\in\\mathcal{G}^{c}}|\\Omega_{i j}|.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "First, we show that $\\Omega^{\\dagger}=\\bar{\\Omega}$ . Trivially, $\\bar{\\Omega}$ is a feasible solution for (51) and hence $f(\\Omega^{\\dagger})\\leq f(\\bar{\\Omega})$ . Now, we prove that $\\Omega^{\\dagger}$ is a feasible solution. We show that $\\Omega_{i j}^{\\dagger}\\,=\\,0$ for every $(i,j)\\,\\in\\,\\mathcal{G}^{c}$ . By contradiction, suppose $\\Omega_{i j}^{\\dagger}\\neq0$ for some $(i,j)\\in\\mathcal{G}^{c}$ . Note that, due to the positive definiteness of $(\\Omega^{\\dagger})^{-1}$ , we obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n(\\Omega^{\\dagger})_{i i}^{-1}\\times(\\Omega^{\\dagger})_{j j}^{-1}-((\\Omega^{\\dagger})_{i j}^{-1})^{2}>0.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, based on Lemma 6, it follows that ", "page_idx": 34}, {"type": "equation", "text": "$$\n(\\Omega^{\\dagger})_{i j}^{-1}=\\Sigma_{i j}+2\\operatorname*{max}_{k}\\Sigma_{k k}\\times\\mathrm{sign}(\\Omega_{i j}^{\\dagger}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Considering the fact that $\\Sigma\\succ0$ , we have $|\\Sigma_{i j}|\\leq\\operatorname*{max}_{k}\\Sigma_{k k}$ . It implies that $|(\\Omega^{\\dagger})_{i j}^{-1}|\\geq\\operatorname*{max}_{k}\\Sigma_{k k}$ . Furthermore, due to Lemma 5, we have $(\\Omega^{\\dagger})_{i i}^{-1}=\\Omega_{i i}$ and $(\\Omega^{\\dagger})_{j j}^{-1}=\\Omega_{j j}$ . This leads to ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left(\\Omega^{\\dagger}\\right)_{i i}^{-1}\\times\\left(\\Omega^{\\dagger}\\right)_{j j}^{-1}-\\left(\\left(\\Omega^{\\dagger}\\right)_{i j}^{-1}\\right)^{2}=\\Sigma_{i i}\\times\\Sigma_{j j}-\\left(\\operatorname*{max}_{k}\\left\\{\\Sigma_{k k}\\right\\}\\right)^{2}\\leq0,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which contradicts with (52). Therefore, $\\Omega^{\\dagger}$ is a feasible solution. This implies that $f(\\Omega^{\\dagger})\\geq f(\\bar{\\Omega})$ and hence, $f(\\bar{\\Omega})=f(\\Omega^{\\dagger})$ . Due to the uniqueness of the solution of (51), we have $\\bar{\\Omega}=\\Omega^{\\dagger}$ . Now, note that (51) can be reformulated as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Omega\\in\\mathbb{S}_{+}^{p}}-\\log\\operatorname*{det}(\\Omega)+\\langle\\tilde{\\Sigma},D^{1/2}\\Omega D^{1/2}\\rangle+\\sum_{(i,j)\\in\\mathcal{G}}\\eta\\,|\\Omega_{i j}|+2\\operatorname*{max}_{k}\\Sigma_{k,k}\\sum_{(i,j)\\in\\mathcal{G}^{c}}|\\Omega_{i j}|\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Upon defining ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tilde{\\Omega}=D^{1/2}\\Omega D^{1/2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and following some algebra, one can verify that (51) is equivalent to ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tilde{\\Omega}\\in\\mathbb{S}_{+}^{d}}-\\log\\operatorname*{det}(\\tilde{\\Omega})+\\langle\\tilde{\\Sigma},\\tilde{\\Omega}\\rangle+\\sum_{(i,j)\\in\\mathcal{G}}\\tilde{\\eta}\\left|\\tilde{\\Omega}_{i j}\\right|+2\\operatorname*{max}_{k}\\left\\{\\tilde{\\Sigma}_{k k}\\right\\}\\sum_{(i,j)\\in\\mathcal{G}^{(c)}}\\left|\\tilde{\\Omega}_{i j}\\right|+\\log\\operatorname*{det}(D).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Dropping the constant term in (57) gives rise to the (49). As a result, $\\bar{\\Omega}=D^{-\\frac{1}{2}}\\times\\tilde{\\Omega}\\times D^{-\\frac{1}{2}}$ holds.   \nThe proof is completed. ", "page_idx": 34}, {"type": "text", "text": "Next, we prove that the newton subproblem can be solved in $O(1)$ CG iteration. Let $\\mathbb{S}^{p}$ be the set of $p\\times p$ real symmetric matrices. Given a sparsity pattern $V$ , we define $\\mathbb{S}_{V}^{p}\\subseteq\\mathbb{S}$ as the set of $p\\times p$ real symmetric matrices with this sparsity pattern. We consider the following minimization problem ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\hat{y}\\equiv\\arg\\operatorname*{min}_{y\\in\\mathbb{R}^{m}}l(y)\\equiv g_{*}(\\Sigma-P(y)).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Here, the problem data $P:\\mathbb{R}^{m}\\to\\mathbb{S}_{F}^{p}$ is an orthogonal basis for a sub-sparsity pattern $F\\subset V$ that excludes the matrix $\\Sigma$ . In other words, the operator $A$ satisfies ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{c}{P(P^{\\top}(\\Omega))=P_{F}(\\Omega),\\quad\\forall\\Omega\\in S_{V}^{p},}\\\\ {P^{\\top}(\\Sigma)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The penalty function $g_{*}$ is the convex conjugate of the \"log-det\" barrier function on $\\mathbb{S}_{V}^{p}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\ng_{*}(S)=-\\operatorname*{min}_{\\Omega\\in\\mathbb{S}_{V}^{p}}\\left\\{S\\bullet\\Omega-\\log\\operatorname*{det}\\Omega\\right\\}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Assuming that $V$ is chordal, the function $g_{*}(S)$ , its gradient $\\nabla g_{*}(S)$ , and its Hessian matrix-vector product ${\\bar{\\nabla}}^{2}g_{*}(S)[Y]$ can all be evaluated in closed-form [62, 63, 52]; see also [64]. Furthermore, if the pattern is sparse, i.e. its number of elements in the pattern satisfies $|V|=O(p)$ , then all of these operations can be performed to arbitrary accuracy in $O(p)$ time and memory. ", "page_idx": 34}, {"type": "text", "text": "It is standard to solve 58 using Newton\u2019s method. Starting from some initial point $y_{1}\\in\\operatorname{dom}g$ ", "page_idx": 34}, {"type": "equation", "text": "$$\ny_{k+1}=y_{k}+\\alpha_{k}\\Delta y_{k}\\quad\\Delta y_{k}\\equiv-\\nabla^{2}l\\left(y_{k}\\right)^{-1}\\nabla l\\left(y_{k}\\right),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "in which the step-size $\\alpha_{k}$ is determined by backtracking line search, selecting the first instance of the sequence $\\{1,\\bar{\\rho,\\rho^{2}},\\rho^{3},\\ldots\\}$ that satisfies the Armijo-Goldstein condition ", "page_idx": 34}, {"type": "equation", "text": "$$\nl(y+\\alpha\\Delta y)\\leq l(y)+\\gamma\\alpha\\Delta y^{\\top}\\nabla l(y),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "in which $\\gamma\\in(0,0.5)$ and $\\rho\\in(0,1)$ . The function $g_{*}$ is strongly self-concordant, and $l$ inherits this property from $g_{*}$ . Accordingly, classical analysis shows that we require at most ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left[\\frac{l\\left(y_{1}\\right)-l(\\hat{y})}{0.05\\gamma\\rho}+\\log\\log(1/\\epsilon)\\right]\\approx O(1)\\;\\mathrm{Newton\\;steps}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "to an $\\epsilon$ -optimal point satisfying $l\\left(y_{k}\\right)-l(\\hat{y})\\leq\\epsilon$ . The bound is very pessimistic, and in practice, no more than 20-30 Newton steps are ever needed for convergence. ", "page_idx": 35}, {"type": "text", "text": "The most computationally expensive of Newton\u2019s method is the solution of the Newton direction $\\Delta y$ via the $m\\times m$ system of equations ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\nabla^{2}l\\left(y_{k}\\right)\\Delta y=-\\nabla l\\left(y_{k}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The main result in this section is a proof that the condition number of $\\nabla^{2}l(y)$ is independent of the problem dimension $n$ . ", "page_idx": 35}, {"type": "text", "text": "Theorem 5. At any $y$ satisfying $l(y)\\leq l\\left(y_{1}\\right)$ and $\\nabla l(y)^{\\top}\\left(y-y_{1}\\right)\\leq\\phi_{\\operatorname*{max}},$ , the condition number $\\kappa_{l}$ of the Hessian matrix $\\nabla^{2}l(y)$ is bound ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\kappa_{l}\\leq4\\left(1+\\frac{\\phi_{\\operatorname*{max}}^{2}\\lambda_{\\operatorname*{max}}\\left(\\Omega_{0}\\right)}{\\lambda_{\\operatorname*{min}}\\left(\\hat{\\Omega}\\right)}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where: ", "page_idx": 35}, {"type": "text", "text": "\u2022 $\\phi_{\\mathrm{max}}=l\\left(y_{1}\\right)-l(\\hat{y})$ is the suboptimality of the initial point, \u2022 $\\mathbf{A}=\\left[\\mathrm{vec}\\,A_{1},\\ldots,\\mathrm{vec}\\,A_{m}\\right]$ is the vectorized version of the problem data, \u2022 $\\Omega_{0}=-\\nabla f_{*}\\left(S_{0}\\right)$ and $S_{0}=\\Sigma-A\\left(y_{0}\\right)$ are the initial primal-dual pair, \u2022 \u2126\u02c6= \u2212\u2207f\u2217( S\u02c6) and S\u02c6 = \u03a3 \u2212A(y\u02c6) are the solution primal-dual pair. ", "page_idx": 35}, {"type": "text", "text": "As a consequence, each Newton direction can be computed in $O\\left(\\log\\epsilon^{-1}\\right)$ iterations using conjugate gradients, over $O\\left(\\log\\log\\epsilon^{-1}\\right)$ total Newton steps. The overall minimization problem is solved to $\\epsilon$ -accuracy in ", "page_idx": 35}, {"type": "equation", "text": "$$\nO\\left(\\log\\epsilon^{-1}\\log\\log\\epsilon^{-1}\\right)\\approx O(1)\\mathrm{CG~iterations}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The leading constant here is dependent polynomially on the problem data and the quality of the initial point, but independent of the problem dimensions. ", "page_idx": 35}, {"type": "text", "text": "Proof To prove the first bound (61), we will instead prove ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{\\operatorname*{max}}(\\Omega)}{\\lambda_{\\operatorname*{min}}(\\Omega)}\\leq2+\\frac{2\\phi_{\\operatorname*{max}}^{2}\\lambda_{\\operatorname*{max}}\\left(\\Omega_{0}\\right)}{\\lambda_{\\operatorname*{min}}(\\hat{\\Omega})},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which yields the desired condition number bound on $\\nabla^{2}g(y)$ . Writing $\\lambda_{1}=\\lambda_{\\mathrm{max}}(\\Omega)$ and $\\lambda_{n}=$ $\\lambda_{\\mathrm{min}}(\\Omega)$ , we have from the two lemmas above: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi_{\\operatorname*{max}}\\geq\\lambda_{\\operatorname*{min}}(\\hat{\\Omega})\\left(\\sqrt{\\lambda_{n}^{-1}}-\\sqrt{\\lambda_{1}^{-1}}\\right)^{2}>0,}\\\\ {2\\phi_{\\operatorname*{max}}\\geq\\lambda_{\\operatorname*{min}}\\left(\\Omega_{0}^{-1}\\right)\\left(\\sqrt{\\lambda_{1}}-\\sqrt{\\lambda_{n}}\\right)^{2}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Multiplying the two upper-bounds and substituing $\\lambda_{\\operatorname*{min}}\\left(\\Omega_{0}^{-1}\\right)=1/\\lambda_{\\operatorname*{max}}\\left(\\Omega_{0}\\right)$ yields ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{2\\phi_{\\operatorname*{max}}^{2}\\lambda_{\\operatorname*{max}}\\left(\\Omega_{0}\\right)}{\\lambda_{\\operatorname*{min}}(\\hat{\\Omega})}\\geq\\left(\\sqrt{\\frac{\\lambda_{1}}{\\lambda_{n}}}-\\sqrt{\\frac{\\lambda_{n}}{\\lambda_{1}}}\\right)^{2}=\\frac{\\lambda_{1}}{\\lambda_{n}}+\\frac{\\lambda_{n}}{\\lambda_{1}}-2\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Finally, bounding $\\lambda_{n}/\\lambda_{1}\\ge0$ yields (62). ", "page_idx": 36}]