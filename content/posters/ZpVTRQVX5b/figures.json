[{"figure_path": "ZpVTRQVX5b/figures/figures_0_1.jpg", "caption": "Figure 1: Overview of our speech-to-speech translation framework: 1) Joint encoder-decoder model for translating speech into the target text, and coarse-grained speech tokens, Co; 2) Non-autoregressive acoustic model for acoustic details, C0:16; 3) Codec model to convert discrete speech tokens back to the waveform. Abbreviation: S/A/I(Semantic/Acoustic/Isochrony Information), Co/C0:16(Codec layer 0/0-15), S/A-Enc(Semantic/Acoustic Encoder), ICM(Isochrony Control Module).", "description": "This figure presents a high-level overview of the TransVIP speech-to-speech translation framework.  The framework consists of three main modules: 1) A joint encoder-decoder model that processes the input speech to produce both text and a sequence of coarse-grained speech tokens; 2) a non-autoregressive acoustic model that refines these tokens using acoustic details; and 3) a codec model that converts the refined tokens back into the final speech waveform.  The figure highlights the flow of information and the roles of different components in preserving speaker voice characteristics and isochrony during the translation process.", "section": "1 Introduction"}, {"figure_path": "ZpVTRQVX5b/figures/figures_2_1.jpg", "caption": "Figure 2: The illustration of the training framework of the Joint Enc-Dec Model. During the training, the losses from the target speech clip, i.e., a sub-part of the whole target speech, which serves as a prompt, are not aggregated when computing the Cross-Entropy (CE) loss. The corresponding codec labels are masked in the implementation. The semantic encoder and the auto-regressive decoder are initialized by a SeamlessM4T X2T model 4. The semantic encoder is frozen during training. In inference, all the target speech input are replaced by source speech input.", "description": "This figure illustrates the training framework of the Joint Encoder-Decoder model.  It shows how the model processes source speech/text and target speech to generate the target speech. During training, only a sub-part of the target speech (a clip) is used as a prompt, and the loss is calculated only on that portion.  The semantic encoder is pretrained and frozen during training, while the decoder is trained to generate text and codec tokens. During inference, the target speech prompt is replaced with source speech. ", "section": "3.2 Consecutive Generation with Joint Inference"}, {"figure_path": "ZpVTRQVX5b/figures/figures_15_1.jpg", "caption": "Figure 1: Overview of our speech-to-speech translation framework: 1) Joint encoder-decoder model for translating speech into the target text, and coarse-grained speech tokens, Co; 2) Non-autoregressive acoustic model for acoustic details, C0:16; 3) Codec model to convert discrete speech tokens back to the waveform. Abbreviation: S/A/I(Semantic/Acoustic/Isochrony Information), Co/C0:16(Codec layer 0/0-15), S/A-Enc(Semantic/Acoustic Encoder), ICM(Isochrony Control Module).", "description": "This figure illustrates the TransVIP speech-to-speech translation framework, which consists of three main components: a joint encoder-decoder model for text translation and generating coarse speech tokens; a non-autoregressive acoustic model for refining the acoustic details; and a codec model for converting discrete tokens into waveforms.  The diagram shows the flow of information from speech input to speech output, highlighting the role of different components in preserving speaker voice characteristics and isochrony.", "section": "Abstract"}]