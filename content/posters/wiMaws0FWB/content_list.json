[{"type": "text", "text": "Implicit Bias of Mirror Flow on Separable Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Scott Pesme Radu-Alexandru Dragomir Nicolas Flammarion EPFL T\u00e9l\u00e9com Paris EPFL ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "page_idx": 0}, {"type": "text", "text": "We examine the continuous-time counterpart of mirror descent, namely mirror flow, on classification problems which are linearly separable. Such problems are minimised \u2018at infinity\u2019 and have many possible solutions; we study which solution is preferred by the algorithm depending on the mirror potential. For exponential tailed losses and under mild assumptions on the potential, we show that the iterates converge in direction towards a $\\phi_{\\infty}$ -maximum margin classifier. The function $\\phi_{\\infty}$ is the horizon function of the mirror potential and characterises its shape \u2018at infinity\u2019. When the potential is separable, a simple formula allows to compute this function. We analyse several examples of potentials and provide numerical experiments highlighting our results. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Heavily over-parametrised yet barely regularised neural networks can easily perfectly fit a noisy training set while still performing very well on unseen data [Zhang et al., 2017]. This statistical phenomenon is surprising since it is known that there exists interpolating solutions which have terrible generalisation performances [Liu et al., 2020]. To understand this benign overfitting, it is essential to take into account the training algorithm. If overfitting is indeed harmless, it must be because the optimisation process has steered us towards a solution with favorable generalisation properties. ", "page_idx": 0}, {"type": "text", "text": "From this simple observation, a major line of work studying the implicit regularisation of gradient methods has emerged. These results show that the recovered solution enjoys some type of low norm property in the infinite space of zero-loss solutions. Gradient descent (and its variations) has therefore been analysed in various settings, the simplest and most emblematic being that of gradient descent for least-squares regression: it converges towards the solution which has the lowest $\\ell_{2}$ distance from the initialisation [Lemaire, 1996]. In the classification setting with linearly separable data, iterates of gradient methods must diverge to infinity to minimise the loss. Therefore, the directional convergence of the iterates is considered and Soudry et al. [2018] show that gradient descent selects the $\\ell_{2}$ -max-margin solution amongst all classifiers. ", "page_idx": 0}, {"type": "text", "text": "Going beyond linear settings, it has been observed that an underlying mirror-descent structure very recurrently emerges when analysing gradient descent in a range of non-linear parametrisations [Woodworth et al., 2020, Azulay et al., 2021]. Providing convergence and implicit regularisation results for mirror descent has therefore gained significant importance. ", "page_idx": 0}, {"type": "text", "text": "In this context, for linear regression, Gunasekar et al. [2018] show that the iterates converge to the solution that has minimal Bregman distance to the initial point. Turning towards the classification setting, an apparent gap emerges as there is still no clear understanding of what happens: Can directional convergence be characterised in terms of a max-margin problem? If so, what is the associated norm? Quite surprisingly, this question remains largely unanswered, as it is only understood for $L$ -homogeneous potentials [Sun et al., 2023]. Our paper bridges this gap by formally characterising the implicit bias of mirror descent for separable classification problems. ", "page_idx": 0}, {"type": "image", "img_path": "wiMaws0FWB/tmp/f2c215c21db3bc69d071b7b86c6d00b9ca05e566c90c8aa87cae38eee4a2e79a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Mirror descent is performed using 3 different potentials on the same toy 2d dataset. Left: the losses converge to zero. Center: the iterates converge in direction towards 3 different vectors $\\bar{\\beta}_{\\infty}$ , the 3 lines passing through the origin correspond to the associated separating hyperplanes. Right: the limit directions are each proportional to arg min $\\phi_{\\infty}(\\bar{\\beta})$ under the constraint m $\\mathrm{in}_{i}\\,y_{i}\\langle x_{i},\\bar{\\beta}\\rangle\\stackrel{\\textstyle\\star}{\\geq}1$ for their respective $\\phi_{\\infty}$ \u2019s, as predicted by our theory (Theorem 1). The full trajectories are plotted Figure 4 and we refer to Section 5 for more details. ", "page_idx": 1}, {"type": "text", "text": "1.1 Informal statement of the main result ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "For a separable dataset $(x_{i},y_{i})_{i\\in[n]}$ , we study the mirror flow $\\mathrm{d}\\nabla\\phi(\\beta_{t})=-\\nabla L(\\beta_{t})\\mathrm{d}t$ with potential $\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ and an exponential tailed classification loss $L$ . We prove that $\\beta_{t}$ converges in direction towards the solution of the $\\phi_{\\infty}$ -maximum margin solution where the (asymmetric) norm $\\phi_{\\infty}$ captures the shape of the potential $\\phi$ \u2018at infinity\u2019 (see Figure 2 for an intuitive illustration). ", "page_idx": 1}, {"type": "text", "text": "Theorem 1 (Main result, Informal). There exists a horizon function $\\phi_{\\infty}$ such that for any separable dataset, the normalised mirror flow iterates $\\bar{\\beta}_{t}:=\\beta_{t}/\\|\\beta_{t}\\|$ converge and satisfy: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\bar{\\beta}_{t}\\;\\;\\;i s\\,p r o p o r t i o n a l\\;t o\\;\\;\\;\\;\\;\\;\\arg\\operatorname*{min}_{\\operatorname*{min}_{i}\\,y_{i}\\,\\langle x_{i},\\bar{\\beta}\\rangle\\geq1}\\phi_{\\infty}(\\bar{\\beta}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Our result holds for a large class of potentials $\\phi$ and recovers previous results obtained for $\\phi=$ $\\|\\cdot\\|_{p_{.}}^{p}$ [Sun et al., 2022] and for $L$ -homogeneous potentials [Sun et al., 2023]. For general potentials, showing convergence towards a maximum margin classifier is much harder because, in stark contrast with homogeneous potentials, $\\phi$ \u2019s geometry changes as the iterates diverge. To capture the behaviour of $\\phi$ at infinity, we geometrically construct its horizon function $\\phi_{\\infty}$ . By considering $\\phi$ \u2019s successive level sets (and re-normalising them to prevent blow up), we show that under mild assumptions, these sets asymptotically converge towards a limiting horizon set $S_{\\infty}$ . The horizon function $\\phi_{\\infty}$ is then simply the asymmetric norm which has $S_{\\infty}$ as its unit ball (see Figure 3 for an illustration). In addition, when the function $\\phi$ is \u2018separable\u2019 and can be written $\\begin{array}{r}{\\phi(\\beta){}^{\\!^{\\circ}}\\!=\\!\\sum_{i}\\varphi(\\beta_{i})}\\end{array}$ for a real valued function $\\varphi$ , then a very simple and explicit formula enables to calculate $\\phi_{\\infty}$ (Theorem 3). ", "page_idx": 1}, {"type": "text", "text": "The paper is organised as follows. The classification setting as well as the assumptions on the loss and the potential are provided in Section 2. The proof sketch and an intuitive construction of the horizon function are given in Section 3. In Section 4, we state the formal definition and results. Simple examples of horizon potentials and numerical experiments supporting our claims are finally given in Section 5. ", "page_idx": 1}, {"type": "text", "text": "1.2 Relevance of mirror descent and related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We first outline the motivations for understanding the implicit regularization of mirror descent and discuss related works that contextualize our contribution within the machine learning context. ", "page_idx": 1}, {"type": "text", "text": "Relevance of studying mirror descent in the context of machine learning. Though mirror descent is not per se an algorithm used by machine learning practitioners, it proves to be a very useful tool for theoreticians in the field. Indeed, when analysing gradient descent (and its stochastic and accelerated variants) on neural-network architectures, an underlying mirror-descent structure very recurrently emerges. Then, results for mirror descent enable to prove convergence as well as characterise the implicit bias of gradient descent for these architectures. Diagonal linear networks, which are ideal proxy models for gaining insights on complex deep-learning phenomenons, is the most notable example of such an architecture. The hyperbolic entropy potential naturally appears and enables to prove countless results: implicit bias of gradient descent in regression [Woodworth et al., 2020, Vaskevicius et al., 2019] and in classification [Moroshko et al., 2020], effect of stochasticity [Pesme et al., 2021] and momentum [Papazov et al., 2024], convergence of gradient descent and effect of the step-size [Even et al., 2023], saddle-to-saddle dynamics [Pesme and Flammarion, 2023]. Unveiling an underlying mirror-like structure goes beyond these simple networks as they also appear in: matrix factorisation with commuting observations [Gunasekar et al., 2017, Wu and Rebeschini, 2021], fully connected linear networks [Azulay et al., 2021, Varre et al., 2023] and 2-layer ReLU networks [Chizat and Bach, 2020]. Building on these examples, Li et al. [2022] investigate the formal conditions that ensure the existence of a mirror flow reformulation for general parametrisations, extending previous results by Amid and Warmuth [2020a,b]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Gradient descent in classification. Numerous works have studied gradient descent in the classification setting. For linear parametrisations, separable data and exponentially tailed losses, Soudry et al. [2018] prove that GD converges in direction towards the $\\ell_{2}$ -maximum margin classifier and provides convergence rates. A very fine description of this divergence trajectory is conducted by Ji and Telgarsky [2018] and a different primal-dual analysis leading to tighter rates is given by Ji and Telgarsky [2021]. Similar results are proven for stochastic gradient descent by Nacson et al. [2019c]. In the case of general loss tails, Ji et al. [2020] prove that gradient descent asymptotically follows the $\\ell_{2}$ -norm regularisation path. A whole \u2018astral theory\u2019 is developed by Dud\u00edk et al. [2022] who provide a framework which enables to handle \u2018minimisation at infinity\u2019. Beyond the linear case, Lyu and Li [2020] proves for homogeneous neural networks that any directional limit point of gradient descent is along a KKT point of the $\\ell_{2}$ -max margin problem. A weaker version of this result was previously obtained by Nacson et al. [2019a]. Furthermore, convergence results for linear networks are provided by Yun et al. [2021]. Finally, for 2-layer networks in the infinite width limit, assuming directional convergence, Chizat and Bach [2020] proves that the limit can be characterised as a max-margin classifier in a certain space of functions. ", "page_idx": 2}, {"type": "text", "text": "1.3 Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We provide here a few notations which will be useful throughout the paper. We let $[n]$ be the integers from 1 to $n$ . We denote by $Z\\in\\mathbb{R}^{n\\times d}$ the feature matrix whose $i^{t h}$ line corresponds to the vector $y_{i}x_{i}$ . When not specified, $\\Vert\\cdot\\Vert$ corresponds to any (definable) norm on $\\mathbb{R}^{d}$ . For a convex function $h$ , $\\partial h(\\beta)$ denotes its subdifferential at $\\beta$ $:\\partial h(\\beta)=\\{g\\in\\mathbb{R}^{d}:h(\\beta^{\\prime})\\geq h(\\beta)+\\langle g,\\beta^{\\prime}-\\beta\\rangle,\\forall\\beta^{\\prime}\\in\\mathbb{R}^{d}\\}$ . For any scalar function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ and vector $u\\in\\mathbb{R}^{p}$ , the vector $f(u)\\in\\mathbb R^{p}$ corresponds to the component-wise application of $f$ over $u$ . We denote by $\\sigma:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n}$ the softmax function equal to $\\begin{array}{r}{\\sigma(z)=\\exp(z)/\\sum_{i=1}^{\\bar{n}}\\exp(z_{i})\\in\\Delta_{n}}\\end{array}$ where $\\Delta_{n}$ is the unit simplex. For a convex potential $\\phi$ , we denote $D_{\\phi}(\\beta,\\beta_{0})$ the Bregman divergence equal to $\\phi(\\beta)-\\left(\\phi(\\bar{\\beta}_{0})+\\langle\\nabla\\phi(\\beta_{0}),\\beta-\\bar{\\beta}_{0}\\rangle\\right)\\geq0$ . ", "page_idx": 2}, {"type": "text", "text": "2 Problem set-up ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a dataset $(x_{i},y_{i})_{1\\leq i\\leq n}$ with points $x_{i}\\in\\mathbb{R}^{d}$ and binary labels $y_{i}\\in\\{-1,1\\}$ . We choose a loss function $\\ell:\\mathbb{R}\\rightarrow\\mathbb{R}$ and seek to minimise the empirical risk ", "page_idx": 2}, {"type": "equation", "text": "$$\nL(\\beta)=\\sum_{i=1}^{n}\\ell(y_{i}\\langle x_{i},\\beta\\rangle).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We propose to study the dynamics of mirror flow, which is the continuous-time limit of the mirror descent algorithm [Beck and Teboulle, 2003]. Mirror descent is a generalisation of gradient descent to non-Euclidean geometries induced by a given convex potential function $\\phi:\\mathbb{R}^{d}\\rightarrow\\bar{\\mathbb{R}}$ . The method generates a sequence $(\\hat{\\beta}_{k})_{k\\geq0}$ with $\\hat{\\beta}_{0}=\\beta_{0}\\in\\mathbb{R}^{d}$ and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla\\phi(\\hat{\\beta}_{k+1})=\\nabla\\phi(\\hat{\\beta}_{k})-\\gamma\\nabla L(\\hat{\\beta}_{k}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "When the step size $\\gamma$ goes to 0, the mirror descent iterates approach the solution $(\\beta_{t})_{t\\geq0}$ to the following differential equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\nabla\\phi(\\beta_{t})=-\\nabla L(\\beta_{t})\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "initialised at $\\beta_{0}$ . Studying the mirror flow (MF) leads to simpler computations than its discrete counterpart, and still allows to obtain rich insights about the algorithm\u2019s behaviour. ", "page_idx": 3}, {"type": "text", "text": "We now state our standing assumptions on the loss function $\\ell$ and potential $\\phi$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. The loss $\\ell$ satisfies: ", "page_idx": 3}, {"type": "text", "text": "1. \u2113is convex, twice continuously differentiable, decreasing and $\\begin{array}{r}{\\operatorname*{lim}_{z\\rightarrow+\\infty}\\ell(z)=0}\\end{array}$ .   \n2. \u2113has an exponential tail, in the sense that $\\ell(z)\\underset{z\\rightarrow\\infty}{\\sim}-\\ell^{\\prime}(z)\\underset{z\\rightarrow\\infty}{\\sim}\\exp(-z).$ ", "page_idx": 3}, {"type": "text", "text": "The first part of the assumptions is very general and ensures that the empirical loss $L$ can be minimised \u2018at infinity\u2019. The exponential tail is crucial: it enables to identify a unique maximum margin solution towards which the iterates converge in direction, independently of the considered loss. Both the exponential $\\ell(z)=\\exp(-z)$ and the logistic loss $\\ell(z)=\\ln(1+\\exp(-z))$ satisfy the conditions. On the other hand, losses with polynomial tails do not satisfy the second criterion. Similar assumptions on the tail appear when investigating the implicit bias of gradient descent for separable data [Soudry et al., 2018, Nacson et al., 2019b, Ji et al., 2020, Ji and Telgarsky, 2021, Chizat and Bach, 2020]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. The potential $\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ satisfies: ", "page_idx": 3}, {"type": "text", "text": "The first two points of the assumption are commonly used to ensure well-posedness of mirror descent [Bauschke et al., 2017]. The third one is necessary in continuous time to ensure the existence and uniqueness over $\\mathbb{R}_{\\geq0}$ of a solution to the (MF) differential equation (in particular, we want to avoid the solution \u201cblowing up in finite time\u201d; see Lemma 2 in Appendix A). The coercive gradient assumption is crucial for our main result and we discuss it in more depth in Section 6. ", "page_idx": 3}, {"type": "text", "text": "Finally, we assume that the dataset is linearly separable. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3. There exists $\\beta^{\\star}\\in\\mathbb{R}^{d}$ such that $y_{i}\\langle\\beta^{\\star},x_{i}\\rangle>0$ for every $i\\in[n]$ . ", "page_idx": 3}, {"type": "text", "text": "Notice that such $\\beta^{\\star}$ \u2019s correspond to minimisation directions: $L(\\lambda\\beta^{\\star})\\ {\\stackrel{\\lambda\\to\\infty}{\\longrightarrow}}\\ 0$ . Under the three previous assumptions, we can show that the mirror flow iterates $(\\beta_{t})_{t\\geq0}$ minimise the loss while diverging to infinity. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Considering the mirror flow $(\\beta_{t})_{t\\geq0}$ , the loss converges towards 0 and the iterates diverge: t\u2192\u221e $\\operatorname*{lim}_{t\\to\\infty}L(\\beta_{t})=0$ and $\\operatorname*{lim}_{t\\to\\infty}\\|\\beta_{t}\\|=+\\infty.$ . ", "page_idx": 3}, {"type": "text", "text": "The proof relies on classical techniques used to analyse gradient methods in continuous time and we defer the proof to Appendix A. We now turn to the main question addressed in this paper: ", "page_idx": 3}, {"type": "text", "text": "We initially offer a heuristic and intuitive answer to this question, setting the stage for the formal construction of the implicit regularisation problem. ", "page_idx": 3}, {"type": "text", "text": "3 Intuitive construction of the implicit regularisation problem ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we give an informal presentation and proof sketch of our main result. A fully rigorous exposition is then provided in Section 4. ", "page_idx": 3}, {"type": "text", "text": "Preliminaries. Assume here for simplicity that $\\ell(z)=\\exp(-z)$ . The mirror flow then writes ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nabla\\phi(\\beta_{t})=L(\\beta_{t})\\cdot Z^{T}q(\\beta_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $q(\\beta_{t})\\,=\\,\\sigma(-Z\\beta_{t})$ , where $\\sigma$ is the softmax function and $Z$ the matrix with rows $(y_{i}x_{i})_{i\\in[n]}$ .   \nNote that $q(\\beta_{t})$ belongs to the unit simplex $\\Delta_{n}$ . ", "page_idx": 4}, {"type": "text", "text": "We simplify the differential equation by performing a time rescaling, which does not change the asymptotical behaviour. As $\\begin{array}{r}{\\theta:t\\mapsto\\int_{0}^{t}\\dot{L}(\\bar{\\beta}_{s})\\mathrm{d}s}\\end{array}$ is a bijection in $\\mathbb{R}_{\\geq0}$ (see Lemma 4), we can speed up time and consider the accelerated iterates $\\tilde{\\beta}_{t}=\\beta_{\\theta-1(t)}$ . 1 By the chain rule, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nabla\\phi(\\tilde{\\beta}_{t})=Z^{\\top}q(\\tilde{\\beta}_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and therefore ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{t}\\nabla\\phi(\\widetilde{\\beta}_{t})=\\frac{1}{t}\\nabla\\phi(\\beta_{0})+Z^{\\top}\\Big(\\frac{1}{t}\\int_{0}^{t}q(\\widetilde{\\beta}_{s})\\mathrm{d}s\\Big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "From now on, we drop the tilde notation and assume that a change of time scale has been done. We want to characterise the directional limit of the diverging iterates $\\beta_{t}$ . To do so, we study their normalisation $\\begin{array}{r}{\\bar{\\beta}_{t}:=\\,\\frac{\\beta_{t}}{\\|\\beta_{t}\\|}}\\end{array}$ . As they form a bounded sequence, and $q(\\beta_{t})\\,\\in\\,\\Delta_{n}$ is also bounded, we can extract a subsequence2 $(\\bar{\\beta}_{t_{s}},q(\\beta_{t_{s}}))_{s\\in\\mathbb{N}}$ , with $\\operatorname*{lim}_{s\\to\\infty}t_{s}\\,=\\,\\infty$ converging to some limit $(\\bar{\\beta}_{\\infty},q_{\\infty})$ . By the C\u00e9saro average property, $\\begin{array}{r}{\\frac{1}{t_{s}}\\int_{0}^{t_{s}}q(\\beta_{s})\\mathrm{d}s}\\end{array}$ also converges towards $q_{\\infty}$ . Equation (1) then yields ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{t_{s}}\\nabla\\phi(\\beta_{t_{s}})\\underset{s\\to\\infty}{\\longrightarrow}Z^{\\top}q_{\\infty}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Observe that $q(\\beta_{t})\\,=\\,\\sigma(-Z\\beta_{t})$ and the softmax function $\\sigma$ approaches the argmax operator at infinity. Hence, as $\\beta_{t}$ diverges, we expect that $q(\\beta_{t})_{k}\\to0$ for coordinates $k$ for which $(-Z\\beta_{t})_{k}$ is not maximal, i.e. $(Z\\beta_{t})_{k}$ not minimal. This observation is made formal in the following lemma. Its proof is straightforward and is given in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. Assume that $(\\bar{\\beta}_{t_{s}},q(\\beta_{t_{s}}))\\overset{s\\to\\infty}{\\longrightarrow}(\\bar{\\beta}_{\\infty},q_{\\infty})$ . It holds that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n(q_{\\infty})_{k}=0\\quad i f\\quad y_{k}\\langle x_{k},\\bar{\\beta}_{\\infty}\\rangle>\\operatorname*{min}_{1\\leq i\\leq n}y_{i}\\langle x_{i},\\bar{\\beta}_{\\infty}\\rangle.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In words, coordinates of $q_{\\infty}$ which do not correspond to support vectors of $\\bar{\\beta}_{\\infty}$ must be zero. Our goal is now to uniquely characterise $\\bar{\\beta}_{\\infty}$ as the solution of a maximum margin problem. ", "page_idx": 4}, {"type": "text", "text": "3.1 Warm-up: gradient flow ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As a warm-up, let us consider standard gradient flow, which corresponds to mirror flow with potential $\\phi=\\|\\cdot\\|_{2}^{2}/2$ . In this case, Equation (2) becomes $\\beta_{t_{s}}/t_{s}\\rightarrow Z^{\\top}\\dot{q}_{\\infty}$ . Since the normalised iterates satisfy $\\overline{{\\beta}}_{t_{s}}^{\\,^{-}}\\to\\bar{\\beta}_{\\infty}$ , we get ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{\\beta}_{\\infty}=\\frac{Z^{\\top}q_{\\infty}}{\\|Z^{\\top}q_{\\infty}\\|_{2}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Now notice that this equation along with the slackness conditions from Lemma 1 exactly correspond to the optimality conditions of the following convex minimisation problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\bar{\\beta}}~\\|\\bar{\\beta}\\|_{2}~~~~\\mathrm{under~the~constraint}~~~\\operatorname*{min}_{i\\in[n]}~y_{i}\\langle x_{i},\\bar{\\beta}\\rangle\\geq1.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "wiMaws0FWB/tmp/588d8538a7385014667657f7e636000bf34b5b137c2851a8f6695a7f490c21c2.jpg", "img_caption": ["Figure 2: Left two: Sketch of the level lines of two different potentials $\\phi^{(1)},\\phi^{(2)}:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ . Right two: Their corresponding horizon functions $\\phi_{\\infty}^{(1)}$ , $\\phi_{\\infty}^{(2)}$ as defined in Section 4.1. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Furthermore, the $\\ell_{2}$ -unit ball being strictly convex, Problem (3) has a unique solution to which $\\bar{\\beta}_{\\infty}$ must therefore be equal. Importantly, notice that Problem (3) uniquely defines the limit of any extraction on the normalised iterates $\\bar{\\beta}_{t}$ : the normalised iterates $\\bar{\\beta}_{t}$ must therefore converge towards the $\\ell_{2}$ -maximum margin. We recover the implicit regularisation result from Soudry et al. [2018]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\beta}_{\\infty}=\\underset{\\operatorname*{min}_{i}\\,y_{i}\\langle x_{i},\\bar{\\beta}\\rangle\\geq1}{\\arg\\operatorname*{min}}\\|\\bar{\\beta}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.2 General potential: introducing the horizon function $\\phi_{\\infty}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now tackle general potentials $\\phi$ . In the general case, the challenge of identifying the max-margin problem to which the iterates converge in direction stems from the fact that if the potential $\\phi$ is not $L$ -homogeneous3, its geometry changes as the iterates diverge. More precisely, its sub-level sets $S_{c}:=\\{\\bar{\\beta}\\in\\mathbb{R}^{d},\\phi(\\beta)\\stackrel{=}{\\leq}c\\}$ change of shape as $c$ increase, as illustrated by Figure 2 (Left). ", "page_idx": 5}, {"type": "text", "text": "However, we can hope that these sets have a limiting shape at infinity, meaning that the normalised sub-level sets $\\bar{S}_{c}:=\\dot{S}_{c}/R_{c}$ where $R_{c}:=\\operatorname*{max}_{\\beta\\in S_{c}}\\|\\beta\\|$ converge to some limiting convex set $S_{\\infty}$ as $c\\rightarrow\\infty$ . We can then construct an asymmetric norm4 $\\phi_{\\infty}$ which has $S_{\\infty}$ as its unit ball. In words, $\\phi_{\\infty}$ captures the shape of $\\phi$ at infinity. This informal construction is made rigorous in Section 4.1. We state here the crucial consequence of this construction. ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. The horizon function $\\phi_{\\infty}$ is such that for any sequence $\\beta_{t}$ diverging to infinity for which $\\frac{\\beta_{t}}{\\|\\beta_{t}\\|}$ and $\\frac{\\nabla\\phi(\\beta_{t})}{\\|\\nabla\\phi(\\beta_{t})\\|}$ both converge, then: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}\\frac{\\nabla\\phi(\\beta_{t})}{\\|\\nabla\\phi(\\beta_{t})\\|}\\in\\lambda\\cdot\\partial\\phi_{\\infty}(\\bar{\\beta}_{\\infty}),\\quad w h e r e\\quad\\bar{\\beta}_{\\infty}=\\operatorname*{lim}_{t\\rightarrow\\infty}\\frac{\\beta_{t}}{\\|\\beta_{t}\\|},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some strictly positive factor $\\lambda$ . ", "page_idx": 5}, {"type": "text", "text": "Using this construction, we can derive the optimality conditions satisfied by $\\bar{\\beta}_{\\infty}$ . From the convergence in Equation (2) and that of $\\bar{\\beta}_{t}\\to\\bar{\\beta}_{\\infty}$ , applying Corollary 1, we obtain that: ", "page_idx": 5}, {"type": "equation", "text": "$$\nZ^{\\top}q_{\\infty}\\in\\lambda\\cdot\\partial\\phi_{\\infty}(\\bar{\\beta}_{\\infty}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Up to a positive multiplicative factor (which is irrelevant due to the positive homogeneity of the quantities involved), this condition along with Lemma 1 are exactly the optimality conditions of the convex problem ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{{\\vec{\\beta}}\\in\\mathbb{R}^{d}}\\ \\ \\phi_{\\infty}(\\bar{\\beta})\\quad\\mathrm{under~the~constraint}\\quad\\operatorname*{min}_{i\\in[n]}\\ y_{i}\\langle x_{i},\\bar{\\beta}\\rangle\\geq1.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The limiting direction $\\bar{\\beta}_{\\infty}$ must therefore belong to the set of its solutions. Assuming that this set contains a single element of norm 1 (we refer to the next section for comments concerning the uniqueness), we deduce that the iterates $\\bar{\\beta}_{t}$ must converge towards it: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\frac{\\beta_{t}}{\\|\\beta_{t}\\|}\\propto\\arg\\operatorname*{min}_{\\operatorname*{min}_{i}y_{i}\\langle x_{i},\\bar{\\beta}\\rangle\\geq1}\\,\\,\\phi_{\\infty}(\\bar{\\beta}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "wiMaws0FWB/tmp/2e1454fb9db5858d441e00143cbcd357bb78cf2005e15b374e247a03150d50c5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: Illustration of the construction of the horizon shape $S_{\\infty}$ . Left: the sub-level sets $S_{c}$ change of shape and are increasing. Middle: in order to avoid the shapes blowing up, we normalise them to keep them in the unit ball (here we choose the arbitrary constraining norm to be the $\\ell_{1}$ -norm). Right: the normalised sub-level sets $\\bar{S}_{c}$ converge to a limiting set $S_{\\infty}$ for the Hausdorff distance. ", "page_idx": 6}, {"type": "text", "text": "4 Main result: directional convergence towards the $\\phi_{\\infty}$ -max margin ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now state our formal results, starting with the precise construction of the horizon function $\\phi_{\\infty}$ , followed by the theorem showing convergence of the iterates towards the $\\phi_{\\infty}$ -max-margin. ", "page_idx": 6}, {"type": "text", "text": "4.1 Construction of the horizon function $\\phi_{\\infty}$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first define the horizon shape of a potential $\\phi$ , and provide sufficient conditions for its existence. Then, we use this shape to construct a horizon function $\\phi_{\\infty}$ , which allows the interpretation of the directional limits of gradients of $\\phi$ at infinity. The proofs require technical elements from variational analysis to ensure that the limits are well-defined; these are deferred to Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Horizon shape. Assume w.l.o.g. that $\\phi(0)=0$ . For $c\\geq0$ , consider the sublevel set: ", "page_idx": 6}, {"type": "equation", "text": "$$\nS_{c}(\\phi)=\\{\\beta\\in\\mathbb{R}^{d}\\,:\\,\\phi(\\beta)\\leq c\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which is nonempty and compact by coercivity of $\\phi$ . We can then define the normalised sublevel set: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bar{S}_{c}=\\frac{1}{R_{c}}S_{c},\\quad R_{c}=\\operatorname*{max}\\{\\|\\beta\\|\\,:\\,\\beta\\in S_{c}\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By construction, the set $\\bar{S}_{c}$ belongs to the unit ball. We are interested in the limit of $\\bar{S}_{c}$ as $c\\rightarrow\\infty$ . Definition 1. We say that $\\phi$ admits a horizon shape if the family of normalized sublevel sets $(\\bar{S}_{c})_{c>0}$ defined in Equation (4) converges to some compact set $S_{\\infty}$ as $c\\rightarrow\\infty$ for the Hausdorff distance. In addition, we say that this shape is non-degenerate if the origin belongs to the interior of $S_{\\infty}$ . ", "page_idx": 6}, {"type": "text", "text": "The Hausdorff distance is a natural distance on compact sets [see Rockafellar and Wets, 1998, Section 4.C., for a definition]. In Proposition 2, we prove the existence of the horizon shape for a large class of functions which contains all the potentials with domain $\\mathbb{R}^{d}$ encountered in practice. Although the horizon shape is guaranteed to exist for most functions, we cannot a priori prove that it is non-degenerate, as the normalized sub-levels $\\bar{S}_{c}$ can become \u2018flat\u2019 as $c\\rightarrow\\infty$ .5Given the technical complexity associated with this case, we now focus exclusively on non-degenerate horizon shapes. ", "page_idx": 6}, {"type": "text", "text": "Horizon function. If $\\phi$ admits a non-degenerate horizon shape $S_{\\infty}$ , we define its horizon function as the Minkowski gauge [Rockafellar and Wets, 1998, Section 11.E] of $S_{\\infty}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\phi_{\\infty}(\\bar{\\beta}):=\\operatorname*{inf}\\big\\{r>0\\,:\\,\\frac{\\bar{\\beta}}{r}\\in S_{\\infty}\\big\\}\\quad\\mathrm{~for~}\\bar{\\beta}\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By construction, the horizon function $\\phi_{\\infty}$ is an asymmetric norm and its sub-level sets correspond to scaled versions of $S_{\\infty}$ [see Rockafellar and Wets, 1998, Section 11.C, for more properties]. For example, in the case of the horizon shape $S_{\\infty}$ illustrated in Figure 3, the corresponding horizon function $\\phi_{\\infty}$ is proportional to the $\\ell_{1}$ -norm. Although the construction of $\\phi_{\\infty}$ presented here is rather abstract, we show in Theorem 3 that for separable potentials defined over $\\mathbb{R}^{d}$ , it can be computed with an explicit formula. Though different, our definition of the horizon function shares many similarities with the classical concept of horizon function from convex analysis [Rockafellar and Wets, 1998]. We discuss the links between the two notions at the end of Section 4.3. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main result: directional convergence of the iterates towards the $\\phi_{\\infty}$ -max-margin ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We can now state our main result which fully characterises the directional convergence of mirror flow. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. Assume that $\\phi$ admits a non-degenerate horizon shape and let $\\phi_{\\infty}$ be its horizon function. Assuming that the following $\\phi_{\\infty}$ -max-margin problem has a unique minimiser, then the mirror flow normalised iterates $\\begin{array}{r}{\\bar{\\beta}_{t}=\\frac{\\beta_{t}}{\\|\\beta_{t}\\|}}\\end{array}$ converge towards a vector $\\bar{\\beta}_{\\infty}$ and ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\bar{\\beta}_{\\infty}\\propto\\underset{\\bar{\\beta}\\in\\mathbb{R}^{d}}{\\arg\\operatorname*{min}}\\ \\phi_{\\infty}(\\bar{\\beta})\\quad u n d e r\\ t h e\\ c o n s t r a i n t\\quad\\underset{i\\in[n]}{\\operatorname*{min}}\\ y_{i}\\langle x_{i},\\bar{\\beta}\\rangle\\geq1,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the symbol $\\propto$ denotes positive proportionality. ", "page_idx": 7}, {"type": "text", "text": "Remark on the uniqueness of the margin problem. If the unit ball of $\\phi_{\\infty}$ is strictly convex, then the $\\phi_{\\infty}$ -max-margin problem has a unique solution. However, in the general case, there may exist an infinity of solutions and weak but ad hoc assumptions on the dataset are required to guarantee uniqueness. For instance, if $\\phi_{\\infty}$ is proportional to the $\\ell_{1}$ -norm, a common assumption which ensures uniqueness is assuming that the data features are in general position [Dossal, 2012]. ", "page_idx": 7}, {"type": "text", "text": "4.3 Assumptions guaranteeing the existence of $\\phi_{\\infty}$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our main result, presented in Theorem 2 relies on the existence of a horizon shape, $S_{\\infty}$ , as described in Definition 1. From this shape, the asymmetric norm $\\phi_{\\infty}$ is constructed. ", "page_idx": 7}, {"type": "text", "text": "We show here that the existence of $S_{\\infty}$ is ensured for a large class of \u2018nice\u2019 functions, specifically those definable in $o$ -minimal structures [Dries, 1998]. For the reader unfamiliar with this notion, this class contains all \u2018reasonable\u2019 functions used in practice, such as polynomials, logarithms, exponentials, and \u2018reasonable\u2019 combinations of those. This is a typical assumption used for instance to prove the convergence of optimisation methods through the Kurdyka\u2013\u0141ojasiewicz property [Attouch et al., 2011]. ", "page_idx": 7}, {"type": "text", "text": "Proposition 2. If any of the three following conditions hold: $(i)\\,\\phi$ is a finite composition of polynomials, exponentials and logarithms, (ii) $\\phi$ is globally subanalytic, (iii) $\\phi$ is definable in a $o$ -minimal structure on $\\mathbb{R}$ ; then $\\phi$ admits a horizon shape $S_{\\infty}$ . ", "page_idx": 7}, {"type": "text", "text": "The proof is technical and we defer it to Appendix B. Although the previous proposition ensures the existence $\\phi_{\\infty}$ for a wide range of potentials, it does not offer a direct method for computing it. In the following, we show that for potentials that are both separable and even, a simple formula exists, allowing for the direct calculation of $\\phi_{\\infty}$ . ", "page_idx": 7}, {"type": "text", "text": "Assumption 4. The potential $\\phi$ is separable, in the sense that there exists $\\varphi:\\mathbb{R}\\rightarrow\\mathbb{R}_{\\geq0}$ such that $\\begin{array}{r}{\\phi(\\beta)=\\sum_{i=1}^{d}\\varphi(\\beta_{i})}\\end{array}$ . We assume that $\\varphi$ satisfies Assumption 2, that it is definable in a $o$ -minimal structur e on $\\mathbb{R}$ and that it is an even function. W.l.o.g. we assume that $\\varphi(0)=0$ . ", "page_idx": 7}, {"type": "text", "text": "We note that $\\varphi$ is a bijection over $\\mathbb{R}_{\\geq0}$ , and denote by $\\varphi^{-1}$ its inverse. We consider the function $\\varphi^{-1}\\circ\\phi$ , which can be seen as a renormalisation of $\\phi$ . It has the same level sets as $\\phi$ and ensures that $\\mathrm{lim}_{\\eta\\rightarrow0}\\,\\eta\\varphi^{-1}(\\phi(\\bar{\\beta}/\\eta))$ exists in $\\mathbb{R}_{>0}$ for all $\\bar{\\beta}$ . These two observations lead to the following theorem. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3. Under Assumption $^{4}$ , the potential $\\phi$ admits a non-degenerate horizon shape and its horizon function is such that there exists $\\lambda>0$ such that for every $\\tilde{\\beta}\\in\\mathbb{R}^{d}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\phi_{\\infty}({\\bar{\\beta}})=\\lambda\\operatorname*{lim}_{\\eta\\to0}\\eta\\cdot\\varphi^{-1}\\left(\\phi\\left({\\frac{\\bar{\\beta}}{\\eta}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We use this simple formula when computing $\\phi_{\\infty}$ for various potentials in the next section. ", "page_idx": 7}, {"type": "text", "text": "Remark on previous notions of horizon function. In the convex analysis literature, the horizon function is typically defined as $\\begin{array}{r}{\\phi_{\\infty}(\\bar{\\beta})=\\operatorname*{lim}_{\\eta\\to0}\\eta\\phi(\\bar{\\beta}/\\eta)}\\end{array}$ [Rockafellar and Wets, 1998, Laghdir and Volle, 1999]. In our context, this definition would yield a function which equals $+\\infty$ everywhere except at the origin. In contrast, our definition ensures that $\\phi_{\\infty}$ attains finite values over $\\bar{\\mathbb{R}^{d}}$ . The distinction stems from our way of normalising the level sets by $R_{c}$ in Section 4.1, or alternatively, from the composition by $\\varphi^{-1}$ in the separable case. The two constructions would coincide only if $\\phi$ was Lipschitz continuous, which is at odds with Assumption 2. ", "page_idx": 7}, {"type": "image", "img_path": "wiMaws0FWB/tmp/d59470e38931a845d5cfeea1da6251041a34f0d29441c968fcbf056b75bdc12c.jpg", "img_caption": ["Iterate trajectories in the $\\mathbb{R}^{2}$ -plane "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Mirror flow trajectories on a 2-dimensional dataset for three different potentials (exact same setting as in Figure 1). Left: the iterates diverge to infinity and the directional convergence depends on the choice of potential. Right: the normalised iterates converge towards their respective $\\phi_{\\infty}$ -maximum-margin predictors (illustrated by stars), as predicted by Theorem 2. ", "page_idx": 8}, {"type": "text", "text": "5 Applications and experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we illustrate our main result using various potentials. ", "page_idx": 8}, {"type": "text", "text": "Homogeneous potentials. We first consider potentials $\\phi$ which are $L$ -homogeneous, i.e., there exists $L>0$ such that for all $c>0$ and $\\beta\\in\\mathbb{R}^{d}$ , $\\phi(c\\beta)=c^{L}\\phi(\\beta)$ . In this case, the sublevel sets $\\Bar{S}_{c}$ are all equal. It follows that $\\phi_{\\infty}\\propto\\phi^{1/L}$ . An important example is the case of $\\phi=\\|\\cdot\\|_{p}$ where $\\|\\cdot\\|_{p}$ corresponds to the $\\ell_{p}$ -norm with $p>1$ , for which we get that $\\phi_{\\infty}\\propto\\|\\cdot\\|_{p}$ and we recover the result from Sun et al. [2022, 2023]. ", "page_idx": 8}, {"type": "text", "text": "Hyperbolic-cosine entropy potential. Finally, we consider $\\begin{array}{r}{\\phi^{\\mathrm{MD_{1}}}(\\beta)\\,=\\,\\sum_{i=1}^{d}(\\cosh(\\beta_{i})\\,-\\,1)}\\end{array}$ .   \nApplying Theorem 3, we get that $\\phi_{\\infty}\\propto\\|\\cdot\\|_{\\infty}$ . ", "page_idx": 8}, {"type": "text", "text": "Hyperbolic entropy potential. The hyperbolic entropy potential: $\\begin{array}{r l}{\\phi^{\\mathrm{MD_{2}}}(\\beta)}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sum_{i=1}^{d}(\\beta_{i}\\mathrm{arcsinh}(\\beta_{i})\\mathrm{~-~}\\sqrt{\\beta_{i}^{2}+1}\\mathrm{~-~}1)}\\end{array}$ plays a central role in works considering diagonal linear networks [Woodworth et al., 2020, Pesme and Flammarion, 2023]. Applying Theorem 3, we obtain that $\\phi_{\\infty}\\propto\\|\\cdot\\|_{1}$ and we recover the result from Lyu and Li [2020] and Moroshko et al. [2020]. ", "page_idx": 8}, {"type": "text", "text": "Experimental details concerning Figure 1. As shown in Figure 1 (Middle), we generate 40 points with positive labels and 40 points with negative labels. Starting from $\\beta_{0}=0$ , we run mirror descent with the exponential loss $\\bar{\\ell(z)}=\\exp(-z)$ and with the three following potentials: ", "page_idx": 8}, {"type": "equation", "text": "$$\n(i)~\\phi^{\\mathrm{GD}}=\\|\\cdot\\|_{2}^{2},\\qquad(i i)~\\phi^{\\mathrm{MD}_{1}}=\\mathrm{cosh-entropy},\\qquad(i i i)~\\phi^{\\mathrm{MD}_{2}}=\\mathrm{Hyperbolic~entropy}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We first observe in Figure 1 (Left) that the training loss converges to zero, as predicted by Proposition 1, with a convergence rate that varies across different potentials. Moreover, as illustrated in Figure 1 (Middle and Right), the iterates converge in direction towards their respective unique $\\phi_{\\infty}$ -max margin solutions associated with the following geometries: ", "page_idx": 8}, {"type": "equation", "text": "$$\n(i)~\\phi_{\\infty}^{\\mathrm{GD}}\\propto\\parallel\\cdot\\parallel_{2},\\qquad(i i)~\\phi_{\\infty}^{\\mathrm{MD_{1}}}=\\parallel\\cdot\\parallel_{\\infty},\\qquad(i i i)~\\phi_{\\infty}^{\\mathrm{MD_{2}}}\\propto\\parallel\\cdot\\parallel_{1}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Therefore, by employing various potentials, we can induce different implicit biases, leading to distinct generalisation properties depending on the data distribution. The trajectories of the mirror descent iterates are shown and commented in Figure 4. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we offer a comprehensive characterisation of the implicit bias of mirror flow for separable classification problems. This characterisation is framed in terms of the horizon function associated with the mirror descent potential, leveraging the asymptotic geometry induced by the potential. Note that we did not cover the discrete mirror descent algorithm; we believe the analysis would extend without additional difficulties compared to the continuous counterpart. ", "page_idx": 9}, {"type": "text", "text": "Extensions and open problems. Our results being purely asymptotic, characterising the rate at which the normalised iterates converge towards the maximum-margin solution is an open direction for future research. Furthermore, we note that our analysis does not cover potentials that are defined only on a strict subset of $\\mathbb{R}^{d}$ (such as the log-barrier and the negative entropy), and with possibly non-coercive gradients. This class of potentials is of interest as it arises when investigating deep architectures, such as diagonal linear networks of depth $D>2$ . In this setting, it is known that gradient flow on the weights lead to a mirror flow on the predictors with a certain potential $\\phi_{D}$ [Woodworth et al., 2020]. Interestingly, the potentials $\\phi_{D}$ have non-coercive gradients and their horizon functions do not depend on the depth $D$ as they are all proportional to the $\\ell_{1}$ -norm. The predictors are, however, known to converge in direction towards a KKT point of the non-convex $\\ell_{2/D}$ -max-margin problem [Lyu and Li, 2020] which can be different from the $\\ell_{1}$ -max-margin problem [Moroshko et al., 2020]. This observation highlights that our coercive gradient assumption is necessary for our result to hold. However, extending our analysis beyond this assumption is a promising direction for understanding gradient dynamics in deep architectures. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "S.P. would like to thank Loucas Pillaud-Vivien for the helpful discussions at the beginning of the project as well as Pierre Quinton for the careful proofreading of the paper. The authors also thank J\u00e9r\u00f4me Bolte and Edouard Pauwels for the precious help on the existence of the Hausdorff limit of definable families, as well as Nati Srebro and L\u00e9na\u00efc Chizat for the remarks on the limitations of our result for potentials with non-coercive gradients. This work was supported by the Swiss National Science Foundation (grant number 212111). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Charalambos Aliprantis and Kim Border. Infinite Dimensional Analysis. Springer Berlin, Heidelberg, 2006.   \nEhsan Amid and Manfred K Warmuth. Winnowing with gradient descent. In Conference on Learning Theory, pages 163\u2013182. PMLR, 2020a.   \nEhsan Amid and Manfred KK Warmuth. Reparameterizing mirror descent as gradient descent. Advances in Neural Information Processing Systems, 33:8430\u20138439, 2020b.   \nH. Attouch, X. Goudou, and P. Redont. The heavy ball with friction method, i. the continuous dynamical system: Global exploration of the local minima of a real-valued function by asymptotic analysis of a dissipitive dynamical system. Communications in Contemporary Mathematics, 2(1): 1\u201334, 2000.   \nHedy Attouch and Gerald Beer. On the convergence of subdifferentials of convex functions. Archiv der Mathematik, 1993.   \nHedy Attouch, J\u00e9r\u00f4me Bolte, and Benar Fux Svaiter. Convergence of descent methods for semialgebraic and tame problems: proximal algorithms, forward\u2013backward splitting, and regularized gauss\u2013seidel methods. Mathematical Programming, 2011.   \nShahar Azulay, Edward Moroshko, Mor Shpigel Nacson, Blake E Woodworth, Nathan Srebro, Amir Globerson, and Daniel Soudry. On the implicit bias of initialization shape: Beyond infinitesimal mirror descent. In International Conference on Machine Learning, pages 468\u2013477. PMLR, 2021.   \nHeinz H Bauschke, J\u00e9r\u00f4me Bolte, and Marc Teboulle. A descent lemma beyond lipschitz gradient continuity: first-order methods revisited and applications. Mathematics of Operations Research, 42(2):330\u2013348, 2017.   \nAmir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167\u2013175, 2003.   \nJ\u00e9r\u00f4me Bolte, Aris Daniilidis, and Adrian Lewis. Tame functions are semismooth. Mathematical Programming, 2007.   \nLenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Conference on Learning Theory, pages 1305\u20131338. PMLR, 2020.   \nC. Combari and L. Thibault. On the graph convergence of subdifferentials of convex functions. Proceedings of the American Mathematical Society, 1998.   \nRichard Courant and Fritz John. Introduction to Calculus and Analysis. Springer New York, 1989.   \nCharles Dossal. A necessary and sufficient condition for exact sparse recovery by l1 minimization. Comptes Rendus Mathematique, 350(1-2):117\u2013120, 2012.   \nL. P. D. van den Dries. Tame Topology and O-minimal Structures. Cambridge University Press, 1998.   \nMiroslav Dud\u00edk, Robert E Schapire, and Matus Telgarsky. Convex analysis at infinity: An introduction to astral space. arXiv preprint arXiv:2205.03260, 2022.   \nMathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. (s)gd over diagonal linear networks: Implicit bias, large stepsizes and edge of stability. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nSuriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. Advances in neural information processing systems, 30, 2017.   \nSuriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In International Conference on Machine Learning, pages 1832\u20131841. PMLR, 2018.   \nZiwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint arXiv:1803.07300, 2018.   \nZiwei Ji and Matus Telgarsky. Characterizing the implicit bias via a primal-dual analysis. In Algorithmic Learning Theory, pages 772\u2013804. PMLR, 2021.   \nZiwei Ji, Miroslav Dud\u00edk, Robert E Schapire, and Matus Telgarsky. Gradient descent follows the regularization path for general losses. In Conference on Learning Theory, pages 2109\u20132136. PMLR, 2020.   \nBeata Kocel-Cynk, Wies\u0142aw Paw\u0142ucki, and Anna Valette. A short geometric proof that hausdorff limits are definable in any o-minimal structure. advg, 2014.   \nMohammed Laghdir and Michel Volle. A general formula for the horizon function of a convex composite function. Archiv der Mathematik, 1999.   \nB Lemaire. An asymptotical variational principle associated with the steepest descent method for a convex function. Journal of Convex Analysis, 3:63\u201370, 1996.   \nZhiyuan Li, Tianhao Wang, Jason D Lee, and Sanjeev Arora. Implicit bias of gradient descent on reparametrized models: On equivalence to mirror descent. Advances in Neural Information Processing Systems, 35:34626\u201334640, 2022.   \nShengchao Liu, Dimitris Papailiopoulos, and Dimitris Achlioptas. Bad global minima exist and sgd can reach them. Advances in Neural Information Processing Systems, 33:8543\u20138552, 2020.   \nKaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In International Conference on Learning Representations, 2020.   \nEdward Moroshko, Blake E Woodworth, Suriya Gunasekar, Jason D Lee, Nati Srebro, and Daniel Soudry. Implicit bias in deep linear classification: Initialization scale vs training accuracy. Advances in neural information processing systems, 33:22182\u201322193, 2020.   \nMor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models. In International Conference on Machine Learning, pages 4683\u20134692. PMLR, 2019a.   \nMor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3420\u20133428. PMLR, 2019b.   \nMor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3051\u20133059. PMLR, 2019c.   \nHristo Papazov, Scott Pesme, and Nicolas Flammarion. Leveraging continuous time to understand momentum when training diagonal linear networks. In International Conference on Artificial Intelligence and Statistics, pages 3556\u20133564. PMLR, 2024.   \nScott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks. Neurips, 2023.   \nScott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear networks: a provable benefti of stochasticity. Advances in Neural Information Processing Systems, 34, 2021.   \nR. Tyrrell Rockafellar and Roger J. B. Wets. Variational Analysis. Springer Berlin Heidelberg, 1998.   \nRalph Tyrell Rockafellar. Convex Analysis. Princeton University Press, 1970.   \nDaniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit   \nHaoyuan Sun, Kwangjun Ahn, Christos Thrampoulidis, and Navid Azizan. Mirror descent maximizes generalized margin and can be implemented efficiently. Advances in Neural Information Processing Systems, 35, 2022.   \nHaoyuan Sun, Khashayar Gatmiry, Kwangjun Ahn, and Navid Azizan. A unified approach to controlling implicit regularization via mirror descent. arXiv preprint arXiv:2306.13853, 2023.   \nLou Van den Dries and Chris Miller. Geometric categories and o-minimal structures. 1996.   \nAditya Vardhan Varre, Maria-Luiza Vladarean, Loucas Pillaud-Vivien, and Nicolas Flammarion. On the spectral bias of two-layer linear networks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nTomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal sparse recovery. Advances in Neural Information Processing Systems, 32, 2019.   \nBlake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Conference on Learning Theory, pages 3635\u20133673. PMLR, 2020.   \nFan Wu and Patrick Rebeschini. Implicit regularization in matrix sensing via mirror descent. Advances in Neural Information Processing Systems, 34:20558\u201320570, 2021.   \nChulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in training linear neural networks. In International Conference on Learning Representations, 2021.   \nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Organisation of the Appendix. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. In Appendix A, we provide the proofs of the existence and uniqueness of (MF), of the convergence of the loss, the divergence of the iterates and the proof of Lemma 1. 2. In Appendix B, we provide all the proofs concerning the construction of the horizon shape and that of our main Theorems 2 and 3. ", "page_idx": 13}, {"type": "text", "text": "A Proofs of properties of the mirror flow in the classification setting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We start by proving the existence and uniqueness of (MF) over $\\mathbb{R}_{\\geq0}$ . The proof is standard and relies on ensuring that the iterates do not diverge in finite time. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2. For any initialisation $\\beta_{0}\\in\\mathbb{R}^{d}$ , there exists a unique solution defined over $\\mathbb{R}_{\\geq0}$ which satisfies (MF) for all $t\\geq0$ and with initial condition $\\beta_{t=0}=\\beta_{0}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. From Assumption 2, we have that $\\phi$ is differentiable, strictly convex and its gradient is coercive. Consequently, $\\nabla\\phi$ is bijective over $\\mathbb{R}^{d}$ (see Rockafellar [1970], Theorem 26.6). Furthermore, the Fenchel conjugate $\\phi^{*}$ is differentiable over $\\mathbb{R}^{d}$ and $(\\nabla\\phi)^{-1}=\\nabla\\phi^{*}$ . ", "page_idx": 14}, {"type": "text", "text": "To prove the existence and uniqueness of a global solution of (MF), we first consider the following differential equation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{d}u_{t}=-\\nabla L(\\nabla\\phi^{*}(u_{t}))\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with initial condition $u_{t=0}=\\nabla\\phi^{*}(\\beta_{0})$ . ", "page_idx": 14}, {"type": "text", "text": "Since $L$ is $\\mathcal{C}^{2}$ , $\\nabla L$ is Lipschitz on all compact sets. Furthermore, since $\\nabla^{2}\\phi$ is p.s.d., $\\nabla\\phi^{*}=(\\nabla\\phi)^{-1}$ is $\\mathcal{C}^{1}$ and therefore Lipschitz on all compact sets. Hence $\\nabla L\\circ\\nabla\\phi^{*}$ is Lipschitz on all compact sets and from the Picard-Lindel\u00f6f theorem, there exists a unique maximal (i.e. which cannot be extended) solution $\\left(u_{t}\\right)$ satisfying eq. (5) such that $u_{t=0}=\\nabla\\phi^{*}(\\beta_{0})$ . We denote $[0,T_{\\mathrm{max}})$ the intersection of this maximal interval of definition (which must be open) and $\\mathbb{R}_{\\geq0}$ . Our goal is now to prove that $T_{\\mathrm{max}}\\,=\\,+\\infty$ . To do so, we assume that $T_{\\mathrm{max}}$ is finite and we will show that this leads to a contradiction due to the fact that the iterates $\\beta_{t}$ cannot diverge in finite time. Let $\\beta_{t}:=\\nabla\\phi^{*}(u_{t})$ and notice that $\\beta_{t}$ is therefore the unique solution satisfying (MF) over $[0,T_{\\mathrm{max}})$ with $\\beta_{t=0}=\\beta_{0}$ . ", "page_idx": 14}, {"type": "text", "text": "Bounding the trajectory of $\\beta_{t}$ over $[0,T_{\\mathrm{max}})$ . Pick any $\\beta\\in\\mathbb{R}^{d}$ and notice that by convexity of $L$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}D_{\\phi}(\\beta,\\beta_{t})=-\\langle\\nabla L(\\beta_{t}),\\beta_{t}-\\beta\\rangle\\le-(L(\\beta_{t})-L(\\beta))\\le L(\\beta)-L_{\\mathrm{min}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Where $L_{\\mathrm{min}}$ is a lower bound on the loss. Integrating from 0 to $t<T_{\\mathrm{max}}$ we get: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\phi}(\\beta,\\beta_{t})\\le t\\cdot(L(\\beta)-L_{\\operatorname*{min}})+D_{\\phi}(\\beta,\\beta_{0})}\\\\ &{\\qquad\\qquad\\le T_{\\operatorname*{max}}\\cdot(L(\\beta)-L_{\\operatorname*{min}})+D_{\\phi}(\\beta,\\beta_{0})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, due to Assumption 2, the iterates $\\beta_{t}$ are bounded over $[0,T_{\\mathrm{max}})$ . The proof from here is standard (see e.g. Attouch et al. [2000], Theorem 3.1): from eq. (5) we get that $\\dot{u}_{t}$ is bounded over $[0,T_{\\mathrm{max}})$ and $\\operatorname{sup}_{t\\in[0,T_{\\operatorname*{max}{}}]}$ ) $\\|\\dot{u}_{t}\\|\\;=:C\\,<\\,+\\infty$ which means that $\\|u_{t}\\,-\\,u_{t^{\\prime}}\\|\\,\\le\\,C|t-t^{\\prime}|$ . Hence $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow T_{\\mathrm{max}}}u_{t}\\,=:\\,u_{\\infty}}\\end{array}$ must exist. Applying the Picard-Lindel\u00f6f again at time $T_{\\mathrm{max}}$ with initial condition $u_{\\infty}$ violates the initial maximal interval assumption. Therefore $T_{\\mathrm{max}}=+\\infty$ which concludes the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "We now recall and prove classical results on the mirror flow in the classification setting. ", "page_idx": 14}, {"type": "text", "text": "Proposition 1. Considering the mirror flow $(\\beta_{t})_{t\\geq0}$ , the loss converges towards 0 and the iterates diverge: $\\operatorname*{lim}_{t\\to\\infty}L(\\beta_{t})=0$ and $\\operatorname*{lim}_{t\\to\\infty}\\|\\beta_{t}\\|=+\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. The loss is decreasing. $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}t}L(\\beta_{t})=\\langle\\nabla L(\\beta_{t}),\\dot{\\beta}_{t}\\rangle=-\\langle\\nabla^{2}\\phi(\\beta_{t})^{-1}\\nabla L(\\beta_{t}),\\nabla L(\\beta_{t})\\rangle\\leq0,}\\end{array}$   \nwhere the inequality is due to the convexity of the potential $\\phi$ . ", "page_idx": 14}, {"type": "text", "text": "Convergence of the loss towards 0. Now consider the Bregman divergence between an arbitrary point $\\beta$ and $\\beta_{t}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{\\phi}(\\beta,\\beta_{t})=\\phi(\\beta)-\\phi(\\beta_{t})-\\langle\\nabla\\phi(\\beta_{t}),\\beta-\\beta_{t}\\rangle\\geq0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is such that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}D_{\\phi}(\\beta,\\beta_{t})=\\langle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nabla\\phi(\\beta_{t}),\\beta_{t}-\\beta\\rangle}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=-\\langle\\nabla L(\\beta_{t}),\\beta_{t}-\\beta\\rangle}\\\\ {\\displaystyle\\qquad\\qquad\\leq-(L(\\beta_{t})-L(\\beta))}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the inequality is by convexity of the loss. Integrating and due to the decrease of the loss, we get that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\beta_{t})\\le\\displaystyle\\frac{1}{t}\\int_{0}^{t}L(\\beta_{s})\\,\\mathrm{d}s}\\\\ &{\\qquad\\le L(\\beta)+\\displaystyle\\frac{D_{\\phi}(\\beta,\\beta_{0})-D_{\\phi}(\\beta,\\beta_{t})}{t}}\\\\ &{\\qquad\\le L(\\beta)+\\displaystyle\\frac{D_{\\phi}(\\beta,\\beta_{0})}{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since this is true for all point \u03b2, we get that L(\u03b2t) \u2264inf\u03b2\u2208Rd L(\u03b2) + D\u03d5(t\u03b2,\u03b20). It remains to show that the right hand term goes to 0 as $t$ goes to infinity. To show this, let $\\varepsilon>0$ , by the separability assumption we get that there exists $\\beta^{\\star}$ such that $\\operatorname*{min}y_{i}\\langle x_{i},\\beta^{\\star}\\rangle>0$ . Since $L(\\lambda\\beta_{\\star})\\underset{\\lambda\\rightarrow\\infty}{\\longrightarrow}0$ , we can choose $\\lambda$ big enough such that $L(\\lambda\\beta^{\\star})<\\varepsilon$ and then $t_{\\lambda}$ large enough such that $\\begin{array}{r}{\\frac{1}{t_{\\lambda}}D_{\\phi}(\\lambda\\beta^{\\star},\\beta_{0})<\\varepsilon}\\end{array}$ . The loss therefore converges to 0. ", "page_idx": 15}, {"type": "text", "text": "Divergence of the iterates. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For all $i\\in[n]$ $[n],\\ell(y_{i}\\langle x_{i},\\beta_{t}\\rangle)\\leq L(\\beta_{t})\\mathop{\\longrightarrow}_{t\\to\\infty}0$ . Due to the assumptions on the loss, this translates into $y_{i}\\langle x_{i},\\beta_{t}\\rangle\\xrightarrow[t\\to\\infty]{}\\infty$ , hence $\\|\\beta_{t}\\|\\underset{t\\rightarrow\\infty}{\\longrightarrow}+\\infty$ . ", "page_idx": 15}, {"type": "text", "text": "Introducing a few notations. Before giving two important lemmas, we provide a few notations. Recall that $Z$ is the data matrix of size $n\\times d$ whose $\\bar{i}^{t h}$ row is $y_{i}x_{i}$ , we then have that $\\nabla L(\\beta)=$ $Z^{\\top}\\ell^{\\prime}(Z\\beta)$ , where $\\ell^{\\prime}$ is applied component wise. We now denote by $q(\\beta)$ the vector in $\\mathbb{R}^{n}$ equal to: ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(\\beta)=\\frac{\\ell^{\\prime}(Z\\beta)}{\\ell^{\\prime}(\\ell^{-1}(\\sum_{i}\\ell(y_{i}\\langle x_{i},\\beta\\rangle)))}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Notice that due to $\\ell>0,\\ell^{\\prime}<0,\\ell^{-1}$ is increasing and $\\ell^{\\prime}$ is decreasing, we have that $q(\\beta)>0$ and that for all $i_{0}\\in[n]$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(\\beta)_{i_{0}}=\\frac{\\ell^{\\prime}(y_{i_{0}}\\langle x_{i_{0}},\\beta\\rangle)}{\\ell^{\\prime}(\\ell^{-1}(\\sum_{i}\\ell(y_{i}\\langle x_{i},\\beta\\rangle)))}\\leq\\frac{\\ell^{\\prime}(y_{i_{0}}\\langle x_{i_{0}},\\beta\\rangle)}{\\ell^{\\prime}(\\ell^{-1}(\\ell(y_{i_{0}}\\langle x_{i_{0}},\\beta\\rangle)))}\\leq1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore $q(\\beta)\\in(0,1]^{n}$ . ", "page_idx": 15}, {"type": "text", "text": "We further denote ", "page_idx": 15}, {"type": "equation", "text": "$$\na_{t}:=-\\ell^{\\prime}(\\ell^{-1}(\\sum_{i}\\ell(y_{i}\\langle x_{i},\\beta_{t}\\rangle)))>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This way we can simply write $\\nabla L(\\beta_{t})=-a_{t}Z^{\\top}q_{t}$ with $q_{t}:=q(\\beta_{t})$ . ", "page_idx": 15}, {"type": "text", "text": "Integrating the flow (MF), we write: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla\\phi(\\beta_{t})=\\nabla\\phi(\\beta_{0})-\\int_{0}^{t}\\nabla L(\\beta_{s})\\mathrm{d}s}}\\\\ &{}&{=\\nabla\\phi(\\beta_{0})+Z^{\\top}\\int_{0}^{t}a_{s}q_{s}\\mathrm{d}s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Two lemmas. In the following lemma we recall and prove that a coordinate $q_{\\infty}[k]$ must be equal to 0 if datapoint $x_{k}$ is not a support vector of $\\bar{\\beta}_{\\infty}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 3. For some function $C_{t}\\to\\infty,$ , if the iterates $\\begin{array}{r}{\\bar{\\beta}_{t}=\\frac{\\beta_{t}}{C_{t}}}\\end{array}$ converge towards a vector which we denote $\\bar{\\beta}_{\\infty}$ and $q_{t}$ converges towards a vector $q_{\\infty}\\in[0,1]^{n}$ . Then it holds that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nq_{\\infty}[k]=0\\qquad i f\\qquad y_{k}\\langle x_{k},\\bar{\\beta}_{\\infty}\\rangle>\\operatorname*{min}_{1\\leq i\\leq n}y_{i}\\langle x_{i},\\bar{\\beta}_{\\infty}\\rangle.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In words, $q_{\\infty}[k]=0$ if $x_{k}$ is not a support vector. ", "page_idx": 15}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 16}, {"type": "equation", "text": "$$\nq(\\beta_{t})=\\frac{\\ell^{\\prime}(Z\\beta_{t})}{\\ell^{\\prime}(\\ell^{-1}(\\sum_{i}\\ell(y_{i}\\langle x_{i},\\beta_{t}\\rangle)))}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From Proposition 1, we have that $\\operatorname*{min}_{i\\in[n]}y_{i}\\langle x_{i},\\bar{\\beta}_{\\infty}\\rangle>0$ and we denote this margin as $\\gamma$ . Now consider $k\\in[n]$ which is not a support vector, i.e, $\\begin{array}{r}{y_{k}\\langle x_{k},\\bar{\\beta}_{\\infty}\\rangle>\\operatorname*{min}_{i\\in[n]}y_{i}\\langle x_{i},\\bar{\\beta}_{\\infty}\\rangle}\\end{array}$ and without loss of generality assume that $\\begin{array}{r}{y_{1}\\langle x_{1},\\bar{\\beta}_{\\infty}\\rangle=\\operatorname*{min}_{1\\leq i\\leq n}y_{i}\\langle x_{i},\\bar{\\beta}_{\\infty}\\rangle}\\end{array}$ . We denote by $\\delta\\,=\\,\\langle y_{k}x_{k}\\mathrm{~-~}$ $y_{1}x_{1},\\bar{\\beta}_{\\infty}\\rangle>0$ the gap. Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\beta_{t})_{k}=\\frac{\\ell^{\\prime}\\left(C_{t}\\langle x_{k},\\,\\bar{\\beta}_{t}\\rangle\\right)}{\\ell^{\\prime}\\left(\\ell^{-1}\\left(\\sum_{i}\\ell\\left(C_{t}y_{i}\\langle x_{i},\\bar{\\beta}_{t}\\rangle\\right)\\right)\\right)}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\ell^{\\prime}\\left(C_{t}y_{k}\\langle x_{k},\\,\\bar{\\beta}_{t}\\rangle\\right)}{\\ell^{\\prime}\\left(C_{t}y_{1}\\langle x_{1},\\,\\bar{\\beta}_{t}\\rangle\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We write $\\bar{\\beta}_{t}\\,=\\,\\bar{\\beta}_{\\infty}+r_{t}$ where $(r_{t})_{t\\geq0}\\;\\in\\;\\mathbb{R}^{d}$ converges to 0. For $t$ big enough, we have that $y_{k}\\langle x_{k},\\bar{\\beta}_{t}\\rangle\\ge y_{k}\\langle x_{k},\\bar{\\beta}_{\\infty}\\rangle-\\frac{\\delta}{4}$ and $y_{1}\\langle\\bar{x_{1}},\\bar{\\beta}_{t}\\rangle\\leq y_{1}\\langle x_{1},\\bar{\\beta}_{\\infty}\\rangle+\\frac{\\delta}{4}$ . Therefore for $t$ large enough, since $\\ell^{\\prime}$ is negative and increasing: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\beta_{t})[k]\\leq\\frac{\\ell^{\\prime}\\left(C_{t}\\left(y_{k}\\langle x_{k},\\bar{\\beta}_{\\infty}\\rangle-\\delta/4\\right)\\right)}{\\ell^{\\prime}\\left(C_{t}\\left(y_{1}\\langle x_{1},\\bar{\\beta}_{\\infty}\\rangle+\\delta/4\\right)\\right)}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\ell^{\\prime}\\left(C_{t}\\left(\\gamma+\\delta/2+\\delta/4\\right)\\right)}{\\ell^{\\prime}\\left(C_{t}\\left(\\gamma+\\delta/2\\right)\\right)}\\underset{t\\to\\infty}{\\longrightarrow}0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last term converge to 0 due to the exponential tail of $-\\ell^{\\prime}$ and that $C_{t}\\to\\infty$ . ", "page_idx": 16}, {"type": "text", "text": "We here reformulate and prove Lemma 1. ", "page_idx": 16}, {"type": "text", "text": "Lemma 4 (Reformulation of Lemma 1). Denoting $\\begin{array}{r}{a(\\beta_{t}):=-\\ell^{\\prime}(\\ell^{-1}(\\sum_{i}\\ell(y_{i}\\langle x_{i},\\beta_{t}\\rangle)))>0}\\end{array}$ , we have that $\\begin{array}{r}{\\int_{0}^{t}a(\\beta_{s})\\mathrm{d}s\\underset{t\\to\\infty}{\\longrightarrow}+\\infty.}\\end{array}$ . For $\\ell(z)=\\exp(-z),$ , this translates to $\\begin{array}{r}{\\int_{0}^{t}L(\\beta_{s})\\mathrm{d}s\\rightarrow\\infty}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Recall that $\\begin{array}{r}{\\nabla\\phi(\\beta_{t})=\\nabla\\phi(\\beta_{0})+Z^{\\top}\\int_{0}^{t}a(\\beta_{s})q(\\beta_{s})\\mathrm{d}s}\\end{array}$ , therefore ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\nabla\\phi(\\beta_{t})\\|\\le\\|\\nabla\\phi(\\beta_{0})\\|+\\displaystyle\\sum_{i=1}^{n}\\|x_{i}\\|\\displaystyle\\int_{0}^{t}a(\\beta_{s})q(\\beta_{s})[i]\\mathrm{d}s}}\\\\ &{}&{\\le\\|\\nabla\\phi(\\beta_{0})\\|+\\big(\\displaystyle\\sum_{i=1}^{n}\\|x_{i}\\|\\big)\\displaystyle\\int_{0}^{t}a(\\beta_{s})\\mathrm{d}s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Where the first inequality is due to the triangle inequality and the second to the fact $q(\\beta)\\in(0,1]^{n}$ . Since the iterates diverge, we have from Assumption 2 that $\\|\\nabla\\phi(\\beta_{t})\\|\\underset{t\\to\\infty}{\\longrightarrow}\\infty$ and therefore that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int_{0}^{t}a(\\beta_{s})\\mathrm{d}s\\underset{t\\to\\infty}{\\longrightarrow}+\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B Differed proofs on the construction of $\\phi_{\\infty}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As mentioned in the main text, the following property highlights the fact that all \u2018reasonable\u2019 potentials have a horizon shape. ", "page_idx": 17}, {"type": "text", "text": "Proposition 2. If any of the three following conditions hold: $(i)\\,\\phi$ is a finite composition of polynomials, exponentials and logarithms, (ii) $\\phi$ is globally subanalytic, (iii) $\\phi$ is definable in a $o$ -minimal structure on $\\mathbb{R}$ ; then $\\phi$ admits a horizon shape $S_{\\infty}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Note that points (i) and (ii) are particular cases of (iii) [Dries, 1998, Bolte et al., 2007]. If $h$ is definable in a o-minimal structure, then so is the sublevel set $S_{c}$ for $c>0$ , and so is the normalization factor $R_{c}$ since it can be defined in first-order logic as ", "page_idx": 17}, {"type": "equation", "text": "$$\nR_{c}=\\{r\\in\\mathbb{R}\\,:\\,\\exists\\beta^{*}\\in S_{c},\\|\\beta^{*}\\|=r{\\mathrm{~and~}}\\forall\\beta\\in S_{c},\\|\\beta\\|\\leq r\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, $(\\bar{S}_{c})_{c>0}$ if a definable family of definable and compact sets. Then so is the family $(\\bar{S}_{t^{-1}})_{t\\in(0,1]}$ . Since all the sets belong to the unit ball of $\\mathbb{R}^{d}$ , they lie in the sets of compact subsets of $B(0,1)$ . This set is compact for the Hausdorff metric [Aliprantis and Border, 2006, Thm 3.85]; therefore, there exists a sequence $(t_{k})_{k\\in\\mathbb{N}}$ such that $t_{k}\\rightarrow0$ and $(\\bar{S}_{t_{k}^{-1}})_{k\\in\\mathbb{N}}$ converges to some set $\\bar{S}$ . ", "page_idx": 17}, {"type": "text", "text": "We can then apply Corollary 2 of Kocel-Cynk et al. [2014], which states that there exists a definable arc $\\gamma:(0,1]\\to(0,1]$ such that $\\dim_{\\tau\\rightarrow0}\\gamma(\\tau)=0$ and $\\begin{array}{r}{\\bar{S}^{\"}{=}\\operatorname*{lim}_{\\tau{\\rightarrow}0}\\bar{S}_{\\gamma(\\tau)^{-1}}}\\end{array}$ . This implies that the limit $\\bar{S}$ is uniquely defined and therefore that $\\textstyle\\operatorname*{lim}_{t\\to0}{\\bar{S}}_{t^{-1}}={\\bar{S}}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "The next corollary is a more general restatement of Corollary 1. It shows that the construction of $\\phi_{\\infty}$ enables to take the limit limt\u2207\u03d5t(\u03b2 $\\begin{array}{r}{\\operatorname*{lim}_{t}\\frac{\\^{\\circ}\\nabla\\phi(\\beta_{t})}{t}\\propto\\partial\\phi_{\\infty}(\\bar{\\beta}_{\\infty})}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Corollary 2. Assume that $\\phi$ admits a non-degenerate horizon shape $S_{\\infty}$ . Then its horizon function $\\phi_{\\infty}$ satisfies the following properties. ", "page_idx": 17}, {"type": "text", "text": "1. $\\phi_{\\infty}$ is convex and finite-valued on Rd, ", "page_idx": 17}, {"type": "equation", "text": "$$\n(a)\\ \\left\\|\\beta_{s}\\right\\|\\to\\infty,\\ \\ \\left(b\\right)\\ {\\frac{\\beta_{s}}{\\left\\|\\beta_{s}\\right\\|}}\\to{\\bar{\\beta}}\\ f o r\\,s o m e\\ \\bar{\\beta}\\in\\mathbb{R}^{d},\\ \\ \\left(c\\right)\\ {\\frac{\\nabla\\phi(\\beta_{s})}{\\left\\|\\nabla\\phi(\\beta_{s})\\right\\|}}\\to\\bar{g}\\ f o r\\,s o m e\\ \\bar{\\beta}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then $\\bar{g}$ is proportional to a subgradient of $\\phi_{\\infty}$ at $\\bar{\\beta}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{g}\\in\\lambda\\partial\\phi_{\\infty}(\\bar{\\beta})\\quad f o r\\,s o m e\\,\\,\\lambda>0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The sequence of sets $(\\bar{S}_{c})$ is contained in the compact ball $B(0,1)$ ; therefore, Hausdorff convergence is equivalent to Painlev\u00e9-Kuratowski convergence [Rockafellar and Wets, 1998, Section 4.C]. Hence, as $\\left(\\bar{S}_{c}\\right)$ are convex, so is their limit $S_{\\infty}$ [Rockafellar and Wets, 1998, Prop 4.15]. It follows that $\\phi_{\\infty}$ is convex [Rockafellar and Wets, 1998, Ex 3.50]. ", "page_idx": 17}, {"type": "text", "text": "Since $S_{\\infty}$ is non-degenerate, there exists a radius $r_{0}$ such that $B(0,r_{0})\\subset S_{\\infty}$ , which implies that $\\phi_{\\infty}(\\beta)$ is finite-valued for every $\\beta$ . ", "page_idx": 17}, {"type": "text", "text": "To prove point (ii), consider the sequence of functions $(\\eta_{c})_{c>0}$ formed by the indicators of convex sets $\\bar{S}_{c}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta_{c}(\\beta)=I_{\\bar{S}_{c}}(\\beta)=\\left\\{{0\\atop+\\infty}\\atop\\mathrm{~otherwise.}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that the epigraph of $\\eta_{c}$ is $\\bar{S}_{c}\\times\\mathbb{R}_{+}$ ; these sets also converge to $S_{\\infty}\\times\\mathbb{R}_{+}$ [Rockafellar and Wets, 1998, Ex 4.29], from which we conclude that function $\\eta_{c}$ converge epigraphically to the indicator function $\\eta_{\\infty}$ of $S_{\\infty}$ $\\langle\\eta_{\\infty}=I_{S_{\\infty}}$ ). We can then apply Attouch\u2019s theorem [Attouch and Beer, 1993, Combari and Thibault, 1998] ensuring that the graph of the subdifferentials of $\\eta_{c}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{G}(\\partial\\eta_{c})=\\{(\\beta,g)\\,:\\,g\\in\\partial\\eta_{c}(\\beta)\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "converge in Painlev\u00e9-Kuratowski sense to the graph $\\mathcal{G}(\\partial\\eta_{\\infty})$ of subdifferential of $\\eta_{\\infty}$ . This means that if a sequence $(\\beta_{c},g_{c})_{c>0}$ such that $(\\beta_{c},g_{c})\\in\\mathcal G(\\partial\\eta_{c})$ for every $c>0$ converges, then its limit belongs to ${\\mathcal{G}}(\\partial{\\eta}_{\\infty})$ . ", "page_idx": 17}, {"type": "text", "text": "Consider now a sequence $(\\beta_{s})_{s>0}$ satisfying the conditions described in (ii). Since it diverges to infinity and $\\phi$ is coercive, we have $\\phi(\\beta_{s})\\rightarrow\\infty$ , and we may assume w.l.o.g that $\\phi(\\beta_{s})>0$ for all $s$ . ", "page_idx": 18}, {"type": "text", "text": "We have by definition of sublevel sets that $\\beta_{s}\\in S_{\\phi(\\beta_{s})}$ , and therefore the gradient $\\nabla\\phi(\\beta_{s})$ belongs to the normal cone of $S_{\\phi(\\beta_{s})}$ at $\\beta_{s}$ (indeed, the gradient is orthogonal to the level sets; see e.g., [Courant and John, 1989, Chapter 1.5]). Since the normal cone of a convex set is the subdifferential of its indicator [Rockafellar, 1970, Section 23], we thus have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla\\phi(\\beta_{s})\\in\\partial I_{S_{\\phi(\\beta_{s})}}(\\beta_{s}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Consider now the normalized levels sets as defined in (4). Denoting ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{\\beta}_{s}=\\frac{\\beta_{s}}{R_{\\phi(\\beta_{s})}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we have $\\bar{\\beta}_{s}\\in\\bar{S}_{\\phi(\\beta_{s})}$ and thus by simple rescaling (8) becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla\\phi(\\beta_{s})\\in\\partial I_{\\bar{S}_{\\phi(\\beta_{s})}}\\left(\\bar{\\beta}_{s}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\partial I_{\\bar{S}_{c}}$ is a cone (the normal cone to $\\bar{S}_{c.}$ ), this also holds for any positive multiple of $\\nabla\\phi(\\beta_{s})$ . We deduce that for every $s>0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\bar{\\beta}_{s},\\frac{\\nabla\\phi(\\beta_{s})}{\\|\\nabla\\phi(\\beta_{s})\\|}\\right)\\in\\mathcal{G}(\\partial\\eta_{\\phi(\\beta_{s})}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that since $\\bar{\\beta}_{s}$ belongs to the normalized level sets, this sequence is bounded. We can extract a subsequence $(\\bar{\\beta}_{s_{k}},\\frac{\\nabla\\phi(\\bar{\\beta}_{s_{k}})}{\\|\\nabla\\phi(\\beta_{s_{k}})\\|})_{k\\geq0}$ which converges to a limit point $({\\hat{\\beta}},{\\hat{g}})$ . By the previous remark on graphical convergence of subdifferentials, we have $(\\hat{\\beta},\\hat{g})\\in\\mathcal{G}(\\partial I_{S_{\\infty}})$ , i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{g}\\in\\partial I_{S_{\\infty}}(\\hat{\\beta}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We need to prove that $\\hat{\\beta}$ is not 0. Since $\\phi$ is strictly convex, the level set $\\{\\phi(\\beta)=c\\}$ is exactly the boundary of the sublevel set $\\{\\phi(\\beta)\\leq c\\}$ . Therefore, $\\beta_{s}$ lies on the boundary of $S_{\\phi(\\beta_{s})}$ , and hence so does $\\bar{\\beta}_{s}$ lie on the boundary of $\\bar{S}_{\\phi(\\beta_{s})}$ . Since 0 is in the interior of $S_{\\infty}$ , it also belongs to the interior of $\\bar{S}_{\\phi(\\beta_{s})}$ for $s$ larger than some $s_{0}$ . Then, there exists $r_{0}>0$ such that $B(0,r_{0})\\subset\\bar{S}_{\\phi(\\beta_{s})}$ for $s\\geq s_{0}$ . By definition of boundary, we then have for $s\\geq s_{0}\\,\\,\\|\\bar{\\beta}_{s}\\|>r_{0}$ , which leads to $\\|{\\hat{\\beta}}\\|>0$ . ", "page_idx": 18}, {"type": "text", "text": "To achieve the desired result; we need to relate $({\\hat{\\beta}},{\\hat{g}})$ to $({\\bar{\\beta}},{\\bar{g}})$ . First, notice that by construction we have necessarily $\\begin{array}{r}{\\hat{g}=\\bar{g}=\\operatorname*{lim}_{s\\rightarrow\\infty}\\nabla\\phi(\\beta_{s})/\\|\\nabla\\phi(\\bar{\\beta}_{s})\\|}\\end{array}$ . Then, note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\bar{\\beta}}=\\operatorname*{lim}_{k\\to\\infty}{\\frac{\\beta_{s_{k}}}{\\|\\beta_{s_{k}}\\|}},\\quad{\\hat{\\beta}}=\\operatorname*{lim}_{k\\to\\infty}{\\frac{\\beta_{s_{k}}}{R_{\\phi(\\beta_{s_{k}})}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking the norm of the second limit, we have \u2225\u03b2\u02c6\u2225= limk\u2192\u221eR\u2225\u03d5\u03b2(s\u03b2ks \u2225 ) . Injecting back in the first limit yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{\\beta}=\\operatorname*{lim}_{k\\to\\infty}\\frac{\\beta_{s_{k}}}{R_{\\phi(\\beta_{s_{k}})}}\\cdot\\frac{R_{\\phi(\\beta_{s_{k}})}}{\\|\\beta_{s_{k}}\\|}=\\frac{\\hat{\\beta}}{\\|\\hat{\\beta}\\|}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, (9) becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{g}\\in\\partial I_{S_{\\infty}}(\\lVert\\hat{\\boldsymbol{\\beta}}\\rVert\\bar{\\boldsymbol{\\beta}}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This means that $\\bar{g}$ belongs to the normal cone of $S_{\\infty}$ at $\\|{\\hat{\\beta}}\\|{\\bar{\\beta}}$ [Rockafellar, 1970, Sec. 23]. We note the level set $\\{\\beta:\\,\\phi_{\\infty}(\\beta)\\leq\\phi_{\\infty}\\left(\\|\\hat{\\beta}\\|\\bar{\\beta}\\right)\\}$ is exactly $\\tau S_{\\infty}$ for some $\\tau>0$ . We use Corollary 23.7.1 from Rockafellar [1970] which states that if a vector is in the normal cone of the level set of $\\phi_{\\infty}$ , then it must be a positive multiple of a subgradient. This implies that that there exists $\\lambda\\geq0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{g}\\in\\lambda\\partial\\phi_{\\infty}(\\|\\hat{\\beta}\\|\\bar{\\beta}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, $\\lambda>0$ since $\\|{\\bar{g}}\\|=1$ , and $\\partial\\phi_{\\infty}(\\|\\hat{\\beta}\\|\\bar{\\beta})=\\partial\\phi_{\\infty}(\\bar{\\beta})$ by positive homogenity of $\\phi_{\\infty}$ . ", "page_idx": 18}, {"type": "text", "text": "We can now prove our main result, which we restate here. ", "page_idx": 18}, {"type": "text", "text": "Theorem 2. Assume that $\\phi$ admits a non-degenerate horizon shape and let $\\phi_{\\infty}$ be its horizon function. Assuming that the following $\\phi_{\\infty}$ -max-margin problem has a unique minimiser, then the mirror flow normalised iterates \u03b2\u00aft = \u03b2\u03b2t converge towards a vector $\\bar{\\beta}_{\\infty}$ and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bar{\\beta}_{\\infty}\\propto\\underset{\\bar{\\beta}\\in\\mathbb{R}^{d}}{\\arg\\operatorname*{min}}\\ \\phi_{\\infty}(\\bar{\\beta})\\quad u n d e r\\ t h e\\ c o n s t r a i n t\\quad\\underset{i\\in[n]}{\\operatorname*{min}}\\ y_{i}\\langle x_{i},\\bar{\\beta}\\rangle\\geq1,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the symbol $\\propto$ denotes positive proportionality. ", "page_idx": 19}, {"type": "text", "text": "The proof essentially follows exactly the same lines as in Section 3 but taking into account the fact that the loss is not exactly the exponential one. ", "page_idx": 19}, {"type": "text", "text": "Proof. Recall the definitions of the quantities $a_{t}$ and $q_{t}$ given above Equation (7) which enable to write: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla\\phi(\\beta_{t})=\\nabla\\phi(\\beta_{0})+Z^{\\top}\\int_{0}^{t}a_{s}q_{s}\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similar to the time change we performed in Section 3, we consider $\\begin{array}{r}{\\theta(t)=\\int_{0}^{t}a_{s}\\mathrm{d}s}\\end{array}$ . From Lemma 4, $\\theta$ is a bijection over $\\mathbb{R}_{\\geq0}$ and perform the time change $\\tilde{\\beta}_{t}=\\beta_{\\theta^{-1}(t)}$ . Due to the chain rule, after the time change and dropping the tilde notation we obtain: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla\\phi(\\beta_{t})=\\nabla\\phi(\\beta_{0})+Z^{\\top}\\int_{0}^{t}q_{s}\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Dividing by $t$ we get: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{t}\\nabla\\phi(\\beta_{t})=\\frac{1}{t}\\nabla\\phi(\\beta_{0})+Z^{\\top}\\bar{q}_{t},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{q}_{t}:=\\frac{1}{t}\\int_{0}^{t}q_{s}\\mathrm{d}s}\\end{array}$ corresponds to the average of $(q_{s})_{s\\leq t}$ . ", "page_idx": 19}, {"type": "text", "text": "Extracting a convergent subsequence: We now consider the normalised iterates $\\begin{array}{r}{\\bar{\\beta}_{t}=\\frac{\\beta_{t}}{\\|\\beta_{t}\\|}}\\end{array}$ and up to an extraction we get that $\\bar{\\beta}_{t}\\to\\bar{\\beta}_{\\infty}$ . Since $q_{t}$ is a bounded function, up to a second extraction, we have that $q_{t}\\to q_{\\infty}$ , and the same holds for its average: $\\bar{q}_{t}\\to q_{\\infty}$ . Taking the limit in Equation (10) we immediately obtain that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t}\\frac{1}{t}\\nabla\\phi(\\beta_{t})=Z^{\\top}q_{\\infty},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which also means that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\nabla\\phi(\\beta_{t})}{\\|\\nabla\\phi(\\beta_{t})\\|}\\underset{t\\to\\infty}{\\longrightarrow}\\frac{Z^{\\top}q_{\\infty}}{\\|Z^{\\top}\\bar{q}_{\\infty}\\|}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can now directly apply Corollary 2 and there exists $\\lambda>0$ such that: ", "page_idx": 19}, {"type": "equation", "text": "$$\nZ^{\\top}q_{\\infty}\\in\\lambda\\partial\\phi_{\\infty}(\\bar{\\beta}_{\\infty})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The end of the proof is then as explained in Section 3. ", "page_idx": 19}, {"type": "text", "text": "Finally we recall and prove Theorem 3 which provides a simple formula for the horizon function in the case of separable potentials. ", "page_idx": 19}, {"type": "text", "text": "Theorem 3. Under Assumption 4, the potential $\\phi$ admits a non-degenerate horizon shape and its horizon function is such that there exists $\\lambda>0$ such that for every $\\tilde{\\beta}\\in\\mathbb{R}^{d}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\phi_{\\infty}({\\bar{\\beta}})=\\lambda\\operatorname*{lim}_{\\eta\\to0}\\eta\\cdot\\varphi^{-1}\\left(\\phi\\left({\\frac{\\bar{\\beta}}{\\eta}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Lipschitzness, upper and lower boundedness. For $\\eta\\,>\\,0$ , let us denote by $h_{\\eta}\\,:\\,\\beta\\,\\mapsto$ $\\eta\\cdot\\varphi^{-1}(\\phi(\\beta/\\eta))$ and notice that $\\begin{array}{r}{\\nabla h_{\\eta}(\\beta)=\\big(\\frac{\\varphi^{\\prime}(\\beta_{k}/\\eta)}{\\varphi^{\\prime}(\\varphi^{-1}(\\sum_{i}\\varphi(\\beta_{i}/\\eta)))}\\big)_{k\\in[d]}\\,\\geq\\,0}\\end{array}$ Since $\\varphi\\geq0$ and that $\\varphi^{-1}$ and $\\varphi^{\\prime}$ are increasing we get that that $\\nabla h_{\\eta}(\\beta)\\,\\in\\,[0,1]^{d}$ . Therefore $(h_{\\eta})_{\\eta>0}$ are uniformly Lipschitz-continuous. Consequently, for all $\\beta$ , $h_{\\eta}(\\beta)$ is upper-bounded independently of $\\eta$ . Lastly, since $\\varphi\\geq0$ , notice that $h_{\\eta}(\\beta)\\bar{>}\\operatorname*{min}_{i}|\\beta_{i}|>0$ for all $\\beta\\neq0$ . ", "page_idx": 20}, {"type": "text", "text": "Point-wise and epi-convergence of $h_{\\eta}$ . For all $\\bar{\\beta}$ , by composition, $\\eta\\,\\mapsto\\,\\eta\\cdot\\varphi^{-1}(\\phi(\\bar{\\beta}/\\eta))$ is a definable function, the monotonicity Lemma [Van den Dries and Miller, 1996] (Theorem 4.1) ensures that it has a unique limit in $\\mathbb{R}$ which we denote $h_{0}(\\beta)$ . From the uniform Lipschitzness of $h_{\\eta}$ , we get that $(\\eta,\\beta)\\in\\mathbb{R}_{\\ge0}\\times\\mathbb{R}^{d}\\mapsto h_{\\eta}(\\beta)$ is continuous. Hence for all sequence $\\eta_{k}\\to0$ , we get that $h_{\\eta_{k}}$ epi-converges to $h_{0}$ . Therefore $(\\mathrm{epi}\\,h_{\\eta_{k}})_{k}$ converges in the Painlev\u00e9\u2013Kuratowski sense towards epi $h_{\\eta_{0}}$ . ", "page_idx": 20}, {"type": "text", "text": "Link between the level sets of $h_{\\eta}$ and those of $\\phi$ . To conclude the proof it remains to notice that for all $c\\geq0$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\{\\beta\\in\\mathbb{R}^{d},\\phi(\\beta)\\leq c\\}=\\frac{1}{\\eta}\\{\\bar{\\beta}\\in\\mathbb{R}^{d},h_{\\eta}(\\bar{\\beta})\\leq\\eta\\varphi^{-1}(c)\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore letting $\\eta_{c}=1/\\varphi^{-1}(c)$ we get that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\eta_{c}\\cdot S_{c}=\\{\\bar{\\beta}\\in\\mathbb{R}^{d},h_{\\eta_{c}}(\\bar{\\beta})\\leq1\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This simply means that $\\eta_{c}$ is an appropriate normalising quantity, it replaces the normalisation by the radius of $S_{c}$ . Since $\\{\\bar{\\beta}\\,\\in\\,\\dot{\\mathbb{R}}^{\\dot{d}},\\dot{h_{\\eta_{c}}}(\\bar{\\beta})\\,\\le\\,1\\}$ converges in the Painlev\u00e9\u2013Kuratowski sense towards $\\{\\bar{\\beta}\\in\\mathbb{R}^{d},h_{0}(\\bar{\\beta})\\leq1\\}$ , we get that $R_{c}\\eta_{c}\\cdot\\bar{S}_{c}$ converges towards the same set. However, with our previous construction, we also have that $\\bar{S}_{c}$ converges towards $\\bar{S}_{\\infty}$ . The sets $\\bar{S}_{\\infty}$ and $\\{\\bar{\\beta}\\in\\mathbb{R}^{d},\\mathbf{\\dot{\\}}{h}_{0}(\\bar{\\beta})\\leq1\\}$ are therefore proportional and $h_{0}\\propto\\phi_{\\infty}$ which concludes the proof. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 21}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 21}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 21}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 21}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 21}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Theorems 2 and 3 give the results which are claimed in the abstract and introduction. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We included a \"Limitations\" paragraph at the end of the main paper. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The main assumptions on the loss, potential and dataset are given at the beginning of the paper. If an additional assumption is required, it is clearly stated in the result. The proofs are all deferred to the appendix but we clearly state in the main text where they can be found. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide extremely toy examples which are very easy to reproduce. The dataset is shown in Figure 1 and, as stated in Section 5, we perform mirror descent over the dataset with the exponential loss. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The code behind the experiments is straightforward and can easily be reproduced. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 23}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The considered potentials and loss are given. The value of the step-size is not given as it does not have any relevance. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: The use of error bars is not relevant in our work: there is no source of stochasticity. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: It is clear that our experiments can easily be reproduced by any computer. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have checked the Code of Ethics and our work respects it. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Given the theoretical aspect of our work, we don\u2019t think are work leads to any direct positive nor negative societal impacts. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: There is no use of existing assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]