[{"figure_path": "wiMaws0FWB/figures/figures_1_1.jpg", "caption": "Figure 1: Mirror descent is performed using 3 different potentials on the same toy 2d dataset. Left: the losses converge to zero. Center: the iterates converge in direction towards 3 different vectors \u03b2\u221e, the 3 lines passing through the origin correspond to the associated separating hyperplanes. Right: the limit directions are each proportional to arg min\u221e(\u03b2) under the constraint mini yi(xi,\u03b2) \u2265 1 for their respective \u221e's, as predicted by our theory (Theorem 1). The full trajectories are plotted Figure 4 and we refer to Section 5 for more details.", "description": "This figure shows a comparison of three different mirror descent algorithms on a linearly separable 2D dataset.  The left panel shows the training loss curves for gradient descent (GD) and two mirror descent methods (MD1 and MD2) with different potentials.  The center panel illustrates the directional convergence of the iterates toward different separating hyperplanes, highlighting how the choice of potential influences the algorithm's behavior.  The right panel visualizes the maximum margin solutions corresponding to each potential, illustrating that the limiting direction is proportional to the argument that minimizes the horizon function of the potential under a margin constraint, as stated by Theorem 1.  The full trajectories are shown in Figure 4, and more detail is provided in Section 5.", "section": "1.1 Informal statement of the main result"}, {"figure_path": "wiMaws0FWB/figures/figures_5_1.jpg", "caption": "Figure 2: Left two: Sketch of the level lines of two different potentials \u03c6(1), \u03c6(2) : R2 \u2192 R. Right two: Their corresponding horizon functions \u03c6(1)\u221e, \u03c6(2)\u221e as defined in Section 4.1.", "description": "This figure illustrates the concept of horizon functions. The left two subfigures show level lines of two different potential functions, \u03c6(1) and \u03c6(2).  The right two subfigures display the corresponding horizon functions, \u03c6(1)\u221e and \u03c6(2)\u221e, which capture the asymptotic shape of the potential functions 'at infinity'.  The horizon functions are crucial for characterizing the implicit bias of mirror descent.", "section": "3.2 General potential: introducing the horizon function \u03c6\u221e"}, {"figure_path": "wiMaws0FWB/figures/figures_6_1.jpg", "caption": "Figure 3: Illustration of the construction of the horizon shape S\u221e. Left: the sub-level sets Sc change of shape and are increasing. Middle: in order to avoid the shapes blowing up, we normalise them to keep them in the unit ball (here we choose the arbitrary constraining norm to be the l\u2081-norm). Right: the normalised sub-level sets \u0160c converge to a limiting set S\u221e for the Hausdorff distance.", "description": "This figure illustrates the construction of the horizon shape S\u221e, which is crucial for understanding the implicit bias of mirror flow in the classification setting.  The left panel shows how the sub-level sets Sc of a potential function change shape and increase as c grows.  To prevent the sub-level sets from blowing up, the middle panel shows the normalized sub-level sets \u0160c, which are constrained to the unit ball using the l1-norm.  Finally, the right panel shows that as c approaches infinity, the normalized sub-level sets converge to a limiting set S\u221e, using the Hausdorff distance, which defines the horizon function \u03c6\u221e.", "section": "4.1 Construction of the horizon function \u03c6\u221e"}, {"figure_path": "wiMaws0FWB/figures/figures_8_1.jpg", "caption": "Figure 4: Mirror flow trajectories on a 2-dimensional dataset for three different potentials (exact same setting as in Figure 1). Left: the iterates diverge to infinity and the directional convergence depends on the choice of potential. Right: the normalised iterates converge towards their respective \u221e-maximum-margin predictors (illustrated by stars), as predicted by Theorem 2.", "description": "This figure shows the trajectories of mirror flow for three different potentials (GD, MD1, MD2) on a 2D linearly separable dataset. The left panel illustrates the iterates diverging to infinity, demonstrating that the algorithm's behavior depends on the potential used. The right panel presents the rescaled trajectories, where the iterates are normalized. They converge to their respective \u221e-maximum-margin predictors, confirming the theoretical findings of Theorem 2, which formally characterizes the implicit bias of mirror flow for separable classification problems.", "section": "5 Applications and experiments"}]