{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model that is extensively used and adapted in the current work."}, {"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-01", "reason": "This paper describes GPT-4, a large language model that is used for generating structured rationales in the proposed method."}, {"fullname_first_author": "Marco Tulio Ribeiro", "paper_title": "\"why should i trust you?\" explaining the predictions of any classifier", "publication_date": "2016-08-01", "reason": "This paper introduces LIME, a well-known model explanation technique that is compared to the proposed method for faithfulness."}, {"fullname_first_author": "Cynthia Rudin", "paper_title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "publication_date": "2019-01-01", "reason": "This paper emphasizes the importance of using interpretable models in high-stakes applications, which motivates the work on ensuring correct rationales."}, {"fullname_first_author": "Sachit Menon", "paper_title": "Visual classification via description from large language models", "publication_date": "2022-01-01", "reason": "This paper proposes a method of using large language models for visual classification, a related approach that is compared to the proposed method."}]}