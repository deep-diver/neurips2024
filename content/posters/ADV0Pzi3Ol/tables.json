[{"figure_path": "ADV0Pzi3Ol/tables/tables_3_1.jpg", "caption": "Table 1: Weakly-supervised segmentation accuracy on ImageNet-Seg [45]. We threshold explanation heatmaps from CLIP-ViT-L-14 as segmentation masks. Our method outperforms existing explanation methods in segmentation accuracy, demonstrating the high faithfulness of our explanations.", "description": "This table presents the results of a weakly-supervised segmentation task performed on the ImageNet-Seg dataset.  The performance of various explanation methods is compared using three metrics: Pixel Accuracy, mean Intersection over Union (mIoU), and mean Average Precision (mAP). The proposed method outperforms all other methods across all three metrics.  This highlights the accuracy and faithfulness of the proposed explanation method.", "section": "3.2 Faithful Explanation Method"}, {"figure_path": "ADV0Pzi3Ol/tables/tables_5_1.jpg", "caption": "Table 2: Evaluation results of rationale quality. Both machine and human evaluators receive the same instructions about the metrics. The scores for all three metrics are nearly identical between machine and human evaluators, indicating that over 90.3% of our rationales are of high quality.", "description": "This table presents the results of a human and machine evaluation of the quality of the structured rationale dataset created by the authors. Three metrics were used to assess quality: Factual Consistency, Comprehensiveness, and Visual Disentanglement.  Each metric was scored on a 5-point Likert scale by four human evaluators and two large language models (GPT-4 and GPT-4v).  The results show a strong agreement between human and machine evaluations, indicating that the vast majority (over 90%) of the rationales are of high quality.", "section": "4.1 Evaluation of Rationale Quality"}, {"figure_path": "ADV0Pzi3Ol/tables/tables_6_1.jpg", "caption": "Table 3: Comparison of prediction accuracy (%) on nine benchmark datasets. Our results are on the average of three trials of experiments using different random seeds. We highlight the best results and the second best results. Surprisingly, different from most interpretability methods that compromise benchmark performance, our method also enhances prediction accuracy.", "description": "This table presents a comparison of prediction accuracy across nine benchmark datasets for various models, including the proposed model and several state-of-the-art models.  It showcases zero-shot and linear probe accuracies, demonstrating the model's performance and comparing it to fine-tuned versions of existing models.  Noteworthy is the observation that the proposed method enhances prediction accuracy, unlike most interpretability methods that often compromise accuracy for improved interpretability.", "section": "4.4 Evaluation on Prediction Correctness"}, {"figure_path": "ADV0Pzi3Ol/tables/tables_7_1.jpg", "caption": "Table 4: Comparison of rationale localizability on CUB-Part [67] and PartImageNet [68]. As detailed in Sec. 4.3, we threshold rationales' explanation heatmaps as segmentation masks and calculate their mIoU (\u2191) with ground truth masks of corresponding object parts. Our model significantly improves the localization accuracy of fine-grained object parts. Full table in Appendix C.", "description": "This table presents a comparison of rationale localizability, a metric evaluating how well a model can pinpoint the visual evidence supporting its rationales.  The comparison is made across different models (CLIP, DeCLIP, NegCLIP, FILIP, PyramidCLIP, fine-tuned CLIP variants, and the proposed 'Ours' model) on two datasets: CUB-Part and PartImageNet.  The results are shown as mean Intersection over Union (mIoU) scores for several object parts (head, beak, tail, wings, eyes, torso) and an average mIoU across all parts for each dataset. Higher mIoU values indicate better localization accuracy. The table highlights the superior performance of the proposed model in accurately localizing the visual evidence relevant to each rationale.", "section": "4.5 Evaluation on Rationale Correctness"}, {"figure_path": "ADV0Pzi3Ol/tables/tables_8_1.jpg", "caption": "Table 3: Comparison of prediction accuracy (%) on nine benchmark datasets. Our results are on the average of three trials of experiments using different random seeds. We highlight the best results and the second best results. Surprisingly, different from most interpretability methods that compromise benchmark performance, our method also enhances prediction accuracy.", "description": "This table presents a comparison of prediction accuracy across nine benchmark datasets (C10, C100, CUB, CAL, PETS, F101, SUN, CARS, DTD) for various models. The models include CLIP, DeCLIP, NegCLIP, FILIP, PyramidCLIP, CLIP fine-tuned (CLIP-ft), CLIP fine-tuned with vision-encoder-only (CLIP-ft-vision), and the proposed method (Ours).  The results are averaged across three trials, each with different random seeds, showcasing the consistency and robustness of the proposed method.  The table highlights the best and second-best performances for each dataset.  Notably, unlike many interpretability methods that often sacrifice accuracy, the proposed method achieves higher prediction accuracy than other state-of-the-art models across the board.", "section": "4.4 Evaluation on Prediction Correctness"}, {"figure_path": "ADV0Pzi3Ol/tables/tables_8_2.jpg", "caption": "Table 6: Comparison of zero-shot image-text retrieval accuracy (%). Double-correct prediction enhances the model's visual understanding. (Note that NegCLIP is trained on MSCOCO [66])", "description": "This table presents the zero-shot image-text retrieval accuracy of different models on two benchmark datasets, MSCOCO and Flickr30K.  The accuracy is measured in two directions: Image-to-Text (I2T) and Text-to-Image (T2I).  The table shows that the proposed model ('Ours') significantly improves the retrieval accuracy compared to existing state-of-the-art models, particularly in the I2T direction on the MSCOCO dataset.  This improvement highlights the effectiveness of the model in integrating structured rationales to achieve a deeper and more accurate understanding of visual concepts, ultimately leading to better performance in retrieval tasks.", "section": "4.7 Evaluation on Retrieval Tasks"}, {"figure_path": "ADV0Pzi3Ol/tables/tables_8_3.jpg", "caption": "Table 8: Comparison of rationale-based prediction accuracy (%) on ImageNet [18].", "description": "This table compares the prediction accuracy of different models on the ImageNet dataset when using rationale-based methods.  It shows the performance of CLIP, CLIP fine-tuned (CLIP-ft), and the proposed method ('Ours').  The comparison is made under three conditions: using structured concepts for rationales, using random strings instead of concepts, and the full method. The delta column shows the improvement over the CLIP baseline for each condition.", "section": "4.4 Evaluation on Prediction Correctness"}, {"figure_path": "ADV0Pzi3Ol/tables/tables_17_1.jpg", "caption": "Table 9: The machine evaluation results on the quality of the full rationale dataset.", "description": "This table presents the results of evaluating the quality of the full rationale dataset using machine evaluators (GPT-4v and GPT-40).  Three metrics are used to assess the rationales: Factual Consistency, Comprehensiveness, and Visual Disentanglement. Each metric is scored on a 5-point Likert scale, with higher scores indicating better performance. The table shows the average score for each metric across both GPT-4v and GPT-40.", "section": "4.1 Evaluation of Rationale Quality"}, {"figure_path": "ADV0Pzi3Ol/tables/tables_17_2.jpg", "caption": "Table 3: Comparison of prediction accuracy (%) on nine benchmark datasets. Our results are on the average of three trials of experiments using different random seeds. We highlight the best results and the second best results. Surprisingly, different from most interpretability methods that compromise benchmark performance, our method also enhances prediction accuracy.", "description": "This table presents a comparison of prediction accuracy across nine benchmark datasets for various models, including the proposed model and several state-of-the-art baselines.  The accuracy is reported for three different settings: zero-shot, linear probe, and fine-tuning.  The table highlights that the proposed model achieves superior performance across all settings, and notably, unlike many interpretability methods, it does not compromise prediction accuracy.", "section": "4.4 Evaluation on Prediction Correctness"}, {"figure_path": "ADV0Pzi3Ol/tables/tables_18_1.jpg", "caption": "Table 11: Datasets for classification task.", "description": "This table presents the details of nine benchmark datasets used for image classification in the paper. For each dataset, it lists its abbreviation, the number of classes, the size of the training set, and the size of the testing set.  The datasets vary significantly in size and number of classes, allowing for a comprehensive evaluation of the model's performance on diverse visual recognition tasks.", "section": "4.2 Benchmark Datasets and Implementation Details"}]