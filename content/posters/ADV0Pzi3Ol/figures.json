[{"figure_path": "ADV0Pzi3Ol/figures/figures_0_1.jpg", "caption": "Figure 1: Unsafe prediction examples. Correct prediction, incorrect rationale: CLIP identifies a red light, but wrongly based on red balloons. Incorrect prediction, correct rationale: GPT-4V incorrectly predicts a closed door, yet based on plausible visual evidence.", "description": "This figure showcases two examples where large language models (LLMs) produce predictions that are either factually incorrect or based on flawed reasoning. In the first example, CLIP correctly identifies a red traffic light in an image but incorrectly attributes this identification to a cluster of red balloons in the background. The second example shows GPT-4V making an incorrect prediction (that a truck's door is closed) based on a seemingly plausible rationale derived from visual details such as the door's alignment with the truck's body and the visibility of the side mirror. These examples highlight the importance of evaluating not only the accuracy of model predictions but also the validity of their underlying rationales.", "section": "1 Introduction"}, {"figure_path": "ADV0Pzi3Ol/figures/figures_2_1.jpg", "caption": "Figure 2: Our structured rationales capture the major attributes and their sub-attributes that lead to the recognition of objects. Our dataset offers over 4,000 unique rationales covering all 1,000 categories from ImageNet [18].", "description": "This figure illustrates the structure of the structured rationale dataset created for the paper.  It shows examples of ontologies for three ImageNet categories: American Robin, Airliner, and Wombat. Each ontology is a tree-like structure where the root node is the category.  The nodes below the root represent attributes of that category, and the leaf nodes show sub-attributes. This structured approach helps to represent the detailed reasoning process involved in visual recognition. The figure highlights that the dataset contains over 4,000 unique rationales, providing detailed reasoning for each of the 1,000 ImageNet categories.", "section": "3.1 Structured Rationale Dataset"}, {"figure_path": "ADV0Pzi3Ol/figures/figures_3_1.jpg", "caption": "Figure 3: Multi-head Self Attention (MSA) accumulated mean-ablation study. Based on Eq. 2, we replace the direct effects of MSAS up to a specific layer with their mean values calculated across the ImageNet [18] validation set. Most of the performance gains can be attributed to the final layers of the ViT.", "description": "This figure shows the results of a multi-head self-attention (MSA) ablation study on Vision Transformer (ViT) models.  By replacing the direct effects of MSAs up to a specific layer with their mean values (calculated across ImageNet validation set), the researchers assessed the impact on ImageNet accuracy. The plot demonstrates that most performance gains come from the final layers of the ViT architecture.", "section": "3.2 Faithful Explanation Method"}, {"figure_path": "ADV0Pzi3Ol/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative results of rationale disentanglement and localization. The rationales' visual evidence of the CLIP model [1] typically highlights the entire object, lacking precise localization. In contrast, our model can correctly localize rationales, thereby enhancing trust in its predictions.", "description": "This figure shows a qualitative comparison of rationale explanations generated by the CLIP model and the proposed model.  The top row displays the original images and the corresponding rationales (e.g., for an American Robin, the rationales might be \"gray wings\", \"pointed beak\", etc.). The middle row shows the attention heatmaps generated by CLIP for each rationale. Note that CLIP's heatmaps often highlight the entire object, rather than focusing specifically on the visual evidence relevant to a particular rationale. The bottom row shows the attention heatmaps generated by the proposed model; these heatmaps demonstrate much better localization of the evidence supporting each rationale, highlighting only the relevant parts of the image.", "section": "4.5 Evaluation on Rationale Correctness"}, {"figure_path": "ADV0Pzi3Ol/figures/figures_9_1.jpg", "caption": "Figure 5: Qualitative results of zero-shot text-to-image retrieval on MSCOCO [66]. The task is to retrieve the top-5 images with a given rationale presented. The CLIP results reveal a significant entangle of rationales with a specific category, such as \u201clong neck\u201d with giraffes and \u201cwings\u201d with airliners. In contrast, our model treats rationales independently from categories, thus offering diverse retrieval results. For example, the \u201clong neck\u201d found in birds, giraffes, dears, and bottles.", "description": "This figure shows a qualitative comparison of zero-shot text-to-image retrieval results between the CLIP model and the proposed model.  The task was to retrieve the top 5 images given a specific rationale (e.g., \"a photo of long neck\", \"a photo of wings\").  CLIP's results show a strong bias towards retrieving images of specific categories (giraffes for long necks, airplanes for wings). In contrast, the proposed model shows a more diverse and accurate retrieval, demonstrating an improved understanding of the rationales independently from the category.", "section": "4.7 Evaluation on Retrieval Tasks"}]