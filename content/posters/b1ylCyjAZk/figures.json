[{"figure_path": "b1ylCyjAZk/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the actual vs. perceived reasoning abilities of GPT-2, GPT-35-turbo and GPT-4 for a simple arithmetic problem. We posed two distinct types of questions (direct and counterfactual) to the models, each repeated 10 times, for every {number} from 1 to 50. All three models showed an inflated sense of reasoning capability when answering the direct questions. The discrepancy is especially pronounced in GPT-35-turbo, which performed nearly flawlessly on direct questions, but experienced a surge in error rate, exceeding 25%, when handling counterfactual questions.", "description": "This figure shows the error rates of three different language models (GPT-2, GPT-3.5-turbo, and GPT-4) when answering simple arithmetic questions about divisibility by 6.  Two types of questions were used: direct questions simply asked if a number was divisible by 6; counterfactual questions introduced a hypothetical change (assuming the number had 3 as a prime factor).  The results reveal that all models perform much better on the direct questions, showcasing a significant difference in their ability to handle counterfactual reasoning, especially GPT-3.5-turbo.", "section": "1 Introduction"}, {"figure_path": "b1ylCyjAZk/figures/figures_2_1.jpg", "caption": "Figure 2: Reasoning test for assessing an LLM's reasoning abilities. A) Divisibility rule and the corresponding reasoning graph. B) Dataset generation for computing PN and PS. C) Analysis comparing actual values of PN and PS with PN and PS estimates for the LLM-generated data.", "description": "This figure illustrates a three-part framework for evaluating the reasoning capabilities of Large Language Models (LLMs) using the concepts of probability of necessity (PN) and probability of sufficiency (PS). Part A shows the divisibility rule for 6 and its corresponding reasoning graph. Part B details the process of generating factual and counterfactual datasets from both the true reasoning graph and the LLM's responses to prompts based on this graph.  Part C shows how to assess the LLM's reasoning by comparing the PN and PS values derived from the LLM's generated data to the true PN and PS values obtained from the factual and counterfactual datasets. The comparison helps determine if the LLM truly understands the underlying logic or merely replicates statistical patterns.", "section": "Reasoning test for assessing an LLM's reasoning abilities"}, {"figure_path": "b1ylCyjAZk/figures/figures_3_1.jpg", "caption": "Figure 3: The HEX diagram depicts two approaches for solving the problem (Q, \u03c3\u2080) outlined in Example 1. The dotted path corresponds to the actual process of solving the problem, while the solid path represents the one taken by the LLM.", "description": "This figure illustrates the HEX (Heterogeneous Execution) framework for understanding how LLMs solve problems.  It shows a query-state pair (Q,\u03c3\u2080), representing a problem and its initial state.  The dotted line represents the ideal solution process, while the solid line shows how an LLM approaches the problem. The LLM uses a prompt (\u03b1) to transform the initial state (\u03c3\u2080) into a latent state (\u00f4\u2080), processes it using Q_LLM (LLM's internal query process), then transforms the resulting latent state (\u00f4\u2081) back into a concrete output state (\u03c3\u2081) using an output mapping (\u03b3). The diagram highlights the difference between the ideal and LLM-based solution paths.", "section": "LLMs as abstract machines"}, {"figure_path": "b1ylCyjAZk/figures/figures_5_1.jpg", "caption": "Figure 2: Reasoning test for assessing an LLM's reasoning abilities. A) Divisibility rule and the corresponding reasoning graph. B) Dataset generation for computing PN and PS. C) Analysis comparing actual values of PN and PS with PN and PS estimates for the LLM-generated data.", "description": "This figure illustrates a reasoning test to evaluate LLMs' reasoning abilities using the concepts of probability of necessity (PN) and probability of sufficiency (PS). It breaks down the process into three parts:\n\nA) Defines a divisibility rule (if a number is divisible by both 2 and 3, it's divisible by 6) and its corresponding reasoning graph.\n\nB) Shows how to generate factual and counterfactual datasets from the reasoning graph.  Factual data reflects actual scenarios, while counterfactual data explores what happens if a condition is hypothetically changed.\n\nC) Explains how to assess an LLM's reasoning by comparing its PN and PS estimates (derived from the LLM-generated data) to the actual PN and PS values obtained from the factual and counterfactual datasets.", "section": "Reasoning test for assessing an LLM's reasoning abilities"}, {"figure_path": "b1ylCyjAZk/figures/figures_6_1.jpg", "caption": "Figure 5: Left: Heatmaps comparing the consistency of data generated by GPT-2, GPT-3.5-turbo, and GPT-4 for the Div6 problem. Each heatmap cell represents the error rate of the corresponding model for each element of the problem across 10 replicated tests. Right: Sensitivity of the simulated PN relative to varying levels of random noise introduced in the true counterfactuals.", "description": "The figure shows two parts. The left part presents heatmaps that illustrate the consistency of data generated by three different language models (GPT-2, GPT-3.5-turbo, and GPT-4) for the Div6 problem. Each heatmap visualizes the error rate of a model for each element of the problem across ten replicated tests. The right part of the figure displays how sensitive the simulated probability of necessity (PN) is to the introduction of random noise into the true counterfactual data. It shows that even small amounts of noise can significantly affect the estimated PN.", "section": "4 Empirical illustrations"}, {"figure_path": "b1ylCyjAZk/figures/figures_7_1.jpg", "caption": "Figure 6: True PN and PS vs. inferred PN and PS using GPT-2, GPT-35-turbo and GPT-4. The densities of the estimated probabilities capture the uncertainty associated with the responses by each model.", "description": "This figure compares the true probabilities of necessity (PN) and sufficiency (PS) with those estimated using three different language models (GPT-2, GPT-3.5-turbo, and GPT-4) across three different mathematical problems. The densities of the estimated probabilities are shown, reflecting the uncertainty associated with model responses.  The closeness of the estimated distributions to the actual PN and PS values indicates the models' reasoning abilities.  A closer alignment suggests better reasoning capabilities.", "section": "4.1 Factual vs. counterfactual predictions"}, {"figure_path": "b1ylCyjAZk/figures/figures_8_1.jpg", "caption": "Figure 6: True PN and PS vs. inferred PN and PS using GPT-2, GPT-35-turbo and GPT-4. The densities of the estimated probabilities capture the uncertainty associated with the responses by each model.", "description": "This figure compares the true probabilities of necessity (PN) and sufficiency (PS) against those estimated using three different language models: GPT-2, GPT-3.5-turbo, and GPT-4.  The heatmaps show the densities of the estimated probabilities, illustrating the uncertainty inherent in the models' responses.  Each model's estimates are plotted against the true values of PN and PS for comparison, allowing for the assessment of the models' reasoning abilities.", "section": "4.1 Factual vs. counterfactual predictions"}, {"figure_path": "b1ylCyjAZk/figures/figures_16_1.jpg", "caption": "Figure 8: Reasoning graph for the ConPref problem.", "description": "This figure shows the causal graph representing the logical steps involved in solving the ConPref problem.  The nodes represent boolean variables (conditions), and the arrows indicate causal relationships. The graph shows how the conditions Cnm (N \u2264 M) and Cmt (M \u2264 T) causally influence the final condition Cnmt (N \u2264 T). This graph is used in the paper to assess the reasoning capabilities of LLMs by comparing simulated probabilistic measures (Probability of Necessity and Probability of Sufficiency) with actual values derived from the graph.", "section": "B ConPref problem"}, {"figure_path": "b1ylCyjAZk/figures/figures_16_2.jpg", "caption": "Figure 6: True PN and PS vs. inferred PN and PS using GPT-2, GPT-35-turbo and GPT-4. The densities of the estimated probabilities capture the uncertainty associated with the responses by each model.", "description": "This figure compares the true probabilities of necessity (PN) and sufficiency (PS) with those estimated by three different language models (GPT-2, GPT-3.5-turbo, and GPT-4) for three different mathematical reasoning problems. The densities represent the uncertainty in the model's estimations, which are caused by the randomness of their responses. For each problem, the models' estimated PN and PS values are plotted against the true values. This visualization allows for assessment of how well each model approximates the true probabilities. Overall, the figure suggests varying degrees of success in each model's ability to perform causal reasoning, highlighting GPT-4's superior performance.", "section": "4.1 Factual vs. counterfactual predictions"}, {"figure_path": "b1ylCyjAZk/figures/figures_20_1.jpg", "caption": "Figure 11: The HEX diagram for a counterfactual query in the Div6 problem. We split the query in two sub-queries QC3=True and QC6 that performs the two operations need to compute the counterfactual state. QC3=True only sets the value of C3 to True. QC6 replaces the value of C6 by its counterfactual. This operation can be executed via the concrete path (using the structural causal model of the problem) or by using an LLM.", "description": "This figure shows the HEX diagram for a counterfactual query in the divisibility by six problem (Div6). It breaks down the query into two sub-queries:  QC3=True (setting C3 to True) and QC6 (replacing C6 with its counterfactual).  The diagram illustrates how the counterfactual state can be computed either through the direct application of the structural causal model or by using an LLM (Large Language Model).", "section": "3 Probabilities of causation for an LLM"}, {"figure_path": "b1ylCyjAZk/figures/figures_21_1.jpg", "caption": "Figure 1: Illustration of the actual vs. perceived reasoning abilities of GPT-2, GPT-35-turbo and GPT-4 for a simple arithmetic problem. We posed two distinct types of questions (direct and counterfactual) to the models, each repeated 10 times, for every {number} from 1 to 50. All three models showed an inflated sense of reasoning capability when answering the direct questions. The discrepancy is especially pronounced in GPT-35-turbo, which performed nearly flawlessly on direct questions, but experienced a surge in error rate, exceeding 25%, when handling counterfactual questions.", "description": "This figure compares the performance of three large language models (GPT-2, GPT-3.5-turbo, and GPT-4) on a simple arithmetic task involving divisibility by 6.  Two types of prompts were used: direct prompts which simply ask if a number is divisible by 6, and counterfactual prompts which introduce a hypothetical change to the number's prime factorization before asking the same question. The results show that all three models perform well on the direct prompts, but their accuracy significantly decreases when presented with the counterfactual prompts. This demonstrates the models' limitation in handling hypothetical scenarios and genuine causal reasoning.", "section": "1 Introduction"}, {"figure_path": "b1ylCyjAZk/figures/figures_22_1.jpg", "caption": "Figure 13: In other families of models. We re-run the Div6 problem with the same setup used in the paper with two new families of models: Llama (7-7b and 13b) and Phi (1,2). The results are consistent with the findings discussed in the main body of the paper", "description": "This figure compares the performance of different large language models (LLMs) on a divisibility problem (Div6).  It shows the necessity and sufficiency probabilities estimated by LLMs from the Llama and Phi families, contrasted against the true probabilities.  The results demonstrate that even across different LLMs, the findings are consistent with the study's overall conclusions.", "section": "J Experiments with other model families"}]