[{"type": "text", "text": "Online Budgeted Matching with General Bids ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jianyi Yang University of Houston Houston, TX, USA jyang71@central.uh.edu ", "page_idx": 0}, {"type": "text", "text": "Pengfei Li University of California, Riverside Riverside, CA, USA pli081@ucr.edu ", "page_idx": 0}, {"type": "text", "text": "Adam Wierman California Institute of Technology Pasadena, CA, USA adamw@caltech.edu ", "page_idx": 0}, {"type": "text", "text": "Shaolei Ren University of California, Riverside Riverside, CA, USA shaolei@ucr.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Online Budgeted Matching (OBM) is a classic problem with important applications in online advertising, online service matching, revenue management, and beyond. Traditional online algorithms typically assume a small bid setting, where the maximum bid-to-budget ratio $(\\kappa)$ is infinitesimally small. While recent algorithms have tried to address scenarios with non-small or general bids, they often rely on the Fractional Last Matching (FLM) assumption, which allows for accepting partial bids when the remaining budget is insufficient. This assumption, however, does not hold for many applications with indivisible bids. In this paper, we remove the FLM assumption and tackle the open problem of OBM with general bids. We first establish an upper bound of $1\\,-\\,\\kappa$ on the competitive ratio for any deterministic online algorithm. We then propose a novel meta algorithm, called MetaAd, which reduces to different algorithms with first known provable competitive ratios parameterized by the maximum bid-to-budget ratio $\\kappa\\in[0,1]$ . As a by-product, we extend MetaAd to the FLM setting and get provable competitive algorithms. Finally, we apply our competitive analysis to the design learningaugmented algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Online Budgeted Matching (OBM) with general bids is a fundamental online optimization problem that generalizes to many important settings, such as online bipartite matching and Adwords with equal bids [23]. It has applications in various domains, including online advertising, online resource allocation, and revenue management among others [5, 16, 32]. OBM is defined on a bipartite graph with a set of offline nodes (bidders) and a set of online nodes (queries). The task is to select an available offline node to match with an online query in each round. When an offline node is matched to an online node, a bid value is subtracted from the budget of the offilne node, and a reward equal to the consumed budget is obtained. If the remaining budget of an offilne node is less than the bid value of an online query, the offline node cannot be matched to the online query. The goal is to maximize the total reward throughout the entire online matching process. ", "page_idx": 0}, {"type": "text", "text": "OBM is challenging due to the nature of online discrete decisions. Previous works have studied this problem under one of the following two additional assumptions on bids or matching rules: ", "page_idx": 0}, {"type": "text", "text": "\u2022 Small bids. The small-bid assumption is a special case of general bids corresponding to the maximum bid-budget ratio $\\kappa\\rightarrow0$ . That is, while the bid values can vary arbitrarily, the size of each individual bid is infinitely small compared to each offline node\u2019s budget, and there is always enough budget for matching. Under this assumption, the first online algorithm was provided by [24], achieving an optimal competitive ratio of $1-1/e$ [23]. This competitive ratio has also been attained by subsequent algorithms based on primal-dual techniques [4, 7]. However, the small-bid assumption significantly limits these algorithms for broader applications in practice. Take the application of matching Virtual Machines (VMs) to physical servers as an example. An online VM request typically takes up a non-negligible fraction of the total computing units in a server. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "\u2022 Fractional last match (FLM). Under FLM, if an offilne node has an insufficient budget for an online query, the offline node can still be matched to the query, obtaining a partial reward equal to the remaining budget. Given the limitations of small bids, some recent studies [15, 29, 30] have studied competitive algorithms for OBM with general bids by making the additional assumption of FLM. For example, under FLM, the greedy algorithm (Greedy) achieves a competitive ratio of $1/2$ , while other studies [4, 15, 29, 30] aim to achieve a competitive ratio greater than $1/2$ under various settings and/or using randomized algorithms. Although FLM allows fractional matching of a query to an offline node with insufficient budgets, it essentially assumes that any bids are potentially divisible. This assumption may not hold in many real applications, e.g., allocating fractional physical resources to a VM can result in significant performance issues that render the allocation unacceptable, and charging a fractional advertising fee may not be allowed in online advertising. ", "page_idx": 1}, {"type": "text", "text": "Despite its practical relevance and theoretical importance, OBM with general bids has remained a challenging open problem in the absence of the small-bid and FLM assumptions. Specifically, an offline node may have insufficient budget and cannot be matched to a later query with a large value, potentially causing large sub-optimality in the worst case. This issue does not apply to small bids, as the small-bid setting implies that insufficient budgets will never occur. Additionally, this challenge is alleviated in the FLM setting, where fractional matching in cases of insufficient budgets can reduce sub-optimality. Indeed, removing the small-bid and FLM assumptions fundamentally changes and add significant challenges to the problem of OBM [30]. To further highlight the intrinsic difficulty of OBM with general bids, we formally prove in Proposition 4.1 an upper bound of the competitive ratio, i.e., $1-\\kappa$ , achieved by any deterministic online algorithm, where $\\kappa\\in[0,1]$ is the maximum bid-budget ratio. ", "page_idx": 1}, {"type": "text", "text": "Contributions: In this paper, we address OBM without the small-bid or FLM assumptions and design a meta algorithm called MetaAd, which adapts to different algorithms with provable competitive ratios. To our knowledge, MetaAd is the first provable competitive algorithm for general bids without the FLM assumption. Specifically, MetaAd generates a discounted score for each offline node by a general discounting function, which is then used to select the offilne node. The discounting function evaluates the degree of budget insufficiency given a bid-budget ratio $\\kappa\\,\\in\\,[0,1]$ , addressing the challenge of infeasible matching due to insufficient budgets. Given different discounting functions, MetaAd yields concrete algorithms, and their competitive ratios are derived from Theorem 4.2, established through a novel proof technique. We show that with small bids (i.e., $\\kappa\\rightarrow0$ ), MetaAd recovers the optimal competitive ratio of $\\bar{1}-\\frac{1}{e}$ . Furthermore, we show that MetaAd, with discounting functions from the exponential and polynomial function classes, achieves a positive competitive ratio for $\\kappa\\in[0,1)$ . As an extension, we adapt the design of MetaAd to the FLM setting, resulting in a meta-algorithm with provable competitive ratios for $\\kappa\\in[0,1]$ (Theorem 4.3). The framework of MetaAd potentially opens an interesting direction for exploring concrete discounting function designs that yield high competitive ratios for settings both with and without FLM. Finally, we apply our competitive analysis to the design of LOBM, a learning-augmented algorithm for OBM, which enhances average performance while still guaranteeing a competitive ratio (Theorem 5.1). We validate the empirical beneftis of MetaAd and LOBM through numerical experiments on the applications of an online movie matching an VM placement on physical servers. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "OBM originates from the online bipartite matching problem defined by [19] 30 years ago. In 2007, [24] generalized the online b-matching problem to OBM (a.k.a. Adwords) [17]. Under the special case of small bids, [24] proposes an algorithm that achieves the competitive ratio of $1-1/e$ , which is also the optimal competitive ratio under the small-bid setting [17]. In the same year, [4] provides the primal-dual algorithm and analysis for OBM under the small-bid assumption and achieves the competitive ratio of $\\textstyle1-{\\frac{1}{e}}$ . Subsequently, [7] gives a randomized primal-dual analysis for online bipartite matching and generalizes it to OBM. In addition, OBM has also been studied under the stochastic settings [9, 8, 6, 14, 25]. ", "page_idx": 1}, {"type": "text", "text": "It is known to be very challenging to go beyond the small-bid assumption and develop a non-trivial competitive ratio for OBM with general bids in an adversarial setting. Recently, [30] points out the inherent difficulty of OBM with general bids and explains the necessity of the assumption of FLM is needed in easing up the challenges. With the FLM assumption, a greedy algorithm can achieve a competitive ratio of $1/2$ . Additionally, a deterministic algorithm proposed in [4] achieves a competitive ratio of $\\begin{array}{r}{(1-\\kappa\\stackrel{!}{-}\\frac{1-\\kappa}{(1+\\kappa)^{1/\\kappa}})}\\end{array}$ (1+1\u03ba\u2212)\u03ba1/\u03ba ), increasing the competitive ratio when the maximum bid-budget ratio $\\kappa$ is no larger than 0.17. Some other works on OBM with FLM employ randomized algorithm designs. For example, [15] proposes a semi-random algorithm that achieves a competitive ratio of 0.5016, which is known as the best competitive ratio achieved by randomized algorithms up to now. Besides, [30] extends the random algorithm of Ranking to OBM and achieves a competitive ratio of $1-1/e$ with a strong assumption of the \u201dfake\u201d budget. Moreover, [29] proposes a randomized algorithm without the knowledge of budget for the FLM setting with competitive ratio $\\textstyle{\\frac{1}{1+\\kappa}}(1-{\\frac{1}{e}})$ parameterized by the maximum bid-budget ratios $\\kappa$ , which relies on a strong assumption that the bids are decomposable (i.e. $w_{u,t}={w_{u}}\\cdot{w_{t}})$ ). ", "page_idx": 2}, {"type": "text", "text": "Despite the progress on the OBM settings with FLM, OBM without FLM has remained an open challenge except for under the small-bid assumption. The recent study [30] points out that OBM without FLM is difficult because when the offline node has less leftover budget than the bid value of an online arrival, the offline node is not allowed to be matched to the arrival, potentially causing a loss equivalent the leftover budget. [20] considers multi-tier budget constraints with a laminar structure and provides a competitive ratio without FLM, but the result does not apply to the settings without the laminar structure. Additionally, [18] proves an online randomized algorithm with the competitive ratio (in expectation) upper bound of 0.612, an upper bound for deterministic algorithms is still lacking to formally evaluate the difficulty of OBM without FLM. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider OBM with general bids. Specifically, there is a bipartite graph described as $G(\\mathcal{U},\\mathcal{V},E)$ , where the vertices $u\\in\\mathcal{U}$ (i.e. offilne nodes or bidders) are fixed and the vertices $v\\in\\mathcal{V}$ (i.e. online nodes or queries) arrive sequentially. The edge corresponding to vertices $u\\in\\mathcal{U}$ and $v\\in\\mathcal{V}$ has a bid value $w_{u,v}\\ge0$ which is the amount the offline node $u$ would like to pay for the online node $v$ if matched. The sizes of the vertex sets are denoted as $|\\mathcal{U}|=U$ and $|\\mathcal{V}|=V$ , respectively. We index each online node by its arriving order, i.e. online node $t$ arrives at the $t$ -th round. ", "page_idx": 2}, {"type": "text", "text": "At the beginning, each offline node $u\\,\\in\\,\\mathcal{U}$ has an initial budget $b_{u,0}\\,=\\,B_{u}\\,\\ge\\,0$ , which is the maximum amount the offline node can pay in total. At round $t$ , a query $t\\in\\mathcal{V}$ arrives, the edges connected to this arrival $t$ are revealed, and the agent chooses to match the arrival to an available offilne node from $\\mathcal{U}_{t}=\\{u\\in\\mathcal{U}\\mid w_{u,t}>0,b_{u,t-1}\\geq w_{u,t}\\}$ or skip this query without any matching. We denote the agent\u2019s action as $x_{t}\\in\\mathcal{U}_{t}\\bigcup\\left\\{\\mathrm{null}\\right\\}$ , where \u201cnull\u201d represents skipping this arrival. If at round $t$ , an offline node $x_{t}$ is matched to the query $t$ , a reward $r_{t}=w_{x_{t},t}$ is earned and a bid value $w_{x_{t},t}$ is charged from the offline node $x_{t}$ , i.e., $b_{x_{t},t}=b_{x_{t},t-1}-w_{x_{t},t}$ ; for the other offline nodes $u\\ne x_{t}$ , the budget remains unchanged $b_{u,t}=b_{u,t-1}$ . If $x_{t}=\\mathrm{null}$ , no reward is earned, i.e. $r_{t}=0$ and the budgets of all offline nodes remain the same as the last round, i.e. $b_{u,t}=b_{u,t-1}\\,\\forall u\\in\\mathcal{U}$ . The cumulative consumed budget at the end of round $t$ is denoted as $c_{u,t}=B_{u}-b_{u,t}$ , $\\forall u\\in\\mathcal{U}$ and $t\\in[V]$ . The agent aims to maximize the total reward $\\textstyle P=\\sum_{t=1}^{V}r_{t}$ over the entire $V$ rounds. ", "page_idx": 2}, {"type": "text", "text": "The offline version of OBM can be written as Linear Programming (LP) with its primal and dual problems given in (1), where $P$ is the primal objective and $D$ is the dual objective. While we only need to solve the primal problem for OBM, we present the dual problem with dual variables $\\alpha_{u},u\\in\\mathcal{U}$ and $\\beta_{t},t\\in\\mathcal{V}$ to facilitate the subsequent algorithm design and analysis. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\operatorname*{max}{P:=\\displaystyle\\sum_{t=1}^{V}\\sum_{u\\in\\mathcal{U}}w_{u,t}x_{u,t}\\quad\\quad}}&{\\mathrm{~min}\\quad D:=\\displaystyle\\sum_{u\\in\\mathcal{U}}B_{u}\\alpha_{u}+\\displaystyle\\sum_{t=1}^{V}\\beta_{t}}\\\\ {\\mathrm{~s.t.~}\\forall u\\in\\mathcal{U},\\displaystyle\\sum_{t=1}^{V}w_{u,t}x_{u,t}\\leq B_{u},}&{\\mathrm{~\\mathrm{~s.t.~}~}\\forall u\\in\\mathcal{U},t\\in[V],w_{u,t}\\alpha_{u}+\\beta_{t}\\geq w_{u,t},}\\\\ {\\forall t\\in[V],\\displaystyle\\sum_{u\\in\\mathcal{U}}x_{u,t}\\leq1,}&{\\forall u\\in\\mathcal{U},\\alpha_{u}\\geq0,}\\\\ {\\forall u\\in\\mathcal{U},v\\in[V],x_{u,t}\\geq0.}&{\\forall t\\in[V],\\beta_{t}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "A common performance metric for online algorithms is the competitive ratio defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\eta=\\operatorname*{min}_{G\\in\\mathcal{G}}\\{P(G)/P^{*}(G)\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathcal{G}^{\\mathrm{~l~}}$ $\\begin{array}{r}{P(G)=\\sum_{t=1}^{V}w_{x_{t},t}}\\end{array}$ is the total reward obtained by an (online) algorithm for a graph $G\\in{\\mathcal{G}}$ , and $\\begin{array}{r}{P^{*}(G)=\\sum_{t=1}^{V}w_{x_{t}^{*},t}}\\end{array}$ is the corresponding offline optimal total reward with $\\boldsymbol{x}_{t}^{*}$ being the offline optimal solution to (1). ", "page_idx": 3}, {"type": "text", "text": "Next, we formally define the bid-budget ratio $\\kappa\\in[0,1]$ in Definition 1 which is the maximum ratio of the bid value of an offilne node to its total budget. We use $\\eta(\\kappa)$ to denote the competitive ratio of an algorithm for OBM with bid-budget ratio $\\kappa$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Bid-budget ratio). The bid-budget ratio $\\kappa\\in[0,1]$ for an example $G(\\mathcal{U},\\mathcal{V},E)$ is defined as \u03ba = supu\u2208U,t\u2208V Bu ", "page_idx": 3}, {"type": "text", "text": "Many previous works [23, 24, 15, 4] assume FLM which allows for accepting partial bids when remaining budget is insufficient (i.e. modifying each bid $w_{u,t}$ to $\\bar{w}_{u,t}=\\operatorname*{min}\\{w_{u,t},b_{u,t-1}\\}$ given the remaining budget $b_{u,t-1})$ . Without the FLM assumption, the only known competitive ratio for OBM is for the small-bid setting where $\\kappa$ is infinitely small and approaches zero [23, 24]. However, the small-bid and FLM assumptions do not hold in many real-world applications as illustrated by the following examples: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Online VM placement. In this problem, a cloud manager allocates virtual machines (VMs, online nodes) to heterogeneous physical servers (offline nodes), each with a computing resource capacity of $B_{u}^{\\prime}$ [10, 28]. When a VM request with a computing load of $z_{t}$ arrives, the manager assigns it to a server. If the VM is placed on server $u$ , the manager receives a utility of $w_{u,v}=r_{u}z_{v}$ due to the heterogeneity of servers. The goal is to maximize the total utility $\\begin{array}{r}{\\sum_{t=1}^{V}\\sum_{u\\in\\mathcal{U}}w_{u,t}x_{u,t}}\\end{array}$ subject to the computing resource constraint $\\begin{array}{r}{\\sum_{t=1}^{V}z_{t}x_{u,t}\\le B_{u}^{\\prime}}\\end{array}$ for each server $u$ , which can also be written as $\\begin{array}{r}{\\sum_{t=1}^{V}w_{u,t}x_{u,t}\\le B_{u}}\\end{array}$ with $B_{u}=r_{u}B_{u}^{\\prime}$ . In this problem, VMs are not divisible and consume up a non-negligible portion of the server capacity, violating both the small-bid and FLM assumptions. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Inventory management with indivisible goods. Here, a manager must match several indivisible goods (online nodes) to various resource nodes (offline nodes), each with a limited capacity (e.g., matching parcels to mail trucks or food orders to delivery vehicles). Each good can only be assigned to one node without being split, and a good $t$ can occupy a substantial portion of the resource node\u2019s capacity, $w_{u,t}$ . The goal is to maximize the total utilization $\\begin{array}{r}{\\sum_{t=1}^{V}\\sum_{u\\in\\mathcal{U}}w_{u,t}x_{u,t},}\\end{array}$ , subject to the capacity constraint tV= $\\begin{array}{r}{\\sum_{t=1}^{V}w_{u,t}x_{u,t}\\leq1}\\end{array}$ for each node $u$ . In this problem, neither the small-bid nor FLM assumption a pplies. ", "page_idx": 3}, {"type": "text", "text": "In this paper, without relying on the small-bid or FLM assumptions, we move beyond small bids and propose a meta algorithm (MetaAd) for general $\\kappa\\in[0,1]$ which can reduce to many concrete competitive algorithms. ", "page_idx": 3}, {"type": "text", "text": "4 MetaAd: Meta Algorithm for OBM ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 An Upper Bound on the Competitive Ratio ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the absence of small-bid and FLM assumptions, OBM faces a unique challenge: when an online query with large bid arrives, there may be no offline node that both connects to the query and has sufficient remaining budgets for it. This leads to missed matches for the queries with large bids, ultimately resulting in a low competitive ratio. To formally show the inherent difficulty of OBM without the small-bid and FLM assumptions, we present an upper bound on the competitive ratio for any deterministic online algorithms in the following proposition. ", "page_idx": 3}, {"type": "text", "text": "Proposition 4.1. For OBM without small-bid or FLM assumptions, the competitive ratio of any deterministic online algorithm is upper bounded by $1-\\kappa$ for $\\kappa\\in(0,1]$ . Specifically, the competitive ratio for any deterministic algorithm is zero when $\\kappa=1$ without the FLM assumption. ", "page_idx": 3}, {"type": "text", "text": "The proof of the upper bound is deferred to Appendix A.1. The key ingredients of the proof is given below. The best competitive ratio for any deterministic algorithm is $\\mathrm{max}_{\\pi}\\,\\mathrm{min}_{G\\in\\mathcal G}\\,C R(\\pi,G)$ which is no larger than $\\mathrm{max}_{\\pi}\\operatorname*{min}_{G\\in{\\mathcal G}^{\\prime}}C R(\\pi,G)$ where $\\mathcal{G}^{\\prime}\\subset\\mathcal{G}$ . Thus, we can prove the upper bound by constructing a subset $\\mathcal{G}^{\\prime}$ with difficult instances and deriving the best competitive ratio among the deterministic algorithms for this subset $\\mathcal{G}^{\\prime}$ . In our constructed $\\mathcal{G}^{\\prime}$ , each example has one offilne node and the bid values for the first $V-1$ rounds sum up to $1-\\kappa+\\epsilon$ with $\\epsilon>0$ being infinitely small. We let $\\mathcal{G}^{\\prime}=\\mathcal{G}_{1}^{\\prime}\\bigcup\\mathcal{G}_{2}^{\\prime}$ where we have $w_{u,V}=\\kappa B_{u}$ for examples in $\\mathcal{G}_{1}^{\\prime}$ , and $w_{u,V}=0$ for examples in $\\mathcal{G}_{2}^{\\prime}$ . The instances in $\\mathcal{G}^{\\prime}$ illustrate the dilemma between matching a query for immediate reward or saving the budget for future matches. If an algorithm chooses to match all the queries in the first $V-1$ rounds, it can lose a bid of $\\kappa B_{u}$ for instances in $\\mathcal{G}_{1}^{\\prime}$ because there is no sufficient budget to match the final query. Conversely, if an algorithm chooses to skip some queries in the first $V-1$ rounds to save the budget, it can lose a bid of $\\kappa B_{u}$ for the instances in $\\mathcal{G}_{2}^{\\prime}$ because matching the final query of instances in $\\mathcal{G}_{2}^{\\prime}$ earns zero bid. For these difficult instances in $\\mathcal{G}^{\\prime}$ , we formally derive the largest reward ratio of a deterministic algorithm to the offilne optimal one which is the upper bound of the competitive ratio. ", "page_idx": 3}, {"type": "table", "img_path": "Vtxy8wFpTj/tmp/cb36d62cc7ba7061d8973b3889eb11ff8937e42fed84daaa7bec48851c977b87.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The upper bound of the competitive ratio $1-\\kappa$ shows that OBM becomes more difficult when the bid-budget ratio $\\kappa$ gets larger. Intuitively, since the bid value is not fractional for each matching, given a larger $\\kappa\\in[0,1]$ , it is more likely for offilne nodes to have insufficient budgets (i.e., unable to be matched to a query with a large bid value). Skipping a query to save budget is also risky because it can happen that the following queries have no positive bid. The FLM assumption can alleviate this difficulty because the remaining budget can be fully spent even if it is insufficient. A greedy algorithm can achieve a competitive ratio of $1/2$ for general bids with FLM [23]. By contrast, the upper bound of the competitive ratio without FLM cannot reach $1/2$ when the bid-budget ratio $\\kappa$ is larger than $1/2$ . There is even no non-zero competitive ratio when $\\kappa=1$ . These observations reveal that without FLM, OBM becomes more difficult. The analysis without FLM assumption will help us to understand OBM better. ", "page_idx": 4}, {"type": "text", "text": "4.2 Meta Algorithm Design ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now present MetaAd in Algorithm 1, which is a meta online algorithm that reduces to many concrete algorithms with provable competitive ratios for OBM in the absence of small-bid and FLM assumptions. ", "page_idx": 4}, {"type": "text", "text": "MetaAd relies on a general discounting function $\\phi:[0,1]\\rightarrow[0,1]$ . Given a new query $t$ , MetaAd uses $\\phi$ to score each offilne node by discounting its bid value in Line 3, and selects the node with the largest score $s_{u,t}$ from Line 4 to Line 7. An offilne node is scored zero if it has insufficient budget for the query $t$ $(b_{u,t-1}-w_{u,t}<0)$ or it has zero bid for the query $t$ $(w_{u,t}=0)$ ). If all the offilne nodes are scored zero, the algorithm skips this query $t$ . ", "page_idx": 4}, {"type": "text", "text": "The scoring in MetaAd reflects a balance between selecting an offline node with a large bid and saving budget for future. To select offline nodes with large bids, the score scales with the bid value $w_{u,t}$ . Simultaneously, an increasing function $\\phi$ maps the normalized remaining budget bu,t\u22121 to a discounting value within $[0,1]$ . If an offilne node $u$ has less remaining budget, a smaller discounting value is obtained to encourage conserving budget for $u$ . ", "page_idx": 4}, {"type": "table", "img_path": "Vtxy8wFpTj/tmp/3b2157fee02d3eeff1763bb61f7f6d625d945c58f8dff0d0d2fdc00c11d61eb6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.3 Competitive analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given any monotonically increasing function $\\phi:[0,1]\\rightarrow[0,1]$ in Algorithm 1, we can get a concrete algorithm for OBM. For different bid-budget ratio $\\kappa$ , the competitive ratio of MetaAd is given in the main theorem below. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2. If the function $\\phi:[0,1]\\rightarrow[0,1]$ in Algorithm $^{\\,l}$ satisfies that given an integer $n\\geq1$ , $\\forall i\\leq n,\\,\\varphi^{(i)}(x)>0$ where $\\varphi(x)=1-\\phi(1-x),$ , the competitive ratio of Algorithm $^{\\,I}$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta(\\kappa)=\\frac{1}{1+\\kappa^{n+1}R+\\operatorname*{max}_{y\\in[0,1]}\\Delta(y)+\\frac{\\phi(\\kappa)}{1-\\kappa}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "owthheerrew $R$ ei.s  tAhded iLtiiopsncahllityz, $\\varphi^{(n)}(x)\\;i f\\varphi^{(n)}(x)$ $R=0$ $\\begin{array}{r}{\\Delta(y)=\\frac{\\varphi(y)}{y}-\\frac{1}{y}\\int_{x=0}^{y}\\varphi(x)d x+\\frac{1}{y}\\sum_{i=1}^{n}\\kappa^{i}\\left(\\varphi^{(i-1)}(y)-\\varphi^{(i-1)}(0)\\right)}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "By Theorem 4.2, we can easily get a competitive algorithm for OBM with any bid-budget ratio $\\kappa$ by choosing a function $\\phi$ . The only requirement is that the function $\\phi$ is a monotonically increasing function. In the next section, we will give some concrete examples of competitive algorithms by assigning $\\phi$ with different function classes. ", "page_idx": 5}, {"type": "text", "text": "We defer the complete proof of Theorem 4.2 to Appendix A.2. The analysis is based on the fundamental conditions in Lemma 1 that guarantee the competitive ratio and presents new challenges due to the absence of the small-bid and FLM assumptions. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 (Conditions for competitive ratio). An online algorithm achieves a competitive ratio of $\\eta\\in[0,1]$ if it selects a series of feasible actions $\\{x_{1},\\ldots,x_{V}\\}$ and there exist dual variables $\\{\\beta_{1},\\cdot\\cdot\\cdot,\\beta_{V}\\}$ , $\\{\\alpha_{1},\\cdot\\cdot\\cdot,\\alpha_{U}\\}$ such that ", "page_idx": 5}, {"type": "text", "text": "\u2022 (Dual feasibility) $\\forall u\\in\\mathcal{U},t\\in[V],\\beta_{t}\\geq w_{u,t}(1-\\alpha_{u})$ ", "page_idx": 5}, {"type": "text", "text": "\u2022 (Primal-Dual Ratio ) $P\\geq\\eta\\cdot D,$ , where $\\textstyle P=\\sum_{t=1}^{V}w_{x_{t},t}$ and $\\begin{array}{r}{D=\\sum_{u\\in\\mathcal{U}}B_{u}\\alpha_{u}\\!+\\!\\sum_{t=1}^{V}\\beta_{t}.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Without the small-bid and FLM assumptions, the competitive analysis presents the following new challenges to satisfy the conditions in Lemma 1: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Dual construction for general bids. When an offilne node has an insufficient budget to match a query, the remaining budget is almost zero for the small-bid setting, but it can be large and uncertain without the small-bid assumption. This introduces a new challenge to construct dual variables that satisfy the dual feasibility due to budget insufficiency. ", "page_idx": 5}, {"type": "text", "text": "To address this challenge, we present a new dual construction in Algorithm 2 where dual variables are determined based on the remaining budget and adjusted at the end of the algorithm. The constructed dual variables satisfy the dual feasibility in Lemma 1, as explained below. We define $\\beta_{t}$ as the score of selected offline node $u$ . For any $u$ with sufficient remaining budget $(b_{u,t-1}\\,\\geq\\,w_{u,t})$ , we have \u03b2t \u2265su,t = wu,t(1 \u2212\u03c6( cuB,t\u2212 . By choosing $\\begin{array}{r}{\\alpha_{u,t}=\\varphi(\\frac{c_{u,t}}{B_{u}})}\\end{array}$ , the dual feasibility in Lemma 1 is satisfied for $t$ and $u$ with sufficient budget $(b_{u,t-1}\\geq w_{u,t})$ ) since by an increasing function $\\varphi$ , it holds that $\\alpha_{u}\\geq\\alpha_{u,t}$ for any $t\\in[T]$ . ", "page_idx": 5}, {"type": "text", "text": "Different from the small-bid setting, we need to adjust the dual variables at the end of the dual construction (Line 7 in Algorithm 2) to satisfy dual feasibility. We set $\\alpha_{u}\\,=\\,1\\$ for any $u$ with insufficient budget $(b_{u,t-1}<w_{u,t})$ at the end of the dual construction. This ensures that the dual feasibility is always satisfied without FLM. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Guarantee the primal-dual ratio. The challenges in guaranteeing the primal-dual ratio in Lemma 1 come from the unspecified discounting function $\\phi$ and the absence of the small-bid and FLM assumptions. To solve this challenge, we derive a condition to satisfy the primal dual ratio $\\begin{array}{r}{P_{t}\\,\\ge\\,\\frac{1}{\\gamma}D_{t}}\\end{array}$ for any round $t$ where $\\gamma\\geq1$ , $\\begin{array}{r}{P_{t}=\\sum_{i=1}^{t}w_{x_{i},i}}\\end{array}$ is the cumulative primal reward and $\\begin{array}{r}{D_{t}=\\sum_{u\\in\\mathcal{U}}B_{u}\\alpha_{u,t}+\\sum_{i=1}^{t}\\beta_{t}}\\end{array}$ is the cumulative dual. Thus, we prove that the primal dual ratio $\\begin{array}{r}{P_{t}\\ge\\frac{1}{\\gamma}D_{t}}\\end{array}$ is satisfied for any $t\\in[T]$ if for any $y\\in[0,1]$ it holds that, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\varphi(y)-\\int_{x=0}^{y}\\varphi(x)d x+\\sum_{i=1}^{n}\\kappa^{i}\\varphi^{(i-1)}(y)+(\\kappa^{n+1}R-\\gamma+1)y\\leq\\sum_{i=1}^{n}\\kappa^{i}\\varphi^{(i-1)}(0),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\varphi(x)=1-\\phi(1-x)$ and $\\phi$ is the discounting function. Given that the dual increase due tfoin tahl ep rfiinmaall -dduuaall  ardajtiuos tams $P\\,\\geq\\,\\frac{1}{\\gamma\\!+\\!\\frac{\\phi(\\kappa)}{1-\\kappa}}D$ A. lTgohriist lhema d2s) t ios  ab coounmdpeedti tbivye ${\\frac{\\phi(\\kappa)}{1-\\kappa}}\\,\\cdot\\,P$ , $\\frac{1}{\\gamma+\\frac{\\phi(\\kappa)}{1-\\kappa}}$ .n  bGoivuennd  atnhey discounting function $\\phi$ , we can solve for $\\gamma$ that satisfies the condition in (3), thereby obtaining the competitive ratio of MetaAd. ", "page_idx": 6}, {"type": "text", "text": "4.4 Competitive Algorithm Examples ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we assign $\\phi$ with different functions to get concrete algorithms and competitive ratios. ", "page_idx": 6}, {"type": "text", "text": "4.4.1 Small Bid ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first verify that MetaAd reduces to the optimal algorithm for small-bid setting $\\left(\\kappa\\rightarrow0\\right)$ ) [24, 23]. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.2.1. By choosing \u03d5(x) = e\u2212ee\u221211\u2212 x, MetaAd reduces to the algorithm in [24] and achieves the optimal competitive ratio of $1-{\\frac{1}{e}}$ for small-bid setting $\\left(\\kappa\\rightarrow0\\right)$ ). ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.2.1 shows that the competitive ratio in Theorem 4.2 is consistent with the classical results for small bids. Interestingly, our analysis shows how the optimal $\\phi$ is obtained which is explained as follows. By solving (3) with $\"{=}\"$ and $\\kappa=0$ , we get ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\varphi(x)=(\\gamma-1)e^{x}+1-\\gamma,\\;\\;\\phi(x)=\\gamma-(\\gamma-1)e^{1-x},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\gamma\\le\\frac{e}{e\\!-\\!1}$ to make sure $\\phi({\\boldsymbol{x}})\\geq0$ for $x\\in[0,1]$ . By Theorem 4.2, we get the competitive ratio $\\begin{array}{r}{\\operatorname*{lim}_{\\kappa\\to0}\\frac{1}{\\gamma+\\frac{\\rho\\phi(\\kappa)}{1-\\kappa}}=\\frac{1}{(2-e)\\gamma+e}}\\end{array}$ . By optimally choosing $\\gamma=\\frac{e}{e\\!-\\!1}$ , we get the optimal competitive ratio as $1-{\\frac{1}{e}}$ . ", "page_idx": 6}, {"type": "text", "text": "4.4.2 Exponential Function Class ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, we consider an exponential function class $\\varphi(x)=C_{1}e^{\\theta x}+C_{2}$ with $0\\leq\\theta\\leq1$ . To ensure $\\varphi(x)$ is an increasing function, we choose $C_{1}\\,\\geq\\,0$ . Also, we choose $C_{2}=-C_{1}$ to simplify the expression of the competitive ratio. We can observe that $\\varphi(x)$ has positive $n-$ th derivative for any $n\\geq1$ . Thus, we choose $n=\\infty$ in Theorem 4.2 to eliminate the term $\\kappa^{n+1}R$ . By substituting $\\varphi(x)$ into $\\eta(\\kappa)$ in Theorem 4.2, we get the corollary below. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.2.2. If we assign $\\varphi(x)=C e^{\\theta x}-C$ with $C\\geq0$ and $0\\leq\\theta\\leq1$ in MetaAd in Algorithm $^{\\,I}$ , we get the competitive ratio as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\eta(\\kappa)=\\left\\{\\frac{1}{1+C+C(1-\\frac{1}{\\theta}+\\frac{\\kappa}{1-\\kappa\\theta})(e^{\\theta}-1)+\\frac{1+C-C e^{\\theta(1-\\kappa)}}{1-\\kappa}},1-\\frac{1}{\\theta}+\\frac{\\kappa}{1-\\kappa\\theta}\\geq0\\right.}\\\\ {\\left.\\frac{1}{1+C+C(1-\\frac{1}{\\theta}+\\frac{\\kappa}{1-\\kappa\\theta})\\theta+\\frac{1+C-C e^{\\theta(1-\\kappa)}}{1-\\kappa}},1-\\frac{1}{\\theta}+\\frac{\\kappa}{1-\\kappa\\theta}<0\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We numerically solve the optimal $\\eta(\\kappa)$ for each $\\kappa\\in[0,1]$ by adjusting the parameters $\\theta$ and $C$ and show the results in Figure 1. We observe that MetaAd achieves a non-zero competitive ratio for $\\kappa\\in[0,1)$ . The competitive ratio for $\\kappa=0$ is the optimal competitive ratio of $1\\div{\\frac{1}{e}}$ for small-bid setting. The competitive ratio monotonically decreases with $\\kappa$ . This coincides with the intuition that when $\\kappa$ gets larger, it is more likely to trigger budget insufficiency and the problem becomes more challenging. Also, we can find that for a large enough $\\kappa$ , the competitive ratio of MetaAd with the exponential function is very close to the upper bound. However, there can exist other forms of exponential discounting function that can achieve higher competitive ratio. ", "page_idx": 6}, {"type": "text", "text": "Interestingly, the optimal choices of the exponential function $\\varphi(x)$ for different $\\kappa$ reveal the insights into designing deterministic algorithms for OBM. When $\\kappa$ is less than a critical point $\\bar{\\kappa}\\approx0.26$ , the optimal choice of the exponential function is $\\begin{array}{r}{\\bar{\\varphi}(x)=\\frac{1-e^{\\theta x}}{1-e^{\\theta}}}\\end{array}$ 11\u2212\u2212ee\u03b8 and the optimal choice of $\\theta$ decreases with $\\kappa\\,\\in\\,[0,\\bar{\\kappa}]$ . In this counting $\\kappa$ $[0,{\\bar{\\kappa}}]$ be$\\begin{array}{r}{\\phi(\\frac{b_{u,t-1}}{B_{u}})\\;=\\;1\\,-\\,\\varphi(1\\,-\\,\\frac{b_{u,t-1}}{B_{u}})}\\end{array}$ comes smaller, indicating a more conservative approach to budget usage in preparation for potentially high future bids. However, as $\\kappa$ gets larger than $\\bar{\\kappa}$ , the optimal choice of the discounting function becomes $\\phi(x)\\,=\\,1$ with $C\\,=\\,0$ , yielding a greedy algorithm. This suggests that for large enough $\\kappa$ ", "page_idx": 7}, {"type": "image", "img_path": "Vtxy8wFpTj/tmp/6c78caa4f98efff08fc8c8afcc1bb9a70c1d5d7f93d387611b557b8bb255e674.jpg", "img_caption": ["Figure 1: Competitive ratio without FLM. MetaAd (Exp) represents the MetaAd with $\\varphi(x)=C(e^{\\theta x}-$ 1) and MetaAd (Quad) represents the MetaAd with $\\varphi(x)=C x^{2};$ ). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": ", the algorithm benefits more by matching a node with a large bid immediately than by conserving more budget for future. ", "page_idx": 7}, {"type": "text", "text": "4.4.3 Polynomial Function Class ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we explore another function class to show that MetaAd is general enough to provide competitive algorithms given different discounting functions. We consider a function class of $n$ \u2212th polynomial function, i.e. $\\begin{array}{r}{\\varphi(x)=\\sum_{j=0}^{n}C_{j}x^{j}}\\end{array}$ . We set $\\textstyle\\sum_{j=1}^{n}C_{j}\\,\\leq\\,1$ to ensure $\\varphi(1)\\leq1$ and set $C_{0}=0$ to simplify the competitive ratio. We summarize the competitive ratio of the polynomial function class and provide a concrete example for quadratic function in the next corollary. ", "page_idx": 7}, {"type": "text", "text": "Corollary 4.2.3. If we assign $\\begin{array}{r}{\\varphi(x)=\\sum_{j=1}^{n}C_{j}x^{j}}\\end{array}$ with $\\textstyle\\sum_{j=1}^{n}C_{j}\\leq1$ and $i$ -th derivative $\\varphi^{(i)}(x)\\geq$ ( $)\\left(0\\leq i\\leq n\\right)$ , MetaAd achieves a competitive ratio as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\eta(\\kappa)=\\frac{1}{1+\\operatorname*{max}_{y\\in[0,1]}\\Delta(y)+\\frac{1}{1-\\kappa}-\\sum_{j=1}^{n}C_{j}(1-\\kappa)^{j-1}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "$\\begin{array}{r l r}{\\Delta(y)}&{=}&{-\\frac{C_{n}}{n+1}y^{n}\\;+\\;\\left((1\\;+\\;\\kappa)C_{n}\\;-\\;\\frac{C_{n-1}}{n}\\right)y^{n-1}\\;+\\;\\sum_{j=0}^{n-2}((1\\;+\\;\\kappa)C_{j+1}\\;-\\;\\frac{C_{j}}{j+1}\\;+\\;\\frac{1}{2})^{j}\\;\\frac{y^{n-1}}{j}}\\end{array}$ $\\begin{array}{r}{\\sum_{i=2}^{n-j}\\kappa^{i}C_{i+j}\\frac{(i+j)!}{(j+1)!})y^{j}}\\end{array}$ $\\varphi(x)=C x^{2}$ $C=1$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\eta(\\kappa)=({\\frac{11}{4}}\\kappa^{2}+{\\frac{5}{2}}\\kappa+{\\frac{3}{4}}+{\\frac{1}{1-\\kappa}})^{-1}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We numerically show the results of $\\eta(\\kappa)$ in Figure 1. We observe that MetaAd with a simple quadratic function $\\varphi(x)\\stackrel{\\bullet}{=}x^{2}$ can also achieve non-zero competitive ratio for $\\kappa\\,\\in\\,[0,1)$ . However, this competitive ratio is lower than the best competitive ratio achieved by the exponential function $\\varphi(x)^{\\'}=C(e^{\\theta x}-1)$ . ", "page_idx": 7}, {"type": "text", "text": "The examples of exponential functions and quadratic functions demonstrate the strength of MetaAd in providing competitive algorithms for OBM with general bids. While MetaAd provides the first framework to get non-zero competitive ratio for OBM with $\\kappa\\in[0,1)$ (in the absence of the FLM assumption), it is interesting to explore other functions $\\phi$ under the MetaAd framework with better competitive ratios. ", "page_idx": 7}, {"type": "text", "text": "4.5 Extension to OBM with FLM ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "While MetaAd is designed for the more challenging OBM without FLM, this section demonstrates that MetaAd can be extended to provide competitive algorithms for OBM with FLM. ", "page_idx": 7}, {"type": "text", "text": "Due to the space limitation, we defer the algorithm of MetaAd with FLM (Algorithm 3) and its analysis to Appendix B. Instead of scoring based on the true bid $w_{u,t}$ , Algorithm 3 determines the scores based on a modified bid $\\operatorname*{min}\\{w_{u,t},b_{u,t-1}\\}$ . Based on the modified dual construction in Algorithm 4, we can get the competitive ratio for OBM with FLM in the next theorem. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.3. If the function $\\phi:[0,1]\\rightarrow[0,1]$ in Algorithm $^{\\,l}$ satisfies that given an integer $n\\geq2$ , $\\forall i\\leq n-1$ , $\\varphi^{(i)}(x)>0$ and $R=\\operatorname*{max}_{x\\in[0,1]}\\varphi^{(n)}(x)$ where $\\varphi(x)=1-\\phi(1-x)$ , the competitive ratio of Algorithm $^{\\,I}$ is ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\eta(\\kappa)=\\frac{1}{1+\\kappa^{n}R+\\operatorname*{max}_{y\\in[0,1]}\\Delta(y)+\\phi(\\kappa)},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta(y)=\\frac{\\varphi(y)}{y}-\\frac{1}{y}\\int_{x=0}^{y}\\varphi(x)d x+\\frac{1}{y}\\sum_{i=1}^{n}\\kappa^{i}\\,\\big(\\varphi^{(i-1)}(y)-\\varphi^{(i-1)}(0)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The competitive ratio with FLM in Theorem 4.3 differs from that in Theorem 4.2 only in the final terms of the denominators, which are \u03d5(\u03ba) and \u03d5(\u03ba) respectively. Thus, the competitive ratio with FLM is always larger than that without FLM given the same values of $\\kappa$ and $\\phi$ . This improvement arises because FLM allows for accepting partial bids when budgets are insufficient, thereby reducing the potential budget waste. ", "page_idx": 8}, {"type": "text", "text": "Similar as MetaAd without FLM, we assign an exponential function class $\\varphi(x)=C(e^{\\theta x}{-1})$ to get a concrete algorithm with competitive ratio in Corollary B.1.1. We numerically solve the optimal $\\eta(\\kappa)$ for each $\\kappa\\in[0,1]$ by adjusting $\\theta$ ", "page_idx": 8}, {"type": "image", "img_path": "Vtxy8wFpTj/tmp/7b998cb784baf22f5f9086f42f72d4fce4e02686aa824b9bf56aa0207ce24a50.jpg", "img_caption": ["Figure 2: Competitive ratio with FLM. MetaAd (Exp) represents MetaAd with $\\varphi(x)=C(e^{\\theta x}-1)$ and BJN2007 represents the algorithm in [4]. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "and $C$ and compare the results with an existing competitive algorithm BJN2007 [4] for FLM in Figure 2. As $\\kappa\\rightarrow0$ , both MetaAd and BJN2007 achieve the optimal competitive ratio $1-1/e$ in the small-bid setting. However, as $\\kappa$ approaches 1, the competitive ratio of BJN2007 decreases to zero while MetaAd, reducing to a greedy algorithm, maintains a competitive ratio of $\\frac{1}{2}$ , the best known competitive ratio of the deterministic algorithms for the OBM with FLM. ", "page_idx": 8}, {"type": "text", "text": "5 Competitive Learning-Augmented Design ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we demonstrate the application of our competitive analysis for designing learningaugmented algorithms which guarantee a competitive ratio of ML-based solutions for OBM. ", "page_idx": 8}, {"type": "text", "text": "Our competitive analysis directly motivates a learning-augmented algorithm for OBM called LOBM. The algorithm of LOBM and analysis are deferred to Appendix C. In LOBM, we apply a ML model which at each round takes the features of the arriving query and the offilne nodes as inputs and gives the output $\\tilde{z}_{u,t}$ . Directly using $1-\\tilde{z}_{u,t}$ as a discounting value to set the score as $w_{u,t}\\bigl(1-\\tilde{z}_{u,t}\\bigr)$ can result in arbitrarily bad worst-case performance for adversarial examples. To provide a competitive guarantee for OBM, LOBM projects the ML output $\\tilde{z}_{u,t}$ into a competitive solution space $\\mathcal{D}_{u,t}$ in (28) and obtains a projected value $z_{u,t}$ . The score is then set as $w_{u,t}(1-z_{u,t})$ based on the projected ML output $z_{u,t}$ . The key design of the competitive solution space is motivated by the conditions in Lemma 1, which ensures that any $z$ value in $\\mathcal{D}_{u,t}$ leads to the satisfactions of the dual feasibility and primal-dual ratio. The competitive solution space is based on the dual construction given the discounting function \u03c6(x) = ee\u03b8\u03b8x\u2212\u221211 where \u03b8 > 0 in Algorithm 2. Importantly, we introduce a slackness parameter $\\lambda\\in[0,1]$ in the design of $\\mathcal{D}_{u,t}$ in (28). The parameter $\\lambda$ controls the size of the competitive space $\\mathcal{D}_{u,t}$ and further regulates the competitive ratio of LOBM. Given a smaller $\\lambda$ , we can get a larger competitive space $\\mathcal{D}_{u,t}$ , and so LOBM has more flexibility to exploit the benefits of ML predictions. However, a smaller $\\lambda$ also leads to a smaller competitive ratio shown in the theorem below. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.1. Given the maximum bid-budget ratio $\\kappa\\in[0,1]$ , $\\theta>0$ , and the slackness parameter $\\lambda\\in[0,1]$ , with any ML predictions, LOBM in Algorithm 5 achieves a competitive ratio of ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\eta}(\\kappa)=\\frac{\\lambda(1-\\frac{1}{e^{\\theta}})}{1+\\lambda\\left(\\frac{1-e^{-\\theta\\kappa}}{1-\\kappa}+(1-\\frac{1}{e^{\\theta}})\\frac{1}{\\theta}\\left[\\frac{e^{\\theta\\kappa}}{\\kappa}-\\frac{1}{\\kappa}-1\\right]^{+}\\right)}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "table", "img_path": "Vtxy8wFpTj/tmp/bc78a00cc9886ff0393c18aae44e5b97a22eb7990502b3af3f1e9ed2eb0057f2.jpg", "table_caption": [], "table_footnote": ["Table 1: Worst-case and and average normalized reward on the MovieLens dataset. The best results among algorithms w/o ML predictions and the best results among ML-based algorithms are highlighted in bold font. "], "page_idx": 9}, {"type": "text", "text": "Theorem 5.1 shows that LOBM with a slackness parameter $\\lambda\\in[0,1]$ can guarantee a competitiveness ratio of $\\hat{\\eta}(\\kappa)$ regardless of the ML prediction quality. The parameter $\\lambda\\in\\ [0,1]$ determines the worst-case competitive ratio and the degree of flexibility to exploit the benefit of ML predictions. When $\\lambda=0$ , there is no competitive ratio guarantee and LOBM reduces to a pure ML-based algorithm. This can also be seen from the inequalities in the competitive solution space (28), which are all satisfied automatically when $\\lambda=0$ . On the other hand, when $\\lambda=1$ , LOBM achieves the highest competitive ratio. When $\\lambda$ increases from 0 to 1, the competitive solution space in (28) varies from whole solution space (with $\\lambda=0$ ) to the smallest competitive solution space (with $\\lambda=1$ ). Therefore, the choice of the slackness parameter $\\lambda$ provides a trade-off between the competitive guarantee and the average performance by adjusting the level of exploiting the ML predictions. ", "page_idx": 9}, {"type": "text", "text": "6 Empirical Results ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We evaluate the empirical performance of MetaAd and LOBM on two applications. The first application is Online Movie Matching where the platform needs to match each query to a movie advertiser with limited budget. The empirical results are obtained based on the MovieLens Dataset [12]. The main empirical results are shown in Table 1. We compare MetaAd with the algorithms without using ML (Greedy and PrimalDual introduced in Section D.1.1) and show that MetaAd achieves the best worstcase and average performance among them. Additionally, we validate that LOBM with a guarantee of competitive ratio in Theorem 5.1 achieves the best worst-case reward with a good average reward. Other empirical ablation studies can be found in Section D.1.2. ", "page_idx": 9}, {"type": "text", "text": "The second application is Online VM Placement introduced in Section 3. We generate the bipartite graphs with connections between physical servers and VMs by the Barab\u00e1si\u2013Albert method [3] and assign utility values according to the prices of Amazon EC2 compute-optimized instances [2]. We defer the empirical results and ablation studies to Appendix D.2.3. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we consider a challenging setting for OBM without the FLM and small-bid assumption. First, we highlight the challenges by proving an upper bound on the competitive ratio for any deterministic algorithms in OBM. Then, we design the first meta algorithm MetaAd that achieves a provable competitive ratios parameterized by the maximum bid-budget ratio $\\kappa\\in[0,1]$ . We also extend LOBM under the additional FLM assumption. Additionally, based on the competitive analysis, we propose LOBM to take advantage of ML predictions to improve the performance with a competitive ratio guarantee, followed by its empirical validations. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Directions. While we provide the first provable meta algorithms for OBM with general bids, determining the best choice of the discounting function $\\phi$ remains an open question and an interesting problem for future exploration. ", "page_idx": 9}, {"type": "text", "text": "Broader impacts. By introducing a provable algorithm for OBM under more general settings, our work has the potential to advance the applications and motivate new algorithms. For applications like advertising, if large budget disparities among offline nodes exist, those with larger initial budgets could have a higher chance of being matched due to their smaller bid-to-budget ratios. This fairness issue, also observed in prior algorithms [23, 4, 24], warrants further investigation. ", "page_idx": 9}, {"type": "text", "text": "Aknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Jianyi Yang, Pengfei Li and Shaolei Ren were supported in part by NSF grants CNS-2007115 and CCF-2324941. Adam Wierman was supported by NSF grants CCF-2326609, CNS-2146814, CPS2136197, CNS-2106403, and NGSDI-2105648 as well as funding from the Resnick Sustainability Institute. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Mohammad Ali Alomrani, Reza Moravej, and Elias B Khalil. Deep policies for online bipartite matching: A reinforcement learning approach. arXiv preprint arXiv:2109.10380, 2021. [2] Amazon EC2. https://aws.amazon.com/ec2/.   \n[3] Allan Borodin, Christodoulos Karavasilis, and Denis Pankratov. An experimental study of algorithms for online bipartite matching. Journal of Experimental Algorithmics (JEA), 25:1\u201337, 2020.   \n[4] Niv Buchbinder, Kamal Jain, and Joseph Seff iNaor. Online primal-dual algorithms for maximizing ad-auctions revenue. In European Symposium on Algorithms, pages 253\u2013264. Springer, 2007.   \n[5] Nikhil Devanur and Aranyak Mehta. Online matching in advertisement auctions, 2022.   \n[6] Nikhil R Devanur and Thomas P Hayes. The adwords problem: online keyword matching with budgeted bidders under random permutations. In Proceedings of the 10th ACM conference on Electronic commerce, pages 71\u201378, 2009.   \n[7] Nikhil R Devanur, Kamal Jain, and Robert D Kleinberg. Randomized primal-dual analysis of ranking for online bipartite matching. In Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms, pages 101\u2013107. SIAM, 2013.   \n[8] Gagan Goel and Aranyak Mehta. Adwords auctions with decreasing valuation bids. In International Workshop on Web and Internet Economics, pages 335\u2013340. Springer, 2007.   \n[9] Gagan Goel and Aranyak Mehta. Online budgeted matching in random input models with applications to adwords. In SODA, volume 8, pages 982\u2013991. Citeseer, 2008.   \n[10] Robert Grandl, Ganesh Ananthanarayanan, Srikanth Kandula, Sriram Rao, and Aditya Akella. Multi-resource packing for cluster schedulers. ACM SIGCOMM Computer Communication Review, 44(4):455\u2013466, 2014.   \n[11] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2024.   \n[12] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):1\u201319, 2015.   \n[13] Nguyen Trung Hieu, Mario Di Francesco, and Antti Yl\u00e4 J\u00e4\u00e4ski. A virtual machine placement algorithm for balanced resource utilization in cloud data centers. In 2014 IEEE 7th International Conference on Cloud Computing, pages 474\u2013481. IEEE, 2014.   \n[14] Zhiyi Huang, Zhihao Gavin Tang, Xiaowei Wu, and Yuhao Zhang. Online vertex-weighted bipartite matching: Beating 1-1/e with random arrivals. ACM Transactions on Algorithms (TALG), 15(3):1\u201315, 2019.   \n[15] Zhiyi Huang, Qiankun Zhang, and Yuhao Zhang. Adwords in a panorama. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 1416\u20131426. IEEE, 2020.   \n[16] Patrick Jaillet and Xin Lu. Online resource allocation problems. Rock & Soil Mechanics, 86:3701\u20133704, 2011.   \n[17] Bala Kalyanasundaram and Kirk R Pruhs. An optimal deterministic algorithm for online b-matching. Theoretical Computer Science, 233(1-2):319\u2013325, 2000.   \n[18] Michael Kapralov, Ian Post, and Jan Vondr\u00e1k. Online submodular welfare maximization: Greedy is optimal. In Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms, pages 1216\u20131225. SIAM, 2013.   \n[19] Richard M Karp, Umesh V Vazirani, and Vijay V Vazirani. An optimal algorithm for on-line bipartite matching. In Proceedings of the twenty-second annual ACM symposium on Theory of computing, pages 352\u2013358, 1990.   \n[20] Nathaniel Kell and Debmalya Panigrahi. Online budgeted allocation with general budgets. In Proceedings of the 2016 ACM Conference on Economics and Computation, pages 419\u2013436, 2016.   \n[21] Pengfei Li, Jianyi Yang, Adam Wierman, and Shaolei Ren. Robust learning for smoothed online convex optimization with feedback delay. In NeurIPS, 2023.   \n[22] Wubin Li, Johan Tordsson, and Erik Elmroth. Modeling for dynamic cloud scheduling via migration of virtual machines. In 2011 IEEE Third International Conference on Cloud Computing Technology and Science, pages 163\u2013171. IEEE, 2011.   \n[23] Aranyak Mehta. Online matching and ad allocation. Foundations and Trends\u00ae in Theoretical Computer Science, 8(4):265\u2013368, 2013.   \n[24] Aranyak Mehta, Amin Saberi, Umesh Vazirani, and Vijay Vazirani. Adwords and generalized online matching. Journal of the ACM (JACM), 54(5):22\u2013es, 2007.   \n[25] Vahab S Mirrokni, Shayan Oveis Gharan, and Morteza Zadimoghaddam. Simultaneous approximations for adversarial and stochastic online budgeted allocation. In Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms, pages 1690\u20131701. SIAM, 2012.   \n[26] Mayank Mishra and Anirudha Sahoo. On theory of vm placement: Anomalies in existing methodologies and their mitigation using a novel vector based approach. In 2011 IEEE 4th International Conference on Cloud Computing, pages 275\u2013282. IEEE, 2011.   \n[27] Zhonghong Ou, Hao Zhuang, Jukka K Nurminen, Antti Yl\u00e4-J\u00e4\u00e4ski, and Pan Hui. Exploiting hardware heterogeneity within the same instance type of amazon ec2. In 4th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 12), 2012.   \n[28] Benjamin Speitkamp and Martin Bichler. A mathematical programming approach for server consolidation problems in virtualized data centers. IEEE Transactions on services computing, 3(4):266\u2013278, 2010.   \n[29] Rajan Udwani. Adwords with unknown budgets and beyond. arXiv preprint arXiv:2110.00504, 2021.   \n[30] Vijay V Vazirani. Towards a practical, budget-oblivious algorithm for the adwords problem under small bids.   \n[31] Alexander Wei and Fred Zhang. Optimal robustness-consistency trade-offs for learningaugmented online algorithms. In NeurIPS, 2020.   \n[32] Dan Zhang and William L Cooper. Revenue management for parallel flights with customerchoice behavior. Operations Research, 53(3):415\u2013431, 2005. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proof of theorems in Section 4 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A.1 Proof of Proposition 4.1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Proof. Proposition 4.1 can be proved as follows. Denote $C R(\\pi;G)$ as the competitive ratio of a deterministic algorithm $\\pi$ on the graph instance $G$ . The competitive ratio for any deterministic algorithm is $\\mathrm{max}_{\\pi}\\,\\mathrm{min}_{G\\in\\mathcal G}\\,C R(\\pi,G)$ which is no larger than $\\begin{array}{r}{\\operatorname*{max}_{\\pi}\\operatorname*{min}_{G\\in\\mathcal G^{\\prime}}C\\dot{R}(\\pi,G)}\\end{array}$ where $\\mathcal{G}^{\\prime}\\subset\\mathcal{G}$ . Thus, we can prove the upper bound of the competitive ratio by constructing an example subset $\\mathcal{G}^{\\prime}$ and deriving the resulting competitive ratio for any deterministic algorithm. Specifically, an example subset $\\mathcal{G}^{\\prime}$ is constructed as below. ", "page_idx": 11}, {"type": "text", "text": "Example 1. Consider a setting with only one offilne node and a total budget of 1. The agent needs to decide whether or not to match an online node with a bid value $w_{u,t}\\le\\kappa,\\kappa\\in(0,1]$ to the offline node for $V\\geq2$ rounds. The bid values for the first $V-1$ rounds are equivalent to $\\omega$ and sum up to $1-\\kappa+\\epsilon$ where $\\epsilon$ is infinitely small, so we have $\\begin{array}{r}{\\omega\\in(0,(1-\\kappa+\\epsilon)/\\left\\lceil\\frac{\\bar{1}-\\kappa+\\epsilon}{\\kappa}\\right\\rceil\\right]}\\end{array}$ . The bid value $w_{u,V}$ in the last round is either zero or \u03ba and is not known to the agent. Thus, the constructed example subset is composed of two smaller subsets, i.e. $\\mathcal{G}^{\\prime}=\\mathcal{G}_{1}^{\\prime}+\\mathcal{G}_{2}^{\\prime}$ . In the example of the subset $\\mathcal{G}_{1}^{\\prime}$ , we have $w_{u,V}=\\kappa,$ , and in the examples of the subset $\\mathcal{G}_{2}^{\\prime}$ , we have $w_{u,V}=0$ . ", "page_idx": 12}, {"type": "text", "text": "The offilne optimal solutions are different for $\\mathcal{G}_{1}^{\\prime}$ and $\\mathcal{G}_{2}^{\\prime}$ in Example 1. For $\\mathcal{G}_{1}^{\\prime}$ with the last bid value as $w_{u,V}=\\kappa$ , the optimal solution is to skip one of the first $(V-1)$ rounds. In this way, the last online node with bid $\\kappa$ can be matched and the total reward is $1+\\epsilon-\\omega$ . For $\\mathcal{G}_{2}^{\\prime}$ with the last bid value as $w_{u,V}=0$ , the optimal solution is to match all the online nodes for the first $V-1$ rounds and obtain a total reward of $1-\\kappa+\\epsilon$ . ", "page_idx": 12}, {"type": "text", "text": "For the examples in $\\mathcal{G}^{\\prime}$ in Example 1, the optimal online algorithm can be chosen from the following two. First, the algorithm can choose to match the online node to the offline node in all the first $(V-1)$ rounds. This algorithm is optimal for $\\mathcal{G}_{2}^{\\prime}$ , but for $\\mathcal{G}_{1}^{\\prime}$ with $w_{u,V}\\,=\\,\\kappa$ , the total reward is $1-\\kappa+\\epsilon$ which is less than the offline optimal reward $1+\\epsilon-\\omega$ . Therefore, the competitive ratio of this algorithm in the worst case is lim\u03f5\u21920 min\u03c9\u2208(0,(1\u2212\u03ba+\u03f5)/\u23081\u2212\u03ba+\u03f5\u2309] 11+\u2212\u03f5\u03ba\u2212+\u03c9\u03f5 \u21921 \u2212\u03ba when \u03c9 is infinitely small. Second, the algorithm can choose to skip one round in the first $V-1$ rounds such that the last online node can be matched if it has a bid value of $\\kappa$ in $\\mathcal{G}_{1}^{\\prime}$ . However, for $\\mathcal{G}_{2}^{\\prime}$ with $w_{u,V}=0$ and the total reward is $1-\\kappa+\\epsilon-\\omega$ which is less than the optimal reward as $1-\\kappa+\\epsilon$ . Thus, the competitive ratio of this algorithm is lim\u03f5\u21920 min\u03c9\u2208(0,(1\u2212\u03ba+\u03f5)/(\u23081\u2212\u03ba\u03ba+\u03f5\u2309])1\u22121\u03ba\u2212+\u03ba\u03f5+\u2212\u03f5\u03c9 = 1 \u2212 for \u03ba \u2208 (0, 1). When \u03ba = 1, the competitive ratio of this algorithm is min\u03c9\u2208(0,\u03f5)1\u22121\u03ba\u2212+\u03ba\u03f5+\u2212\u03f5\u03c9 $\\begin{array}{r}{\\operatorname*{min}_{\\omega\\in(0,\\epsilon)}\\frac{\\epsilon-\\omega}{\\epsilon}\\,=\\,0}\\end{array}$ . Therefore, the competitive ratio for any deterministic algorithm for $\\mathcal{G}^{\\prime}$ is $\\begin{array}{r}{\\operatorname*{max}_{\\pi}\\operatorname*{min}_{G\\in\\mathcal{G}^{\\prime}}C R(\\pi,G)=\\operatorname*{max}\\{1-\\kappa,1-\\frac{1}{\\lceil\\frac{1-\\kappa}{\\kappa}\\rceil}\\}=1-\\kappa}\\end{array}$ for $\\kappa\\,\\in\\,(0,1)$ , and 0 for $\\kappa\\,=\\,1$ Combining both cases of $\\kappa\\in(0,1)$ and $\\kappa=1$ , we get the upper bound of the competitive ratio for any deterministic algorithm for $\\mathcal{G}^{\\prime}$ as $1-\\kappa$ , which is also an upper bound of the competitive ratio of any deterministic algorithm for OBM. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "A.2 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "To prove Theorem A.2, we first prove Lemma 1. ", "page_idx": 12}, {"type": "text", "text": "Proof of Lemma 1. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof. The first condition guarantees that the dual variables are feasible. The second condition is to guarantee the competitive performance. Let $D^{*}$ and $P^{*}$ be the optimal dual and primal objectives. If the second condition is satisfied, then we have ", "page_idx": 12}, {"type": "equation", "text": "$$\nP\\geq\\eta D\\geq\\eta D^{*}\\geq\\eta P^{*},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the second inequality holds since $D^{*}$ is the minimum dual objective, and the third inequality comes from weak duality. This completes the proof. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Proof. To satisfy the primal-dual ratio $\\begin{array}{r}{P_{t}\\ge\\frac{1}{\\gamma}D_{t}}\\end{array}$ , we can get an inequality of $\\alpha_{u,t}$ as below. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{u\\in\\mathcal{U}}B_{u}\\alpha_{u,t}+\\sum_{i=1}^{t}\\beta_{t}\\leq\\gamma\\cdot\\displaystyle\\sum_{i=1}^{t}w_{x_{i},i}}\\\\ &{\\Leftrightarrow\\displaystyle\\sum_{u\\in\\mathcal{U}}B_{u}\\alpha_{u,t}+\\sum_{u\\in\\mathcal{U}}\\sum_{i=1,x_{i}=u}^{t}w_{u,i}(1-\\varphi(\\frac{c_{u,i-1}}{B_{u}}))\\leq\\gamma\\cdot\\displaystyle\\sum_{u\\in\\mathcal{U}}c_{u,t}}\\\\ &{\\Leftrightarrow\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\begin{array}{r}{c_{u,t}=\\sum_{i=1,{x_{i}}=u}^{t}w_{x_{i},i}}\\end{array}$ . Since $\\begin{array}{r}{\\alpha_{u,t}=\\varphi\\big(\\frac{c_{u,t}}{B_{u}}\\big)}\\end{array}$ , we get the condition of $\\varphi$ as below. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\varphi(\\frac{c_{u,t}}{B_{u}})\\leq\\sum_{i=1,x_{i}=u}^{t}\\frac{w_{u,i}}{B_{u}}\\varphi(\\frac{c_{u,i-1}}{B_{u}})+(\\gamma-1)\\frac{c_{u,t}}{B_{u}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Further, since $\\varphi$ is an increasing function, we have the following bound for the discrete sum. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=1,x_{i}=u}^{t}\\frac{w_{u,i}}{B_{u}}\\varphi(\\frac{c_{u,i-1}}{B_{u}})}}\\\\ &{=\\sum_{i=1,x_{i}=u}^{t}\\frac{w_{u,i}}{B_{u}}\\varphi(\\frac{c_{u,i}}{B_{u}})-\\sum_{i=1,x_{i}=u}^{t}\\frac{w_{u,i}}{B_{u}}(\\varphi(\\frac{c_{u,i}}{B_{u}})-\\varphi(\\frac{c_{u,i-1}}{B_{u}}))}\\\\ &{\\geq\\int_{x=0}^{\\frac{c_{u,t}}{B_{u}}}\\varphi(x)d x-\\sum_{i=1,x_{i}=u}^{t}\\frac{w_{u,i}}{B_{u}}(\\varphi(\\frac{c_{u,i}}{B_{u}})-\\varphi(\\frac{c_{u,i-1}}{B_{u}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the inequality holds by the integral inequality $\\begin{array}{r}{\\int_{x=0}^{y}\\varphi(x)d x\\,\\le\\,\\sum_{i=1}^{N}x_{i}\\varphi(\\sum_{j=1}^{i}x_{j})}\\end{array}$ with $\\textstyle y=\\sum_{i=1}^{N}x_{i}$ for a positive increasing function $\\varphi$ . If $\\varphi^{\\prime\\prime}(x)\\leq0$ for $x\\in[0,1]$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{i=1,x_{i}=u}{\\overset{t}{\\sum}}\\frac{w_{u,i}}{B_{u}}\\varphi(\\frac{c_{u,i-1}}{B_{u}})}\\\\ &{\\geq\\int_{x_{0}}^{\\frac{c_{u,i}}{B_{u}}}\\varphi(x)d x-\\underset{i=1,x_{i}=u}{\\overset{t}{\\sum}}(\\frac{w_{u,i}}{B_{u}})^{2}\\varphi^{\\prime}(\\frac{c_{u,i-1}}{B_{u}})}\\\\ &{\\geq\\int_{x_{0}}^{\\frac{c_{u,i}}{B_{u}}}\\varphi(x)d x-\\kappa\\int_{x=0}^{\\frac{c_{u,i}}{B_{u}}}\\varphi^{\\prime}(x)d x}\\\\ &{=\\int_{x=0}^{\\frac{c_{u,i}}{B_{u}}}\\varphi(x)d x-(\\kappa\\varphi(\\frac{c_{u,i}}{B_{u}})-\\kappa\\varphi(0)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first inequality holds since $\\varphi^{\\prime\\prime}(x)\\leq0$ for $x\\in[0,1]$ , the second inequality holds by the bid bound $\\kappa$ and another integral inequality $\\begin{array}{r}{\\int_{x=0}^{y}\\varphi(x)d x\\geq\\sum_{i=1}^{N}x_{i}\\varphi(\\sum_{j=1}^{i-1}x_{j})}\\end{array}$ with $\\textstyle y=\\sum_{i=1}^{N}x_{i}$ . ", "page_idx": 13}, {"type": "text", "text": "If $0<\\varphi^{\\prime\\prime}(x)\\le R$ for $x\\in[0,1]$ , following Eqn. (12), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{i=1,x_{i}=u}{\\overset{t}{\\sum}}\\frac{w_{u,i}}{B_{u}}\\varphi(\\frac{c_{u,i-1}}{B_{u}})}\\\\ &{\\geq\\int_{x=0}^{\\frac{c_{u,i}}{B_{u}}}\\varphi(x)d x-\\underset{i=1,x_{i}=u}{\\overset{t}{\\sum}}\\frac{w_{u,i}}{B_{u}}\\varphi^{\\prime}(\\frac{c_{u,i-1}}{B_{u}})-\\underset{i=1,x_{i}=u}{\\overset{t}{\\sum}}\\frac{w_{u,i}}{B_{u}}(\\varphi^{\\prime}(\\frac{c_{u,i}}{B_{u}})-\\varphi^{\\prime}(\\frac{c_{u,i-1}}{B_{u}})),}\\\\ &{\\geq\\int_{x=0}^{\\frac{c_{u,i}}{B_{u}}}\\varphi(x)d x-\\kappa\\int_{x=0}^{\\frac{c_{u,i}}{B_{u}}}\\varphi^{\\prime}(x)d x-\\kappa\\underset{i=1,x_{i}=u}{\\overset{t}{\\sum}}(\\frac{w_{u,i}}{B_{u}})^{2}R}\\\\ &{=\\int_{x=0}^{\\frac{c_{u,i}}{B_{u}}}\\varphi(x)d x-(\\kappa\\varphi(\\frac{c_{u,i}}{B_{u}})-\\kappa\\varphi(0))-\\kappa^{2}R\\frac{c_{u,i}}{B_{u}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second inequality holds by the integral inequality and $\\varphi^{\\prime\\prime}(x)\\leq R$ . ", "page_idx": 14}, {"type": "text", "text": "Therefore, we can extend to the case where $\\varphi$ has $n-$ the derivative. If $\\varphi^{(i)}(x)>0,\\forall i\\leq n,\\forall x\\in$ $[0,1]$ , then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1,x_{i}=u}^{t}\\frac{w_{u,i}}{B_{u}}\\varphi(\\frac{c_{u,i-1}}{B_{u}})}\\\\ &{\\geq\\displaystyle\\int_{x=0}^{\\frac{c_{u,t}}{B_{u}}}\\varphi(x)d x-\\sum_{i=1}^{n}\\kappa^{i}\\varphi^{(i-1)}(\\frac{c_{u,t}}{B_{u}})+\\sum_{i=1}^{n}\\kappa^{i}\\varphi^{(i-1)}(0)-\\kappa^{n+1}R\\frac{c_{u,t}}{B_{u}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $R$ is the Lipschitz constant of $\\varphi^{(n)}(x)$ if $\\varphi^{(n)}(x)$ is not monotonically decreasing and $R=0$ otherwise. ", "page_idx": 14}, {"type": "text", "text": "Substituting (15) into (11), the condition becomes ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\varphi(\\frac{c_{u,t}}{B_{u}})\\leq(\\gamma-1)\\frac{c_{u,t}}{B_{u}}+\\int_{x=0}^{\\frac{c_{u,t}}{B_{u}}}\\varphi(x)d x-\\sum_{i=1}^{n}\\kappa^{i}\\varphi^{(i-1)}(\\frac{c_{u,t}}{B_{u}})+\\sum_{i=1}^{n}\\kappa^{i}\\varphi^{(i-1)}(0)-\\kappa^{n+1}R\\frac{c_{u,t}}{B_{u}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, a $\\varphi$ function satisfies the primal dual ratio $\\begin{array}{r}{P_{t}\\ge\\frac{1}{\\gamma}D_{t}}\\end{array}$ if it satisfies for any $y\\in[0,1]$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\varphi(y)-\\int_{x=0}^{y}\\varphi(x)d x+\\sum_{i=1}^{n}\\kappa^{i}\\varphi^{(i-1)}(y)+(\\rho\\kappa^{n+1}R-\\gamma+1)y\\leq\\rho\\sum_{i=1}^{n}\\kappa^{i}\\varphi^{(i-1)}(0).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Finally, we bound $\\sum_{u\\in\\mathcal{U}^{\\circ}}B_{u}\\Lambda_{u}$ where $\\Lambda_{u}=\\alpha_{u}-\\alpha_{u,V}$ is the dual increase at the end of the dual construction 2. $\\Lambda_{u}>0$ can hold for $u\\in\\mathcal{U}^{\\circ}$ because $\\alpha_{u}=1$ for $u\\in\\mathcal{U}^{\\circ}$ Thus, after the loops, we have for $u\\in\\mathcal{U}^{\\circ}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{B_{u}\\Lambda_{u}=B_{u}\\left(1-\\alpha_{u,V}\\right)=B_{u}\\left(1-\\varphi(\\frac{c_{u,V}}{B_{u}})\\right)}&{{}}\\\\ {=B_{u}\\phi(\\frac{b_{u,V}}{B_{u}})\\leq B_{u}\\phi(\\kappa)}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the inequality holds since $b_{u,V}\\leq\\kappa B_{u}$ for any $u\\in\\mathcal{U}^{\\circ}$ . Thus, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{u\\in\\mathcal{U}^{\\circ}}B_{u}\\Lambda_{u}\\leq\\sum_{u\\in\\mathcal{U}^{\\circ}}B_{u}\\phi(\\kappa)\\leq\\phi(\\kappa)\\cdot P\\cdot\\frac{\\sum_{u\\in\\mathcal{U}^{\\circ}}B_{u}}{\\sum_{u\\in\\mathcal{U}^{\\circ}}(1-\\kappa)B_{u}}=\\frac{\\phi(\\kappa)}{1-\\kappa}\\cdot P,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second inequality holds because $\\begin{array}{r}{P\\ge\\sum_{u\\in\\mathcal{U}^{\\circ}}(1-\\kappa)B_{u}}\\end{array}$ given that $c_{u,V}\\geq(1-\\kappa)B_{u}$ for $u\\in\\mathcal{U}^{\\circ}$ . ", "page_idx": 14}, {"type": "text", "text": "Putting them together, we have $\\begin{array}{r}{P\\,\\ge\\,\\frac{1}{\\gamma}(D-\\,\\frac{\\phi(\\kappa)}{1-\\kappa}\\,\\cdot\\,P)}\\end{array}$ which leads to the primal dual ratio as $P\\ge\\frac{1}{\\gamma+\\frac{\\phi(\\kappa)}{1-\\kappa}}D$ \u03b3+ 1\u03d5(\u03ba) D. To satisfy (17) for any \u03c6, we can choose \u03b3 \u22651 + \u03ban+1R + \u03c6(yy)\u2212y1 xy=0 \u03c6(x)dx + $\\textstyle{\\frac{1}{y}}\\sum_{i=1}^{n}\\kappa^{i}\\left(\\varphi^{(i-1)}(y)-\\varphi^{(i-1)}(0)\\right)$ for any $y\\ \\in\\ [0,1]$ . Combining with Lemma 1, we get the competitive ratio in Theorem 4.2. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Algorithm 3 Meta Algorithm (MetaAd with FLM)   \nRequire: The function $\\phi:[0,1]\\rightarrow[0,1]$ Initialization: $\\forall u\\in\\mathcal{U}$ , the remaining budget $b_{u,0}=B_{u}$ . for $\\scriptstyle t=1$ to $V$ , a new vertex $t\\in\\mathcal{V}$ arrives do For $u\\in\\mathcal{U}$ , set $\\begin{array}{r}{s_{u,t}\\,=\\,w_{u,t}\\phi\\big(\\frac{b_{u,t-1}}{B_{u}}\\big)}\\end{array}$ if $b_{u,t-1}-w_{u,t}\\,\\geq\\,0$ , and set $\\begin{array}{r}{s_{u,t}\\,=\\,b_{u,t-1}\\phi\\bigl(\\frac{b_{u,t-1}}{B_{u}}\\bigr)}\\end{array}$ , otherwise. if $\\forall u\\in\\mathcal{U},s_{u,t}=0$ then Skip the online arrival $t$ $\\left\\langle x_{t}=\\mathrm{null}\\right\\rangle$ . else Select xt = arg maxu\u2208U su,t. end if Update budget: If $x_{t}\\neq\\mathrm{null}$ $\\neq\\mathrm{null},b_{x_{t},t}=b_{x_{t},t-1}-w_{x_{t},t}$ ; and $\\forall u\\neq x_{t},b_{u,t}=b_{u,t-1}$ . end for ", "page_idx": 15}, {"type": "text", "text": "B MetaAd for OBM with FLM ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we extend MetaAd to the setting with FLM by allowing offilne nodes with insufficient budgets to accept fractional bid values equal to their remaining budgets in their last matching. In other words, by matching an online arrival $t$ to an offline node $u\\in\\mathcal{U}$ , the agent receives an actual reward of $\\operatorname*{min}\\{w_{u,t},b_{u,t-1}\\}$ , where $w_{u,t}$ is the bid value and $b_{u,t-1}$ is the available budget at the beginning of round $t$ . ", "page_idx": 15}, {"type": "text", "text": "B.1 Algorithm Design ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Even under the FLM assumption, OBM with general bids is challenging because when an online node $t$ arrives, if the remaining budget $b_{u,t-1}$ of an offline node $u$ is smaller than the bid $w_{u,t}$ , matching the arrival to this offilne node can cause a reward loss of $w_{u,t}-b_{u,t-1}$ , which increases with the bid vTahleu ec $w_{u,t}$ e.t itWiviteh r aFtiLo Ma,c thhiee vgerde ebdyy a  adlgetoerritmhimni s(tGicr ealegdoyr)i tchamn  ianc [hi4e] vies .of 0.5 [23]. $\\begin{array}{r}{\\big(1-\\kappa-\\frac{1-\\kappa}{(1+\\kappa)^{1/\\kappa}}\\big)}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "For OBM with FLM, we use a different meta algorithm as in Algorithm 3. When the remaining budget $b_{u,t-1}$ for an offline node $u$ is enough to accept arrival $t$ (i.e. $b_{u,t-1}\\,\\geq\\,w_{u,t})$ , the scoring strategy is the same as Algorithm 1 which sets the score as su,t = wu,t\u03d5( buB,tu\u2212 1 . Nonetheless, the scoring strategy is different from Algorithm 1 when the remaining budget $b_{u,t-1}$ of an offline node $u$ is insufficient for an online arrival $t$ (i.e. $b_{u,t-1}\\,<\\,w_{u,t})$ . Without FLM, Algorithm 1 directly sets the score $s_{u,t}$ as zero to avoid the selection of offline node $u$ . However, FLM allows matching an offline node $u$ to the online arrival $t$ and consuming all the remaining budget $b_{u,t-1}$ to obtain a reward of $b_{u,t-1}$ . Thus, Algorithm 3 can be greedier and sets the score as $\\begin{array}{r}{s_{u,t}=b_{u,t-1}\\phi(\\frac{b_{u,t-1}}{B_{u}})}\\end{array}$ balance the actual reward increment and the budget consumption. Given an increasing function $\\phi$ , the score increases with the remaining budget, and it is still possible to select an offline node with an insufficient but large enough remaining budget. ", "page_idx": 15}, {"type": "text", "text": "B.2 Competitive Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide the competitive ratio of Algorithm 3 for OBM with FLM and discuss the insights and analysis techniques. The competitive ratio is given in Theorem B.1 with its proof deferred to Appendix B.3. ", "page_idx": 15}, {"type": "text", "text": "To prove the competitive ratio of MetaAd for FLM, we still need to construct dual variables $\\alpha_{u},u\\in$ $\\mathcal{U}\\ \\beta_{t},t\\,\\in\\,[V]$ , which assists with online matching with a provable competitive ratio. The dual construction procedure is given in Algorithm 4. At each round $t$ , same as the dual construction without FLM in Algorithm 2, $\\beta_{t}$ is set as the score of the selected offline node and $\\alpha_{u,t}$ is set as $\\varphi\\big(\\frac{c_{u,t}}{B_{u}}\\big)$ . Different from Algorithm 2, $\\alpha_{u}$ is set at the end of the algorithm as below to satisfy dual feasibility with the FLM assumption. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{u}=\\operatorname*{max}\\left\\{\\alpha_{u,V},\\left\\{1-\\frac{b_{u,t-1}}{w_{u,t}}\\phi(\\frac{b_{u,t-1}}{B_{u}}),t\\in\\mathcal{T}^{\\circ}\\right\\}\\right\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Require: The function $\\phi:[0,1]\\rightarrow[0,1]$ , $\\varphi(x)=1-\\phi(1-x)$ , $\\rho\\geq1$ . Initialization: $\\forall u\\in\\mathcal{U}$ , $\\alpha_{u,0}=0$ , $b_{u,0}=B_{u}$ , $\\mathcal{U}^{\\circ}=\\varnothing$ , $\\mathcal{T}^{\\circ}=\\varnothing$ , and $\\forall t\\in[V],\\beta_{t}=0$ . for $\\scriptstyle t=1$ to $V$ , a new vertex $t\\in\\mathcal{V}$ arrives do If there exist $u$ such that $b_{u,t-1}-w_{u,t}<0.$ , append $\\{u\\mid b_{u,t-1}-w_{u,t}<0\\}$ into $\\mathcal{U}^{\\circ}$ and append $t$ to $\\mathcal{T}^{\\circ}$ . Score $s_{u,t}$ , select $x_{t}$ for the arrival $t$ , and update budget $b_{u,t}$ by Algorithm 3. Set the dual variable $\\beta_{t}=s_{x_{t},t}$ , and set the dual variable $\\begin{array}{r}{\\dot{\\alpha}_{u,t}=\\bar{\\varphi}\\big(\\frac{c_{u,t}}{B_{u}}\\big)}\\end{array}$ . end for For $u\\notin\\mathcal{U}^{\\circ}$ , set $\\alpha_{u}=\\alpha_{u,V}$ ; and for $u\\in\\mathcal{U}^{\\circ}$ , set $\\alpha_{u}$ as Eqn. (20). ", "page_idx": 16}, {"type": "text", "text": "where $t\\in\\mathcal{T}^{\\circ}$ is the round when budget insufficiency happens. This is to guarantee the dual feasibility $\\beta_{t}\\geq b_{u,t-1}(1-\\alpha_{u,t-1})\\geq w_{u,t}(1-\\alpha_{u})$ when the offline node has an insufficient budget for an arrival. Note that the dual increment $\\Lambda_{u}=\\alpha_{u}-\\alpha_{u,V}$ at the end of the Algorithm 4 can be less than the dual increment at the end of Algorithm 2, thus resulting in a better competitive ratio for OBM with FLM than without FLM. ", "page_idx": 16}, {"type": "text", "text": "With the constructed dual variables, the competitive ratio of MetaAd for OBM with FLM is given in the next theorem. ", "page_idx": 16}, {"type": "text", "text": "Theorem B.1. If the function $\\phi:[0,1]\\rightarrow[0,1]$ in Algorithm 1 satisfies that given an integer $n\\geq2$ , $\\forall i\\leq n-1$ , $\\varphi^{(i)}(x)>0$ and $R=\\operatorname*{max}_{x\\in[0,1]}\\varphi^{(n)}(x)$ where $\\varphi(x)=1-\\phi(1-x),$ , the competitive ratio of Algorithm $^{\\,I}$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\eta(\\kappa)=\\frac{1}{1+\\kappa^{n}R+\\operatorname*{max}_{y\\in[0,1]}\\Delta(y)+\\phi(\\kappa)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$\\begin{array}{r}{\\Delta(y)=\\frac{\\varphi(y)}{y}-\\frac{1}{y}\\int_{x=0}^{y}\\varphi(x)d x+\\frac{1}{y}\\sum_{i=1}^{n}\\kappa^{i}\\,\\big(\\varphi^{(i-1)}(y)-\\varphi^{(i-1)}(0)\\big).}\\end{array}$ ", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given different $\\varphi$ , we can get concrete competitive algorithms for OBM with FLM. In this paper, we show the example of the competitive algorithm where $\\varphi$ is from the exponential function class. ", "page_idx": 16}, {"type": "text", "text": "Corollary B.1.1. If we assign $\\varphi(x)=C e^{\\theta x}-C$ with $C\\geq0$ and $0\\leq\\theta\\leq1$ in MetaAd in Algorithm $^3$ , we get the competitive ratio as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\eta(\\kappa)=\\left\\{\\!\\!\\!\\begin{array}{c}{{\\frac{1}{1+C+C(1-\\frac{1}{\\theta}+\\frac{\\kappa}{1-\\kappa\\theta})(e^{\\theta}-1)+1+C-C e^{\\theta(1-\\kappa)}},1-\\frac{1}{\\theta}+\\frac{\\kappa}{1-\\kappa\\theta}\\geq0}}\\\\ {{\\frac{1}{1+C+C(1-\\frac{1}{\\theta}+\\frac{\\kappa}{1-\\kappa\\theta})\\theta+1+C-C e^{\\theta(1-\\kappa)}},1-\\frac{1}{\\theta}+\\frac{\\kappa}{1-\\kappa\\theta}<0}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.3 Proof of Theorem B.1 (Theorem 4.3) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Denote an equivalent bid as $\\bar{w}_{u,t}=\\operatorname*{min}\\{w_{u,t},b_{u,t-1}\\}$ . To guarantee the primal-dual ratio $\\begin{array}{r}{P_{t}\\ge\\frac{1}{\\gamma}D_{t}}\\end{array}$ , we get the condition as below. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{u\\in\\mathcal{U}}B_{u}\\alpha_{u,t}+\\sum_{i=1}^{t}\\beta_{t}\\leq\\gamma\\cdot\\displaystyle\\sum_{i=1}^{t}\\overline{{w}}_{x_{i},i}}\\\\ &{\\Leftrightarrow\\displaystyle\\sum_{u\\in\\mathcal{U}}B_{u}\\alpha_{u,t}+\\sum_{u\\in\\mathcal{U}}\\displaystyle\\sum_{u=1,x_{i}=u}^{t}\\overline{{w}}_{u,i}(1-\\varphi(\\frac{c_{u,i-1}}{B_{u}}))\\leq\\gamma\\cdot\\displaystyle\\sum_{u\\in\\mathcal{U}}c_{u,t}}\\\\ &{\\Leftrightarrow\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where cu,t = it=1,x=u w\u00afxi,i. ", "page_idx": 16}, {"type": "text", "text": "Since $\\begin{array}{r}{\\alpha_{u,t}=\\varphi(\\frac{c_{u,t}}{B_{u}})}\\end{array}$ , we get the condition of $\\varphi$ as below. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\varphi(\\frac{c_{u,t}}{B_{u}})\\leq\\sum_{i=1,x_{i}=u}^{t}\\frac{\\bar{w}_{u,i}}{B_{u}}\\varphi(\\frac{c_{u,i-1}}{B_{u}})+(\\gamma-1)\\frac{c_{u,t}}{B_{u}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Same as the setting with FLM, we can bound the discrete sum as below. If for $i\\leq n-1,\\varphi^{(i)}(x)>0$ , $x\\in[0,1]$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1,x_{i}=u}^{t}\\frac{\\bar{w}_{u,i}}{B_{u}}\\varphi(\\frac{c_{u,i-1}}{B_{u}})}\\\\ &{\\geq\\displaystyle\\int_{x=0}^{\\frac{c_{u,t}}{B_{u}}}\\varphi(x)d x-\\sum_{i=1}^{n-1}\\kappa^{i}\\varphi^{(i-1)}(\\frac{c_{u,t}}{B_{u}})+\\sum_{i=1}^{n-1}\\kappa^{i}\\varphi^{(i-1)}(0)-\\kappa^{n}R\\frac{c_{u,t}}{B_{u}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $R$ is the Lipschitz constant of $\\varphi^{(n)}(x)$ if $\\varphi^{(n)}(x)$ is not monotonically decreasing and $R=0$ otherwise. Thus, a $\\varphi$ function satisfies the primal dual ratio $\\begin{array}{r}{P_{t}\\ge\\frac{1}{\\gamma}D_{t}}\\end{array}$ if it holds for any $y\\in[0,1]$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\varphi(y)-\\int_{x=0}^{y}\\varphi(x)d x+\\sum_{i=1}^{n-1}\\kappa^{i}\\varphi^{(i-1)}(y)+(\\kappa^{n}R-\\gamma+\\rho)y\\leq\\sum_{i=1}^{n-1}\\kappa^{i}\\varphi^{(i-1)}(0).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, we bound $\\textstyle\\sum_{u\\in\\mathcal{U}^{\\circ}}B_{u}\\Lambda_{u}$ where $\\Lambda_{u}=\\alpha_{u}-\\alpha_{u,V}$ . $\\Lambda_{u}>0$ can hold for $u\\in\\mathcal{U}^{\\circ}$ because $\\begin{array}{r}{\\alpha_{u}\\;=\\;\\operatorname*{max}\\left\\{\\alpha_{u,V},\\left\\{1-\\frac{b_{u,t-1}}{w_{u,t}}\\phi\\!\\left(\\frac{b_{u,t-1}}{B_{u}}\\right)\\!,t\\in\\mathcal{T}^{\\circ}\\right\\}\\right\\}}\\end{array}$ by Eqn. (20). Thus, for a request $t\\ \\in\\ T^{\\circ}$ , we have $b_{u,t-1}-w_{u,t}<0$ and $b_{u,t-1}\\,\\leq\\,\\kappa B_{u}$ . Since $\\varphi$ is an increasing function, it holds that $\\alpha_{u,V}\\geq\\alpha_{u,t}$ for $t\\in[V]$ . Thus, after the $V$ loops, we have for $u\\in\\mathcal{U}^{\\circ}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{u}\\Lambda_{u}\\leq B_{u}\\left(1-\\frac{b_{u,t-1}}{w_{u,t}}\\phi(\\frac{b_{u,t-1}}{B_{u}})-\\alpha_{u,V}\\right)}\\\\ &{\\qquad\\leq B_{u}\\left(1-\\frac{b_{u,t-1}}{w_{u,t}}\\phi(\\frac{b_{u,t-1}}{B_{u}})-\\alpha_{u,t-1}\\right)}\\\\ &{\\qquad=B_{u}\\left(1-\\frac{b_{u,t-1}}{w_{u,t}}\\right)\\left(1-\\varphi(\\frac{b_{u,t-1}}{B_{u}})\\right)}\\\\ &{\\qquad\\leq\\left(B_{u}-b_{u,t-1}\\right)\\left(1-\\varphi(\\frac{b_{u,t-1}}{B_{u}})\\right)}\\\\ &{\\qquad\\leq c_{u,V}\\left(1-\\varphi(1-\\kappa)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the third inequality holds since $w_{u,t}\\le B_{u}$ and the last inequality holds since $B_{u}-b_{u,t-1}=$ $c_{u,t-1}\\leq c_{u,V}$ . Thus, we have $\\begin{array}{r}{\\sum_{u\\in\\mathcal{U}^{\\circ}}B_{u}\\Lambda_{u}\\le P\\cdot(1-\\varphi(1-\\kappa))}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Putting them together, we have $P\\;\\ge\\;\\;{\\textstyle{\\frac{1}{\\gamma}}}(D\\;-\\;\\phi(\\kappa)\\;\\cdot\\;P)$ which leads to the primal dual $\\begin{array}{r l r}{P}&{{}\\ge}&{\\frac{1}{\\gamma+\\phi(\\kappa)}D}\\end{array}$ fowre  anhya $\\begin{array}{r l r}{\\gamma}&{{}\\ge}&{1\\;+\\;\\kappa^{n+1}R\\,+\\,\\frac{\\varphi(y)}{y}\\;-\\;\\frac{1}{y}\\int_{x=0}^{y}\\varphi(x)d x\\;+}\\end{array}$ $\\textstyle{\\frac{1}{y}}\\sum_{i=1}^{n}\\kappa^{i}\\left(\\varphi^{(i-1)}(y)-\\varphi^{(i-1)}(0)\\right)$ $y\\ \\in\\ [0,1]$ 1, we get the competitive ratio in Theorem 4.3. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "C Learning Augmented OBM ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we exploit the competitive solution space in MetaAd and propose to augment MetaAd with ML predictions (called LOBM) to improve the average performance while still offering a guaranteed competitive ratio in the worst case. ", "page_idx": 17}, {"type": "text", "text": "C.1 Algorithm Design ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A main technical challenge for OBM is to estimate the discounting value that discounts the bid values by a factor for the matching decision at round $t$ . Thus, an ML model can be potentially leveraged to replace the manual design of score assignment. More specifically, we can utilize an ML model to predict a discounting factor $z_{u,t}$ and set the score as $s_{u,t}=w_{u,t}(1-z_{u,t})$ for matching when the offilne node $u$ has a sufficient budget for the online arrival $t$ . That is, we incorporate ML predictions into MetaAd (i.e., LOBM) to explore alternative score assignment strategies that can outperform manual designs on average while still offering guaranteed competitiveness. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "In learning-augmented online algorithms [31, 21], there exists an intrinsic trade-off between following ML predictions for average performance improvement and achieving better robustness in the worst case. Such trade-off between average and worst-case performances also exist in learning-augmented OBM ( LOBM). To better control the trade-off, we introduce a slackness parameter $\\lambda\\in[0,1]$ to relax the competitiveness requirement while allowing LOBM to improve the average performance through ML-based scoring within the competitive solution space. ", "page_idx": 18}, {"type": "text", "text": "If we blindly use the ML prediction as the discounting factor for matching, competitive ratio cannot be satisfied due to the lack of worst-case competitiveness for ML predictions. Thus, to ensure that LOBM still offers guaranteed competitiveness, we consider a competitive solution space based on the conditions for dual variables specified in Lemma 1. ", "page_idx": 18}, {"type": "text", "text": "Based on the competitive analysis with the exponential function class, we design a learning-augmented algorithm (i.e., LOBM) in Algorithm 5, which leverages ML prediction $\\tilde{z}_{u,t}$ to improve the average performance while guaranteeing the worst-case competitive ratio. The key idea is to construct dual variables as we solve the primal problem online and utilize the dual variables to calibrate the ML prediction $\\tilde{z}_{u,t}$ . In this way, the matching decisions by LOBM are guaranteed to be competitive in the worst case while utilizing the potential benefits of ML predictions. ", "page_idx": 18}, {"type": "text", "text": "We describe LOBM in Algorithm 5 as follows. At the beginning, we initialize the dual variables as zero. Whenever an online node arrives, the agent receives a ML prediction $\\tilde{z}_{u,t}$ indicating the discounting factors for all the offline nodes $u\\in\\mathcal{U}$ . Instead of directly using the ML prediction to set the scores and selecting the offilne node, LOBM projects the ML prediction $\\tilde{z}_{u,t}$ into the competitive space $\\mathcal{D}_{u,t}$ by solving the following for all $u\\in\\mathcal{U}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nz_{u,t}=\\arg\\operatorname*{min}_{z\\in\\mathcal{D}_{u,t}}\\left|z-\\tilde{z}_{u,t}\\right|,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is a key step to ensure the competitive ratio. In order to better utilize the potential benefit of ML predictions, we use the projection operation in (27) to select the discounting factor $z_{u,t}$ out of competitive space $\\mathcal{D}_{u,t}$ , such that the selected $z_{u,t}$ is the closest to the ML prediction $\\tilde{z}_{u,t}$ . Then, the projected value $z_{u,t}$ is used to set the scores $s_{u,t}$ for offilne nodes with sufficient budgets for the online arrival $t$ , and the scores for offline nodes with insufficient budgets are set as zero and these offline nodes are appended to $\\mathcal{U}^{\\circ}$ . The scores based on calibrated ML predictions are then used to select the offline node for matching. ", "page_idx": 18}, {"type": "text", "text": "As the key design to guarantee the competitive ratio, the competitive space $\\mathcal{D}_{u,t}$ is based on the conditions for dual variables in Lemma 1 and the dual construction by the exponential function class (Corollary 4.2.2). The dual variables are constructed as follows. For the selected note $x_{t}$ and its score $s_{x_{t},t}$ , we update the dual variable $\\alpha_{x_{t},t}$ as $\\begin{array}{r}{\\alpha_{x_{t},t-1}+\\frac{w_{x_{t},t}z_{x_{t},t}}{\\lambda\\rho_{\\theta}B_{x_{t}}}+\\delta_{x_{t},t}}\\end{array}$ , where $\\begin{array}{r}{\\rho_{\\theta}\\,=\\,1\\,-\\,\\frac{1}{e^{\\theta}}}\\end{array}$ with $\\theta>0$ , $z_{x_{t},t}$ is the discounting factor when setting the score of $x_{t}$ (Line 4 of Algorithm 5), and $\\begin{array}{r}{\\delta_{x_{t},t}=\\frac{\\exp(\\theta(1-b_{x_{t},t-1}/B_{x_{t}}))}{e^{\\theta}-1}\\left[\\exp(\\frac{\\theta w_{x_{t},t}}{B_{x_{t}}})-1-\\frac{w_{x_{t},t}}{B_{x_{t}}}\\right]}\\end{array}$ is a variable relying on the bid value $w_{x_{t},t}$ and the remaining budget $b_{x_{t},t-1}$ . For unselected offline nodes, we keep their dual variables $\\alpha_{u,t}$ the same as $\\alpha_{u,t-1}$ for $u\\ne x_{t}$ . The dual variable $\\beta_{t}$ is set based on the score of the selected offline node, i.e. $\\begin{array}{r}{\\beta_{t}=\\frac{1}{\\lambda\\rho_{\\theta}}s_{x_{t},t}=\\frac{1}{\\lambda\\rho_{\\theta}}w_{x_{t},t}(1-z_{x_{t},t})}\\end{array}$ . By constructing dual variables in this way, when an action $x_{t}$ is selected, the primal objective $\\begin{array}{r}{P_{t}=\\sum_{\\tau=1}^{t}w_{x_{\\tau},\\tau}}\\end{array}$ increases by $w_{x_{t},t}$ and the dual objective $\\begin{array}{r}{D_{t}=\\sum_{\\tau=1}^{t}B_{x_{\\tau}}\\alpha_{x_{\\tau},\\tau}+\\beta_{\\tau}}\\end{array}$ increases by $\\begin{array}{r}{B_{x_{t}}(\\alpha_{x_{t},t}-\\alpha_{x_{t-1},t-1})+\\beta_{t}=\\frac{w_{x_{t},t}}{\\lambda\\rho_{\\theta}}+B_{x_{t}}\\delta_{x_{t},t}}\\end{array}$ w\u03bbx\u03c1t,t + Bxt\u03b4xt,t. When an online arrival $t$ is skipped without any matching, both primal and dual objectives remain the same with no updates. Thus, we can always ensure that the primal objective and the dual objective satisfy $\\begin{array}{r}{D_{t}=\\frac{1}{\\lambda\\rho_{\\theta}}P_{t}+\\sum_{\\tau=1}^{t}B_{x_{\\tau},\\tau}\\delta_{x_{\\tau},\\tau}}\\end{array}$ for each $t\\in[T]$ , leading to a bounded ratio of the primal objective to the dual objective at the end of each round. The parameter $\\lambda$ can be used to adjust the bound of primal-dual ratio, leading to different competitive ratios. ", "page_idx": 18}, {"type": "text", "text": "Next, we need to ensure that conditions in Lemma 1 are always satisfied no matter which offline node $u\\in\\mathcal{U}$ is selected at each round $t$ . Thus, we construct the competitive space $\\mathcal{D}_{u,t}$ as below and ", "page_idx": 18}, {"type": "text", "text": "1: Initialization: $\\forall u\\in\\mathcal{U},b_{u,0}=B_{u},\\forall u\\in\\mathcal{U},\\alpha_{u,0}=0,\\beta_{0},\\cdot\\cdot\\cdot,\\beta_{V}=0$ .   \n2: for $\\scriptstyle t=1$ to $V$ , a new request $t\\in\\mathcal{V}$ arrives do   \n3: Get the ML prediction $\\tilde{z}_{u,t},\\forall u\\in\\mathcal{U}$   \n4: Project $\\tilde{z}_{u,t}$ into $\\mathcal{D}_{u,t}$ in (28) and get $\\boldsymbol{z}_{u,t},\\forall u\\in\\boldsymbol{\\mathcal{U}}$ .   \n5: For all $u\\in\\mathcal{U}$ , if $b_{u,t-1}-w_{u,t}\\geq0$ , set score $s_{u,t}\\,=\\,w_{u,t}(1-z_{u,t})$ ; otherwise, set score   \n$s_{u,t}=0$ and append $\\{u\\mid b_{u,t-1}-w_{u,t}<0\\}$ into $\\mathcal{U}^{\\circ}$ .   \n6: if $\\forall u\\in\\mathcal{U}$ , $s_{u,t}=0$ then   \n7: Skip the online arrival $t$ $\\ x_{t}=\\mathrm{null})$ .   \n8: else   \n9: Select xt = arg maxu\u2208U su,t.   \n10: end if   \n11: If $x_{t}\\neq\\mathrm{null}$ , update budget $b_{x_{t},t}=b_{x_{t},t-1}-w_{x_{t},t}$ ; and $\\forall u\\neq x_{t},b_{u,t}=b_{u,t-1}$ .   \n12: Update dual variables $\\begin{array}{r c l}{\\beta_{t}}&{=}&{\\frac{1}{\\lambda\\rho_{\\theta}}s_{x_{t},t}}\\end{array}$ . If $\\begin{array}{r l}{x_{t}}&{{}\\neq}\\end{array}$ null, update $\\begin{array}{r c l}{\\alpha_{x_{t},t}}&{=}&{\\alpha_{x_{t},t-1}\\ +}\\end{array}$   \n$\\frac{1}{\\lambda\\rho_{\\theta}B_{x_{t}}}w_{x_{t},t}z_{x_{t},t}+\\delta_{x_{t},t}.$ .   \n13: end for   \n14: For $u\\notin\\mathcal{U}^{\\circ}$ , set $\\alpha_{u}=\\alpha_{u,V}$ ; and for $u\\in\\mathcal{U}^{\\circ}$ , set $\\alpha_{u}=1$ . ", "page_idx": 19}, {"type": "text", "text": "project the ML predictions $\\tilde{z}_{u,t}$ into $\\mathcal{D}_{u,t}$ if they fall outside $\\mathcal{D}_{u,t}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathscr{D}_{u,t}=\\Bigg\\{z\\geq0\\ \\Big|\\ \\frac{1}{\\lambda\\rho_{\\theta}}w_{u,t}(1-z)\\geq w_{u,t}-w_{u,t}\\alpha_{u,t-1},}\\\\ {\\displaystyle\\alpha_{u,t-1}+\\frac{w_{u,t}}{\\lambda\\rho_{\\theta}B_{u}}z+\\delta_{u,t}\\geq\\frac{\\exp(\\theta(1-(b_{u,t-1}-w_{u,t})/B_{u}))-1}{e^{\\theta}-1}\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the dual variable $\\beta_{t}$ is set as ${\\frac{1}{\\lambda\\rho\\theta}}\\,S_{x_{t},t}$ after selecting the offline node $x_{t}$ with the highest $s_{u,t}$ and sufficient budgets, \u03b2t is no less than\u03bb1\u03c1\u03b8 su,t = \u03bb1\u03c1\u03b8 w $\\begin{array}{r}{\\frac{1}{\\lambda\\rho_{\\theta}}s_{u,t}=\\frac{1}{\\lambda\\rho_{\\theta}}w_{u,t}(1-z_{u,t})}\\end{array}$ for any $u\\in\\mathcal{U}$ . Thus, as long as the first inequality in (28) is satisfied, we always have the dual feasibility $\\beta_{t}\\geq w_{u,t}-w_{u,t}\\alpha_{u,t-1}$ in Lemma 1 if all the offilne nodes have sufficient budgets for the arrival $t$ . For the offilne nodes with insufficient budgets for the online arrival $t$ , we ensure the dual feasibility $\\beta_{t}\\geq w_{u,t}-w_{u,t}\\alpha_{u,t-1}$ by setting their corresponding dual variable $\\alpha_{u}$ as one after the matching process (Line 14 in Algorithm 5). ", "page_idx": 19}, {"type": "text", "text": "The second inequality in (28) sets a target for the increment of the dual variables $\\alpha_{u,t}$ , which forces the dual variable $\\alpha_{u,t}$ to be larger when the remaining budget becomes less. In this way, the score of an offline node $u$ with fewer remaining budgets can be set lower to be conservative in consuming budgets. Also, since $\\alpha_{u,t}$ is larger when the remaining budget is less, the second inequality in (28) guarantees a large enough dual variable $\\alpha_{u,t}$ when $u$ has insufficient budget for an arrival $t$ . This keeps the additional dual increment after the matching process (Line 14 in Algorithm 5) bounded and further guarantees a bounded primal-dual ratio in the second condition of Lemma 1. ", "page_idx": 19}, {"type": "text", "text": "As we discussed, if the discounting factor $z_{u,t}$ at each round satisfies the inequalities in (28), the primal variables and the constructed dual variables will satisfy the conditions in Lemma 1, and so a competitive ratio for OBM is guaranteed. The size of the set $\\mathcal{D}_{u,t}$ is controlled by the hyper-parameter $\\lambda$ : with smaller $\\lambda$ , the size of $\\mathcal{D}_{u,t}$ becomes larger because the inequalities are easier to be satisfied. We will rigorously prove that $\\mathcal{D}_{u,t}$ is always non-empty given any $\\lambda\\in[0,1]$ to enable feasible competitive solutions that guarantee the competitive ratio bound in (29) in the robustness analysis of LOBM in Section C.2. ", "page_idx": 19}, {"type": "text", "text": "ML model training and inference. Given any ML predictions, Algorithm 5 provides a guarantee for the competitive ratio. Nonetheless, the average performance $\\mathbb{E}_{\\boldsymbol{\\mathcal{G}}}[\\bar{P(}\\pi,\\mathcal{G})]$ depends on the ML model that yields the ML prediction. Here, we briefly discuss how to achieve high average performance by training the ML model in an environment that is aware of the design of Algorithm 5. Note first that the projection operation is differentiable while the discrete matching decision is not differentiable. Thus, we apply policy gradient to train the ML model. Once the ML model is trained offilne, it can be applied online to provide $\\tilde{z}_{u,t}$ as advice for scoring and matching by LOBM (Line 3 in Algorithm 5). ", "page_idx": 19}, {"type": "text", "text": "C.2 Analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Now, we provide a robustness analysis of LOBM and formally show that LOBM always guarantees the competitive ratio. ", "page_idx": 20}, {"type": "text", "text": "A learning-augmented algorithm is robust if its competitive ratio is guaranteed for any problem instance given arbitrary ML predictions. We show that LOBM is robust in the sense that it offers competitive guarantees regardless of the quality of ML predictions. ", "page_idx": 20}, {"type": "text", "text": "Theorem C.1. Given the maximum bid-budget ratio $\\kappa\\in[0,1]$ and the slackness parameter $\\lambda\\in[0,1]$ , with any ML predictions, LOBM in Algorithm 5 achieves a competitive ratio of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\eta}(\\kappa)=\\frac{\\lambda(1-\\frac{1}{e^{\\theta}})}{1+\\lambda\\left(\\frac{1-e^{-\\theta\\kappa}}{1-\\kappa}+(1-\\frac{1}{e^{\\theta}})\\frac{1}{\\theta}\\left[\\frac{e^{\\theta\\kappa}}{\\kappa}-\\frac{1}{\\kappa}-1\\right]^{+}\\right)},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $[x]^{+}=x$ if $x>0$ and $[x]^{+}=0\\,i f\\,x\\leq0$ . ", "page_idx": 20}, {"type": "text", "text": "Theorem C.1 shows that LOBM can guarantee a competitiveness ratio of $\\hat{\\eta}(\\kappa)$ regardless of the ML prediction quality for any slackness parameter $\\lambda\\in[0,1]$ . The parameter $\\lambda\\in[0,1]$ determines the requirement for the worst-case competitive ratio and the flexibility to exploit the benefit of ML predictions. When $\\lambda=0$ , there is no competitiveness requirement, the inequalities in the competitive space (28) always hold, and LOBM reduces to a pure ML-based algorithm with no competitive ratio guarantee. On the other hand, when $\\lambda=1$ , LOBM achieves the highest competitive ratio. When $\\lambda$ is flexibly chosen between the competitive solution space also varies from whole solution space (with $\\lambda=0$ ) to the smallest competitive solution space (with $\\lambda=1$ ) in (28). Thus, LOBM achieves a flexible trade-off between the competitive guarantee and average performance by varying the levels of trust in ML predictions. ", "page_idx": 20}, {"type": "text", "text": "C.3 Proof of Theorem C.1 (Theorem 5.1in the main text) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. The sequence of dual variables is constructed by Algorithm 5 We prove three claims leading to Theorem C.1. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The dual feasibility is satisfied,i.e. $\\forall u\\in\\mathcal{U},t\\in[V],w_{u,t}\\alpha_{u}+\\beta_{t}\\geq w_{u,t}.$ .   \n\u2022 The primal-dual ratio is guaranteed, i.e. $P\\ge\\eta D$ .   \n\u2022 The solution of projection (28) always exists, i.e. the feasible set of (28) is not empty for each round. ", "page_idx": 20}, {"type": "text", "text": "First, we prove the feasibility of dual variables (The first condition in Lemma 1). If $\\beta_{t}=0$ for a round $t\\in[T]$ , we have either $w_{u,t}=0$ or $w_{u,t}>0$ and $\\alpha_{u}=\\alpha_{u,V}\\geq1$ holds for a slot $u\\in\\mathcal{U}$ by Line 14, so $\\bar{\\beta}_{t}=0\\ge w_{u,t}(1-\\dot{\\alpha}_{u})$ holds for the dual construction. On the other hand, if $\\beta_{t}>0$ holds for round $t\\in[T]$ , then the score $s_{x_{t},t}$ must be calculated based on the projected $z_{x_{t},t}$ . Thus, we have $\\begin{array}{r}{\\beta_{t}=\\frac{1}{\\lambda\\rho_{\\theta}}s_{x_{t},t}\\geq\\frac{1}{\\lambda\\rho_{\\theta}}s_{u,t}=\\frac{1}{\\lambda\\rho_{\\theta}}w_{u,t}(1-z_{u,t})\\geq w_{u,t}(1-\\alpha_{u,t})\\geq w_{x_{t},t}(1-\\alpha_{u})}\\end{array}$ where $\\rho_{\\theta}\\,=\\,{\\frac{e^{\\theta}-1}{e^{\\theta}}}$ e\u03b8e\u03b8\u22121and the second inequality holds by the first inequality of the set (28). This proves the feasibility of the dual variables ", "page_idx": 20}, {"type": "text", "text": "Next, we prove the primal-dual ratio (the second condition in Lemma 1 ) is satisfied. At round $t$ , if no vertex is selected for a vertex $t$ , the primal objective $P$ and dual objective $D$ do not increase. rweahseeres $w_{x_{t},t}$ $B_{x_{t}}(\\alpha_{x_{t},t}\\textrm{--}$ $\\begin{array}{r}{\\alpha_{x_{t-1},t-1})+\\beta_{t}=\\frac{w_{x_{t},t}}{\\lambda\\rho_{\\theta}}+B_{x_{t}}\\delta_{x_{t},t}}\\end{array}$ $\\begin{array}{r}{\\delta_{x_{t},t}=\\frac{\\exp(\\theta(1-b_{x_{t},t-1}/B_{x_{t}}))}{e^{\\theta}-1}(\\exp(\\frac{\\theta w_{x_{t},t}}{B_{x_{t}}})-1-\\frac{\\ w_{x_{t},t}}{B_{x_{t}}})}\\end{array}$ By Line 14 at the end of Algorithm 5, the dual variable $\\alpha_{u,V}$ increases to $\\alpha_{u}$ by $\\Lambda_{u}=\\alpha_{u}-\\alpha_{u,V}$ . Thus, the dual objective can be written as $\\begin{array}{r}{D=\\frac{1}{\\lambda\\rho_{\\theta}}\\sum_{t=1}^{V}w_{x_{t},t}+\\sum_{t=1}^{V}B_{x_{t}}\\delta_{x_{t},t}+\\sum_{u\\in\\mathcal{U}}B_{u}\\Lambda_{u},}\\end{array}$ and further we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nP=\\sum_{t=1}^{V}w_{x_{t},t}\\geq\\lambda(1-\\frac{1}{e^{\\theta}})\\left(D-\\sum_{u\\in\\mathcal{U}^{\\circ}}B_{u}\\Lambda_{u}-\\sum_{t=1}^{V}B_{x_{t}}\\delta_{x_{t},t}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To bound the right-hand-side, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{u\\in\\mathcal{U}^{0}}B_{u}\\Lambda_{u}\\leq\\frac{e^{\\theta}(1-e^{-\\theta\\kappa})}{e^{\\theta}-1}\\sum_{u\\in\\mathcal{U}^{\\circ}}B_{u}\\leq\\frac{e^{\\theta}(1-e^{-\\theta\\kappa})P}{e^{\\theta}-1}\\frac{\\sum_{u\\in\\mathcal{U}^{\\circ}}B_{u}}{P}}\\\\ &{\\qquad\\qquad\\leq\\frac{e^{\\theta}(1-e^{-\\theta\\kappa})P}{e^{\\theta}-1}\\frac{\\sum_{u\\in\\mathcal{U}^{\\circ}}B_{u}}{\\sum_{u\\in\\mathcal{U}^{\\circ}}(1-\\kappa)B_{u}}=\\frac{e^{\\theta}}{e^{\\theta}-1}\\frac{1-e^{-\\theta\\kappa}}{(1-\\kappa)}\\cdot P,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For each $u\\in\\mathcal{U}$ , we sum up $B_{u}\\delta_{u,t}$ and get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1,\\tau_{i}=u}^{V}B_{x_{i}}\\delta_{x_{i},t}=}&{\\displaystyle\\sum_{t=1,\\tau_{i}=u}^{V}\\frac{\\exp(\\theta(1-b_{u,t-1}/B_{u}))}{\\epsilon^{\\theta}-1}(B_{u}\\exp(\\frac{\\theta w_{u,t}}{B_{u}})-B_{u}-w_{u,t})}\\\\ &{=\\displaystyle\\sum_{t=1,\\tau_{i}=u}^{V}\\frac{\\exp(\\theta(1-b_{u,t-1}/B_{u}))}{\\epsilon^{\\theta}-1}\\cdot w_{u,t},\\Big\\langle\\frac{B_{u}}{w_{u,t}}\\exp(\\frac{\\theta w_{u,t}}{B_{u}})-\\frac{B_{u}}{w_{u,t}}-1\\Big\\rangle}\\\\ &{\\leq B_{u}\\displaystyle\\sum_{t=1,\\tau_{i}=u}^{V}\\frac{\\exp(\\theta(1-b_{u,t-1}/B_{u}))}{\\epsilon^{\\theta}-1}\\cdot\\frac{w_{u,t}}{B_{u}}\\cdot\\Big[\\frac{\\epsilon^{\\theta_{k}}}{\\kappa}-\\frac{1}{\\kappa}-1\\Big]^{+}}\\\\ &{\\leq B_{u}\\frac{\\exp(\\theta^{\\top}\\!_{B_{u}})-1}{\\theta(\\epsilon^{\\theta}-1)}\\cdot\\Big[\\frac{\\epsilon^{\\theta_{k}}}{\\kappa}-\\frac{1}{\\kappa}-1\\Big]^{+}}\\\\ &{\\leq c_{u,V}\\cdot\\frac{1}{\\theta}\\cdot\\Big[\\frac{\\epsilon^{\\theta_{k}}}{\\kappa}-\\frac{1}{\\kappa}-1\\Big]^{+},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality holds because $\\frac{e^{\\theta x}}{x}~-~\\frac{1}{x}~-~1$ is an increasing function for $x\\in$ $[0,1],\\theta~\\in~[0,1]$ , the second inequality holds since $\\begin{array}{r}{\\sum_{t=1,x_{t}=u}^{V}\\exp(\\theta(1\\mathrm{~-~}\\frac{b_{u,t-1}}{B_{u}}))}\\end{array}$ $\\frac{w_{u,t}}{B_{u}}\\;\\;=\\;\\;$ $\\begin{array}{r}{\\sum_{t=1,x_{t}=u}^{V}\\exp(\\theta\\frac{c_{u,t}}{B_{u}})\\cdot\\frac{w_{u,t}}{B_{u}}\\,\\leq\\,\\int_{x=0}^{\\frac{c_{u,V}}{B_{u}}}\\exp(\\theta x){\\mathrm{d}}x\\,=\\,\\frac{1}{\\theta}(\\exp(\\theta\\frac{c_{u,V}}{B_{u}})-1)}\\end{array}$ and the last inequality holds because $\\begin{array}{r}{B_{u}\\frac{\\exp(\\theta\\frac{c_{u,V}}{B_{u}})-1}{\\theta(e^{\\theta}-1)}=c_{u,V}\\cdot\\frac{\\exp(\\theta\\frac{c_{u,V}}{B_{u}})-1}{\\theta(e^{\\theta}-1)\\frac{c_{u,V}}{B_{u}}}\\leq c_{u,V}\\cdot\\frac{1}{\\theta}}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "By summing up all bidders in $\\boldsymbol{\\mathcal{U}}$ , we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{V}B_{x_{t}}\\delta_{t}=\\sum_{u\\in\\mathcal{U}}\\sum_{t=1,x_{t}=u}^{V}B_{x_{t}}\\delta_{x_{t},t}\\leq P\\cdot\\frac{1}{\\theta}\\cdot\\left[\\frac{e^{\\theta\\kappa}}{\\kappa}-\\frac{1}{\\kappa}-1\\right]^{+}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Continuing with inequality (30), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nP\\geq\\lambda(1-\\frac{1}{e^{\\theta}})\\left(D-\\frac{e^{\\theta}}{e^{\\theta}-1}\\frac{1-e^{-\\theta\\kappa}}{(1-\\kappa)}\\cdot P-\\frac{1}{\\theta}\\cdot\\left[\\frac{e^{\\theta\\kappa}}{\\kappa}-\\frac{1}{\\kappa}-1\\right]^{+}\\cdot P\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, by moving terms, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nP\\geq\\frac{\\lambda(1-\\frac{1}{e^{\\theta}})D}{1+\\lambda\\left(\\frac{1-e^{-\\theta\\kappa}}{1-\\kappa}+(1-\\frac{1}{e^{\\theta}})\\frac{1}{\\theta}\\left[\\frac{e^{\\theta\\kappa}}{\\kappa}-\\frac{1}{\\kappa}-1\\right]^{+}\\right)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This proves the second condition in Theorem 1. ", "page_idx": 21}, {"type": "text", "text": "Finally, we prove that the solution of the projection into (28) always exists, i.e. the feasible set of (28) is not empty for each round. To do this, we prove by induction that ", "page_idx": 21}, {"type": "equation", "text": "$$\nz_{u,t}^{\\dagger}=\\frac{\\lambda_{1}\\rho_{\\theta}\\exp(\\theta(1-b_{u,t-1}/B_{u}))}{(e^{\\theta}-1)},\\forall\\lambda_{1}\\in[\\lambda,1]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "is always feasible for the set (28). For the first round, the initialized dual variable $\\alpha_{u,0}\\,=\\,0$ , the initialized remaining budget $b_{u,0}=B_{u}$ , so we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\alpha_{u,0}+\\frac{w_{u,t}}{\\lambda\\rho_{\\theta}B_{u}}z_{u,1}^{\\dag}+\\delta_{u,1}\\geq\\frac{\\exp(\\theta(\\frac{w_{u,t}}{B_{u}}))-1}{e^{\\theta}-1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "image", "img_path": "Vtxy8wFpTj/tmp/fc18823668d667e5143435e51406f970326677e3c501828fd78b14de0336a181.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 3: Illustration of the scoring strategies in MetaAd and LOBM. The example has 3 offilne nodes $(u_{1},u_{2},u_{3})$ . The algorithms select the offline node with the largest score. ", "page_idx": 22}, {"type": "text", "text": "where the inequality holds because $\\lambda_{1}\\geq\\lambda$ , so the second inequality of (28) holds for $t=1$ . Also, it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{\\lambda\\rho_{\\theta}}w_{u,1}\\left(1-z_{u,1}^{\\dagger}\\right)=w_{u,1}\\left(\\frac{e^{\\theta}}{\\lambda(e^{\\theta}-1)}-\\frac{\\lambda_{1}}{\\lambda(e^{\\theta}-1)}\\right)\\geq\\frac{w_{u,1}}{\\lambda}\\geq w_{u,1}=w_{u,1}(1-\\alpha_{u,0}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality holds since $\\lambda_{1}\\leq1$ and the second inequality holds since $\\lambda\\leq1$ . Thus, we can prove the first inequality of (28) holds for initialization. ", "page_idx": 22}, {"type": "text", "text": "Then if the constraints are satisfied for the round before arrival $t$ , then we have $\\alpha_{u,t-1}~\\geq$ $\\frac{\\exp(\\theta(1\\!-\\!b_{u,t-1}/B_{u}))\\!-\\!1}{e^{\\theta}\\!-\\!1}$ , and thus we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\alpha_{u,t-1}+\\frac{w_{u,t}}{\\lambda\\rho_{\\theta}B_{u}}z_{u,t}^{\\dagger}+\\delta_{u,t}}\\\\ &{\\geq\\!\\frac{\\exp(\\theta(1-b_{u,t-1})/B_{u})}{e^{\\theta}-1}\\exp(\\frac{\\theta w_{u,t}}{B_{u}})=\\frac{\\exp(\\theta(1-(b_{u,t-1}-w_{u,t})/B_{u})}{e^{\\theta}-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the inequality holds because $\\lambda_{1}\\geq\\lambda$ . Thus the second inequality of (28) holds. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{\\lambda\\rho_{\\theta}}w_{u,t}\\left(1-z_{u,t}^{\\dagger}\\right)}\\\\ &{=\\!w_{u,t}\\left(\\frac{e^{\\theta}}{\\lambda(e^{\\theta}-1)}-\\frac{\\lambda_{1}\\exp(\\theta(1-\\frac{b_{u,t-1}}{B_{u}}))}{\\lambda(e^{\\theta}-1)}\\right)\\geq w_{u,t}(1-\\alpha_{u,t-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the inequality holds since $\\lambda\\leq1$ and $\\lambda_{1}\\leq1$ . Thus we prove the first inequality of (28) holds.   \nIn conclusion, we can always find a feasible dual variable update $z_{u,t}^{\\dagger}$ in the projection set (28). ", "page_idx": 22}, {"type": "text", "text": "D Empirical Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To complement the theoretical analysis, we validate the empirical beneftis of proposed algorithms by conducting numerical experiments for an online movie matching application. ", "page_idx": 22}, {"type": "text", "text": "D.1 Online Movie Matching ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We evaluate the performances of our algorithms on the online movie matching application based on the MovieLens Dataset [12]. ", "page_idx": 22}, {"type": "text", "text": "D.1.1 Setup ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In the application of online movie matching, each movie (i.e., an offilne node) has a maximum budget set by advertisers. Once an online query arrives, the bid values of the query for all the movies are revealed to the matching platform agent. The platform agent needs to match a movie to each query, generating a reward equivalent to the bid value and consuming a budget of the bid value from the total budget of the matched movie. The bid value is determined by the relevance of the movie and the query. For example, if a movie is more relevant to the online query, there is a potentially higher value. The goal of the advertising platform is to maximize the total reward while satisfying the budget constraints of each movie. In this application, the platform agent does not allow a fractional fee for any matching. Thus, this matching problem is a OBM without FLM. ", "page_idx": 22}, {"type": "table", "img_path": "Vtxy8wFpTj/tmp/8933829317ca7b51482fb28ed3eb0a981c86f6902ec3b064ff2662c49259c2c2.jpg", "table_caption": ["Table 2: Worst-case and and average normalized reward on the MovieLens dataset. The best results among algorithms w/o ML predictions and the best results among ML-based algorithms are highlighted in bold font. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "We run the online movie matching application based on a real dataset of MovieLens [12]. The MovieLens dataset provides data on the relevance of movies and users. We generate bipartite graphs, each with $U=10$ offline nodes (movies) and $V=100$ online nodes (queries/users) based on the MovieLens dataset. For each graph instance, we sample 10 movies uniformly without replacement and 100 users uniformly with replacement. A bid value scaled based on relevance is assigned to each edge between the offline node and the online node. The total budget for each offline node is sampled from a normal distribution with a mean of 1 and a standard deviation of 0.1, and the maximum bid value is 0.1 (i.e., $\\kappa=0.1$ ). We generate 10k, 1k, and 1k samples of graph instances based on the MovieLens dataset for training, validation and testing, respectively. To test the ML performance with out-of-distribution/adversarial examples, we also create examples by modifying $10\\%$ examples in the testing dataset and randomly removing edges and/or rescaling the weights. ", "page_idx": 23}, {"type": "text", "text": "We compare our algorithms with the most common baselines for OBM as listed below. ", "page_idx": 23}, {"type": "text", "text": "\u2022 OPT: The offline optimal solution is obtained using Gurobi [11] for each graph instance.   \n\u2022 Greedy: The greedy algorithm [23] matches an online node to the available offline node that is connected to the node and has the highest bid value. Greedy has a strong empirical performance and is a special case of MetaAd with $\\theta\\rightarrow\\infty$ .   \n\u2022 PrimalDual: PrimalDual [23] calculates the scores of each bidder for each online node based on both the bid values and the remaining budgets, and then selects for an online node the available bidder with the highest score. It is a special case of MetaAd with $\\theta\\rightarrow1$ .   \n\u2022 ML: A policy-gradient algorithm that solves the OBM problem [1]. The inputs to the policy model are the available history information including the current bid value, the remaining budget of each offline node and the average matched bid value. ", "page_idx": 23}, {"type": "text", "text": "We evaluate the performances of MetaAd with the discounting function $\\begin{array}{r}{\\varphi(x)=\\frac{e^{\\theta x}-1}{e^{\\theta}-1}}\\end{array}$ ee\u03b8\u2212\u221211 and LOBM in Algorithm 5. The illustration of MetaAd for round $t$ is shown in Figure 3(a). LOBM- $\\cdot\\lambda$ is LOBM with the slackness parameter $\\lambda$ in the competitive solution space (28). The illustration of LOBM for round $t$ is given in Figure 3(b). The The optimal parameter $\\theta$ governing the level of conservativeness in MetaAd is tuned based on the validation dataset. We also evaluate the performance of MetaAd under different choices of $\\theta$ , and evaluate LOBM with ML predictions under different choices of the hyper-parameter $\\lambda\\in[0,1]$ and use LOBM- $\\lambda$ to represent LOBM with the hyper-parameter $\\lambda$ in $\\mathcal{D}_{u,t}$ in (28). For a fair comparison, we use the same neural architecture as ML in LOBM. The neural network has two layers, each with 200 hidden neurons. The neural networks are trained by Adam optimizer with a learning rate of $10^{-3}$ for 50 epochs. The training process on a laptop takes around 1 hour, while the inference process over each instance takes less than one second. ", "page_idx": 23}, {"type": "text", "text": "D.1.2 Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The empirical worst-case and average reward (normalized by the optimal reward) based on the MovieLens dataset are shown in Table 2. In this table, the parameter $\\theta$ of MetaAd is 0.7 by default which is obtained by tuning on a validation dataset. We find that MetaAd can achieve a higher worst-case reward ratio than alternative competitive algorithms without predictions (i.e., Greedy and PrimalDual). Through training, ML can achieve a higher average reward than competitive algorithms without predictions. However, due to the existence of out-of-distribution testing examples, ML has a lower worst-case reward ratio than competitive algorithms that have theoretical worst-case performance guarantees. LOBM can significantly improve the worst-case performance of ML. This is because the projection of ML predictions onto the competitive solution space in 28 corrects low-quality ML predictions. Interestingly, LOBM with $\\lambda=0.8$ achieves the best empirical worstcase and average performance, demonstrating the superiority of LOBM despite that its competitive ratio is lower than that of MetaAd. The high average performance of LOBM shows that LOBM can effectively utilize the benefits of good ML predictions to improve the average performance while offering guaranteed competitiveness. Importantly, when $\\lambda\\in[0,1]$ decreases, the requirements for the worst-case performance are more relaxed, and hence LOBM achieves a higher average reward but a lower worst-case reward. ", "page_idx": 23}, {"type": "image", "img_path": "Vtxy8wFpTj/tmp/ae69945feb40ff987afbaf7ca0c9eba2ead4e672ae2befd3f189be3cec2d5430.jpg", "img_caption": ["(a) Worst-case/average reward of (b) Worst-case reward of LOBM (c) Average reward of LOBM MetaAd ", "Figure 4: (a) Worst-case and average reward of MetaAd with different choices of $\\theta$ . (b) Worst-case reward of LOBM with different choices of $\\theta$ and $\\lambda$ . (c)Average reward of LOBM with different choices of $\\theta$ and $\\lambda$ . "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "The effects of $\\theta$ in MetaAd. To validate the effects of the hyper-parameter $\\theta$ on the performance of MetaAd with $\\begin{array}{r}{\\varphi(x)=\\frac{e^{\\theta x}-1}{e^{\\theta}-1}}\\end{array}$ = ee\u03b8\u2212\u221211 , we give more details of the performances of MetaAd under different choices of $\\theta$ in Fig. 4(a). We give both the empirical worst-case and average reward of MetaAd with different choices of $\\theta$ . The results show that the average reward of MetaAd is not significantly affected by the choice of $\\theta$ , but $\\theta$ has a large effect on the empirical worst-case reward. This is because $\\theta$ controls the conservativeness of MetaAd and hence is crucial for the worst-case competitive ratio when $\\kappa\\neq0$ as discussed in Section 4.2. More specifically, a larger worst-case reward can be obtained with a smaller $\\theta$ for the MovieLens dataset. The reason is that a higher level of conservativeness is needed when the maximum bid-budget ratio $\\kappa$ is not zero. ", "page_idx": 24}, {"type": "text", "text": "The effects of $\\theta$ and $\\lambda$ in LOBM. The empirical worst-case and average rewards of LOBM with different choices of $\\theta$ and $\\lambda$ are provided in Fig. 4(b) and Fig. 4(c), respectively. Different choices of $\\theta$ yield different competitive solution spaces, while the choices of $\\lambda$ specify the relaxed robustness requirements of the worst-case competitive ratio for LOBM. Thus, we can get a different competitive ratio for LOBM by setting different $\\theta$ and $\\lambda$ as shown in Theorem C.1. These theoretical findings are validated by our numerical results. ", "page_idx": 24}, {"type": "text", "text": "As we can see from Fig. 4(b) and Fig 4(c), when $\\lambda=$ 0, the inequalities in the robust region (28) always hold, and hence LOBM reduces to pure ML and gives the same competitive ratio and average reward as ML. When $\\lambda=1$ , LOBM guarantees the same competitive ratio as MetaAd, but does not necessarily always follow the solutions of MetaAd for each problem instance, since there exist other solutions that also satisfy the robustness requirement for certain problem instances. Therefore, when $\\lambda=1$ , the competitive ratio and average reward of LOBM are close to but can be higher than those of MetaAd when the ML model used by LOBM is well trained. When $\\lambda$ lies between 0 and 1, we can find that for some choices of $\\theta$ , LOBM can achieve an even better average reward than ML. This improvement comes from the fact that the competitive solution space in (28) can correct some low-quality ML predictions on certain problem instances. Also, for certain choices of $\\theta$ , LOBM can empirically achieve a better worst-case reward than MetaAd, because LOBM can perform well due to ML predictions on some problem instances where MetaAd does not perform well. This observation validates that LOBM can effectively utilize the ML predictions to improve the average performance while guaranteeing a worst-case competitive ratio. ", "page_idx": 24}, {"type": "image", "img_path": "Vtxy8wFpTj/tmp/901b59e3761889e6e4f46221d0461ce090d3253e944d2fec0cca33cd6b078da1.jpg", "img_caption": ["Figure 5: Reward (normalized by the offilne optimal reward) at high percentiles $(95\\%\\textrm{-}100\\%)$ . $\\theta$ is chosen as 1. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Tail reward performance. Last but not least, to evaluate the performance on adversarial/out-ofdistribution instances, we show in Fig. 5 the reward (normalized by the offline optimal reward) at high percentiles from $95\\%$ to $100\\%$ . We observe that the reward of ML quickly decreases when the percentile becomes higher and becomes the lowest at the high percentiles (larger than $98\\%$ ), showing that ML is vulnerable to adversarial instances. Due to the worst-case competitiveness guarantees, MetaAd achieves a relatively higher reward even at high percentiles. Moreover, since LOBM guarantees the worst-case performance by the competitive solution space, the rewards of LOBM with different $\\lambda$ are all higher than ML at high percentiles. The high percentile reward of LOBM increases with $\\lambda$ because a larger choice of $\\lambda$ guarantees a higher competitive ratio according to Theorem C.1. Interestingly, we can find that the rewards of LOBM at high percentiles are even larger than MetaAd when $\\lambda$ is 0.6 or 0.8. This validates that when the ML model is well trained to provide high-quality predictions, LOBM can become more powerful and explore better matching decisions than the purely manual design of MetaAd. ", "page_idx": 25}, {"type": "text", "text": "D.2 Online VM Placement ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.2.1 Problem setting ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Virtual Machine (VM) placement is the process of matching the newly-created VMs to the most suitable servers in cloud data centers [13, 22, 26]. In this problem, once an end user send a VM request, the cloud operator needs to select a physical server for it. Different VM requests require different amount of physical computing resources. For example, the compute-optimized instances of Amazon EC2 [2] have different sizes, each requires different amount of computing resources. Due to the hardware heterogeneity [27], the available computing resources on different servers are different and the utilities of different servers can also be different. Our goal is to optimize the total utility of VM placement. ", "page_idx": 25}, {"type": "text", "text": "We consider a setup where the cloud manager allocates $V$ VMs (online nodes) to $U$ different physical servers (offilne nodes). Based on the requirement of VMs, a VM request can be matched to a subset of the physical servers. The connections between VM requests and physical servers are represented by a bipartite graph $G$ . A VM request $t$ at round $t,t\\le V$ has a computing load in the number of computing units denoted as $z_{t}$ . Each server $u\\in\\mathcal{U}$ has a limited capacity of the computing units (e.g., virtual cores) denoted as $B_{u}^{\\prime}$ . If the VM request $v$ is placed on a server $u$ , the manager receives a utility proportional to the computing load $w_{u,t}=r_{u}\\cdot z_{t}$ where $r_{u}$ is the utility of one computing unit on server $u$ . Denoting $x_{u,t}\\in\\{0,1\\}$ as the decision on whether to place request $t$ on server $u$ , the objective of the VM placement problem can be formulated as an OBM: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{max}{P:=\\sum_{t=1}^{V}\\sum_{u\\in\\mathcal{U}}w_{u,t}x_{u,t}}}\\\\ {\\mathrm{~s.t.~}\\,\\forall u\\in\\mathcal{U},\\displaystyle\\sum_{t=1}^{V}w_{u,t}x_{u,t}\\leq B_{u},\\forall t\\in[V],\\displaystyle\\sum_{u\\in\\mathcal{U}}x_{u,t}\\leq1,\\forall u\\in\\mathcal{U},v\\in[V],x_{u,t}\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $w_{u,t}=r_{u}\\cdot z_{t}$ and $\\boldsymbol{B}_{u}=\\boldsymbol{r}_{u}\\cdot\\boldsymbol{B}_{u}^{\\prime}$ . In this OBM, the VM request is not divisible, which means fractional matching is not allowed at any time and FLM does not apply. ", "page_idx": 25}, {"type": "text", "text": "D.2.2 Experiment setting ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In the experiment, the cloud manager allocates $V=100$ VMs (online nodes) to $U=10$ different physical servers (offline nodes). We randomly generate graphs by Barab\u00e1si\u2013Albert method [3]. For an online node $v$ , we sample its degree (the number of offline nodes connected to it) by a Binomial distribution $B(U,d_{v}/U)$ where $d_{v}$ is the average degree of node $v$ . The average degrees of online nodes are chosen from 4, 2, and 0.5. Each server has a capacity on the number of the computing units. The capacity $B_{u}^{\\prime}$ is sampled from a uniform distribution on the range [20, 40]. The computing load of a VM request is sampled from a uniform distribution on the range [1, 4]. The utility per computing unit $r_{u}$ is the price of a computing unit on the server $u$ . We choose the price (in dollars) in the range [0.08, 0.12] according to the prices of the compute-optimized instances on Amazon EC2 [2]. We randomly generate 20k, 1k, and 1k samples of BA graphs for training, validation and testing, respectively. ", "page_idx": 25}, {"type": "table", "img_path": "Vtxy8wFpTj/tmp/5386c94d21b8c994d92c1702d6ba201a4e394f9f59665d08751d47061669ff28.jpg", "table_caption": [], "table_footnote": ["Table 3: Worst-case and average rewards of different algorithms for VM placement. The worst-case and average rewards are normalized by optimal rewards. We compare MetaAd with the algorithms without using ML (Greedy and PrimalDual introduced in Section D.1.1). Additionally, we compare our learning-augmented algorithm LOBM with the ML algorithm. LOBM- $\\lambda$ means LOBM with a slackness parameter $\\lambda$ in Eqn. (28). "], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "We compare our algorithms with baselines for OBM listed below. We compare our algorithms with the most common baselines for OBM as listed below. ", "page_idx": 26}, {"type": "text", "text": "\u2022 OPT: The offline optimal solution is obtained using Gurobi [11] for each graph instance.   \n\u2022 Greedy: The greedy algorithm [23] matches an online node to the available offline node that is connected to the node and has the highest bid value. Greedy has a strong empirical performance and is a special case of MetaAd with $\\theta\\rightarrow\\infty$ .   \n\u2022 PrimalDual: PrimalDual [23] calculates the scores of each bidder for each online node based on both the bid values and the remaining budgets, and then selects for an online node the available bidder with the highest score. It is a special case of MetaAd with $\\theta\\rightarrow1$ .   \n\u2022 ML: A policy-gradient algorithm that solves the OBM problem [1]. The inputs to the policy model are the available history information including the current bid value, the remaining budget of each offline node and the average matched bid value. ", "page_idx": 26}, {"type": "text", "text": "For learning-based algorithms, we use the neural networks which have two layers, each with 200 hidden neurons for fair comparison. The neural networks are trained by Adam optimizer with a learning rate of $10^{-3}$ for 50 epochs. Likewise, we use LOBM- $\\lambda$ to refer to LOBM with a hyper-parameter $\\lambda$ governing the competitiveness requirement in (28). ", "page_idx": 26}, {"type": "text", "text": "D.2.3 Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We first show Worst-case and average rewards of different algorithms for VM placement in Table 3. We observe that MetaAd achieves a higher worst-case and average reward than the the other algorithms without using ML (i.e., Greedy and PrimalDual). This is because MetaAd is more flexible to adjust the discounting function. Additionally, we can find that ML predictions can significantly improve the average performance compared to algorithms without ML predictions. In particular, ML even has an empirically higher worst-case reward than Greedy and PrimalDual, although it does not have a theoretical guarantee in terms of the worst-case competitive ratio. Note also that the worstcase reward ratio on the finite testing dataset is an empirical evaluation, and the true competitive ratio of ML without the theoretical guarantee can be even much lower than presented in the table. Importantly, we can find that LOBM can achieve a high average performance while guaranteeing a worst-case competitive ratio theoretically as shown in Theorem 5.1. Moreover, LOBM achieves the highest empirical worst-case reward among all the algorithms, because LOBM can effectively correct low-quality ML predictions for some difficult testing examples by learning augmented design and meanwhile also leverage good ML predictions to improve the performance for other testing examples. ", "page_idx": 26}, {"type": "text", "text": "Effects of $\\theta$ and $\\lambda.$ . Next, we give the ablation study of MetaAd and LOBM for different choices of parameters $\\theta$ and $\\lambda$ in Fig. 6. The parameter $\\theta$ is the constant in the exponential discounting function and the parameter $\\lambda$ is the slackness parameter in the competitive space in Eqn. (28). First, we give the worst-case and average rewards of MetaAd under different choices of $\\theta$ in Fig. 6(a). We can find that compared with PrimalDual (i.e., $\\theta=1$ ), MetaAd can further improve the worst-case performance for the general bid settings by decreasing $\\theta$ which is consistent with the competitive analysis in Corollary4.2.2. ", "page_idx": 26}, {"type": "text", "text": "Moreover, we provide the worst-case and average rewards for LOBM under different $\\theta$ and $\\lambda$ in Fig. 6(b) and Fig. 6(c), respectively. The results show that when $\\lambda=0$ , LOBM reduces to pure ML and achieves the same worst-case and average performances as $\\tt M L$ . The worst-case performance can be improved by increasing $\\lambda$ since LOBM with a larger $\\lambda$ has a higher competitive ratio guarantee according to Theorem C.1. However, the average performance can be affected when $\\lambda$ becomes larger, because a larger $\\lambda$ results in a smaller solution space for increased robustness and hence may exclude some solutions with high rewards for average cases. When $\\lambda=1$ , LOBM shares the same theoretical competitive ratio bound as MetaAd, but it can still achieve better empirical worst-case and average rewards than MetaAd when the ML model is well trained. ", "page_idx": 26}, {"type": "image", "img_path": "Vtxy8wFpTj/tmp/af14558b38c1217466c34e5056aa851abe71fa788f367b5271beb39136e122e0.jpg", "img_caption": ["(a) MetaAd with exponential func- (b) Worst-case reward of LOBM (c) Average reward of LOBM tion class ", "Figure 6: (a) Worst-case and average rewards of MetaAd with the exponential function class (a) Worst-case reward of LOBM. (b)Average reward of LOBM. The worst-case and average rewards are normalized by optimal rewards and are calculated empirically based on a testing dataset with 1000 samples. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 28}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 28}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 28}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 28}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 28}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 28}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The abstract and introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The limitations are discussed in the conclusion section which are mainly about the open question about the best choice of the discounting function. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Our paper summarizes the assumptions listed in Section 3. The proofs of our theorems are given in the appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper has provided detailed settings of the experiments in Section D Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We will open source the codes upon the publication of the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 30}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The training and test details are summarized in Section D. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper reports the high percentile performance in Figure D.1.2. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The experiments can be reproduced by a personal computer with CPU. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not foresee negative societal impacts or the need of safeguards due to the theoretical nature of our work. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We didn\u2019t foresee the need of safeguards due to the theoretical nature of our work. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All original sources of the datasets used in the paper are cited. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]