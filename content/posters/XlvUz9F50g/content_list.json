[{"type": "text", "text": "A Simple and Adaptive Learning Rate for FTRL in Online Learning with Minimax Regret of $\\Theta(T^{2/3})$ and its Application to Best-of-Both-Worlds ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Taira Tsuchiya The University of Tokyo and RIKEN tsuchiya@mist.i.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Shinji Ito The University of Tokyo and RIKEN shinji@mist.i.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Follow-the-Regularized-Leader (FIRL) is a powerful framework for various online learning problems. By designing its regularizer and learning rate to be adaptive to past observations, FTRL is known to work adaptively to various properties of an underlying environment. However, most existing adaptive learning rates are for online learning problems with a minimax regret of $\\Theta({\\sqrt{T}})$ for the number of rounds $T$ , and there are only a few studies on adaptive learning rates for problems with a minimax regret of $\\dot{\\Theta}(T^{2/3})$ , which include several important problems dealing with indirect feedback. To address this limitation, we establish a new adaptive learning rate framework for problems with a minimax regret of $\\Theta(T^{2/3})$ . Our learning rate is designed by matching the stability, penalty, and bias terms that naturally appear in regret upper bounds for problems with a minimax regret of $\\Theta(T^{2/3})$ As applications of this framework, we consider three major problems with a minimax regret of $\\Theta(T^{2/3})$ : partial monitoring, graph bandits, and multi-armed bandits with paid observations. We show that FTRL with our learning rate and the Tsallis entropy regularizer improves existing Best-of-Both-Worlds (BOBW) regret upper bounds, which achieve simultaneous optimality in the stochastic and adversarial regimes. The resulting learning rate is surprisingly simple compared to the existing learning rates for BOBW algorithms for problems with a minimax regret Oof $\\Theta(\\Bar{T}^{2/3})$ ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Online learning is a problem setting in which a learner interacts with an environment for $T$ rounds with the goal of minimizing their cumulative loss. This framework includes many important online decision-making problems, such as expert problems [21, 38, 57], multi-armed bandits [6, 8, 33], linear bandits [1, 14], graph bandits [4, 42], and partial monitoring [9, 11]. ", "page_idx": 0}, {"type": "text", "text": "For the sake of discussion in a general form, we consider the following general online learning framework. In this framework, a learner is initially given a finite action set ${\\mathcal{A}}=[k]:=\\{1,\\ldots,k\\}$ and an observation set $\\scriptscriptstyle\\mathcal{O}$ .At each round $t\\in[T]$ , the environment determines a loss function $\\ell_{t}\\colon A\\to$ [0, 1], and the learner selects an action $A_{t}\\in\\mathcal A$ based on past observations without knowing $\\ell_{t}$ . The learner then suffers a loss $\\boldsymbol{\\ell}_{t}(\\boldsymbol{A}_{t})$ and observes a feedback $o_{t}~\\in~\\mathcal{O}$ . The goal of the learner is to minimize the (pseudo-)regret $\\mathsf{R e g}_{T}$ , which is defined as the expectation of the difference between the cumulativeloss of the selected actions $(A_{t})_{t=1}^{T}$ and that of an optimal action $a^{*}\\in\\mathcal{A}$ fixed in hindsight. That is, $\\begin{array}{r}{{\\mathsf{R e g}}_{T}=\\mathbb{E}\\!\\left[\\sum_{t=1}^{T}\\ell_{t}(A_{t})-\\dot{\\sum_{t=1}^{T}\\ell_{t}}(a^{*})\\right]}\\end{array}$ for $\\begin{array}{r}{a^{*}\\in\\arg\\operatorname*{min}_{a\\in A}\\mathbb{E}\\!\\left[\\sum_{t=1}^{T}\\ell_{t}(a)\\right]}\\end{array}$ \uff0c For example in the multi-armed bandit problem, the observation is $o_{t}=\\ell_{t}(A_{t})$ ", "page_idx": 0}, {"type": "text", "text": "Follow-the-Regularized-Leader (FTRL) is a highly powerful framework for such online learning problems. In FTRL, a probability vector $q_{t}$ over $\\boldsymbol{\\mathcal{A}}$ , which is used for determining action selection probability $p_{t}$ so that $A_{t}\\sim p_{t}$ , is obtained by solving the following convex optimization problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\nq_{t}\\in\\arg\\operatorname*{min}_{q\\in\\mathcal{P}_{k}}\\Biggl\\{\\sum_{s=1}^{t-1}\\widehat{\\ell}_{s}(q)+\\beta_{t}\\psi(q)\\Biggr\\}\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathcal{P}_{k}$ is the set of probability distributions over $\\boldsymbol{\\mathcal{A}}=[k],\\,\\boldsymbol{\\widehat{\\ell}}_{t}\\colon\\boldsymbol{\\mathcal{P}}_{k}\\rightarrow\\mathbb{R}$ is an estimator of loss function $\\ell_{t}$ $\\beta_{t}>0$ is (a reciprocal of) learning rate at round $t$ , and $\\psi$ is a convex regularizer. FTRL is known for its usefulness in various online learning problems [1, 4, 8, 27, 37]. Notably, FTRL can be viewed as a generalization of Online Gradient Descent [63] and the Hedge algorithm [21, 38, 57], and is closely related to Online Mirror Descent [36, 45]. ", "page_idx": 1}, {"type": "text", "text": "The benefit of FTRL due to its generality is that one can design its regularizer $\\psi$ and learning rate $(\\beta_{t})_{t}$ so that it can perform adaptively to various properties of underlying loss functions. The adaptive learning rate, which exploits past observations, is often used to obtain such adaptivity. In order to see how it is designed, we consider the following stability-penalty decomposition, well-known in the literature [36, 45]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}\\lesssim\\underbrace{\\sum_{t=1}^{T}\\frac{z_{t}}{\\beta_{t}}}_{\\mathrm{stability~term}}+\\underbrace{\\beta_{1}h_{1}+\\sum_{t=2}^{T}(\\beta_{t}-\\beta_{t-1})h_{t}}_{\\mathrm{penalty~term}}\\,.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Intuitively, the stability term arises from the regret when the difference in FTRL outputs, $x_{t}$ and $x_{t+1}$ , is large, and the penalty term is due to the strength of the regularizer. For example, in the Exp3 algorithm for multi-armed bandits [8], $h_{t}$ is the Shannon entropy of $x_{t}$ or its upper bound, and $z_{t}$ is the expectation of $(\\nabla^{2}\\psi(x_{t}))^{-1}$ -norm of the importance-weighted estimator $\\tilde{\\ell_{t}}$ or its upper bound. ", "page_idx": 1}, {"type": "text", "text": "Adaptive learning rates have been designed so that it depends on the stability or penalty. For example, the well-known AdaGrad [19, 44] and the first-order algorithm [2] depend on stability components $(z_{s})_{s=1}^{t-1}$ todetermine $\\beta_{t}$ . More recently, there are learning rates that depend on penalty components $(h_{s})_{s=1}^{t-1}$ [25, 54] and that depend on both stability and penalty components [26, 28, 5]. ", "page_idx": 1}, {"type": "text", "text": "However, almost all adaptive learning rates developed so far have been limited to problems with a minimax regret of $\\Theta({\\sqrt{T}})$ , and there has been limited investigation into problems with a minimax regret of $\\Theta(T^{2/3})$ [25, 54]. Such online learning are primarily related to indirect feedback and includes many important problems, such as partial monitoring [9, 34], graph bandits [4], dueling bandits [51], online ranking [12], bandits with switching costs [18], and multi-armed bandits with paid observations [53]. The $\\Theta(T^{2/3})$ problem is distinctive also due to the classification theorem in partial monitoring [9, 34, 35], which is a very general problem that includes a wide range of sequential decision-making problems as special cases. It is known that, the minimax regret of partial monitoring games can be classified into one of four categories: O, $\\Theta({\\sqrt{T}})$ \uff0c $\\Theta(T^{2/3})$ ,or $\\Omega(T)$ .Among these, the classes with non-trivial difficulties and particular importance are the problems with a minimax regret of $\\Theta({\\sqrt{T}})$ and $\\Theta(T^{2/3})$ ", "page_idx": 1}, {"type": "text", "text": "Contributions  To address this limitation, we establish a new learning rate framework for online learning with a minimax regret of $\\Theta(T^{2/3})$ . Henceforth, we will refer to problems with a minimax regret of $\\Theta(T^{2/3})$ as hard problems to avoid repetition, abusing the terminology of partial monitoring. For hard problems, it is common to combine FTRL with forced exploration [4, 17, 34, 51]. In this study, we first observe that the regret of FTRL with forced exploration rate $\\gamma_{t}$ is roughly bounded as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}\\lesssim\\underbrace{\\sum_{t=1}^{T}\\frac{z_{t}}{\\beta_{t}\\gamma_{t}}}_{\\mathrm{stability~term}}+\\underbrace{\\beta_{1}h_{1}+\\sum_{t=2}^{T}(\\beta_{t}-\\beta_{t-1})h_{t}}_{\\mathrm{penalty~term}}+\\underbrace{\\sum_{t=1}^{T}\\gamma_{t}}_{\\mathrm{bias~term}}\\,.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, the third term, called the bias term, represents the regret incurred by forced exploration. In the aim of minimizing the RHS of (3), we will determine the exploration rate $\\gamma_{t}$ and learning rate $\\beta_{t}$ so that the above stability, penalty, and bias elements for each $t\\,\\in\\,[T]$ are matched, where the resulting learning rate is called Stability-Penalty-Bias matching learning rate (SPB-matching). This was inspired by the learning rate designed by matching the stability and penalty terms for problems with a minimax regret of $\\Theta({\\sqrt{T}})$ [26]. Our learning rate is simultaneously adaptive to the stability component $z_{t}$ and penalty component $h_{t}$ , which have attracted attention in very recent years [26, 28, 55]. The SPB-matching learning rate allows us to upper bound the RHS of (3) as follows: ", "page_idx": 1}, {"type": "table", "img_path": "XlvUz9F50g/tmp/47bac82f1099722214cff88cc26c26685d9ffa1694dd6b764b7d07aa5f72a741.jpg", "table_caption": ["Table 1: Regret bounds for partial monitoring, graph bandits, and multi-armed bandits (MAB) with paid observations. The number of rounds is denoted as $T$ , the number of actions as $k$ , and the minimum suboptimality gap as $\\Delta_{\\mathrm{min}}$ . The variables $c_{\\mathcal{G}}$ is defined in Section 5, $D$ is a constant dependent on the outcome distribution. The graph complexity measures $\\delta,\\delta^{*}$ , satisfying $\\delta^{*}\\leq\\delta$ for graphs with no self-loops, are defined in Section 6, and $\\tilde{\\delta}^{*}\\leq\\bar{\\delta}$ is the fractional weak domination number [13]. The parameter $c$ is the paid cost for observing a loss of actions. AwSB is the abbreviation of the adversarial regime with a self-bounding constraint. MS-type means that the bound in AdvSB has a form similar to the bound established by Masoudian and Seldin [43]. "], "table_footnote": ["aThe framework in [15] is a hierarchical reduction-based approach, rather than a direct FTRL method, discarding past observations as doubling-trick. bThe bounds in [15] depend on $\\delta$ , but their framework with the algorithm in [13] can achieve improved bounds replacing $\\delta$ with $\\tilde{\\delta}^{\\ast}\\leq\\delta$ "], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Theorem 1 (informal version of Theorem 6). There exists learning rate $(\\beta_{t})_{t}$ and exploration rate $(\\gamma_{t})_{t}$ forwhich the RHS of (3)is bouded by $\\begin{array}{r}{O\\big(\\big(\\sum_{t=1}^{T}\\sqrt{z_{t}h_{t}\\log(\\varepsilon T)}\\big)^{2/3}+\\big(\\sqrt{z_{\\mathrm{max}}h_{\\mathrm{max}}}/\\varepsilon\\big)^{2/3}\\big)}\\end{array}$ for any $\\varepsilon\\geq1/T$ where $z_{\\operatorname*{max}}=\\operatorname*{max}_{t\\in[T]}z_{t}$ and $h_{\\operatorname*{max}}=\\operatorname*{max}_{t\\in[T]}h_{t}$ ", "page_idx": 2}, {"type": "text", "text": "Within the general online learning framework, this theorem allows us to prove the following Bestof-Both-Worlds (BOBW) guarantee [10, 58, 61], which achieves an ${\\cal O}(\\log T)$ regret in the stochastic regime and an $O(T^{2/3})$ regret in the adversarial regime simultaneously: ", "page_idx": 2}, {"type": "text", "text": "Theorem 2 (informal version of Theorem 7). Under some regularity conditions, an FTRL-based algorithm with SPB-matching achieves $\\mathsf{R e g}_{T}\\lesssim(z_{\\mathrm{max}}h_{\\mathrm{max}})^{1/3}T^{2/3}$ in the adversarial regime. In the stochastic regime, $i f\\sqrt{z_{t}h_{t}}\\le\\sqrt{\\rho_{1}}(1-q_{t a^{*}})$ holds for FTRL output $q_{t}\\in\\mathcal{P}_{k}$ and $\\rho_{1}>0$ for all $t\\!\\in\\![T].$ thesamealgorithmachieves $\\mathsf{R e g}_{T}\\!\\lesssim\\!\\rho_{1}\\!\\log T/\\Delta_{\\operatorname*{min}}^{2}$ fortheminimumsuboptimalitygap $\\Delta_{\\mathrm{min}}$ ", "page_idx": 2}, {"type": "text", "text": "To assess the usefulness of the above result that holds for the general online learning framework, this study focuses on two major hard problems: partial monitoring with global observability, graph bandits with weak observability, and multi-armed bandits with paid observations. We demonstrate that the assumptions in Theorem 2 are indeed satisfied for these problems by appropriately choosing the parameters in SPB-matching, thereby improving the existing BOBW regret upper bounds in several respects. To obtain better bounds in this analysis, we leverage the smallness of stability components $z_{t}$ , which results from the forced exploration. Additionally, SPB-matching is the first unified framework to achieve a BOBW guarantee for hard online learning problems. Our learning rate is based on a surprisingly simple principle, whereas existing learning rates for graph bandits and partial monitoring are extremely complicated (see [25, Eq. (15)] and [54, Eq. (16)]). Due to its simplicity, we believe that SPB-matching will serve as a foundation for building new BOBW algorithms for a variety of hard online learning problems. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The SPB-matching framework, though omitted from the main text due to the space constraints, is also applicable to the multi-armed bandits with paid observations [53], whose minimax regret with costs is $\\Theta(T^{2/3})$ : We can show that the regret with paid costs, ${\\mathsf{R e g}}_{T}^{\\mathsf{c}}$ , is roughly bounded by $\\mathsf{R e g}_{T}^{\\mathtt{c}}\\,=\\,\\mathsf{\\overset{\\cdot}{O}}\\bigl((\\dot{c}k\\log k)^{1/3}T^{2/3}+\\sqrt{T\\log k}\\bigr)$ in the adversarial regime and ${\\mathsf{R e g}}_{T}^{\\mathsf{c}}\\,=$ $O\\big(\\mathrm{max}\\{c,1\\}k\\log k\\log T/\\Delta_{\\mathrm{min}}^{2}\\big)$ in the stochastic regime for the cost of observation $c$ The bound for adversarial regime is of the same order as [53, Theorem 3]. The detailed problem setup, regret upper bounds, and regret analysis can be found in Appendix G. ", "page_idx": 3}, {"type": "text", "text": "Although omitted in Theorem 2, our approach achieves a refined regret bound devised by Masoudian and Seldin [43] in the adversarial regime with a self-bounding constraint [61], which includes the stochastic regime, adversarial regime, and the stochastic regime with adversarial corruptions [41] as special cases. We call the refind bound MS-type bound, named after the author. The MS-type bound maintains an ideal form even when $C=\\Theta(T)$ Or $\\Delta_{\\mathrm{min}}=\\Theta(1/\\sqrt{T})$ (see [43] for details), and our bounds are the first MS-type bounds for hard problems. A comparison with existing regret bounds is summarized in Table 1. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notation  For a natural number $n\\in\\mathbb N$ , we let $[n]=\\{1,\\ldots,n\\}$ . For vector $x$ ,let $x_{i}$ denote its $i$ -th element and $\\Vert x\\Vert_{p}$ the $\\ell_{p}$ -norm for $p\\in[1,\\infty]$ . Let $\\mathcal{P}_{k}=\\{p\\in[0,1]^{k}\\colon\\|p\\|_{1}=1\\}$ be the $(k-1)$ dimensional probability simplex. The vector $e_{i}$ is the $i$ -th standard basis and 1 is the all-ones vector. Let $D_{\\psi}(x,y)$ denote the Bregman divergence from $y$ to $x$ induced by a differentiable convex function $\\psi$ $D_{\\psi}(x,y)=\\psi(x)\\!-\\!\\psi(y)\\!-\\!\\langle\\nabla\\psi(y),x\\!-\\!y\\rangle$ . To simplify the notation, we sometimes write $(a_{t})_{t=1}^{T}$ as $a_{1:T}$ and $f=O(g)$ as $f\\lesssim g$ . We regard function $\\bar{f}\\colon\\bar{\\mathcal{A}}=[k]\\rightarrow\\mathbb{R}$ as a $k$ -dimensional vector. ", "page_idx": 3}, {"type": "text", "text": "General online learning framework  To provide results that hold for a wide range of settings, we consider the following general online learning framework introduced in Section 1. ", "page_idx": 3}, {"type": "table", "img_path": "XlvUz9F50g/tmp/78a128ccc85b4f0d1c21f8873cc166631926a43207c737509489dfc5535623f3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "This framework includes many problems such as the expert problem, multi-armed bandits, graph bandits, partial monitoring as special cases. ", "page_idx": 3}, {"type": "text", "text": "Stochastic, adversarial, and their intermediate regimes  Within the above general online framework, we study three different regimes for a sequence of loss functions $(\\ell_{t})_{t}$ . In the stochastic regime, the sequence of loss functions is sampled from an unknown distribution $\\mathcal{D}$ in an i.i.d. manner. The suboptimality gap for action $a\\in A$ is given by $\\Delta_{a}=\\mathbb{E}_{\\ell_{t}\\sim\\mathcal{D}}[\\ell_{t}(a)-\\ell_{t}(a^{*})]$ and the minimum suboptimality gap by $\\begin{array}{r}{\\Delta_{\\operatorname*{min}}=\\operatorname*{min}_{a\\neq a^{*}}\\Delta_{a}}\\end{array}$ . In the adversarial regime, the loss functions can be selected arbitrarily, possibly based on the past history up to round $t-1$ ", "page_idx": 3}, {"type": "text", "text": "We also investigate, the adversarial regime with a self-bounding constraint [61], which is an intermediate regime between the stochastic and adversarial regimes. ", "page_idx": 3}, {"type": "text", "text": "Definition 3. Let $\\Delta\\,\\in\\,[0,1]^{k}$ and $C~\\ge~0$ . The environment is in an adversarial regime with $a$ $(\\Delta,C,T)$ self-bounding constraint if it holds for any algorithm that $\\begin{array}{r}{\\mathsf{R e g}_{T}\\geq\\mathbb{E}\\bigl[\\sum_{t=1}^{T}\\Delta_{A_{t}}-C\\bigr]}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "From the definition, the stochastic and adversarial regimes are special cases of this regime. Additionally, the well-known stochastic regime with adversarial corruptions [41] also falls within this regime. For the adversarial regime with a self-bounding constraint, we assume that there exists a unique optimal action $a^{*}$ . This assumption is standard in the literature of BOBW algorithms (e.g., [22, 39, 58]). ", "page_idx": 4}, {"type": "text", "text": "3  SBP-matching: Simple and adaptive learning rate for hard problems ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section designs a new learning rate framework for hard online learning problems. ", "page_idx": 4}, {"type": "text", "text": "3.1 Objective function that adaptive learning rate aims to minimize ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In hard problems, the regret of FTRL with somewhat large exploration rate $\\gamma_{t}$ is known to be bounded in the following form [4, 25, 54]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}\\lesssim\\sum_{t=1}^{T}\\frac{z_{t}}{\\beta_{t}\\gamma_{t}}+\\sum_{t=1}^{T}(\\beta_{t}-\\beta_{t-1})h_{t}+\\sum_{t=1}^{T}\\gamma_{t}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some stability component $z_{t}$ and penalty component $h_{t}$ ,whereweset $\\beta_{T+1}=\\beta_{T}$ and $\\beta_{0}=0$ for simplicity. Recall that the first term is the stability term, the second term is the penalty term, and the third term is the bias term, which arises from the forced exploration. ", "page_idx": 4}, {"type": "text", "text": "The goal when designing the adaptive learning rate is to minimize (4), under the constraints that $(\\beta_{t})_{t}$ is non-decreasing and $\\beta_{t}$ depends on $\\left(z_{1:t},h_{1:t}\\right)$ or $\\left(z_{1:t-1},h_{1:t}\\right)$ . A naive way to choose $\\gamma_{t}$ to minimize (4) is to set $\\gamma_{t}=\\sqrt{z_{t}/\\beta_{t}}$ so that the stability term and the bias term match. However, this choice does not work well in hard problems because to obtain a regret bound of (4), a lower bound of $\\gamma_{t}\\geq u_{t}/\\beta_{t}$ for some $u_{t}>0$ is needed. This lower bound is used to control the magnitude of the loss estimator $\\widehat{\\ell}_{t}$ (see e.g., Eq. (61) for partial monitoring and Eq. (79) for graph bandits).1 Therefore, we consider exploration rate of $\\gamma_{t}=\\gamma_{t}^{\\prime}+u_{t}/\\beta_{t}$ for $\\gamma_{t}^{\\prime}=\\sqrt{z_{t}/\\beta_{t}}$ and some $u_{t}>0$ , where $\\gamma_{t}^{\\prime}$ is chosen so that the stability and bias terms are matched. With these choices, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Eq.~}(4)\\leq\\displaystyle\\sum_{t=1}^{T}\\biggl(\\frac{z_{t}}{\\beta_{t}\\gamma_{t}^{\\prime}}+(\\beta_{t}-\\beta_{t-1})h_{t}+\\biggl(\\gamma_{t}^{\\prime}+\\frac{u_{t}}{\\beta_{t}}\\biggr)\\biggr)}\\\\ &{\\qquad=\\displaystyle\\sum_{t=1}^{T}\\biggl(2\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\frac{u_{t}}{\\beta_{t}}+(\\beta_{t}-\\beta_{t-1})h_{t}\\biggr)=:F\\bigl(\\beta_{1:T},z_{1:T},u_{1:T},h_{1:T}\\bigr)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the first two terms in $F$ \uff0c $2\\sqrt{z_{t}/\\beta_{t}}+u_{t}/\\beta_{t}$ , come from the stability and bias terms and the last term, $(\\beta_{t}-\\beta_{t-1})h_{t}$ , is the penalty term. In the following, we investigate adaptive learning rate $(\\beta_{t})_{t=1}^{T}$ that minimizes $F$ in (5) instead of (4). ", "page_idx": 4}, {"type": "text", "text": "3.2  Stability-penalty-bias matching learning rate ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We consider determining $(\\beta_{t})_{t}$ by matching the stability-bias terms and the penalty term as $2\\sqrt{z_{t}/\\beta_{t}}+u_{t}/\\beta_{t}=(\\beta_{t}-\\beta_{t-1})h_{t}$ Assume that when choosing $\\beta_{t}$ , we have an access to $\\widehat{h}_{t}$ such that $h_{t}\\leq\\widehat{h}_{t}$ 2 Then, inspired by the above matching, we consider the following two update rules: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left(\\mathrm{Rule~}1\\right)\\,\\beta_{t}=\\beta_{t-1}+\\frac{1}{\\hat{h}_{t}}\\!\\left(2\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\frac{u_{t}}{\\beta_{t}}\\right),\\,\\left(\\mathrm{Rule~}2\\right)\\,\\beta_{t}=\\beta_{t-1}+\\frac{1}{\\hat{h}_{t}}\\!\\left(2\\sqrt{\\frac{z_{t-1}}{\\beta_{t-1}}}+\\frac{u_{t-1}}{\\beta_{t-1}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We call these update rules Stability-Penalty-Bias Matching (SPB-matching). These are designed by following the simple principle of matching the stability, penalty, and bias elements, and Rules 1 and ", "page_idx": 4}, {"type": "text", "text": "2 differ only in the way indices are shifted.3 For the sake of convenience, we define $G_{1}$ and $G_{2}$ by ", "page_idx": 5}, {"type": "equation", "text": "$$\nG_{1}(z_{1:T},h_{1:T})=\\sum_{t=1}^{T}\\frac{\\sqrt{z_{t}}}{\\left(\\sum_{s=1}^{t}\\sqrt{z_{s}}/h_{s}\\right)^{1/3}}\\;,\\ G_{2}(u_{1:T},h_{1:T})=\\sum_{t=1}^{T}\\frac{u_{t}}{\\sqrt{\\sum_{s=1}^{t}u_{s}/h_{s}}}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Define $z_{\\operatorname*{max}}\\,=\\,\\operatorname*{max}_{t\\in[T]}\\,z_{t}$ \uff0c $u_{\\mathrm{max}}\\,=\\,\\operatorname*{max}_{t\\in[T]}\\,u_{t}$ and $h_{\\operatorname*{max}}\\,=\\,\\operatorname*{max}_{t\\in[T]}h_{t}$ . Then, using SPBmatching rules in (6), we can upper-bound $F$ in terms of $G_{1}$ and $G_{2}$ as follows: ", "page_idx": 5}, {"type": "text", "text": "Lemma 4. Consider SPB-matching (6) and suppose that $h_{t}\\leq\\widehat{h}_{t}$ for all $t\\,\\in\\,[T]$ .Then, Rule $^{\\,\\,\\,I}$ achieves $F(\\beta_{1:T},z_{1:T},u_{1:T},h_{1:T})\\ \\leq\\ 3.2G_{1}(z_{1:T},\\widehat{h}_{1:T})\\ +\\ 2G_{2}(u_{1:T},\\widehat{h}_{1:T})\\ a n$ d Rule2achieves F $\\begin{array}{r}{\\Upsilon(\\beta_{1:T},z_{1:T},u_{1:T},h_{1:T})\\le4G_{1}(z_{1:T},\\widehat{h}_{2:T+1})+3G_{2}\\big(u_{1:T},\\widehat{h}_{2:T+1}\\big)+10\\sqrt{z_{\\operatorname*{max}}/\\beta_{1}}+5u_{\\operatorname*{max}}\\big/\\beta_{1}+\\mathtt{N}_{2:T},}\\end{array}$ $\\beta_{1}h_{1}$ ", "page_idx": 5}, {"type": "text", "text": "The proof of Lemma 4 can be found in Appendix B.1. One can see from the proof that the effect of using $\\gamma_{t}=\\sqrt{z_{t}/\\beta_{t}}+u_{t}/\\beta_{t}$ instead of $\\gamma_{t}=\\sqrt{z_{t}/\\beta_{t}}$ only appears in $G_{2}$ , which has a less impact than $G_{1}$ when bounding $F$ . We can further upper-bound $G_{1}$ as follows: ", "page_idx": 5}, {"type": "text", "text": "Lemma 5. Let $(z_{t})_{t=1}^{T}~\\subseteq~\\mathbb{R}_{\\geq0}$ and $(h_{t})_{t=1}^{T}~\\subseteq~\\mathbb{R}_{>0}$ be any non-negative and positive sequences, respectively. Let $\\theta_{0}~>~\\theta_{1}~>~\\cdot\\cdot~>~\\theta_{J}~>~\\theta_{J+1}~=~0$ and $\\theta_{0}~\\ge~h_{\\mathrm{max}}$ and define ${\\cal T}_{j}\\ =\\ \\left\\{t\\in[T]\\colon\\theta_{j-1}\\geq h_{t}>\\theta_{j}\\right\\}$ for $j\\ \\in\\ [J]$ and $T_{J+1}~=~\\{t\\in[T]:\\theta_{J}\\geq h_{t}\\}$ .Then, $\\begin{array}{r}{G_{1}(z_{1:T},h_{1:T})\\leq\\frac{3}{2}\\sum_{j=1}^{J+1}\\bigl(\\sqrt{\\theta_{j-1}}\\sum_{t\\in\\mathcal{T}_{j}}\\sqrt{z_{t}}\\bigr)^{2/3}}\\end{array}$ This implies that for all $J\\in\\mathbb{N}$ it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{\\tilde{r}}_{1}(z_{1:T},h_{1:T})\\leq\\frac{3}{2}\\operatorname*{min}\\Biggl\\{\\left(\\sqrt{2J}\\sum_{t=1}^{T}\\sqrt{z_{t}h_{t}}\\right)^{\\frac{2}{3}}+\\left(2^{-J/2}\\sqrt{z_{\\mathrm{max}}h_{\\mathrm{max}}}\\right)^{\\frac{2}{3}}T^{\\frac{2}{3}}\\,,\\left(\\sum_{t=1}^{T}\\sqrt{z_{t}h_{\\mathrm{max}}}\\right)^{\\frac{2}{3}}\\Biggr\\}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Combining Lemmas 4 and 5 and the bound on $G_{2}$ in [26, Lemma 3], we obtain the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 6. Let $(z_{t})_{t=1}^{T},(u_{t})_{t=1}^{T}\\subseteq\\mathbb{R}_{\\geq0}$ and $(h_{t})_{t=1}^{T}\\subseteq\\mathbb{R}_{>0}$ Ssuppose that $\\widehat{h}_{t}$ satisfies $h_{t}\\leq\\widehat{h}_{t}$ for all $t\\in[T]$ . Then, $i f\\,\\beta_{t}$ is given by Rule $^{\\,l}$ in (6), then for all $\\varepsilon\\geq1/T$ it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{I}(\\beta_{1:T},z_{1:T},u_{1:T},h_{1:T})\\lesssim\\operatorname*{min}\\Biggl\\{\\left(\\sum_{t=1}^{T}\\sqrt{z_{t}\\hat{h}_{t}\\log(\\varepsilon T)}\\right)^{\\frac{2}{3}}\\!\\!+\\!\\left(\\sqrt{z_{\\operatorname*{max}}\\hat{h}_{\\operatorname*{max}}}\\Big/\\varepsilon\\right)^{\\frac{2}{3}}\\!,\\left(\\sum_{t=1}^{T}\\sqrt{z_{t}\\hat{h}_{\\operatorname*{max}}}\\right)^{\\frac{2}{3}}}\\\\ &{\\quad\\quad\\quad+\\operatorname*{min}\\Biggl\\{\\sqrt{\\sum_{t=1}^{T}u_{t}\\hat{h}_{t}\\log(\\varepsilon T)}+\\sqrt{u_{\\operatorname*{max}}\\hat{h}_{\\operatorname*{max}}/\\varepsilon}\\:,\\:\\sqrt{\\sum_{t=1}^{T}u_{t}\\hat{h}_{\\operatorname*{max}}}\\Biggr\\}\\:.\\quad\\:(8)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If $\\beta_{t}$ is given by Rule 2 in (6), then for all $\\varepsilon\\geq1/T$ it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathcal{I}(\\beta_{1:T},z_{1:T},u_{1:T},h_{1:T})\\lesssim\\operatorname*{min}\\left\\{\\left(\\sum_{t=1}^{T}\\sqrt{z_{t}\\hat{h}_{t+1}\\log(\\varepsilon T)}\\right)^{\\frac{2}{3}}\\!+\\!\\left(\\sqrt{z_{\\operatorname*{max}}\\hat{h}_{\\operatorname*{max}}}/\\varepsilon\\right)^{\\frac{2}{3}},\\left(\\sum_{t=1}^{T}\\sqrt{z_{t}\\hat{h}_{\\operatorname*{max}}}\\right)\\right.}\\\\ &{\\displaystyle+\\operatorname*{min}\\left\\{\\sqrt{\\sum_{t=1}^{T}u_{t}\\hat{h}_{t+1}\\log(\\varepsilon T)}\\!+\\!\\sqrt{u_{\\operatorname*{max}}\\hat{h}_{\\operatorname*{max}}/\\varepsilon}\\,,\\,\\sqrt{\\sum_{t=1}^{T}u_{t}\\hat{h}_{\\operatorname*{max}}}\\right\\}\\!+\\!\\sqrt{\\frac{z_{\\operatorname*{max}}}{\\beta_{1}}}\\!+\\!\\frac{u_{\\operatorname*{max}}}{\\beta_{1}}\\!+\\!\\beta_{1}h_{1}\\,.\\,\\,\\,\\,\\left(9\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that these bounds are for problems with a minimax regret of $\\Theta(T^{2/3})$ . Roughly speaking, our bounds have an order of $\\begin{array}{r}{\\left(\\sum_{t=1}^{T}\\sqrt{z_{t}\\widehat{h}_{t+1}\\log T}\\right)^{1/3}}\\end{array}$ and diffe rom the existing taity-penatyadaptive-type bounds of $\\sqrt{\\sum_{t=1}^{T}z_{t}\\hat{h}_{t+1}\\log T}$ for problems with a minimax regret of $\\Theta({\\sqrt{T}})$ [26, 55]. We will see in the subsequent sections that our bounds are benefitical as they give nearly optimal regret bounds in stochastic and adversarial regimes in partial monitoring, graph bandits, and multiarmed bandits with paid observations. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1: Best-of-both-worlds framework based on FTRL with SPB-matching learning rate and Tsallis entropy for online learning with minimax regret of $\\Theta(T^{2/3})$ ", "page_idx": 6}, {"type": "text", "text": "1 input: action set $\\boldsymbol{\\mathcal{A}}$ , observation set $\\scriptscriptstyle\\mathcal{O}$ , exponent of Tsallis entropy $\\alpha,\\,\\beta_{1},$ $\\bar{\\beta}$   \n2 for $t=1,2,\\dots{\\bf d}$ 0   \n3 Compute $q_{t}\\in\\mathcal P_{k}$ by (10) with a loss estimator $\\widehat{\\ell}_{t}$   \n4 Set $h_{t}=H_{\\alpha}(q_{t})$ and $z_{t},u_{t}\\geq0$ defined for each problem.   \n5 Compute action selection probability $p_{t}$ from $q_{t}$ by (11).   \n6 Choose $A_{t}\\in\\mathcal A$ so that $\\operatorname*{Pr}[A_{t}=i\\mid p_{t}]=p_{t i}$ and observe feedback $o_{t}\\in\\mathcal{O}$   \n7 Compute loss estimator $\\widehat{\\ell}_{t}$ based on $p_{t}$ and $o_{t}$   \n8 Compute $\\beta_{t+1}$ by Rule 2 of SPB-matching in (6) with $\\widehat{h}_{t+1}=h_{t}$ ", "page_idx": 6}, {"type": "text", "text": "4   Best-of-both-worlds framework for hard online learning problems ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Using the SPB-matching learning rate established in Section 3, this section provides a BOBW algorithm framework for hard online learning problems. We consider the following FTRL update: ", "page_idx": 6}, {"type": "equation", "text": "$$\nq_{t}=\\mathop{\\arg\\operatorname*{min}}_{p\\in\\mathcal{P}_{k}}\\left\\{\\sum_{s=1}^{t-1}\\langle\\widehat{\\ell}_{t},p\\rangle+\\beta_{t}(-H_{\\alpha}(p))+\\bar{\\beta}(-H_{\\bar{\\alpha}}(p))\\right\\},\\quad\\alpha\\in\\left(0,1\\right),\\,\\bar{\\alpha}=1-\\alpha\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $H_{\\alpha}$ isthe $\\alpha$ Tsallis entropy defined as $\\begin{array}{r}{H_{\\alpha}(p)=\\frac{1}{\\alpha}\\sum_{i=1}^{k}(p_{i}^{\\alpha}-p_{i})}\\end{array}$ which satisfes $H_{\\alpha}(p)\\geq0$ and $H_{\\alpha}(e_{i})=0$ . Based on this FTRL output $q_{t}$ , we set $h_{t}=H_{\\alpha}(q_{t})$ , which satisfies $h_{1}=h_{\\operatorname*{max}}$ Additionally, for $q_{t}$ and some $p_{0}\\in\\mathcal{P}_{k}$ , we use the action selection probability $p_{t}\\in\\mathcal{P}_{k}$ defined by ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{t}=(1-\\gamma_{t})q_{t}+\\gamma_{t}\\,p_{0}\\quad\\mathrm{for}\\quad\\gamma_{t}=\\gamma_{t}^{\\prime}+\\frac{u_{t}}{\\beta_{t}}=\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\frac{u_{t}}{\\beta_{t}}\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\beta_{1}$ is chosen so that $\\gamma_{t}\\,\\in\\,[0,1/2]$ . Let $\\kappa=\\sqrt{z_{\\mathrm{max}}/\\beta_{1}}+u_{\\mathrm{max}}/\\beta_{1}+\\beta_{1}h_{1}+\\bar{\\beta}\\bar{h}$ for $\\bar{h}=$ $H_{\\bar{\\alpha}}({\\bf1}/k)$ and let $\\mathbb{E}_{t}[\\,\\cdot\\,]$ be the expectation given all observations before round $t$ .Then the above procedure with Rule 2 of SPB-matching in (6), summarized in Algorithm 1, achieves the following BOBW bound: ", "page_idx": 6}, {"type": "text", "text": "Theorem 7. Consider the general online learning framework in Section 2 with $\\|\\ell_{t}\\|_{\\infty}\\leq1$ Suppose thatAlgorithm $^{\\,l}$ satisfies the following three conditions (i)-(ii): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{i})\\,\\mathsf{R e g}_{T}\\leq\\mathbb{E}\\!\\left[\\displaystyle\\sum_{t=1}^{T}\\langle\\widehat{\\ell}_{t},q_{t}-e_{a^{*}}\\rangle+2\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}\\right]\\!,}\\\\ &{\\mathrm{ii})\\,\\mathbb{E}_{t}\\!\\left[\\langle\\widehat{\\ell}_{t},q_{t}-q_{t+1}\\rangle-\\beta_{t}{\\cal D}_{(-H_{\\alpha})}(q_{t+1},q_{t})\\right]\\lesssim\\displaystyle\\frac{z_{t}}{\\beta_{t}\\gamma_{t}^{\\prime}}\\,,\\quad(\\mathrm{ii})\\,h_{t}\\lesssim h_{t-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, in the adversarial regime, Algorithm $^{\\,I}$ achieves ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}=O\\Big((z_{\\mathrm{max}}h_{1})^{1/3}T^{2/3}+\\sqrt{u_{\\mathrm{max}}h_{1}T}+\\kappa\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the adversarial regime with a $(\\Delta,C,T)$ -self-bounding constraint, further suppose that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sqrt{z_{t}h_{t}}\\leq\\sqrt{\\rho_{1}}\\cdot(1-q_{t a^{*}})\\quad a n d\\quad u_{t}h_{t}\\leq\\rho_{2}\\cdot(1-q_{t a^{*}})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "are satisfied for some $\\rho_{1},\\rho_{2}>0$ for all $t\\in[T]$ . Then, the same algorithm achieves ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}=O\\left(\\frac{\\rho}{\\Delta_{\\mathsf{m i n}}^{2}}\\log\\bigl(T\\Delta_{\\mathsf{m i n}}^{2}\\bigr)+\\left(\\frac{C^{2}\\rho}{\\Delta_{\\mathsf{m i n}}^{2}}\\log\\biggl(\\frac{T\\Delta_{\\mathsf{m i n}}}{C}\\biggr)\\right)^{1/3}+\\kappa^{\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for $\\rho=\\operatorname*{max}\\{\\rho_{1},\\rho_{2}\\}$ and $\\kappa^{\\prime}=\\kappa+\\big((z_{\\mathrm{max}}h_{1})^{1/3}+\\sqrt{u_{\\mathrm{max}}h_{1}}\\big)\\big(1/\\Delta_{\\mathrm{min}}^{2}+C/\\Delta_{\\mathrm{min}}\\big)^{2/3}$ 2/3 when T \u2265 $1/\\Delta_{\\sf m i n}^{2}+C/\\Delta_{\\sf m i n}=:\\tau,$ and $\\mathsf{R e g}_{T}=O\\big((z_{\\operatorname*{max}}h_{1})^{1/3}\\tau^{2/3}+\\sqrt{u_{\\operatorname*{max}}h_{1}\\tau}\\big)$ when $T<\\tau$ ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 7 relies on Theorem 6 established in the last section and can be found in Appendix C. Note that the bound (15) becomes the bound for the stochastic regime when $C=0$ ", "page_idx": 6}, {"type": "text", "text": "5  Case study (1): Partial monitoring with global observability ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section provides a new BOBW algorithm for globally observable partial monitoring games. ", "page_idx": 7}, {"type": "text", "text": "5.1 Problem setting and some concepts in partial monitoring ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Partial monitoring games A Partial Monitoring (PM) game $\\mathcal{G}=(\\mathcal{L},\\Phi)$ consists of a loss matrix $\\mathcal{L}\\in[0,1]^{k\\times d}$ and feedback matrix $\\Phi\\,\\in\\,\\Sigma^{k\\times d}$ , where $k$ and $d$ are the number of actions and outcomes, respectively, and $\\Sigma$ is the set of feedback symbols. The game unfolds over $T$ rounds between the learner and the environment. Before the game starts, the learner is given $\\mathcal{L}$ and $\\Phi$ . At each round $t\\in[T]$ , the environment picks an outcome $\\bar{x_{t}}\\in[d]$ , and then the learner chooses an action $A_{t}\\in[k]$ without knowing $x_{t}$ . Then the learner incurs an unobserved loss $\\mathcal{L}_{A_{t}x_{t}}$ and only observes a feedback symbol $\\sigma_{t}:=\\Phi_{A_{t}x_{t}}$ . This framework can be indeed expressed as the general online learning framework in Section 2, by setting $O=\\Sigma$ $\\ell_{t}(a)=\\mathcal{L}_{a x_{t}}=e_{a}^{\\dagger}\\mathcal{L}e_{x_{t}}$ and $o_{t}=\\sigma_{t}=\\Phi_{A_{t}x_{t}}$ ", "page_idx": 7}, {"type": "text", "text": "We next introduce fundamental concepts for PM games. Based on the loss matrix $\\mathcal{L}$ , we can decompose all distributions over outcomes. For each action $a~\\in~[k]$ , the cell of action $a$ ,denoted as $\\mathcal{C}_{a}$ , is the set of probability distributions over $[d]$ for which action $a$ is optimal. That is, $\\mathcal{C}_{a}=\\{u\\in\\mathcal{P}_{d}\\colon\\operatorname*{max}_{b\\in[k]}(\\ell_{a}-\\ell_{b})^{\\top}u\\leq0\\}$ , where $\\ell_{a}\\in\\mathbb{R}^{d}$ is the $a$ -th row of $\\mathcal{L}$ ", "page_idx": 7}, {"type": "text", "text": "To avoid the heavy notions and concepts of PM, we assume that the PM game has no duplicate actions $a\\neq b$ such that $\\ell_{a}=\\ell_{b}$ and its all actions are Pareto optimal; that is, $\\mathrm{di}\\bar{\\mathrm{m}}(\\mathcal{C}_{a})=d-1$ for all $a\\in[k]$ The discussion of the effect of this assumption can be found e.g., in [34, 37]. ", "page_idx": 7}, {"type": "text", "text": "Observability and loss estimation  Two Pareto optimal actions $a$ and $b$ are neighbors if $\\dim(\\mathcal{C}_{a}\\cap)$ $\\mathcal{C}_{b})\\,=\\,d-\\,2$ . Then, this neighborhood relations defines globally observable games, for which the minimax regret of $\\Theta(T^{2/3})$ is known in the literature [9, 34]. Two neighbouring actions $a$ and $b$ are globally observable if there exists a function $w_{e(a,b)}\\colon[k]\\times\\Sigma\\rightarrow\\mathbb{R}$ satisfying ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{c=1}^{k}w_{e(a,b)}(c,\\Phi_{c x})=\\mathcal{L}_{a x}-\\mathcal{L}_{b x}\\;\\;\\mathrm{for}\\;\\mathrm{all}\\;x\\in[d]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $e(a,b)=\\{a,b\\}$ . A PM game is said to be globally observable if all neighboring actions are globally observable. To the end, we assume that $\\mathcal{G}$ is globally observable.4 ", "page_idx": 7}, {"type": "text", "text": "Based on the neighborhood relations, we can estimate the loss difference between actions, instead of estimating the loss itself. The in-tree is the edges of a directed tree with vertices $[k]$ and let $\\mathcal{T}\\subseteq$ $[k]\\times[k]$ be an in-tree over the set of actions induced by the neighborhood relations with an arbitrarily chosen root $r\\,\\in\\,[k]$ . Then, we can estimate the loss differences between Pareto optimal actions as follows. Let $\\begin{array}{r}{G(a,\\bar{\\sigma})_{b}=\\sum_{e\\in\\mathrm{path}_{\\mathcal{T}}(b)}w_{e}(a,\\sigma)}\\end{array}$ for $a\\in[k]$ ,where $\\mathrm{path}_{\\mathcal{T}}(b)$ is the seto edges from $b\\in[k]$ to the root $r$ on $\\mathcal{T}$ . Then, it is known that this $G$ satisfies that for any Pareto optimal actions $a$ and $b$ $\\begin{array}{r}{\\sum_{c=1}^{k}(G(c,\\Phi_{c x})_{a}-G(b,\\Phi_{c x})_{b})=\\mathcal{L}_{a x}-\\mathcal{L}_{b x}}\\end{array}$ for all $x\\in[d]$ (e.g., [37, Lemma 4]). From this fact, one can see that we can use $\\widehat{y}_{t}=G(A_{t},\\Phi_{A_{t}x_{t}})/p_{t A_{t}}\\in\\mathbb{R}^{k}$ as the loss (difference) estimator, following the standard construction of the importance-weighted estimator [8, 36]. In fact, $\\widehat{y}_{t}$ satisfies $\\begin{array}{r}{\\mathbb{E}_{A_{t}\\sim p_{t}}[\\widehat{y}_{t a}-\\widehat{y}_{t b}]=\\sum_{c=1}^{k}(G(c,\\sigma_{t})_{a}-G(c,\\sigma_{t})_{b})=\\mathcal{L}_{a x}-\\mathcal{L}_{b x}.}\\end{array}$ We let $c_{\\mathcal{G}}=\\operatorname*{max}\\{1,k\\|G\\|_{\\infty}\\}$ be a game-dependent constant, where $\\begin{array}{r}{\\|G\\|_{\\infty}=\\operatorname*{max}_{a\\in[k],\\sigma\\in\\Sigma}|G(a,\\sigma)|}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "5.2  Algorithm and regret upper bounds ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here, we present a new BOBW algorithm based on Algorithm 1. We use the following parameters for Algorithm 1. We use the loss (difference) estimator of $\\widehat{\\ell_{t}}=\\widehat{y_{t}}$ . We set $p_{0}$ in (11) to $p_{0}={\\bf1}/k$ For $\\tilde{I_{t}}^{-}\\!\\in\\!\\arg\\operatorname*{max}_{i\\in[k]}q_{t i}$ and $q_{t*}=\\operatorname*{min}\\{q_{t\\tilde{I}_{t}},1-q_{t\\tilde{I}_{t}}\\}$ let ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\beta_{1}\\geq\\frac{64c_{Q}^{2}}{1-\\alpha}\\,,\\,\\bar{\\beta}=\\frac{32c_{Q}\\sqrt{k}}{(1-\\alpha)^{2}\\sqrt{\\beta_{1}}}\\,,\\,z_{t}=\\frac{4c_{Q}^{2}}{1-\\alpha}\\left(\\sum_{i\\neq\\tilde{I}_{t}}q_{t i}^{2-\\alpha}+q_{t*}^{2-\\alpha}\\right)\\,,\\,u_{t}=\\frac{8c_{Q}}{1-\\alpha}q_{t*}^{1-\\alpha}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that Zmax $\\begin{array}{r}{z_{\\mathrm{max}}=\\frac{4c_{\\mathcal{G}}^{2}}{1-\\alpha}}\\end{array}$ $\\begin{array}{r}{u_{\\mathrm{max}}=\\frac{8c_{\\mathcal{G}}}{1-\\alpha}}\\end{array}$ and $\\begin{array}{r}{h_{\\operatorname*{max}}=h_{1}=\\frac{1}{\\alpha}k^{1-\\alpha}}\\end{array}$ Then, we can prove te foloing ", "page_idx": 7}, {"type": "text", "text": "Theorem 8. In globally observable partial monitoring, for any $\\alpha\\in(0,1)$ ,Algorithm $^{\\,I}$ with (17) satisfies the assumptions of Theorem 7 with $\\begin{array}{r}{\\rho_{1}=\\Theta\\Big(\\frac{c_{\\mathcal{G}}^{2}k^{1-\\alpha}}{\\alpha(1-\\alpha)}\\Big)}\\end{array}$ Cndl $\\begin{array}{r}{\\rho_{2}=\\Theta\\Big(\\frac{c_{\\mathcal{G}}k^{1-\\alpha}}{\\alpha(1-\\alpha)}\\Big)}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "The proof of Theorem 8 is given in Appendix E. Setting $\\alpha=1-1/(\\log k)$ gives the following: ", "page_idx": 8}, {"type": "text", "text": "Corollary 9. In globally observable partial monitoring with $T\\ \\geq\\ \\tau$ Algorithm $^{\\,l}$ with (17) for $\\alpha=1-1/(\\log k)$ achieves ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}=\\left\\{O\\left(\\frac{c_{g}T}{\\Delta_{\\mathsf{m i n}}^{2}}\\log k\\right)^{1/3}+\\kappa\\right)\\qquad i n\\;a d v e r s a r i a l\\;r e g i m e}\\\\ {\\mathsf{R e g}_{T}=\\left\\{O\\left(\\frac{c_{g}^{2}\\log k}{\\Delta_{\\mathsf{m i n}}^{2}}\\log\\left(T\\Delta_{\\mathsf{m i n}}^{2}\\right)+\\left(\\frac{C^{2}c_{g}^{2}\\log k}{\\Delta_{\\mathsf{m i n}}^{2}}\\log\\left(\\frac{T\\Delta_{\\mathsf{m i n}}}{C}\\right)\\right)^{1/3}+\\kappa^{\\prime}\\right)\\right.}\\\\ {\\qquad\\qquad\\qquad\\left.i n\\;a d v e r s a r i a l\\;r e g i m e\\;w i t h\\;a\\left(\\Delta,C,T\\right)\\!\\!-\\!s e l f\\!\\cdot\\!b o u n d i n g\\;e f i m e\\;f o r\\;s\\;.\\right\\}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Here, $i f$ weuse $\\beta_{1}\\,=\\,64c_{\\mathcal{G}}^{2}/(1-\\alpha),$ which satisfies (17), $\\kappa\\,=\\,O(c_{\\mathcal{G}}^{2}\\log k+k^{3/2}(\\log k)^{5/2})$ and $\\begin{array}{r}{\\kappa^{\\prime}=\\kappa+O((c_{\\mathcal{G}}^{2/3}(\\log k)^{1/3}+\\sqrt{c_{\\mathcal{G}}\\log k})(\\frac{1}{\\Delta_{\\operatorname*{min}}^{2}}+\\frac{C}{\\Delta_{\\operatorname*{min}}})^{2/3}).}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "This regret upper bound is better than the bound based on FTRL in [54, 56] in both stochastic and adversarial regimes, notably by a factor of $\\log T$ or $k$ in the stochastic regime. The bound for the adversarial regime with a $(\\bar{\\Delta},\\bar{C},T)$ -self-bounding constraint is the first MS-type bound in PM. The upper bounds for the adversarial regime and stochastic regime are optimal in terms of $T$ [9, 30]; however, even without considering BOBW guarantees, the optimality with respect to other variables $k,m$ , and $d$ is unclear $(c f.$ , [36, Section 37.9]), and exploring this is an important direction for future work. As discussed in Section 1, employing the black-box reduction approach in [15] also allows us to achieve an upper bound of the same order as our upper bound. Nevertheless, as previously mentioned, the blackbox approach is a complicated approach involving multi-stage reductions and has the drawback of discarding past observations, similar to the doubling-trick. Hence, demonstrating that using the FTRL framework alone can achieve the same upper bound is a significant theoretical advancement. ", "page_idx": 8}, {"type": "text", "text": "6 Case study (2): Graph bandits with weak observability ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section presents a new BOBW algorithm for weakly observable graph bandits. ", "page_idx": 8}, {"type": "text", "text": "6.1 Problem setting and some concepts in graph bandits ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Problem setting  In the graph bandit problem, the learner is given a directed feedback graph $G=$ $(V,E)$ with $V\\,=\\,[k]$ and $E\\subseteq V\\times V$ . For each $i\\in V$ , let $N^{\\mathsf{i n}}(i)\\,=\\,\\{j\\in V\\colon(j,i)^{-}\\!\\in\\,\\mathbf{\\bar{\\calE}}\\}$ and $N^{\\mathsf{o u t}}(i)\\;=\\;\\{j\\in\\bar{V}\\colon(i,j)\\in E\\}$ be the in-neighborhood and out-neighborhood of vertex $i\\in V$ respectively. The game proceeds as the general online learning framework provided in Section 2, with action set ${\\mathcal{A}}=V$ , loss function $\\ell_{t}\\colon V\\to[0,1]$ ,andobservation $o_{t}=\\{\\bar{\\ell}_{t}(j)\\colon j\\in N^{\\mathrm{out}}(I_{t})\\}$ ", "page_idx": 8}, {"type": "text", "text": "Observability and domination number Similar to partial monitoring, the minimax regret of graph bandits is characterized by the properties of the feedback graph $G$ [4]. A graph $G$ is $o b$ servable if it contains no self-loops, $N^{\\mathsf{i n}}(i)\\neq\\emptyset$ for all $i\\in V$ . A graph $G$ is strongly observable if $i\\in N^{\\mathsf{i n}}(i)$ or $V\\backslash\\{i\\}\\subseteq N^{\\mathsf{i n}}(i)$ for all $i\\in V$ . Then, a graph $G$ is weakly observable if it is observable but not strongly observable.5 The minimax regret of the weakly observable is known to be $\\Theta(T^{2/3})$ ", "page_idx": 8}, {"type": "text", "text": "The weak domination number characterizes precisely the minimax regret. The weakly dominating set $D\\subseteq V$ is a set of vertices such that $\\{i\\stackrel{.}{\\in}V\\colon i\\stackrel{.}{\\not\\in}N^{\\mathsf{o u t}}(i)\\}\\subseteq\\bigcup_{i\\in D}^{}N^{\\mathsf{o u t}}(i)$ . Then, the weak domination number $\\delta(G)$ of graph $G$ is the size of the smallest weakly dominating set. For weakly observable $G$ , the minimax regret of $\\tilde{\\Theta}(\\delta^{1/3}T^{2/3})$ is known [4]. Instead, our bound depends on the fractionaldominationnumber $\\delta^{*}(G)$ , defined by the optimal value of the following linear program: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{minimize~}\\sum_{i\\in V}x_{i}\\quad\\mathrm{subject~to}\\quad\\sum_{i\\in N^{\\mathrm{in}}(j)}x_{i}\\geq1\\;\\forall j\\in V\\;,\\;0\\leq x_{i}\\leq1\\;\\forall i\\in V\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Weuse $(x_{i}^{*})_{i\\in V}$ to denote the optimal solution of (19) and define its normalized version $u\\in\\mathcal P_{k}$ by $u_{i}=x_{i}^{*}/\\sum_{j\\in V}x_{j}^{*}$ . The advantage of using the fractional domination number mainly lies i its computational complexity; further details are provided in Appendix F.1. ", "page_idx": 8}, {"type": "text", "text": "Here, we present a new BOBW algorithm based on Algorithm 1. We use the following parameters for Algorithm 1.We use the estimator $\\widehat{\\ell}_{t}\\,\\in\\,\\mathbb{R}^{k}$ defined by $\\begin{array}{r}{\\widehat{\\ell}_{t i}\\;=\\;\\frac{\\ell_{t i}}{P_{t i}}\\mathbb{1}[i\\in N^{\\sf o u t}(I_{t})]}\\end{array}$ for $P_{t i}\\;=\\;$ $\\textstyle\\sum_{j\\in N^{\\mathsf{i n}}(i)}p_{t j}$ which is unbiased and has been employed in the literature [4, 13]. We set $p_{0}$ in (11) 10 $p_{0}=u$ . For $\\tilde{I}_{t}\\in\\arg\\operatorname*{max}_{i\\in[k]}q_{t i}$ and $q_{t*}=\\operatorname*{min}\\{q_{t\\tilde{I}_{t}},1-q_{t\\tilde{I}_{t}}\\}$ let ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\beta_{1}\\geq\\frac{64\\delta^{*}}{1-\\alpha}\\,,\\,\\bar{\\beta}=\\frac{32\\sqrt{k\\delta^{*}}}{(1-\\alpha)^{2}\\sqrt{\\beta_{1}}}\\,,\\,z_{t}\\frac{4\\delta^{*}}{1-\\alpha}\\left(\\sum_{i\\in V\\backslash\\{\\tilde{I}_{t}\\}}q_{t i}^{2-\\alpha}+q_{t*}^{2-\\alpha}\\right)\\,,\\,u_{t}=\\frac{8\\delta^{*}}{1-\\alpha}q_{t*}^{1-\\alpha}\\,.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Note that $\\begin{array}{r}{z_{\\mathrm{max}}=\\frac{4\\delta^{*}}{1-\\alpha}}\\end{array}$ \uff0c $\\begin{array}{r}{u_{\\mathrm{max}}=\\frac{8\\delta^{*}}{1-\\alpha}}\\end{array}$ and $\\begin{array}{r}{h_{\\operatorname*{max}}=h_{1}=\\frac{1}{\\alpha}k^{1-\\alpha}}\\end{array}$ Then, we can prove the fllowing: Theorem 10. In the weakly observable graph bandit problem, for any $\\alpha~\\in~(0,1)$ Algorithm $^{\\,\\,\\,I}$ with (20) satisfies the assumptions of Theorem 7 with $\\begin{array}{r}{\\rho_{1}=\\rho_{2}=\\Theta\\Big(\\frac{\\delta^{*}k^{1-\\alpha}}{\\alpha(1-\\alpha)}\\Big)}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "The proof of Theorem 10 is given in Appendix F. Setting $\\alpha=1-1/(\\log k)$ gives the following: Corollary 11. In weakly observable graph bandits with $T\\geq\\operatorname*{max}\\{\\delta^{*}(\\log k)^{2},\\tau\\}$ Algorithm $^{\\,l}$ with (20)for $\\dot{\\alpha}=1\\!-\\!1/(\\log\\dot{k})$ achieves ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}=\\left\\{O\\left(\\frac{\\delta^{*1/3}T^{2/3}(\\log k)^{1/3}+\\kappa}{\\Delta_{\\operatorname*{min}}^{*}}\\right)+\\left(\\frac{C^{2}\\delta^{*}\\log k}{\\Delta_{\\operatorname*{min}}^{2}}\\log\\left(\\frac{T\\Delta_{\\operatorname*{min}}}{C}\\right)\\right)^{1/3}+\\kappa^{\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Here, if we use $\\beta_{1}\\,=\\,64\\delta^{*}/(1\\,-\\,\\alpha),$ which satisfies (20), $\\kappa\\,=\\,O(\\delta^{*}\\log k+k^{3/2}(\\log k)^{5/2})$ and $\\begin{array}{r}{\\kappa^{\\prime}=\\kappa+O(((\\delta^{*}\\log k)^{1/3}+\\sqrt{\\delta^{*}\\log k})(\\frac{1}{\\Delta_{\\operatorname*{min}}^{2}}+\\frac{C}{\\Delta_{\\operatorname*{min}}})^{2/3})}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "Our bound is the first BOBW FTRL-based algorithm with the ${\\cal O}(\\log T)$ bound in the stochastic regime, improving the existing best FTRL-based algorithm in [25]. Compared to the reduction-based approach in [15], the dependences on $T$ are the same. However, our bound unfortunately depends on the fractional domination number $\\delta^{*}$ instead of the weak domination number $\\delta$ , which can be smaller than $\\delta^{*}$ . Roughly speaking, this comes from the use of Tsallis entropy instead of Shannon entropy employed for the existing BOBW bound [25]. The technical challenges of making our bound depend on $\\delta$ instead of $\\delta^{*}$ or the weak fractional domination number $\\tilde{\\delta}^{*}$ are further discussed in Appendix F.3. Still, we believe that our algorithm can perform better since the reduction-based algorithm discards past observations as the doubling trick. Furthermore, the bound for the adversarial regime with a $(\\Delta,C,T)$ -self-bounding constraint is the first MS-type bound in weakly observable graph bandits. ", "page_idx": 9}, {"type": "text", "text": "7   Conclusion and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we investigated hard online learning problems, that is online learning with a minimax regret of $\\Theta(T^{2/3})$ , and established a simple and adaptive learning rate framework called stability-- penalty-bias matching (SPB-matching). The SPB-matching allows us to prove a regret bound of $\\textstyle\\left(\\sum_{t=1}^{T}{\\sqrt{z_{t}{\\widehat{h}}_{t+1}\\log T}}\\right)^{1/3}$ for the stability component $z_{t}$ and the penalty component $\\widehat{h}_{t+1}$ ,which differs from the existing stability-penalty-adaptive-type bounds for problems with a minimax regret of $\\Theta({\\sqrt{T}})$ [26, 55]. We showed that FTRL with the SPB-matching learning rate and the Tsallis entropy regularizer improves the existing BOBW regret bounds based on FTRL for two typical hard problems with indirect feedback, partial monitoring with global observability, graph bandits with weak observability. We also showed that the SPB-matching can be applied to derive the first BOBW regret bounds for multi-armed bandits with paid observations. ", "page_idx": 9}, {"type": "text", "text": "Interestingly, the optimal exponent of Tsallis entropy in these settings is $1\\!-\\!1/(\\log k)$ ,suggestingthe reasonableness of using Shannon entropy in existing algorithms for partial monitoring [37] and graph bandits [4]. Our learning rate is surprisingly simple compared to existing ones for hard problems [25, 54]. Hence, it is important future work to investigate whether this simplicity can be leveraged to apply SPB-matching to other hard problems, such as bandits with switching costs [18] and dueling bandits with Borda winner [51]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors are grateful to the anonymous reviewers for their insightful feedback and constructive suggestions, which have helped to significantly improve the manuscript. TT was supported by JST ACT-X Grant Number JPMJAX210E and JSPS KAKENHI Grant Number JP24K23852. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In The 21st Annual Conference on Learning Theory, pages 263-274, 2008. [2]  Jacob D. Abernethy, Elad Hazan, and Alexander Rakhlin. Interior-point methods for fullinformation and bandit online learning. IEEE Transactions on Information Theory, 58(7): 4164-4175, 2012.   \n[3] Jacob D Abernethy, Chansoo Lee, and Ambuj Tewari. Fighting bandits with a new kind of smoothness. In Advances in Neural Information Processing Systems, volume 28, pages 2197- 2205, 2015.   \n[4]  Noga Alon, Nicolo Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback graphs: Beyond bandits. In Proceedings of The 28th Conference on Learning Theory, volume 40, pages 23-35, 2015. [5]  Jean-Yves Audibert and Sebastien Bubeck.  Minimax policies for adversarial and stochastic bandits. In Conference on Learning Theory, volume 7, pages 1-122, 2009. [6]  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397-422, 2002. [7]  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits. In 29th Annual Conference on Learning Theory, volume 49, pages 116-120, 2016. [8]  Peter Auer, Nicol6 Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.   \n[9] Gabor Bartok, David Pal, and Csaba Szepesvari. Minimax regret of finite partial-monitoring games in stochastic environments. In Proceedings of the 24th Annual Conference on Learning Theory, volume 19, pages 133-154, 2011.   \n[10] Sebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial bandits. In Proceedings of the 25th Annual Conference on Learning Theory, volume 23, pages 42.1-42.23, 2012.   \n[11]  Nicolo Cesa-Bianchi, Gabor Lugosi, and Gilles Stoltz. Regret minimization under partial monitoring. Mathematics of Operations Research, 31(3):562-580, 2006.   \n[12]  Sougata Chaudhuri and Ambuj Tewari. Online learning to rank with top-k feedback. Journal of Machine Learning Research, 18(103):1-50, 2017.   \n[13] Houshuang Chen, zengfeng Huang, Shuai Li, and Chihao Zhang. Understanding bandits with graph feedback. In Advances in Neural Information Processing Systems, volume 34, pages 24659-24669, 2021.   \n[14] Varsha Dani, Thomas P. Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. In The 21st Annual Conference on Learning Theory, volume 2, pages 355- 366, 2008.   \n[15]  Chris Dann, Chen-Yu Wei, and Julian Zimmert. A blackbox approach to best of both worlds in bandits and beyond. In Proceedings of Thirty Sixth Conference on Learning Theory, volume 195, pages 5503-5570, 2023.   \n[16] Steven de Rooij, Tim van Erven, Peter D. Gruinwald, and Wouter M. Koolen. Follow the leader if you can, hedge if you must. Journal of Machine Learning Research, 15(37):1281-1316, 2014.   \n[17] Ofer Dekel, Ambuj Tewari, and Raman Arora. Online bandit learning against an adaptive adversary: from regret to policy regret. In Proceedings of the 29th International Conference on Machine Learning, pages 1747-1754, 2012.   \n[18] Ofer Dekel, Jjian Ding, Tomer Koren, and Yuval Peres. Bandits with switching costs: $T^{2/3}$ regret. In Procedings of the Forty-Sixth Annual ACM Symposium on Theory of Computing, pages 459-467, 2014.   \n[19] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(61):2121-2159, 2011.   \n[20] Liad Erez and Tomer Koren. Towards best-of-all-worlds online learning with feedback graphs. In Advances in Neural Information Processing Systems, volume 34, pages 28511-28521, 2021.   \n[21]  Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119-139, 1997.   \n[22] Pierre Gaillard, Gilles Stoltz, and Tim van Erven. A second-order bound with excess losses. In Proceedings of The 27th Conference on Learning Theory, volume 35, pages 176-196, 2014.   \n[23] Pratik Gajane and Tanguy Urvoy. Utility-based dueling bandits as a partial monitoring game. arXiv preprint arXiv:1507.02750, 2015.   \n[24] Shinji Ito. Parameter-free multi-armed bandit algorithms with hybrid data-dependent regret bounds. In Proceedings of Thirty Fourth Conference on Learning Theory, volume 134, pages 2552-2583, 2021.   \n[25]  Shinji Ito, Taira Tsuchiya, and Junya Honda. Nearly optimal best-of-both-worlds algorithms for online learning with feedback graphs. In Advances in Neural Information Processing Systems, volume 35, pages 28631-28643, 2022.   \n[26]  Shinji Ito, Taira Tsuchiya, and Junya Honda. Adaptive learning rate for follow-the-regularizedleader: Competitive analysis and best-of-both-worlds. In Proceedings of Thirty Seventh Conference on Learning Theory, volume 247, pages 2522-2563, 2024.   \n[27] Tiancheng Jin, Longbo Huang, and Haipeng Luo. The best of both worlds: Stochastic and adversarial episodic MDPs with unknown transition. In Advances in Neural Information Processing Systems, volume 34, pages 20491-20502, 2021.   \n[28]  Tiancheng Jin, Junyan Liu, and Haipeng Luo. Improved best-of-both-worlds guarantees for multi-armed bandits: FTRL with general regularizers and multiple optimal arms. In Advances in Neural Information Processing Systems, volume 36, pages 30918-30978, 2023.   \n[29] Robert Kleinberg and Tom Leighton. The value of knowing a demand curve: Bounds on regret for online posted-price auctions. In the 44th Annual IEEE Symposium on Foundations of Computer Science, pages 594-605, 2003.   \n[30] Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Regret lower bound and optimal algorithm in finite stochastic partialmonitoring. InAdvances in Neural Information Processing Systems, volume 28, pages 1792-1800, 2015.   \n[31] Fang Kong, Yichi Zhou, and Shuai Li. Simultaneously learning stochastic and adversarial bandits with general graph feedback. In Proceedings ofthe 39th International Conference on Machine Learning, volume 162, pages 11473-11482, 2022.   \n[32]  Joon Kwon and Vianney Perchet. Gains and losses are fundamentally different in regret minimization: The sparse case. Journal of Machine Learning Research, 17(227):1-32, 2016.   \n[33]  T. L. Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4-22, 1985.   \n[34]  Tor Lattimore and Csaba Szepesvari. Cleaning up the neighborhood: A full classification for adversarial partial monitoring. In Proceedings of the 30th International Conference on Algorithmic Learning Theory, volume 98, pages 529-556, 2019.   \n[35] Tor Latimore and Csaba Szepesvari. An information-theoretic approach to minimax regret in partial monitoring. In the 32nd Annual Conference on Learning Theory, volume 99, pages 2111-2139, 2019.   \n[36]  Tor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020.   \n[37]  Tor Latimore and Csaba Szepesvari. Exploration by optimisation in partial monitoring. In Proceedings of Thirty Third Conference on Learning Theory, volume 125, pages 2488-2515, 2020.   \n[38]  Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. Information and computation, 108(2):212-261, 1994.   \n[39] Haipeng Luo and Robert E. Schapire. Achieving all with no parameters: AdaNormalHedge. In Proceedings of The 28th Conference on Learning Theory, volume 40, pages 1286-1304, 2015.   \n[40] Haipeng Luo, Chen-Yu Wei, and Kai Zheng. Efficient online portfolio with logarithmic regret. In Advances in Neural Information Processing Systems, volume 31, pages 8235-8245, 2018.   \n[41]  Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial corruptions. In Proceedings of the 5Oth Annual ACM SIGACT Symposium on Theory of Computing, pages 114-122, 2018.   \n[42]  Shie Mannor and Ohad Shamir. From bandits to experts: On the value of side-observations. In Advances in Neural Information Processing Systems, volume 24, pages 684-692, 2011.   \n[43]  Saeed Masoudian and Yevgeny Seldin.  Improved analysis of the Tsallis-INF algorithm in stochasticaly constrained adversarial bandits and stochastic bandits with adversarial corruptions. In Proceedings of Thirty Fourth Conference on Learning Theory, volume 134, pages 3330-3350, 2021.   \n[44] H. Brendan McMahan and Matthew J. Streeter. Adaptive bound optimization for online convex optimization. In The 23rd Conference on Learning Theory, pages 244-256, 2010.   \n[45] Francesco Orabonaoio A modern introduction to online learning. arXiv preprint arXiv:1912.13213, 2019.   \n[46]  Aldo Pacchiano, Christoph Dann, and Claudio Gentile. Best of both worlds model selection. In Advances in Neural Information Processing Systems, volume 35, pages 1883-1895, 2022.   \n[47]  Antonio Piccolboni and Christian Schindelhauer. Discrete prediction games with arbitrary feedback and loss (extended abstract). In Computational Learning Theory, pages 208-223, 2001.   \n[48]  Chloe Rouyer and Yevgeny Seldin. Tsalls-INF for decoupled exploration and exploitation in multi-armed bandits. In Proceedings of Thirty Third Conference on Learning Theory, volume 125, pages 3227-3249, 2020.   \n[49]  Chloe Rouyer, Dirk van der Hoeven, Nicolo Cesa-Bianchi, and Yevgeny Seldin. A near-optimal best-of-both-worlds algorithm for online learning with feedback graphs. In Advances in Neural Information Processing Systems, volume 35, pages 35035-35048, 2022.   \n[50]  Aldo Rustichini. Minimizing regret: The general case. Games and Economic Behavior, 29(1): 224-243, 1999.   \n[51]  Aadirupa Saha, Tomer Koren, and Yishay Mansour. Adversarial dueling bandits. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 9235-9244, 2021.   \n[52]  Yvgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial bandits. InProceedings of the 31st International Conference on Machine Learning, volume 32, pages 1287-1295, 2014.   \n[53] Yevgeny Seldin, Peter Bartlett, Koby Crammer, and Yasin Abbasi-Yadkori. Prediction with limited advice and multiarmed bandits with paid observations. In Proceedings of the 31st International Conference on Machine Learning, volume 32, pages 280-287, 2014.   \n[54] Taira Tsuchiya, Shinji Ito, and Junya Honda. Best-of-both-worlds algorithms for partial monitoring. In Proceedings of The 34th International Conference on Algorithmic Learning Theory, pages 1484-1515, 2023.   \n[55]  Taira Tsuchiya, Shinji Ito, and Junya Honda. Stability-penalty-adaptive follow-the-regularizedleader: Sparsity, game-dependency, and best-of-both-worlds. In Advances in Neural Information Processing Systems, volume 36, pages 47406-47437, 2023.   \n[56]  Taira Tsuchiya, Shinji Ito, and Junya Honda. Exploration by optimization with hybrid regularizers: Logarithmic regret with adversarial robustness in partial monitoring. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pages 48768-48790, 2024.   \n[57]  Vladimir Vovk.  Aggregating strategies. In Proceedings of the Third Annual Workshop on Computational Learning Theory, pages 371-383, 1990.   \n[58]  Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In Proceedings of the 31st Conference On Learning Theory, volume 75, pages 1263-1291, 2018.   \n[59]  Julian Zimmert and Tor Lattimore. Connections between mirror descent, thompson sampling and the information ratio. In Advances in Neural Information Processing Systems, pages 11973- 11982,2019.   \n[60]  Julian Zimmert and Yevgeny Seldin. An optimal algorithm for stochastic and adversarial bandits. In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89, pages 467-475, 2019.   \n[61]  Julian Zimmert and Yevgeny Seldin.  Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28): 1-49, 2021.   \n[62] Julian Zimmert, Haipeng Luo, and Chen-Yu Wei. Beating stochastic and adversarial semibandits optimally and simultaneously. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 7683-7692, 2019.   \n[63]  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In the Twentieth International Conference on Machine Learning, pages 928-935, 2003. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Additional related work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Best-of-both-worlds algorithms The study of BOBW algorithms was initiated by Bubeck and Slivkins [1o], who focused on multi-armed bandits. The motivation arises from the difficulty of determining in advance whether the underlying environment is stochastic or adversarial in real-world problems. Since then, BOBW algorithms have been extensively studied [7, 16, 22, 40, 46, 52], and recently, FTRL is the common approach for developing BOBW algorithms [24, 28, 60, 62]. One reason is by appropriately designing the learning rate and regularizer of FTRL, we can prove a BOBW guarantee for various problem settings. Another reason is that FTRL-based approaches not only perform well in both stochastic and adversarial regimes but also achieve favorable regret bounds in the adversarial regime with a self-bounding constraint, intermediate settings including stochastically constrained adversarial regime [58] and stochastic regime with adversarial corruptions [41]. This intermediate regime is particularly useful, considering that real-world problems often lie between purely stochastic and purely adversarial regimes. ", "page_idx": 14}, {"type": "text", "text": "This study is closely related to FTRL with the Tsallis entropy regularization. Tsallis entropy in online learning was introduced in [3, 5], and its significance for BOBW algorithms was established in [61]. In the multi-armed bandit problem, using the exponent of Tsallis entropy $\\alpha=1/2$ provides optimal upper bounds, up to logarithmic factors, in both stochastic and adversarial regimes [61]. However, in the graph bandits, where the dependence on $k$ is critical or in decoupled settings, optimal upper bounds can be achieved with $\\alpha\\neq\\Bar{1}/2$ [26, 32, 48, 59]. In this work, we demonstrate that using the exponent tofo $\\alpha=1-1/(\\log k)$ for the number of actions $k$ results in favorable regret bounds, as shown in Corollaries 9 and 11. ", "page_idx": 14}, {"type": "text", "text": "Partial monitoring  Partial monitoring [11, 47, 50] is a very general online decision-making framework and includes a wide range of problems such as multi-armed bandits, (utility-based) dueling bandits [23], online ranking [12], and dynamic pricing [29]. The characterization of the minimax regret in partial monitoring has been progressively understood through various studies. It is known that all partial monitoring games can be classified into trivial, easy, hard, and hopeless games, where their minimax regrets are 0, $\\Theta({\\sqrt{T}})$ \uff0c $\\Theta(T^{2/3})$ and $\\Omega(T)$ . For comprehensive literature, refer to [9] and the improved results presented in [34, 35]. The games for which we can achieve a regret bound of $O(T^{2/3})$ correspond to globally observable games. ", "page_idx": 14}, {"type": "text", "text": "There is limited research on BOBW algorithms for partial monitoring with global observability [54, 56]. The existing bounds exhibit suboptimal dependencies on $k$ and $T$ , particularly in the stochastic regime, which comes from the use of the Shannon entropy or the log-barrier regularization. By employing Tsallis entropy, our algorithm is the first to achieve ideal dependencies on both $k$ and $T$ It remains uncertain whether our upper bound in the stochastic regime is optimal with respect to variables other than $T$ . While there is an asymptotic lower bound for the stochastic regime [30], its coefficient is expressed as a complex optimization problem. Investigating this lower bound further is important future work. ", "page_idx": 14}, {"type": "text", "text": "Graph bandits The study on the graph bandit problem, which is also known as online learning with feedback graphs, was initiated by [42]. This problem includes several important problems such as the expert setting, multi-armed bandits, and label-efficient prediction. For example, considering a feedback graph with only self-loops, one can see that this corresponds to the multi-armed bandit problem. One of the most seminal studies on the graph bandit problem is by Alon et al. [4], who elucidated how the structure of the feedback graph influences its minimax regret. They demonstrated that the minimax regret is characterized by the observability of the feedback graph, introducing the notions of weakly observable graphs and strongly observable graphs. Of particular relevance to this study is the minimax regret of $\\tilde{O}(\\delta T^{2/3})$ for weakly observable graphs, where $\\delta$ is the weak domination number and $\\bar{O}(\\bar{\\cdot})$ ignores logarithmic factors. Recently, this upper bound was improved to $\\tilde{O}(\\delta^{*}T^{2/3})$ by replacing the weak domination number with the fractional weak domination number $\\tilde{\\delta}^{*}$ [13]. ", "page_idx": 14}, {"type": "text", "text": "There are several BOBW algorithms for graph bandits [15, 20, 25, 31, 49]. However, only a few of these studies consider the weakly observable setting [15, 25, 31]. The existing results based on FTRL rely on the domination number rather than the weak domination number [31] or exhibit poor dependenceon $T$ [25, 31], and the best regret bound of them still exhibited a dependence on $T$ Oof $(\\log T)^{2}$ [25]. Our algorithm is the first FTRL-based algorithm in the weakly observable setting that achieves an ${\\cal O}(\\log T)$ stochastic bound. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "BProofs for SPB-matching learning rate (Section 3) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Lemma 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 4. We first consider Rule 1 in (6). The learning rate $\\beta_{t}$ is lower-bounded as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta_{t}^{3/2}\\geq\\beta_{t}^{1/2}\\bigg(\\beta_{t-1}+\\frac{2}{\\widehat{h_{t}}}\\sqrt{\\frac{z_{t}}{\\beta_{t}}}\\bigg)\\geq\\beta_{t-1}^{3/2}+\\frac{2\\sqrt{z_{t}}}{\\widehat{h}_{t}}\\geq2\\sum_{s=1}^{t}\\frac{\\sqrt{z_{s}}}{\\widehat{h}_{s}}\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first inequality follows from the definition of $\\beta_{t}$ in (6) and the second inequality from the fact that $(\\beta_{t})_{t}$ is non-decreasing. We also have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta_{t}^{2}\\geq\\beta_{t}\\bigg(\\beta_{t-1}+\\frac{1}{\\widehat{h}_{t}}\\frac{u_{t}}{\\beta_{t}}\\bigg)\\geq\\beta_{t-1}^{3/2}+\\frac{u_{t}}{\\widehat{h}_{t}}\\geq\\sum_{s=1}^{t}\\frac{u_{s}}{\\widehat{h}_{s}}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the last two lower bounds on $\\beta_{t}$ , we can bound $F$ in (5) as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\beta_{1:T},z_{1:T},u_{1:T},h_{1:T})\\le\\displaystyle\\sum_{t=1}^{T}\\biggl(2\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\frac{u_{t}}{\\beta_{t}}+(\\beta_{t}-\\beta_{t-1})\\hat{h}_{t}\\biggr)}\\\\ &{\\qquad\\qquad\\le\\displaystyle\\sum_{t=1}^{T}\\biggl(4\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+2\\frac{u_{t}}{\\beta_{t}}\\biggr)}\\\\ &{\\qquad\\qquad\\le4\\displaystyle\\sum_{t=1}^{T}\\sqrt{\\frac{z_{t}}{\\left(2\\sum_{s=1}^{t}\\sqrt{z_{s}}\\sqrt{\\hat{h}_{s}}\\right)^{1/3}}}+2\\displaystyle\\sum_{t=1}^{T}\\frac{u_{t}}{\\sqrt{\\sum_{s=1}^{t}u_{t}/\\hat{h}_{t}}}}\\\\ &{\\qquad=3.2G_{1}(z_{1:T},\\widehat{h}_{1:T})+2G_{2}(u_{1:T},\\widehat{h}_{1:T})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the secoind inequality follows from the definition of $\\beta_{t}$ in (6) and the third inequality from (22) and (23). This completes the proof of the first statement in Lemma 4. ", "page_idx": 15}, {"type": "text", "text": "We next consider Rule 2 in (6). In this case, we can bound $F$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{F(\\beta_{1:T},z_{1:T},u_{1:T},h_{1:T})\\leq2\\sqrt{\\frac{z_{1}}{\\beta_{1}}}+\\frac{u_{1}}{\\beta_{1}}+\\beta_{1}h_{1}+\\sum_{t=2}^{T}\\bigg(2\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\frac{u_{t}}{\\beta_{t}}+(\\beta_{t}-\\beta_{t-1})\\widehat{h}_{t}\\bigg)}}\\\\ &{}&{\\quad=2\\sqrt{\\frac{z_{1}}{\\beta_{1}}}+\\frac{u_{1}}{\\beta_{1}}+\\beta_{1}h_{1}+\\displaystyle\\sum_{t=2}^{T}\\bigg(2\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\frac{u_{t}}{\\beta_{t}}+2\\sqrt{\\frac{z_{t-1}}{\\beta_{t-1}}}+\\frac{u_{t-1}}{\\beta_{t-1}}\\bigg)}\\\\ &{}&{\\quad\\leq\\beta_{1}h_{1}+\\displaystyle\\sum_{t=1}^{T}\\bigg(4\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+2\\frac{u_{t}}{\\beta_{t}}\\bigg)\\:,\\qquad\\qquad\\qquad\\qquad\\:(2\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\frac{u_{t}}{\\beta_{t}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the equality follows from (6). ", "page_idx": 15}, {"type": "text", "text": "We then first consider bounding $\\textstyle\\sum_{t=1}^{T}{\\sqrt{z_{t}/\\beta_{t}}}$ . We can lower-bound $\\beta_{t}^{3/2}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta_{t}^{3/2}\\geq\\beta_{t}^{1/2}\\bigg(\\beta_{t-1}+\\frac{2}{\\widehat{h}_{t}}\\sqrt{\\frac{z_{t-1}}{\\beta_{t-1}}}\\bigg)\\geq\\beta_{t-1}^{3/2}+\\frac{2\\sqrt{z_{t-1}}}{\\widehat{h}_{t}}\\geq\\beta_{1}^{3/2}+2\\sum_{s=2}^{t}\\frac{\\sqrt{z_{s-1}}}{\\widehat{h}_{s}}=:\\big(\\beta_{t}^{(1)}\\big)^{3/2}\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta_{t}^{(1)}=\\left(\\beta_{1}^{3/2}+2\\sum_{s=2}^{t}\\frac{\\sqrt{z_{s-1}}}{\\widehat{h}_{s}}\\right)^{2/3}=\\left(\\beta_{1}^{3/2}+2\\sum_{s=1}^{t-1}\\frac{\\sqrt{z_{s}}}{\\widehat{h}_{s+1}}\\right)^{2/3}\\leq\\beta_{t}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Inthefonwewill $\\begin{array}{r}{\\sum_{t=1}^{T}\\sqrt{z_{t}/\\beta_{t}}\\le\\sum_{t=1}^{T}\\sqrt{z_{t}/\\beta_{t}^{(1)}}}\\end{array}$ Let $c=(1\\!+\\!\\delta)^{2}$ $\\delta>0$ and and we then defne $S\\,=\\,\\{t\\,\\in\\,[T]\\colon\\beta_{t+1}^{(1)}\\,\\le\\,c^{2}\\beta_{t}^{(1)}\\}$ and $\\mathcal{S}^{\\mathtt{c}}=[T]\\setminus\\mathcal{S}\\mathrm{~=~}\\{t\\in[T]\\colon\\beta_{t+1}^{(1)}>\\$ $c^{2}\\beta_{t}^{(1)}\\}$ . From these definitions, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t\\in S^{\\mathrm{c}}}\\sqrt{\\frac{z_{t}}{\\beta_{t}^{(1)}}}\\leq\\sum_{t\\in S^{\\mathrm{c}}}\\sqrt{\\frac{z_{\\operatorname*{max}}}{\\beta_{t}^{(1)}}}\\leq\\sum_{s=0}^{\\infty}\\biggl(\\frac{1}{c}\\biggr)^{s}\\sqrt{\\frac{z_{\\operatorname*{max}}}{\\beta_{1}}}\\leq\\frac{1}{1-1/c}\\sqrt{\\frac{z_{\\operatorname*{max}}}{\\beta_{1}}}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, using the last inequality, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\sqrt{\\frac{z_{t}}{\\beta_{t}}}\\leq\\sum_{t\\in\\mathcal{S}}\\sqrt{\\frac{z_{t}}{\\beta_{t}^{(1)}}}+\\sum_{t\\in\\mathcal{S}}\\sqrt{\\frac{z_{t}}{\\beta_{t}^{(1)}}}}\\\\ &{\\leq c\\sum_{t\\in\\mathcal{S}}\\sqrt{\\frac{z_{t}}{\\beta_{t+1}^{(1)}}}+\\frac{1}{1-1/c}\\sqrt{\\frac{z_{\\operatorname*{max}}}{\\beta_{1}}}}\\\\ &{\\leq c\\sum_{t\\in\\mathcal{S}}\\sqrt{\\frac{z_{t}}{\\left(2\\sum_{s=1}^{t}\\sqrt{z_{s}}/\\hat{h}_{s+1}\\right)^{2/3}}}+\\frac{1}{1-1/c}\\sqrt{\\frac{z_{\\operatorname*{max}}}{\\beta_{1}}}}\\\\ &{=\\frac{c}{2^{1/3}}G_{1}(z_{1:T},\\hat{h}_{2:T+1})+\\frac{c}{c-1}\\sqrt{\\frac{z_{\\operatorname*{max}}}{\\beta_{1}}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the third inequality follows from the definition of $\\beta^{(1)}$ in (26). ", "page_idx": 16}, {"type": "text", "text": "We next bound $\\textstyle\\sum_{t=1}^{T}u_{t}/\\beta_{t}$ . We can lower-bound $\\beta_{t}^{2}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\beta_{t}^{2}\\geq\\beta_{t}\\bigg(\\beta_{t-1}+\\frac{1}{\\widehat{h}_{t}}\\frac{u_{t-1}}{\\beta_{t-1}}\\bigg)\\geq\\beta_{t-1}^{2}+\\frac{u_{t-1}}{\\widehat{h}_{t}}\\geq\\beta_{1}^{2}+\\sum_{s=2}^{t}\\frac{u_{s-1}}{\\widehat{h}_{s}}=:\\left(\\beta_{t}^{(2)}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\beta_{t}^{(2)}=\\sqrt{\\beta_{1}^{2}+\\sum_{s=2}^{t}\\frac{u_{s-1}}{\\widehat h_{s}}}=\\sqrt{\\beta_{1}^{2}+\\sum_{s=1}^{t-1}\\frac{u_{s}}{\\widehat h_{s+1}}}\\le\\beta_{t}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the followin,wewilluerbod $\\begin{array}{r l r}{\\sum_{t=1}^{T}u_{t}/\\beta_{t}}&{{}\\le}&{\\sum_{t=1}^{T}u_{t}/\\beta_{t}^{(2)}}\\end{array}$ Let us defne $\\tau\\ =$ $\\left\\{t\\in[T]\\colon\\beta_{t+1}^{(2)}\\leq c\\beta_{t}^{(2)}\\right\\}$ and $\\mathcal{T}^{\\mathtt{c}}\\,=\\,[T]\\setminus\\mathcal{T}\\,=\\,\\big\\{t\\in[T]\\colon\\beta_{t+1}^{(2)}>c\\beta_{t}^{(2)}\\big\\}$ . From these definitions, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{T}^{\\mathsf{c}}}\\frac{u_{t}}{\\beta_{t}^{(2)}}\\leq\\sum_{t\\in\\mathcal{T}^{\\mathsf{c}}}\\frac{u_{\\operatorname*{max}}}{\\beta_{t}^{(2)}}\\leq\\sum_{s=0}^{\\infty}\\biggl(\\frac{1}{c}\\biggr)^{s}\\frac{u_{\\operatorname*{max}}}{\\beta_{1}}\\leq\\frac{1}{1-1/c}\\frac{u_{\\operatorname*{max}}}{\\beta_{1}}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, using the last inequality, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\frac{u_{t}}{\\beta_{t}}\\leq\\sum_{t\\in\\mathcal{T}}\\frac{u_{t}}{\\beta_{t}^{(2)}}+\\sum_{t\\in\\mathcal{T}_{\\epsilon}}\\frac{u_{t}}{\\beta_{t}^{(2)}}}}\\\\ &{\\leq c\\sum_{t\\in\\mathcal{T}}\\frac{u_{t}}{\\beta_{t+1}^{(2)}}+\\frac{1}{1-1/c}\\frac{u_{\\operatorname*{max}}}{\\beta_{1}}}\\\\ &{\\leq c\\sum_{t\\in\\mathcal{T}}\\frac{u_{t}}{\\sqrt{\\sum_{s=1}^{t}u_{s}/\\hat{h}_{s+1}}}+\\frac{1}{1-1/c}\\frac{u_{\\operatorname*{max}}}{\\beta_{1}}}\\\\ &{=c G_{2}(u_{1:T},\\hat{h}_{2:T+1})+\\frac{c}{c-1}\\frac{z_{\\operatorname*{max}}}{\\beta_{1}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, combining (25) with (29) and (33), we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\beta_{1:T},z_{1:T},u_{1:T},h_{1:T})\\leq3.2c\\,G_{1}(z_{1:T},\\widehat{h}_{2:T+1})+2c\\,G_{2}(u_{1:T},\\widehat{h}_{2:T+1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\frac{c}{c-1}\\bigg(2\\sqrt{\\frac{z_{\\operatorname*{max}}}{\\beta_{1}}}+\\frac{u_{\\operatorname*{max}}}{\\beta_{1}}\\bigg)+\\beta_{1}h_{1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Setting $c=1.25$ completes the proof. ", "page_idx": 16}, {"type": "text", "text": "B.2Proof of Lemma 5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Before proving Lemma 5, we prepare the following lemma, a variant of [45, Lemma 4.13]. Lemma 12.Let $\\mathcal{T}\\subseteq[T]=\\{1,\\ldots,T\\}$ and $(x_{t})_{t\\in{\\mathcal{T}}}$ be a non-negative sequence. Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t\\in T}\\frac{x_{t}}{\\left(\\sum_{s\\in[t]\\cap T}x_{s}\\right)^{1/3}}\\leq\\frac{3}{2}\\left(\\sum_{t\\in T}x_{t}\\right)^{2/3}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof Let $\\begin{array}{r}{S_{t}=\\sum_{s\\in[t]\\cup T}x_{s}}\\end{array}$ . Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{x_{t}}{\\left(\\sum_{s\\in[t]\\cap\\mathcal{T}}x_{s}\\right)^{1/3}}=\\frac{x_{t}}{S_{t}^{1/3}}=\\int_{S_{t-1}}^{S_{t}}S_{t}^{-1/3}\\mathrm{d}z\\leq\\int_{S_{t-1}}^{S_{t}}z^{-1/3}\\mathrm{d}z=\\frac{3}{2}\\left(S_{t}^{2/3}-S_{t-1}^{2/3}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Summing up the last inequality over $\\tau$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{T}}\\frac{x_{t}}{\\left(\\sum_{s\\in[t]\\cap\\mathcal{T}}x_{s}\\right)^{1/3}}=\\frac{3}{2}\\sum_{t\\in\\mathcal{T}}\\Bigl(S_{t}^{2/3}-S_{t-1}^{2/3}\\Bigr)\\le\\frac{3}{2}S_{T}^{2/3}\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality follows from the telescoping argument with the assumption that $x_{t}\\geq0$ .\u53e3 ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 5. We upper-bound $G_{1}$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle G_{1}(z_{1:T},h_{1:T})=\\sum_{t=1}^{T}\\frac{\\sqrt{z_{t}}}{\\left(\\sum_{s=1}^{t}\\sqrt{z_{s}}/h_{s}\\right)^{1/3}}=\\sum_{j=1}^{J+1}\\sum_{t\\in T_{j}}\\frac{\\sqrt{z_{t}}}{\\left(\\sum_{s=1}^{t}\\sqrt{z_{s}}/h_{s}\\right)^{1/3}}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\qquad\\leq\\sum_{j=1}^{J+1}\\sum_{t\\in T_{j}}\\frac{\\sqrt{z_{t}}}{\\left(\\sum_{s\\in T_{j}\\cap\\{t\\}}\\sqrt{z_{s}}/h_{s}\\right)^{1/3}}\\leq\\sum_{j=1}^{J+1}\\sum_{t\\in T_{j}}\\frac{\\sqrt{z_{t}}}{\\left(\\sum_{s\\in T_{j}\\cap\\{t\\}}\\sqrt{z_{s}}/\\theta_{j-1}\\right)^{1/3}}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\sum_{j=1}^{J+1}\\theta_{j-1}^{1/3}\\sum_{t\\in T_{j}}\\frac{\\sqrt{z_{t}}}{\\left(\\sum_{s\\in T_{j}\\cap\\{t\\}}\\sqrt{z_{s}}\\right)^{1/3}}\\leq\\frac{3}{2}\\sum_{j=1}^{J+1}\\left(\\sqrt{\\theta_{j-1}}\\sum_{t\\in T_{j}}\\sqrt{z_{t}}\\right)^{2/3},\\quad\\mathrm{~(2)~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality follows from Lemma 12. This completes the proof of the first statement in Lemma 5. Setting $J=0$ and $\\theta_{0}=h_{\\operatorname*{max}}$ in (38) yields that ", "page_idx": 17}, {"type": "equation", "text": "$$\nG_{1}(z_{1:T},h_{1:T})\\leq\\frac{3}{2}\\left(\\sum_{t=1}^{T}\\sqrt{z_{t}h_{\\operatorname*{max}}}\\right)^{2/3}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Setting $\\theta_{j}=2^{-j}h_{\\operatorname*{max}}$ for $j\\in\\{0\\}\\cup[J]$ in (38) also gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{1}(z_{1:T},h_{1:T})\\leq\\frac{3}{2}\\displaystyle\\sum_{j=1}^{J+1}\\left(\\sqrt{\\theta_{j-1}}\\sum_{t\\in T_{j}}\\sqrt{z_{t}}\\right)^{2/3}}\\\\ &{\\qquad\\qquad\\leq\\frac{3}{2}\\displaystyle\\sum_{j=1}^{J}\\left(\\sqrt{\\frac{\\theta_{j-1}}{\\theta_{j}}}\\sum_{t\\in T_{j}}\\sqrt{z_{t}h_{t}}\\right)^{2/3}+\\frac{3}{2}\\left(\\sqrt{\\theta_{J}}\\sum_{t\\in T_{j}}\\sqrt{z_{t}}h_{t a x}^{-}\\right)^{2/3}}\\\\ &{\\qquad\\qquad=\\frac{3}{2}\\displaystyle\\sum_{j=1}^{J}\\left(\\sqrt{2}\\sum_{t\\in T_{j}}\\sqrt{z_{t}h_{t}}\\right)^{2/3}+\\frac{3}{2}\\left(2^{-J/2}\\sum_{t\\in T_{j}}\\sqrt{z_{t}h_{\\operatorname*{max}}}\\right)^{2/3}}\\\\ &{\\qquad\\qquad\\leq\\frac{3}{2}\\left(\\sqrt{2J}\\displaystyle\\sum_{j=1}^{J}\\sum_{t\\in T_{j}}\\sqrt{z_{t}h_{t}}\\right)^{2/3}+\\frac{3}{2}\\left(2^{-J/2}\\sum_{t\\in T_{j}}\\sqrt{z_{t}h_{\\operatorname*{max}}}\\right)^{2/3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(Holder's inequality) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\leq\\frac{3}{2}\\Bigg(\\sqrt{2J}\\sum_{t=1}^{T}\\sqrt{z_{t}h_{t}}\\Bigg)^{2/3}+\\frac{3}{2}\\Big(2^{-J/2}\\sqrt{z_{\\mathrm{max}}h_{\\mathrm{max}}}\\Big)^{2/3}T^{2/3}\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second inequality follows from $(x+y)^{2/3}\\leq x^{2/3}+y^{2/3}$ for $x,y\\geq0$ .Combining the last inequality and (39) completes the proof of the second statement in Lemma 5. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C   Proof for best-of-both-worlds analysis in general online learning framework (Theorem 7, Section 4) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section provides the proof of Theorem 7. ", "page_idx": 18}, {"type": "text", "text": "Proof. From Assumption (i), the regret is bounded as ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\sf R e g}_{T}\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\widehat{\\ell}_{t},q_{t}-e_{a^{*}}\\rangle+2\\sum_{t=1}^{T}\\gamma_{t}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From the standard FTRL analysis in [36, Exercise 28.12], we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle\\widehat{\\ell}_{t},q_{t}-e_{a^{*}}\\rangle\\leq\\sum_{t=1}^{T}\\Bigl(\\Bigl\\langle\\widehat{\\ell}_{t},q_{t}-q_{t+1}\\Bigr\\rangle-\\beta_{t}D_{(-H_{\\alpha})}(q_{t+1},q_{t})+(\\beta_{t}-\\beta_{t-1})h_{t}\\Bigr)+\\bar{\\beta}\\bar{h}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining the last two inequalities, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathsf{R e g}_{T}\\leq\\mathbb{E}\\Bigg[\\sum_{t=1}^{T}\\bigg(\\Big\\langle\\widehat{\\ell}_{t},q_{t}-q_{t+1}\\Big\\rangle-\\beta_{t}D_{(-H_{\\alpha})}\\big(q_{t+1},q_{t}\\big)+\\big(\\beta_{t}-\\beta_{t-1}\\big)h_{t}+2\\gamma_{t}\\bigg)+\\bar{\\beta}\\bar{h}\\Bigg]}&{}\\\\ {\\lesssim\\mathbb{E}\\Bigg[\\sum_{t=1}^{T}\\bigg(\\frac{z_{t}}{\\beta_{t}\\gamma_{t}^{\\prime}}+\\big(\\beta_{t}-\\beta_{t-1}\\big)h_{t}+\\gamma_{t}\\bigg)+\\bar{\\beta}\\bar{h}\\Bigg]}&{\\mathrm{(Assumption~(\\hat{u})~i n~,}}\\\\ {\\lesssim\\mathbb{E}\\Bigg[\\sum_{t=1}^{T}\\bigg(\\frac{z_{t}}{\\beta_{t}\\gamma_{t}^{\\prime}}+\\big(\\beta_{t}-\\beta_{t-1}\\big)h_{t}+\\gamma_{t}^{\\prime}+\\frac{u_{t}}{\\beta_{t}}\\bigg)+\\bar{\\beta}\\bar{h}\\Bigg]}&{\\mathrm{(definition~of~}\\gamma_{t}\\mathrm{~in~.}}\\\\ {\\lesssim\\mathbb{E}\\Bigg[\\sum_{t=1}^{T}\\bigg(\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\frac{u_{t}}{\\beta_{t}}+\\big(\\beta_{t}-\\beta_{t-1}\\big)h_{t-1}\\bigg)+\\bar{\\beta}\\bar{h}\\Bigg]}&{\\mathrm{(definition~of~}\\gamma_{t}^{\\prime}\\mathrm{~and~Assumption}}\\\\ {\\lesssim\\mathbb{E}\\big[F(\\beta_{t},\\gamma_{t+1},\\mu_{t},\\gamma_{t},h_{t},\\gamma_{t-1})\\big]+\\bar{\\beta}\\bar{h}\\,,}&{\\mathrm{(definition~of~}\\gamma_{t}^{\\prime}\\mathrm{~and~Assumption}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality follows from (5). Now, since $\\beta_{t}$ follows Rule 2 in (6) with $\\widehat{h}_{t}\\,=\\,h_{t-1}$ \uff0c Eq. (9) in Theorem 6 gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\beta_{1:T},z_{1:T},u_{1:T},h_{0:T-1})\\lesssim\\displaystyle\\left(\\sum_{t=1}^{T}\\sqrt{z_{t}h_{1}}\\right)^{\\frac{2}{3}}+\\sqrt{\\displaystyle\\sum_{t=1}^{T}u_{t}h_{1}}+\\sqrt{\\frac{z_{\\operatorname*{max}}}{\\beta_{1}}}+\\frac{u_{\\operatorname*{max}}}{\\beta_{1}}+\\beta_{1}h_{1}\\,,}\\\\ &{F(\\beta_{1:T},z_{1:T},u_{1:T},h_{0:T-1})\\lesssim\\displaystyle\\operatorname*{inf}_{\\varepsilon\\geq1/T}\\Biggl\\{\\left(\\sum_{t=1}^{T}\\sqrt{z_{t}h_{t}\\log\\left(\\varepsilon T\\right)}\\right)^{\\frac{2}{3}}+\\left(\\frac{\\sqrt{z_{\\operatorname*{max}}h_{1}}}{\\varepsilon}\\right)^{\\frac{2}{3}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\sqrt{\\displaystyle\\sum_{t=1}^{T}u_{t}h_{t}\\log\\left(\\varepsilon T\\right)}+\\sqrt{\\frac{u_{\\operatorname*{max}}h_{1}}{\\varepsilon}}\\Biggr\\}+\\sqrt{\\frac{z_{\\operatorname*{max}}}{\\beta_{1}}}+\\frac{u_{\\operatorname*{max}}}{\\beta_{1}}+\\beta_{1}h_{1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, in the adversarial regime, combining (43) and (44) gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}\\lesssim\\mathbb{E}\\left[\\left(\\sum_{t=1}^{T}\\sqrt{z_{t}h_{1}}\\right)^{2/3}+\\sqrt{\\sum_{t=1}^{T}u_{t}h_{1}}\\right]+\\kappa\\leq(z_{\\operatorname*{max}}h_{1})^{1/3}T^{2/3}+\\sqrt{u_{\\operatorname*{max}}h_{1}T}+\\kappa\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we recall that $\\kappa=\\sqrt{z_{\\mathrm{max}}/\\beta_{1}}+u_{\\mathrm{max}}/\\beta_{1}+\\beta_{1}h_{1}+\\bar{\\beta}\\bar{h}$ . This completes the proof of (13). ", "page_idx": 18}, {"type": "text", "text": "We next consider the adversarial regime with a $(\\Delta,C,T)$ -self-bounding constraint. For any $\\varepsilon\\geq1/T$ combining (43) and (45) gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\wp}_{Q_{T}}\\lesssim\\mathbb{E}\\left[\\left(\\sum_{t=1}^{T}\\sqrt{z_{t}h_{t}\\log(\\varepsilon T)}\\right)^{\\frac{2}{3}}+\\sqrt{\\sum_{t=1}^{T}u_{t}h_{t}\\log(\\varepsilon T)}\\right]+\\left(\\frac{\\sqrt{z_{\\operatorname*{max}}h_{1}}}{\\varepsilon}\\right)^{\\frac{2}{3}}+\\sqrt{\\frac{u_{\\operatorname*{max}}h_{1}}{\\varepsilon}}+\\kappa}\\\\ {\\displaystyle\\leq\\left(\\mathbb{E}\\left[\\sum_{t=1}^{T}\\sqrt{z_{t}h_{t}}\\right]\\sqrt{\\log(\\varepsilon T)}\\right)^{\\frac{2}{3}}+\\sqrt{\\mathbb{E}\\left[\\sum_{t=1}^{T}u_{t}h_{t}\\right]\\log(\\varepsilon T)}+\\left(\\frac{\\sqrt{z_{\\operatorname*{max}}h_{1}}}{\\varepsilon}\\right)^{\\frac{2}{3}}+\\sqrt{\\frac{u_{\\operatorname*{max}}h_{1}}{\\varepsilon}}+\\kappa\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality follows from Jensen's inequality. Now, using the assumption (14) and defining $Q(a^{*})=\\mathbb{E}\\bigl[\\dot{\\sum_{t=1}^{T}(1-q_{t a^{*}})}\\bigr]\\in[0,T]$ wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T}\\sqrt{z_{t}h_{t}}\\right]\\leq\\sqrt{\\rho_{1}}\\,\\mathbb{E}\\left[\\sum_{t=1}^{T}(1-q_{t a^{*}})\\right]=\\sqrt{\\rho_{1}}\\,Q(a^{*})\\,,}\\\\ {\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T}u_{t}h_{t}\\right]\\leq\\rho_{2}\\,\\mathbb{E}\\left[\\sum_{t=1}^{T}(1-q_{t a^{*}})\\right]=\\rho_{2}\\,Q(a^{*})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since we consider the adversarial regime with a $(\\Delta,C,T)$ -self-bounding constraint, the regret is lower-bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{R e g}_{T}\\geq\\mathbb{E}\\!\\left[\\displaystyle\\sum_{t=1}^{T}\\langle\\Delta,p\\rangle\\right]-C\\geq\\displaystyle\\frac{1}{2}\\mathbb{E}\\!\\left[\\displaystyle\\sum_{t=1}^{T}\\langle\\Delta,q\\rangle\\right]-C}\\\\ &{\\qquad\\geq\\displaystyle\\frac{1}{2}\\Delta_{\\mathsf{m i n}}\\mathbb{E}\\!\\left[\\displaystyle\\sum_{t=1}^{T}(1-q_{t a^{*}})\\right]-C=\\displaystyle\\frac{1}{2}\\Delta_{\\mathsf{m i n}}Q(a^{*})-C\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequality follows from $p=(1-\\gamma_{t})q_{t}+\\gamma_{t}p_{0}\\,\\geq\\,q_{t}/2$ . Hence, combining (47) with (48), (49) and (50), we can bound the regret for any $\\lambda\\in(0,1]$ as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{t e g}_{T}=(1+\\lambda)\\mathsf{R e g}_{T}-\\lambda\\mathsf{R e g}_{T}}\\\\ &{\\le(1+\\lambda)\\bigg(\\sqrt{\\rho_{1}}Q(a^{*})\\sqrt{\\log(\\{c T\\})}\\bigg)^{2/3}-\\frac{\\lambda}{4}\\Delta_{m n}Q(a^{*})+(1+\\lambda)\\sqrt{\\rho_{2}Q(a^{*})\\log(\\varepsilon T)}-\\frac{\\lambda}{4}\\Delta_{m n}Q(a^{*})}\\\\ &{\\qquad+(1+\\lambda)\\bigg(\\bigg(\\frac{\\sqrt{2\\operatorname*{max}h}}{\\varepsilon}\\bigg)^{2/3}+\\sqrt{\\frac{\\operatorname*{max}h}{\\varepsilon}}+\\kappa\\bigg)+\\lambda C}\\\\ &{\\lesssim\\frac{(1+\\lambda)^{3}}{\\lambda^{2}}\\frac{\\rho_{1}\\log(\\varepsilon T)}{\\Delta_{m n}^{2}}+\\frac{(1+\\lambda)^{2}}{\\lambda}\\frac{\\rho_{2}\\log(\\varepsilon T)}{\\Delta_{m n}}+\\bigg(\\frac{\\sqrt{2\\operatorname*{max}h}}{\\varepsilon}\\bigg)^{2/3}+\\sqrt{\\frac{\\operatorname*{max}h_{1}}{\\varepsilon}}+\\kappa+\\lambda C}\\\\ &{\\lesssim\\frac{\\rho_{1}\\log(\\varepsilon T)}{\\Delta_{m n}^{2}}+\\frac{\\rho_{2}\\log(\\varepsilon T)}{\\Delta_{m n}}+\\frac{1}{\\lambda^{2}}\\bigg(\\frac{\\rho_{1}\\log(\\varepsilon T)}{\\Delta_{m n}^{2}}+\\frac{\\rho_{2}\\log(\\varepsilon T)}{\\Delta_{m n}}\\bigg)+\\bigg(\\frac{\\sqrt{z_{\\operatorname*{max}h}}}{\\varepsilon}\\bigg)^{2/3}+\\sqrt{\\frac{\\operatorname*{max}h_{1}}{\\varepsilon}}+}\\\\ &{\\lesssim\\frac{\\rho\\log(\\varepsilon T)}{\\Delta_{m n}^{2}}+\\frac{1}{\\lambda^{2}}\\frac{\\rho\\log(\\varepsilon T)}{\\Delta_{m n}^{2}}+\\bigg(\\frac{\\sqrt{z_{\\operatorname*{max}h}}}{\\varepsilon}\\bigg)^{2/3}+\\sqrt{\\frac{\\operatorname*{max}h_{1}}{\\varepsilon}}+\\kappa+\\lambda C\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where in the first inequality we used (47) with (48), (49), (50), and Jensen's inequality, in the second inequalityweused $\\dot{a}x^{2}\\,\\dot{-}\\,b x^{3}\\,\\leq\\,\\dot{4a}^{3}/(27b^{2})$ for $a\\;\\geq\\;0,b\\;>\\;0$ and $x~\\ge~0$ and $a x\\mathrm{~-~}b x^{2}~\\leq$ $a^{2}/{\\bar{(4b)}}$ for $a\\;\\ge\\;0,b\\;>\\;0$ and $x~\\ge~0$ and in the third inequality we used $\\lambda\\ \\in\\ (0,1]$ .Setting $\\lambda=\\Theta\\big((\\rho\\log(\\varepsilon T)/C)^{1/3}\\big)$ in the last inequality, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}\\lesssim\\frac{\\rho\\log(\\varepsilon T)}{\\Delta_{\\operatorname*{min}}^{2}}+\\left(\\frac{C^{2}\\rho\\log(\\varepsilon T)}{\\Delta_{\\operatorname*{min}}^{2}}\\right)^{1/3}+\\left(\\frac{\\sqrt{z_{\\operatorname*{max}}h_{1}}}{\\varepsilon}\\right)^{2/3}+\\sqrt{\\frac{u_{\\operatorname*{max}}h_{1}}{\\varepsilon}}+\\kappa\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, when $T\\geq\\tau=1/\\Delta_{\\operatorname*{min}}^{2}+C/\\Delta_{\\operatorname*{min}}$ setting ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\varepsilon=\\frac{1}{\\rho^{2}/\\Delta_{\\mathrm{min}}^{2}+C\\rho/\\Delta_{\\mathrm{min}}}\\geq\\frac{1}{T}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "yields that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{R e g}_{T}\\lesssim\\frac{\\rho}{\\Delta_{\\operatorname*{min}}^{2}}\\log_{+}\\bigg(\\frac{T}{1/\\Delta_{\\operatorname*{min}}^{2}+C/\\Delta_{\\operatorname*{min}}}\\bigg)+\\bigg(\\frac{C^{2}\\rho}{\\Delta_{\\operatorname*{min}}^{2}}\\log_{+}\\bigg(\\frac{T}{1/\\Delta_{\\operatorname*{min}}^{2}+C/\\Delta_{\\operatorname*{min}}}\\bigg)\\bigg)^{1/3}}\\\\ &{\\qquad\\quad+\\,(z_{\\operatorname*{max}}h_{1})^{1/3}\\bigg(\\frac{1}{\\Delta_{\\operatorname*{min}}^{2}}+\\frac{C}{\\Delta_{\\operatorname*{min}}}\\bigg)^{2/3}+\\sqrt{u_{\\operatorname*{max}}h_{1}}\\sqrt{\\frac{1}{\\Delta_{\\operatorname*{min}}^{2}}+\\frac{C}{\\Delta_{\\operatorname*{min}}}}+\\kappa}\\\\ &{\\qquad\\lesssim\\frac{\\rho}{\\Delta_{\\operatorname*{min}}^{2}}\\log_{+}\\big(T\\Delta_{\\operatorname*{min}}^{2}\\big)+\\bigg(\\frac{C^{2}\\rho}{\\Delta_{\\operatorname*{min}}^{2}}\\log_{+}\\bigg(\\frac{T\\Delta_{\\operatorname*{min}}}{C}\\bigg)\\bigg)^{1/3}}\\\\ &{\\qquad\\quad+\\,\\bigg((z_{\\operatorname*{max}}h_{1})^{1/3}+\\sqrt{u_{\\operatorname*{max}}h_{1}}\\bigg)\\bigg(\\frac{1}{\\Delta_{\\operatorname*{min}}^{2}}+\\frac{C}{\\Delta_{\\operatorname*{min}}}\\bigg)^{2/3}+\\kappa\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which completes the proof. ", "page_idx": 20}, {"type": "text", "text": "D Auxiliary lemmas ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This section provides auxiliary lemmas useful for proving the BOBW gurantee. ", "page_idx": 20}, {"type": "text", "text": "Lemma 13. Let $\\alpha\\in(0,1)$ and $i^{*}\\in[k]$ Then, the $\\alpha$ Tsallis entropy $H_{\\alpha}$ is bounded from above as ", "page_idx": 20}, {"type": "equation", "text": "$$\nH_{\\alpha}(q)=\\frac{1}{\\alpha}\\sum_{i=1}^{k}(q_{i}^{\\alpha}-q_{i})\\leq\\frac{1}{\\alpha}(k-1)^{\\alpha}(1-q_{i^{*}})^{\\alpha}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for any $q\\in\\mathcal P_{k}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. From Jensen's inequality and the fact that $x\\mapsto x^{\\alpha}$ is concave for $\\alpha\\in(0,1)$ \uff0c ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{i=1}^{k}(q_{i}^{\\alpha}-q_{i})\\leq\\sum_{i\\neq i^{*}}q_{i}^{\\alpha}=(k-1)\\sum_{i\\neq i^{*}}\\frac{1}{k-1}q_{i}^{\\alpha}\\leq(k-1)\\left(\\frac{1}{k-1}\\sum_{i\\neq i^{*}}q_{i}\\right)^{\\alpha}}}\\\\ {{\\displaystyle=(k-1)^{1-\\alpha}\\left(\\sum_{i\\neq i^{*}}q_{i}\\right)^{\\alpha}=(k-1)^{1-\\alpha}(1-q_{i^{*}})^{\\alpha}\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which completes the proof. ", "page_idx": 20}, {"type": "text", "text": "Lemma 14 ([26, Lemma 10]). Let $q~\\in~\\mathcal{P}_{k}$ and $\\tilde{I}\\ \\in\\ \\arg\\operatorname*{max}_{i\\in[k]}q_{i}$ .For $\\ell\\,\\in\\,\\mathbb{R}^{k}$ $i f\\,|\\ell_{i}|\\,\\leq$ mina,l-ai!-\u03b1 foralli E [k], it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{p\\in\\mathcal{P}_{k}}\\bigl\\{\\langle\\ell,q-p\\rangle-D_{(-H_{\\alpha})}(p,q)\\bigr\\}\\leq\\frac{4}{1-\\alpha}\\left(\\sum_{i\\neq\\tilde{I}}q_{i}^{2-\\alpha}\\ell_{i}^{2}+\\operatorname*{min}\\{q_{\\tilde{I}},1-q_{\\tilde{I}}\\}^{2-\\alpha}\\ell_{\\tilde{I}}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 15 ([26, Lemmas 11 and 12]). Let $L\\,\\in\\,\\mathbb{R}^{k}$ and $\\ell\\in\\mathbb{R}^{k}$ and suppose that $q,r\\,\\in\\,\\mathcal{P}_{k}$ are given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q\\in\\underset{p\\in\\mathcal{P}_{k}}{\\mathrm{arg}\\,\\mathrm{min}}\\bigl\\{\\langle L,p\\rangle+\\beta(-H_{\\alpha}(p))+\\bar{\\beta}(-H_{\\bar{\\alpha}}(p))\\bigr\\}}\\\\ &{r\\in\\underset{p\\in\\mathcal{P}_{k}}{\\mathrm{arg}\\,\\mathrm{min}}\\bigl\\{\\langle L+\\ell,p\\rangle+\\beta^{\\prime}(-H_{\\alpha}(p))+\\bar{\\beta}(-H_{\\bar{\\alpha}}(p))\\bigr\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for the Tsallis entropy $H_{\\alpha}$ and $H_{\\bar{\\alpha}}$ $0<\\beta\\le\\beta^{\\prime}$ . Suppose also that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\ell\\|_{\\infty}\\le\\operatorname*{max}\\Biggl\\{\\frac{1-(\\sqrt{2})^{\\alpha-1}}{2}q_{*}^{\\alpha-1}\\beta,\\frac{1-(\\sqrt{2})^{\\bar{\\alpha}-1}}{2}q_{*}^{\\bar{\\alpha}-1}\\bar{\\beta}\\Biggr\\}\\,,}\\\\ &{0\\le\\beta^{\\prime}-\\beta\\le\\operatorname*{max}\\Biggl\\{\\Big(1-(\\sqrt{2})^{\\alpha-1}\\Big)\\beta,\\frac{1-(\\sqrt{2})^{\\bar{\\alpha}-1}}{\\sqrt{2}}q_{*}^{\\bar{\\alpha}-\\alpha}\\bar{\\beta}\\Biggr\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, it holds that $H_{\\alpha}(r)\\leq2H_{\\alpha}(q)$ ", "page_idx": 20}, {"type": "text", "text": "E Proof for partial monitoring (Theorem 8, Section 5) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This section provides the proof of Theorem 8. ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 8. It suffices to prove that assumptions in Theorem 7 are satified. We first vertify Assumptions (i)-(ii) in (12). Let us start from checking Assumption (i). From the definition of the loss difference estimator $\\widehat{y_{t}}$ , the regret is bounded as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathsf{R e g}_{T}=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}(\\mathcal{L}_{A_{t}x_{t}}-\\mathcal{L}_{a^{*}x_{t}})\\right]=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle p_{t}-e_{a^{*}},\\mathcal{L}e_{x_{t}}\\rangle\\right]}&{}\\\\ {=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle q_{t}-e_{a^{*}},\\mathcal{L}e_{x_{t}}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}\\left\\langle\\displaystyle\\frac{1}{k}1-q_{t},\\mathcal{L}e_{x_{t}}\\right\\rangle\\right]}&{}\\\\ {\\le\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle q_{t}-e_{a^{*}},\\mathcal{L}e_{x_{t}}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}\\right]=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{u=1}^{k}q_{t}(\\mathcal{L}_{a x_{t}}-\\mathcal{L}_{a^{*}x_{t}})+\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}\\right]}&{}\\\\ {=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{a=1}^{k}q_{t a}(\\hat{y}_{t a}-\\hat{y}_{t a^{*}})+\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}\\right]=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle q_{t}-e_{a^{*}},\\hat{y}_{t}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}\\right],}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the inequality holds since $\\mathcal{L}\\in[0,1]^{k\\times d}$ , This implies that Assumption (i) is indeed satisfied. ", "page_idx": 21}, {"type": "text", "text": "We next check Assumption (i) in (12). For any $b\\in[k]$ wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\frac{\\widehat{y}_{t b}}{\\beta_{t}}\\right|=\\left|\\frac{G(A_{t},\\sigma_{t})_{b}}{\\beta_{t}p_{t A_{t}}}\\right|\\leq\\frac{|G(A_{t},\\sigma_{t})_{b}|k}{\\beta_{t}\\gamma_{t}}\\leq\\frac{c_{g}}{\\beta_{t}\\gamma_{t}}\\leq\\frac{c_{g}}{u_{t}}=\\frac{1-\\alpha}{8}\\frac{1}{\\left(\\operatorname*{min}\\{q_{t}\\tilde{I}_{t},1-q_{t}\\tilde{I}_{t}\\}\\right)^{1-\\alpha}}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the third inequality follows from $\\gamma_{t}\\,\\geq\\,u_{t}/\\beta_{t}$ in (11) and the last equality follows from the defintition of $u_{t}$ in (17). Hence, from Lemma 14 the LHS of Assumption (i) is bounded as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\big[\\langle\\widehat{y}_{t},q_{t}-q_{t+1}\\rangle-\\beta_{t}\\,D_{(-H_{\\alpha})}(q_{t+1},q_{t})\\big]=\\beta_{t}\\mathbb{E}_{t}\\bigg[\\bigg\\langle\\frac{\\widehat{y}_{t}}{\\beta_{t}},q_{t}-q_{t+1}\\bigg\\rangle-D_{(-H_{\\alpha})}(q_{t+1},q_{t})\\bigg]}\\\\ &{\\le\\mathbb{E}_{t}\\left[\\frac{4}{\\beta_{t}(1-\\alpha)}\\left(\\sum_{i\\neq\\bar{I}_{t}}q_{t i}^{2-\\alpha}\\widehat{y}_{t i}^{2}+\\left(\\operatorname*{min}\\{q_{t\\bar{I}_{t}},1-q_{t\\bar{I}_{t}}\\}\\right)^{2-\\alpha}\\widehat{y}_{t\\bar{I}_{t}}^{2}\\right)\\right]}\\\\ &{=\\frac{4}{\\beta_{t}(1-\\alpha)}\\left(\\sum_{i\\neq\\bar{I}_{t}}q_{t i}^{2-\\alpha}\\mathbb{E}_{t}\\big[\\widehat{y}_{t i}^{2}\\big]+q_{t*}^{2-\\alpha}\\mathbb{E}_{t}\\Big[\\widehat{y}_{t\\bar{I}_{t}}^{2}\\Big]\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since the variance of $\\widehat{y}_{t}$ is bounded from above as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}\\big[\\widehat{y}_{t i}^{2}\\big]=\\sum_{a=1}^{k}p_{t a}\\frac{G(a,\\sigma_{t})_{i}^{2}}{p_{t a}^{2}}\\leq\\sum_{a=1}^{k}\\frac{k\\|G\\|_{\\infty}^{2}}{\\gamma_{t}}=\\frac{c_{g}^{2}}{\\gamma_{t}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $i\\in[k]$ , the LHS of Assumption (i) is further bounded as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}[\\langle\\widehat{y}_{t},q_{t}-q_{t+1}\\rangle-\\beta_{t}D_{\\psi_{t}}(q_{t+1},q_{t})]\\le\\frac{4c_{Q}^{2}}{\\beta_{t}\\gamma_{t}(1-\\alpha)}\\left(\\sum_{i\\neq\\widetilde{I}_{t}}q_{t i}^{2-\\alpha}+q_{t*}^{2-\\alpha}\\right)=\\frac{z_{t}}{\\beta_{t}\\gamma_{t}}\\le\\frac{z_{t}}{\\beta_{t}\\gamma_{t}^{\\prime}}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which implies that Assumption (i) in (12) is satisfied. ", "page_idx": 21}, {"type": "text", "text": "Next, we will prove $h_{t+1}\\lesssim h_{t}$ of Assumption (ii in (12). To prove this, we will check the conditions (58) and (59) in Lemma i5. For any $a\\in[k]$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\widehat{y}_{t a}|\\leq\\frac{\\|G\\|_{\\infty}}{p_{t A_{t}}}\\leq\\frac{k\\|G\\|_{\\infty}}{\\gamma_{t}}\\leq\\frac{c_{\\mathcal{G}}\\beta_{t}}{u_{t}}\\leq\\frac{1-\\alpha}{8}\\frac{\\beta_{t}}{q_{t^{*}}^{1-\\alpha}}\\leq\\frac{1-(\\sqrt{2})^{\\alpha-1}}{2}\\frac{\\beta_{t}}{q_{t^{*}}^{1-\\alpha}}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second inequality follows from $p_{t a}\\geq\\gamma_{t}/k$ , the third inequality from $\\gamma_{t}\\geq u_{t}/\\beta_{t}$ , and the last inequality from the fact that $(1-x)/4\\leq1-(\\sqrt{2})^{x-1}$ for $x\\in[0,1]$ . Thus, the condition (58) is satisfied. ", "page_idx": 21}, {"type": "text", "text": "We next check the condition (59). Recalling $q_{t*}\\,=\\,\\mathrm{min}\\{q_{t\\tilde{I}_{t}},1-q_{t\\tilde{I}_{t}}\\}$ , the parameters $z_{t}$ and $u_{t}$ satisfy ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sqrt{z_{t}}=\\frac{2c_{g}}{\\sqrt{1-\\alpha}}\\sqrt{\\sum_{i\\neq\\tilde{I}_{t}}q_{t i}^{2-\\alpha}+q_{t*}^{2-\\alpha}}\\leq\\frac{2\\sqrt{k}c_{\\mathcal{Q}}}{\\sqrt{1-\\alpha}}q_{t*}^{1-\\frac{1}{2}\\alpha}\\,,\\quad u_{t}=\\frac{8c_{\\mathcal{Q}}}{1-\\alpha}q_{t*}^{1-\\alpha}\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the inequality follows from $q_{t i}\\leq q_{t*}$ for $i\\neq\\tilde{I}_{t}$ . The penalty component $h_{t}$ is lower-bounded as ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{t}=H_{\\alpha}(q_{t})=\\frac{1}{\\alpha}\\sum_{i=1}^{k}(q_{t i}^{\\alpha}-q_{t i})\\geq\\frac{1-(1/2)^{1-\\alpha}}{\\alpha}q_{t*}^{\\alpha}\\geq\\frac{1-\\alpha}{4\\alpha}q_{t*}^{\\alpha}\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality in (67) follows from $1-(1/2)^{1-x}\\,\\geq\\,(1-x)/4$ for $x\\le0$ , and the first inequality can be proven as folows: when $q_{t\\tilde{I}_{t}}\\leq1/2$ it holds that $\\begin{array}{r}{\\sum_{i=1}^{k}(q_{t i}^{\\alpha}-q_{t i})\\geq q_{t\\tilde{I}_{t}}^{\\alpha}-q_{t\\tilde{I}_{t}}=}\\end{array}$ $q_{t\\tilde{I}_{t}}^{\\alpha}(1-q_{t\\tilde{I}_{t}}^{1-\\alpha})\\,\\geq\\,q_{t\\tilde{I}_{t}}^{\\alpha}(1-(1/2)^{1-\\alpha})\\,=\\,q_{t*}^{\\alpha}(1-(1/2)^{1-\\alpha}),$ and when $q_{t\\tilde{I}_{t}}>1/2$ , it holds that $\\begin{array}{r l}&{\\quad\\quad\\cdots t_{l t}\\quad,\\quad\\cdots\\quad\\cdots t_{l t}\\quad}\\\\ &{={\\mathrm{1}}(q_{t i}^{\\alpha}-q_{t i})\\,\\geq\\,\\sum_{i=1}^{k}q_{t i}^{\\alpha}-1\\,\\geq\\,\\sum_{i\\not=l_{t}}^{\\prime}q_{t i}^{\\alpha}+(1/2)^{\\alpha}\\,-\\,1\\,\\geq\\,(\\sum_{i\\not=l_{t}}^{\\infty}q_{t i})^{\\alpha}+(1/2)^{\\alpha}\\,-\\,1\\,=}\\end{array}$ $(1-q_{t\\tilde{I}_{t}})^{\\alpha}+(1/2)^{\\alpha}-1=q_{t*}^{\\alpha}+(1/2)^{\\alpha}-1\\geq q_{t*}^{\\alpha}(1-(1/2)^{1-\\alpha})$ . Using the bounds on $z_{t},\\,u_{t}$ \uff0c and $h_{t}$ in (66) and (67), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{t+1}-\\beta_{t}=\\cfrac{1}{\\hat{h}_{t+1}}\\left(2\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\frac{u_{t}}{\\beta_{t}}\\right)=\\cfrac{2}{h_{t}}\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\cfrac{1}{h_{t}}\\frac{u_{t}}{\\beta_{t}}}\\\\ &{\\phantom{2p c}\\leq\\cfrac{16\\alpha c_{{g}}\\sqrt{k}}{\\sqrt{\\beta_{1}}(1-\\alpha)^{3/2}}q_{t*}^{1-\\frac{3}{2}\\alpha}+\\cfrac{32\\alpha c_{{g}}}{\\sqrt{\\beta_{1}}(1-\\alpha)^{2}}q_{t*}^{1-2\\alpha}}\\\\ &{\\phantom{2p c}\\leq\\alpha\\bar{\\beta}q_{t*}^{1-\\frac{3}{2}\\alpha}+\\alpha\\bar{\\beta}q_{t*}^{1-2\\alpha}}\\\\ &{\\phantom{2p c}\\leq2(1-\\bar{\\alpha})\\bar{\\beta}q_{t*}^{\\bar{\\alpha}-\\alpha}\\leq2\\cfrac{1-(\\sqrt{2})^{\\bar{\\alpha}-1}}{\\sqrt{2}}\\bar{\\beta}q_{t*}^{\\bar{\\alpha}-\\alpha}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality follows from (66),(67), and the fact that $\\beta_{t}\\geq\\beta_{1}\\geq1$ , the second inequality from the definition of $\\bar{\\boldsymbol\\beta}$ in (17), the third inequality from $\\mathrm{min}\\{1-\\frac{3}{2}\\alpha,1-2\\alpha\\}\\,\\geq\\,\\bar{\\alpha}\\,-\\,\\alpha$ since $\\bar{\\alpha}=1-\\alpha$ , and the last inequality from $1-x\\leq(1-(\\sqrt{2})^{x-1})/\\sqrt{2}$ for $x\\leq1$ .Therefore, the condition (59) is satified. Hence, from Lemma 15, we have $h_{t+1}\\stackrel{}{=}H_{\\alpha}(q_{t+1})\\stackrel{}{\\leq}2H_{\\alpha}(q_{t})=2h_{t}$ which implies that Assumption (ii) in (12) is satisfied. ", "page_idx": 22}, {"type": "text", "text": "Finally, we check the assumption (14) in Theorem 7. We first consider the first inequality in (14). From the definition of $z_{t}$ and the fact that $q_{t i}\\leq q_{t\\tilde{I}_{t}}$ for $i\\neq\\tilde{I}_{t}$ , the stability component $z_{t}$ is bounded as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{t}=\\cfrac{4c_{Q}^{2}}{1-\\alpha}\\left\\{\\underset{i\\neq\\tilde{I}_{t}}{\\sum_{q_{t}^{2}=\\tilde{I}_{t}}q_{t}^{2-\\alpha}}+\\left(\\operatorname*{min}\\{q_{t}\\bar{I}_{t},1-q_{t\\bar{I}_{t}}\\}\\right)^{2-\\alpha}\\right\\}}\\\\ &{\\quad\\leq\\cfrac{4c_{Q}^{2}}{1-\\alpha}\\left\\{\\underset{i\\neq\\tilde{I}_{t}}{\\sum_{q_{t}^{2}=\\tilde{I}_{t}}q_{t}^{2-\\alpha}}+\\left(\\underset{i\\neq\\tilde{I}_{t}}{\\sum_{q_{t}=\\tilde{I}}q_{t}}\\right)^{2-\\alpha}\\right\\}}\\\\ &{\\quad\\leq\\cfrac{8c_{Q}^{2}}{1-\\alpha}\\left(\\underset{i\\neq\\tilde{I}_{t}}{\\sum_{q_{t}=\\tilde{I}_{t}}q_{t}}\\right)^{2-\\alpha}\\cfrac{8c_{Q}^{2}}{1-\\alpha}\\left(\\underset{i\\neq\\tilde{I}_{t}}{\\sum_{q_{t}=\\tilde{I}_{t}}q_{t}}\\right)^{2-\\alpha}=\\cfrac{8c_{Q}^{2}}{1-\\alpha}\\left(1-q_{t a^{*}}\\right)^{2-\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality holds from the inequality $x^{a}+y^{a}\\leq(x+y)^{a}$ for $x,y\\geq0$ and $a\\geq1$ \uff0c and the third inequality from $q_{t i}\\leq q_{t\\tilde{I}_{t}}$ for $i\\neq\\tilde{I}_{t}$ . From Lemma 13, we also obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{t}=H_{\\alpha}(q_{t})\\leq\\frac{1}{\\alpha}(k-1)^{1-\\alpha}(1-q_{t a^{*}})^{\\alpha}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, combining (69) and (70), we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\nz_{t}h_{t}\\leq\\frac{8c_{Q}^{2}}{1-\\alpha}(1-q_{t a^{*}})^{2-\\alpha}\\cdot\\frac{1}{\\alpha}(k-1)^{1-\\alpha}(1-q_{t a^{*}})^{\\alpha}=\\underbrace{\\frac{8c_{Q}^{2}(k-1)^{1-\\alpha}}{\\alpha(1-\\alpha)}}_{=\\rho_{1}}(1-q_{t a^{*}})^{2}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We next consider the second inequality in (14). We can bound $u_{t}$ from above as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{u_{t}=\\frac{8c g}{1-\\alpha}\\left(\\operatorname*{min}\\{q_{t\\tilde{I}_{t}},1-q_{t\\tilde{I}_{t}}\\}\\right)^{1-\\alpha}\\leq\\frac{8c g}{1-\\alpha}\\left(\\sum_{i\\neq\\tilde{I}_{t}}q_{t i}\\right)^{1-\\alpha}}}\\\\ {\\displaystyle\\quad\\leq\\frac{8c g}{1-\\alpha}\\left(\\sum_{i\\neq a^{*}}q_{t i}\\right)^{1-\\alpha}=\\frac{8c g}{1-\\alpha}(1-q_{t a^{*}})^{1-\\alpha}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the second inequality follows from $q_{t\\tilde{I}_{t}}\\ge q_{t i}$ for all $i\\in[k]$ . Hence, combining the last two inequality and (70), ", "page_idx": 23}, {"type": "equation", "text": "$$\nu_{t}h_{t}\\leq\\underbrace{\\frac{4c_{\\mathcal{G}}(k-1)^{1-\\alpha}}{\\alpha(1-\\alpha)}}_{=\\rho_{2}}(1-q_{t a^{*}})\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, the assumption (14) is satified with above $\\rho_{1}$ and $\\rho_{2}$ , and thus we have completed the proof. ", "page_idx": 23}, {"type": "text", "text": "F Proof for graph bandits (Theorem 10, Section 6) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This section provides the missing detail of Section 6. ", "page_idx": 23}, {"type": "text", "text": "F.1  Fractional domination number ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Before introducing the fractional domination number, we define the domination number $\\tilde{\\delta}\\leq\\delta$ .A dominating set $D\\subseteq V$ is a set of vertices such that $V\\subseteq\\bigcup_{i\\in D}N^{\\mathrm{out}}(i)$ . The domination number $\\tilde{\\delta}(G)$ of graph $G$ is the size of the smallest dominating set. From the definition, the domination number $\\tilde{\\delta}$ can also be written as the optimal value of the following optimization problem: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{minimize}\\,\\sum_{i\\in V}x_{i}\\quad\\mathrm{subject\\,to}\\quad\\sum_{\\substack{i\\in N^{\\mathrm{in}}(j)}}x_{i}\\geq1\\,\\forall j\\in V\\,,\\,x_{i}\\in\\{0,1\\}\\,\\forall i\\in V\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $x_{i}\\in\\{0,1\\}$ a binary variable indicating whether vertex $i$ is in the dominating set ( $x_{i}=1$ or not $(x_{i}=0)$ ", "page_idx": 23}, {"type": "text", "text": "Then, one can see that the fractional domination number $\\delta^{*}$ is defined as the optimal value of the following optimization problem, in which the variables $(x_{i})_{i\\in V}$ are allowed to take values in $[0,1]$ instead of $\\bar{\\{0,1\\}}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{minimize}\\,\\sum_{i\\in V}x_{i}\\quad\\mathrm{subject\\,to}\\quad\\sum_{i\\in N^{\\mathrm{in}}(j)}x_{i}\\geq1\\,\\forall j\\in V\\,,\\,0\\leq x_{i}\\leq1\\,\\forall i\\in V\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is the linear program provided in (19). From the definitions, the fractional domination number is less than or equal to the domination number, $\\delta^{*}\\leq\\tilde{\\delta}$ . Another advantage of using $\\delta^{*}$ instead of $\\tilde{\\delta}$ .s that the fractional domination number $\\delta^{*}$ can be computed in polynomial time, while the computation of the domination number $\\tilde{\\delta}$ is NP-hard. See [13] for more benefits of using the fractional version of the (weak) domination number. ", "page_idx": 23}, {"type": "text", "text": "F.2Proof of Theorem 10 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Here, we provide the proof of Theorem 10. ", "page_idx": 23}, {"type": "text", "text": "Proof. It suffices to prove that assumptions in Theorem 7 are satified. We first vertify Assumptions (i)-(ii) in (12). We start from checking Assumption (i). The regret is bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{1}\\mathrm{eg}_{T}=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\ell_{t}(A_{t})-\\sum_{t=1}^{T}\\ell_{t}(a^{*})\\right]=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\ell_{t},p_{t}-e_{a^{*}}\\rangle\\right]=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\ell_{t},q_{t}-e_{a^{*}}\\rangle+\\sum_{t=1}^{T}\\langle\\ell_{t},p_{t}-e_{a^{*}}\\rangle\\right]\\,,}\\\\ {\\displaystyle\\qquad=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\ell_{t},q_{t}-e_{a^{*}}\\rangle+\\sum_{t=1}^{T}\\gamma_{t}\\langle\\ell_{t},q_{t}-u\\rangle\\right]\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\widehat{\\ell}_{t},q_{t}-e_{a^{*}}\\rangle+\\sum_{t=1}^{T}\\gamma_{t}\\right],\\qquad\\displaystyle(76)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the third equality follows from the defintion of $\\gamma_{t}$ . This implies that Assumption (i) is indeed satisfied. ", "page_idx": 24}, {"type": "text", "text": "We next check Assumption (i) in (12). Now, recalling the defintion of the fractional domination number and the optimal value $x^{*}$ of (19), and $\\begin{array}{r}{u_{i}=x_{i}^{*}\\bar{/}\\sum_{j\\in V}x_{j}^{*}}\\end{array}$ wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{j\\in N^{\\mathrm{in}}(i)}u_{j}=\\frac{\\sum_{j\\in N^{\\mathrm{in}}(i)}x_{j}^{*}}{\\sum_{i\\in V}x_{i}^{*}}\\ge\\frac{1}{\\sum_{i\\in V}x_{i}^{*}}=\\frac{1}{\\delta^{*}}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the inequality follows from the first constraint in (19). Hence, combining this with the definitionof $p_{t}=(1-\\gamma_{t})q_{t}+\\gamma_{t}u$ wecanlower-bound $P_{t i}$ as ", "page_idx": 24}, {"type": "equation", "text": "$$\nP_{t i}=\\sum_{j\\in N^{\\mathrm{in}}(i)}p_{t j}\\geq\\gamma_{t}\\sum_{j\\in N^{\\mathrm{in}}(i)}u_{j}\\geq\\frac{\\gamma_{t}}{\\delta^{*}}\\quad\\mathrm{for~all~}i\\in V\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This lower bound yields that for any $i\\in V$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\frac{\\widehat{\\ell}_{t i}}{\\beta_{t}}\\right|\\leq\\frac{\\ell_{t i}}{\\beta_{t}P_{t i}}\\leq\\frac{\\delta^{*}}{\\beta_{t}\\gamma_{t}}\\leq\\frac{\\delta^{*}}{u_{t}}=\\frac{1-\\alpha}{8}\\frac{1}{\\left(\\operatorname*{min}\\{q_{t\\tilde{I}_{t}},1-q_{t\\tilde{I}_{t}}\\}\\right)^{1-\\alpha}}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second inequality follows from (78) and the third inequality from $\\gamma_{t}\\:\\geq\\:u_{t}/\\beta_{t}$ in (11). Hence, from Lemma 14 we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\Big[\\Big\\langle\\widehat{\\ell}_{t},q_{t}-q_{t+1}\\Big\\rangle-\\beta_{t}\\,D_{(-H_{\\alpha})}(q_{t+1},q_{t})\\Big]=\\beta_{t}\\mathbb{E}_{t}\\Bigg[\\Bigg\\langle\\frac{\\widehat{\\ell}_{t}}{\\beta_{t}},q_{t}-q_{t+1}\\Bigg\\rangle-D_{(-H_{\\alpha})}(q_{t+1},q_{t})\\Bigg]}\\\\ &{\\le\\mathbb{E}_{t}\\Bigg[\\frac{4}{\\beta_{t}(1-\\alpha)}\\left(\\sum_{i\\in V\\setminus\\{\\widehat{\\ell}_{t}\\}}q_{t i}^{2-\\alpha}\\widehat{\\ell}_{t i}^{2}+\\big(\\operatorname*{min}\\{q_{t\\widehat{\\iota}_{t}},1-q_{t\\widetilde{\\iota}_{t}}\\}\\big)^{2-\\alpha}\\widehat{\\ell}_{t\\widehat{\\iota}_{t}}^{2}\\right)\\Bigg]}\\\\ &{=\\frac{4}{\\beta_{t}(1-\\alpha)}\\left(\\sum_{i\\in V\\setminus\\{\\widehat{\\ell}_{t}\\}}q_{t i}^{2-\\alpha}\\mathbb{E}_{t}\\Big[\\widehat{\\ell}_{t i}^{2}\\Big]+q_{t*}^{2-\\alpha}\\mathbb{E}_{t}\\Big[\\widehat{\\ell}_{t\\widehat{\\iota}_{t}}^{2}\\Big]\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, by using the lower bound of $P_{t}$ in (78), for any $i\\in V$ the variance of the loss estimator $\\widehat{\\ell}_{t i}$ is bounded as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}\\Big[\\widehat{\\ell}_{t i}^{2}\\Big]=\\sum_{j=1}^{k}p_{t j}\\frac{\\ell_{t i}^{2}}{P_{t i}^{2}}\\mathbb{1}\\big[i\\in N^{\\mathrm{out}}(j)\\big]=\\frac{\\ell_{t i}^{2}}{P_{t i}^{2}}\\sum_{j\\in V\\colon i\\in N^{\\mathrm{out}}(j)}p_{t j}=\\frac{\\ell_{t i}^{2}}{P_{t i}}\\leq\\frac{\\delta^{*}}{\\gamma_{t}}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, combining (80) with (99), we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{t}[\\langle\\widehat{y}_{t},q_{t}-q_{t+1}\\rangle-\\beta_{t}D_{\\psi_{t}}(q_{t+1},q_{t})]\\leq\\frac{4\\delta^{*}}{\\beta_{t}\\gamma_{t}(1-\\alpha)}\\left(\\sum_{i\\in V\\backslash\\{\\tilde{I}_{t}\\}}q_{t i}^{2-\\alpha}+q_{t*}^{2-\\alpha}\\right)=\\frac{z_{t}}{\\beta_{t}\\gamma_{t}}\\leq\\frac{z_{t}}{\\beta_{t}\\gamma_{t}^{\\prime}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which implies that Assumption (i) in (12) is satisfied. ", "page_idx": 24}, {"type": "text", "text": "Next, we will prove $h_{t+1}\\lesssim h_{t}$ of Assumption (i in (12). To prove this, we will check the conditions (58) and (59) in Lemma 15. For any $i\\in V$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\widehat{\\ell}_{t i}|\\leq\\frac{1}{P_{t i}}\\leq\\frac{\\delta^{*}}{\\gamma_{t}}\\leq\\frac{\\delta^{*}\\beta_{t}}{u_{t}}=\\frac{1-\\alpha}{8}\\frac{\\beta_{t}}{q_{t*}^{1-\\alpha}}\\leq\\frac{1-(\\sqrt{2})^{\\alpha-1}}{2}\\frac{\\beta_{t}}{q_{t*}^{1-\\alpha}}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second inequality follows from (78), the third inequality from $\\gamma_{t}\\,\\geq\\,u_{t}/\\beta_{t}$ , and the last inequality from the fact that $(1-x)/4\\leq1-(\\sqrt{2})^{x-1}$ for $x\\in[0,1]$ . Thus, the condition (58) is satisfied. ", "page_idx": 24}, {"type": "text", "text": "We next check the condition (59). Recalling $q_{t*}=\\operatorname*{min}\\{q_{t\\tilde{I}_{t}},1\\!-\\!q_{t\\tilde{I}_{t}}\\}$ , we observe that the parameters $z_{t}$ and $u_{t}$ satisfy ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sqrt{z_{t}}=\\sqrt{\\frac{4\\delta^{*}}{1-\\alpha}\\left(\\sum_{i\\in V\\backslash\\{\\tilde{I}_{t}\\}}q_{t i}^{2-\\alpha}+q_{t*}^{2-\\alpha}\\right)}\\leq\\frac{2\\sqrt{k\\delta^{*}}}{\\sqrt{1-\\alpha}}q_{t*}^{1-\\frac{1}{2}\\alpha}\\,,\\quad u_{t}=\\frac{8\\delta^{*}}{1-\\alpha}q_{t*}^{1-\\alpha}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality follows from $q_{t i}\\leq q_{t*}$ for $i\\neq\\tilde{I}_{t}$ . We can also lower-bound $h_{t}$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\nh_{t}=H_{\\alpha}(q_{t})=\\frac{1}{\\alpha}\\sum_{i=1}^{k}(q_{t i}^{\\alpha}-q_{t i})\\geq\\frac{1-(1/2)^{1-\\alpha}}{\\alpha}q_{t*}^{\\alpha}\\geq\\frac{1-\\alpha}{4\\alpha}q_{t*}^{\\alpha}\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which can be proven by the same manner as in (67). Hence, using the upper bounds on $z_{t},\\,u_{t}$ ,and $h_{t}$ in (84) and (85), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{t+1}-\\beta_{t}=\\cfrac{1}{\\hat{h}_{t+1}}\\bigg(2\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\frac{u_{t}}{\\beta_{t}}\\bigg)=\\cfrac{2}{h_{t}}\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\cfrac{1}{h_{t}}\\frac{u_{t}}{\\beta_{t}}}\\\\ &{\\qquad\\leq\\cfrac{16\\alpha\\sqrt{k\\delta^{*}}}{\\sqrt{\\beta_{1}}(1-\\alpha)^{3/2}}q_{t*}^{1-\\frac{3}{2}\\alpha}+\\cfrac{32\\alpha\\delta^{*}}{\\sqrt{\\beta_{1}}(1-\\alpha)^{2}}q_{t*}^{1-2\\alpha}}\\\\ &{\\qquad\\leq\\alpha\\bar{\\beta}q_{t*}^{1-\\frac{3}{2}\\alpha}+\\alpha\\bar{\\beta}q_{t*}^{1-2\\alpha}}\\\\ &{\\qquad\\qquad\\leq2(1-\\bar{\\alpha})\\bar{\\beta}q_{t*}^{\\bar{\\alpha}-\\alpha}\\leq2\\cfrac{1-(\\sqrt{2})^{\\bar{\\alpha}-1}}{\\sqrt{2}}\\bar{\\beta}q_{t*}^{\\bar{\\alpha}-\\alpha}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first inequality follows from (84), (85), and $\\beta_{t}\\,\\geq\\,\\beta_{1}\\,\\geq\\,1$ , the second inequality from the definition of $\\bar{\\beta}$ the third inequality from $\\mathrm{min}\\{1-\\frac{3}{2}\\alpha,1-2\\alpha\\}\\,\\geq\\,\\bar{\\alpha}\\,-\\,\\alpha$ since $\\bar{\\alpha}\\,=\\,1\\,-\\,\\alpha.$ and the last inequality from $1-x\\,\\leq\\,(1-({\\sqrt{2}})^{x-1})/{\\sqrt{2}}$ for $x\\leq1$ .Thus the condition (59) is satified. Therefore, from Lemma 15, we have $h_{t+1}=H_{\\alpha}(q_{t+1})\\leq2H_{\\alpha}(q_{t})=2h_{t}$ , which implies that Assumption (ii) in (12) is satisfied. ", "page_idx": 25}, {"type": "text", "text": "Finally, we check the assumption (14) in Theorem 7. We first consider the first inequality in (14). From the definition of $z_{t}$ and the fact that $q_{t i}\\leq q_{t\\tilde{I}_{t}}$ for $i\\neq\\tilde{I}_{t}$ ,we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{t}=\\cfrac{4\\delta^{*}}{1-\\alpha}\\left\\{\\displaystyle\\sum_{i\\in V\\backslash\\{\\tilde{I}_{t}\\}}q_{t i}^{2-\\alpha}+\\left(\\operatorname*{min}\\{q_{t\\tilde{I}_{t}},1-q_{t\\tilde{I}_{t}}\\}\\right)^{2-\\alpha}\\right\\}}\\\\ &{\\quad\\leq\\cfrac{4\\delta^{*}}{1-\\alpha}\\left\\{\\displaystyle\\sum_{i\\in V\\backslash\\{\\tilde{I}_{t}\\}}q_{t i}^{2-\\alpha}+\\left(\\displaystyle\\sum_{i\\neq\\tilde{I}_{t}}q_{t i}\\right)^{2-\\alpha}\\right\\}}\\\\ &{\\quad\\leq\\cfrac{8\\delta^{*}}{1-\\alpha}\\left(\\displaystyle\\sum_{i\\in V\\backslash\\{\\tilde{I}_{t}\\}}q_{t i}\\right)^{2-\\alpha}\\leq\\cfrac{8\\delta^{*}}{1-\\alpha}\\left(\\displaystyle\\sum_{i\\neq a^{*}}q_{t i}\\right)^{2-\\alpha}=\\cfrac{8\\delta^{*}}{1-\\alpha}\\left(1-q_{t a^{*}}\\right)^{2-\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second inequality holds from the inequality $x^{a}+y^{a}\\leq(x+y)^{a}$ for $x,y\\geq0$ and $a\\geq1$ \uff0c and the third inequality from $q_{t i}\\leq q_{t\\tilde{I}_{t}}$ . Hence, combining (87) and the upper bound on $h_{t}$ in (70), we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\nz_{t}h_{t}\\leq\\frac{8\\delta^{*}}{1-\\alpha}(1-q_{t a^{*}})^{2-\\alpha}\\cdot\\frac{1}{\\alpha}(k-1)^{1-\\alpha}(1-q_{t a^{*}})^{\\alpha}=\\underbrace{\\frac{8\\delta^{*}(k-1)^{1-\\alpha}}{\\alpha(1-\\alpha)}}_{=\\rho_{1}}(1-q_{t a^{*}})^{2}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We next consider the second inequality in (14). We can bound $u_{t}$ from above as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle u_{t}=\\frac{8\\delta^{*}}{1-\\alpha}\\big(\\operatorname*{min}\\{q_{t\\tilde{I}_{t}},1-q_{t\\tilde{I}_{t}}\\}\\big)^{1-\\alpha}\\leq\\frac{8\\delta^{*}}{1-\\alpha}\\left(\\sum_{i\\neq\\tilde{I}_{t}}q_{t i}\\right)^{1-\\alpha}}\\\\ {\\displaystyle\\quad\\leq\\frac{8\\delta^{*}}{1-\\alpha}\\left(\\sum_{i\\neq a^{*}}q_{t i}\\right)^{1-\\alpha}=\\frac{8\\delta^{*}}{1-\\alpha}(1-q_{t a^{*}})^{1-\\alpha}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second inequality follows from $q_{t\\tilde{I}_{t}}~\\ge~q_{t i}$ for all $i\\ne\\tilde{I}_{t}$ . Hence, combining the last inequality with (70), ", "page_idx": 25}, {"type": "equation", "text": "$$\nu_{t}h_{t}\\leq\\underbrace{\\frac{4\\delta^{*}(k-1)^{1-\\alpha}}{\\alpha(1-\\alpha)}}_{=\\rho_{2}}(1-q_{t a^{*}})\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "F.3 Technical challenges to derive best-of-both-worlds bounds depending on (fractional) weak domination number ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here, we discuss the technical challenges of making our upper bound in Theorem 10 depend on the weakdomination number $\\delta$ instead of the fracional domination number $\\delta^{*}$ or the weak fractional dominationnumber $\\tilde{\\delta}^{*}\\leq\\delta$ ", "page_idx": 26}, {"type": "text", "text": "First, we need to use Tsallis entropy to derive a regret upper bound with a stochastic bound of $\\log T$ While we can prove a BOBW bound if we use the Shannon entropy regularizer [25], the bound in the stochastic regime is $O((\\log T)^{2})$ , which is not desirable. Hence, a possible approach is to use the log-barrier regularizer or the Tsallis entropy. The log-barrier regularizer has a penalty term of $\\Omega(k)$ due to the strength of its regularization, and the regret upper bound in the final adversarial regime $\\Omega(k^{1/3})$ , which can be much larger than $\\delta^{1/3}$ . Therefore, the most hopeful solution would be to use Tsallis entropy with an appropriate exponent $\\alpha\\simeq1$ , where we note that the Tsallis entropy with $\\alpha\\rightarrow1$ corresponds to the Shannon entropy. ", "page_idx": 26}, {"type": "text", "text": "Recalling the definition of the weak domination number in Section 6, we can see that the weak dominating set dominates only vertices without self-loop $U=\\{i\\in V\\colon i\\notin N^{\\mathsf{o u t}}(i)\\}$ .Thus,to achieve a BOBW bound that depends on the weak domination number, vertices with self-loop and those without self-loop should be treated separately by decomposing the stability term as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\widehat{\\ell}_{t},q_{t}-q_{t+1}\\rangle-\\beta_{t}\\,D_{(-H_{\\alpha})}(q_{t+1},q_{t})}\\\\ &{=\\displaystyle\\sum_{i\\in U}\\left(\\widehat{\\ell}_{t i}(q_{t i}-q_{t+1,i})-\\beta_{t}\\,d(q_{t+1,i},q_{t,i})\\right)+\\sum_{i\\in V\\backslash U}\\left(\\widehat{\\ell}_{t i}(q_{t i}-q_{t+1,i})-\\beta_{t}\\,d(q_{t+1,i},q_{t,i})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $d(p,q)$ is the Bregman divergence induced by the real-valued convex function $\\begin{array}{r}{x\\mapsto-\\frac{1}{\\alpha}(x^{\\alpha}-}\\end{array}$ $x)$ . However, if we use this approach, we cannot use Lemma 14, which is useful to prove an upper bound with $(1\\mathrm{~-~}q_{t a^{*}})$ (see (14)). This is because this lemma exploits the fact that $q$ and $r$ are probability vectors. This prevents us from deriving an upper bound with an ${\\cal O}(\\log T)$ stochastic bound depending on the weak domination number. ", "page_idx": 26}, {"type": "text", "text": "G Case study (3): Multi-armed bandits with paid observations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "G.1  Problem setting and existing approach ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Multi-armed bandits with paid observations, which is first investigated by Seldin et al. [53], is a variant of the multi-armed bandits problem. At each round $t\\,\\in\\,[T]$ , the environment determines a loss vector $\\ell_{t}\\colon\\mathcal{A}=[k]\\to[0,1]$ and the learner observes cost vector $c_{t}\\in\\mathbb{R}_{\\geq0}^{k}$ . Then, the learner selects an action $A_{t}\\in[k]$ and chooses a set of actions $S_{t}\\subseteq[k]$ , for which we can observe losses. Then the learner suffers a loss of $\\textstyle\\ell_{t A_{t}}+\\sum_{i\\in S_{t}}c_{t i}$ and observes a set of losses $\\{\\ell_{t i}\\colon i\\in S_{t}\\}$ .The goal of the learner is to minimize the sum of the standard regret and the observation costs given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\mathsf{R e g}}_{T}^{\\mathrm{cost}}={\\mathsf{R e g}}_{T}+\\mathbb{E}\\left[\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}c_{t i}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We next provide an existing approach to determine the set $S_{t}$ and to estimate losses, which are given by Seldin et al. [53]. To determine $S_{t}$ , we prepare a vector $r_{t}\\in[0,1]^{k}$ . For this $r_{t}$ , we then sample $\\bar{b_{t i}}\\sim\\mathrm{Ber}(r_{t i})$ for each $i\\in[k]$ , and use this to construct the set of actions $S_{t}=\\{i\\in[k]\\colon b_{t i}=\\bar{1}\\}$ We use the loss estimator defined by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{\\ell}_{t i}=\\frac{\\ell_{t i}}{r_{t i}}\\mathbb{1}[i\\in S_{t}]\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is indeed unbiased, $\\mathbb{E}_{S_{t}}[\\widehat{\\ell}_{t i}]=\\ell_{t i}$ ", "page_idx": 26}, {"type": "text", "text": "In the following, we assume that the cost is same for each arm at each time, $c_{t i}=c\\ge0$ Accordingly, welet $r_{t i}\\,=\\,r_{t}\\,\\in\\,[0,1]$ for each $i\\in[k]$ , where we abuse the notation. Analyzing the case where ", "page_idx": 26}, {"type": "text", "text": "Algorithm 2: Best-of-both-worlds algorithm based on FTRL with SPB-matching learning rate and Tsallis entropy in multi-armed bandits with paid observations ", "page_idx": 27}, {"type": "text", "text": "1 input: action set $A=[k]$ , exponent of Tsallis entropy $\\alpha$ $\\beta_{1},\\bar{\\beta}$   \n2 for $t=1,2,\\ldots$ do   \n3 Compute $q_{t}\\in\\mathcal P_{k}$ by (10) with a loss estimator $\\widehat{\\ell}_{t}$ in (92).   \n4 Set $h_{t}=H_{\\alpha}(q_{t})$ and $z_{t},u_{t}\\geq0$ in (94).   \n5 Compute action selection probability $p_{t}=q_{t}$ without forced exploration.   \n6 For $r_{t}\\in[0,1]$ in (93), sample $b_{t i}\\sim\\mathrm{Ber}(r_{t})$ for each $i\\in\\mathcal{A}$ and let $S_{t}=\\{i\\in[k]\\colon b_{t i}=1\\}$   \n7 Choose $\\bar{A}_{t}\\in[k]$ so that $\\operatorname*{Pr}[A_{t}=i\\mid p_{t}]=p_{t i}$   \n8 Observes the set of losses $\\{\\ell_{t i}\\colon i\\in S_{t}\\}$ and suffes a loss of $\\begin{array}{r}{\\ell_{t A_{t}}+\\sum_{i\\in S_{t}}c_{t i}}\\end{array}$   \n9 Compute loss estimator $\\widehat{\\ell}_{t}$ based on $r_{t}$ and $S_{t}$   \nCompute $\\beta_{t+1}$ by Rule 2 of SPB-matching in (6) with $\\widehat{h}_{t+1}=h_{t}$ ", "page_idx": 27}, {"type": "text", "text": "each action has a different cost and deriving an upper bound that depends on the cost of each action is difficult. This is essentially due to the same reason as the problem in graph bandits with self-loops, where the regret upper bound depends on the domination number (see Appendix F.3). ", "page_idx": 27}, {"type": "text", "text": "The setting of multi-armed bandits with paid observations is not directly reducible to the general online learning framework defined in Section 2. However, the parameter $r_{t}$ play the same role as the forced exploration parameter $\\gamma_{t}$ in partial monitoring and graph bandits, and thus their regret upper bounds have the similar structure. Roughly speaking, we will see in the regret analysis of multiarmed bandits with paid observations with cost $c\\geq0$ can be regarded as the general online learning set up with the exploration rate of $\\gamma_{t}\\simeq c k r_{t}$ ", "page_idx": 27}, {"type": "text", "text": "G.2Algorithm ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We use FTRL provided in (10) and (11) as for graph bandits and partial monitoring with no forced exploration, that is, $p_{t}=q_{t}$ . Here we recall that $p_{t}\\in\\mathcal{P}_{k}$ is the action selection probability at round $t$ and $q_{t}\\in\\mathcal P_{k}$ the output of FTRL at round $t$ .We use $r_{t}\\in[0,1]$ given by ", "page_idx": 27}, {"type": "equation", "text": "$$\nr_{t}=\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\frac{u_{t}}{\\beta_{t}}\\,,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which plays a role of exploration rate $\\gamma_{t}$ . We will choose $\\beta_{1}$ so that $r_{t}\\leq1/2$ . Next we specify the parameters in (11). For $\\tilde{I}_{t}\\in\\arg\\operatorname*{max}_{i\\in[k]}q_{t i}$ and $q_{t*}=\\operatorname*{min}\\{q_{t\\tilde{I}_{t}},1-q_{t\\tilde{I}_{t}}\\}$ we use ", "page_idx": 27}, {"type": "equation", "text": "$$\n)_{1}\\geq\\frac{64\\operatorname*{max}\\{c,1\\}k}{1-\\alpha}\\,,\\,\\bar{\\beta}=\\frac{32k\\sqrt{c}}{(1-\\alpha)^{2}\\sqrt{\\beta_{1}}}\\,,\\,z_{t}\\mathrm{=}\\frac{4c k}{1-\\alpha}\\left(\\sum_{i\\neq\\bar{t}_{t}}q_{t i}^{2-\\alpha}\\mathrm{+}q_{t*}^{2-\\alpha}\\right)\\,,\\,u_{t}\\mathrm{=}\\frac{8\\operatorname*{max}\\{c,1\\}}{1-\\alpha}q_{t*}^{1-\\alpha}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that $\\begin{array}{r}{z_{\\operatorname*{max}}\\,=\\,\\frac{4c}{1-\\alpha}}\\end{array}$ \uff0c $\\begin{array}{r}{u_{\\mathrm{max}}\\,=\\,\\frac{8\\,\\operatorname*{max}\\{c,1\\}}{1-\\alpha}}\\end{array}$ 8maxfc.1, and hmax = h1 = k1-\u03b1, and the above \u03b2i implies $r_{t}\\leq1/2$ . To follow the analysis in the general online learning framework, we also let $r_{t}^{\\prime}=\\sqrt{z_{t}/\\beta_{t}}$ and $\\gamma_{t}^{\\prime}=c k r_{t}^{\\prime}\\leq\\gamma_{t}$ . To make the algorithm clear, we provide the full description of our algorithm in Algorithm 2. ", "page_idx": 27}, {"type": "text", "text": "G.3 Regret analysis ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We can prove the following. ", "page_idx": 27}, {"type": "text", "text": "Theorem 16. In multi-armed bandits with paid observations, for any $\\alpha\\in(0,1)$ , Algorithm 2 satisfies the assumptions of Theorem 7 with $\\gamma_{t}=c k r_{t}$ $\\begin{array}{r}{\\rho_{1}=\\Theta\\Big(\\frac{c k^{2-\\alpha}}{\\alpha(1-\\alpha)}\\Big)}\\end{array}$ ), and p2 = 0 (maxfc.1)k-\u03b1) where the regret $\\mathsf{R e g}_{T}$ in the statement is repalced with ${\\mathsf{R e g}}_{T}^{\\mathsf{c o s t}}$ ", "page_idx": 27}, {"type": "text", "text": "Note that here we are abusing the statement of Theorem 7 since Theorem 7 is for the general online learning framework given in Section 2 but the multi-armed bandits with paid observation is not a special case of the general online learning framework. Still, if we set the exploration rate $\\gamma_{t}$ to $\\gamma_{t}=c k r_{t}$ , then the minimization of the regret with costs, ${\\mathsf{R e g}}_{T}^{\\mathsf{c o s t}}$ , in multi-armed bandits with paid observation under paid cost $c$ and parameter $r_{t}$ can be seen as the minimization of the regret $\\mathsf{R e g}_{T}$ in the general online learning framework with exploration rate $\\gamma_{t}\\,=\\,c k r_{t}$ . A formal proof of the theorem for multi-armed bandits with paid observations corresponding to Theorem 7 follows the same argument and we omit it. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Setting $\\alpha=1-1/(\\log k)$ in the last theorem gives the following: ", "page_idx": 28}, {"type": "text", "text": "Corollary 17. In multi-armed bandits with paid observation with $T\\geq\\tau$ Algorithm2with $\\alpha=$ $1-1/(\\log k)$ achieves ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathsf{t e g}_{T}^{\\mathrm{cost}}=\\left\\{\\!O\\!\\left(\\frac{(c k)^{1/3}T^{2/3}(\\log k)^{1/3}+\\sqrt{T\\log k}+\\kappa)}{\\Delta\\rho_{\\mathsf{m i n}}^{2}}\\!+\\!\\left(\\frac{C^{2}\\operatorname*{max}\\{c,1\\}k\\log k}{\\Delta_{\\mathsf{m i n}}^{2}}\\log\\!\\left(\\frac{T\\Delta_{\\mathsf{m i n}}}{C}\\right)\\right)^{1/3}\\!+\\kappa^{\\prime}\\!\\right)\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here, $i f$ weuse $\\beta_{1}\\,=\\,64\\operatorname*{max}\\{c,1\\}k/(1\\,-\\,\\alpha),$ which satisfies (94), $\\kappa\\,=\\,O(\\operatorname*{max}\\{c,1\\}k\\log k\\,+$ k3/2(log k)5/2) and k' =  + O(clog k)1/3 + \u221amax{c,1} log k)(m ", "page_idx": 28}, {"type": "text", "text": "This regret upper bound is the first BOBW bounds in multi-armed bandits with paid observations. The upper bound in the adversarial regime becomes $O({\\sqrt{T\\log k}})$ as $c\\ \\to\\ 0$ , as observed in [53]. The bound in the stochastic regime can also match the nearly optimal regret bound of $O(\\log k\\log T/\\Delta_{\\operatorname*{min}})$ in the expert problem when $c\\to0$ . To formally check this, it suffices to refine the analysis in Theorem 7 by analyzing $\\rho_{1}$ and $\\rho_{2}$ separately, which is unified into $\\rho=\\operatorname*{max}\\{\\rho_{1},\\rho_{2}\\}$ for simplicity of notation in the proof of Theorem 7. ", "page_idx": 28}, {"type": "text", "text": "ProofofTheorem $^ \u1e0a I6 \u1e0c$ From the observation that the variable $r_{t}$ plays the same role as the exploration parameter $\\gamma_{t}$ , it suffices to prove that assumptions in Theorem 7 are satified. We first vertify Assumptions (i)-(ii) in (12). We start from checking Assumption (i). The regret with costs is bounded as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\sf R e g}_{T}^{\\mathrm{cost}}={\\mathbb E}\\left[\\sum_{t=1}^{T}\\ell_{t A_{t}}-\\displaystyle\\sum_{t=1}^{T}\\ell_{t a^{*}}+\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in O_{t}}c_{t i}\\right]={\\mathbb E}\\left[\\sum_{t=1}^{T}\\langle\\ell_{t},p_{t}-e_{a^{*}}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\langle r_{t},c_{t}\\rangle\\right]}}\\\\ {~~}\\\\ {{\\displaystyle~~={\\mathbb E}\\left[\\sum_{t=1}^{T}\\langle\\widehat{\\ell}_{t},p_{t}-e_{a^{*}}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\langle r_{t}{\\bf1},c{\\bf1}\\rangle\\right]}}\\\\ {~~}\\\\ {{\\displaystyle~~={\\mathbb E}\\left[\\sum_{t=1}^{T}\\langle\\widehat{\\ell}_{t},p_{t}-e_{a^{*}}\\rangle+k c\\displaystyle\\sum_{t=1}^{T}r_{t}\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we recallthat we are abusing the notation so that $r_{t i}=r_{t}\\in[0,1]$ . This implies that Assumption (i) is satisfied with $\\gamma_{t}=c k r_{t}$ ", "page_idx": 28}, {"type": "text", "text": "We next check Assumption (i in (12). For any $i\\in[k]$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|\\frac{\\widehat{\\ell}_{t i}}{\\beta_{t}}\\right|\\leq\\frac{\\ell_{t i}}{\\beta_{t}r_{t i}}\\leq\\frac{1}{u_{t}}\\leq\\frac{1-\\alpha}{8}\\frac{1}{\\left(\\operatorname*{min}\\{q_{t\\tilde{I}_{t}},1-q_{t\\tilde{I}_{t}}\\}\\right)^{1-\\alpha}}\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the first inequality follows from the definition of $\\widehat{\\ell}_{t}$ , the second inequality from $r_{t}\\geq u_{t}/\\beta_{t}$ and the last inequality from the definition of $u_{t}$ . Note that this is where $\\operatorname*{max}\\{c,1\\}$ in $u_{t}$ is used. Hence, from Lemma 14 we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbb{E}_{t}\\Big[\\Big\\langle\\widehat{\\ell}_{t},q_{t}-q_{t+1}\\Big\\rangle-\\beta_{t}\\,D_{(-H_{\\alpha})}\\big(q_{t+1},q_{t}\\Big)\\Big]=\\beta_{t}\\mathbb{E}_{t}\\Bigg[\\Bigg\\langle\\frac{\\widehat{\\ell}_{t}}{\\beta_{t}},q_{t}-q_{t+1}\\Bigg\\rangle-D_{(-H_{\\alpha})}\\big(q_{t+1},q_{t}\\Big)\\Bigg]}\\\\ &{\\le\\mathbb{E}_{t}\\Bigg[\\frac{4}{\\beta_{t}\\big(1-\\alpha\\big)}\\left(\\sum_{i\\neq\\widehat{I}_{t}}q_{t i}^{2-\\alpha}\\widehat{\\ell}_{t i}^{2}+\\left(\\operatorname*{min}\\{q_{t\\widehat{I}_{t}},1-q_{t\\widetilde{I}_{t}}\\}\\right)^{2-\\alpha}\\widehat{\\ell}_{t\\widehat{I}_{t}}^{2}\\right)\\Bigg]}\\\\ &{=\\frac{4}{\\beta_{t}(1-\\alpha)}\\left(\\sum_{i\\neq\\widehat{I}_{t}}q_{t i}^{2-\\alpha}\\mathbb{E}_{t}\\Big[\\widehat{\\ell}_{t i}^{2}\\Big]+q_{t*}^{2-\\alpha}\\mathbb{E}_{t}\\Big[\\widehat{\\ell}_{t\\widehat{I}_{t}}^{2}\\Big]\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now, for any $i\\in[k]$ the variance of the loss estimator $\\widehat{\\ell}_{t i}$ is bounded as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}\\left[\\widehat{\\ell}_{t i}^{2}\\right]=\\mathbb{E}_{t}\\left[\\frac{\\ell_{t i}^{2}}{r_{t}^{2}}\\mathbb{1}[i\\in S_{t}]\\right]\\le\\frac{1}{r_{t}}\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, combining (98) with (99), we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}[\\langle\\widehat{y}_{t},q_{t}-q_{t+1}\\rangle-\\beta_{t}D_{\\psi_{t}}(q_{t+1},q_{t})]}\\\\ &{\\le\\displaystyle\\frac{4}{\\beta_{t}r_{t}(1-\\alpha)}\\left(\\sum_{i\\neq\\bar{I}_{t}}q_{t i}^{2-\\alpha}+q_{t*}^{2-\\alpha}\\right)=\\frac{4c k}{\\beta_{t}\\gamma_{t}(1-\\alpha)}\\left(\\sum_{i\\neq\\bar{I}_{t}}q_{t i}^{2-\\alpha}+q_{t*}^{2-\\alpha}\\right)=\\frac{z_{t}}{\\beta_{t}\\gamma_{t}}\\le\\frac{z_{t}}{\\beta_{t}\\gamma_{t}^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the first equality follows from $\\gamma_{t}=c k r_{t}$ . This implies that Assumption (i) in (12) is satisfied. Next, we will prove $h_{t+1}\\lesssim h_{t}$ of Assumption (ii) in (12). To prove this, we will check the conditions (58) and (59) in Lemma 15. For any $i\\in[k]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n|\\widehat{\\ell}_{t i}|\\leq\\frac{1}{r_{t}}\\leq\\frac{\\beta_{t}}{u_{t}}=\\frac{1-\\alpha}{8}\\frac{\\beta_{t}}{q_{t*}^{1-\\alpha}}\\leq\\frac{1-(\\sqrt{2})^{\\alpha-1}}{2}\\frac{\\beta_{t}}{q_{t*}^{1-\\alpha}}\\,,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second inequality follows from $r_{t}\\geq u_{t}/\\beta_{t}$ and the last inequality from the fact that $\\left(1-\\right.$ $x)/4\\leq1-(\\sqrt{2})^{x-1}$ for $x\\in[0,1]$ . Thus, the condition (58) is satisfied. ", "page_idx": 29}, {"type": "text", "text": "We next check the condition (59). Recalling $q_{t*}=\\operatorname*{min}\\{q_{t\\tilde{I}_{t}},1\\!-\\!q_{t\\tilde{I}_{t}}\\}$ , we observe that the parameters $z_{t}$ and $u_{t}$ satisfy ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sqrt{z_{t}}=\\sqrt{\\frac{4c k}{1-\\alpha}\\left(\\sum_{i\\neq\\tilde{I}_{t}}q_{t i}^{2-\\alpha}+q_{t*}^{2-\\alpha}\\right)}\\,\\le\\frac{2k\\sqrt{c}}{\\sqrt{1-\\alpha}}q_{t*}^{1-\\frac{1}{2}\\alpha}\\,,\\quad u_{t}=\\frac{8\\operatorname*{max}\\{c,1\\}}{1-\\alpha}q_{t*}^{1-\\alpha}\\,,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the inequality follows from $q_{t i}\\leq q_{t*}$ for $i\\neq\\tilde{I}_{t}$ . We can also lower bound $h_{t}$ as ", "page_idx": 29}, {"type": "equation", "text": "$$\nh_{t}=H_{\\alpha}(q_{t})=\\frac{1}{\\alpha}\\sum_{i=1}^{k}(q_{t i}^{\\alpha}-q_{t i})\\geq\\frac{1-(1/2)^{1-\\alpha}}{\\alpha}q_{t*}^{\\alpha}\\geq\\frac{1-\\alpha}{4\\alpha}q_{t*}^{\\alpha}\\,,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which can be proven by the same manner as in (67). Hence, using the upper bounds on $z_{t},\\,u_{t}$ ,and $h_{t}$ in (102) and (103), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{t+1}-\\beta_{t}=\\cfrac{1}{\\hat{h}_{t+1}}\\left(2\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\frac{u_{t}}{\\beta_{t}}\\right)=\\cfrac{2}{h_{t}}\\sqrt{\\frac{z_{t}}{\\beta_{t}}}+\\cfrac{1}{h_{t}}\\frac{u_{t}}{\\beta_{t}}}\\\\ &{\\phantom{\\beta_{t+1}}\\leq\\cfrac{16\\alpha\\sqrt{k c}}{\\sqrt{\\beta_{1}}(1-\\alpha)^{3/2}}q_{t*}^{1-\\frac{3}{2}\\alpha}+\\cfrac{32\\alpha\\operatorname*{max}\\{c,1\\}}{\\sqrt{\\beta_{1}}(1-\\alpha)^{2}}q_{t*}^{1-2\\alpha}}\\\\ &{\\phantom{\\beta_{t+1}}\\leq\\alpha\\bar{\\beta}q_{t*}^{1-\\frac{3}{2}\\alpha}+\\alpha\\bar{\\beta}q_{t*}^{1-2\\alpha}}\\\\ &{\\phantom{\\beta_{t+1}}\\leq2(1-\\bar{\\alpha})\\bar{\\beta}q_{t*}^{\\bar{\\alpha}-\\alpha}\\leq2\\cfrac{1-(\\sqrt{2})^{\\bar{\\alpha}-1}}{\\sqrt{2}}\\bar{\\beta}q_{t*}^{\\bar{\\alpha}-\\alpha}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the first inequality follows from (102), (103), and $\\beta_{t}\\,\\geq\\,\\beta_{1}\\,\\geq\\,1$ , the second inequality from the definition of $\\bar{\\beta}$ , the third inequality from $\\mathrm{min}\\{1-\\frac{3}{2}\\alpha,1-2\\alpha\\}\\,\\geq\\,\\bar{\\alpha}\\,-\\,\\alpha$ since $\\bar{\\alpha}\\,=\\,1\\,-\\,\\alpha.$ and the last inequality from $1-x\\,\\leq\\,(1-({\\sqrt{2}})^{x-1})/{\\sqrt{2}}$ for $x\\leq1$ . Thus the condition (59) is satified. Therefore, from Lemma 15, we have $h_{t+1}=H_{\\alpha}(q_{t+1})\\leq2H_{\\alpha}(q_{t})=2h_{t}$ , which implies that Assumption (ii) in (12) is satisfied. ", "page_idx": 29}, {"type": "text", "text": "Finally, we check the assumption (14) in Theorem 7. We first consider the first inequality in (14). From the definition of $z_{t}$ and the fact that $q_{t i}\\leq q_{t\\tilde{I}_{t}}$ for $i\\neq\\tilde{I}_{t}$ ,we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle z_{t}=\\frac{4c k}{1-\\alpha}\\left\\{\\sum_{i\\neq\\tilde{I}_{t}}q_{i}^{2-\\alpha}+\\left(\\operatorname*{min}\\{q_{t}\\bar{I}_{t},1-q_{t\\bar{I}_{t}}\\}\\right)^{2-\\alpha}\\right\\}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\le\\frac{4c k}{1-\\alpha}\\left\\{\\sum_{i\\neq\\tilde{I}_{t}}q_{i}^{2-\\alpha}+\\left(\\sum_{i\\neq\\tilde{I}_{t}}q_{i}\\right)^{2-\\alpha}\\right\\}}\\\\ {\\displaystyle}\\\\ {\\displaystyle\\le\\frac{8c k}{1-\\alpha}\\left(\\sum_{i\\neq\\tilde{I}_{t}}q_{i}\\right)^{2-\\alpha}\\le\\frac{8c k}{1-\\alpha}\\left(\\sum_{i\\neq\\alpha}q_{t i}\\right)^{2-\\alpha}=\\frac{8c k}{1-\\alpha}(1-q_{t\\alpha^{*}})^{2-\\alpha}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the second inequality holds from the inequality $x^{a}+y^{a}\\leq(x+y)^{a}$ for $x,y\\geq0$ and $a\\geq1$ \uff0c and the third inequality from $q_{t i}\\leq q_{t\\tilde{I}_{t}}$ . Hence, combining (105) and the upper bound on $h_{t}$ in (70), we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\nz_{t}h_{t}\\leq\\frac{8c k}{1-\\alpha}(1-q_{t a^{*}})^{2-\\alpha}\\cdot\\frac{1}{\\alpha}(k-1)^{1-\\alpha}(1-q_{t a^{*}})^{\\alpha}=\\underbrace{\\frac{8c k(k-1)^{1-\\alpha}}{\\alpha(1-\\alpha)}}_{=\\rho_{1}}(1-q_{t a^{*}})^{2}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We next consider the second inequality in (14). We can upper bound $u_{t}$ as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{u_{t}=\\displaystyle\\frac{8\\operatorname*{max}\\{c,1\\}}{1-\\alpha}\\big(\\!\\operatorname*{min}\\{q_{t\\tilde{I}_{t}},1-q_{t\\tilde{I}_{t}}\\}\\big)^{1-\\alpha}\\leq\\frac{8\\operatorname*{max}\\{c,1\\}}{1-\\alpha}\\left(\\sum_{i\\not=f\\tilde{I}_{t}}q_{t i}\\right)^{1-\\alpha}}}\\\\ {{\\displaystyle\\leq\\frac{8\\operatorname*{max}\\{c,1\\}}{1-\\alpha}\\left(\\sum_{i\\not=a^{*}}q_{t i}\\right)^{1-\\alpha}=\\frac{8\\operatorname*{max}\\{c,1\\}}{1-\\alpha}\\big(1-q_{t a^{*}}\\big)^{1-\\alpha}\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the second inequality follows from $q_{t\\tilde{I}_{t}}~\\ge~q_{t i}$ for all $i\\ne\\tilde{I}_{t}$ .Hence, combining the last inequality with (70), ", "page_idx": 30}, {"type": "equation", "text": "$$\nu_{t}h_{t}\\leq\\underbrace{\\frac{4\\operatorname*{max}\\{c,1\\}(k-1)^{1-\\alpha}}{\\alpha(1-\\alpha)}}_{=\\rho_{2}}(1-q_{t a^{*}})\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, the assumption (14) is satified with above $\\rho_{1}$ and $\\rho_{2}$ , and thus we have completed the proof. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In the abstract and introduction, we claims that we consider adaptive learning rate in online learning with a minimax regret of $\\Theta(T^{2/3})$ and develop best-of-both-worlds algorithms in partial monitoring, graph bandits, and multi-armed bandits with paid observations. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We provided a comparison of our regret bounds with existing regret bounds in Table 1 and after Corollaries 9 and 11. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The problem settings are detailed in Sections 1, 2, 5 and 6 and Appendix G and assumptions are fully provided of each proposition. The complete proofs are fully provided in the appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This study is primarily theoretical and does not involve experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d)  We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This study is primarily theoretical and does not provide open access to the data nor code. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible,so $^{\\epsilon}\\mathrm{No}^{\\;\\;\\breve{,}\\ast}$ is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This study is primarily theoretical and does not involve experiments. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This study is primarily theoretical and does not involve experiments. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This study is primarily theoretical and does not involve experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: This study is primarily theoretical and does not include contents that can violate the NeurIPS Code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This study is primarily theoretical and does not have societal impacts. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 34}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This study is primarily theoretical and does not involve experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This study is primarily theoretical and does not involve experiments using existing assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license ofadataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This study is primarily theoretical and does not involve any assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This study is primarily theoretical and does not involve crowdsourcing and human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This study is primarily theoretical and does not involve study participants. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}]