{"importance": "This paper is crucial for researchers in online learning because it introduces a novel adaptive learning rate framework for problems with a minimax regret of O(T\u00b2/\u00b3). This expands the applicability of Follow-the-Regularized-Leader (FTRL) algorithms, which are known for their effectiveness in various online learning scenarios, but have been limited in their use with indirect feedback.  The proposed framework offers significantly simpler algorithms with improved regret bounds compared to existing approaches. It also provides a unified approach to achieving the best-of-both-worlds (BOBW) guarantee for hard problems, significantly advancing the field.", "summary": "A new adaptive learning rate for FTRL achieves minimax regret of O(T\u00b2/\u00b3) in online learning, improving existing best-of-both-worlds algorithms for various hard problems.", "takeaways": ["A novel adaptive learning rate framework for FTRL is developed, achieving a minimax regret of O(T\u00b2/\u00b3).", "This framework offers significantly simpler algorithms and improved regret bounds for several hard online learning problems compared to existing methods.", "The work provides a unified best-of-both-worlds guarantee for hard online learning problems, offering simultaneous optimality in stochastic and adversarial regimes."], "tldr": "Many online learning problems, such as partial monitoring and graph bandits, exhibit a minimax regret of O(T\u00b2/\u00b3),  representing a significant challenge for algorithm designers.  Existing adaptive learning rates primarily focus on problems with a different regret bound, lacking efficient solutions for these \"hard\" problems.  Additionally, most adaptive learning rates aren't designed to seamlessly handle both stochastic and adversarial environments.\nThis research introduces a novel adaptive learning rate framework for FTRL, which greatly simplifies the design of online learning algorithms. By meticulously matching stability, penalty, and bias terms in the regret bound, a surprisingly simple learning rate is obtained. This method improves existing best-of-both-worlds regret upper bounds for partial monitoring, graph bandits, and multi-armed bandits with paid observations. The resulting learning rates are surprisingly simple compared to existing ones and achieve nearly optimal regret bounds in both stochastic and adversarial scenarios.", "affiliation": "University of Tokyo", "categories": {"main_category": "AI Theory", "sub_category": "Optimization"}, "podcast_path": "XlvUz9F50g/podcast.wav"}