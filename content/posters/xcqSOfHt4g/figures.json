[{"figure_path": "xcqSOfHt4g/figures/figures_2_1.jpg", "caption": "Figure 1: Masking schedules in the literature: (Left) at; (Right) weight of the cross-entropy loss w.r.t. t; Equations for these schedules are given in Tab. 4 in Appendix.", "description": "This figure shows different masking schedules used in the literature for masked diffusion models. The left panel displays the function a_t, which represents the probability of a token remaining unmasked at time t. The right panel shows the weight of the cross-entropy loss with respect to time t. Various schedules are presented, including linear, geometric, cosine, and polynomial functions, each having different properties and potentially influencing the training and sampling process.", "section": "2 Masked Diffusion"}, {"figure_path": "xcqSOfHt4g/figures/figures_4_1.jpg", "caption": "Figure 2: Left: FID evaluation for 50K samples randomly generated from our model on pixel-level modeling of ImageNet 64\u00d764. Right: Number of tokens revealed in each step of generation.", "description": "This figure presents two sub-figures. The left sub-figure is a bar chart that shows the Fr\u00e9chet Inception Distance (FID) score for 50,000 image samples generated by the model on the ImageNet 64x64 dataset. The FID score is a metric for evaluating the quality of generated images, with lower scores indicating better image quality. Different sampling configurations are compared, including linear and cosine masking schedules with and without class conditioning. The right sub-figure is a line chart that shows the number of tokens revealed at each generation step for linear and cosine masking schedules. This illustrates how the unmasking process unfolds over time for the different schedules.", "section": "Impact of schedules and discretization"}, {"figure_path": "xcqSOfHt4g/figures/figures_6_1.jpg", "caption": "Figure 3: Iterative unmasking process for an unconditionally generated sample by MD4. This visualization only includes a subsequence from a generated sequence of 1024 tokens. \"?\" represents masks. Masked tokens are revealed sequentially: green (steps 500-700), yellow (700-850), and red (850-1000). Additional unconditional generation from MD4 can be found in App. K.4.", "description": "This figure shows an example of the iterative unmasking process during text generation using the MD4 model.  The process starts with a sequence of masked tokens (represented by '?').  The model then progressively unmasks these tokens in steps, revealing them sequentially in the colors green, yellow, and red, until a complete sentence is generated. The figure highlights how the model gradually reconstructs the text from a masked state, showcasing the iterative nature of the generation process.", "section": "4 Sampling"}, {"figure_path": "xcqSOfHt4g/figures/figures_7_1.jpg", "caption": "Figure 4: Perplexity on OpenWebText (OWT) validation set during training. The final numbers are reported in Tab. 5 in Appendix.", "description": "This figure shows the perplexity on the OpenWebText validation set during the training process for several models: Gaussian Diffusion-S, SEDD-S, MD4-S, GenMD4-S, and MD4-M.  The x-axis represents the training steps (in units of 1000 steps), and the y-axis represents the perplexity. The plot illustrates the training progress of different models and their final perplexities on this dataset.  The final perplexity values are also detailed in Table 5 of the Appendix.", "section": "7 Experiments"}, {"figure_path": "xcqSOfHt4g/figures/figures_9_1.jpg", "caption": "Figure 8: More unconditional samples from MD4 trained on ImageNet 64x64.", "description": "This figure shows several unconditional image samples generated by the MD4 model trained on the ImageNet 64x64 dataset.  The images demonstrate the model's ability to generate diverse and visually coherent images at 64x64 resolution by treating each pixel as a discrete token.", "section": "K.3 Additional unconditional generation from MD4 trained on ImageNet 64x64"}, {"figure_path": "xcqSOfHt4g/figures/figures_26_1.jpg", "caption": "Figure 7: Generative perplexity evaluated by GPT-2 Large following Lou et al. [32]. We compare MD4 against the GPT-2 checkpoint (autoregressive baseline) and SEDD (the previous best discrete diffusion model on this task) in generating 1024-token text sequences. We investigate the effects of two orthogonal factors on sample quality: model size and decoding steps. The numbers for GPT-2 and SEDD are from Lou et al. [32].", "description": "This figure compares the performance of MD4 and SEDD, two discrete diffusion models, against GPT-2, an autoregressive model, in generating 1024-token text sequences.  The comparison is made using the generative perplexity metric, as evaluated by GPT-2 Large.  The plot shows how perplexity changes with different decoding steps and model sizes (small vs. medium).  This helps understand the impact of model size and decoding length on sample quality.", "section": "K.1 Sample quality evaluation by GPT-2"}, {"figure_path": "xcqSOfHt4g/figures/figures_27_1.jpg", "caption": "Figure 8: More unconditional samples from MD4 trained on ImageNet 64\u00d764.", "description": "This figure displays several more unconditional samples generated by the MD4 model trained on the ImageNet 64x64 dataset.  These samples demonstrate the model's ability to generate a variety of images from the dataset without any specific conditional input. The quality of the samples varies, reflecting the challenges inherent in generating high-quality images from discrete data. The model is trained to maximize likelihood rather than visual quality.", "section": "K.3 Additional unconditional generation from MD4 trained on ImageNet 64\u00d764"}, {"figure_path": "xcqSOfHt4g/figures/figures_30_1.jpg", "caption": "Figure 1: Masking schedules in the literature: (Left) at; (Right) weight of the cross-entropy loss w.r.t. t; Equations for these schedules are given in Tab. 4 in Appendix.", "description": "This figure shows a comparison of different masking schedules used in the literature for masked diffusion models.  The left panel displays the function at over time, where at represents the probability that a data point remains unmasked at time t.  The right panel shows the corresponding weight of the cross-entropy loss with respect to time t in the ELBO (Evidence Lower Bound) objective function.  Several different schedules are plotted, including linear, geometric, cosine and polynomial functions, highlighting their distinct behaviors in controlling the unmasking process over time.", "section": "2 Masked Diffusion"}]