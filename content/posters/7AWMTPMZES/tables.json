[{"figure_path": "7AWMTPMZES/tables/tables_5_1.jpg", "caption": "Table 1: Result of BLEU scores on machine translation and ROUGE scores on text summarization.", "description": "This table presents the results of the proposed approach and several baselines on four different tasks: three machine translation tasks (IWSLT14 DE-EN, WMT14 EN-DE, WMT16 EN-RO) and one text summarization task (GIGAWORD).  For each task, the table shows the BLEU scores (BLEU-1/2/3/4) for machine translation tasks and ROUGE scores (ROUGE-1/2/L) for the summarization task.  The baselines include various autoregressive and diffusion-based language models. The table highlights the superior performance of the proposed method, particularly compared to other diffusion models, and its competitive performance against autoregressive transformers.", "section": "4 Language Modeling"}, {"figure_path": "7AWMTPMZES/tables/tables_5_2.jpg", "caption": "Table 1: Result of BLEU scores on machine translation and ROUGE scores on text summarization.", "description": "This table presents the results of the proposed method and several baselines on four tasks: three machine translation tasks (IWSLT14 DE-EN, WMT14 EN-DE, WMT16 EN-RO) and one text summarization task (GIGAWORD).  The results are shown in terms of BLEU scores (BLEU-1/2/3/4) for the translation tasks and ROUGE scores (ROUGE-1/2/L) for the summarization task.  It compares the performance of the proposed approach against autoregressive transformers and several existing continuous and discrete diffusion language models.", "section": "4 Language Modeling"}, {"figure_path": "7AWMTPMZES/tables/tables_6_1.jpg", "caption": "Table 1: Result of BLEU scores on machine translation and ROUGE scores on text summarization.", "description": "This table presents the performance comparison of different models on four tasks: three machine translation tasks (IWSLT14 DE-EN, WMT14 EN-DE, and WMT16 EN-RO) and one text summarization task (GIGAWORD).  The models are categorized into Auto-Regressive Modeling and Diffusion Process.  For each task, BLEU scores (1-4) are reported for machine translation tasks, and ROUGE scores (1,2,L) for the summarization task.  The table allows readers to compare the performance of the proposed 'Ours' model against state-of-the-art autoregressive and diffusion models.", "section": "4 Language Modeling"}, {"figure_path": "7AWMTPMZES/tables/tables_7_1.jpg", "caption": "Table 3: Analysis on the training objectives.", "description": "This table presents the results of an ablation study comparing different training objectives used in the model.  It shows the error in predicting x0 (the original data), the error in predicting the vector field, the accuracy of predicting whether x0 is within the discrete area (Cxo), and the final BLEU score achieved on a machine translation task.  The comparison helps to determine which objective function yields the best performance.", "section": "4 Language Modeling"}, {"figure_path": "7AWMTPMZES/tables/tables_7_2.jpg", "caption": "Table 2: Ablation studies.", "description": "This table presents the ablation study results, comparing the performance of the proposed model with different configurations against the baseline model (Difformer). The configurations include using only the forward process, using both forward and reverse processes, and using optimal transport for trajectory rescaling. The results are evaluated using BLEU scores on the IWSLT14 DE-EN and WMT16 EN-RO datasets.", "section": "4 Language Modeling"}, {"figure_path": "7AWMTPMZES/tables/tables_8_1.jpg", "caption": "Table 4: FID scores on CIFAR-10.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores achieved by different models on the CIFAR-10 image dataset.  It compares the performance of various methods, including continuous diffusion models (DDPM, DDIM), discrete ordinal pixel models (D3PM, TLDR), and the proposed boundary conditional diffusion models using different image representations (binary coding, fixed embedding, and trainable embedding). Lower FID scores indicate better image generation quality.", "section": "5 Discrete Image Generation"}, {"figure_path": "7AWMTPMZES/tables/tables_8_2.jpg", "caption": "Table 5: Confidence factors.", "description": "This table shows the FID scores on CIFAR-10 for different image generation settings using three different discrete image representations (Binary Coding, Fixed Embedding, and Trainable Embedding).  The results are shown for various values of the confidence factor 'r', ranging from 0 to 0.5.  A confidence factor of 0 represents the original diffusion process (without discrete priors), while higher values indicate increased reliance on the discrete boundaries. The FID score is a measure of image quality, with lower scores indicating higher quality. The table demonstrates the effect of the confidence factor on the generated image quality for each image representation.", "section": "5 Discrete Image Generation"}, {"figure_path": "7AWMTPMZES/tables/tables_21_1.jpg", "caption": "Table 4: FID scores on CIFAR-10.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores achieved by different models on the CIFAR-10 dataset.  The FID score is a metric used to evaluate the quality of generated images. Lower FID scores indicate better image quality.  The table compares the performance of the proposed approach using various discrete image representations (Binary Coding, Fixed Embedding, Trainable Embedding) and sampling methods (Gaussian, Deterministic).  It also includes comparisons to baseline methods like DDPM and DDIM.", "section": "5 Discrete Image Generation"}, {"figure_path": "7AWMTPMZES/tables/tables_21_2.jpg", "caption": "Table 1: Result of BLEU scores on machine translation and ROUGE scores on text summarization.", "description": "This table presents the results of the proposed approach and several baselines on four different tasks: three machine translation tasks (IWSLT14 DE-EN, WMT14 EN-DE, and WMT16 EN-RO) and one text summarization task (GIGAWORD).  The BLEU scores (BLEU-1/2/3/4) measure the performance of machine translation, while the ROUGE scores (ROUGE-1/2/L) evaluate the performance of text summarization.  The table compares the proposed approach against several autoregressive and diffusion-based models, highlighting its superior performance in several cases.  The 'Ours + Rerank' row indicates that reranking improved results further. ", "section": "4 Language Modeling"}, {"figure_path": "7AWMTPMZES/tables/tables_23_1.jpg", "caption": "Table 1: Result of BLEU scores on machine translation and ROUGE scores on text summarization.", "description": "This table presents the performance comparison of different models on machine translation and text summarization tasks.  It shows BLEU scores (BLEU-1/2/3/4) for three machine translation datasets (IWSLT14 DE-EN, WMT14 EN-DE, WMT16 EN-RO) and ROUGE scores (ROUGE-1/2/L) for a text summarization dataset (GIGAWORD).  The models compared include autoregressive transformers, several continuous diffusion models (D3PM, DiffuSeq, SeqDiffuSeq, Difformer, SEDD, Dinoiser), and the proposed \"Ours\" model.  The table highlights the superior performance of the proposed model, particularly surpassing previous state-of-the-art continuous diffusion language models and achieving competitive results compared to autoregressive transformers.", "section": "4 Language Modeling"}, {"figure_path": "7AWMTPMZES/tables/tables_24_1.jpg", "caption": "Table 1: Result of BLEU scores on machine translation and ROUGE scores on text summarization.", "description": "This table presents the performance comparison of different models on machine translation and text summarization tasks.  The models compared include autoregressive transformers and various diffusion models.  The evaluation metrics are BLEU scores (BLEU-1, BLEU-2, BLEU-3, BLEU-4) for machine translation and ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L) for summarization.  The results show how well each model performs compared to the state-of-the-art in these tasks.", "section": "4 Language Modeling"}]