[{"figure_path": "NtNTfRTjE8/tables/tables_4_1.jpg", "caption": "Table 1: Cross-scene detection experiments on ResNet-50 models from ForenSynths [4]. We use 2 sets of training images on different scenes, Bedroom and Church, to retrain the detectors. Detection accuracy (Acc.) (at a threshold of 50%) and Average Precision (AP) are reported.", "description": "This table presents the results of cross-scene detection experiments using ResNet-50 models.  Two different scene datasets (Bedroom and Church) were used to train the model separately. The table shows the accuracy (at 50% threshold) and Average Precision (AP) for real and fake images within the same scene and across different scenes, highlighting the impact of semantic artifacts on cross-scene generalization.", "section": "3.2 Analysis of the Impact of Semantic Artifacts"}, {"figure_path": "NtNTfRTjE8/tables/tables_4_2.jpg", "caption": "Table 1: Cross-scene detection experiments on ResNet-50 models from ForenSynths [4]. We use 2 sets of training images on different scenes, Bedroom and Church, to retrain the detectors. Detection accuracy (Acc.) (at a threshold of 50%) and Average Precision (AP) are reported.", "description": "This table presents the results of cross-scene detection experiments using ResNet-50 models.  Two different scene datasets (Bedroom and Church) were used to train the models, and each trained model was then tested on both datasets. The table reports both the accuracy (at 50% threshold) and the average precision (AP) for each model/dataset combination, demonstrating the impact of scene changes on the model's performance.", "section": "3.2 Analysis of the Impact of Semantic Artifacts"}, {"figure_path": "NtNTfRTjE8/tables/tables_7_1.jpg", "caption": "Table 3: Results of cross-scene generalization and open-world generalization. For cross-scene generalization, we average the results on 6 variants of Latent Diffusion (LSUN-Bedroom, LSUN-Church, ImageNet, CelebA, FFHQ, LAION). For open-world generalization, we average the results on all 31 test sets (including 18 DMs, 7 GANs, and 6 CNN-based generators). Bold represents the best and underline represents the second best. More Detailed results are shown in Table 4 and Table 5.", "description": "This table presents a comparison of the cross-scene and open-world generalization performance of different methods for detecting AI-generated images.  The cross-scene results average the performance across six variations of Latent Diffusion models trained on different datasets. The open-world results average performance across all 31 test datasets, encompassing various GANs, diffusion models, and CNN-based generative models.  The table highlights the superior performance of the proposed approach.", "section": "4 Experiments"}, {"figure_path": "NtNTfRTjE8/tables/tables_8_1.jpg", "caption": "Table 3: Results of cross-scene generalization and open-world generalization. For cross-scene generalization, we average the results on 6 variants of Latent Diffusion (LSUN-Bedroom, LSUN-Church, ImageNet, CelebA, FFHQ, LAION). For open-world generalization, we average the results on all 31 test sets (including 18 DMs, 7 GANs, and 6 CNN-based generators). Bold represents the best and underline represents the second best. More Detailed results are shown in Table 4 and Table 5.", "description": "This table presents a comparison of various methods for detecting AI-generated images.  It shows the average accuracy (Acc.) and mean average precision (mAP) across two different evaluation settings: cross-scene generalization (testing on images from different datasets but generated by the same model) and open-world generalization (testing on images from a wide variety of generative models and datasets). The results highlight the performance differences between methods and show which methods are more robust and generalize well.", "section": "4.2 Experimental Results"}, {"figure_path": "NtNTfRTjE8/tables/tables_8_2.jpg", "caption": "Table 3: Results of cross-scene generalization and open-world generalization. For cross-scene generalization, we average the results on 6 variants of Latent Diffusion (LSUN-Bedroom, LSUN-Church, ImageNet, CelebA, FFHQ, LAION). For open-world generalization, we average the results on all 31 test sets (including 18 DMs, 7 GANs, and 6 CNN-based generators). Bold represents the best and underline represents the second best. More Detailed results are shown in Table 4 and Table 5.", "description": "This table presents the results of a comprehensive evaluation on the proposed approach and other existing methods. The evaluation is conducted on two aspects: cross-scene generalization and open-world generalization. Cross-scene generalization assesses the performance of the methods when the input images are from different scenes but generated using the same generative model. Open-world generalization, on the other hand, evaluates the performance on a much broader range of datasets including diverse generative models and real-world images. The table shows the average accuracy and mean average precision (mAP) achieved by each method on both aspects and indicates the best performing methods.", "section": "4.2 Experimental Results"}, {"figure_path": "NtNTfRTjE8/tables/tables_9_1.jpg", "caption": "Table 6: Ablation study results on model depth.", "description": "This ablation study investigates the effect of varying the number of same-convolutional blocks (N) in the patch-based feature extraction network on the model's performance.  Different values of N correspond to varying receptive field sizes and model depths. The table presents the average accuracy (Avg. Acc.) and mean average precision (mAP) across three generalization tasks: cross-diffusion, cross-GAN/CNN, and open-world, for each value of N.  The results show that there is an optimal depth that balances the trade-off between underfitting (too few layers) and overfitting (too many layers).", "section": "A.3.2 Ablation studies on training set size"}, {"figure_path": "NtNTfRTjE8/tables/tables_9_2.jpg", "caption": "Table 7: Ablation study results on patch size.", "description": "This table presents the ablation study results focusing on the impact of different patch sizes on the model's performance.  It shows the average accuracy (Avg. Acc.) and mean average precision (mAP) across three different evaluation settings: cross-diffusion, cross-GAN/CNN, and open-world generalization.  The results demonstrate the effect of changing patch size (P) while keeping the number of convolutional blocks (N) consistent.  The optimal patch size seems to be P=32, indicating a balance between capturing local features and avoiding overfitting to global semantic artifacts.", "section": "4 Experiments"}, {"figure_path": "NtNTfRTjE8/tables/tables_14_1.jpg", "caption": "Table 3: Results of cross-scene generalization and open-world generalization. For cross-scene generalization, we average the results on 6 variants of Latent Diffusion (LSUN-Bedroom, LSUN-Church, ImageNet, CelebA, FFHQ, LAION). For open-world generalization, we average the results on all 31 test sets (including 18 DMs, 7 GANs, and 6 CNN-based generators). Bold represents the best and underline represents the second best. More Detailed results are shown in Table 4 and Table 5.", "description": "This table presents the average accuracy and mean average precision (mAP) achieved by various methods on two distinct generalization tasks: cross-scene and open-world.  Cross-scene generalization tests the models on Latent Diffusion models trained on six different datasets, while open-world generalization assesses performance across all 31 datasets.  The best and second-best results are highlighted.", "section": "4.2 Experimental Results"}, {"figure_path": "NtNTfRTjE8/tables/tables_15_1.jpg", "caption": "Table 3: Results of cross-scene generalization and open-world generalization. For cross-scene generalization, we average the results on 6 variants of Latent Diffusion (LSUN-Bedroom, LSUN-Church, ImageNet, CelebA, FFHQ, LAION). For open-world generalization, we average the results on all 31 test sets (including 18 DMs, 7 GANs, and 6 CNN-based generators). Bold represents the best and underline represents the second best. More Detailed results are shown in Table 4 and Table 5.", "description": "This table presents the average accuracy (Avg. Acc.) and mean Average Precision (mAP) for cross-scene and open-world generalization.  Cross-scene generalization uses 6 variants of the Latent Diffusion model, while open-world includes 31 test sets (7 GANs, 18 Diffusion Models, 6 CNN-based models).  The best performing methods are highlighted.", "section": "4 Experiments"}]