[{"figure_path": "deZpmEfmTo/tables/tables_6_1.jpg", "caption": "Table 1: Benchmarking over autonomous driving datasets under various weather and time conditions. \u2020 signifies that the methods employ WordNet to retrieve category definitions given category names, and CLIP to predict classification pseudo labels for objects. We adopt AP50 in evaluations. The results of all methods are acquired with the same baseline [3] as shown in the first row.", "description": "This table presents a comparison of the domain adaptation performance of the proposed method (KGD) against existing methods on 11 autonomous driving datasets under various weather and time conditions.  The AP50 (Average Precision at 50% Intersection over Union) metric is used to evaluate the performance, and the results are compared with a common baseline. The \"\u2020\" symbol indicates that certain methods used WordNet and CLIP for enhanced performance.", "section": "4.1 Results"}, {"figure_path": "deZpmEfmTo/tables/tables_7_1.jpg", "caption": "Table 2: Benchmarking over common objects datasets, intelligent surveillance datasets, and artistic illustration datasets. \u2020 signifies that the methods employ WordNet to retrieved category definitions given category names, and CLIP to predict classification pseudo labels for objects. We adopt AP50 in evaluations. The results of all methods are acquired with the same baseline [3] as shown in first row.", "description": "This table compares the performance of the proposed KGD method against several state-of-the-art domain adaptation methods on three different types of datasets: common objects, intelligent surveillance, and artistic illustration.  The results are reported in terms of Average Precision at 50% Intersection over Union (AP50), a standard metric for object detection. The baseline method used for comparison is Detic [3]. The '+' symbol indicates that the methods used both WordNet and CLIP.", "section": "4.1 Results"}, {"figure_path": "deZpmEfmTo/tables/tables_7_2.jpg", "caption": "Table 3: Ablation studies of KGD with Language Knowledge Graph Distillation (KGD-L) and Vision Knowledge Graph Distillation (KGD-V). The experiments are conducted on the Cityscapes.", "description": "This table presents the ablation study results for the Knowledge Graph Distillation (KGD) method. It shows the impact of using Language Knowledge Graph Distillation (KGD-L) and Vision Knowledge Graph Distillation (KGD-V) separately and together on the Cityscapes dataset.  The baseline is Detic [3] without any adaptation, and the results are measured using the AP50 metric.", "section": "4.2 Ablation studies"}, {"figure_path": "deZpmEfmTo/tables/tables_7_3.jpg", "caption": "Table 4: Comparisons with existing CLIP knowledge distillation methods on LVD adaptation. For a fair comparison, we incorporate them with Mean Teacher Method (the columns with 'MT+'). The results of all methods are acquired with the same baseline [3] as shown in the first column.", "description": "This table compares the proposed KGD method with other state-of-the-art CLIP knowledge distillation methods for Large Vocabulary Object Detector (LVD) adaptation.  It shows the AP50 (Average Precision at 50% Intersection over Union) improvement achieved by each method over a baseline Detic [3] model on the Cityscapes dataset. The comparison highlights KGD's superior performance.", "section": "4.1 Results"}, {"figure_path": "deZpmEfmTo/tables/tables_8_1.jpg", "caption": "Table 5: Study of different KGD-L strategies. The experiments are conducted on the Cityscapes.", "description": "This table presents the ablation study of different Language Knowledge Graph Distillation (KGD-L) strategies. It compares the performance of using only category names, WordNet synset definitions, and WordNet hierarchy for LKG extraction. The results show that using WordNet hierarchy for LKG extraction yields the best performance, suggesting its effectiveness in capturing comprehensive language knowledge for improved object classification.  The experiments were conducted on the Cityscapes dataset.", "section": "4.2 Ablation studies"}, {"figure_path": "deZpmEfmTo/tables/tables_8_2.jpg", "caption": "Table 6: Study of different KGD-L strategies. The experiments are conducted on the Cityscapes.", "description": "This table presents an ablation study on the Cityscapes dataset to analyze the impact of different strategies within the Language Knowledge Graph Distillation (KGD-L) component of the proposed Knowledge Graph Distillation (KGD) method.  Specifically, it compares the performance of using just feature distance for LKG encapsulation versus the full LKG encapsulation method. The results highlight the effectiveness of the complete LKG encapsulation approach in improving the AP50 score.", "section": "4.2 Ablation studies"}, {"figure_path": "deZpmEfmTo/tables/tables_9_1.jpg", "caption": "Table 7: Studies of different KGD-V strategies. The experiments are conducted on the Cityscapes.", "description": "This table presents the ablation study of different KGD-V strategies on the Cityscapes dataset.  It compares the performance of using a static VKG, a dynamic VKG without smoothing, and a dynamic VKG with smoothing.  The results show the AP50 scores for each method, highlighting the impact of the dynamic VKG and smoothing on the overall performance.", "section": "4.2 Ablation studies"}, {"figure_path": "deZpmEfmTo/tables/tables_9_2.jpg", "caption": "Table 8: Parameter analysis of KGD for the pseudo label generation threshold \u03c4.", "description": "This table shows the result of parameter study on the impact of pseudo label generation threshold (\u03c4) on the performance of KGD.  The experiment is conducted on the Cityscapes dataset, varying \u03c4 from 0.15 to 0.35. The AP50 (Average Precision at 50% Intersection over Union) metric is used to evaluate the performance.", "section": "4 Experiments"}, {"figure_path": "deZpmEfmTo/tables/tables_17_1.jpg", "caption": "Table 9: Parameter analysis of KGD for \u03bb.", "description": "This table shows the result of the parameter study for \u03bb in Equation (12) of the paper.  The authors varied \u03bb from 0.99 to 0.999999 and measured the AP50 performance on the Cityscapes dataset.  The results show that an appropriate value of \u03bb (0.9999) is necessary to balance prompt updating and noise reduction.", "section": "A.4 Additional Discussion"}, {"figure_path": "deZpmEfmTo/tables/tables_18_1.jpg", "caption": "Table 10: Parameter analysis of KGD for \u03b1.", "description": "This table presents the results of a parameter study on the effect of the \u03b1 parameter in the KGD model on the AP50 metric.  Different values of \u03b1 were tested on the Cityscapes dataset.  The results show that an optimal value of \u03b1 exists, with performance degrading when the value is too high or too low.", "section": "4.2 Ablation studies"}, {"figure_path": "deZpmEfmTo/tables/tables_18_2.jpg", "caption": "Table 11: Ablation studies of KGD with Language Knowledge Graph Distillation (KGD-L) and Vision Knowledge Graph Distillation (KGD-V). The experiments are conducted on the Cityscapes, BAAI, VOC, and Clipart1k.", "description": "This table presents the ablation study results of the proposed Knowledge Graph Distillation (KGD) method. It shows the performance improvements achieved by using only Language Knowledge Graph Distillation (KGD-L), only Vision Knowledge Graph Distillation (KGD-V), and the combination of both KGD-L and KGD-V. The experiments were conducted on four different datasets: Cityscapes, BAAI, VOC, and Clipart1k, demonstrating the effectiveness and complementarity of both KGD-L and KGD-V in improving the performance of object detection.", "section": "4.2 Ablation studies"}, {"figure_path": "deZpmEfmTo/tables/tables_19_1.jpg", "caption": "Table 12: Combination of language knowledge graph extraction and vision knowledge graph strategies. The experiments are conducted on the Cityscapes.", "description": "This table presents ablation study results on the Cityscapes dataset, comparing different combinations of language knowledge graph (LKG) extraction and vision knowledge graph (VKG) extraction methods within the Knowledge Graph Distillation (KGD) framework. It shows the impact of using WordNet hierarchy for LKG extraction and dynamic VKG extraction on the overall performance (AP50).", "section": "4.2 Ablation studies"}, {"figure_path": "deZpmEfmTo/tables/tables_19_2.jpg", "caption": "Table 3: Ablation studies of KGD with Language Knowledge Graph Distillation (KGD-L) and Vision Knowledge Graph Distillation (KGD-V). The experiments are conducted on the Cityscapes.", "description": "This table shows the ablation study results of the proposed Knowledge Graph Distillation (KGD) method.  It compares the performance of KGD using only language knowledge graph distillation (KGD-L), only vision knowledge graph distillation (KGD-V), and both combined. The results demonstrate the individual contributions and complementary effects of both language and vision knowledge graph distillation in improving the performance of object detection.", "section": "4.2 Ablation studies"}, {"figure_path": "deZpmEfmTo/tables/tables_19_3.jpg", "caption": "Table 14: Study of different adaptation strategies for LVDs on Cityscapes dataset [73].", "description": "This table compares the performance of different adaptation strategies for large vocabulary object detectors (LVDs) on the Cityscapes dataset.  It shows the AP50 (average precision at 50% Intersection over Union) for various methods, including the baseline Detic [3], several methods using different pseudo-label generation strategies (offline Detic, offline CLIP, online VL-PLM, online RegionCLIP), and finally the proposed Knowledge Graph Distillation (KGD) method. The table highlights the superior performance of KGD compared to other approaches in adapting the LVD to this specific dataset.", "section": "4.1 Results"}, {"figure_path": "deZpmEfmTo/tables/tables_20_1.jpg", "caption": "Table 1: Benchmarking over autonomous driving datasets under various weather and time conditions. \u2020 signifies that the methods employ WordNet to retrieve category definitions given category names, and CLIP to predict classification pseudo labels for objects. We adopt AP50 in evaluations. The results of all methods are acquired with the same baseline [3] as shown in the first row.", "description": "This table presents a comparison of the domain adaptation performance of the proposed Knowledge Graph Distillation (KGD) method against existing state-of-the-art methods on 11 widely used downstream detection datasets related to autonomous driving.  The datasets are categorized by weather (rainy, snowy, overcast, cloudy, foggy) and time of day (daytime, dawn/dusk, night). The results are reported in terms of AP50 improvements over a baseline method.  The '+' symbol next to some methods indicates that WordNet and CLIP were used in conjunction with that method.", "section": "4.1 Results"}, {"figure_path": "deZpmEfmTo/tables/tables_20_2.jpg", "caption": "Table 2: Benchmarking over common objects datasets, intelligent surveillance datasets, and artistic illustration datasets. \u2020 signifies that the methods employ WordNet to retrieved category definitions given category names, and CLIP to predict classification pseudo labels for objects. We adopt AP50 in evaluations. The results of all methods are acquired with the same baseline [3] as shown in first row.", "description": "This table compares the performance of the proposed KGD method against several state-of-the-art domain adaptation methods on three types of datasets: common objects, intelligent surveillance, and artistic illustrations.  The results are measured using AP50 (Average Precision at 50% Intersection over Union), a common metric in object detection.  The \"\u2020\" symbol indicates methods that leverage WordNet for category definition retrieval and CLIP for pseudo label generation.", "section": "4.1 Results"}, {"figure_path": "deZpmEfmTo/tables/tables_21_1.jpg", "caption": "Table 17: Study of different distance metrics for constructing KG. The experiments are conducted on the Cityscapes dataset.", "description": "This table presents the results of an ablation study comparing different distance metrics used in constructing the knowledge graph (KG) within the Knowledge Graph Distillation (KGD) framework. The experiment was conducted on the Cityscapes dataset, and the performance is measured by the Average Precision at 50% Intersection over Union (AP50).  The results show that the choice of distance metric has a relatively small impact on the overall performance.", "section": "4.2 Ablation studies"}, {"figure_path": "deZpmEfmTo/tables/tables_21_2.jpg", "caption": "Table 18: Training and inference time analysis of all the compared methods. The experiments are conducted on one RTX 2080Ti. \u2020 signifies that the methods employ WordNet to retrieve category descriptions given category names, and CLIP to predict classification pseudo labels for objects.", "description": "This table presents a detailed comparison of the training and inference time, memory usage, and computational overhead of several domain adaptive detection methods, including the proposed KGD, on a single RTX 2080Ti GPU.  The methods compared include various baselines and state-of-the-art approaches. The use of WordNet and CLIP in some methods is indicated with a dagger symbol (\u2020).  The table provides a comprehensive performance comparison across all methods, highlighting the efficiency and resource requirements of the proposed KGD compared to existing approaches.", "section": "A.4.8 Training and inference overhead analysis"}, {"figure_path": "deZpmEfmTo/tables/tables_22_1.jpg", "caption": "Table 1: Benchmarking over autonomous driving datasets under various weather and time conditions. \u2020 signifies that the methods employ WordNet to retrieve category definitions given category names, and CLIP to predict classification pseudo labels for objects. We adopt AP50 in evaluations. The results of all methods are acquired with the same baseline [3] as shown in the first row.", "description": "This table presents a comparison of the domain adaptation performance of the proposed Knowledge Graph Distillation (KGD) method against existing methods on 11 widely-used downstream detection datasets related to autonomous driving.  The datasets are categorized by weather (rainy, snowy, overcast, cloudy, foggy) and time of day (daytime, dawn & dusk, night). The performance metric used is AP50 (Average Precision at 50% Intersection over Union).  The \"\u2020\" symbol indicates methods that leverage WordNet and CLIP for enhanced performance.  The results are benchmarked against a common baseline [3], which is included in the first row of the table.", "section": "4.1 Results"}, {"figure_path": "deZpmEfmTo/tables/tables_22_2.jpg", "caption": "Table 20: Experiments with Open-Vocabulary Detectors over Cityscapes dataset. We adopt AP50 in evaluations. We can observe that our proposed KGD can also improve the performance of OVDs (e.g., GLIP [96], VILD [84], RegionKD [85], UniDet [97], and RegionCLIP [92]) significantly, validating the generalization ability of our KGD on different detectors.", "description": "This table shows the performance improvement when using KGD with several open-vocabulary detectors on the Cityscapes dataset.  The baseline performance of each detector is compared to its performance when combined with KGD, demonstrating KGD's effectiveness across different detection models. The AP50 metric is used to evaluate performance.", "section": "A.4.10 Experiments with open-vocabulary detectors"}, {"figure_path": "deZpmEfmTo/tables/tables_23_1.jpg", "caption": "Table 21: Aggregate results over 11 widely studied datasets. \u2020 signifies that the methods employ WordNet to retrieve category definitions given category names, and CLIP to predict classification pseudo labels for objects. The results of all methods are acquired with the same baseline [3] as shown in the first column.", "description": "This table presents a comparison of the proposed KGD method against existing state-of-the-art domain adaptation methods across eleven diverse downstream detection datasets.  It highlights the consistent superior performance of KGD in terms of AP50 (average precision at 50% IoU), demonstrating its effectiveness in adapting pre-trained large vocabulary object detectors to various domains.", "section": "4.1 Results"}, {"figure_path": "deZpmEfmTo/tables/tables_23_2.jpg", "caption": "Table 22: Benchmarking Detic over Cityscapes [73] dataset with AP50, Category-agnostic AP50, and GT bounding box-corrected AP50.", "description": "This table presents a breakdown of the performance of the Detic model [3] on the Cityscapes dataset [73],  analyzing its object detection capabilities by separating object localization and classification accuracy.  It shows the standard AP50 metric, and two additional metrics that isolate the contributions of localization and classification independently. The improvement shown in parentheses highlights the effect of correcting either classification or localization errors on the overall AP50 score.", "section": "4.1 Results"}]