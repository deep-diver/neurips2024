[{"type": "text", "text": "Provable Partially Observable Reinforcement Learning with Privileged Information ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yang Cai1 Xiangyu Liu2 Argyris Oikonomou1 Kaiqing Zhang2 1 Yale University 2University of Maryland, College Park yang.cai@yale.edu, xyliu999@umd.edu argyris.oikonomou@yale.edu, kaiqing@umd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Partial observability of the underlying states generally presents significant challenges for reinforcement learning (RL). In practice, certain privileged information, e.g., the access to states from simulators, has been exploited in training and has achieved prominent empirical successes. To better understand the beneftis of privileged information, we revisit and examine several simple and practically used paradigms in this setting. Specifically, we first formalize the empirical paradigm of expert distillation (also known as teacher-student learning), demonstrating its pitfall in finding near-optimal policies. We then identify a condition of the partially observable environment, the deterministic fliter condition, under which expert distillation achieves sample and computational complexities that are both polynomial. Furthermore, we investigate another successful empirical paradigm of asymmetric actor-critic, and focus on the more challenging setting of observable partially observable Markov decision processes. We develop a belief-weighted asymmetric actor-critic algorithm with polynomial sample and quasi-polynomial computational complexities, in which one key component is a new provable oracle for learning belief states that preserves filter stability under a misspecified model, which may be of independent interest. Finally, we also investigate the provable efficiency of partially observable multi-agent RL (MARL) with privileged information. We develop algorithms featuring centralized-training-with-decentralized-execution, a popular framework in empirical MARL, with polynomial sample and (quasi)polynomial computational complexities in both paradigms above. Compared with a few recent related theoretical studies, our focus is on understanding practically inspired algorithmic paradigms, without computationally intractable oracles. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In most real-world applications of reinforcement learning (RL), e.g., perception-based robot learning [45, 3], autonomous driving [72, 40], dialogue systems [87], and clinical trials [76], only partial observations of the environment state are available for sequential decision-making. Such partial observability presents significant challenges for efficient decision-making and learning, with known computational [65] and statistical [42, 35] barriers under the general model of partially observable Markov decision processes (POMDPs). The curse of partial observability becomes severer when multiple RL agents interact, where not only the environment state, but also other agents\u2019 information, are not fully-observable in decision-making [84, 79]. ", "page_idx": 0}, {"type": "text", "text": "On the other hand, a flurry of empirical paradigms has made partially observable (multi-agent) RL promising in practice. One notable example is to exploit the privileged information that may be available (only) during training. The privileged information usually includes direct access to the underlying states, as well as access to other agents\u2019 observations/actions in multi-agent RL (MARL), due to the use of simulators and/or high-precision sensors for training. The latter is also known as the centralized-training-with-decentralized-execution (CTDE) framework in deep MARL, and has become prevalent in practice [52, 69, 22, 81]. These approaches can be mainly categorized into two types: i) privileged policy learning, where an expert/teacher policy is trained with privileged information, and then distilled into a student partially observable policy. This expert distillation, also known as teacher-student learning, approach has been the key to some empirical successes in robotic locomotion [44, 58] and autonomous driving [14]; ii) privileged value learning, where a value function is trained conditioned on privileged information, and used to improve a partially observable policy. It is typically instantiated as the asymmetric actor-critic algorithm [67], and serves as the backbone of some high-profiled successes in robotic manipulation [45, 3] and MARL [52, 81]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the remarkable empirical successes, theoretical understandings of partially observable RL with privileged information have been rather limited, except for a few recent prominent advances in RL with hindsight observability [43, 30] (see Appendix B for a detailed discussion). However, most of these theoretically sound algorithms are different from those used in practice, and require computationally intractable oracles to achieve provable sample efficiency. The soundness and efficiency of the aforementioned paradigms used in practice remain elusive. In this work, we examine both paradigms of expert distillation and asymmetric actor-critic, with foresight privileged information as in these empirical works. In contrast to [43, 30], which purely focused on sample efficiency, we aim to understand the beneftis of privileged information by examining these practically inspired paradigms under several POMDP models, without computationally intractable oracles. We defer a detailed literature review to Appendix B, and summarize our contribution as follows. ", "page_idx": 1}, {"type": "text", "text": "Contributions. We first formalize the empirical paradigm of expert distillation, and demonstrate its pitfall in distilling near-optimal policies even in observable POMDPs, a model class that was recently shown to allow provable partially observable RL without computationally intractable oracles [25]. We then identify a new condition for POMDPs, the deterministic fliter condition, and establish sample and computational complexities that are both polynomial for expert distillation. The new condition is weaker and thus encompasses several known (statistically) tractable POMDP models (see Figure 1 for a summary). Further, we revisit the asymmetric actor-critic paradigm and analyze its efficiency under the more challenging setting of observable POMDPs above (where expert distillation fails). Identifying the inefficiency of vanilla asymmetric actor-critic, and inspired by the empirical success in belief-state-learning, we develop a new belief-weighted version of asymmetric actorcritic, with polynomial-sample and quasi-polynomial-time complexities. Key to the results is a new belief-state learning oracle that preserves filter stability under a misspecified model, which may be of independent interest. Finally, we also investigate the provable efficiency of partially observable multi-agent RL with privileged information, by studying algorithms under the CTDE framework, with polynomial-sample and (quasi-)polynomial-time complexities in both paradigms above. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Partially Observable RL (with Privileged Information) ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Model. Consider a POMDP characterized by a tuple $\\mathcal{P}\\,=\\,(H,S,\\mathcal{A},\\mathcal{O},\\mathbb{T},\\mathbb{O},\\mu_{1},r)$ , where $H$ denotes the length of each episode, $\\boldsymbol{S}$ is the state space with $|S|=S$ , $\\boldsymbol{\\mathcal{A}}$ denotes the action space with $|{\\mathcal{A}}|\\;=\\;A$ . We use $\\mathbb{T}\\,=\\,\\{\\mathbb{T}_{h}\\}_{h\\in[H]}$ to denote the collection of transition matrices, so that $\\mathbb{T}_{h}(\\cdot\\,|\\,s,a)\\in\\Delta(S)$ gives the probability of the next state if action $a$ is taken at state $s$ and step $h$ . In the following discussions, for any given $a$ , we treat $\\mathbb{T}_{h}(a)\\in\\mathbb{R}^{|S|\\times|S|}$ as a matrix, where each row gives the probability for reaching each next state from different current states. We use $\\mu_{1}$ to denote the distribution of the initial state $s_{1}$ , and $\\scriptscriptstyle\\mathcal{O}$ to denote the observation space with $|\\mathcal{O}|=O$ . We use $\\mathbb{O}=\\{\\mathbb{O}_{h}\\}_{h\\in[H]}$ to denote the collection of emission matrices, so that $\\mathbb{O}_{h}(\\cdot\\,|\\,s)\\in\\dot{\\Delta}(\\mathcal{O})$ gives the emission distribution over the observation space $\\scriptscriptstyle\\mathcal{O}$ at state $s$ and step $h$ . For notational convenience, we will at times adopt the matrix convention, where $\\mathbb{O}_{h}$ is a matrix with rows $\\mathbb{O}_{h}(\\cdot\\,|\\,s)$ for each $s\\in S$ . Finally, $r=\\{r_{h}\\}_{h\\in[H]}$ is a collection of reward functions, so that $r_{h}(s,a)\\in[0,1]$ is the reward given the state $s$ and action $a$ at step $h$ . When privileged information is available, the agent can observe the underlying state $s\\in S$ directly during training (only). We thus denote the trajectory until step $h$ with states as $\\overline{{\\tau}}_{h}=(s_{1:h},o_{1:h},a_{1:h-1})$ , the one without states as $\\tau_{h}=(o_{1:h},a_{1:h-1})$ , and its space as $\\mathcal{T}_{h}$ . Finally, we use $b_{h}(\\tau_{h})$ to denote the posterior distribution over the underlying state at step $h$ given history $\\tau_{h}$ , which is known as the belief state (c.f. Appendix C.1 for more details). ", "page_idx": 1}, {"type": "text", "text": "Policy and value function. We define a stochastic policy at step $h$ as: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\pi_{h}:{\\mathcal{O}}^{h}\\times{\\mathcal{A}}^{h-1}\\to\\Delta(A),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the agent bases on the entire (partially observable) history for decision-making. The corresponding policy class is denoted as $\\Pi_{h}$ . We further denote $\\Pi\\,=\\,\\times\\,_{h\\in[H]}\\Pi_{h}$ . We also define $\\Pi^{\\mathrm{gen}}:=\\{\\pi_{1:H}\\,|\\,\\pi_{h}:{\\mathcal{S}}^{h}\\times{\\mathcal{O}}^{h}\\times{\\mathcal{A}}^{h-1}\\rightarrow\\Delta(A)$ for $h\\in[H]\\}$ to be the most general policy space in partially observable RL with privileged state information, which can potentially depend on all historical states, observations, and actions. It can be seen that $\\Pi\\Sigma\\Pi^{\\mathrm{gen}}$ . We may also use policies that only receive a finite memory instead of the whole history as inputs: fix an integer $L>0$ , we define the policy space $\\Pi^{L}$ to be the space of all possible policies $\\pi=\\pi_{1:H}:=(\\pi_{h})_{h\\in[H]}$ such that $\\pi_{h}:{\\mathcal{Z}}_{h}\\to\\Delta(A)$ with $\\mathcal{Z}_{h}:=\\mathcal{O}^{\\operatorname*{min}\\{L,h\\}}\\times\\mathcal{A}^{\\operatorname*{min}\\{L,h\\}}$ for each $h\\in[H]$ . Finally, we define the space of state-based policies as $\\Pi_{S}$ , i.e., for any $\\pi=\\pi_{1:H}\\in\\Pi_{S}$ , $\\pi_{h}:S\\to\\Delta(A)$ for all $h\\in[H]$ . ", "page_idx": 2}, {"type": "text", "text": "Given the POMDP model $\\mathcal{P}$ , we write PsP1:H+1,a1:H,o1:H\u223c\u03c0(E) to denote the event E when $\\left({{s_{1:H+1}},{a_{1:H}},{o_{1:H}}}\\right)$ is drawn as a trajectory following the policy $\\pi$ in the model $\\mathcal{P}$ . We will also use the shorthand notation $\\mathbb{P}^{\\pi,\\mathcal{P}}(\\cdot)$ if $\\left({{s_{1:H+1}},{a_{1:H}},{o_{1:H}}}\\right)$ is evident. We write $\\mathbb{E}_{\\pi}^{\\mathcal{P}}[\\cdot]$ to denote the expectation similarly. We define the value function at step h as V h\u03c0,P(yh) := E\u03c0P [ tH= $\\begin{array}{r}{V_{h}^{\\pi,\\mathcal{P}}(y_{h}):=\\mathbb{E}_{\\pi}^{\\mathcal{P}}[\\sum_{t=h}^{H}r_{t}(s_{t},a_{t})\\:|\\:y_{h}]}\\end{array}$ denoting the expected accumulated rewards from step $h$ , where $y_{h}\\,\\subseteq\\,(s_{1:h},o_{1:h},a_{1:h-1})$ , and we slightly abuse the notation by treating as a set the sequence of states $s_{1:h}$ , the sequence of observations $O_{1:h}$ , and the sequence of actions $a_{1:h-1}$ up to time $h$ , which are the available information to the agent at step $h$ . We say $y_{h}$ is reachable if there exists some policy $\\pi\\in\\Pi^{\\mathrm{gen}}$ such that $\\mathbb{P}^{\\pi,\\mathcal{P}}(y_{h})>0$ For $h=1$ , we adopt the simplified notation $\\begin{array}{r}{\\boldsymbol{v}^{\\mathcal{P}}(\\pi)=\\mathbb{E}_{\\pi}^{\\mathcal{P}}[\\sum_{h=1}^{H}r_{h}(s_{h},a_{h})]}\\end{array}$ . Meanwhile, we also define $\\begin{array}{r}{Q_{h}^{\\pi,\\mathcal{P}}(y_{h},a_{h}):=\\mathbb{E}_{\\pi}^{\\mathcal{P}}[\\sum_{t=h}^{H}r_{t}(s_{t},a_{t})\\:|\\:y_{h},a_{h}]}\\end{array}$ . We denote the occupancy measure on the state space as $d_{h}^{\\pi,\\mathcal{P}}(s_{h})=\\mathbb{P}^{\\pi,\\mathcal{P}}(s_{h})$ . The goal of learning in POMDPs is to find the optimal policy that maximizes the expected accumulated reward over the policies that take $\\tau_{h}$ as input at each step $h\\in[H]$ , i.e., those $\\pi\\in\\Pi$ . Formally, we define: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 $\\epsilon$ -optimal policy). Given $\\epsilon\\mathrm{~>~0~}$ , a policy $\\pi^{\\star}\\in\\Pi$ is $\\epsilon$ -optimal, if $v^{\\mathcal{P}}(\\pi^{\\star})\\;\\geq\\;$ $\\begin{array}{r}{\\operatorname*{max}_{\\pi\\in\\Pi}v^{\\mathcal{P}}(\\pi)-\\epsilon}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Learning with privileged information. Common RL algorithms for POMDPs deal with the scenario where during both the training and test time, the agent can only observe its historical observations and actions $\\tau_{h}$ at step $h$ , while the states are not accessible. In other words, the agent can only utilize policies from $\\Pi$ to interact with the environment. In contrast, in settings with privileged information, e.g., training in simulators and/or using sensors with higher precision, the underlying state can be used in training. Thus, the agent is allowed to utilize policies from the class $\\mathrm{\\Pi_{\\Pi}^{\\mathrm{gen}}}$ during training. Meanwhile, the objective is still to find the optimal history-dependent policy in the space of $\\Pi$ , since at test time, the agent cannot access the state information anymore, and it is the performance for such policies that matters eventually. For simplicity, we assume the reward function is known since under our privileged information setting, learning the reward function is much easier than learning the transition and emission, and the sample/computational complexity for the former is dominated by that for the latter. This assumption has also been made for learning in POMDPs without privileged information [35, 46, 47]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Partially Observable Multi-agent RL with Information Sharing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Partially observable stochastic games (POSGs) are a natural generalization of POMDPs with multiple agents of potentially independent interests. We define a POSG with $n$ agents by a tuple $\\mathcal{G}\\,=$ $(H,S,\\{A_{i}\\}_{i=1}^{n},\\{\\mathcal{O}_{i}\\}_{i=1}^{n},\\mathbb{T},\\mathbb{O},\\mu_{1},\\{r_{i}\\}_{i=1}^{n})$ , where each agent $i$ has its individual action space $A_{i}$ , observation space $O_{i}$ , and reward function $r_{i}\\,=\\,\\{r_{i,h}\\}_{h\\in[H]}$ with $r_{i,h}(s,a)\\in[0,1]$ denoting the reward given state $s$ and joint action $a$ for agent $i$ at step $h$ . An episode of POSG proceeds as follows: at each step $h$ and state $s_{h}$ , a joint observation is drawn from $\\bar{(o_{i,h})}_{i\\in[n]}\\sim\\mathbb{O}_{h}(\\cdot\\,\\bar{|\\,s_{h})}$ , and each agent receives its own observation $\\sigma_{i,h}$ , takes the corresponding action $a_{i,h}$ , obtains the reward $r_{i,h}(s_{h},a_{h})$ , where $a_{h}:=(a_{i,h})_{i\\in[n]}$ , and then the system transitions to the next state as $s_{h+1}\\sim\\mathbb{T}_{h}\\big(\\cdot\\,\\big|\\,s_{h},a_{h}\\big)$ Notably, each agent $i$ may not only know its local information $\\left(o_{i,1:h},a_{i,1:h-1}\\right)$ , but also information from some other agents. Therefore, we denote the information available to each agent $i$ at step $h$ also as $\\tau_{i,h}\\subseteq(o_{1:h},a_{1:h-1})$ and define the common information as $c_{h}=\\cap_{i\\in[n]}\\tau_{i,h}$ and $p$ rivate information as $p_{i,h}=\\tau_{i,h}\\setminus c_{h}$ . We denote the space for common information and private information as $\\ensuremath{\\mathcal{C}}_{h}$ and $\\mathcal{P}_{i,h}$ for each agent $i$ and step $h$ . The joint private information at step $h$ is denoted as $p_{h}=(p_{i,h})_{i\\in[n]}$ where the collection of the joint private information is given by $\\mathcal{P}_{h}\\,=\\,\\mathcal{P}_{1,h}\\,\\times\\,\\cdot\\,\\cdot\\,\\times\\,\\mathcal{P}_{n,h}$ . We refer more examples of this setting of POSG with information-sharing to Appendix C.2 (and also [61, 62, 50]). Correspondingly, the policy each agent $i$ deploys at test time takes the form of $\\pi_{i,h}:\\Omega_{h}\\times\\mathcal{C}_{h}\\times\\mathcal{P}_{i,h}\\to\\Delta(A_{i})$ , where $\\Omega_{h}$ is the space of random seeds. We denote the policy ", "page_idx": 2}, {"type": "table", "img_path": "o3i1JEfzKw/tmp/ea6b37b86b09693ba16a828ef145d7b33e3f17df355f2c14a9ddebeffff355a2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Table 1: Comparison of the theoretical guarantees   \nwith and without privileged information. PI: priv  \nileged information; STD: structural assumptions Figure 1: A landscape of POMDP models that on transition dynamics, e.g., deterministic transi- partially observable RL with privileged information or reachability of all states; SL: supervised tion can/cannot address. The axes denote the \u201crelearning; FA: function approximation; WSE: well- strictiveness\u201d of the assumptions, on the emission separated emission. channels and transition dynamics, respectively. ", "page_idx": 3}, {"type": "text", "text": "space for agent $i$ as $\\Pi_{i}$ . If $\\pi_{i,h}$ takes the state $s_{h}$ instead of $(c_{h},p_{i,h})$ as input, we denote its policy space as $\\Pi_{S,i}$ , e.g., for each agent $i$ , and policy $\\pi_{1:H}\\in\\Pi_{S,i}$ , we have $\\pi_{i,h}:S\\to\\Delta(A_{i})$ for each step $h\\in[H]$ . Similar to the POMDP setting, we define $\\mathrm{\\Pi_{\\Pi}^{\\mathrm{gen}}}$ to be the most general policy space, i.e., $\\Pi^{\\mathrm{gen}}:=\\bar{\\{\\pi_{1:H}\\,|\\,}}\\pi_{h}:\\!S^{h}\\times\\mathcal{O}^{h}\\times\\mathcal{A}^{h-1}\\rightarrow\\Delta(A)$ for $h\\in[H]\\}$ . Note that this model covers several recent POSG models studied for partially observable MARL, e.g., [48, 27]. For example, at each step $h$ , if there is no shared information, then $c_{h}=\\emptyset$ , and if all history information is shared, then $p_{i,h}=\\emptyset$ for all $i\\in[n]$ . In privileged-information-based learning, the training algorithm may exploit not only the underlying state information, but also the observations and actions of other agents. ", "page_idx": 3}, {"type": "text", "text": "Solution concepts. The solution concepts for POSGs are usually the equilibria, particularly Nash equilibrium (NE) for two-player zero-sum games (i.e., when $n\\,=\\,2$ and $r_{1,h}+r_{2,h}\\,=\\,1)$ ,1 and correlated equilibrium (CE) or coarse correlated equilibrium (CCE) for general-sum games. We defer the formal definitions of these standard solution concepts to Appendix C.2. ", "page_idx": 3}, {"type": "text", "text": "2.3 Technical Assumptions for Computational Tractability ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A key technical assumption is that the POMDPs/POSGs we consider satisfy an observability assumption, as outlined below. This observability assumption allows us to use short memory policies to approximate the optimal policy, and yields quasi-polynomial-time complexity for both planning and learning in POMDPs/POSGs [26, 25, 50]. Meanwhile, we defer an additional assumption to ensure the traceability for solving POSGs to Appendix C.3. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.2 $\\gamma$ -observability [20, 26, 25]). Let $\\gamma>0$ . For $h\\in[H]$ , we say that the matrix $\\mathbb{O}_{h}$ satisfies the $\\gamma.$ -observability assumption if for each $h\\in[H]$ , for any $b,b^{\\prime}\\in\\Delta(S)$ , $\\left\\|\\mathbb{O}_{h}^{\\top}b-\\mathbb{O}_{h}^{\\top}b^{\\prime}\\right\\|_{1}\\geq$ $\\gamma\\left\\|b-b^{\\prime}\\right\\|_{1}$ . A POMDP/POSG satisfies $\\gamma.$ -observability if all its $\\mathbb{O}_{h}$ for $h\\in[H]$ do so. ", "page_idx": 3}, {"type": "text", "text": "3 Revisiting Empirical Paradigms of RL with Privileged Information ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Most empirical paradigms of RL with privileged information can be categorized into two types: i) privileged policy learning, where the policy in training is conditioned on the privileged information, and the trained policy is then distilled to a policy that does not take the privileged information as input. This is usually referred to as either expert distillation [14, 63, 57] or teacher-student learning [44, 58, 74] in the literature; ii) privileged value learning, where the value function is conditioned on the privileged information, and is then used to directly output a policy that takes partial observation (history) as input. One prominent example of ii) is asymmetric-actor-critic [67, 3]. It is worth noting that asymmetric-actor-critic is also closely related to one of the most successful paradigms for multi-agent RL, centralized-training-with-decentralized-execution [52, 85, 21], which is usually instantiated under the actor-critic framework, with the critic taking privileged information as input in training. Here we formalize and revisit the potential pitfalls of these two paradigms, and further develop theoretical guarantees under certain additional conditions and/or algorithm variants. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.1 Privileged Policy Learning: Expert Policy Distillation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The motivation behind expert policy distillation is that learning an optimal fully observable policy in MDPs is a much easier and better-studied problem with many known efficient algorithms. The (expected) distillation objective can be formalized as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\pi}^{\\star}\\in\\arg\\operatorname*{min}_{\\pi\\in\\Pi}\\;\\mathbb{E}_{\\pi^{\\prime}}^{\\mathcal{P}}\\left[\\sum_{h=1}^{H}D_{f}\\left(\\pi_{h}^{\\star}\\big(\\cdot\\,\\big|\\;s_{h}\\big)\\;\\big|\\;\\pi_{h}\\big(\\cdot\\;\\big|\\;\\tau_{h}\\big)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pi^{\\prime}~~\\in~\\Pi^{\\mathrm{gen}}$ is some given behavior policy to collect exploratory trajectories, $\\pi^{\\star}\\ \\in$ arg $\\operatorname*{max}_{\\pi\\in\\Pi_{S}}v^{\\mathcal{P}}(\\pi)$ denotes the optimal fully observable policy, and $D_{f}$ denotes the general $f$ - divergence to measure the discrepancy between $\\pi^{\\star}$ and $\\pi$ . ", "page_idx": 4}, {"type": "text", "text": "Such a formulation looks promising since it essentially circumvents the challenging issue of exploration in partially observable environments, by directly mimicking an expert policy that can be obtained from any off-the-shelf MDP learning algorithm. However, we point out in the following proposition that even if the POMDP satisfies Assumption 2.2, the distilled policy can still be strictly suboptimal even with infinite data, i.e., by solving the expected objective Equation (3.1) completely. We postpone the proof of Proposition 3.1 to Appendix E. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1 (Pitfall of expert policy distillation). For any \u03f5, $\\gamma\\in(0,1)$ , there exists a $\\gamma$ -observable POMDP $\\mathcal{P}^{\\epsilon}$ with $H=1$ , $S=O=A=2$ such that for any behavior policy $\\pi^{\\prime}\\in\\Pi^{\\mathrm{gen}}$ and choice of Df in Equation (3.1), it holds that vP\u03f5(\u03c0\u22c6) \u2264max\u03c0\u2208\u03a0 vP\u03f5(\u03c0) \u2212(1\u2212\u03f5)4(1\u2212\u03b3). ", "page_idx": 4}, {"type": "text", "text": "The key reason Equation (3.1) fails is that in general, the underlying state can remain highly uncertain even given the full history. Thus, the distilled policy may not be able to mimic the state-based expert policy well at different states $s_{h}$ if the associated $\\bar{\\pi}_{h}^{\\star}(\\cdot\\,|\\,\\bar{s}_{h})$ differs significantly across $s_{h}$ . ", "page_idx": 4}, {"type": "text", "text": "To see how we may rule out such problems, we notice that if $\\gamma=1^{2}$ , implying that the observation can decode the underlying state, the bound in Proposition 3.1 becomes vacuous. Inspired by this observation, we propose the following condition that can incorporate this case of $\\gamma=1$ , and will be shown to suffice to make expert distillation effective. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.2 (Deterministic filter condition). We say a POMDP $\\mathcal{P}$ satisfies the deterministic filter condition if for each $h\\ \\geq\\ 2$ , the belief update operator under $\\mathcal{P}$ satisfies that there exists an unknown function $\\psi_{h}:{\\mathcal{S}}\\times{\\mathcal{A}}\\times{\\mathcal{O}}\\to{\\mathcal{S}}$ such that for any reachable $s_{h-1}\\,\\in\\,S$ , $o_{h}~\\in~{\\mathcal{O}}$ , $a_{h-1}\\in A$ , $U_{h}(b^{s_{h-1}};a_{h-1},o_{h})=b^{\\psi_{h}(s_{h-1},a_{h-1},o_{h})},$ , where we define for any $s\\in S$ , $b^{s}\\,\\in\\,\\Delta(S)$ and $b^{s}(s)\\,=\\,1$ is a one-hot vector. In addition, for $h\\,=\\,1$ , there exists a function $\\psi_{1}:{\\mathcal{O}}\\rightarrow{\\mathcal{S}}$ such that for any reachable $o_{1}$ , $B_{1}(\\mu_{1};o_{1})=b^{\\psi_{1}(o_{1})}$ , where $B_{h}(b;o_{h}):=\\mathbb{P}_{s_{h}\\sim b}^{\\mathcal{P}}(\\cdot\\,|\\,o_{h})\\in\\Delta(S)$ , $U_{h}(b;a_{h-1},o_{h}):=\\mathbb{P}_{s_{h-1}\\sim b}^{\\mathcal{P}}(\\cdot\\,|\\,a_{h-1},o_{h-1})\\in\\Delta(S)$ are the belief update operathors under the Bayes rule for any $b\\in\\Delta(S)$ , for which we defer the formal introduction to Appendix C.1. ", "page_idx": 4}, {"type": "text", "text": "Notably, this condition is weaker than and thus covers several known tractable classes of POMDPs with sample and computation efficiency guarantees including Block MDP, deterministic POMDP, $k$ -decodable POMDP as well as a new setting we have identified and existing literature cannot handle. We refer the formal introduction to Appendix E and Figure 1 for an illustration. ", "page_idx": 4}, {"type": "text", "text": "In light of the pitfall in Proposition 3.1, we will analyze both the computational and statistical efficiencies of expert distillation in Section 4, under the condition in Definition 3.2. ", "page_idx": 4}, {"type": "text", "text": "3.2 Privileged Value Learning: Asymmetric Actor-Critic ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Asymmetric actor-critic [67] iterates between two procedures as in standard actor-critic algorithms [41]: policy improvement and policy evaluation. As the name suggests, its key difference from the standard actor-critic is that the algorithm maintains $Q$ -value functions (the critic) based on the state/privileged information, while the policy receives only the (partially observable) history as input. ", "page_idx": 4}, {"type": "text", "text": "Policy evaluation. At iteration $t-1$ , given the policy $\\pi^{t-1}$ , the algorithm estimates $Q$ -functions in the form of $\\{Q_{h}^{t-1}(\\tau_{h},s_{h},a_{h})\\}_{h\\in[H]}$ , where we adopt the \u201cunbiased\u201d version [6] such that $Q$ - functions are conditioned on both the history and the states.3 One key to achieving sample efficiency is by adding some bonus terms in policy evaluation to encourage exploration, i.e., obtaining some optimistic $Q$ -function estimates, similarly as in the fully-observable MDP setting, see e.g., [11, 73], for which we defer the detailed introduction to Section 4. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Policy improvement. At each iteration $t$ , given the critic $\\{Q_{h}^{t-1}(\\tau_{h},s_{h},a_{h})\\}_{h\\in[H]}$ for $\\pi^{t-1}$ , the vanilla asymmetric actor-critic algorithm updates the policy according to the sample-based gradient estimation via $K$ trajectories $\\{s_{1:H+1}^{k^{\\ }},o_{1:H}^{k},a_{1:H}^{k}\\}_{k\\in[K]}$ sampled from $\\pi^{t-1}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi^{t}\\gets\\mathsf{P R O I I}\\left(\\pi^{t-1}+\\frac{\\lambda_{t}}{K}\\sum_{k\\in[K]}\\sum_{h\\in[H]}\\nabla_{\\pi}\\log\\pi_{h}^{t-1}(a_{h}^{k}\\,|\\,\\tau_{h}^{k})Q_{h}^{t-1}(\\tau_{h}^{k},s_{h}^{k},a_{h}^{k})\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda_{t}$ is the step-size and $\\mathrm{PROJ_{II}}$ is the projection operator onto the space of $\\Pi$ , which corresponds to projecting onto the simplex of $\\Delta(A)$ for each $h\\in[H]$ . Here we point out the potential drawback of the vanilla algorithm as in [67, 6], where the key insight is that for each iteration of policy evaluation and improvement, one roughly only performs the computation of order $\\mathcal{O}(K H)$ , while needing to collect $K$ new episodes of samples. Thus, the sample complexity will scale in the same order as the computational complexity when the algorithm converges after some iterations to an $\\epsilon$ -optimal solution, which will be super-polynomial even for $\\gamma.$ -observable POMDPs [27]. Proof of the result is deferred to Appendix E. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.3 (Inefficiency of vanilla asymmetric actor-critic). Under the tabular parameterization for both the policy and the value function, the vanilla asymmetric actor-critic algorithm (Equation (3.2)) suffers from super-polynomial sample complexity for $\\gamma$ -observable POMDPs under standard hardness assumptions. ", "page_idx": 5}, {"type": "text", "text": "To address such an issue, one may need to perform more computation per iteration, so that although the total computational complexity (iteration number $\\times$ per-iteration computational complexity) is super-polynomial, the total iteration number can be lower such that the total sample complexity may be lower as well. This desideratum is hard to achieve if one computes policy update only on the sampled trajectories $\\tau_{h}$ per iteration, i.e., update asynchronously, since this will couple the scales of computational and sample complexities similarly as Equation (3.2). In contrast, we first propose to update all trajectories per iteration in a synchronous way, with the following proximal-policy optimization-type [71] policy improvement update with the state-history-dependent $Q$ -functions $\\{\\bar{Q}_{h}^{t-1}(\\tau_{h},s_{h},\\stackrel{\\cdot\\cdot}{a_{h}})\\}_{h\\in[H]}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{h}^{t}(\\cdot\\,|\\,\\tau_{h})\\propto\\pi_{h}^{t-1}(\\cdot\\,|\\,\\tau_{h})\\exp\\left(\\eta\\mathbb{E}_{s_{h}\\sim b_{h}(\\tau_{h})}\\left[Q_{h}^{t-1}(\\tau_{h},s_{h},\\cdot)\\right]\\right),\\forall h\\in[H],\\tau_{h}\\in\\mathcal{T}_{h},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we recall $\\pmb{b}_{h}(\\tau_{h})\\;\\in\\;\\Delta(\\mathcal{S})$ denotes the belief state and $\\eta\\:>\\:0$ is the learning rate. This update rule also reduces to the natural policy gradient (NPG) [39] update under the softmax policy parameterization in the fully-observable case [1], when updated for each state $s_{h}$ separately [73]. We defer the detailed derivation of Equation (3.3) to Appendix E. ", "page_idx": 5}, {"type": "text", "text": "However, such an update presents two challenges: (1) It requires enumerating all possible $\\tau_{h}$ , whose number scales exponentially with the horizon, making it still computationally intractable; (2) An explicit belief function $b_{h}$ is needed. Motivated by these two challenges, we propose to consider finitememory-based policy and assume access to an approximate belief function $\\{\\bar{b}_{h}^{\\mathrm{apx}^{-}};\\,\\mathcal{Z}_{h}\\to\\Delta(S)\\}_{h\\in[H]}$ (the learning for which will be made clear later). Correspondingly, the policy update is modified as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{h}^{t}(\\cdot\\,|\\,z_{h})\\propto\\pi_{h}^{t-1}(\\cdot\\,|\\,z_{h})\\exp\\left(\\eta\\mathbb{E}_{s_{h}\\sim b_{h}^{\\mathrm{spx}}(z_{h})}\\left[Q_{h}^{t-1}(z_{h},s_{h},\\cdot)\\right]\\right),\\forall h\\in[H],z_{h}\\in\\mathcal{Z}_{h}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then we develop and analyze one possible approach to learning such an approximate belief efficiently (c.f. Section 5). It is worth noting that the policy optimization algorithm we aim to develop and analyze does not depend on the specific algorithm approximate belief learning. Such a decoupling enables a more modular algorithm design framework, and can potentially incorporate the rich literature on learning approximate beliefs in practice, see e.g., [24, 64, 16, 86, 82], which has mostly not been theoretically analyzed before. We will thus analyze such an oracle in Section 5. ", "page_idx": 5}, {"type": "text", "text": "4 Provably Efficient Expert Policy Distillation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now focus on the provable correctness and efficiency of expert policy distillation, under the deterministic filter condition in Definition 3.2. We will defer all the proofs in this section to ", "page_idx": 5}, {"type": "text", "text": "Appendix F. Definition 3.2 motivates us to consider only succinct policies that incorporate an auxiliary parameter representing the most recent state, as well as the most recent observations and actions. We consider policies that are the composition of two functions: at step $h$ a function $g_{h}:S\\times A\\times O\\to S$ that decodes the state based on the previous state, the most recent action, and the most recent observation, and a policy $\\pi^{E}\\in\\Pi_{{\\mathcal{S}}}$ that takes as input the current (decoded) underlying state and outputs a distribution over actions. ", "page_idx": 6}, {"type": "text", "text": "Definition 4.1. We define a policy class $\\Pi^{D}$ as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Pi^{D}=\\left\\{\\pi_{h}^{E}\\left(g_{h}(s_{h-1},a_{h-1},o_{h})\\right):g_{h}:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{O}\\rightarrow\\mathcal{S},\\pi_{h}^{E}:\\mathcal{S}\\rightarrow\\Delta(\\mathcal{A})\\right\\}_{h\\in[H]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\pi^{E}$ stands for an arbitrary expert policy, and $\\Pi^{D}$ stands for the distilled policy class, and for $h=1$ , $a_{0}$ , $s_{0}$ are some fixed dummy action and state. Intuitively, the distilled policy $\\pi\\in\\Pi^{D}$ executes as follows: it firstly decodes the underlying states by applying $\\{g_{h}\\}_{h\\in[H]}$ recursively along the history, and then takes actions using $\\pi^{E}$ based on the decoded states. ", "page_idx": 6}, {"type": "text", "text": "Our goal is to learn the two functions independently, that is, we want to learn an approximately optimal policy $\\pi^{E}\\,\\in\\,\\Pi_{{\\mathcal{S}}}$ with respect to the MDP $\\mathcal{M}$ derived from POMDP $\\mathcal{P}$ by omitting the observations and observing the underlying state (see Definition 4.2 for a formal definition), and for each step $h\\in[H]$ , a decoding function $g_{h}{\\bigl(}s_{h-1},a_{h-1},o_{h}{\\bigr)}$ such that the probability of incorrectly decoding a state-action-observation triplet over the trajectories induced by the policy $\\pi^{E}$ is low. ", "page_idx": 6}, {"type": "text", "text": "Definition 4.2 (POMDP-induced MDP). Given a POMDP $\\mathcal{P}=(H,S,A,\\mathcal{O},\\mathbb{T},\\mathbb{O},\\mu_{1},r)$ , we define its associated Markov Decision Process (MDP) $\\mathcal{M}$ as $\\mathcal{M}=(H,S,\\mathcal{A},\\mathbb{T},\\mu_{1},r)$ without observations. Definition 4.3. Consider a POMDP $\\mathcal{P}$ that satisfies Definition 3.2, and let $\\psi=\\{\\psi_{h}\\}_{h\\in[H]}$ be the promised set of functions that always correctly decode a state-action-observation triplet into an underlying state. Consider policy $\\widetilde{\\pi^{E}}=\\{\\pi_{h}^{E}(\\psi(\\cdot)):\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{O}\\rightarrow\\Delta(\\mathcal{A})\\}_{h\\in[H]}\\in\\Pi^{D}$ . We slightly abuse the notation and simply denote by $v^{\\mathcal{P}}(\\pi^{E})=v^{\\mathcal{P}}(\\widetilde{\\pi^{E}})$ . ", "page_idx": 6}, {"type": "text", "text": "aLnedm cmoan s4i.d4e.r  Lae t $\\mathcal{P}\\ =\\ (H,S,A,\\mathcal{O},\\mathbb{T},\\mathbb{O},\\mu_{1},r)$ $\\pi^{E}\\;\\;\\;\\in\\;\\;\\;\\Pi_{{\\cal S}}$ e rb ea  as ePt OoMf DdPe ctohdaitn gsa tifsufniecsti oDnesf $\\{g_{h}\\}_{h\\in[H]}$ such that, $\\begin{array}{r l r}{\\mathbb{P}^{\\pi^{E},\\mathcal{P}}}&{\\left[\\exists h\\in\\left[H\\right]:g_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)\\neq s_{h}\\right]}&{\\leq}&{\\epsilon.}\\end{array}$ . Consider the policy $\\pi\\quad=$ $\\left\\{\\pi_{h}^{E}\\left(g_{h}(\\cdot)\\right):S\\times\\boldsymbol{\\mathcal{A}}\\times\\boldsymbol{\\mathcal{O}}\\rightarrow\\Delta(\\boldsymbol{\\mathcal{A}})\\right\\}_{h\\in\\left[H\\right]}$ on the POMDP $\\mathcal{P}$ , then: $v^{\\mathcal{P}}(\\pi)\\geq v^{\\mathcal{P}}(\\widetilde{\\pi^{E}})-H\\epsilon$ . ", "page_idx": 6}, {"type": "text", "text": "We can use any off-the-shelf algorithm to learn an approximate optimal policy $\\pi^{E}$ for the associated MDP $\\mathcal{M}$ (see Definition 4.2). Thus, in the rest of the section, we focus on learning the decoding function $\\{g_{h}\\}_{h\\in[H]}$ . To efficiently learn the decoding function, we model the access to the underlying state by keeping track of the most recent pair of the action and the observation, as well as the two most recent states. We summarize the algorithm of decoding-function learning in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.5. Consider a POMDP $\\mathcal{P}$ that satisfies Definition 3.2, a policy $\\pi^{E}\\,\\in\\,\\Pi_{{\\mathcal{S}}}$ , and let $\\{g_{h}\\}_{h\\in[H]}$ be the output of Algorithm 1 with AOS+l\u03f5o2g(H/\u03b4). Then, with probability at least $1-\\delta$ , for each step $h\\in[H]$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}^{\\pi^{E},\\mathcal{P}}\\ [\\exists h\\in[H]:g_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)\\neq s_{h}]\\leq\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "using P $\\begin{array}{r l}{\\mathrm{)LY}(H,A,O,S,\\frac{1}{\\epsilon},\\log\\left(\\frac{1}{\\delta}\\right))}\\end{array}$ episodes in time POLY(H, A, O, S, \u03f51 , log \u03b41 ). ", "page_idx": 6}, {"type": "text", "text": "The following is an immediate consequence of Lemma 4.4 and Theorem 4.5. Note that both the sample and computation complexities are polynomial, which is in stark contrast to the $k$ -decodable POMDP case [19] (a special one covered by our Definition 3.2), for which the sample complexity is necessarily exponential in $k$ when there is no privileged information [19]. In fact, thanks to privileged information, the complexities are only polynomial in horizon $H$ even when the decodable length is unknown. For the benefits of using privileged information in several other subclasses of problems, we refer to Table 1 for more details. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.6. Let $\\mathcal{P}$ satisfy Definition 3.2 and consider any policy $\\pi^{E}\\;\\;\\in\\;\\;\\Pi_{{\\cal S}}$ . Using $\\begin{array}{r l}{\\mathrm{POLY}\\!\\left(H,A,O,S,\\frac{1}{\\epsilon},\\log\\left(\\frac{1}{\\delta}\\right)\\right)}\\end{array}$ episodes and in time P $\\scriptstyle\\operatorname{oLY}(H,A,\\tilde{O_{,}}\\,\\hat{S_{,}^{\\prime}},\\log\\left(\\frac{1}{\\delta}\\right))$ , we can compute a policy $\\pi\\in\\Pi^{D}$ (see Definition 4.1) such that with probability at least $1-\\delta$ , $\\dot{v}^{\\mathcal{P}}(\\pi)\\geq v^{\\mathcal{P}}(\\pi^{E})-\\epsilon$ . ", "page_idx": 6}, {"type": "text", "text": "Extension to the case with general function approximation. Due to the modularity of our algorithmic framework and its compatibility with supervised learning oracles, it can be readily generalized to the function approximation setting to handle large observation spaces. We defer the corresponding results to Appendix G. ", "page_idx": 6}, {"type": "text", "text": "5 Provable Asymmetric Actor-Critic with Approximate Belief Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Unlike most existing theoretical studies on provably sample-efficient partially observable RL [35, 25, 46], which directly learn an approximate POMDP model for planning near-optimal policies, we consider a general framework with two steps: firstly learning an approximate belief function, followed by adopting a fully observable RL subroutine on the belief state space. ", "page_idx": 7}, {"type": "text", "text": "5.1 Belief-Weighted Optimistic Asymmetric Actor-Critic ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now introduce our main algorithmic contribution to the privileged policy learning setting. Our algorithm is conceptually similar to the natural policy gradient methods [39, 1, 73] in the fullyobservable setting, with additional weighting over the states $s_{h}$ using some learned belief states, to handle the additional state-dependence in the asymmetric critic. The overall algorithm is presented in Algorithm 2. The algorithm requires a belief-learning subroutine that takes the stored memory as input and outputs a belief about the underlying state (c.f. $\\{b_{h}^{\\mathrm{apx}}\\}_{h\\in[H]})$ . Additionally, similar to the fully observable setting, we include a subroutine to estimate the $\\dot{Q}$ -function, which introduces additional challenges due to partial observability (see Appendix H). We establish the performance guarantee of Algorithm 2 in the following theorem. We defer the proof to Appendix H. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1 (Near-optimal policy). Fi $\\mathfrak{c},\\delta\\in(0,1)$ . Given a POMDP $\\mathcal{P}$ and an approximate belief $\\{b_{h}^{\\operatorname{apx}}:{\\mathcal{Z}}_{h}\\to\\Delta(S)\\}_{h\\in[H]}^{\\cdot}$ , with probability at least $1-\\delta$ , Algorithm 2 can learn an approximate optimal policy $\\pi^{\\star}$ of $\\mathcal{P}$ in the space of $\\Pi^{L}$ such that $\\begin{array}{r}{v^{\\mathcal{P}}(\\pi^{\\star})\\geq\\operatorname*{max}_{\\pi\\in\\Pi^{L}}v^{\\mathcal{P}}(\\pi)-\\mathcal{O}(\\epsilon+H^{2}\\epsilon_{\\mathrm{belief}})}\\end{array}$ , with sample complexity $\\begin{array}{r}{\\operatorname{POLY}\\!\\left(S,A,O,H,\\frac{1}{\\epsilon},\\log\\frac{1}{\\delta}\\right)}\\end{array}$ and time complexity POL $\\mathrm{Y}(S,A,O,H,Z,{\\textstyle\\frac{1}{\\epsilon}},\\log{\\textstyle\\frac{1}{\\delta}})$ , where $\\mathrm{\\epsilon_{belief}}$ is the belief-learning error defined as $\\begin{array}{r}{\\epsilon_{\\mathrm{belief}}\\;:=\\;\\operatorname*{max}_{h\\in[H]}\\operatorname*{max}_{\\pi\\in\\Pi^{L}}\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\|b_{h}(\\tau_{h})\\,-}\\end{array}$ $b_{h}^{\\mathrm{apx}}(z_{h})\\|_{1}$ and $Z:=\\operatorname*{max}_{h\\in[H]}|{\\mathcal{Z}}_{h}|$ . Furthermore, if $\\mathcal{P}$ is additionally $\\gamma.$ -observable (c.f. Assumption 2.2), then $\\pi^{\\star}$ is also an approximate optimal policy in the space of $\\Pi$ such that $\\begin{array}{r}{v^{\\mathcal{P}}(\\pi^{\\star})\\geq\\operatorname*{max}_{\\pi\\in\\Pi}v^{\\mathcal{P}}(\\pi)-\\mathcal{O}(\\epsilon+H^{2}\\epsilon_{\\mathrm{belief}})}\\end{array}$ , as long as $L\\stackrel{!}{\\geq}\\widetilde\\Omega(\\gamma^{-4}\\log(S\\bar{H}/\\epsilon))$ . ", "page_idx": 7}, {"type": "text", "text": "5.2 Approximate Belief Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "At a high level, our belief-learning algorithm first learns an approximate POMDP model $\\widehat{\\mathcal{P}}$ by explicitly exploring the state space. The main technical challenge here is that there may exist states that are reachable with very low probability, making it infeasible to collect enough samples to sufficiently explore them, thus potentially breaking the $\\gamma$ -observability property of the ground-truth model $\\mathcal{P}$ . To circumvent this issue, we ignore such hard-to-visit states and redirect probabilities flowing to them to certain other states. Thus, in our truncated POMDP, where each state is sufficiently explored, we can approximate the transition and emission matrices to a desired accuracy uniformly across all the states and preserve the $\\gamma.$ -observability property. This ensures that the learned approximate belief function in the truncated POMDP is sufficiently close to the actual belief function of the original POMDP $\\mathcal{P}$ . Note that the key to achieving belief learning with both polynomial sample and time complexities is our explicit exploration in the state space, which relies on executing fully observable policies from an MDP learning subroutine. We remark that the belief function may also be learned even if the state space is only explored by partially observable policies, thus utilizing only hindsight observability may be sufficient for this purpose [43]. However, for such exploration to be computationally tractable, one requires to avoid using computationally intractable oracles for POMDP learning, which is in fact our final goal. We present the guarantees in the next theorem and postpone the proof to Appendix H. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2. Consider a $\\gamma$ -observable POMDP $\\mathcal{P}$ (c.f. Assumption 2.2) and assume that $L\\geq\\widetilde\\Omega(\\gamma^{-4}\\log(S H/\\epsilon))$ for an $\\epsilon>0$ . Then, we can learn an approximate belief $\\{b_{h}^{\\mathrm{apx}}\\}_{h\\in[H]}$ from Algorithm 4 using $\\begin{array}{r}{\\widetilde{\\mathcal{O}}(\\frac{S^{2}A H^{2}O+S^{3}A H^{2}}{\\epsilon^{2}}+\\frac{S^{4}A^{2}H^{6}O}{\\epsilon\\gamma^{2}})}\\end{array}$ episodes in time POLY $\\begin{array}{r l}{\\left(S,H,A,O,\\frac{1}{\\gamma},\\frac{1}{\\epsilon},\\log\\left(\\frac{1}{\\delta}\\right)\\right)}\\end{array}$ such that with probability at least $1-\\delta$ , for any $\\pi\\in\\Pi^{L}$ and $h\\in[H],\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\|b_{h}(\\tau_{h})-b_{h}^{\\mathrm{apx}}(z_{h})\\|_{1}\\leq\\epsilon$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2 shows that an approximate belief can be learned with both polynomial samples and time, which, combined with Theorem 5.1, yields the final polynomial sample and quasi-polynomial time guarantee below. In contrast to the case without privileged information [25, 27], the sample complexity is reduced from quasi-polynomial to polynomial for $\\gamma$ -observable POMDPs. Note that the computational complexity remains quasi-polynomial, which is known to be unimprovable even for planning [27]. The key to such an improvement, as pointed out in Section 3.2, is the more practical update rule of actor-critic (in conjunction with our belief-weighted idea), which allows more computation at each iteration (instead of only performing computation at the sampled finite-memory). This allows the total computation to remain quasi-polynomial, while the overall sample complexity becomes polynomial. A detailed comparison can be found in Table 1. ", "page_idx": 7}, {"type": "image", "img_path": "o3i1JEfzKw/tmp/b3ad2ec02d73e55f55dc1165140cd0ab8b812dbc6c794edf6a4a9935824081b0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: Results for POMDPs of different sizes, where our methods achieve the best performance with the lowest sample complexity (VI: value iteration; AAC: asymmetric actor-critic). ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.3. Let $\\mathcal{P}$ be a $\\gamma$ -observable POMDP (c.f. Assumption 2.2), and consider $\\textit{L}\\geq$ $\\widetilde\\Omega(\\gamma^{-4}\\log(S H/\\epsilon))$ for an $\\epsilon\\,>\\,0$ . With probability at least $1-\\delta$ , Algorithm 2 can learn a policy $\\pi\\in\\Pi^{L}$ such that $v^{\\mathcal{P}}(\\pi)\\geq\\operatorname*{max}_{\\pi^{\\prime}\\in\\bar{\\Pi}}\\dot{v^{\\mathcal{P}}}(\\pi^{\\prime})-\\dot{\\epsilon}$ , using $\\mathrm{POLY}(S,\\bar{H^{\\prime}},1/\\epsilon,1/\\gamma,\\log(1/\\delta),\\bar{O_{\\star}^{'}}A)$ episodes and in time POLY $(S,H,1/\\epsilon,\\log(1/\\delta),O^{L},A^{L})$ . ", "page_idx": 8}, {"type": "text", "text": "6 Numerical Validation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now provide some numerical results for both of our principled algorithms. Here we mainly compare with two baselines, the vanilla asymmetric actor-critic [67], and asymmetric $Q$ -learning [7], on two settings, POMDP under the deterministic filter condition (c.f. Definition 3.2) and general POMDPs. We report the results in Table 2 and Figure 2, where our algorithms converge faster to higher rewards. We defer the implementation details and discussions to Appendix I. ", "page_idx": 8}, {"type": "text", "text": "7 Extension to Partially Observable MARL with Privileged Information ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "7.1 Privileged Policy Learning: Equilibrium Distillation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To understand how the deterministic filter condition may be extended for POSGs, we first note the following equivalent characterization of Definition 3.2, the proof of which is deferred to Appendix J. ", "page_idx": 8}, {"type": "text", "text": "Proposition 7.1. Definition 3.2 is equivalent to the following: for each $h\\in[H]$ , there exists an unknown function $\\phi_{h}:T_{h}\\to S$ such that $\\mathbb{P}^{\\mathcal{P}}(s_{h}=\\phi_{h}(\\tau_{h})\\,|\\,\\tau_{h}\\overset{\\ \u3001\\bullet}{)}=1$ for any reachable $\\tau_{h}\\in\\tau_{h}$ . ", "page_idx": 8}, {"type": "text", "text": "Proposition 7.1 implies that at each step $h$ , given the entire history information, the agent can uniquely decode the current underlying state $s_{h}$ . Thus, we generalize this condition to POSGs by requiring each agent to uniquely decode the current state $s_{h}$ given the information it has collected so far. ", "page_idx": 8}, {"type": "text", "text": "Definition 7.2 (Deterministic fliter condition for POSGs). We say a POSG $\\mathcal{G}$ satisfies the deterministic fliter condition if for each $i\\in[n]$ , $h\\in[H]$ , there exists an unknown function $\\phi_{i,h}:{\\mathcal{C}}_{h}\\times{\\mathcal{P}}_{i,h}\\to S$ such that $\\mathbb{P}^{\\mathcal{G}}(s_{h}=\\phi_{i,h}(c_{h},p_{i,h})\\,|\\,c_{h},p_{i,h})=1$ for any reachable $(c_{h},p_{i,h})$ . ", "page_idx": 8}, {"type": "text", "text": "Here we have required that each agent can decode the underlying state through their own information individually. Naturally, one may wonder whether one can relax it so that only the joint history information of all the agents can decode the underlying state. However, we point out in the following that it does not circumvent the computational hardness of POSG, the proof of which is deferred to Appendix J. Note that the computational hardness result can not be mitigated even with privileged state information, as the hardness we state here holds even for the planning problem with model knowledge, with which one can simulate the RL problems with privileged information. ", "page_idx": 8}, {"type": "text", "text": "Proposition 7.3. Computing CCE in POSGs that satisfy that for each step $h\\in[H]$ , there exists a function $\\phi_{h}:{\\mathcal{C}}_{h}\\times{\\mathcal{P}}_{h}\\to S$ such that $\\mathbb{P}^{\\mathcal{G}}(s_{h}=\\phi_{h}(c_{h},\\bar{p}_{h})\\,|\\,c_{h},p_{h})=1$ for any reachable $(c_{h},p_{h})$ is still PSPACE-hard. ", "page_idx": 8}, {"type": "text", "text": "Learning multi-agent individual decoding functions with unilateral exploration. Similar to our framework for POMDPs, our framework for POSGs is also decoupled into two steps: i) learning an expert equilibrium policy that is fully observable, ii) learning the decoding function, where the first step can be instantiated by any provable off-the-shelf algorithm of learning in Markov games. The major difference from the framework for POMDPs lies in how to learn the decoding function. In Theorem J.1, we prove that the difference of the NE/CE/CCE-gap between the expert policy and the distilled student policy is bounded by the decoding errors under policies from the unilateral deviation of the expert policy. Hence, given the expert policy $\\pi$ , the key algorithmic principle is to perform unilateral exploration for each agent $i$ to make sure the decoding function is accurate under policies $(\\pi_{i}^{\\prime},\\pi_{-i})$ for any $\\pi_{i}^{\\prime}$ , keeping $\\pi_{-i}$ fixed. We refer the detailed algorithm to Algorithm 5, and present below the guarantees for learning the decoding functions and the corresponding distilled policy for learning NE/CCE, while we defer the results for learning CE to Theorem J.6. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Theorem 7.4 (Equilibria learning; Combining Theorem J.1 and Theorem J.4). Under Assumption C.8 and conditions of Definition 7.2, given a $\\frac{\\epsilon}{2}$ -NE/CCE $\\pi^{E}$ for the associated Markov game of $\\mathcal{G}$ , Algorithm 5 can learn decoding function $\\{\\widehat{g}_{i,h}\\}_{i\\in[n],h\\in[H]}$ such that with probability at least $1-\\delta$ , it is guaranteed that $\\begin{array}{r}{\\operatorname*{max}_{u_{i}\\in\\Pi_{i},j\\in[n]}\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathcal{G}}\\big(s_{h}\\ne\\widehat{g}_{j,h}\\big(c_{h},p_{j,h}\\big)\\big)\\le\\frac{\\epsilon}{4n H^{2}}}\\end{array}$ , for any $i\\in[n],h\\in[H]$ with both sample and computational complexities $\\begin{array}{r}{\\mathrm{POLY}\\big(S,A,H,O,\\frac{1}{\\epsilon},\\log\\frac{1}{\\delta}\\big)}\\end{array}$ . Consequently, policy $\\pi$ distilled from $\\pi^{E}$ (c.f. Theorem J.1 for the formal distillation procedures) is an $\\epsilon$ -NE/CCE of $\\mathcal{G}$ . ", "page_idx": 9}, {"type": "text", "text": "7.2 Privileged Value Learning: Asymmetric MARL with Approximate Belief Learning ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "For POMDPs, we have used finite-memory policies for computational efficiency. We generalize to POSGs with information sharing by defining the compression of the common information. ", "page_idx": 9}, {"type": "text", "text": "Definition 7.5 (Compressed approximate common information [56, 78, 50]). For each $h\\,\\in\\,[H]$ , given a set $\\widehat{\\mathcal{C}}_{h}$ , we say ${\\mathrm{Compress}}_{h}$ is a compression function if ${\\mathrm{Compress}}_{h}\\;\\in\\;\\{f\\;:\\;{\\mathcal{C}}_{h}\\;\\to\\;{\\widehat{\\mathcal{C}}}_{h}\\}$ . For each $c_{h}\\,\\in\\,\\mathcal{C}_{h}$ , we denote $\\widehat{c}_{h}:=\\mathrm{Compress}_{h}(c_{h})$ . We also require the compression function to satisfy the regularity condition that for each $h\\,\\in\\,[H]$ , there exists a function $\\widehat{\\Lambda}_{h+1}$ such that $\\widehat{c}_{h+1}=\\widehat{\\Lambda}_{h+1}(\\widehat{c}_{h},\\varpi_{h+1})$ , for any $c_{h}\\in\\mathcal{C}_{h}$ , $\\varpi_{h+1}\\in\\Upsilon_{h+1}$ , where we recall $c_{h+1}:=c_{h}\\cup\\varpi_{h+1}$ and t he defi nition o f $\\Upsilon_{h+1}$ in Assumption C.7. ", "page_idx": 9}, {"type": "text", "text": "Similar to the framework we developed for POMDPs in Section 5, we firstly develop the multi-agent RL algorithm based on some approximate belief, and then instantiate it with one provable approach for learning such an approximate belief. ", "page_idx": 9}, {"type": "text", "text": "Optimistic value iteration of POSGs with approximate belief. For POMDPs, the sufficient statistics for optimal decision-making is the posterior distribution over the state given history. However, for POSGs with information-sharing, as shown in [62, 61, 50], the sufficient statistics become the posterior distribution over the state and the private information given the common information, instead of only the state. Therefore, we consider the approximate belief in the form of $\\widehat{P}_{h}:\\widehat{\\mathcal{C}}_{h}\\to\\Delta(\\mathcal{P}_{h}\\times\\mathcal{S})$ for each $h\\in[H]$ , where we define the error compared with the ground-truth belief to be $\\begin{array}{r}{\\epsilon_{\\mathrm{belief}}:=\\operatorname*{max}_{h\\in[H]}\\operatorname*{max}_{\\pi\\in\\Pi}\\mathbb{E}_{\\pi}^{\\mathcal{G}}\\sum_{s_{h},p_{h}}\\left|\\mathbb{P}^{\\mathcal{G}}(s_{h},p_{h}\\mid c_{h})-\\widehat{P}_{h}(s_{h},p_{h}\\mid\\widehat{c}_{h})\\right|}\\end{array}$ sh,ph |PG(sh, ph | ch) \u2212P h(sh, ph |ch)|, i.e., the expected total variation distance from the true one. Note that both $\\widehat{P}_{h}$ and thus $\\mathbf{\\epsilon}_{\\mathrm{belief}}$ have implicit dependencies on ${\\mathrm{Compress}}_{h}$ , as $\\widehat{c}_{h}:=\\mathrm{Compress}_{h}(c_{h})$ . We outlin e our algorithm in Algorithm 7, which is conceptually similar to  t he algorithm for POMDPs (Algorithm 2), maintaining the asymmetric critic (i.e., value function), and performing the actor update (i.e., policy update) using the belief-weighted value function. ", "page_idx": 9}, {"type": "text", "text": "Theorem 7.6 (Equilibria learning; Combining Theorem J.15 and Theorem J.16). Fix $\\begin{array}{r l r}{\\epsilon,\\delta}&{{}\\in}&{(0,1)}\\end{array}$ . Under Assumption C.8, with probability at least $1\\ -\\ \\delta$ , Algorithm 7 can learn an $(\\epsilon\\;+\\;H^{2}\\epsilon_{\\mathrm{belief}})\\mathrm{{-NE}}$ if $\\mathcal{G}$ is zero-sum and $\\dot{(\\epsilon\\mathrm{\\boldmath~+~}H^{2}\\epsilon_{\\mathrm{belief}})}$ -CE/CCE if $\\mathcal{G}$ is $\\mathcal{O}\\big(\\frac{H^{4}S A O\\log(S A H O/\\delta)}{\\epsilon^{2}}\\big)$ and computational complexity $\\begin{array}{r}{\\operatorname{POLY}(S,(A O)^{\\mathcal{O}(\\gamma^{-4}\\log(S H/\\epsilon))},H,\\frac{1}{\\epsilon},\\log\\frac{1}{\\delta})}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "Learning approximate belief with model truncation. The belief learning algorithm we design for POSGs is conceptually similar to that we designed for POMDPs, where the key to achieving both polynomial sample and computational complexity is still to firstly learn approximate models, i.e., transitions and emissions, and then carefully truncate (as in Section 5.2) its transition and emission to build the approximate belief, where we defer the detailed algorithm to Algorithm 8. Next, we provide its provable guarantees, which leads to a final polynomial-sample and quasi-polynomial-time complexity result when combined with Theorem 7.6. ", "page_idx": 9}, {"type": "text", "text": "Theorem 7.7. For any $\\epsilon\\mathrm{~>~0~}$ , under Assumption 2.2, it holds that one can learn the approximate belief $\\{\\widehat{P}_{h}:\\widehat{\\mathcal{C}}_{h}\\to\\Delta(S\\times\\mathcal{P}_{h})\\}_{h\\in[H]}$ such that $\\begin{array}{r}{\\epsilon_{\\mathrm{belief}}\\:\\leq\\:\\frac{\\epsilon}{H^{2}}}\\end{array}$ with both polynomial sample complexity a nd co mputational complexity $\\begin{array}{r}{\\operatorname{\\check{PoLY}}(S,A,O,H,\\frac{1}{\\gamma},\\frac{1}{\\epsilon},\\log\\frac{1}{\\delta})}\\end{array}$ for all the examples in Appendix C.3. As a consequence, Algorithm 7 can learn an $\\epsilon{-}\\mathrm{NE}$ if $\\mathcal{G}$ is zero-sum and $\\epsilon$ -CE/CCE if $\\mathcal{G}$ $\\mathcal{O}\\big(\\frac{H^{4}S A O\\log(S A H O/\\delta)}{\\epsilon^{2}}\\big)$ and computational complexity $\\begin{array}{r}{\\operatorname{POLY}(S,(A O)^{\\mathcal{O}(\\gamma^{-4}\\log(S H/\\epsilon))},H,\\frac{1}{\\epsilon},\\log\\frac{1}{\\delta})}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank the anonymous reviewers and area chair from NeurIPS 2024 for the valuable feedback. Y.C. acknowledges the support from the NSF Awards CCF-1942583 (CAREER) and CCF-2342642. X.L. and K.Z. acknowledge the support from Army Research Laboratory (ARL) Grant W911NF-24-1-0085. A.O. acknowledges financial support from a Meta PhD fellowship, a Sloan Foundation Research Fellowship and the NSF Award CCF-1942583 (CAREER). Part of this work was done while the authors were visiting the Simons Institute for the Theory of Computing. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. Optimality and approximation with policy gradient methods in Markov decision processes. In Conference on Learning Theory, pages 64\u201366, 2020.   \n[2] E. Altman, V. Kambley, and A. Silva. Stochastic games with one step delay sharing information pattern with application to power control. In 2009 International Conference on Game Theory for Networks, pages 124\u2013129. IEEE, 2009. [3] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3\u201320, 2020.   \n[4] R. J. Aumann, M. Maschler, and R. E. Stearns. Repeated games with incomplete information. MIT press, 1995. [5] R. Avalos, F. Delgrange, A. Nowe, G. Perez, and D. M. Roijers. The wasserstein believer: Learning belief updates for partially observable environments through reliable latent space models. In The Twelfth International Conference on Learning Representations, 2023. [6] A. Baisero and C. Amato. Unbiased asymmetric reinforcement learning under partial observability. In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, 2022. [7] A. Baisero, B. Daley, and C. Amato. Asymmetric DQN for partially observable reinforcement learning. In Uncertainty in Artificial Intelligence, pages 107\u2013117. PMLR, 2022. [8] N. Brukhim, D. Carmon, I. Dinur, S. Moran, and A. Yehudayoff. A characterization of multiclass learnability, 2022.   \n[9] N. Brukhim, D. Carmon, I. Dinur, S. Moran, and A. Yehudayoff. A characterization of multiclass learnability. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 943\u2013955. IEEE, 2022.   \n[10] S. Bubeck. Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn., 8(3-4):231\u2013357, 2015.   \n[11] Q. Cai, Z. Yang, C. Jin, and Z. Wang. Provably efficient exploration in policy optimization. In International Conference on Machine Learning, pages 1283\u20131294. PMLR, 2020.   \n[12] Q. Cai, Z. Yang, and Z. Wang. Reinforcement learning from partial observation: Linear function approximation with provable sample efficiency. In International Conference on Machine Learning, pages 2485\u20132522. PMLR, 2022.   \n[13] C. L. Canonne. A short note on learning discrete distributions. arXiv preprint arXiv:2002.11457, 2020.   \n[14] D. Chen, B. Zhou, V. Koltun, and P. Kr\u00e4henb\u00fchl. Learning by cheating. In Conference on Robot Learning, pages 66\u201375. PMLR, 2020.   \n[15] F. Chen, Y. Bai, and S. Mei. Partially observable RL with b-stability: Unified structural condition and sharp sample-efficient algorithms. In The Eleventh International Conference on Learning Representations, 2023.   \n[16] X. Chen, Y. M. Mu, P. Luo, S. Li, and J. Chen. Flow-based recurrent belief state learning for pomdps. In International Conference on Machine Learning, pages 3444\u20133468. PMLR, 2022.   \n[17] C. Dann, N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. On oracleefficient pac rl with rich observations. Advances in neural information processing systems, 31, 2018.   \n[18] S. Du, A. Krishnamurthy, N. Jiang, A. Agarwal, M. Dudik, and J. Langford. Provably efficient rl with rich observations via latent state decoding. In International Conference on Machine Learning, pages 1665\u20131674. PMLR, 2019.   \n[19] Y. Efroni, C. Jin, A. Krishnamurthy, and S. Miryoosef.i Provable reinforcement learning with a short-term memory. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv\u00e1ri, G. Niu, and S. Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 5832\u20135850. PMLR, 2022.   \n[20] E. Even-Dar, S. M. Kakade, and Y. Mansour. The value of observation for monitoring dynamic systems. In M. M. Veloso, editor, IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, pages 2474\u20132479, 2007.   \n[21] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson. Learning to communicate with deep multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 29, 2016.   \n[22] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.   \n[23] K. Fujii. Bayes correlated equilibria and no-regret dynamics. arXiv preprint arXiv:2304.05005, 2023.   \n[24] T. Gangwani, J. Lehman, Q. Liu, and J. Peng. Learning belief representations for imitation learning in pomdps. In uncertainty in artificial intelligence, pages 1061\u20131071. PMLR, 2020.   \n[25] N. Golowich, A. Moitra, and D. Rohatgi. Learning in observable POMDPs, without computationally intractable oracles. In Advances in Neural Information Processing Systems, 2022.   \n[26] N. Golowich, A. Moitra, and D. Rohatgi. Planning in observable pomdps in quasipolynomial time. arXiv preprint arXiv:2201.04735, 2022.   \n[27] N. Golowich, A. Moitra, and D. Rohatgi. Planning and learning in partially observable systems via fliter stability. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 349\u2013362, 2023.   \n[28] N. Golowich, A. Moitra, and D. Rohatgi. Exploration is harder than prediction: Cryptographically separating reinforcement learning from supervised learning. arXiv preprint arXiv:2404.03774, 2024.   \n[29] G. J. Gordon, A. Greenwald, and C. Marks. No-regret learning in convex games. In Proceedings of the 25th international conference on Machine learning, pages 360\u2013367, 2008.   \n[30] J. Guo, M. Chen, H. Wang, C. Xiong, M. Wang, and Y. Bai. Sample-efficient learning of pomdps with multiple observations in hindsight. In The Twelfth International Conference on Learning Representations, 2023.   \n[31] A. Gupta, A. Nayyar, C. Langbort, and T. Basar. Common information based markov perfect equilibria for linear-gaussian games with asymmetric information. SIAM Journal on Control and Optimization, 52(5):3228\u20133260, 2014.   \n[32] J. Hartline, V. Syrgkanis, and E. Tardos. No-regret learning in bayesian games. Advances in Neural Information Processing Systems, 28, 2015.   \n[33] H. Hu, A. Lerer, N. Brown, and J. Foerster. Learned belief search: Efficiently improving policies in partially observable settings. arXiv preprint arXiv:2106.09086, 2021.   \n[34] N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual decision processes with low bellman rank are pac-learnable. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1704\u20131713. PMLR, 2017.   \n[35] C. Jin, S. Kakade, A. Krishnamurthy, and Q. Liu. Sample-efficient reinforcement learning of undercomplete POMDPs. Advances in Neural Information Processing Systems, 33:18530\u2013 18539, 2020.   \n[36] C. Jin, A. Krishnamurthy, M. Simchowitz, and T. Yu. Reward-free exploration for reinforcement learning. In International Conference on Machine Learning, pages 4870\u20134879. PMLR, 2020.   \n[37] C. Jin, Q. Liu, Y. Wang, and T. Yu. V-learning\u2013a simple, efficient, decentralized algorithm for multiagent rl. arXiv preprint arXiv:2110.14555, 2021.   \n[38] S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In International Conference on Machine Learning, volume 2, pages 267\u2013274, 2002.   \n[39] S. M. Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems, pages 1531\u20131538, 2002.   \n[40] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yogamani, and P. P\u00e9rez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on Intelligent Transportation Systems, 23(6):4909\u20134926, 2021.   \n[41] V. R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information Processing Systems, pages 1008\u20131014, 2000.   \n[42] A. Krishnamurthy, A. Agarwal, and J. Langford. Pac reinforcement learning with rich observations. Advances in Neural Information Processing Systems, 29, 2016.   \n[43] J. Lee, A. Agarwal, C. Dann, and T. Zhang. Learning in pomdps is sample-efficient with hindsight observability. In International Conference on Machine Learning, pages 18733\u201318773. PMLR, 2023.   \n[44] J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter. Learning quadrupedal locomotion over challenging terrain. Science robotics, 5(47):eabc5986, 2020.   \n[45] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334\u20131373, 2016.   \n[46] Q. Liu, A. Chung, C. Szepesvari, and C. Jin. When is partially observable reinforcement learning not scary? In Conference on Learning Theory, pages 5175\u20135220, 2022.   \n[47] Q. Liu, P. Netrapalli, C. Szepesvari, and C. Jin. Optimistic MLE: A generic model-based algorithm for partially observable sequential decision making. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 363\u2013376, 2023.   \n[48] Q. Liu, C. Szepesv\u00e1ri, and C. Jin. Sample-efficient reinforcement learning of partially observable Markov games. In Advances in Neural Information Processing Systems, 2022.   \n[49] Q. Liu, T. Yu, Y. Bai, and C. Jin. A sharp analysis of model-based reinforcement learning with self-play. In International Conference on Machine Learning, pages 7001\u20137010. PMLR, 2021.   \n[50] X. Liu and K. Zhang. Partially observable multi-agent RL with (quasi-)efficiency: the blessing of information sharing. In International Conference on Machine Learning, pages 22370\u201322419. PMLR, 2023.   \n[51] X. Liu and K. Zhang. Partially observable multi-agent reinforcement learning with information sharing, 2024.   \n[52] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in Neural Information Processing Systems, 30, 2017.   \n[53] M. Lu, Y. Min, Z. Wang, and Z. Yang. Pessimism in the face of confounders: Provably efficient offline reinforcement learning in partially observable markov decision processes. In The Eleventh International Conference on Learning Representations, 2023.   \n[54] X. Lyu, A. Baisero, Y. Xiao, and C. Amato. A deeper understanding of state-based critics in multi-agent reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 9396\u20139404, 2022.   \n[55] X. Lyu, A. Baisero, Y. Xiao, B. Daley, and C. Amato. On centralized critics in multi-agent reinforcement learning. Journal of Artificial Intelligence Research, 77:295\u2013354, 2023.   \n[56] W. Mao, K. Zhang, E. Miehling, and T. Ba\u00b8sar. Information state embedding in partially observable cooperative multi-agent reinforcement learning. In 2020 59th IEEE Conference on Decision and Control (CDC), pages 6124\u20136131. IEEE, 2020.   \n[57] G. B. Margolis, T. Chen, K. Paigwar, X. Fu, D. Kim, S. bae Kim, and P. Agrawal. Learning to jump from pixels. In 5th Annual Conference on Robot Learning, 2021.   \n[58] T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter. Learning robust perceptive locomotion for quadrupedal robots in the wild. Science Robotics, 7(62):eabk2822, 2022.   \n[59] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learning. In International conference on machine learning, pages 6961\u20136971. PMLR, 2020.   \n[60] P. Moreno, J. Humplik, G. Papamakarios, B. A. Pires, L. Buesing, N. Heess, and T. Weber. Neural belief states for partially observed domains. In NeurIPS 2018 Workshop on Reinforcement Learning under Partial Observability, 2018.   \n[61] A. Nayyar, A. Gupta, C. Langbort, and T. Ba\u00b8sar. Common information based markov perfect equilibria for stochastic games with asymmetric information: Finite games. IEEE Transactions on Automatic Control, 59(3):555\u2013570, 2013.   \n[62] A. Nayyar, A. Mahajan, and D. Teneketzis. Decentralized stochastic control with partial history sharing: A common information approach. IEEE Transactions on Automatic Control, 58(7):1644\u20131658, 2013.   \n[63] H. Nguyen, A. Baisero, D. Wang, C. Amato, and R. Platt. Leveraging fully observable policies for learning under partial observability. In Conference on Robot Learning, 2022.   \n[64] H. Nguyen, B. Daley, X. Song, C. Amato, and R. Platt. Belief-grounded networks for accelerated robot learning under partial observability. In Conference on Robot Learning, pages 1640\u20131653. PMLR, 2021.   \n[65] C. H. Papadimitriou and J. N. Tsitsiklis. The complexity of markov decision processes. Mathematics of operations research, 12(3):441\u2013450, 1987.   \n[66] A. Pathak, H. Pucha, Y. Zhang, Y. C. Hu, and Z. M. Mao. A measurement study of internet delay asymmetry. In Passive and Active Network Measurement: 9th International Conference, PAM 2008, Cleveland, OH, USA, April 29-30, 2008. Proceedings 9, pages 182\u2013191. Springer, 2008.   \n[67] L. Pinto, M. Andrychowicz, P. Welinder, W. Zaremba, and P. Abbeel. Asymmetric actor critic for image-based robot learning. Robotics: Science and Systems XIV, 2018.   \n[68] S. Qiu, Z. Dai, H. Zhong, Z. Wang, Z. Yang, and T. Zhang. Posterior sampling for competitive rl: Function approximation and partial observation. Advances in Neural Information Processing Systems, 36, 2024.   \n[69] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and S. Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine learning, pages 681\u2013689, 2018.   \n[70] T. Roughgarden. Algorithmic game theory. Communications of the ACM, 53(7):78\u201386, 2010.   \n[71] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[72] S. Shalev-Shwartz, S. Shammah, and A. Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.   \n[73] L. Shani, Y. Efroni, A. Rosenberg, and S. Mannor. Optimistic policy optimization with bandit feedback. In International Conference on Machine Learning, pages 8604\u20138613. PMLR, 2020.   \n[74] I. Shenfeld, Z.-W. Hong, A. Tamar, and P. Agrawal. Tgrl: An algorithm for teacher guided reinforcement learning. In International Conference on Machine Learning, pages 31077\u201331093. PMLR, 2023.   \n[75] M. Shi, Y. Liang, and N. Shroff. Theoretical hardness and tractability of pomdps in rl with partial online state information, 2024.   \n[76] S. M. Shortreed, E. Laber, D. J. Lizotte, T. S. Stroup, J. Pineau, and S. A. Murphy. Informing sequential clinical decision-making through reinforcement learning: an empirical study. Machine learning, 84:109\u2013136, 2011.   \n[77] Z. Song, S. Mei, and Y. Bai. When can we learn general-sum Markov games with a large number of players sample-efficiently? arXiv preprint arXiv:2110.04184, 2021.   \n[78] J. Subramanian, A. Sinha, R. Seraj, and A. Mahajan. Approximate information state for approximate planning and reinforcement learning in partially observed systems. J. Mach. Learn. Res., 23:12\u20131, 2022.   \n[79] J. Tsitsiklis and M. Athans. On the complexity of decentralized decision making and detection problems. IEEE Transactions on Automatic Control, 30(5):440\u2013446, 1985.   \n[80] M. Uehara, A. Sekhari, J. D. Lee, N. Kallus, and W. Sun. Computationally efficient pac rl in pomdps with latent determinism and conditional embeddings. In International Conference on Machine Learning, pages 34615\u201334641. PMLR, 2023.   \n[81] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019.   \n[82] A. Wang, A. C. Li, T. Q. Klassen, R. T. Icarte, and S. A. McIlraith. Learning belief representations for partially observable deep rl. In International Conference on Machine Learning, pages 35970\u201335988. PMLR, 2023.   \n[83] L. Wang, Q. Cai, Z. Yang, and Z. Wang. Represent to control partially observed systems: Representation learning with provable sample efficiency. In The Eleventh International Conference on Learning Representations, 2022.   \n[84] H. S. Witsenhausen. A counterexample in stochastic optimum control. SIAM Journal on Control, 6(1):131\u2013147, 1968.   \n[85] G. Yang, M. Liu, W. Hong, W. Zhang, F. Fang, G. Zeng, and Y. Lin. Perfectdou: Dominating doudizhu with perfect information distillation. Advances in Neural Information Processing Systems, 35:34954\u201334965, 2022.   \n[86] Y. Yang, Y. Jiang, J. Chen, S. E. Li, Z. Gu, Y. Yin, Q. Zhang, and K. Yu. Belief state actor-critic algorithm from separation principle for POMDP. In 2023 American Control Conference (ACC), pages 2560\u20132567. IEEE, 2023.   \n[87] S. Young, M. Ga\u0161i\u00b4c, B. Thomson, and J. D. Williams. Pomdp-based statistical spoken dialog systems: A review. Proceedings of the IEEE, 101(5):1160\u20131179, 2013.   \n[88] A. Zanette and E. Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In International Conference on Machine Learning, pages 7304\u20137312. PMLR, 2019.   \n[89] W. Zhan, M. Uehara, W. Sun, and J. D. Lee. PAC reinforcement learning for predictive state representations. In The Eleventh International Conference on Learning Representations, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Supplementary Materials for \u201cProvable Partially Observable Reinforcement Learning with Privileged Information\u201d ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Contents ", "page_idx": 16}, {"type": "text", "text": "1 Introduction 1   \n2 Preliminaries 2   \n2.1 Partially Observable RL (with Privileged Information) 2   \n2.2 Partially Observable Multi-agent RL with Information Sharing 3   \n2.3 Technical Assumptions for Computational Tractability 4   \n3 Revisiting Empirical Paradigms of RL with Privileged Information 4   \n3.1 Privileged Policy Learning: Expert Policy Distillation . . 5   \n3.2 Privileged Value Learning: Asymmetric Actor-Critic 5   \nProvably Efficient Expert Policy Distillation 6   \n5 Provable Asymmetric Actor-Critic with Approximate Belief Learning 8   \n5.1 Belief-Weighted Optimistic Asymmetric Actor-Critic 8   \n5.2 Approximate Belief Learning . . . 8   \n6 Numerical Validation 9   \nExtension to Partially Observable MARL with Privileged Information 9   \n7.1 Privileged Policy Learning: Equilibrium Distillation . 9   \nPrivileged Value Learning: Asymmetric MARL with Approximate Belief Learning 10   \nA Societal Impact 19   \nB Related Work 19   \nC Additional Preliminaries 20   \nC.1 Additional Preliminaries on POMDPs 20   \nC.2 Additional Preliminaries for POSGs 20   \nC.2.1 Evolution of the Common and Private Information 23   \nC.3 Strategy Independence of Belief and Examples 23   \nD Collection of Algorithms 24   \nE Missing Details in Section 3 31   \nF Missing Details in Section 4 33 ", "page_idx": 16}, {"type": "text", "text": "G Provably Efficient Expert Policy Distillation with Function Approximation 34 ", "page_idx": 17}, {"type": "text", "text": "H Missing Details in Section 5 35   \nH.1 Supporting Technical Lemmas 44 ", "page_idx": 17}, {"type": "text", "text": "I Missing Details in Section 6 47 ", "page_idx": 17}, {"type": "text", "text": "Missing Details in Section 7 48   \nJ.1 Background on Bayesian Games 62 ", "page_idx": 17}, {"type": "text", "text": "K Concluding Remarks and Limitations 62 ", "page_idx": 17}, {"type": "text", "text": "A Societal Impact ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our work is theoretical by nature, and aimed at better understanding reinforcement learning under partial observability with privileged information. As such, we do not anticipate any direct positive or negative societal impact from this research. ", "page_idx": 18}, {"type": "text", "text": "B Related Work ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Provable partial observable RL. While POMDPs are generally known to be both statistically hard [42] and computationally intractable [65], a productive line of research has identified several structured subclasses of POMDPs that can be efficiently solved. [42] introduced the class of POMDPs in the rich-observation setting, where the observation space can be large and fully reveal the underlying state, where sample-efficient RL becomes possible [34, 59]. [19] introduced $k$ -step decodable POMDPs, where the last $k$ observation-action pairs can uniquely determine the state, and proposed polynomial-sample complexity algorithms (assuming $k$ is a small constant). Beyond settings where the underlying state can be exactly recovered, [35, 46] proposed weakly revealing POMDPs, where the observations are assumed to be informative enough. Under the weakly revealing condition (and its variant), there has been a fast-growing line of recent works developing sample-efficient RL algorithms for various settings, see e.g., [83, 15, 12, 53, 47, 89]. Notably, these algorithms are typically computationally inefficient, requiring access to an optimistic planning oracle for POMDPs. On a promising note, [26] showed that in observable POMDPs (see Assumption 2.2), one can have quasi-polynomial-time algorithms for planning the near-optimal policy, which further leads to provable RL algorithms [25, 27] with both quasi-polynomial samples and time. ", "page_idx": 18}, {"type": "text", "text": "RL under hindsight observability. The closest line of research to ours are the recent theoretical studies for Hindsight Observable Markov Decision Processes (HOMDPs) [43], where the underlying state is revealed at the end of the episode; see also subsequent related works in [30, 75] with different observation feedback models. These works focused purely on sample efficiency, and showed that polynomial sample complexity can be achieved without (or by further relaxing) aforementioned structural assumptions of the model (e.g., observability or decodability), in both tabular and/or function approximation settings. However, the algorithms (also) require an oracle for planning or even optimistic planning in a learned approximate POMDP, which are not computationally tractable in general. Indeed, without any structural assumption, learning the optimal policy in HOMDPs is computationally no easier than the planning problem, which thus remains PSPACE-hard . [65]. Meanwhile, even under the additional assumption of observability on the underlying POMDP model, it is still not clear if these algorithms can avoid computationally intractable oracles, since the approximate POMDP that [43] needs to do planning on at every iteration during learning can be quite different from the underlying model. For example, at the beginning of exploration when not enough samples are collected, or when there exist certain states that remain less explored during the entire learning process, the potentially misspecified emission (and transition) may break the observability (or other structural) assumptions made for the underlying POMDP. This makes that single iteration even computationally intractable. In contrast, our focus is on better understanding practically inspired algorithmic paradigms, without computationally intractable oracles, which in practice often do have and use the privileged state information during each episode (instead of only at the end) [67, 44, 14]. ", "page_idx": 18}, {"type": "text", "text": "Most related empirical works. Privileged information has been widely used in empirical partially observable RL, with two main types of approaches based on privileged policy and privileged value learning, respectively. For the former, one prominent example is expert distillation [14, 63, 57], also known as teacher-student learning [44, 58, 74], as we analyze in Section 4. For the latter, asymmetric actor-critic [67] represents one of the well-known examples, with other studies in [7, 3]. Learning privileged value functions (to improve the policies) has also been widely used in multi-agent RL, featured in centralized-training-decentralized-execution, see e.g., [52, 22, 69, 85, 81]. Intriguingly, it was shown that if the privileged value function depends only on the state, the associated actor will cause bias [6, 54, 55]. This has thus necessitated the use of history/belief in asymmetric actor-critic, as in our Section 5. Notably, the empirical framework in [82] exactly matches ours, where they exploited the privileged state information in training for belief learning, followed by policy optimization over the learned belief states. Indeed, many empirical works explicitly separate the procedures of explicit belief-state learning and planning [24, 64, 33, 16, 86] as we study in Section 5, oftentimes with privileged state information to supervise the belief learning procedure [60, 5]. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "C Additional Preliminaries ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Additional Preliminaries on POMDPs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Belief and approximate belief. Although in a POMDP, the agent cannot see the underlying state directly, it can still form a belief over the underlying state by the historical observations and actions. Definition C.1 (Belief state update). For each $h\\in[H+1]$ , the Bayes operator $B_{h}:\\Delta(S)\\times{\\mathcal{O}}\\to$ $\\Delta(S)$ is defined for $b\\in\\Delta(S)$ , and $o\\in{\\mathcal{O}}$ by: ", "page_idx": 19}, {"type": "equation", "text": "$$\nB_{h}(b;o)(x)=\\frac{\\mathbb{O}_{h}(o\\,|\\,x)b(x)}{\\sum_{z\\in\\mathcal{S}}\\mathbb{O}_{h}(o\\,|\\,z)b(z)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for each $x\\in S$ . For each $h\\in[H+1]$ , the belief update operator $U_{h}:\\Delta(S)\\times A\\times O\\to\\Delta(S)$ , is defined by ", "page_idx": 19}, {"type": "equation", "text": "$$\nU_{h}(b;a,o)=B_{h+1}\\left(\\mathbb{T}_{h}(a)\\cdot b;o\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbb{T}_{h}(a)\\cdot b$ represents the matrix multiplication. We use the notation $b_{h}$ to denote the belief update function, which receives a sequence of actions and observations and outputs a distribution over states at the step $h$ : the belief state at step $h=1$ is defined as $b_{1}(\\emptyset)=\\mu_{1}$ . For any $2\\leq h\\leq H$ and any action-observation sequence $\\left(a_{1:h},o_{1:h}\\right)$ , we inductively define the belief state: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{h+1}\\big(a_{1:h},o_{1:h}\\big)=\\mathbb{T}_{h}\\big(a_{h}\\big)\\cdot b_{h}\\big(a_{1:h-1},o_{1:h}\\big),}\\\\ &{b_{h}\\big(a_{1:h-1},o_{1:h}\\big)=B_{h}\\big(b_{h}\\big(a_{1:h-1},o_{1:h-1}\\big);o_{h}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We also define the approximate belief update using the most recent $L$ -step history. For $1\\leq h\\leq H$ , we follow the notation of [26] and define ", "page_idx": 19}, {"type": "equation", "text": "$$\nb_{h}^{\\mathrm{apx},\\mathcal{P}}(\\emptyset;D)=\\left\\{\\mu_{1}\\ \\ \\ \\mathrm{if}\\ h=1\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $D\\in\\Delta(S)$ is the prior for the approximate belief update. Then for any $1\\leq h-L<h\\leq H$ and any action-observation sequence $(a_{h-L:h},o_{h-L+1:h})$ , we inductively define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad b_{h+1}^{\\mathrm{apx},\\mathcal{P}}(a_{h-L:h},o_{h-L+1:h};D)=\\mathbb{T}_{h}(a_{h})\\cdot b_{h}^{\\mathrm{apx},\\mathcal{P}}(a_{h-L:h-1},o_{h-L+1:h};D),}\\\\ &{b_{h}^{\\mathrm{apx},\\mathcal{P}}(a_{h-L:h-1},o_{h-L+1:h};D)=B_{h}(b_{h}^{\\mathrm{apx},\\mathcal{P}}(a_{h-L:h-1},o_{h-L+1:h-1};D);o_{h}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the remainder of our paper, we will use an important initialization for the approximate belief, which are defined as $b_{h}^{\\prime}(\\cdot):=b_{h}^{\\mathrm{apx},\\mathcal{P}}(\\cdot;\\mathrm{Unif}(\\mathcal{S}))$ . ", "page_idx": 19}, {"type": "text", "text": "C.2 Additional Preliminaries for POSGs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Model. We use a general framework of partially observable stochastic games (POSGs) as the model for partially observable MARL. Formally, we define a POSG with $n$ agents by a tuple $\\mathcal{G}\\,=\\,\\left(H,\\bar{S},\\{A_{i}\\}_{i=1}^{\\bar{n}},\\{\\mathcal{O}_{i}\\}_{i=1}^{n},\\mathbb{T},\\mathbb{O},\\mu_{1},\\{r_{i}\\}_{i=1}^{n}\\right)$ , where $H$ denotes the length of each episode, $\\boldsymbol{S}$ is the state space with $|{\\cal S}|={\\cal S}$ , $\\mathcal{A}_{i}$ denotes the action space for agent $i$ with $|{\\mathcal{A}}_{i}|\\,=\\,A_{i}$ . We denote by $a:=(a_{1},\\cdot\\cdot\\cdot,\\dot{a}_{n})$ the joint action of all the $n$ agents, and by ${\\mathcal{A}}={\\mathcal{A}}_{1}\\times\\cdots\\times{\\mathcal{A}}_{n}$ the joint action space with $\\textstyle|{\\dot{A}}|=A=\\prod_{i=1}^{n}A_{i}$ . We use $\\mathbb{T}=\\{\\mathbb{T}_{h}\\}_{h\\in[H]}$ to denote the collection of transition matrices, so that $\\mathbb{T}_{h}(\\cdot\\,|\\,s,a)\\in\\Delta(S)$ gives the probability of the next state if joint action $a\\in A$ is taken at state $s\\in S$ and step $h$ . In the following discussions, for any given $a$ , we treat $\\mathbb{T}_{h}(a)\\in\\mathbb{R}^{|S|\\times|S|}$ as a matrix, where each row gives the probability for the next state from different current states. We use $\\mu_{1}$ to denote the distribution of the initial state $s_{1}$ , and $O_{i}$ to denote the observation space for agent $i$ with $|\\mathcal{O}_{i}|=O_{i}$ . We denote by $o:=(o_{1},\\ldots,o_{n})$ the joint observation of all $n$ agents, and by $\\mathcal{O}:=\\mathcal{O}_{1}\\times\\cdot\\cdot\\times\\mathcal{O}_{n}$ with $\\begin{array}{r}{|\\mathcal{O}|=\\dot{O}=\\prod_{i=1}^{n}O_{i}}\\end{array}$ . We use $\\mathbb{O}=\\{\\mathbb{O}_{h}\\}_{h\\in[H]}$ to denote the collection of the joint emission matrices, so that $\\mathbb{O}_{h}(\\cdot\\,|\\,s)\\in\\Delta(\\mathcal{O})$ gives the emission distribution over the joint observation space $\\scriptscriptstyle\\mathcal{O}$ at state $s$ and step $h$ . For notational convenience, we will at times adopt the matrix convention, where $\\mathbb{O}_{h}$ is a matrix with rows $\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})$ . We also denote $\\mathbb{O}_{i,h}(\\cdot\\,|\\,s)\\in\\Delta(\\mathcal{O}_{i})$ as the marginalized emission for agent $i$ agent. Finally, $r_{i}=\\{r_{i,h}\\}_{h\\in[H]}$ is a collection of reward functions, so that $r_{i,h}(s_{h},a_{h})$ is the reward of agent $i$ agent given the state $s_{h}$ and joint action $a_{h}$ at step $h$ . ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Similar to a POMDP, in a POSG, the states are not observable to the agents, and each agent can only access its own individual observations. The game proceeds as follows. At the beginning of each episode, the environment samples $s_{1}$ from $\\mu_{1}$ . At each step $h$ , each agent $i$ observes its own observation $^{O_{i,h}}$ , where $o_{h}:=(o_{1,h},\\ldots,o_{n,h})$ is sampled jointly from $\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})$ . Then each agent $i$ takes the action $a_{i,h}$ and receives the reward $r_{i,h}(s_{h},a_{h})$ . After that the environment transitions to the next state $s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})$ . The current episode terminates once $s_{H+1}$ is reached. ", "page_idx": 20}, {"type": "text", "text": "Information sharing, common and private information. Each agent $i$ in the POSG maintains its own information, $\\tau_{i,h}$ , a collection of historical observations and actions at step $h$ , namely, $\\tau_{i,h}\\subseteq\\{o_{1},a_{1},o_{2},\\cdot\\cdot\\cdot\\,,a_{h-1},o_{h}\\}$ , and the collection of such history at step $h$ is denoted by $\\tau_{i,h}$ . ", "page_idx": 20}, {"type": "text", "text": "In many practical examples, agents may share part of the history with each other, which may introduce some information structures of the game that may lead to both sample and computation efficiencies. The information sharing splits the history into common/shared and private information for each agent. The common information at step $h$ is a subset of the joint history $\\tau_{h}=(\\tau_{i,h})_{i\\in[n]}$ : $c_{h}\\subseteq\\{o_{1},a_{1},o_{2},\\cdot\\cdot\\cdot,a_{h-1},o_{h}\\}$ , which is available to all the agents in the system, and the collection of the common information is denoted as $\\ensuremath{\\mathcal{C}}_{h}$ and we define $C_{h}=|\\mathcal{C}_{h}|$ . Given the common information $c_{h}$ , each agent also has her private information $p_{i,h}=\\tau_{i,h}\\setminus c_{h}$ , where the collection of the private information for agent $i$ is denoted as $\\mathcal{P}_{i,h}$ and its cardinality as $P_{i,h}$ . The cardinality of the joint private information is $\\begin{array}{r}{P_{h}=\\prod_{i=1}^{n}P_{i,h}}\\end{array}$ . We allow $c_{h}$ or $p_{i,h}$ to take the special value $\\varnothing$ when there is no common or private information. In particular, when ${\\mathcal{C}}_{h}=\\{\\varnothing\\}$ , the problem reduces to the general POSG without any favorable information structure; when $\\bar{\\mathcal{P}_{i,h}}=\\{\\varnothing\\}$ , every agent holds the same history, and it reduces to a POMDP when the agents share a common reward function, where the goal is usually to find the team-optimal solution. ", "page_idx": 20}, {"type": "text", "text": "Policies and value functions. We define a stochastic policy for agent $i$ at step $h$ as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi_{i,h}:\\Omega_{h}\\times\\mathcal{P}_{i,h}\\times\\mathcal{C}_{h}\\to\\Delta(A_{i}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Omega_{h}$ is the random seed space, which is shared among agents and $\\omega_{i,h}\\in\\Omega_{h}$ is the random seed for agent $i$ . The corresponding policy class is denoted as $\\Pi_{i,h}$ . Hereafter, unless otherwise noted, when referring to policies, we mean the policies given in the form of (C.1), which maps the available information of agent $i$ , i.e., the private information together with the common information, to the distribution over her actions. ", "page_idx": 20}, {"type": "text", "text": "We define $\\pi_{i}$ as a sequence of policies for agent $i$ at all steps $h\\in[H]$ , i.e., $\\pi_{i}=(\\pi_{i,1},\\cdot\\cdot\\cdot\\,,\\pi_{i,H})$ . We further denote $\\Pi_{i}=\\times_{h\\in[H]}\\Pi_{i,h}$ as the policy space for agent $i$ and $\\begin{array}{r}{\\Pi=\\prod_{i\\in[n]}\\Pi_{i}}\\end{array}$ as the joint policy space. As a special case, we define the space of deterministic policy as $\\widetilde{\\Pi}_{i}$ , where $\\widetilde{\\pi}_{i}\\in\\widetilde{\\Pi}_{i}$ maps the private information and common information to a deterministic action  for agent $i$ and the joint space as $\\begin{array}{r}{\\widetilde{\\Pi}=\\prod_{i\\in[n]}\\widetilde{\\Pi}_{i}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "A product policy is denoted as $\\pi=\\pi_{1}\\times\\pi_{2}\\cdot\\cdot\\cdot\\times\\pi_{n}\\in\\Pi$ if the distributions of drawing each seed $\\omega_{i,h}$ for different agents are independent, and a (potentially correlated) joint policy is denoted as $\\pi=\\pi_{1}\\odot\\pi_{2}\\cdot\\cdot\\cdot\\odot\\pi_{n}\\in\\Pi$ . ", "page_idx": 20}, {"type": "text", "text": "We are now ready to define the value function conditioned on the common information under our model of POSG with information sharing: ", "page_idx": 20}, {"type": "text", "text": "Definition C.2 (Value function with information sharing). For each agent $i\\in[n]$ and step $h\\in[H]$ , given common information $c_{h}$ and joint policy $\\pi=(\\pi_{i})_{i=1}^{n}\\in\\Pi$ , the value function conditioned on the common information of agent $i$ is defined as: $\\begin{array}{r}{V_{i,h}^{\\pi,\\mathcal{G}}(c_{h}):=\\mathbb{E}_{\\pi}^{\\mathcal{G}}\\:\\Bigl[\\sum_{h^{\\prime}=h}^{H}r_{i,h^{\\prime}}(s_{h^{\\prime}},a_{h^{\\prime}})\\:\\big|\\:c_{h}\\Bigr],}\\end{array}$ where the expectation is taken over the randomness from the model $\\mathcal{G}$ , policy $\\pi$ , and the random seeds. For any $c_{H+1}\\in\\mathcal{C}_{H+1}:V_{i,H+1}^{\\pi,\\mathcal{G}}(c_{H+1}):=0$ . From now on, we will refer to it as value function for short. ", "page_idx": 20}, {"type": "text", "text": "Another key concept in our analysis is the belief about the state and the private information conditioned on the common information among agents. Formally, at step $h$ , given policies from 1 to $h-1$ , we consider the common-information-based conditional belief P\u03c0h1:h\u22121,G( $\\mathbb{P}_{h}^{\\pi_{1:h-1},\\mathcal{G}}\\left(s_{h},p_{h}\\,\\big|\\,c_{h}\\right)$ . This belief not only infers the current underlying state $s_{h}$ , but also all agents\u2019 private information $p_{h}$ . With the common-information-based conditional belief, the value function given in Definition C.2 has the following recursive structure: ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\nV_{i,h}^{\\pi,\\mathcal{G}}(c_{h})=\\mathbb{E}_{\\pi}^{\\mathcal{G}}[r_{i,h}(s_{h},a_{h})+V_{i,h+1}^{\\pi,\\mathcal{G}}(c_{h+1})\\,|\\,c_{h}],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the expectation is taken over the randomness of $\\left(s_{h},p_{h},a_{h},o_{h+1}\\right)$ . With this relationship, we can define the prescription-value function correspondingly, a generalization of the action-value function in (fully observable) stochastic games and MDPs to POSGs with information sharing, as follows. ", "page_idx": 21}, {"type": "text", "text": "Definition C.3 (Prescription-value function with information sharing). At step $h$ , given the common information $c_{h}$ , joint policies $\\pi=(\\pi_{i})_{i=1}^{n}\\in\\Pi$ , and prescriptions $(\\gamma_{i,h})_{i=1}^{n}\\in\\Gamma_{h}$ , the prescriptionvalue function conditioned on the common information and joint prescription of agent $i$ is defined as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{i,h}^{\\pi,\\mathcal{G}}(c_{h},(\\gamma_{j,h})_{j\\in[n]}):=\\mathbb{E}_{\\pi}^{\\mathcal{G}}\\left[r_{i,h}(s_{h},a_{h})+V_{i,h+1}^{\\pi,\\mathcal{G}}(c_{h+1})\\bigm|c_{h},(\\gamma_{j,h})_{j\\in[n]}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where prescription $\\gamma_{i,h}\\,\\in\\,\\Delta(A_{i})^{P_{i,h}}$ replaces the partial function $\\pi_{i,h}(\\cdot\\,|\\,\\omega_{i,h},c_{h},\\cdot)$ in the value function. From now on, we will refer to it as prescription-value function for short. With such a prescription-value function, agents can take actions purely based on their local private information [62, 61, 50]. ", "page_idx": 21}, {"type": "text", "text": "This prescription-value function indicates the expected return for agent $i$ when all the agents firstly adopt the prescriptions $(\\gamma_{j,h})_{j\\in[n]}$ and then follow the policy $\\pi$ . ", "page_idx": 21}, {"type": "text", "text": "Equilibrium notions. With the definition of value functions, we can accordingly define the solution concepts. Here we define the notions of $\\epsilon$ -NE, $\\epsilon$ -CCE, $\\epsilon{-}\\mathrm{CE}$ , and $\\epsilon$ -team optimum under the information-sharing framework as follows. For a joint policy $(\\pi_{i})_{i=1}^{n}\\in\\Pi$ we denote the expected reward of agent $i$ by $\\begin{array}{r}{v_{i}^{\\mathcal{G}}(\\pi)=\\mathbb{E}_{\\pi}^{\\mathcal{P}}[\\sum_{h=1}^{H}r_{i,h}(s_{h},a_{h})]}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "Definition C.4 ( $\\epsilon$ -approximate Nash equilibrium with information sharing). For any $\\epsilon\\geq0$ , a product policy $\\pi^{\\star}\\in\\Pi$ is an $\\epsilon$ -approximate Nash equilibrium of the POSG $\\mathcal{G}$ with information sharing if: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{NE-gap}(\\pi^{\\star}):=\\operatorname*{max}_{i}\\left(\\operatorname*{max}_{\\pi_{i}^{\\prime}\\in\\Pi_{i}}v_{i}^{\\mathcal{G}}(\\pi_{i}^{\\prime}\\times\\pi_{-i}^{\\star})-v_{i}^{\\mathcal{G}}(\\pi^{\\star})\\right)\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Definition C.5 ( $\\epsilon$ -approximate coarse correlated equilibrium with information sharing). For any $\\epsilon\\geq0$ , a joint policy $\\pi^{\\star}\\in\\Pi$ is an $\\epsilon_{}$ -approximate coarse correlated equilibrium of the POSG $\\mathcal{G}$ with information sharing if: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{CCE-gap}(\\pi^{\\star}):=\\operatorname*{max}_{i}\\left(\\operatorname*{max}_{\\pi_{i}^{\\prime}\\in\\Pi_{i}}v_{i}^{\\mathcal{G}}(\\pi_{i}^{\\prime}\\times\\pi_{-i}^{\\star})-v_{i}^{\\mathcal{G}}(\\pi^{\\star})\\right)\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Definition C.6 ( $\\epsilon_{}$ -approximate correlated equilibrium with information sharing). For any $\\epsilon\\geq0$ , a joint policy $\\pi^{\\star}\\in\\Pi$ is an $\\epsilon$ -approximate correlated equilibrium of the POSG $\\mathcal{G}$ with information sharing if: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{CE-gap}(\\pi^{\\star}):=\\operatorname*{max}_{i}\\left(\\operatorname*{max}_{\\phi_{i}}v_{i}^{\\mathcal{G}}\\big((m_{i}\\diamond\\pi_{i}^{\\star})\\odot\\pi_{-i}^{\\star}\\big)-v_{i}^{\\mathcal{G}}(\\pi^{\\star})\\right)\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $m_{i}$ is called strategy modification for agent $i$ , and $m_{i}~=~\\{m_{i,h,c_{h},p_{i,h}}\\}_{h,c_{h},p_{i,h}}$ , with each $m_{i,h,c_{h},p_{i,h}}\\ :\\ A_{i}\\ \\rightarrow\\ A_{i}$ being a mapping from the action set to itself. The space of $m_{i}$ is denoted as $\\mathcal{M}_{i}$ . The composition $m_{i}\\diamond\\pi_{i}$ is defined as follows: at step $h$ , when agent $i$ is given $c_{h}$ and $p_{i,h}$ , the joint action chosen to be $\\left(a_{1,h},\\cdot\\cdot\\cdot,a_{i,h},\\cdot\\cdot\\cdot,a_{n,h}\\right)$ will be modified to $(a_{1,h},\\cdot\\cdot\\cdot,m_{i,h,c_{h},p_{i,h}}(a_{i,h}),\\cdot\\cdot\\cdot,a_{n,h})$ . Note that this definition follows from that in [77, 49, 37, 48, 50] when there exists certain common information, and is a natural generalization of the definition in the normal-form game case [70]. We denote by $\\mathcal{M}_{i}^{\\mathrm{gen}}$ the space of all possible strategy modifications $m_{i}$ if it conditions on any history information instead of only $(c_{h},p_{i,h})$ . Similarly, we use $\\mathcal{M}_{\\mathcal{S},i}$ to denote the space of all possible strategy modifications $m_{i}$ if it only conditions on the current state e.g., a modification $m_{i,h,s}:A_{i}\\rightarrow A_{i}$ . ", "page_idx": 21}, {"type": "text", "text": "C.2.1 Evolution of the Common and Private Information ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Assumption C.7 (Evolution of common and private information). We assume that common information and private information evolve over time as follows: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Common information $c_{h}$ is non-decreasing over time, that is, $c_{h}\\,\\subseteq\\,c_{h+1}$ for all $h$ . Let $\\varpi_{h+1}=c_{h+1}\\setminus c_{h}$ . Thus, $c_{h+1}=\\{c_{h},\\varpi_{h+1}\\}$ . Further, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\varpi_{h+1}=\\chi_{h+1}(p_{h},a_{h},o_{h+1}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\chi_{h+1}$ is a fixed transformation. We use $\\Upsilon_{h+1}$ to denote the collection of $\\varpi_{h+1}$ at step $h$ . ", "page_idx": 22}, {"type": "text", "text": "\u2022 Private information evolves according to: ", "page_idx": 22}, {"type": "equation", "text": "$$\np_{i,h+1}=\\xi_{i,h+1}(p_{i,h},a_{i,h},o_{i,h+1}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\xi_{i,h+1}$ is a fixed transformation. ", "page_idx": 22}, {"type": "text", "text": "Equation (C.3) states that the increment in the common information depends on the \u201cnew\" information $(a_{h},o_{h+1})$ generated between steps $h$ and $h+1$ and part of the old information $p_{h}$ . The incremental common information can be obtained by certain sharing and communication protocols among the agents. Equation (C.4) implies that the evolution of private information only depends on the newly generated private information $a_{i,h}$ and $O_{i,h+1}$ . These evolution rules are standard in the literature [61, 62], specifying the source of common information and private information. Based on such evolution rules, we define $\\{f_{h}\\}_{h\\in[H]}$ and $\\{g_{h}\\}_{h\\in[H]}$ , where $f_{h}:{\\mathcal{A}}^{h}\\times{\\mathcal{O}}^{h}\\,\\to{\\mathcal{C}}_{h}$ and $g_{h}:\\mathcal{A}^{h}\\times\\mathcal{O}^{h}\\to\\mathcal{P}_{h}$ for $h\\in[H]$ , as the mappings that map the joint history to common information and joint private information, respectively. ", "page_idx": 22}, {"type": "text", "text": "C.3 Strategy Independence of Belief and Examples ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To solve a POSG without computationally intractable oracles, certain information-sharing is needed even under the observability assumption [50]. We thus make the following assumption as in [50]. ", "page_idx": 22}, {"type": "text", "text": "Assumption C.8 (Strategy independence of beliefs [61, 31, 50]). Consider any step $h\\in[H]$ , any policy $\\pi\\in\\Pi$ , and any realization of common information $c_{h}$ that has a non-zero probability under the trajectories generated by $\\pi_{1:h-1}$ . Consider any other policies $\\pi_{1:h-1}^{\\prime}$ , which also give a nonzero probability to $c_{h}$ . Then, we assume that: for any such $c_{h}\\in\\mathcal{C}_{h}$ , and any $p_{h}\\,\\in\\,\\mathcal{P}_{h},s_{h}\\,\\in\\,\\mathcal{S}$ , $\\begin{array}{r}{\\mathbb{P}_{h}^{\\pi_{1:h-1},\\mathcal{G}}\\left(s_{h},p_{h}\\,|\\,c_{h}\\right)=\\mathbb{P}_{h}^{\\pi_{1:h-1}^{\\prime},\\mathcal{G}}\\left(s_{h},p_{h}\\,|\\,c_{h}\\right).}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "We provide examples satisfying this assumption in Appendix C.2, which include the fully-sharing structure as in [27, 68] as a special case. Finally, we also assume that common information and private information evolve over time properly in Assumption C.7, as standard in [61, 62, 50], which covers the models considered in [27, 68, 48]. ", "page_idx": 22}, {"type": "text", "text": "Here we take the examples from [51, 50] to illustrate the generality of the information-sharing framework. ", "page_idx": 22}, {"type": "text", "text": "Example C.9 (One-step delayed sharing). At any step $h\\in[H]$ , the common and private information are given as $c_{h}=\\left\\{o_{2:h-1},a_{1:h-1}\\right\\}$ and $p_{i,h}=\\dot{\\{o_{i,h}\\}}$ , respectively. In other words, the players share all the action-observation history until the previous step $h-1$ , with only the new observation being the private information. This model has been shown useful for power control [2]. ", "page_idx": 22}, {"type": "text", "text": "Example C.10 (State controlled by one controller with asymmetric delay sharing). We assume there are 2 players for convenience. It extends naturally to $n$ -player settings. Consider the case where the state dynamics are controlled by player 1, i.e., $\\mathbb{T}_{h}\\bar{(}\\cdot\\,\\vert\\;\\dot{s}_{h},a_{1,h},\\bar{a_{2,h}})\\;=\\;\\mathbb{T}_{h}\\big(\\cdot\\,\\vert\\;s_{h},a_{1,h},a_{2,h}^{\\prime}\\big)$ for all $(s_{h},a_{1,h},a_{2,h},a_{2,h}^{\\prime},h)$ . There are two kinds of delay-sharing structures we could consider: Case A: the information structure is given as $c_{h}\\,=\\,\\left\\{o_{1,2:h},o_{2,2:h-d},a_{1,1:h-1}\\right\\}$ , $p_{1,h}\\,=\\,\\emptyset$ , $p_{2,h}=\\{o_{2,h-d+1:h}\\}$ , i.e., player 1\u2019s observations are available to player 2 instantly, while player 2\u2019s observations are available to player 1 with a delay of $d\\geq1$ time steps. Case B: similar to Case A but player 1\u2019s observation is available to player 2 with a delay of 1 step. The information structure is given as $c_{h}\\,=\\,\\{o_{1,2:h-1},o_{2,2:h-d},a_{1,1:h-1}\\}$ , $p_{1,h}\\,=\\,\\{o_{1,h}\\},\\,p_{2,h}\\,=\\,\\stackrel{\\cdot}{\\big\\{}o_{2,h-d+1:h}\\big\\}$ , where $d\\geq1$ . This kind of asymmetric sharing is common in network routing [66], where packages arrive at different hosts with different delays, leading to asymmetric delay sharing among hosts. ", "page_idx": 22}, {"type": "text", "text": "Example C.11 (Symmetric information game). Consider the case when all observations and actions are available for all the agents, and there is no private information. Essentially, we have $c_{h}=$ $\\left\\{o_{2:h},a_{1:h-1}\\right\\}$ and $p_{i,h}=\\bar{\\emptyset}$ . We will also denote this structure as fully sharing hereafter. ", "page_idx": 23}, {"type": "text", "text": "Example C.12 (Information sharing with one-directional-one-step delay). Similar to the previous cases, we also assume there are 2 players for ease of exposition, and the case can be generalized to multi-player cases straightforwardly. Similar to the one-step delay case, we consider the situation where all observations of player 1 are available to player 2, while the observations of player 2 are available to player 1 with one-step delay. All the past actions are available to both players. That is, in this case, $c_{h}=\\{o_{1,2:h},o_{2,2:h-1},a_{1:h-1}\\}.$ , and player 1 has no private information, i.e., $p_{1,h}=\\emptyset$ , and player 2 has private information $p_{2,h}=\\bar{\\{o_{2,h}\\}}$ . ", "page_idx": 23}, {"type": "text", "text": "Example C.13 (Uncontrolled state process). Consider the case where the state transition does not depend on the actions, that is, $\\mathbb{T}_{h}\\big(\\cdot\\big|\\ s_{h},a_{h}\\big)=\\mathbb{T}_{h}\\big(\\cdot\\mid s_{h},a_{h}^{\\prime}\\big)$ for any $s_{h},a_{h},a_{h}^{\\prime},h$ . Note that the agents are still coupled through the joint reward. An example of this case is the information structure where controllers share their observations with a delay of $d\\geq1$ time steps. In this case, the common information is $c_{h}=\\{o_{2:h-d}\\}$ and the private information is $p_{i,h}=\\{o_{i,\\bar{h}-d+1:h}\\}$ . Such information structures can be used to model repeated games with incomplete information [4]. ", "page_idx": 23}, {"type": "text", "text": "D Collection of Algorithms ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Algorithm 1 Learning Decoding Function with Privileged Information ", "page_idx": 23}, {"type": "text", "text": "Require: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1: \u2022 POMDP $\\mathcal{P}$ , \u2022 Expert policy $\\pi^{E}\\in\\Pi_{{\\mathcal{S}}}$ , \u2022 Number of sampled episodes per step $M$ . ", "page_idx": 23}, {"type": "text", "text": "2: For the $\\textit{b}=\\textit{1}$ step: Collect $M$ state-observation pairs from the first-step $\\begin{array}{r l}{\\widehat{D}_{1}}&{{}=}\\end{array}$ $\\left\\{\\left(s_{1}^{(i)},o_{1}^{(i)}\\right)\\right\\}_{i\\in[M]}$ on POMDP $\\mathcal{P}$ and define the decoding function $g_{1}$ for the first step as: ", "page_idx": 23}, {"type": "equation", "text": "$$\ng_{1}(o_{1})=\\Big\\{s_{1}:(s_{1},o_{1})\\in\\widehat{D}_{1}\\Big\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "4: Collect M episodes  s(1i:)H+1, o(1i:)H, a(1i:)H  i\u2208[M] o n POMDP $\\mathcal{P}$ using policy $\\pi^{E}$ and let: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{D}_{h}:=\\left\\{\\left(s_{h-1}^{(i)},a_{h-1}^{(i)},o_{h}^{(i)},s_{h}^{(i)}\\right)\\right\\}_{i\\in[M]}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "5: Define the decoding function $g_{h}$ for step $h$ as: ", "page_idx": 23}, {"type": "equation", "text": "$$\ng_{h}\\big(s_{h-1},a_{h-1},o_{h}\\big)=\\bigg\\{s_{h}:\\big(s_{h-1},a_{h-1},o_{h},s_{h}\\big)\\in\\widehat{D}_{h}\\bigg\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "6: end for ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "7: return {gh}h\u2208[H] ", "page_idx": 23}, {"type": "text", "text": "Algorithm 2 Belief-Weighted Optimistic Asymmetric Actor-Critic with Privileged Information ", "page_idx": 24}, {"type": "text", "text": "Require: \u2022 POMDP $\\mathcal{P}$ , \u2022 Subroutine $T_{Q}$ that given policy $\\pi\\quad\\in\\ \\Pi^{L}$ , outputs $\\{\\widetilde{Q}_{h}^{\\pi}\\}_{h\\in[H]}$ that approximates $\\{Q_{h}^{\\pi,\\mathcal{P}}\\}_{h\\in[H]},$ , \u2022 Subroutine $T_{b}$ that outputs $\\{b_{h}^{\\mathrm{apx}}\\}_{h\\in[H]}$ that approximate $\\{b_{h}\\}_{h\\in[H]}$ , \u2022 Initial finite-memory policy $\\pi^{0}\\,=\\,\\{\\pi_{h}^{0}\\}_{h\\in[H]}\\,\\in\\,\\Pi^{L}$ for the POMDP $\\mathcal{P}$ , step-size $\\eta$ , and number of iterations $T$ . ", "page_idx": 24}, {"type": "text", "text": "Ensure: A near-optimal policy $\\{b_{h_{-}}^{\\mathrm{apx}}\\}_{h\\in[H]}\\gets\\bar{T}_{b}(\\mathcal{P})$ . for Iterations $t=1\\ldots,T$ do $\\{\\widetilde{Q}_{h}^{t}\\}_{h\\in[H]}\\gets T_{Q}(\\mathcal{P},\\pi^{t-1})$ U pdate the policy for each $a_{h}\\in A$ , $z_{h}\\in\\mathcal{Z}_{h}$ as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pi_{h}^{t}(a_{h}\\,|\\,z_{h})\\propto\\pi_{h}^{t-1}(a_{h}\\,|\\,z_{h})\\cdot\\exp\\left(\\eta\\mathbb{E}_{s_{h}\\sim b_{h}^{\\mathrm{apx}}(z_{h})}\\left[\\widetilde{Q}_{h}^{t-1}(z_{h},s_{h},a_{h})\\right]\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "end for return A policy uniform at random from set $\\{\\pi^{t}\\}_{t\\in[T]}$ ", "page_idx": 24}, {"type": "text", "text": "Algorithm 3 Optimistic $Q$ -function Estimation with Privileged Information ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Require: \u2022 POMDP $\\mathcal{P}$ , policy $\\pi_{1:H}\\in\\Pi^{L}$ such that $\\pi_{h}:S\\to\\Delta(A)$ , \u2022 Number of episodes $M$ per step. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widetilde{Q}_{H+1}^{\\pi}(z_{H+1},s_{H+1},a_{H+1})\\gets0,\\qquad\\pi_{H+1}(a_{H+1}\\,|\\,z_{H+1})\\gets\\frac{1}{|A|}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for step $h=H,\\ldots,1$ do Collect $M$ trajectories using policy $\\pi$ and let $D_{h}=\\{\\overline{{\\tau}}^{(i)}\\}_{i\\in[M]}$ be the collected trajectories. Compute empirical counts and define empirical distributions: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbb{T}}_{h}\\big(s_{h+1}\\mid s_{h},a_{h}\\big)=\\frac{\\lvert\\{\\mp\\in D_{h}:(s_{h}^{\\prime},a_{h}^{\\prime},s_{h+1}^{\\prime})=(s_{h},a_{h},s_{h+1})\\}\\rvert}{\\lvert\\{\\mp\\in D_{h}:(s_{h}^{\\prime},a_{h}^{\\prime})=(s_{h},a_{h})\\}\\rvert}}\\\\ &{\\qquad\\widehat{\\mathbb{Q}}_{h}\\big(o_{h}\\mid s_{h}\\big)=\\frac{\\lvert\\{\\mp\\in D_{h}:(s_{h}^{\\prime},o_{h}^{\\prime})=(s_{h},o_{h})\\}\\rvert}{\\lvert\\{\\mp\\in D_{h}:s_{h}^{\\prime}=s_{h}\\}\\rvert}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for each memory-state pair $(z_{h},s_{h})\\in\\mathcal{Z}_{h}\\times\\mathcal{S}$ do ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{Q}_{h}^{\\pi}(z_{h},s_{h},a_{h})=\\operatorname*{min}\\Bigg(H-h+1,\\mathbb{E}_{s_{h+1}\\sim\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h}),}[\\widetilde{V}_{h+1}^{\\pi}(z_{h+1},s_{h+1})]}\\\\ &{\\qquad\\qquad\\qquad\\qquad o_{h+1}\\sim\\widehat{\\mathbb{Q}}_{h+1}(\\cdot\\,|\\,s_{h+1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,r(s_{h},a_{h})+H\\cdot\\operatorname*{min}\\Bigg(2,C\\cdot\\sqrt{\\frac{|S|\\log(1/\\delta_{1})}{\\operatorname*{max}(N_{h}(s_{h},a_{h}),1)}}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}_{s_{h+1}\\sim\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})}H\\cdot\\operatorname*{min}\\Bigg(2,C\\cdot\\sqrt{\\frac{|\\mathcal{O}|\\log(1/\\delta_{1})}{\\operatorname*{max}(N_{h+1}(s_{h+1}),1)}}\\Bigg)\\Bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widetilde{V}_{h+1}^{\\pi}(z_{h+1},s_{h+1})=\\mathbb{E}_{a_{h+1}\\sim\\pi_{h+1}(\\cdot\\mid z_{h+1})}[\\widetilde{Q}_{h+1}^{\\pi}(z_{h+1},s_{h+1},a_{h+1})]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "end for return {Q\u03c0h}h\u2208[H] ", "page_idx": 24}, {"type": "text", "text": "Require: \u2022 POMDP $\\mathcal{P}=(H,\\mathcal{S},\\mathcal{A},\\mathcal{O},\\{\\mathbb{T}_{h}\\}_{h\\in[H]},\\{\\mathbb{O}_{h}\\}_{h\\in[H]},\\mu_{1},\\{r_{h}\\}_{h\\in[H]}),$ \u2022 An MDP learning oracle MDP_Learning that efficiently learns an approximate optimal policy of an MDP, \u2022 Number of trajectories $N$ , ", "page_idx": 25}, {"type": "text", "text": "\u03f5. Ensure: Approximate belief $\\{b_{h}^{\\mathrm{apx}}\\}_{h\\in[H]}$ (See Theorem H.5) for $h\\in[H]$ , $s_{h}\\in\\mathcal{S}$ do $\\widehat{r}_{h^{\\prime}}(\\bar{s}_{h}^{\\prime},\\bar{a}_{h}^{\\prime})\\gets\\mathbb{1}[h^{\\prime}=h,s_{h}^{\\prime}=s_{h}]$ for any $(h^{\\prime},s_{h}^{\\prime},a_{h}^{\\prime})\\in[H]\\times{\\mathcal{S}}\\times{\\mathcal{A}}$ . $\\mathcal{M}\\leftarrow(H,S,A,\\{\\mathbb{T}_{h}\\}_{h\\in[H]},\\mu_{1},\\{\\widehat{r}_{h}\\}_{h\\in[H]})$ to be the MDP associated with $\\mathcal{P}$ $\\Psi(h,s_{h})\\leftarrow\\mathbf{MDP}_{.}$ _Learning $(\\mathcal{M})$ Collect $N$ trajectories by executing policy $\\Psi(h,s_{h})$ for the first $h-1$ steps then take action $a_{h}$ for each $a_{h}\\in A$ deterministically and denote the dataset $\\{(s_{h}^{i},o_{h}^{i},a_{h}^{i},s_{h+1}^{i})\\}_{i\\in[N A]}$ for $(o_{h},a_{h},s_{h+1})\\in\\mathcal{O}\\times\\mathcal{A}\\times\\mathcal{S}$ do $\\begin{array}{r l}&{N_{h}(s_{h})\\gets\\sum_{i\\in[N A]}\\mathbb{1}[s_{h}^{i}=s_{h}]}\\\\ &{N_{h}(s_{h},a_{h})\\gets\\sum_{i\\in[N A]}\\mathbb{1}[s_{h}^{i}=s_{h},a_{h}^{i}=a_{h}]}\\\\ &{N_{h}(s_{h},a_{h},s_{h+1})\\gets\\sum_{i\\in[N A]}\\mathbb{1}[s_{h}^{i}=s_{h},a_{h}^{i}=a_{h},s_{h+1}^{i}=s_{h+1}]}\\\\ &{N_{h}(s_{h},o_{h})\\gets\\sum_{i\\in[N A]}\\mathbb{1}[s_{h}^{i}=s_{h},o_{h}^{i}=o_{h}]}\\\\ &{\\widehat{\\mathbb{T}}_{h}(s_{h+1}\\,|\\,s_{h},a_{h})\\gets\\frac{N_{h}(s_{h},a_{h},s_{h+1})}{N_{h}(s_{h},a_{h})}}\\\\ &{\\widehat{\\mathbb{O}}_{h}(o_{h}\\,|\\,s_{h})\\gets\\frac{N_{h}(s_{h},o_{h})}{N_{h}(s_{h})}}\\end{array}$ end for ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h\\in[H]\\,{\\bf d o}}\\\\ &{S_{h}^{\\mathrm{low}}\\leftarrow\\left\\{s_{h}\\in{\\cal S}\\,\\vert\\,\\frac{N_{h}(s_{h})}{N{\\cal A}}\\leq\\epsilon\\right\\}}\\\\ &{S_{h}^{\\mathrm{high}}\\leftarrow\\left\\{s_{h}\\in{\\cal S}\\,\\vert\\,\\frac{N_{h}(s_{h})}{N{\\cal A}}>\\epsilon\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{(h,s_{h},o_{h},a_{h},s_{h+1})}\\in[H]\\times S_{h}^{\\mathrm{high}}\\times\\mathcal{O}\\times\\mathcal{A}\\times S_{h}^{\\mathrm{high}}\\,\\mathbf{\\Phi}\\mathbf{d}\\mathbf{\\Phi}}\\\\ &{\\widehat{\\mathbb{T}}_{h}^{\\mathrm{tunc}}(s_{h+1}\\,|\\,s_{h},a_{h})\\gets\\widehat{\\mathbb{T}}_{h}(s_{h+1}\\,|\\,s_{h},a_{h})+\\frac{\\sum_{s_{h+1}^{\\prime}\\in S_{h+1}^{\\mathrm{tow}}}\\widehat{\\mathbb{T}}_{h}(s_{h+1}^{\\prime}\\,|\\,s_{h},a_{h})}{|S_{h+1}^{\\mathrm{high}}|}}\\\\ &{\\widehat{\\mathbb{D}}_{h}^{\\mathrm{tunc}}(o_{h}\\,|\\,s_{h})\\gets\\widehat{\\mathbb{O}}_{h}(o_{h}\\,|\\,s_{h})}\\\\ &{\\widehat{\\mu}_{1}^{\\mathrm{tunc}}(s_{1}):=\\widehat{\\mu}_{1}(s_{1})+\\frac{\\sum_{s_{1}^{\\prime}\\in S_{1}^{\\mathrm{low}}}\\widehat{\\mu}_{1}(s_{1}^{\\prime})}{|S_{1}^{\\mathrm{high}}|},\\forall s_{1}\\in S_{1}^{\\mathrm{high}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathcal{P}}^{\\mathrm{sub}}:=(H,\\{\\mathcal{S}_{h}^{\\mathrm{high}}\\}_{h\\in[H]},\\mathcal{A},\\mathcal{O},\\{\\widehat{\\mathbb{T}}_{h}^{\\mathrm{tunc}}\\}_{h\\in[H]},\\{\\widehat{\\mathbb{O}}_{h}^{\\mathrm{tunc}}\\}_{h\\in[H]},\\widehat{\\mu}_{1}^{\\mathrm{tunc}},\\{r_{h}\\}_{h\\in[H]})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Define $\\{\\widehat{b}_{h}^{\\prime,\\mathrm{sub}}:\\mathcal{Z}_{h}\\to\\Delta(S_{h}^{\\mathrm{high}})\\}_{h\\in[H]}$ to be the approximate belief w.r.t. $\\widehat{\\mathcal{P}}^{\\mathrm{sub}}$ Define $\\{b_{h}^{\\mathrm{apx}}:{\\mathcal{Z}}_{h}\\to\\Delta(S)\\}_{h\\in[H]}$ such that $b_{h}^{\\mathrm{apx}}(z_{h})(s_{h})=\\widehat{b}_{h}^{\\prime,\\mathrm{sub}}(z_{h})(s_{h})$ for $s_{h}\\in\\ensuremath{\\mathcal{S}}_{h}^{\\mathrm{high}}$ and 0 otherwise. ", "page_idx": 25}, {"type": "text", "text": "return $\\{b_{h}^{\\mathrm{apx}}\\}_{h\\in[H]}$ ", "page_idx": 25}, {"type": "text", "text": "Algorithm 5 Learning Multi-Agent (Individual) Decoding Functions with Privileged Information (NE/CCE Version) ", "page_idx": 26}, {"type": "text", "text": "Require: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 POSG $\\mathcal{G}=(H,\\mathcal{S},\\mathcal{A},\\mathcal{O},\\{\\mathbb{T}_{h}\\}_{h\\in[H]},\\{\\mathbb{O}_{h}\\}_{h\\in[H]},\\mu_{1},\\{r_{i}\\}_{i\\in[n]})$   \n\u2022 $\\pi\\in\\Pi_{S}$ , controller set $\\{{\\mathcal{T}}_{h}\\subseteq[n]\\}_{h\\in[H]}$ .   \n\u2022 Procedure MDP_Learning $(\\cdot,\\cdot)$ that takes as input an MDP and a reward function and returns an approximate optimal policy,   \n\u2022 Number of trajectories $N$ . ", "page_idx": 26}, {"type": "text", "text": "Ensure: A decoding function for each agent and step $\\{\\widehat{g}_{j,h}\\}_{j\\in[n],h\\in[H]}$ (see Theorem J.6) for $h\\in[H]$ , $s_{h}\\in{\\mathcal{S}}$ do ", "page_idx": 26}, {"type": "text", "text": "for $i\\in[n]$ do $\\widehat{r}_{i,h^{\\prime}}(\\bar{s}_{h}^{\\prime},a_{h}^{\\prime})\\gets\\mathbb{1}[h^{\\prime}=h,s_{h}^{\\prime}=s_{h}]$ for any $h^{\\prime},s_{h}^{\\prime},a_{h}^{\\prime}\\in[H]\\times{\\mathcal{S}}\\times{\\mathcal{A}}$ . Define $\\mathcal{M}$ to be the MDP associated with $\\mathcal{G}$ , $\\mathcal{M}(\\pi_{-i})$ to be the MDP marginalized by $\\pi_{-i}$ $\\Psi_{i}(h,s_{h})\\gets\\mathbf{MDP\\_Learning}(\\mathcal{M}(\\pi_{-i}),\\widehat{r}_{i})$ ", "page_idx": 26}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Collect $N$ trajectories by executing policy $\\Psi_{i}(h,s_{h})\\times\\pi_{-i}$ for the first $h-1$ steps then take action $a_{h}$ deterministically for each $a_{h}\\in A$ and denote the dataset $\\{(s_{h}^{k,i},o_{h}^{k,i},a_{h}^{k,i},s_{h+1}^{k,i})\\}_{k\\in[N A]}$ for each $i\\in[n]$ ", "page_idx": 26}, {"type": "equation", "text": "$(o_{h},\\bar{a_{h}},s_{h+1})\\in\\mathcal{O}\\times\\mathcal{A}\\times\\mathcal{S}$ $$\n\\begin{array}{r l}&{\\begin{array}{r l}&{:o_{h},u_{h},s_{h+1})\\in\\cup\\times\\times\\nearrow\\times\\Rightarrow\\mathbf{u}\\;\\mathbf{u}=}\\\\ &{N_{h}(s_{h})\\leftarrow\\sum_{k\\in[N],i\\in[n]}\\mathbb{I}\\bigl[s_{h}^{k,i}=s_{h}\\bigr]}\\\\ &{N_{h}(s_{h},a_{\\mathcal{T}_{h},h},s_{h+1})\\leftarrow\\sum_{k\\in[N],i\\in[n]}\\mathbb{I}\\bigl[s_{h}^{k,i}=s_{h},a_{T_{h},h}^{k,i}=a_{T_{h},h},s_{h+1}^{k,i}=s_{h+1}\\bigr]}\\end{array}}\\\\ &{\\begin{array}{r l}&{N_{h}(s_{h},o_{h})\\leftarrow\\sum_{k\\in[N],i\\in[n]}\\mathbb{I}\\bigl[s_{h}^{k,i}=s_{h},o_{h}^{k,i}=o_{h}\\bigr]}\\\\ &{\\widehat{\\mathbb{T}}_{h}(s_{h+1}\\,|\\,s_{h},a_{T_{h},h})\\leftarrow\\frac{N_{h}(s_{h},a_{\\mathcal{T}_{h},h},s_{h+1})}{N_{h}(s_{h},a_{T_{h},h})}}\\end{array}}\\\\ &{\\begin{array}{r l}&{\\widehat{\\mathbb{O}}_{h}(o_{h}\\,|\\,s_{h})\\leftarrow\\frac{N_{h}(s_{h},o_{h})}{N_{h}(s_{h})}}\\end{array}}\\\\ &{\\begin{array}{r l}&{z_{s}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Define $\\begin{array}{r l}&{\\overset{\\cdot}{\\mathcal{G}}:=(H,\\mathscr{S},\\mathscr{A},\\mathscr{O},\\{\\widehat{\\mathbb{T}}_{h}\\}_{h\\in[H]},\\{\\widehat{\\mathbb{O}}_{h}\\}_{h\\in[H]},\\mu_{1},\\{r_{i}\\}_{i\\in[n]})}\\\\ &{\\widehat{g}_{j,h}(s_{h}\\,|\\,c_{h},p_{j,h}):=\\mathbb{P}\\widehat{\\mathcal{G}}(s_{h}\\,|\\,c_{h},p_{j,h})\\mathrm{~for~each~}j\\in[n],\\,h\\in[H],\\,c_{h}\\in\\mathcal{C}_{h},\\,p_{j,h}\\in\\mathcal{P}_{j,h}}\\end{array}$   \nDefine   \nreturn {gj,h}j\u2208[n],h\u2208[H] ", "page_idx": 26}, {"type": "text", "text": "Algorithm 6 Learning Multi-Agent (Individual) Decoding Functions with Privileged Information (CE Version) ", "page_idx": 27}, {"type": "text", "text": "Require: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 POSG $\\mathcal{G}=(H,\\mathcal{S},\\mathcal{A},\\mathcal{O},\\{\\mathbb{T}_{h}\\}_{h\\in[H]},\\{\\mathbb{O}_{h}\\}_{h\\in[H]},\\mu_{1},\\{r_{i}\\}_{i\\in[n]})$   \n\u2022 $\\pi\\in\\Pi_{S}$ , controller set $\\{\\mathbb{Z}_{h}\\subseteq[n]\\}$ ,   \n\u2022 Procedure MDP_Learning $(\\cdot,\\cdot)$ that takes as input an MDP and a reward function and returns an approximate optimal policy,   \n\u2022 Number of trajectories $N$ . ", "page_idx": 27}, {"type": "text", "text": "for $h\\in[H]$ , $s_{h}\\in{\\mathcal{S}}$ do for $i\\in[n]$ do $\\widehat{r}_{i,h^{\\prime}}(\\bar{s}_{h}^{\\prime},a_{h}^{\\prime})\\gets\\mathbb{1}[h^{\\prime}=h,s_{h}^{\\prime}=s_{h}]$ for any $h^{\\prime},s_{h}^{\\prime},a_{h}^{\\prime}\\in[H]\\times{\\mathcal{S}}\\times{\\mathcal{A}}$ . Define $\\mathcal{M}^{\\mathrm{extended}}(\\pi)$ to be the extended MDP, which is defined in Definition J.5. $\\Psi_{i}(h,s_{h})\\leftarrow\\mathbf{M}\\mathrm{DP}_{-}$ Learning $(\\mathcal{M}^{\\mathrm{extended}}(\\pi),\\widehat{r_{i}})$ ", "page_idx": 27}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Collect $N$ trajectories by executing policy $\\Psi_{i}(h,s_{h})\\times\\pi_{-i}$ for the first $h-1$ steps then take action $a_{h}$ deterministically for each $a_{h}\\in A$ and denote the dataset $\\{(s_{h}^{k,i},o_{h}^{k,i},a_{h}^{k,i},s_{h+1}^{k,i})\\}_{k\\in[N A]}$ for each $i\\in[n]$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[o_{h},a_{h},s_{h+1})\\in\\cup\\times\\mathcal{A}\\times\\mathcal{S}\\mathrm{\\bf~a}=}\\\\ &{N_{h}(s_{h})\\leftarrow\\sum_{k\\in[N],i\\in[n]}\\mathbb{I}\\big[{s}_{h}^{k,i}=s_{h}\\big]}\\\\ &{N_{h}(s_{h},a_{\\mathcal{T}_{h},h},s_{h+1})\\leftarrow\\sum_{k\\in[N],i\\in[n]}\\mathbb{I}\\big[{s}_{h}^{k,i}=s_{h},a_{T_{h},h}^{k,i}=a_{T_{h},h},{s}_{h+1}^{k,i}=s_{h+1}\\big]}\\\\ &{N_{h}(s_{h},o_{h})\\leftarrow\\sum_{k\\in[N],i\\in[n]}\\mathbb{I}\\big[{s}_{h}^{k,i}=s_{h},{o}_{h}^{k,i}=o_{h}\\big]}\\\\ &{\\widehat{\\mathbb{T}}_{h}(s_{h+1}\\,|\\,s_{h},a_{T_{h},h})\\leftarrow\\frac{N_{h}(s_{h},a_{T_{h},h},s_{h+1})}{N_{h}(s_{h},a_{T_{h},h})}}\\\\ &{\\widehat{\\mathbb{O}}_{h}(o_{h}\\,|\\,s_{h})\\leftarrow\\frac{N_{h}(s_{h},o_{h})}{N_{h}(s_{h})}}\\\\ &{\\mathbf{\\widehat{\\mathbb{O}}}_{h}(o_{h}\\,|\\,s_{h})\\leftarrow\\frac{N_{h}(s_{h},o_{h})}{N_{h}(s_{h})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Define $\\begin{array}{r l}&{\\overset{\\cdot}{\\mathcal{G}}:=(H,S,A,\\mathcal{O},\\{\\widehat{\\mathbb{T}}_{h}\\}_{h\\in[H]},\\{\\widehat{\\mathbb{O}}_{h}\\}_{h\\in[H]},\\mu_{1},\\{r_{i}\\}_{i\\in[n]})}\\\\ &{\\widehat{g}_{j,h}(s_{h}\\,|\\,c_{h},p_{j,h}):=\\mathbb{P}\\widehat{\\mathcal{G}}(s_{h}\\,|\\,c_{h},p_{j,h})\\mathrm{~for~each~}j\\in[n],h\\in[H],c_{h}\\in\\mathcal{C}_{h},{p_{j,h}}\\in\\mathcal{P}_{j,h}}\\end{array}$ return {gj,h}j\u2208[n],h\u2208[H] ", "page_idx": 27}, {"type": "text", "text": "Require: POSG $\\mathcal{G}$ , $\\epsilon_{e}$ , $\\{\\widehat{P}_{h}:\\widehat{\\mathcal{C}}_{h}\\to\\Delta(S\\times\\mathcal{P}_{h})\\}_{h\\in[H]}$ for $k=1,2,\\cdots,K$ d o for $h\\leftarrow H,H-1,\\cdots\\,,1\\,\\epsilon$ do for $\\widehat{c}_{h}\\in\\widehat{\\mathcal{C}}_{h}$ do ", "page_idx": 28}, {"type": "text", "text": "$\\begin{array}{r l}&{Q_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h},p_{h},s_{h},a_{h})}\\\\ &{\\leftarrow\\operatorname*{min}\\left\\{r_{i,h}(s_{h},a_{h})+b_{h}^{k-1}(s_{h},a_{h})+\\mathbb{E}_{o_{h+1}\\sim\\widehat{J}_{h}^{k-1}(\\cdot\\mid s_{h},a_{h})}\\left[V_{i,h+1}^{\\mathrm{high},k}(\\widehat{c}_{h+1})\\right],H-h+1\\right\\}\\mathrm{~for~}i\\in[n]}\\end{array}$ $\\begin{array}{r l}&{Q_{i,h}^{\\mathrm{low},k}(\\widehat{c}_{h},p_{h},s_{h},a_{h})}\\\\ &{\\leftarrow\\operatorname*{max}\\left\\{r_{i,h}(s_{h},a_{h})-b_{h}^{k-1}(s_{h},a_{h})+\\mathbb{E}_{o_{h+1}\\sim\\widehat{\\mathbb{J}}_{h}^{k-1}(\\cdot\\,\\vert\\,\\,s_{h},a_{h})}\\left[V_{i,h+1}^{\\mathrm{low},k}(\\widehat{c}_{h+1})\\right],0\\right\\}\\mathrm{for}\\ i\\in[n]}\\end{array}$ Define   \n$\\begin{array}{r}{Q_{i,h}^{\\mathrm{high},k}\\big(\\widehat{c}_{h},\\gamma_{h}\\big):=\\mathbb{E}_{\\boldsymbol{s}_{h},\\boldsymbol{p}_{h}\\sim\\widehat{P}_{h}(\\cdot,\\cdot|\\,\\widehat{c}_{h})}\\mathbb{E}_{\\{\\boldsymbol{a}_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,\\boldsymbol{p}_{j,h})\\}_{j\\in[n]}}\\left[Q_{i,h}^{\\mathrm{high},k}\\big(\\widehat{c}_{h},\\boldsymbol{p}_{h},\\boldsymbol{s}_{h},\\boldsymbol{a}_{h}\\big)\\right]\\mathrm{for}\\;i\\in[n]}\\end{array}$ Define   \n$\\begin{array}{r}{Q_{i,h}^{\\mathrm{low},k}(\\widehat{c}_{h},\\gamma_{h}):=\\mathbb{E}_{s_{h},p_{h}\\sim\\widehat{P}_{h}(\\cdot,\\cdot|\\,\\widehat{c}_{h})}\\mathbb{E}_{\\{a_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,p_{j,h})\\}_{j\\in[n]}}\\left[Q_{i,h}^{\\mathrm{low},k}(\\widehat{c}_{h},p_{h},s_{h},a_{h})\\right]\\mathrm{for}\\;i\\in[n]}\\end{array}$ $\\begin{array}{r l}&{\\{\\pi_{j,h}^{k}(\\cdot\\,|\\,\\cdot,\\widehat{c}_{h},\\cdot)\\}_{j\\in[n]}\\gets\\mathrm{Bayesian.CE/CCE}(\\{Q_{j,h}^{\\mathrm{high},k}(\\widehat{c}_{h},\\cdot)\\}_{j\\in[n]})\\left(\\mathrm{c.f}\\right)}\\\\ &{V_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h})\\gets\\mathbb{E}_{\\omega_{h}}\\left[Q_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h},\\{\\pi_{j,h}^{k}(\\cdot\\,|\\,\\omega_{j,h},\\widehat{c}_{h},\\cdot)\\}_{j\\in[n]})\\right]\\mathrm{for}\\ i\\in[n]}\\\\ &{V_{i,h}^{\\mathrm{low},k}(\\widehat{c}_{h})\\gets\\mathbb{E}_{\\omega_{h}}\\left[Q_{i,h}^{\\mathrm{low},k}(\\widehat{c}_{h},\\{\\pi_{j,h}^{k}(\\cdot\\,|\\,\\omega_{j,h},\\widehat{c}_{h},\\cdot)\\}_{j\\in[n]})\\right]\\mathrm{for}\\ i\\in[n]}\\end{array}$ . Appendix J.1) end for   \nend for   \nExecute for $\\begin{array}{r l}&{\\mathrm{scute~}\\pi^{k}\\mathrm{~and~get~trajectory~}(s_{1:H}^{k},a_{1:H}^{k},a_{1:H+1}^{k})}\\\\ &{\\cdot h\\in[H],s_{h}\\in\\mathcal{S},a_{h}\\in\\mathcal{A},o_{h+1}\\in\\mathcal{O}\\,\\mathbf{do}}\\\\ &{N_{h}^{k}(s_{h},a_{h})\\gets\\sum_{l\\in[k]}\\mathbb{1}[s_{h}^{l}=s_{h},a_{h}^{l}=a_{h}]}\\\\ &{N_{h}^{k}(s_{h},a_{h},o_{h+1})\\gets\\sum_{l\\in[k]}\\mathbb{1}[s_{h}^{l}=s_{h},a_{h}^{l}=a_{h},o_{h+1}^{l}=o_{h+1}]}\\\\ &{\\widehat{\\mathbb{J}}_{h}^{k}(o_{h+1}\\,|\\,s_{h},a_{h})\\gets\\frac{N_{h}^{k}(s_{h},a_{h},o_{h+1})}{N_{h}^{k}(s_{h},a_{h})}}\\end{array}$   \nend for   \nend for ", "page_idx": 28}, {"type": "text", "text": "Algorithm 8 Approximate Belief Learning for MARL with Privileged Information ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Require: POSG $\\mathcal{G}\\;=\\;(H,S,A,\\mathcal{O},\\{\\mathbb{T}_{h}\\}_{h\\in[H]},\\{\\mathbb{O}_{h}\\}_{h\\in[H]},\\mu_{1},\\{r_{i,h}\\}_{i\\in[n],h\\in[H]})$ , controller set $\\{{\\mathcal{T}}_{h}\\subseteq[n]\\}_{h\\in[H]}$ , procedure MDP_Learning $(\\cdot,\\cdot)$ , number of trajectories $N$ , and accuracy $\\epsilon$ . ", "page_idx": 29}, {"type": "text", "text": "for $h\\in[H],s_{h}\\in S\\,{\\bf d}$ o $\\widehat{r}_{h^{\\prime}}(\\bar{s}_{h}^{\\prime},\\bar{a}_{h}^{\\prime})\\gets\\mathbb{1}[h^{\\prime}=h,s_{h}^{\\prime}=s_{h}]$ for any $(h^{\\prime},s_{h}^{\\prime},a_{h}^{\\prime})\\in[H]\\times{\\mathcal{S}}\\times{\\mathcal{A}}$ . $\\mathcal{M}\\leftarrow(H,S,\\mathcal{A},\\{\\mathbb{T}_{h}\\}_{h\\in[H]},\\mu_{1},$ $\\{\\widehat{r}_{h}\\}_{h\\in[H]})$ to be the MDP associated with $\\mathcal{P}$ $\\Psi(h,s_{h})\\gets\\mathbf{MDP}_{\\star}$ _Learning $(\\mathcal{M})$ ", "page_idx": 29}, {"type": "text", "text": "Collect $N$ trajectories by executing policy $\\Psi(h,s_{h})$ for the first $h-1$ steps then take action $a_{h}$ for each $a_{h}\\in A$ deterministically and denote the dataset $\\{(s_{h}^{i},o_{h}^{i},a_{h}^{i},s_{h+1}^{i})\\}_{i\\in[N A]}$ ", "page_idx": 29}, {"type": "text", "text": "for $(o_{h},a_{h},s_{h+1})\\in\\mathcal{O}\\times\\mathcal{A}\\times\\mathcal{S}$ do $\\begin{array}{r}{N_{h}(s_{h})\\gets\\sum_{i\\in[N A]}\\mathbb{1}[s_{h}^{i}=s_{h}]}\\end{array}$ $\\begin{array}{r l}&{\\quad_{N_{h}}^{\\mathrm{{\\tiny~\\it~\\it~\\it~\\it~\\it~\\it~\\it~\\it~\\frac{~1}{~2}}~t}}\\leftarrow\\sum_{i\\in\\{N A\\}}^{\\mathrm{{\\tiny~\\it~\\it~\\it~\\it~\\#~}~}^{n}}\\mathrm{\\tiny~\\it~\\#~}_{h}^{\\mathrm{{\\tiny~\\it~\\it~\\it~\\it~\\cdots~}~}}\\cdots}\\\\ &{N_{h}(s_{h},a_{T_{h},h})\\leftarrow\\sum_{i\\in[N A]}\\mathbb{1}\\big[s_{h}^{i}=s_{h},a_{T_{h},h}^{i}=a_{T_{h},h}\\big]}\\\\ &{N_{h}(s_{h},a_{T_{h},h},s_{h+1})\\leftarrow\\sum_{i\\in[N A]}\\mathbb{1}\\big[s_{h}^{i}=s_{h},a_{T_{h},h}^{i}=a_{T_{h},h},s_{h+1}^{i}=s_{h+1}\\big]}\\\\ &{N_{h}(s_{h},o_{h})\\leftarrow\\sum_{i\\in[N A]}\\mathbb{1}\\big[s_{h}^{i}=s_{h},o_{h}^{i}=o_{h}\\big]}\\\\ &{\\widehat{\\mathbb{T}}_{h}(s_{h+1}\\,|\\,s_{h},a_{T_{h},h})\\leftarrow\\frac{N_{h}(s_{h},a_{h},s_{h+1})}{N_{h}(s_{h},a_{T_{h},h})}}\\\\ &{\\widehat{\\mathbb{O}}_{h}(o_{h}\\,|\\,s_{h})\\leftarrow\\frac{N_{h}(s_{h},o_{h})}{N_{h}(s_{h})}}\\\\ &{\\widehat{\\mathbb{O}}_{h}\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h\\in[H]\\,{\\bf d o}}\\\\ &{S_{h}^{\\mathrm{low}}\\gets\\Big\\{s_{h}\\in S\\,|\\,\\frac{N_{h}(s_{h})}{N A}\\le\\epsilon\\Big\\}}\\\\ &{S_{h}^{\\mathrm{high}}\\gets\\Big\\{s_{h}\\in S\\,|\\,\\frac{N_{h}(s_{h})}{N A}>\\epsilon\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{(h,s_{h},o_{h},a_{h},s_{h+1})}\\in[H]\\times S_{h}^{\\mathrm{high}}\\times\\mathcal{O}\\times\\mathcal{A}\\times S_{h}^{\\mathrm{high}}\\,\\mathbf{d}\\mathbf{0}}\\\\ &{\\widehat{\\mathbb{T}}_{h}^{\\mathrm{tunc}}(s_{h+1}\\,|\\,s_{h},a_{h})\\gets\\widehat{\\mathbb{T}}_{h}(s_{h+1}\\,|\\,s_{h},a_{h})+\\frac{\\sum_{s_{h+1}^{\\prime}\\in S_{h+1}^{\\mathrm{tow}}}\\widehat{\\mathbb{T}}_{h}(s_{h+1}^{\\prime}\\,|\\,s_{h},a_{h})}{|S_{h+1}^{\\mathrm{high}}|}}\\\\ &{\\widehat{\\mathbb{O}}_{h}^{\\mathrm{tunc}}(o_{h}\\,|\\,s_{h})\\gets\\widehat{\\mathbb{O}}_{h}(o_{h}\\,|\\,s_{h})}\\\\ &{\\widehat{\\mu}_{1}^{\\mathrm{tunc}}(s_{1}):=\\widehat{\\mu}_{1}(s_{1})+\\frac{\\sum_{s_{1}^{\\prime}\\in S_{1}^{\\mathrm{low}}}\\widehat{\\mu}_{1}(s_{1}^{\\prime})}{|S_{1}^{\\mathrm{high}}|},\\forall s_{1}\\in S_{1}^{\\mathrm{high}}}\\\\ &{\\bullet}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Let ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathcal{G}}^{\\operatorname*{sub}}:=(H,\\{\\mathcal{S}_{h}^{\\mathrm{high}}\\}_{h\\in[H]},\\mathcal{A},\\mathcal{O},\\{\\widehat{\\mathbb{T}}_{h}^{\\mathrm{tunc}}\\}_{h\\in[H]},\\{\\widehat{\\mathbb{O}}_{h}^{\\mathrm{tunc}}\\}_{h\\in[H]},\\widehat{\\mu}_{1}^{\\mathrm{tunc}},\\{r_{i,h}\\}_{i\\in[n],h\\in[H]})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Define $\\{\\widetilde{P}_{h}:\\widehat{\\mathcal{C}}_{h}\\to\\Delta(\\mathcal{P}_{h}\\times\\mathcal{S}_{h}^{\\mathrm{high}})\\}_{h\\in[H]}$ to be the approximate belief w.r.t. $\\widehat{\\mathcal{G}}^{\\mathrm{sub}}$   \nDefine $\\{\\widehat{P}_{h}:\\widehat{\\mathcal{C}}_{h}\\to\\Delta(S\\times\\mathcal{P}_{h})\\}_{h\\in[H]}$ such that $\\widehat{P}_{h}(s_{h},p_{h}\\,|\\,\\widehat{c}_{h})=\\widetilde{P}_{h}(s_{h},p_{h}\\,|\\,s_{h})$ for $s_{h}\\in\\mathcal{S}_{h}^{\\mathrm{high}}$ and 0 oth erwi se.   \nreturn $\\{\\widehat{P}_{h}\\}_{h\\in[H]}$ ", "page_idx": 29}, {"type": "text", "text": "E Missing Details in Section 3 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof of Proposition 3.1: We recall that $b_{h}(\\cdot)$ is the belief of the agent about the underlying state, see Appendix C for a detailed introduction. Note that Equation (3.1) can be written as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\widehat{\\pi}^{\\star}\\in\\arg\\operatorname*{min}_{\\pi\\in\\Pi}\\ \\ \\sum_{h=1}^{H}\\mathbb{E}_{\\tau_{h}\\sim\\pi^{\\prime}}^{\\mathcal{P}}\\mathbb{E}_{s_{h}\\sim b_{h}(\\tau_{h})}\\left[D_{f}\\big(\\pi_{h}^{\\star}\\big(\\cdot\\,\\vert\\,s_{h}\\big)\\,\\vert\\,\\pi_{h}\\big(\\cdot\\,\\vert\\,\\tau_{h}\\big)\\big)\\right].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, for any $h\\in[H]$ and $\\tau_{h}$ such that $\\mathbb{P}^{\\pi^{\\prime},\\mathcal{P}}(\\tau_{h})>0$ , we can optimize $\\pi$ separately for each $h\\in[H]$ and $\\tau_{h}$ as: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{h}^{\\star}\\!\\left(\\cdot\\:\\middle|\\:\\tau_{h}\\right)\\in\\underset{q\\in\\Delta({\\cal A})}{\\mathrm{argmin}}\\ \\:\\mathbb{E}_{s_{h}\\sim b_{h}\\left(\\tau_{h}\\right)}\\left[D_{f}\\!\\left(\\pi_{h}^{\\star}\\!\\left(\\cdot\\:\\middle|\\:s_{h}\\right)\\thinspace\\middle|\\:q\\right)\\right].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now we are ready to construct the counter-example of $\\gamma$ -observable POMDP $\\mathcal{P}^{\\epsilon}$ for some $\\epsilon\\in(0,1)$ with $H\\,=\\,1$ , $S\\ =\\ \\left\\{s^{1},s^{2}\\right\\}$ , ${\\cal A}\\;=\\;\\{a^{1},a^{2}\\}$ , and $\\mathcal{O}\\;=\\;\\left\\{o^{1},o^{2}\\right\\}$ . We let $\\begin{array}{r}{\\mu_{1}\\;=\\;(\\frac{1-\\gamma}{2-\\gamma},\\frac{1}{2-\\gamma})}\\end{array}$ , $\\mathbb{O}_{1}(o^{1}\\,|\\,s^{1})\\,=\\,1$ , and $\\mathbb{O}_{1}(o^{1}\\,|\\,s^{2})\\,=\\,1\\,-\\,\\gamma$ , $\\mathbb{O}_{1}(o^{2}\\,|\\,s^{2})\\,=\\,\\gamma$ . Therefore, it is direct to see that $\\mathbb{O}_{1}$ is exactly $\\gamma$ -observable. Most importantly, we choose $r_{1}(s^{1},a^{1})\\,=\\,1$ , $r_{1}(s^{1},a^{2})\\,=\\,0$ , and $r_{1}\\bigl(s^{2},a^{1}\\bigr)=0$ , $r_{1}(s^{2},a^{2})=\\epsilon$ . ", "page_idx": 30}, {"type": "text", "text": "Therefore, given such a reward function, the fully observable expert policy is given by $\\pi_{1}^{\\star}(a^{1}\\,|\\,s^{1})=1$ and $\\pi_{1}^{\\star}(a^{2}\\,\\bar{|}\\,s^{2})=1$ , i.e., choosing $a^{1}$ at state $s^{\\mathrm{i}}$ and $a^{2}$ at state $s^{2}$ deterministically. Meanwhile, by our construction, one can compute that the belief given observation $o^{1}$ ensures $b_{1}^{\\cdot}(o^{1})=\\operatorname{Unif}(S)$ Hence, the corresponding \u201cdistilled\u201d partially observable policy under observation $o^{1}$ is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\pi}_{1}^{\\star}(\\cdot\\,|\\,o^{1})=\\underset{q\\in\\Delta(A)}{\\operatorname{argmin}}\\,\\mathbb{E}_{s_{1}\\sim b_{1}(o^{1})}\\,[D_{f}(\\pi_{1}^{\\star}(\\cdot\\,|\\,s_{1})\\,|\\,q)]}\\\\ &{\\qquad\\qquad=\\underset{q\\in\\Delta(A)}{\\operatorname{argmin}}\\,\\frac{D_{f}\\left(\\pi_{1}^{\\star}(\\cdot\\,|\\,s^{1})\\,|\\,q\\right)+D_{f}\\left(\\pi_{1}^{\\star}(\\cdot\\,|\\,s^{2})\\,|\\,q\\right)}{2}}\\\\ &{\\qquad\\qquad=\\underset{q\\in\\Delta(A)}{\\operatorname{argmin}}\\,\\frac{f(1/q(a^{1}))q(a^{1})+f(0)q(a^{2})+f(0)q(a^{1})+f(1/q(a^{2}))q(a^{2})}{2}}\\\\ &{\\qquad\\qquad=\\underset{q\\in\\Delta(A)}{\\operatorname{argmin}}\\,\\frac{f(0)+f(1/q(a^{1}))q(a^{1})+f(1/q(a^{2}))q(a^{2})}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last step is due to $q\\in\\Delta(A)$ . Now consider the function $g(x)=x f(1/x)$ for $x>0$ . It is direct to compute that $\\begin{array}{r}{g^{\\prime}(x)=f(1/x)-\\frac{f^{\\prime}(1/x)}{x}}\\end{array}$ , and $\\begin{array}{r}{g^{\\prime\\prime}(x)=\\frac{f^{\\prime\\prime}(1/x)}{x^{3}}\\geq0}\\end{array}$ due to the convexity of the function $f$ . Thus, we conclude that $g$ is also convex. By Jensen\u2019s inequality, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{f(1/q(a^{1}))q(a^{1})+f(1/q(a^{2}))q(a^{2})}{2}\\geq f(2/(q(a^{1})+q(a^{2})))(q(a^{1})+q(a^{2}))/2=f(2)/2,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the equality holds when $\\begin{array}{r}{q(a^{1})=q(a^{2})=\\frac{1}{2}}\\end{array}$ . This indicates that $\\widehat{\\pi}_{1}^{\\star}(\\cdot\\,|\\,o_{1})=\\operatorname{Unif}(A)$ . On the other hand, combining the fact that $b_{1}(o^{1})=\\operatorname{Unif}(S)$ with $\\epsilon<1$ , it is direct to see that the optimal partially observable policy $\\widetilde{\\pi}\\in\\arg\\operatorname*{max}_{\\pi\\in\\Pi}v^{\\mathcal P}(\\pi)$ satisfies $\\widetilde{\\pi}_{1}(a^{1}\\,|\\,o^{1})=1$ . Now we are ready to evaluate the optimality gap between $\\widetilde{\\pi}$ and $\\widehat{\\pi}^{\\star}$ as follows ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{\\mathcal{P}^{\\epsilon}}(\\widetilde{\\pi})-v^{\\mathcal{P}^{\\epsilon}}(\\widehat{\\pi}^{\\star})=\\mathbb{P}^{\\mathcal{P}^{\\epsilon}}(o^{1})(V_{1}^{\\widetilde{\\pi},\\mathcal{P}^{\\epsilon}}(o^{1})-V_{1}^{\\widehat{\\pi}^{\\star},\\mathcal{P}^{\\epsilon}}(o^{1}))+\\mathbb{P}^{\\mathcal{P}^{\\epsilon}}(o^{2})(V_{1}^{\\widetilde{\\pi},\\mathcal{P}^{\\epsilon}}(o^{2})-V_{1}^{\\widehat{\\pi}^{\\star},\\mathcal{P}^{\\epsilon}}(o^{2}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\mathbb{P}^{\\mathcal{P}^{\\epsilon}}(o^{1})(V_{1}^{\\widetilde{\\pi},\\mathcal{P}^{\\epsilon}}(o^{1})-V_{1}^{\\widehat{\\pi}^{\\star},\\mathcal{P}^{\\epsilon}}(o^{1})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last step is due to the fact that $\\widetilde{\\pi}$ is the optimal policy, leading to the fact that $V_{1}^{\\widetilde{\\pi},\\mathcal{P}^{\\epsilon}}(o^{2})-$ $V_{1}^{\\widehat{\\pi}^{\\star},\\mathcal{P}^{\\epsilon}}(o^{2})\\geq0$ . Now it is not hard to compute that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\mathcal{P}^{\\epsilon}}(o^{1})\\geq1-\\gamma.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Meanwhile, we can evaluate that ", "page_idx": 30}, {"type": "equation", "text": "$$\nV_{1}^{\\tilde{\\pi},\\mathcal{P}^{\\epsilon}}(o^{1})=\\frac{1}{2},\\quad V_{1}^{\\widehat{\\pi}^{\\star},\\mathcal{P}^{\\epsilon}}(o^{1})=\\frac{1+\\epsilon}{4}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "aTnhdi sc coorrnecslpuodnedsi nogulr y $\\begin{array}{r}{V_{1}^{\\widetilde{\\pi},\\mathcal{P}^{\\epsilon}}(o^{1})\\!-\\!V_{1}^{\\widehat{\\pi}^{\\star},\\mathcal{P}^{\\epsilon}}(o^{1})=\\frac{1-\\epsilon}{4}}\\end{array}$ , implying that $\\begin{array}{r}{v^{\\mathcal{P}^{\\epsilon}}(\\widetilde{\\pi})\\!-\\!v^{\\mathcal{P}^{\\epsilon}}(\\widehat{\\pi}^{\\star})\\geq\\frac{(1-\\gamma)(1-\\epsilon)}{4}}\\end{array}$ . ", "page_idx": 30}, {"type": "text", "text": "Example E.1 (Deterministic POMDP [35, 80]). We say a POMDP $\\mathcal{P}$ is of deterministic transition if entries of matrices $\\{\\mathbb{T}_{h}\\}_{h\\in[H]}$ and the vector $\\mu_{1}$ are either 0 or 1. Note that we do not make any assumptions on the emission matrices. ", "page_idx": 31}, {"type": "text", "text": "Example E.2 (Block MDP [42, 18]). We say a POMDP $\\mathcal{P}$ is a block MDP if for any $h\\,\\in\\,[H]$ , $s_{h},s_{h}^{\\prime}\\in\\mathcal{S}$ , it holds that $\\mathrm{supp}(\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h}))\\cap\\mathrm{supp}(\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h}^{\\prime}))=\\emptyset$ when $s_{h}\\neq s_{h}^{\\prime}$ . ", "page_idx": 31}, {"type": "text", "text": "Example E.3 ( $k$ -step decodable POMDP [19]). We say a POMDP $\\mathcal{P}$ is a $k$ -step decodable POMDP if there exists an unknown decoder $\\phi^{\\star}=\\{\\phi_{h}^{\\star}:\\mathcal{Z}_{h}\\,\\rangle_{h\\in[H]}$ such that for any $h\\,\\in\\,[H]$ and reachable trajectory $\\tau_{h}$ , $\\mathbb{P}^{\\mathcal{P}}(s_{h}\\,=\\,\\phi_{h}^{\\star}(z_{h})\\,|\\,\\tau_{h})\\,=\\,1$ , where $\\bar{\\mathcal{Z}}_{h}\\,^{\\prime}=\\,(\\mathcal{O}\\times\\mathcal{A})^{\\operatorname*{max}\\{h-1,k-1\\}}\\,\\times\\,\\mathcal{O}$ , $z_{h}=((o,a)_{k(h):h-1}^{\\ \\bar{}},o_{h})$ , and $k(h)=\\operatorname*{max}\\{h-k+1,1\\}$ . ", "page_idx": 31}, {"type": "text", "text": "Finally, to understand how our condition can extend beyond known examples in the literature, we show that one can indeed allow the decoding length of Example E.3 to be unknown and arbitrary (instead of being a small known constant as in [19] to have provably efficient algorithms). ", "page_idx": 31}, {"type": "text", "text": "Example E.4 (POMDP with arbitrary, unknown decodable length). This example is similar to Example E.3, but the decoding length $m$ is unknown and not necessarily a small constant. ", "page_idx": 31}, {"type": "text", "text": "In light of the pitfall in Proposition 3.1, we will analyze both the computational and statistical efficiencies of expert distillation in Section 4, under the condition in Definition 3.2. ", "page_idx": 31}, {"type": "text", "text": "Proof of Example E.1 & Example E.2 & Example E.3 & Example E.4: To see why those examples follow our Definition 3.2, it is indeed an immediate result of Proposition 7.1. \u25a0 ", "page_idx": 31}, {"type": "text", "text": "Proof of Proposition 3.3: Here we evaluate the computational complexity and sample complexity of each iteration $t$ as follows. ", "page_idx": 31}, {"type": "text", "text": "Sample complexity: The algorithm executes the policy $\\pi^{t-1}$ and collect $K$ episodes, denoted as $\\{s_{1:H+1}^{k},o_{1:H}^{k},a_{1:H}^{k}\\}_{k\\in[K]}$ . Thus, the sample complexity of each iteration is $\\Theta(K)$ . ", "page_idx": 31}, {"type": "text", "text": "Computational complexity for policy evaluation: The policy evaluation of the vanilla asymmetric actor-critic is done by minimizing the Bellman error. In the finite-horizon setting with tabular parameterization, it is equivalent to performing the following update for each $h\\in[H]$ in a backward way and each $k\\in[K]$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\textstyle\\sum_{h}^{t}(\\tau_{h}^{k},s_{h}^{k},a_{h}^{k})\\leftarrow(1-\\alpha)Q_{h}^{t-1}(\\tau_{h}^{k},s_{h}^{k},a_{h}^{k})}\\\\ &{\\textstyle\\qquad\\qquad\\qquad\\qquad\\qquad+\\alpha\\left(r_{h}(s_{h}^{k},a_{h}^{k})+\\frac{1}{|\\mathcal{I}(\\tau_{h}^{k},s_{h}^{k},a_{h}^{k})|}\\sum_{j\\in\\mathcal{I}(\\tau_{h}^{k},s_{h}^{k},a_{h}^{k})}Q_{h+1}^{t}(\\tau_{h+1}^{j},s_{h+1}^{j},a_{h+1}^{j})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for some stepsize $\\alpha\\,\\in\\,(0,1)$ , where $\\mathcal{I}(\\tau_{h}^{k},s_{h}^{k},a_{h}^{k})\\;:=\\;\\{j\\;\\in\\;[K]\\,|\\,(\\tau_{h}^{j},s_{h}^{j},a_{h}^{j})\\,=\\;(\\tau_{h}^{k},s_{h}^{k},a_{h}^{k})\\}.$ .   \nTherefore, the computational complexity for this procedure is of $\\mathrm{POLY}\\big(H,K\\big)$ . ", "page_idx": 31}, {"type": "text", "text": "Computational complexity for policy improvement: For tabular parameterization, computing $\\nabla\\log\\pi_{h}^{t-1}(a_{h}^{k}\\,|\\,\\tau_{h}^{k})$ takes $O(1)$ computation. Hence the policy update in Equation (3.2) performs $\\mathrm{POLY}\\big(\\dot{H},K\\big)$ computation. ", "page_idx": 31}, {"type": "text", "text": "Meanwhile, under the exponential time hypothesis, there is no polynomial time algorithm for even planning an $\\epsilon$ -approximate optimal policy in $\\gamma$ -observable POMDPs [26]. This implies that the vanilla asymmetric actor-critic needs to take super-polynomial time to find an approximately optimal policy. This implies the corresponding sample complexity has to be super-polynomial. ", "page_idx": 31}, {"type": "text", "text": "Finally, we remark that even if we let the policy and the $Q$ -function not depend on the entire history $\\tau_{h}$ but only the finite-memory $z_{h}$ , the proof still holds. The key is that whenever one only computes at the sampled history/finite-memories, i.e., updates the policy in an asynchronous way (in contrast to the synchronous one where the policies at all histories/finite-memories are updated), the sample and computational complexities will be coupled with the same order per iteration, which implies a super-polynomial sample complexity due to the super-polynomial computational complexity. \u25a0 ", "page_idx": 31}, {"type": "text", "text": "Derivation for the closed-form update Equation (3.3). Note that the proximal policy optimization [71] update has the policy improvement as follows ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\pi^{t}\\leftarrow\\arg\\operatorname*{max}_{\\pi}\\left\\{L^{t-1}(\\pi)-\\eta^{-1}\\mathbb{E}_{\\pi^{t-1}}^{\\mathcal{P}}\\left[\\sum_{h\\in[H]}{\\mathrm{KL}}(\\pi_{h}(\\cdot\\mid\\tau_{h})\\mid\\pi_{h}^{t-1}(\\cdot\\mid\\tau_{h}))\\right]\\right\\},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\eta$ is some learning rate and $L^{t-1}(\\pi)$ is a first-order approximation of the expected accumulated rewards at \u03c0t\u22121: ", "page_idx": 32}, {"type": "equation", "text": "$$\nL^{t-1}(\\pi):=v^{\\mathcal{P}}(\\pi^{t-1})+\\mathbb{E}_{\\pi^{t-1}}^{\\mathcal{P}}\\left[\\sum_{h\\in[H]}\\left\\langle Q_{h}^{t-1}(\\tau_{h},s_{h},\\cdot),\\pi_{h}(\\cdot\\,|\\,\\tau_{h})-\\pi_{h}^{t-1}(\\cdot\\,|\\,\\tau_{h})\\right\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By plugging $L^{t-1}(\\pi)$ into Equation (E.1), with simple algebric manipulations, we prove that: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\pi_{h}^{t}(\\cdot\\,|\\,\\tau_{h})\\propto\\pi_{h}^{t-1}(\\cdot\\,|\\,\\tau_{h})\\exp\\left(\\eta\\mathbb{E}_{s_{h}\\sim b_{h}(\\tau_{h})}\\left[Q_{h}^{t-1}(\\tau_{h},s_{h},\\cdot)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "F Missing Details in Section 4 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof of Lemma 4.4: The proof follows by the assumption that the total cumulative reward is at most $H$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{p}(\\pi)\\geq\\mathbb{E}_{n}^{p}\\Bigg[\\Bigg(\\displaystyle\\sum_{i\\in[n]}r_{i}\\wedge\\in[H]:\\phi_{i}(s_{i-1},a_{i-1},a_{i})=s_{1}\\Bigg)\\Bigg]}\\\\ &{\\qquad=\\mathbb{E}_{n}^{p}\\Bigg[\\Bigg(\\displaystyle\\sum_{i\\in[n]}r_{i}\\wedge\\in[H]:\\phi_{i}(s_{i-1},a_{i},a_{i})=s_{1}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad+\\mathbb{E}_{n}^{p}\\Bigg[\\Bigg(\\displaystyle\\sum_{i\\in[n]}r_{i}\\wedge\\in[H]:\\phi_{i}(s_{i-1},a_{i-1},a_{i})\\neq s_{1}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad-\\mathbb{E}_{n}^{p}\\Bigg[\\Bigg(\\displaystyle\\sum_{i\\in[n]}r_{i}\\wedge\\in[H]:\\phi_{i}(s_{i-1},a_{i-1},a_{i})\\neq s_{1}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\mathbb{E}_{n}^{p}\\Bigg]}\\\\ &{\\qquad=\\mathbb{E}_{n}^{p}\\Bigg[\\Bigg(\\displaystyle\\sum_{i\\in[n]}r_{i}\\wedge\\Bigg[\\sum_{i\\in[n]}1\\uparrow\\hbar\\times\\in[H]:\\phi_{i}(s_{i-1},a_{i-1},a_{i})\\neq s_{1}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad-\\mathbb{E}_{n}^{p}\\Bigg[\\Bigg(\\displaystyle\\sum_{i\\in[n]}r_{i}\\wedge\\in[H]:\\phi_{i}(s_{i-1},a_{i},a_{i-1},a_{i})\\neq s_{1}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\pi^{p}(\\pi^{p})-H\\mathbb{E}^{p-1}\\Bigg[\\left(\\displaystyle\\sum_{i\\in[n]}r_{i}\\wedge\\in[H]:\\phi_{i}(s_{i-1},a_{i},a_{i-1},a_{i})\\neq s_{1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\times\\pi^{p}(\\pi^{p})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\mathbb{E}_{n}^{p}(\\pi^{p})-H\\mathbb{E}_{n}^{p}\\Bigg[\\big(\\mu);\\phi_{i}(s_{i-1},a_{\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which completes the proof. ", "page_idx": 32}, {"type": "text", "text": "Proof of Theorem 4.5: For each step $\\textit{h}\\in\\ [H]$ we define $D_{h}$ to be the distribution over the underlying state $s h\\!-\\!1$ at step $h-1$ , taken action $a_{h-1}\\in{\\mathcal{A}}$ based on $\\pi^{E}$ , the underlying state transitioned to $s_{h}\\,\\in\\,{\\cal S}$ , and the observation $o_{h}\\sim\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})$ . We remind that we include at step zero, a dummy state-observation pair $\\left(s_{0},o_{0}\\right)$ . Formally, $D_{h}$ is defined as $D_{h}(s_{h-1},a_{h-1},o_{h},s_{h}):=$ P\u03c0 ,P [sh\u22121, ah\u22121, oh, sh] . ", "page_idx": 32}, {"type": "text", "text": "We first use union bound to decompose the probability that we incorrectly decode, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{\\pi^{E},\\mathcal{P}}\\,\\left[\\exists h\\in\\left[H\\right]:g_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)\\neq s_{h}\\right]\\leq\\displaystyle\\sum_{h\\in\\left[H\\right]}\\mathbb{P}^{\\pi^{E},\\mathcal{P}}\\,\\left[g_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)\\neq s_{h}\\right]}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{h\\in\\left[H\\right]}\\mathbb{P}_{(s_{h-1},a_{h-1},o_{h},s_{h})\\sim D_{h}}\\,\\left[g_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)\\neq s_{h}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For each $h\\in[H]$ , we can use $M$ episodes to collect $M$ samples from the distribution $D_{h}$ . In addition, since state $s H{+}1$ is dummy, we need not to collect episodes for $D_{H+1}$ . Denote the set of collected samples by $\\widehat{D}_{h}^{M}$ . We define the decoding $g_{h}$ for step $h\\in[H]$ as follows: ", "page_idx": 32}, {"type": "equation", "text": "$$\ng_{h}\\big(s_{h-1},a_{h-1},o_{h}\\big)=\\left\\{s_{h}\\mid\\big(s_{h-1},a_{h-1},o_{h},s_{h}\\big)\\in\\widehat{D}_{h}^{M}\\right\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Observe that by Definition 3.2, $\\{s_{h}\\ |\\ (s_{h-1},a_{h-1},o_{h},s_{h})\\in\\widehat{D}_{h}^{M}\\}$ is either the empty set or contains only a single elements, in which case, it is true that $g_{h}\\bigl(s_{h-1},\\stackrel{\\cdot\\cdot}{a_{h-1}},o_{h}\\bigr)=\\psi_{h}\\bigl(\\dot{s_{h-1}},a_{h-1},o_{h}\\bigr)\\,\\,\\mathrm{()}$ $(\\psi$ is the real decoding function, see Definition 3.2). Moreover, we slightly abuse the notation and let $\\widetilde{D}_{h}^{M}$ denote the empirical distribution induced by the samples in $\\widehat{D}_{h}^{M}$ . Thus, with probability at least $\\begin{array}{r}{1-\\frac{\\delta}{H}}\\end{array}$ and setting $\\begin{array}{r}{M=\\Theta\\left(\\frac{|\\mathcal{A}|\\cdot|\\mathcal{O}|\\cdot|\\mathcal{S}|+\\log(H/\\delta)}{\\epsilon^{2}}\\right)}\\end{array}$ for each step $h\\in[H]$ , we obtain the following by the result in [13]: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}^{\\pi^{E},\\mathcal{P}}\\,\\left[g_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)\\neq s_{h}\\right]=\\mathbb{P}^{\\pi^{E},\\mathcal{P}}\\,\\left[g_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)=\\emptyset\\right]}&{}\\\\ {=\\!\\mathbb{P}_{(s_{h-1},a_{h-1},o_{h},s_{h})\\sim D_{h}}\\,\\left[\\left(s_{h-1},a_{h-1},o_{h},s_{h}\\right)\\notin\\widehat{D}_{h}^{M}\\right]}&{}\\\\ {=\\!\\underset{u\\in\\mathrm{supp}(D_{h})}{\\sum}\\mathbb{P}_{u^{\\prime}\\sim D_{h}}\\!\\left[u=u^{\\prime}\\right]\\!\\mathbb{I}[u\\notin\\operatorname*{supp}(\\widetilde{D}_{h}^{M})]}&{}\\\\ {\\leq\\!d_{T V}\\!\\left(\\!D_{h},\\widetilde{D}_{h}^{M}\\right)}&{}\\\\ {\\leq\\!\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, by union bound, with probability at least $1-\\delta$ , we have that for each step $h\\in[H]$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\left(s_{h-1},a_{h-1},o_{h},s_{h}\\right)\\sim D_{h}}\\,\\left[g_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)\\neq s_{h}\\right]\\le\\epsilon,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which in combination with Equation (F.1) concludes the proof. Finally, we note that we used a total of \u0398 H \u00b7 |A|\u00b7|O|\u00b7|S|2+log(H/\u03b4) episodes from the POMDP, and the computational time was $\\begin{array}{r l}{\\mathrm{POLY}\\!\\left(H,A,O,S,\\frac{1}{\\epsilon},\\log\\left(\\frac{1}{\\delta}\\right)\\right)}\\end{array}$ . \u25a0 ", "page_idx": 33}, {"type": "text", "text": "G Provably Efficient Expert Policy Distillation with Function Approximation ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We now turn our attention to the rich-observation setting under our deterministic filter condition. Definition 3.2 motivates us to consider only succinct policies that incorporate an auxiliary parameter representing the most recent state, as well as the most recent observations and actions. To handle the large observation space, we further assume that for each step $h\\in[H]$ , the agent selects a decoding function $g_{h}$ from a family of multi-class classifiers ${\\mathcal{F}}_{h}\\subset\\{S\\times{\\bar{A}}\\times{\\mathcal{O}}\\rightarrow S\\}$ . For the function class ${\\mathcal{F}}_{h}$ , we make the standard realizability assumption. We formally summarize our assumptions in Assumption G.1. ", "page_idx": 33}, {"type": "text", "text": "Assumption G.1. We consider a POMDP that satisfies Definition 3.2. In addition, to derive learning algorithms that do not dependent on $|\\mathcal{O}|$ , for each step $h\\in[H]$ , we assume that we have access to a class of functions $\\mathcal{F}_{h}:S\\times A\\times O\\to S$ such that the perfect decoding function $\\psi_{h}\\in\\mathcal{F}_{h}$ . ", "page_idx": 33}, {"type": "text", "text": "We aim for our final bounds to depend on a complexity measure of the function class $\\mathcal{F}=\\{\\mathcal{F}_{h}\\}_{h\\in[H]}$ rather than the cardinality of the observation space $\\mathbb{O}$ . We utilize the Daniely and Shalev-ShwartzDimension (DS Dimension) (Theorem G.2), which characterizes the PAC learnability for multi-class classification [8]. Defining the DS dimension is beyond the scope of our paper; we direct interested readers to [8] for further details. For intuition, readers can think of the DS Dimension as a certificate of PAC learnability without loss of intuition. ", "page_idx": 33}, {"type": "text", "text": "Theorem G.2 (Theorem 1 in [8]). Consider a family of multi-class classifiers $\\mathcal{F}$ that map features in space $x\\in\\mathscr{X}$ to labels in space $y\\in\\mathcal{V}$ . Moreover, assume there is a joint probability distribution $D$ over features in $\\mathcal{X}$ and labels in $\\boldsymbol{\\wp}$ , and that there exists $g^{\\ast}\\in\\mathcal{F}$ such that for each $(\\dot{x_{}},y)\\in\\mathrm{supp}(D)$ , $g^{*}(x)=y$ . Given $n$ samples from $D$ , there exists an algorithm that with probability at least $1-\\delta$ outputs $\\widetilde g\\in\\mathcal F$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(x,y)\\sim D}[\\widetilde{g}(x)\\neq y]\\leq\\widetilde{\\mathcal{O}}\\left(\\frac{d_{D S}^{3/2}(\\mathcal{F})+\\log\\left(\\frac{1}{\\delta}\\right)}{n}\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $d_{D S}(\\mathcal{F})$ denotes the Daniely and Shalev-Shwartz-Dimension of the function class $\\mathcal{F}$ . ", "page_idx": 33}, {"type": "text", "text": "We are now ready to present the main theorem of this section. ", "page_idx": 33}, {"type": "text", "text": "Theorem G.3. Consider a POMDP $\\mathcal{P}$ that satisfies Definition 3.2, a policy $\\pi^{E}\\,\\in\\,\\Pi_{{\\cal{S}}}$ , and let $\\{\\mathcal{F}_{h}\\subseteq\\{S\\times A\\times\\mathcal{O}\\to S\\}\\}_{h\\in[H]}$ be the decoding function class, and $\\psi_{h}\\in\\mathcal{F}_{h}$ for each $h\\in[H]$ , ", "page_idx": 33}, {"type": "text", "text": "i.e., $\\{{\\mathcal{F}}_{h}\\}_{h\\in[H]}$ is realizable. Then given access to the classification oracle of [9], there exists an algorithm learning the decoding function $\\{g_{h}\\}_{h\\in[H]}$ such that with probability at least $1-\\delta$ , for each step $h\\in[H]$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}^{\\pi^{E},\\mathcal{P}}\\ [\\exists h\\in[H]:g_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)\\neq s_{h}]\\leq\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "using $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{H^{2}\\left(\\operatorname*{max}_{h\\in[H]}d_{D S}^{3/2}(\\mathcal{F}_{h})+\\log\\left(\\frac{1}{\\delta}\\right)\\right)}{\\epsilon}\\right)}\\end{array}$ episodes, where $d_{D S}(\\mathcal{F}_{h})$ is the Daniely and ShalevShwartz-Dimension of ${\\mathcal{F}}_{h}$ [8]. ", "page_idx": 34}, {"type": "text", "text": "Combining Theorem G.3 and Lemma 4.4, we obtain the final polynomial sample complexity in this function approximation setting, using classification (supervised learning) oracles (c.f. Table 1). ", "page_idx": 34}, {"type": "text", "text": "Proof of Theorem G.3: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "For each step $h\\in[H]$ , we define $D_{h}$ to be the distribution over the underlying state $s h\\!-\\!1$ at step $h\\!-\\!1$ , taken action $a_{h-1}\\in A$ from $\\pi^{E}$ , the underlying state transitioned to $s_{h}\\in{\\mathcal{S}}$ , and the hallucinated observation $o_{h}\\sim\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})$ (we remind readers that for step 0, we use dummy state $s_{0}$ and action $a_{0}\\ '$ ). Formally, the probability that the sequence $\\left(s_{h-1},a_{h-1},o_{h},s_{h}\\right)$ is sampled from $D_{h}$ equals to ", "page_idx": 34}, {"type": "equation", "text": "$$\nD_{h}(s_{h-1},a_{h-1},o_{h},s_{h}):=\\mathbb{P}^{\\pi^{E},\\mathcal{P}}\\left[s_{h-1},a_{h-1},o_{h},s_{h}\\right].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We first use union bound to decompose the misclassification error, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{\\pi^{E},\\mathcal{P}}\\,\\left[\\exists h\\in[H]:\\widetilde{g}_{h}\\,(s_{h-1},a_{h-1},o_{h})\\neq s_{h}\\right]\\le\\displaystyle\\sum_{h\\in[H]}\\mathbb{P}^{\\pi^{E},\\mathcal{P}}\\,\\left[\\widetilde{g}_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)\\neq s_{h}\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{h\\in[H]}\\mathbb{P}_{(s_{h-1},a_{h-1},o_{h},s_{h})\\sim D_{h}}\\,\\left[\\widetilde{g}_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)\\neq s_{h}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For each $\\textit{h}\\in\\ [H]$ , we can use $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{H}{\\epsilon}\\cdot\\left(\\operatorname*{max}_{h\\in[H]}d_{D S}^{3/2}(\\mathcal{F}_{h})+\\log\\left(\\frac{1}{\\delta}\\right)\\right)\\right)}\\end{array}$ episodes to collect $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{H}{\\epsilon}\\cdot\\left(\\operatorname*{max}_{h\\in[H]}d_{D S}^{3/2}(\\mathcal{F}_{h})+\\log\\left(\\frac{\\dot{1}}{\\delta}\\right)\\right)\\right)}\\end{array}$ samples from distribution $D_{h}$ . Hence, by Theorem G.2, with probability at least $1\\,-\\,{\\frac{\\delta}{H}}$ , we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(s_{h-1},a_{h-1},o_{h},s_{h})\\sim D_{h}}\\,\\left[\\widetilde{g}_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)\\neq s_{h}\\right]\\le\\frac{\\epsilon}{H}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, by union bound, with probability at least $\\mathrm{~\\ensuremath~{~1~}~}-\\mathrm{~\\ensuremath~{~\\delta~}~}$ , using a total of $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{H^{2}}{\\epsilon}\\cdot\\left(\\operatorname*{max}_{h\\in[H]}d_{D S}^{3/2}(\\mathcal{F}_{h})+\\log\\left(\\frac{1}{\\delta}\\right)\\right)\\right)}\\end{array}$ episodes we have that, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{h\\in[H]}\\mathbb{P}_{(s_{h-1},a_{h-1},o_{h},s_{h})\\sim D_{h}}\\,\\left[\\widetilde{g}_{h}\\left(s_{h-1},a_{h-1},o_{h}\\right)\\neq s_{h}\\right]\\le\\epsilon,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which in combination with Equation (G.1) concludes the proof. ", "page_idx": 34}, {"type": "text", "text": "H Missing Details in Section 5 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof of Theorem 5.1: Let $\\pi^{*}\\;\\in\\;\\mathrm{argmax}_{\\pi\\in\\Pi^{L}}\\;\\mathbb{E}_{s_{1}\\sim\\mu_{1}}\\left[V_{1}^{\\pi}\\big(s_{1}\\big)\\right]$ , where we define $V_{1}^{\\pi}(s_{1})\\,:=$ $\\mathbb{E}_{o_{1}\\sim\\mathbb{O}_{1}(\\cdot\\,|\\,s_{1}),a_{1}\\sim\\pi_{1}(\\cdot\\,|\\,o_{1})}[Q_{h}^{\\pi}(z_{1}=(o_{1}),s_{1},a_{1})]$ . We first note the following equation ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{T}\\sum_{t\\in[T]}\\mathbb{E}_{s_{1}\\sim\\mu_{1}}[V_{1}^{\\pi^{t}}(s_{1})]\\!=\\!\\!\\mathbb{E}_{s_{1}\\sim\\mu_{1}}[V_{1}^{\\pi^{*}}(s_{1})]+\\displaystyle\\frac{1}{T}\\sum_{t\\in[T]}\\mathbb{E}_{s_{1}\\sim\\mu_{1}}\\left(\\widetilde{V}_{1}^{\\pi^{t}}(s_{1})-V_{1}^{\\pi^{*}}(s_{1})\\right)}&{}\\\\ {\\displaystyle\\frac{1}{T}\\sum_{t\\in[T]}\\mathbb{E}_{s_{1}\\sim\\mu_{1}}\\left(V_{1}^{\\pi^{t}}(s_{1})-\\widetilde{V}_{1}^{\\pi^{t}}(s_{1})\\right).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We make use of the performance difference lemma [38, 1, 73] on the extended space $\\prod_{h\\in[H]}\\left({\\mathcal{Z}}_{h}\\times{\\mathcal{S}}\\right)$ . ", "page_idx": 34}, {"type": "text", "text": "Definition H.1. Consider the class of policies $\\Pi^{P L}$ such that at step $h\\in[H]$ , the policies in $\\Pi^{P L}$ take an action based on finite memory up to this step and the use of the underlying state, e.g., for each policy $\\pi_{1:H}\\in\\Pi^{P L}$ , $\\pi_{h}:{\\mathcal{Z}}_{h}\\times{\\dot{\\mathcal{S}}}\\to\\Delta(A)$ . ", "page_idx": 34}, {"type": "text", "text": "Observation 1. Note that $\\Pi^{L}\\subseteq\\Pi^{P L}$ . ", "page_idx": 35}, {"type": "text", "text": "Lemma H.2 (Performance difference Lemma [38, 1, 73]; see e.g., Lemma 1 in [73]). For any pair of policies $\\pi=\\{\\pi_{h}\\}_{h\\in[H]},\\pi^{\\prime}=\\{\\pi_{h}^{\\prime}\\}_{h\\in[H]}\\in\\Pi^{P L}$ , and approximation of the $Q$ -function of policy $\\pi$ , we have that for each state $s_{1}\\in\\mathrm{supp}(\\mu_{1})$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\widetilde{V}_{1}^{\\pi}(z_{1},s_{1})-V_{1}^{\\pi^{\\prime}}(z_{1},s_{1})}\\\\ &{=\\displaystyle\\sum_{h\\in[H]}\\mathbb{E}_{\\overline{{\\tau}}_{h}\\sim\\pi^{\\prime}|z_{1}}\\left[\\left\\langle\\widetilde{Q}_{h}^{\\pi}(z_{h},s_{h},\\cdot),\\pi_{h}(\\cdot\\mid z_{h},s_{h})-\\pi_{h}^{\\prime}(\\cdot\\mid z_{h},s_{h})\\right\\rangle\\right]}\\\\ &{\\displaystyle\\ +\\sum_{h\\in[H]}\\mathbb{E}_{\\overline{{\\tau}}_{h}\\sim\\pi^{\\prime}|z_{1}}\\left[\\widetilde{Q}_{h}^{\\pi}(z_{h},s_{h},a_{h})-\\mathbb{E}_{\\mathbf{\\Phi}_{s h+\\sim\\nabla\\mathbb{h}}(\\cdot\\mid s_{h},a_{h})},\\left[r_{h}(s_{h},a_{h})+\\widetilde{V}_{h+1}^{\\pi}(z_{h+1},s_{h+1})\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\widetilde{V}_{h}^{\\pi}(z_{h},s_{h})=\\mathbb{E}_{a_{h}\\sim\\pi_{h}(\\cdot\\mid z_{h})}[\\widetilde{Q}_{h}^{\\pi}(z_{h},s_{h},a_{h})].$ . ", "page_idx": 35}, {"type": "text", "text": "Setting $\\pi\\;=\\;\\pi^{t}\\;\\in\\;\\Pi^{L}\\;\\subseteq\\;\\Pi^{L P}$ , and $\\pi^{\\prime}\\;=\\;\\pi^{*}\\;\\in\\;\\Pi^{L}\\;\\subseteq\\;\\Pi^{L P}$ , where we remind that $\\pi^{*}\\in$ argmax\u03c0\u2208\u03a0L $V_{1}^{\\pi}(s_{1})$ , and for each $z_{h}\\in\\mathcal{Z}_{h}$ and $h\\in[H]$ , we abuse the notation by letting $\\pi_{h}^{t}(\\cdot\\,|\\,$ $z_{h},s_{h})=\\bar{\\pi}_{h}^{t}(\\cdot\\mid z_{h})$ and $\\pi_{h}^{*}(\\cdot\\mid z_{h},s_{h})=\\pi_{h}^{*}(\\cdot\\mid z_{h})$ for all $s_{h}\\in{\\mathcal{S}}$ . The above formulation is thus simplified to ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{s_{1}\\sim\\mu_{1}}[\\widetilde{V}_{1}^{\\pi^{t}}(s_{1})-V_{1}^{\\pi^{*}}(s_{1})]}\\\\ &{\\ =\\displaystyle\\sum_{h\\in[H]}\\mathbb{E}_{\\overline{{\\tau}}_{h}\\sim\\pi^{*}}\\left[\\left\\langle\\widetilde{Q}_{h}^{\\pi^{t}}(z_{h},s_{h},\\cdot),\\pi_{h}^{t}(\\cdot\\mid z_{h},s_{h})-\\pi_{h}^{*}(\\cdot\\mid z_{h},s_{h})\\right\\rangle\\right]}\\\\ &{\\ \\ \\ +\\displaystyle\\sum_{h\\in[H]}\\mathbb{E}_{\\overline{{\\tau}}_{h}\\sim\\pi^{*}}\\left[\\widetilde{Q}_{h}^{\\pi}(z_{h},s_{h},a_{h})-\\mathbb{E}_{s_{h+1}\\sim\\nabla_{h}(\\cdot\\mid s_{h},a_{h})},\\left[r_{h}(s_{h},a_{h})+\\widetilde{V}_{h+1}^{\\pi}(z_{h+1},s_{h+1})\\right]\\right]}\\\\ &{\\ \\ \\ \\ge\\displaystyle\\sum_{h\\in[H]}\\mathbb{E}_{\\overline{{\\tau}}_{h}\\sim\\pi^{*}}\\left[\\left\\langle\\widetilde{Q}_{h}^{\\pi^{t}}(z_{h},s_{h},\\cdot),\\pi_{h}^{t}(\\cdot\\mid z_{h},s_{h})-\\pi_{h}^{*}(\\cdot\\mid z_{h},s_{h})\\right\\rangle\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where in the inequality above we used Lemma H.3. Since our policy does not depend on the realized underlying state $s_{h}$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\left[\\left(T_{h}\\right)^{T}\\right]}\\mathbb{E}_{\\left[\\sigma_{h}^{\\top}\\right]}\\Big[\\sigma_{h}^{\\top}\\big(T_{h}^{*}(\\cdot)_{h}\\big)}\\\\ &{\\geq\\sum_{t=1}^{T}\\mathbb{E}_{\\left[\\sigma_{h}^{\\top}\\big(T_{h}^{*}(\\cdot)_{h}\\big)\\right]}\\mathcal{L}_{\\left[\\sigma_{h}^{\\top}\\right]}\\Big[(1-\\sigma_{h})-\\sigma_{h}^{\\top}\\big(\\frac{1}{\\sigma_{h}^{\\top}}\\big([1+\\sigma_{h})\\big)\\Big]}\\\\ &{=\\frac{1}{\\sqrt{h}}\\mathbb{E}_{\\sigma_{h}^{\\top}\\sim\\mathcal{V}}\\left[\\left(\\mathbb{E}_{\\left(T_{h}\\right)^{T}}\\Big(T_{h}^{*}(\\cdot)_{h}\\Big)\\right]\\sigma_{h}^{\\top}\\big([1-\\sigma_{h}^{\\top}(\\cdot)_{h}]\\big)\\right]}\\\\ &{=\\frac{1}{\\sqrt{h}}\\mathbb{E}_{\\left[\\sigma_{h}^{\\top}\\right]}\\mathbb{E}_{\\left[\\sigma_{h}^{\\top}\\right]}\\Big[\\Big(T_{h}^{*}(\\cdot)_{h}+\\sigma_{h}\\Big([\\sigma_{h}^{\\top}(\\cdot)_{h}]\\Big)\\sigma_{h}^{\\top}\\big([1+\\sigma_{h}^{\\top}(\\cdot)_{h}]\\Big)\\Big]}\\\\ &{\\qquad+\\sum_{t=1}^{T}\\mathbb{E}_{\\left[\\sigma_{h}^{\\top}\\right]}\\Big[\\Big(T_{h}^{*}(\\cdot)_{h}+\\sigma_{h}\\Big([\\sigma_{h}^{\\top}(\\cdot)_{h}]\\Big)\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\mathbb{E}_{\\sigma_{h}^{\\top}\\sim\\mathcal{V}}\\Big[\\Big(T_{h}^{*}(\\cdot)_{h}\\Big)\\Big(T_{h}^{*}(\\cdot)_{h}+\\sigma_{h}^{\\top}\\big([1+\\sigma_{h}^{\\top}(\\cdot)_{h}]\\Big)\\Big]}\\\\ &{\\geq\\sum_{t=1}^{T}\\mathbb{E}_{\\sigma_{h}^{\\top}\\sim\\mathcal{V}}\\Big[\\Big(T_{h}^{*}(\\cdot)_{h}+\\sigma_{h}\\Big([\\sigma_{h}^{\\top}(\\cdot)_{h}]\\Big \n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The last inequality follows by Lemma H.3. By averaging we get, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{T}\\displaystyle\\sum_{x:\\varrho_{i}=1\\atop i\\neq j_{1}}\\overline{{y_{i}^{\\prime}(x_{1})}}|\\hat{V}_{i}^{\\dagger}(x_{1})|}\\\\ &{\\overset{,,}{\\geq}\\mathbb{E}_{\\underline{{\\nu}}\\underline{{\\nu}}\\underline{{\\nu}}}|V_{i}^{\\top}(x_{1})|}\\\\ &{\\qquad+\\frac{1}{T}\\displaystyle\\sum_{k\\in[T]}\\mathbb{E}_{\\underline{{\\nu}}\\underline{{\\nu}}\\underline{{\\nu}}}\\left[\\sum_{l\\in[T]}\\left\\langle\\overline{{g}}_{k}^{\\star}(z_{k},s_{k},s_{l})\\right\\rangle\\right.\\bigg.\\bigg.\\eta_{k}^{\\ell}\\big(\\cdot|z_{k}|-x_{k}^{\\star}(\\cdot|\\,z_{k})\\big)\\right]}\\\\ &{\\qquad-2\\cdot H\\cdot\\sum_{l\\in[T]}\\mathbb{E}_{\\underline{{\\nu}}\\underline{{\\nu}}\\underline{{\\nu}}\\underline{{\\nu}}}\\left[\\widehat{\\mu}_{l}(\\gamma_{l})\\delta_{l k}^{\\ell\\star}(z_{l})\\right]}\\\\ &{\\overset{,}{\\geq}\\mathbb{E}_{\\underline{{\\nu}}\\underline{{\\nu}}\\underline{{\\nu}}}|U_{i}^{\\ell}(s_{l})|}\\\\ &{\\qquad+\\frac{H}{T}\\displaystyle\\sum_{k\\in[T]}\\mathbb{E}_{\\underline{{\\nu}}\\underline{{\\nu}}}\\left[\\sum_{l\\in[T]}\\left\\langle\\overline{{g}}_{k},s_{k}^{\\star}+v_{l}s_{l}\\right\\rangle\\left\\langle\\overline{{g}}_{k}^{\\ell\\star}(z_{l},s_{k},s_{l})\\right\\rangle\\right]}\\\\ &{\\qquad-2\\cdot H^{2}\\operatorname*{max}_{i\\in[T]}\\mathbb{E}_{\\underline{{\\nu}}\\underline{{\\nu}}}\\left\\langle\\int_{T}\\big(\\delta_{l}\\gamma_{(k})(\\underline{{\\nu}}_{l},b_{l k}^{\\star}(s_{l}))\\big)\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\left.\\underline{{\\eta}}\\mathbb{E}_{\\underline{{\\nu}}\\underline{{\\nu}}}\\left[\\langle\\underline{{\\nu}}\\rangle_{l k}^{\\top}(s_{l})|-2\\cdot H^{2}\\operatorname*{max}_{i\\in[T]}\\mathbb{E}_{\\underline{{\\nu} \n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last inequality follows since for fixed $h\\in[H]$ and $z_{h}\\in\\mathcal{Z}_{h}$ , the agent updates her policy on memory $z_{h}$ according to MWU on feedback $\\left\\{\\mathbb{E}_{s_{h}\\sim b^{\\mathrm{apx}}(z_{h})}\\left[\\widetilde{Q}_{h}^{\\pi^{t}}\\big(z_{h},s_{h},a\\big)\\right]\\right\\}_{a\\in\\mathcal{A}}$ , and thus, the accumulate regret is bounded by (Section 4.3 in [10]): ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{t\\in[T]}{\\sum}\\left\\langle\\mathbb{E}_{s_{h}\\sim b^{\\mathrm{spx}}(z_{h})}\\left[\\widetilde{Q}_{h}^{\\pi^{t}}(z_{h},s_{h},\\cdot)\\right],\\pi_{h}^{*}(\\cdot\\mid z_{h})-\\pi_{h}^{t}(\\cdot\\mid z_{h})\\right\\rangle}\\\\ &{\\leq\\!\\frac{\\log{(|A|)}}{\\eta}+\\eta\\cdot T\\cdot\\left\\|Q_{h}^{\\pi^{t}}(z_{h},s_{h},\\cdot)\\right\\|_{+\\infty}}\\\\ &{\\leq\\!\\frac{\\log{(|A|)}}{\\eta}+\\eta\\cdot T\\cdot H}\\\\ &{=\\!2\\sqrt{T\\cdot H\\log{(|A|)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The proof follows by combining Equation (H.1) and the inequality above. Finally, to achieve the near optimality in the class of $\\Pi^{L}$ , we bound the optimistic estimate using Equation (H.2) in Lemma H.3, and its global near-optimality for a large enough $L$ under $\\gamma.$ -observability is a direct consequence of Theorem 4.1 in [26]. ", "page_idx": 36}, {"type": "text", "text": "Lemma H.3 (Optimistic $Q$ -function - adapted from [47]). Given a policy $\\pi\\in\\Pi^{L}$ , and a parameter $M\\in\\mathbb{N}$ , let $\\{\\widetilde{Q}_{h}^{\\pi}:\\mathcal{Z}_{h}\\times\\mathcal{S}\\times\\mathcal{A}\\rightarrow[0,H]\\}_{h\\in[H]}$ be the output of Algorithm 3. Then, with probability at least $1-\\delta$ : $\\forall z_{h}\\in\\mathcal{Z}_{h},s_{h}\\in S,a_{h}\\in\\mathcal{A}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H-h+1\\geq\\widetilde{Q}_{h}^{\\pi}(z_{h},s_{h},a_{h})\\geq\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\,\\vert\\,s_{h},a_{h})},\\,\\left[r_{h}(s_{h},a_{h})+\\widetilde{V}_{h+1}^{\\pi}(z_{h+1},s_{h+1})\\right],}\\\\ &{\\mathbb{E}_{s_{1}\\sim\\mu_{1}}[\\widetilde{V}_{1}^{\\pi}(s_{1})]-\\mathbb{E}_{s_{1}\\sim\\mu_{1}}[V_{1}^{\\pi}(s_{1})]\\leq}\\\\ &{O\\left(H^{2}\\cdot\\sqrt{\\frac{\\operatorname*{max}(\\vert\\mathcal{O}\\vert,\\vert\\,S\\vert)\\cdot\\vert\\mathcal{S}\\vert\\cdot\\vert\\mathcal{A}\\vert}{M}\\cdot\\log\\left(\\frac{\\vert\\mathcal{S}\\vert\\cdot\\vert\\mathcal{A}\\vert}{\\delta}\\right)\\log\\left(\\frac{M\\cdot\\vert\\mathcal{S}\\vert\\cdot\\vert\\mathcal{A}\\vert\\cdot H}{\\delta}\\right)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\begin{array}{r l r}{V_{1}^{\\pi}(s_{1})}&{{}\\;=\\;}&{\\;\\mathbb{E}_{o_{1}\\sim\\mathbb{O}_{1}(\\cdot\\,|\\,s_{1}),a_{1}\\sim\\pi_{1}(\\cdot\\,|\\,o_{1})}[Q_{h}^{\\pi}(z_{1}}&{{}\\;=\\;}&{\\;(o_{1}),s_{1},a_{1})],}\\end{array}$ $\\begin{array}{r l}{\\widetilde{V}_{1}^{\\pi}(s_{1})}&{{}=}\\end{array}$ $\\mathbb{E}_{o_{1}\\sim\\mathbb{O}_{1}(\\cdot\\mid s_{1}),a_{1}\\sim\\pi_{1}(\\cdot\\mid o_{1})}[\\widetilde{Q}_{h}^{\\pi}(z_{1}\\,=\\,(o_{1}),s_{1},a_{1})]$ and $\\widetilde{V}_{h}^{\\pi}(z_{h},s_{h})=\\mathbb{E}_{a_{h}\\sim\\pi_{h}(\\cdot\\,|\\,z_{h})}[\\widetilde{Q}_{h}^{\\pi}(z_{h},s_{h},a_{h})]$ . Moreover, Algorithm 3  needs a total of $H\\cdot M$ e pisodes from POMDP $\\mathcal{P}$ an d runs in time $\\mathrm{POLY}\\big(H,M,\\check{S},A^{L},O^{L}\\big)$ . ", "page_idx": 36}, {"type": "text", "text": "Proof. For each step $h\\in[H]$ , collect $M$ trajectories with states using policy $\\pi$ on POMDP $\\mathcal{P}$ and let $D_{h}=\\{\\overline{{\\tau}}^{(i)}\\}_{i\\in[M]}$ be those collected trajectories. Define the empirical transition, observation and reward distribution as follows: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\cal N}_{h}(s_{h},a_{h},s_{h+1})=\\left\\vert\\left\\{\\overline{{\\tau}}=(s_{1}^{\\prime},a_{1}^{\\prime},a_{1}^{\\prime},r_{1}^{\\prime},\\ldots,s_{h}^{\\prime},a_{h}^{\\prime},a_{h}^{\\prime},r_{h}^{\\prime})\\in{\\cal D}_{h}\\right.}\\\\ &{\\qquad\\qquad\\left.\\qquad{\\cal N}_{h}(s_{h},a_{h})=\\sum_{b_{h}+1\\in{\\cal S}_{h}}{\\cal N}_{h}(s_{h},a_{h},s_{h+1}),\\right.}\\\\ &{\\qquad\\qquad\\left.\\qquad\\qquad{\\cal N}_{h}(s_{h},a_{h})=\\sum_{b_{h}+1\\in{\\cal S}_{h}}{\\cal N}_{h}(s_{h},a_{h}),}\\\\ &{\\qquad\\qquad\\quad\\left.\\qquad{\\cal N}_{h}(s_{h})=\\sum_{b_{h}\\in{\\cal S}_{h}}{\\cal N}_{h}(s_{h},a_{h}),\\right.}\\\\ &{\\qquad\\qquad\\quad\\left.\\qquad{\\cal N}_{h}(s_{h},a_{h})=\\left\\vert\\left\\{\\overline{{\\tau}}=(s_{1}^{\\prime},a_{1}^{\\prime},a_{1}^{\\prime},r_{1}^{\\prime},\\ldots,s_{h}^{\\prime},a_{h}^{\\prime},a_{h}^{\\prime},r_{h}^{\\prime})\\in{\\cal D}_{h}:(s_{h},o_{h})=(s_{h}^{\\prime},a_{h}^{\\prime})\\right\\}\\right\\vert,}\\\\ {\\widehat{\\Pi}_{h}(s_{h+1}\\mid s_{h},a_{h})=\\frac{{\\cal N}_{h}(s_{h},a_{h},s_{h+1})}{{\\cal N}_{h}(s_{h},a_{h})},}\\\\ &{\\qquad\\qquad\\left.\\qquad\\widehat{\\Omega}_{h}(o_{h}\\mid s_{h})=\\frac{{\\cal N}_{h}(s_{h},a_{h})}{{\\cal N}_{h}(s_{h})}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Set $\\begin{array}{r}{\\delta_{1}=\\frac{\\delta}{2\\cdot|S|\\cdot(|A|+1)}}\\end{array}$ . By [13], there exists a constant $C>0$ such that for each step $h\\in[H]$ , state $s\\in S$ and action $a\\in{\\mathcal{A}}$ with probability at least $1-\\delta_{1}$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{T}_{h}(\\cdot\\mid s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\mid s_{h},a_{h})\\|_{1}\\leq\\operatorname*{min}\\left(2,C\\cdot\\sqrt{\\frac{|S|\\log(1/\\delta_{1})}{\\operatorname*{max}(N_{h}(s_{h},a_{h}),1)}}\\right),}\\\\ &{\\|\\mathbb{O}_{h}(\\cdot\\mid s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\mid s_{h})\\|_{1}\\leq\\operatorname*{min}\\left(2,C\\cdot\\sqrt{\\frac{|\\mathcal{O}|\\log(1/\\delta_{1})}{\\operatorname*{max}(N_{h}(s_{h}),1)}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For the rest of the proof we condition on this event. By union bound, this happen with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ . We define the optimistic $Q$ function recursively as follows for a memory-state pair $(z_{h},s_{h})\\in\\bar{\\mathcal{Z}_{h}}\\times\\mathcal{S}$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{Q}_{H+1}^{\\pi}(z_{H+1},s_{H+1},\\cdot)=0,\\qquad\\qquad\\forall z_{H+1}\\in\\mathcal{Z}_{H+1},s_{H+1}\\in\\mathcal{S}}\\\\ &{\\qquad\\qquad\\widetilde{Q}_{h}^{\\pi}(z_{h},s_{h},a_{h})=\\operatorname*{min}\\Bigg(H-h+1,\\mathbb{E}_{s_{h+1}\\sim\\widehat{\\mathbb{U}}_{h}(\\cdot\\,|\\,s_{h},a_{h})},[\\widetilde{V}_{h+1}^{\\pi}(z_{h+1},s_{h+1})]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad o_{h+1}\\sim\\widehat{\\mathbb{O}}_{h+1}(\\cdot\\,|\\,s_{h+1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,r(s_{h},a_{h})+H\\cdot\\operatorname*{min}\\Bigg(2,C\\cdot\\sqrt{\\frac{|\\mathcal{S}|\\log(1/\\delta_{1})}{\\operatorname*{max}(N_{h}(s_{h},a_{h}),1)}}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}_{s_{h+1}\\sim\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})}H\\cdot\\operatorname*{min}\\Bigg(2,C\\cdot\\sqrt{\\frac{|\\mathcal{O}|\\log(1/\\delta_{1})}{\\operatorname*{max}(N_{h+1}(s_{h+1}),1)}}\\Bigg)\\Bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\widetilde{V}_{h+1}^{\\pi}(z_{h+1},s_{h+1})\\,=\\,\\mathbb{E}_{a_{h+1}\\sim\\pi_{h+1}(\\cdot\\,|\\,{z_{h+1}})}[\\widetilde{Q}_{h+1}^{\\pi}(z_{h+1},s_{h+1},a_{h+1})]$ . Hence the time complexity of our algorithm is $\\operatorname{POLY}\\!\\left(H,M,S,A^{L},O^{L}\\right)$ . To prove the first condition we fix step $h\\in[H],z_{h}\\in{\\mathcal{Z}}_{h},a_{h}\\in{\\mathcal{A}}$ and state $s_{h}\\in{\\mathcal{S}}$ and take condition whether $\\widetilde{Q}_{h}^{\\pi}(z_{h},s_{h},a_{h})=H\\!-\\!h\\!+\\!1$ . In this case, since by assumption on the POMDP $\\mathcal{P}$ , $r_{h}(s_{h},a_{h})\\ \\leq^{\\dagger}1$ , and by definition of $\\widetilde{Q}_{h+1}^{\\pi}(\\cdot,\\cdot,\\cdot)\\le H-h$ , we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{Q}_{h}^{\\pi}(z_{h},s_{h},a_{h})=1+H-h\\ge\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\mid s_{h},a_{h}),\\atop{o_{h+1}\\sim\\Omega_{h+1}(\\cdot\\mid s_{h+1})}},[r_{h}(s_{h},a_{h})+\\widetilde{V}_{h+1}^{\\pi}(z_{h+1},s_{h+1})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "If $\\widetilde{Q}_{h}^{\\pi}(z_{h},s_{h},a_{h})\\neq H-h+1$ , observe that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{Q}_{h}^{\\pi}(z_{h},s_{h},a_{h})=\\!\\mathbb{E}_{s_{h+1}\\sim\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})},[\\widetilde{V}_{h+1}^{\\pi}(z_{h+1},s_{h+1})]\\!}\\\\ &{\\qquad\\qquad\\qquad o_{h+1}\\!\\sim\\!\\widehat{\\Omega}_{h+1}(\\cdot\\,|\\,s_{h+1})}\\\\ &{\\qquad\\qquad\\qquad\\quad+\\,r(s_{h},a_{h})+H\\cdot\\operatorname*{min}\\left(2,C\\cdot\\sqrt{\\frac{|\\mathcal{S}|\\log(1/\\delta_{1})}{\\operatorname*{max}(N_{h}(s_{h},a_{h}),1)}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\quad+\\,\\mathbb{E}_{s_{h+1}\\sim\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})}H\\cdot\\operatorname*{min}\\left(2,C\\cdot\\sqrt{\\frac{|\\mathcal{O}|\\log(1/\\delta_{1})}{\\operatorname*{max}(N_{h+1}(s_{h+1}),1)}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})},[r_{h}(s_{h},a_{h})+\\widetilde{V}_{h+1}^{\\pi}(z_{h+1},s_{h+1})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and hence, $\\{\\widetilde{Q}_{h}^{\\pi}\\}_{h\\in[H]}$ satisfies the first condition. Moreover, it is true that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}_{h}^{\\pi}(z_{h},s_{h},a_{h})\\leq\\underline{{v}}_{s_{h+1}\\sim\\mathcal{T}_{h}(\\cdot\\vert s_{h},a_{h})}\\,\\Big[r_{h}(s_{h},a_{h})+\\bar{V}_{h+1}^{\\pi}(z_{h+1},s_{h+1})\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\quad+2H\\cdot\\operatorname*{min}\\Bigg(2,C\\cdot\\sqrt{\\frac{\\vert S\\vert_{\\mathcal{H}}(1/\\delta_{h})}{\\operatorname*{max}(S_{h}(\\cdot\\vert s_{h},a_{h}),1)}}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\quad+2\\cdot\\mathbb{E}_{s_{h+1}\\sim\\bar{\\pi}_{h+1}^{\\pi}(\\cdot\\vert s_{h},a_{h})}H\\cdot\\operatorname*{min}\\Bigg(2,C\\cdot\\sqrt{\\frac{\\vert\\mathcal{O}\\vert\\log(1/\\delta_{h})}{\\operatorname*{max}(S_{h+1}(s_{h+1},1))}}\\Bigg)}\\\\ &{\\qquad\\qquad\\leq\\underline{{v}}_{s_{h+1}\\sim\\mathcal{T}_{h}(\\cdot\\vert s_{h+1})}\\,\\Big[r_{h}(s_{h},a_{h})+\\bar{V}_{h+1}^{\\pi}(z_{h+1},s_{h+1})\\Big]}\\\\ &{\\qquad\\qquad\\quad+o t H\\cdot\\operatorname*{min}\\Bigg(2,C\\cdot\\sqrt{\\frac{\\vert S\\vert_{\\mathcal{H}}(1/\\delta_{h})}{\\operatorname*{max}(S_{h}(\\cdot\\vert s_{h},a_{h}),1)}}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\quad+2\\cdot\\mathbb{E}_{s_{h+1}\\sim\\mathcal{T}_{h+1}(\\cdot\\vert s_{h},a_{h})}H\\cdot\\operatorname*{min}\\Bigg(2,C\\cdot\\sqrt{\\frac{\\vert\\mathcal{O}\\vert\\log(1/\\delta_{h})}{\\operatorname*{max}(S_{h}(\\cdot\\vert s_{h+1},1))}}\\Bigg)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, it holds that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{V}_{h}^{\\pi}(z_{h},s_{h})-V_{h}^{\\pi}(z_{h},s_{h})\\leq\\!\\!\\mathbb{E}_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, we conclude that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{s_{1}\\sim\\mu_{1}}[\\widetilde{V}_{1}^{\\pi}(s_{1})]-\\mathbb{E}_{s_{1}\\sim\\mu_{1}}[V_{1}^{\\pi}(s_{1})]}\\\\ &{\\quad\\quad\\le\\mathbb{E}_{\\tau=(s_{1},a_{1},\\ldots,s_{H+1})\\sim\\pi}\\left[\\displaystyle\\sum_{h\\in[H]}8\\cdot H\\cdot C\\cdot\\sqrt{\\frac{\\operatorname*{max}(|S|,|\\mathcal{O}|)\\log(1/\\delta_{1})}{\\operatorname*{max}(N_{h}(s_{h},a_{h}),1)}}\\right]}\\\\ &{\\quad\\quad=8\\cdot H\\sqrt{\\operatorname*{max}(|\\mathcal{O}|,|\\mathcal{S}|)\\cdot\\log(1/\\delta_{1})}\\cdot C}\\\\ &{\\qquad\\qquad\\qquad\\cdot\\displaystyle\\sum_{h\\in[H]}\\mathbb{E}_{\\tau=(s_{1},a_{1},\\ldots,s_{H+1})\\sim\\pi}\\left[\\sqrt{\\frac{1}{\\operatorname*{max}(N_{h}(s_{h},a_{h}),1)}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "To finish the proof, we make use of the following lemma. ", "page_idx": 38}, {"type": "text", "text": "Lemma H.4 (Lemma 6 in [47]). For each step $h\\in[H]$ , and state-action pair $\\left(s_{h},a_{h}\\right)\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ , with probability at least $1-\\delta_{2}$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{1}{\\operatorname*{max}(N_{h}(s_{h},a_{h}),1)}}=O\\left(\\sqrt{\\frac{|S|\\cdot|A|\\log(M/\\delta_{2})}{M}}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By setting $\\begin{array}{r}{\\delta_{2}=\\frac{\\delta}{2\\cdot|S|\\cdot|A|\\cdot H}}\\end{array}$ , and taking union bound we have that with probability at least $1-\\delta$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{s_{1}\\sim\\mu_{1}}[\\widetilde{V}_{1}^{\\pi}(s_{1})]-\\mathbb{E}_{s_{1}\\sim\\mu_{1}}[V_{1}^{\\pi}(s_{1})]}\\\\ &{\\quad=8\\sqrt{\\operatorname*{max}(|\\mathcal{O}|,|\\mathcal{S}|)\\cdot\\log(1/\\delta_{1})}\\cdot C\\cdot\\displaystyle\\sum_{h\\in[H]}\\mathbb{E}_{\\tau=(s_{1},a_{1},\\dots,s_{H+1})\\sim\\pi}\\left[\\sqrt{\\frac{1}{\\operatorname*{max}(N_{h}(s_{h},a_{h}),1)}}\\right]}\\\\ &{\\quad\\le O\\left(H^{2}\\cdot\\sqrt{\\frac{\\operatorname*{max}(|\\mathcal{O}|,|\\mathcal{S}|)\\cdot|\\mathcal{S}|\\cdot|\\mathcal{A}|}{M}}\\cdot\\log\\left(\\frac{|\\mathcal{S}|\\cdot|\\mathcal{A}|}{\\delta}\\right)\\log\\left(\\frac{M\\cdot|\\mathcal{S}|\\cdot|\\mathcal{A}|\\cdot H}{\\delta}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof of Theorem 5.2: The proof of Theorem 5.2 follows by combining Theorem H.5 and Theorem H.6 below. Theorem H.5 proves that we can approximately learn a POMDP model $\\mathcal{P}$ computationally and sample efficiently, thanks to the privileged information. ", "page_idx": 39}, {"type": "text", "text": "Theorem H.5. Fix any $\\epsilon,\\delta\\in(0,1)$ . Algorithm 4 can learn the approximate POMDP model with transition $\\widehat{\\mathbb{T}}_{1:H}$ and emission $\\widehat{\\mathbb{O}}_{1:H}$ such that with probability at least $1-\\delta$ , for any policy $\\pi\\in\\Pi^{\\mathrm{gen}}$ and $h\\in[H]$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\left[\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}+\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}\\right]\\le O(\\epsilon),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "using P $\\begin{array}{r}{\\operatorname{oLY}(S,A,H,O,\\frac{1}{\\epsilon},\\log(\\frac{1}{\\delta}))}\\end{array}$ episodes in time $\\begin{array}{r}{\\mathtt{P O L Y}\\big(S,A,H,O,\\frac{1}{\\epsilon},\\log(\\frac{1}{\\delta})\\big)}\\end{array}$ . ", "page_idx": 39}, {"type": "text", "text": "Proof. Note that by Lemma H.10, it suffices to consider only $\\pi\\in\\Pi_{S}$ as the optimal value for policies in $\\Pi^{\\mathrm{gen}}$ can be achieved by those in $\\Pi_{S}$ (by considering $r_{h}(s_{h},a_{h})\\,:=\\,2\\bar{H}-(\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\,-$ $\\widehat{\\mathbb{T}}_{h}\\big(\\cdot\\,\\big|\\,s_{h},a_{h}\\big)\\big\\|_{1}+\\big\\|\\mathbb{O}_{h}\\big(\\cdot\\,\\big|\\,s_{h}\\big)-\\widehat{\\mathbb{O}}_{h}\\big(\\cdot\\,\\big|\\,s_{h}\\big)\\big\\|_{1}\\big))$ . For each $h\\in[H]$ and $s_{h}\\in{\\mathcal{S}}$ , we define ", "page_idx": 39}, {"type": "equation", "text": "$$\np_{h}(s_{h})=\\operatorname*{max}_{\\pi\\in\\Pi_{S}}d_{h}^{\\pi}(s_{h}).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Fix $\\epsilon_{1}~>~0$ , we define $\\mathcal{U}(h,\\epsilon_{1})\\;=\\;\\{s_{h}\\;\\in\\;\\mathcal{S}\\:|\\:p_{h}(s_{h})\\;\\geq\\;\\epsilon_{1}\\}$ . By the guarantee of the EULER algorithm from [88, 36], one can learn a policy $\\Psi(h,s_{h})$ with sample complexity $\\widetilde{\\mathcal{O}}\\big(\\frac{S^{2}A H^{4}}{\\epsilon_{1}}\\big)$ such that $\\begin{array}{r}{d_{h}^{\\Psi(h,s_{h})}(s_{h})\\ge\\frac{p_{h}(s_{h})}{2\\,.}}\\end{array}$ for each $s_{h}\\in\\mathcal{U}(h,\\epsilon_{1})$ with probability $1-\\delta_{1}$ . Now  we as1sume this event holds for any $h\\in[H]$ and $s_{h}\\in\\mathcal{U}(h,\\epsilon_{1})$ . For each $s_{h}\\in{\\mathcal{S}}$ and $a_{h}\\in A$ , we have executed in Algorithm 4 the policy $\\bar{\\Psi}(h,s_{h})$ followed by an action $a_{h}\\in A$ for $N$ episodes, and denote the number of episodes that $s_{h}$ and $a_{h}$ are visited as $N_{h}(s_{h},a_{h})$ . Then with probability $1-e^{N\\epsilon_{1}/8}$ , $\\begin{array}{r}{N_{h}(s_{h},a_{h})\\ge\\frac{N p_{h}(s_{h})}{2}}\\end{array}$ by Chernoff bound. Now conditioned on this event, we are ready to evaluate ", "page_idx": 39}, {"type": "text", "text": "the following: for any $\\pi\\in\\Pi_{S}$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\cdot\\mathbb{E}_{n}^{p}\\Vert\\mathcal{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\Vert_{1}}\\\\ &{=\\frac{1}{2}\\sum_{s_{h},a_{h}}d_{h}^{n}(s_{h})\\pi_{}(a_{h})\\,|s_{h}|\\rangle\\Vert\\mathcal{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\Vert_{1}}\\\\ &{\\leq S\\epsilon_{1}+\\frac{1}{2}\\sum_{s_{h}\\in\\mathcal{U}(h,\\cdot),a_{h}}d_{h}^{n}(s_{h})\\pi_{}(a_{h}\\,|\\,s_{h}\\rangle)\\sqrt{\\frac{S\\log(1/\\delta_{2})}{N p_{h}(s_{h})}}}\\\\ &{\\leq S\\epsilon_{1}+\\underset{s_{h}\\in\\mathcal{U}(h,\\cdot),a_{h}}{\\sum}d_{h}^{\\psi(h,\\cdot),a_{h}}(s_{h})\\pi_{}(a_{h}\\,|\\,s_{h}\\rangle)\\sqrt{\\frac{S\\log(1/\\delta_{2})}{N p_{h}(s_{h})}}}\\\\ &{\\leq S\\epsilon_{1}+\\sum_{s_{h}\\in\\mathcal{U}(h,\\cdot),a_{h}}\\sqrt{\\frac{S\\log(1/\\delta_{h})}{N p_{h}}}\\left(\\sum_{k_{h}\\in\\mathcal{U}(h),a_{h}}\\right)}\\\\ &{\\leq S\\epsilon_{1}+S\\sqrt{\\frac{\\log(1/\\delta_{h})}{N}}\\left(\\sum_{k_{h}}\\log(1/\\delta_{h})-\\right.}\\\\ &{\\leq S\\epsilon_{1}+S\\sqrt{\\frac{\\log(1/\\delta_{2})}{N}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the first inequality follows by [13] with probability at least $1-\\delta_{2}$ , and the second inequality uses $\\begin{array}{r}{d_{h}^{\\Psi(h,s_{h})}(s_{h})\\geq\\frac{p_{h}(s_{h})}{2}}\\end{array}$ for all $s_{h}\\in\\mathcal{U}(h,\\epsilon_{1})$ . Similarly, for any $\\pi\\in\\Pi_{S}$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\cdot\\mathbb{E}_{\\pi}^{p}\\|\\mathbb{D}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}}\\\\ &{\\leq S\\epsilon_{1}+\\frac{1}{2}\\displaystyle\\sum_{s_{h}\\in\\mathcal{U}(h,\\cdot)_{\\epsilon_{1}}}d_{h}^{\\pi}(s_{h})\\sqrt{\\frac{\\widehat{\\pi\\log(1/\\delta_{2})}}{N p_{h}(s_{h})}}}\\\\ &{\\leq S\\epsilon_{1}+\\displaystyle\\sum_{s_{h}\\in\\mathcal{U}(h,\\cdot)_{\\epsilon_{1}}}d_{h}^{\\psi(h,s_{h})}(s_{h})\\sqrt{\\frac{{\\widehat{\\pi\\log(1/\\delta_{2})}}}{N p_{h}(s_{h})}}}\\\\ &{\\leq S\\epsilon_{1}+\\displaystyle\\sum_{s_{h}\\in\\mathcal{U}(h,\\cdot)_{\\epsilon_{1}}}\\sqrt{d_{h}^{\\psi(h,s_{h})}(s_{h})}\\sqrt{\\frac{{\\widehat{\\pi\\log(1/\\delta_{2})}}}{N}}}\\\\ &{\\leq S\\epsilon_{1}+\\sqrt{\\frac{S O\\log(1/\\delta_{2})}{N}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where similarly the first inequality follows by [13] with probability at least $1-\\delta_{2}$ , and the second inequality uses dh\u03a8(h,sh)(sh) \u2265 ph(2sh)for all sh \u2208U(h, \u03f51). Therefore, by a union bound, all the high probability events above hold with probability ", "page_idx": 40}, {"type": "equation", "text": "$$\n1-S H\\delta_{1}-S H A e^{N\\epsilon_{1}/8}-2S A H\\delta_{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Therefore, we can choose $\\begin{array}{r}{N=\\widetilde\\Theta(\\frac{S^{2}+S O}{\\epsilon^{2}})}\\end{array}$ and $\\epsilon_{1}=\\Theta(\\frac{\\epsilon}{S})$ , leading to the total sample complexity ", "page_idx": 40}, {"type": "equation", "text": "$$\nS H A(N+\\tilde{\\mathcal{O}}(\\frac{S^{3}A H^{4}}{\\epsilon}))=\\widetilde{\\mathcal{O}}(\\frac{S^{2}A H O+S^{3}A H}{\\epsilon^{2}}+\\frac{S^{4}A^{2}H^{5}}{\\epsilon}),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which completes the proof. ", "page_idx": 40}, {"type": "text", "text": "Now with such a learned model in a reward-free way, we are ready to present our main result for approximate belief learning. ", "page_idx": 40}, {"type": "text", "text": "Theorem H.6. Consider a $\\gamma.$ -observable POMDP $\\mathcal{P}$ (c.f. Assumption 2.2), an $\\epsilon>0$ and letP be the outcome of Algorithm 4 such that for any $\\pi\\in\\Pi^{\\mathrm{gen}}$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\left[\\lVert\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\rVert_{1}+\\lVert\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\rVert_{1}\\right]\\le\\mathcal{O}\\left(\\frac{\\epsilon}{3\\cdot H}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then we can construct in time $\\begin{array}{r}{\\operatorname{POLY}(H,A,S,O,\\frac{1}{\\epsilon},\\log\\frac{1}{\\delta})}\\end{array}$ a belief $\\{b_{h}^{\\mathrm{apx}}:{\\mathcal{Z}}_{h}\\to\\Delta(S)\\}_{h\\in[H]}$ with no further samples. In addition, if the parameter in Algorithm 4 satisfies $\\begin{array}{r}{N=\\widetilde\\Theta(\\frac{O\\log(S H/\\delta)}{\\gamma^{2}\\epsilon_{1}})}\\end{array}$ and ", "page_idx": 40}, {"type": "text", "text": "our class of finite memory policies $\\Pi^{L}$ satisfies $L\\ge\\widetilde\\Omega(\\gamma^{-4}\\log(S/\\epsilon)$ , then for any $\\pi\\in\\Pi^{\\mathrm{gen}}$ and $h\\in[H]$ : ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\lVert b_{h}(\\tau_{h})-b_{h}^{\\mathrm{apx}}(z_{h})\\rVert_{1}\\le O(\\epsilon).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. We firstly consider the following simple while important fact: for the estimated emission ${\\widehat{\\mathbb{O}}}_{h}$ , its observability can be evaluated as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\mathbb{O}}_{h}^{\\top}(b-b^{\\prime})\\|_{1}\\geq\\|\\mathbb{O}_{h}^{\\top}(b-b^{\\prime})\\|_{1}-\\|(\\mathbb{O}_{h}^{\\top}-\\widehat{\\mathbb{O}}_{h}^{\\top})(b-b^{\\prime})\\|_{1}\\geq(\\gamma-\\|\\widehat{\\mathbb{O}}_{h}-\\mathbb{O}_{h}\\|_{\\infty})\\|b-b^{\\prime}\\|_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for any $b,b^{\\prime}\\in\\Delta(S)$ and $\\begin{array}{r}{\\|\\widehat{\\mathbb{O}}_{h}-\\mathbb{O}_{h}\\|_{\\infty}:=\\operatorname*{max}_{s_{h}\\in\\mathcal{S}}\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}}\\end{array}$ . Therefore, if one can ensure that the emission at any state $s_{h}$ is learned accurately in the sense that $\\left\\|\\mathbb{O}_{h}\\right(\\cdot\\mid s_{h})-$ $\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\Vert_{1}\\,\\leq\\,\\frac{\\gamma}{2}$ , we can conclude that ${\\widehat{\\mathbb{O}}}_{h}$ is also $\\gamma/2$ -observable. However, the key challenge here is that there could exist some states $s_{h}$ at step $h$ that can only be visited with a low probability no matter what exploration policy is used. Therefore, emissions at such states may not be learned accurately. To address this issue, our key technique here is to redirect the transition probability into states that cannot be explored sufficiently to some highly visited states, in a new POMDP that is close to the original one in some sense. ", "page_idx": 41}, {"type": "text", "text": "Specifically, first, for any $\\epsilon_{1}>0$ , we define ", "page_idx": 41}, {"type": "equation", "text": "$$\nS_{h}^{\\mathrm{low}}:=\\left\\{s_{h}\\in S\\,\\Big\\vert\\,\\frac{N_{h}(s_{h})}N<\\frac{\\epsilon_{1}}2\\right\\},\\quad S_{h}^{\\mathrm{high}}:=S\\,\\backslash\\,S_{h}^{\\mathrm{low}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By Chernoff bound, with probability at least $1-S e^{-N\\epsilon_{1}/8}$ , it holds that ", "page_idx": 41}, {"type": "equation", "text": "$$\nS_{h}^{\\mathrm{low}}\\subseteq\\{s_{h}\\in{\\cal S}\\,|\\,p_{h}(s_{h})<\\epsilon_{1}\\},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $p_{h}(s_{h}):=\\operatorname*{max}_{\\pi\\in\\Pi_{S}}{d_{h}^{\\pi}(s_{h})}$ . To see the reason, we notice that for any $s_{h}$ such that $p_{h}(s_{h})\\ge$ $\\epsilon_{1}$ , with probability $1-e^{-N\\epsilon_{1}/8}$ , it holds that NhN(sh) \u2265 \u03f521 . Therefore, the last step is by taking a union bound for all $s_{h}$ . Now with $S_{h}^{\\mathrm{low}}$ defined, we are ready to construct a truncated POMDP $\\mathcal{P}^{\\mathrm{trunc}}$ such that for each $h\\in[H]$ , we define the transition as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{T}_{h}^{\\mathrm{trunc}}(s_{h+1}\\,|\\,s_{h},a_{h}):=\\mathbb{T}_{h}(s_{h+1}\\,|\\,s_{h},a_{h})}\\\\ &{\\phantom{m m m m m m}+\\frac{\\sum_{s_{h+1}^{\\prime}\\in S_{h+1}^{\\mathrm{ov}}}\\mathbb{T}_{h}(s_{h+1}^{\\prime}\\,|\\,s_{h},a_{h})}{|S_{h+1}^{\\mathrm{inp}}|},\\ \\ \\forall s_{h}\\in S,s_{h+1}\\in S_{h+1}^{\\mathrm{hgh}},a_{h}\\in\\mathcal{A},}\\\\ &{\\mathbb{T}_{h}^{\\mathrm{trunc}}(s_{h+1}\\,|\\,s_{h},a_{h}):=0,\\ \\ \\forall s_{h}\\in\\mathcal{S},s_{h+1}\\in\\mathcal{S}_{h+1}^{\\mathrm{low}},a_{h}\\in\\mathcal{A}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Meanwhile, for the initial state distribution, we define ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{1}^{\\mathrm{trunc}}(s_{1}):=\\mu_{1}(s_{1})+\\frac{\\sum_{s_{1}^{\\prime}\\in S_{1}^{\\mathrm{low}}}\\mu_{1}\\left(s_{1}^{\\prime}\\right)}{\\vert S_{1}^{\\mathrm{high}}\\vert},\\ \\ \\forall s_{1}\\in S_{1}^{\\mathrm{high}},}\\\\ &{\\mu_{1}^{\\mathrm{trunc}}(s_{1}):=0,\\ \\ \\forall s_{1}\\in S_{1}^{\\mathrm{low}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "For emission, we simply define ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{O}_{h}^{\\mathrm{trunc}}(o_{h}\\,|\\,s_{h}):=\\mathbb{O}_{h}(o_{h}\\,|\\,s_{h}),\\ \\ \\forall h\\in[H],s_{h}\\in S,o_{h}\\in\\mathcal{O}_{h}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Finally, we define the rewards for $\\mathcal{P}^{\\mathrm{trunc}}$ arbitrarily. Now we examine the total variation distance between the trajectory distributions in $\\mathcal{P}$ and $\\mathcal{P}^{\\mathrm{trunc}}$ . For any policy $\\pi\\in\\Pi^{\\mathrm{gen}}$ , it is easy to see that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\pi,\\mathcal{P}}(\\overline{{\\tau}}_{h})\\leq\\mathbb{P}^{\\pi,\\mathcal{P}^{\\mathrm{trunc}}}(\\overline{{\\tau}}_{h}),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for any $\\overline{{\\tau}}_{h}\\in\\overline{{T}}_{h}^{\\mathrm{high}}:=\\{(s_{1:h},o_{1:h},a_{1:h-1})\\,|\\,s_{h}^{\\prime}\\in S_{h^{\\prime}}^{\\mathrm{high}},\\forall h^{\\prime}\\in[h]\\}.$ , since some rarely visited states\u2019 probability has been redirected to the highly visited ones in $\\mathcal{P}^{\\mathrm{trunc}}$ . Meanwhile, it holds by a union bound that for any $h\\in[H]$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\pi,\\mathcal{P}}(\\overline{{\\tau}}_{h}\\not\\in\\overline{{\\mathcal{T}}}_{h}^{\\mathrm{high}})\\leq\\sum_{h^{\\prime}\\in[h]}\\mathbb{P}^{\\pi,\\mathcal{P}}(s_{h^{\\prime}}\\in S_{h^{\\prime}}^{\\mathrm{low}})\\leq H S\\epsilon_{1}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Therefore, by noticing that $\\mathbb{P}^{\\pi,\\mathcal{P}^{\\mathrm{trunc}}}(\\overline{{\\tau}}_{h})=0$ for any $\\overline{{\\tau}}_{h}\\notin\\overline{{\\tau}}_{h}^{\\mathrm{high}}$ and $h\\in[H]$ , the total variation distance between the trajectory distributions in $\\mathcal{P}$ and $\\mathcal{P}^{\\mathrm{trunc}}$ can be bounded by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{\\overline{{\\tau}}_{h}}|\\mathbb{P}^{\\pi,\\mathcal{P}}(\\overline{{\\tau}}_{h})-\\mathbb{P}^{\\pi,\\mathcal{P}^{\\mathrm{tunc}}}(\\overline{{\\tau}}_{h})|\\leq2H S\\epsilon_{1}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "On the other hand, by Equation (H.10) of Lemma H.9, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}^{\\mathcal{P}}[\\|b_{h}(\\tau_{h})-b_{h}^{\\mathrm{tunc}}(\\tau_{h})\\|_{1}]\\le2\\sum_{\\overline{{\\tau}}_{h}}|\\mathbb{P}^{\\pi,\\mathcal{P}}(\\overline{{\\tau}}_{h})-\\mathbb{P}^{\\pi,\\mathcal{P}^{\\mathrm{tunc}}}(\\overline{{\\tau}}_{h})|\\le4H S\\epsilon_{1}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "With such an intermediate quantity $\\mathcal{P}^{\\mathrm{trunc}}$ , we define the transition of its approximate version $\\widehat{\\mathcal{P}}^{\\mathrm{trunc}}$ as follows: we define the transition as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbb{T}}_{h}^{\\mathrm{trunc}}(s_{h+1}\\,|\\,s_{h},a_{h}):=\\widehat{\\mathbb{T}}_{h}(s_{h+1}\\,|\\,s_{h},a_{h})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\frac{\\sum_{s_{h+1}^{\\prime}\\in S_{h+1}^{\\mathrm{lor}}}\\widehat{\\mathbb{T}}_{h}(s_{h+1}^{\\prime}\\,|\\,s_{h},a_{h})}{|S_{h+1}^{\\mathrm{high}}|},\\ \\ \\forall s_{h}\\in S,s_{h+1}\\in S_{h+1}^{\\mathrm{high}},a_{h}\\in A,}\\\\ &{\\widehat{\\mathbb{T}}_{h}^{\\mathrm{trunc}}(s_{h+1}\\,|\\,s_{h},a_{h}):=0,\\ \\ \\forall s_{h}\\in\\mathcal{S},s_{h+1}\\in\\mathcal{S}_{h+1}^{\\mathrm{low}},a_{h}\\in A.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Meanwhile, for the initial state distribution, we define ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mu}_{1}^{\\mathrm{tunc}}(s_{1}):=\\widehat{\\mu}_{1}\\big(s_{1}\\big)+\\frac{\\sum_{s_{1}^{\\prime}\\in S_{1}^{\\mathrm{low}}}\\widehat{\\mu}_{1}\\big(s_{1}^{\\prime}\\big)}{\\vert S_{1}^{\\mathrm{high}}\\vert},\\ \\ \\forall s_{1}\\in S_{1}^{\\mathrm{high}},}\\\\ &{\\widehat{\\mu}_{1}^{\\mathrm{tunc}}(s_{1}):=0,\\ \\ \\forall s_{1}\\in S_{1}^{\\mathrm{low}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For emission, we define ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{O}}_{h}^{\\mathrm{trunc}}(o_{h}\\,|\\,s_{h}):=\\widehat{\\mathbb{O}}_{h}(o_{h}\\,|\\,s_{h}),\\quad\\forall h\\in[H],s_{h}\\in S,o_{h}\\in\\mathcal{O}_{h}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Now we define $\\widehat{\\mathbb{O}}_{h}^{\\mathrm{sub}}\\in\\mathbb{R}^{|S_{h}^{\\mathrm{high}}|\\times O}$ to be the sub-matrix of $\\widehat{\\mathbb{O}}_{h}^{\\mathrm{trunc}}$ , where we only keep those rows ofO thruncthat correspond to the states in Shhig . Similarly,  we define $\\mathbb{O}_{h}^{\\mathrm{sub}}\\,\\in\\,\\mathbb{R}^{|S_{h}^{\\mathrm{high}}|\\,\\times\\,O}$ to be the sub-matrix of $\\mathbb{O}_{h}$ , where we only keep those rows of $\\mathbb{O}_{h}$ that correspond to the states in Shigh. It is direct to see that $\\mathbb{O}^{\\mathrm{sub}}$ is still $\\gamma$ -observable. Meanwhile, we notice that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\mathbb{O}}_{h}^{\\mathrm{sub}}-\\mathbb{O}_{h}^{\\mathrm{sub}}\\|_{\\infty}=\\underset{s_{h}\\in S_{h}^{\\mathrm{Mgh}}}{\\operatorname*{max}}\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\underset{s_{h}\\in S_{h}^{\\mathrm{Mgh}}}{\\operatorname*{max}}\\sqrt{\\frac{O\\log(S H/\\delta)}{N_{h}(s_{h})}}\\leq\\underset{s_{h}\\in S_{h}^{\\mathrm{Mgh}}}{\\operatorname*{max}}\\sqrt{\\frac{2O\\log(S H/\\delta)}{N\\epsilon_{1}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the first inequality is by Lemma J.9, and the second inequality is by the definition of $\\ensuremath{\\boldsymbol{S}}_{h}^{\\mathrm{high}}$ Therefore, if we take ", "page_idx": 42}, {"type": "equation", "text": "$$\nN\\geq\\frac{8O\\log(S H/\\delta)}{\\gamma^{2}\\epsilon_{1}},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "it is guaranteed that $\\begin{array}{r}{\\|\\widehat{\\mathbb{O}}_{h}^{\\mathrm{sub}}-\\mathbb{O}_{h}^{\\mathrm{sub}}\\|_{\\infty}\\leq\\frac{\\gamma}{2}}\\end{array}$ . Therefore, we conclude that $\\widehat{\\mathbb{O}}_{h}^{\\mathrm{sub}}$ is also $\\gamma/2$ -observable. Now we are ready to examine $\\widehat{b}_{h}^{\\prime,\\mathrm{trunc}}$ . We firstly define the following POMDP $\\widehat{\\mathcal{P}}^{\\mathrm{sub}}$ , which essentially deletes all states in $S_{h}^{\\mathrm{low}}$ from the state space of $\\widehat{\\mathcal{P}}^{\\mathrm{trunc}}$ at each step $h$ , which does not affect the trajectory distribution as they were not reachable in $\\widehat{\\mathcal{P}}^{\\mathrm{trunc}}$ . Notice that the emission of $\\widehat{\\mathcal{P}}^{\\mathrm{sub}}$ is exactly $\\widehat{\\mathbb{O}}_{h}^{\\mathrm{sub}}$ , implying that $\\widehat{\\mathcal{P}}^{\\mathrm{sub}}$ is a $\\gamma/2$ -observable POMD P. Therefore, for policy class with  finite memory $\\Pi^{L}$ with $L\\ge\\widetilde\\Omega(\\gamma^{-4}\\log(S/\\epsilon)$ , by Theorem 4.1 in [25], it is guaranteed that for any $\\pi\\in\\Pi$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}^{\\widehat{\\mathcal{P}}^{\\mathrm{sub}}}\\lVert\\widehat{b}_{h}^{\\mathrm{sub}}(\\tau_{h})-\\widehat{b}_{h}^{\\prime,\\mathrm{sub}}(z_{h})\\rVert_{1}\\le\\epsilon,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\widehat{b}_{h}^{\\mathrm{sub}}(\\tau_{h}),\\widehat{b}_{h}^{\\prime,\\mathrm{sub}}(z_{h})\\in\\Delta(S_{h}^{\\mathrm{high}})$ . Now we claim that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pi}^{\\widehat{\\mathcal{P}}^{\\mathrm{trunc}}}\\|\\widehat{\\boldsymbol{b}}_{h}^{\\mathrm{trunc}}(\\tau_{h})-\\boldsymbol{b}_{h}^{\\mathrm{apx}}(\\boldsymbol{z}_{h})\\|_{1}\\le\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where we define $b_{h}^{\\mathrm{apx}}(z_{h})~\\in~\\Delta(S)$ by augmenting $\\widehat{b}_{h}^{\\prime,\\mathrm{sub}}(z_{h})$ with 0 for states from $S_{h}^{\\mathrm{low}}$ . To see the reason, we notice that simulating $\\widehat{\\mathcal{P}}^{\\mathrm{trunc}}$ is exactly equivalent to simulating $\\widehat{\\mathcal{P}}^{\\mathrm{sub}}$ , and that $\\widehat{b}_{h}^{\\mathrm{trunc}}(\\tau_{h})(s_{h})=0$ for $s_{h}\\in\\ensuremath{S_{h}^{\\mathrm{low}}}$ $\\begin{array}{r}{{\\underline{{\\mathrm{low}}}},\\,\\widehat{b}_{h}^{\\mathrm{trunc}}(\\bar{\\tau_{h}})(s_{h})=\\widehat{b}_{h}^{\\mathrm{sub}}(\\bar{\\tau_{h}})(s_{h}^{\\;\\bullet})}\\end{array}$ for $s_{h}\\in\\ensuremath{\\boldsymbol{S}}_{h}^{\\mathrm{high}}$ . ", "page_idx": 43}, {"type": "text", "text": "For the total variation distance between the trajectory distributions in $\\mathcal{P}^{\\mathrm{trunc}}$ and $\\widehat{\\mathcal{P}}^{\\mathrm{trunc}}$ , it holds that by Lemma H.9 ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\overline{{\\tau}}_{H}}|\\mathbb{P}^{\\pi,\\mathcal{P}^{\\mathrm{tunc}}}(\\overline{{\\tau}}_{H})-\\mathbb{P}^{\\pi,\\widehat{\\mathcal{P}}^{\\mathrm{tunc}}}(\\overline{{\\tau}}_{H})|}\\\\ &{\\le\\mathbb{E}_{\\pi}^{\\mathcal{P}^{\\mathrm{tunc}}}\\displaystyle\\sum_{h\\in[H]}\\|\\mathbb{T}_{h}^{\\mathrm{tunc}}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}^{\\mathrm{tunc}}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}+\\|\\mathbb{O}_{h}^{\\mathrm{tunc}}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}^{\\mathrm{tunc}}(\\cdot\\,|\\,s_{h})\\|_{1}}\\\\ &{\\le\\displaystyle\\sum_{h\\in[H]}\\mathbb{E}_{\\pi}^{\\mathcal{P}^{\\mathrm{tunc}}}\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}+\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the last step is by Lemma H.7. ", "page_idx": 43}, {"type": "text", "text": "Now by Equation (H.5), we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{h}\\mathbb{E}_{\\pi}^{\\mathcal{P}^{\\mathrm{runc}}}\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}+\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}}\\\\ &{\\displaystyle=\\sum_{h}4H S\\epsilon_{1}+\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}+\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}}\\\\ &{\\displaystyle\\leq\\frac{\\epsilon}{3}+4H^{2}S\\epsilon_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the last step is by Theorem H.5. Hence, by Lemma H.9, it holds that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{P}^{\\pi,\\mathcal{P}^{\\mathrm{tunc}}}-\\mathbb{P}^{\\pi,\\widehat{\\mathcal{P}}^{\\mathrm{tunc}}}\\|_{1}\\leq\\frac{\\epsilon}{3}+4H^{2}S\\epsilon_{1},}\\\\ &{\\mathbb{E}_{\\pi}^{\\mathcal{P}^{\\mathrm{tunc}}}\\|b_{h}^{\\mathrm{trunc}}(\\tau_{h})-\\widehat{b}_{h}^{\\mathrm{trunc}}(\\tau_{h})\\|_{1}\\leq\\frac{2\\epsilon}{3}+8H^{2}S\\epsilon_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Finally, we are ready to prove ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\|b_{h}(\\tau_{h})-b_{h}^{\\scriptscriptstyle\\mathrm{apx}}(z_{h})\\|_{1}}\\\\ &{\\le\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\|b_{h}(\\tau_{h})-b_{h}^{\\scriptscriptstyle\\mathrm{tme}}(\\tau_{h})\\|_{1}+\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\|b_{h}^{\\scriptscriptstyle\\mathrm{tme}}(\\tau_{h})-\\widehat b_{h}^{\\scriptscriptstyle\\mathrm{tme}}(\\tau_{h})\\|_{1}+\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\|\\widehat b_{h}^{\\scriptscriptstyle\\mathrm{tme}}(\\tau_{h})-b_{h}^{\\scriptscriptstyle\\mathrm{apx}}(z_{h})\\|_{1}}\\\\ &{\\le\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\|b_{h}(\\tau_{h})-b_{h}^{\\scriptscriptstyle\\mathrm{tme}}(\\tau_{h})\\|_{1}+\\mathbb{E}_{\\pi}^{\\mathcal{P}^{\\mathrm{tme}}}\\|b_{h}^{\\scriptscriptstyle\\mathrm{tme}}(\\tau_{h})-\\widehat b_{h}^{\\scriptscriptstyle\\mathrm{tme}}(\\tau_{h})\\|_{1}+\\mathbb{E}_{\\pi}^{\\mathcal{P}^{\\mathrm{tme}}}\\|\\widehat b_{h}^{\\scriptscriptstyle\\mathrm{tme}}(\\tau_{h})-b_{h}^{\\scriptscriptstyle\\mathrm{apx}}(z_{h})\\|_{1}}\\\\ &{\\qquad+4\\|\\mathbb{E}^{\\pi,\\mathcal{P}}-\\mathbb{P}^{\\pi,\\mathcal{P}^{\\mathrm{tme}}}\\|_{1}+2\\|\\mathbb{P}^{\\pi,\\mathcal{P}^{\\mathrm{tme}}}-\\mathbb{P}^{\\pi,\\widehat\\mathcal{P}^{\\mathrm{tme}}}\\|_{1}}\\\\ &{\\le O(H^{2}S\\epsilon_{1})+O(\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Therefore, by setting $\\begin{array}{r}{\\epsilon_{1}=\\Theta\\left(\\frac{\\epsilon}{H^{2}S}\\right)}\\end{array}$ , we prove our lemma. Observe that our algorithm needed no further samples. The computational complexity follows by computing belief $b_{h}^{\\mathrm{apx}}$ on POMDP $\\widehat{\\mathcal{P}}^{\\mathrm{sub}}$ using finite-memory policies of size $\\widetilde{\\Theta}(\\gamma^{-4}\\log(S/\\epsilon))$ . For the final sample complexity, we only need to ensure $\\begin{array}{r}{N=\\widetilde\\Theta(\\frac{O\\log(S H/\\delta)}{\\gamma^{2}\\epsilon_{1}})}\\end{array}$ i n Equation (H.4), thus concluding our proof. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "We conclude the proof of Theorem 5.2 by combining Theorem H.5 and Theorem H.6. ", "page_idx": 43}, {"type": "text", "text": "H.1 Supporting Technical Lemmas ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In the following, we provide some technical lemmas and their proofs. ", "page_idx": 43}, {"type": "text", "text": "Lemma H.7. Fix $n\\,>\\,0$ and an index set $S\\subseteq[n]$ . For two sequences $x_{1:n}$ and $y_{1:n}$ such that $x_{i},y_{i}\\in[0,1]$ for $i\\in[n]$ and $\\begin{array}{r}{\\sum_{i}x_{i}=1,\\sum_{i}y_{i}=\\dot{1}}\\end{array}$ , we define ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widehat{x}_{i}=x_{i}+\\frac{\\sum_{j\\in S}x_{j}}{n-|S|},\\quad\\forall i\\notin S,}\\\\ {\\displaystyle\\widehat{x}_{i}=0,\\quad\\forall i\\in S.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We define $\\widehat{y}_{1:n}$ similarly. Then, it holds that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{i}|x_{i}-y_{i}|\\geq\\sum_{i}|{\\widehat{x}}_{i}-{\\widehat{y}}_{i}|.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. Note that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i}|\\widehat{x}_{i}-\\widehat{y}_{i}|=\\sum_{i\\notin S}|\\widehat{x}_{i}-\\widehat{y}_{i}|}\\\\ {\\displaystyle=\\sum_{i\\notin S}\\left|x_{i}+\\frac{\\sum_{j\\in S}x_{j}}{n-|S|}-y_{i}-\\frac{\\sum_{j\\in S}y_{j}}{n-|S|}\\right|}\\\\ {\\displaystyle\\leq\\left(n-|S|\\right)\\left|\\frac{\\sum_{j\\in S}x_{j}}{n-|S|}-\\frac{\\sum_{j\\in S}y_{j}}{n-|S|}\\right|+\\sum_{i\\notin S}|x_{i}-y_{i}|}\\\\ {\\displaystyle\\leq\\sum_{i}|x_{i}-y_{i}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "which completes the proof. ", "page_idx": 44}, {"type": "text", "text": "Lemma H.8. Fix two finite sets $\\mathcal{X},\\mathcal{Y}$ and two joint distributions $P_{1},P_{2}\\in\\Delta(\\mathcal{X}\\times\\mathcal{Y})$ . It holds that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{-\\mathbb{E}_{P_{1}(x)}\\sum_{y}|P_{1}(y\\,|\\,x)-P_{2}(y\\,|\\,x)|\\leq\\sum_{x,y}|P_{1}(x,y)-P_{2}(x,y)|-\\sum_{x}|P_{1}(x)-P_{2}(x)|}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{E}_{P_{1}(x)}\\sum_{y}|P_{1}(y\\,|\\,x)-P_{2}(y\\,|\\,x)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. For the second inequality, it holds that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\lefteqn{\\sum_{x,y}|P_{1}(x,y)-P_{2}(x,y)|=\\sum_{x,y}|P_{1}(x,y)-P_{1}(x)P_{2}(y\\,|\\,x)+P_{1}(x)P_{2}(y\\,|\\,x)-P_{2}(x,y)|}}\\\\ &{}&{\\leq\\displaystyle\\sum_{x,y}|P_{1}(x,y)-P_{1}(x)P_{2}(y\\,|\\,x)|+|P_{1}(x)P_{2}(y\\,|\\,x)-P_{2}(x,y)|}\\\\ &{}&{=\\displaystyle\\sum_{x,y}|P_{1}(x)(P_{1}(y\\,|\\,x)-P_{2}(y\\,|\\,x))|+|P_{2}(y\\,|\\,x)(P_{1}(x)-P_{2}(x))|}\\\\ &{}&{=\\mathbb{E}_{P_{1}(x)}\\displaystyle\\sum_{y}|P_{1}(y\\,|\\,x)-P_{2}(y\\,|\\,x)|+\\displaystyle\\sum_{x}|P_{1}(x)-P_{2}(x)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Meanwhile, for the first inequality, it holds that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{x,y}|P_{1}(x,y)-P_{2}(x,y)|=\\displaystyle\\sum_{x,y}|P_{1}(x,y)-P_{1}(x)P_{2}(y\\,|\\,x)+P_{1}(x)P_{2}(y\\,|\\,x)-P_{2}(x,y)|}&{}\\\\ {\\displaystyle\\geq\\displaystyle\\sum_{x,y}-|P_{1}(x,y)-P_{1}(x)P_{2}(y\\,|\\,x)|+|P_{1}(x)P_{2}(y\\,|\\,x)-P_{2}(x,y)|}&{}\\\\ {\\displaystyle=\\displaystyle\\sum_{x,y}-|P_{1}(x)(P_{1}(y\\,|\\,x)-P_{2}(y\\,|\\,x))|+|P_{2}(y\\,|\\,x)(P_{1}(x)-P_{2}(x))|}&{}\\\\ {\\displaystyle=\\mathbb{E}_{P_{1}(x)}-\\displaystyle\\sum_{y}|P_{1}(y\\,|\\,x)-P_{2}(y\\,|\\,x)|+\\displaystyle\\sum_{x}|P_{1}(x)-P_{2}(x)|,}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "concluding our lemma. ", "page_idx": 44}, {"type": "text", "text": "Lemma H.9. Consider any two POMDP instances $\\mathcal{P}$ and $\\widehat{\\mathcal{P}}$ and define the belief functions as $b_{1:H}$ and $\\widehat{\\pmb{b}}_{1:H}$ , respectively (see Appendix C for the definition of belief function $b_{1:H}$ ). It holds that for any  \u03c0 \u2208\u03a0gen ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Big\\|\\mathbb{P}^{\\pi,\\mathcal{P}}-\\mathbb{P}^{\\pi,\\widehat{\\mathcal{P}}}\\Big\\|_{1}=\\sum_{\\tau_{H}}|\\mathbb{P}^{\\pi,\\mathcal{P}}(\\overline{{\\tau}}_{H})-\\mathbb{P}^{\\pi,\\widehat{\\mathcal{P}}}(\\overline{{\\tau}}_{H})|}\\\\ &{\\qquad\\le\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\sum_{h\\in[H-1]}\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}+\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\sum_{h\\in[H]}\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1},}\\\\ &{\\displaystyle\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\|b_{H}(\\tau_{H})-\\widehat{b}_{H}(\\tau_{H})\\|_{1}}\\\\ &{\\qquad\\le2\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\sum_{h\\in[H-1]}\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}+2\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\sum_{h\\in[H]}\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where we remind readers that we denote by $\\overline{{\\tau}}_{H}=\\left(s_{1:H},o_{1:H},a_{1:H-1}\\right)$ the trajectory with states from an episode of the POMDP, and without loss of generality, we assume $\\widehat{\\mathbb{T}}_{H}=\\mathbb{T}_{H}$ for notational convenience, since one can assume the episode ends deterministically at a  state $s H{+}1$ . ", "page_idx": 45}, {"type": "text", "text": "Proof. The first inequality also appears in [43] and we provide a simplified proof here for completeness. By Lemma H.8, it holds that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{\\tau\\:H}\\big|\\mathbb{P}^{\\pi,\\mathcal{P}}(\\overline{\\tau}H)-\\mathbb{P}^{\\pi,\\widehat{\\mathcal{P}}}(\\overline{\\tau}H)\\big|\\leq\\sum_{\\tau\\:H=1}\\big|\\mathbb{P}^{\\pi,\\mathcal{P}}(\\overline{\\tau}H_{H-1})-\\mathbb{P}^{\\pi,\\widehat{\\mathcal{P}}}(\\overline{\\tau}H_{H-1})\\big|}}\\\\ &{\\quad\\quad+\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\sum_{a_{H-1},s_{H,O\\pi}}\\Bigg|\\pi_{H-1}(a_{h-1}|\\,\\overline{\\tau}_{H-1})\\mathbb{T}_{H-1}(s_{H}\\,|\\,s_{H-1},a_{H-1})\\mathbb{O}_{H}(o_{H}\\,|\\,s_{H})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad-\\pi_{H-1}(a_{h-1}\\,|\\,\\overline{\\tau}_{H-1})\\widehat{\\mathbb{T}}_{H-1}(s_{H}\\,|\\,s_{H-1},a_{H-1})\\widehat{\\Theta}_{H}(o_{H}\\,|\\,s_{H})\\Bigg|}\\\\ &{\\leq\\sum_{\\eta\\:H=1}\\big|\\mathbb{P}^{\\pi,\\mathcal{P}}(\\overline{\\tau}_{H-1})-\\mathbb{P}^{\\pi,\\widehat{\\mathcal{P}}}(\\overline{\\tau}_{H-1})\\big|+\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\big|\\mathbb{T}_{H-1}(\\cdot\\,|\\,s_{H-1},a_{H-1})-\\widehat{\\mathbb{T}}_{H-1}(\\cdot\\,|\\,s_{H-1},a_{H-1})\\big|\\|_{1}}\\\\ &{\\quad\\quad\\quad+\\|\\mathbb{O}_{H}(\\cdot\\,|\\,s_{H})-\\widehat{\\mathbb{Q}}_{H}(\\cdot\\,|\\,s_{H})\\|_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the last step is again from Lemma H.8. Therefore, by rolling out the inequality repeatedly, we proved the first result. ", "page_idx": 45}, {"type": "text", "text": "For the second result, we notice that by Lemma H.8, it holds that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau_{h},s_{h}}|\\mathbb{P}_{h}^{\\pi,\\mathcal{P}}(\\tau_{h},s_{h})-\\mathbb{P}_{h}^{\\pi,\\widehat{\\mathcal{P}}}(\\tau_{h},s_{h})|}\\\\ &{\\displaystyle\\quad\\ge-\\displaystyle\\sum_{\\tau_{h}}|\\mathbb{P}_{h}^{\\pi,\\mathcal{P}}(\\tau_{h})-\\mathbb{P}_{h}^{\\pi,\\widehat{\\mathcal{P}}}(\\tau_{h})|+\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\displaystyle\\sum_{s_{h}}|\\mathbb{P}_{h}^{\\pi,\\mathcal{P}}(s_{h}\\,|\\,\\tau_{h})-\\mathbb{P}_{h}^{\\pi,\\widehat{\\mathcal{P}}}(s_{h}\\,|\\,\\tau_{h})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Notice the fact that $\\mathbb{P}_{h}^{\\pi,\\mathcal{P}}(s_{h}\\,|\\,\\tau_{h})$ does not depend on $\\pi$ since it is exactly $b_{h}(\\tau_{h})(s_{h})$ , we conclude that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{\\pi}^{\\mathcal{P}}\\|b_{h}(\\tau_{h})-\\widehat{b}_{h}(\\tau_{h})\\|_{1}\\leq\\sum_{\\tau_{h},s_{h}}|\\mathbb{P}_{h}^{\\pi,\\mathcal{P}}(\\tau_{h},s_{h})-\\mathbb{P}_{h}^{\\pi,\\widehat{\\mathcal{P}}}(\\tau_{h},s_{h})|+\\sum_{\\tau_{h}}|\\mathbb{P}_{h}^{\\pi,\\mathcal{P}}(\\tau_{h})-\\mathbb{P}_{h}^{\\pi,\\widehat{\\mathcal{P}}}(\\tau_{h})|}}\\\\ &{}&{\\leq2\\displaystyle\\sum_{\\overline{\\tau}_{H}}|\\mathbb{P}^{\\pi,\\mathcal{P}}(\\overline{\\tau}_{h})-\\mathbb{P}^{\\pi,\\widehat{\\mathcal{P}}}(\\overline{\\tau}_{h})|,}\\end{array}{\\quad\\mathrm{(H.~}~}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the last step comes from the fact that after marginalization, the total variation distance will not increase. By combining it with the first result, we proved the second result. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "Lemma H.10. For any reward function $r_{1:H}$ of $\\mathcal{P}$ with $r_{h}:S\\times A\\to\\mathbb{R}$ for any $h\\in[H]$ , it holds that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi\\in\\Pi^{\\mathrm{gen}}}v^{\\mathcal{P}}(\\pi)\\leq\\operatorname*{max}_{\\pi\\in\\Pi_{\\mathcal{S}}}v^{\\mathcal{P}}(\\pi).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "table", "img_path": "o3i1JEfzKw/tmp/8f7535e5ebb1d832d1c0ede97fb731aaee098295aa0146eb738ba858c880635a.jpg", "table_caption": ["Table 2: Rewards of different approaches for POMDPs under the deterministic filter condition. "], "table_footnote": [], "page_idx": 46}, {"type": "text", "text": "Proof. Denote $\\pi^{\\star}\\in\\Pi_{\\cal{S}}$ to be the optimal policy obtained by running value iteration only on the state space $\\boldsymbol{S}$ for $\\mathcal{P}$ . Now we are ready to prove the following argument for any $\\pi\\in\\Pi^{\\mathrm{gen}}$ inductively: ", "page_idx": 46}, {"type": "equation", "text": "$$\n{Q_{h}^{\\pi^{\\star},\\mathcal{P}}(s_{h},a_{h})\\geq Q_{h}^{\\pi,\\mathcal{P}}(s_{1:h},o_{1:h},a_{1:h}).}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "It is easy to see the argument holds for $h=H$ . Fix state-action pair $\\left(s_{h},a_{h}\\right)\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ and trajectory $\\left(s_{1:h-1},o_{1:h},a_{1:h-1}\\right)$ , we note that: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{Q}_{h}^{\\pi^{*,\\mathcal{P}}}(s_{h},a_{h})}\\\\ &{\\quad=r_{h}(s_{h},a_{h})+\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\mid s_{h},a_{h})}\\left[\\!\\operatorname*{max}_{a_{h+1}}\\!\\!(g_{h+1}^{\\pi^{*,\\mathcal{P}}}(s_{h+1},a_{h+1})\\!\\right]}\\\\ &{\\quad\\ge r_{h}(s_{h},a_{h})+\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\mid s_{h},a_{h})},\\left[\\!\\operatorname*{max}_{a_{h+1}}\\!\\!Q_{h+1}^{\\pi,\\mathcal{P}}\\!(s_{1:h+1},o_{1:h+1},a_{1:h+1})\\!\\right]}\\\\ &{\\quad\\ge r_{h}(s_{h},a_{h})}\\\\ &{\\quad\\quad\\ +\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\mid s_{h},a_{h})},\\left[\\!\\operatorname*{max}_{a_{h+1}\\sim\\pi_{h+1}(\\cdot\\mid s_{1:h+1},o_{1:h+1},a_{1:h+1},a_{1:h+1})}\\!\\right]}\\\\ &{\\quad\\quad\\quad\\ge\\!\\!\\!\\!\\!\\alpha_{h+1}\\!\\sim\\!\\!\\!\\!\\operatorname{Q}_{h+1}\\!\\!\\!\\sim\\!\\!\\!(\\cdot\\!\\mid s_{h+1}\\!\\sim\\!\\pi_{h+1}(\\cdot\\mid s_{1:h+1},\\!\\!\\dots\\!\\mid\\!s_{1:h+1},\\!\\!\\dots\\!\\mid\\!0_{1:h+1},\\!\\!\\dots\\!0_{1:h+1},a_{1:h+1})\\!\\right]}\\\\ &{\\quad\\quad\\quad\\ \\ \\:\\sigma_{h+1}^{\\pi^{*}}\\!\\sim\\!\\!\\!(\\!\\!x_{h},a_{h+1},\\!\\!\\sim\\!\\!\\!\\lim_{h+1}\\!\\cdot\\!\\mid s_{h+1}\\!\\sim\\!\\pi_{h+1}(\\cdot\\mid s_{1:h+1},\\!\\!\\dots\\!\\mid\\!0_{1:h+1},\\!\\!\\dots\\!\\mid\\!0_{1:h+1},\\!\\!\\dots\\!\\mid\\!0_{1:h+1},a_{1:h+1})\\!\\!\\right]}\\\\ &{\\quad\\quad\\quad\\ =Q_{h}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the first inequality comes from the inductive hypothesis. ", "page_idx": 46}, {"type": "text", "text": "I Missing Details in Section 6 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "POMDP under deterministic fliter condition. We first evaluate our algorithms on POMDPs with certain structures, i.e., the deterministic conditions. In particular, we generate POMDPs, where either the transition dynamics are deterministic or the emission ensures decodability. We test three of our approaches, expert policy distillation, asymmetric optimistic natural policy gradient. We summarize our results in Table 2, where the four cases corresponds to POMDPs with $\\langle S=A=2,O=3,H=$ 5), $^{'}S=A=2,O=3,H=10)$ , $(S=3,A=2,O=4,H=5)$ , $(S=3,A=2,O=4,H=5)$ , and we can see that our approach based on expert distillation outperforms all the other methods, which is consistent with the fact that such methods have exploited the special structures of the POMDPs achieving both polynomial sample and computational complexity. ", "page_idx": 46}, {"type": "text", "text": "General POMDPs. Here we also evaluate our methods for general randomly generated POMDPs without any structures. Hence, we compare the baselines with asymmetric optimistic natural policy gradient and asymmetric optimistic value iteration (i.e., the single-agent version of Algorithm 5). In Figure 2, we show the performance of different algorithms in POMDPs of different size, where the four cases corresponds to POMDPs with $^{'}S=A=O=2,H=5)$ , $^{'}S=A=O=2,H=10^{`}$ , $^{\\prime}S=O=3,A=2,H=10)$ , $\\langle S=A=3,O=2,H=10\\rangle$ ), and our approaches achieves the highest rewards with small number of episodes. ", "page_idx": 46}, {"type": "text", "text": "Implementation details. For each problem setting, we generated 20 POMDPs randomly and report the average performance and its standard deviation for each algorithms. For our algorithms based on privileged value learning methods, we find that using a finite memory of 3 already provides strong performance. For our algorithms based privileged policy learning, we instantiate the MDP learning algorithm with the fully observable optimistic natural policy gradient algorithm [73]. Meanwhile, for both the decoder learning and belief learning, we find that just utilizing all the historic trajectories gives us reasonable performance without additional samples. For baselines, the hyperparameters $\\alpha$ for $Q$ -value update and step size for the policy update are tuned by grid search, where $\\alpha$ controls the update of temporal difference learning (recall the update rule of temporal difference learning as $Q\\leftarrow(1-\\alpha)Q+\\alpha Q^{\\mathrm{target}})$ . For asymmetric $Q$ -learning, we use $\\epsilon$ -greedy exploration, where we use the seminal decreasing rate $\\begin{array}{r}{\\epsilon_{t}=\\frac{H+1}{H+t}}\\end{array}$ . Finally, all simulations are conducted on a personal laptop with Apple M1 CPU and 16 GB memory. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "Empirical insights and interpretation of the experimental results. To understand intuitively why our approach outperforms those baseline algorithms, we notice the key difference in the value and policy update style between our approaches and vanilla asymmetric actor critic and asymmetric $Q$ -learning. The baselines often roll-out the polices, collect trajectories, and only update the value and the policies on the states/history the trajectories have visited. Therefore, ideally, to learn a good policy for baselines, the number of trajectories to collect is at least as large as the history size, which is indeed exponential in the horizon $H$ . This is known as curse of history for partially observable RL. In contrast, in our algorithms, we explicitly estimate the empirical transition and emissions, which is indeed of polynomial size. Thus, the sample complexity avoids suffering from the potential exponential dependency of horizon or the length of the finite memory. Finally, we acknowledge that the baselines are developed to handle complex, high-dimensional deep RL problems, while scaling our methods to deep RL benchmarks requires non-trivial engineering efforts. ", "page_idx": 47}, {"type": "text", "text": "J Missing Details in Section 7 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Proof of Proposition 7.1: ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Given the condition of Definition 3.2, the function $\\phi_{h}$ that satisfies the condition of Proposition 7.1 can be constructed recursively as follows for any reachable $\\tau_{h}$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi_{h}(\\tau_{h}):=\\psi_{h}(\\phi_{h-1}(\\tau_{h-1}),a_{h-1},o_{h}),\\forall\\tau_{h}\\in\\mathcal{T}_{h-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and $\\phi_{1}(o_{1}):=\\psi_{1}(o_{1})$ . Therefore, we can prove by induction that by belief update rule ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\mathcal{P}}(s_{h}\\,|\\,\\tau_{h})=U_{h}(b^{\\phi_{h-1}(\\tau_{h-1})};a_{h-1},o_{h})=b^{\\psi_{h}(\\phi_{h-1}(\\tau_{h-1}),a_{h-1},o_{h})},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where the last step is by the definition of our $\\phi_{h}$ . Therefore, we have $\\begin{array}{r l}{\\mathbb{P}^{\\mathcal{P}}(s_{h}}&{{}=}\\end{array}$ $\\psi_{h}(\\phi_{h-1}(\\tau_{h-1}),a_{h-1},o_{h})\\:|\\:\\dot{\\tau}_{h})=1$ . ", "page_idx": 47}, {"type": "text", "text": "For the other direction, it is similar to the proof of Lemma C.1 in [19] for $m$ -step decodable POMDP. Here we prove it for completeness. For any reachable trajectory $\\tau_{h}\\in\\tau_{h}$ , it holds by the belief update and induction that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\mathcal{P}}(s_{h}\\,|\\,\\tau_{h})=U_{h}(b^{\\phi_{h-1}(\\tau_{h-1})},a_{h-1},o_{h})=\\mathbb{P}^{\\mathcal{P}}(s_{h}\\,|\\,s_{h-1}=\\phi_{h-1}(\\tau_{h-1}),a_{h-1},o_{h}).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Meanwhile, since we know $\\mathbb{P}^{\\mathcal{P}}(\\cdot\\,|\\,\\tau_{h})$ is a one-hot vector, we can construct $\\psi_{h}$ such that $\\psi_{h}{\\big(}s_{h-1},a_{h-1},o_{h}{\\big)}$ is the unique $s_{h}$ that makes $\\mathbb{P}^{\\mathcal{P}}(s_{h}\\,|\\,\\tau_{h})\\,>\\,0$ with $s_{h-1}\\,=\\,\\phi_{h-1}\\bigl(\\tau_{h-1}\\bigr)$ . If this procedure does not complete the definition of $\\psi$ for some $(s_{h-1},a_{h-1},o_{h})$ , it implies that either $s_{h-1}$ is not reachable or $o_{h}$ is not reachable given $s_{h-1}$ , i.e., $\\mathbb{P}^{\\mathcal{P}}(o_{h}\\,|\\,s_{h-1},a_{h-1}^{\\prime})\\,=\\,0$ for any $a_{h-1}^{\\prime}\\in\\mathcal{A}$ , thus recovering the conditions of Definition 3.2. \u25a0 ", "page_idx": 47}, {"type": "text", "text": "Proof of Proposition 7.3: ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Note that for any given problem instance of a POMDP, we can construct a POSG by adding a dummy agent that has the observation being the exact state at each time step, and has only one dummy action that does not affect the transition or reward. Therefore, even the local private information $p_{i,h}$ of the dummy agent can decode the underlying state, and hence $(c_{h},p_{h})$ reveals the underlying state. Therefore, the corresponding POSG with the dummy agent satisfies the condition in Proposition 7.3. Meanwhile, it is direct to see that any CCE of the POSG is an optimal policy for the original POMDP. Now by the PSPACE-hardness of planning for POMDPs [65], we proved our proposition. \u25a0 ", "page_idx": 47}, {"type": "text", "text": "Theorem J.1. For any $\\pi\\in\\Pi_{S}$ and (potentially stochastic) decoding functions $\\widehat{g}=\\{\\widehat{g}_{i,h}\\}_{i\\in[n],h\\in[H]}$ with $\\widehat{g}_{i,h}:\\mathcal{C}_{h}\\times\\mathcal{P}_{i,h}\\to\\Delta(S)$ for each $i\\in[n],h\\in[H]$ , it holds that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{NE/CCE-gap}(\\pi^{\\widehat{\\boldsymbol{g}}})-\\mathrm{NE/CCE-gap}(\\pi)}\\\\ &{\\quad\\le2n H^{2}\\underset{i\\in[n]}{\\operatorname*{max}}\\underset{u_{i}\\in[n],i\\in[H]}{\\operatorname*{max}}\\underset{h\\in[H]}{\\operatorname*{max}}\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathcal{G}}(s_{h}\\neq\\widehat{g}_{j,h}(c_{h},p_{j,h}))}\\\\ &{\\mathrm{CE-gap}(\\pi^{\\widehat{\\boldsymbol{g}}})-\\mathrm{CE-gap}(\\pi)}\\\\ &{\\quad\\le2n H^{2}\\underset{i\\in[n]}{\\operatorname*{max}}\\underset{m_{i}\\in[n],h\\in[H]}{\\operatorname*{max}}\\mathbb{P}^{(m_{i}\\diamond\\pi_{i})\\odot\\pi_{-i},\\mathcal{G}}(s_{h}\\neq\\widehat{g}_{j,h}(c_{h},p_{j,h}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $\\pi^{\\widehat{g}}$ is the distilled policy of $\\pi$ through the decoding functions $\\widehat g$ , where at step $h$ , each agent $i$ firstly individually decodes the state by sampling $s_{h}\\sim\\widehat{g}_{i,h}(\\cdot\\,\\vert\\,c_{h},p_{i,h})$ , and then act according to the expert $\\pi_{i,h}$ . In other words, the decoding process does not need correlations among the agents. ", "page_idx": 48}, {"type": "text", "text": "Proof. For notational simplicity, we write $v_{i}$ instead of $v_{i}^{\\mathcal{G}}$ when the underlying model is clear in the context. Firstly, we consider any deterministic decoding function $\\widehat{\\phi}=\\{\\widehat{\\phi}_{i,h}\\}_{i\\in[n],h\\in[H]}$ with $\\widehat{\\phi}_{i,h}:\\mathcal{C}_{h}\\times\\mathcal{P}_{i,h}\\to\\mathcal{S}$ for each $i\\in[n],h\\in[H]$ , and note the following that for any $i\\in[n]$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{\\pi}^{\\mathcal{G}}[R]-\\mathbb{E}_{\\pi^{\\hat{\\sigma}}}^{\\mathcal{G}}[R]}\\\\ &{=\\mathbb{E}_{\\pi}^{\\mathcal{G}}[R\\mathbb{1}[\\forall j\\in[n],h\\in[H]:s_{h}=\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})]]-\\mathbb{E}_{\\pi^{\\hat{\\sigma}}}^{\\mathcal{G}}[R\\mathbb{1}[\\forall j\\in[n],h\\in[H]:s_{h}=\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})]]}\\\\ &{\\phantom{=}+\\mathbb{E}_{\\pi}^{\\mathcal{G}}[R\\mathbb{1}[\\exists j\\in[n],h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})]]}\\\\ &{\\phantom{=}-\\mathbb{E}_{\\pi^{\\hat{\\sigma}}}^{\\mathcal{G}}[R\\mathbb{1}[\\exists j\\in[n],h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})]]}\\\\ &{=\\mathbb{E}_{\\pi}^{\\mathcal{G}}[R\\mathbb{1}[\\exists j\\in[n],h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})]]-\\mathbb{E}_{\\pi\\widehat{\\phi}}^{\\mathcal{G}}[R\\mathbb{1}[\\exists j\\in[n],h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})]],}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where we define $\\begin{array}{r}{R:=\\sum_{h}r_{i,h}\\bigl(s_{h},a_{h}\\bigr)}\\end{array}$ and the last step is by the definition of $\\pi^{\\widehat{\\phi}}$ . Therefore, we conclude that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{i}(\\pi)-v_{i}(\\pi^{\\widehat{\\phi}})\\leq H\\mathbb{P}^{\\pi,\\mathcal{G}}(\\exists j\\in[n],h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Therefore, by noting the fact that $\\widehat g$ is equivalent to a mixture of deterministic decoding functions, where at the beginning of each episode, one can firstly independently sample the outcome for each $c_{h},p_{j,h}$ for $j\\in[n]$ and $h\\in[H]$ , we conclude that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{i}(\\pi)-v_{i}(\\pi^{\\widehat{g}})=v_{i}(\\pi)-\\mathbb{E}_{\\widehat{\\phi}\\sim\\widehat{g}}v_{i}(\\pi^{\\widehat{\\phi}})\\leq H\\mathbb{E}_{\\widehat{\\phi}\\sim\\widehat{g}}\\mathbb{P}^{\\pi,\\mathcal{G}}(\\exists j\\in[n],h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h}))}\\\\ &{\\qquad\\qquad\\qquad=H\\mathbb{P}^{\\pi,\\mathcal{G}}(\\exists j\\in[n],h\\in[H]:s_{h}\\neq\\widehat{g}_{j,h}(c_{h},p_{j,h})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Now we prove our result for NE and CCE first. Due to similar arguments for evaluating $v_{i}(\\pi)\\!-\\!v_{i}(\\pi^{\\widehat{\\phi}})$ , for any $u_{i}\\in\\Pi_{i}\\cup\\Pi_{S,i}$ , it holds that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{v_{i}(u_{i}\\times\\pi_{-i}^{\\hat{\\phi}})-v_{i}(u_{i}\\times\\pi_{-i})\\leq\\mathbb{E}_{u_{i}\\times\\pi_{-i}^{\\hat{\\phi}}}^{\\mathcal{G}}[R{\\mathbb{1}}[\\exists j\\in[n]\\setminus\\{i\\},h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})]]}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq H\\mathbb{P}^{u_{i}\\times\\pi_{-i}^{\\hat{\\phi}}}{\\mathcal{G}}(\\exists j\\in[n]\\setminus\\{i\\},h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We notice the following fact that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathcal{G}}(\\forall j\\in[n]\\backslash\\{i\\},h\\in[H]:s_{h}=\\widehat{\\phi}_{j,h}(c_{h},p_{j,h}))=\\sum_{\\bar{\\tau}_{H}\\in\\overline{{\\mathcal{T}}}_{H}(\\widehat{\\phi})}\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathcal{G}}(\\bar{\\tau}_{H}),\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where we define $\\overline{{T}}_{H}(\\widehat{\\phi})\\,:=\\,\\{\\overline{{\\tau}}_{H}\\,\\in\\,\\overline{{\\mathcal{T}}}_{H}\\,|\\,\\forall j\\,\\in\\,[n]\\,\\backslash\\,\\{i\\},h\\,\\in\\,[H]\\,:\\,s_{h}\\,=\\,\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})\\}$ . By definition of $\\pi^{\\widehat{\\phi}}$ , it holds that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\forall\\bar{\\tau}_{H}\\in\\overline{{\\mathcal{T}}}_{H}(\\widehat{\\phi}):\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathcal{G}}(\\bar{\\tau}_{H})=\\mathbb{P}^{u_{i}\\times\\pi_{-i}^{\\hat{\\phi}},\\mathcal{G}}(\\bar{\\tau}_{H}).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Therefore, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathcal{G}}(\\forall j\\in[n]\\setminus\\{i\\},h\\in[H]:s_{h}=\\widehat{\\phi}_{j,h}(c_{h},p_{j,h}))}\\\\ &{\\quad=\\mathbb{P}^{u_{i}\\times\\pi_{-i}^{\\widehat{\\phi}},\\mathcal{G}}(\\forall j\\in[n]\\setminus\\{i\\},h\\in[H]:s_{h}=\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Correspondingly, it holds that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathcal{G}}(\\exists j\\in[n]\\setminus\\{i\\},h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h}))}\\\\ &{\\quad=\\mathbb{P}^{u_{i}\\times\\pi_{-i}^{\\widehat{\\phi}},\\mathcal{G}}(\\exists j\\in[n]\\setminus\\{i\\},h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "which implies that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{v_{i}(u_{i}\\times\\pi_{-i}^{\\widehat\\phi})-v_{i}(u_{i}\\times\\pi_{-i})\\leq H\\mathbb{P}^{u_{i}\\times\\pi_{-i}^{\\widehat\\phi},\\mathscr{G}}(\\exists j\\in[n]\\setminus\\{i\\},h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h}))}\\\\ &{}&{=H\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathscr{G}}(\\exists j\\in[n]\\setminus\\{i\\},h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Again by the fact that $\\widehat g$ is equivalent to a mixture of deterministic decoding functions, it holds ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{i}(u_{i}\\times\\pi_{-i}^{\\widehat{\\mathcal{P}}})-v_{i}(u_{i}\\times\\pi_{-i})=\\mathbb{E}_{\\widehat{\\phi}\\sim\\widehat{\\mathcal{P}}}v_{i}(u_{i}\\times\\pi_{-i}^{\\widehat{\\phi}})-v_{i}(u_{i}\\times\\pi_{-i})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq H\\mathbb{E}_{\\widehat{\\phi}\\sim\\widehat{\\mathcal{P}}}\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathcal{G}}(\\exists j\\in[n]\\setminus\\{i\\},h\\in[H]:s_{h}\\not=\\widehat{\\phi}_{j,h}(c_{h},p_{j,h}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad=H\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathcal{G}}(\\exists j\\in[n]\\setminus\\{i\\},h\\in[H]:s_{h}\\not=\\widehat{g}_{j,h}(c_{h},p_{j,h})).\\quad\\hfill\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now we are ready to evaluate the NE/CCE gap of policy $\\pi^{\\widehat{g}}$ as follows: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{NE/CCE-gap}(\\pi^{\\widehat{g}})-\\mathrm{NE/CCE-gap}(\\pi)}\\\\ &{\\leq\\underset{i\\in[n]}{\\operatorname*{max}}\\left(\\underset{u_{i}\\in\\Pi_{i}}{\\operatorname*{max}}v_{i}(u_{i}\\times\\pi_{-i}^{\\widehat{g}})-\\underset{u_{i}\\in\\Pi_{S,i}}{\\operatorname*{max}}v_{i}(u_{i}\\times\\pi_{-i})\\right)}\\\\ &{\\quad\\quad+H\\mathbb{P}^{\\pi,\\mathcal{G}}(\\exists j\\in[n],h\\in[H]:s_{h}\\neq\\widehat{g}_{j,h}(c_{h},p_{j,h})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now we notice that $\\begin{array}{r}{\\operatorname*{max}_{u_{i}\\in\\Pi_{S,i}}v_{i}\\big(u_{i}\\times\\pi_{-i}\\big)=\\operatorname*{max}_{u_{i}\\in\\Pi_{i}}v_{i}\\big(u_{i}\\times\\pi_{-i}\\big)}\\end{array}$ since $\\Pi_{S,i}\\subseteq\\Pi_{i}$ by the deterministic fliter condition Definition 3.2 and $\\pi_{-i}$ is a Markov policy. Therefore, we conclude that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{NE/CCE-gap}(\\overline{{\\alpha}}^{0})-\\mathrm{NE/CCE-gap}(\\pi)}\\\\ &{\\leq\\underset{i\\in[n]}{\\operatorname*{max}}\\left(\\underset{u_{i}\\in\\mathbb{N}_{i}}{\\operatorname*{max}}v_{i}(u_{i}\\times\\overline{{\\pi}}_{-i}^{\\beta})-\\underset{u_{i}\\in\\mathbb{N}_{i}}{\\operatorname*{max}}v_{i}(u_{i}\\times\\pi_{-i})\\right)+H\\mathbb{P}^{\\pi,\\sigma}(\\overline{{\\alpha}}j_{i}\\in[n],h\\in[H]:s_{h}\\neq\\widehat{g}_{j,h}(c_{h},p_{j,h}))}\\\\ &{\\leq\\underset{i\\in[n]}{\\operatorname*{max}}\\left(\\underset{u_{i}\\in\\mathbb{N}_{i}}{\\operatorname*{max}}\\left(v_{i}(u_{i}\\times\\pi_{-i}^{\\beta})-v_{i}(u_{i}\\times\\pi_{-i})\\right)\\right)+H\\mathbb{P}^{\\pi,\\sigma}(\\overline{{\\alpha}}j_{i}\\in[n],h\\in[H]:s_{h}\\neq\\widehat{g}_{j,h}(c_{h},p_{j,h}))}\\\\ &{\\leq\\underset{i\\in[n]}{\\operatorname*{max}}\\left(\\underset{u_{i}\\in\\mathbb{N}_{i}}{\\operatorname*{max}}H\\mathbb{P}^{\\mu,\\kappa\\times\\pi_{-i},\\sigma}(\\overline{{\\alpha}}j_{i}\\in[n]\\setminus\\{i,h\\},h\\in[H]:s_{h}\\neq\\widehat{g}_{j,h}(c_{h},p_{j,h}))\\right)}\\\\ &{\\quad\\quad\\quad+H\\mathbb{P}^{\\pi,\\sigma}(\\overline{{\\alpha}}j\\notin[n],h\\in[H]:s_{h}\\neq\\widehat{g}_{j,h}(c_{h},p_{j,h}))}\\\\ &{\\leq2H\\underset{u\\in[n],n_{i}\\in[n]}{\\operatorname*{max}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\|h\\varphi\\|^{\\alpha_{i}-\\mu}(s_{h}\\neq\\widehat{g}_{j,h}(c_{h},p_{j,h}))}\\\\ &{\\leq2n H^{2}(\\underset{u\\in[n],n_{i}\\in[n]}{\\operatorname*{max}}\\mu\\mathbb{E}[\\mu],h\\in[H])^{n_{i}\\times\\pi_{-i},\\sigma}(s_{h}\\neq\\widehat{g}_{j,h}(c_{h},p \n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where the second last step is by a union bound, thus proving our result for NE and CCE. ", "page_idx": 49}, {"type": "text", "text": "For CE, consider any strategy modification $m_{i}\\in\\mathcal{M}_{i}\\cup\\mathcal{M}_{S,i}.$ , it holds that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{CE-gap}(\\pi^{\\widehat{\\mathcal{G}}})-\\mathrm{CE-gap}(\\pi)}\\\\ &{\\le\\displaystyle\\operatorname*{max}_{m_{i}\\in\\mathcal{M}_{i}}v_{i}\\big(\\big(m_{i}\\diamond\\pi_{i}^{\\widehat{\\mathcal{G}}}\\big)\\odot\\pi_{-i}^{\\widehat{\\mathcal{G}}}\\big)-\\displaystyle\\operatorname*{max}_{m_{i}\\in\\mathcal{M}_{s,i}}v_{i}\\big(\\big(m_{i}\\diamond\\pi_{i}\\big)\\odot\\pi_{-i}\\big)}\\\\ &{\\qquad+\\ H\\mathbb{P}^{\\pi,\\mathcal{G}}(\\exists j\\in[n],h\\in[H]:s_{h}\\neq\\widehat{g}_{j,h}\\big(c_{h},p_{j,h}\\big)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now we notice that $\\begin{array}{r}{\\operatorname*{max}_{m_{i}\\in\\Pi_{S,i}}v_{i}((m_{i}\\diamond\\pi_{i})\\odot\\pi_{-i})=\\operatorname*{max}_{m_{i}\\in\\mathcal{M}_{S,i}}v_{i}\\big(\\big(m_{i}\\diamond\\pi_{i}\\big)\\odot\\pi_{-i}\\big)}\\end{array}$ since $\\mathcal{M}_{S,i}\\,\\subseteq\\,\\mathcal{M}_{i}$ by the deterministic filter condition Definition 3.2 and Lemma J.2. Therefore, we ", "page_idx": 49}, {"type": "text", "text": "conclude that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{CE-gay}(\\overline{{\\sigma}}^{*})-\\mathrm{CE-gay}(\\overline{{\\sigma}})}\\\\ &{\\le\\operatorname*{max}_{i\\in[m_{i}+1]}\\operatorname*{max}_{\\tau_{i}}\\:v_{i}((m_{i}+\\pi_{i})^{\\overline{{\\alpha}}})\\circ\\sigma_{\\tau_{i}}^{\\overline{{\\beta}}})-\\operatorname*{max}_{\\tau_{i}\\in[m_{i}+1]}v_{i}((m_{i}+\\pi_{i})\\odot\\pi_{-i})}\\\\ &{\\quad+H\\mathbb{P}^{\\pi,\\sigma}(\\overline{{\\beta}})\\in\\{n\\},h\\in[H]:s_{h}\\neq\\overline{{\\beta}}_{h}j\\delta_{h}(c_{h},p_{j,h}))}\\\\ &{\\le\\operatorname*{max}_{i\\in[m_{i}+1]}\\operatorname*{max}_{i\\in[m_{i}+\\pi_{i}^{\\overline{{\\alpha}}}]\\odot\\tau_{i}}\\:\\sigma_{\\tau_{i}}^{\\overline{{\\beta}}})-v_{i}((m_{i}\\otimes\\pi_{i})\\odot\\pi_{-i})}\\\\ &{\\quad+H\\mathbb{P}^{\\pi,\\sigma}(\\overline{{\\beta}})\\in[n],h\\in[H]:s_{h}\\neq\\overline{{\\beta}}_{h}j\\delta_{h}(c_{h},p_{j,h}))}\\\\ &{\\le\\operatorname*{max}_{i\\in[n+1]}\\operatorname*{max}_{i\\in[m_{i}+\\pi_{i}^{\\overline{{\\alpha}}}]\\odot\\tau_{i}}\\:\\sigma(\\mathfrak{Z})\\in[n]\\setminus\\{i\\},h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h}))}\\\\ &{\\quad+H\\mathbb{P}^{\\pi,\\sigma}(\\overline{{\\beta}})\\in[n],h\\in[H]:s_{h}\\neq\\widehat{\\phi}_{j,h}(c_{h},p_{j,h})}\\\\ &{\\le2H\\operatorname*{max}_{i\\in[m_{i}+1]}\\sum_{\\underline{{\\alpha}}\\in[m_{i}+1]}\\sum_{\\overline{{\\beta}}}\\:\\Gamma(s_{h})\\:\\sigma_{\\tau_{i}}^{\\overline{{\\alpha}}}(s_{h},p_{j,h}))}\\\\ &{\\le2H\\operatorname*{max}_{i\\in[m_{i},m+1]}\\sum_{\\underline{{\\alpha}}\\in[m_{i}+1\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where the third step is due to the same reason as Equation (J.1). ", "page_idx": 50}, {"type": "text", "text": "Lemma J.2. For any $\\pi\\in\\Pi_{S}$ , it holds that for any $i\\in[n]$ that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{m_{i}\\in\\mathcal{M}_{i}^{\\mathrm{gen}}}v_{i}\\big(\\big(m_{i}\\diamond\\pi_{i}\\big)\\odot\\pi_{-i}\\big)=\\operatorname*{max}_{m_{i}\\in\\mathcal{M}_{S,i}}v_{i}\\big(\\big(m_{i}\\diamond\\pi_{i}\\big)\\odot\\pi_{-i}\\big).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof. Denote $m_{i}^{\\star}\\,\\in\\,\\mathrm{argmax}_{m_{i}\\in\\mathcal{M}_{i}^{\\mathrm{gen}}}\\,v_{i}\\bigl(\\bigl(m_{i}\\diamond\\pi_{i}\\bigr)\\odot\\pi_{-i}\\bigr)$ and $\\widehat{m}_{i}^{\\star}\\,\\in\\,\\mathrm{argmax}_{m_{i}\\in\\mathcal{M}_{S,i}}\\,v_{i}\\bigl(\\bigl(m_{i}\\,\\circ$ $\\pi_{i})\\odot\\pi_{-i})$ . Now we shall prove that $V_{i,h}^{(m_{i}^{\\star}\\diamond\\pi_{i})\\odot\\pi_{-i},\\mathcal{G}}(s_{1:h},o_{1:h},a_{1:h-1})\\,\\leq\\,V_{i,h}^{(\\widehat{m}_{i}^{\\star}\\diamond\\pi_{i})\\odot\\pi_{-i},\\mathcal{G}}(s_{h})$ inductively for each $h\\in[H]$ . Note that it holds for $h=H+1$ . Now we consider the following ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{i,k}^{(m,n),(m,-a,\\beta)}(\\mathbf{s}_{1},\\partial_{t_{1}}\\mathbf{b}_{1},\\boldsymbol{\\alpha}_{1},\\mathbf{b}_{-1})}\\\\ &{=\\mathbb{E}_{\\mathbf{s}_{i+1}\\sim\\mathcal{K}_{k}^{(m,n)}(\\cdot\\mathbf{b}_{i},\\mathbf{b}_{i}^{(m,n)}(\\cdot\\mathbf{b}_{i}^{(m,n)}(\\cdot\\mathbf{b}_{i}^{(m,n)},\\cdot\\mathbf{a}_{i})),a,\\beta,-\\boldsymbol{i},\\mathbf{b}_{i}^{(m,n)}(\\cdot\\mathbf{b}_{i}^{(m,n)}(\\cdot\\mathbf{b}_{1},\\cdot\\mathbf{a}_{i},\\mathbf{b}_{1},a_{1},\\cdot\\mathbf{a}_{i},\\mathbf{b}_{i}),a,-\\boldsymbol{i},\\mathbf{b}_{i}^{(m,n)}(\\cdot\\mathbf{b}_{i}^{(m,n)}(\\cdot\\mathbf{b}_{1},\\cdot\\mathbf{a}_{i},\\mathbf{b}_{1},a_{1},\\cdot\\mathbf{a}_{i},\\mathbf{b}_{i}),a,-\\boldsymbol{i},\\mathbf{b}_{i}^{(m,n)}(\\cdot\\mathbf{b}_{1}^{(m,n)}(\\cdot\\mathbf{b}_{1}^{(m,n)}(\\cdot\\mathbf{b}_{1}^{(m,n)}(\\cdot\\mathbf{a}_{i},\\cdot\\mathbf{b}_{1},a_{1},\\cdot\\mathbf{a}_{i},\\mathbf{b}_{i}),a,-\\boldsymbol{i},\\mathbf{b}_{i}^{(m,n)}(\\cdot\\mathbf{b}_{1}^{(m,n)}(\\cdot\\mathbf{b}_{1}^{(m,n)}(\\cdot\\mathbf{a}_{i},\\cdot\\mathbf{b}_{1},a_{1},\\cdot\\mathbf{a}_{i})),a,\\cdot\\mathbf{b}_{i}^{(m,n)}(\\cdot\\mathbf{b}_{1}^{(m,n)}(\\cdot\\mathbf{b}_{1}^{(m,n)}(\\cdot\\mathbf{b}_{1}^{(m,n)}(\\cdot\\mathbf{a}_{i},\\cdot\\mathbf{b}_{1},a_{1},\\cdot\\mathbf{a}_{i},\\mathbf{b}_{i}),a,-\\boldsymbol{i},\\mathbf{b}_{i}^{(m,n)}(\\cdot\\mathbf{b}_{1}^{(m,n)}(\\cdot\\mathbf{b}_{1}^{(m,n) \n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where the second inequality follows from the inductive hypothesis and the third step is by the definition of $\\widehat{m}_{i}^{\\star}\\in\\mathrm{argmax}_{m_{i}\\in\\mathcal{M}_{S,i}}\\,v_{i}\\big(\\big(m_{i}\\diamond\\pi_{i}\\big)\\odot\\pi_{-i}\\big)$ . \u53e3 ", "page_idx": 50}, {"type": "text", "text": "Lemma J.3. Given an approximate POSG $\\widehat{\\mathcal G}$ that satisfies Assumption C.8 with approximate transitions and emissions being $\\{\\widehat{\\mathbb{T}}_{h},\\widehat{\\mathbb{O}}_{h}\\}_{h\\in[H]}$ , we define the approximate decoding function $\\widehat g$ to be ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\widehat{g}_{i,h}(s_{h}\\,|\\,c_{h},p_{i,h}):=\\mathbb{P}^{\\widehat{\\mathcal{G}}}(s_{h}\\,|\\,c_{h},p_{i,h}),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "for each $h\\in[H]$ , $s_{h}\\in\\mathcal{S}$ , $c_{h}\\in\\mathcal{C}_{h}$ , $p_{i,h}\\in\\mathscr{P}_{i,h}$ . Then it holds that for any $\\pi\\in\\Pi_{S}$ , ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i\\in[n],u_{i}\\in\\Pi_{i},j\\in[n],h\\in[H]}{\\operatorname*{max}}~\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathcal{G}}(s_{h}\\ne\\widehat{g}_{j,h}(c_{h},p_{j,h}))}\\\\ &{\\le\\underset{i\\in[n],u_{i}\\in\\Pi_{s,i}}{\\operatorname*{max}}~\\mathbb{E}_{u_{i}\\times\\pi_{-i}}^{\\mathcal{G}}[\\displaystyle\\sum_{h\\in[H]}]|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})||_{1}+\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname*{max}_{i\\in[n],m_{i}\\in\\mathcal{M}_{i},j\\in[n],h\\in[H]}\\mathbb{P}^{(m_{i}\\circ\\pi_{i})\\odot\\pi_{-i},\\mathcal{G}}(s_{h}\\ne\\widehat{g}_{j,h}(c_{h},p_{j,h}))}}\\\\ &{\\le\\operatorname*{max}_{i\\in[n],m_{i}\\in\\mathcal{M}_{s,i}}\\mathbb{E}_{(m_{i}\\circ\\pi_{i})\\odot\\pi_{-i}}^{\\mathcal{G}}[\\displaystyle\\sum_{h\\in[H]}\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Proof. Note for any $i\\in[n],u_{i}\\in\\Pi_{i},j\\in[n],h\\in[H]$ , it holds ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathcal{G}}(s_{h}\\neq\\widehat{g}_{j,h}(c_{h},p_{j,h}))=\\frac{1}{2}\\mathbb{E}_{u_{i}\\times\\pi_{-i}}^{\\mathcal{G}}\\sum_{s_{h}}|\\mathbb{P}^{\\mathcal{G}}(s_{h}\\,|\\,c_{h},p_{j,h})-\\mathbb{P}^{\\widehat{\\mathcal{G}}}(s_{h}\\,|\\,c_{h},p_{j,h})|,\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "due to the condition in Definition 7.2. Meanwhile, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{2}\\mathbb{E}_{u_{i}\\times\\pi-i}^{Q}\\sum_{s_{h}}|\\mathbb{P}^{Q}(s_{h}\\mid c_{h},p_{j,h})-\\mathbb{P}^{\\widehat{Q}}(s_{h}\\mid c_{h},p_{j,h})|}}\\\\ &{\\le\\sum_{s_{h},c_{h},p_{j,h}}|\\mathbb{P}^{u_{i}\\times\\pi-i,Q}(s_{h},c_{h},p_{j,h})-\\mathbb{P}^{u_{i}\\times\\pi-i,\\widehat{Q}}(s_{h},c_{h},p_{j,h})|}\\\\ &{\\le\\sum_{\\widehat{\\tau_{h}}}|\\mathbb{P}^{u_{i}\\times\\pi-i,Q}(\\overline{{\\tau_{h}}})-\\mathbb{P}^{u_{i}\\times\\pi-i,\\widehat{Q}}(\\overline{{\\tau}}_{h})|}\\\\ &{\\le\\displaystyle\\sum_{\\tau_{H}}|\\mathbb{P}^{u_{i}\\times\\pi-i,Q}(\\overline{{\\tau}}_{H})-\\mathbb{P}^{u_{i}\\times\\pi-i,\\widehat{Q}}(\\overline{{\\tau}}_{H})|}\\\\ &{\\le\\displaystyle\\sum_{\\tau_{H}}|\\mathbb{P}^{\\widehat{Q}}(\\sum_{h}|\\mathbb{T}|)\\mathbb{T}\\mathbb{H}(\\cdot|s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot|s_{h},a_{h})|\\|_{1}+\\|\\mathbb{O}_{h}(\\cdot|s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot|\\ s_{h})\\|_{1}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where the first inequality is by Lemma J.7, the second and third inequalities are due to the fact that TV distance does not increase after marginalization, and the last inequality is by Lemma H.9. Since $\\pi_{-i}$ is a fixed and fully-observable Markov policy, by Lemma H.10, we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{u_{i}\\times\\pi_{-i}}^{\\mathcal{G}}\\left[\\displaystyle\\sum_{h\\in[H]}||\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})||_{1}+||\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})||_{1}\\right]}\\\\ &{\\le\\displaystyle\\operatorname*{max}_{u_{i}\\in\\Pi_{S,i}}\\mathbb{E}_{u_{i}\\times\\pi_{-i}}^{\\mathcal{G}}\\left[\\displaystyle\\sum_{h\\in[H]}||\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})||_{1}+||\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})||_{1}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "thus proving the first result of our lemma. ", "page_idx": 51}, {"type": "text", "text": "For the second result of our lemma, it can be proved similarly that for any $i\\in[n],m_{i}\\in\\mathcal{M}_{i},j\\in[n]$ , $h\\in[H]$ , ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{P}^{(m_{i}\\circ\\pi_{i})\\odot\\pi_{-i},\\mathcal{G}}(s_{h}\\not=\\widehat{g}_{j,h}(c_{h},p_{j,h}))}\\\\ {\\qquad\\le\\mathbb{E}_{(m_{i}\\circ\\pi_{i})\\odot\\pi_{-i}}^{\\mathcal{G}}\\left[\\displaystyle\\sum_{h\\in[H]}\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}+\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "By Lemma J.2, we proved the second result. ", "page_idx": 51}, {"type": "text", "text": "Theorem J.4. Fix any $\\epsilon,\\delta\\in(0,1)$ . Algorithm 5 can learn a decoding function $\\widehat g$ such that with probability $1-\\delta$ ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[n],u_{i}\\in\\Pi_{i},j\\in[n],h\\in[H]}\\mathbb{P}^{u_{i}\\times\\pi_{-i},\\mathcal{G}}\\big(s_{h}\\ne\\widehat{g}_{j,h}(c_{h},p_{j,h})\\big)\\le\\epsilon,\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "with total sample complexity $\\widetilde{\\mathcal{O}}({\\frac{n S^{2}A H O+n S^{3}A H}{\\epsilon^{2}}}\\ +\\ {\\frac{S^{4}A^{2}H^{5}}{\\epsilon}})$ and computational complexity $\\mathrm{POLY}\\!\\left(S,A,H,O,{\\textstyle\\frac{1}{\\epsilon}}\\right)$ . ", "page_idx": 52}, {"type": "text", "text": "Proof. With the help of Lemma J.3, it suffices to prove ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[n],u_{i}\\in\\Pi_{S,i}}\\mathbb{E}_{u_{i}\\times\\pi_{-i}}^{\\mathcal{G}}[\\sum_{h\\in[H]}\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}+\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}]\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "The following proof procedure follows similarly to that of Theorem H.5. For each $h\\,\\in\\,[H]$ and $s_{h}\\in\\mathcal{S}$ , we define ", "page_idx": 52}, {"type": "equation", "text": "$$\np_{h}(s_{h})=\\operatorname*{max}_{i\\in[n],u_{i}\\in\\Pi_{S,i}}d_{h}^{u_{i}\\times\\pi_{-i}}(s_{h}).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Fix $\\epsilon_{1},\\delta_{1}\\,>\\,0$ , we define $\\mathcal{U}(h,\\epsilon_{1})\\,=\\,\\{s_{h}\\,\\in\\,\\mathcal{S}\\,|\\,p_{h}(s_{h})\\,\\geq\\,\\epsilon_{1}\\}$ . By [36], one can learn the pol$\\{\\Psi_{i}(h,s_{h})\\}_{i\\in[n]}$ e wciothm pplreoxbitayb $\\widetilde{\\mathcal{O}}\\big(\\frac{S^{2}A_{i}H^{4}}{\\epsilon_{1}}\\big)$ ) suc  maxi\u2208[n] dh .h  tNhaot $\\begin{array}{r}{\\operatorname*{max}_{i\\in[n]}d_{h}^{\\Psi_{i}(h,s_{h})\\times\\pi_{-i}}(s_{h})\\geq}\\end{array}$ $\\frac{p_{h}(s_{h})}{2}$ $s_{h}\\;\\in\\;\\mathcal{U}(h,\\epsilon_{1})$ $1\\,-\\,n\\,\\cdot\\,\\delta_{1}$   \nfor any $\\textit{h}\\in[H]$ and $s_{h}\\,\\in\\,\\mathcal{U}(h,\\epsilon_{1})$ . For each $s_{h}~\\in~{\\cal S}$ and $a_{h}~\\in~{\\cal A}$ , we have executed each policy $\\{\\Psi_{i}(h,s_{h})\\,\\times\\,\\pi_{-i}\\}_{i\\in[n]}$ for the first $h\\,-\\,1$ steps followed by an action $a_{h}~\\in~{\\cal A}$ for $N$ episodes and denote the total number of episodes that $s_{h}$ and $a_{h}$ are visited as $N_{h}(s_{h},a_{h})$ , and $\\begin{array}{r}{N_{h}(s_{h})=\\sum_{a\\in\\mathcal{A}}N_{h}(s_{h},a)}\\end{array}$ . Then with probability $1-e^{-N\\epsilon_{1}/8}$ , we have $\\begin{array}{r}{N_{h}(s_{h},a_{h})\\ge\\frac{N p_{h}(s_{h})}{2}}\\end{array}$ by Cherno ff bound. Now conditioned on this event, we are ready to evaluate the following for any $i\\in[n]$ , and $u_{i}=\\Psi_{i}(h,s_{h})\\in\\Pi_{S,i}$ : ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{u_{k}}^{Q}(x_{k-\\ell-1}^{*}\\vert\\vert\\nabla_{k}(\\cdot\\vert\\,s_{k},a_{k})-\\widehat{\\nabla}_{k}(\\cdot\\vert\\,s_{k},a_{k})\\vert\\vert_{1}}\\\\ &{\\ =\\underset{s_{k}\\neq a_{k}}{\\sum}\\frac{d^{\\mathsf{a}_{k}\\mathsf{a}_{k}}\\mathbf{x}^{\\top}-\\mathbf{\\Phi}(s_{k})\\mathbf{(a}_{k}\\vert\\mathbf{x}_{k}\\vert\\mathbf{)}\\Bigg\\vert\\mathbb{T}_{k}\\Bigg\\vert\\cdot\\vert\\mathbf{\\bar{x}}_{k}\\cdot\\vert\\mathbf{s}_{k},a_{k}\\rangle-\\widehat{\\nabla}_{k}(\\cdot\\vert\\,s_{k},a_{k})\\vert\\vert_{1}}\\\\ &{\\ \\leq2\\cdot S_{\\ell+1}\\underset{s_{k}\\in\\mathbb{R}(\\bar{h}_{k}^{\\mathbb{Q}}(\\cdot\\vert\\,s_{k}),a_{k})}{\\sum}\\frac{d^{\\mathsf{a}_{k}\\wedge\\tau-\\ell}\\mathbf{\\Phi}(s_{k})(u_{i}\\times\\pi-i)_{k}(a_{k})\\vert\\mathbf{s}_{k}\\rangle\\sqrt{\\frac{S\\log(1/\\delta_{2})}{N_{k}(s_{k},a_{k})}}}{\\sum}}\\\\ &{\\ \\leq2\\cdot S_{\\ell+1}\\underset{s_{k}\\in\\mathbb{R}(\\bar{h}_{k}^{\\mathbb{Q}}(\\cdot\\vert\\,s_{k}),a_{k})}{\\sum}\\frac{d^{\\mathsf{a}_{k}\\wedge\\tau-\\ell}(s_{k})(u_{i}\\times\\pi-i)_{k}(a_{k})\\vert\\mathbf{s}_{k}\\rangle\\sqrt{\\frac{2S\\log(1/\\delta_{2})}{N_{P h}(s_{k})}}}{\\sum}}\\\\ &{\\ \\leq2\\cdot S_{\\ell+1}\\underset{s_{k}\\in\\mathbb{R}(\\bar{h}_{k}^{\\mathbb{Q}}(\\cdot\\vert\\,s_{k}),a_{k})}{\\sum}\\sqrt{\\frac{S\\log(1/\\delta_{2})}{N_{k}}}}\\\\ &{\\ \\leq2\\cdot S_{\\ell+1}\\sum_{s_{k}\\in\\mathbb{R}(\\bar{h}_{k}^{\\mathbb{Q}}(\\cdot\\vert\\,s_{k})\\vert\\leqslant\\bar{h}_{k}^{\\mathbb{Q}}(\\cdot\\vert\\,s_{k}))}}\\\\ &{\\ \\leq2\\cdot S_{\\ell+1} \n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where the second step is by Lemma J.8, and the last step is by Cauchy-Schwarz inequality. Similarly, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{u_{t}^{\\star}\\times z_{t-1}}^{\\mathcal{B}_{\\alpha_{t}}\\times z_{t-1}}|\\mathcal{O}_{h}(\\cdot\\,|\\,s_{h})=\\hat{Q}_{h}(\\cdot\\,|\\,s_{h})||}\\\\ &{\\quad=\\sum_{s_{h}}d_{h}^{\\star\\times z_{t}}(s_{h})||\\mathcal{O}_{h}(\\cdot\\,|\\,s_{h})-\\hat{Q}_{h}(\\cdot\\,|\\,s_{h})||_{1}}\\\\ &{\\quad\\le2\\cdot S_{t}+\\displaystyle\\sum_{s_{h}\\in\\mathbb{Z}[h,c_{t}]}d_{h}^{\\star\\times z_{t-1}}(s_{h})\\sqrt{\\frac{Q\\log(1/\\delta_{2})}{N_{h}(s_{h})}}}\\\\ &{\\quad\\le2\\cdot S_{t+1}\\displaystyle\\sum_{s_{h}\\in\\mathbb{Z}[h,c_{t}]}d_{h}^{\\star\\times z_{t}}(s_{h})\\sqrt{\\frac{Q\\log(1/\\delta_{2})}{N_{h}(s_{h})}}}\\\\ &{\\quad\\le2\\cdot S_{t+1}+\\displaystyle\\sum_{s_{h}\\in\\mathbb{Z}[h,c_{t}]}\\sqrt{d_{h}^{\\star\\times z_{t}-\\varepsilon_{t}}(s_{h})}\\sqrt{\\frac{Q\\log(1/\\delta_{2})}{N_{h}(s_{h})}}}\\\\ &{\\quad\\le2\\cdot S_{t}+\\displaystyle\\sum_{s_{h}\\in\\mathbb{Z}[h,c_{t}]}\\sqrt{d_{h}^{\\star\\times z_{t}-\\varepsilon_{t}}(s_{h})}\\sqrt{\\frac{Q\\log(1/\\delta_{2})}{N}}}\\\\ &{\\quad\\le2\\cdot S_{t+1}+\\displaystyle\\sqrt{\\frac{S Q\\log(1/\\delta_{2})}{N_{h}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the second step is by Lemma J.9, and the last step is by Cauchy-Schwarz inequality. Therefore, by a union bound, all high probability events hold with probability ", "page_idx": 53}, {"type": "equation", "text": "$$\n1-S H n\\delta_{1}-S H A e^{-N\\epsilon_{1}/8}-S A H\\delta_{2}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Therefore, we can choose $\\begin{array}{r}{N=\\widetilde{\\mathcal{O}}(\\frac{S^{2}+S O}{\\epsilon^{2}})}\\end{array}$ and $\\epsilon_{1}=O(\\frac{\\epsilon}{,}S).$ leading to the total sample complexity ", "page_idx": 53}, {"type": "equation", "text": "$$\nS H A(n N+\\widetilde{\\mathcal{O}}(\\frac{S^{3}A H^{4}}{\\epsilon}))=\\widetilde{\\mathcal{O}}(\\frac{n S^{2}A H O+n S^{3}A H}{\\epsilon^{2}}+\\frac{S^{4}A^{2}H^{5}}{\\epsilon}).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Note that although our Algorithm 5 and Theorem J.4 are stated for NE/CCE, it can also handle CE with simple modifications, where the key observation is that the strategy modification $m_{i}\\in\\mathcal{M}_{\\mathcal{S},i}$ can also be regarded as a Markov policy in an extended MDP marginalized by $\\pi_{-i}$ defined below. ", "page_idx": 53}, {"type": "text", "text": "Definition J.5. We define $\\mathcal{M}^{\\mathrm{extended}}(\\pi)$ to be an MDP for agent $i$ , where for each $\\begin{array}{r l r}{h}&{{}\\in}&{[H]}\\end{array}$ , the state is $(s_{h},a_{i,h})$ , the action is some modified action a\u2032i,h , the transition is defined as Tehxtended(sh+1, ai,h+1 | sh, ai,h, a\u2032i,h) $:=$ $\\mathbb{E}_{a_{-i,h}\\sim\\pi_{h}(\\cdot\\mid s_{h},a_{i,h})}[\\mathbb{T}_{h}(s_{h+1}\\mid s_{h},a_{i,h}^{\\prime},a_{-i,h})\\pi_{h+1}(a_{i,h+1}\\mid s_{h+1})]$ , where we slightly abuse the notation of $\\pi_{h}(a_{-i,h}\\mid s_{h},a_{i,h})$ and $\\pi_{h}(a_{i,h}\\mid s_{h})$ by defining them as the posterior and marginal distribution induced by the joint distribution $\\pi_{h}(a_{h}\\mid s_{h})$ . Similarly, the reward is given by $r_{h}^{\\mathrm{extended}}(s_{h},a_{i,h},a_{i,h}^{\\prime}):=\\mathbb{E}_{a_{-i,h}\\sim\\pi_{h}(\\cdot\\,|\\,{s_{h}},a_{i,h})}[r_{h}(s_{h},a_{i,h}^{\\prime},\\bar{a}_{-i,h})]$ . ", "page_idx": 53}, {"type": "text", "text": "With the help of such an extended MDP, we can develop Algorithm 6, which is a CE version of Algorithm 5 with the following guarantees. ", "page_idx": 53}, {"type": "text", "text": "Theorem J.6. Fix any $\\epsilon,\\delta\\in(0,1)$ . Algorithm 6 can learn a decoding function $\\widehat g$ such that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[n],m_{i}\\in\\mathcal{M}_{i},j\\in[n],h\\in[H]}\\mathbb{P}^{(m_{i}\\diamond\\pi_{i})\\odot\\pi_{-i},\\mathcal{G}}(s_{h}\\neq\\widehat{g}_{j,h}(c_{h},p_{j,h}))\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "with total sample complexity $\\begin{array}{r}{\\widetilde{\\mathcal{O}}(\\frac{n S^{2}A^{3}H O+n S^{3}A^{4}H}{\\epsilon^{2}}+\\frac{S^{4}A^{6}H^{5}}{\\epsilon})}\\end{array}$ and computational complexity $\\mathrm{POLY}\\!\\left(S,A,H,O,{\\textstyle\\frac{1}{\\epsilon}}\\right)$ . ", "page_idx": 53}, {"type": "text", "text": "Proof. Due to the construction of $\\mathcal{G}^{\\mathrm{extended}}(\\pi_{-i})$ , the proof of Theorem J.4 readily applies, where the only difference is that the state space of $\\mathcal{G}^{\\mathrm{extended}}(\\pi_{-i})$ is $S A_{i}$ , larger than that of $\\mathcal{G}(\\pi_{-i})$ by a factor of $A_{i}$ thus proving our theorem. \u53e3 ", "page_idx": 53}, {"type": "text", "text": "Lemma J.7. Suppose we can sample from a joint distribution $P\\in\\Delta(\\mathcal{X}\\times\\mathcal{Y})$ for some finite $\\mathcal{X}$ , $\\boldsymbol{\\wp}$ i.i.d. Then we can learn an approximate distribution $Q\\in\\Delta(\\mathcal{X}\\times\\mathcal{Y})$ with sample complexity $\\Theta\\left({\\frac{|\\chi||y|{+}{\\log{1/\\delta}}}{\\epsilon^{2}}}\\right)$ such that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim P}\\sum_{y\\in\\mathcal{Y}}\\left|P(y\\,|\\,x)-Q(y\\,|\\,x)\\right|\\leq2\\sum_{x\\in\\mathcal{X},y\\in\\mathcal{Y}}\\left|P(x,y)-Q(x,y)\\right|\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "with probability $1-\\delta$ . ", "page_idx": 53}, {"type": "text", "text": "Proof. Note the following holds ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x\\in\\mathcal{X},y\\in\\mathcal{Y}}|P(x,y)-Q(x,y)|=\\displaystyle\\sum_{x\\in\\mathcal{X},y\\in\\mathcal{Y}}|P(x,y)-P(x)Q(y\\,|\\,x)+P(x)Q(y\\,|\\,x)-Q(x,y)|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\geq\\displaystyle\\sum_{x\\in\\mathcal{X},y\\in\\mathcal{Y}}|P(x,y)-P(x)Q(y\\,|\\,x)|-|P(x)Q(y\\,|\\,x)-Q(x,y)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Therefore, we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{x\\sim P}\\sum_{y\\in\\mathcal{Y}}\\left|P(y\\,|\\,x)-Q(y\\,|\\,x)\\right|\\leq\\displaystyle\\sum_{x\\in\\mathcal{X},y\\in\\mathcal{Y}}\\left|P(x,y)-Q(x,y)\\right|+\\sum_{x\\in\\mathcal{X},y\\in\\mathcal{Y}}\\left|P(x)-Q(x)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2\\displaystyle\\sum_{x\\in\\mathcal{X},y\\in\\mathcal{Y}}\\left|P(x,y)-Q(x,y)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "By the sample complexity of learning discrete distributions [13], we can learn $Q$ such that $\\begin{array}{r}{\\sum_{x\\in\\mathcal{X},y\\in\\mathcal{Y}}|P(x,y)-Q(x,y)|\\le\\epsilon}\\end{array}$ in sample complexity $\\Theta\\left({\\frac{|\\chi||y|{+}{\\log{1/\\delta}}}{\\epsilon^{2}}}\\right)$ with probability $1-\\delta$ . Thus, we proved our lemma. \u53e3 ", "page_idx": 54}, {"type": "text", "text": "Lemma J.8 (Concentration on transition). Fix $\\delta>0$ and dataset $\\{\\mp_{H}^{k}\\}_{k\\in[N]}$ sampled from $\\mathcal{P}$ under policy $\\pi\\in\\Pi^{\\mathrm{gen}}$ . We define for each $,\\in[H],(s_{h},a_{h},s_{h+1})\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{N_{h}(s_{h},a_{h})=\\displaystyle\\sum_{k\\in[N]}\\mathbb{1}[s_{h}^{k}=s_{h},a_{h}^{k}=a_{h}]}}\\\\ {{N_{h}(s_{h},a_{h},s_{h+1})=\\displaystyle\\sum_{k\\in[N]}\\mathbb{1}[s_{h}^{k}=s_{h},a_{h}^{k}=a_{h},s_{h+1}^{k}=s_{h}].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Then with probability $1-\\delta$ , it holds that for any $k\\in[K],h\\in[H],s_{h}\\in\\mathcal{S},a_{h}\\in\\mathcal{A}$ : ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}\\leq C_{1}\\sqrt{\\frac{S\\log(S A H K/\\delta)}{\\operatorname*{max}\\{N_{h}(s_{h},a_{h}),1\\}}},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "for some absolute constant $C_{1}>0$ , where we define $\\begin{array}{r}{\\widehat{\\mathbb{T}}_{h}(s_{h+1}\\,|\\,s_{h},a_{h})=\\frac{N_{h}(s_{h},a_{h},s_{h+1})}{\\operatorname*{max}\\{N_{h}(s_{h},a_{h}),1\\}}}\\end{array}$ ", "page_idx": 54}, {"type": "text", "text": "Proof. This is done by firstly bounding $\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}$ for specific $k,h,s_{h},a_{h}$ according to [13] and then taking union bound for all $k\\in[K],h\\in[H],s_{h}\\in{\\mathcal{S}},a_{h}\\in{\\mathcal{A}}$ . \u53e3 ", "page_idx": 54}, {"type": "text", "text": "Lemma J.9 (Concentration on emission). Fix $\\delta>0$ and dataset $\\{\\overline{{\\tau}}_{H}^{k}\\}_{k\\in[N]}$ sampled from $\\mathcal{P}$ under some policy $\\pi\\in\\Pi^{\\mathrm{gen}}$ . We define for each $h\\in[H]$ , $(s_{h},o_{h})\\in S\\times O$ ", "page_idx": 54}, {"type": "equation", "text": "$$\nN_{h}(s_{h},o_{h})=\\sum_{k\\in[N]}\\mathbb{1}[s_{h}^{k}=s_{h},o_{h}^{k}=o_{h}],\\qquad N_{h}(s_{h})=\\sum_{k\\in[N]}\\mathbb{1}[s_{h}^{k}=s_{h}].\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Then, with probability at least $1-\\delta$ , it holds that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}\\leq C_{2}\\sqrt{\\frac{O\\log(S H K/\\delta)}{\\operatorname*{max}\\{N_{h}^{k}(s_{h}),1\\}}},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "for some absolute constant $C_{2}>0$ , where we define $\\begin{array}{r}{\\widehat{\\mathbb{O}}_{h}(o_{h}\\,|\\,s_{h})=\\frac{N_{h}(s_{h},o_{h})}{\\operatorname*{max}\\{N_{h}(s_{h}),1\\}}}\\end{array}$ . ", "page_idx": 54}, {"type": "text", "text": "Proof. This is done by firstly bounding $\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}$ for specific $k,h,s_{h}$ according to [13] and then taking union bound for all $k\\in[K],h\\in[H],s_{h}\\in\\mathcal{S}$ . \u53e3 ", "page_idx": 54}, {"type": "text", "text": "Lemma J.10. Fix $\\delta>0$ . With probability $1-\\delta$ , it holds that for any $k\\in[K],h\\in[H],s_{h}\\in\\mathcal{S}$ : ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\sum_{o_{h+1}}\\left|\\mathbb{P}^{\\mathcal{G}}(o_{h+1}\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{J}}_{h}^{k}(o_{h+1}\\,|\\,s_{h},a_{h})\\right|\\leq C_{3}\\sqrt{\\frac{O\\log(S H K A/\\delta)}{N_{h}^{k}(s_{h},a_{h})}},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $\\widehat{\\mathbb{J}}_{h}^{k}$ is defined in Algorithm 7. ", "page_idx": 54}, {"type": "text", "text": "Proof. This is done by firstly bounding $\\begin{array}{r}{\\sum_{o_{h+1}}\\Big|\\mathbb{P}^{\\mathcal{G}}\\big(o_{h+1}\\,|\\,s_{h},a_{h}\\big)-\\widehat{\\mathbb{J}}_{h}^{k}\\big(o_{h+1}\\,|\\,s_{h},a_{h}\\big)\\Big|}\\end{array}$ for specific $k,h,s_{h},a_{h}$ according to [13] and then taking union bound for all $k\\in[K],h\\in[H],s_{h}\\in{\\mathcal{S}},a_{h}\\in$ $\\boldsymbol{\\mathcal{A}}$ . \u53e3 ", "page_idx": 55}, {"type": "text", "text": "From now on, we shall use the bonus as ", "page_idx": 55}, {"type": "equation", "text": "$$\nb_{h}^{k}(s_{h},a_{h})=\\operatorname*{min}\\left\\{C_{3}(H-h)\\sqrt{\\frac{O\\log(S A H K/\\delta)}{\\operatorname*{max}\\{N_{h}^{k}(s_{h},a_{h}),1\\}}},2(H-h)\\right\\}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "for some absolute constant $C_{3}>0$ . ", "page_idx": 55}, {"type": "text", "text": "Before presenting our technical analysis, we define the following notation for the ease of intermediate analysis. We define the following approximate value function for any policy $\\pi\\in\\Pi$ in a backward way for $h\\in[H]$ : ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat V_{i,h}^{\\pi,\\mathcal{G}}(c_{h}):=\\mathbb{E}_{s_{h},p_{h}\\sim\\widehat{P}_{h}(\\cdot,\\cdot)}\\mathbb{E}_{\\omega_{h}},\\{a_{j,h}\\sim\\pi_{j,h}(\\cdot\\,|\\,\\omega_{j,h},c_{h},p_{j,h})\\}_{j\\in[n]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h}),o_{h+1}\\sim\\mathcal{O}_{h+1}(\\cdot\\,|\\,s_{h+1})}\\left[r_{i,h}(s_{h},a_{h})+V_{i,h+1}^{\\pi,\\mathcal{G}}(c_{h+1})\\right],}\\\\ &{\\widehat Q_{i,h}^{\\pi,\\mathcal{G}}(c_{h},\\gamma_{h}):=\\mathbb{E}_{s_{h},p_{h}\\sim\\widehat{P}_{h}(\\cdot,\\cdot)}\\mathbb{E}_{\\{a_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,p_{j,h})\\}_{j\\in[n]}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h}),o_{h+1}\\sim\\mathcal{O}_{h+1}(\\cdot\\,|\\,s_{h+1})}\\left[r_{i,h}(s_{h},a_{h})+V_{i,h+1}^{\\pi,\\mathcal{G}}(c_{h+1})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "for each $(i,c_{h})\\in[n]\\times{\\mathcal{C}}_{h}$ , where we define $\\widehat{V}_{i,H+1}^{\\pi,\\mathcal{G}}(c_{H+1})=0$ . ", "page_idx": 55}, {"type": "text", "text": "Intuitively, this definition of $\\widehat{V}_{i,h}^{\\pi,\\mathcal{G}}(c_{h})$ mimics the Bellman equation of the ground-truth value function $V_{i,h}^{\\pi,\\mathcal{G}}(c_{h})$ by replacing the ground-truth belief $\\mathbb{P}^{\\mathcal{G}}(s_{h},p_{h}\\mid c_{h})$ by $\\widehat{P}_{h}(s_{h},p_{h}\\,|\\,\\widehat{c}_{h})$ . Next, we point out the following quantitative bound when using $\\widehat{V}_{i,h}^{\\pi,\\mathcal{G}}(c_{h})$ to approximate $V_{i,h}^{\\pi,\\mathcal{G}}(c_{h})$ . ", "page_idx": 55}, {"type": "text", "text": "Meanwhile, we also define the error for the belief as follows ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{belief}}:=\\operatorname*{max}_{\\pi\\in\\Pi}\\|\\mathbb{P}^{\\mathcal{G}}(\\cdot,\\cdot\\,|\\,c_{h})-\\widehat{P}_{h}(\\cdot,\\cdot\\,|\\,\\widehat{c}_{h})\\|_{1}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Lemma J.11. For any $\\pi^{\\prime},\\pi\\in\\Pi$ , it holds ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pi^{\\prime}}^{\\mathcal{G}}|V_{i,h}^{\\pi,\\mathcal{G}}(c_{h})-\\widehat{V}_{i,h}^{\\pi,\\mathcal{G}}(c_{h})|\\leq(H-h+1)^{2}\\epsilon_{\\mathrm{belief}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Proof. It follows directly by combining Lemma 4 and Lemma 8 of [50]. ", "page_idx": 55}, {"type": "text", "text": "Meanwhile, note that although in Algorithm 7, the value we maintain has input $\\widehat{c}_{h}$ instead of $c_{h}$ for computational efficiency, we extend the definition of those values to also accept $c_{h}$ as inputs as follows (with a slight abuse of notation): ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{Q_{i,h}^{\\mathrm{high},k}(c_{h},\\gamma_{h}):=Q_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h},\\gamma_{h})\\quad\\quad\\quad}\\\\ &{}&{Q_{i,h}^{\\mathrm{high},k}(c_{h},p_{h},s_{h},a_{h}):=Q_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h},p_{h},s_{h},a_{h})}\\\\ &{}&{V_{i,h}^{\\mathrm{high},k}(c_{h}):=V_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h})\\quad\\quad\\quad}\\\\ &{}&{Q_{i,h}^{\\mathrm{low},k}(c_{h},\\gamma_{h}):=Q_{i,h}^{\\mathrm{low},k}(\\widehat{c}_{h},\\gamma_{h})\\quad\\quad\\quad}\\\\ &{}&{Q_{i,h}^{\\mathrm{low},k}(c_{h},p_{h},s_{h},a_{h}):=Q_{i,h}^{\\mathrm{low},k}(\\widehat{c}_{h},p_{h},s_{h},a_{h})}\\\\ &{}&{V_{i,h}^{\\mathrm{low},k}(c_{h}):=V_{i,h}^{\\mathrm{low},k}(\\widehat{c}_{h}),\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where we recall that $\\widehat{c}_{h}=\\mathrm{Compress}_{h}(c_{h})$ ", "page_idx": 55}, {"type": "text", "text": "Lemma J.12 (Optimism 1 for NE/CCE). With probability $1-\\delta$ , for any $k\\in[K]$ , for Algorithm 7, it holds that for any $i\\in[n],\\pi_{i}^{\\prime}\\in\\Pi_{i},h\\in[H]$ ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h},\\gamma_{h})\\geq\\widehat{Q}_{i,h}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{h},\\gamma_{h})}\\\\ &{\\quad\\quad V_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h})\\geq\\widehat{V}_{i,h}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{h}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where we recall that $\\widehat{c}_{h}=\\mathrm{Compress}_{h}(c_{h})$ . ", "page_idx": 55}, {"type": "text", "text": "Proof. We will prove by backward induction. Obviously, it holds for $h=H+1$ . Now we assume the lemma holds for $h\\dot{+}1$ . Now we notice that by definition, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i,h}^{\\mathrm{high},k}(c_{h},\\gamma_{h})=\\mathbb{E}_{s_{h},p_{h}\\sim\\hat{P}_{h}(\\cdot,\\cdot|\\,\\hat{\\varepsilon}_{h})}\\mathbb{E}_{\\{a_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,p_{j,h})\\}_{j\\in[n]}}\\left[Q_{i,h}^{\\mathrm{high},k}(c_{h},p_{h},s_{h},a_{h})\\right]}\\\\ &{=\\mathbb{E}_{s_{h},p_{h}\\sim\\hat{P}_{h}(\\cdot,\\cdot|\\,\\hat{\\varepsilon}_{h})}\\mathbb{E}_{\\{a_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,p_{j,h})\\}_{j\\in[n]}}\\operatorname*{min}\\{r_{i,h}(s_{h},a_{h})+b_{h}^{k-1}(s_{h},a_{h})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\mathbb{E}_{o_{h+1}\\sim\\hat{\\gamma}_{h}^{k-1}(\\cdot\\,|\\,s_{h},a_{h})}\\left[V_{i,h+1}^{\\mathrm{high},k}(c_{h+1})\\right],H-h+1\\}}\\\\ &{\\geq\\mathbb{E}_{s_{h},p_{h}\\sim\\hat{P}_{h}(\\cdot,\\cdot|\\,\\hat{\\varepsilon}_{h})}\\mathbb{E}_{\\{a_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,p_{j,h})\\}_{j\\in[n]}}}\\\\ &{\\qquad\\qquad\\operatorname*{min}\\left\\{r_{i,h}(s_{h},a_{h})+b_{h}^{k-1}(s_{h},a_{h})+\\mathbb{E}_{o_{h+1}\\sim\\hat{\\gamma}_{h}^{k-1}(\\cdot\\,|\\,s_{h},a_{h})}\\left[\\widehat{V}_{i,h+1}^{\\pi_{i}^{\\prime}\\times\\pi_{i}^{k},\\mathcal{G}}(c_{h+1})\\right],H-h+1\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where the last step is by inductive hypothesis. Now note that for any $s_{h},p_{h},a_{h}$ , we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{h}^{k-1}(s_{h},a_{h})+\\mathbb{E}_{o_{h+1}\\sim\\hat{\\mathbb{J}}_{h}^{k-1}(\\cdot\\vert\\ s_{h},a_{h})}\\left[\\widehat{V}_{i,h+1}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{h+1})\\right]}\\\\ &{\\geq b_{h}^{k-1}(s_{h},a_{h})-(H-h)\\vert\\vert\\widehat{\\mathbb{J}}_{h}^{k-1}(\\cdot\\vert\\ s_{h},a_{h})-\\mathbb{P}^{\\mathcal{G}}(\\cdot\\vert\\,s_{h},a_{h})\\vert\\vert_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\,\\vert\\,s_{h},a_{h}),o_{h+1}\\sim\\mathbb{O}_{h+1}(\\cdot\\,\\vert\\,s_{h+1})}\\left[\\widehat{V}_{i,h+1}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{h+1})\\right]}\\\\ &{\\geq\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\,\\vert\\,s_{h},a_{h}),o_{h+1}\\sim\\mathbb{O}_{h+1}(\\cdot\\,\\vert\\,s_{h+1})}\\left[\\widehat{V}_{i,h+1}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{h+1})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where we notice $\\begin{array}{r}{\\mathbb{P}^{\\mathcal{G}}(o_{h+1}\\,|\\,s_{h},a_{h})\\,=\\,\\sum_{s_{h+1}}\\mathbb{O}_{h+1}(o_{h+1}\\,|\\,s_{h+1})\\mathbb{T}_{h}(s_{h+1}\\,|\\,s_{h},a_{h})}\\end{array}$ for the first inequality, and the second inequality comes from the construction of our bonus $b_{h}^{k-1}(s_{h},a_{h})$ in Equation (J.3) and Lemma J.10. Meanwhile, by the definition of value functions, it holds that $\\begin{array}{r}{\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\mid s_{h},a_{h}),o_{h+1}\\sim\\mathbb{O}_{h+1}(\\cdot\\mid s_{h+1})}\\left[\\widehat{V}_{i,h+1}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\bar{\\mathcal{G}}}(c_{h+1})\\right]\\le H-h.}\\end{array}$ Therefore, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{min}\\left\\{r_{i,h}(s_{h},a_{h})+b_{h}^{k-1}(s_{h},a_{h})+\\mathbb{E}_{s_{h+1},o_{h+1}\\sim\\hat{\\mathcal{I}}_{h}^{k-1}(\\cdot,\\cdot\\vert\\ s_{h},a_{h})}\\left[V_{i,h+1}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{h+1})\\right],H-h+1\\right\\}}\\\\ &{\\quad\\quad\\geq r_{i,h}(s_{h},a_{h})+\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\vert\\ s_{h},a_{h}),o_{h+1}\\sim\\mathcal{O}_{h+1}(\\cdot\\vert\\ s_{h+1})}\\left[\\widehat{V}_{i,h+1}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{h+1})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Now we conclude ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i,h}^{\\mathrm{high},k}(c_{h},\\gamma_{h})}\\\\ &{\\geq\\mathbb{E}_{s_{h},p_{h}\\sim\\widehat{P}_{h}(\\cdot,\\cdot\\,|\\,\\widehat{c}_{h})}\\mathbb{E}_{\\{a_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,p_{j,h})\\}_{j\\in[n]}}\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h}),o_{h+1}\\sim\\mathcal{O}_{h+1}(\\cdot\\,|\\,s_{h+1})}\\big[r_{i,h}(s_{h},a_{h})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\widehat{V}_{i,h+1}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{h+1})\\big]}\\\\ &{\\quad\\widehat{\\omega}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "By definition, we have $Q_{i,h}^{\\mathrm{high},k}(c_{h},\\gamma_{h})\\;\\;=\\;\\;Q_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h},\\gamma_{h})$ , thus proving ${\\cal Q}_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h},\\gamma_{h})\\;\\;\\geq$ Q i\u03c0,i\u2032h\u00d7\u03c0k\u2212i,G(ch, \u03b3h). Now for the value function, note that ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{i,h}^{\\mathrm{high},k}(c_{h})=\\mathbb{E}_{\\omega_{h}}Q_{i,h}^{\\mathrm{high},k}(c_{h},\\{\\pi_{j,h}^{k}(\\cdot\\,|\\,\\omega_{j,h},\\widehat{c}_{h},\\cdot)\\}_{j\\in[n]})}\\\\ &{\\quad\\quad\\quad\\quad\\geq\\mathbb{E}_{\\omega_{h}^{\\prime}}\\mathbb{E}_{\\omega_{h}}Q_{i,h}^{\\mathrm{high},k}(c_{h},\\{\\pi_{j,h}^{k}(\\cdot\\,|\\,\\omega_{j,h},\\widehat{c}_{h},\\cdot)\\}_{j\\in[n]\\backslash\\{i\\}},\\pi_{i,h}^{\\prime}(\\cdot\\,|\\,\\omega_{i,h}^{\\prime},c_{h},\\cdot))}\\\\ &{\\quad\\quad\\quad\\geq\\mathbb{E}_{\\omega_{h}^{\\prime}}\\mathbb{E}_{\\omega_{h}}\\widehat{Q}_{i,h}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{h},\\{\\pi_{j,h}^{k}(\\cdot\\,|\\,\\omega_{j,h},\\widehat{c}_{h},\\cdot)\\}_{j\\in[n]\\backslash\\{i\\}},\\pi_{i,h}^{\\prime}(\\cdot\\,|\\,\\omega_{i,h}^{\\prime},c_{h},\\cdot))}\\\\ &{\\quad\\quad\\quad=\\widehat{V}_{i,h}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{h}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where the first step is by the property of Bayesian CCE, and the second step is by $Q_{i,h}^{\\mathrm{high},k}(c_{h},\\gamma_{h})\\geq$ Q i\u03c0,i\u2032h\u00d7\u03c0k\u2212i,G(c for any $\\gamma_{h}\\in\\Gamma_{h}$ . Again by definition, we proved $V_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h})=V_{i,h}^{\\mathrm{high},k}(c_{h})\\geq$ $\\widehat{V}_{i,h}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{h})$ \u53e3 ", "page_idx": 56}, {"type": "text", "text": "Lemma J.13 (Optimism 1 for CE). With probability $1-\\delta$ , for any $k\\,\\in\\,[K]$ , for Algorithm 7, it holds that for any $i\\in[n],m_{i}\\in\\mathcal{M}_{i},h\\in[\\bar{H}]$ ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h},\\gamma_{h})\\geq\\widehat{Q}_{i,h}^{(m_{i}\\diamond\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}(c_{h},\\gamma_{h})}\\\\ &{\\quad\\quad V_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h})\\geq\\widehat{V}_{i,h}^{(m_{i}\\diamond\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}(c_{h}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Proof. We will prove by backward induction. Obviously, it holds for $h=H+1$ . Now we assume the lemma holds for $h+1$ . Now we notice that by definition, ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i,h}^{\\mathrm{high},k}(c_{h},\\gamma_{h})=\\mathbb{E}_{s_{h},p_{h}\\sim\\hat{P}_{h}(\\cdot,\\cdot|\\,\\hat{c}_{h})}\\mathbb{E}_{\\{a_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,p_{j,h})\\}_{j\\in[n]}}\\left[Q_{i,h}^{\\mathrm{high},k}(c_{h},p_{h},s_{h},a_{h})\\right]}\\\\ &{=\\mathbb{E}_{s_{h},p_{h}\\sim\\hat{P}_{h}(\\cdot,\\cdot|\\,\\hat{c}_{h})}\\mathbb{E}_{\\{a_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,p_{j,h})\\}_{j\\in[n]}}\\operatorname*{min}\\{r_{i,h}(s_{h},a_{h})+b_{h}^{k-1}(s_{h},a_{h})\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.+\\mathbb{E}_{o_{h+1}\\sim\\hat{\\mathbb{J}}_{h}^{k-1}(\\cdot\\,|\\,s_{h},a_{h})}\\left[V_{i,h+1}^{\\mathrm{high},k}(c_{h+1})\\right],H-h+1\\}}\\\\ &{\\geq\\mathbb{E}_{s_{h},p_{h}\\sim\\hat{P}_{h}(\\cdot,\\cdot|\\,\\hat{c}_{h})}\\mathbb{E}_{\\{a_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,p_{j,h})\\}_{j\\in[n]}}}\\\\ &{\\operatorname*{min}\\left\\{r_{i,h}(s_{h},a_{h})+b_{h}^{k-1}(s_{h},a_{h})+\\mathbb{E}_{o_{h+1}\\sim\\hat{\\mathbb{J}}_{h}^{k-1}(\\cdot\\,|\\,s_{h},a_{h})}\\left[\\widehat{V}_{i,h+1}^{(m_{i}\\circ\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}(c_{h+1})\\right],H-h+1\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where the last step is by inductive hypothesis. Now note that for any $s_{h},p_{h},a_{h}$ , we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{h}^{k-1}\\big(s_{h},a_{h}\\big)+\\mathbb{E}_{o_{h+1}\\sim\\widehat{\\mathbb{I}}_{h}^{k-1}(\\cdot\\ |\\ s_{h},a_{h})}\\left[\\widehat{V}_{i,h+1}^{(m_{i}\\circ\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}\\big(c_{h+1}\\big)\\right]}\\\\ &{\\geq b_{h}^{k-1}\\big(s_{h},a_{h}\\big)-\\big(H-h\\big)\\|\\widehat{\\mathbb{J}}_{h}^{k-1}\\big(\\cdot\\,\\big|\\ s_{h},a_{h}\\big)-{\\mathbb P}^{\\mathcal{G}}\\big(\\cdot\\,\\big|\\ s_{h},a_{h}\\big)\\big\\|_{1}}\\\\ &{\\qquad\\qquad\\qquad+\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\ |\\ s_{h},a_{h}),o_{h+1}\\sim\\mathbb{O}_{h+1}(\\cdot\\,|\\ s_{h+1})}\\left[\\widehat{V}_{i,h+1}^{(m_{i}\\circ\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}\\big(c_{h+1}\\big)\\right]}\\\\ &{\\geq\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\ |\\ s_{h},a_{h}),o_{h+1}\\sim\\mathbb{O}_{h+1}(\\cdot\\,|\\ s_{h+1})}\\left[\\widehat{V}_{i,h+1}^{(m_{i}\\circ\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}\\big(c_{h+1}\\big)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where we notice $\\begin{array}{r}{\\mathbb{P}^{\\mathcal{G}}(o_{h+1}\\,|\\,s_{h},a_{h})\\,=\\,\\sum_{s_{h+1}}\\mathbb{O}_{h+1}(o_{h+1}\\,|\\,s_{h+1})\\mathbb{T}_{h}(s_{h+1}\\,|\\,s_{h},a_{h})}\\end{array}$ for the first inequality, and the second inequality comes from the construction of our bonus $b_{h}^{k-1}(s_{h},a_{h})$ in Equation (J.3) and Lemma J.10. Meanwhile, by the definition of value functions, it holds that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\mid s_{h},a_{h}),o_{h+1}\\sim\\circ\\mathcal{O}_{h+1}(\\cdot\\mid s_{h+1})}\\left[\\widehat{V}_{i,h+1}^{(m_{i}\\circ\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}(c_{h+1})\\right]\\le H-h.\\mathrm{~Therefore,we~have~}}\\\\ &{\\quad\\operatorname*{min}\\{r_{i,h}(s_{h},a_{h})+b_{h}^{k-1}(s_{h},a_{h})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\mathbb{E}_{s_{h+1},o_{h+1}\\sim\\widehat{\\mathbb{J}}_{h}^{k-1}(\\cdot,\\cdot\\mid s_{h},a_{h})}\\left[V_{i,h+1}^{(m_{i}\\circ\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}(c_{h+1})\\right],H-h+1\\}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\geq r_{i,h}(s_{h},a_{h})+\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\mid s_{h},a_{h}),o_{h+1}\\sim\\mathcal{O}_{h+1}(\\cdot\\mid s_{h+1})}\\left[\\widehat{V}_{i,h+1}^{(m_{i}\\circ\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}(c_{h+1})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Now we conclude ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i,h}^{\\mathrm{high},k}\\big(c_{h},\\gamma_{h}\\big)}\\\\ &{\\geq\\mathbb{E}_{s_{h},p_{h}\\sim\\hat{P}_{h}(\\cdot,\\cdot\\,|\\,\\hat{c}_{h})}\\mathbb{E}_{\\{a_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,p_{j,h})\\}_{j\\in[n]}}}\\\\ &{\\qquad\\qquad\\qquad\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h}),o_{h+1}\\sim0_{h+1}(\\cdot\\,|\\,s_{h+1})}\\left[r_{i,h}\\big(s_{h},a_{h}\\big)+\\widehat{V}_{i,h+1}^{(m_{i}\\diamond\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}\\big(c_{h+1}\\big)\\right]}\\\\ &{=\\widehat{Q}_{i\\;h}^{(m_{i}\\diamond\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}\\big(c_{h},\\gamma_{h}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By definition, we have $Q_{i,h}^{\\mathrm{high},k}(c_{h},\\gamma_{h})\\;\\;=\\;\\;Q_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h},\\gamma_{h})$ , thus proving ${\\cal Q}_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h},\\gamma_{h})\\;\\;\\geq$ $\\widehat{Q}_{i,h}^{(m_{i}\\diamond\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}(c_{h},\\gamma_{h})$ . Now for the value function, no te that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{i,h}^{\\mathrm{high},k}(c_{h})=\\mathbb{E}_{\\omega_{h}}Q_{i,h}^{\\mathrm{high},k}(c_{h},\\{\\pi_{j,h}^{k}(\\cdot\\,|\\,\\omega_{j,h},\\widehat{c}_{h},\\cdot)\\}_{j\\in[n]})}\\\\ &{\\quad\\quad\\quad\\quad\\geq\\mathbb{E}_{\\omega_{h}}Q_{i,h}^{\\mathrm{high},k}(c_{h},\\{\\pi_{j,h}^{k}(\\cdot\\,|\\,\\omega_{j,h},\\widehat{c}_{h},\\cdot)\\}_{j\\in[n]\\backslash\\{i\\}},(m_{i,h}\\diamond\\pi_{i,h}^{k})(\\cdot\\,|\\,\\omega_{i,h},\\widehat{c}_{h},\\cdot))}\\\\ &{\\quad\\quad\\quad\\geq\\mathbb{E}_{\\omega_{h}}\\widehat{Q}_{i,h}^{(m_{i}\\circ\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}(c_{h},\\{\\pi_{j,h}^{k}(\\cdot\\,|\\,\\omega_{j,h},\\widehat{c}_{h},\\cdot)\\}_{j\\in[n]\\backslash\\{i\\}},(m_{i,h}\\diamond\\pi_{i,h}^{k})(\\cdot\\,|\\,\\omega_{i,h},\\widehat{c}_{h},\\cdot))}\\\\ &{\\quad\\quad\\quad=\\widehat{V}_{i,h}^{(m_{i}\\circ\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}(c_{h}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where the first step is by the property of Bayesian CE, and the second step is by $Q_{i,h}^{\\mathrm{high},k}(c_{h},\\gamma_{h})\\geq$ $\\widehat{Q}_{i,h}^{(m_{i}\\diamond\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}(c_{h},\\gamma_{h})$ for any $\\gamma_{h}~\\in~\\Gamma_{h}$ . Again by definition, we proved $V_{i,h}^{\\mathrm{high},k}(\\widehat{c}_{h})\\;\\;=$ $V_{i,h}^{\\mathrm{high},k}(c_{h})\\geq\\widehat{V}_{i,h}^{(m_{i}\\diamond\\pi_{i}^{k})\\odot\\pi_{-i}^{k},\\mathcal{G}}(c_{h})$ \u53e3 ", "page_idx": 58}, {"type": "text", "text": "Lemma J.14 (Pessimism). With probability $1-\\delta$ , for any $k\\in[K]$ , for Algorithm 7, it holds that for any $i\\in[n],h\\in[H]$ ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i,h}^{\\mathrm{low},k}(\\widehat{c}_{h},\\gamma_{h})\\leq\\widehat{Q}_{i,h}^{\\pi^{k},\\mathcal{G}}(c_{h},\\gamma_{h})}\\\\ &{\\quad\\quad V_{i,h}^{\\mathrm{low},k}(\\widehat{c}_{h})\\leq\\widehat{V}_{i,h}^{\\pi^{k},\\mathcal{G}}(c_{h}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Proof. We prove by backward induction on $h$ . Obviously, the lemma holds for $h=H+1$ . Now we assume the lemma holds for $h+1$ . Similar to the proof of the previous lemma, we note by inductive hypothesis ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i,h}^{\\mathrm{low},k}(c_{h},\\gamma_{h})\\leq\\mathbb{E}_{s_{h},p_{h}\\sim\\hat{P}_{h}(\\cdot,\\cdot\\,|\\,\\hat{c}_{h})}\\mathbb{E}_{\\{a_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,p_{j,h})\\}_{j\\in[n]}}}\\\\ &{\\qquad\\qquad\\operatorname*{max}\\left\\{r_{i,h}(s_{h},a_{h})-b_{h}^{k-1}(s_{h},a_{h})+\\mathbb{E}_{s_{h+1},o_{h+1}\\sim\\hat{\\tilde{\\beta}}_{h}^{k-1}(\\cdot,\\cdot\\,|\\,s_{h},a_{h})}\\left[\\widehat{V}_{i,h+1}^{\\pi^{k},\\mathcal{G}}(c_{h+1})\\right],0\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where for any $s_{h},p_{h},a_{h}$ , we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\,b_{h}^{k-1}(s_{h},a_{h})+\\mathbb{E}_{o_{h+1}\\sim\\hat{\\mathbb{J}}_{h}^{k-1}(\\cdot\\,|\\,s_{h},a_{h})}\\left[V_{i,h+1}^{\\mathrm{low},k}(c_{h+1})\\right]}\\\\ &{\\leq-b_{h}^{k-1}(s_{h},a_{h})+(H-h)\\|\\widehat{\\mathbb{J}}_{h}^{k-1}(\\cdot\\,|\\,s_{h},a_{h})-\\mathbb{P}^{\\mathcal{G}}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h}),o_{h+1}\\sim\\mathbb{O}_{h+1}(\\cdot\\,|\\,s_{h+1})}\\left[\\widehat{V}_{i,h+1}^{\\pi^{k},\\mathcal{G}}(c_{h+1})\\right]}\\\\ &{\\leq\\mathbb{E}_{s_{h+1}\\sim\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h}),o_{h+1}\\sim\\mathbb{O}_{h+1}(\\cdot\\,|\\,s_{h+1})}\\left[\\widehat{V}_{i,h+1}^{\\pi^{k},\\mathcal{G}}(c_{h+1})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where the last step again comes from the construction of our bonus in Equation (J.3) and Lemma J.10. Therefore, we conclude ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i,h}^{\\mathrm{low},k}(c_{h},\\gamma_{h})\\leq\\mathbb{E}_{s_{h},p_{h}\\sim\\hat{P}_{h}(\\cdot,\\cdot\\,|\\,\\hat{\\tau}_{h})}\\mathbb{E}_{\\{a_{j,h}\\sim\\gamma_{j,h}(\\cdot\\,|\\,p_{j,h})\\}_{j\\in[n]}}}\\\\ &{\\qquad\\qquad\\qquad\\mathbb{E}_{o_{h+1}\\sim\\hat{\\mathbb{J}}_{h}^{k-1}(\\cdot\\,|\\,s_{h},a_{h})}\\left[r_{i,h}(s_{h},a_{h})+\\widehat{V}_{i,h+1}^{\\pi^{k},\\mathscr{G}}(c_{h+1})\\right]}\\\\ &{\\qquad\\qquad=\\widehat{Q}_{i,h}^{\\pi^{k},\\mathscr{G}}(c_{h},\\gamma_{h}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Similarly, for value function, it holds that ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{V_{i,h}^{\\mathrm{low},k}(c_{h})=\\mathbb{E}_{\\omega_{h}}Q_{i,h}^{\\mathrm{low},k}(c_{h},\\{\\pi_{j,h}^{k}(\\cdot\\,\\vert\\,\\omega_{j,h},\\widehat{c}_{h},\\cdot)\\}_{j\\in[n]})}\\\\ &{}&{\\quad\\le\\!\\mathbb{E}_{\\omega_{h}}\\widehat{Q}_{i,h}^{\\pi^{k},\\mathcal{G}}(c_{h},\\{\\pi_{j,h}^{k}(\\cdot\\,\\vert\\,\\omega_{j,h},\\widehat{c}_{h},\\cdot)\\}_{j\\in[n]})=\\widehat{V}_{i,h}^{\\pi^{k},\\mathcal{G}}(c_{h}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "thus proving our lemma. ", "page_idx": 58}, {"type": "text", "text": "Theorem J.15 (NE/CCE version). With probability $1-\\delta$ , Algorithm 7 enjoys the regret guarantee of ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k\\in[K]}\\operatorname*{max}_{i\\in[n]}\\left(\\operatorname*{max}_{\\pi_{i}^{\\prime}\\in\\Pi_{i}}V_{i,1}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{1}^{k})-V_{i,1}^{\\pi^{k},\\mathcal{G}}(c_{1}^{k})\\right)}\\\\ &{\\leq\\!\\mathcal{O}(K H^{2}\\epsilon_{\\mathrm{belief}}+H^{2}\\sqrt{S A O K\\log(S A H K/\\delta)}+H^{2}S A\\sqrt{O\\log(S A H K/\\delta)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Correspondingly, this implies one can learn an $(\\epsilon+H^{2}\\epsilon_{\\mathrm{belief}})\u2013\\mathrm{N}$ E if $\\mathcal{G}$ is zero-sum and $(\\epsilon+H_{\\mathrm{belief}}^{\\epsilon})$ - CCE if $\\mathcal{G}$ is general-sum with sample complexity $\\mathcal{O}\\big(\\frac{H^{4}S A O\\log(S A H O/\\delta)}{\\epsilon^{2}}\\big)$ and computational complexity $\\begin{array}{r}{\\operatorname{POLY}\\!\\left(S,A,O,H,\\frac{1}{\\epsilon},\\log\\frac{1}{\\delta}\\right)}\\end{array}$ . ", "page_idx": 58}, {"type": "text", "text": "Proof. Note for any given $i\\in[n]$ and $\\pi_{i}^{\\prime}\\in\\Pi_{i}$ , by Lemma J.12 and Lemma J.14, it holds ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi_{i}^{\\prime}\\in\\Pi_{i}}V_{i,1}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{1}^{k})-V_{i,1}^{\\pi^{k},\\mathcal{G}}(c_{1}^{k})\\le V_{i,h}^{\\mathrm{high},k}(c_{h}^{k})-V_{i,h}^{\\mathrm{low},k}(c_{h}^{k}).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Therefore, it suffices to bound $V_{i,h}^{\\mathrm{high},k}(c_{h}^{k})-V_{i,h}^{\\mathrm{low},k}(c_{h}^{k})$ ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{i,n}^{(k)}\\sim(\\mathcal{E}_{k_{1},n}^{(k)}\\sim V_{i,n}^{(k)},\\mathcal{E}_{k_{2},n}^{(k)})}\\\\ &{=\\mathbb{E}_{u_{1},\\mathcal{H}_{n}\\sim\\mathcal{H}_{i,n}^{(k)}\\sim\\mathcal{E}_{k_{2},n}^{(k)}}\\mathbb{E}_{o_{2},\\Big[\\mathcal{H}_{n}^{(k)},\\big(\\mathcal{E}_{k_{1}}^{\\texttt{H}_{k}},\\big(\\mathcal{E}_{k_{2},n}^{\\texttt{H}_{k}},\\big);\\mathcal{E}_{k_{1}}(\\pi)\\big)-Q_{i,n}^{\\texttt{H}_{k}},\\big(\\mathcal{E}_{k_{2},n}^{\\texttt{H}_{k}},\\big\\{\\pi_{k_{1}}^{\\texttt{H}_{k}},\\big\\};\\mathcal{E}_{k_{2},n}^{\\texttt{H}_{k}},\\big)\\Big]}\\\\ &{\\leq\\mathbb{E}_{u_{1},\\mathcal{H}_{n}\\sim\\mathcal{H}_{i,n}^{(k)}\\sim\\mathcal{E}_{k_{2},n}^{(k)}}\\mathbb{E}_{o_{2},\\Big[\\mathcal{H}_{n}^{(k)},\\big(\\mathcal{E}_{k_{1}}^{\\texttt{H}_{k}},\\big\\{\\pi_{k_{2}}^{\\texttt{H}_{k}},\\big\\};\\mathcal{E}_{k_{1}}(\\pi)\\big)-Q_{i,n}^{\\texttt{H}_{k}\\texttt{H}_{n}^{(k)}},\\big\\{\\pi_{k_{2},n}^{\\texttt{H}_{k}},\\big\\};\\mathcal{E}_{k_{1}}(\\pi)\\big]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +(H-h+1)\\epsilon_{1}(\\mathcal{E}_{k}^{\\texttt{H}})}\\\\ &{\\leq\\mathbb{E}_{u_{1},\\mathcal{H}_{n}\\sim\\mathcal{H}_{i,n}^{(k)}\\sim\\mathcal{E}(\\mathcal{E}_{k_{1},n}^{\\texttt{H}_{k}},\\mathcal{E}_{k_{2},n}^{\\texttt{H}_{k}},\\big(\\mathcal{E}_{n},\\Lambda_{2}^{\\texttt{H}_{n}},\\Lambda_{\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where we define the Martingale difference sequence as follows ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{k,h}^{1}:=\\mathbb{E}_{s_{h},p_{h}\\sim\\mathbb{P}^{Q}(\\cdot,\\cdot\\,|\\,c_{h}^{k})\\mathbb{E}_{\\omega_{h}}\\mathbb{E}_{\\{a_{j,h}\\sim\\pi_{j,h}^{k}(\\cdot\\,|\\,\\omega_{j,h},\\hat{c}_{h}^{k},p_{j,h})\\}_{j\\in[n]}}\\bigg[Q_{i,h}^{\\mathrm{high},k}(c_{h}^{k},p_{h},s_{h},a_{h})}\\\\ &{\\phantom{Z_{k,h}^{1}\\cdot\\big(c_{h}^{k},p_{h},s_{h}^{k},a_{h}^{k})}-Q_{i,h}^{\\mathrm{low},k}(c_{h}^{k},p_{h}^{k},s_{h},a_{h})\\bigg]}\\\\ &{\\quad\\quad-\\left(Q_{i,h}^{\\mathrm{high},k}(c_{h}^{k},p_{h}^{k},s_{h}^{k},a_{h}^{k})-Q_{i,h}^{\\mathrm{low},k}(c_{h}^{k},p_{h}^{k},s_{h}^{k},a_{h}^{k})\\right)}\\\\ &{Z_{k,h}^{2}:=\\mathbb{E}_{o_{h+1}\\sim\\mathbb{P}^{Q}(\\cdot\\,|\\,s_{h}^{k},a_{h}^{k})}\\left[V_{i,h+1}^{\\mathrm{high},k}(c_{h+1})-V_{i,h+1}^{\\mathrm{low},k}(c_{h+1})\\right]-\\left(V_{i,h+1}^{\\mathrm{high},k}(c_{h+1}^{k})-V_{i,h+1}^{\\mathrm{low},k}(c_{h+1}^{k})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "and the error of the belief is defined as ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\epsilon_{h}(c_{h}^{k}):=\\|\\widehat{P}_{h}(\\cdot,\\cdot\\,|\\,\\widehat{c}_{h}^{k})-\\mathbb{P}^{\\mathcal{G}}(\\cdot,\\cdot\\,|\\,c_{h}^{k})\\|_{1}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Since $|Z_{k,h}^{1}|\\leq H$ , $|Z_{k,h}^{2}|\\leq H$ , and $\\epsilon_{h}(c_{h}^{k})\\leq2$ , by Azuma-Hoeffding bound, we conclude ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k,h}Z_{k,h}^{1}\\leq\\mathcal{O}(H\\sqrt{H K}),}\\\\ &{\\displaystyle\\sum_{k,h}Z_{k,h}^{2}\\leq\\mathcal{O}(H\\sqrt{H K}),}\\\\ &{\\displaystyle\\sum_{k,h}\\epsilon_{h}(c_{h}^{k})\\leq\\sum_{k}\\mathbb{E}_{\\pi^{k}}^{\\mathcal{G}}\\left[\\displaystyle\\sum_{h}\\epsilon_{h}(c_{h})\\right]+\\mathcal{O}(\\sqrt{H K})\\leq K H\\epsilon_{\\mathrm{belief}}+\\mathcal{O}(\\sqrt{H K}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Meanwhile, by the pigeonhole principle, it holds that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k,h}b_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\\leq H\\sqrt{O\\log(S A H K/\\delta)}\\sum_{k,h}\\frac{1}{\\sqrt{\\operatorname*{max}\\{1,N_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\\}}}}\\\\ &{\\quad\\quad\\leq\\mathcal{O}\\left(H\\sqrt{O\\log(S A H K/\\delta)}(H\\sqrt{S A K}+H S A)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Now by Lemma J.12 and Lemma J.14 and putting everything together, we conclude ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k\\in[K]}\\operatorname*{max}_{i\\in[n]}\\left(\\operatorname*{max}_{\\pi_{i}^{\\prime}\\in\\Pi_{i}}\\widehat V_{i,1}^{\\pi_{i}^{\\prime}\\times\\pi_{-i},\\mathcal{G}}(c_{1}^{k})-\\widehat V_{i,1}^{\\pi,\\mathcal{G}}(c_{1}^{k})\\right)}\\\\ &{\\leq K H^{2}\\epsilon_{\\mathrm{belief}}+\\mathcal{O}(H^{2}\\sqrt{S A O K\\log(S A H K/\\delta)}+H^{2}S A\\sqrt{O\\log(S A H K/\\delta)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Now by Lemma J.11, we proved the regret guarantees as follows ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k\\in[K]}\\operatorname*{max}_{i\\in[n]}\\left(\\operatorname*{max}_{\\pi_{i}^{\\prime}\\in\\Pi_{i}}V_{i,1}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal{G}}(c_{1}^{k})-V_{i,1}^{\\pi^{k},\\mathcal{G}}(c_{1}^{k})\\right)}\\\\ &{\\leq\\mathcal{O}(K H^{2}\\epsilon_{\\mathrm{belief}}+H^{2}\\sqrt{S A O K\\log(S A H K/\\delta)}+H^{2}S A\\sqrt{O\\log(S A H K/\\delta)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "For the PAC guarantees, since we define $k^{\\star}\\in\\arg\\operatorname*{min}_{k\\in[K]}V_{i,1}^{\\mathrm{high},k}(c_{1}^{k})-V_{i,1}^{\\mathrm{low},k}(c_{1}^{k})$ , we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{CCE-gap}(\\pi^{k^{\\star}})\\leq\\mathcal{O}(H^{2}\\epsilon_{\\mathrm{belief}})+V_{i,h}^{\\mathrm{high},k^{\\star}}(c_{1}^{k^{\\star}})-V_{i,h}^{\\mathrm{low},k^{\\star}}(c_{1}^{k^{\\star}})}\\\\ &{\\phantom{\\sum}\\leq\\mathcal{O}(H^{2}\\epsilon_{\\mathrm{belief}})+\\frac{1}{K}\\displaystyle\\sum_{k\\in[K]}V_{i,h}^{\\mathrm{high},k}(c_{1}^{k})-V_{i,h}^{\\mathrm{low},k}(c_{1}^{k})}\\\\ &{\\phantom{\\sum}\\leq\\mathcal{O}(H^{2}\\epsilon_{\\mathrm{belief}}+H^{2}\\sqrt{S A O\\log(S A H K/\\delta)/K}+\\frac{H^{2}S A}{K}\\sqrt{O\\log(S A H K/\\delta)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Finally, for two-player zero-sum games, we denote $\\widehat{\\pi}^{k^{\\star}}$ to be the marginalized policy of $\\pi^{k^{\\star}}$ . Then we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{NE-gap}(\\widehat{\\pi}^{k^{\\star}})\\leq\\mathrm{CCE-gap}(\\pi^{k^{\\star}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "thus concluding our theorem. ", "page_idx": 60}, {"type": "text", "text": "Theorem J.16 (CE version). With probability $1-\\delta$ , Algorithm 7 enjoys the regret guarantee of ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k\\in[K]}\\operatorname*{max}_{i\\in[n]}\\left(\\operatorname*{max}_{m_{i}^{\\prime}\\in\\mathcal{M}_{i}}V_{i,1}^{m_{i}\\diamond\\pi_{i}^{k}\\odot\\pi_{-i}^{k},\\mathcal{G}}(c_{1}^{k})-V_{i,1}^{\\pi^{k},\\mathcal{G}}(c_{1}^{k})\\right)}\\\\ &{\\leq\\mathcal{O}(K H^{2}\\epsilon_{\\mathrm{belief}}+H^{2}\\sqrt{S A O K\\log(S A H K/\\delta)}+H^{2}S A\\sqrt{O\\log(S A H K/\\delta)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Correspondingly, this implies one can learn an $(\\epsilon\\,+\\,H^{2}\\epsilon_{\\mathrm{belief}}){-}\\mathrm{C}$ E with sample complexity $\\mathcal{O}\\big(\\frac{H^{4}S A O\\log(S A H O/\\delta)}{\\epsilon^{2}}\\big)$ . ", "page_idx": 60}, {"type": "text", "text": "Proof. Then proof follows as that of Theorem J.15, where we only need to change the first step of the proof as ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\widehat V_{i,h}^{\\pi_{i}^{\\prime}\\times\\pi_{-i}^{k},\\mathcal G}(c_{h}^{k})-\\widehat V_{i,h}^{\\pi^{k},\\mathcal G}(c_{h}^{k})\\leq V_{i,h}^{\\mathrm{high},k}(c_{h}^{k})-V_{i,h}^{\\mathrm{low},k}(c_{h}^{k}),\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "by Lemma J.13 and Lemma J.14, and the remaining steps are exactly the same. ", "page_idx": 60}, {"type": "text", "text": "Lemma J.17 (Adapted from Theorem H.5). Algorithm 8 can learn the approximate POMDP with transition $\\widehat{\\mathbb{T}}_{1:H}$ and emission $\\widehat{\\mathbb{O}}_{1:H}$ such that for any policy $\\pi\\in\\Pi^{\\mathrm{gen}}$ and $h\\in[H]$ ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pi}^{\\mathcal{G}}\\left[\\|\\mathbb{T}_{h}(\\cdot\\,|\\,s_{h},a_{h})-\\widehat{\\mathbb{T}}_{h}(\\cdot\\,|\\,s_{h},a_{h})\\|_{1}+\\|\\mathbb{O}_{h}(\\cdot\\,|\\,s_{h})-\\widehat{\\mathbb{O}}_{h}(\\cdot\\,|\\,s_{h})\\|_{1}\\right]\\le\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "using sample complexity $\\begin{array}{r}{\\widetilde{\\mathcal{O}}(\\frac{S^{2}A H O+S^{3}A H}{\\epsilon^{2}}+\\frac{S^{4}A^{2}H^{5}}{\\epsilon})}\\end{array}$ with probability $1-\\delta$ ", "page_idx": 60}, {"type": "text", "text": "Proof. Note that Algorithm 8 is essentially treating the POSG $\\mathcal{G}$ as a centralized MDP and running Algorithm 4, where the only modifications we make in Algorithm 8 is that we take the controller set into considerations when learning the models. Specifically, for the transition $\\widehat{\\mathbb{T}}_{h}$ , what we estimate is only $\\widehat{\\mathbb{T}}_{h}\\big(s_{h+1}\\,\\vert\\,s_{h},a_{\\mathcal{T}_{h},h}\\big)$ instead of $\\widehat{\\mathbb{T}}_{h}\\big(s_{h+1}\\,\\vert\\,s_{h},a_{h}\\big)$ . Therefore, the sample complexity of Algorithm 8  will not be worse than that of A lgorithm 4. \u53e3 ", "page_idx": 60}, {"type": "text", "text": "Proof of Theorem 7.7: ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Note that the proof idea essentially resembles that of Theorem H.6, where we construct the model $\\mathcal{G}^{\\mathrm{trunc}}$ for $\\mathcal{G}$ as exactly the same way of constructing $\\mathcal{P}^{\\mathrm{trunc}}$ for $\\mathcal{P}$ . Therefore, by Lemma H.9, we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}^{\\mathcal{G}}[\\|\\mathbb{P}^{\\mathcal{G}}(\\cdot,\\cdot\\,|\\,c_{h})-\\mathbb{P}^{\\mathcal{G}^{\\mathrm{max}}}(\\cdot,\\cdot\\,|\\,c_{h})\\|_{1}]\\le2\\sum_{\\overline{{\\tau}}_{h}}|\\mathbb{P}^{\\pi,\\mathcal{G}}(\\overline{{\\tau}}_{h})-\\mathbb{P}^{\\pi,\\mathcal{G}^{\\mathrm{max}}}(\\overline{{\\tau}}_{h})|\\le4H S\\epsilon_{1}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Meanwhile, we can construct $\\widehat{\\mathcal{G}}^{\\mathrm{trunc}}$ and $\\widehat{\\mathcal{G}}^{\\mathrm{sub}}$ using the exactly way as for $\\widehat{\\mathcal{P}}^{\\mathrm{trunc}}$ and $\\widehat{\\mathcal{P}}^{\\mathrm{sub}}$ , where $\\widehat{\\mathcal{G}}^{\\mathrm{sub}}$ is an $\\gamma$ -observable POSGs. ", "page_idx": 60}, {"type": "text", "text": "Now, according to [50], for all examples in Appendix C.3, there exists a compression function that maps $c_{h}$ to $\\widehat{c}_{h}$ such that the size of the compressed common information is quasi-polynomial, ", "page_idx": 60}, {"type": "text", "text": "i.e., $\\begin{array}{r}{\\widehat{C}_{h}\\leq(A O)^{C\\gamma^{-4}}\\log\\frac{S H}{\\epsilon_{2}}}\\end{array}$ for some absolute constant $C$ , and corresponding approximate belief $\\{\\widehat{P}_{h}:\\widehat{\\mathcal{C}}_{h}\\to\\Delta(S\\times\\mathcal{P}_{h})\\}_{h\\in[H]}$ such that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}^{\\widehat{\\mathcal{G}}^{\\mathrm{sub}}}\\|\\mathbb{P}^{\\widehat{\\mathcal{G}}^{\\mathrm{sub}}}(\\cdot,\\cdot\\,|\\,c_{h})-\\widetilde{P}_{h}(\\cdot,\\cdot\\,|\\,\\widehat{c}_{h})\\|_{1}\\le\\epsilon_{2}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Therefore, we can do the same augmentation for ${\\widetilde{P}}_{h}$ on states from $S_{h}^{\\mathrm{low}}$ to construct the approximate belief $\\widehat{P}_{h}$ as in the proof of Theorem H.6, and the remaining steps are exactly the same as the proof of The orem H.6. This will lead to a total of polynomial-time and polynomial-sample complexities. $\\mid$ ", "page_idx": 61}, {"type": "text", "text": "J.1 Background on Bayesian Games ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "The Bayesian game is a generalization of normal-form games in partially observable settings. Specifically, a bayesian game is specified as $(n,\\{A_{i}\\}_{i\\in[n]},\\{\\bar{\\Theta}_{i}\\}_{i\\in[n]},\\{r_{i}\\}_{i\\in[n]},\\mu)$ , where $n$ is the number of players, $A_{i}$ is the actor space, $\\Theta_{i}$ is the type space, $r_{i}:\\Theta\\times\\mathcal{A}\\rightarrow[0,1]$ is the reward function, and $\\mu$ is the prior distribution of the joint type. At the beginning of the game, a type $\\theta=(\\theta_{i})_{i\\in[n]}$ is drawn from the prior distribution $\\mu\\in\\Delta(\\Theta)$ . Then each agent $i$ gets its only type $\\theta_{i}$ and take the action $a_{i}$ . With a slight abuse of the notation, we define a strategy of an agent as $\\gamma_{i}\\in\\Gamma_{i}=\\{\\Theta_{i}\\rightarrow\\Delta(A_{i})\\}$ . We define $J_{i}(\\gamma_{i},\\gamma_{-i})$ to be the expected rewards for agent $i$ , given the joint strategy $\\gamma_{i},\\gamma_{-i}$ . ", "page_idx": 61}, {"type": "text", "text": "By definition, $J_{i}(\\gamma_{i},\\gamma_{-i})$ can be evaluated as ", "page_idx": 61}, {"type": "equation", "text": "$$\nJ_{i}(\\gamma_{i},\\gamma_{-i}):=\\mathbb{E}_{\\theta\\sim\\mu}\\mathbb{E}_{\\{a_{j}\\sim\\gamma_{j}(\\cdot\\,\\vert\\,\\theta_{j})\\}_{j\\in[n]}}r_{i}(\\theta,a).\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Bayesian NE. We define $\\gamma^{\\star}$ is an $\\epsilon_{}$ -NE is it satisfies that ", "page_idx": 61}, {"type": "equation", "text": "$$\nJ_{i}(\\gamma^{\\star})\\geq J_{i}(\\gamma_{i}^{\\prime},\\gamma_{-i}^{\\star})-\\epsilon,\\forall i\\in[n],\\gamma_{i}^{\\prime}\\in\\Gamma_{i}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Bayesian CCE. We say a distribution of joint strategies $s\\in\\Delta(\\Gamma)$ to be a $\\epsilon$ -Bayesian CCE if it satisfies ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\gamma\\sim s}J_{i}(\\gamma)\\ge\\mathbb{E}_{\\gamma\\sim s}J_{i}(\\gamma_{i}^{\\prime},\\gamma_{-i})-\\epsilon,\\forall i\\in[n],\\gamma_{i}^{\\prime}\\in\\Gamma_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "(Agent-form) Bayesian CE. We say a distribution of joint strategies $s\\in\\Delta(\\Gamma)$ to be an $\\epsilon$ -agentform Bayesian CE if it satisfies ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\gamma\\sim s}J_{i}(\\gamma)\\geq\\mathbb{E}_{\\gamma\\sim s}J_{i}(m_{i}\\diamond\\gamma_{i},\\gamma_{-i})-\\epsilon,\\forall i\\in[n],m_{i}^{\\prime}\\in\\mathcal{M}_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where $\\mathcal{M}_{i}\\,=\\,\\{\\Theta_{i}\\,\\times\\,A_{i}\\,\\to\\,A_{i}\\}$ is the space for strategy modification, where $m_{i}$ modifies $\\gamma_{i}$ as follows: given current type $\\theta_{i}$ and the recommended action $a_{i}$ , the strategy modification changes the action to the another action $m_{i}(\\theta_{i},a_{i})$ . ", "page_idx": 61}, {"type": "text", "text": "Note that Bayesian NE for zero-sum games, and (agent-form) Bayesian CE/CCE are all tractable solution concepts and can be computed with polynomial computational complexity, e.g., [29, 32, 23]. ", "page_idx": 61}, {"type": "text", "text": "K Concluding Remarks and Limitations ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "In this paper, we aim to understand the provable benefits of privileged information for partially observable RL problems under two empirically successful paradigms, expert distillation [14, 63, 57] and asymmetric actor-critic [67, 7, 3], which represent privileged policy and privileged value learning, respectively, with an emphasis on studying both the computational and sample efficiencies of the algorithms. Our results (as summarized in Table 1) showed that privileged information does improve learning efficiency in a series of known POMDP subclasses. One potential limitation of our work is that we only focused on the case with exact state information. It remains to explore whether such an assumption can be further relaxed, e.g., when privileged state information may be biased, partially observable, or delayed, as usually happens in practice, and how our theoretical results may be affected. Meanwhile, as an initial theoretical study, we have been primarily focusing on the tabular settings (except Appendix G), and it would be interesting to extend the results to function-approximation settings to handle massively large state, action, and observation spaces in practice. ", "page_idx": 61}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: Our abstract indeed accurately summarizes out paper\u2019s contribution and scope. Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 62}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: We clearly outline all of our assumption needed to derive our theoretical results. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 62}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: We prove every theorem or lemma in this paper. Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 63}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: Yes, detailed are introduced in Appendix I ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 63}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: We have included detailed introductions on the algorithms and environment to reproduce our results. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 64}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: In Appendix I. Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 64}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Justification: We have reported the number of repeated experiments in Appendix I and error/variance bars in Figure 2 and Table 2. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 64}, {"type": "text", "text": "", "page_idx": 65}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: We have provides all details in Appendix I. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 65}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: It does. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 65}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: We address the societal impacts of our paper in Appendix A. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 65}, {"type": "text", "text": "", "page_idx": 66}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 66}, {"type": "text", "text": "Justification: There is no such risk. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 66}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 66}, {"type": "text", "text": "Justification: No external codes, data, models are used. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 66}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 67}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: We do not release any new assets. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 67}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: No such experiments were involved. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 67}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: No such approval was needed. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 67}]