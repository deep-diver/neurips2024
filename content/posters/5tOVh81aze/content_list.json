[{"type": "text", "text": "Language models scale reliably with over-training and on downstream tasks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Scaling laws are useful guides for derisking expensive training runs, as they predict   \n2 performance of large models using cheaper, small-scale experiments. However,   \n3 there remain gaps between current scaling studies and how language models are   \n4 ultimately trained and evaluated. For instance, scaling is usually studied in the   \n5 compute-optimal training regime (i.e., \u201cChinchilla optimal\u201d regime). In contrast,   \n6 models are often over-trained to reduce inference costs. Moreover, scaling laws   \n7 mostly predict loss on next-token prediction, but models are usually compared on   \n8 downstream task performance. To address both shortcomings, we create a testbed   \n9 of 104 models with 0.011B to 6.9B parameters trained with various numbers of   \n10 tokens on three data distributions. First, we fti scaling laws that extrapolate in both   \n11 the amount of over-training and the number of model parameters. This enables us   \n12 to predict the validation loss of a 1.4B parameter, 900B token run (i.e., $32\\times$ over  \n13 trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)\u2014each   \n14 from experiments that take $300\\times$ less compute. Second, we relate the perplexity of   \n15 a language model to its downstream task performance by proposing a power law.   \n16 We use this law to predict top-1 error averaged over downstream tasks for the two   \n17 aforementioned models, using experiments that take $20\\times$ less compute. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 Training large language models is expensive. Furthermore, training high-quality models requires a   \n20 complex recipe of algorithmic techniques and training data. To reduce the cost of finding successful   \n21 training recipes, researchers first evaluate ideas with small experiments and then extrapolate their   \n22 efficacy to larger model and data regimes via scaling laws. With reliable extrapolation, it is possible   \n23 to quickly iterate at small scale and still pick the method that will perform best for the final large   \n24 training run. Indeed, this workflow has become commonplace for training state-of-the-art language   \n25 models like Chinchilla 70B [45], PaLM 540B [19], GPT-4 [76], and many others.   \n26 Despite their importance for model development, published scaling laws differ from the goals of   \n27 training state-of-the-art models in important ways. For instance, scaling studies usually focus on the   \n28 compute-optimal training regime (\u201cChinchilla optimality\u201d [45]), where model and dataset size are set   \n29 to yield minimum loss for a given compute budget. However, this setting ignores inference costs.   \n30 As larger models are more expensive at inference, it is now common practice to over-train smaller   \n31 models [113]. Another potential mismatch is that most scaling laws quantify model performance by   \n32 perplexity in next-token prediction instead of accuracy on widely used benchmark datasets. However,   \n33 practitioners usually turn to benchmark performance, not loss, to compare models.   \n34 In this paper, we conduct an extensive set of experiments to address both scaling in the over-trained   \n35 regime and benchmark performance prediction.   \n36 Motivated by the practice of training beyond compute-optimality, we first investigate whether scaling   \n37 follows reliable trends in the over-trained regime. We notice, as implied by Hoffmann et al. [45], for a   \n38 set of models of different sizes trained with a constant ratio of tokens to parameters, models\u2019 reducible   \n39 loss $L^{\\prime}$ [43, 45] follows a power law $(L^{\\prime}=\\lambda\\cdot C^{-\\eta})$ ) in the amount of training compute $C$ . We   \n40 find that as one increases the ratio of tokens to parameters, corresponding to more over-training, the   \n41 scaling exponent $\\eta$ remains about the same, while the scalar $\\lambda$ changes. We explain our observations   \n42 by reparameterizing existing scaling laws in relation to the amount of over-training.   \n43 To establish empirically that scaling extrapolates in the over-trained regime, we further experiment   \n44 with a testbed of 104 models, trained from scratch on three different datasets: C4 [88, 27],   \n45 RedPajama [112], and RefinedWeb [82]. We find that scaling laws fti to small models can accurately   \n46 predict the performance of larger models that undergo more over-training. Figure 1 (left) illustrates our   \n47 main over-training result, where we invest $2.4e19$ FLOPs to extrapolate the C4 validation performance   \n48 of a 1.4B parameter model trained on 900B tokens, which requires $300\\times$ more compute to train.   \n49 In addition to over-training, we also investigate if scaling laws can predict the performance of a   \n50 model on downstream tasks. We establish a power law relationship between language modeling   \n51 perplexity and the average top-1 error on a suite of downstream tasks. While it can be difficult to   \n52 predict the error on individual tasks, we find it possible to predict aggregate performance from a   \n53 model\u2019s perplexity among models trained on the same training data. Figure 1 (right) presents our   \n54 main downstream error prediction result, where we invest $2.7e20$ FLOPs to predict the average top-1   \n55 error over a set of downstream tasks to within 1 percentage point for a 6.9B compute-optimal model,   \n56 which requires $20\\times$ more compute to train.   \n57 Our results suggest that the proposed scaling laws are promising to derisk (i) the effects of over  \n58 training models and (ii) the downstream performance of scaling up training recipes. To facilitate   \n59 further research on reliable scaling, we will release all experiments and models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "5tOVh81aze/tmp/3bde27771b9366bd52ef19ebbd1299aa99da340d6e5385e591dc1d422261c10b.jpg", "img_caption": ["Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit a scaling law for model validation loss, parameterized by (i) a token multiplier $M=N/D$ , which is the ratio of training tokens $D$ to parameters $N$ and (ii) the compute $C$ in FLOPs used to train a model, approximated by $C=6N D$ . Larger values of $M$ specify more over-training. We are able to extrapolate, in both $N$ and $M$ , the validation performance of models requiring more than $300\\times$ the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over $20\\times$ the compute. For this figure, we train all models on RedPajama [112]. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "60 2 Developing scaling laws for over-training and downstream tasks ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "61 In this section, we develop scaling laws to predict over-trained and downstream performance. First,   \n62 we provide key definitions (Section 2.1). We next present a scaling law for over-training drawing on   \n63 empirical observation and prior work (Section 2.2). To connect loss scaling and downstream error   \n64 prediction, we observe that average top-1 error decreases exponentially as a function of validation loss,   \n65 which we formalize as a novel scaling law (Section 2.3). In later sections, we build an experimental   \n66 setup (Section 3) to quantify the extent to which our scaling laws extrapolate reliably (Section 4). ", "page_idx": 1}, {"type": "image", "img_path": "5tOVh81aze/tmp/5eb43c8d55193828aa85b9085ce0a6402d96c4d87374017deb4f9f8e63e497c6.jpg", "img_caption": ["Figure 2: Scaling in the over-trained regime follows consistent power law exponents. We notice parallel lines in the log-log plots of reducible loss vs. training compute for a range of token multipliers $M$ , which give the ratio of training tokens to model parameters. Larger $M$ corresponds to more over-training. For a power law giving reducible loss as a function of compute: $L^{\\prime}(C)\\,{\\stackrel{-}{=}}\\,\\lambda\\cdot C^{-\\eta}$ , the exponent $\\eta$ remains relatively constant resulting in lines with approximately fixed slope (Figure 17). The scalar $\\lambda$ that determines the $y$ -intercept, however, shifts with different token multipliers. This suggests $\\lambda$ is a function of the token multiplier, while $\\eta$ is not. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "67 2.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "68 Scaling laws for loss. Typically, scaling laws predict model loss $L$ as a function of the compute   \n69 $C$ in FLOPs used for training. If one increases the number of parameters $N$ in a model or the   \n70 number of tokens $D$ that a model is trained on, compute requirements naturally increase. Hence, we   \n71 assume $C$ is a function of $N,D$ . Following Kaplan et al. [51], we use the approximation $C=6N D$ ,   \n72 which Hoffmann et al. [45] independently verify. We consider, ", "page_idx": 2}, {"type": "equation", "text": "$$\nL(C)=E+L^{\\prime}(C),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "73 where $E$ is an irreducible loss and $L^{\\prime}$ is the reducible loss. $E$ captures the Bayes error or minimum   \n74 possible loss achievable on the validation domain. The $L^{\\prime}(C)$ term captures what can possibly be   \n75 learned about the validation domain by training on a source domain. $\\bar{L}^{\\prime}(C)$ should approach zero   \n76 with increased training data and model capacity. $L^{\\prime}(C)$ is often assumed to follow a power law:   \n77 $L^{\\prime}(C)=\\lambda\\cdot C^{-\\eta}$ (i.a., Hestness et al. [43], OpenAI [76]). It is also often helpful to consider a power   \n78 law in a log-log plot, where it appears as a line with slope $-\\eta$ and $y$ -intercept $\\log\\left(\\lambda\\right)$ .   \n79 Token multipliers. We define a token multiplier $M=D/N$ as the ratio of training tokens to model   \n80 parameters for notational convenience. $M$ allows us to consider fixed relationships between $D$ and   \n81 $N$ even as a model gets bigger (i.e., as $N$ becomes larger).   \n82 Compute-optimal training. Hoffmann et al. [45] establish compute-optimal training, where, for   \n83 any compute budget $H$ , the allocation of parameters and tokens is given by, ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{N,D}L(N,D){\\mathrm{~s.t.~}}C(N,D)=H.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "84 To solve for the optimal $N^{*},D^{*}$ , one can sweep $N,D$ for each compute budget, retaining the   \n85 best configurations. Hoffmann et al. [45] find that as the compute budget increases, $N^{*}$ and $D^{*}$   \n86 scale roughly evenly. Assuming equal scaling, there is a fixed compute-optimal token multiplier   \n87 $M^{*}=D^{*}/\\dot{N}^{*}$ per training distribution.   \n88 Over-training. We define over-training as the practice of allocating compute sub-optimally, so   \n89 smaller models train on a disproportionately large number of tokens (i.e., $M>M^{*}$ ). While loss   \n90 should be higher than in the compute-optimal allocation for a given training budget, the resulting   \n91 models have fewer parameters and thus incur less inference cost. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "92 2.2 Scaling laws for over-training ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "93 To propose a scaling law for over-trained models, we first turn to empirical observation. We train four   \n94 model configurations with parameter counts between 0.011B and 0.411B for token multipliers $M$   \n95 between 20 and 640, where $M=20$ points lie roughly on the compute-optimal frontier, and larger   \n96 $M$ corresponds to more over-training. We defer experimental details to Section 3 to focus on our   \n97 observations first. In Figure 2, we show loss against compute in a log-log plot for the models trained   \n98 on three datasets and evaluated on the C4 eval set. We notice parallel lines when ftiting power laws to   \n99 the reducible loss, which suggests a near-constant scaling exponent even with increased over-training.   \n100 This indicates that scaling behavior should be describable in the amount of over-training.   \n101 In search of an analytic expression for the observations in Figure 2, we consider existing scaling   \n102 literature. A common functional form for the risk of a model, as proposed in prior work [93, 45] is, ", "page_idx": 2}, {"type": "image", "img_path": "5tOVh81aze/tmp/c60d71d66bc66e1624a2d743a1162883dc37e3e842d42c51d072fd4f423c0e4b.jpg", "img_caption": ["Figure 3: Average top-1 error scales as a function of loss. We plot models trained on three datasets and notice an exponential decay of average top-1 error as C4 eval loss, on the $\\mathbf{X}$ -axis, decreases. We consider on the y-axes average error on 17 evaluations where performance is at least 10 points above random chance for at least one 0.154B scale model. These observations suggest that average top-1 error should be predictable with reliable loss estimates. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nL(N,D)=E+A N^{-\\alpha}+B D^{-\\beta}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "103 Recall from Section 2.1, $N$ is the number of parameters and $D$ the number of training tokens. The   \n104 constants $E,A,\\alpha,B,\\beta$ are fit from data. By fitting this parametric form, Hoffmann et al. [45]   \n105 find that scaling exponents $\\alpha$ and $\\beta$ are roughly equal, suggesting that one should scale $N$ and $D$   \n106 equally as compute increases. Hence, we assume $\\alpha=\\beta$ . With this assumption, we reparameterize   \n107 Equation (3) in terms of compute $C=6N D$ and a token multiplier $M=\\bar{D}/N$ . We get, ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(C,M)=E+\\left(a M^{\\eta}+b M^{-\\eta}\\right)C^{-\\eta},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "108 where $\\eta=\\alpha/2$ , $a=A(1/6)^{-\\eta}$ , $b=B(1/6)^{-\\eta}$ gives the relation to Equation (3). For a complete   \n109 derivation, see Appendix A.   \n110 Equation (4) has the following interpretation: (i) The scaling exponent $\\eta$ is not dependent on $M$ .   \n111 Thus, we always expect lines with the same slope in the log-log plot\u2014as in Figure 2. (ii) The term   \n112 $a M^{\\eta}+b M^{-\\eta}$ determines the offsets between curves with different token multipliers. Hence, we   \n113 expect non-overlapping, parallel lines in the log-log plot for the range of $M$ we consider\u2014also   \n114 consistent with Figure 2.   \n115 Recall that we make the assumption $\\alpha=\\beta$ , which implies equal scaling of parameters and tokens   \n116 as more compute is available. However, as explained in Appendix A, even if $\\alpha\\neq\\beta$ , we get a   \n117 parameterization that implies the power-law exponent remains constant with over-training. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "118 2.3 Scaling laws for downstream error ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "119 Scaling is typically studied in the context of loss [51, 45, 72], which Schaeffer et al. [100] note   \n120 is smoother than metrics like accuracy. However, practitioners often use downstream benchmark   \n121 accuracy as a proxy for model quality and not loss on perplexity evaluation sets. To better connect   \n122 scaling laws and over-training to task prediction, we revisit the suite of models plotted in Figure 2. In   \n123 Figure 3, we plot average downstream top-1 errors over evaluations sourced from LLM-Foundry [69]   \n124 against the C4 eval loss. We defer details of the setup to Section 3 to focus here on a key observation:   \n125 average error appears to follow exponential decay as loss decreases.   \n126 Based on the exponential decay we observe in Figure 3, we propose the following relationship   \n127 between downstream average top-1 error Err and loss $L$ , ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{E r r}(L)=\\epsilon-k\\cdot\\exp{(-\\gamma L)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "5tOVh81aze/tmp/b95987b9e7978019f8fdf7aeae2c84bfe67364aeea0076e4a271d064bec4b12e.jpg", "img_caption": ["Figure 4: Search, fliter, fit: A recipe for selecting configurations for scaling. (left) To generate the final configurations presented in Table 3, we run a 435 model grid search over model width, hidden dimension, number of attention heads, batch size, and warmup steps. All models are trained near compute-optimally. (center) We plot the efficient frontier of models, which appear to follow a trend, excluding models from $5.2\\times10^{\\bar{1}6}$ to $5.2\\times10^{17}$ , which fall below the trend. (right) We fit a power law with irreducible error to the remaining configurations, picking four configurations that closely track the full model suite (\u201cSelected models\u201d). These models extrapolate the performance of 1.4B, 6.9B target models. Shaded regions represent bootstrap $95\\%$ confidence intervals. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "128 where $\\epsilon,k,\\gamma$ are fit from data. Equation (5) also has an interpretation in terms of model perplexity   \n129 $\\mathsf{P P}(L)=\\exp\\left(L\\right)$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{E r r}(\\mathsf{P P})=\\epsilon-k\\cdot\\mathsf{P P}^{-\\gamma}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "130 Namely, Err follows a power law in PP that is bounded from above by $\\epsilon$ signifying arbitrarily high   \n131 error and from below by $\\epsilon-k\\cdot\\exp(-\\gamma E)$ , where $E$ is the Bayes error from Equation (4).   \n132 Equation (5) in conjunction with Equation (4) suggests a three-step method to predict Err as a function   \n133 of compute and the amount of over-training. For choices of training and validation distributions, (i)   \n134 fti a scaling law to Equation (4) using triplets of compute $C$ , token multiplier $M$ , and measured loss   \n135 $L$ on a validation set to yield $(C,M)\\mapsto L$ . (ii) Fit a scaling law to Equation (5) using pairs of loss $L$   \n136 and downstream error Err for models to get $L\\mapsto\\mathsf{E r r}$ . (iii) Chain predictions to get $(C,M)\\mapsto\\mathsf E\\mathsf{r r}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "137 3 Constructing a scaling testbed ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "138 In this section, we discuss our experimental setup to test the predictions suggested by Equations (4)   \n139 and (5). We first present our general language modeling setup (Section 3.1). Next, we discuss our   \n140 strategy for determining model configurations for our scaling investigation (Section 3.2) and fitting   \n141 scaling laws (Section 3.3). We then present metrics to validate how well scaling laws predict loss and   \n142 downstream performance (Section 3.4). ", "page_idx": 4}, {"type": "text", "text": "143 3.1 Training setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "144 We train transformers [116] for next token prediction, based on architectures like GPT-2 [85] and   \n145 LLaMA [113]. We employ GPT-NeoX [15] as a standardized tokenizer for all data. See Appendix B   \n146 for architecture, optimization, and hyperparameter details. ", "page_idx": 4}, {"type": "text", "text": "147 3.2 Model configurations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "148 To get final configurations for the 0.011B to 0.411B parameter models plotted in Figures 2 and 3, we   \n149 first conduct a wide grid search over a total of 435 models, trained from scratch, from 0.01B to 0.5B   \n150 parameters (Figure 4 (left)). We train on the original OpenLM data mix [39], which largely consists   \n151 of RedPajama [112] and The Pile [31]. While we eventually plan to over-train models, at this step   \n152 we search for base configurations near compute-optimality. We train on 20 tokens per parameter   \n153 $M=20$ ), which, in early experiments, gives models near the compute-optimal frontier. This is   \n154 similar to findings in Hoffmann et al. [45]\u2019s Table 3, which suggests that $M=20$ is near-optimal for   \n155 the Chinchilla experimental setup.   \n156 To find maximally performant small-scale models on validation data, we tune model width, number   \n157 of layers, number of attention heads, warmup steps, and batch size. Our validation set, OpenLM   \n158 eval, contains tokens from recent arXiv papers, the OpenLM codebase itself, and news articles. We   \n159 find in early experiments that qk-LayerNorm makes models less sensitive to learning rate, which   \n160 is a phenomenon Wortsman et al. [123] report in their Figure 1. Hence, we fix the learning rate   \n161 $(3e{-3})$ for our sweeps. We also perform smaller grid searches over 1.4B and 6.9B parameter model   \n162 configurations at $M=20$ , retaining the best configurations.   \n163 At this point, we have many models, several of which give poor performance; following prior   \n164 work [51, 45], we want to keep only models that give best performance. Hence, in Figure 4 (center),   \n165 we filter out models that do not lie on the Pareto frontier. While there appears to be a general trend,   \n166 configurations between $5.2\\times10^{16}$ and $5.2\\times10^{17}$ FLOPs lie below the frontier established by other   \n167 models. We hypothesize these models over-perform as they are trained for more optimization steps   \n168 than their neighbors based on our power-of-two batch sizes. We provide support for this hypothesis   \n169 in Appendix E, but opt to remove these models from our investigation.   \n170 To ensure tractable compute requirements for our scaling experiments, we require a subset of models   \n171 that follows the trend of the entire Pareto frontier. In Figure 4 (right), we fit trends to the Pareto   \n172 models and to a subset of four models. We notice that the trends closely predict both the performance   \n173 of the 1.4B and 6.9B models, suggesting that our small-scale configurations reliably extrapolate in   \n174 the compute-optimal setting.   \n175 Moving forward, we do not tune hyperparameters for other token multipliers (i.e., $M\\ne20]$ ), on   \n176 other training or evaluation distributions, or on validation sets for downstream tasks. For more details   \n177 including specific hyperparameters, see Appendix C.   \n178 To create our scaling testbed, we start with the four small-scale, base configurations from our   \n179 grid search: $N\\in\\{0.0\\mathrm{i}11\\mathbf{B},0.079\\mathbf{B},0.154\\mathbf{B},0.411\\mathbf{B}\\}$ . To ensure our conclusions are not particular   \n180 to a single training distribution, we train models on each of C4 [88, 27], RedPajama [112], and   \n181 RefinedWeb [82], which have 138B, 1.15T, and 600B tokens, respectively, for different token   \n182 multipliers $M\\in\\{5,10,20,40,80,160,320,640\\}.$ . We omit runs that require more tokens than are   \n183 present in a dataset (i.e., $N=0.411{\\bf B}$ , $M=640$ for $\\mathrm{C}4$ ). We additionally train $N=1.4\\mathrm{B}$ models at   \n184 $M=20$ and at the largest token multiplier possible without repeating tokens (i.e., 80 for C4, 640 for   \n185 RedPajama, and 320 for RefinedWeb). We train $N=6.9\\mathrm{B}$ , $M=20$ models on each dataset given   \n186 the relevance of 7B parameter models [113, 49]. In total this results in a testbed of 104 models. ", "page_idx": 4}, {"type": "table", "img_path": "5tOVh81aze/tmp/04d7fc399514b8719eb38ed9f7eea21e3cc4d1275fb6767f2a28d7227302b133.jpg", "table_caption": ["Table 1: Default number of parameters $N$ and token multiplier $M$ to fit our scaling laws. We invest $\\mathord{\\sim}100$ A100 hours to fit Equation (4) and $\\mathord{\\sim}1\\mathrm{,}000$ A100 hours to fit Equation (5). "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "187 3.3 Fitting scaling laws ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "188 We fit Equation (4) to approximate $E,a,b,\\eta$ using curve-fitting in SciPy [117] (i.e., Levenberg  \n189 Marquardt to minimize non-linear least squares). We repeat this process to fit Equation (5) to   \n190 approximate $\\epsilon,k,\\gamma$ . We invest $\\mathord{\\sim}100$ A100 hours to train the models required to fti a scaling law for   \n191 loss and $\\mathord{\\sim}1\\mathrm{,}000$ A100 hours for a corresponding law for downstream error. Unless otherwise specified,   \n192 we fit to the $N,M$ pairs in Table 1, which are a subset of our full testbed. Our configurations allow   \n193 us to test for extrapolation to the $N=1.4\\mathrm{B}$ , $M=640$ (900B token) and the $N=6.9\\mathrm{{B}}$ , $M=20$   \n194 (138B token) regimes. ", "page_idx": 5}, {"type": "image", "img_path": "5tOVh81aze/tmp/171c6209c6c009a9908bf85b9dc573e0fb2fafd3d67f1267ec216924f755c2d1.jpg", "img_caption": ["Figure 5: Relative error on C4 eval for different training distributions. Boxes highlighted in yellow correspond to pairs\u2014number of parameters $N$ , token multiplier $M\\cdot$ \u2014used to fti Equation (4). Larger values of $M$ correspond to more over-training. The prediction error is low in both interpolation and extrapolation ranges. Below $N=1.4\\mathrm{B}$ , empty squares correspond to runs that were not possible due to the limited dataset size for single epoch training. At $N=1.4\\mathrm{B}$ we run at $M=20$ and at the largest possible multiplier. At $N=6.9\\mathbf{B}$ , we run at $M=20$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "195 3.4 Evaluation setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "196 Evaluation datasets. Unless otherwise stated, our default validation loss dataset is C4 eval. For   \n197 downstream tasks, we adopt a subset from 46 tasks from LLM-foundry [69], which includes standard   \n198 tasks with both zero-shot and few-shot evaluations. Specifically, we consider a 17-task subset where,   \n199 for each evaluation, at least one 0.154B scale model\u2014trained with as many as 99B tokens\u2014gets   \n200 10 percentage points above chance accuracy: ARC-Easy [23], BIG-bench: CS algorithms [11],   \n201 BIG-bench: Dyck languages [11], BIG-bench: Novel Concepts [11], BIG-bench: Operators [11],   \n202 BIG-bench: QA WikiData [11], BoolQ [21], Commonsense QA [107], COPA [92], CoQA [91],   \n203 HellaSwag (zero-shot) [126], HellaSwag (10-shot) [126], LAMBADA [77], PIQA [14], PubMed   \n204 QA Labeled [50], SQuAD [90], and WinoGrand [55]. For more details on evaluation datasets   \n205 see Appendix D. We focus on this subset to ensure we are measuring signal, not noise. Including   \n206 downstream tasks like MMLU [40], where performance is close to random chance, however, does   \n207 not invalidate our results as we show in our evaluation set ablations (Appendix E).   \n208 Metrics. We consider three main metrics: Validation loss, which is the cross entropy between a   \n209 model\u2019s output and the one-hot ground truth token, averaged over all tokens in a sequence and over   \n210 all sequences in a dataset. Average top- $^{\\,l}$ error, which is a uniform average over the 17 downstream   \n211 evaluations, as mentioned in the above paragraph. To measure how good a prediction $\\zeta(C,M)$ is,   \n212 we measure Relative prediction error: $\\lvert\\zeta(C,M)-\\zeta_{G T}\\rvert/\\zeta_{G T}$ , where $\\zeta$ is the predicted loss $L$ or the   \n213 average top-1 error Err. $\\zeta_{G T}$ is the ground truth measurement to predict. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "214 4 Results: Reliable extrapolation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "215 In this Section, we quantify the extent to which the scaling laws developed in Section 2 extrapolate   \n216 larger model performance using the scaling testbed from Section 3. By default, we fit Equations (4)   \n217 and (5) to the configurations in Table 1, use C4 eval for loss, and the 17-task split from Section 3.4   \n218 for average top-1 error.   \n219 Over-trained performance is predictable. We highlight our main over-training results in   \n220 Figure 1 (left). Namely, we are able to extrapolate both in the number of parameters $N$ and the   \n221 token multiplier $M$ to closely predict the C4 eval performance of a 1.4B parameter model trained on   \n222 900B RedPajama tokens $(N=1.4{\\bf B},M=640)$ . Our prediction, which takes $300\\times$ less compute   \n223 to construct than the final 1.4B run, is accurate to within $0.7\\%$ relative error. Additionally, for the   \n224 $N=6.9\\mathrm{B}$ , $M=20$ run, near compute-optimal, the relative error is also $0.7\\%$ .   \n225 These results support several key takeaways. (i) Scaling can be predictable even when one increases   \n226 both the model size and the amount of over-training compared to the training runs used to fti a scaling   \n227 law. (ii) The form presented in Equation (4) is useful in practice for predicting over-trained scaling   \n228 behavior. (iii) Fitting to Equation (4) gives good prediction accuracy near compute-optimal. More   \n229 specifically, predictions are accurate both for the 1.4B over-trained model and the 6.7B compute  \n230 optimal model using a single scaling fit.   \n231 While Figure 1 explores a specific case of making predictions in the over-trained regime, we aim to   \n232 understand the error proflie of our predictions across training datasets, token multipliers, and number   \n233 of parameters. Hence, Figure 5 shows the relative error between ground truth loss and predicted   \n234 loss on C4 eval for models in our testbed. We notice uniformly low prediction error suggesting that   \n235 predictions are accurate in many settings.   \n236 Average top-1 error is predictable. Figure 1 (right) presents our main result in estimating scaling   \n237 laws for downstream error. Concretely, we use the models indicated in Table 1 to fit Equations (4)   \n238 and (5), chaining the scaling fits to predict the average top-1 error as a function of training compute   \n239 $C$ and the token multiplier $M$ . Our ftis allow us to predict, using $20\\times$ less compute, the downstream   \n240 performance of a 6.9B model trained on 138B RedPajama tokens to within $0.05\\%$ relative error and a   \n241 1.4B model trained on RedPajama 900B tokens to within $3.6\\%$ relative error.   \n242 Table 2 additionally shows the relative error of our downstream performance predictions for models   \n243 trained on C4, RedPajama, and RefinedWeb, indicating that our scaling law functional forms are   \n244 applicable on many training datasets. We note that while average accuracy is predictable, individual   \n245 downstream task predictions are significantly more noisy. We report relative error for more model   \n246 predictions in Figures 11 and 12. We also find that if we remove the 1.4B model for the Equation (5)   \n247 fit, relative error jumps, for instance, from $0.05\\%$ to $10.64\\%$ on the 17-task split for the $6.9\\mathrm{B}$ ,   \n248 138B token RedPajama prediction. This highlights the importance of investing more compute when   \n249 constructing scaling laws for downstream task prediction compared to loss prediction.   \n50 Under-training, out-of-distribution scaling, and compute-reliability trade-offs. In addition to   \n51 our main results presented above, we include additional results in Appendix E, which we summarize   \n52 here. First, we notice that when token multipliers become too small (i.e., $M=5$ ) scaling becomes   \n53 unreliable and lies off the trend. Additionally, multipliers other than 20, such as 10, 40, and 80, garner   \n54 points that are roughly on the compute optimal frontier (Figure 9). This observation suggests that the   \n55 compute-optimal multiplier may lie in a range rather than take a single value. To probe the limits   \n56 of reliable scaling, we attempt to break our scaling laws in out-of-distribution settings. We find that   \n57 models trained on C4\u2014English filtered\u2014and evaluated on next token prediction on code domains   \n58 have a high relative error in many cases. Perhaps surprisingly, evaluating the same models on German   \n59 next token prediction gives reliable loss scaling (Figure 10). We additionally examine the compute   \n60 necessary to create accurate scaling laws, finding that scaling laws can be constructed more cheaply   \n61 for loss prediction than for downstream error prediction (Figures 15 and 16). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "5tOVh81aze/tmp/ebd0aa639587b2e8ff1d34e5ec3ea43541516d0ec6c02e6e0229d1f64349bd12.jpg", "table_caption": ["Table 2: Downstream relative prediction error at 6.9B parameters and 138B tokens. While predicting accuracy on individual zero-shot downstream evaluations can be challenging (\u201cIndividual\u201d), predicting averages across downstream datasets is accurate (\u201cAvg.\u201d). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "262 5 Related work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "263 We review the most closely related work in this section. For additional related work, see Appendix F. ", "page_idx": 7}, {"type": "text", "text": "264 Scaling laws. Early works on scaling artificial neural networks observe predictable power-law   \n265 scaling in the training set size and number of model parameters [43, 44, 93]. Alabdulmohsin et al.   \n266 [2] stress the importance of looking at the extrapolation regime of a scaling law. Yang et al. [124]   \n267 prescribe architectural and hyperparameter changes when scaling model width to realize performant   \n268 models; Yang et al. [125] make analogous recommendations when scaling model depth. Bi et al.   \n269 [13] propose hyperparameter aware scaling laws. Unlike the aforementioned work, our investigation   \n270 focuses on over-training and predicting downstream accuracy.   \n271 Hoffmann et al. [45] investigate how the number of model parameters $N$ and training tokens $D$   \n272 should be chosen to minimize loss $L$ given a compute budget $C$ . Hoffmann et al. [45] find that when   \n273 scaling up $C$ , both $N$ and $D$ should be scaled equally up to a multiplicative constant (i.e., $N\\propto C^{\\sim0.5}$   \n274 and $D\\propto C^{\\sim0.5}$ ) to realize compute-optimality. Appendix C of the Chinchilla paper additionally   \n275 suggests that these findings hold across three datasets. However, Hoffmann et al. [45] do not verify   \n276 their scaling laws for training beyond compute-optimality, or for downstream error prediction\u2014both   \n277 of which are central to our work.   \n278 Sardana & Frankle [98] propose modifications to the Chinchilla formulation to incorporate inference   \n279 costs into the definition of compute-optimality and solve for various fixed inference budgets. Their   \n280 key finding, which is critical for our work, is that when taking into account a large enough inference   \n281 budget, it is optimal to train smaller models for longer than the original Chinchilla recommendations.   \n282 Our work presupposes that over-training can be beneficial. Instead of solving for inference  \n283 optimal schemes, we support empirically a predictive theory of scaling in the over-trained regime.   \n284 Additionally, we provide experiments across many validation and training sets.   \n285 For predicting downstream scaling beyond loss, Isik et al. [47] relate the number of pre-training tokens   \n286 to downstream cross-entropy and machine translation BLEU score [78] after fine-tuning. In contrast,   \n287 we take a holistic approach to evaluation by looking at top-1 error over many natural language tasks.   \n288 Schaeffer et al. [100] argue that emergent abilities [120] are a product of non-linear metrics and   \n289 propose smoother alternatives. As a warmup for why non-linear metrics may be hard to predict,   \n290 Schaeffer et al. [100] consider predicting an $\\ell$ length sequence exactly: ${\\mathsf{E r r}}(N,^{\\cdot}\\ell)\\approx1-{\\mathsf{P}}{\\mathsf{P}}(N)^{-\\ell}$ ,   \n291 where $N$ is the number of parameters in a model and PP is its perplexity. This is a special case of   \n292 our Equations (5) and (6), where the number of training tokens does not appear, $\\epsilon=1,k=1$ , and   \n293 $\\gamma=\\ell$ . In contrast, we treat $\\epsilon,k,\\gamma$ as free parameters for a scaling law fit, finding that average error   \n294 over downstream tasks can make for a predictable metric.   \n295 Over-training in popular models. There has been a rise in over-trained models [113, 114] and   \n296 accompanying massive datasets [112, 82, 104, 3]. For example, Chinchilla 70B [45] is trained with a   \n297 token multiplier of 20, while LLaMA-2 7B [114] uses a token multiplier of 290. In our investigation,   \n298 we look at token multipliers from 5 to 640 to ensure coverage of popular models and relevance for   \n299 future models that may be trained on even more tokens. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "300 6 Limitations, future work, and conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "301 Limitations and future work. We identify limitations, which provide motivation for future work. ", "page_idx": 8}, {"type": "text", "text": "302 \u2022 Hyperparameters. While our configurations are surprisingly amenable to reliable scaling across   \n303 many training and testing distributions without further tuning, there is a need to develop scaling   \n304 laws that do not require extensive hyperparameter sweeps.   \n305 \u2022 Scaling up. Validating the trends in this paper for even larger runs is a valuable direction.   \n306 Additionally, repeating our setup for models that achieve non-trivial performance on harder   \n307 evaluations like MMLU is left to future work.   \n308 \u2022 Scaling down. Actualizing predictable scaling with even cheaper runs is important to make this   \n309 area of research more accessible, especially for downstream error prediction.   \n310 \u2022 Failure cases. While we present a preliminary analysis of when scaling is unreliable, future work   \n311 should investigate conditions under which scaling breaks down.   \n312 \u2022 Post-training. It is common to employ fine-tuning interventions after pre-training, which we do   \n313 not consider. Quantifying to what degree over-training the base model provides benefits after   \n314 post-training is an open area of research.   \n315 \u2022 Individual downstream task prediction. While we find that averaging over many task error   \n316 metrics can make for a predictable metric, per-task predictions are left to future work.   \n317 \u2022 In-the-wild performance. Downstream task performance is a proxy for the in-the-wild user   \n318 experience. Analyzing scaling trends in the context of this experience is timely.   \n319 \u2022 Dataset curation. Our work only deals with existing training datasets. Exploring dataset curation   \n320 for improved model scaling is another promising direction.   \n321 Conclusion. We show that the loss of over-trained models, trained past compute-optimality, is   \n322 predictable. Furthermore, we propose and validate a scaling law relating loss to average downstream   \n323 task performance. We hope our work will inspire others to further examine the relationship between   \n324 model training and downstream generalization. Our testbed will be made publicly available, and we   \n325 hope it will make scaling research more accessible to researchers and practitioners alike. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "326 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "327 [1] Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits   \n328 of large scale pre-training. In International Conference on Learning Representations (ICLR),   \n329 2022. https://arxiv.org/abs/2110.02095.   \n330 [2] Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling   \n331 laws in language and vision. In Advances in Neural Information Processing Systems (NeuIPS),   \n332 2022. https://arxiv.org/abs/2209.06640.   \n333 [3] Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi   \n334 Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on   \n335 data selection for language models. arXiv preprint, 2024. https://arxiv.org/abs/2402.   \n336 16827.   \n337 [4] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki,   \n338 Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al.   \n339 Santacoder: don\u2019t reach for the stars! arXiv preprint, 2023. https://arxiv.org/abs/   \n340 2301.03988.   \n341 [5] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and   \n342 Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with   \n343 operation-based formalisms. In Conference of the North American Chapter of the Association   \n344 for Computational Linguistics (NACCL), 2019. https://aclanthology.org/N19-1245.   \n345 [6] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael   \n346 Voznesensky, Bin Bao, David Berard, Geeta Chauhan, Anjali Chourdia, Will Constable,   \n347 Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind,   \n348 Brian Hirsh, Sherlock Huang, Laurent Kirsch, Michael Lazos, Yanbo Liang, Jason Liang,   \n349 Yinghai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark   \n350 Saroufim, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William   \n351 Wen, Shunting Zhang, Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan,   \n352 Peng Wu, and Soumith Chintala. Pytorch 2: Faster machine learning through dynamic   \n353 python bytecode transformation and graph compilation. In International Conference on   \n354 Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2024.   \n355 https://pytorch.org/blog/pytorch-2-paper-tutorial.   \n356 [7] Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer,   \n357 Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giridharan Anantharaman,   \n358 Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh   \n359 Koura, Brian O\u2019Horo, Jeffrey Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and   \n360 Veselin Stoyanov. Efficient large scale language modeling with mixtures of experts. In   \n361 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022. https:   \n362 //aclanthology.org/2022.emnlp-main.804.   \n363 [8] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint,   \n364 2016. https://arxiv.org/abs/1607.06450.   \n365 [9] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining   \n366 neural scaling laws. arXiv preprint, 2021. https://arxiv.org/abs/2102.06701.   \n367 [10] Yamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao Zhang, Maxim Krikun, Colin Cherry,   \n368 Behnam Neyshabur, and Orhan Firat. Data scaling laws in nmt: The effect of noise and   \n369 architecture. In International Conference on Machine Learning (ICML), 2022. https:   \n370 //proceedings.mlr.press/v162/bansal22b.html.   \n371 [11] BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities   \n372 of language models. In Transactions on Machine Learning Research (TMLR), 2023. https:   \n373 //openreview.net/forum?id=uyTL5Bvosj.   \n374 [12] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On   \n375 the dangers of stochastic parrots: Can language models be too big? In Proceedings ACM   \n376 conference on fairness, accountability, and transparency (FAccT), 2021. https://dl.acm.   \n377 org/doi/10.1145/3442188.3445922.   \n378 [13] DeepSeek-AI Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi   \n379 Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao,   \n380 Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He,   \n381 Wen-Hui Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng   \n382 Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu   \n383 Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui   \n384 Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Jun-Mei Song,   \n385 Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Min Tang, Bing-Li Wang, Peiyi Wang, Shiyu   \n386 Wang, Yaohui Wang, Yongji Wang, Tong Wu, Yu Wu, Xin Xie, Zhenda Xie, Ziwei Xie,   \n387 Yi Xiong, Hanwei Xu, Ronald X Xu, Yanhong Xu, Dejian Yang, Yu mei You, Shuiping Yu,   \n388 Xin yuan Yu, Bo Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang,   \n389 Minghu Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou,   \n390 Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language   \n391 models with longtermism. arXiv preprint, 2024. https://arxiv.org/abs/2401.02954.   \n392 [14] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning   \n393 about physical commonsense in natural language. In Association for the Advancement of   \n394 Artificial Intelligence (AAAI), 2020. https://arxiv.org/abs/1911.11641.   \n395 [15] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,   \n396 Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai   \n397 Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel   \n398 Weinbach. Gpt-neox-20b: An open-source autoregressive language model. BigScience   \n399 Episode #5 \u2013 Workshop on Challenges & Perspectives in Creating Large Language Models,   \n400 2022. https://aclanthology.org/2022.bigscience-1.9.   \n401 [16] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla   \n402 Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini   \n403 Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya   \n404 Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric   \n405 Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam   \n406 McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few  \n407 shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n408 https://arxiv.org/abs/2005.14165.   \n409 [17] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. In   \n410 International Conference on Learning Representations (ICLR), 2023. https://openreview.   \n411 net/forum?id=sckjveqlCZ.   \n412 [18] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade   \n413 Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling   \n414 laws for contrastive language-image learning. In Conference on Computer Vision and Pattern   \n415 Recognition (CVPR), 2023. https://arxiv.org/abs/2212.07143.   \n416 [19] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam   \n417 Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,   \n418 Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,   \n419 Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson,   \n420 Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju   \n421 Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda,   \n422 Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan,   \n423 Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani   \n424 Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie   \n425 Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,   \n426 Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason   \n427 Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.   \n428 Palm: Scaling language modeling with pathways. In Journal of Machine Learning Research   \n429 (JMLR), 2022. https://arxiv.org/abs/2204.02311.   \n430 [20] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan   \n431 Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned   \n432 language models. arXiv preprint, 2022. https://arxiv.org/abs/2210.11416.   \n433 [21] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and   \n434 Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In   \n435 Conference of the North American Chapter of the Association for Computational Linguistics   \n436 (NAACL), 2019. https://aclanthology.org/N19-1300.   \n437 [22] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA:   \n438 Pre-training text encoders as discriminators rather than generators. In International   \n439 Conference on Learning Representations (ICLR), 2020. https://openreview.net/pdf?   \n440 id=r1xMH1BtvB.   \n441 [23] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,   \n442 and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning   \n443 challenge. arXiv preprint, 2018. https://arxiv.org/abs/1803.05457.   \n444 [24] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast   \n445 and memory-efficient exact attention with IO-awareness. In Advances in Neural Information   \n446 Processing Systems (NeurIPS), 2022. https://arxiv.org/abs/2205.14135.   \n447 [25] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin   \n448 Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.   \n449 Scaling vision transformers to 22 billion parameters. In International Conference on Machine   \n450 Learning (ICML), 2023. https://proceedings.mlr.press/v202/dehghani23a.html.   \n451 [26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training   \n452 of deep bidirectional transformers for language understanding. In Conference of the North   \n453 American Chapter of the Association for Computational Linguistics (NAACL), 2019. https:   \n454 //aclanthology.org/N19-1423.   \n455 [27] Jesse Dodge, Maarten Sap, Ana Marasovic\u00b4, William Agnew, Gabriel Ilharco, Dirk Groeneveld,   \n456 Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on   \n457 the colossal clean crawled corpus. In Conference on Empirical Methods in Natural Language   \n458 Processing (EMNLP), 2021. https://aclanthology.org/2021.emnlp-main.98.   \n459 [28] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,   \n460 Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten   \n461 Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin   \n462 Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le,   \n463 Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient scaling of language models   \n464 with mixture-of-experts. In International Conference on Machine Learning (ICML), 2022.   \n465 https://arxiv.org/abs/2112.06905.   \n466 [29] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto:   \n467 Model alignment as prospect theoretic optimization. arXiv preprint, 2024. https://arxiv.   \n468 org/abs/2402.01306.   \n469 [30] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao   \n470 Nguyen, Mitchell Wortsman Ryan Marten, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim   \n471 Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe,   \n472 Stephen Mussmann, Mehdi Cherti Richard Vencu, Ranjay Krishna, Pang Wei Koh, Olga   \n473 Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,   \n474 Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt.   \n475 Datacomp: In search of the next generation of multimodal datasets. In Advances in Neural   \n476 Information Processing Systems (NeurIPS), 2023. https://arxiv.org/abs/2304.14108.   \n477 [31] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,   \n478 Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.   \n479 The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint, 2020.   \n480 https://arxiv.org/abs/2101.00027.   \n481 [32] Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia,   \n482 Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. arXiv preprint,   \n483 2021. https://arxiv.org/abs/2109.07740.   \n484 [33] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural   \n485 machine translation. In Conference on Empirical Methods in Natural Language Processing   \n486 (EMNLP), 2021. https://aclanthology.org/2021.emnlp-main.478.   \n487 [34] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,   \n488 Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating   \n489 the science of language models. arXiv preprint, 2024. https://arxiv.org/abs/2402.   \n490 00838.   \n491 [35] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.   \n492 arXiv preprint, 2023. https://arxiv.org/abs/2312.00752.   \n493 [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher   \n494 R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space   \n495 layers. In Advances in Neural Information Processing Systems (NeurIPS), 2021. https:   \n496 //openreview.net/forum?id=yWd42CWN3c.   \n497 [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with   \n498 structured state spaces. In International Conference on Learning Representations (ICLR),   \n499 2022. https://arxiv.org/abs/2111.00396.   \n500 [38] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar, Teodoro Mendes, Allie Del Giorno,   \n501 Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi,   \n502 Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen   \n503 Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you   \n504 need. Preprint, 2023. https://www.microsoft.com/en-us/research/publication/   \n505 textbooks-are-all-you-need.   \n506 [39] Suchin Gururangan, Mitchell Wortsman, Samir Yitzhak Gadre, Achal Dave, Maciej Kilian,   \n507 Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard   \n508 Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, and Ludwig Schmidt. OpenLM:   \n509 a minimal but performative language modeling (lm) repository, 2023. https://github.   \n510 com/mlfoundations/open_lm.   \n511 [40] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and   \n512 Jacob Steinhardt. Measuring massive multitask language understanding. In International   \n513 Conference on Learning Representations (ICLR), 2021. https://arxiv.org/abs/2009.   \n514 03300.   \n515 [41] T. J. Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,   \n516 Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann,   \n517 Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei,   \n518 and Sam McCandlish. Scaling laws for autoregressive generative modeling. arXiv preprint,   \n519 2020. https://arxiv.org/abs/2010.14701.   \n520 [42] Danny Hernandez, Jared Kaplan, T. J. Henighan, and Sam McCandlish. Scaling laws for   \n521 transfer. arXiv preprint, 2021. https://arxiv.org/abs/2102.01293.   \n522 [43] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Frederick Diamos, Heewoo Jun,   \n523 Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning   \n524 scaling is predictable, empirically. arXiv preprint, 2017. https://arxiv.org/abs/1712.   \n525 00409.   \n526 [44] Joel Hestness, Newsha Ardalani, and Gregory Diamos. Beyond human-level accuracy:   \n527 Computational challenges in deep learning. In Principles and Practice of Parallel   \n528 Programming (PPoPP), 2019. https://arxiv.org/abs/1909.01736.   \n529 [45] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,   \n530 Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark,   \n531 et al. Training compute-optimal large language models. In Advances in Neural Information   \n532 Processing Systems (NeurIPS), 2022. https://arxiv.org/abs/2203.15556.   \n533 [46] Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word   \n534 classifiers: A loss framework for language modeling. In International Conference on Learning   \n535 Representations (ICLR), 2017. https://arxiv.org/abs/1611.01462.   \n536 [47] Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii,   \n537 and Sanmi Koyejo. Scaling laws for downstream task performance of large language models.   \n538 arXiv, 2024. https://arxiv.org/abs/2402.04177.   \n539 [48] Maor Ivgi, Yair Carmon, and Jonathan Berant. Scaling laws under the microscope: Predicting   \n540 transformer performance from small scale experiments. In Conference on Empirical Methods   \n541 in Natural Language Processing (EMNLP), 2022. https://aclanthology.org/2022.   \n542 findings-emnlp.544.   \n543 [49] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh   \n544 Chaplot, Florian Bressand Diego de las Casas, Gianna Lengyel, Guillaume Lample, Lucile   \n545 Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut   \n546 Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b. arXiv preprint,   \n547 2023. https://arxiv.org/abs/2310.06825.   \n548 [50] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A   \n549 dataset for biomedical research question answering. In Conference on Empirical Methods in   \n550 Natural Language Processing (EMNLP), 2019. https://aclanthology.org/D19-1259.   \n551 [51] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon   \n552 Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural   \n553 language models. arXiv preprint, 2020. https://arxiv.org/abs/2001.08361.   \n554 [52] Tobit Klug, Dogukan Atik, and Reinhard Heckel. Analyzing the sample complexity of self  \n555 supervised image reconstruction methods. arXiv preprint, 2023. https://arxiv.org/abs/   \n556 2305.19079.   \n557 [53] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu   \n558 Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv   \n559 preprint, 2019. http://arxiv.org/abs/1909.11942.   \n560 [54] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,   \n561 Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel   \n562 Haziza. xformers: A modular and hackable transformer modelling library, 2022. https:   \n563 //github.com/facebookresearch/xformers.   \n564 [55] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In   \n565 International conference on the principles of knowledge representation and reasoning, 2012.   \n566 https://aaai.org/papers/59-4492-the-winograd-schema-challenge.   \n567 [56] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,   \n568 Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to  \n569 sequence pre-training for natural language generation, translation, and comprehension. In   \n570 Annual Meeting of the Association for Computational Linguistics (ACL), 2020. https:   \n571 //aclanthology.org/2020.acl-main.703.   \n572 [57] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao   \n573 Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source   \n574 be with you! arXiv preprint, 2023. https://arxiv.org/abs/2305.06161.   \n575 [58] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A   \n576 challenge dataset for machine reading comprehension with logical reasoning. In International   \n577 Joint Conference on Artificial Intelligence, 2020. https://arxiv.org/abs/2007.08124.   \n578 [59] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,   \n579 Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT   \n580 pretraining approach. arXiv preprint, 2019. http://arxiv.org/abs/1907.11692.   \n581 [60] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining   \n582 Xie. A convnet for the 2020s. Conference on Computer Vision and Pattern Recognition   \n583 (CVPR), 2022. https://arxiv.org/abs/2201.03545.   \n584 [61] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William   \n585 Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. The   \n586 data provenance initiative: A large scale audit of dataset licensing & attribution in ai. arXiv   \n587 preprint, 2023. https://arxiv.org/abs/2310.16787.   \n588 [62] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint,   \n589 2017. https://arxiv.org/abs/1711.05101.   \n590 [63] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,   \n591 Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian,   \n592 Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov,   \n593 Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo,   \n594 Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau\u00df, Naman Jain, Yixuan   \n595 Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang,   \n596 Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra,   \n597 Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu,   \n598 Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane   \n599 Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu\u00f1oz   \n600 Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra,   \n601 and Harm de Vries. Starcoder 2 and the stack v2: The next generation. arXiv preprint, 2024.   \n602 https://arxiv.org/abs/2402.19173.   \n603 [64] Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna  \n604 Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, et al.   \n605 Fingpt: Large generative models for a small language. In Conference on Empirical Methods   \n606 in Natural Language Processing (EMNLP), 2023. https://aclanthology.org/2023.   \n607 emnlp-main.164.   \n608 [65] Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind   \n609 Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groenveld, Iz Beltagy,   \n610 Hanneneh Hajishirz, Noah A. Smith, Kyle Richardson, and Jesse Dodge. Paloma: A benchmark   \n611 for evaluating language model fit. arXiv preprint, 2023. https://paloma.allen.ai.   \n612 [66] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large   \n613 annotated corpus of English: The Penn Treebank. In Computational Linguistics, 1993.   \n614 https://aclanthology.org/J93-2004.   \n615 [67] William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah A. Smith.   \n616 Effects of parameter norm growth during transformer training: Inductive bias from gradient   \n617 descent. In Conference on Empirical Methods in Natural Language Processing (EMNLP),   \n618 2021. https://aclanthology.org/2021.emnlp-main.133.   \n619 [68] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct   \n620 electricity? a new dataset for open book question answering. In Conference on Empirical   \n621 Methods in Natural Language Processing (EMNLP), 2018. https://arxiv.org/abs/1809.   \n622 02789.   \n623 [69] MosaicML. Llm evaluation scores, 2023. https://www.mosaicml.com/llm-evaluation.   \n624 [70] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,   \n625 Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al.   \n626 Crosslingual generalization through multitask finetuning. In Annual Meeting of the   \n627 Association for Computational Linguistics (ACL), 2022. https://aclanthology.org/   \n628 2023.acl-long.891.   \n629 [71] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue   \n630 Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack:   \n631 Instruction tuning code large language models. arXiv preprint, 2023. https://arxiv.org/   \n632 abs/2308.07124.   \n633 [72] Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus,   \n634 Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained   \n635 language models. In Advances in Neural Information Processing Systems (NeuIPS), 2023.   \n636 https://arxiv.org/abs/2305.16264.   \n637 [73] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet   \n638 Singh, and Douwe Kiela. Generative representational instruction tuning. arXiv preprint, 2024.   \n639 https://arxiv.org/abs/2402.09906.   \n640 [74] Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig,   \n641 Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech   \n642 Kryscinski, Lidiya Murakhovs\u2019ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng,   \n643 Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Rayhan   \n644 Joty, and Caiming Xiong. Long sequence modeling with xgen: A 7b llm trained on 8k input   \n645 sequence length. arXiv preprint, 2023. https://arxiv.org/abs/2309.03450.   \n646 [75] OpenAI. Triton, 2021. https://github.com/openai/triton.   \n647 [76] OpenAI. Gpt-4 technical report, 2023. https://arxiv.org/abs/2303.08774.   \n648 [77] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella   \n649 Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The   \n650 LAMBADA dataset: Word prediction requiring a broad discourse context. In Annual Meeting   \n651 of the Association for Computational Linguistics (ACL), 2016. http://www.aclweb.org/   \n652 anthology/P16-1144.   \n653 [78] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic   \n654 evaluation of machine translation. In Annual Meeting of the Association for Computational   \n655 Linguistics (ACL), 2002. https://aclanthology.org/P02-1040.   \n656 [79] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana   \n657 Thompson, Phu Mon Htut, and Samuel Bowman. BBQ: A hand-built bias benchmark for   \n658 question answering. In Annual Meeting of the Association for Computational Linguistics   \n659 (ACL), 2022. https://aclanthology.org/2022.findings-acl.165.   \n660 [80] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,   \n661 Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative   \n662 style, high-performance deep learning library. In Advances in Neural Information Processing   \n663 Systems (NeurIPS), 2019. https://arxiv.org/abs/1912.01703.   \n664 [81] Patronus AI. EnterprisePII dataset, 2023. https://tinyurl.com/2r5x9bst.   \n665 [82] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro   \n666 Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The   \n667 RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web   \n668 data only. arXiv preprint, 2023. https://arxiv.org/abs/2306.01116.   \n669 [83] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman,   \n670 Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella,   \n671 Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong,   \n672 Bart\u0142omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi   \n673 Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanis\u0142aw Wo\u00b4zniak, Zhenyuan Zhang,   \n674 Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer   \n675 era. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023.   \n676 https://aclanthology.org/2023.findings-emnlp.936.   \n677 [84] Ofir Press and Lior Wolf. Using the output embedding to improve language models. In   \n678 Proceedings of the Conference of the European Chapter of the Association for Computational   \n679 Linguistics (EACL), 2017. https://aclanthology.org/E17-2025.   \n680 [85] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya   \n681 Sutskever. Language models are unsupervised multitask learners. Preprint, 2019.   \n682 https://d4mucfpksywv.cloudfront.net/better-language-models/language_   \n683 models_are_unsupervised_multitask_learners.pdf.   \n684 [86] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis   \n685 Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford,   \n686 Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche,   \n687 Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl,   \n688 Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins,   \n689 Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar,   \n690 Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini,   \n691 L. Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena   \n692 Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,   \n693 Maria Tsimpoukelli, N. K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias   \n694 Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi,   \n695 Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris   \n696 Jones, James Bradbury, Matthew G. Johnson, Blake A. Hechtman, Laura Weidinger, Iason   \n697 Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol   \n698 Vinyals, Kareem W. Ayoub, Jeff Stanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu,   \n699 and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training   \n700 gopher. arXiv preprint, 2021. https://arxiv.org/abs/2112.11446.   \n701 [87] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and   \n702 Chelsea Finn. Direct preference optimization: Your language model is secretly a reward   \n703 model. In Advances in Neural Information Processing Systems (NeurIPS), 2023. https:   \n704 //arxiv.org/abs/2305.18290.   \n705 [88] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,   \n706 Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified   \n707 text-to-text transformer. arXiv preprint, 2019. https://arxiv.org/abs/1910.10683.   \n708 [89] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,   \n709 Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified   \n710 text-to-text transformer. In The Journal of Machine Learning Research (JMLR), 2020. https:   \n711 //arxiv.org/abs/1910.10683.   \n712 [90] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: $100{,}000{+}$   \n713 questions for machine comprehension of text. In Conference on Empirical Methods in Natural   \n714 Language Processing (EMNLP), 2016. https://aclanthology.org/D16-1264.   \n715 [91] Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question   \n716 answering challenge. In Transactions of the Association for Computational Linguistics (TACL),   \n717 2019. https://aclanthology.org/Q19-1016.   \n718 [92] Melissa Roemmele, Cosmin Adrian Bejan, , and Andrew S. Gordon. Choice of plausible   \n719 alternatives: An evaluation of commonsense causal reasoning. In Association for the   \n720 Advancement of Artificial Intelligence (AAAI) Spring Symposium, 2011. https://people.   \n721 ict.usc.edu/\\~gordon/copa.html.   \n722 [93] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive   \n723 prediction of the generalization error across scales. In International Conference on Learning   \n724 Representations (ICLR), 2020. https://arxiv.org/abs/1909.12673.   \n725 [94] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias   \n726 in coreference resolution. In Conference of the North American Chapter of the Association for   \n727 Computational Linguistics (NAACL), 2018. https://aclanthology.org/N18-2002.   \n728 [95] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An   \n729 adversarial winograd schema challenge at scale. arXiv preprint, 2019. https://arxiv.org/   \n730 abs/1907.10641.   \n731 [96] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled   \n732 version of bert: smaller, faster, cheaper and lighter. arXiv preprint, 2019. http://arxiv.   \n733 org/abs/1910.01108.   \n734 [97] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa:   \n735 Commonsense reasoning about social interactions. In Empirical Methods in Natural Language   \n736 Processing (EMNLP), 2019. https://aclanthology.org/D19-1454.   \n737 [98] Nikhil Sardana and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference   \n738 in language model scaling laws. In NeurIPS Workshop on Efficient Natural Language and   \n739 Speech Processing (ENLSP), 2023. https://arxiv.org/abs/2401.00448.   \n740 [99] Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari,   \n741 Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et al. What language   \n742 model to train if you have one million gpu hours? In Conference on Empirical Methods   \n743 in Natural Language Processing (EMNLP), 2022. https://aclanthology.org/2022.   \n744 findings-emnlp.54.   \n745 [100] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language   \n746 models a mirage? In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n747 https://arxiv.org/abs/2304.15004.   \n748 [101] Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data   \n749 manifold. In Journal of Machine Learning Research (JMLR), 2022. https://arxiv.org/   \n750 abs/2004.10802.   \n751 [102] Noam Shazeer. Glu variants improve transformer. arXiv preprint, 2020. https://arxiv.   \n752 org/abs/2002.05202.   \n753 [103] Shivalika Singh, Freddie Vargus, Daniel Dsouza, B\u00f6rje F Karlsson, Abinaya Mahendiran,   \n754 Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, et al.   \n755 Aya dataset: An open-access collection for multilingual instruction tuning. arXiv preprint   \n756 arXiv:2402.06619, 2024. https://arxiv.org/abs/2402.06619.   \n757 [104] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell   \n758 Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open   \n759 corpus of three trillion tokens for language model pretraining research. arXiv preprint, 2024.   \n760 https://arxiv.org/abs/2402.00159.   \n761 [105] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond   \n762 neural scaling laws: beating power law scaling via data pruning. In Advances in Neural   \n763 Information Processing Systems (NeurIPS), 2022. https://openreview.net/forum?id $=$   \n764 UmvSlP-PyV.   \n765 [106] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:   \n766 Enhanced transformer with rotary position embedding. arXiv preprint, 2021. https://   \n767 arxiv.org/abs/2104.09864.   \n768 [107] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA:   \n769 A question answering challenge targeting commonsense knowledge. In Conference of the   \n770 North American Chapter of the Association for Computational Linguistics (NAACL), 2019.   \n771 https://aclanthology.org/N19-1421.   \n772 [108] Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won   \n773 Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale   \n774 efficiently: Insights from pre-training and fine-tuning transformers. In International   \n775 Conference on Learning Representations (ICLR), 2022. https://openreview.net/forum?   \n776 id=f2OYVDyfIB.   \n777 [109] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Chung, William Fedus, Jinfeng Rao,   \n778 Sharan Narang, Vinh Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model   \n779 architectures: How does inductive bias influence scaling? In Conference on Empirical   \n780 Methods in Natural Language Processing (EMNLP), 2023. https://aclanthology.org/   \n781 2023.findings-emnlp.825.   \n782 [110] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially   \n783 usable llms, 2023. www.mosaicml.com/blog/mpt-7b.   \n784 [111] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha,   \n785 Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee,   \n786 Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun,   \n787 Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,   \n788 Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch,   \n789 Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel   \n790 Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen,   \n791 Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina,   \n792 Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm,   \n793 Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera  \n794 Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog   \n795 applications. arXiv preprint, 2022. https://arxiv.org/abs/2201.08239.   \n796 [112] Together Computer. Redpajama: an open dataset for training large language models, 2023.   \n797 https://github.com/togethercomputer/RedPajama-Data.   \n798 [113] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,   \n799 Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien   \n800 Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and   \n801 Efficient Foundation Language Models. arXiv preprint, 2023. https://arxiv.org/abs/   \n802 2302.13971.   \n803 [114] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine   \n804 Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel,   \n805 Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude   \n806 Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman   \n807 Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor   \n808 Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne   \n809 Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier   \n810 Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,   \n811 Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael   \n812 Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,   \n813 Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie   \n814 Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas   \n815 Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint, 2023.   \n816 https://arxiv.org/abs/2307.09288.   \n817 [115] Ahmet \u00dcst\u00fcn, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D\u2019souza, Gbemileke   \n818 Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al. Aya model:   \n819 An instruction finetuned open-access multilingual language model. arXiv preprint, 2024.   \n820 https://arxiv.org/abs/2402.07827.   \n[116] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,   \n822 \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural   \n823 Information Processing Systems (NeurIPS), 2017. https://arxiv.org/abs/1706.03762.   \n824 [117] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David   \n825 Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J.   \n826 van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew   \n827 R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \u02d9Ilhan Polat, Yu Feng, Eric W.   \n828 Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A.   \n829 Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul   \n830 van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific   \n831 Computing in Python. Nature Methods, 2020. https://rdcu.be/b08Wh.   \n832 [118] Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, and   \n833 Nan Duan. From lsat: The progress and challenges of complex reasoning. Transactions on   \n834 Audio, Speech, and Language Processing, 2021. https://arxiv.org/abs/2108.00648.   \n835 [119] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan   \n836 Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "837 International Conference on Learning Representations (ICLR), 2022. https://openreview.   \n838 net/forum?id=gEZrGCozdqR.   \n839 [120] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani   \n840 Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,   \n841 Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large   \n842 language models. In Transactions on Machine Learning Research (TMLR), 2022. https:   \n843 //openreview.net/forum?id=yzkSU5zdwD.   \n844 [121] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang,   \n845 Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of   \n846 harm from language models. arXiv preprint, 2021. https://arxiv.org/abs/2112.04359.   \n847 [122] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana   \n848 Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, et al.   \n849 Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint, 2022.   \n850 https://arxiv.org/abs/2211.05100.   \n[123] Mitchell Wortsman, Peter J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D   \n852 Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies for   \n853 large-scale transformer training instabilities. arXiv preprint, 2023. https://arxiv.org/   \n854 abs/2309.14322.   \n855 [124] Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick   \n856 Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs V: Tuning large   \n857 neural networks via zero-shot hyperparameter transfer. In Advances in Neural Information   \n858 Processing Systems (NeuIPS), 2021. https://arxiv.org/abs/2203.03466.   \n859 [125] Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Feature learning in infinite depth   \n860 neural networks. In International Conference on Learning Representations (ICLR), 2024.   \n861 https://openreview.net/forum?id $\\equiv$ 17pVDnpwwl.   \n862 [126] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a   \n863 machine really finish your sentence? In Annual Meeting of the Association for Computational   \n864 Linguistics (ACL), 2019. https://aclanthology.org/P19-1472.   \n865 [127] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision   \n866 transformers. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n867 https://arxiv.org/abs/2106.04560.   \n868 [128] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Advances in Neural   \n869 Information Processing Systems (NeuIPS), 2019. https://arxiv.org/abs/1910.07467.   \n870 [129] Biao Zhang, Ivan Titov, and Rico Sennrich. Improving deep transformer with depth-scaled   \n871 initialization and merged attention. In Empirical Methods in Natural Language Processing   \n872 (EMNLP), 2019. https://aclanthology.org/D19-1083.   \n873 [130] Yanli Zhao, Andrew Gu, Rohan Varma, Liangchen Luo, Chien chin Huang, Min Xu, Less   \n874 Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Bernard   \n875 Nguyen, Geeta Chauhan, Yuchen Hao, and Shen Li. Pytorch fsdp: Experiences on scaling   \n876 fully sharded data parallel. In Very Large Data Bases Conference (VLDB), 2023. https:   \n877 //dl.acm.org/doi/10.14778/3611540.3611569.   \n878 [131] Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun.   \n879 Jec-qa: A legal-domain question answering dataset. In Association for the Advancement of   \n880 Artificial Intelligence (AAAI), 2020. https://arxiv.org/abs/1911.12011.   \n881 [132] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,   \n882 Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation   \n883 models. arXiv preprint, 2023. https://arxiv.org/abs/2304.06364.   \n884 [133] Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries,   \n885 Qian Liu, and Niklas Muennighoff. Astraios: Parameter-efficient instruction tuning code large   \n886 language models. arXiv preprint, 2024. https://arxiv.org/abs/2401.00788. ", "page_idx": 19}, {"type": "text", "text": "887 Contents ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "888 1 Introduction 1   \n889 2 Developing scaling laws for over-training and downstream tasks 2   \n890 2.1 Preliminaries 3   \n891 2.2 Scaling laws for over-training . 3   \n892 2.3 Scaling laws for downstream error 4   \n893 3 Constructing a scaling testbed 5   \n894 3.1 Training setup . . 5   \n895 3.2 Model configurations . 5   \n896 3.3 Fitting scaling laws 6   \n897 3.4 Evaluation setup . 7   \n898 4 Results: Reliable extrapolation 7   \n899 5 Related work 8   \n900 6 Limitations, future work, and conclusion 9   \n901 A Scaling-law derivations 22   \n902 B Additional training details 23   \n903 C Additional grid search details 23   \n904 D Evaluation dataset details 23   \n905 E Additional results 23   \n906 F Additional related work 30   \n907 G Broader impact 31   \n908 H Licensing 31 ", "page_idx": 20}, {"type": "text", "text": "909 A Scaling-law derivations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "910 We first show that reparameterizing Equation (3) in terms of the compute $C$ and token multiplier $M$   \n911 for $\\alpha=\\beta$ yields Equation (4). Combining $C=6N D$ and $M=D/N$ yields $N=\\sqrt{C/(6M)}$ and   \n912 $D=\\sqrt{C M/6}$ . Inserting these into Equation (3) yields, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}(C,M)=E+A\\left(\\frac{C}{6M}\\right)^{-\\frac{\\alpha}{2}}+B\\left(\\frac{C M}{6}\\right)^{-\\frac{\\alpha}{2}},}}\\\\ {{\\displaystyle{=E+\\left(A\\left(\\frac{1}{6}\\right)^{-\\frac{\\alpha}{2}}M^{\\frac{\\alpha}{2}}+B\\left(\\frac{1}{6}\\right)^{-\\frac{\\alpha}{2}}M^{-\\frac{\\alpha}{2}}\\right)C^{-\\frac{\\alpha}{2}}.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "913 This is equal to Equation (4), making the substitutions $\\eta=\\alpha/2$ , $a=A(1/6)^{-\\eta}$ , $b=B(1/6)^{-\\eta}$ , as   \n914 noted in the main body.   \n915 Relation to compute-optimal training. Recall that we made the assumption $\\alpha=\\beta$ , which implies   \n916 equal scaling of parameters and tokens to realize compute-optimal models. While this assumption   \n917 is empirically justified [45], even if $\\alpha\\neq\\beta$ , we get a parameterization that implies the power law   \n918 exponent in Equation (4) remains constant with over-training, while the power law scalar changes.   \n919 To find a compute-optimal training setting, Hoffmann et al. [45] propose to minimize the right-hand   \n920 side of Equation (3) subject to the compute constraint $C=6N D$ . This yields, $N^{*}=\\gamma^{\\frac{1}{\\alpha+\\beta}}(C/6)^{\\frac{\\beta}{\\alpha+\\beta}}$   \n921 and $D^{*}=\\gamma^{-{\\frac{1}{\\alpha+\\beta}}}(C/6)^{\\frac{\\alpha}{\\alpha+\\beta}}$ , where $\\begin{array}{r}{\\gamma=\\frac{\\alpha A}{\\beta B}}\\end{array}$ , for notational convenience. The associated risk is, ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\nL(N^{*},D^{*})=E+\\left(A\\gamma^{\\frac{-\\alpha}{\\beta+\\alpha}}+B\\gamma^{\\frac{\\beta}{\\beta+\\alpha}}\\right)\\left(\\frac{C}{6}\\right)^{-\\frac{\\alpha\\beta}{\\alpha+\\beta}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "922 We now deviate from compu\u221ate-optimal training by modifying the model size and tokens by   \n923 multiplication with a constant $\\sqrt{m}$ , according to ", "page_idx": 21}, {"type": "equation", "text": "$$\nN_{m}=\\frac{1}{\\sqrt{m}}N^{*},\\quad D_{m}=\\sqrt{m}D^{*}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "924 This modification keeps the compute constant (i.e., $6N_{m}D_{m}=6N^{*}D^{*})$ . The risk, then, becomes ", "page_idx": 21}, {"type": "equation", "text": "$$\nL(f_{N_{m},D_{m}})=E+\\left(m^{\\frac{\\alpha}{2}}A\\gamma^{\\frac{-\\alpha}{\\beta+\\alpha}}+m^{-\\frac{\\beta}{2}}B\\gamma^{\\frac{\\beta}{\\beta+\\alpha}}\\right)C^{-\\frac{\\alpha\\beta}{\\alpha+\\beta}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "925 We again expect the same power law exponent and changing power law scalar. Note that $m$ in   \n926 Equation (8) is similar to $M$ in Equation (4). Specifically, $m$ is a multiple of the Chinchilla-optimal   \n927 token multiplier $M^{*}=D^{*}/N^{*}$ , which is no longer fixed as a compute budget changes for $\\alpha\\neq\\beta$ . ", "page_idx": 21}, {"type": "table", "img_path": "5tOVh81aze/tmp/d3f5a6c30c7b6ff3b7a67a05592c1f7e460d0abb691cb430f920a1d657472d6f.jpg", "table_caption": ["Table 3: Main models and hyperparameters used in our investigation. Models have number of parameters $N$ , with number of layers $n_{\\mathrm{layers}}$ , number of attention heads $n_{\\mathrm{heads}}$ , model width $d_{\\mathrm{model}}$ , and width per attention head $d_{\\mathrm{head}}$ . Batch sizes are global and in units of sequences. Each sequence has 2,048 tokens. A100 GPU hours are at $M=20$ , which are near compute-optimal runs. For the 1.4B scale, a batch size of 256 performs slightly better than 512. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "928 B Additional training details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "929 Architecture. As stated in the main paper, we train transformers [116], based on auto  \n930 regressive, decoder-only, pre-normalization architectures like GPT-2 [85] and LLaMA [113]. We   \n931 adopt OpenLM [39] for modeling, which utilizes PyTorch [80, 6], xformers [54], triton [75],   \n932 FlashAttention [24], FSDP [130], and bfloat16 automatic mixed precision. Like LLaMA, we omit   \n933 bias terms, but replace RMSNorm [128] with LayerNorm [8], which has readily available fused   \n934 implementations. Following Wortsman et al. [123], we apply qk-LayerNorm [25], which adds   \n935 robustness to otherwise poor hyperparameter choices (e.g., learning rate). We use SwiGLU [102]   \n936 activations and depth-scaled initialization [129]. We use a sequence length of 2,048, rotary positional   \n937 embeddings [106], and the GPT-NeoX-20B tokenizer [15], which yields a vocabulary size of $50\\mathrm{k}$ .   \n938 We do not use weight tying [84, 46]. We sample without replacement during training and employ   \n939 sequence packing without attention masking. We separate documents in our training corpora with   \n940 end-of-text tokens.   \n941 Objectives and optimization. We train with a standard causal language modeling objective (i.e.,   \n942 next token prediction) with an additive ${\\bf Z}$ -loss [19] (coefficient 1e-4), which mitigates output logit   \n943 norm growth [67] instabilities. We use the AdamW optimizer [62] (PyTorch defaults except beta2 $=$   \n944 0.95), with independent weight decay [123] (coefficient 1e-4). For the learning rate schedule, we use   \n945 linear warmup and cosine decay. We cool down to a low learning rate (3e-5). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "946 C Additional grid search details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "947 Final model configurations. We present our final hyperparameters in Table 3. ", "page_idx": 22}, {"type": "text", "text": "948 Grid search configuration selection. Recall in Section 3.3, we run a grid search over many   \n949 configurations. We present the architectures we sweep over in Table 4. ", "page_idx": 22}, {"type": "text", "text": "950 D Evaluation dataset details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "951 All 46 downstream evaluations are based on MosaicML\u2019s LLM-foundry evaluation suite [69]. We   \n952 specifically consider the datasets given in Table 5. Recall that we use a subset of 17 of these   \n953 evaluations that give signal (are above random chance) for the compute range we consider. See   \n954 Appendix E, where we ablate over the 17 subset design choice by including more and less evaluations. ", "page_idx": 22}, {"type": "text", "text": "955 E Additional results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "956 Scaling law fits. We present specific coefficients for our fits in Table 6. ", "page_idx": 22}, {"type": "text", "text": "957 Small-scale experiments can predict model rank order. We expect to be able to rank hypothetical   \n958 models based on their predicted performance, which is useful when deciding what large-scale runs   \n959 to train. To verify, we rank 9 testbed models with $N\\ge1.4\\mathsf{B}$ by ground-truth top-1 error and by   \n960 estimated top-1 error. We find high rank correlation of 0.88 for the 17-task split.   \n961 Over-performing grid search models experience more optimization steps. As mentioned in   \n962 Section 3.3 and Figure 4, we notice that models between 0.011B to 0.079B (i.e., $5.2\\times10^{16}$ to   \n963 $5.2\\times10^{17}$ FLOPs trained near compute-optimal) over-perform compared to the trend established by   \n964 other models in our initial grid searches. This results in a bump in the scaling plot. While we choose   \n965 to exclude this range of models for our scaling study, we additionally investigate this phenomenon.   \n966 In Figure 6 we color grid search configurations by the number of optimization steps (i.e., number   \n967 of tokens seen divided by batch size divided by sequence length). We notice that models in the   \n968 aforementioned range experience more optimization steps than their $\\mathbf{X}$ -axis neighbors. For context,   \n969 Figure 1 (left) in Kaplan et al. [51] also shows a bump; however, there the performance is worse than   \n970 the general trend instead of better as in our work. We leave understanding more fully the interactions   \n971 between hyperparameters, scaling, and performance to future work.   \n972 Scaling is largely predictable in-distribution $(\\mathbf{ID})$ . Prior work focuses on understanding scaling   \n973 using ID loss, often using training loss directly [51, 45]. Hence, we also consider Paloma [65] loss   \n974 evaluation sets, which are designed to probe performance in specific domains. We use Paloma\u2019s   \n975 C4 [88, 27], RedPajama [112], and Falcon-RefinedWeb [82] splits to probe for ID loss. As seen   \n976 in Figure 7, relative error is mostly low. Relative error is largest for the $N\\,=\\,1.4{\\bf B},M\\,=\\,640$   \n977 RedPajama run at $15.4\\%$ . Examining this case specifically, we find that the model performs better   \n978 than the scaling law prediction. We hypothesize that as a model sees more tokens there is an increased   \n979 likelihood of near-duplicate sequences ID, resulting in performance that is better than predicted.   \n980 Relative error is stable across many choices of downstream evaluation suites. To understand   \n981 how sensitive our investigation is to our choices of downstream evaluation sets, we consider several   \n982 other options as seen in Figure 8. We find that our prediction errors are fairly (i) low and (ii) consistent   \n983 for many choices of downstream evaluation sets including the whole suite of 46 evaluations.   \n984 Scaling can break down when under-training. We find that when a token multiple is too small   \n985 (i.e., under-training regime), scaling appears unreliable. In Figure 9 we see for $M=5$ the scaling   \n986 trend is different. We hypothesize that tuning hyperparameters (e.g., warmup, batch size) directly for   \n987 smaller multipliers may help mitigate the breakdown in predictability. ", "page_idx": 22}, {"type": "image", "img_path": "5tOVh81aze/tmp/158581d3aff6b4f89458b31f47aa1f771e2eb05f0fc05c890604afcc6daf388a.jpg", "img_caption": ["Figure 6: Understanding over-performing models in our grid search. (left) Models trained with $5.\\bar{2}\\times10^{16}$ to $5.2\\times10^{17}$ FLOPs over-perform relative to their neighbors. In looking at the number of optimization steps, we notice that the over-performing models experience more optimization steps than their ${\\bf X}$ -axis neighbors. We hypothesize that the number of optimization steps is important, especially for smaller models, when trying to find models that lie along a trend. (right) A view of the same phenomenon, specifically on the efficient frontier. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "image", "img_path": "5tOVh81aze/tmp/3b7eeda45c7620e8bd4476c5fa2b6fefcdd21ece28a390334f265d2497397690.jpg", "img_caption": ["Figure 7: In-distribution $(\\mathbf{ID})$ settings. Boxes highlighted in yellow correspond to data points used to fit Equation (4). Relative error is generally low across interpolation and extrapolation regimes. Relative error is largest for the RedPajama $N=1.4\\mathrm{B}$ , $M=640$ prediction at $15.4\\%$ . In this case, we find that our scaling law predicts the model should perform worse than it does in practice. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "5tOVh81aze/tmp/29c31cff69685e82a28a39e7a16df81bbbf9a18dbbebd4ecdf95695eee4d0770.jpg", "img_caption": ["t percentage points above random chance at 0.154B scales) "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "5tOVh81aze/tmp/96e5228a95b01fe1d6814a5cafeb843dd0b73e9f580164861c6985e05e826aec.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 8: Downstream evaluation set ablation for 6.9B parameter, 138B token runs. Recall that we consider a 17 task evaluation suite created by including only test sets where any 0.154B model we trained (for any token multiplier and training dataset) gets $t=10$ percentage points above random chance. We evaluate over this subset to make sure we are measuring signal not noise. Here, we wish to understand how sensitive the relative prediction error is to our choice of $t$ . (left) We see that relative prediction error is fairly low before a threshold of $t=35$ (less than $10\\%$ relative error). When too many tasks are excluded (i.e., $t\\geq40$ ) relative error spikes. Averaging over all 46 datasets $t=-5$ as some evals are worse than random chance) also makes for a predictable metric (less than $3\\%$ relative error). (right) A parallel view, showing how many tasks are removed as $t$ increases. 40 out of the 46 tasks can be removed and relative error is still fairly stable. ", "page_idx": 24}, {"type": "image", "img_path": "5tOVh81aze/tmp/070ad38f93b03bddf9e3ff2c9a53d52ae01b833ffe4ff80e6e9c5e715a30eaa4.jpg", "img_caption": ["Figure 9: Scaling with small token multipliers. For smaller multipliers (e.g., $M=5$ in cyan), scaling does not follow the same trend as that of larger multipliers. Additionally, many token multipliers (e.g., $M\\in\\{10,20,40,80\\})$ garner points close to the compute-optimal frontier. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "5tOVh81aze/tmp/35f18136a3b5b1efc35ac8b00661e4adcac68a655cf5f82aab3bbb0e63ae4cbe.jpg", "img_caption": ["Figure 10: Out-of-distribution (OOD) settings. Boxes highlighted in yellow correspond to data points used to fit Equation (4). Recall that the C4 training set is English-filtered. Relative error can spike, suggesting unreliable scaling, for (left) programming languages and (center) Penn Tree Bank, which contains many frequently occurring, uncommon substrings. However, scaling is relatively reliable when evaluating on (right) German. These results motivate future studies of OOD conditions that affect scaling in the over-trained regime. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "5tOVh81aze/tmp/3a19fa5cdc55065564d08a2d0debac681ad3621e791336a87b4275f20d918142.jpg", "img_caption": ["Figure 11: Relative error on average top-1 predictions (46 task split). Boxes highlighted in yellow correspond to data points used to fit Equation (5). Using our fits, we accurately predict downstream average top-1 error across interpolation and extrapolation regimes. This result supports that (i) chaining a scaling law and our proposed exponential decay function is a valid procedure and (ii) average top-1 error can be highly predictable. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "988 Scaling can be unpredictable out-of-distribution (OOD). Our main result shows reliable C4 eval   \n989 loss predictions with models trained on RedPajama, which is an OOD evaluation setting. However,   \n990 both C4 and RedPajama both contain tokens sourced from CommonCrawl.   \n991 To further probe OOD performance, we measure the relative error of scaling laws fti to models trained   \n992 on C4 and evaluated on Paloma\u2019s 100 programming languages [65], Paloma\u2019s Penn Tree Bank (PTB)   \n993 split [66], and a German version of C4 [27]. Recall that the C4 training set we use has been filtered   \n994 for English text. Hence we expect (i) the proportion of code is minimal, (ii) the \u201c<unk>\u201d substrings in   \n995 PTB raw text do not appear frequently, and (iii) German is not prevalent. We notice that extrapolation   \n996 relative error tends to be high for large $M,N$ on programming languages and PTB (Figure 10 (left,   \n997 center)). In contrast, for German C4, relative error is still low across the extrapolation range, with a   \n998 maximum relative error of $7.6\\%$ at the $N=1.4\\mathrm{B}$ , $M=80$ scale (Figure $10\\left(r i g h t\\right))$ . We hypothesize   \n999 that further modifications to scaling laws are necessary to predict when scaling should be reliable as a   \n1000 function of the training and evaluation distributions.   \n1001 Small-scale experiments can predict average downstream top-1 error. To verify that chaining   \n1002 Equations (4) and (5) is effective in practice, we collect C4 eval loss and downstream error pairs for   \n1003 the configurations in Table 1. In Figure 11, we look at relative error for our scaling predictions in the   \n1004 context of Average top-1 error over 46 evals and in Figure 12 over the high-signal 17 eval subset. We   \n1005 again notice reliable scaling in interpolation and extrapolation regimes, suggesting the validity of our   \n1006 procedure to predict downstream average top-1 error.   \n1007 Loss evaluation ablations for downstream trends. Figure 13 presents the correlation between   \n1008 downstream error and loss evaluated on different validation sets (C4, RedPajama, and RefinedWeb).   \n1009 Regardless of the validation set $\\mathbf{\\chi}_{\\mathbf{X}}$ -axis), models follow the exponential decay relationship given   \n1010 in Equation (5), suggesting the choice of validation loss is not critical for the appearance of this   \n1011 phenomenon.   \n1012 Investing more compute in a scaling law makes it more predictive. Thus far we have looked   \n1013 at standard configurations from Table 1 to construct our scaling laws, mainly to demonstrate   \n1014 extrapolation to larger $N,M$ . However, for practitioners, the main constraint is often training   \n1015 compute. Hence, we wish to understand the trade-offs between the amount of compute invested   \n1016 in creating a scaling law and the relative error of the resulting law in the over-trained regime. In   \n1017 Figure 14 (left), we see that as one increases the amount of compute, it is possible to get better fits   \n1018 with lower relative error. In Figure 14 (right), we see a similar trend as one increases the number of   \n1019 data points used to fti a scaling law. Blue stars indicate the configurations from Table 1, which provide   \n1020 accurate predictions relative to the general trends\u2014hinting at their usefulness for our investigation.   \n1021 In Figures 15 and 16 we repeat the compute analysis comparing trade-offs for loss prediction and   \n1022 error prediction for our RedPajama 1.4B parameter, 900B token and 6.9B parameter, 138B token   \n1023 runs respectively. We find that less compute is generally necessary to construct a loss scaling law that   \n1024 achieves the same relative error as that of an error prediction scaling law.   \n1025 On compute-optimal token multipliers. We consider 20 tokens per parameter as close to compute  \n1026 optimal for our experiments. Here we investigate, using different approaches, what the compute  \n1027 optimal token multipliers are for each dataset\u2014assuming one should scale number of parameter and   \n1028 training tokens equally as Hoffmann et al. [45] suggest.   \n1029 Turning to Figure 9, we notice that there are many multipliers, between 10 and 80 that yield models   \n1030 close to the frontier. Hence, empirically, it appears choices within this range should be suitable for   \n1031 the optimal token multiplier.   \n1032 We can also compute an optimal token multiplier using the coefficients in Table 6. Based on Hoffmann   \n1033 et al. [45]\u2019s Equation (4) and the assumption that $\\alpha=\\beta$ , we write, ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "image", "img_path": "5tOVh81aze/tmp/623fa3354f7e16ffb896576490efb15c632b1b693dfd7f6716de96fd7f513c3b.jpg", "img_caption": ["Figure 12: Relative error on average top-1 predictions (17 task split). Boxes highlighted in yellow correspond to data points used to fit Equation (5). Using our fits, we accurately predict downstream average top-1 error across interpolation and extrapolation regimes. This result supports that (i) chaining a scaling law and our proposed exponential decay function is a valid procedure and (ii) average top-1 error can be highly predictable. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "5tOVh81aze/tmp/7b4e3a3091ed60bbb429c9e6f6d9a58bda8e496121511bc4e75f09d791825f4f.jpg", "img_caption": ["Figure 13: Correlation between average top-1 error and evaluation loss. We observe that regardless of evaluation loss distribution ( $\\mathbf{\\widetilde{X}}^{\\prime}$ -axis), models tend to follow Equation (5). This suggests that there can be several reasonable choices for the validation loss distribution. Additionally, ID models trained on C4 and evaluated on a C4 validation set, perform best in terms of loss, but these gains don\u2019t necessarily translate to lower error downstream (e.g., (left column)). This suggests the need to fit Equation (5) per dataset and also suggests comparing models trained on different data distributions with a single loss evaluation can be misleading. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "image", "img_path": "5tOVh81aze/tmp/f2da171d82f2d45748d9b1cc29a07a3ca1d4fe2b1326d02f074143bc7a37ed08.jpg", "img_caption": ["Figure 14: Trade-offs between scaling law for loss fitting considerations and reliability. Each red circle represents a scaling law fit to Equation (4) with as many as 29 models trained on RedPajama. Specifically, a grid formed by $\\mathbf{\\bar{\\calN}_{\\mathrm{~\\rightmoon~}}}\\{0.011\\mathbf{B},0.079\\mathbf{B},\\bar{0}.154\\mathbf{B},0.411\\mathbf{B}\\},M\\;\\in\\;\\mathbf{\\bar{\\Sigma}}$ $\\{5,10,20,40,80,160,320\\}$ gives 28 models and a $N=1.4B,M=20$ run gives the last model. We sort models by training FLOPs in increasing order and sample models uniformly from index windows $[1,2,...,n]$ for $n\\in[5,6,..,29]$ to fti Equation (4). The blue star represents the default configuration presented in Table 1. The prediction target is a $N=1.4B,M=640$ $\\mathbf{\\mathit{D}}=900\\mathbf{\\mathit{B}}$ ) model. As the amount of compute (left) and the number of points (right) used to fti the scaling law increases, relative error trends downwards. Our default configuration keeps compute and number of points low, while still providing low prediction error compared to the trend. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "5tOVh81aze/tmp/7d63549913ab0be75c4b845df7883a07afe6b177047bf9b37566537639462993.jpg", "img_caption": ["Figure 15: Compute vs. relative error for the 1.4B, 900B token RedPajama run. (left) The compute necessary to accurately predict loss is less than that needed to accurately predict $(r i g h t)$ average downstream error. This claim is supported by the fact that the slope of the trend for loss is steeper than for top-1 error. These findings corroborate Figure 16. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "image", "img_path": "5tOVh81aze/tmp/f4b67a685038d35917a1d5abe459970c1b4e51bbe7bef6f80eba9f4cb4504564.jpg", "img_caption": ["Figure 16: Compute vs. relative error for the 6.9B, 138B token RedPajama run. (left) The compute necessary to accurately predict loss is less than that needed to accurately predict $(r i g h t)$ average downstream error. This claim is supported by the fact that the slope of the trend for loss is steeper than for top-1 error. These findings corroborate Figure 15. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "5tOVh81aze/tmp/f5727aae3ba8fe97a938fe9c44cd0b2ba19ddaa738a503a2f40a326b7f41978a.jpg", "img_caption": ["Figure 17: Scaling exponent vs. token multiplier. In Figure 2, we notice roughly parallel lines (i.e., roughly constant scaling exponent $\\eta$ ) in the log-log plot of loss vs. compute, even as the token multiplier $M$ changes. Here we plot $\\eta$ vs. $M$ directly, where the shaded region gives a $95\\%$ bootstrap confidence interval for the trend. This view supports that $\\eta$ is relatively constant. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "equation", "text": "$$\nN^{*}(C)=G\\left(\\frac{C}{6}\\right)^{\\frac{1}{2}},D^{*}(C)=G^{-1}\\left(\\frac{C}{6}\\right)^{\\frac{1}{2}},G=\\left(\\frac{a}{b}\\right)^{\\frac{1}{4\\eta}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "1034 To compute $M^{*}=D^{*}/N^{*}$ , we then have, ", "page_idx": 28}, {"type": "equation", "text": "$$\nM^{*}=\\left({\\frac{b}{a}}\\right)^{\\frac{1}{2\\eta}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "1035 Using the values from Table 6 and plugging into Equation (10), we find $M_{\\mathrm C4}^{*}=2.87$ , $M_{\\mathrm{RedPajama}}^{*}=$   \n1036 4.30, $M_{\\mathrm{RefinedWeb}}^{\\ast}=3.79$ , where the subscript gives the dataset name. These values confilct with the   \n1037 observation in Figure 9, which suggests $M=5$ is already too small to give points on the Pareto   \n1038 frontier. We hypothesize this mismatch arises because we fit our scaling laws using models with   \n1039 $M\\geq20$ . ", "page_idx": 28}, {"type": "image", "img_path": "5tOVh81aze/tmp/6a2bb292d3a25d1549c20f3eb80b6191872bf9d3eb5c27814f7d4f0320568fcf.jpg", "img_caption": ["Figure 18: Downstream top-1 error vs. C4 eval loss for each of the 46 downstream evals. Here we plot models from our testbed for each scatter plot. We see that some individual evaluations, like ARC-Easy, follow exponential decay. Others, like BIG-bench: CS algorithms, show step function behavior. Still others, like MathQA, hover around random chance. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "1040 F Additional related work ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1041 Language modeling. Language models can be grouped into encoder-only [26, 53, 59, 96, 22],   \n1042 encoder-decoder [56, 89], and decoder-only architectures [85, 113, 114, 110, 49, 38, 74, 7, 111,   \n1043 28, 64, 99, 122, 4, 57, 63, 34]. Most current implementations are based on the transformer [116].   \n1044 However, there has been a recent resurgence in scaling language models based on non-transformer   \n1045 architectures [83, 36, 37, 35]. Further, there has been substantial work on adapting pre-trained   \n1046 language models to better follow instructions [119, 20, 70, 61, 71, 133, 87, 29, 115, 103, 73].   \n1047 However, following prior work [45, 72] and given their overall prevalence, we limit ourselves to   \n1048 GPT-style, decoder-only transformers that have solely been pre-trained.   \n1049 Scaling laws. Kaplan et al. [51] investigate scaling trends in GPT language models. Bahri et al.   \n1050 [9] investigate different scaling regimes theoretically, and Sharma & Kaplan [101] relate scaling   \n1051 coefficients to data manifold dimensions. Tay et al. [108, 109] elucidate the connection between   \n1052 model architecture and scaling trends, while Hernandez et al. [42], Tay et al. [108] develop scaling   \n1053 laws for transfer learning. Ivgi et al. [48] also consider transfer learning scaling laws and highlight   \n1054 the importance of hyperparameter selection in the low-compute regime. Ghorbani et al. [32], Gordon   \n1055 et al. [33], Bansal et al. [10] develop scaling laws for neural machine translation. Caballero et al. [17]   \n1056 propose a scaling law functional form, which they demonstrate is predictive in several domains.   \n1057 Scaling beyond language modeling. There is a large body of work on scaling neural networks   \n1058 beyond language modeling, for example in computer vision [60, 127, 105, 1, 2], multimodal   \n1059 learning [41, 18, 30], and image reconstruction [52].   \n1060 Over-training in existing models. To contextualize the extent to which we over-train, we provide   \n1061 token multipliers for popular models in Table 8. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "1062 G Broader impact ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1063 Language models have known risks in terms harmful language, toxicity, and human automation\u2014to   \n1064 name a few [121, 12]. We will include the following for our public release \u201cWARNING: These are   \n1065 base models and not aligned with post-training. They are provided as is and intended as research   \n1066 artifacts only.\u201d However, even as research artifacts, we recognize that models can still be misused   \n1067 by malicious actors or can be harmful to benevolent actors. When deciding to release our models   \n1068 and experiments, we considered (i) the benefit to the scientific community and (ii) the benchmark   \n1069 performance relative to other models that have already been released. For (i) we feel that our testbed   \n1070 is of use to others in the community who want to do scaling research, but do not necessarily have the   \n1071 means to train these model artifacts themselves. Hence, we predict (and hope) releasing all models   \n1072 and experiments will be helpful to others wanting to participate in scaling research. For (ii), we note   \n1073 that there are publicly available models [113, 114, 49], which outperform models from our testbed   \n1074 and that are more likely to be widely adopted. Finally, we recognize that advancing scaling science   \n1075 also has potential for harm. Specifically, while we are concerned with loss and downstream task   \n1076 performance for popular evaluation settings, it is possible that nefarious actors may use scaling laws   \n1077 to help design more harmful models. ", "page_idx": 30}, {"type": "text", "text": "1078 H Licensing ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1079 In terms of licensing, we will release our code, models, and experiments under an MIT licence, which   \n1080 is also attached to our supplementary submission. ", "page_idx": 30}, {"type": "table", "img_path": "5tOVh81aze/tmp/3621d33dd5b3e2bf487b6b76bd155ce057f779274f75e7cce68e54eb6f70c532.jpg", "table_caption": ["Table 4: Topologies for our grid searches. We consider 130 architectures for our grid search. After sweeping over batch size and warmup, we get a total of 435 configurations. "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "5tOVh81aze/tmp/42069da2fb4380c138595669a8836e2a3a238b5bb19de1659ee3cf847ab298e8.jpg", "table_caption": ["Table 5: 46 downstream tasks. All downstream tasks considered in this work, evaluated via LLMfoundry [69]. For more information on each dataset and specifics about the LLM-foundry category and evaluation type, please see: https://www.mosaicml.com/llm-evaluation. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "5tOVh81aze/tmp/6cd80284565e5e01896b3cdf69661883abd5930f46b036c905f3015d6903d6ee.jpg", "table_caption": ["Table 6: Scaling law fit parameters. Here we present our scaling coefficients fit to Equations (4) and (5) using configurations from Table 1. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "5tOVh81aze/tmp/a6f0e4a7f51736e3d8d5a01f214ebb374927c7ff6c25b2bb7e846f281769cde8.jpg", "table_caption": ["Table 7: Downstream relative prediction error at 6.9B, 138B tokens, with and without the 1.4B data point. Recall in Table 1, we introduce a $N=1.4\\mathrm{B}$ , $M=20$ run to get better downstream error predictions. Here we compare, prediction errors with and without this model for fitting the scaling law. Note that without the model (i.e., rows with \u201cw/o 1.4B\u201d) average top-1 predictions, over the 17 tasks. are less accurate. "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "5tOVh81aze/tmp/195de7545cf976c69fc6743fbb777f87ca4f94b3fd19b43969d54f059f71c36c.jpg", "table_caption": ["Table 8: Token multipliers of existing models. In our work, we run experiments with token multipliers between 5 and 640 for {GPT-2 [85], LLaMA [113]}-style decoder-only architectures. "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "1081 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "082 Claims   \n083 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n084 paper\u2019s contributions and scope?   \n085 Answer: [Yes]   \n086 Justification: The experiment section justify the claims made in the abstract and introduction,   \n087 namely that the developed scaling laws for over-training and downstream task prediction are   \n088 predictive in practice for larger scale runs.   \n089 Guidelines:   \n090 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n091 made in the paper.   \n092 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n093 contributions made in the paper and important assumptions and limitations. A No or   \n094 NA answer to this question will not be perceived well by the reviewers.   \n095 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n096 much the results can be expected to generalize to other settings.   \n097 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n098 are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The final section discusses limitations, which provide motivation for future work. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "1148 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "149 Question: Does the paper fully disclose all the information needed to reproduce the   \n150 main experimental results of the paper to the extent that it affects the main claims and/or   \n151 conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Justification: We point to all public datasets and open source training infrastructure. We additionally specify all hyperparameters used for training. ", "page_idx": 35}, {"type": "text", "text": "6 \u2022 The answer NA means that the paper does not include experiments. \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n8 well by the reviewers: Making the paper reproducible is important, regardless of   \n9 whether the code and data are provided or not. \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n2 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n3 For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n65 be necessary to either make it possible for others to replicate the model with the same   \n6 dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed   \n8 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n9 of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. \u2022 While NeurIPS does not require releasing code, the conference does require all   \n2 submissions to provide some reasonable avenue for reproducibility, which may depend   \n3 on the nature of the contribution. For example   \n74 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n5 to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n8 (c) If the contribution is a new model (e.g., a large language model), then there should   \n9 either be a way to access this model for reproducing the results or a way to reproduce   \n0 the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n2 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n3 authors are welcome to describe the particular way they provide for reproducibility.   \n4 In the case of closed-source models, it may be that access to the model is limited in   \n5 some way (e.g., to registered users), but it should be possible for other researchers   \n6 to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "1187 5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1188   \n1189   \n1190   \n1191   \n1192   \n1193   \n1194   \n1195   \n1196   \n1197   \n1198   \n1199   \n1200   \n1201   \n1202   \n1203   \n1204   \n1205   \n1206   \n1207   \n1208   \n1209   \n1210   \n1211   \n1212   \n1213   \n1214   \n1215   \n1216   \n1217   \n1218   \n1219   \n1220   \n1221   \n1222   \n1223   \n1224   \n1225   \n1226   \n1227   \n1228   \n1229   \n1230   \n1231   \n1232   \n1233   \n1234   \n1235   \n1236   \n1237   \n1238 ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We include code and data needed to reproduce all figures in the paper. Our datasets are sourced from HuggingFace and our training code utilizes OpenLM, which is open-source. Guidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.   \n6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We explicitly have sections and appendices that detail our experimental setup (training and evaluation) and title the sections and appendices to indicate this. Guidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material.   \n7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: When appropriate we report bootstrap $95\\%$ confidence intervals (e.g., in Figure 4 and Figure 17). We do not train models with many seeds, which is prohibitively expensive. Given the large size of the C4 validation set, we observe that bootstrap $95\\%$ confidence intervals for loss (computed over either token an sequence sampling) are close to zero. Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 36}, {"type": "text", "text": "1239 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars,   \n1240 confidence intervals, or statistical significance tests, at least for the experiments that   \n1241 support the main claims of the paper.   \n1242 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n1243 example, train/test split, initialization, random drawing of some parameter, or overall   \n1244 run with given experimental conditions).   \n1245 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n1246 call to a library function, bootstrap, etc.)   \n1247 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n1248 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1249 of the mean.   \n1250 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1251 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1252 of Normality of errors is not verified.   \n1253 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1254 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1255 error rates).   \n1256 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1257 they were calculated and reference the corresponding figures or tables in the text.   \n1258 8. Experiments Compute Resources   \n1259 Question: For each experiment, does the paper provide sufficient information on the   \n1260 computer resources (type of compute workers, memory, time of execution) needed to   \n1261 reproduce the experiments?   \n1262 Answer: [Yes]   \n1263 Justification: We are transparent about how many GPU hours it takes to construct our scaling   \n1264 laws and train our models (e.g., in Table 1).   \n1265 Guidelines:   \n1266 \u2022 The answer NA means that the paper does not include experiments.   \n1267 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1268 or cloud provider, including relevant memory and storage.   \n1269 \u2022 The paper should provide the amount of compute required for each of the individual   \n1270 experimental runs as well as estimate the total compute.   \n1271 \u2022 The paper should disclose whether the full research project required more compute   \n1272 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1273 didn\u2019t make it into the paper).   \n1274 9. Code Of Ethics   \n1275 Question: Does the research conducted in the paper conform, in every respect, with the   \n1276 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1277 Answer: [Yes]   \n1278 Justification: We have reviewed the code of ethics and feel that our research abides by this   \n1279 code in every respect.   \n1280 Guidelines:   \n1281 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1282 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1283 deviation from the Code of Ethics.   \n1284 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special   \n1285 consideration due to laws or regulations in their jurisdiction).   \n1286 10. Broader Impacts   \n87 Question: Does the paper discuss both potential positive societal impacts and negative ", "page_idx": 37}, {"type": "text", "text": "88 societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "1290 Justification: This work is related to predicting the performance of language models, before   \n1291 they are trained. As such, it falls under the category of basic research. However, because we   \n1292 produce generative language model artifacts as part of our paper, we recognize that these   \n1293 pre-trained models can pose risk. We provide a discussion of risks in Appendix G.   \n1294 Guidelines:   \n1295 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1296 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1297 impact or why the paper does not address societal impact.   \n1298 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1299 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1300 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1301 groups), privacy considerations, and security considerations.   \n1302 \u2022 The conference expects that many papers will be foundational research and not tied   \n1303 to particular applications, let alone deployments. However, if there is a direct path to   \n1304 any negative applications, the authors should point it out. For example, it is legitimate   \n1305 to point out that an improvement in the quality of generative models could be used to   \n1306 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1307 that a generic algorithm for optimizing neural networks could enable people to train   \n1308 models that generate Deepfakes faster.   \n1309 \u2022 The authors should consider possible harms that could arise when the technology is   \n1310 being used as intended and functioning correctly, harms that could arise when the   \n1311 technology is being used as intended but gives incorrect results, and harms following   \n1312 from (intentional or unintentional) misuse of the technology.   \n1313 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1314 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1315 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1316 feedback over time, improving the efficiency and accessibility of ML).   \n1317 11. Safeguards   \n1318 Question: Does the paper describe safeguards that have been put in place for responsible   \n1319 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1320 image generators, or scraped datasets)?   \n1321 Answer: [Yes]   \n1322 Justification: We provide discussion of responsible release in Appendix G. Specifically,   \n1323 models in this release are know to be less capable than state-of-the-art, publicly available   \n1324 models [113, 114, 49], and, hence, we feel the risk for misuse is low.   \n1325 Guidelines:   \n1326 \u2022 The answer NA means that the paper poses no such risks.   \n1327 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1328 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1329 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1330 safety filters.   \n1331 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1332 should describe how they avoided releasing unsafe images.   \n1333 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1334 not require this, but we encourage authors to take this into account and make a best   \n1335 faith effort.   \n1336 12. Licenses for existing assets   \n1337 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1338 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1339 properly respected?   \n1340 Answer: [Yes]   \n1341 Justification: We utilize data-sources publicly available on the HuggingFace platform and   \n1342 abide by the terms of use. For C4: Open Data Commons License Attribution family, for   \n1343 RedPajama: a list of licenses (found here.), for RefinedWeb: Open Data Commons License   \n1344 Attribution family. We use the OpenLM repo for training and also abide by their MIT license.   \n1345 We cite all papers and repos in the main text.   \n1346 Guidelines:   \n1347 \u2022 The answer NA means that the paper does not use existing assets.   \n1348 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1349 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1350 URL.   \n1351 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1352 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1353 service of that source should be provided.   \n1354 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1355 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1356 has curated licenses for some datasets. Their licensing guide can help determine the   \n1357 license of a dataset.   \n1358 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1359 the derived asset (if it has changed) should be provided.   \n1360 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1361 the asset\u2019s creators.   \n1362 13. New Assets   \n1363 Question: Are new assets introduced in the paper well documented and is the documentation   \n1364 provided alongside the assets?   \n1365 Answer: [Yes]   \n1366 Justification: Our code release documents all new model assets under the exp_db/ folder   \n1367 and includes a MIT license. This is also specified in Appendix H.   \n1368 Guidelines:   \n1369 \u2022 The answer NA means that the paper does not release new assets.   \n1370 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1371 submissions via structured templates. This includes details about training, license,   \n1372 limitations, etc.   \n1373 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1374 asset is used.   \n1375 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1376 create an anonymized URL or include an anonymized zip file.   \n1377 14. Crowdsourcing and Research with Human Subjects   \n1378 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1379 include the full text of instructions given to participants and screenshots, if applicable, as   \n1380 well as details about compensation (if any)?   \n1381 Answer: [NA]   \n1382 Justification: This research does not involve crowdsourcing or human subjects.   \n1383 Guidelines:   \n1384 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1385 human subjects.   \n1386 \u2022 Including this information in the supplemental material is fine, but if the main   \n1387 contribution of the paper involves human subjects, then as much detail as possible   \n1388 should be included in the main paper.   \n1389 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1390 or other labor should be paid at least the minimum wage in the country of the data   \n1391 collector.   \n1392 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "Subjects ", "page_idx": 39}, {"type": "text", "text": "1394 Question: Does the paper describe potential risks incurred by study participants, whether   \n1395 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1396 approvals (or an equivalent approval/review based on the requirements of your country or   \n1397 institution) were obtained?   \n1398 Answer: [NA]   \n1399 Justification: This paper does not involve research with human subjects.   \n1400 Guidelines:   \n1401 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1402 human subjects.   \n1403 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1404 may be required for any human subjects research. If you obtained IRB approval, you   \n1405 should clearly state this in the paper.   \n1406 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n1407 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n1408 guidelines for their institution.   \n1409 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n1410 applicable), such as the institution conducting the review. ", "page_idx": 40}]