[{"figure_path": "5tOVh81aze/figures/figures_1_1.jpg", "caption": "Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6ND. Larger values of M specify more over-training. We are able to extrapolate, in both N and M, the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute. For this figure, we train all models on RedPajama [112].", "description": "The figure shows two plots. The left plot shows how a scaling law for model validation loss can be extrapolated to larger models and more over-training than those used to construct the law. The right plot shows a scaling law for downstream task error, which is predicted using the model's validation loss.  Both scaling laws demonstrate reliable extrapolation to models requiring significantly more compute than those used for the fitting.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_2_1.jpg", "caption": "Figure 2: Scaling in the over-trained regime follows consistent power law exponents. We notice parallel lines in the log-log plots of reducible loss vs. training compute for a range of token multipliers M, which give the ratio of training tokens to model parameters. Larger M corresponds to more over-training. For a power law giving reducible loss as a function of compute: L'(C) = x \u00b7 C\u00af\", the exponent \u03b7 remains relatively constant resulting in lines with approximately fixed slope (Figure 17). The scalar X that determines the y-intercept, however, shifts with different token multipliers. This suggests A is a function of the token multiplier, while \u03b7 is not.", "description": "This figure shows that the scaling laws for reducible loss remain consistent even in the over-trained regime. The plots show reducible loss vs training compute for different token multipliers (M). Larger M indicates more overtraining. Although the scaling exponent (\u03b7) remains constant, the scalar (\u03bb) changes with different M values. This suggests \u03bb depends on M, while \u03b7 doesn't.", "section": "2 Developing scaling laws for over-training and downstream tasks"}, {"figure_path": "5tOVh81aze/figures/figures_3_1.jpg", "caption": "Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6ND. Larger values of M specify more over-training. We are able to extrapolate, in both N and M, the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute. For this figure, we train all models on RedPajama [112].", "description": "This figure shows two scaling laws. The left panel shows a scaling law for validation loss as a function of compute and token multiplier.  The extrapolation shows that models with much larger compute can be accurately predicted using smaller models. The right panel shows the relationship between validation loss and downstream task error; a separate scaling law is fit to predict downstream task performance.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_4_1.jpg", "caption": "Figure 4: Search, filter, fit: A recipe for selecting configurations for scaling. (left) To generate the final configurations presented in Table 3, we run a 435 model grid search over model width, hidden dimension, number of attention heads, batch size, and warmup steps. All models are trained near compute-optimally. (center) We plot the efficient frontier of models, which appear to follow a trend, excluding models from 5.2 \u00d7 1016 to 5.2 \u00d7 1017, which fall below the trend. (right) We fit a power law with irreducible error to the remaining configurations, picking four configurations that closely track the full model suite (\u201cSelected models\u201d). These models extrapolate the performance of 1.4B, 6.9B target models. Shaded regions represent bootstrap 95% confidence intervals.", "description": "This figure demonstrates the process of selecting model configurations for scaling experiments.  It starts with a large grid search to find many candidate models, then filters these models to identify the efficient frontier (best performance for given compute). Finally, it uses a power law fit to extrapolate the performance of larger models from the smaller ones, using just a subset of the models for the fit.", "section": "3 Constructing a scaling testbed"}, {"figure_path": "5tOVh81aze/figures/figures_6_1.jpg", "caption": "Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6ND. Larger values of M specify more over-training. We are able to extrapolate, in both N and M, the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute. For this figure, we train all models on RedPajama [112].", "description": "The figure shows two plots illustrating the scaling laws for model validation loss and downstream task performance. The left plot demonstrates extrapolation of model validation loss using a scaling law parameterized by token multiplier and compute. The right plot shows a scaling law to predict average downstream top-1 error as a function of validation loss, highlighting the benefit of using more expensive models for downstream error prediction. Both plots demonstrate reliable scaling with over-training.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_23_1.jpg", "caption": "Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6ND. Larger values of M specify more over-training. We are able to extrapolate, in both N and M, the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute. For this figure, we train all models on RedPajama [112].", "description": "The figure shows two scaling laws: one for validation loss and one for downstream top-1 error. The left plot demonstrates the ability to extrapolate model validation loss with over 300x less compute than needed for the largest model. The right plot shows the relationship between validation loss and downstream error, enabling error prediction with 20x less compute.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_24_1.jpg", "caption": "Figure 7: In-distribution (ID) settings. Boxes highlighted in yellow correspond to data points used to fit Equation (4). Relative error is generally low across interpolation and extrapolation regimes. Relative error is largest for the RedPajama N = 1.4B, M = 640 prediction at 15.4%. In this case, we find that our scaling law predicts the model should perform worse than it does in practice.", "description": "This figure shows the relative error in predicting the C4 eval loss for different model sizes and training token multiples, using three different training datasets (C4, RedPajama, and RefinedWeb). The yellow boxes highlight the data points used to fit the scaling law in Equation (4). Overall, the relative error is low across both interpolation and extrapolation, showing that the scaling law is generally accurate. However, there is a notable exception in the case of the RedPajama dataset with a 1.4B parameter model and a training token multiple of 640. In this case, the model performs significantly better than predicted by the scaling law.", "section": "4 Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_24_2.jpg", "caption": "Figure 8: Downstream evaluation set ablation for 6.9B parameter, 138B token runs. Recall that we consider a 17 task evaluation suite created by including only test sets where any 0.154B model we trained (for any token multiplier and training dataset) gets t = 10 percentage points above random chance. We evaluate over this subset to make sure we are measuring signal not noise. Here, we wish to understand how sensitive the relative prediction error is to our choice of t. (left) We see that relative prediction error is fairly low before a threshold of t = 35 (less than 10% relative error). When too many tasks are excluded (i.e., t > 40) relative error spikes. Averaging over all 46 datasets (t = -5 as some evals are worse than random chance) also makes for a predictable metric (less than 3% relative error). (right) A parallel view, showing how many tasks are removed as t increases. 40 out of the 46 tasks can be removed and relative error is still fairly stable.", "description": "This figure shows the results of an ablation study on the downstream evaluation dataset used in the paper.  The authors investigated how sensitive their prediction error was to the inclusion threshold (t).  The left panel shows that the relative prediction error remained low until t reached 35 (less than 10% error), but spiked when more than 40 tasks were excluded.  The right panel complements this by showing how many tasks were excluded as t increased, indicating that the relative error was stable even with 40 of the 46 tasks removed. This suggests that the model's ability to predict downstream performance is robust and not overly sensitive to the specific choice of evaluation tasks.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_24_3.jpg", "caption": "Figure 8: Downstream evaluation set ablation for 6.9B parameter, 138B token runs. Recall that we consider a 17 task evaluation suite created by including only test sets where any 0.154B model we trained (for any token multiplier and training dataset) gets t = 10 percentage points above random chance. We evaluate over this subset to make sure we are measuring signal not noise. Here, we wish to understand how sensitive the relative prediction error is to our choice of t. (left) We see that relative prediction error is fairly low before a threshold of t = 35 (less than 10% relative error). When too many tasks are excluded (i.e., t > 40) relative error spikes. Averaging over all 46 datasets (t = -5 as some evals are worse than random chance) also makes for a predictable metric (less than 3% relative error). (right) A parallel view, showing how many tasks are removed as t increases. 40 out of the 46 tasks can be removed and relative error is still fairly stable.", "description": "This figure explores the sensitivity of downstream prediction error to the choice of evaluation tasks.  The left panel shows that relative prediction error remains low until a threshold where only tasks where at least one model performs 10% above random chance are included. The right panel shows the number of tasks excluded at various thresholds. The key takeaway is that while prediction accuracy suffers when using too few tasks, average downstream error remains predictable even when many tasks are excluded.", "section": "4 Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_24_4.jpg", "caption": "Figure 2: Scaling in the over-trained regime follows consistent power law exponents. We notice parallel lines in the log-log plots of reducible loss vs. training compute for a range of token multipliers M, which give the ratio of training tokens to model parameters. Larger M corresponds to more over-training. For a power law giving reducible loss as a function of compute: L'(C) = x \u00b7 C\u00af\", the exponent \u03b7 remains relatively constant resulting in lines with approximately fixed slope (Figure 17). The scalar \u03bb that determines the y-intercept, however, shifts with different token multipliers. This suggests \u03bb is a function of the token multiplier, while \u03b7 is not.", "description": "This figure shows that the over-training regime follows a consistent power law, even with different token multipliers (the ratio of training tokens to model parameters).  The exponent of the power law remains constant, indicating that the slope of the lines in the log-log plots of reducible loss vs. compute is approximately fixed. However, the scalar (y-intercept) changes with the token multiplier, suggesting it is a function of the multiplier and not the exponent.", "section": "2 Developing scaling laws for over-training and downstream tasks"}, {"figure_path": "5tOVh81aze/figures/figures_25_1.jpg", "caption": "Figure 10: Out-of-distribution (OOD) settings. Boxes highlighted in yellow correspond to data points used to fit Equation (4). Recall that the C4 training set is English-filtered. Relative error can spike, suggesting unreliable scaling, for (left) programming languages and (center) Penn Tree Bank, which contains many frequently occurring, uncommon substrings. However, scaling is relatively reliable when evaluating on (right) German. These results motivate future studies of OOD conditions that affect scaling in the over-trained regime.", "description": "This figure shows the relative error of loss prediction on three different out-of-distribution (OOD) datasets compared to the in-distribution (ID) dataset.  The three OOD datasets are: 100 programming languages, Penn Tree Bank, and C4 German eval.  The ID dataset is C4 eval. The figure illustrates that while the scaling law works well on the ID dataset, it is less reliable on OOD datasets, especially those with characteristics such as frequent occurrences of uncommon substrings.", "section": "4 Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_25_2.jpg", "caption": "Figure 5: Relative error on C4 eval for different training distributions. Boxes highlighted in yellow correspond to pairs\u2014number of parameters N, token multiplier M\u2014used to fit Equation (4). Larger values of M correspond to more over-training. The prediction error is low in both interpolation and extrapolation ranges. Below N = 1.4B, empty squares correspond to runs that were not possible due to the limited dataset size for single epoch training. At N = 1.4B we run at M = 20 and at the largest possible multiplier. At N = 6.9B, we run at M = 20.", "description": "This figure shows the relative error in predicting the C4 evaluation loss for various model sizes (N) and training token multipliers (M).  The yellow boxes highlight the models used to create the scaling law.  The figure demonstrates that the scaling law provides accurate predictions across different datasets, even when extrapolating beyond the training data.  The lower accuracy for smaller models below 1.4B is attributed to limited data in those training runs.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_26_1.jpg", "caption": "Figure 5: Relative error on C4 eval for different training distributions. Boxes highlighted in yellow correspond to pairs\u2014number of parameters N, token multiplier M\u2014used to fit Equation (4). Larger values of M correspond to more over-training. The prediction error is low in both interpolation and extrapolation ranges. Below N = 1.4B, empty squares correspond to runs that were not possible due to the limited dataset size for single epoch training. At N = 1.4B we run at M = 20 and at the largest possible multiplier. At N = 6.9B, we run at M = 20.", "description": "This figure shows the relative error between predicted and actual validation loss on the C4 dataset for different model sizes and over-training levels.  The low error demonstrates the reliability of the proposed scaling laws across various interpolation and extrapolation ranges.  The empty squares highlight limitations due to dataset size.", "section": "4 Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_26_2.jpg", "caption": "Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6ND. Larger values of M specify more over-training. We are able to extrapolate, in both N and M, the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute. For this figure, we train all models on RedPajama [112].", "description": "This figure shows two plots. The left plot shows the scaling law for model validation loss, which is parameterized by token multiplier and compute.  The right plot shows the scaling law for average downstream top-1 error, which is a function of validation loss. Both plots demonstrate the reliability of scaling laws even when extrapolating to models with significantly more compute than those used to create the law, highlighting the ability to predict performance of large models using smaller-scale experiments.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_27_1.jpg", "caption": "Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6ND. Larger values of M specify more over-training. We are able to extrapolate, in both N and M, the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute. For this figure, we train all models on RedPajama [112].", "description": "This figure shows two scaling laws. The left panel shows a scaling law for validation loss that is parameterized by compute and the ratio of training tokens to model parameters (token multiplier). This scaling law can extrapolate to models with over 300x more compute than was used to fit it. The right panel shows a scaling law for predicting average downstream top-1 error as a function of validation loss, which benefits from using more expensive models. This scaling law can extrapolate to models with over 20x more compute than was used to fit it.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_27_2.jpg", "caption": "Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6ND. Larger values of M specify more over-training. We are able to extrapolate, in both N and M, the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute. For this figure, we train all models on RedPajama [112].", "description": "The figure shows two plots that demonstrate reliable scaling of language models with over-training. The left plot shows how a scaling law can extrapolate the validation loss of models with significantly more training compute than used to fit the scaling law. The right plot shows how another scaling law can extrapolate the average top-1 error on downstream tasks based on validation loss, achieving predictions with over 20x less compute. Both plots use RedPajama dataset for training.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_28_1.jpg", "caption": "Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6ND. Larger values of M specify more over-training. We are able to extrapolate, in both N and M, the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute. For this figure, we train all models on RedPajama [112].", "description": "This figure shows two scaling laws: one for validation loss and one for downstream error.  The left panel demonstrates the extrapolation of validation loss for significantly larger, over-trained models (300x more compute) than those used to create the scaling law, showing that over-training doesn't prevent reliable scaling. The right panel uses a different scaling law to predict downstream task performance from validation loss. This showcases the ability to extrapolate downstream task error for models trained with 20x more compute than used for the scaling law.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_28_2.jpg", "caption": "Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6ND. Larger values of M specify more over-training. We are able to extrapolate, in both N and M, the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute. For this figure, we train all models on RedPajama [112].", "description": "This figure shows two scaling laws. The left panel shows a scaling law for validation loss as a function of compute and token multiplier (overtraining).  It demonstrates extrapolation to models requiring 300x more compute than used for the scaling law fit. The right panel shows a scaling law that relates validation loss to downstream task performance (top-1 error), predicting performance of models with 20x more compute than the fit. Both scaling laws were tested using models trained on the RedPajama dataset.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/figures/figures_29_1.jpg", "caption": "Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6ND. Larger values of M specify more over-training. We are able to extrapolate, in both N and M, the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute. For this figure, we train all models on RedPajama [112].", "description": "This figure shows two plots. The left plot shows that validation loss scales reliably with both the number of parameters and the amount of over-training. The right plot shows that downstream error can be predicted from validation loss using a power law, and this prediction improves when using more expensive models for fitting the scaling law. Both plots demonstrate the reliability of scaling laws for extrapolating model performance to larger models and higher levels of over-training, significantly reducing the computational cost of experimentation.", "section": "4 Results: Reliable extrapolation"}]