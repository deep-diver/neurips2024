{"importance": "This paper is crucial for researchers in image-to-video generation because it identifies and solves a critical, previously overlooked issue: **conditional image leakage**. This problem, where models over-rely on the input image and produce videos with less motion than expected, is widely applicable. The solutions proposed\u2014an improved inference strategy and a novel time-dependent noise distribution\u2014are generalizable and significantly improve motion control. This work opens new avenues for enhancing the realism and dynamics of AI-generated videos. ", "summary": "Researchers solve the conditional image leakage problem in image-to-video diffusion models by proposing a new inference strategy and a time-dependent noise distribution for training. This yields videos with more natural motion.", "takeaways": ["Conditional image leakage causes limited motion in generated videos.", "Starting video generation from an earlier time step improves motion while minimizing the training-inference gap.", "A time-dependent noise distribution during training effectively reduces model dependency on the input image."], "tldr": "Image-to-video (I2V) generation using diffusion models has made significant strides. However, existing models often struggle to generate videos with sufficient motion, a phenomenon attributed to 'conditional image leakage'. This occurs because the model excessively relies on the static input image during the generative process, neglecting the dynamic information embedded within the noisy input at later time steps. This over-reliance on the input image leads to videos with noticeably less motion and less dynamic scenes. \nTo address this issue, the authors propose a two-pronged approach. Firstly, they modify the inference process by initiating video generation from an earlier time step. Secondly, they introduce a time-dependent noise distribution to the conditional image during training, progressively increasing noise at larger timesteps to reduce the model's dependence on it. This dual strategy is validated across various I2V diffusion models using benchmark datasets, resulting in videos with significantly improved motion scores, reduced errors, and maintained image alignment and temporal consistency.  The results demonstrate a superior overall performance, particularly in terms of motion accuracy and control.", "affiliation": "Tsinghua University", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "o9Lkiv1qpc/podcast.wav"}