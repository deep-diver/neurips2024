{"references": [{"fullname_first_author": "Sivaraman Balakrishnan", "paper_title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis", "publication_date": "2014", "reason": "This paper is foundational for the study of EM algorithm convergence, providing a framework and initial results that many subsequent works build upon, including this paper."}, {"fullname_first_author": "Constantinos Daskalakis", "paper_title": "Ten steps of EM suffice for mixtures of two Gaussians", "publication_date": "2017-07-10", "reason": "This paper establishes a significant milestone by showing that EM converges globally in a small number of steps for mixtures of two Gaussians, which is a foundational result that inspired much of this paper's research."}, {"fullname_first_author": "Ji Xu", "paper_title": "Global analysis of expectation maximization for mixtures of two Gaussians", "publication_date": "2016", "reason": "This paper provides a global convergence analysis for EM in a specific case (two Gaussians), which helps set the stage for this paper's exploration of the general case and overparameterized setting."}, {"fullname_first_author": "Raaz Dwivedi", "paper_title": "Theoretical guarantees for EM under misspecified Gaussian mixture models", "publication_date": "2018", "reason": "This paper extends the convergence analysis to the misspecified setting of Gaussian mixtures, which is closely related to the overparameterized setting investigated in this paper."}, {"fullname_first_author": "Jeongyeol Kwon", "paper_title": "The EM algorithm gives sample-optimality for learning mixtures of well-separated Gaussians", "publication_date": "2020", "reason": "This paper demonstrates sample-optimality of EM under certain conditions, providing a stronger convergence guarantee than many previous results and influencing this paper's analysis."}]}