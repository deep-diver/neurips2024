[{"figure_path": "Ku31aRq3sW/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of CODA: We create fictitious transitions from goal examples to terminal states under the given context in the action-augmented MDP with reward 1, which enables the supervised signal to propagate back to unsupervised transitions via Bellman equation.", "description": "This figure illustrates the core idea of the CODA method.  CODA augments the original Markov Decision Process (MDP) by adding fictitious transitions. These transitions originate from goal examples in the context-goal dataset and lead to a terminal state, with a reward of 1.  The fictitious actions are associated with the given context.  This augmentation allows the supervised learning signal from the context-goal pairs to propagate through the Bellman equation to the unlabeled transitions from the dynamics dataset, effectively creating a fully labeled dataset for offline reinforcement learning.", "section": "4 Contextual Goal-Oriented Data Augmentation (CODA)"}, {"figure_path": "Ku31aRq3sW/figures/figures_6_1.jpg", "caption": "Figure 2: Illustration of the context-goal relationship with increasing complexity (Each red boundary defines a goal set with its center location as context). (a) Contexts and goal sets are very similar such that it could be approximately solved by a context-agnostic policy. (b) Contexts are finite, and different contexts map to distinct goal sets, which requires context-dependent policies. (c) Contexts are continuous and infinite. The context-goal mapping is neither one-to-many nor many-to-one, creating a CGO problem with full complexity.", "description": "This figure illustrates three levels of complexity in the context-goal relationship in contextual goal-oriented (CGO) problems. (a) shows a simple scenario where contexts and goals are very similar, making it solvable by a context-agnostic policy. (b) demonstrates a scenario with finite contexts, each mapping to distinct goal sets, necessitating context-dependent policies. Finally, (c) depicts the most complex scenario with continuous and infinite contexts and a non-trivial mapping between contexts and goals, showcasing the full complexity of CGO problems.", "section": "Diverse spectrum of practical CGO problems"}, {"figure_path": "Ku31aRq3sW/figures/figures_23_1.jpg", "caption": "Figure 3: Reward model evaluation for the large-diverse dataset for original AntMaze environment. Green dots are outliers.", "description": "This figure shows the reward model evaluation for the large-diverse dataset in the original AntMaze environment.  The box plots display the distribution of predicted rewards for both positive (context-goal pairs) and negative (context-non-goal pairs) samples.  Green dots represent outliers, which are data points that fall significantly outside the typical range of values in the dataset. The figure visually compares the performance of three reward learning models: PDS, UDS+RP, and RP in distinguishing positive from negative samples.", "section": "B.3 Reward Model Evaluation"}, {"figure_path": "Ku31aRq3sW/figures/figures_23_2.jpg", "caption": "Figure 3: Reward model evaluation for the large-diverse dataset for original AntMaze environment. Green dots are outliers.", "description": "The figure shows box plots visualizing the reward distributions obtained from evaluating the reward models (PDS, UDS+RP, and RP) on both positive and negative datasets within the large-diverse environment of original AntMaze. It demonstrates that PDS performs better at separating positive and negative samples than the other two models, although all three struggle to fully separate the distributions.", "section": "B.3 Reward Model Evaluation"}, {"figure_path": "Ku31aRq3sW/figures/figures_23_3.jpg", "caption": "Figure 3: Reward model evaluation for the large-diverse dataset for original AntMaze environment. Green dots are outliers.", "description": "The box plot shows the distribution of predicted rewards from three different reward models (PDS, UDS+RP, and RP) on positive and negative samples for the large-diverse dataset in the original AntMaze environment.  The positive samples are context-goal pairs, and negative samples are randomly selected from the context and non-goal states in the dataset. The plots illustrate the performance of each reward model in separating positive and negative samples. Green dots indicate outliers.", "section": "B.3 Reward Model Evaluation"}, {"figure_path": "Ku31aRq3sW/figures/figures_24_1.jpg", "caption": "Figure 2: Illustration of the context-goal relationship with increasing complexity (Each red boundary defines a goal set with its center location as context). (a) Contexts and goal sets are very similar such that it could be approximately solved by a context-agnostic policy. (b) Contexts are finite, and different contexts map to distinct goal sets, which requires context-dependent policies. (c) Contexts are continuous and infinite. The context-goal mapping is neither one-to-many nor many-to-one, creating a CGO problem with full complexity.", "description": "This figure illustrates three levels of context-goal relationships with increasing complexity.  The first shows very similar contexts and goal sets, meaning a context-agnostic policy may suffice. The second shows finite contexts mapping to distinct goal sets, requiring context-dependent policies. The third depicts continuous and infinite contexts with a complex, neither one-to-many nor many-to-one, context-goal mapping, representing the full complexity of the CGO problem.", "section": "6 Experiments"}, {"figure_path": "Ku31aRq3sW/figures/figures_24_2.jpg", "caption": "Figure 3: Reward model evaluation for the large-diverse dataset for original AntMaze environment. Green dots are outliers.", "description": "This figure shows the box plots of reward model evaluation for the large-diverse dataset in the original AntMaze environment. The box plots show the distribution of the predicted reward for both positive and negative datasets. Green dots represent outliers.", "section": "B.3 Reward Model Evaluation"}]