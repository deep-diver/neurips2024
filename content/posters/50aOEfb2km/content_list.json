[{"type": "text", "text": "Exploring Low-Dimensional Subspaces in Diffusion Models for Controllable Image Editing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Siyi Chen1\u2217 Huijie Zhang1\u2217 Minzhe Guo1 Yifu Lu1 Peng Wang1 Qing Qu1 ", "page_idx": 0}, {"type": "text", "text": "1University of Michigan {siyich,huijiezh,vincegmz,yifulu,pengwa,qingqu}@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, diffusion models have emerged as a powerful class of generative models. Despite their success, there is still limited understanding of their semantic spaces. This makes it challenging to achieve precise and disentangled image generation without additional training, especially in an unsupervised way. In this work, we improve the understanding of their semantic spaces from intriguing observations: among a certain range of noise levels, (1) the learned posterior mean predictor (PMP) in the diffusion model is locally linear, and (2) the singular vectors of its Jacobian lie in low-dimensional semantic subspaces. We provide a solid theoretical basis to justify the linearity and low-rankness in the PMP. These insights allow us to propose an unsupervised, single-step, training-free LOw-rank COntrollable image editing (LOCO Edit) method for precise local editing in diffusion models. LOCO Edit identified editing directions with nice properties: homogeneity, transferability, composability, and linearity. These properties of LOCO Edit benefit greatly from the low-dimensional semantic subspace. Our method can further be extended to unsupervised or text-supervised editing in various text-to-image diffusion models (T-LOCO Edit). Finally, extensive empirical experiments demonstrate the effectiveness and efficiency of LOCO Edit. The code and the arXiv version can be found on the project website.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, diffusion models have emerged as a powerful new family of deep generative models with remarkable performance in many applications such as image generation across various domains [1, 2, 3, 4, 5, 6], audio synthesis [7, 8], solving inverse problem [9, 10, 11, 12, 13, 14], and video generation [15, 16, 17]. For example, recent advances in AI-based image generation, revolutionized by diffusion models such as Dalle-2 [18], Imagen [19], and stable diffusion [4], have taken the world of \u201cAI Art generation\u201d, enabling the generation of images directly from descriptive text inputs. These models corrupt images by adding noise through multiple steps of forward process and then generate samples by progressive denoising through multiple steps of the reverse generative process. ", "page_idx": 0}, {"type": "text", "text": "Although modern diffusion models are capable of generating photorealistic images from text prompts, manipulating the generated content by diffusion models in practice has remaining challenges. Unlike generative adversarial networks [20], the understanding of semantic spaces in diffusion models is still limited. Thus, achieving disentangled and localized control over content generation by direct manipulation of the semantic spaces remains a difficult task for diffusion models. Although effective, some existing editing methods in diffusion models often demand additional training procedures and are limited to global control of content generation [21, 22, 23]. Some methods are trainingfree or localized but are still based upon heuristics, lacking clear mathematical interpretations, or for text-supervised editing only [24, 25, 26, 27, 28]. Others provide analysis in diffusion models [29, 30, 31, 32, 33], but also have difficulty in local edits such as hair color. ", "page_idx": 0}, {"type": "image", "img_path": "50aOEfb2km/tmp/72762a334d110abe4b480473acd2e582d13685e65a6c2fd040e2263022411bc0.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: LOCO Edit. (a) The proposed method can perform precise localized editing in the region of interest. The editing direction is (b) homogeneous, (c) composable, and (d) linear. ", "page_idx": 1}, {"type": "text", "text": "In this study, we address the above problem by studying the low-rank semantic subspaces in diffusion models and proposing the LOw-rank COntrollable edit (LOCO Edit) approach. LOCO is the first local editing method that is single-step, training-free, requiring no text supervision, and having other intriguing properties (see Figure 1 for an illustration). Our method is highly intuitive and theoretically grounded, originating from a simple while intriguing observation in the learned posterior mean predictor (PMP) in diffusion models: for a large portion of denoising time steps, ", "page_idx": 1}, {"type": "text", "text": "The PMP is a locally linear mapping between the noise image and the estimated clean image, and the singular vectors of its Jacobian reside within low-dimensional subspaces. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The empirical evidence in Figure 2 consistently shows that this phenomenon occurs when training diffusion models using different network architectures on a range of real-world image datasets. Theoretically, we validated this observation by assuming a mixture of low-rank Gaussian distributions for the data. We then prove the local linearity of the PMP, the low-rank nature of its Jacobian, and that the singular vectors of the Jacobian span the low-dimensional subspaces. ", "page_idx": 1}, {"type": "text", "text": "By utilizing the linearity of the PMP, we can edit within the singular vector subspace of its Jacobian to achieve linear control of the image content with no label or text supervision. The editing direction can be efficiently computed using the generalized power method (GPM) [30, 34]. Furthermore, we can manipulate specific regions of interest in the image along a disentangled direction through efficient nullspace projection, taking advantage of the low-rank properties of the Jacobian. ", "page_idx": 1}, {"type": "text", "text": "Benefits of LOCO Edit. Compared to existing editing methods (e.g., [29, 35, 23, 24]) based on diffusion models, the proposed LOCO Edit offers several benefits that we highlight below: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Precise, single-step, training-free, and unsupervised editing. LOCO enables precise localized editing (Figure 1a) in a single timestep without any training. Further, it requires no text supervision based on CLIP [36], thus integrating no intrinsic biases or flaws from CLIP [37]. LOCO is applicable to various diffusion models and datasets (Figure 5).   \n\u2022 Linear, transferable, and composable editing directions. The identified editing direction is linear, meaning that changes along this direction produce proportional changes in a semantic feature in the image space (Figure 1d). These editing directions are homogeneous and can be transferred across various images and noise levels (Figure 1b). Moreover, combining disentangled editing directions leads to simultaneous semantic changes in the respective region, while maintaining consistency in other areas (Figure 1c).   \n\u2022 An intuitive and theoretically grounded approach. Unlike previous works, by leveraging the local linearity of the PMP and the low-rankness of its Jacobian, our method is highly interpretable. The identified properties are well supported by both our empirical observation (Figure 2) and theoretical justifications in Section 4. ", "page_idx": 1}, {"type": "text", "text": "Moreover, LOCO Edit is generalizable to T-LOCO Edit for T2I diffusion models including DeepFloyd IF [19], Stable Diffusion [4], and Latent Consistency Models [38], with or without text supervision (Figure 4). A more detailed discussion on the relationship with prior arts can be found in Appendix B. ", "page_idx": 1}, {"type": "text", "text": "Notations. Throughout the paper, we use $\\mathcal{X}_{t}\\subseteq\\mathbb{R}^{d}$ to denote the noise-corrupted image space at the time-step $t\\in[0,1]$ . In particular, $\\scriptstyle{\\mathcal{X}}_{0}$ denotes the clean image space with distribution $p_{\\mathrm{data}}(x)$ , and $\\pmb{x}_{0}\\in\\mathcal{X}_{0}$ denote an image. $\\chi_{0,t}$ denote the posterior mean space at time-step $t\\in(0,1]$ . Here, $\\mathbb{S}^{d-1}$ denotes a unit hypersphere in $\\mathbb{R}^{d}$ , and $\\operatorname{St}(d,r):=\\{Z\\in\\mathbb{R}^{d\\times r}\\mid Z^{\\top}Z=I_{r}\\}$ denotes the Stiefel manifold. $\\widetilde{\\mathtt{r a n k}}(A)$ denotes the numerical rank of $\\pmb{A}$ . $\\mathbb{E}_{\\mathbf{x}_{0}\\sim p_{\\mathrm{data}}(\\mathbf{x})}[\\mathbf{x}_{0}|\\mathbf{x}_{t}]$ denotes the posterior mean and is written as $\\mathbb{E}[{\\pmb x}_{0}|{\\pmb x}_{t}]$ . range(A) denotes the span of the columns of $A.\\mathrm{\\null}(A)$ denotes the set of solutions to $\\mathbf{A}\\mathbf{x}=0$ . $\\operatorname{oroj}_{\\operatorname{null}A}(x)$ denotes the projection of $\\textbf{\\em x}$ onto $\\mathrm{null}(A)$ . ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries on Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we start by reviewing the basics of diffusion models [1, 2, 39], followed by several key techniques that will be used in our approach, such as Denoising Diffusion Implicit Models (DDIM) [3] and its inversion [40], T2I diffusion model, and classifier-free guidance [41]. ", "page_idx": 2}, {"type": "text", "text": "Basics of Diffusion Models. In general, diffusion models consist of two processes: ", "page_idx": 2}, {"type": "text", "text": "\u2022 The forward diffusion process. The forward process progressively perturbs the original data $\\pmb{x}_{0}$ to a noisy sample $\\pmb{x}_{t}$ for $t\\in[0,1]$ with the Gaussian noise. As in [1], this can be characterized by a conditional Gaussian distribution $p_{t}(\\mathbf{\\boldsymbol{x}}_{t}|\\mathbf{\\boldsymbol{x}}_{0})=\\mathcal{N}(\\mathbf{\\boldsymbol{x}}_{t};\\sqrt{\\alpha_{t}}\\mathbf{\\boldsymbol{x}}_{0},(1\\!-\\!\\alpha_{t})\\mathbf{I}_{d})$ . Particularly, parameters $\\{\\alpha_{t}\\}_{t=0}^{1}$ sastify: (i) $\\alpha_{0}=1$ , and thus $p_{0}=p_{\\mathrm{data}}$ , and (ii) $\\alpha_{1}=0$ , and thus $p_{1}=\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{d})$ . \u2022 The reverse sampling process. To generate a new sample, previous works [1, 3, 42, 43] have proposed various methods to approximate the reverse process of diffusion models. Typically, these methods involve estimating the noise $\\epsilon_{t}$ and removing the estimated noise from $\\pmb{x}_{t}$ recursively to obtain an estimate of $\\scriptstyle x_{0}$ . Specifically, the sampling step from $\\pmb{x}_{t}$ to $x_{t-\\Delta t}$ with a small $\\Delta t>0$ can be described as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\pmb x}_{t-\\Delta t}=\\sqrt{\\alpha_{t-\\Delta t}}\\left(\\frac{{\\pmb x}_{t}-\\sqrt{1-\\alpha_{t}}{\\pmb\\epsilon}_{\\theta}({\\pmb x}_{t},t)}{\\sqrt{\\alpha_{t}}}\\right)+\\sqrt{1-\\alpha_{t-\\Delta t}}\\epsilon_{\\theta}({\\pmb x}_{t},t),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\epsilon_{\\theta}(\\boldsymbol{x}_{t},t)$ is parameterized by a neural network and trained to predict the noise at time $t$ . ", "page_idx": 2}, {"type": "text", "text": "Denoiser and Posterior Mean Predictor (PMP). According to [1], the denoiser $\\epsilon_{\\theta}(\\boldsymbol{x}_{t},t)$ is optimized by solving the following problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\ell(\\theta):=\\mathbb{E}_{t\\sim[0,1],x_{t}\\sim p_{t}(x_{t}|x_{0}),\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})}\\left[\\|\\epsilon_{\\theta}(x_{t},t)-\\epsilon\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{\\theta}$ denotes the network parameters of the denoiser. Once $\\epsilon_{\\theta}$ is well trained, recent studies [44, 45] show that the posterior mean $\\mathbb{E}[{\\pmb x}_{0}|{\\pmb x}_{t}]$ , i.e., predicted clean image at time $t$ , can be estimated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\pmb{x}}_{0,t}=\\pmb{f}_{\\pmb{\\theta},t}(\\pmb{x}_{t};t):=\\frac{\\pmb{x}_{t}-\\sqrt{1-\\alpha_{t}}\\pmb{\\epsilon}_{\\pmb{\\theta}}(\\pmb{x}_{t},t)}{\\sqrt{\\alpha_{t}}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\pmb{f}_{\\pmb{\\theta},t}(\\pmb{x}_{t};t)$ denotes the posterior mean predictor (PMP) [45, 44], and $\\hat{\\pmb{x}}_{0,t}\\in\\mathcal{X}_{0,t}$ denotes the estimated posterior mean output from PMP given $\\pmb{x}_{t}$ and $t$ as the input. For simplicity, we denote $\\boldsymbol{f}_{\\theta,t}(\\boldsymbol{x}_{t};t)$ as ${\\pmb f}_{\\pmb\\theta,t}({\\pmb x}_{t})$ . ", "page_idx": 2}, {"type": "text", "text": "DDIM and DDIM Inversion. Given a noisy sample $\\pmb{x}_{t}$ at time $t$ , DDIM [3] can generate clean images by multiple denoising steps. Given a clean sample $\\pmb{x}_{0}$ , DDIM inversion [3] can generate a noisy $\\pmb{x}_{t}$ at time $t$ by adding multiple steps of noise following the reversed trajectory of DDIM. DDIM inversion has been widely in image editing methods [40, 46, 29, 35, 47, 26] to obtain $\\pmb{x}_{t}$ given the original $\\scriptstyle{x_{0}}$ and then performing editing starting from $\\pmb{x}_{t}$ . In our work, after getting $\\pmb{x}_{t}$ given $\\scriptstyle x_{0}$ via DDIM inversion, we edit $\\pmb{x}_{t}$ to $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ only at the single time step $t$ with the help of PMP, and then utilize DDIM to generate the edited image $\\pmb{x}_{0}^{\\prime}$ . ", "page_idx": 2}, {"type": "text", "text": "For ease of exposition, for any $t_{1}$ and $t_{2}$ with $t_{2}>t_{1}$ , we denote DDIM operator and its inversion as $\\pmb{x}_{t_{1}}=\\tt D D I M(\\pmb{x}_{t_{2}},t_{1})$ and $\\pmb{x}_{t_{2}}=\\tt D D I M-I n v(\\pmb{x}_{t_{1}},t_{2})$ . ", "page_idx": 2}, {"type": "text", "text": "Text-to-image (T2I) Diffusion Models $\\pmb{\\&}$ Classifier-Free Guidance. So far, our discussion has only focused on unconditional diffusion models. Moreover, our approach can be generalized from unconditional diffusion models to T2I diffusion models [38, 4, 48, 19], where the latter enables controllable image generation $\\scriptstyle{x_{0}}$ guided by a text prompt $c$ . In more detail, when training T2I diffusion models, we optimize a conditional denoising function $\\epsilon_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t},t,\\boldsymbol{c})$ . For sampling, we ", "page_idx": 2}, {"type": "image", "img_path": "50aOEfb2km/tmp/6010f2af421151173121ef0bbc4bd6cb01ad4bcaee9f583a4538f156a2cc171b.jpg", "img_caption": ["(a) Low-rankness of the Jacobian ", "(b) Local linearity of PMP "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Low-rankness of the Jacobian $J_{\\theta,t}(x_{t})$ and Local linearity of the PMP ${\\pmb f}_{\\pmb\\theta,t}({\\pmb x}_{t})$ . We evaluated DDPM (U-Net [49]) on CIFAR-10 dataset [50], U-ViT [51] (Transformer) on CelebA [52], ImageNet [53] datasets and DeepFloy IF [19] trained on LAION-5B [54] dataset. (a) The rank ratio of $\\bar{J}_{\\theta,t}({\\bf x}_{t})$ against timestep $t$ . (b) The norm ratio (Top) and cosine similarity (Bottom) between ${\\pmb f}_{\\pmb\\theta,t}({\\pmb x}_{t}+\\lambda\\Delta{\\pmb x})$ and $l_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t};\\lambda\\Delta\\mathbf{\\boldsymbol{x}})$ against step size $\\lambda$ at timestep $t=0.7$ . ", "page_idx": 3}, {"type": "text", "text": "employ a technique called classifier-free guidance [41], which substitutes the unconditional denoiser $\\epsilon_{\\theta}(\\boldsymbol{x}_{t},t)$ in Equation (1) with its conditional counterpart $\\tilde{\\epsilon}_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t},t,\\boldsymbol{c})$ that can be described as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{\\epsilon}_{\\theta}(x_{t},t,c)=\\epsilon_{\\theta}(x_{t},t,\\emptyset)+\\eta(\\epsilon_{\\theta}(x_{t},t,c)-\\epsilon_{\\theta}(x_{t},t,\\emptyset)).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\mathcal{Q}$ denotes the empty prompt and $\\eta>0$ denotes the strength for the classifier-free guidance. ", "page_idx": 3}, {"type": "text", "text": "3 Exploring Linearity & Low-Dimensionality for Image Editting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we formally introduce the identified low-rank subspace in diffusion models and the proposed LOCO Edit method with the underlying intuitions. In Section 3.1, we present the benign properties in PMP that our method utilizes. Followed by this, in Section 3.3 we provide a detailed description of our method. ", "page_idx": 3}, {"type": "text", "text": "3.1 Local Linearity and Intrinsic Low-Dimensionality in PMP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "First, let us delve into the key intuitions behind the proposed LOCO Edit method, which lie in the benign properties of the PMP $f_{{\\pmb\\theta},t}({\\pmb x}_{t})$ . At one given timestep $t\\in[0,1]$ , let us consider the first-order Taylor expansion of ${\\pmb f}_{\\pmb\\theta,t}({\\pmb x}_{t}+\\lambda\\Delta{\\pmb x})$ at the point $\\pmb{x}_{t}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boxed{\\begin{array}{r l}{\\mathbf{\\chi}_{\\mathbf{\\mathcal{\\theta}}}(\\mathbf{{x}}_{t};\\lambda\\Delta\\mathbf{x})\\;:=\\;f_{\\mathbf{{\\theta}},t}(\\mathbf{x}_{t})+\\lambda J_{\\mathbf{{\\theta}},t}(\\mathbf{x}_{t})\\cdot\\Delta\\mathbf{x},}\\end{array}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Delta\\pmb{x}\\in\\mathbb{S}^{d-1}$ is a perturbation direction with unit length, $\\lambda\\in\\mathbb R$ is the perturbation strength, and $J_{\\theta,t}(\\mathbf{x}_{t})=\\nabla_{x_{t}}f_{\\theta,t}(x_{t})$ is the Jacobian of ${\\pmb f}_{\\pmb\\theta,t}({\\pmb x}_{t})$ . Interestingly, we discovered that within a certain range of noise levels, the learned PMP ${{f}_{\\theta,t}}$ exhibits local linearity, and the singular subspace of its Jacobian $J_{\\theta,t}$ is low rank. Notably, these properties are universal across various network architectures (e.g., UNet and Transformers) and datasets. ", "page_idx": 3}, {"type": "text", "text": "We measure the low-rankness with rank ratio and the local linearity with norm ratio and cosine similarity. Specifically, (i) rank ratio is the ratio of $\\widetilde{\\mathtt{r a n k}}(J_{\\theta,t}(\\mathbf{x}_{t}))$ and the ambient dimension $d$ ; (ii) norm ratio is the ratio of $\\|{\\pmb{f}}_{\\pmb{\\theta},t}({\\pmb x}_{t}+\\lambda\\Delta{\\pmb x})\\|_{2}$ and $\\|l_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t};\\lambda\\Delta\\mathbf{\\boldsymbol{x}})\\|_{2}$ ; $(i i i)$ cosine similarity is between ${\\pmb f}_{\\pmb\\theta,t}({\\pmb x}_{t}+\\lambda\\Delta{\\pmb x})$ and $l_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t};\\lambda\\Delta\\mathbf{\\boldsymbol{x}})$ . The detailed experiment settings are provided in Appendix D.1, and results are illustrated in Figure 2, from which we observe: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Low-rankness of the Jacobian $J_{\\theta,t}(x_{t})$ . As shown in Figure 2(a), the rank ratio for $t\\in[0,1]$ consistently displays a U-shaped pattern across various network architectures and datasets: $(i)$ it is close to 1 near either the pure noise $t=1$ or the clean image $t=0$ , (ii) $J_{\\theta,t}(x_{t})$ is low-rank (i.e., rank ratio less than $10^{-1}$ ) for all diffusion models within the range $t\\in[0.2,0.7]$ , $(i i i)$ it achieves the lowest value around mid-to-late timestep, slightly differs depending on architecture and dataset. \u2022 Local linearity of the PMP ${\\pmb f}_{\\pmb\\theta,t}({\\pmb x}_{t})$ . Moreover, the mapping ${\\pmb f}_{\\pmb\\theta,t}({\\pmb x}_{t})$ exhibits strong linearity across a large portion of the timesteps; see Figure 2(b) and Figure 10. Specifically, in Figure 2(b), we evaluate the linearity of ${\\bf\\nabla}f_{\\pmb{\\theta},t}({\\bf x}_{t})$ at $t=0.7$ where the rank ratio is close to the lowest value. We can see that $\\pmb{f}_{\\pmb{\\theta},t}(\\pmb{x}_{t}+\\lambda\\Delta\\pmb{x})\\approx l_{\\pmb{\\theta}}(\\pmb{x}_{t};\\lambda\\Delta\\pmb{x})$ even when $\\lambda=40$ , which is consistently true among different architectures trained on different datasets. ", "page_idx": 3}, {"type": "text", "text": "In addition to comprehensive experimental studies, we will also demonstrate in Section 4 that both properties can be theoretically justified. ", "page_idx": 4}, {"type": "text", "text": "3.2 Key Intuitions for Our Image Editing Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The two benign properties offer valuable insights for image editing with precise control. Here, we first present the high-level intuitions behind our method, with further details postponed to Section 3.3. Specifically, for any given time-step $t\\in[0,1]$ , let us denote the compact singular value decomposition (SVD) of the Jacobian $J_{\\theta,t}(x_{t})$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nJ_{\\theta,t}(\\boldsymbol{x}_{t})\\ =\\ U\\Sigma V^{\\top}\\ =\\ \\sum_{i=1}^{r}\\sigma_{i}\\mathbf{u}_{i}\\boldsymbol{v}_{i}^{\\top},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $r$ is the rank of $J_{\\theta,t}(x_{t}),U=[u_{1}\\quad\\cdots\\quad u_{r}]\\in\\operatorname{St}(d,r)$ and $V=[{\\boldsymbol{v}}_{1}\\quad\\cdot\\cdot\\cdot\\quad{\\boldsymbol{v}}_{r}]\\in\\operatorname{St}(d,r)$ denote the left and right singular vectors, and $\\Sigma=\\operatorname{diag}(\\sigma_{1},\\cdot\\cdot\\cdot\\,,\\sigma_{r})$ denote the singular values. We write $J_{\\theta,t}(x_{t})\\;=\\;J_{\\theta,t}$ in short for a specific $\\pmb{x}_{t}$ , and denote $\\mathrm{range}(J_{\\theta,t}^{\\top})\\;=\\;\\mathrm{span}(V)$ and $\\mathrm{null}(J_{\\theta,t})=\\left\\{w\\mid J_{\\theta,t}w=0\\right\\}$ . ", "page_idx": 4}, {"type": "text", "text": "\u2022 Local linearity of PMP for one-step, training-free, and supervision-free editing. Given the PMP $f_{\\pmb{\\theta},t}(\\pmb{x}_{t})$ is locally linear at the $t$ -th timestep, if we perturb $\\pmb{x}_{t}$ by $\\Delta{\\pmb x}=\\lambda{\\pmb v}_{i}$ , using one right singular vector $\\pmb{v}_{i}$ of $J_{\\theta,t}(x_{t})$ as an example editing direction, then by orthogonality ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\theta,t}(x_{t}+\\lambda v_{i})\\ \\approx\\ f_{\\theta,t}(x_{t})+J_{\\theta,t}(x_{t})v_{i}\\ =\\ f_{\\theta,t}(x_{t})+\\lambda\\sigma_{i}u_{i}\\ =\\ \\hat{x}_{0,t}+\\rho_{i}u_{i}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This implies we can achieve one-step editing along the semantic direction $\\pmb{u}_{i}$ . Notably, the method is training-free and supervision-free since $\\pmb{v}_{i}$ can be simply found via the SVD of $J_{\\theta,t}(x_{t})$ . ", "page_idx": 4}, {"type": "text", "text": "\u2022 Local linearity of PMP for linear, homogeneous, and composable image editing. $(i)$ First, the editing direction $\\pmb{v}=\\pmb{v}_{i}$ is linear, where any linear $\\lambda\\in\\mathbb R$ change along $\\pmb{v}_{i}$ results in a linear change $\\rho_{i}\\,=\\,\\lambda\\sigma_{i}$ along $\\pmb{u}_{i}$ for the edited image. $(i i)$ Second, the editing direction $\\pmb{v}=\\pmb{v}_{i}$ is homogeneous due to its independence of $\\hat{\\pmb{x}}_{0,t}$ , where it could be applied on any images from the same data distribution and results in the same semantic editing. $(i i i)$ Third, editing directions are composable. Any linearly combined editing direction $\\begin{array}{r}{\\pmb{v}=\\sum_{i\\in\\mathcal{T}}\\lambda_{i}\\pmb{v}_{i}\\in\\mathrm{range}\\left(\\pmb{J}_{\\theta,t}^{\\top}\\right)}\\end{array}$ is a valid editing direction which would result in a composable change $\\sum_{i\\in\\mathbb{Z}}\\rho_{i}\\pmb{u}_{i}$ in the edited image. On the contrary, $\\pmb{w}\\in\\mathrm{null}\\left(\\pmb{J}_{\\pmb{\\theta},t}\\right)$ results in no editing since $f_{\\theta,t}(\\pmb{x}_{t}+\\lambda\\pmb{w})\\approx f_{\\theta,t}(\\pmb{x}_{t})$ . ", "page_idx": 4}, {"type": "text", "text": "\u2022 Low-rankness of Jacobian for localized and efficient editing. $J_{\\theta,t}(x_{t})$ is for the entire predicted clean image, thus $J_{\\theta,t}(x_{t})$ finds editing directions in the entire image. Denote $\\tilde{J}_{\\theta,t}$ the Jacobian only for a certain region of interest (ROI), and ${\\bar{J}}_{\\theta,t}$ the Jacobian for regions outside ROI. Similarly, $\\pmb{v}\\in\\mathrm{range}\\left(\\tilde{J}_{\\theta,t}^{\\top}\\right)$ can edit mainly regions within the ROI, and null $\\left(\\bar{J}_{\\theta,t}^{\\top}\\right)$ contain directions that do not edit regions outside of ROI. Further projection of $\\pmb{v}$ onto $\\mathrm{null}\\left(\\bar{\\pmb{J}}_{\\theta,t}^{\\top}\\right)$ can result in a more localized editing direction for ROI. To perform such nullspace projection, computing the full SVD can be very expensive. But we can highly reduce the computation by the low-rank estimation of Jacobians with rank $r^{\\prime}\\ll d$ . The estimation is efficient yet effective with $t\\in[0.5,0.7]$ when the rank of the Jacobian achieves the lowest value. ", "page_idx": 4}, {"type": "text", "text": "3.3 Low-rank Controllable Image Editing Method with Nullspace Projection ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we provide a detailed introduction to LOCO Edit, expanding on the discussion in Section 3.1. We first introduce the supervision-free LOCO Edit, where we further enable localized image editing through nullspace projection with masks. Second, we generalize to T-LOCO Edit for T2I diffusion models w/wo text-supervision to define the semantic editing directions. ", "page_idx": 4}, {"type": "text", "text": "LOCO Edit. We first introduce the general pipeline of LOCO Edit. As illustrated in Figure 3, given an original image $\\pmb{x}_{0}$ , we first use ${\\pmb x}_{t}=\\tt D D I M-I n v}({\\pmb x}_{0},t)$ to generate a noisy image $\\pmb{x}_{t}$ . In particular, we choose $t\\in[0.5,0.7]$ so that the PMP ${\\pmb f}_{\\pmb\\theta,t}({\\pmb x}_{t})$ is locally linear and its Jacobian $J_{\\theta,t}(x_{t})$ is close to its lowest rank. From Section 3.1, we know that we can edit the image by changing $\\pmb{x}_{t}^{\\prime}=\\pmb{x}_{t}+\\lambda\\pmb{v}_{p}$ , where $\\pmb{v_{p}}$ is the identified editing direction. After editing $\\pmb{x}_{t}$ to $\\ensuremath{\\boldsymbol{{x}}}_{t}^{\\prime}$ , we use $\\mathbf{x}_{0}^{\\prime}=\\mathsf{D D I M}\\left(\\mathbf{x}_{t}^{\\prime},0\\right)$ to generate the edited image. ", "page_idx": 4}, {"type": "text", "text": "In many practical applications, we often need to edit only specific local regions of an image while leaving the rest unchanged. As discussed in Section 3.2, we can achieve this task by finding a precise local editing direction with localized Jacobians and nullspace projection. Overall, the complete method is in Algorithm 1. We describe the key details as follows. ", "page_idx": 4}, {"type": "image", "img_path": "50aOEfb2km/tmp/6cee4d469a8b215290aa0e10b78846056febd6baaf5ceb2f6ad4544c7e69b42b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Illustration of the unsupervised LOCO Edit for unconditional diffusion models. Given an image $\\pmb{x}_{0}$ , we perform DDIM-Inv until time $t$ to get $\\pmb{x}_{t}$ , and estimate $\\hat{x}_{0,t}$ from $\\pmb{x}_{t}$ . After masking to get the region of interest (ROI) $\\tilde{x}_{0,t}$ and its counterparts $\\bar{x}_{0,t}$ , we find the edit direction $\\pmb{v_{p}}$ via SVD and nullspace projection based on their Jacobians (Algorithm 1). By denoising $\\pmb{x}_{t}+\\lambda\\pmb{v}_{p}$ , an image $\\pmb{x}_{0}^{\\prime}$ with localized editing is generated. In this paper, the variables and notions related to $R O I$ nullspace, and final direction are respectively highlighted by green, blue, and red colors. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Unsupervised LOCO Edit ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Input: original image $\\mathbf{\\nabla}_{x_{0}}$ , the mask $\\Omega$ , pretrained diffusion model $\\epsilon_{\\theta}$ , editing strength $\\lambda$ , semantic index $k$ ,   \nnumber of semantic directions $r$ , editing timestep $t\\in[0.5,0.7]$ , the rank $r^{\\prime}=5$ .   \n2: Output: edited image $\\ensuremath{\\mathbf{{x}}}_{0}^{\\prime}$ ,   \n3: Generate $\\pmb{x}_{t}\\gets\\tt D D I M-I n v(\\pmb{x}_{0},t)$ $\\triangleright$ noisy image at $t$ -th timestep   \n4: Compute the top- $^{r}$ SVD $(\\tilde{U},\\tilde{\\Sigma},\\tilde{V})$ of $\\tilde{J}_{\\theta,t}=\\nabla_{x_{t}}P_{\\Omega}(\\mathbf{f}_{\\theta,t}(\\mathbf{x}_{t}))$   \n5: Compute the top- $\\cdot r^{\\prime}$ SVD $(\\bar{U},\\bar{\\Sigma},\\bar{V})$ of $\\bar{J}_{\\theta,t}=\\nabla_{\\mathbf{x}_{t}}P_{\\Omega}c\\left(\\mathbf{f}_{\\theta,t}(\\mathbf{x}_{t})\\right)$   \n6: Pick direction $v\\leftarrow\\tilde{V}[:,i]$ \u25b7 1 Pick the $k^{t h}$ singular vector for the editing direction   \n7: Compute $\\boldsymbol{v}_{p}\\gets(\\boldsymbol{I}-\\bar{V}\\bar{V}^{\\top})\\cdot\\boldsymbol{v}$ \u25b7 2 Nullspace projection for editing within the mask $\\Omega$   \n8: $v_{p}\\leftarrow v_{p}/\\|v_{p}\\|_{2}$ \u25b7Normalize the editing direction   \n9: Return: $\\pmb{x}_{0}^{\\prime}\\gets\\tt D D I M(\\pmb{x}_{t}+\\lambda\\pmb{v}_{p},0)$ \u25b7Editing with forward DDIM along the direction $\\pmb{v}_{p}$ ", "page_idx": 5}, {"type": "text", "text": "\u2022 Finding localized Jacobians via masking. To enable local editing, we use a mask $\\Omega$ (i.e., an index set of pixels) to select the region of interest,2 with $\\mathcal{P}_{\\Omega}(\\cdot)$ denoting the projection onto the index set $\\Omega$ . For picking a local editing direction, we calculate the Jacobian of ${\\pmb f}_{\\pmb\\theta,t}({\\pmb x}_{t})$ restricted to the region of interest, $\\tilde{J}_{\\theta,t}=\\nabla_{\\mathbf{x}_{t}}P_{\\Omega}(\\mathbf{\\{\\phi}}_{\\theta,t}(\\mathbf{x}_{t}))=\\tilde{U}\\tilde{\\Sigma}\\tilde{V}^{\\top}$ , and select the localized editing direction $\\boldsymbol{v}$ from the top- ${\\bf\\nabla}r$ singular vectors of $\\Tilde{V}$ (e.g., $v=\\tilde{V}[:,k]\\in\\mathrm{range}\\,\\tilde{J}_{\\theta,t}^{\\top}$ for some index $k\\in[r])$ . In practice, a top- ${\\bf\\nabla}r$ rank estimation for $\\Tilde{V}$ is calculated through the generalized power method (GPM) Algorithm 2 with $r=5$ to improve efficiency. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Better semantic disentanglement via nullspace projection. However, the projection $\\mathcal{P}_{\\Omega}(\\cdot)$ introduces extra nonlinearity into the mapping $P_{\\Omega}(f_{\\theta,t}(\\mathbf{\\boldsymbol{x}}_{t}))$ , causing the identified direction to have semantic entanglements with the area $\\Omega^{C}$ outside of the mask. Here, $\\Omega^{C}$ denotes the complimentary set of $\\Omega$ . To address this issue, we can use the nullspace projection method [56, 57]. Specifically, given $\\bar{J}_{\\theta,t}=\\nabla_{\\mathbf{x}_{t}}P_{\\Omega^{C}}({f}_{\\theta,t}(\\mathbf{x}_{t}))=\\bar{U}\\bar{\\Sigma}\\bar{V}^{\\top}$ , nullspace projection projects $_v$ onto $\\mathrm{null}\\left(\\bar{\\pmb{J}}_{\\theta,t}^{\\top}\\right)$ . The projection can be computed as $v_{p}=\\mathrm{proj}_{\\mathrm{null}\\left(\\bar{J}_{\\theta,t}\\right)}(v)=(I-\\bar{V}\\bar{V}^{\\top})v$ so that the modified $\\pmb{v_{p}}$ does not change the image in $\\Omega^{C}$ . In practice, we calculate a top- $\\cdot r^{\\prime}$ rank estimation for $\\bar{V}$ through the generalized power method (GPM) Algorithm 2 with $r^{\\prime}=5$ . ", "page_idx": 5}, {"type": "text", "text": "T-LOCO Edit. The unsupervised edit method can be seamlessly applied to T2I diffusion models with classifier-free guidance (3) (Algorithm 3). Besides, we can further enable text-supervised image editing with an editing prompt (Algorithm 4). See results in Figure 4(a). This is useful because the additional text prompt allows us to enforce a specified editing direction that cannot be found easily in the semantic subspace of the vanilla Jacobian $J_{\\theta,t}$ . As illustrated in Figure 4(b), this includes adding glasses or changing the curly hair of a human face. For simplicity, we introduce the key ideas of text-supervised T-LOCO Edit based upon DeepFloyd IF [19]. Similar procedures are also generalized to Stable Diffusion and Latent Consistency Models with an additional decoding step [4, 38]. We discuss the key intuition below, see Appendix E.2 and Appendix E.3 for method details. ", "page_idx": 5}, {"type": "image", "img_path": "50aOEfb2km/tmp/a75befd01e7070b442801887830df3cc63c168ee8d7e938dccffab53b5b3f0b4.jpg", "img_caption": ["(b) Text-supervised T2I Edit "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: T-LOCO Edit on T2I diffusion models. (a) Unsupervised editing direction is found only via the given mask without editing prompt. (b) Text-supervised editing direction is found with both a mask and an editing prompt such as \"with glasses\". Experiment details can be found in Appendix G.3. ", "page_idx": 6}, {"type": "text", "text": "We first introduce some notations. Let $c_{o}$ denote the original prompt, and $c_{e}$ denote the editing prompt. For example, in Figure 4(b), $c_{o}$ can be \u201cportrait of a man\u201d, while $c_{e}$ can be \u201cportrait of a man with glasses\u201d. Correspondingly, given the noisy image $\\pmb{x}_{t}$ for the clean image $\\scriptstyle x_{0}$ generated with $c_{o}$ , let ${f_{\\theta,t}^{o}}(x_{t})$ and $J_{\\theta,t}^{o}(x_{t})$ be the estimated posterior mean and its Jacobian conditioned on the original prompt $c_{o}$ , and let ${\\pmb f}_{\\pmb\\theta,t}^{e}({\\pmb x}_{t})$ and $J_{\\theta,t}^{e}(x_{t})$ be the estimated posterior mean and its Jacobian conditioned on both the editing prompt $c_{e}$ and $c_{o}$ . ", "page_idx": 6}, {"type": "text", "text": "According to the classifier-free guidance (3), we can estimate the difference of estimated posterior means caused by the editing prompt as $d=f_{\\theta,t}^{e}(x_{t})-f_{\\theta,t}^{o}(x_{t})$ , and then set $v=J_{\\theta,t}^{e}(x_{t})^{\\top}d$ as an initial estimator of the editing direction.3 Based upon this, to enable localized editing, similar to the unsupervised case, we can apply masks $\\Omega$ to select ROI in $^d$ and calculate localized Jacobian to get $_v$ . After that, similarly, we can perform nullspace projection of $\\boldsymbol{v}$ for better disentanglement to get the final editing direction vp. ", "page_idx": 6}, {"type": "text", "text": "4 Justification of Local Linearity, Low-rankness, & Semantic Direction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we provide theoretical justification for the benign properties in Section 3.1. First, we assume that the image distribution $p_{\\mathrm{data}}$ follows mixture of low-rank Gaussians defined as follows. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1. The data $\\pmb{x}_{0}\\in\\mathbb{R}^{d}$ generated distribution $p_{d a t a}$ lies on a union of $K$ subspaces. The basis of each subspace $\\{M_{k}\\in\\mathrm{St}(d,r_{k})\\}_{k=1}^{K}$ are orthogonal to each other with $M_{i}^{\\top}M_{j}=\\mathbf{0}$ for all $1\\leq i\\neq j\\leq K$ , and the subspace dimension $r_{k}$ is much smaller than the ambient dimension $d$ . Moreover, for each $k\\in[K]$ , $\\pmb{x}_{0}$ follows degenerated Gaussian with $\\mathbb{P}\\left(\\pmb{x}_{0}=M_{k}\\pmb{a}_{k}\\right)=1/K,\\pmb{a}_{k}\\sim$ $\\mathcal{N}(\\mathbf{0},\\pmb{I}_{r_{k}})$ . Without loss of generality, suppose $\\pmb{x}_{t}$ is from the $h$ -th class, that is $\\pmb{x}_{t}\\,=\\,\\sqrt{\\alpha_{t}}\\pmb{x}_{0}\\,+$ $\\sqrt{1-\\alpha_{t}}\\epsilon$ where $\\pmb{x}_{0}\\in\\mathrm{range}(\\pmb{M}_{h}),$ , i.e. $\\pmb{x}_{0}=M_{h}\\pmb{a}_{h}$ . Both $||\\pmb{x}_{0}||_{2},||\\pmb{\\epsilon}||_{2}$ is bounded. ", "page_idx": 6}, {"type": "text", "text": "Our data assumption is motivated by the intrinsic low-dimensionality of real-world image dataset [58].Additionally, Wang et al. [59] demonstrated that images generated by an analytical score function derived from a mixture of Gaussians distribution exhibit conceptual similarities to those produced by practically trained diffusion models. Given that ${\\bf\\nabla}f_{\\pmb{\\theta},t}({\\bf x}_{t})$ is an estimator of the posterior mean $\\mathbb{E}[{\\pmb x}_{0}|{\\pmb x}_{t}]$ , we show that the posterior mean $\\mathbb{E}[{\\pmb x}_{0}|{\\pmb x}_{t}]$ can analytically derived as follows. ", "page_idx": 6}, {"type": "text", "text": "Lemma 1. Under Assumption 1, for $t\\in(0,1],$ , the posterior mean is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathbf{\\boldsymbol{x}}_{0}|\\mathbf{\\boldsymbol{x}}_{t}\\right]=\\sqrt{\\alpha_{t}}\\frac{\\sum_{k=1}^{K}\\exp\\left(\\frac{\\alpha_{t}}{2\\left(1-\\alpha_{t}\\right)}\\|\\boldsymbol{M}_{k}^{\\top}\\boldsymbol{x}_{t}\\|^{2}\\right)M_{k}M_{k}^{\\top}\\boldsymbol{x}_{t}}{\\sum_{k=1}^{K}\\exp\\left(\\frac{\\alpha_{t}}{2\\left(1-\\alpha_{t}\\right)}\\|\\boldsymbol{M}_{k}^{\\top}\\boldsymbol{x}_{t}\\|^{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Lemma 1 shows that the posterior mean $\\mathbb{E}\\left[\\pmb{x}_{0}\\middle|\\pmb{x}_{t}\\right]$ could be viewed as a convex combination of $M_{k}M_{k}^{\\top}\\pmb{x}_{t}$ , i.e. $\\pmb{x}_{t}$ projected onto each subspace $M_{k}$ . This lemma leads to the following theorem: ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Based upon Assumption $^{\\,l}$ , we can show the following three properties for the posterior mean $\\mathbb{E}[{\\pmb x}_{0}|{\\pmb x}_{t}]$ : ", "page_idx": 7}, {"type": "text", "text": "\u2022 The Jacobian of posterior mean satisfies rank $(\\nabla_{\\mathbf{x}_{t}}\\mathbb{E}[\\mathbf{x}_{0}|\\mathbf{x}_{t}])\\leq r:=\\sum_{k=1}^{K}r_{k}$ for all $t\\in(0,1]$ . \u2022 The posterior mean $\\mathbb{E}[{\\pmb x}_{0}|{\\pmb x}_{t}]$ has local linearity such that   \n$\\|\\mathbb{E}\\left[x_{0}|x_{t}+\\lambda\\Delta x\\right]-\\mathbb{E}\\left[x_{0}|x_{t}\\right]-\\lambda\\nabla_{x_{t}}\\mathbb{E}[x_{0}|x_{t}]\\cdot\\Delta x\\|=\\lambda\\frac{\\alpha_{t}}{(1-\\alpha_{t})}\\mathcal{O}(\\lambda),$ (8) where $\\Delta{\\pmb x}\\in\\mathbb S^{d-1}$ and $\\lambda\\in\\mathbb{R}$ is the step size.   \n\u2022 $\\nabla_{\\pmb{x}_{t}}\\mathbb{E}[\\pmb{x}_{0}|\\pmb{x}_{t}]$ is symmetric and the full SVD of $\\nabla_{\\pmb{x}_{t}}\\mathbb{E}[\\pmb{x}_{0}|\\pmb{x}_{t}]$ could be written as $\\nabla_{\\mathbf{x}_{t}}\\mathbb{E}[\\mathbf{x}_{0}|\\mathbf{x}_{t}]=$ $\\pmb{U}_{t}\\pmb{\\Sigma}_{t}\\pmb{V}_{t}^{\\top}$ , where $U_{t}~=~[\\mathbf{u}_{t,1},\\mathbf{u}_{t,2},\\ldots,\\mathbf{u}_{t,d}]~\\in~\\mathrm{St}(d,d)$ , $\\Sigma_{t}~=~\\mathrm{diag}(\\sigma_{t,1},\\dots,\\sigma_{t,r},\\dots,0)$ with $\\sigma_{t,1}\\quad\\geq\\quad\\cdot\\cdot\\quad\\geq\\quad\\sigma_{t,r}\\quad\\geq$ 0 and $\\begin{array}{r l r}{V_{t}}&{{}=}&{\\big[{\\boldsymbol v}_{t,1},{\\boldsymbol v}_{t,2},\\ldots,{\\boldsymbol v}_{t,d}\\big]\\quad\\in\\quad\\mathrm{St}(d,d).}\\end{array}$ Let $\\begin{array}{r l r}{U_{t,1}}&{{}:=}&{\\left[\\pmb{u}_{t,1},\\pmb{u}_{t,2},\\dots,\\pmb{u}_{t,r}\\right]}\\end{array}$ and $\\begin{array}{r l r}{M}&{{}:=}&{[M_{1},M_{2},\\cdot\\cdot\\cdot,M_{K}]}\\end{array}$ . $I t$ holds that $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow1}\\left\\|\\left(I_{d}-U_{t,1}U_{t,1}^{\\top}\\right)M\\right\\|_{F}=0.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "The proof is deferred to Appendix F. Admittedly, there are gap between our theory and practice, such as the approximation error between $f_{\\pmb{\\theta},t}(\\pmb{x}_{t})$ and $\\mathbb{E}[{\\pmb x}_{0}|{\\pmb x}_{t}]$ , assumptions about the data distribution, and the high rankness of $J_{\\theta,t}$ for $t<0.2$ and $t>0.9$ in Figure 2. Nonetheless, Theorem 1 largely supports our empirical observation in Section 3 that we discuss below: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Low-rankness of the Jacobian. The first property in Theorem 1 demonstrates that the rank of $\\nabla_{\\pmb{x}_{t}}\\mathbb{E}[\\pmb{x}_{0}|\\pmb{x}_{t}]$ is always no greater than the intrinsic dimension of the data distribution. Given that the intrinsic dimension of the real data distribution is usually much lower than the ambient dimension [58], the rank of $J_{\\theta,t}$ on the real dataset should also be low. The results align with our empirical observations in Figure 2 when $t\\in[0.2,0.7]$ . ", "page_idx": 7}, {"type": "text", "text": "\u2022 Linearity of the posterior mean. The second property in Theorem 1 shows that the linear approximation error is within the order of $\\lambda\\alpha_{t}/(1-\\bar{\\alpha}_{t})\\bar{\\cdot}\\mathcal{O}(\\lambda)$ . This implies that when $t$ approaches 1, $\\alpha_{t}/(1-\\alpha_{t})$ becomes small, resulting in a small approximation error even for large $\\lambda$ . Empirically, Figure 2 shows that the linear approximation error of ${\\bf\\nabla}f_{\\pmb{\\theta},t}({\\bf x}_{t})$ is small when $t=0.7$ and $\\lambda=40$ , whereas Figure 10 shows a much larger error for $t=0.0$ under the same $\\lambda$ . These observations align well with our theory. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Low-dimensional semantic subspace. The third property in Theorem 1 shows that, when $t$ is close to 1, left singular vectors associated with the top- $^r$ singular values form the basis of the image distribution. Since the editing direction consists of basis, the edited image remains within the image distribution. This explains why $\\pmb{u}_{i}$ found in Equation (6) is a semantic direction for image editing. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we perform extensive experiments to demonstrate the effectiveness and efficiency of LOCO Edit. We first showcase LOCO Edit has strong localized editing ability across a variety of datasets in Section 5.1. Moreover, we conduct comprehensive comparisons with other methods to show the superiority of the LOCO Edit method in Section 5.2. Besides, we provide ablation studies on multiple components in our method in Appendix C.1, and analyze the editing directions in Appendix C.2, with extra experimental details postponed to Appendix G. ", "page_idx": 7}, {"type": "text", "text": "5.1 Demonstration on Localized Editing and Other Benign Properties ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "First, we demonstrate benign properties of LOCO Edit in Algorithm 1 on a variety of datasets, including LSUN-Church [60], Flower [61], AFHQ [62], CelebA-HQ [52], and FFHQ [63]. ", "page_idx": 7}, {"type": "text", "text": "As shown in Figure 5 and Figure 1a, our method enables editing specific localized regions such as eye size/focus, hair curvature, length/amount, and architecture, while preserving the consistency of other regions. Besides the ability of precise local editing, Figure 1 demonstrates the benign properties of the identified editing directions and verify our analysis in Section 4: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Linearity. As shown Figure 1(d), the semantic editing can be strengthened through larger editing scales and can be flipped by negating the scale.   \n\u2022 Homogeneity and transferability. As shown Figure 1(b), the discovered editing direction can be transferred across samples and timesteps in $\\scriptstyle{\\mathcal{X}}_{t}$ . ", "page_idx": 7}, {"type": "image", "img_path": "50aOEfb2km/tmp/569b0e72a9a7edcdc96c4a5cf8c2dbfa732d349e0f8ef7fd9da702371258d326.jpg", "img_caption": ["Figure 5: Benchmarking LOCO Edit across various datasets. For each group of three images, in the center is the original image, and on the left and right are edited images along the negative and the positive directions accordingly. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "\u2022 Composability. As shown Figure 1(c), the identified disentangled editing directions in the low-rank subspace allow direct composition without influencing each other. ", "page_idx": 8}, {"type": "text", "text": "5.2 Comprehensive Comparison with Other Image Editing Methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We compare LOCO Edit with several notable and recent image editing techniques, including Asyrp [29], Pullback [30], NoiseCLR [23], and BlendedDifusion [24]. We also compare with an unexplored method using the Jacobians \u2202\u2202x\u03f5tt to find the editing direction, named as \u2202\u2202x\u03f5tt . ", "page_idx": 8}, {"type": "text", "text": "Metrics. We evaluate our method using the below metrics and summarize the results in Table 1. Besides the image generation quality, we also compared other attributes such as the local edit ability, efficiency, the requirement for supervision, and theoretical justifications. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Local Edit Success Rate evaluates whether the editing successfully changes the target semantics and preserves unrelated regions by human evaluators. \u2022 LPIPS [64] and SSIM [65] measure the consistency between edited and original images. \u2022 Transfer Success Rate measures whether the editing transferred to other images successfully changes the target semantics and preserves unrelated regions by human evaluators. \u2022 Learning time to measure the time required to identify the edit directions. \u2022 Transfer Edit Time to measure the time required to transfer the editing to other images directly. \u2022 #Images for Learning measures the number of images used to find the editing directions. \u2022 One-step Edit, No Additional Supervision, Theoretically Grounded, and Localized Edit are attributes of the editing methods, where each of them measures a specific property for the method. ", "page_idx": 8}, {"type": "text", "text": "Moreover, we visualize the editing results on non-cherry-picked images in Figure 6. The detailed evaluation settings are provided in Appendix G.2. ", "page_idx": 8}, {"type": "text", "text": "Benefits of Our Method. Based upon the qualitative and quantitative comparisons, our method shows several clear advantages that we summarize as follows. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Superior local edit ability with one-step edit. Table 1 shows LOCO Edit achieves the best Local Edit Success Rate. Such local edit ability only requires one-step edit at a specific time $t$ . For LPIPS and SSIM, our method performs better than most methods but worse than BlendedDiffusion. However, BlendedDiffusion sometimes fails the edit within the masks (as visualized in Figure 6, rows 1, 3, 4, and 5). Other methods find semantic direction more globally, leading to worse performance in Local Edit Success Rate, LPIPS, and SSIM for localized edits. ", "page_idx": 8}, {"type": "table", "img_path": "50aOEfb2km/tmp/d6aba550156a37d310ad203baee9081b4c7247b0b5feec29c451711ce5ccb3b1.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparisons with existing methods. Our LOCO Edit excels in localized editing, transferability and efficiency, with other intriguing properties such as one-step edit, supervision-free, and theoretically grounded. "], "page_idx": 9}, {"type": "image", "img_path": "50aOEfb2km/tmp/c847712a6f948e62efd964bc11ec0912c08beaf40e37c1cd033c00f397cbf09c.jpg", "img_caption": ["Figure 6: Compare local edit ability with other works on non-cherry-picked images. LOCO has consistent and accurate local edit ability, while other methods have wrong, global, or no edits. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "\u2022 Transferability and efficiency. First, LOCO Edit requires less learning time than most of the other methods and requires learning only for a single time step with a single image. Moreover, LOCO Edit is highly transferable, having the highest Transfer Success Rate in Table A. In contrast, BlendedDiffusion cannot transfer and requires optimization for each individual image. NoiseCLR has the second-best yet lower transfer success rate, while other methods exhibit worse transferability. ", "page_idx": 9}, {"type": "text", "text": "\u2022 Theoretically-grounded and supervision-free. LOCO Edit is theoretically grounded. Besides, it is supervision-free, thus integrating no biases from other modules such as CLIP [36]. [37] shows CLIP sometimes can\u2019t capture detailed semantics such as color. We can observe failures in capturing detailed semantics for methods that utilize CLIP guidance such as BlendedDiffusion and Asyrp in Figure 6, where there are no edits or wrong edits. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed a new low-rank controllable image editing method, LOCO Edit, which enables precise, one-step, localized editing using diffusion models. Our approach stems from the discovery of the locally linear posterior mean estimator in diffusion models and the identification of a lowdimensional semantic subspace in its Jacobian, theoretically verified under certain data assumptions. The identified editing directions possess several beneficial properties, such as linearity, homogeneity, and composability. Additionally, our method is versatile across different datasets and models and is applicable to text-supervised editing in T2I diffusion models. Through various experiments, we demonstrate the superiority of our method compared to existing approaches. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We acknowledge support from NSF CAREER CCF-2143904, NSF CCF-2212066, NSF CCF2212326, NSF IIS 2312842, NSF IIS 2402950, ONR N00014-22-1-2529, a gift grant from KLA, an Amazon AWS AI Award, MICDE Catalyst Grant. The authors acknowledge valuable discussions with Mr. Zekai Zhang (U. Michigan), Dr. Ismail R. Alkhouri (U. Michigan and MSU), Mr. Jinfan Zhou (U. Michigan), and Mr. Xiao Li (U. Michigan). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020. [2] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. [3] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. [4] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022. [5] Huijie Zhang, Yifu Lu, Ismail Alkhouri, Saiprasad Ravishankar, Dogyoon Song, and Qing Qu. Improving training efficiency of diffusion models via multi-stage framework and tailored multi-decoder architectures. In Conference on Computer Vision and Pattern Recognition 2024, 2024. [6] Ismail Alkhouri, Shijun Liang, Rongrong Wang, Qing Qu, and Saiprasad Ravishankar. Diffusion-based adversarial purification for robust deep mri reconstruction. ArXiv preprint arXiv:2309.05794, 2023. [7] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021. [8] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In International Conference on Learning Representations, 2021. [9] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[10] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2023.   \n[11] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, 2023.   \n[12] Xiang Li, Soo Min Kwon, Ismail R Alkhouri, Saiprasad Ravishanka, and Qing Qu. Decoupled data consistency with diffusion purification for image restoration. ArXiv preprint arXiv:2403.06054, 2024.   \n[13] Ismail Alkhouri, Shijun Liang, Rongrong Wang, Qing Qu, and Saiprasad Ravishankar. Robust physicsbased deep mri reconstruction via diffusion purification. In Conference on Parsimony and Learning (Recent Spotlight Track), 2023.   \n[14] Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, and Liyue Shen. Solving inverse problems with latent diffusion models via hard data consistency. In The Twelfth International Conference on Learning Representations, 2024.   \n[15] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18456\u201318466, 2023.   \n[16] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. ArXiv preprint arXiv:2311.15127, 2023.   \n[17] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15954\u201315964, 2023.   \n[18] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv preprint arXiv:2204.06125, 2022.   \n[19] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.   \n[20] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4396\u20134405, 2018.   \n[21] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.   \n[22] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500\u201322510, 2023.   \n[23] Yusuf Dalva and Pinar Yanardag. Noiseclr: A contrastive learning approach for unsupervised discovery of interpretable directions in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24209\u201324218, 2024.   \n[24] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18208\u201318218, June 2022.   \n[25] Theodoros Kouzelis, Manos Plitsis, Mihalis A. Nicolaou, and Yannis Panagakis. Enabling local editing in diffusion models by joint and individual component analysis, 2024.   \n[26] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. In The Eleventh International Conference on Learning Representations, 2023.   \n[27] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. SEGA: Instructing text-to-image models using semantic guidance. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[28] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1900\u20131910, 2023.   \n[29] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent space. In The Eleventh International Conference on Learning Representations, 2023.   \n[30] Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understanding the latent space of diffusion models through the lens of riemannian geometry. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[31] Ye Zhu, Yu Wu, Zhiwei Deng, Olga Russakovsky, and Yan Yan. Boundary guided learning-free semantic control with diffusion models. In Conference on Neural Information Processing Systems (NeurIPS), 2023.   \n[32] Hila Manor and Tomer Michaeli. Zero-shot unsupervised and text-based audio editing using DDPM inversion. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 34603\u201334629. PMLR, 21\u201327 Jul 2024.   \n[33] Hila Manor and Tomer Michaeli. On the posterior distribution in denoising: Application to uncertainty quantification. In The Twelfth International Conference on Learning Representations, 2024.   \n[34] Yousef Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM, 2011.   \n[35] Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understanding the latent space of diffusion models through the lens of riemannian geometry. Advances in Neural Information Processing Systems, 36:24129\u201324142, 2023.   \n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.   \n[37] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9568\u20139578, 2024.   \n[38] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. ArXiv preprint arXiv:2310.04378, 2023.   \n[39] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \n[40] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6038\u20136047, 2023.   \n[41] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.   \n[42] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022.   \n[43] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \n[44] Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Peng Wang, Liyue Shen, and Qing Qu. The emergence of reproducibility and consistency in diffusion models. In Forty-first International Conference on Machine Learning, 2024.   \n[45] Calvin Luo. Understanding diffusion models: A unified perspective. ArXiv preprint arXiv:2208.11970, 2022.   \n[46] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2426\u20132435, 2022.   \n[47] Ren\u00e9 Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, and Tomer Michaeli. Discovering interpretable directions in the semantic latent space of diffusion models. International Conference on Automatic Face and Gesture Recognition, abs/2303.11073, 2024.   \n[48] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024.   \n[49] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-assisted Intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015.   \n[50] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[51] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22669\u201322679, 2023.   \n[52] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pages 3730\u20133738, 2015.   \n[53] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. Ieee, 2009.   \n[54] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \n[55] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4015\u20134026, October 2023.   \n[56] S. Banerjee and A. Roy. Linear Algebra and Matrix Analysis for Statistics. Chapman & Hall/CRC Texts in Statistical Science. CRC Press, 2014.   \n[57] Jiapeng Zhu, Ruili Feng, Yujun Shen, Deli Zhao, Zhengjun Zha, Jingren Zhou, and Qifeng Chen. Low-rank subspaces in gans. In Neural Information Processing Systems, 2021.   \n[58] Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. In International Conference on Learning Representations, 2021.   \n[59] Binxu Wang and John J Vastola. The hidden linear structure in score-based models and its application. ArXiv preprint arXiv:2311.10892, 2023.   \n[60] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. ArXiv, abs/1506.03365, 2015.   \n[61] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages 722\u2013729, 2008.   \n[62] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8188\u20138197, 2020.   \n[63] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4401\u20134410, 2019.   \n[64] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 586\u2013595, 2018.   \n[65] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, 2004.   \n[66] Peng Wang, Huikang Liu, Druv Pai, Yaodong Yu, Zhihui Zhu, Qing Qu, and Yi Ma. A global geometric analysis of maximal coding rate reduction. In Forty-first International Conference on Machine Learning, 2024.   \n[67] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.   \n[68] Can Yaras, Peng Wang, Laura Balzano, and Qing Qu. Compressible dynamics in deep overparameterized low-rank learning & adaptation. In Forty-first International Conference on Machine Learning, 2024.   \n[69] Michael Fuest, Pingchuan Ma, Ming Gui, Johannes S Fischer, Vincent Tao Hu, and Bjorn Ommer. Diffusion models and representation learning: A survey. ArXiv preprint arXiv:2407.00783, 2024.   \n[70] Xiang Li, Yixiang Dai, and Qing Qu. Understanding generalizability of diffusion models requires rethinking the hidden gaussian structure. 2024.   \n[71] Peng Wang, Xiao Li, Yaras Can, Zhihui Zhu, Laura Balzano, Wei Hu, and Qing Qu. Understanding deep representation learning via layerwise feature compression and discrimination. ArXiv preprint arXiv:2311.02960, 2023.   \n[72] Siyi Chen, Minkyu Choi, Zesen Zhao, Kuan Han, Qing Qu, and Zhongming Liu. Unfolding videos dynamics via taylor expansion, 2024.   \n[73] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instructnerf2nerf: Editing 3d scenes with instructions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.   \n[74] Shengyi Qian, Linyi Jin, Chris Rockwell, Siyi Chen, and David F. Fouhey. Understanding 3d object articulation in internet videos. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1589\u20131599, 2022.   \n[75] Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris CallisonBurch, and Rene Vidal. Pace: Parsimonious concept engineering for large language models. ArXiv preprint arXiv:2406.04331, 2024.   \n[76] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Neural Information Processing Systems, 2014.   \n[77] Ren\u00e9 Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, and Tomer Michaeli. Discovering interpretable directions in the semantic latent space of diffusion models. International Conference on Automatic Face and Gesture Recognition, abs/2303.11073, 2024.   \n[78] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-toprompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2023.   \n[79] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka. Instructedit: Improving automatic masks for diffusion-based image editing with user instructions. ArXiv, abs/2305.18047, 2023.   \n[80] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18392\u201318402, 2022.   \n[81] Shanglin Li, Bo-Wen Zeng, Yutang Feng, Sicheng Gao, Xuhui Liu, Jiaming Liu, Li Lin, Xu Tang, Yao Hu, Jianzhuang Liu, and Baochang Zhang. Zone: Zero-shot instruction-guided local editing. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \n[82] Bradley Efron. Tweedie\u2019s formula and selection bias. Journal of the American Statistical Association, 106(496):1602\u20131614, 2011.   \n[83] A Woodbury Max. Inverting modified matrices. In Memorandum Rept. 42, Statistical Research Group, page 4. Princeton Univ., 1950.   \n[84] Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM Journal on Numerical Analysis, 7(1):1\u201346, 1970.   \n[85] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS \u201920, Red Hook, NY, USA, 2020. Curran Associates Inc.   \n[86] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo J. Kim, and Sung-Hoon Yoon. Perception prioritized training of diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11462\u201311471, 2022.   \n[87] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Future Direction ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We identify several future directions and limitations of the current work. The current theoretical framework explains mainly the unsupervised image editing part. A more solid and thorough analysis of text-supervised image editing is of significant importance in understanding T2I diffusion models, which is yet a difficult open problem in the field. For example, there is still a lack of geometric analysis of the relationship between subspaces under different text-prompt conditions [4, 19, 38, 66]. Based on such understandings, it may be possible to further discover benign properties of editing directions in T2I diffusion models, or design more efficient fine-tuning [67, 68] accordingly. Besides, the current method has the potential to be extended for combining coarse to fine editing across different time steps. Furthermore, it is worth exploring the direct manipulation of semantic spaces in flow-matching diffusion models and transformer-architecture diffusion models. Lastly, it is possible to connect the current finding to image or video representation learning in diffusion models [69, 70, 71, 72], extend to 3D editing of pose or shape [73, 74], or utilize the low-rank structures to build dictionaries [75]. ", "page_idx": 15}, {"type": "text", "text": "B Discussion on Related Works ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Study of Latent Semantic Space in Generative Models. Although diffusion models have demonstrated their strengths in state-of-the-art image synthesis, the understanding of diffusion models is still far behind the other generative models such as Generative Adversarial Networks (GAN) [76, 57], the understanding of which can provide tools as well as inspiration for the understanding of diffusion models. Some recent works have identified such gaps, discovered latent semantic spaces in diffusion models [29], and further studied the properties of the latent space from a geometrical perspective [30]. These prior arts deepen our understanding of the latent semantic space in diffusion models, and inspire later works to study the structures of information represented in diffusion models from various angles. However, their semantic space is constrained to diffusion models using UNet architecture, and can not represent localized semantics. Our work explores an alternative space to study the semantic expression in diffusion models, inspired by our observation of the low-rank and locally linear Jacobian of the denoiser over the noisy images. We provide a theoretical framework for demonstrating and understanding such properties, which can deepen the interpretation of the learned data distribution in diffusion models. ", "page_idx": 15}, {"type": "text", "text": "Image Editing in Unconditional Diffusion Models. Recent research has significantly improved the understanding of latent semantic spaces in diffusion models, enabling global image editing through either training-free methods [29, 30, 31] or by incorporating an additional lightweight model [30, 77]. However, these methods result in poor performance for localized edit. In contrast, our approach achieves localized editing without requiring supervised training. For localized edits, [25] builds on [30], enabling local edits by altering the intermediate layers of UNet. However, these approaches are restricted to UNet-based architectures in diffusion models and have largely ignored intrinsic properties like linearity and low-rankness. In comparison, our work provides a rigorous theoretical analysis of low-rankness and local linearity in diffusion models, and we are the first to offer a principled justification of the semantic significance of the basis used for editing. Moreover, our method is independent of specific network architectures. ", "page_idx": 15}, {"type": "text", "text": "Other recent works, such as [32], introduce training-free global audio and image editing based on a theoretical understanding of the posterior covariance matrix [33], also independent of UNet architectures. However, our approach offers a distinct perspective, providing complementary insights and new findings. We explore the low-rank nature and local linearity in PMP, offering rigorous theoretical analyses. Based on this, our proposed LOCO Edit method allows unsupervised and localized editing, which enables several advantageous properties including transferability, composability, and linearity \u2013 benign features that have not been explored in prior work. Further, we extend the method to unsupervised and text-supervised editing in various text-to-image models. Additionally, while [24] supports localized editing, it requires supervision from CLIP, lacks a theoretical basis, and is time-consuming for editing each image. In contrast, our method is more efficient, theoretically grounded, and free from failures or biases in CLIP. The CLIP-supervised may also exhibit a bias toward the CLIP score, leading to suboptimal editing results, as shown in Figure 6. In comparison, our method consistently enables high-quality edits without such bias. ", "page_idx": 15}, {"type": "image", "img_path": "50aOEfb2km/tmp/4714bbc8ba1c158b8e61deb3d7851452bd5d3ceb4f86a5af7cd746a8009c1c1e.jpg", "img_caption": ["Figure 7: Ablation Study. (a) Effects of one-step edit time. (b)Effects of using nullspace projection and rank. (c)Effects of editing strengths. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Image Editing in T2I Diffusion Models. T2I image editing usually requires much more complicated sampling and training procedures, such as providing certainly learned guidance in the reverse sampling process [11], training an extra neural network [21], or fine-tuning the models for certain attributes [22]. Although effective, these methods often require extra training or even human intervention. Some other T2I image editing methods are training-free [46, 27, 28], and further enable editing with identifying masks [46], or optimizing the soft combination of text prompts [28]. These methods involve a continuous injection of the edit prompt during the generation process to gradually refine the generated image to have the target semantics. Though effective, all of the above methods (either training-free or not) as well as instruction-guided ones [78, 79, 80, 81] lack clear mathematical interpretations and requires text supervision. [23] discovers editing directions in T2I diffusion models through contrastive learning without text supervision, but is not generalizable to editing with text supervision. [30] has some theoretical basis and extends to an editing approach in T2I diffusion models with text supervision, but such supervision is only for unconditional sampling. In contrast, our extended T-LOCO Edit, which originated from the understanding of diffusion models, is the first method exploring single-step editing with or without text supervision for conditional sampling. ", "page_idx": 16}, {"type": "text", "text": "C More Experiment Results on LOCO-Edit ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Ablation Studies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We conduct several important ablation studies on noise levels, the rank of nullspace projection, and editing strength, which demonstrates the robustness of our method. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Noise levels (i.e., editing time step $t$ ). We conducted an ablation study on different noise levels, with representative examples shown in Figure 7a. The key observations are summarized as follows: (a) Larger noise levels (i.e., edit on $x_{t}$ with larger $t$ ) perform more coarse edit while small noise levels perform finer edit; (b) LOCO Edit is applicable to a generally large range of noise levels ([0.2T, 0.7T]) for precise edit.   \n\u2022 Rank of nullspace projection $r^{\\prime}$ . Ablation study on nullspace projection is in Figure 7b (definition of $r^{\\prime}$ is in Algorithm 1). We present the key observations: (a) the local edit ability with no nullspace projection is weaker than that with nullspace projection; (b) when conducting nullspace projection, an effective low-rank estimation with $r^{\\prime}=5$ can already achieve good local edit results.   \n\u2022 Editing strength $\\lambda$ . The linearity with respect to editing strengths is visualized in Figure 7c, with the key observations in addition to linearity: LOCO Edit is applicable to a generally wide range of editing strengths ([-15, 15]) to achieve localized edit. ", "page_idx": 16}, {"type": "table", "img_path": "50aOEfb2km/tmp/1d58a60ee82641085782d8a5d9cfab7ce312c98b1f23337f16d3447ebc344063.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "50aOEfb2km/tmp/512508b07c3bd23bb1cfac8f173a8aa4cce6db6f4dee933a8a46cc41ca25c0d7.jpg", "img_caption": ["Figure 8: Visualizing edit directions identified via LOCO Edit. The edit directions are semantically meaningful. ", "Figure 9: Analyzing transferability of edit directions to objects with different positions and shapes, images from different datasets, or images with no corresponding semantics. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.2 Visualization and Analysis of Editing Directions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We visualize the identified editing direction $\\pmb{v_{p}}$ (see Algorithm 1) in Figure 8. The editing directions are semantically meaningful to the region of interest for editing. For example, the editing directions for eyes, lips, nose, etc., have similar shapes to eyes, lips, nose, etc. ", "page_idx": 17}, {"type": "text", "text": "Further, since the objects in datasets Flower, AFHQ, CelebA-HQ, and FFHQ are usually positioned at the center, the identified editing directions also tend to be at the center. Besides, objects could have different shapes, and semantics in some images do not exist in other images. To further study the robustness of transferability for the editing directions, we transfer editing directions to images with objects at different positions, from different datasets, with different shapes, and with no corresponding semantics. We present the results in Figure 9, with key observations that: (a) the edit directions are generally robust to gender differences, shape differences, moderate position differences, and dataset differences, illustrated in the first five rows of Figure 9 (b) transferring editing direction to images without corresponding semantics results in almost no editing (shown in the last row of Figure 9). Therefore, in practical applications, meaningful transfer editing scenarios for LOCO Edit occur when the transferred editing directions correspond to existing semantics in the target image (e.g., transferring the editing direction of \"eyes\" is effective only if the target image also contains eyes). ", "page_idx": 17}, {"type": "image", "img_path": "50aOEfb2km/tmp/39e63234b27cfac334b0fec7392dc22769ddd31d1059d4c631c1a10cc29cb8fe.jpg", "img_caption": ["Figure 10: More results on the linearity of $\\pmb{f}_{\\pmb{\\theta},t}(\\pmb{x}_{t},t)$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D More Empirical Study on Low-rankness & Local Linearity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Experiment Setup for Section 3.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We evaluate the numerical rank of the denoiser function $\\pmb{x}_{\\pmb{\\theta}}(\\pmb{x}_{t},t)$ for DDPM (U-Net [49] architecture) on CIFAR-10 dataset [50] $(d=32\\times32\\times3)$ , U-ViT [51] (Transformer based networks) on CelebA [52] $(d=64\\times64\\times3)$ , ImageNet [53] datasets $(d=64\\times64\\times3)$ and DeepFloy IF [19] trained on LAION-5B [54] dataset $(d=64\\times64\\times3)$ ). Notably, U-ViT architecture uses the autoencoder to compress the image $\\pmb{x}_{0}$ to embedding vector $z_{0}=\\mathtt{E n c o d e r}(\\pmb{x}_{0})$ , and adding noise to $\\boldsymbol{z}_{t}$ for the diffusion forward process; and the reverse process replaces $\\mathbf{\\boldsymbol{x}}_{t},\\mathbf{\\boldsymbol{x}}_{t-\\Delta t}$ with $z_{t},z_{t-\\Delta t}$ in Equation (1). And the generated image $\\pmb{x}_{0}=\\mathtt{D e c o d e r}(\\pmb{z}_{0})$ . The PMP defined for U-ViT is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\pmb{x}}_{0,t}=f_{\\theta,t}(\\pmb{z}_{t};t):=\\tt D e c o d e r}\\left(\\frac{\\boldsymbol{z}_{t}-\\sqrt{1-\\alpha_{t}}\\pmb{\\epsilon}_{\\theta}(\\pmb{z}_{t},t)}{\\sqrt{\\alpha_{t}}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The $J_{\\theta,t}(z_{t};t)\\,=\\,\\nabla_{z_{t}}f_{\\theta,t}(z_{t};t)$ for $f_{\\theta,t}(z_{t};t)$ defined above. For DeepFloy IF, there are three diffusion models, one for generation and the other two for super-resolution. Here we only evaluate $J_{\\theta,t}(z_{t};t)$ for diffusion generating the images. ", "page_idx": 18}, {"type": "text", "text": "Given a random initial noise ${\\mathbf{}}x_{T}$ , diffusion model $\\boldsymbol{x}_{\\theta}$ generate image sequence $\\left\\{x_{t}\\right\\}$ follows reverse sampler Equation (1). Along the sampling trajectory $\\left\\{{\\pmb x}_{t}\\right\\}$ , for each $\\pmb{x}_{t}$ , we calculate $J_{\\theta,t}(z_{t};t)$ and compute its numerical rank via ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{\\mathrm{rank}}(J_{\\theta,t}(x_{t}))=\\arg\\operatorname*{min}_{r}\\left\\{r:\\frac{\\sum_{i=1}^{r}\\sigma_{i}^{2}\\left(J_{\\theta,t}(x_{t};t)\\right)}{\\sum_{i=1}^{n}\\sigma_{i}^{2}\\left(J_{\\theta,t}(x_{t};t)\\right)}>\\eta^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\sigma_{i}(A)$ denotes the $i$ th largest singular value of $\\pmb{A}$ . In our experiments, we set $\\eta=0.99$ . We random generate 15 initialize noise $\\pmb{x}_{t}$ ( $\\scriptstyle z_{t}$ for U-ViT). We only use one prompt for DeepFloyd IF. We use DDIM with 100 steps for DDPM and DeepFloyd IF, DPM-Solver with 20 steps for U-ViT, and select some of the steps to calculate r $\\mathtt{a n k}(J_{\\theta,t}(\\mathbf{\\boldsymbol{x}}_{t};t))$ , reported the averaged rank in Figure 2. To report the norm ratio and cosine similarity, we select the closest $t$ to 0.7 along the sampling trajectory and reported in Figure 2, i.e. $t=0.71$ for DDPM, $t=0.66$ for U-ViT and $t=0.69$ for DeepFloyd IF. The norm ratio and cosine similarity are also averaged over 15 samples. ", "page_idx": 18}, {"type": "text", "text": "D.2 More Experiments for Section 3.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We illustrated the norm ratio and cosine similarity for more timesteps in Figure 10, more text prompts, and flow-matching-based diffusion model in Figure 11. More specifically, for the plot of $t=0.0$ , we exactly use $t=0.04$ for DDPM, $t=0.005$ for U-ViT and $t=0.09$ for DeepFloyd IF; for the plot of $t=0.5$ , we exactly use $t=0.49$ for DDPM, $t=0.50$ for U-ViT and $t=0.49$ for DeepFloyd IF. The results aligned with our results in Theorem 1 that when $t$ is closer the 1, the linearity of $\\pmb{f}_{\\pmb{\\theta},t}(\\pmb{x}_{t},t)$ is better. ", "page_idx": 18}, {"type": "image", "img_path": "50aOEfb2km/tmp/2144ebae63d7076a08ba1ce16b6488da61206b0c604c319609ed490bbd771b02.jpg", "img_caption": ["Figure 11: More empirical study on low-rankness and local linearity on more prompts and models trained with flow-matching objectives. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "50aOEfb2km/tmp/3183c9897b34a54b1c5ff019c8a4b590a3844f96797337b1a5feb33d7e4f8664.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 12: (Left) Numerical rank of different jacobian $_{J}$ at different timestep $t$ . (Right) Frobenius norm of different jacobian $_{J}$ at different timestep $t$ ", "page_idx": 19}, {"type": "text", "text": "D.3 Comparison for Low-rankness & Local Linearity for Different Manifold ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This section is an extension of Section 3.1. We study the low rankness and local linearity of more mappings between spaces of diffusion models. The sampling process of diffusion model involved the following space: $\\mathbf{\\mathcal{x}}_{t}\\in\\mathcal{X}_{t}$ , $\\hat{\\pmb{x}}_{0,t}\\in\\mathcal{X}_{0,t}$ , $\\pmb{h}_{t}\\in\\mathcal{H}_{t}$ , $\\epsilon_{t}\\in\\mathcal{E}_{t}$ , where $\\mathcal{H}_{t}$ is the $_\\mathrm{h}$ -space of U-Net\u2019s bottleneck feature space [29] and $\\mathcal{E}_{t}$ is the predict noise space. First, we explore the rank ratio of Jacobian $J_{\\theta,t}$ and Frobenius norm $||J_{\\theta,t}||_{F}$ for: \u2202ht, \u2202\u03f5t, \u2202x\u02c60,t, \u2202\u03f5t, \u2202x\u02c60,t. We use DDPM with U-Net architecture, trained on CIFAR-10 dataset, and other experiment settings are the same as Appendix D.1, results are shown in Figure 12. The conclusion could be summarized as : ", "page_idx": 19}, {"type": "text", "text": "$\\frac{\\partial h_{t}}{\\partial\\mathbf{x}_{t}},\\frac{\\partial\\epsilon_{t}}{\\partial h_{t}},\\frac{\\partial\\hat{\\mathbf{x}}_{0,t}}{\\partial h_{t}},\\frac{\\partial\\hat{\\mathbf{x}}_{0,t}}{\\partial\\mathbf{x}_{t}}$ are low rank jacobian when $t\\;\\in\\;[0.2,0.7]$ . As shown in the left of   \nFigure 12, rank ratio for \u2202ht, \u2202\u03f5t, \u2202x\u02c60,t, \u2202x\u02c60,t is less than 0.1. It should be noted that: $\\widetilde{\\mathbf{rank}}(\\frac{\\partial\\pmb{\\epsilon}_{t}}{\\partial\\pmb{x}_{t}})\\geq d-\\widetilde{\\mathbf{rank}}(\\frac{\\partial\\hat{\\pmb{x}}_{0,t}}{\\partial\\pmb{x}_{t}})$ (\u2202x\u02c60,t). This is because $\\widetilde{\\mathbf{rank}}(\\frac{\\sqrt{1-\\alpha_{t}}}{\\sqrt{\\alpha_{t}}}\\frac{\\partial\\epsilon_{t}}{\\partial x_{t}})\\geq\\widetilde{\\mathbf{rank}}(\\frac{1}{\\sqrt{\\alpha_{t}}}I_{d})-\\widetilde{\\mathbf{rank}}(\\frac{\\partial\\hat{\\pmb{x}}_{0,t}}{\\partial x_{t}}).$ Therefore, \u2202\u03f5t is high rank when \u2202x\u02c60,t is low rank. ", "page_idx": 19}, {"type": "image", "img_path": "50aOEfb2km/tmp/ea4c0bd94fc0cb1b78bda7de3d8c86b07e5d4a0b0158c29e59c5d31537815980.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 13: (Left, Middle) Cosine similarity and norm ration of different mappings with respect to $\\lambda$ . (Right) Symmetric property of $\\hat{\\frac{\\partial\\mathbf{x}_{0,t}^{\\mathrm{~\\,~}}}{\\partial\\mathbf{x}_{t}}}$ with respect to timestep $t$ . ", "page_idx": 20}, {"type": "text", "text": "$\\widetilde{\\mathbf{rank}}(\\frac{\\partial\\hat{\\mathbf{x}}_{0,t}}{\\partial h_{t}})=\\widetilde{\\mathbf{rank}}(\\frac{\\partial\\hat{\\mathbf{x}}_{0,t}}{\\partial\\mathbf{x}_{t}})$ This is because $\\hat{\\pmb{x}}_{0,t}=\\frac{\\pmb{x}_{t}-\\sqrt{1-\\alpha_{t}}\\pmb{\\epsilon}_{\\theta}(\\pmb{x}_{t},t)}{\\sqrt{\\alpha_{t}}}$ and $\\frac{\\partial\\pmb{x}_{t}}{\\partial\\pmb{h}_{t}}=0$ \u2022 When $\\pmb{x}_{t}$ fixed, $\\hat{\\pmb{x}}_{0,t},\\pmb{\\epsilon}_{t}$ will change little when changing $h_{t}$ . As shown in the right of Figure 12, $||\\frac{\\partial\\hat{\\pmb{x}}_{0,t}}{\\partial{\\pmb h}_{t}}||_{F}\\ll||\\frac{\\partial\\hat{\\pmb{x}}_{0,t}}{\\partial{\\pmb x}_{t}}||_{F}$ $\\frac{\\partial\\pmb{\\epsilon}_{t}}{\\partial\\pmb{h}_{t}}\\ll\\frac{\\partial\\pmb{\\epsilon}_{t}}{\\partial\\pmb{x}_{t}}$ \u2202\u2202\u03f5xt. This means when xt fixed, x\u02c60,t, \u03f5t will change little when changing $h_{t}$ . ", "page_idx": 20}, {"type": "text", "text": "Then, we also study the linearity of $h_{t}$ and $\\hat{\\pmb{x}}_{0,t}$ given $\\pmb{x}_{t}$ , using DDPM with U-Net architecture trained on CIFAR-10 dataset. We change the step size $\\lambda$ defined in Equation (4). Results are shown in Figure 13, both $h_{t}$ and $\\hat{\\pmb x}_{0,t}$ have good linearity with respect to $\\mathbf{\\nabla}x_{t}.$ .. ", "page_idx": 20}, {"type": "text", "text": "In Theorem 1, the jacobian $\\nabla_{\\pmb{x}_{t}}\\mathbb{E}[\\pmb{x}_{0}|\\pmb{x}_{t}]$ is a symmetric matrix. Therefore, we also verify the symmetry of the jacobian over the PMP $J_{\\theta,t}$ . We use DDPM with U-Net architecture trained on CIFAR-10 dataset. At different timestep $t$ , we measure $||J_{\\theta,t}-J_{\\theta,t}^{\\top}||_{F}$ . Results are shown on the right of Figure 13. $J_{\\theta,t}$ has good symmetric property when $t<0.1$ and $t\\in[0.6,0.7]$ . Additionally, $J_{\\theta,t}$ is low rank when $t\\in[\\bar{0.6},0.7\\bar{]}$ . So $J_{\\theta,t}$ aligned with Theorem $1\\,t\\in[0.6,0.7]$ . ", "page_idx": 20}, {"type": "text", "text": "To the end, we want to based on the experiments in Figure 12 and Figure 13 to select the best space for out image editing method. \u2202\u2202\u03f5xt is the high-rank matrix, not suitable for efficiently estimate the nullspace; \u2202h and \u2202\u2202x\u02c6h0,t has too small Frobenius norm to edit the image. Therefore, only $\\frac{\\partial h_{t}}{\\partial{\\pmb x}_{t}}$ and $\\frac{\\partial\\hat{\\mathbf{x}}_{0,t}}{\\partial\\mathbf{x}_{t}}$ are low-rank and linear for image editing. What\u2019s more, $h_{t}$ space is restricted to UNet architecture, but the property of th e \u2202\u2202x\u02c6x0,t does not depend on the UNet architecture and is verified in diffusion models using transformer architectures. Additionally, we could only apply masks on $\\hat{\\pmb{x}}_{0,t}$ but cannot on $h_{t}$ . Therefore, the PMP ${{f}_{\\theta,t}}$ is the best mapping for image editing. ", "page_idx": 20}, {"type": "text", "text": "E Extra Details of LOCO Edit and T-LOCO Edit ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 Generalized Power Method ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The Generalized Power Method [34, 30] for calculating the op- $\\cdot t$ singular vectors of the Jacobian is summarized in Algorithm 2. It efficiently computes the top- $k$ singular values and singular vectors of the Jacobian with a randomly initialized orthonormal $V\\stackrel{\\star}{\\in}\\mathbb{R}^{d\\times k}$ . ", "page_idx": 20}, {"type": "text", "text": "E.2 Unsupervised T-LOCO Edit ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The overall method for DeepFloyd is summarized in Algorithm 3. For T2I diffusion models in the latent space such as Stable Diffusion and Latent Consistency Model, at time $t$ , we additionally decode $\\hat{z}_{0}$ into the image space $\\scriptstyle{\\hat{x}}_{0}$ to enable masking and nullspace projection. The editing is still in the space of $\\mathscr{z}_{t}$ . ", "page_idx": 20}, {"type": "text", "text": "1: Input: $\\pmb{f}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ , $\\pmb{x}\\in\\mathbb{R}^{d}$ and $V\\in\\mathbb{R}^{d\\times k}$   \n2: Output: $\\big(U,\\Sigma,V^{\\top}\\big)-k$ top singular values and vectors of the Jacobian $\\frac{\\partial f}{\\partial\\mathbf{\\boldsymbol{x}}}$   \n3: ${\\pmb y}\\gets f({\\pmb x})$   \n4: if $\\boldsymbol{V}$ is empty then   \n5: $V\\leftarrow\\mathrm{i.i.d}$ . standard Gaussian samples   \n6: end if   \n7: $Q,R\\gets\\mathrm{QR}(V)$ \u25b7Reduced QR decomposition   \n8: $V\\leftarrow Q$ $\\triangleright$ Ensures $V^{\\bar{\\top}}V=I$   \n9: while stopping criteria do   \n10: $U\\leftarrow{\\frac{\\bar{\\partial}f(\\pmb{x}+a V)}{\\partial a}}$ at $a=0$ \u25b7Batch forward   \n11: V\u02c6 \u2190 \u2202 U \u22a4y   \n\u2202x   \n12: $V,\\Sigma^{2},R\\gets\\mathrm{SVD}(\\hat{V})$ \u25b7Reduced SVD   \n13: end while   \n14: Orthonormalize $\\b{U}$ ", "page_idx": 21}, {"type": "text", "text": "Algorithm 3 Unsupervised T-LOCO Edit for T2I diffusion models ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1: Input: Random noise $x_{T}$ , the mask $\\Omega$ , edit timestep $t$ , pretrained diffusion model $\\epsilon_{\\theta}$ , editing scale $\\lambda$ , noise   \nscheduler $\\alpha_{t},\\sigma_{t}$ , selected semantic index $k$ , nullspace approximate rank $r$ , original prompt $c_{o}$ , null prompt   \n$c_{n}$ , classifier free guidance scale $s$ .   \n2: Output: Edited image $\\pmb{x}_{0}^{\\prime}$ ,   \n3: $\\begin{array}{r}{\\mathbf{\\boldsymbol{x}}_{t}\\gets\\mathtt{D D I M}(\\boldsymbol{x}_{T},1,t,\\epsilon_{\\theta}(\\boldsymbol{x}_{T},t,c_{n})+s(\\epsilon_{\\theta}(\\boldsymbol{x}_{T},t,c_{o})-\\epsilon_{\\theta}(\\boldsymbol{x}_{T},t,c_{n})))}\\end{array}$   \n4: $\\hat{\\pmb{x}}_{0,t}\\gets f_{\\pmb{\\theta},t}^{o}(\\pmb{x}_{t})$   \n5: Masking by $\\tilde{x}_{0,t}\\gets\\mathcal{P}_{\\Omega}(\\hat{\\pmb{x}}_{0,t})$ and $\\bar{\\pmb{x}}_{0,t}\\leftarrow\\hat{\\pmb{x}}_{0,t}-\\tilde{\\pmb{x}}_{0,t}$ \u25b7Use the mask for local image editing   \n6: The top- $k$ SVD $(\\tilde{U}_{t,k},\\tilde{\\Sigma}_{t,k},\\tilde{V}_{t,k})$ of $\\tilde{J}_{\\theta,t}=\\frac{\\partial\\tilde{x}_{0,t}}{\\partial{\\pmb x}_{t}}\\circ$ \u2202\u2202x\u02dcx0,t \u25b7Efficiently computed via generalized power method   \n7: The top- $^{r}$ SVD $(\\bar{U}_{t,r},\\bar{\\Sigma}_{t,r},\\bar{V}_{t,r})$ of $\\bar{J}_{\\theta,t}=\\frac{\\partial\\bar{{\\pmb x}}_{0,t}}{\\partial{\\pmb x}_{t}}\\circ$ \u2202\u2202x\u00afx0,t \u25b7Efficiently computed via generalized power method   \n8: Pick direction $v\\leftarrow\\tilde{V}_{t,k}[:,i]$ $\\triangleright$ Pick the $i^{t h}$ singular vector for editing within the mask $\\Omega$   \n9: Compute $\\boldsymbol{v}_{p}\\gets(\\boldsymbol{I}-\\bar{V}_{t,r}\\bar{V}_{t,r}^{\\top})\\cdot\\boldsymbol{v}$ $\\triangleright$ Nullspace projection for editing within the mask $\\Omega$   \n10: vp \u2190\u2225vpp\u22252 \u25b7Normalize the editing direction   \n11: $\\pmb{x}_{t}^{\\prime}\\leftarrow\\pmb{x}_{t}+\\lambda\\pmb{v}_{p}$   \n12: x\u20320 \u2190DDIM(x\u2032t, t, 0, \u03f5\u03b8(xt, t, cn) + s(\u03f5\u03b8(xt, t, co) \u2212\u03f5\u03b8(xt, t, cn))) ", "page_idx": 21}, {"type": "text", "text": "E.3 Text-suprvised T-LOCO Edit ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Before introducing the algorithm, we define: ", "page_idx": 21}, {"type": "equation", "text": "$$\nf_{\\theta,t}^{o}(x_{t})=\\frac{x_{t}-\\alpha_{t}\\sigma_{t}(\\epsilon_{\\theta}(x_{t},t,c_{n})+s(\\epsilon_{\\theta}(x_{t},t,c_{o})-\\epsilon_{\\theta}(x_{t},t,c_{n})))}{\\alpha_{t}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\nf_{\\theta,t}^{e}(x_{t})=f_{\\theta,t}^{o}(x_{t})+\\frac{m(\\epsilon_{\\theta}(x_{t},t,c_{e})-\\epsilon_{\\theta}(x_{t},t,c_{n})))}{\\alpha_{t}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "to be the posterior mean predictors when using classifier-free guidance on the original prompt $c_{o}$ , and both the original prompt $c_{o}$ and the edit prompt $c_{e}$ accordingly. ", "page_idx": 21}, {"type": "text", "text": "Algorithm. The overall method for DeepFloyd is summarized in Algorithm 4. For T2I diffusion models in the latent space such as Stable Diffusion and Latent Consistency Model, at time $t$ , we additionally decode $\\hat{z}_{0}$ into the image space $\\pmb{\\hat{x}}_{0}$ to enable masking and nullspace projection. The editing is in the space of $\\mathscr{z}_{t}$ for Stable Diffusion and Latent Consistency Model. The proposed method is not proposed as an approach beating other T2I editing methods, but as a way to both understand semantic correspondences in the low-rank subspaces of T2I diffusion models and utilize subspaces for semantic control in a more interpretable way. We hope to inspire and open up directions in understanding T2I diffusion models and utilize the understanding in versatile applications. ", "page_idx": 21}, {"type": "text", "text": "Algorithm 4 Text-supervised T-LOCO Edit for T2I diffusion models ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1: Input: Random noise $x_{T}$ , the mask $\\Omega.$ \u201e edit timestep $t$ , pretrained diffusion model $\\epsilon_{\\theta}$ , editing scale $\\lambda$ , noise   \nscheduler $\\alpha_{t},\\sigma_{t}$ , selected semantic index $k$ , nullspace approximate rank $r$ , original prompt $c_{o}$ , edit prompt   \n$c_{e}$ , null prompt $c_{n}$ , classifier free guidance scale $s$ .   \n2: Output: Edited image $\\pmb{x}_{0}^{\\prime}$ ,   \n3: xt \u2190DDIM(xT , 1, t, \u03f5\u03b8(xT , t, cn) + s(\u03f5\u03b8(xT , t, co) \u2212\u03f5\u03b8(xT , t, cn)))   \n4: $\\hat{\\pmb{x}}_{0,t}^{o}\\gets f_{\\pmb{\\theta},t}^{o}(\\pmb{x}_{t})$   \n5: $\\hat{\\pmb{x}}_{0,t}^{e}\\gets f_{\\pmb{\\theta},t}^{e}(\\pmb{x}_{t})$   \n6: $d\\gets\\mathcal{P}_{\\Omega}\\left(\\hat{\\pmb{x}}_{0,t}^{e}-\\hat{\\pmb{x}}_{0,t}^{o}\\right)$   \n7: $\\tilde{x}_{0,t}\\gets\\mathcal{P}_{\\Omega}(\\dot{\\pmb{x}}_{0,t}^{e})$   \n8: $v\\leftarrow\\frac{\\partial(d^{\\top}\\tilde{x}_{0,t})}{\\partial x_{t}}$ \u25b7Get text-supervised editing direction within the mask   \n9: $\\bar{\\pmb{x}}_{0,t}\\leftarrow\\hat{\\pmb{x}}_{0,t}^{o}-\\mathcal{P}_{\\Omega}(\\hat{\\pmb{x}}_{0,t}^{o})$   \n10: The top- $^{r}$ SVD $(\\bar{U}_{t,r},\\bar{\\Sigma}_{t,r},\\bar{V}_{t,r})$ of $\\bar{J}_{\\theta,t}=\\frac{\\partial\\bar{{\\pmb x}}_{0,t}}{\\partial{\\pmb x}_{t}}\\circ$ \u2202\u2202x\u00afx0,t\u25b7Efficiently computed via generalized power method   \n11: $\\boldsymbol{v}_{p}\\gets(\\boldsymbol{I}-\\bar{V}_{t,r}\\bar{V}_{t,r}^{\\top})\\cdot\\boldsymbol{v}$ $\\triangleright$ nullspace projection for editing within the mask   \n12: vp \u2190\u2225vpp\u22252 $\\triangleright$ Normalize the editing direction   \n13: $\\pmb{x}_{t}^{\\prime}\\leftarrow\\pmb{x}_{t}+\\lambda\\pmb{v}_{p}$   \n14: $\\boldsymbol{x}_{0}^{\\prime}\\gets\\mathtt{D D I M}(\\boldsymbol{x}_{t}^{\\prime},t,0,\\epsilon_{\\theta}(x_{t},t,c_{n})+s(\\epsilon_{\\theta}(x_{t},t,c_{o})-\\epsilon_{\\theta}(x_{t},t,c_{n})))$ ", "page_idx": 22}, {"type": "text", "text": "Here, we want to find a specific change direction $\\pmb{v_{p}}$ in the $\\pmb{x}_{t}$ space that can provide target edited images in the space of $\\scriptstyle x_{0}$ by directly moving $\\pmb{x}_{t}$ along $\\pmb{v_{p}}$ : the whole generation is not conditioned on $c_{e}$ at all, except that we utilize $c_{e}$ in finding the editing direction $\\pmb{v_{p}}$ . This is in contrast to the method proposed in [30], where additional semantic information is injected via indirect $\\mathbf{X}_{\\mathrm{~}}$ -space guidance conditioned on the edit prompt at time $t$ . We hope to discover an editing direction that is expressive enough by itself to perform semantic editing. ", "page_idx": 22}, {"type": "text", "text": "Intuition. Let $\\hat{\\pmb x}_{0,t}^{o}$ be the estimated posterior mean conditioned on the original prompt $c_{o}$ , and $\\hat{\\pmb x}_{0,t}^{e}$ be the estimated posterior mean conditioned on both the original prompt $c_{o}$ and the edit prompt $c_{e}$ . Let $J_{\\theta,t}^{o}$ and $J_{\\theta,t}^{e}$ be their Jacobian over the noisy image $\\pmb{x}_{t}$ accordingly. The key intuition inspired by the unconditional cases are: i) the target editing direction $\\pmb{v}$ in the $\\pmb{x}_{t}$ space is homogeneous between the subspaces in $J_{\\theta,t}^{o}$ and $J_{\\theta,t}^{e}$ ; ii) the founded editing direction $\\pmb{v}$ can effectively reside in the direction of a right singular vector for both $J_{\\theta,t}^{o}$ and $J_{\\theta,t}^{e}$ ; iii) $\\hat{\\pmb x}_{0,t}^{e}$ and $\\hat{\\pmb x}_{0,t}^{o}$ are locally linear. ", "page_idx": 22}, {"type": "text", "text": "Define $\\hat{\\pmb x}_{0,t}^{e}-\\hat{\\pmb x}_{0,t}^{o}=\\pmb d$ as the change of estimated posterior mean. Let ${\\cal J}_{\\theta,t}^{e}=U_{t}^{e}{\\cal S}_{t}^{e}{\\cal V}_{t}^{e^{T}}$ , then ${\\pmb v}=\\pm{\\pmb v}_{i}^{e}$ for some $i$ . Besides, we have $\\hat{\\pmb x}_{0,t}^{e}=\\hat{\\pmb x}_{0,t}^{o}+\\lambda^{o}J_{\\pmb\\theta,t}^{o}{\\pmb v}$ and $\\hat{\\pmb x}_{0,t}^{o}=\\hat{\\pmb x}_{0,t}^{e}+\\lambda^{e}J_{\\pmb\\theta,t}^{e}{\\bf v}$ due to homogeneity and linearity. Hence, ${\\pmb d}=-\\lambda^{e}J_{{\\pmb\\theta},t}^{e}{\\pmb v}=\\pm\\lambda^{e}s_{i}^{e}{\\pmb u}_{i}^{e}$ and then $J_{\\theta,t}^{e^{T}}d=\\pm\\lambda^{e}s_{i}^{e}s_{i}^{e}{\\pmb v}_{i}^{e}=$ $\\pm\\lambda^{e}s_{i}^{e}s_{i}^{e}\\pmb{v}$ , which is along the desired direction $\\pmb{v}$ . And this $\\pmb{v}$ identified through the subspace in $J_{\\theta,t}^{e}$ can be effectively transferred in $J_{\\theta,t}^{o}$ for controlling the editing of target semantics. We further apply nullspace projection based on $J_{\\theta,t}^{o}$ to obtain the final editing direction $\\pmb{v_{p}}$ . ", "page_idx": 22}, {"type": "text", "text": "F Proofs in Section 4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.1 Proofs of Lemma 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . Under the Assumption 1, we could calculate the noised distribution $p_{t}(\\pmb{x}_{t})$ at any timestep $t$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle p_{t}(\\pmb{x}_{t})=\\frac{1}{K}\\sum_{k=1}^{K}p_{t}(\\pmb{x}_{t}|^{\\ast}\\pmb{x}_{0}\\mathrm{~belongs~to~class~}k^{\\ast})}\\\\ {\\displaystyle=\\frac{1}{K}\\sum_{k=1}^{K}\\int p_{t}(\\pmb{x}_{t}|\\pmb{x}_{0}=M_{k}\\pmb{a}_{k},\\\"\\pmb{x}_{0}\\mathrm{~belongs~to~class~}k^{\\ast})\\mathcal{N}(\\pmb{a}_{k};\\pmb{0},\\pmb{I}_{r_{k}})d\\pmb{a}_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Because $a_{k}\\,\\sim\\mathcal{N}(\\mathbf{0},I_{r_{k}})$ , $p_{t}(\\pmb{x}_{t}|\\pmb{x}_{0}\\,=\\,M_{k}\\pmb{a}_{k},\\,\\\"\\,\\pmb{x}_{0}$ belongs to class $k^{\\ast\\ast})\\,\\sim\\,{\\mathcal N}(\\sqrt{\\alpha_{t}}M_{k}{\\pmb a}_{k},(1\\,-\\,$ $\\alpha_{t})I_{d})$ . From the relationship between conditional Gaussian distribution and marginal Gaussian distribution, it is easy to show that $p_{t}(\\pmb{x}_{t}|^{\\gamma}\\pmb{x}_{0}$ belongs to class $k^{\\ y_{3}}\\sim\\mathcal{N}(\\mathbf{0},\\alpha_{t}M_{k}M_{k}^{\\top}+(1-\\alpha_{t})I_{d})$ Then, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\np_{t}(\\pmb{x}_{t})=\\frac{1}{K}\\sum_{k=1}^{K}\\mathcal{N}(\\pmb{0},\\alpha_{t}M_{k}\\pmb{M}_{k}^{\\top}+(1-\\alpha_{t})\\pmb{I}_{d}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, we compute the score function as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x_{t}}\\mathrm{log}p_{t}(x_{t})=\\frac{\\nabla_{x_{t}}p_{t}(x_{t})}{p_{t}(x_{t})}}\\\\ &{\\qquad\\qquad=\\frac{\\displaystyle\\sum_{k=1}^{K}\\!\\mathcal{N}(\\mathbf{0},\\alpha_{t}M_{k}M_{k}^{\\top}+(1-\\alpha_{t})I_{d})\\left(-\\frac{1}{1-\\alpha_{t}}x_{t}+\\frac{\\alpha_{t}}{1-\\alpha_{t}}M_{k}M_{k}^{\\top}x_{t}\\right)}{\\displaystyle\\sum_{k=1}^{K}\\!\\mathcal{N}(\\mathbf{0},\\alpha_{t}M_{k}M_{k}^{\\top}+(1-\\alpha_{t})I_{d})}}\\\\ &{\\qquad\\qquad=-\\frac{1}{1-\\alpha_{t}}x_{t}+\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\frac{\\displaystyle\\sum_{k=1}^{K}\\!\\mathcal{N}(\\mathbf{0},\\alpha_{t}M_{k}M_{k}^{\\top}+(1-\\alpha_{t})I_{d})M_{k}M_{k}^{\\top}x_{t}}{\\displaystyle\\sum_{k=1}^{K}\\!\\mathcal{N}(\\mathbf{0},\\alpha_{t}M_{k}M_{k}^{\\top}+(1-\\alpha_{t})I_{d})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Based on Tweedie\u2019s formula [45, 82], the relationship between the score function and posterior is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\pmb x}_{0}|{\\pmb x}_{t}]=\\frac{{\\pmb x}_{t}+(1-\\alpha_{t})\\nabla_{{\\pmb x}_{t}}\\mathrm{log}p_{t}({\\pmb x}_{t})}{\\sqrt{\\alpha_{t}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, the posterior mean is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathbf{x}_{0}|\\mathbf{x}_{l}]=\\sqrt{\\alpha_{l}}\\frac{\\sum_{k=1}^{K}N(0,\\alpha_{l}M_{k}M_{l}^{\\top}+(1-\\alpha_{l})I_{d})M_{k}M_{l}^{\\top}x_{t}}{\\sum_{k=1}^{K}N(0,\\alpha_{l}M_{k}M_{k}^{\\top}+(1-\\alpha_{l})I_{d})}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sum_{k=1}^{K}\\exp\\left(-\\frac{1}{2}x_{l}^{\\top}\\left(\\alpha_{l}M_{k}M_{k}^{\\top}+(1-\\alpha_{l})I_{d}\\right)^{-1}x_{t}\\right)M_{k}M_{k}^{\\top}x_{t}}\\\\ &{=\\sqrt{\\alpha_{l}}\\frac{\\sum_{k=1}^{K}\\exp\\left(-\\frac{1}{2}x_{l}^{\\top}\\left(\\alpha_{l}M_{k}M_{k}^{\\top}+(1-\\alpha_{l})I_{d}\\right)^{-1}x_{t}\\right)}{\\sum_{k=1}^{K}\\exp\\left(-\\frac{1}{2}x_{l}^{\\top}\\left(\\alpha_{l}M_{k}M_{k}^{\\top}+(1-\\alpha_{l})I_{d}\\right)^{-1}x_{t}\\right)}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sum_{k=1}^{K}\\exp\\left(-\\frac{1}{2(1-\\alpha_{l})}\\left(\\|x_{l}\\|^{2}-\\alpha_{l}\\|M_{k}^{\\top}x_{t}\\|^{2}\\right)\\right)M_{k}M_{k}^{\\top}x_{t}}\\\\ &{=\\sqrt{\\alpha_{l}}\\frac{\\sum_{k=1}^{K}\\exp\\left(-\\frac{1}{2}(1-\\alpha_{l})\\left(\\|x_{l}\\|^{2}-\\alpha_{l}\\|M_{k}^{\\top}x_{t}\\|^{2}\\right)\\right)}{\\sum_{k=1}^{K}\\exp\\left(-\\frac{1}{2}(1-\\alpha_{l})\\left(\\|x_{l}\\|^{2}-\\alpha_{l}\\|M_{k}^{\\top}x_{t}\\|^{2}\\right)\\right)}}\\\\ &{=\\sqrt{\\alpha_{l}}\\frac{\\sum_{k=1}^{K}\\exp\\left(\\frac{\\alpha_{l}}{2 \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the third equation is obtained by Woodbury formula [83] $(\\alpha_{t}M_{k}M_{k}^{\\top}+(1-\\alpha_{t})I_{d})^{-1}=$ $\\frac{1}{1-\\alpha_{t}}\\left(I_{d}-\\alpha_{t}M_{k}M_{k}^{\\top}\\right).$ \u2294\u2293 ", "page_idx": 23}, {"type": "text", "text": "F.2 Proofs of Theorem 1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma 2. The jacobian of the poster mean is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x_{t}}\\mathbb{E}\\left[x_{0}|x_{t}\\right]=\\sqrt{\\alpha_{t}}\\displaystyle\\sum_{k=1}^{K}\\omega_{k}(x_{t})M_{k}M_{k}^{\\top}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{\\alpha_{t}\\sqrt{\\alpha_{t}}}{(1-\\alpha_{t})}\\displaystyle\\sum_{k=1}^{K}\\omega_{k}(x_{t})M_{k}M_{k}^{\\top}x_{t}x_{t}^{\\top}M_{k}M_{k}^{\\top}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle\\frac{B\\cdot=}{B\\cdot=}}\\\\ &{\\displaystyle-\\,\\frac{\\alpha_{t}\\sqrt{\\alpha_{t}}}{(1-\\alpha_{t})}\\displaystyle\\sum_{k=1}^{K}\\omega_{k}(x_{t})M_{k}M_{k}^{\\top}\\Bigg)\\,x_{t}x_{t}^{\\top}\\left(\\displaystyle\\sum_{k=1}^{K}\\omega_{k}(x_{t})M_{k}M_{k}^{\\top}\\right)^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$\\omega_{k}(\\pmb{x}_{t}):=\\frac{\\exp\\left(\\frac{\\alpha_{t}}{2\\left(1-\\alpha_{t}\\right)}\\|\\pmb{M}_{k}^{\\top}\\pmb{x}_{t}\\|^{2}\\right)}{\\sum_{l=1}^{K}\\exp\\left(\\frac{\\alpha_{t}}{2(1-\\alpha_{t})}\\|\\pmb{M}_{l}^{\\top}\\pmb{x}\\|^{2}\\right)}$ ", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma 2. Let $\\omega_{k}(\\pmb{x}_{t}):=\\frac{\\exp\\left(\\frac{\\alpha_{t}}{2\\left(1-\\alpha_{t}\\right)}\\|\\pmb{M}_{k}^{\\top}\\pmb{x}_{t}\\|^{2}\\right)}{\\sum_{l=1}^{K}\\exp\\left(\\frac{\\alpha_{t}}{2(1-\\alpha_{t})}\\|\\pmb{M}_{l}^{\\top}\\pmb{x}\\|^{2}\\right)}$ , so we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}\\left[{\\pmb x}_{0}|{\\pmb x}_{t}\\right]=\\sqrt{\\alpha_{t}}\\sum_{k=1}^{K}\\omega_{k}({\\pmb x}_{t})M_{k}M_{k}^{\\top}{\\pmb x}_{t}}\\ ~}\\\\ {{\\displaystyle\\nabla_{{\\pmb x}_{t}}\\omega_{k}({\\pmb x}_{t})=\\frac{\\alpha_{t}}{(1-\\alpha_{t})}\\omega_{k}({\\pmb x}_{t})\\left[M_{k}M_{k}^{\\top}{\\pmb x}_{t}-\\sum_{l=1}^{K}\\omega_{l}({\\pmb x}_{t})M_{l}M_{l}^{\\top}{\\pmb x}_{t}\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla_{\\mathbf{x}_{t}}\\mathbb{E}\\left[\\mathbf{x}_{0}|\\mathbf{x}_{t}\\right]=\\sqrt{\\alpha_{t}}\\sum_{k=1}^{K}\\omega_{k}(\\mathbf{x}_{t})M_{k}M_{k}^{\\top}+\\sqrt{\\alpha_{t}}\\sum_{k=1}^{K}\\nabla_{\\mathbf{x}_{t}}\\omega_{k}(\\mathbf{x}_{t})\\mathbf{x}_{t}^{\\top}M_{k}M_{k}^{\\top}}}\\\\ {~~}\\\\ {{\\displaystyle=\\sqrt{\\alpha_{t}}\\sum_{k=1}^{K}\\omega_{k}(\\mathbf{x}_{t})M_{k}M_{k}^{\\top}}}\\\\ {~~}\\\\ {{\\displaystyle+\\left.\\frac{\\alpha_{t}\\sqrt{\\alpha_{t}}}{(1-\\alpha_{t})}\\sum_{k=1}^{K}\\omega_{k}(\\mathbf{x}_{t})M_{k}M_{k}^{\\top}\\mathbf{x}_{t}x_{t}^{\\top}M_{k}M_{k}^{\\top}\\right.}}\\\\ {~}\\\\ {{\\displaystyle\\left.-\\frac{\\alpha_{t}\\sqrt{\\alpha_{t}}}{(1-\\alpha_{t})}\\left(\\sum_{k=1}^{K}\\omega_{k}(\\mathbf{x}_{t})M_{k}M_{k}^{\\top}\\right)x_{t}x_{t}^{\\top}\\left(\\sum_{k=1}^{K}\\omega_{k}(\\mathbf{x}_{t})M_{k}M_{k}^{\\top}\\right)^{\\top}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma 3. Assume second-order partial derivatives of $p_{t}(\\pmb{x}_{t})$ exist for any $\\pmb{x}_{t}$ , then the posterior mean $\\nabla_{\\mathbf{x}_{t}}\\mathbb{E}\\left[\\mathbf{x}_{0}\\vert\\mathbf{x}_{t}\\right]$ satisfied $\\nabla_{\\pmb{x}_{t}}\\mathbb{E}\\left[\\pmb{x}_{0}|\\pmb{x}_{t}\\right]=\\nabla_{\\pmb{x}_{t}}\\mathbb{E}^{\\top}\\left[\\pmb{x}_{0}|\\pmb{x}_{t}\\right]$ . ", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma 3. By taking the gradient of Equation (13) with respect to $\\pmb{x}_{t}$ for both side, because the second-order partial derivatives of $p_{t}(\\pmb{x}_{t})$ exist for any $\\pmb{x}_{t}$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}_{t}}\\mathbb{E}[\\mathbf{x}_{0}|\\mathbf{x}_{t}]=\\frac{I+(1-\\alpha_{t})\\nabla_{x_{t}}^{2}\\mathrm{log}p_{t}(\\mathbf{x}_{t})}{\\sqrt{\\alpha_{t}}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The hessian of $\\log p_{t}(\\pmb{x}_{t})$ is symmetric, so we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\nabla_{x_{t}}\\mathbb{E}^{\\top}[x_{0}|x_{t}]=\\frac{I+(1-\\alpha_{t})\\left(\\nabla_{x_{t}}^{2}\\log p_{t}(x_{t})\\right)^{\\top}}{\\sqrt{\\alpha_{t}}}=\\frac{I+(1-\\alpha_{t})\\nabla_{x_{t}}^{2}\\log p_{t}(x_{t})}{\\sqrt{\\alpha_{t}}}=\\nabla_{x_{t}}\\mathbb{E}[x_{0}|x_{t}].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Notably, the symmetric of $\\nabla_{\\pmb{x}_{t}}\\mathbb{E}[\\pmb{x}_{0}|\\pmb{x}_{t}]$ holds without the Assumption 1. ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem 1. First, let\u2019s prove the low-rankness of the posterior mean. From Lemma 2, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x_{t}}\\mathbb{E}\\left[x_{0}\\middle|x_{t}\\right]=\\sqrt{\\alpha_{t}}A+\\frac{\\alpha_{t}\\sqrt{\\alpha_{t}}}{\\left(1-\\alpha_{t}\\right)}B-\\frac{\\alpha_{t}\\sqrt{\\alpha_{t}}}{\\left(1-\\alpha_{t}\\right)}C}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{k=1}^{K}M_{k}M_{k}^{\\top}\\left(\\sqrt{\\alpha_{t}}A+\\frac{\\alpha_{t}\\sqrt{\\alpha_{t}}}{\\left(1-\\alpha_{t}\\right)}B-\\frac{\\alpha_{t}\\sqrt{\\alpha_{t}}}{\\left(1-\\alpha_{t}\\right)}C\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second equation is obtained due to the fact that $\\begin{array}{r l}{\\sum_{k=1}^{K}M_{k}M_{k}^{\\top}A}&{{}=}\\end{array}$ $\\begin{array}{r}{A,\\sum_{k=1}^{K}M_{k}M_{k}^{\\top}B=B,\\sum_{k=1}^{K}M_{k}M_{k}^{\\top}C=C,}\\end{array}$ . Therefore, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{r a n k}\\left(\\nabla_{x_{t}}\\mathbb{E}\\left[x_{0}|x_{t}\\right]\\right)={r a n k}\\left(\\displaystyle\\sum_{k=1}^{K}{M_{k}M_{k}^{\\top}\\left(\\sqrt{\\alpha_{t}}A+\\frac{\\alpha_{t}\\sqrt{\\alpha_{t}}}{\\left(1-\\alpha_{t}\\right)}B-\\frac{\\alpha_{t}\\sqrt{\\alpha_{t}}}{\\left(1-\\alpha_{t}\\right)}C\\right)}\\right)}\\\\ {\\displaystyle\\leq{r a n k}\\left(\\displaystyle\\sum_{k=1}^{K}{M_{k}M_{k}^{\\top}}\\right)=\\displaystyle\\sum_{k=1}^{K}{r_{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, we prove the linearity: ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\odot:||\\mathbb E[{\\mathbf x}_{0}|{\\mathbf x}_{t}+\\lambda\\Delta{\\mathbf x}]-|\\mathbb E[{\\mathbf x}_{0}|{\\mathbf x}_{t}]-\\lambda\\nabla_{x}\\mathbb E[{\\mathbf x}_{0}|{\\mathbf x}_{t}]|\\Delta{\\mathbf x}||_{2}}\\\\ &{=||\\sqrt{\\alpha_{t}}\\sum_{k=1}^{K}(\\omega_{k}({\\mathbf x}_{t}+\\lambda\\Delta{\\mathbf x})-\\omega_{k}({\\mathbf x}_{t}))\\,M_{k}M_{k}^{\\top}({\\mathbf x}_{t}+\\lambda\\Delta{\\mathbf x})-\\lambda\\displaystyle\\sum_{k=1}^{K}\\nabla_{x_{t}}\\omega_{k}({\\mathbf x}_{t}){\\mathbf x}_{t}^{\\top}M_{k}M_{k}^{\\top}\\Delta{\\mathbf x}|}\\\\ &{=||\\sqrt{\\alpha_{t}}\\displaystyle\\sum_{k=1}^{K}\\left(\\lambda\\nabla_{x_{t}}^{\\top}\\omega_{k}({\\mathbf x}_{t}+\\lambda_{1}\\Delta{\\mathbf x})\\Delta{\\mathbf x}\\right)M_{k}M_{k}^{\\top}({\\mathbf x}_{t}+\\lambda\\Delta{\\mathbf x})-\\lambda\\displaystyle\\sum_{k=1}^{K}\\nabla_{x_{t}}\\omega_{k}({\\mathbf x}_{t}){\\mathbf x}_{t}^{\\top}M_{k}M_{k}^{\\top}\\Delta{\\mathbf x}}\\\\ &{\\le\\lambda\\left(\\displaystyle\\sum_{k=1}^{K}\\sqrt{\\alpha_{t}}\\nabla_{x_{t}}^{\\top}\\omega_{k}({\\mathbf x}_{t}+\\lambda_{1}\\Delta{\\mathbf x})\\Delta{\\mathbf x}||M_{k}^{\\top}({\\mathbf x}_{t}+\\lambda\\Delta{\\mathbf x})||_{2}+x_{t}^{\\top}M_{k}M_{k}^{\\top}\\Delta{\\mathbf x}||\\nabla_{x_{t}}^{\\top}\\omega_{k}({\\mathbf x}_{t})||_{2}\\right)}\\\\ &{\\le\\lambda\\displaystyle\\sum_{k=1}^{K}\\Big(\\sqrt{\\alpha_{t}}||\\nabla_{x_{t}}|\\omega_{k}({\\mathbf x}_{t}+\\lambda_{1}\\Delta{\\mathbf x})||_{2}||M_{k}^{\\top}({\\mathbf x}_{t}+\\lambda\\Delta{\\mathbf x})||_{2}+||\\nabla_{x_{t}}\\omega_{k}({\\mathbf x}_{t})||_{2}||M_{k}^\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "w\u221ahere the first equation plug in the formula of $\\begin{array}{r}{\\nabla_{\\pmb{x}_{t}}\\mathbb{E}\\left[\\pmb{x}_{0}|\\pmb{x}_{t}\\right]\\;=\\;\\sqrt{\\alpha_{t}}\\sum_{k=1}^{K}\\omega_{k}(\\pmb{x}_{t})M_{k}M_{k}^{\\top}\\;+}\\end{array}$ $\\begin{array}{r}{\\sqrt{\\alpha_{t}}\\sum_{k=1}^{K}\\nabla_{\\pmb{x}_{t}}\\omega_{k}(\\pmb{x}_{t})\\pmb{x}_{t}^{\\top}M_{k}\\pmb{M}_{k}^{\\top}}\\end{array}$ and the second equation use the mean value theorem $\\omega_{k}(\\pmb{x}_{t}+$ $\\lambda\\Delta{\\pmb x})-\\omega_{k}({\\pmb x}_{t})=\\lambda\\nabla_{{\\pmb x}_{t}}^{\\top}\\omega_{k}({\\pmb x}_{t}+\\lambda_{1}\\Delta{\\pmb x})\\Delta{\\pmb x},\\lambda_{1}\\in(0,\\lambda).$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(\\exists\\cdot|\\vert\\nabla_{\\pi}\\omega_{i}\\vert\\omega_{\\pi}+\\lambda_{1}\\Delta x)\\vert\\vert_{2}}\\\\ &{=\\frac{\\alpha_{\\mathrm{tr}}}{(1-\\alpha_{\\mathrm{t}})}\\omega_{\\mathrm{t}}|\\vert M_{k}M_{k}^{\\top}(\\alpha_{t}+\\lambda_{1}\\Delta x)-\\displaystyle\\sum_{l=1}^{K}\\omega_{l}M_{l}M_{l}^{\\top}(\\alpha_{t}+\\lambda_{1}\\Delta x)\\vert\\vert_{2}}\\\\ &{\\le\\frac{\\alpha_{\\mathrm{tr}}}{(1-\\alpha_{\\mathrm{t}})^{\\alpha_{\\mathrm{t}}}}\\omega_{\\mathrm{t}}\\left(\\vert\\vert M_{k}^{\\top}x_{\\mathrm{t}}\\vert\\vert_{2}+\\displaystyle\\sum_{l=1}^{K}\\omega_{l}\\vert\\vert M_{l}^{\\top}x_{\\mathrm{t}}\\vert\\vert_{2}+\\lambda_{1}\\vert\\vert M_{k}^{\\top}\\Delta x\\vert\\vert_{2}+\\lambda_{1}\\displaystyle\\sum_{l=1}^{K}\\omega_{l}\\vert\\vert M_{l}^{\\top}\\Delta x\\vert\\vert_{2}\\right)}\\\\ &{\\le\\frac{\\alpha_{\\mathrm{t}}}{(1-\\alpha_{\\mathrm{t}})^{\\alpha_{\\mathrm{t}}}}\\omega_{\\mathrm{t}}\\left(\\vert\\vert M_{k}^{\\top}\\vert\\vert_{K}\\vert\\vert_{K}\\vert\\vert_{\\alpha_{l}}+\\displaystyle\\sum_{l=1}^{K}\\omega_{l}\\vert\\vert M_{l}^{\\top}\\vert\\vert_{F}\\vert\\vert x_{\\mathrm{t}}\\vert\\vert_{2}+\\lambda_{1}\\vert\\vert M_{k}^{\\top}\\vert\\vert_{F}+\\lambda_{1}\\displaystyle\\sum_{l=1}^{K}\\omega_{l}\\vert\\vert M_{l}^{\\top}\\vert\\vert_{F}\\right)}\\\\ &{\\le\\frac{\\alpha_{\\mathrm{t}}}{(1-\\alpha_{\\mathrm{t}})^{\\alpha_{\\mathrm{t}}}}\\omega_{\\mathrm{t}}\\left(r_{k}+\\displaystyle\\sum_{l=1}^{K}\\omega_{l}r_{\\mathrm{t}}\\right)\\Big(\\sqrt{2}\\operatorname*{max}\\{\\vert\\vert\\alpha_{\\mathrm{t}}\\vert\\vert_{2}\\vert_{\\cdot}\\vert\\epsilon\\vert_{2}\\}+\\lambda_{1}\\Big)}\\\\ &{\\le\\frac{\\alpha_{\\mathrm{t}}}{(1-\\alpha_{\\mathrm{t}})^{\\alpha_{\\mathrm{t} \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the third in\u221aequality use the fact that $||\\pmb{x}_{t}||_{2}\\,=\\,||\\sqrt{\\alpha_{t}}\\pmb{x}_{0}+\\sqrt{1-\\alpha_{t}}\\pmb{\\epsilon}||_{2}\\,\\leq\\,||\\sqrt{\\alpha_{t}}\\pmb{x}_{0}||_{2}\\,+$ $||\\sqrt{1-\\alpha_{t}}\\epsilon||_{2}\\leq\\sqrt{2}\\operatorname*{max}\\{||x_{0}||_{2},||\\epsilon||_{2}\\}$ , we simplified $\\omega_{k}(\\mathbf{x}_{t}+\\lambda_{1}\\Delta\\mathbf{x})$ as $\\omega_{k}$ in this prove, and $C_{1}$ defined in the last inequality is independent of $t$ . Similarly, we could prove that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\odot:\\Vert M_{k}M_{k}^{\\top}\\left(\\mathbf{x}_{t}+\\lambda\\Delta\\mathbf{x}\\right)\\Vert_{2}\\le\\underbrace{\\underset{k}{\\operatorname*{max}}r_{k}\\cdot\\left(\\sqrt{2}\\operatorname*{max}\\{\\Vert\\mathbf{x}_{0}\\Vert_{2},\\Vert\\epsilon\\Vert_{2}\\}+\\lambda\\right)}_{C_{2}:=},}\\\\ &{\\odot:\\Vert\\nabla_{\\mathbf{x}_{t}}\\omega_{k}(\\mathbf{x}_{t})\\Vert_{2}\\le\\frac{\\alpha_{t}}{(1-\\alpha_{t})}\\omega_{k}(\\mathbf{x}_{t})\\underbrace{2\\sqrt{2}\\cdot\\underset{k}{\\operatorname*{max}}r_{k}\\cdot\\operatorname*{max}\\{\\Vert\\mathbf{x}_{0}\\Vert_{2},\\Vert\\epsilon\\Vert_{2}\\}}_{C_{3}==\\infty},}\\\\ &{\\odot:\\Vert M_{k}M_{k}^{\\top}\\mathbf{x}_{t}\\Vert_{2}\\le\\underbrace{\\sqrt{2}\\operatorname*{max}r_{k}\\cdot\\operatorname*{max}\\{\\Vert\\mathbf{x}_{0}\\Vert_{2},\\Vert\\epsilon\\Vert_{2}\\}}_{C_{4}:=\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here, $C_{1}=\\mathcal{O}(\\lambda),C_{2}=\\mathcal{O}(\\lambda),C_{3}=\\mathcal{O}(\\lambda),C_{4}=\\mathcal{O}(\\lambda)$ . After plugin $\\mathcal{O},\\widehat{\\mathfrak{o}},\\widehat{\\mathfrak{o}},\\widehat{\\mathfrak{s}}$ to $\\odot$ , we could obtain: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\vert\\vert\\mathbb{E}\\left[\\pmb{x}_{0}\\middle\\vert\\pmb{x}_{t}+\\lambda\\Delta\\pmb{x}\\right]-\\mathbb{E}\\left[\\pmb{x}_{0}\\middle\\vert\\pmb{x}_{t}\\right]-\\lambda\\nabla_{\\pmb{x}_{t}}\\mathbb{E}[\\pmb{x}_{0}\\middle\\vert\\pmb{x}_{t}]\\Delta\\pmb{x}\\vert\\vert_{2}}\\\\ &{\\le\\lambda\\sqrt{\\alpha_{t}}\\displaystyle\\sum_{k=1}^{K}\\frac{\\alpha_{t}}{\\left(1-\\alpha_{t}\\right)}\\omega_{k}(\\pmb{x}_{t}+\\lambda_{1}\\Delta\\pmb{x})C_{1}C_{2}+\\lambda\\displaystyle\\sum_{k=1}^{K}\\frac{\\alpha_{t}}{\\left(1-\\alpha_{t}\\right)}\\omega_{k}(\\pmb{x}_{t})C_{3}C_{4}}\\\\ &{=\\lambda\\displaystyle\\frac{\\alpha_{t}}{\\left(1-\\alpha_{t}\\right)}\\mathcal{O}(\\lambda)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, let\u2019s prove the property of the left singular vector of $\\nabla_{\\mathbf{x}_{t}}\\mathbb{E}\\left[\\mathbf{x}_{0}\\vert\\mathbf{x}_{t}\\right]$ : ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "From Lemma 3, the eigenvalue decomposition of $\\nabla_{\\mathbf{x}_{t}}\\mathbb{E}\\left[\\mathbf{x}_{0}\\vert\\mathbf{x}_{t}\\right]$ could be written as $\\nabla_{\\mathbf{x}_{t}}\\mathbb{E}\\left[\\pmb{x}_{0}|\\pmb{x}_{t}\\right]=$ $U_{t}\\Lambda_{t}U_{t}^{\\top}$ , where $\\boldsymbol{\\Lambda}_{t}=\\operatorname{diag}(\\lambda_{t,1},\\dots,\\lambda_{t,r},\\dots,0)$ , and the relation between eigenvalue decomposition and singular value decomposition of $\\nabla_{\\pmb{x}_{t}}\\mathbb{E}\\left[\\pmb{\\mathscr{x}}_{0}\\vert\\pmb{x}_{t}\\right]$ could be summarized as for all $i\\in[r]$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sigma_{t,i}=\\left|\\lambda_{t,i}\\right|,\\;\\;{\\pmb v}_{i}=\\mathrm{sign}\\left(\\lambda_{t,i}\\right){\\pmb u}_{i},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where sign $(\\cdot)$ is the sign function. Therefore, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U_{t,1}U_{t,1}^{\\top}=V_{t,1}V_{t,1}^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "given $V_{t,1}:=\\left[\\boldsymbol{v}_{t,1},\\boldsymbol{v}_{t,2},\\ldots,\\boldsymbol{v}_{t,r}\\right]$ . From Lemma 2, we define: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}_{t}}\\mathbb{E}\\left[x_{0}\\middle|x_{t}\\right]=\\sqrt{\\alpha_{t}}\\sum_{k=1}^{K}\\omega_{k}(\\mathbf{x}_{t})M_{k}M_{k}^{\\top}+\\sqrt{\\alpha_{t}}\\sum_{k=1}^{K}\\nabla_{x_{t}}\\omega_{k}(x_{t})x_{t}^{\\top}M_{k}M_{k}^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From the full singular value decomposition of $\\nabla_{\\mathbf{x}_{t}}\\mathbb{E}\\left[\\mathbf{x}_{0}\\vert\\mathbf{x}_{t}\\right]$ and $\\begin{array}{r}{\\sqrt{\\alpha_{t}}\\sum_{k=1}^{K}\\omega_{k}({\\pmb x}_{t})M_{k}M_{k}^{\\top}}\\end{array}$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x_{t}}\\mathbb{E}\\left[x_{0}|x_{t}\\right]=\\left[U_{t,1}\\quad U_{t,2}\\right]\\left[\\Sigma_{t,1}\\quad\\mathbf{0}\\right]\\left[{V}_{t,1}\\right]^{\\top},}\\\\ &{\\sqrt{\\alpha_{t}}\\displaystyle\\sum_{k=1}^{K}\\omega_{k}(x_{t})M_{k}M_{k}^{\\top}=\\left[\\hat{U}_{t,1}\\quad\\hat{U}_{t,2}\\right]\\left[\\hat{\\Sigma}_{t,1}\\quad\\mathbf{0}\\right]\\left[\\hat{V}_{t,1}\\right]^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\uptau}_{t,1}=\\left[\\begin{array}{l l l}{\\boldsymbol{\\sigma}_{t,1}}&&\\\\ &{\\ddots}&\\\\ &&{\\boldsymbol{\\sigma}_{t,r}}\\end{array}\\right],\\boldsymbol{\\upSigma}_{t,2}=\\left[\\begin{array}{l l l}{\\boldsymbol{\\sigma}_{t,r+1}}&&\\\\ &{\\ddots}&\\\\ &&{\\boldsymbol{\\sigma}_{t,n}}\\end{array}\\right],}\\\\ &{\\boldsymbol{\\uptau}_{t,1}=\\left[\\begin{array}{l l l}{\\boldsymbol{\\hat{\\sigma}}_{t,1}}&&\\\\ &{\\ddots}&\\\\ &&{\\boldsymbol{\\hat{\\sigma}}_{t,r}}\\end{array}\\right],\\boldsymbol{\\hat{\\Sigma}}_{t,2}=\\left[\\begin{array}{l l l}{\\boldsymbol{\\hat{\\sigma}}_{t,r+1}}&&\\\\ &{\\ddots}&\\\\ &&{\\boldsymbol{\\hat{\\sigma}}_{t,n}}\\end{array}\\right]}\\\\ &{\\boldsymbol{\\up t}_{t,1}\\ge\\boldsymbol{\\sigma}_{t,2}\\ge\\dots\\ge\\boldsymbol{\\sigma}_{t,r}\\ge\\dots\\ge\\boldsymbol{\\sigma}_{t,d},\\quad\\boldsymbol{\\hat{\\sigma}}_{t,1}\\ge\\boldsymbol{\\hat{\\sigma}}_{t,2}\\ge\\dots\\ge\\boldsymbol{\\hat{\\sigma}}_{t,r}\\ge\\dots\\ge\\boldsymbol{\\hat{\\sigma}}_{t,d},\\,\\,r=\\displaystyle\\sum_{k=1}^{K}\\boldsymbol{r}_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "From Equation (15), we know that $\\sigma_{t,r+1}=.\\ldots=\\sigma_{t,d}=0$ . It is easy to show that: ", "page_idx": 27}, {"type": "equation", "text": "$$\nM:=\\hat{V}_{t,1}=\\left[M_{s_{1}}\\quad M_{s_{2}}\\quad.\\dots\\quad M_{s_{K}}\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\{s_{1},s_{2},...\\,,s_{K}\\}\\,=\\,\\{1,2,...\\,,K\\}$ satisfied $\\omega_{s_{1}}({\\pmb x}_{t})\\,\\geq\\,\\omega_{s_{2}}({\\pmb x}_{t})\\,\\geq\\,.\\,.\\,\\geq\\,\\omega_{s_{K}}({\\pmb x}_{t})$ . And $\\hat{\\sigma}_{t,r}=\\sqrt{\\alpha_{t}}\\omega_{s_{K}}({\\pmb x}_{t})=\\sqrt{\\alpha_{t}}\\operatorname*{min}_{k}\\omega_{k}({\\pmb x}_{t})$ . Based on the Davis-Kahan theorem [84], we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\vert\\vert\\left(I_{d}-V_{t,1}V_{t,1}^{\\top}\\right)M\\vert\\vert_{F}\\leq\\frac{\\vert\\vert\\Delta_{t}\\vert\\vert_{F}}{\\operatorname*{min}_{1\\leq i\\leq r,r+1\\leq j\\leq d}\\vert\\hat{\\sigma}_{t,i}-\\sigma_{t,j}\\vert}}}\\\\ &{=\\frac{\\vert\\vert\\sqrt{\\alpha_{t}}\\sum_{k=1}^{K}\\nabla_{\\alpha_{t}}\\omega_{k}\\left(x_{t}\\right)x_{t}^{\\top}M_{k}M_{k}^{\\top}\\vert\\vert_{F}}{\\sqrt{\\alpha_{t}}\\operatorname*{min}_{k}\\omega_{k}\\left(x_{t}\\right)}}\\\\ &{\\leq\\frac{\\sum_{k=1}^{K}\\vert\\vert\\nabla_{\\alpha_{t}}\\omega_{k}\\left(x_{t}\\right)\\vert\\vert_{F}\\vert\\vert x_{t}^{\\top}M_{k}M_{k}^{\\top}\\vert\\vert_{F}}{\\operatorname*{min}_{k}\\omega_{k}\\left(x_{t}\\right)}}\\\\ &{=\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\frac{C_{3}C_{4}}{\\operatorname*{min}_{k}\\omega_{k}\\left(x_{t}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Because $\\operatorname*{lim}_{t\\to1}\\operatorname*{min}_{k}\\omega_{k}(\\pmb{x}_{t})=\\frac{1}{K},\\operatorname*{lim}_{t\\to1}\\frac{\\alpha_{t}}{1-\\alpha_{t}}=0.$ , so: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow1}\\lvert|\\left(I_{d}-V_{t,1}V_{t,1}^{\\top}\\right)M\\rvert\\rvert_{F}=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "And from Equation (16), we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow1}||\\left(I_{d}-U_{t,1}U_{t,1}^{\\top}\\right)M||_{F}=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "G Image Editing and Evaluation Experiment Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "All the experiments can be conducted with a single A40 GPU having 48G memory. ", "page_idx": 27}, {"type": "text", "text": "G.1 Editing in Unconditional Diffusion Models of Different Datasets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Datasets. We demonstrate the unconditional editing method in various dataset: FFHQ [63], CelebaA-HQ [52], AFHQ [62], Flowers [61], MetFace [85], and LSUN-church [60]. ", "page_idx": 27}, {"type": "text", "text": "Models. Following [30], we use DDPM [1] for CelebaA-HQ and LSUN-church, and DDPM trained with P2 weighting [86] for FFHQ, AFHQ, Flowers, and MetFaces. We download the official pre-trained checkpoints of resolution $256\\times256$ , and keep all model parameters frozen. We use the same linear schedule including 100 DDIM inversion steps [3] as [30]. Further, we apply quanlity boosting after $t=0.2$ as proposed in [87]. ", "page_idx": 28}, {"type": "text", "text": "Edit Time Steps. We empirically choose the edit time step $t$ for different datasets in the range [0.5, 0.8]. In practice, we found time steps within the above range give similar editing results. In most of the experiments, the edit time steps chosen are: 0.5 for FFHQ, 0.6 for CelebaA-HQ and LSUN-church, 0.7 for AFHQ, Flowers, and MetFace. ", "page_idx": 28}, {"type": "text", "text": "Editing Strength. In the empirical study of local linearity, we observed that the local linearity is well-preserved even with a strength of 300. In practice, we choose the edit strength $\\lambda$ in the range of $[-15.0,15.0]$ , where a larger $\\alpha$ leads to stronger semantic editing and a negative $\\alpha$ leads to the change of semantics in the opposite direction. ", "page_idx": 28}, {"type": "text", "text": "G.2 Comparing with Alternative Manifolds and Methods ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Existing Methods We compare with four existing methods: NoiseCLR [23], BlendedDiffusion [24], Pullback [30], and Asyrp [29]. ", "page_idx": 28}, {"type": "text", "text": "Alternative Manifolds. There are two alternative manifolds where similar training-free approaches   \ncan be applied, and each of the alternative involves evaluation of the Jacobians $\\frac{\\bar{\\partial}\\epsilon_{t}}{\\partial h_{t}}$ (equivalently   \n$\\frac{\\partial\\hat{\\pmb{x}}_{0}}{\\partial h_{t}})$ , and $\\frac{\\partial\\epsilon_{t}}{\\partial{\\pmb x}_{t}}$ accordingly. $\\frac{\\partial\\epsilon_{t}}{\\partial h_{t}}$ (or equivalently $\\frac{\\partial\\hat{\\mathbf{x}}_{0,t}}{\\partial h_{t}}$ up to a scale) calculates the Jacobian of the noise residual $\\epsilon_{t}$ with respect to the bottleneck feature of $\\pmb{x}_{t}$ . $\\frac{\\partial\\pmb{\\epsilon}_{t}}{\\partial\\pmb{x}_{t}}$ calculates the Jacobian of the noise residual $\\epsilon_{t}$ with respect to the input $\\pmb{x}_{t}$ .   \nNotably , \u2202\u2202\u03f5ht has hardly notable editing results on images, and hence we present the editing results   \nof \u2202\u2202\u03f5xt. Besides, with masking and nullspace projection, \u2202\u2202 \u03f5xt also leads to hardly notable changes   \non images, thus the final comparison is without masking and nullspace projection. ", "page_idx": 28}, {"type": "text", "text": "Evaluation Dataset Setup. In human evaluation, for each method, we randomly select 15 editing direction on 15 images. Each direction is transferred to 3 other images along both the negative and positive directions, in total 90 transferability testing cases. Learning time and transfer edit time are averaged over 100 examples. LPIPS [64] and SSIM [65] are calculated over 400 images for each method. ", "page_idx": 28}, {"type": "text", "text": "Human Evaluation Metrics. We measure both Local Edit Success Rate and Transfer Success Rate via human evaluation on CelebA-HQ. i) Local Edit Success Rate: The subject will be given the source image with the edited one, if the subject judges only one major feature among {\"eyes\", \"nose\", \"hair\", \"skin\", \"mouth\", \"views\", \"Eyebrows\"} are edited, the subject will respond a success, otherwise a failure. ii) Transfer Success Rate: The subject will be given the source image with the edited one, and another image with the edited one via transferring the editing direction from the source image. The subject will respond a success if the two edited images have the same features changed, otherwise a failure. We calculate the average success rate among all subjects for both Local Edit Success Rate and Transfer Success Rate. Lastly, we have ensured no harmful contents are generated and presented to the human subjects. ", "page_idx": 28}, {"type": "text", "text": "Learning Time. Learning time is a measure of the time it takes to compute local basis(training free approaches), to train an implicit function, or to optimize certain variables that help achieve editing for a specific edit method. ", "page_idx": 28}, {"type": "text", "text": "G.3 Editing in T2I Diffusion Models ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Models. We generalize our method to three types of T2I diffusion models: DeepFloyd [19], Stable Diffusion [4], and Latent Consistency Model [38]. We download the official checkpoints and keep all model parameters frozen. The same scheduling as that in the unconditional models is applied to DeepFloyd and Stable Diffusion, except that no quality boosting is applied. We follow the original schedule for Latent Consistency Model [38] with the number of inference steps set as 4. ", "page_idx": 29}, {"type": "text", "text": "Edit Time Steps. We empirically choose the the edit time step $t$ as 0.75 for DeepFloyd and 0.7 for Stable DIffusin. As for Latent Consistency Model, image editing is performed at the second inference step. ", "page_idx": 29}, {"type": "text", "text": "Editing Strength. For unsupervised image editing, we choose $\\lambda\\in\\left[-5.0,5.0\\right]$ in Stable Diffusion, $\\lambda\\,\\in\\,[-15.0,15.0]$ in DeepFloyd, and $\\lambda\\;\\in\\;[-5.0,5.0]$ in Latent Consistency Model. For textsupervised image editing, we choose $\\lambda\\in[-\\mathrm{i}0.0,10.0]$ in Stable Diffusion, $\\bar{\\lambda}\\in[-50.0,50.0]$ in DeepFloyd, and $\\lambda\\in\\left[-10.0,10.0\\right]$ in Latent Consistency Model. ", "page_idx": 29}, {"type": "text", "text": "H Social Impacts and Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The paper originally presents a new image manipulation method, with a theoretical framework to deepen the understanding of diffusion models. However, there exist potential social impacts that the proposed methods can be misused in generating and manipulating harmful content. Therefore, we will release our code and models with license and ethics commitments in the future. Besides, methods for identifying and preventing such harmful behaviors are of great significance in generative models. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We present the empirical observation in Section 3.1, image edit method in Section 3, theoretical proof in Section 4 and Appendix F, and experiment results in Section 5 in details in the paper and appendix. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We discuss the limitations and future direction of our works in Appendix A. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: For the theory presented in Section 4, we provide detailed proofs in Appendix F. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] Justification: We provide detailed experiment setup in Appendix G for reproducing our result. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide codes and documentations at https://github.com/ChicyChen/ LOCO-Edit. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide detailed experiment setup, evaluation setup, and metrics setup in Appendix G for better interpretation of our results. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Our generation experiments are conducted randomly for hundreds of times across different dataset and models. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide computation resources information in Appendix G. ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We discuss the paper\u2019s impacts in introduction and related works, as well as potential misuse and our commitment in preventing harmful behaviors in Appendix H. ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We discuss potential misuse and our commitment in preventing harmful behaviors in Appendix H. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have properly credited all existing models and datasets that are related to the paper. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide detailed descriptions and implementation details for the proposed new method. We have also released codes for reproducibility. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide details on human evaluation for the generated images in details in Appendix G. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}]