[{"figure_path": "FLNnlfBGMo/figures/figures_2_1.jpg", "caption": "Figure 1: The commonly adopted prompt optimization pipeline. Previous works mostly investigate the generation component and ignore costs during selection, where GrIPS and APE are proposed in Prasad et al. [59], Zhou et al. [90]. This work, instead, focuses on the selection component under an explicit budget constraint.", "description": "This figure illustrates the common pipeline for prompt optimization.  Previous works mainly focused on generating a pool of candidate prompts.  This paper focuses on selecting the best prompt from that pool, which is often overlooked but is resource-intensive. The selection process is depicted with an emphasis on the budget constraint and the iterative loop that might be present in certain methods.", "section": "2 Prompt Optimization under a Limited Budget"}, {"figure_path": "FLNnlfBGMo/figures/figures_5_1.jpg", "caption": "Figure 2: Clusters for 30 prompts for \u201cmovie recommendation\u201d (left) [69] and \u201crhymes\u201d (right) [30]. Prompts in the same cluster are labeled by the same color and shape. The performance of each prompt is represented by the size of its shape (the larger the better). The embeddings are projected using t-SNE [29].", "description": "This figure shows the results of clustering 30 prompts for two different tasks, \"movie recommendation\" and \"rhymes.\"  Each prompt is represented as a point in a 2D space created using t-SNE dimensionality reduction.  Prompts are grouped into clusters based on their similarity.  The color and shape of each point indicate the cluster it belongs to. The size of the point represents the prompt's performance, with larger points indicating better performance.  The figure visually demonstrates that prompts within the same cluster tend to exhibit similar performance, supporting the effectiveness of clustering for prompt optimization.", "section": "4 Handling Large Candidate Pools via Prompt Embeddings"}, {"figure_path": "FLNnlfBGMo/figures/figures_6_1.jpg", "caption": "Figure 3: Performance comparisons of various prompt selection methods on the selected tasks. The red dashed lines label the performances normalized over (i.e., 1 on the y-axis) and the red stars mark the best methods. The reported results are aggregated over 20 independent runs. The full results on 47 tasks are reported in Appendix F.", "description": "This figure compares the performance of different prompt selection methods (Uniform, UCB, BO-EI, BO-PI, NeuralUCB, TRIPLE-SH, TRIPLE-CR, TRIPLE-CLST, TRIPLE-GSE) across multiple tasks using two different LLMs (GPT-3.5 and Llama2).  It shows normalized evaluation scores for each method with different budget constraints (N=150 and N=100) and various number of candidate prompts (|P|=30 and |P|=150). Red dashed lines represent the average performance of the \"Uniform\" method, and red stars mark the best performing method in each task.", "section": "5 Experiments"}, {"figure_path": "FLNnlfBGMo/figures/figures_6_2.jpg", "caption": "Figure 3: Performance comparisons of various prompt selection methods on the selected tasks. The red dashed lines label the performances normalized over (i.e., 1 on the y-axis) and the red stars mark the best methods. The reported results are aggregated over 20 independent runs. The full results on 47 tasks are reported in Appendix F.", "description": "This figure compares the performance of different prompt selection methods across multiple tasks using two different LLMs (GPT-3.5 and Llama2).  It shows normalized test accuracy for each method against a baseline ('Uniform').  The results indicate that the TRIPLE methods (TRIPLE-SH, TRIPLE-CR, TRIPLE-CLST, TRIPLE-GSE) generally outperform other baselines (Uniform, UCB, BO-EI, BO-PI, NeuralUCB) across different budget and prompt pool sizes, highlighting the effectiveness of the proposed TRIPLE framework.", "section": "5 Experiments"}, {"figure_path": "FLNnlfBGMo/figures/figures_8_1.jpg", "caption": "Figure 4: Gains over Uniform under different budgets on GPT-3.5.", "description": "This figure displays the relative improvement in performance of various prompt selection methods, including TRIPLE variants and baselines like UCB and BO, compared to a uniform selection approach.  The x-axis represents the budget allocated for prompt selection, while the y-axis shows the relative improvement in performance.  The results demonstrate the increasing superiority of TRIPLE methods, especially at lower budgets, highlighting their efficiency in finding good prompts with limited resources.", "section": "5 Experiments"}, {"figure_path": "FLNnlfBGMo/figures/figures_19_1.jpg", "caption": "Figure 2: Clusters for 30 prompts for \u201cmovie recommendation\u201d (left) [69] and \u201crhymes\u201d (right) [30]. Prompts in the same cluster are labeled by the same color and shape. The performance of each prompt is represented by the size of its shape (the larger the better). The embeddings are projected using t-SNE [29].", "description": "This figure shows the result of clustering 30 prompts into clusters based on their embeddings for two different tasks: movie recommendation and rhymes.  The visualization uses t-SNE to project the high-dimensional embeddings into a 2D space. Each point represents a prompt, with color and shape indicating cluster membership. The size of the point represents the prompt's performance, with larger points indicating better performance.  The figure visually demonstrates that prompts within the same cluster tend to have similar performance, supporting the effectiveness of the clustering approach.", "section": "4 Handling Large Candidate Pools via Prompt Embeddings"}, {"figure_path": "FLNnlfBGMo/figures/figures_20_1.jpg", "caption": "Figure 2: Clusters for 30 prompts for \u201cmovie recommendation\u201d (left) [69] and \u201crhymes\u201d (right) [30]. Prompts in the same cluster are labeled by the same color and shape. The performance of each prompt is represented by the size of its shape (the larger the better). The embeddings are projected using T-SNE [29].", "description": "This figure shows the results of clustering prompts based on their embeddings for two different tasks: movie recommendation and rhymes.  Each point represents a prompt, with color and shape indicating cluster membership.  The size of the point corresponds to the prompt's performance (larger points indicate better performance). T-SNE is used to reduce the dimensionality of the embeddings for visualization.", "section": "4 Handling Large Candidate Pools via Prompt Embeddings"}, {"figure_path": "FLNnlfBGMo/figures/figures_20_2.jpg", "caption": "Figure 2: Clusters for 30 prompts for \u201cmovie recommendation\u201d (left) [69] and \u201crhymes\u201d (right) [30]. Prompts in the same cluster are labeled by the same color and shape. The performance of each prompt is represented by the size of its shape (the larger the better). The embeddings are projected using t-SNE [29].", "description": "This figure shows the results of clustering prompts for two different tasks: \"movie recommendation\" and \"rhymes\".  Prompts are clustered based on their embeddings, and prompts within the same cluster are visually grouped by color and shape. The size of each shape represents the prompt's performance, with larger shapes indicating better performance.  t-SNE is used to project the high-dimensional embeddings into a 2D space for visualization.", "section": "Handling Large Candidate Pools via Prompt Embeddings"}, {"figure_path": "FLNnlfBGMo/figures/figures_21_1.jpg", "caption": "Figure 2: Clusters for 30 prompts for \u201cmovie recommendation\u201d (left) [69] and \u201crhymes\u201d (right) [30]. Prompts in the same cluster are labeled by the same color and shape. The performance of each prompt is represented by the size of its shape (the larger the better). The embeddings are projected using t-SNE [29].", "description": "This figure visualizes the results of clustering 30 prompts for two different tasks, \"movie recommendation\" and \"rhymes.\"  Each prompt is represented as a point in a 2D embedding space, generated using t-SNE for dimensionality reduction.  Prompts within the same cluster (same color and shape) have similar performances, as indicated by the size of the shape, with larger shapes signifying better performance. The visualization helps illustrate the effectiveness of clustering similar prompts together before applying best-arm identification.", "section": "Handling Large Candidate Pools via Prompt Embeddings"}, {"figure_path": "FLNnlfBGMo/figures/figures_22_1.jpg", "caption": "Figure 11: Probability for different algorithms to select a good prompt under different budgets (right), collected with GPT-3.5 and averaged over 5 runs.", "description": "This figure shows the probability of different prompt selection algorithms (Uniform, UCB, SH, CR, FuncApprox, Cluster) to select a good prompt (either the optimal prompt or one achieving at least 95% of the optimal prompt's performance) under various budget constraints. The x-axis represents the budget, and the y-axis represents the probability of selecting a good prompt. The results are obtained using GPT-3.5 and averaged over 5 runs.  The figure demonstrates how the probability of selecting a good prompt improves with increasing budget and how the different algorithms compare in terms of their efficiency in finding good prompts under budget limitations.", "section": "F.1 Selection of Budgets"}, {"figure_path": "FLNnlfBGMo/figures/figures_26_1.jpg", "caption": "Figure 3: Performance comparisons of various prompt selection methods on the selected tasks. The red dashed lines label the performances normalized over (i.e., 1 on the y-axis) and the red stars mark the best methods. The reported results are aggregated over 20 independent runs. The full results on 47 tasks are reported in Appendix F.", "description": "This figure compares different prompt selection methods across several tasks using GPT-3.5 and Llama2 language models.  The y-axis represents normalized evaluation scores (test accuracy), with 1 indicating the average performance of the 'Uniform' baseline. The x-axis displays the various tasks.  The red dashed line represents the average 'Uniform' performance. Red stars highlight the best-performing method for each task. The figure shows that TRIPLE methods (TRIPLE-SH, TRIPLE-CR, TRIPLE-CLST, and TRIPLE-GSE) consistently outperform baselines such as Uniform, UCB, BO, and NeuralUCB across various tasks. The results are averaged over 20 runs for robustness.", "section": "5.1 Evaluating TRIPLE with Fixed Prompt Pools"}, {"figure_path": "FLNnlfBGMo/figures/figures_26_2.jpg", "caption": "Figure 3: Performance comparisons of various prompt selection methods on the selected tasks. The red dashed lines label the performances normalized over (i.e., 1 on the y-axis) and the red stars mark the best methods. The reported results (y-axis) are test accuracies of each method normalized to the mean performance of \u201cUniform\u201d on that task. The reported results are aggregated over 20 independent runs. The full results on 47 tasks are reported in Appendix F.", "description": "This figure compares the performance of different prompt selection methods across various tasks.  It shows normalized test accuracy for each method on several tasks, using two different numbers of candidate prompts (30 and 150) and budgets. The red dashed line represents the baseline performance, and red stars indicate the best performing method for each task.  The results highlight the performance improvement of TRIPLE over existing methods.", "section": "5 Experiments"}, {"figure_path": "FLNnlfBGMo/figures/figures_27_1.jpg", "caption": "Figure 3: Performance comparisons of various prompt selection methods on the selected tasks. The red dashed lines label the performances normalized over (i.e., 1 on the y-axis) and the red stars mark the best methods. The reported results are aggregated over 20 independent runs. The full results on 47 tasks are reported in Appendix F.", "description": "This figure compares the performance of different prompt selection methods across multiple tasks.  The y-axis represents normalized evaluation scores, with 1.0 indicating the average performance of the Uniform baseline.  The figure shows that TRIPLE methods (TRIPLE-SH, TRIPLE-CR, TRIPLE-CLST, TRIPLE-GSE) generally outperform the baselines (Uniform, UCB, BO-EI, BO-PI, NeuralUCB) across various tasks and with both small and large numbers of candidate prompts.  Red stars show the best performing method for each task.", "section": "5 Experiments"}, {"figure_path": "FLNnlfBGMo/figures/figures_28_1.jpg", "caption": "Figure 13: Complete results on the Instruction-Induction dataset with |P| = 30 prompts and budget N = 150.", "description": "This figure shows a detailed comparison of various prompt selection methods across multiple tasks from the Instruction-Induction dataset.  The results are presented for two different LLMs, GPT-3.5 and Llama2, with a fixed prompt pool size of 30 and a budget of 150.  Each bar represents the normalized evaluation score for a specific task and method. This allows for a comprehensive comparison of the relative performance of TRIPLE against other baselines, such as Uniform, UCB, BO, and NeuralUCB.  The figure visualizes the effectiveness of TRIPLE across various tasks under the specified budget constraint.", "section": "5 Experiments"}, {"figure_path": "FLNnlfBGMo/figures/figures_29_1.jpg", "caption": "Figure 3: Performance comparisons of various prompt selection methods on the selected tasks. The red dashed lines label the performances normalized over (i.e., 1 on the y-axis) and the red stars mark the best methods. The reported results (y-axis) are test accuracies of each method normalized to the mean performance of \u201cUniform\u201d (a) or \u201cNeuralUCB\u201d (b) on that task. The reported results are aggregated over 20 independent runs. The full results on 47 tasks are reported in Appendix F.", "description": "This figure compares the performance of various prompt selection methods, including TRIPLE variants, against baselines like Uniform, UCB, BO, and NeuralUCB.  It shows normalized test accuracy across multiple tasks for GPT-3.5 and Llama2 models, with varying numbers of candidate prompts and budgets. The results highlight TRIPLE's superior performance, especially when utilizing prompt embeddings.", "section": "5 Experiments"}, {"figure_path": "FLNnlfBGMo/figures/figures_30_1.jpg", "caption": "Figure 3: Performance comparisons of various prompt selection methods on the selected tasks. The red dashed lines label the performances normalized over (i.e., 1 on the y-axis) and the red stars mark the best methods. The reported results are aggregated over 20 independent runs. The full results on 47 tasks are reported in Appendix F.", "description": "The figure compares the performance of several prompt selection methods (Uniform, UCB, BO, NeuralUCB, TRIPLE-SH, TRIPLE-CR, TRIPLE-CLST, TRIPLE-GSE) on various tasks using GPT-3.5 and Llama2 language models.  It shows the test accuracy of each method, normalized to the baseline method's performance (Uniform for (a), NeuralUCB for (b)).  The results highlight the improved performance of TRIPLE methods, especially when using prompt embeddings.", "section": "5.1 Evaluating TRIPLE with Fixed Prompt Pools"}, {"figure_path": "FLNnlfBGMo/figures/figures_31_1.jpg", "caption": "Figure 3: Performance comparisons of various prompt selection methods on the selected tasks. The red dashed lines label the performances normalized over (i.e., 1 on the y-axis) and the red stars mark the best methods. The reported results are aggregated over 20 independent runs. The full results on 47 tasks are reported in Appendix F.", "description": "This figure compares different prompt selection methods across multiple tasks using two different LLMs (GPT-3.5 and Llama2).  It shows the normalized evaluation scores for each method, allowing for easy comparison of their relative performance.  The red dashed lines represent the baseline performance (Uniform), and red stars indicate the best performing method for each task. The figure highlights the superior performance of TRIPLE methods, especially when considering a limited budget for prompt evaluation.", "section": "5 Experiments"}, {"figure_path": "FLNnlfBGMo/figures/figures_31_2.jpg", "caption": "Figure 13: Complete results on the Instruction-Induction dataset with |P| = 30 prompts and budget N = 150.", "description": "This figure presents a comprehensive comparison of various prompt selection methods on 24 tasks from the Instruction-Induction dataset.  The experiment parameters were a fixed pool size of 30 prompts and a budget of 150 interactions with the language model. Results are shown for GPT-3.5 and Llama2 separately.  Each bar represents the normalized evaluation score of a particular method for a given task, enabling a direct comparison of performance. The red dashed line indicates the normalized average performance of the Uniform baseline, which evaluates all prompts uniformly.", "section": "5 Experiments"}]