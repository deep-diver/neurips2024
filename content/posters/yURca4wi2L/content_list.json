[{"type": "text", "text": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haoming Cai\\* 1, Jingxi Chen\u22171, Brandon Y. Feng2, Weiyun Jiang3, Mingyang $\\mathbf{X}\\mathbf{ie}^{1}$ , Kevin Zhang1, Cornelia Fermuller1, Yiannis Aloimonos1, Ashok Veeraraghavan3, Christopher A. Metzler\u2020 1 ", "page_idx": 0}, {"type": "text", "text": "University of Maryland, 2Massachusetts Institute of Technology, 3Rice University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Atmospheric turbulence, caused by random fluctuations in the atmosphere\u2019s refractive index, introduces complex spatio-temporal distortions in imagery captured at long range. Video Atmospheric Turbulence Mitigation (ATM) aims to restore videos affected by these distortions. However, existing video ATM methods, both supervised and self-supervised, struggle to maintain temporally consistent mitigation across frames, leading to visually incoherent results. This limitation arises from the stochastic nature of atmospheric turbulence, which varies across space and time. Inspired by the observation that atmospheric turbulence induces high-frequency temporal variations, we propose ConVRT, a novel framework for consistent video restoration through turbulence. ConVRT introduces a neural video representation that explicitly decouples spatial and temporal information into a spatial content field and a temporal deformation field, enabling targeted regularization of the network\u2019s temporal representation capability. By leveraging the low-pass filtering properties of the regularized temporal representations, ConVRT effectively mitigates turbulence-induced temporal frequency variations and promotes temporal consistency. Furthermore, our training framework seamlessly integrates supervised pre-training on synthetic turbulence data with self-supervised learning on real-world videos, significantly improving the temporally consistent mitigation of ATM methods on diverse real-world data. More information can be found on our project page: https://convrt-2024.github.io/ ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Atmospheric turbulence poses a significant challenge in long-range imaging applications, causing unique distortions in captured videos. These turbulence-distorted videos suffer from spatially-varying and time-varying degradations, including blur and warping effects, due to the random fluctuations of the refractive index in the atmosphere. These distortions significantly hinder the performance of computer vision applications like object detection, recognition, and surveillance systems by obscuring the true shapes, edges, and visual details of objects. Therefore, this work focuses on Video Atmospheric Turbulence Mitigation (ATM), aiming to recover videos degraded by these atmospheric distortions. ", "page_idx": 0}, {"type": "text", "text": "Mathematically, the process of capturing video through atmospheric turbulence can be modeled by the following equation ", "page_idx": 0}, {"type": "image", "img_path": "yURca4wi2L/tmp/05b78de5ef4463419a32fad01bbd645c248a0317618abd8025e12801f03c030c.jpg", "img_caption": ["Figure 1: Temporally consistent restoration in video ATM is challenging. State-of-the-art methods like DATUM (2) (CVPR\u201924) and TMT (3)(TCI\u201923), designed for video ATM, fail to maintain temporal consistency in real-world atmospheric turbulence. For instance, they produce flickering artifacts on a static pole. "], "img_footnote": [], "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underbrace{y_{t}}_{\\mathrm{distorted\\;frame}}=\\underbrace{\\left[{B_{t}\\circ T_{t}}\\right]}_{\\mathrm{tilt\\-then\\-blur\\}\\mathcal{H}}\\left(\\underbrace{I_{t}}_{\\mathrm{clean\\:frame}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $B_{t}$ and $T_{t}$ are blur and tilt process at t time stamp. \u25e6denotes the application of tilt followed by blur (1). ", "page_idx": 1}, {"type": "text", "text": "As described by Equation (1), the key challenge arises from the stochastic nature of atmospheric turbulence, which varies across space and time, making temporally consistent video restoration difficult. Figure 1 illustrates the challenge of temporal inconsistency by showing a static scene captured with a stationary camera and object. Despite the static setup, the atmospheric turbulence introduces erratic movements of the stationary traffic cone across frames, causing filckering artifacts in the resulting video sequence. Notably, even state-of-the-art methods like DATUM (2), designed for video turbulence mitigation, fail to produce temporally consistent mitigation results, result in filckering video. This underscores the critical need for novel solutions tailored to address the challenge of temporal consistency in video atmospheric turbulence mitigation. ", "page_idx": 1}, {"type": "text", "text": "1.1 Current State-of-the-Art in Atmospheric Turbulence Mitigation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To address this complex and variable degradation in images and videos, various methodologies have been developed. Current state-of-the-art methods can generally be categorized into supervised and self-supervised learning manner. ", "page_idx": 1}, {"type": "text", "text": "Supervised learning techniques in ATM use turbulence simulators to generate paired training data (clean and distorted images/video) that can be used for training (14; 15; 16; 1; 17; 18). Fig.3(A) and the supervised learning section of Table 1 depict methods that achieve significant results based ", "page_idx": 1}, {"type": "table", "img_path": "yURca4wi2L/tmp/a579b1425f433c2ce6aa8a4dd9bebaf0711adb8322a82eec248ad31e4e525806.jpg", "table_caption": ["Table 1: Comparison of recent supervised (S), selfsupervised (SS), and hybrid $(\\mathsf{S}\\!+\\!\\mathsf{S}\\!\\mathsf{S})$ learning approaches for image and video ATM. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "on large amounts of paired data. Despite the continual evolution of simulators, the persistent gap between simulated and real-world atmospheric turbulence poses challenges for this design in handling unseen real-world data. In videos, this drawback is further amplified, leading to issues like temporally inconsistent mitigation. To address this temporal inconsistency issue, in addition to better simulators, enlarging the dataset and model capacity are necessary, which substantially increases the computational costs of training. ", "page_idx": 1}, {"type": "text", "text": "Self-supervised learning approaches for ATM employ internal learning techniques to leverage data priors such as lucky images, internal data distributions, or blind degradation estimation, as depicted in Fig. 3(B) and self-supervised learning section of Table 1. A key advantage of these methods is their test-time optimization capability, allowing them to adapt to any test data. However, to date these approaches have not been used to enforce temporal consistency in video ATM. Furthermore, because they don\u2019t exploit accurate learned image priors, the performance of self-supervised methods in real-world turbulence mitigation often falls short of supervised learning approaches. ", "page_idx": 1}, {"type": "image", "img_path": "yURca4wi2L/tmp/116e7dbfc8cb1cf5bb62552b7504843b1bfeaae0696b2d7260ff761ce4658905.jpg", "img_caption": ["Figure 2: Inspiration of our method: Atmospheric turbulence introduces high-frequency temporal variations in videos due to the chaotic motion of air caused by temperature gradients and other energy sources. These variations manifest as time-varying tilt and blur, deviating from the ground truth, as evident in the rapidly fluctuating patterns along the temporal dimension (vertical axis) of the y-t slice of the turbulence-distorted video (left). In contrast, the restored video of SOTA method DATUM (2) (right) exhibits smoother temporal variations, indicating the mitigation of turbulenceinduced distortions. This key insight highlights the potential for regularizing temporal information to effectively restore videos affected by atmospheric turbulence. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "This paper develops a hybrid algorithm that can consistently mitigate real-world atmospheric turbulence across video frames. Our pipeline can leverage the knowledge encoded in pre-trained models while leveraging test-time optimization to adapt to the complexities of real-world turbulence. As shown in Fig. 3(C) and the final section of Table 1, our pipeline leverages the strengths of both self-supervised learning and simulation-based pre-training. ", "page_idx": 2}, {"type": "text", "text": "1.2 Motivation and Contribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our work is motivated by the insight, illustrated in Figure 2, that state-of-the-art ATM methods struggle to remove the temporal distortions introduced by turbulence. That is, while existing methods are reasonably effective at removing the spatial distortions (e.g., blur) introduced by turbulence, they do not effectively remove the temporal distortions. ", "page_idx": 2}, {"type": "text", "text": "To address this challenge, we develop an approach that explicitly decouples spatial and temporal information. This method leverages the low-pass filtering properties of neural networks to reduce turbulence-induced degradations. Specifically, we propose a self-supervised method called ConVRT (Consistent Video Restoration through Turbulence). ConVRT forms a neural representation of the reconstucted video that explicitly decouples spatial and temporal information: The video is represented with a spatial content field and a temporal deformation field. This decoupling allows ConVRT to effectively regularize the temporal information while preserving spatial information and fine details. ", "page_idx": 2}, {"type": "text", "text": "Through extensive evaluations, we demonstrate that ConVRT substantially improves temporally consistency while also marginally improving per-frame restoration quality. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Implicit neural representations. Our work leverages a coordinate-based implicit neural representation (INRs), which has been commonly adopted to model 2D images or 3D videos as multilayer perceptions (MLPs). INRs take 2D pixel coordinates $(x,y)$ , or 3D pixel coordinates with temporal encoding, $\\left({x,y,t}\\right)$ and output the corresponding pixel values. These INRs demonstrate exceptional performance when fitting images $(19;\\,20;\\,21;\\,22;\\,23;\\,24;\\,25)$ , videos (26; 22), 3D shapes (27; 28; 26; 29; 30; 31), and optical components (32). Not only they are able to represent these 2D or 3D signals, but they also show strong priors for solving inverse problems, such as image super resolution (33), phase retrieval (34), and reducing optical aberration (35; 36; 37; 38). ", "page_idx": 2}, {"type": "text", "text": "Neural video representation. Our work aligns closely with the evolving field of neural video representation (39; 40; 41; 42). While there are existing approaches (43; 44; 42; 45) that seek to represent a video into decomposed layers, these primarily focus on clean videos and are not applicable to videos with severe degradation turbulence. Our work extends the application of neural video representation to scenarios heavily impacted by atmospheric turbulence. This extension is not trivial, as it involves addressing the unique challenges posed by the dynamic and unpredictable nature of turbulence, which are not considered in conventional video representations. ", "page_idx": 2}, {"type": "image", "img_path": "yURca4wi2L/tmp/49d908ed6dd43014e5f608bd11988f2d5fa622d76f17570b3d18801f8bd32aee.jpg", "img_caption": ["Figure 3: Comparison of different learning approaches for image and video ATM. (a) ATM methods in supervised learning face challenges in domain adaptation for real-world data. (b) Self-supervised learning ATM methods mostly explore static video sequences, outputting one frame from multiple frames as input. (c) Our hybrid pipeline is tailored for video ATM, combining supervised pre-training and self-supervised learning to achieve consistent video restoration through turbulence "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Atmospheric turbulence mitigation. Attempts to mitigate atmospheric turbulence (46; 47) have applied optical flow (9; 48), B-spline grid (49), and diffeomorphism (50) to unwarp each distorted image and then fuse and combine these registered distorted images into a clean and sharp image. The fusion is usually modeled as patch-wise stitching (9) or blind deconvolution (51). Recent development of high-performance GPUs and fast turbulence simulators (16; 18; 17; 15; 14) leads to new progress in turbulence mitigation (15; 5; 11; 7; 12). However, previous efforts tend to overlook the importance of temporal consistency on the reconstructed video. Our method, ConVRT, is specifically designed to restore temporal consistency with on test-time optimization of a neural video representation. ", "page_idx": 3}, {"type": "text", "text": "Blind Video Restoration via deep video prior. Supervised video restoration methods (52; 53; 54; 55) have made significant advancements but are constrained by the need for paired data, which increases the value of blind video restoration. One promising direction involves leveraging deep priors. The deep video prior (DVP) and DVP-based blind video consistency methods (56; 57) use convolutional neural networks (CNNs) to learn image operators that exploit the implicit priors in CNNs to remove video artifacts. These approaches have demonstrated impressive results in tasks such as colorization and white-balancing. However, turbulence mitigation presents a more complex challenge compared to these common degradations, involving spatially and temporally varying blur and tilt. This complexity raises unexplored questions for these types of methods ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Overview of the Pipeline ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The framework of our method, ConVRT, is presented in Figure 4. In this subsection, we provide a high-level overview covering the design inspiration, the video representation mechanism, and the training process. ", "page_idx": 3}, {"type": "text", "text": "Design Inspiration. As discussed in Section 1.2, the core design logic of ConVRT is to apply temporal-wise regularization in video representation learning. For the representation, our method is inspired by a series of works on tensor decomposition, commonly used to parameterize 3D volumes in implicit neural representations (INR). These approaches enhance the ability to represent 3D signals while reducing the number of required parameters (33; 58; 59). Building upon this, we developed the ConVRT method. ", "page_idx": 3}, {"type": "text", "text": "Video Representation. ConVRT represents videos using two main components: the 3D SpatialTemporal Deformation Field $(T_{\\mathrm{field}})$ and the 2D Spatial Content Field $(S_{\\mathrm{field}})$ . The process begins with $T_{\\mathrm{field}}$ , which receives the pixel location $\\left({x,y,t}\\right)$ as input, where $(x,y)$ are spatial coordinates and $t$ is the temporal frame index. $T_{\\mathrm{field}}$ outputs deformation offsets $(\\Delta x,\\Delta y)$ , indicating changes in the pixel\u2019s spatial position across frames relative to a canonical frame. These offsets are then used by ", "page_idx": 3}, {"type": "image", "img_path": "yURca4wi2L/tmp/6540d03e714141db335e7a298c33531e9b66eb87ab56a4fede3ea786015af8a9.jpg", "img_caption": ["Figure 4: Illustration of the proposed method. ConVRT represents a video with two fields: the Temporal Deformation Field $(T_{f i e l d})$ and the Spatial Content Field $(S_{f i e l d})$ . Regularization is applied by constraining the dimensions of the Temporal Feature Map. Similarly, reducing the size of the deformation MLP serves as additional regularization to promote temporal consistency. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "$S_{\\mathrm{field}}$ . $S_{\\mathrm{field}}$ queries the Canonical Spatial Feature Map $(C)$ at the modified location $(x+\\Delta x,y+\\Delta y)$ to retrieve the corresponding feature from a trainable feature map. This feature is subsequently processed by an MLP to predict the RGB intensity values for the pixel location $\\left({x,y,t}\\right)$ . ", "page_idx": 4}, {"type": "text", "text": "Training Overview. During training, the trainable parameters include all feature maps and the Multi-Layer Perceptrons (MLP) described below. The loss function measures the difference between the predicted RGB intensity values and the corresponding pixel colors in the restored video, obtained using any ATM method. Since no ground-truth data is available, ConVRT is designed to overfti each partially restored video; however, its limited capacity for capturing temporal information prevents it from overfitting to turbulence artifacts ", "page_idx": 4}, {"type": "text", "text": "3.2 Temporal Deformation Field $T_{f i e l d}$ with Regularization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We represent the input video\u2019s spatial-temporal features using two main components: the Spatial Feature Map and the Temporal Feature Map. The Spatial Feature Map $(M)$ acts as a dictionary for spatial features, with dimensions $\\mathbb{R}^{H\\times W\\times Q_{1}^{\\textbf{x}}}$ , where $H$ is the frame height, $W$ is the frame width, and $Q_{1}$ is the number of spatial feature channels. Each pixel coordinate $(x,y)$ serves as a key to retrieve the corresponding spatial feature vector $M_{x,y}\\in\\mathbb{R}^{Q_{1}}$ . The Temporal Feature Map $(N)$ functions as a dictionary for temporal features, with dimensions $\\mathbb{R}^{T_{r e s}\\times Q_{1}}$ . Here, $T_{r e s}$ is the regularized temporal resolution and $Q_{1}$ is the number of temporal feature channels. ", "page_idx": 4}, {"type": "text", "text": "To construct the spatial-temporal feature vector $V_{x,y,t}$ at a specific pixel location $(x,y,t)$ , we first query the Spatial Feature Map $M$ using the pixel coordinates $(x,y)$ , extracting the spatial feature vector $M_{x,y}^{\\mathrm{~\\,~\\star~}}\\in\\,\\mathbb{R}^{Q_{1}}$ . Next, we query the Temporal Feature Map $N$ using the time coordinate $t$ , extracting the temporal feature vector $N_{t}\\,\\in\\,\\mathbb{R}^{Q_{1}}$ . These vectors are then combined using the Hadamard product, which performs element-wise multiplication, to form the spatial-temporal feature vector: ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{x,y,t}=M_{x,y}\\odot N_{t}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This Hadamard product effectively combines the spatial and temporal features, creating a compact and efficient representation of the video\u2019s spatial-temporal characteristics. This $V_{x,y,t}$ is then fed into a compact MLP, referred to as the deformation MLP. The details of the deformation MLP are provided in the supplementary material. The deformation MLP outputs the offsets $(\\Delta x,\\Delta y)$ necessary for warping the canonical spatial feature map. ", "page_idx": 4}, {"type": "text", "text": "To regularize the temporal representation capability of the Temporal Feature Map $(N)$ , we constrain its dimensions to $\\mathbb{R}^{\\hat{T}_{r e s}\\times Q_{1}}$ , where $T_{r e s}$ is much smaller than the total number of video frames $(T)$ . Consequently, multiple neighboring frames share the same temporal feature. For example, frames at $t-1,t$ , and $t+1$ may query the same temporal feature $N_{t}$ due to the reduced temporal resolution. Additionally, we define the deformation MLP with a reduced number of parameters. Both regularizations decrease the representation capacity of the temporal features, promoting smoother and more consistent temporal dynamics across frames, as inspired by our motivation experiment. ", "page_idx": 4}, {"type": "text", "text": "3.3 Spatial Content Field ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The Spatial Content Field focuses on accurately representing the spatial details of each video frame. Unlike the Spatial Feature Map $(M)$ used in the Temporal Deformation Field, we initialize a new optimizable feature map, denoted as the Canonical Spatial Feature Map $(C)$ , with dimensions $\\mathbb{R}^{\\!\\!\\!\\star}\\!\\!\\!H\\!\\times\\!W\\!\\times\\!Q_{2}$ , where $H$ is the frame height, $W$ is the frame width, and $Q_{2}$ is the number of spatial feature channels specific to this field. ", "page_idx": 5}, {"type": "text", "text": "Each pixel coordinate $(x,y)$ is adjusted by the deformation offsets $(\\Delta x,\\Delta y)$ , resulting in new coordinates $(x+\\Delta x,y+\\Delta y)$ . These adjusted coordinates are then used to query the Canonical Spatial Feature Map $(C)$ , retrieving the spatial feature vector $C_{x+\\Delta x,y+\\Delta y}\\in\\mathbf{\\bar{R}}^{Q_{2}}$ . These spatial features are processed by a Content MLP, which transforms the spatial feature vector $C_{x+\\Delta x,y+\\Delta y}$ into the final RGB intensity values for the corresponding pixel. The details of the Content MLP are provided in the supplementary material. This transformation ensures that the spatial details of the video frame are accurately captured and represented. ", "page_idx": 5}, {"type": "text", "text": "3.4 Training Objectives ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Temporal Consistency Regularization. To ensure temporal stability across video frames, we use a disparity estimation network (MiDas (60)) to calculate pixel-wise disparities. These disparities serve as weights for the predicted warp (one of $D_{\\mathrm{field}}$ \u2019s outputs), helping to maintain spatial consistency over time. The loss is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t e m p}=(1-\\mathrm{Disparity}(I))\\cdot\\|\\mathrm{Predicted\\;Warp}\\|_{1}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where Disparity $(I)$ measures the pixel-level disparity, and $\\parallel$ Predicted $\\operatorname{Warp}\\|_{1}$ enforces sparsity in the grid changes. The design of $\\mathcal{L}_{t e m p}$ minimizes the L1 norm of the predicted warp, conditioned by $1-\\mathrm{Disparity}(I)$ , to prioritize consistency in far regions based on the depth information. This focused approach on temporal consistency significantly reduces the propagation of turbulence-induced distortions, ensuring a smooth transition between frames. ", "page_idx": 5}, {"type": "text", "text": "Similarity Loss. The Similarity Loss Term is given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s i m}=\\lambda_{m s e}\\mathcal{L}_{m s e}+\\lambda_{s s i m}\\mathcal{L}_{s s i m}+\\lambda_{l p i p s}\\mathcal{L}_{l p i p s}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda_{m s e}$ , $\\lambda_{s s i m}$ , and $\\lambda_{l p i p s}$ are weights for each term. This loss term assesses the fidelity of the predicted output compared to the outputs of arbitrary ATM methods, incorporating Mean Squared Error (MSE), Structural Similarity Index Measure (SSIM) (61), and Learned Perceptual Image Patch Similarity (LPIPS) (62). This multifaceted approach ensures a comprehensive evaluation of reconstruction quality. ", "page_idx": 5}, {"type": "text", "text": "Overall Loss. The overall loss combines the similarity loss with temporal consistency and semantic enhancement: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t o t a l}=\\mathcal{L}_{s i m}+\\lambda_{t e m p}\\mathcal{L}_{t e m p}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Datasets and Training Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We adopt several real-world datasets for evaluation, including the OTIS (63), HeatChamber (5), subset of BVI-CLEAR dataset (64), TSR-WGAN dataset (4) and DOST (65). We trained the ConVRT model individually on each video clip with a learning rate of $2\\times10^{-3}$ , using the Adam optimizer (66). For each video clip, the batch size equals to the number of frames in that clip. The spatial resolution of both the trainable spatial feature map and the canonical spatial feature map matches the original frame resolution after square cropping. The temporal resolution parameter $T_{\\mathrm{res}}$ was set to 5, with parameters $Q_{1}$ and $Q_{2}$ configured to 128 and 256, respectively. More details about the network settings are provided in the supplementary material. Training was conducted on a single RTX A6000. ", "page_idx": 5}, {"type": "text", "text": "4.2 Evaluation Strategy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We selected VRT(52), TMT(3), and DATUM(2) as the base video methods for ConVRT due to their state-of-the-art performance in video restoration and video ATM. TurbNet(5) is selected for base image method. We also directly applied ConVRT to the original video without the base methods to assess its standalone performance. To evaluate the consistency of turbulence removal in videos, we employed four metrics for quantitative evaluation and two interframe-related methods for qualitative assessment. ", "page_idx": 5}, {"type": "image", "img_path": "yURca4wi2L/tmp/f96563dc5f0a27c23d82f6413ff7fb84d48a9720c5990906ac1cdd9ec8fe913d.jpg", "img_caption": ["Figure 5: Visualization of our method\u2019s effectiveness in mitigating real-world atmospheric turbulence compared to existing methods. The leftmost image shows the original frame with a green box marking the zoom-in crop area for KLT tracking and a red line for the Y-t slice, shown in the bottom left. The right side displays two rows: the first shows zoom-in KLT tracking results for baseline methods and their outputs enhanced by our method, and the second shows zoom-in $\\mathrm{Y-t}$ slices highlighting the temporal consistency achieved. Note the significant reduction in erratic movements in our results. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Temporal Consistency and Per-frame Quality. We used PSNR and SSIM to measure the per-frame reconstruction quality. Following (67), we utilized the average warp error to quantify the temporal consistency of the restored video. The warp error between two consecutive frames is defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nE_{\\mathrm{warp}}(V_{t},V_{t+1})=\\frac{1}{\\sum_{i=1}^{N}M_{t}^{(i)}}\\sum_{i=1}^{N}M_{t}^{(i)}\\left|V_{t}^{(i)}-\\hat{V}_{t+1}^{(i)}\\right|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{V}_{t+1}^{(i)}$ is the warped frame by optical flow at time $t+1$ and $M_{t}^{(i)}\\in\\boldsymbol{0},1$ is the occlusion mask estimated by the methods proposed in (68). The average warp error across the entire video sequence is calculated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nE_{\\mathrm{warp}}(V)={\\frac{1}{T-1}}\\sum_{t=1}^{T-1}E_{\\mathrm{warp}}(V_{t},V_{t+1}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "table", "img_path": "yURca4wi2L/tmp/f0d073849a6e30e1782f1cf680267f57f6b27f0be48f426ab1f1f0ca7874e2ea.jpg", "table_caption": [], "table_footnote": ["Table 2: Performance improvements achieved by applying our proposed ConVRT across various model architectures and datasets. The No Base Method columns show the results when the methodology was applied directly to the original frames, labeled as Ori. Gains are highlighted for each metric, showing the effectiveness of ConVRT in enhancing the temporal consistency in video ATM. "], "page_idx": 7}, {"type": "image", "img_path": "yURca4wi2L/tmp/d8e4ea52ec74bab2f654f43ebdcc96d19fc9c2ac90d7940e3429547b78324a3c.jpg", "img_caption": ["Figure 6: Comparison of turbulence mitigation techniques on a synthetic dataset. Left: Ground truth (GT) frame and corresponding Y-t slices. Right: Zoom-in views of KLT tracking (top row) and Y-t slices (bottom row) for baseline methods and the enhancements brought by our method. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "In addition to the warp loss, inspired by (69), we also calculate the Total Variation loss of the X-t slice, $\\operatorname{Slice}_{t v}$ , and Total Variation of the optical flow, $\\operatorname{Flow}_{t v}$ , to quantitatively measure whether the temporal variation of time slices in restored videos are small. ", "page_idx": 7}, {"type": "text", "text": "KLT Trajectories. We employed the KLT tracker (70) to track feature points and plot their trajectories, as shown in Figure 5. KLT tracking is directly based on image gradient information, such that common issues in turbulence restoration, i.e., blurriness, artifacts, and temporal inconsistency, are reflected in the tracked trajectories. Smooth and coherent trajectories indicate temporally consistent restoration, while erratic or discontinuous trajectories suggest the presence of artifacts or inconsistencies. ", "page_idx": 7}, {"type": "text", "text": "$x$ - $\\cdot t$ Slice. We plotted $x{-}t$ slices to visualize the motion of a row of pixels, as illustrated in Figure 5. If the video restoration is temporally consistent, the $x{-}t$ slice plot will exhibit smooth and continuous curves. In contrast, non-smooth or jagged curves in the $x{-}t$ slice indicate temporal inconsistencies or artifacts in the restored video. ", "page_idx": 7}, {"type": "text", "text": "4.3 Qualitative and Quantitative Improvements on Existing Methods. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Qualitative Real-world Cases. Our method, ConVRT, achieves notable temporal consistency in videos distorted by real atmospheric turbulence. As shown in Figure 5, the original turbulence and baseline methods exhibit \"zig-zag\" KLT tracking trajectories, indicating erratic motion caused by turbulence. In contrast, incorporating ConVRT results in smoother trajectories, demonstrating its effectiveness in consistently removing turbulence artifacts throughout the video. The ${\\bf X-}{\\bf t}$ slice further illustrates that ConVRT effectively smooths row pixel motion over time, reducing the flickering effects typi", "page_idx": 7}, {"type": "text", "text": "Table 3: Ablation Study of $\\mathrm{L}_{t e m p}$ and $\\mathrm{T}_{r e s}$ . Comparison of $\\mathrm{PSNR}_{i m g}$ , SSIM, and $\\mathrm{PSNR}_{x-t}$ scores, showing the impact of $\\mathrm{L}_{t e m p}$ and ${\\mathrm{T}}_{r e s}$ . The experiment is conducted on a synthetic dataset created using turbulence simulator(15). The base model is TurbNet. ", "page_idx": 7}, {"type": "table", "img_path": "yURca4wi2L/tmp/b1593c1df101bcb347dc70e86210434fa65868ca79a35dac9f56dec3a7ef5b91.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "yURca4wi2L/tmp/d60399c238b933e6223c5fddbeabc71937c01afa3150f656ae3f7803f0a28110.jpg", "img_caption": ["Figure 7: Ablation study and canonical image visualization. Our method mitigates residual turbulence using $L_{\\mathrm{temp}}$ and lower $T_{\\mathrm{res}}$ . Canonical image is visualizvisualized from Canonical Spatial Feature Map C. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "yURca4wi2L/tmp/307a5347e448ca2820d5dc8ec5a087529440daa19573013b4f1cbcb5ee2e5e26.jpg", "img_caption": ["Figure 8: Illustration of camera shake simulation using Brownian Motion. In the Y-t slice plots, we observe similarities between camera shake and turbulence. The plots also demonstrate the effectiveness of our approach in handling both camera shake alone and in combination with turbulence. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "yURca4wi2L/tmp/9a01d560d7c6dad0b10f6a79d65498530ea2a7df1375979abbad590094ff4ca7.jpg", "img_caption": ["Figure 9: Experimental results compare ConVRT with unsupervised and test-time optimization methods by Li et al. (10) and Mao et al. (9) on moving objects. Both baselines fail to capture motion, replacing moving parts with the average background, while ConVRT effectively handles them "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "cally observed in turbulence-distorted videos. This improved performance underscores the capability of ConVRT to handle real-world turbulence, providing more stable and visually coherent video sequences. ", "page_idx": 8}, {"type": "text", "text": "Qualitative Synthetic Cases. Similarly, on synthetic video, As shown in Figure 6, our method enhances temporal turbulence removal when applied to baseline methods. The video dynamics generated by ConVRT closely resemble the ground-truth videos, effectively smoothing out atmospheric turbulence. The improved KLT trajectories further demonstrate this temporal consistency. ", "page_idx": 8}, {"type": "text", "text": "Quantitative Results. We evaluated the performance of our proposed ConVRT method across realworld datasets containing both static and dynamic scenes, as shown in Table 2. ConVRT demonstrates consistent improvements across models and most of datasets, underscoring its broad applicability. On the HeatChamber dataset, which provides real-world paired data through a controlled heating mechanism, we calculated PSNR values to further substantiate ConVRT\u2019s effectiveness. onVRT consistently improves PSNR, demonstrating robust enhancement of temporal consistency, especially given PSNR\u2019s sensitivity to pixel misalignment. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Regularized temporal resolution $T_{\\mathrm{res}}$ is critical for ensuring temporal consistency. Lowering it results in smoother transitions but loses fine details, while a higher value preserves details but increases the risk of flickering. We conducted an ablation study on the impact of $T_{\\mathrm{res}}$ and $L_{\\mathrm{temp}}$ , as shown in Table 3, with qualitative results in Figure 7. These results demonstrate the effectiveness of our representation field design in regularizing irregular turbulence motion. ", "page_idx": 8}, {"type": "image", "img_path": "yURca4wi2L/tmp/de5351c8371b9a941ef768063d4b7a8e461dbc8c6b7609bb31ad2a5cf9d6cf1d.jpg", "img_caption": ["Figure 10: Mitigation capability of our method without using base restoration methods for preprocessing. The scene is an elephant raising its head. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.5 Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Why It Works. Our method\u2019s effectiveness stems from two key factors. First, it leverages the distinct differences in motion patterns, particularly the regularity of optical flow directions within a short time window, between regular object movement and atmospheric turbulence, as discussed in Section 1.2. This distinction also enables our method to handle camera shake, which shares similar irregular patterns with turbulence. The results of this capability are illustrated in Figure 8. Second, our method includes a robust video representation that overcomes limitations in unsupervised methods for static scenes. As illustrated in Figure 9, these methods often struggle on moving objects, blending them into static backgrounds. In contrast, our approach preserves object integrity across frames, making it well-suited for video-based turbulence mitigation. ", "page_idx": 9}, {"type": "text", "text": "Mitigation Capability without Base Restoration Methods. Even without a base restoration method to provide partially restored frames, our approach could improve temporal consistency, as shown in Figure 10. However, we recommend combining our method with other restoration techniques. This allows user to benefit from the sharpness improvements offered by supervised methods, while also taking advantage of the temporal consistency improvements provided by ConVRT. ", "page_idx": 9}, {"type": "text", "text": "Visualizing Trainable Feature Maps. We visualize the canonical image by inputting the canonical spatial feature map into the content MLP without applying $\\Delta x$ and $\\Delta y$ , as shown in the Figure 7. The canonical image contains most of the video\u2019s content, providing a base representation from which other frames can be derived. Consequently, the canonical spatial field in our video representation functions similarly to a key frame in video compression, serving as a reference for other frames in the sequence to query information. ", "page_idx": 9}, {"type": "text", "text": "5 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While ConVRT offers significant improvements in video atmospheric turbulence mitigation, there are two limitations. First, as a neural representation method, ConVRT\u2019s performance depends on accurate video representation and currently optimized to capture motion with precision in short clips. Extending this to longer sequences and more complex motions is a potential area for future exploration. Second, ConVRT processes a 25-frame video at 540x540 resolution in approximately 10 minutes, including DATUM as base method. Although much faster than Mao\u2019s (165 minutes) and Li\u2019s (300 minutes) methods, there is still room for improving computational efficiency, especially for larger or more complex sequences. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present ConVRT, a novel approach aimed at enhancing temporal consistency in video ATM tasks. ConVRT uses a dual-field approach\u2014Temporal Deformation Field and Spatial Content Field\u2014to accurately capture spatial information while regularizing temporal information, focusing on regular object motion rather than irregular turbulence. Combined with any ATM method, ConVRT leads to visibly improved temporal consistency. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment. H.C., M.X., K.Z., and C.A.M. were was supported in part by AFOSR Young Investigator Program Award no. FA9550-22-1-0208, ONR award no. N000142312752, and NSF CAREER Award no. 2339616. W.J. and A.V. were supported in part by ONR award no. N00014-23- 1-2714. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] S. H. Chan, \u201cTilt-then-blur or blur-then-tilt? clarifying the atmospheric turbulence model,\u201d IEEE Signal Processing Letters, vol. 29, pp. 1833\u20131837, 2022.   \n[2] X. Zhang, N. Chimitt, Y. Chi, Z. Mao, and S. H. Chan, \u201cSpatio-temporal turbulence mitigation: A translational perspective,\u201d arXiv preprint arXiv:2401.04244, 2024.   \n[3] X. Zhang, Z. Mao, N. Chimitt, and S. H. Chan, \u201cImaging through the atmosphere using turbulence mitigation transformer,\u201d IEEE Transactions on Computational Imaging, vol. 10, pp. 115\u2013128, 2024.   \n[4] D. Jin, Y. Chen, Y. Lu, J. Chen, P. Wang, Z. Liu, S. Guo, and X. Bai, \u201cNeutralizing the impact of atmospheric turbulence on complex scene imaging via deep learning,\u201d Nature Machine Intelligence, vol. 3, no. 10, pp. 876\u2013884, 2021.   \n[5] Z. Mao, A. Jaiswal, Z. Wang, and S. H. Chan, \u201cSingle frame atmospheric turbulence mitigation: A benchmark study and a new physics-inspired transformer model,\u201d in European Conference on Computer Vision, pp. 430\u2013446, Springer, 2022.   \n[6] A. Jaiswal, X. Zhang, S. H. Chan, and Z. Wang, \u201cPhysics-driven turbulence image restoration with stochastic refinement,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12170\u201312181, 2023.   \n[7] X. Zhang, Z. Mao, N. Chimitt, and S. H. Chan, \u201cImaging through the atmosphere using turbulence mitigation transformer,\u201d arXiv preprint arXiv:2207.06465, 2022.   \n[8] R. K. Saha, D. Qin, N. Li, J. Ye, and S. Jayasuriya, \u201cTurb-seg-res: A segment-then-restore pipeline for dynamic videos with atmospheric turbulence,\u201d arXiv preprint arXiv:2404.13605, 2024.   \n[9] Z. Mao, N. Chimitt, and S. H. Chan, \u201cImage reconstruction of static and dynamic scenes through anisoplanatic turbulence,\u201d IEEE Transactions on Computational Imaging, vol. 6, pp. 1415\u20131428, 2020.   \n[10] N. Li, S. Thapa, C. Whyte, A. W. Reed, S. Jayasuriya, and J. Ye, \u201cUnsupervised non-rigid image distortion removal via grid deformation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2522\u20132532, 2021.   \n[11] B. Y. Feng, M. Xie, and C. A. Metzler, \u201cTurbugan: An adversarial learning approach to spatially-varying multiframe blind deconvolution with applications to imaging through turbulence,\u201d IEEE Journal on Selected Areas in Information Theory, vol. 3, no. 3, pp. 543\u2013556, 2022.   \n[12] W. Jiang, V. Boominathan, and A. Veeraraghavan, \u201cNert: Implicit neural representations for unsupervised atmospheric turbulence mitigation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4235\u20134242, 2023.   \n[13] D. Lao, C. Wang, A. Wong, and S. Soatto, \u201cDiffeomorphic template registration for atmospheric turbulence mitigation,\u201d arXiv preprint arXiv:2405.03662, 2024.   \n[14] N. Chimitt and S. H. Chan, \u201cSimulating anisoplanatic turbulence by sampling correlated zernike coefficients,\u201d in 2020 IEEE International Conference on Computational Photography (ICCP), pp. 1\u201312, IEEE, 2020.   \n[15] Z. Mao, N. Chimitt, and S. H. Chan, \u201cAccelerating atmospheric turbulence simulation via learned phaseto-space transform,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14759\u201314768, 2021.   \n[16] N. Chimitt, X. Zhang, Z. Mao, and S. H. Chan, \u201cReal-time dense field phase-to-space simulation of imaging through atmospheric turbulence,\u201d IEEE Transactions on Computational Imaging, vol. 8, pp. 1159\u20131169, 2022.   \n[17] N. Chimitt and S. Chan, \u201cAnisoplanatic optical turbulence simulation for near-continuous c n 2 profiles without wave propagation,\u201d Optical Engineering, vol. 62, no. 7, pp. 078103\u2013078103, 2023.   \n[18] N. Chimitt, X. Zhang, Y. Chi, and S. H. Chan, \u201cScattering and gathering for spatially varying blurs,\u201d IEEE Transactions on Signal Processing, 2024.   \n[19] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, \u201cNerf: Representing scenes as neural radiance fields for view synthesis,\u201d Communications of the ACM, vol. 65, no. 1, pp. 99\u2013106, 2021.   \n[20] M. Tancik, P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. Barron, and R. Ng, \u201cFourier features let networks learn high frequency functions in low dimensional domains,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 7537\u20137547, 2020.   \n[21] I. Mehta, M. Gharbi, C. Barnes, E. Shechtman, R. Ramamoorthi, and M. Chandraker, \u201cModulated periodic activations for generalizable local functional representations,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14214\u201314223, 2021.   \n[22] B. Y. Feng and A. Varshney, \u201cSignet: Efficient neural representation for light fields,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14224\u201314233, 2021.   \n[23] B. Y. Feng, S. Jabbireddy, and A. Varshney, \u201cViinter: View interpolation with implicit neural representations of images,\u201d in SIGGRAPH Asia 2022 Conference Papers, pp. 1\u20139, 2022.   \n[24] B. Y. Feng and A. Varshney, \u201cNeural subspaces for light fields,\u201d IEEE Transactions on Visualization and Computer Graphics, 2022.   \n[25] T. M\u00fcller, A. Evans, C. Schied, and A. Keller, \u201cInstant neural graphics primitives with a multiresolution hash encoding,\u201d ACM Transactions on Graphics (ToG), vol. 41, no. 4, pp. 1\u201315, 2022.   \n[26] V. Sitzmann, J. Martel, A. Bergman, D. Lindell, and G. Wetzstein, \u201cImplicit neural representations with periodic activation functions,\u201d Advances in neural information processing systems, vol. 33, pp. 7462\u20137473, 2020.   \n[27] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove, \u201cDeepsdf: Learning continuous signed distance functions for shape representation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 165\u2013174, 2019.   \n[28] B. Y. Feng, Y. Zhang, D. Tang, R. Du, and A. Varshney, \u201cPrif: Primary ray-based implicit function,\u201d in European Conference on Computer Vision, pp. 138\u2013155, Springer, 2022.   \n[29] M. Qadri, K. Zhang, A. Hinduja, M. Kaess, A. Pediredla, and C. A. Metzler, \u201cAoneus: A neural rendering framework for acoustic-optical sensor fusion,\u201d in ACM SIGGRAPH 2024 Conference Papers, pp. 1\u201312, 2024.   \n[30] M. Xie, H. Cai, S. Shah, Y. Xu, B. Y. Feng, J.-B. Huang, and C. A. Metzler, \u201cFlash-splat: 3d reflection removal with flash cues and gaussian splats,\u201d in European Conference on Computer Vision, pp. 122\u2013139, Springer, 2025.   \n[31] H. Alzayer, K. Zhang, B. Feng, C. A. Metzler, and J.-B. Huang, \u201cSeeing the world through your eyes,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4864\u20134873, 2024.   \n[32] S. Shah, M. A. Chan, H. Cai, J. Chen, S. Kulshrestha, C. D. Singh, Y. Aloimonos, and C. A. Metzler, \u201cCodedevents: Optimal point-spread-function engineering for 3d-tracking with event cameras,\u201d Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \n[33] H. Zhou, B. Y. Feng, H. Guo, M. Liang, C. A. Metzler, C. Yang, et al., \u201cFpm-inr: Fourier ptychographic microscopy image stack reconstruction using implicit neural representations,\u201d arXiv preprint arXiv:2310.18529, 2023.   \n[34] H. Wang and L. Tian, \u201cLocal conditional neural fields for versatile and generalizable large-scale reconstructions in computational imaging,\u201d arXiv preprint arXiv:2307.06207, 2023.   \n[35] B. Y. Feng, H. Guo, M. Xie, V. Boominathan, M. K. Sharma, A. Veeraraghavan, and C. A. Metzler, \u201cNeuws: Neural wavefront shaping for guidestar-free imaging through static and dynamic scattering media,\u201d Science Advances, vol. 9, no. 26, p. eadg4671, 2023.   \n[36] E. Y. Lin, Z. Wang, R. Lin, D. Miau, F. Kainz, J. Chen, X. C. Zhang, D. B. Lindell, and K. N. Kutulakos, \u201cLearning lens blur fields,\u201d arXiv preprint arXiv:2310.11535, 2023.   \n[37] E. Bostan, R. Heckel, M. Chen, M. Kellman, and L. Waller, \u201cDeep phase decoder: self-calibrating phase microscopy with an untrained deep neural network,\u201d Optica, vol. 7, no. 6, pp. 559\u2013562, 2020.   \n[38] M. Xie, H. Guo, B. Y. Feng, L. Jin, A. Veeraraghavan, and C. A. Metzler, \u201cWavemo: Learning wavefront modulations to see through scattering,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 25276\u201325285, 2024.   \n[39] Z. Li, Q. Wang, F. Cole, R. Tucker, and N. Snavely, \u201cDynibar: Neural dynamic image-based rendering,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4273\u20134284, 2023.   \n[40] Q. Wang, Y.-Y. Chang, R. Cai, Z. Li, B. Hariharan, A. Holynski, and N. Snavely, \u201cTracking everything everywhere all at once,\u201d arXiv preprint arXiv:2306.05422, 2023.   \n[41] B. Y. Feng, H. Alzayer, M. Rubinstein, W. T. Freeman, and J.-B. Huang, \u201c3d motion magnification: Visualizing subtle motions from time-varying radiance fields,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9837\u20139846, 2023.   \n[42] H. Ouyang, Q. Wang, Y. Xiao, Q. Bai, J. Zhang, K. Zheng, X. Zhou, Q. Chen, and Y. Shen, \u201cCodef: Content deformation fields for temporally consistent video processing,\u201d arXiv preprint arXiv:2308.07926, 2023.   \n[43] Y. Kasten, D. Ofri, O. Wang, and T. Dekel, \u201cLayered neural atlases for consistent video editing,\u201d ACM Transactions on Graphics (TOG), vol. 40, no. 6, pp. 1\u201312, 2021.   \n[44] Y.-C. Lee, J.-Z. G. Jang, Y.-T. Chen, E. Qiu, and J.-B. Huang, \u201cShape-aware text-driven layered video editing,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14317\u201314326, 2023.   \n[45] V. Ye, Z. Li, R. Tucker, A. Kanazawa, and N. Snavely, \u201cDeformable sprites for unsupervised video decomposition,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2657\u20132666, 2022.   \n[46] D. L. Fried, \u201cProbability of getting a lucky short-exposure image through turbulence,\u201d JOSA, vol. 68, no. 12, pp. 1651\u20131658, 1978.   \n[47] R. J. Noll, \u201cZernike polynomials and atmospheric turbulence,\u201d JOsA, vol. 66, no. 3, pp. 207\u2013211, 1976.   \n[48] T. Caliskan and N. Arica, \u201cAtmospheric turbulence mitigation using optical flow,\u201d in 2014 22nd International Conference on Pattern Recognition, pp. 883\u2013888, Ieee, 2014.   \n[49] M. Shimizu, S. Yoshimura, M. Tanaka, and M. Okutomi, \u201cSuper-resolution from image sequence under influence of hot-air optical turbulence,\u201d in 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1\u20138, IEEE, 2008.   \n[50] J. Gilles, T. Dagobert, and C. De Franchis, \u201cAtmospheric turbulence restoration by diffeomorphic image registration and blind deconvolution,\u201d in Advanced Concepts for Intelligent Vision Systems: 10th International Conference, ACIVS 2008, Juan-les-Pins, France, October 20-24, 2008. Proceedings 10, pp. 400\u2013409, Springer, 2008.   \n[51] N. Anantrasirichai, A. Achim, and D. Bull, \u201cAtmospheric turbulence mitigation for sequences with moving objects using recursive image fusion,\u201d in 2018 25th IEEE international conference on image processing (ICIP), pp. 2895\u20132899, IEEE, 2018.   \n[52] J. Liang, J. Cao, Y. Fan, K. Zhang, R. Ranjan, Y. Li, R. Timofte, and L. Van Gool, \u201cVrt: A video restoration transformer,\u201d IEEE Transactions on Image Processing, 2024.   \n[53] K. C. Chan, X. Wang, K. Yu, C. Dong, and C. C. Loy, \u201cBasicvsr: The search for essential components in video super-resolution and beyond,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4947\u20134956, 2021.   \n[54] H. Chen, J. Ren, J. Gu, H. Wu, X. Lu, H. Cai, and L. Zhu, \u201cSnow removal in video: A new dataset and a novel method,\u201d in 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 13165\u201313176, IEEE, 2023.   \n[55] X. Zhang, H. Dong, J. Pan, C. Zhu, Y. Tai, C. Wang, J. Li, F. Huang, and F. Wang, \u201cLearning to restore hazy video: A new real-world dataset and a new method,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9239\u20139248, 2021.   \n[56] C. Lei, Y. Xing, and Q. Chen, \u201cBlind video temporal consistency via deep video prior,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 1083\u20131093, 2020.   \n[57] C. Lei, Y. Xing, H. Ouyang, and Q. Chen, \u201cDeep video prior for video consistency and propagation,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 1, pp. 356\u2013371, 2022.   \n[58] A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su, \u201cTensorf: Tensorial radiance fields,\u201d in European conference on computer vision, pp. 333\u2013350, Springer, 2022.   \n[59] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa, \u201cK-planes: Explicit radiance fields in space, time, and appearance,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12479\u201312488, 2023.   \n[60] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun, \u201cTowards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 44, no. 3, pp. 1623\u20131637, 2020.   \n[61] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \u201cImage quality assessment: from error visibility to structural similarity,\u201d IEEE transactions on image processing, vol. 13, no. 4, pp. 600\u2013612, 2004.   \n[62] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, \u201cThe unreasonable effectiveness of deep features as a perceptual metric,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586\u2013595, 2018.   \n[63] J. Gilles and N. B. Ferrante, \u201cOpen turbulent image set (otis),\u201d Pattern Recognition Letters, vol. 86, pp. 38\u201341, 2017.   \n[64] N. Anantrasirichai, \u201cAtmospheric turbulence removal with complex-valued convolutional neural network,\u201d Pattern Recognition Letters, vol. 171, pp. 69\u201375, 2023.   \n[65] D. Qin, R. K. Saha, W. Chung, S. Jayasuriya, J. Ye, and N. Li, \u201cUnsupervised moving object segmentation with atmospheric turbulence,\u201d in European Conference on Computer Vision, pp. 18\u201337, Springer, 2025.   \n[66] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.   \n[67] W.-S. Lai, J.-B. Huang, O. Wang, E. Shechtman, E. Yumer, and M.-H. Yang, \u201cLearning blind video temporal consistency,\u201d in Proceedings of the European conference on computer vision (ECCV), pp. 170\u2013185, 2018.   \n[68] M. Ruder, A. Dosovitskiy, and T. Brox, \u201cArtistic style transfer for videos,\u201d in Pattern Recognition: 38th German Conference, GCPR 2016, Hannover, Germany, September 12-15, 2016, Proceedings 38, pp. 26\u201336, Springer, 2016.   \n[69] Z. Li, R. Tucker, N. Snavely, and A. Holynski, \u201cGenerative image dynamics,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 24142\u201324153, 2024.   \n[70] B. D. Lucas and T. Kanade, \u201cAn iterative image registration technique with an application to stereo vision,\u201d in IJCAI\u201981: 7th international joint conference on Artificial intelligence, vol. 2, pp. 674\u2013679, 1981. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 MLP Network Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our network architecture consists of two MLPs: a content MLP and a deformation MLP. The content MLP has 4 fully-connected layers, with an input dimension of $Q_{1}$ , a hidden dimension of 128, and an output dimension of 2, representing $\\Delta x$ and $\\Delta y$ . The deformation MLP comprises 6 fully-connected layers, with an input dimension of $Q_{2}$ , a hidden dimension of 256, and an output of 3 channels representing RGB intensity. ", "page_idx": 13}, {"type": "text", "text": "A.2 Position Encoding ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Position encoding for spatial and temporal indices is embedded within the trainable feature map, as these indices are trainable. We directly use $x,\\,y_{:}$ , and $t$ to query the corresponding feature tensors from the feature maps. Notably, in the temporal feature map, neighboring features are shared across multiple frames, with each frame weighted differently due to explicit regularization. ", "page_idx": 13}, {"type": "image", "img_path": "yURca4wi2L/tmp/ec0f14245b375a59eab8218f58c07f928758618bc873123d7dc9b7c8ca0e8c63.jpg", "img_caption": ["Figure 11: Mitigation capability of our method without using base restoration methods for preprocessing. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.3 Additional Results on Mitigation Capability without Base Restoration Methods. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Additional results highlighting our method\u2019s mitigation capability independently of base restoration techniques are presented in Figure 11. ", "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: As shown in sections 1, 2, 3, 4, 5. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: Section 5 discusses the limitation of our method ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "This paper does not provide theoretical results. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: This paper provides detailed explanation of proposed method and experimental setup in sections 3, 4 and supplementary. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have a website to host all open-source resources, including code, data, and results. The website link is attached to the abstract of the main paper. We will organize code base and make it easy for users to use. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: All those experiment details are reported in section 4 Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: We didnt include error bars calculation. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Section 4 discusses the resource needed for our work ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The research is conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: 1 discusses the positive influence this work could bring to related research fields. For social-wise impact, the influence is difficult for us to figure out. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This work doesn\u2019t involve any possibility of being misused. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All assets used in this paper are open-source. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 19}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All related documents are well-studied. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Justification: We don\u2019t have experiments related to human subject. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We don\u2019t have experiments related to human subject. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]