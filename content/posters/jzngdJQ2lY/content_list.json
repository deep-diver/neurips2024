[{"type": "text", "text": "Solving Minimum-Cost Reach Avoid using Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Oswin So\\* Cheng Ge\\* Department of Aeronautics and Astronautics Department of Aeronautics and Astronautics MIT MIT oswinso@mit.edu gec_mike@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Chuchu Fan Department of Aeronautics and Astronautics MIT chuchu@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Current reinforcement-learning methods are unable to directly learn policies that solve the minimum cost reach-avoid problem to minimize cumulative costs subject to the constraints of reaching the goal and avoiding unsafe states, as the structure of this new optimization problem is incompatible with current methods. Instead, a surrogate problem is solved where all objectives are combined with a weighted sum. However, this surrogate objective results in suboptimal policies that do not directly minimize the cumulative cost. In this work, we propose RC-PPO, a reinforcement-learning-based method for solving the minimum-cost reach-avoid problem by using connections to Hamilton-Jacobi reachability. Empirical results demonstrate that RC-PPO learns policies with comparable goal-reaching rates to while achieving up to $57\\%$ lower cumulative costs compared to existing methods on a suite of minimum-cost reach-avoid benchmarks on the Mujoco simulator. The project page can be found at https://oswinso.xyz/rcppo/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many real-world tasks can be framed as a constrained optimization problem where reaching a goal at the terminal state and ensuring safety (i.e., reach-avoid) is desired while minimizing some cumulative cost as an objective function, which we term the minimum-cost reach-avoid problem. ", "page_idx": 0}, {"type": "text", "text": "The cumulative cost, which differentiates this from the traditional reach-avoid problem, can be used to model desirable aspects of a task such as minimizing energy consumption, maximizing smoothness, or any other pseudo-energy function, and allows for choosing the most desirable policy among many policies that can satisfy the reach-avoid requirements. For example, energy-efficient autonomous driving [1, 2] can be seen as a task where the vehicle must reach a destination, follow traffic rules, and minimize fuel consumption. Minimizing fuel use is also a major concern for low-thrust or energy-limited systems such as spacecraft [3] and quadrotors [4]. Quadrotors often have to choose limited battery life to meet the payload capacity. Hence, minimizing their energy consumption, which can be done by taking advantage of wind patterns, is crucial for keeping them airborne to complete more tasks. Other use-cases important for climate change include plasma fusion (reach a desired current, minimize the total risk of plasma disruption) [5] and voltage control (reach a desired voltage level, minimize the load shedding amount) [6]. ", "page_idx": 0}, {"type": "text", "text": "If only a single control trajectory is desired, this class of problems can be solved using numerical trajectory optimization by either optimizing the timestep between knot points [7] or a bilevel optimization approach that adjusts the number of knot points in an outer loop [8, 9, 10]. However, in this setting, the dynamics are assumed to be known, and only a single trajectory is obtained. Therefore, the computation will needs to be repeated when started from a different initial state. The computational complexity of trajectory optimization prevents it from being used in real time. Moreover, the use of nonlinear numerical optimization may result in poor solutions that lie in suboptimal local minim [11]. ", "page_idx": 1}, {"type": "text", "text": "Alternatively, to obtain a control policy, reinforcement learning (RL) can be used. However, existing methods are unable to directly solve the minimum-cost reach-avoid problem. Although RL has been used to solve many tasks where reaching a goal is desired, goal-reaching is encouraged as a reward instead of as a constraint via the use of either a sparse reward at the goal [12, 13, 14], or a surrogate dense reward $[14,15]^{1}$ . However, posing the reach constraint as a reward then makes it difficult to optimize for the cumulative cost at the same time. In many cases, this is done via a weighted sum of the two terms [18, 19, 20]. However, the optimal policy of this new surrogate objective may not necessarily be the optimal policy of the original problem. Another method of handling this is to treat the cumulative cost as a constraint and solve for a policy that maximizes the reward while keeping the cumulative cost under some fixed threshold, resulting in a new constrained optimization problem that can be solved as a constrained Markov decision process (CMDP) [21]. However, the choice of this fixed threshold becomes key: too small and the problem is not feasible, destabilizing the training process. Too large, and the resulting policy will simply ignore the cumulative cost. ", "page_idx": 1}, {"type": "text", "text": "To tackle this issue, we propose Reach Constrained Proximal Policy Optimization(RC-PPO), a new algorithm that targets the minimum-cost reach-avoid problem. We first convert the reach-avoid problem to a reach problem on an augmented system and use the corresponding reach value function to compute the optimal policy. Next, we use a novel two-step PPO-based RL-based framework to learn this value function and the corresponding optimal policy. The first step uses a PPO-inspired algorithm to solve for the optimal value function and policy, conditioned on the cost upper bound. The second step fine-tunes the value function and solves for the least upper bound on the cumulative cost to obtain the final optimal policy. Our main contributions are summarized below: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We prove that the minimum-cost reach-avoid problem can be solved by defining a set of augmented dynamics and a simplified constrained optimization problem. \u2022 We propose RC-PPO, a novel algorithm based on PPO that targets the minimum-cost reach-avoid problem, and prove that our algorithm converges to a locally optimal policy. \u2022 Simulation experiments show that RC-PPO achieves reach rates comparable with the baseline method with the highest reach rate while achieving significantly lower cumulative costs. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Terminal-horizon state-constrained optimization Terminal state constraints are quite common in the dynamic optimization literature. For the finite-horizon case, for example, one method of guaranteeing the stability of model predictive control (MPC) is with the use of a terminal state constraint [22]. Since MPC is implemented as a discrete-time finite-horizon numerical optimization problem, the terminal state constraints can be easily implemented in an optimization program as a normal state constraint. The case of a flexible-horizon constrained optimization is not as common but can still be found. For example, one method of time-optimal control is to treat the integration timestep as a control variable while imposing state constraints on the initial and final knot points [7]. Another method is to consider a bilevel optimization problem, where the number of knot points is optimized for in the outer loop [8, 9, 10]. ", "page_idx": 1}, {"type": "text", "text": "Goal-conditioned Reinforcement Learning There have been many works on goal-conditioned reinforcement learning. These works mainly focus on the challenges of tackling sparse rewards [12, 14, 23, 15] or even learning without rewards completely, either via representation learning objectives [24, 25, 26, 27, 28, 29, 30, 31, 32, 33] or by using contrastive learning to learn reward functions [34, 35, 36, 37, 38, 39, 40, 41, 42, 43], often in imitation learning settings [44, 45]. However, the manner in which these goals are reached is not considered, and it is difficult to extend these works to additionally minimize some cumulative cost. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Constrained Reinforcement Learning One way of using existing techniques to approximately tackle the minimum-cost reach-avoid problem is to filp the role of the cumulative-cost objective and the goal-reaching constraint by treating the goal-reaching constraint as an objective via a (sparse or dense) reward and the cumulative-cost objective as a constraint with a cost threshold, turning the problem into a CMDP [21]. In recent year, there has been significant interest in deep RL methods for solving CMDPs [46, 47, 48]. While these methods are effective at solving the transformed CMDP problem, the optimal policy to the CMDP may not be the optimal policy to the original minimum-cost reach-constrained problem, depending on the choice of the cost constraint. ", "page_idx": 2}, {"type": "text", "text": "State Augmentation in Constrained Reinforcement Learning To improve reward structures in constrained reinforcement learning, especially in safety-critical systems, one effective approach is state augmentation. This technique integrates constraints, such as safety or energy costs, into the augmented state representation, allowing for more effective constraint management through the reward mechanism [49, 50, 51]. While these methods enhance the reward structure for solving the transformed CMDP problems, they still face the inherent limitation of the CMDP framework: the optimal policy for the transformed CMDP may not always correspond to the optimal solution for the original problem. ", "page_idx": 2}, {"type": "text", "text": "Reachability Analysis Reachability analysis looks for solutions to the reach-avoid problem. That is, to solve for the set of initial conditions and an appropriate control policy to drive a system to a desired goal set while avoiding undesireable states. Hamilton-Jacobi (HJ) reachability analysis [52, 53, 54, 55, 56] provides a methodology for the case of dynamics in continuous-time via the solution of a partial differential equation (PDE) and is conventionally solved via numerical PDE techniques that use state-space discretization [54]. This has been extended recently to the case of discrete-time dynamics and solved using off-policy [57, 58] and on-policy [59, 60] reinforcement learning. While reachability analysis concerns itself with the reach-avoid problem, we are instead interested in solutions to the minimum-cost reach-avoid problem. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we consider a class of minimum-cost reach-avoid problems defined by the tuple $\\mathcal{M}:=\\langle\\mathcal{X},\\mathcal{U},f,c,g,h\\rangle$ . Here, $\\mathcal{X}\\subseteq\\mathbb{R}^{n}$ is the state space and $\\mathcal{U}\\subseteq\\mathbb{R}^{m}$ is the action space. The system states $x_{t}\\in\\mathscr{X}$ evolves under the deterministic discrete dynamics $f:\\mathcal{X}\\times\\mathcal{U}\\rightarrow\\mathcal{X}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t+1}=f(x_{t},u_{t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The control objective for the system states $x_{t}$ is to reach the goal region $\\mathcal{G}$ and avoid the unsafe set $\\mathcal{F}$ while minimizing the cumulative cost $\\textstyle\\sum_{t=0}^{T-1}c(x_{t},\\pi(x_{t}))$ under control input $u_{t}=\\pi(x_{t})$ for a designed control policy $\\pi:\\mathcal{X}\\rightarrow\\mathcal{U}$ . He re, $T$ denotes the first timestep that the agent reaches the goal $\\mathcal{G}$ . The sets $\\mathcal{G}$ and $\\mathcal{F}$ are given as the 0-sublevel and strict 0-superlevel sets $g:\\mathcal{X}\\to\\mathbb{R}$ and $h:\\mathcal{X}\\to\\mathbb{R}$ respectively, i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathcal{G}}:=\\{x\\in\\mathcal{X}\\mid g(x)\\leq0\\},\\quad\\mathcal{F}:=\\{x\\in\\mathcal{X}\\mid h(x)>0\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This can be formulated formally as finding a policy $\\pi$ that solves the following constrained flexible final-time optimization problem for a given initial state $x_{0}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\boldsymbol{\\pi},\\boldsymbol{T}}}&{\\sum_{t=0}^{T-1}c\\big(\\boldsymbol{x}_{t},\\boldsymbol{\\pi}(\\boldsymbol{x}_{t})\\big)}\\\\ {\\mathrm{s.t.}\\quad}&{x_{T}\\in\\mathcal{G},}\\\\ &{x_{t}\\notin\\mathcal{F}\\quad\\forall t\\in\\{0,\\ldots,T\\},}\\\\ &{x_{t+1}=f\\big(\\boldsymbol{x}_{t},\\boldsymbol{\\pi}(\\boldsymbol{x}_{t})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that as opposed to either traditional finite-horizon constrained optimization problems where $T$ is fixed or infinite-horizon problems where $T=\\infty$ , the time horizon $T$ is also a decision variable. Moreover, the goal constraint (3b) is only enforced at the terminal timestep $T$ . These two differences prevent the straightforward application of existing RL methods to solve (3). ", "page_idx": 2}, {"type": "text", "text": "3.1 Reachability Analysis for Reach-Avoid Problems ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In discrete time, the set of initial states that can reach the goal $\\mathcal{G}$ without entering the avoid set $\\mathcal{F}$ can be represented by the 0-sublevel set of a reach-avoid value function $V_{g,h}^{\\pi}$ [58]. Given functions $g$ , $h$ describing $\\mathcal{G}$ and $\\mathcal{F}$ and a policy $\\pi$ , the reach-avoid value function $V_{g,h}^{\\pi}:\\mathcal{X}\\to\\mathbb{R}$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nV_{g,h}^{\\pi}(x_{0})=\\underset{T\\in\\mathbb{N}}{\\mathrm{min}}\\,\\mathrm{max}\\,\\Big\\{g(x_{T}^{\\pi}),\\,\\underset{t\\in\\{0,\\dots,T\\}}{\\mathrm{max}}\\,h(x_{t}^{\\pi})\\Big\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\boldsymbol{x}_{t}^{\\pi}$ denote the system state at time $t$ under a policy $\\pi$ starting from an initial state $x_{0}^{\\pi}=x_{0}$ . In the rest of the paper, we suppress the argument $x_{0}$ for brevity whenever clear from the context. It can be shown that the reach-avoid value function satisfies the following recursive relationship via the reach-avoid Bellman equation (RABE) [58] ", "page_idx": 3}, {"type": "equation", "text": "$$\nV_{g,h}^{\\pi}(x_{t}^{\\pi})=\\operatorname*{max}\\Big\\{h(x_{t}^{\\pi}),\\ \\operatorname*{min}\\{g(x_{t}^{\\pi}),\\,V_{g,h}^{\\pi}(x_{t+1}^{\\pi})\\}\\Big\\},\\quad\\forall t\\geq0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The Bellman equation (5) can then be used in a reinforcement learning framework (e.g., via a modification of soft actor-critic[61, 62]) as done in [58] to solve the reach-avoid problem. ", "page_idx": 3}, {"type": "text", "text": "Note that existing methods of solving reach-avoid problems through this formulation focus on minimizing the value function $V_{g,h}^{\\pi}$ . This is not necessary as any policy that results in $V_{g,h}^{\\pi}\\leq0$ solves the reach-avoid problem, albeit without any cost considerations. However, it is often the case that we wish to minimize a cumulative cost (e.g., (3a)) on top of the reach-avoid constraints (3b)-(3c) for a minimum-cost reach-avoid problem. To address this class of problems, we next present a modification to the reach-avoid framework that additionally enables the minimization of the cumulative cost. ", "page_idx": 3}, {"type": "text", "text": "3.2 Reachability Analysis for Minimum-cost Reach-Avoid Problems ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now provide a new framework to solve the minimum-cost reach-avoid by lifting the original system to a higher dimensional space and designing a set of augmented dynamics that allow us to convert the original problem into a reachability problem on the augmented system. ", "page_idx": 3}, {"type": "text", "text": "Let I denote the shifted indicator function defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{I}_{b\\in B}:={\\left\\{\\begin{array}{l l}{+1}&{b\\in B,}\\\\ {-1}&{b\\not\\in B.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Define the augmented state as $\\hat{x}=(x,y,z)\\subseteq\\hat{x}:=\\mathcal{X}\\!\\times\\!\\{-1,1\\}\\!\\times\\!\\mathbb{R}$ . We now define a corresponding augmented dynamics function $f^{\\prime}:\\hat{\\mathcal{X}}\\times\\mathcal{U}\\rightarrow\\hat{\\mathcal{X}}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{f}\\big(x_{t},y_{t},z_{t},u_{t}\\big)=\\big(f(x_{t}),\\ \\operatorname*{max}\\{\\mathbb{I}_{f(x_{t})\\in\\mathcal{F}},\\,y_{t}\\},\\ z_{t}-c(x_{t},\\ u_{t})\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $y_{0}=\\mathbb{I}_{x_{0}\\in\\mathcal{F}}$ . Note that $y_{t}=1$ if the state has entered the avoid set $\\mathcal{F}$ at some timestep from 0 to $t$ and is unsafe, and $y_{t}=0$ if the state has not entered the avoid set $\\mathcal{F}$ at any timestep from 0 to $t$ and is safe. Moreover, $z_{t}$ is equal to $z_{0}$ minus the cost-to-come, i.e., for state trajectory $x_{0:t}$ and action trajectory $\\boldsymbol{u}_{0:t}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\nz_{t+1}=z_{0}-\\sum_{k=0}^{t}c(x_{t},u_{t}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Under the augmented dynamics, we now define the following augmented goal function ${\\hat{g}}:{\\hat{\\mathcal{X}}}\\rightarrow\\mathbb{R}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{g}(x,y,z):=\\operatorname*{max}\\{g(x),\\,C y,\\,-z\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C>0$ is an arbitrary constant.2 With this definition of $\\hat{g}$ , an augmented goal region $\\hat{\\mathcal G}$ can be defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\hat{\\mathcal{G}}}:=\\{{\\hat{x}}\\mid{\\hat{g}}({\\hat{x}})\\leq0\\}=\\{(x,y,z)\\mid x\\in{\\mathcal{G}},\\;y=-1,\\;z\\geq0\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In other words, starting from initial condition $\\hat{x}_{0}=(x_{0},y_{0},z_{0})$ , reaching the goal on the augmented system $\\hat{x}_{T}\\,\\in\\,\\hat{g}$ at timestep $T$ implies that 1) the goal is reached at $x_{T}$ for the original system, 2) the state trajectory remains safe and does not enter the avoid set $\\mathcal{F}$ , and 3) $z_{0}$ is an upper-bound on itnhteu ittoitoanl  ocno stth-et on-ecwolmy ed: e $\\sum_{t=0}^{T-1}c(x_{t},u_{t})\\,\\leq\\,z_{0}$ .m  iWs ef ocramlla tlihzies dt hine  tuhpe pfeorl-lboowuinngd  tphreoopreermt,y . wThohsee  apbroovoef is provided in Appendix D.1. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. For given initial conditions $x_{0}\\in\\mathcal{X}$ , $z_{0}\\in\\mathbb{R}$ and control policy $\\pi$ , consider the trajectory for the original system $\\{x_{0},\\ldots x_{T}\\}$ and its corresponding trajectory for the augmented system $\\left\\{(x_{0},y_{0},z_{0}),\\dots(x_{T},y_{T},z_{T})\\right\\}$ for some $T\\,>\\,0$ . Then, the reach constraint $x_{T}\\,\\in\\,\\mathcal{G}$ (3b), avoid constraint $x_{t}\\notin\\mathcal{F}\\,\\forall t\\in\\{0,1,\\dots,T\\}$ (3c) and the upper-bound property $\\begin{array}{r}{z_{0}\\ge\\sum_{k=0}^{T-1}c\\big(x_{k},\\pi(x_{k})\\big)}\\end{array}$ hold if and only if the augmented state reaches the augmented goal at time $T$ , i.e., $(x_{T},y_{T},z_{T})\\in\\hat{\\mathcal{G}}$ . ", "page_idx": 4}, {"type": "text", "text": "With this construction, we have folded the avoid constraints $x_{t}\\notin\\mathcal{F}\\left(3\\mathrm{c}\\right)$ into the reach specification on the augmented system. In other words, solving the reach problem on the augmented system results in a reach-avoid solution of the original system. As a result, we can simplify the value function (4) and Bellman equation (5), resulting in the following definition of the reach value function $\\tilde{V}_{\\hat{g}}:\\hat{\\mathcal{X}}\\rightarrow\\mathbb{R}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{V}_{\\hat{g}}^{\\pi}(\\hat{x}_{0})=\\operatorname*{min}_{t\\in\\mathbb{N}}\\hat{g}(\\hat{x}_{t}^{\\pi}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similar to (4), the 0-sublevel set of $\\tilde{V}_{\\hat{g}}$ describes the set of augmented states $\\hat{x}$ that can reach the augmented goal $\\hat{\\mathcal G}$ . We can also similarly obtain a recursive definition of the reach value function $\\tilde{V}_{\\hat{g}}$ given by the reachability Bellman equation (RBE) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{V}_{\\hat{g}}^{\\pi}(x_{t}^{\\pi},y_{t}^{\\pi},z_{t}^{\\pi})=\\operatorname*{min}\\left\\{\\hat{g}(x_{t}^{\\pi},\\ y_{t}^{\\pi},z_{t}^{\\pi}),\\tilde{V}_{\\hat{g}}^{\\pi}(x_{t+1}^{\\pi},y_{t+1}^{\\pi},z_{t+1}^{\\pi})\\right\\}\\ \\ \\forall t\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "whose proof we provide in Appendix D.2. ", "page_idx": 4}, {"type": "text", "text": "We now solve the minimum-cost reach-avoid problem using this augmented system. By Theorem 1, the $z_{0}$ is an upper bound on the cumulative cost to reach the goal while avoiding the unsafe set if and only if the augmented state $\\hat{x}$ reaches the augmented goal. Since this upper bound is tight, the least upper bound $z_{0}$ that still reaches the augmented goal thus corresponds to the minimum-cost policy that satisfies the reach-avoid constraints. In other words, the minimum-cost reach-avoid problem for a given initial state $x_{0}$ can be reformulated as the following optimization problem. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\pi,\\,z_{0}}{\\mathrm{min}}}&{{}z_{0}}\\\\ {\\mathrm{s.t.}\\;\\;}&{{}\\tilde{V}_{\\hat{g}}^{\\pi}(x_{0},\\mathbb{I}_{x_{0}\\in\\mathcal{F}},z_{0})\\leq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We refer to Appendix B for a detailed derivation of the equivalence between the transformed Problem 13 and the original minimum-cost reach-avoid Problem 3. ", "page_idx": 4}, {"type": "text", "text": "Remark 1 (Connections to the epigraph form in constrained optimization). The resulting optimization problem (13) can be interpreted as an epigraph reformulation [63] of the minimum-cost reach-avoid problem (3). The epigraph reformulation results in a problem with linear objective but yields the same solution as the original problem [63]. The construction we propose in this work can be seen as a dynamic version of this epigraph reformulation technique originally developed for static problems and is similar to recent results that also make use of the epigraph form for solving infinite-horizon constrained optimization problems [59]. ", "page_idx": 4}, {"type": "text", "text": "4 Solving with Reinforcement Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the previous section, we reformulated the minimum-cost reach-avoid problem by constructing an augmented system and used its reach value function (11) in a new constrained optimization problem (13) over the cost upper-bound $z_{0}$ . In this section, we propose Reachability Constrained Proximal Policy Optimization (RC-PPO), a two-phase RL-based method for solving (13) (see Figure 1). ", "page_idx": 4}, {"type": "text", "text": "4.1 Phase 1: Learn $z$ -conditioned policy and value function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the first step, we learn the optimal policy $\\pi$ and value function $\\tilde{V}_{\\hat{g}}^{\\pi}$ , as functions of the cost upper-bound $z_{0}$ , using RL. To do so, we consider the policy gradient framework [64]. However, since the policy gradient requires a stochastic policy in the case of deterministic dynamics [65], we consider an analog of the developments made in the previous section but for the case of a stochastic policy. To this end, we redefine the reach value function $\\tilde{V}_{\\hat{g}}^{\\pi}$ using a similar Bellman equation under a stochastic policy as follows. ", "page_idx": 4}, {"type": "image", "img_path": "jzngdJQ2lY/tmp/26da35036540b620a067248670306c78c38652736add1291e286d920d8174afe.jpg", "img_caption": ["Figure 1: Summary of the RC-PPO algorithm. In phase one, the original dynamic system is transformed into the augmented dynamic system defined in (7). Then RL is used to optimize value function $\\tilde{V}_{\\hat{g}}^{\\pi}$ and learn a stochastic policy $\\pi$ . In phase two, we fine-tune $\\tilde{V}_{\\hat{g}}^{\\pi}$ on a deterministic version of $\\pi$ and compute the optimal upper-bound $z^{*}$ to obtain the optimal deterministic policy $\\pi^{*}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Definition 1 (Stochastic Reachability Bellman Equation). Given function $\\hat{g}$ in (9), a stochastic policy $\\pi$ , and initial conditions $x_{0}\\in\\mathcal{X},z_{0}\\in\\mathbb{R}$ , the stochastic reach value function $\\tilde{V}_{\\hat{g}}^{\\pi}$ is defined as the solution to the following stochastic reachability Bellman equation (SRBE): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{V}_{\\hat{g}}^{\\pi}(\\hat{x}_{t})=\\mathbb{E}_{\\tau\\sim\\pi}[\\operatorname*{min}\\{\\hat{g}(\\hat{x}_{t}),\\tilde{V}_{\\hat{g}}^{\\pi}(\\hat{x}_{t+1})\\}]\\quad\\forall t\\ge0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{x}_{0}=(x_{0},y_{0},z_{0})$ with $y_{0}=\\mathbb{I}_{x_{0}\\in\\mathcal{F}}$ . ", "page_idx": 5}, {"type": "text", "text": "For this stochastic Bellman equation, the Q function [66] is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{Q}_{\\hat{g}}^{\\pi}(\\hat{x}_{t},u_{t})=\\operatorname*{min}\\{\\hat{g}(\\hat{x}_{t}),\\tilde{V}_{g}(\\hat{x}_{t+1})\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We next define the dynamics of our problem with stochastic policy below. ", "page_idx": 5}, {"type": "text", "text": "Definition 2 (Reachability Markov Decision Process). The Reachability Markov Decision Process is defined on the augmented dynamic in Equation (7) with an added absorbing state $s_{0}$ . We define the transition function $f_{r}^{\\prime}$ with the absorbing state as ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{r}^{\\prime}(\\hat{x},u)=\\left\\{\\begin{array}{l l}{\\hat{f}(\\hat{x},u),\\quad}&{\\mathrm{if}\\;\\tilde{V}_{\\hat{g}}^{\\pi}(\\hat{x})>\\hat{g}(\\hat{f}(\\hat{x},u)),}\\\\ {s_{0},}&{\\mathrm{if}\\;\\tilde{V}_{\\hat{g}}^{\\pi}(\\hat{x})\\le\\hat{g}(\\hat{f}(\\hat{x},u)).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Denote by $d_{\\pi}^{\\prime}(\\hat{x})$ the stationary distribution under stochastic policy $\\pi$ starting at $\\hat{x}\\in\\mathcal{X}\\times\\{-1,1\\}\\times\\mathbb{R}$ ", "page_idx": 5}, {"type": "text", "text": "We now derive a policy gradient theorem for the Reachability MDP in Definition 2 which yields an almost identical expression for the policy gradient. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. (Policy Gradient Theorem) For policy $\\pi_{\\theta}$ parameterized by $\\theta$ , the gradient of the policy value function $\\tilde{V}_{\\hat{g}}^{\\pi_{\\theta}}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\tilde{V}_{\\hat{g}}^{\\pi_{\\theta}}\\left(\\hat{x}\\right)\\propto\\mathbb{E}_{\\hat{x}^{\\prime}\\sim d_{\\pi}^{\\prime}(\\hat{x}),u\\sim\\pi_{\\theta}}\\left[\\tilde{Q}^{\\pi_{\\theta}}\\left(\\hat{x}^{\\prime},u\\right)\\nabla_{\\theta}\\ln\\pi_{\\theta}\\left(u\\mid\\hat{x}^{\\prime}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "under the stationary distribution $d_{\\pi}^{'}(\\hat{x})$ for Reachability MDP in Definition 2 ", "page_idx": 5}, {"type": "text", "text": "The proof of this new policy gradient theorem (Theorem 2) follows the proof of the normal policy gradient theorem [66], differing only in the expression of the stationary distribution. We provide the proof in Appendix D.3. ", "page_idx": 5}, {"type": "text", "text": "Since the stationary distribution $d_{\\pi}^{\\prime}(\\hat{x})$ in Definition 2 is hard to simulate during the learning process, we instead consider the stationary distribution under the original augmented dynamic system. Note that Definition 1 does not induce a contraction map, which harms performance. To fix this, we apply the same trick as [58] by introducing an additional discount factor $\\gamma$ into the Bellman equation (12): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{V}_{\\hat{g}}^{\\pi}(\\hat{x}_{t})=(1-\\gamma)\\hat{g}(\\hat{x}_{t})+\\gamma\\mathbb{E}_{\\hat{x}_{t+1}\\sim\\tau}[\\operatorname*{min}\\{\\hat{g}(\\hat{x}_{t}),\\tilde{V}_{\\hat{g}}^{\\pi}(\\hat{x}_{t+1})\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This provides us with a contraction map (proved in [58]) and we leave the discussion of choosing $\\gamma$ in Appendix C. The Q-function corresponding to (18) is then given as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{Q}_{\\hat{g}}^{\\pi}(\\hat{x}_{t},u_{t})=(1-\\gamma)\\hat{g}(\\hat{x}_{t})+\\gamma\\operatorname*{min}\\{\\hat{g}(\\hat{x}_{t}),\\tilde{V}_{\\hat{g}}^{\\pi}(\\hat{x}_{t+1})\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Following proximal policy optimization (PPO) [67], we use generalized advantage estimation (GAE) [68] to compute a variance-reduced advantage function $\\tilde{A}_{\\hat{g}}^{\\pi}\\,=\\,\\tilde{Q}_{\\hat{g}}^{\\pi}\\,-\\,\\tilde{V}_{\\hat{g}}^{\\pi}$ for the policy gradient (Theorem 2) using the $\\lambda$ -return [66]. We refer to Appendix A for the definition of $\\hat{A}_{\\hat{g}}^{\\pi(\\mathrm{GAE})}$ and denote the loss function when $\\theta=\\theta_{l}$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\quad\\ \\mathcal{I}_{\\pi}(\\theta)=\\mathbb{E}_{\\hat{x},u\\sim\\pi_{\\theta_{l}}}\\left[\\ A^{\\pi_{\\theta_{l}}}(\\hat{x},u)\\right],}\\\\ &{\\overline{{A^{\\pi_{\\theta_{l}}}}}(\\hat{x},u)=\\operatorname*{max}\\left(-\\frac{\\pi_{\\theta}(u\\ |\\ \\hat{x})}{\\pi_{\\theta_{l}}(u\\ |\\ \\hat{x})}\\hat{A}_{\\hat{g}}^{\\pi_{\\theta_{l}}(G A E)}(\\hat{x},u),\\ \\mathrm{CLIP}\\left(\\epsilon,-\\hat{A}_{\\hat{g}}^{\\pi_{\\theta_{l}}(G A E)}(\\hat{x},u)\\right)\\right).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We wish to obtain the optimal policy $\\pi$ and the value function $\\tilde{V}_{\\hat{g}}^{\\pi_{\\theta}}$ conditioned on $z_{0}$ . Hence, at the beginning of each rollout, we uniformly sample $z_{0}$ within a user-specified range $[z_{\\mathrm{min}},z_{\\mathrm{max}}]$ . Since the optimal $z_{0}$ is the cumulative cost of the policy that solves the minimum-cost reach-avoid problem, $z_{\\mathrm{min}}$ and $z_{\\mathrm{max}}$ are user-specified bounds on the optimal cost. In particular, when the cost-function is bounded and the optimal cost is non-negative, we can choose $z_{\\mathrm{min}}$ to be some negative number and $z_{\\mathrm{max}}$ to be the maximum possible discounted cost. ", "page_idx": 6}, {"type": "text", "text": "4.2 Phase 2: Solving for the optimal $z$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the second phase, we first compute a deterministic version $\\pi^{*}$ of the stochastic policy $\\pi$ from phase 1 by taking the mode. Next, we fine-tune $V_{\\hat{g}}^{\\pi}$ based on the now deterministic $\\pi^{*}$ to obtain $\\tilde{V}_{\\hat{g}}^{*}$ . ", "page_idx": 6}, {"type": "text", "text": "Given any state $x$ , the final policy is then obtained by solving for the optimal cost upper-bound $z^{*}$ from Equation (13), which is a 1D root-finding problem and can be easily solved using bisection. Note that Equation (13) must be solved online for $z^{*}$ at each state $x$ . Alternatively, to avoid performing bisection online, we can instead learn the map $(x,y)\\mapsto z^{*}$ offline using regression with randomly sampled $(x,y)$ pairs and $z^{*}$ labels obtained from bisection offline. ", "page_idx": 6}, {"type": "text", "text": "We provide a convergence proof of an actor-critic version of our method without the GAE estimator in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Baselines We consider two categories of RL baselines. The first is goal-conditioned reinforcement learning which focuses on goal-reaching but does not consider minimization of the cost. For this category, we consider the Contrastive Reinforcement Learning (CRL) [33] method. We also compare against safe RL methods that solve CMDPs. As the minimum-cost reach-avoid problem (3) cannot be posed as a CMDP, we reformulate (3) into the following surrogate CMDP: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\pi}}&{\\mathbb{E}_{\\boldsymbol{x}_{t},\\boldsymbol{u}_{t}\\sim d_{\\pi}}\\displaystyle\\sum_{t}\\big[-\\gamma^{t}\\boldsymbol{r}(\\boldsymbol{x}_{t},\\boldsymbol{u}_{t})\\big]}\\\\ {\\mathrm{s.t.}\\quad\\mathbb{E}_{\\boldsymbol{x}_{t},\\boldsymbol{u}_{t}\\sim d_{\\pi}}\\displaystyle\\sum_{t}\\big[\\gamma^{t}\\mathbb{1}_{\\boldsymbol{x}_{t}\\in\\mathcal{F}}\\times C_{\\mathrm{fail}}\\big]\\leq0,}\\\\ &{\\mathbb{E}_{\\boldsymbol{x}_{t},\\boldsymbol{u}_{t}\\sim d_{\\pi}}\\displaystyle\\sum_{t}\\big[\\gamma^{t}c(\\boldsymbol{x}_{t},\\boldsymbol{u}_{t})\\big]\\leq\\mathcal{X}_{\\mathrm{threshold}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the reward $r$ incentivies goal-reaching, $C_{\\mathrm{fail}}$ is a term balancing two constraint terms, and $\\mathcal{X}_{\\mathrm{threshold}}$ is a hyperparameter on the cumulative cost. For this category, we consider the CPPO [48] and RESPO [60]. Note that RESPO also incorporates reachability analysis to adapt the Lagrange multipliers for each constraint term. We implement the above CMDP-based baselines with three different choices of $\\mathcal{X}_{\\mathrm{thresholds}};\\,\\mathcal{X}_{\\mathrm{L}},\\,\\mathcal{X}$ $\\mathcal{X}_{\\mathrm{M}}$ and $\\mathcal{X}_{\\mathrm{H}}$ . For RESPO, we found $\\mathcal{X}_{\\mathrm{M}}$ to outperform both $\\scriptstyle\\mathcal{X}_{\\mathrm{L}}$ and $\\chi_{\\mathrm{{H}}}$ and thus only report results for $\\mathcal{X}_{\\mathrm{M}}$ . ", "page_idx": 6}, {"type": "text", "text": "We also consider the static Lagrangian multiplier case. In this setting, the reward function becomes $r(x_{t})-\\beta(\\mathbb{1}_{x_{t}\\in\\mathcal{F}}\\times C_{\\mathrm{fail}}+c(\\bar{x_{t}},u_{t}^{'}))$ for a constant Lagrange multiplier $\\beta$ . We consider two different levels of $\\beta\\ (\\beta_{\\mathrm{L}},\\,\\beta_{\\mathrm{H}})$ in our experiments, resulting in the baselines $\\mathrm{PPO}_{-}\\beta_{\\mathrm{L}}$ , $\\mathrm{PPO}_{-}\\beta_{\\mathrm{H}}$ , $\\mathbf{SAC}_{-}\\beta_{\\mathrm{L}}$ , $\\mathrm{SAC}_{-}\\beta_{\\mathrm{H}}$ . More details are provided in Appendix $\\boldsymbol{\\mathrm F}$ . ", "page_idx": 6}, {"type": "image", "img_path": "jzngdJQ2lY/tmp/daed92b3edd2551ac33f9754b24040414125b907c741c3f36b5a0896ac0750c7.jpg", "img_caption": ["Figure 2: Illustrations of the benchmark tasks. In each picture, red denotes the unsafe region to be avoided, while green denotes the goal region to be reached. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "jzngdJQ2lY/tmp/4136825712b93254514e76a16aaa3aab47f105a317120b11abd964a3198a9f1f.jpg", "img_caption": ["Figure 3: Reach rates under the sparse reward setting. RC-PPO consistently achieves the highest reach rates in all benchmark tasks. Error bars denote the standard error. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Benchmarks We compare RC-PPO with baseline methods on several minimum-cost reach-avoid environments. We consider an inverted pendulum (Pendulum), an environment from Safety Gym [69] (PointGoal) and two custom environments from MuJoCo [70], (Safety Hopper, Safety HalfCheetah) with added hazard regions and goal regions. We also consider a 3D quadrotor navigation task in a simulated wind field for an urban environment [71, 72] (WindField) and an Fixed-Wing avoid task from [59] with an additional goal region (FixedWing). More details on the benchmark can be found in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics Since the goal of RC-PPO is minimizing cost consumption while reaching goal without entering the unsafe region $\\mathcal{F}$ . We evaluate algorithm performance based on (i) reach rate, (ii) cost. The reach rate is the ratio of trajectories that enter goal region $\\mathcal{G}$ without violating safety along the trajectory. The cost denotes the cumulative cost over the trajectory $\\textstyle\\sum_{k=0}^{T}c(x_{k},\\pi({\\bar{x}}_{k}))$ . ", "page_idx": 7}, {"type": "text", "text": "5.1 Sparse Reward Setting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first compare our algorithm with other baseline algorithms under a sparse reward setting (Figure 3). In all environments, the reach rate for the baseline algorithms is very low. Also, there is a general trend between the reach rate and the Lagrangian coefficient. $\\mathrm{CPPO}_{-}\\chi_{\\mathrm{L}}$ , $\\mathrm{PPO}_{-}\\beta_{\\mathrm{H}}$ and $\\mathsf{S A C}_{-}\\beta_{\\mathrm{H}}$ have higher Lagrangian coefficients which lead to a lower reach rate. ", "page_idx": 7}, {"type": "text", "text": "5.2 Comparison under Reward Shaping ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Reward shaping is a common method that can be used to improve the performance of RL algorithms, especially in the sparse reward setting [73, 74]. To see whether the same conclusions still hold even in the presence of reward shaping, we retrain the baseline methods but with reward shaping using a distance function-based potential function (see Appendix F for more details). ", "page_idx": 7}, {"type": "text", "text": "The results in Figure 4 demonstrate that RC-PPO remains competitive against the best baseline algorithms in reach rate while achieving significantly lower cumulative costs. The baseline methods (PPO_ $\\beta_{\\mathrm{H}}$ , SAC_ $\\beta_{\\mathrm{H}}$ , $\\mathrm{CPPO}_{-}\\mathcal{X}_{\\mathrm{L}})$ ) fail to achieve a high reach rate due to the large weights placed on minimizing the cumulative cost. CRL can reach the goal for simpler environments (Pendulum) but struggles with more complex environments. However, since goal-conditioned methods do not consider minimize cumulative cost, it achieves a higher cumulative cost relative to other methods. ", "page_idx": 7}, {"type": "image", "img_path": "jzngdJQ2lY/tmp/3f2a8b0e6b7ac046ea0295083502cfe58a1448b210ca378ce659aa2470ada4c8.jpg", "img_caption": ["Figure 4: Cumulative cost (IQM) and reach rates under reward shaping on four selected benchmarks. RC-PPO achieves significantly lower cumulative costs while retaining comparable reach rates even when compared with baseline methods that use reward shaping. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "jzngdJQ2lY/tmp/4baf6d7ad1df7d1826f50590b7ecb48c442af380545668d9931e47966a965a91.jpg", "img_caption": ["Figure 5: Trajectory comparisons. On Pendulum, RC-PPO learns to perform an extensive energy pumping strategy to reach the goal upright position (green line), resulting in vastly lower cumulative energy. On WindField, RC-PPO takes advantage instead of fighting against the wind field, resulting in a faster trajectory to the goal region (green box) that uses lower cumulative energy. The start of the trajectory is marked by $\\mid$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Other baselines focus more on goal-reaching tasks while putting less emphasis on the cost part. As a result, they suffer from higher costs than RC-PPO. We can also observe that RESPO achieves lower cumulative cost compared to $\\mathrm{CPPO}_{-}\\chi_{\\mathrm{M}}$ which shares the same $\\mathcal{X}_{\\mathrm{threshold}}$ . This is due to RESPO making use of reachability analysis to better satisfy constraints. ", "page_idx": 8}, {"type": "text", "text": "To see how RC-PPO achieves lower cumulative costs, we visualize the resulting trajectories for Pendulum and WindField in Figure 5. For Pendulum, we see that RC-PPO learns to perform energy pumping to reach the goal in more time but with a smaller cumulative cost. The optimal behavior is opposite in the case of WindField, which contains an additional constant term in the cost to model the energy draw of quadcopters (see Appendix G). Here, we see that RC-PPO takes advantage of the wind at the beginning by moving downwind, arriving at the goal faster and with less cumulative cost. ", "page_idx": 8}, {"type": "text", "text": "We also visualize the learned RC-PPO policy for different values of $z$ on the Pendulum benchmark (see Appendix H.2). For small values of $z$ , the policy learns to minimize the cost, but at the expense of not reaching the goal. For large values of $z$ , the policy reaches the goal quickly but at the expense of a large cost. The optimal $z_{\\mathrm{opt}}$ found using the learned value function $\\tilde{V}_{\\hat{g}}^{\\pi_{\\theta}}$ finds the $z$ that minimizes the cumulative cost but is still able to reach the goal. ", "page_idx": 8}, {"type": "text", "text": "5.3 Optimal solution of minimum-cost reach-avoid cannot be obtained using CMDP ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Though the previous subsections show the performance benefits of RC-PPO over existing methods, this may be due to badly chosen hyperparameters for the baseline methods, particularly in the formulation of the surrogate CMDP (22). We thus pose the following question: Can CMDP methods perform well under the right parameters of the surrogate CMDP problem (22) ?. ", "page_idx": 8}, {"type": "image", "img_path": "jzngdJQ2lY/tmp/bee20575db27fa65a3ad45cfe1dc9fafad53be1534198038a8714946abe95dcc.jpg", "img_caption": ["Figure 6: Pareto front of PPO across different reward coefficients. RC-PPO outperforms the entire Pareto front of what can be achieved by varying the reward function coefficients of the surrogate CMDP problem when solved using PPO. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Empirical Study. To answer this question, we first perform an extensive grid search over both the different coefficients in (22) and the static Lagrange multiplier for PPO (see Appendix H.3) and plot the result in Figure 6. RC-PPO outperforms the entire Pareto front formed from this grid search, providing experimental evidence that the performance improvements of RC-PPO stem from having a better problem formulation as opposed to badly chosen hyperparameters for the baselines. ", "page_idx": 9}, {"type": "text", "text": "Theoretical Analysis on Simple Example. To complement the empirical study, we provide an example of a simple minimum-cost reach-avoid problem where we prove that no choice of hyperparameter leads to the optimal solution in Appendix I. ", "page_idx": 9}, {"type": "text", "text": "5.4 Robustness to Noise ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Finally, we investigate the robustness to varying levels of control noise in Appendix H.4. Even with the added noise, RC-PPO achieves the lowest cumulative cost while maintaining a comparable reach rate to other methods. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have proposed RC-PPO, a novel reinforcement learning algorithm for solving minimum-cost reach-avoid problems. We have demonstrated the strong capabilities of RC-PPO over prior methods in solving a multitude of challenging benchmark problems, where RC-PPO learns policies that match the reach rates of existing methods while achieving significantly lower cumulative costs. ", "page_idx": 9}, {"type": "text", "text": "However, it should be noted that RC-PPO is not without limitations. First, the use of augmented dynamics enables folding the safety constraints within the goal specifications through an additional binary state variable. While this reduces the complexity of the resulting algorithm, it also means that two policies that are both unable to reach the goal can have the same value $\\tilde{V}_{g^{\\prime}}^{\\pi}$ even if one is unsafe, which can be undesirable. Next, the theoretical developments of RC-PPO are dependent on the assumptions of deterministic dynamics, which can be quite restrictive as it precludes the use of commonly used techniques for real-world deployment such as domain randomization. We acknowledge these limitations and leave resolving these challenges as future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partly supported by the National Science Foundation (NSF) CAREER Award #CCF2238030, the MIT Lincoln Lab, and the MIT-DSTA program. Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and don\u2019t necessarily reflect the views of the sponsors. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Xuewei Qi, Yadan Luo, Guoyuan Wu, Kanok Boriboonsomsin, and Matthew Barth. Deep reinforcement learning enabled self-learning control for energy efficient driving. Transportation Research Part C: Emerging Technologies, 99:67\u201381, 2019.   \n[2] Ying Zhang, Tao You, Jinchao Chen, Chenglie Du, Zhaoyang Ai, and Xiaobo Qu. Safe and energy-saving vehicle-following driving decision-making framework of autonomous vehicles. IEEE Transactions on Industrial Electronics, 69(12):13859\u201313871, 2021.   \n[3] Mirco Rasotto, Roberto Armellin, and Pierluigi Di Lizia. Multi-step optimization strategy for fuel-optimal orbital transfer of low-thrust spacecraft. Engineering Optimization, 48(3):519\u2013542, 2016.   \n[4] Jack Langelaan. Long distance/duration trajectory optimization for small uavs. In AIAA guidance, navigation and control conference and exhibit, page 6737, 2007.   \n[5] Allen M Wang, Oswin So, Charles Dawson, Darren T Garnier, Cristina Rea, and Chuchu Fan. Active disruption avoidance and trajectory design for tokamak ramp-downs with neural differential equations and reinforcement learning. arXiv preprint arXiv:2402.09387, 2024.   \n[6] Renke Huang, Yujiao Chen, Tianzhixi Yin, Xinya Li, Ang Li, Jie Tan, Wenhao Yu, Yuan Liu, and Qiuhua Huang. Accelerated deep reinforcement learning based load shedding for emergency voltage control. arXiv preprint arXiv:2006.12667, 2020.   \n[7] Christoph R\u00f6smann, Frank Hoffmann, and Torsten Bertram. Timed-elastic-bands for timeoptimal point-to-point nonlinear model predictive control. In 2015 european control conference (ECC), pages 3352\u20133357. IEEE, 2015.   \n[8] Robin M Pinson and Ping Lu. Trajectory design employing convex optimization for landing on irregularly shaped asteroids. Journal of Guidance, Control, and Dynamics, 41(6):1243\u20131256, 2018.   \n[9] Haichao Hong, Arnab Maity, and Florian Holzapfel. Free final-time constrained sequential quadratic programming\u2013based flight vehicle guidance. Journal of Guidance, Control, and Dynamics, 44(1):181\u2013189, 2021.   \n[10] Kyle Stachowicz and Evangelos A Theodorou. Optimal-horizon model predictive control with differential dynamic programming. In 2022 International Conference on Robotics and Automation (ICRA), pages 1440\u20131446. IEEE, 2022.   \n[11] Damien Ernst, Mevludin Glavic, Florin Capitanescu, and Louis Wehenkel. Reinforcement learning versus model predictive control: a comparison on a power system problem. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(2):517\u2013529, 2008.   \n[12] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017.   \n[13] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.   \n[14] Alexander Trott, Stephan Zheng, Caiming Xiong, and Richard Socher. Keeping your distance: Solving sparse reward tasks using self-balancing shaped rewards. Advances in Neural Information Processing Systems, 32, 2019.   \n[15] Minghuan Liu, Menghui Zhu, and Weinan Zhang. Goal-conditioned reinforcement learning: Problems and solutions. arXiv preprint arXiv:2201.08299, 2022.   \n[16] Jack Clark and Dario Amodei. Faulty reward functions in the wild, 2016.   \n[17] Pulkit Agrawal. The task specification problem. In Conference on Robot Learning, pages 1745\u20131751. PMLR, 2022.   \n[18] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.   \n[19] Gabriel B Margolis, Tao Chen, Kartik Paigwar, Xiang Fu, Donghyun Kim, Sangbae Kim, and Pulkit Agrawal. Learning to jump from pixels. arXiv preprint arXiv:2110.15344, 2021.   \n[20] Dhawal Gupta, Yash Chandak, Scott Jordan, Philip S Thomas, and Bruno C da Silva. Behavior alignment via reward function optimization. Advances in Neural Information Processing Systems, 36, 2024.   \n[21] Eitan Altman. Constrained Markov decision processes. Routledge, 2004.   \n[22] Lorenzo Fagiano and Andrew R Teel. Generalized terminal state constraint for model predictive control. Automatica, 49(9):2622\u20132631, 2013.   \n[23] Yifan Wu, George Tucker, and Ofir Nachum. The laplacian in rl: Learning representations with efficient approximations. arXiv preprint arXiv:1810.04586, 2018.   \n[24] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE international conference on robotics and automation (ICRA), pages 6292\u20136299. IEEE, 2018.   \n[25] Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals via iterated supervised learning. arXiv preprint arXiv:1912.06088, 2019.   \n[26] Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. Advances in neural information processing systems, 31, 2018.   \n[27] David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. arXiv preprint arXiv:1811.11359, 2018.   \n[28] Hao Sun, Zhizhong Li, Xiaotong Liu, Bolei Zhou, and Dahua Lin. Policy continuation with hindsight inverse dynamics. Advances in Neural Information Processing Systems, 32, 2019.   \n[29] Suraj Nair and Chelsea Finn. Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation. arXiv preprint arXiv:1909.05829, 2019.   \n[30] Andres Campero, Roberta Raileanu, Heinrich K\u00fcttler, Joshua B Tenenbaum, Tim Rockt\u00e4schel, and Edward Grefenstette. Learning with amigo: Adversarially motivated intrinsic goals. arXiv preprint arXiv:2006.12122, 2020.   \n[31] Suraj Nair, Silvio Savarese, and Chelsea Finn. Goal-aware prediction: Learning to model what matters. In International Conference on Machine Learning, pages 7207\u20137219. PMLR, 2020.   \n[32] Russell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner, and Deepak Pathak. Discovering and achieving goals via world models. Advances in Neural Information Processing Systems, 34:24379\u201324391, 2021.   \n[33] Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Russ R Salakhutdinov. Contrastive learning as goal-conditioned reinforcement learning. Advances in Neural Information Processing Systems, 35:35603\u201335620, 2022.   \n[34] David Fischinger, Markus Vincze, and Yun Jiang. Learning grasps for unknown objects in cluttered scenes. In 2013 IEEE international conference on robotics and automation, pages 609\u2013616. IEEE, 2013.   \n[35] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.   \n[36] Annie Xie, Avi Singh, Sergey Levine, and Chelsea Finn. Few-shot goal inference for visuomotor learning and planning. In Conference on Robot Learning, pages 40\u201352. PMLR, 2018.   \n[37] Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse control with events: A general framework for data-driven reward definition. Advances in neural information processing systems, 31, 2018.   \n[38] Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations. In International conference on machine learning, pages 783\u2013792. PMLR, 2019.   \n[39] Ksenia Konyushkova, Konrad Zolna, Yusuf Aytar, Alexander Novikov, Scott Reed, Serkan Cabi, and Nando de Freitas. Semi-supervised reward learning for offilne reinforcement learning. arXiv preprint arXiv:2012.06899, 2020.   \n[40] Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv preprint arXiv:2104.08212, 2021.   \n[41] Danfei Xu and Misha Denil. Positive-unlabeled reward learning. In Conference on Robot Learning, pages 205\u2013219. PMLR, 2021.   \n[42] Konrad Zolna, Scott Reed, Alexander Novikov, Sergio Gomez Colmenarejo, David Budden, Serkan Cabi, Misha Denil, Nando de Freitas, and Ziyu Wang. Task-relevant adversarial imitation learning. In Conference on Robot Learning, pages 247\u2013263. PMLR, 2021.   \n[43] Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, et al. Learning languageconditioned robot behavior from offilne data and crowd-sourced annotation. In Conference on Robot Learning, pages 1303\u20131315. PMLR, 2022.   \n[44] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016.   \n[45] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.   \n[46] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International conference on machine learning, pages 22\u201331. PMLR, 2017.   \n[47] Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization. arXiv preprint arXiv:1805.11074, 2018.   \n[48] Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning by pid lagrangian methods. In International Conference on Machine Learning, pages 9133\u20139143. PMLR, 2020.   \n[49] Aivar Sootla, Alexander I Cowen-Rivers, Taher Jafferjee, Ziyan Wang, David H Mguni, Jun Wang, and Haitham Ammar. Saut\u00e9 rl: Almost surely safe reinforcement learning using state augmentation. In International Conference on Machine Learning, pages 20423\u201320443. PMLR, 2022.   \n[50] Hao Jiang, Tien Mai, Pradeep Varakantham, and Minh Huy Hoang. Solving richly constrained reinforcement learning through state augmentation and reward penalties. arXiv preprint arXiv:2301.11592, 2023.   \n[51] Hao Jiang, Tien Mai, Pradeep Varakantham, and Huy Hoang. Reward penalties on augmented states for solving richly constrained rl effectively. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19867\u201319875, 2024.   \n[52] Claire J Tomlin, John Lygeros, and S Shankar Sastry. A game theoretic approach to controller design for hybrid systems. Proceedings of the IEEE, 88(7):949\u2013970, 2000.   \n[53] John Lygeros. On reachability and minimum cost optimal control. Automatica, 40(6):917\u2013927, 2004.   \n[54] Ian M Mitchell, Alexandre M Bayen, and Claire J Tomlin. A time-dependent hamilton-jacobi formulation of reachable sets for continuous dynamic games. IEEE Transactions on automatic control, 50(7):947\u2013957, 2005.   \n[55] Kostas Margellos and John Lygeros. Hamilton\u2013jacobi formulation for reach\u2013avoid differential games. IEEE Transactions on automatic control, 56(8):1849\u20131861, 2011.   \n[56] Somil Bansal, Mo Chen, Sylvia Herbert, and Claire J Tomlin. Hamilton-jacobi reachability: A brief overview and recent advances. In 2017 IEEE 56th Annual Conference on Decision and Control (CDC), pages 2242\u20132253. IEEE, 2017.   \n[57] Jaime F Fisac, Neil F Lugovoy, Vicen\u00e7 Rubies-Royo, Shromona Ghosh, and Claire J Tomlin. Bridging hamilton-jacobi safety analysis and reinforcement learning. In 2019 International Conference on Robotics and Automation (ICRA), pages 8550\u20138556. IEEE, 2019.   \n[58] Kai Chieh Hsu, Vicen\u00e7 Rubies-Royo, Claire J Tomlin, and Jaime F Fisac. Safety and liveness guarantees through reach-avoid reinforcement learning. In 17th Robotics: Science and Systems, RSS 2021. MIT Press Journals, 2021.   \n[59] Oswin So and Chuchu Fan. Solving stabilize-avoid optimal control via epigraph form and deep reinforcement learning. arXiv preprint arXiv:2305.14154, 2023.   \n[60] Milan Ganai, Zheng Gong, Chenning Yu, Sylvia Herbert, and Sicun Gao. Iterative reachability estimation for safe reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[61] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.   \n[62] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[63] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[64] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999.   \n[65] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International conference on machine learning, pages 387\u2013395. Pmlr, 2014.   \n[66] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[67] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[68] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.   \n[69] Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement learning. arXiv preprint arXiv:1910.01708, 7(1):2, 2019.   \n[70] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026\u20135033. IEEE, 2012.   \n[71] Steven Waslander and Carlos Wang. Wind disturbance estimation and rejection for quadrotor position control. In AIAA Infotech@ Aerospace conference and AIAA unmanned... Unlimited conference, page 1983, 2009.   \n[72] Sanjeeb T Bose and George Ilhwan Park. Wall-modeled large-eddy simulation for complex turbulent flows. Annual review of fluid mechanics, 50:535\u2013561, 2018.   \n[73] Maja J Mataric. Reward functions for accelerated learning. In Machine learning proceedings 1994, pages 181\u2013189. Elsevier, 1994.   \n[74] Andrew $\\textrm{Y N g}$ , Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pages 278\u2013287, 1999.   \n[75] Dongjie Yu, Haitong Ma, Shengbo Li, and Jianyu Chen. Reachability constrained reinforcement learning. In International Conference on Machine Learning, pages 25636\u201325655. PMLR, 2022.   \n[76] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A GAE estimator Definition ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Note, however, that the definition of return (19) is different from the original definition and hence will result in a different equation for the GAE. ", "page_idx": 15}, {"type": "text", "text": "To simplify the form of the GAE, we first define a \u201creduction\u201d function $\\phi^{(n)}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ that applies itself recursively to its $n$ arguments, i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi^{(n)}(x_{1},x_{2},\\ldots,x_{n}):=\\phi^{(1)}\\left(x_{1},\\,\\phi^{(n-1)}(x_{2},\\ldots,x_{n})\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi^{(1)}(x,y):=(1-\\gamma)x+\\gamma\\operatorname*{min}\\{x,y\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The k-step advantage function A\u02c6g\u02c6\u03c0(k) can then be written as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{A}_{\\hat{g}}^{\\pi(k)}(\\hat{x}_{t})=\\phi^{(k)}\\Big(\\hat{g}(\\hat{x}_{t}),\\cdot\\cdot\\cdot,\\hat{g}(\\hat{x}_{t+k-1}),\\tilde{V}_{\\hat{g}}^{\\pi}(\\hat{x}_{t+k})\\Big)-\\tilde{V}_{g}^{\\pi}(\\hat{x}_{t}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can then construct the GAE A\u02c6g\u02c6\u03c0(GAE)a s the $\\lambda^{k}$ -weighted sum over the $k$ -step advantage functions A\u02c6g\u02c6\u03c0(k): Overall, the GAE estimator can be described as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{A}_{\\hat{g}}^{\\pi(\\mathrm{GAE})}(\\hat{x}_{t})=\\frac{1}{1-\\lambda}\\sum_{k=1}^{\\infty}\\lambda^{k}\\hat{A}_{\\hat{g}}^{\\pi(k)}(\\hat{x}_{t}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B Equivalence of Problem 3 and Problem 13 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The equivalence between the transformed Problem 13 and the original minimum-cost reach-avoid Problem 3 can be shown in the following sequence of optimization problems that yield the exact same solution if feasible: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x,l}{\\operatorname*{min}}\\ \\ \\ \\sum_{k=0}^{T}(x_{k},\\eta(x_{k}))}&{\\ s.t\\ \\ \\ g(x_{T})\\leq0,\\ \\ \\ \\underset{k=0}{\\operatorname*{max}}\\ \\ h(x_{k})\\leq0}\\\\ {=\\underset{x,l}{\\operatorname*{min}}\\ \\ \\sum_{k=0}^{T}(x_{k},\\eta(x_{k}))}&{\\ s.t\\ \\ \\ g(x_{T})\\leq0,\\ \\ \\ I_{(\\operatorname*{max}_{k},\\eta_{k})>0}\\leq0}\\\\ {=\\underset{y,l}{\\operatorname*{min}}\\ \\ \\ \\sum_{k=0}^{T}(x_{k},\\eta(x_{k}))}&{\\ s.t\\ \\ \\ g(x_{T})\\leq0,\\ \\ \\ I_{(\\operatorname*{max}_{k},\\eta_{k})\\leq0}\\leq0}\\\\ {=\\ \\ \\underset{y,l}{\\operatorname*{min}}\\ \\ \\ \\sum_{\\ell=0}^{T}\\ \\ \\underset{k=0}{\\operatorname*{sut}}\\ \\ \\ \\sum_{k=0}^{T}(x_{k},\\eta(x_{k}))\\leq z_{0},\\ \\ \\ g(x_{T})\\leq0,\\ \\ I_{(\\operatorname*{max}_{k},\\eta_{k})\\leq0}\\leq0}\\\\ {=\\ \\ \\ \\underset{y,l}{\\operatorname*{min}}\\ \\ \\ \\ \\sum_{\\ell=0}^{T}\\ \\ \\ \\underset{k=0}{\\operatorname*{sut}}\\ \\ \\ \\underset{j\\leq0}{\\operatorname*{max}}\\ \\ \\left(\\sum_{k=0}^{T}(x_{k},\\eta(x_{k}))-z_{0},\\ g(x_{T}),\\ I_{(\\operatorname*{max}_{k},\\eta_{k})>0}\\right)\\leq0}\\\\ {=\\ \\ \\underset{y,l}{\\operatorname*{min}}\\ \\ \\ \\sum_{\\ell=0}^{T}\\ \\ \\ \\ s.t\\ \\ \\ \\hat{y}(\\hat{x}_{T})\\leq0}\\\\ {=\\ \\underset{y,l}{\\operatorname*{min}}\\ \\ \\ \\ z_{0}\\ \\ \\ \\ \\ \\underset{k=0}{\\operatorname*{sut}}\\ \\ \\ \\ \\frac{\\eta(x_{l})}{\\operatorname*{min}}\\hat{g}(\\hat{x}_{T})\\leq0}\\\\ {=\\ \\underset{y,l}{\\operatorname*{min}}\\ \\ \\ \\ z_{0}\\ \\ \\ \\ \\times1,\\ \\ \\ \\ \\underset{k=0}{\\operatorname*{min}}\\ \\ \\hat{V}_{\\ell}^{*}(\\hat{x}_{T})\\leq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This shows that the minimum-cost reach-avoid Problem 3 is equivalent to the formulation we solve in this work (29), which is Problem 13 in the paper. The formulation of RC-PPO solves (29) and thus also solves Problem 3 because they are equivalent. ", "page_idx": 15}, {"type": "text", "text": "C Optimal Reach Value Function ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As shown in (18), we introduce an additional discount factor into the estimation of $\\tilde{V}_{\\hat{g}}^{\\pi}$ . It will incur imprecision on the calculation of $\\tilde{V}_{\\hat{g}}^{\\pi}$ defined in Definition 1. In this section, we show that for a large enough discount factor $\\gamma<1$ , we could reach unbiased $\\tilde{z}$ in phase two of RC-PPO. ", "page_idx": 15}, {"type": "text", "text": "Theorem 3. We denote $\\begin{array}{r}{\\operatorname*{max}_{\\hat{x}\\in\\hat{\\mathcal X}}\\{\\hat{g}(\\hat{x})\\}=G_{m a x}}\\end{array}$ and maximal episode length $T_{m a x}$ . If there exists a positive value $\\epsilon$ where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{g}(\\hat{x})<0\\Rightarrow\\hat{g}(\\hat{x})<-\\epsilon.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then for any1\u2212\u03b3Tmax > $\\begin{array}{r}{\\frac{\\gamma^{T_{m a x}}}{1-\\gamma^{T_{m a x}}}>\\frac{G_{m a x}}{\\epsilon}}\\end{array}$ , for any deterministic policy $\\pi$ satisfies 18. If there exists a trajectory under given policy $\\pi$ leading to the extended goal region $\\hat{\\mathcal G}$ . We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{V}_{\\hat{g}}^{\\pi}(\\hat{x})<0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The proof for Theorem 3 is provided in Appendix D.4. ", "page_idx": 16}, {"type": "text", "text": "D Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Proof for Theorem 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. We separately consider three elements in augmented state $(x_{T},y_{T},z_{T})$ . First, note that 3b holds if and only if $x_{T}\\in{\\mathcal{G}}$ . For the second element $y$ , from the definition of the augmented dynamics 7, it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\ny_{T}=\\operatorname*{max}_{i\\in\\{0,...,T\\}}\\mathbb{I}_{x_{i}\\in\\mathcal{F}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a result 3c holds if and only if $y_{T}\\;=\\;-1$ . For the third element $z$ , note that $z_{T}~=~z_{0}~-$ $\\begin{array}{r}{\\sum_{k=0}^{T-1}c(x_{k},u(x_{k}))}\\end{array}$ . Hence, $z_{T}\\ge0$ if and only if $\\begin{array}{r}{z_{0}\\ge\\sum_{k=0}^{T-1}c(x_{k},u(x_{k}))}\\end{array}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "D.2 Proof for Property 12 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. From Definition 12, we know ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{V}_{\\hat{g}}^{\\pi}(\\hat{x})=\\underset{t\\in\\mathbb{N}}{\\sin}\\hat{g}(\\hat{x}_{t}\\ \\vert\\ \\hat{x}_{0}=\\hat{x})}\\\\ &{\\qquad\\quad=\\operatorname*{min}\\{\\hat{g}(\\hat{x}),\\underset{t\\in\\mathbb{N}^{+}}{\\operatorname*{min}}\\,\\hat{g}(\\hat{x}_{t}\\ \\vert\\ \\hat{x}_{0}=\\hat{x})\\}}\\\\ &{\\qquad\\quad=\\operatorname*{min}\\{\\hat{g}(\\hat{x}),\\tilde{V}_{g}^{\\pi}(\\hat{x}_{t+1})\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D.3 Proof for Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first derive the state value function in a recursive form similar as [66] ", "page_idx": 16}, {"type": "text", "text": "Proof. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\tilde{V}_{\\theta}^{\\pi*}(\\hat{z})=\\nabla_{\\theta}\\left(\\sum_{u^{0}}\\pi_{\\theta}(u\\mid\\hat{z})\\tilde{Q}_{y}^{\\pi*}(\\hat{z},u)\\right)}\\\\ &{\\qquad\\qquad\\quad=\\underbrace{\\sum_{u^{0}}\\left(\\nabla_{\\theta}\\pi_{\\theta}(u\\mid\\hat{z})\\tilde{Q}_{y}^{\\pi*}(\\theta,u)+\\pi_{\\theta}(u\\mid\\hat{z})\\nabla_{\\theta}\\tilde{Q}_{y}^{\\pi*}(\\hat{z},u)\\right)}_{=\\mathrm{i}}}\\\\ &{\\qquad\\qquad\\quad=\\underbrace{\\sum_{u^{0}}\\left(\\nabla_{\\theta}\\pi_{\\theta}(u\\mid\\hat{z})\\tilde{Q}_{y}^{\\pi*}(\\hat{z},u)\\right.}_{=\\mathrm{i}}}\\\\ &{\\qquad\\qquad\\qquad\\quad+\\left.\\pi_{\\theta}(u\\mid\\hat{z})\\nabla_{\\theta}\\operatorname*{min}\\left(\\hat{y}(\\hat{z}),\\tilde{V}_{z}^{\\pi}(z^{\\prime})\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\sum_{u^{0}}\\left(\\nabla_{\\theta}\\pi_{\\theta}(u\\mid\\hat{z})\\tilde{Q}_{y}^{\\pi*}(\\hat{z},u)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\quad\\left.+\\left.\\pi_{\\theta}(u\\mid\\hat{z})\\mathbf{i}_{\\hat{z}(\\hat{z})\\sim\\hat{\\mathcal{V}}_{z}^{\\pi}(\\hat{z}^{\\prime})}(\\hat{z},u^{\\prime})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\hat{x}^{\\prime}=\\hat{f}(\\hat{x},u)$ ", "page_idx": 17}, {"type": "text", "text": "Next, we consider unrolling $\\tilde{V}_{g}^{\\pi_{\\theta}}(\\hat{x}^{\\prime})$ under Reachability MDP in Definition 2. We define $\\operatorname*{Pr}(\\hat{x}\\to$ $\\hat{x}^{\\dagger},k,\\pi_{\\boldsymbol{\\theta}})$ as the probability of transitioning from state $\\hat{x}$ to $\\hat{x}^{\\dagger}$ in $k$ steps under policy $\\pi_{\\theta}$ in 2. Note that $\\mathbb{1}_{\\hat{g}(\\hat{x})>\\tilde{V}_{g}^{\\pi_{\\theta}}(\\hat{x}^{\\prime})}$ is absorbed using the absorbing state in 2. Then we can get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla_{\\theta}\\tilde{V}_{\\hat{g}}^{\\pi_{\\theta}}(\\hat{x})=\\sum_{\\hat{x}^{\\dag}\\in\\hat{\\mathcal X}}\\left(\\sum_{k=0}^{\\infty}\\operatorname*{Pr}\\big(\\hat{x}\\rightarrow\\hat{x}^{\\dag},k,\\pi\\big)\\right)\\sum_{u\\in\\mathcal U}\\nabla\\pi_{\\theta}(u\\mid\\hat{x}^{\\dag})\\tilde{Q}_{\\hat{g}}^{\\pi_{\\theta}}(\\hat{x}^{\\dag},u)}}\\\\ &{}&{\\propto\\mathbb{E}_{\\hat{x}^{\\prime}\\sim d_{\\pi}^{\\prime}(\\hat{x}),u\\sim\\pi_{\\theta}}\\left[\\tilde{Q}^{\\pi_{\\theta}}\\left(\\hat{x}^{\\prime},u\\right)\\nabla_{\\theta}\\ln\\pi_{\\theta}\\left(u\\mid\\hat{x}^{\\prime}\\right)\\right]~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D.4 Proof for Theorem 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Consider trajectory $\\{\\hat{x}_{0},\\hdots,\\hat{x}_{T}\\}$ where $\\hat{x}_{T}\\in\\hat{\\mathcal{G}}$ . We consider the worst-case scenario where $\\hat{g}(\\hat{x}_{t})=g_{m a x}$ for $t\\in\\{0,\\ldots,T-1\\}$ . Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{V}^{\\pi}(\\hat{x}_{0})=(1-\\gamma)\\hat{g}(\\hat{x}_{0})+\\gamma\\operatorname*{min}\\{\\tilde{V}^{\\pi}(\\hat{x}_{1}),\\hat{g}(\\hat{x}_{1})\\}}\\\\ &{\\qquad\\qquad\\leq(1-\\gamma)g_{m a x}+\\gamma\\tilde{V}^{\\pi}(\\hat{x}_{1})}\\\\ &{\\qquad\\leq(1-\\gamma)g_{m a x}+\\gamma((1-\\gamma)g_{m a x}+\\gamma\\tilde{V}^{\\pi}(\\hat{x}_{1}))}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{i=0}^{T-1}\\gamma^{i}(1-\\gamma)g_{m a x}+\\gamma^{T}\\tilde{V}^{\\pi}(\\hat{x}_{T})}\\\\ &{\\qquad<(1-\\gamma^{T})g_{m a x}+\\gamma^{T}\\epsilon}\\\\ &{\\qquad<0}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "E Convergence Guarantee on an Actor-Critic Version of Our Method ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide the convergence proof of phase one of our method under the actor-critic framework. Notice that similar to Bellman equation (18) for $\\tilde{V}_{\\hat{g}}^{\\pi}$ . We could also derive the Bellman equation for $\\tilde{Q}_{\\hat{g}}^{\\pi}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{Q}_{\\hat{g}}^{\\pi}(\\hat{x}_{t},u_{t})=(1-\\gamma)\\hat{g}(\\hat{x}_{t})+\\gamma\\mathbb{E}_{\\hat{x}_{t+1}\\sim\\tau,u_{t+1}\\sim\\pi}[\\operatorname*{min}\\{\\hat{g}(\\hat{x}_{t}),\\tilde{Q}_{g}^{\\pi}(\\hat{x}_{t+1},u_{t+1})\\}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we show our method under the actor-critic framework without GAE estimator in Algorithm 1 ", "page_idx": 17}, {"type": "text", "text": "Require: Initial policy parameter $\\theta_{0}$ , $\\mathrm{^Q}$ function parameter $\\omega_{0}$ , horizon T, convex projection operator $\\Gamma_{\\Theta}$ , and value function learning rate $\\beta_{1}(k)$ , policy learning rate $\\beta_{2}(k)$   \n1: for $\\mathbf{k}=0,\\,1,\\,\\dots\\,\\cdot$ do   \n2: for $\\mathrm{{t=0}}$ to T-1 do   \n3: Sample trajectories $\\tau_{t}:\\{\\hat{x}_{t},u_{t},\\hat{x}_{t+1}\\}$   \n4: Critic update: $\\omega_{k+1}=\\omega_{k}-\\beta_{1}(k)\\nabla_{\\omega}\\tilde{Q}_{\\hat{g}}\\left(\\hat{x}_{t},u_{t};\\omega_{k}\\right).$ \u00b7   \n5: $\\Big[\\tilde{Q}_{\\hat{g}}\\left(\\hat{x}_{t},u_{t};\\omega_{k}\\right)-\\Big((1-\\gamma)\\hat{g}(\\hat{x}_{t})+\\gamma\\operatorname*{min}\\Big\\{\\hat{g}(\\hat{x}_{t}),\\tilde{Q}_{\\hat{g}}\\left(\\hat{x}_{t+1},u_{t+1};\\omega_{k}\\right)\\Big\\}\\Big)\\Big]$   \n6: Actor Update: $\\theta_{k+1}=\\Gamma_{\\Theta}\\left(\\theta_{k}+\\beta_{2}(k)\\tilde{Q}_{\\hat{g}}\\left(\\hat{x}_{t},u_{t};\\omega_{k}\\right)\\nabla_{\\theta}\\log\\pi_{\\theta}\\left(u_{t}~|~\\hat{x}_{t}\\right)\\right)$   \n7: end for ", "page_idx": 17}, {"type": "text", "text": "9: return parameter $\\theta,\\omega$ ", "page_idx": 17}, {"type": "text", "text": "In this algorithm, the $\\Gamma_{\\Theta}(\\theta)$ operator projects a vector $\\theta\\in\\mathbb{R}^{k}$ to the closest point in a compact and convex set $\\Theta\\subset\\mathbb{R}^{k}$ , i.e., $\\Gamma_{\\Theta}(\\theta)=\\arg\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\|\\theta^{\\prime}-\\theta\\|^{2}$ . ", "page_idx": 18}, {"type": "text", "text": "Next, we provide the convergence analysis for Algorithm 1 under the following assumptions. ", "page_idx": 18}, {"type": "text", "text": "Assumption 1. (Step Sizes) The step size schedules $\\{\\beta_{1}(k)\\}$ and $\\{\\beta_{2}(k)\\}$ have below properties: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{k}\\beta_{1}(k)=\\sum_{k}\\beta_{2}(k)=\\infty}\\\\ {\\displaystyle\\sum_{k}\\beta_{1}(k)^{2},\\sum_{k}\\beta_{2}(k)^{2}<\\infty}\\\\ {\\displaystyle\\sum_{\\/{k}}\\beta_{1}(k)^{2},\\ldots,\\beta_{k}(k)\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\beta_{2}(k)=o(\\beta_{1}(k))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Assumption 2. (Differentiability and and Lipschitz Continuity) For any state and action pair $(\\hat{x},u)$ , $\\Tilde{Q}_{\\hat{g}}(\\hat{x},u;\\omega)$ and $\\pi({\\hat{x}};\\theta)$ are continuously differentiable in $\\omega$ and $\\theta$ . Furthermore, for any state and action pair $({\\hat{x}},u)$ , $\\nabla_{\\omega}\\tilde{Q}_{\\hat{g}}(\\hat{x},u;\\omega)$ and $\\pi(\\hat{x};\\theta)$ are Lipschitz function in $\\omega$ and $\\theta$ . ", "page_idx": 18}, {"type": "text", "text": "Also, we assume that $\\mathcal{X}$ and $\\boldsymbol{\\mathcal{U}}$ are finite and bounded and the horizon $T$ is also bounded by $T_{\\mathrm{max}}$ , then the cost function $c$ can be bounded by $C_{\\mathrm{max}}$ and $g$ can be bounded within $G_{\\mathrm{max}}$ . We can limit the space of cost upper bound $z\\in[-G_{\\mathrm{max}},T\\cdot C_{\\mathrm{max}}]$ instead of $\\mathbb{R}$ . This is due to $\\hat{g}(\\hat{x})=-z$ for $z\\le-G_{\\mathrm{max}}$ . Next, we could do discretization on $[-\\bar{G}_{\\mathrm{max}},T\\cdot C_{\\mathrm{max}}]$ and cost function $c$ to make the augmented state set $\\hat{\\mathcal X}$ finite and bounded. ", "page_idx": 18}, {"type": "text", "text": "With the above assumptions, we can provide a convergence guarantee for Algocrithm 1. ", "page_idx": 18}, {"type": "text", "text": "Theorem 4. Under Assumptions 1 and 2, the policy update in Algorithm 1 converge almost surely to a locally optimal policy. ", "page_idx": 18}, {"type": "text", "text": "Proof. The proof follows from the proof of Theorem 2 in [75], differing only in whether an update exists for the Lagrangian multiplier. We provide a proof sketch as ", "page_idx": 18}, {"type": "text", "text": "\u2022 First, we prove that the critic parameter almost surely converges to a fixed point $\\omega^{*}$ ", "page_idx": 18}, {"type": "text", "text": "This step is guaranteed by the assumption of finite and bounded state and action set and Assumption 1. The $\\gamma$ -contraction property of the following operator ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{B}[\\tilde{Q}_{\\hat{g}}(\\hat{x},u)]\\mathop{=}(1-\\gamma)\\hat{g}(\\hat{x})+\\gamma\\mathbb{E}_{\\hat{x}^{\\prime}\\sim\\tau,u\\sim\\pi}[\\operatorname*{min}\\{\\hat{g}(\\hat{x}),\\tilde{Q}_{g}^{\\pi}(\\hat{x}^{\\prime},u)\\}]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "is also proved in Lemma B.1 in [75] to make sure the convergence of the first step. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Second, due to the fast convergence of $\\omega^{*}$ , we can show policy paramter $\\theta$ converge almost surely to a stationary point $\\theta^{*}$ which can be further proved to be a locally optimal solution. ", "page_idx": 18}, {"type": "text", "text": "We refer to [75] for proof details. ", "page_idx": 18}, {"type": "text", "text": "F Implementation Details of Algorithms ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we will provide more details about CMDP-based baselines (different between optimization goal with multiple constraints) and other hyperparameter settings like $\\chi_{\\mathrm{threshold}}$ . ", "page_idx": 18}, {"type": "text", "text": "F.1 CMDP-based Baselines ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we will clarify the optimization target for CPPO and RESPO under CMDP formulation of both hard and soft constraints. Recall that our formulation of CMDP is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\pi}}&{\\mathbb{E}_{\\boldsymbol{x}_{t},\\boldsymbol{u}_{t}\\sim d_{\\pi}}\\displaystyle\\sum_{t}\\big[-\\gamma^{t}r(\\boldsymbol{x}_{t},\\boldsymbol{u}_{t})\\big]}\\\\ {\\mathrm{s.t.}\\quad\\mathbb{E}_{\\boldsymbol{x}_{t},\\boldsymbol{u}_{t}\\sim d_{\\pi}}\\displaystyle\\sum_{t}\\left[\\gamma^{t}\\mathbb{1}_{\\boldsymbol{x}_{t}\\in\\mathcal{F}}\\times C_{f a i l}\\right]\\leq0,}\\\\ &{\\mathbb{E}_{\\boldsymbol{x}_{t},\\boldsymbol{u}_{t}\\sim d_{\\pi}}\\displaystyle\\sum_{t}\\left[\\gamma^{t}c(\\boldsymbol{x}_{t},\\boldsymbol{u}_{t})\\right]\\leq\\mathcal{X}_{\\mathrm{threshold}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We then denote ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\!\\!\\!\\!\\!\\!V_{r}^{\\pi}(x_{t}):=\\mathbb{E}_{x_{t},u_{t}\\sim d_{\\pi}}\\sum_{t}\\left[\\gamma^{t}r(x_{t},u_{t})\\right]}\\\\ &{}&{\\!\\!\\!\\!\\!\\!V_{f}^{\\pi}(x_{t}):=\\mathbb{E}_{x_{t},u_{t}\\sim d_{\\pi}}\\sum_{t}\\left[\\gamma^{t}\\mathbb{1}_{x_{t}\\in\\mathcal{F}}\\times C_{c o s t}\\right]}\\\\ &{}&{\\!\\!\\!\\!\\!\\!V_{c}^{\\pi}(x_{t}):=\\mathbb{E}_{x_{t},u_{t}\\sim d_{\\pi}}\\sum_{t}\\left[\\gamma^{t}c(x_{t},u_{t})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The optimization goal formulation for CPPO is as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi}\\operatorname*{max}_{\\lambda}\\left(L(\\pi,\\lambda)=-V_{r}^{\\pi}(x)+\\lambda_{c}\\cdot(V_{c}^{\\pi}(x)-\\mathcal{X}_{t h r e s h o l d})+\\lambda_{f}\\cdot V_{f}^{\\pi}(x)\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In this formulation, the soft constraint $V_{c}^{\\pi}$ has the same priority as the hard constraint $V_{f}^{\\pi}$ . This leads to a potential imbalance between soft constraints and hard constraints. Instead, the optimization goal for RESPO is as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\pi}{\\operatorname*{min}}\\underset{\\lambda}{\\operatorname*{max}}\\,L(\\pi,\\lambda)=\\bigr(-V_{r}^{\\pi}(x)+\\lambda_{c}\\cdot(V_{c}^{\\pi}(x)-\\chi_{t h r e s h o l d})}\\\\ &{\\phantom{\\frac{1}{\\lambda_{c}}}+\\lambda_{f}\\cdot V_{f}^{\\pi}(x)\\bigr)\\cdot(1-p(x))+p(x)\\cdot V_{f}^{\\pi}(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $p(x)$ denotes the probability of entering the unsafe region $\\mathcal{F}$ start from state $x$ . It is called the reachability estimation function (REF). This formulation prioritizes the satisfaction of hard constraints but still suffers from balancing soft constraints and reward terms. ", "page_idx": 19}, {"type": "text", "text": "F.2 Hyperparameters ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We first clarify how we set proper $\\mathcal{X}_{\\mathrm{threshold}}$ for each environment. First, we will run our method RC-PPO and calculate the average cost, we denote it as caverage. We set $\\begin{array}{r}{\\mathcal{X}_{\\mathrm{low}}=\\frac{c_{a v e r a g e}}{10}}\\end{array}$ cav1er0age, Xmedium = $\\frac{c_{a v e r a g e}}{3}$ and $\\mathcal{X}_{\\mathrm{high}}=c_{a v e r a g e}$ . For static lagrangian multiplier $\\beta$ , we set $\\beta_{\\mathrm{lo}}=0.1$ and $\\beta_{\\mathrm{hi}}=10$ Also, we set $C_{f a i l}=20$ in every environment. ", "page_idx": 19}, {"type": "text", "text": "Note that CRL is an off-policy algorithm, while RC-PPO and other baselines are on-policy algorithms. We provide Table 1 showing hyperparameters for on-policy algorithms and Table 2 showing hyperparameters for off-policy algorithm (CRL). ", "page_idx": 19}, {"type": "text", "text": "F.3 Implementation of the baselines ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The implementation of the baseline follows their original implementations: ", "page_idx": 19}, {"type": "text", "text": "\u2022 RESPO: https://github.com/milanganai/milanganai.github.io/tree/main/NeurIPS2023/code (No license)   \n\u2022 CRL: https://github.com/google-research/google-research/tree/master/ contrastive_rl (No License) ", "page_idx": 19}, {"type": "text", "text": "G Experiment Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide more details about the benchmarks and the choice of reward function $r,g$ , cost function $c$ and $C_{c o s t}$ in each environment. Under the sparse reward setting, we apply the following structure of reward design ", "page_idx": 19}, {"type": "equation", "text": "$$\nr(x_{t},u_{t},x_{t+1})=R_{g o a l}\\times\\mathbb{1}_{x_{t+1}\\in\\mathcal{G}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $R_{g o a l}$ is an constant. After doing reward shaping, we add an extra term $\\gamma\\phi(x_{t+1})-\\phi(x_{t})$ and the reward becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\nr(x_{t},u_{t},x_{t+1})=R_{g o a l}\\times\\mathbb{1}_{x_{t+1}\\in\\mathcal{G}}+\\gamma\\phi(x_{t+1})-\\phi(x_{t})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\gamma$ denotes the discount factor. ", "page_idx": 19}, {"type": "text", "text": "Note that we set $R_{g o a l}=C_{c o s t}=20$ in all the environments. Note that if there is a gap between $\\operatorname*{max}\\{g(x)~\\mid~g(x)~<~0\\}$ , we could get unbiased $\\tilde{z}$ during phase two of RC-PPO guaranteed by Theorem 3. To achieve better performance in phase two of RC-PPO, we set ", "page_idx": 19}, {"type": "equation", "text": "$$\ng(x)=-300\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $x\\in\\mathcal G$ to maintain such a gap. Also, we implement all the environments in Jax [76] for better scalability and parallelization. ", "page_idx": 19}, {"type": "text", "text": "Table 1: Hyperparameter Settings for On-policy Algorithms ", "page_idx": 20}, {"type": "table", "img_path": "jzngdJQ2lY/tmp/50a492ddbbbf4c00f4ee04205015fcbf4413df4f2e40dbd3fcb1a49608fc35f9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "jzngdJQ2lY/tmp/98a6cfc9246c2c470168a83fdd3761b0ccf27aa36a10fc3a2d617731714e1ff7.jpg", "table_caption": ["Table 2: Hyperparameter Settings for Off-policy Algorithms "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "G.1 Pendulum ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The Pendulum environment is taken from Gym [18] and the torque limit is set to be 1. The state space is given by $x=[\\theta,{\\dot{\\theta}}]$ where $\\theta\\in[-\\pi,\\pi],\\dot{\\theta}\\in[-8,8]$ . In this task, we do not consider unsafe regions and set ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{G}:=\\{[\\theta,\\dot{\\theta}]\\mid\\theta\\cdot(\\theta+\\dot{\\theta}\\cdot d t)<0\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $d t=0.05$ is the time interval during environment simulation. This is for preventing environment overshooting during simulation. ", "page_idx": 21}, {"type": "text", "text": "In the Pendulum environment, cost function $c$ is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\nc(x_{t},u_{t},x_{t+1})={\\binom{0}{8\\|u\\|^{2}}}\\quad{\\mathrm{if}}\\quad\\|u_{t}\\|<0.1\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for better visualization of policies with different energy consumption. $g$ is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\ng(x)={\\binom{100\\theta^{2}}{-300}}\\quad{\\mathrm{if}}\\quad x\\not\\in{\\mathcal{G}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "G.2 Safety Hopper ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The Safety Hopper environment is taken from Safety Mujoco, we add static obstacles in the environment to increase the difficulty of the task. We use $x$ to denote the ${\\bf X}$ -axis position of the head of Hopper, $y$ to be the y-axis position of the head of Hopper. Then the goal region can be described as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{G}:=\\{(x,y)\\ |\\ ||x,y|-[2.0,1.4]||<0.1\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The unsafe set is described as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{F}:=\\{(x,y)\\ |\\ 0.95\\leq x\\leq1.05,y\\geq1.3\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We use $\\tilde{x}^{t h i g h},\\tilde{x}^{l e g},\\tilde{x}^{f o o t}$ to denote the angular velocity of the thigh, leg, foot hinge. The cost function is described as ", "page_idx": 21}, {"type": "equation", "text": "$$\nc(x_{t},u_{t},x_{t+1})=l(x_{t}^{t h i g h},u_{t}^{1})+l(x_{t}^{l e g},u_{t}^{2})+l(x_{t}^{f o o t},u_{t}^{3})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\nl(a,b)={\\binom{0}{0.15a^{2}\\cdot b^{2}}}\\quad{\\mathrm{if}}\\quad\\left\\|a\\cdot b\\right\\|<0.4\\qquad\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$g$ is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\ng(\\tilde{x})=\\left\\{\\!\\!\\!\\begin{array}{l l l}{100\\sqrt{(x-2)^{2}+100(y-1.4)^{2}}-40}&{{\\mathrm{if}}}&{{\\tilde{x}\\notin\\mathcal{G}}}\\\\ {-300}&{{\\mathrm{if}}}&{{\\tilde{x}\\in\\mathcal{G}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "G.3 Safety HalfCheetah ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The Safety HalfCheetah environment is taken from Safety Mujoco, we add static obstacles in the environment to increase the difficulty of the task. We use $x_{f r o n t}$ to denote the $\\mathbf{X}$ -axis position of the front foot of Halfcheetah, $y_{f r o n t}$ to be the $\\mathbf{y}$ -axis position of the back foot of Halfcheetah, $x_{b a c k}$ to denote the $\\mathbf{X}$ -axis position of the back foot of Halfcheetah, $y_{b a c k}$ to be the y-axis position of the back foot of Halfcheetah, $x_{h e a d}$ to denote the $\\mathbf{X}$ -axis position of the head of Halfcheetah, $y_{h e a d}$ to be the y-axis position of the head of Halfcheetah. Then the goal region can be described as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{G}:=\\{(x_{h e a d},y_{h e a d})\\ |\\ |[x_{h e a d},y_{h e a d}]-[5.0,0.0]||<0.2\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The unsafe set is described as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}:=\\{\\left(x_{f r o n t},y_{f r o n t}\\right)\\mid y_{f r o n t}<0.25,2.45<x_{f r o n t}<2.55\\}}\\\\ {\\cup\\left\\{\\left(x_{b a c k},y_{b a c k}\\right)\\mid y_{b a c k}<0.25,2.45<x_{b a c k}<2.55\\right\\}\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The cost function is described as ", "page_idx": 21}, {"type": "equation", "text": "$$\nc(x_{t},u_{t},x_{t+1})=\\|u_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$g$ is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\ng(\\tilde{x})=\\left\\{\\!\\!\\begin{array}{l l l}{100\\sqrt{(x_{h e a d}-2)^{2}+(y_{h e a d}-1.4)^{2}}-20}&{\\mathrm{if}}&{\\tilde{x}\\notin\\mathcal G}\\\\ {-300}&{\\mathrm{if}}&{\\tilde{x}\\in\\mathcal G}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "G.4 FixedWing ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "FixedWing environment is taken from [59] and we follow the same design of $\\mathcal{F}$ as [59]. We denote the $x_{P E}$ as the eastward displacement of F16 with given state $x$ . Then the goal region $\\mathcal{G}$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{G}:=\\{x\\mid1975\\leq x_{P E}\\leq2025\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The cost $c$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\nc(x_{t},u_{t},x_{t+1})=4\\|u_{t}/[1,25,25,25]\\|^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and $g$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\ng(x)={\\left\\{\\begin{array}{l l l}{\\frac{\\|x_{P E}-2000\\|-25}{4}}&{{\\mathrm{if}}}&{x\\not\\in{\\mathcal{G}}}\\\\ {-300}&{{\\mathrm{if}}}&{x\\in{\\mathcal{G}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "G.5 Quadrotor in Wind Field ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We take quadrotor dynamics from crazyfiles and wind field environments in the urban area from [71]. The wind field will disturb the quadrotor with extra movement on both $x$ -axis and $y$ -axis. There are static building obstacles in the environment and we treat them as the unsafe region $\\mathcal{F}$ . The goal for the quadrotor is to reach the mid-point of the city. We divide the whole city into four sections and train single policy on each of the sections. We use $x\\in[-30,30]$ to denote the $\\mathbf{X}$ -axis position of quadrotor, $\\bar{y}\\in[-30,30]$ to be the y-axis position of quadrotor. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{G}:=\\{(x,y)\\mid\\|[x,y]\\|\\leq4\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The cost $c$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\nc(x_{t},u_{t},x_{t+1})={\\frac{\\|u_{t}\\|^{2}}{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$g$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\ng(\\tilde{x})=\\left\\{\\!\\!\\begin{array}{l l l}{{10\\sqrt{(x-x_{g o a l})^{2}+10(y-y_{g o a l})^{2}}-40}}&{{\\mathrm{if}}}&{{x\\not\\in\\mathcal{G}}}\\\\ {{-300}}&{{\\mathrm{if}}}&{{x\\in\\mathcal{G}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "G.6 PointGoal ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The PointGoal environment is taken from Safety Gym [69] We implement PointGoal environments in Jax. In Safety Gym environment, we do not perform reward-shaping and use the original reward defined in Safety Gym environments. In this case, the distance reward is set also to be 20 in order to align\\* with $C_{g o a l}$ and $C_{c o s t}$ . Different from sampling outside the hazard region which is implemented in Safety Gym, we allow Point to be initialized within the hazard region. We use $x$ to denote the $\\mathbf{X}_{\\mathrm{~}}$ -axis position of Point, $y$ to be the y-axis position of Point, $x_{g o a l}$ to denote the $\\mathbf{X}$ -axis position of Goal, and $y_{g o a l}$ to denote the y-axis position of Goal. The goal region is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{G}:=\\{(x,y)\\ |\\ ||[x,y]-[x_{g o a l},y_{g o a l}]||\\leq0.3\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The cost $c$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\nc(x_{t},u_{t},x_{t+1})={\\frac{\\|u_{t}\\|^{2}}{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$g$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\ng(\\tilde{x})=\\left\\{\\!\\!\\!\\begin{array}{l l l}{{100\\sqrt{(x-x_{g o a l})^{2}+(y-y_{g o a l})^{2}}-30}}&{{\\mathrm{if}}}&{{x\\notin\\mathcal{G}}}\\\\ {{-300}}&{{\\mathrm{if}}}&{{x\\in\\mathcal{G}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "G.7 Experiment Harware ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We run all our experiments on a computer with CPU AMD Ryzen Threadripper 3970X 32-Core Processor and with 4 GPUs of RTX3090. It takes at most 4 hours to train on every environment. ", "page_idx": 22}, {"type": "text", "text": "H Additional Experiment Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We put additional experiment results in this section. ", "page_idx": 23}, {"type": "text", "text": "H.1 Additional Cumulative Cost and Reach Rates ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We show the cumulative cost and reach rates of the final converged policies for additional environments (F16 and Safety Hopper) in Figure 7. ", "page_idx": 23}, {"type": "image", "img_path": "jzngdJQ2lY/tmp/66a7eaf8fe21f5eb0d77ccc88033e4f9cfa62c7529a0f83484aa5fe4cad221b8.jpg", "img_caption": ["Figure 7: Cumulative cost and reach rates of the final converged policies. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "H.2 Visualization of learned policy for different $z$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To obtain better intuition for how the learned policy depends on $z$ , we rollout the policy choices of $z_{0}$ in the Pendulum environment and visualize the results in Figure 8. ", "page_idx": 23}, {"type": "image", "img_path": "jzngdJQ2lY/tmp/1aefa2acf690f52f7c3005642ac7958e83e201ec87f0d3a22c7d6d783a6a6c9a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 8: Learned RC-PPO policy for different $z$ on Pendulum. For a smaller cost lower-bound $z$ , cost minimization is prioritized at the expense of not reaching the goal. For a larger cost lower-bound $z$ , the goal is reached using a large cumulative cost. Performing rootfinding to solve for the optimal $z_{\\mathrm{opt}}$ automatically finds the policy that minimizes cumulative costs while still reaching the goal. ", "page_idx": 23}, {"type": "table", "img_path": "jzngdJQ2lY/tmp/8b38a6c696142d729a06d769f5a51a0336261abaa59a41683e8330d52530c21f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 3: Reach rate of final converged policies with different levels of noise to the output control ", "page_idx": 24}, {"type": "table", "img_path": "jzngdJQ2lY/tmp/c3c2eade1a12b95890a2adfe3a080d94d262572b756430981e961ac37751d120.jpg", "table_caption": [], "table_footnote": ["Table 4: Additional cumulative cost of final converged policies with different levels of noise to the output control "], "page_idx": 24}, {"type": "text", "text": "H.3 Grid search ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We perform an extensive grid search over different reward coefficients for the baseline PPO method and plotted the Pareto front across the reach rate and cost in Figure 6. The reward we use is ", "page_idx": 24}, {"type": "equation", "text": "$$\nr=R_{g o a l}\\times\\mathbb{1}_{x\\in\\mathcal{G}}-P_{g o a l}\\times\\mathbb{1}_{x\\notin\\mathcal{G}}-\\beta c(x,u).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and we search over the Cartesian product of $\\begin{array}{l l l l}{{R_{g o a l}}}&{{=}}&{{\\{2,20,200,2000,20000\\},P_{g o a l}}}\\end{array}=$ $\\{1,10,100,1000,10000\\},\\beta=\\{0.1,1,\\bar{1},10\\}$ . ", "page_idx": 24}, {"type": "text", "text": "H.4 Performance with external noise ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We also performed additional experiments to illustrate the performance of RC-PPO under a changing environment. Specifically, we add uniform noise to the output of the learned policy and see what happens in the Pendulum environment. ", "page_idx": 24}, {"type": "text", "text": "We first compare the reach rates of the different methods in Table 3. In this environment, we see that noise does not affect the reach rate too much. ", "page_idx": 24}, {"type": "text", "text": "Next, we look at how the cumulative cost changes with noise by comparing methods with a near $100\\%$ reach rate in Table 4. Unsurprisingly, larger amounts of noise reduce the performance of almost all policies. Even with the added noise, RC-PPO uses the least cumulative cost compared to all other methods. ", "page_idx": 24}, {"type": "text", "text": "I Discussion on Limitation of CMDP-Based Algorithms ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we will use an example to illustrate further why CMDP-based algorithms won\u2019t solve the minimum-cost reach-avoid problem optimally compared with our method. We focus on two parts of CMDP formulation: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Weight coefficient assigned to different objectives \u2022 Threshold assigned to each constraint ", "page_idx": 24}, {"type": "image", "img_path": "jzngdJQ2lY/tmp/910f1d1b89cf1c1b7a824ddc848f6800c62937bc47f548f7e029c7af46f69f61.jpg", "img_caption": ["Figure 9: Minimum-cost reach-avoid example to illustrate the limitation of CMDP-based formulation. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Consider the minimum-cost reach-avoid problem shown in Figure 9, where we use $C$ to denote the cost. States $A$ and $B$ are two initial states with the same initial distribution probability. State $G_{1}$ , $G_{2}$ , and $G_{3}$ are three goal states. State $I$ is the absorbing state (non-goal). We use $p_{A}$ and $p_{B}$ to denote the policy parameter, which represents the probability of choosing left action on state $A$ and $B$ separately. ", "page_idx": 25}, {"type": "text", "text": "The optimal policy for this minimum-cost reach-avoid problem is to take the left action from both $A$ and $B$ , i.e., $p_{A}=p_{B}=1$ , which gives an expected cost of ", "page_idx": 25}, {"type": "equation", "text": "$$\n0.5\\cdot10+0.5\\cdot30=20\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To convert this into a multi-objective problem, we introduce a reward that incentivizes reaching the goal as follows (we use $R$ to denote reward): ", "page_idx": 25}, {"type": "equation", "text": "$$\nR(A,G_{1})=10,\\;R(A,G_{2})=20,\\;R(B,G_{3})=20,\\;R(A,I)=0\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This results in the following multi-objective optimization problem: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{p_{A},p_{B}\\in[0,1]}\\quad(-R,C)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "I.1 Weight assignment ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We first consider solving multi-objective optimization Problem 34 by assigning weights on different objectives. We introduce $w\\geq0$ , giving ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{p_{A},p_{B}\\in[0,1]}\\quad-R+w C\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Solving the scalarized Problem 34 gives us the following solution as a function of $w$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\np_{A}=\\mathbb{1}_{(w\\geq1)},\\quad p_{B}=\\mathbb{1}_{(w\\leq\\frac{2}{3})}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Notice that the \\*true\\* optimal solution of $p_{A}=p_{B}=1$ is NOT an optimal solution to the original minimum-cost reach-avoid problem shown in Figure 9 under any $w$ . ", "page_idx": 25}, {"type": "text", "text": "Hence, the optimal solution of the surrogate multi-objective problem can be suboptimal for the original minimum-cost reach-avoid problem under any weight coefficients. ", "page_idx": 25}, {"type": "text", "text": "Of course, this is just one choice of reward function where the optimal solution of the minimum-cost reach-avoid problem cannot be recovered. Given knowledge of the optimal policy, we can construct the reward such that the multi-objective optimization problem does include the optimal policy as a solution. However, this is impossible to do if we do not have prior knowledge of the optimal policy, as is typically the case. ", "page_idx": 25}, {"type": "text", "text": "I.2 Threshold assignment ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Next, we consider solving multi-objective optimization Problem 34 by assigning a threshold $\\scriptstyle{\\mathcal{X}}_{-}$ thresh on the cost constraint: ", "page_idx": 25}, {"type": "equation", "text": "$$\n0.5(10p_{A}+20(1-p_{A}))+0.5(30p_{B})\\leq\\chi_{\\mathrm{thresh}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The optimal solution to this CMDP can be solved to be ", "page_idx": 25}, {"type": "equation", "text": "$$\np_{A}=0,\\quad p_{B}=\\frac{\\chi_{\\mathrm{thres}}-10}{15}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "However, the true optimal solution of $p_{A}=p_{B}=1$ is NOT an optimal solution to the CMDP.   \nbTou t stehee t hCisM, tDakPi nsgo $\\ensuremath{\\mathcal{X}}_{-}\\mathrm{thresh}=20$ sgoilvuetis $p_{A}=p_{B}=1$ .g ivMeso rae roevwera,r da noyf $R=15$ , $\\begin{array}{r}{p_{A}=0,\\dot{p}_{B}=\\frac{20-1\\dot{0}}{15}=\\frac{2}{3}}\\end{array}$ $R=23.33>15$   \nscaling of the rewards or costs does not change the solution. ", "page_idx": 26}, {"type": "text", "text": "We can \"fix\" this problem if we choose the rewards to be high only along the optimal solution $p_{A}=p_{B}=1$ , but this requires knowledge of the optimal solution beforehand and is not feasible for all problems. ", "page_idx": 26}, {"type": "text", "text": "Another way to \"fix\" this problem is if we consider a \"per-state\" cost threshold, e.g., ", "page_idx": 26}, {"type": "equation", "text": "$$\n10p_{A}+20(1-p_{A})\\leq\\mathcal{X}_{A},\\qquad10p_{B}+20(1-p_{B})\\leq\\mathcal{X}_{B}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Choosing exactly the cost of the optimal policy, i.e., $\\chi_{A}\\,=\\,10$ and $\\chi_{B}\\geq30$ , also recovers the optimal solution of $p_{A}=p_{B}=1$ . This now requires knowing the smallest cost to reach the goal for every state, which is difficult to do beforehand and not feasible. On the other hand, RC-PPO does exactly this in the second phase when optimizing for $z_{0}$ . We can thus interpret RC-PPO as automatically solving for the best cost threshold to use as a constraint for every initial state. ", "page_idx": 26}, {"type": "text", "text": "J Broader impact ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Our proposed algorithm solves an important problem that is widely applicable to many different real-world tasks including robotics, autonomous driving, and drone delivery. Solving this brings us one step closer to more feasible deployment of these robots in real life. However, the proposed algorithm requires GPU training resources, which could contribute to increased energy usage. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 27}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 27}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 27}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 27}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 27}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 27}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The main contributions of the paper are summarized in the Introduction section as well as in the abstract of the paper, describing the proposed method briefly as well as its advantages compared to the existing methods. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Yes, we have discussed about the limitation of the work in Section 6 Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Yes. The completed proofs are provided in the Appendix (see Appendix C for example) ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Yes. We provide details on all experiments and baselines used. Moreover, we provide the code used to reproduce our results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Yes, the code used for generating the results in the paper has been provided. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 29}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Yes. The full details are provided in the Appendix. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Yes, we report confidence intervals in our plots. See Section 5. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Yes, all the required computational resources used for generating the results are provided with the experiment details. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Yes, the research conducted in this paper adheres to the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Yes, see Appendix J. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The license of our baselines are mentioned in Appendix F.3. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: we include the code of the algorithm in the supplementary materials. Implementation details are introduced in Section 4 and Appendix F. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]