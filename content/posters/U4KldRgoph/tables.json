[{"figure_path": "U4KldRgoph/tables/tables_2_1.jpg", "caption": "Table 2: Test performance in five benchmarks from [20]. The results are presented as the mean \u00b1 standard deviation from 5 runs using different random seeds. Baseline results were obtained from their respective original papers. * indicates a statistically significant difference against the baseline w/o HDSE from the one-tailed t-test. Highlighted are the top first, second and third results.", "description": "This table presents the results of graph classification and regression experiments on five benchmark datasets from the Benchmarking GNNs paper [20].  The table compares the performance of several graph transformer models, both with and without the proposed HDSE method. The results are reported as the mean and standard deviation across five runs with different random seeds.  Statistically significant improvements achieved by using HDSE are marked with an asterisk.", "section": "4.2 Results on Graph-level Tasks"}, {"figure_path": "U4KldRgoph/tables/tables_7_1.jpg", "caption": "Table 2: Test performance in five benchmarks from [20]. The results are presented as the mean \u00b1 standard deviation from 5 runs using different random seeds. Baseline results were obtained from their respective original papers. * indicates a statistically significant difference against the baseline w/o HDSE from the one-tailed t-test. Highlighted are the top first, second and third results.", "description": "This table presents the test performance results on five graph-level benchmark datasets from the Benchmarking GNNs paper [20].  It compares the performance of several graph transformer models, both with and without the proposed HDSE method.  The results, averaged over five runs with different random seeds, show the Mean Absolute Error (MAE) for regression tasks and accuracy for classification tasks. Statistically significant improvements (p<0.05) achieved by HDSE are marked with an asterisk (*). The top three performing models for each dataset are highlighted.", "section": "4.2 Results on Graph-level Tasks"}, {"figure_path": "U4KldRgoph/tables/tables_7_2.jpg", "caption": "Table 3: Test performance on two peptide datasets from Long-Range Graph Benchmarks (LRGB) [23].", "description": "This table presents the results of the proposed method and several baselines on two peptide datasets from the Long Range Graph Benchmark.  The metrics used are Average Precision (AP) for the Peptides-func dataset and Mean Absolute Error (MAE) for the Peptides-struct dataset.  The table highlights the improved performance of the proposed method (GraphGPS + HDSE) compared to other methods.", "section": "4.2 Results on Graph-level Tasks"}, {"figure_path": "U4KldRgoph/tables/tables_7_3.jpg", "caption": "Table 4: Ablation experiments of coarsening algorithms on ZINC.", "description": "This table presents the ablation study results on the ZINC dataset using different graph coarsening algorithms.  It compares the Mean Absolute Error (MAE) achieved by the SAT and GraphGPS models when combined with HDSE using various coarsening algorithms (METIS, Spectral, Loukas, Newman, Louvain). The \"w/o\" row shows the baseline MAE without using any coarsening algorithm. This helps determine the best-performing coarsening technique for enhancing the accuracy of graph transformers with HDSE on the ZINC dataset.", "section": "4.2 Results on Graph-level Tasks"}, {"figure_path": "U4KldRgoph/tables/tables_8_1.jpg", "caption": "Table 5: Node classification on large-scale graphs (%). The baseline results were primarily taken from [82], with the remaining obtained from their respective original papers. OOM indicates out-of-memory when training on a GPU with 24GB memory.", "description": "This table presents the results of node classification experiments on eleven large-scale graph datasets.  The table compares the performance of the proposed HDSE method (GOAT + HDSE) against various state-of-the-art graph neural networks (GNNs) and graph transformers. The results are presented as accuracy percentages, with standard deviations from multiple runs.  The table also notes instances where models ran out of memory (OOM) during training due to the size of the datasets.", "section": "4.3 Results on Large-scale Graphs"}, {"figure_path": "U4KldRgoph/tables/tables_9_1.jpg", "caption": "Table 6: Efficiency comparison of GOAT + HDSE and scalable graph transformer competitors; training time per epoch.", "description": "This table compares the training time per epoch for three different graph transformer models: NodeFormer, SGFormer, and GOAT+HDSE. The comparison is done across five different large-scale graph datasets: PubMed, ogbn-proteins, ogbn-arxiv, ogbn-products, and ogbn-papers100M.  The results show that GOAT+HDSE achieves significantly faster training times compared to the other two models, highlighting its efficiency for large-scale graph processing.", "section": "4.3 Scaling HDSE to Large-scale Graphs"}, {"figure_path": "U4KldRgoph/tables/tables_9_2.jpg", "caption": "Table 7: Ablation study of GOAT + HDSE. \"w/o coarsening\" refers to replacing the projection matrix with the original projection matrix used in GOAT.", "description": "This ablation study investigates the impact of removing the high-level HDSE and replacing the coarsening projection matrix with the original projection matrix used in GOAT, demonstrating the contributions of each component to the model's performance across three datasets: Actor, ogbn-proteins, and arxiv-year.", "section": "4.3 Scaling HDSE to Large-scale Graphs"}, {"figure_path": "U4KldRgoph/tables/tables_19_1.jpg", "caption": "Table 2: Test performance in five benchmarks from [20]. The results are presented as the mean \u00b1 standard deviation from 5 runs using different random seeds. Baseline results were obtained from their respective original papers. * indicates a statistically significant difference against the baseline w/o HDSE from the one-tailed t-test. Highlighted are the top first, second and third results.", "description": "This table presents the test performance results on five benchmark datasets from the Benchmarking GNNs paper [20].  The results are shown as mean \u00b1 standard deviation, obtained from five independent runs using different random seeds.  For comparison, baseline results from the original papers are also included.  A one-tailed t-test was used to determine statistical significance between the results with and without HDSE.  The top three performing models for each metric are highlighted.", "section": "4.2 Results on Graph-level Tasks"}, {"figure_path": "U4KldRgoph/tables/tables_20_1.jpg", "caption": "Table 9: Hyperparameters of GraphGPS + HDSE for five datasets from [20].", "description": "This table presents the hyperparameters used for training the GraphGPS model enhanced with HDSE on five benchmark datasets from the Benchmarking GNNs paper [20].  The hyperparameters shown include the number of GPS layers, hidden dimension, type of GPS-MPNN and GPS-GlobAttn modules, the number of heads, attention dropout rate, graph pooling method, positional encoding type and dimension, positional encoding generation method, batch size, learning rate, number of epochs, number of warmup epochs, weight decay, maximum hierarchy level K, coarsening algorithm, and the total number of parameters. These settings are dataset-specific, and the details for each dataset are provided in separate rows.", "section": "4.2 Results on Graph-level Tasks"}, {"figure_path": "U4KldRgoph/tables/tables_20_2.jpg", "caption": "Table 9: Hyperparameters of GraphGPS + HDSE for five datasets from [20].", "description": "This table lists the hyperparameters used for the GraphGPS + HDSE model on five benchmark datasets from the Benchmarking GNNs paper [20].  The hyperparameters include details about the number of GPS layers, hidden dimension, the type of MPNN and global attention used, the number of heads in the attention mechanism, the attention dropout rate, the type of graph pooling used, the type of positional encoding, the dimension of the positional encoding, the positional encoding encoder, the batch size, the learning rate, the number of epochs, the number of warmup epochs, the weight decay, the maximum hierarchy level (K), the coarsening algorithm used, and the total number of parameters.  Each dataset has a slightly different set of hyperparameters to optimize performance.", "section": "4.2 Results on Graph-level Tasks"}, {"figure_path": "U4KldRgoph/tables/tables_21_1.jpg", "caption": "Table 11: GOAT + HDSE dataset-specific hyperparameter settings.", "description": "This table lists the hyperparameters used for the GOAT + HDSE model on various datasets.  It shows how the hyperparameters, such as the number of layers, hidden dimension, number of heads, and learning rate, were adjusted based on the characteristics of each dataset. The table also indicates the type of local GNN used (GCN or GraphSAGE) and the number of GNN layers for each dataset.", "section": "B.3 Hyperparameter and Reproducibility"}, {"figure_path": "U4KldRgoph/tables/tables_21_2.jpg", "caption": "Table 12: Performance on large-scale heterophilic graphs.", "description": "This table presents the results of node classification experiments conducted on two large-scale heterophilic graph datasets: Pokec and snap-patents.  The accuracy is reported for three different models: LINKX (a baseline model), GOAT (a graph transformer model), and GOAT + HDSE (the proposed model that integrates Hierarchical Distance Structural Encoding). The results demonstrate the superior performance of GOAT + HDSE compared to the baseline models on both datasets, indicating the effectiveness of the HDSE method in handling heterophilic graphs.", "section": "4.3 Results on Large-scale Graphs"}, {"figure_path": "U4KldRgoph/tables/tables_22_1.jpg", "caption": "Table 13: Sensitivity analysis on the maximum hierarchy level K of GraphGPS + HDSE on ZINC.", "description": "This table presents the results of a sensitivity analysis conducted to determine the optimal value for the maximum hierarchy level (K) in the GraphGPS + HDSE model.  The analysis was performed on the ZINC dataset, evaluating the Mean Absolute Error (MAE) for different values of K (0, 1, and 2). K=0 represents using only the shortest path distance (SPD), while K=1 and K=2 represent incorporating hierarchical distances at different levels.  The results indicate the impact of incorporating hierarchical information on model performance.", "section": "4.2 Results on Graph-level Tasks"}, {"figure_path": "U4KldRgoph/tables/tables_22_2.jpg", "caption": "Table 14: Sensitivity analysis on the maximum hierarchy level K of GOAT + HDSE.", "description": "This table presents the results of a sensitivity analysis conducted to determine the optimal maximum hierarchy level (K) for the GOAT + HDSE model.  The analysis was performed on three large-scale graph datasets: Squirrel, arxiv-year, and ogbn-arxiv. The table shows the accuracy achieved for each dataset with K values of 1 and 2.  The results help in identifying the best value of K that balances model performance and complexity.", "section": "4.3 Results on Large-scale Graphs"}, {"figure_path": "U4KldRgoph/tables/tables_22_3.jpg", "caption": "Table 15: Overview of the graph diameters of datasets used in graph classification", "description": "This table presents the average and maximum graph diameters for seven datasets used in the graph classification experiments.  The datasets include ZINC, MNIST, CIFAR10, PATTERN, CLUSTER, Peptides-func, and Peptides-struct. The diameter is a measure of the longest shortest path between any two nodes in a graph.  The values illustrate the range of graph sizes and complexities considered in the study.  This information is useful in understanding the scope and challenges of the graph classification task and in selecting appropriate values for hyperparameters like maximum distance length (L) in the HDSE method.", "section": "4.1 Experiment Setting"}, {"figure_path": "U4KldRgoph/tables/tables_22_4.jpg", "caption": "Table 3: Test performance on two peptide datasets from Long-Range Graph Benchmarks (LRGB) [23].", "description": "This table presents the results of the graph classification and regression tasks on two peptide datasets from the Long Range Graph Benchmark [23].  The table compares the performance of GraphGPS, a state-of-the-art graph transformer, with and without HDSE (Hierarchical Distance Structural Encoding). The results show the Average Precision (AP) for the Peptides-func dataset and the Mean Absolute Error (MAE) for the Peptides-struct dataset. The comparison highlights the improvement in performance achieved by incorporating HDSE into the GraphGPS model.", "section": "4.2 Results on Graph-level Tasks"}, {"figure_path": "U4KldRgoph/tables/tables_22_5.jpg", "caption": "Table 17: Empirical runtime of coarsening algorithms.", "description": "This table shows the runtime of different coarsening algorithms (including distance calculation) for four graph-level datasets: ZINC, PATTERN, MNIST, and P-func.  The results highlight the efficiency of the METIS algorithm compared to Newman and Louvain, especially for larger datasets where Newman's runtime becomes impractical.  The table demonstrates the relatively low computational overhead introduced by the HDSE method itself.", "section": "C.4 Coarsening Runtime"}, {"figure_path": "U4KldRgoph/tables/tables_23_1.jpg", "caption": "Table 18: Node classification results with linear coarsening algorithms on Cora, CiteSeer, and PubMed.", "description": "This table presents the results of node classification experiments on three benchmark datasets (Cora, CiteSeer, and PubMed) using the GOAT model with and without the HDSE method.  Two different linear coarsening algorithms (METIS and Loukas) are compared within the HDSE method to evaluate their impact on classification performance. The table shows the accuracy of each method on each dataset, indicating the improvement achieved by incorporating the HDSE method and highlighting the algorithm's comparative efficiency.", "section": "4.3 Results on Large-scale Graphs"}, {"figure_path": "U4KldRgoph/tables/tables_23_2.jpg", "caption": "Table 19: Node classification on synthetic community datasets.", "description": "This table presents the results of node classification experiments on a synthetic dataset called \"Community-small\", which consists of 100 graphs each having two distinct communities.  The experiments compare three different methods: GT (Graph Transformer), GT + SPD (Graph Transformer with Shortest Path Distance encoding), and GT + HDSE (Graph Transformer with Hierarchical Distance Structural Encoding). The results are shown in terms of accuracy, representing the percentage of correctly classified nodes.  The table highlights the superior performance of GT + HDSE compared to the other two methods, demonstrating the effectiveness of incorporating hierarchical distance information for improved node classification in graphs with community structures.", "section": "4.3 Results on Large-scale Graphs"}, {"figure_path": "U4KldRgoph/tables/tables_24_1.jpg", "caption": "Table 5: Node classification on large-scale graphs (%). The baseline results were primarily taken from [82], with the remaining obtained from their respective original papers. OOM indicates out-of-memory when training on a GPU with 24GB memory.", "description": "This table presents the results of node classification experiments on eleven large-scale graph datasets.  It compares the performance of several methods, including the proposed HDSE method integrated into graph transformers, against various GNN baselines. The table shows accuracy scores for each model and dataset.  OOM indicates that the model ran out of memory during training on a 24GB GPU.", "section": "4.3 Results on Large-scale Graphs"}, {"figure_path": "U4KldRgoph/tables/tables_24_2.jpg", "caption": "Table 21: Node classification results of Gapformer with and without HDSE on Cora, CiteSeer, and PubMed.", "description": "This table presents the results of node classification experiments using the Gapformer model, both with and without the Hierarchical Distance Structural Encoding (HDSE) method.  It shows the accuracy achieved on three benchmark datasets: Cora, CiteSeer, and PubMed.  The comparison allows for evaluating the impact of HDSE on the Gapformer's performance.", "section": "4.3 Results on Large-scale Graphs"}, {"figure_path": "U4KldRgoph/tables/tables_24_3.jpg", "caption": "Table 2: Test performance in five benchmarks from [20]. The results are presented as the mean \u00b1 standard deviation from 5 runs using different random seeds. Baseline results were obtained from their respective original papers. * indicates a statistically significant difference against the baseline w/o HDSE from the one-tailed t-test. Highlighted are the top first, second and third results.", "description": "This table presents the results of five graph-level benchmark datasets from [20] comparing the performance of several graph transformer models.  The models are tested using different random seeds, and results are expressed as the mean and standard deviation of five runs.  The table compares baseline models with those enhanced using the HDSE method. A * indicates where a statistically significant improvement was achieved over the baseline models.", "section": "4.2 Results on Graph-level Tasks"}]