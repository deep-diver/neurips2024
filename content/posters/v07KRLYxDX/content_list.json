[{"type": "text", "text": "Achieving Domain-Independent Certified Robustness via Knowledge Continuity ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alan $\\mathbf{Sun}^{1,2}$ , Chiyu $\\mathbf{M}\\mathbf{a}^{2}$ , Kenneth $\\mathbf{Ge}^{1}$ , Soroush Vosoughi2 ", "page_idx": 0}, {"type": "text", "text": "1Carnegie Mellon University, 2Dartmouth College {alansun, kkge}@andrew.cmu.edu, {chiyu.ma.gr, soroush.vosoughi}@dartmouth.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present knowledge continuity, a novel definition inspired by Lipschitz continuity which aims to certify the robustness of neural networks across input domains (such as continuous and discrete domains in vision and language, respectively). Most existing approaches that seek to certify robustness, especially Lipschitz continuity, lie within the continuous domain with norm and distribution-dependent guarantees. In contrast, our proposed definition yields certification guarantees that depend only on the loss function and the intermediate learned metric spaces of the neural network. These bounds are independent of domain modality, norms, and distribution. We further demonstrate that the expressiveness of a model class is not at odds with its knowledge continuity. This implies that achieving robustness by maximizing knowledge continuity should not theoretically hinder inferential performance. Finally, to complement our theoretical results, we present several applications of knowledge continuity such as regularization, a certification algorithm, and show that knowledge continuity can be used to localize vulnerable components of a neural network. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks (DNNs) have demonstrated remarkable generalization capabilities. Their robustness, however, has been considerably more difficult to achieve. Robustness refers to the preservation of model performance under natural or adversarial alterations of the input [18]. DNNs\u2019 lack of robustness, highlighted by seminal works such as [24, 66] and recently [7, 5], poses significant challenges to their adoption in critical applications, underscoring concerns for AI safety and trustworthiness [20, 30, 9, 7]. ", "page_idx": 0}, {"type": "text", "text": "Though issues of robustness emerged from computer vision applications, they have since spanned multiple domains [1, 35, 72, 75, 7]. This research trajectory has not only prompted significant advancements in robustness improvements through architectural, training, and dataset augmentations, but also unveiled the sophistication of adversarial attacks\u2014the process through which counterexamples to robustness are generated [1, 35, 72, 75, 7]. Along the progress made in these parallel directions, a great deal of work has gone into certified robustness which seeks to provide theoretical robustness guarantees. Certification is desirable as it generally transcends any particular task, dataset, or model. ", "page_idx": 0}, {"type": "text", "text": "As a result, Lipschitz continuity has emerged, promising certified robustness by essentially bounding the derivative of a neural network\u2019s output with respect to its input. In this way, Lipschitz continuity directly captures the volatility of a model\u2019s performance, getting at the heart of robustness. Such an approach has proven its merit in computer vision, facilitating robustness under norm and distributional assumptions [29, 59, 78, 76]. Its inherent ease and interpretability has lead to widespread adoption as a means to measure and regulate robustness among practitioners as well [71, 12, 21, 68, 54]. ", "page_idx": 0}, {"type": "text", "text": "Despite these successes in computer vision, there are fundamental obstacles when one tries to apply Lipschitz continuity into discrete or non-metrizable domains such as natural language. Firstly, characterizing distance in this input-output space is highly nontrivial, as language does not have a naturally-endowed distance metric. Additionally, suppose we impose some distance metric on the input-output space [49, 16]. For such a metric to meaningfully characterize adversarial perturbations, it cannot be universally task-invariant. Consider the two sentences (a) \u201cI am happy,\u201d (b) \u201cI am sad.\u201d The ground-truth label of (a) is invariant to the perturbation (a) $\\rightarrow({\\mathfrak{b}})$ , if the task is sentence-structure identification, but it would not be preserved for a task like sentiment classification. Lastly, key architectures such as the Transformer [70] are provably not Lipschitz continuous [36]. Most of these challenges are not unique to language, and they represent a strong divide of our understanding of robustness in discrete/non-metrizable and continuous domains [22, 46]. ", "page_idx": 1}, {"type": "text", "text": "To address these issues, we propose a new conceptual framework which we call knowledge continuity. At its core, we adopt the following axiom: ", "page_idx": 1}, {"type": "text", "text": "Robustness is the stability of a model\u2019s performance with respect to its perceived knowledge of input-output relations. ", "page_idx": 1}, {"type": "text", "text": "Concretely, our framework is grounded on the premise that robustness is better achieved by focusing on the variability of a model\u2019s loss with respect to its hidden representations, rather than forcing arbitrary metrics on its inputs and outputs. Our approach results in certification guarantees independent of domain modality, norms, and distribution. We demonstrate that the expressiveness of a model class is not at odds with its knowledge continuity. In other words, achieving robustness by improving knowledge continuity should not theoretically hinder inferential performance. We show that in continuous settings (i.e. computer vision) knowledge continuity generalizes Lipschitz continuity and inherits its tight robustness bounds. Finally, we present an array of practical applications using knowledge continuity both as an indicator to predict and characterize robustness as well as an additional term in the loss function to train robust classifiers. In sum, our contributions are threefold: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Introduction of knowledge continuity, a new concept that frames robustness as variability of a model\u2019s loss with respect to its hidden representations.   \n\u2022 We theoretically show that knowledge continuity results in certified robustness guarantees that generalize across modalities (continuous, discrete, and non-metrizable). Moreover, this robustness does not come at the expense of inferential performance.   \n\u2022 We present several practical applications of knowledge continuity such as using it train more robust models, in both language processing and vision, identify problematic hidden layers, and using its theoretical guarantees to formulate a novel certification algorithm. ", "page_idx": 1}, {"type": "text", "text": "Although our results apply to all discrete/non-metrizable and continuous spaces, throughout the paper we invoke examples from natural language as it culminates the aforementioned challenges. Further, the ubiquity of large language models make their robustness a timely focus. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "There have been extensive studies on developing robust neural networks with theoretical guarantees.   \nWith respect to our contributions, they can be organized into the following categories. ", "page_idx": 1}, {"type": "text", "text": "Certified robustness with Lipschitz continuity. The exploration of Lipschitz continuity as a cornerstone for improving model robustness has yielded significant insights, particularly in the domain of computer vision. This principle, which ensures bounded derivatives of the model\u2019s output with respect to its input, facilitates a smoother model behavior and inherently encourages robustness against adversarial perturbations. This methodology, initially suggested by [24], has since been rigorously analyzed and expanded upon. Most theoretical results in this area focus on certifying robustness with respect to the $\\ell_{2}$ -norm [11, 86, 25, 2, 38, 29, 4]. A recent push, fueled by new architectural developments, has also expanded these results into $\\ell_{\\infty}$ -norm perturbations [89, 88, 90]. Further, Lipschitz continuity-inspired algorithms also serve practitioners as a computationally effective way to train more robust models [68, 78, 69, 13]. This stands in contrast to (virtual) adversarial training methods which brute-force the set of adversarial examples, then iteratively retrain on them [50, 63, 80]. Though Lipschitz continuity has seen much success in continuous domains, it does not apply to nonmetrizable domains such as language. Further, architectural limitations of prevalent models such as the Transformer [70, 36] exacerbate this problem. These challenges highlight a critical need for a new approach that can accommodate the specificities of discrete and non-metrizable domains while providing robustness guarantees. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Achieving robustness in discrete/non-metrizable spaces. Non-metrizable spaces, where it is nontrivial to construct a distance metric on the input/output domains, pose a unique challenge to certified robustness. Instead of focusing on point-wise perturbations, many studies have opted to examine how the output probability distribution of a model changes with respect to input distribution shifts by leveraging information bottleneck methods [67, 73, 53] (see also out-of-distribution generalization: [42, 83, 60]). Most of these bounds lack granularity and cannot be expressed in closed-form. In contrast to these theoretical approaches, recent efforts have refocused on directly adapting the principles underlying Lipschitz continuity to language. Virtual adversarial training methods such as [43, 85] mimic the measurement of Lipschitz continuity by comparing changes in the textual embeddings with the KL-divergence of the output logits. Along these lines, techniques akin to those used in adversarial training in vision have also been translated to language, reflecting a shift towards robustness centered around the learned representation space [40, 23, 35]. Though these approaches have seen empirical success, they lack theoretical guarantees. As a result, their implementations and success rate is heavily task-dependent [43, 85]. There have also been attempts to mitigate the non-Lipschitzness of Transformers [87, 82] by modifying its architecture. These changes, however, add significant computational overhead. ", "page_idx": 2}, {"type": "text", "text": "Other robustness approaches. In parallel, other certified robustness approaches such as randomized smoothing [12, 39, 37] give state-of-the-art certification for $\\ell_{2}$ -based perturbations. Notable works such as [34, 74] have sought to generalize these techniques into language, but their guarantees strongly depend on the type of perturbation being performed. On the other hand, analytic approaches through convex relaxation inductively bound the output of neurons in a ReLU network across layers [79, 81, 77]. These works, however, are difficult to scale and also do not transfer easily to discrete/non-metrizable domains. ", "page_idx": 2}, {"type": "text", "text": "Our approach, inspired by Lipschitz continuity, distills the empirical intuitions from the works of [43, 85] and provides theoretical certification guarantees independent of perturbation-type [34, 74] and domain modality. We demonstrate that knowledge continuity yields many practical applications analogous to Lipschitz continuity which are easy to implement and are computationally competitive. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. Let $\\mathbb{R}^{\\geq0}:=[0,\\infty)$ . For any function $f:\\mathcal{X}\\to\\mathcal{Y}$ , we denote graph $(f):=\\{(x,y)\\in$ ${\\mathcal{X}}\\times{\\mathcal{Y}}\\;:\\;f(x)\\,=\\,y\\}$ . For $n\\,\\in\\,\\mathbb{N}$ , let $[n]$ denote the set $\\{1,2,\\dots,n\\}$ . $(\\mathcal{X},\\mathcal{F}_{\\mathcal{X}},\\mathbb{P}_{\\mathcal{X}})$ , $(\\mathfrak{H},\\mathcal{F}_{\\mathfrak{H}},\\mathbb{P}_{\\mathfrak{H}})$ are probability spaces and $(\\mathcal{X}\\times\\mathcal{Y},\\mathcal{F}_{\\mathcal{X}}\\otimes\\mathcal{F}_{\\mathcal{Y}},\\mathbb{P}_{\\mathcal{X}}\\times\\mathbb{P}_{\\mathcal{Y}})$ denotes the product measurable space of the probability spaces $\\mathcal X,\\mathcal Y$ . Since our contribution focuses on the supervised learning regime, we colloquially refer to $\\mathcal X,\\mathcal Y$ as the input and labels, respectively. We call any probability measure $\\mathbb{P}_{\\mathcal{X}\\times\\mathcal{X}}$ absolutely continuous to $\\mathbb{P}_{\\mathcal{X}}\\times\\mathbb{P}_{\\mathcal{Y}}$ (i.e. $(\\mathbb{P}_{\\mathcal{X}}\\times\\mathbb{P}_{\\mathcal{Y}})(E)\\;=\\;0\\;\\Rightarrow\\;\\mathbb{P}_{\\mathcal{X}\\times\\mathcal{Y}}(E)\\;=\\;0)$ a data distribution and denote it as $D_{\\mathcal{X},\\mathcal{D}}$ . If $(\\breve{\\mathscr Z},d_{\\mathscr Z})$ is a metric space with metric $d_{\\mathcal{Z}}$ and $A\\subset{\\mathcal{Z}}$ , then for any $z\\in{\\mathcal{Z}}$ , $d_{\\mathcal{Z}}(z,A)\\,=\\,\\operatorname*{inf}_{a\\in A}^{\\ \\sim}d_{\\mathcal{Z}}(a,z)$ . We say that a metric space, $(\\mathcal{Z},\\breve{d}_{\\mathcal{Z}})$ , is bounded by some $B\\in\\mathbb{R}^{\\geq0}$ , if $\\mathrm{sup}_{z^{\\prime},z\\in\\mathcal{Z}}\\,d(z,z^{\\prime})<B$ . Denote by $\\operatorname{Id}_{\\mathcal{Z}}:\\mathcal{Z}\\to\\mathcal{Z}$ the identity function on $\\mathcal{Z}$ . Let $\\mathcal{L}:\\mathcal{V}\\times\\mathcal{V}\\to\\mathbb{R}^{\\geq0}$ be a loss function where $\\mathcal{L}(y,y^{\\prime})=0$ if and only if $y=y^{\\prime}$ . For any $f:\\mathcal{X}\\to\\mathcal{Y}$ and $(x,y),(x^{\\prime},y^{\\prime})\\in\\mathcal{X}\\times\\mathcal{Y}$ , we denote $\\Delta\\mathcal{L}_{f}^{(x,y)}(x^{\\prime},y^{\\prime}):=|\\mathcal{L}(f(x),y)-\\mathcal{L}(f(x^{\\prime}),y^{\\prime})|$ , essentially the absolute difference in loss between $(x,y)$ and $(x^{\\prime},y^{\\prime})$ . Unless otherwise specified, it will be assumed that $f$ is a measurable function from $\\scriptstyle{\\mathcal{X}}$ to $\\boldsymbol{\\mathfrak{y}}$ with a metric decomposition (see Def. 1). ", "page_idx": 2}, {"type": "text", "text": "Lipschitz continuity. Given two metric spaces $(\\mathcal{X},d_{\\mathcal{X}}),(\\mathcal{Y},d_{\\mathcal{Y}})$ a function $f:\\mathcal{X}\\to\\mathcal{Y}$ is $K$ -Lipschitz continuous if there exists $K\\in\\mathbb{R}^{\\geq0}$ such that for all $x,x^{\\prime}\\in\\mathcal{X}$ , $d_{\\mathcal{Y}}(f(x),f(x^{\\prime}))\\leq K d_{\\mathcal{X}}(x,x^{\\prime})$ . ", "page_idx": 2}, {"type": "text", "text": "4 Knowledge Continuity ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we provide the formal definition of knowlege continuity and explore its theoretical properties. ", "page_idx": 2}, {"type": "text", "text": "We start by defining a model\u2019s perceived knowledge through a rigorous treatment of its hidden representation spaces. By considering the distance between inputs in some representation space in conjunction with changes in loss, we result in a measure of volatility analogous to Lipschitz continuity. ", "page_idx": 2}, {"type": "text", "text": "Bounding this volatility in expectation then directly leads to our notion of knowledge continuity. With these tools, we demonstrate a host of theoretical properties of knowledge continuity including its certification of robustness, guarantees of expressiveness, and connections to Lipschitz continuity in continuous settings. We summarize our theoretical contributions as follows: ", "page_idx": 3}, {"type": "text", "text": "\u2022 We define the perceived knowledge of a model as well as volatility and knowledge continuity within a model\u2019s representation space (see Def. 1, 2, 3, 4, respectively). \u2022 We prove that knowledge continuity implies probabilistic certified robustness under perturbations in the representation space and constraining knowledge continuity should not hinder the expressiveness of the class of neural networks (see Thm. 4.1 and Prop. 4.3, 4.4, respectively). \u2022 We prove that in some cases knowledge continuity is equivalent (in expectation) to Lipschitz continuity. This shows that our axiomization of robustness aligns with existing results when perturbation with respect to the input is well-defined (see Prop. 4.6, 4.8). ", "page_idx": 3}, {"type": "text", "text": "4.1 Defining Perceived Knowledge ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Knowledge is generally understood as a relational concept: it arises from the connections we make between ideas, experiences, and stimuli [26]. Herein, we capture the perceived knowledge of a model by focusing on the relations it assigns to input-input pairs. Specifically, these relations are exposed by decomposing a function $f:\\mathcal{X}\\to\\mathcal{Y}$ into projections to intermediate metric spaces. Formally, ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Metric Decomposition). We say that $f$ admits a metric decomposition if there exists metric spaces $(\\mathcal{Z}_{1},d_{1}),\\ldots,(\\mathcal{Z}_{n},d_{n})$ with metrics $d_{k}$ for $k\\in[n]$ such that ", "page_idx": 3}, {"type": "text", "text": "1. $(\\mathcal{Z}_{k},d_{k})$ is endowed with its Borel $\\sigma$ -algebra.   \n2. There exists measurable mappings $h_{0},h_{1},\\ldots,h_{n}$ where $h_{0}:\\mathcal{X}\\to\\mathcal{Z}_{1}$ , $h_{k}:{\\mathcal{Z}}_{k}\\to{\\mathcal{Z}}_{k+1}.$ for $k\\in[n-1],$ , and $h_{n}:\\mathcal{Z}_{n}\\to\\mathcal{D}$ .   \n3. $f=h_{n}\\circ h_{n-1}\\circ\\ldots\\circ h_{1}\\circ h_{0}$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 1. If $\\scriptstyle{\\mathcal{X}}$ is a metric space with metric $d_{\\mathcal{X}}$ and $\\mathcal{F}_{\\mathcal{X}}$ is its Borel $\\sigma$ -algebra, then for any measurable mapping $f:\\mathcal{X}\\to\\mathcal{Y}$ there exists the trivial metric decomposition ", "page_idx": 3}, {"type": "equation", "text": "$$\nf=f\\circ{\\mathrm{Id}}_{\\chi}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Therefore, in computer vision applications where $(\\mathcal{X},d_{\\mathcal{X}})=(\\mathbb{R}^{n},\\ell_{p})$ for some $n\\in\\mathbb{Z}^{+}$ , we can apply this trivial decomposition to yield bounds which mirror the certification guarantees of Lipschitz continuity. This is discussed in detail in Section 4.5. ", "page_idx": 3}, {"type": "text", "text": "To the best of our knowledge, all deep learning architectures admit metric decompositions, since their activations are generally real-valued. So, for all subsequent functions from $\\scriptstyle{\\mathcal{X}}$ to $\\boldsymbol{\\mathfrak{V}}$ , unless otherwise specified, we assume they are measurable and possess a metric decomposition. Further, we denote $\\bar{f}^{k}=h_{k}\\circ h_{k-1}\\circ...\\circ h_{1}\\circ\\dot{h_{0}}$ and adopt the convention of calling $h_{k}$ the $k^{\\mathrm{{th}}}$ hidden layer. In Appendix A, we present several metric decompositions for a variety of architectures. ", "page_idx": 3}, {"type": "text", "text": "For any metric-decomposible function, an immediate consequence of our definition is that its metric decomposition may not be unique. However, in the context of neural networks, this is a desirable property. Seminal works from an array of deep learning subfields such as semi-supervised learning [57], manifold learning [51], and interpretability [10, 47] place great emphasis on the quality of learned representation spaces by examining the induced-topology of their metrics. This often does not affect the typical performance of the estimator, but has strong robustness implications [33]. Our results, which are dependent on particular metric decompositions, capture this trend. In Section 4.4, we discuss in detail the effects of various metric decompositions on our theoretical results. ", "page_idx": 3}, {"type": "text", "text": "4.2 Defining Knowledge Continuity ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first introduce what it means for a model\u2019s performance to be volatile at a data point relative to its metric decomposition. Then, we contrast knowledge continuity with Lipschitz continuity, pointing out key differences that will allow us to prove more general bounds. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 ( $k$ -Volatility). Let $f:\\mathcal{X}\\to\\mathcal{Y}$ and $\\mathcal{L}$ be any loss function. The $k$ -volatility of a point $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ which we denote as $\\sigma_{f}^{k}(x,y)$ is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma_{f}^{k}(x,y):=\\mathbb{E}_{(x^{\\prime},y^{\\prime})\\sim{\\cal D}_{{\\boldsymbol x},{\\boldsymbol y}}}\\left[\\frac{\\Delta\\mathcal{L}_{f}^{(x,y)}({\\boldsymbol x}^{\\prime},y^{\\prime})}{d_{k}(f^{k}({\\boldsymbol x}),f^{k}({\\boldsymbol x}^{\\prime}))}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "v07KRLYxDX/tmp/48a1f28e9eb53b3d47c644cf4cf8501efbe47d8aef1501d14756ea53c297604b.jpg", "img_caption": ["Sparse Knowledge Continuity Knowledge Continuity $\\bigcirc$ Knowledge Discontinuity $\\mathcal{L}(\\boldsymbol{f}(\\boldsymbol{x}),\\boldsymbol{y})$ "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: Examples of knowledge (dis)continuities. $f:\\mathcal{X}\\to\\mathcal{Y}$ is a measurable map, and $(\\mathcal{Z}_{k},d_{k})$ is one of its hidden representations. The color of the points indicates loss. $\\spadesuit$ denotes knowledge continuity induced by sparsity: an isolated concept with no knowledge relations close to it. So, any perturbation moves $\\spadesuit$ far away with high probability. Smooth changes in loss around $\\star$ implies knowledge continuity. Finally, $\\blacktriangle$ is not knowledge continuous due to drastic changes in loss nearby. Notice that the classification of points is independent of input/output clustering behavior since $\\mathcal{X},\\mathcal{Y}$ may not be endowed with a metric. ", "page_idx": 4}, {"type": "text", "text": "where $d_{k}$ is the distance metric associated with $f^{\\,\\prime}s\\;k^{t h}$ hidden layer. ", "page_idx": 4}, {"type": "text", "text": "By performing some algebra on the definition, we see that it decomposes nicely into two distinct terms: sparsity of the representation and variation in loss. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{f}^{k}(x,y)=\\mathbb{E}_{(x^{\\prime},y^{\\prime})\\sim{D_{\\lambda,\\mathcal{Y}}}}\\left[\\frac{\\left|\\mathcal{L}\\left(f(x),y\\right)-\\mathcal{L}\\left(f(x^{\\prime}),y^{\\prime}\\right)\\right|}{d_{k}(f^{k}(x),f^{k}(x^{\\prime}))}\\right],}\\\\ &{\\qquad=\\mathcal{L}(f(x),y)\\,\\mathbb{E}_{(x^{\\prime},y^{\\prime})\\sim{D_{\\lambda,\\mathcal{Y}}}}\\left[\\underbrace{\\frac{1}{d_{k}(f^{k}(x),f^{k}(x^{\\prime}))}}_{\\mathrm{sparsity}}\\cdot\\underbrace{\\left|1-\\frac{\\mathcal{L}(f(x^{\\prime}),y^{\\prime})}{\\mathcal{L}(f(x),y)}\\right|}_{\\mathrm{variation~in~loss}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our notion of volatility essentially measures the change in performance with respect to perturbations to a model\u2019s perceived knowledge. In particular, Eq. 4.3 reveals that there are two interactions in play which we illustrate in Fig. 1. Informally, we say that $(x,y)$ is highly volatile if there is a large discrepancy in performance between it and points that are perceived to be conceptually similar. Therefore, highly volatile points capture inaccurate input-input knowledge relations. Additionally, $(x,y)$ experiences low volatility if the space around it is sparse with respect to $D_{\\mathcal{X},\\mathcal{D}}$ . In other words, any set of perturbations applied in $\\mathcal{Z}_{k}$ would push $(x,y)$ far away, with high probability. This makes $(x,y)$ an isolated concept with little knowledge relationships associated with it. ", "page_idx": 4}, {"type": "text", "text": "Similar to Lipschitz continuity, the boundedness of the $k$ -volatility of $f$ across the data distribution is crucial and we denote this class of functions as knowledge continuous. ", "page_idx": 4}, {"type": "text", "text": "Definition 3 (Pointwise $\\epsilon$ -Knowledge Continuity). We say that $f$ is $\\epsilon$ -knowledge continuous at $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ with respect to a function $f$ , loss function $\\mathcal{L}$ , and hidden layer $k$ if $\\bar{\\sigma}_{f}^{k}(x,y)<\\epsilon$ . ", "page_idx": 4}, {"type": "text", "text": "Conversely, we say that $(x,y)$ is $\\epsilon$ -knowledge discontinuous if the previous inequality does not hold. Further, $(x,y)$ is simply knowledge discontinuous if $\\sigma_{f}^{k}(x,y)$ is unbounded. Now, we extend this definition globally by considering the $k$ -volatility between all pairs of points. ", "page_idx": 4}, {"type": "text", "text": "Definition 4 (Expected $\\epsilon$ -Knowledge Continuity). We say that $f$ is $\\epsilon$ -knowledge continuous with respect to a loss function $\\mathcal{L}$ and hidden layer $k$ if ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(x,y)\\sim\\cal D}[\\sigma_{f}^{k}(x,y)]<\\epsilon.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Though the functional forms of Lipschitz continuity and knowledge continuity are similar, there are important differences that allow us to prove more general results. Firstly, unlike Lipschitz continuity which is an analytical property of the model $f$ , knowledge continuity is a statistical one. In this way, non-typical data points, even if they are volatile, are ignored, whereas Lipschitz continuity treats all points equally. This is necessary in many discrete applications, as projecting a countable input space onto a non-countable metric space inevitably results in a lack of correspondence thereof. Moreover, ground-truth relations from ${\\mathcal{X}}\\to{\\mathcal{Y}}$ may not be well-defined on all of $\\scriptstyle{\\mathcal{X}}$ : consider sentiment classification of an alpha-numeric UUID string or dog-cat classification of Gaussian noise. Secondly, the knowledge continuity of an estimator is measured with respect to the loss function rather than its output. This property allows us to achieve the expressiveness guarantees in Section 4.4, since it places no restrictions on the function class of estimators. Lastly, knowledge continuity measures the distance between inputs with the endowed metric in its hidden layers. This flexibility allows us to define knowledge continuity even when the input domain is not a metric space. ", "page_idx": 5}, {"type": "text", "text": "4.3 Certification of Robustness ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our first main result demonstrates that $\\epsilon$ -knowledge continuity implies probabilistic certified robustness in the hidden representation space. In Theorem 4.1, given some reference set $A\\subset\\mathcal X\\times\\mathcal Y$ , we bound the probability that a $\\delta$ -sized perturbation in the representation space away from $A$ will result in an expected $\\eta$ change in loss. In other words, knowledge continuity is able to characterize the robustness of any subset of data points with positive measure. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Let $\\textit{\\textbf{A}}\\subset\\textit{\\textbf{x}}\\times\\textit{\\textbf{y}}$ such that $\\mathbb{P}_{D_{\\mathcal{X},\\mathcal{Y}}}[A]~>~0$ and $\\delta,\\eta\\ \\ >\\ \\ 0.$ . Let $A^{\\prime}\\quad=$ $\\left\\{(x^{\\prime},y^{\\prime})\\in\\mathcal{X}\\times\\mathcal{Y}:\\mathbb{E}_{(x,y)\\sim D_{\\mathcal{X},\\mathcal{Y}}}\\Delta\\mathcal{L}_{f}^{(x,y)}(x^{\\prime},y^{\\prime})>\\eta\\right\\}$ . If $\\colon f:\\,\\mathcal{X}\\to\\mathcal{D}$ is $\\epsilon$ -knowledge continuous with respect to the hidden layer indexed by $k$ and $(\\mathcal{Z}_{k},d_{k})$ is bounded by $B>0$ , then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(x,y)\\sim D_{\\mathcal{X},\\mathcal{Y}}}[A^{\\prime}\\mid d_{k}(f^{k}(x),f^{k}(A))<\\delta]\\leq\\frac{\\epsilon\\delta}{\\eta\\left(1-\\exp\\left[-\\Omega\\left(\\frac{\\delta}{B}-\\sqrt{\\log\\frac{1}{\\mathbb{P}[A]}}\\right)^{2}\\right]\\right)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof sketch. We apply the definition of conditional probability $P(A|B)\\,=\\,P(A\\cap B)/P(B)$ and bound $P(A\\cap B),P(B)$ , separately. The numerator, $\\mathbb{P}[A^{\\prime}$ and $d_{k}(f^{\\bar{k}}(x),f^{k}(A))<\\delta]$ , is upper-bounded through an application of Markov\u2019s Inequality. On the other hand, we apply known concentration inequalities to lower bound $\\mathbb{P}[d_{k}(f^{k}(x),^{*}f^{k}(\\mathring{A}))<\\delta]$ , combining these results in the theorem. We present the proof in its entirety in Appendix B. \u25a0 ", "page_idx": 5}, {"type": "text", "text": "This demonstrates that knowledge continuity results in certification of robustness, independent of distance metric and domain modality. The assumption of boundedness and requirement to know $\\mathbb{P}[A]$ can be lost by taking limits of Eq. 4.5 with respect to $B$ and $\\mathbb{P}[A]$ . This yields the following corollary. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.2. If $(\\mathcal{Z}_{k},d_{k})$ is unbounded, then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(x,y)\\sim{\\cal D}_{\\mathcal{X},\\mathcal{Y}}}[A^{\\prime}\\mid d_{k}(f^{k}(x),f^{k}(A))<\\delta]\\leq\\frac{\\epsilon\\delta}{\\eta(1-\\mathbb{P}[A])}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$I f\\,\\mathbb{P}[A]=0,$ , then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(x,y)\\sim D_{X,y}}[A^{\\prime}\\mid d_{k}(f^{k}(x),f^{k}(A))<\\delta]\\leq\\frac{\\epsilon\\delta}{\\eta}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. These results follow from directly taking the limit as $B\\to\\infty$ and applying some of the bounds acquired in the proof of Thm. 4.1. This yields Eq. 4.6. Next, jointly taking the limit as $\\mathbb{P}[A]\\to0$ and $B\\to\\infty$ results in Eq. 4.7. \u25a0 ", "page_idx": 5}, {"type": "text", "text": "In both Thm. 4.1 and Cor. 4.7, we yield probabilistic guarantees like [12], rather than deterministic ones. Though deterministic bounds are desirable, the stochasticity of our framework is necessary for its generalization across different domains. For most continuous, metrizable applications (like computer vision), models learn a hidden representation space where most minute changes in this space correspond to tangible inputs. The same cannot be said for many discrete or non-metrizable applications. In natural language processing, the correspondence between the learned representation space and the input is sparse, resulting in lots of \u201cdead space\u201d: portions of the hidden representation space that do not correspond to any input [3, 19]. And so, by incorporating the data distribution into our bounds, we implicitly adjust for this: assigning zero-measure to the aforementioned \u201cdead space.\u201d ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.4 Expressiveness ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our second main result demonstrates that $\\epsilon$ -knowledge continuity can be achieved without theoretically compromising the accuracy of the model. In other words, universal function approximation is an invariant property with respect to $\\epsilon$ -knowledge continuity. Universal approximation results have seen a great deal of theoretical work, as they put limits on what neural networks can represent [15, 31, 45]. As discussed in Section 2, Lipschitz continuous functions do not achieve universal function approximation with respect to the set of all functions, in particular, non-continuous ones. However, we show that under strong conditions this is achievable with knowledge continuity. ", "page_idx": 6}, {"type": "text", "text": "First, let us formally define a universal function approximator. ", "page_idx": 6}, {"type": "text", "text": "Definition 5 (Universal Function Approximator). Suppose that $\\mathcal{L}$ is Lebesgue-integrable in both coordinates. Let $F\\subset\\mathcal{D}^{\\mathcal{X}}$ be a set of measurable functions from ${\\mathcal{X}}\\to{\\mathcal{Y}}$ such that for any $f\\in\\mathcal F$ , there exists $\\mu_{f}\\ll D_{\\mathcal{X},\\mathcal{D}}$ such that $\\mu_{f}(g r a p h(f))=1$ . Then, $\\mathcal{V}\\subset\\mathcal{F}$ is a universal function approximator of $\\mathcal{F}$ if for every $f\\in\\mathcal F$ and every $\\epsilon>0$ , there exists ${\\hat{f}}\\in{\\mathcal{V}}$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\int\\mathcal{L}(\\hat{f}(x),y)\\,d\\mu_{f}<\\epsilon.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We now show any universal function approximator can be made robust through the trivial metric decomposition. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.3. Let $\\mathcal{V}\\subset\\mathcal{V}^{\\mathcal{X}}$ be a universal function approximator of $\\mathcal{V}^{\\mathcal{X}}$ with respect to some loss function $\\mathcal{L}$ . Then, for any $f\\in\\mathcal{D}^{\\mathcal{X}}$ and sequence $\\epsilon_{1},\\epsilon_{2},\\ldots$ such that $\\epsilon_{n}\\rightarrow0$ there are a sequence of $\\epsilon_{n}$ -knowledge continuous functions in $\\nu$ such that $\\begin{array}{r}{\\int\\mathcal{L}(f_{n}(x),y)\\,d\\mu_{f}<\\epsilon_{n},}\\end{array}$ , for $n\\in\\mathbb{N}$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. Choose $f_{n}\\in\\mathcal{V}$ such that $\\begin{array}{r}{\\int\\mathcal{L}(f_{n}(x),y)\\,d\\mu_{f}<\\frac{1}{2}\\epsilon_{n}}\\end{array}$ . Consider the 1-layer metric decomposition of $f,h_{1}:\\mathcal{X}\\to\\mathcal{Z}_{1}$ where $\\mathcal{Z}_{1}=\\mathcal{X}$ equipped with the trivial metric $(d_{1}(x,y)=1$ if $x\\neq y$ and 0 otherwise). Then, $f_{n}=f_{n}{\\circ}h_{1}$ . So, it follows that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb E\\,\\sigma_{f_{n}}^{1}(x,y)=\\int\\frac{\\Delta\\mathcal{L}_{f_{n}}^{(x,y)}(x^{\\prime},y^{\\prime})}{d_{1}(h_{1}(x),h_{1}(x^{\\prime}))}\\,d\\mu_{f},}}\\\\ &{}&{\\leq\\int\\Delta\\mathcal{L}_{f_{n}}^{(x,y)}(x^{\\prime},y^{\\prime})\\;d\\mu_{f},}\\\\ &{}&{\\leq\\epsilon_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and by the construction of $f_{n}$ , the proof is completed. ", "page_idx": 6}, {"type": "text", "text": "In other words, if our estimator was given \u201cinfinite representational capacity,\u201d robustness can be trivially achieved by isolating every point as its own concept (as discussed in Section 4.2). More generally, if we instead considered a generalized discrete metric (fix $c\\in[0,\\infty]$ , $d(x,y)=c$ if and only if $x=y$ and $d(x,y)=0$ , otherwise), then as $c\\to\\infty$ , $k$ -volatility converges pointwise to 0 almost everywhere assuming that the loss is finite almost everywhere. In practice, we find these degenerate decompositions to be unreasonable as they also trivialize robustness. For example, if $c=\\infty$ , then robustness is not well-defined as any perturbation would lead to a point that is perceived to be infinitely far away. In this sense, our framework accounts for different notions of robustness, strong and weak. The next result builds on Prop. 4.3 and demonstrates how a stronger notion of robustness will affect expressiveness. These added constraints make it so that trivial metric decompositions are no longer possible unless the metric in $\\scriptstyle{\\mathcal{X}}$ is also trivial. We state this formally below, note the highlighted differences between this and Prop. 4.3. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.4. Suppose $(\\mathcal{X},d_{\\mathcal{X}}),(\\mathcal{Y},d_{\\mathcal{Y}}):=(\\mathcal{X},d_{\\mathcal{X}})$ are compact metric spaces, $F\\subset\\mathcal{D}^{\\mathcal{X}}$ is the set of all continuous functions from $\\scriptstyle{\\mathcal{X}}$ to $\\boldsymbol{\\mathfrak{y}}$ such that $\\int d_{\\mathcal{X}}(x,x^{\\prime})^{-1}d\\mu_{f}<\\infty$ and $\\mathcal{L}$ be Lipschitz continuous in both coordinates. Then, there exists a universal function approximator $\\nu$ of ${\\boldsymbol{\\mathcal{F}}}$ that is knowledge continuous (i.e. \ud835\udd3c $\\sigma_{f}^{k}(x,y)<\\infty$ for some $k$ ). ", "page_idx": 6}, {"type": "text", "text": "Proof sketch. We show an outline of the proof here and defer the full proof to Appendix C. By the Stone-Weierstrass Theorem, the set of Lipschitz continuous functions is dense in the set of all continuous functions from $\\scriptstyle{\\mathcal{X}}$ to $\\boldsymbol{\\mathfrak{V}}$ . Since $\\mathcal{L}$ is Lipschitz continuous in both coordinates, through some algebra, \ud835\udd3c $\\sigma_{f}^{1}(x,y)<\\infty$ , where $h_{1}=\\mathrm{Id}_{\\chi}$ and we yield the statement of the theorem. \u25a0 ", "page_idx": 7}, {"type": "text", "text": "The additional constraint $\\begin{array}{r}{\\int d_{\\mathcal{X}}(x,x^{\\prime})^{-1}d\\mu_{f_{\\cdot}}}\\end{array}$ requires data points to be sparsely layed out in the representation space. As discussed previously, this assumption is generally reasonable for discrete applications. In conjunction with Prop. 4.3, we have shown that the class of knowledge continuous functions is strictly larger than the class of Lipschitz continuous ones. Though we show that universal approximation by knowledge continuous networks is achievable, it is unclear whether these results still hold if the \u201ctightness\u201d of the metric decompositions is bounded. Specifically, the construction in Prop. 4.3 results in a metric decomposition with infinite Hausdorff dimension. Is it possible to achieve Prop. 4.3 in its most general form if we only consider the set of all knowledge continuous functions with metric decompositions with finite Hausdorff dimension? Based on the theoretical and empirical results of [62, 33], respectively, we conjecture in the negative and leave its resolution open. ", "page_idx": 7}, {"type": "text", "text": "Conjecture 4.5. If $\\mathcal{V}\\subset\\mathcal{V}^{\\mathcal{X}}$ is a universal function approximator with respect to some Lebesgueintegrable loss function $\\mathcal{L}$ . Then, for any $f\\in\\mathcal{D}^{\\mathcal{X}}$ , there does not exist a sequence of functions with metric decompositions of finite Hausdorf fdimension that achieve arbitrarily small approximation error (i.e. $\\textstyle\\int{\\mathcal{L}}({\\hat{f}}(x),y)d\\mu_{f})$ and knowledge continuity. ", "page_idx": 7}, {"type": "text", "text": "4.5 Connections to Lipschitz Continuity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now demonstrate that our axiomization of robustness presented in Section 1 aligns with the notion of robustness2 commonly prescribed in vision [18]. This unifies the certified robustness bounds with respect to the representation space derived in Thm. 4.1 with existing work certifying robustness with respect to the input space in continuous applications such as vision. ", "page_idx": 7}, {"type": "text", "text": "Our first result identifies conditions under which knowledge continuity, implies Lipschitz continuity. ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.6. Suppose that $(\\mathcal{X},d_{\\mathcal{X}}),(\\mathcal{Y},d_{\\mathcal{Y}})$ are metric spaces. Let the first \ud835\udc5bmetric decompositions of $f:\\mathcal{X}\\to\\mathcal{Y}$ be $K_{i}$ -Lipschitz continuous, for $i\\in[n]$ . If $f$ is $\\epsilon$ -knowledge continuous with respect to the $n^{t h}$ hidden layer and $d_{\\mathcal{Y}}(f(x),f(x^{\\prime}))\\leq\\eta\\Delta\\mathcal{L}_{f}^{(x,y)}(x^{\\prime},y)$ for all $x,x^{\\prime}\\in\\mathcal{X}$ , $y\\in\\mathcal{V}$ , and some $\\eta>0$ , then $f$ is Lipschitz continuous in expectation. That is, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(x,y),(x^{\\prime},y^{\\prime})\\sim D_{\\boldsymbol{X},\\boldsymbol{y}}}\\,\\frac{d_{\\boldsymbol{y}}(\\boldsymbol{f}(\\boldsymbol{x}),\\boldsymbol{f}(\\boldsymbol{x}^{\\prime}))}{d_{\\boldsymbol{\\bar{x}}}(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})}\\leq\\epsilon\\eta\\prod_{j=1}^{n}K_{j}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The proof is presented in Appendix D and follows easily through some algebriac manipulation. It is easy to see that if $f$ is knowledge continuous with respect to some identity (or contractive) metric decomposition, then we can loose the repeated product. Analogous to Remark 1, the concepts of Lipschitz continuity and knowledge continuity become similar when we can assign metrics to the input-output spaces. Next, combining this proposition with an auxiliary result from [89], we directly yield a certification on the input space. ", "page_idx": 7}, {"type": "text", "text": "Corollary 4.7. Suppose that assumptions of Prop. 4.6 are true. And also assume that $(\\mathcal{X},d_{\\mathcal{X}})=$ $(\\mathbb{R}^{n},\\ell_{p})$ , $(\\mathfrak{V},d_{\\mathfrak{V}})=(\\mathbb{R}^{m},\\ell_{p}),$ , for $1\\,\\leq\\,p\\,\\leq\\,\\infty$ . Define a classifier from $f:\\,\\mathbb{R}^{n}\\,\\to\\,\\mathbb{R}^{m}$ , $g_{i}$ , where $g(x):=$ arg $\\operatorname*{max}_{k\\in[m]}f_{k}(x)$ for any $x\\in\\mathbb{R}^{n}$ . Then, with probability $\\begin{array}{r}{1-\\frac{\\epsilon\\eta}{t}\\prod_{j=1}^{n}K_{j}.}\\end{array}$ , $g(x)=g(x+\\delta)$ for all $\\left\\Vert\\delta\\right\\Vert_{p}<(2^{1/p}/2t)m a r g i n(f(x))$ and $t>0$ . $f_{k}(x)$ is the $k^{t h}$ coordinate of $f(x)$ and margin $(f(x))$ denotes the difference between the largest and second-largest output logits. ", "page_idx": 7}, {"type": "text", "text": "We present the proof in Appendix D. Our second result identifies conditions under which Lipschitz continuity, implies knowledge continuity. ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.8. Let $(\\mathcal{X},d_{\\mathcal{X}}),(\\mathcal{Y},d_{\\mathcal{Y}})$ be a metric spaces. Let $f:\\mathcal{X}\\to\\mathcal{Y}$ be $\\epsilon$ -Lipschitz continuous and $\\mathcal{L}(f(x),y)$ be $\\eta$ -Lipschitz continuous with respect to both coordinates. If the first \ud835\udc5bmetric decompositions of $f$ are $K_{i}$ -Lipschitz continuous, then $f$ is knowledge continuous with respect to the $n^{t h}$ hidden layer. That is, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(x,y)\\sim D_{\\mathcal{X},\\mathcal{Y}}}\\,\\sigma_{f}^{n}(x,y)\\le\\epsilon\\eta\\prod_{j=1}^{n}\\frac{1}{K_{j}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "v07KRLYxDX/tmp/e42ba024429e76b32793548b33e0feeca5df25abf6e758a5d2077e56ad8286e3.jpg", "img_caption": ["Figure 2: (a) The average percentage of successful adversarial attacks by TextFooler [35] on a host of models [58, 57, 16, 44] and the IMDB [48] dataset regressed with the average of knowledge continuity coefficients across all hidden layers ( $\\bar{R}^{2}=0.35)$ . (b) $k$ -Volatility as $k$ is varied across a model\u2019s relative depth. (c) Correlation between $k$ -volatility and adversarial vulnerability (averaged across all models shown in (b)) with respect to TextFooler [35] as $k$ varies. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We detail the proof of this proposition in Appendix D. We note that in continuous applications such as computer vision, the assumptions of both propositions are generally met (i.e. our input-output spaces are metric spaces, all hidden layers are Lipschitz, and loss functions are locally Lipschitz). Furthermore, common architectures such as fully connected networks, CNNs, RNNs, and even vision transformers are Lipschitz continuous [71, 55]. This implies that our notion of robustness is indeed an appropriate generalization that transcends domain modality since in continuous settings we can recover the strong bounds of Lipschitz continuity while expanding into new discrete and non-metrizable territory. ", "page_idx": 8}, {"type": "text", "text": "5 Practical Applications ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In addition to the theoretical guarantees given by knowledge continuity in Section 4, we also demonstrate that knowledge continuity can be easily applied in practice. First, we find that knowledge continuity, similar to Lipschitz continuity, can be used to gauge adversarial robustness. Along these lines, our measure of volatility (see Def. 2) can be used to isolate particularly vulnerable hidden representations. These applications then directly motivate regulation of knowledge continuity as a means to enforce robustness. ", "page_idx": 8}, {"type": "text", "text": "Unless otherwise specified, we run all of our experiments on the IMDB dataset [48] (a sentiment classification task) using a host of language models from different model families (encoder, decoder, encoder-decoder). We also present additional experiments on vision tasks. These experiments can be found in the Appendix G. ", "page_idx": 8}, {"type": "text", "text": "Knowledge continuity can predict adversarial robustness. For a given model, $f$ , with $n$ hidden representations, choose some $k\\in[n]$ . Then, consider the hidden representation index by $k$ . For this fixed $k$ , we determine its $k$ -volatility by directly estimating Def. 2 through a naive Monte-Carlo algorithm (see Appendix G for more details). Repeating this for all $k\\in[n]$ , we yield a collection of $k$ -volatilities which we denote as $\\{\\epsilon_{1},\\hdots,\\epsilon_{n}\\}$ , one for each hidden layer. When we regress a simple average of these coefficients, \ud835\udc5b\u22121  \ud835\udc5b\ud835\udc58=1 , with the empirical adversarial robustness (estimated using TextFooler [35]), a strong correlation is observed. This is shown in Fig. 2(a). In particular, knowledge continuity alone is able to explain $35\\%$ of the variance in adversarial attack success rate. When we combine $k$ -volatility with other model properties like size, model family, even more variance can be explained $R^{2}=0.48)$ . Thus, knowledge continuity may be used as a computationally efficient method to estimate adversarial vulnerability with respect to the input space as compared to iteratively applying real adversarial attacks. Moreover, when the adversary is unknown a priori, knowledge continuity can also be used in this way as a diagnostic tool. A detailed discussion of these experiments are presented in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "Knowledge continuity can localize vulnerable hidden representations. We plot the relationship between the $k$ -volatility, $\\epsilon_{k}$ , and the relative depth of the model (i.e. $k/n)$ . We find that language models belonging to different model families (encoder, decoder, encoder-decoder) admit different $k$ - volatility trajectories. This is shown in Fig. 2(b). In this way, knowledge continuity may provide a more nuanced picture of a model\u2019s inductive biases and robustness beyond a scalar value like \u201caccuracy under adversarial attack.\u201d We present a detailed analysis of this in Appendix F. Further, these dynamics may act as a diagnostic tool and offer a starting point for designing model-specific robustness interventions or adversarial defenses. For example, when insights from Fig. 2(b) are combined with a knowledge continuity regularization algorithm, this yields superior empirical robustness compared to existing methods. This is shown in the next subsection and in Appendix G. In addition, knowledge continuity can also quantitatively characterize an adversarial attack against a host of models which is useful for online or adaptive defenses [84, 64, 14]. This is shown in in Fig. 2(c), where TextFooler [35] largely exploits the knowledge continuities in middle/final layers of the model to decrease performance. ", "page_idx": 8}, {"type": "table", "img_path": "v07KRLYxDX/tmp/44cfa9dd16fe10e18f377656083b65249f5c0508bfa40bcad3c4fdc2a0426b39.jpg", "table_caption": ["Table 1: Comparison of our knowledge continuity algorithm to existing works across various model families and adversarial attack methods. TF, BA, ANLI denote adversarial attacks [35], [40], and [52], respectively. Regulating knowledge continuity to improve robustness is superior across almost all tasks and attacks. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Regulating knowledge continuity. Motivated by the theoretical results in Section 4, we augment the loss function during training to mitigate knowledge continuity. Specifically, on each training iteration (batch), we start by choosing a hidden layer at random according to a Beta distribution determined $a$ priori: $X\\sim\\operatorname{Beta}(\\alpha,\\beta)$ and let $k=\\lfloor n X\\rfloor$ . Here, $\\alpha,\\beta$ are chosen according to Fig. 2(b,c). We assign larger sampling probability to layers where both $k$ -volatility is high and where knowledge continuity is highly correlated with adversarial robustness. In this way, our regularization objective is both model and attack specific (if the attack method is unknown, then we only apply the former). Then, we devise a Monte-Carlo algorithm to estimate this layer\u2019s $k$ -volatility, $\\epsilon_{k}$ , (see Appendix G) on this minibatch. And so, the augmented loss function becomes $\\mathcal{L}^{\\prime}(f(x),y)=\\ddot{\\mathcal{L}}(f(x),y)+\\lambda\\epsilon_{k}$ with $\\lambda\\geq0$ as a hyperparameter, controlling the regularization strength. In contrast to existing adversarial training methods that perform inner-optimization steps [50, 43, 85], our method requires only additional zeroth-order computations. As a result, it outperforms existing works in training speed (up to $2\\times$ for TextFooler [35] and $3\\times$ for ALUM [43]), while improving robustness. We present a discussion of the results, ablation studies, and training details in Appendix G. ", "page_idx": 9}, {"type": "text", "text": "Certifying robustness with knowledge continuity. We present an algorithm based on Thm. 4.1 to certify robustness during test-time. Similar to [12], we estimate the probability of there existing an adversarial example within some fixed radius (in the representation space, according to a pre-defiend distance metric) through bootstrapping a one-side confidence interval. Applying these methods to our regularization results, we show that regularizing knowledge continuity increases the certified robustness. The certification algorithm, its proof of correctness, and certifications of our regularized models are presented in Appendix H. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a novel definition, knowledge continuity, which addresses some of the key limitations of Lipschitz robustness. We demonstrate that our definition certifies robustness across domain modality, distribution, and norms. We also show that knowledge continuity, in contrast to Lipschitz continuity, does not affect the universal approximation property of neural networks. We also establish conditions under which knowledge continuity and Lipschitz continuity are equivalent. Lastly, we present several practical applications that directly benefit the practitioner. The broader impacts, reproducibility, and limitations of our work can be found in Appendix I, J, K, respectively. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Alan Sun thanks Fengwen Sun for the helpful feedback on early drafts of the work as well as Jeffrey Jiang and Andrew Koulogeorge for thoughtful discussions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Srivastava, and K.-W. Chang. Generating natural language adversarial examples. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2890\u20132896, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. [2] C. Anil, J. Lucas, and R. Grosse. Sorting out lipschitz function approximation. In International Conference on Machine Learning, pages 291\u2013301. PMLR, 2019. [3] S. Arora, Y. Li, Y. Liang, T. Ma, and A. Risteski. A Latent Variable Model Approach to PMI-based Word Embeddings. Transactions of the Association for Computational Linguistics, 4:385\u2013399, 07 2016. [4] P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural networks. Advances in Neural Information Processing Systems, 30, 2017. [5] S. Biderman, U. PRASHANTH, L. Sutawika, H. Schoelkopf, Q. Anthony, S. Purohit, and E. Raff. Emergent and predictable memorization in large language models. Advances in Neural Information Processing Systems, 36, 2023. [6] D. Blackwell. Conditional expectation and unbiased sequential estimation. The Annals of Mathematical Statistics, pages 105\u2013110, 1947. [7] N. Carlini, M. Nasr, C. A. Choquette-Choo, M. Jagielski, I. Gao, P. W. W. Koh, D. Ippolito, F. Tramer, and L. Schmidt. Are aligned neural networks adversarially aligned? In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 61478\u201361500. Curran Associates, Inc., 2023. [8] G. Casella and R. Berger. Statistical inference. CRC Press, 2024. [9] P. Chao, A. Robey, E. Dobriban, H. Hassani, G. J. Pappas, and E. Wong. Jailbreaking black box large language models in twenty queries. In R0-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation Models, 2023.   \n[10] C. Chen, O. Li, D. Tao, A. Barnett, C. Rudin, and J. K. Su. This looks like that: deep learning for interpretable image recognition. Advances in Neural Information Processing Systems, 32, 2019.   \n[11] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pages 854\u2013863. PMLR, 2017.   \n[12] J. Cohen, E. Rosenfeld, and Z. Kolter. Certified adversarial robustness via randomized smoothing. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 1310\u20131320. PMLR, 09\u201315 Jun 2019.   \n[13] Z. Cranko, Z. Shi, X. Zhang, R. Nock, and S. Kornblith. Generalised lipschitz regularisation equals distributional robustness. In International Conference on Machine Learning, pages 2178\u20132188. PMLR, 2021.   \n[14] F. Croce, S. Gowal, T. Brunner, E. Shelhamer, M. Hein, and T. Cemgil. Evaluating the adversarial robustness of adaptive test-time defenses. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 4421\u20134435. PMLR, 17\u201323 Jul 2022.   \n[15] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4):303\u2013314, 1989.   \n[16] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.   \n[17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[18] N. Drenkow, N. Sani, I. Shpitser, and M. Unberath. A systematic review of robustness in deep learning for computer vision: Mind the gap?, 2022.   \n[19] N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby, D. Drain, C. Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022.   \n[20] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and D. Song. Robust physical-world attacks on deep learning visual classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.   \n[21] M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. Pappas. Efficient and accurate estimation of lipschitz constants for deep neural networks. Advances in Neural Information Processing Systems, 32, 2019.   \n[22] F. Gama, J. Bruna, and A. Ribeiro. Stability properties of graph neural networks. IEEE Transactions on Signal Processing, 68:5680\u20135695, 2020.   \n[23] S. Garg and G. Ramakrishnan. BAE: BERT-based adversarial examples for text classification. In B. Webber, T. Cohn, Y. He, and Y. Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6174\u20136181, Online, Nov. 2020. Association for Computational Linguistics.   \n[24] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. Proceedings of 3rd International Conference on Learning Representations, 2014.   \n[25] H. Gouk, E. Frank, B. Pfahringer, and M. J. Cree. Regularisation of neural networks by enforcing lipschitz continuity. Machine Learning, 110:393\u2013416, 2021.   \n[26] G. S. Halford, W. H. Wilson, and S. Phillips. Relational knowledge: the foundation of higher cognition. Trends in Cognitive Sciences, 14(11):497\u2013505, 2010.   \n[27] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.   \n[28] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.   \n[29] M. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[30] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15262\u201315271, June 2021.   \n[31] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359\u2013366, 1989.   \n[32] Y. Huang, H. Zhang, Y. Shi, J. Z. Kolter, and A. Anandkumar. Training certifiably robust neural networks with efficient local lipschitz bounds. Advances in Neural Information Processing Systems, 34:22745\u201322757, 2021.   \n[33] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry. Adversarial examples are not bugs, they are features. Advances in Neural Information Processing Systems, 32, 2019.   \n[34] R. Jia, A. Raghunathan, K. G\u00f6ksel, and P. Liang. Certified robustness to adversarial word substitutions. In K. Inui, J. Jiang, V. Ng, and X. Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4129\u20134142, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.   \n[35] D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits. Is BERT really robust? a strong baseline for natural language attack on text classification and entailment. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8018\u20138025, 2020.   \n[36] H. Kim, G. Papamakarios, and A. Mnih. The lipschitz constant of self-attention. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 5562\u20135571. PMLR, 18\u201324 Jul 2021.   \n[37] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana. Certified robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security and Privacy (SP), pages 656\u2013672. IEEE, 2019.   \n[38] K. Leino, Z. Wang, and M. Fredrikson. Globally-robust neural networks. In International Conference on Machine Learning, pages 6212\u20136222. PMLR, 2021.   \n[39] B. Li, C. Chen, W. Wang, and L. Carin. Certified adversarial robustness with additive noise. Advances in Neural Information Processing Systems, 32, 2019.   \n[40] L. Li, R. Ma, Q. Guo, X. Xue, and X. Qiu. BERT-ATTACK: Adversarial attack against BERT using BERT. In B. Webber, T. Cohn, Y. He, and Y. Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6193\u20136202, Online, Nov. 2020. Association for Computational Linguistics.   \n[41] J. Lin, C. Song, K. He, L. Wang, and J. E. Hopcroft. Nesterov accelerated gradient and scale invariance for adversarial attacks. In International Conference on Learning Representations, 2020.   \n[42] J. Liu, Z. Shen, Y. He, X. Zhang, R. Xu, H. Yu, and P. Cui. Towards out-of-distribution generalization: A survey, 2023.   \n[43] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, and J. Gao. Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994, 2020.   \n[44] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach, 2020.   \n[45] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive power of neural networks: A view from the width. Advances in Neural Information Processing Systems, 30, 2017.   \n[46] B. L\u00fctjens, M. Everett, and J. P. How. Certified adversarial robustness for deep reinforcement learning. In L. P. Kaelbling, D. Kragic, and K. Sugiura, editors, Proceedings of the Conference on Robot Learning, volume 100 of Proceedings of Machine Learning Research, pages 1328\u20131337. PMLR, 30 Oct\u201301 Nov 2020.   \n[47] C. Ma, B. Zhao, C. Chen, and C. Rudin. This Looks Like Those: Illuminating Prototypical Concepts Using Multiple Visualizations. Advances in Neural Information Processing Systems, 36, 2024.   \n[48] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.   \n[49] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013.   \n[50] T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(8):1979\u20131993, 2018.   \n[51] M. Moor, M. Horn, B. Rieck, and K. Borgwardt. Topological autoencoders. In International Conference on Machine Learning, pages 7045\u20137054. PMLR, 2020.   \n[52] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, and D. Kiela. Adversarial NLI: A new benchmark for natural language understanding. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885\u20134901, Online, July 2020. Association for Computational Linguistics.   \n[53] Y. Oren, S. Sagawa, T. B. Hashimoto, and P. Liang. Distributionally robust language modeling. In K. Inui, J. Jiang, V. Ng, and X. Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4227\u20134237, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.   \n[54] P. Pauli, A. Koch, J. Berberich, P. Kohler, and F. Allg\u00f6wer. Training robust neural networks using lipschitz bounds. IEEE Control Systems Letters, 6:121\u2013126, 2021.   \n[55] X. Qi, J. Wang, Y. Chen, Y. Shi, and L. Zhang. Lipsformer: Introducing lipschitz continuity to vision transformers. In The Eleventh International Conference on Learning Representations, 2022.   \n[56] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[57] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training, 2018.   \n[58] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.   \n[59] W. Ruan, X. Huang, and M. Kwiatkowska. Reachability analysis of deep neural networks with provable guarantees. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, IJCAI\u201918, page 2651\u20132659. AAAI Press, 2018.   \n[60] M. Salehi, H. Mirzaei, D. Hendrycks, Y. Li, M. H. Rohban, and M. Sabokrou. A unified survey on anomaly, novelty, open-set, and out of-distribution detection: Solutions and future challenges. Transactions on Machine Learning Research, 2022.   \n[61] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4510\u20134520, 2018.   \n[62] A. Shafahi, W. R. Huang, C. Studer, S. Feizi, and T. Goldstein. Are adversarial examples inevitable? In International Conference on Learning Representations, 2019.   \n[63] A. Shafahi, M. Najibi, M. A. Ghiasi, Z. Xu, J. Dickerson, C. Studer, L. S. Davis, G. Taylor, and T. Goldstein. Adversarial training for free! Advances in Neural Information Processing Systems, 32, 2019.   \n[64] C. Shi, C. Holtz, and G. Mishne. Online adversarial purification based on self-supervised learning. In International Conference on Learning Representations, 2021.   \n[65] M. H. Stone. The generalized weierstrass approximation theorem. Mathematics Magazine, 21(5):237\u2013254, 1948.   \n[66] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks, 2014.   \n[67] N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000.   \n[68] Y. Tsuzuku, I. Sato, and M. Sugiyama. Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[69] M. Usama and D. E. Chang. Towards robust neural networks with lipschitz continuity. In Digital Forensics and Watermarking: 17th International Workshop, IWDW 2018, Jeju Island, Korea, October 22-24, 2018, Proceedings 17, pages 373\u2013389. Springer, 2019.   \n[70] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.   \n[71] A. Virmaux and K. Scaman. Lipschitz regularity of deep neural networks: analysis and efficient estimation. Advances in Neural Information Processing Systems, 31, 2018.   \n[72] E. Wallace, S. Feng, N. Kandpal, M. Gardner, and S. Singh. Universal adversarial triggers for attacking and analyzing NLP. In K. Inui, J. Jiang, V. Ng, and X. Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2153\u20132162, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.   \n[73] B. Wang, S. Wang, Y. Cheng, Z. Gan, R. Jia, B. Li, and J. Liu. Info{bert}: Improving robustness of language models from an information theoretic perspective. In International Conference on Learning Representations, 2021.   \n[74] W. Wang, P. Tang, J. Lou, and L. Xiong. Certified robustness to word substitution attack with differential privacy. In K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y. Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1102\u20131112, Online, June 2021. Association for Computational Linguistics.   \n[75] A. Wei, N. Haghtalab, and J. Steinhardt. Jailbroken: How does llm safety training fail? In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 80079\u201380110. Curran Associates, Inc., 2023.   \n[76] L. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel, D. Boning, and I. Dhillon. Towards fast computation of certified robustness for ReLU networks. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5276\u20135285. PMLR, 10\u201315 Jul 2018.   \n[77] L. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel, D. Boning, and I. Dhillon. Towards fast computation of certified robustness for relu networks. In International Conference on Machine Learning, pages 5276\u20135285. PMLR, 2018.   \n[78] T.-W. Weng, H. Zhang, P.-Y. Chen, J. Yi, D. Su, Y. Gao, C.-J. Hsieh, and L. Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. In International Conference on Learning Representations, 2018.   \n[79] E. Wong and Z. Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pages 5286\u20135295. PMLR, 2018.   \n[80] E. Wong, L. Rice, and J. Z. Kolter. Fast is better than free: Revisiting adversarial training. In International Conference on Learning Representations, 2020.   \n[81] E. Wong, F. Schmidt, J. H. Metzen, and J. Z. Kolter. Scaling provable adversarial defenses. Advances in Neural Information Processing Systems, 31, 2018.   \n[82] X. Xu, L. Li, Y. Cheng, S. Mukherjee, A. H. Awadallah, and B. Li. Certifiably robust transformers with 1-lipschitz self-attention, 2023.   \n[83] J. Yang, K. Zhou, Y. Li, and Z. Liu. Generalized out-of-distribution detection: A survey. International Journal of Computer Vision, pages 1\u201328, 2024.   \n[84] C. Yao, P. Bielik, P. Tsankov, and M. Vechev. Automated discovery of adaptive attacks on adversarial defenses. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 26858\u201326870. Curran Associates, Inc., 2021.   \n[85] J. Y. Yoo and Y. Qi. Towards improving adversarial training of NLP models. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 945\u2013956, Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics.   \n[86] Y. Yoshida and T. Miyato. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017.   \n[87] A. Zhang, A. Chan, Y. Tay, J. Fu, S. Wang, S. Zhang, H. Shao, S. Yao, and R. K.-W. Lee. On orthogonality constraints for transformers. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 375\u2013382, Online, Aug. 2021. Association for Computational Linguistics.   \n[88] B. Zhang, D. Jiang, D. He, and L. Wang. Boosting the certified robustness of l-infinity distance nets. In International Conference on Learning Representations, 2022.   \n[89] B. Zhang, D. Jiang, D. He, and L. Wang. Rethinking lipschitz neural networks and certified robustness: A boolean function perspective. Advances in Neural Information Processing Systems, 35:19398\u201319413, 2022.   \n[90] H. Zhang, H. Chen, C. Xiao, S. Gowal, R. Stanforth, B. Li, D. Boning, and C.-J. Hsieh. Towards stable and efficient training of verifiably robust neural networks. In International Conference on Learning Representations, 2020.   \n[91] A. Zou, Z. Wang, N. Carlini, M. Nasr, J. Z. Kolter, and M. Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1 Introduction 1   \n2 Related Works 2   \n3 Preliminaries 3   \n4 Knowledge Continuity 3   \n4.1 Defining Perceived Knowledge 4   \n4.2 Defining Knowledge Continuity 4   \n4.3 Certification of Robustness 6   \n4.4 Expressiveness 7   \n4.5 Connections to Lipschitz Continuity 8   \n5 Practical Applications 9   \n6 Conclusion 10   \n7 Acknowledgements 11   \nA More on Metric Decompositions 18   \nA.1 Metric Decompositions of Common Neural Architectures . 18   \nA.2 Beyond Neural Networks: Inducing Metric Decompositions 19   \nB Proof of Robustness 20   \nB.1 Technical Lemmas 21   \nC Proof of Expressiveness 22   \nC.1 Technical Lemmas 23   \nD Proof of Equivalence Between Lipschitz Continuity and Knowledge Continuity 23   \nE Predicting Adversarial Robustness with Volatility 25   \nF Localizing Volatile Hidden Representations 26   \nF.1 Layerwise Volatility . 26   \nF.2 Model-Specific Volatility . 27   \nG Regularizing Knowledge Continuity 27   \nG.1 Estimating Knowledge Continuity Algorithmically 27   \nG.2 Theoretical Guarantees of $k$ -Volatility Estimation 28   \nG.3 Computer Vision Results . 29   \nG.4 Ablation Studies . 29   \nG.5 Training Details . 31   \nH Certifying Robustness at Test-Time 32   \nI Broader Impacts 33   \nJ Reproducibility 33   \nK Limitations 33   \nL NeurIPS Paper Checklist 34 ", "page_idx": 16}, {"type": "text", "text": "A More on Metric Decompositions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Section 4.1, we introduced the notion of a metric decomposition to rigorously define the hidden representations of a neural network. Herein, we show that our notion of a metric decomposition well-describes a host of neural architectures and also point to possible applications of this concept beyond just deep learning. Let us first consider possible metric decompositions of common neural architectures. ", "page_idx": 17}, {"type": "text", "text": "A.1 Metric Decompositions of Common Neural Architectures ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Fully-Connected Neural Network. Suppose that $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{m}$ is a fully-connected neural network with $n$ hidden layers. Each hidden layer indexed by $i\\in[n]$ has a weight matrix $W_{i}\\in\\mathbb{R}^{d_{i+1}\\times d_{i}}$ , bias $b_{i}\\in\\mathbb{R}^{d_{i+1}}$ , and activation function $\\sigma_{i}:\\mathbb{R}^{d_{i+1}}\\rightarrow\\mathbb{R}^{d_{i+1}}$ , where $d_{i}\\in\\mathbb{N},d_{1}=d,d_{n}=m$ . Define the hidden layers as ", "page_idx": 17}, {"type": "equation", "text": "$$\nh_{k}(x)=\\sigma_{k}(W_{k}x+b_{k}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $k\\in[n]$ . Clearly, $f=h_{n}\\circ h_{n-1}\\circ\\ldots\\circ h_{1}$ . And our intermediate spaces are simply $\\{\\mathbb{R}^{d_{i}}\\}_{i=1}^{n}$ . It remains to define a metric on these hidden spaces. There are many ways of doing this. For example, \u2022 For any $1\\leq p\\leq\\infty$ , endow each intermediate space with the $\\ell_{p}$ -norm. \u2022 Define $d(x,y)=1-\\cos(\\theta_{x,y})$ where $\\theta_{x,y}$ is the angle between $x,y$ . Then, if we choose $\\sigma_{i}$ to restrict the image of $h_{i}$ to be the unit sphere, we may endow each intermediate space with this cosine distance. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Note here that there are two steps here: we first identify what the intermediate spaces are, then assign metrics to them. The process of identfying these intermediate spaces may be independent of the metrics we end of assigning them. ", "page_idx": 17}, {"type": "text", "text": "Convolutional Neural Network. For simplicity, we only consider the case of a single 2d-convolution layer, a convolutional network with higher dimensions or more layers can be derived inductively. Let $f:\\mathbb{R}^{c\\times h\\times w}\\rightarrow\\mathbb{R}^{c^{\\prime}\\times h^{\\prime}\\times w^{\\prime}}$ . Suppose that this layer is parameterized by kernels $W_{i}\\in\\mathbb{R}^{k\\times k}$ for $i\\in[c^{\\prime}]$ and some $k\\in\\mathbb N$ as well as a bias $b\\in\\mathbb{R}^{c^{\\prime}}$ . Then, it follows that ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(x)_{j}=\\left(1_{h^{\\prime}\\times w^{\\prime}}b_{j}+\\sum_{i=1}^{c}W_{j}\\ast x[i,:,:]\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $j\\in[c^{\\prime}]$ where $f(x)_{j}\\in\\mathbb{R}^{h^{\\prime}\\times w^{\\prime}}$ for $h^{\\prime},w^{\\prime}$ being the resulting dimension after convolution with a $k\\times k$ kernel. Here, $\\mathbf{1}_{h^{\\prime}\\times w^{\\prime}}\\in\\mathbb{R}^{h^{\\prime}\\times w^{\\prime}}$ is a one matrix. To induce a distance metric on this output space, we can simply define a matrix norm on each of the output channels and sum them. Let $\\{\\Vert\\cdot\\Vert_{i}\\}_{i=1}^{c^{\\prime}}$ be a collection of matrix norms. Then, we define ", "page_idx": 17}, {"type": "equation", "text": "$$\nd(f(x),f(x^{\\prime}))=\\sum_{i=1}^{c^{\\prime}}\\left\\|f(x)_{i}-f(x^{\\prime})_{i}\\right\\|_{i}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It is easy to verify that this is a metric. Thus, the availability of a metric decomposition is not affected by parameter sharing. ", "page_idx": 17}, {"type": "text", "text": "Instead of incorporating every individual channel into our metric, we may also consider applying a pooling operation before passing the result through a single matrix norm, $\\lVert\\cdot\\rVert$ . For example, ", "page_idx": 17}, {"type": "equation", "text": "$$\nd(f(x),f(x^{\\prime}))={\\frac{1}{c^{\\prime}}}\\left\\|\\sum_{i=1}^{c^{\\prime}}f(x)_{i}-\\sum_{i=1}^{c^{\\prime}}f(x^{\\prime})_{i}\\right\\|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This, however, is no longer a metric, as definiteness is not preserved. That is, there exists $f(x)\\neq f(x^{\\prime})$ where $d(f(x),f(x^{\\prime}))=0$ . This issue can be easily resolved by having $d(\\cdot,\\cdot)$ operate on a quotient space with respect to the equivalence relation $f(x)\\sim f(x^{\\prime})$ if and only if $\\begin{array}{r}{\\sum_{i=1}^{c^{\\prime}}f(x)_{i}=\\sum_{i=1}^{c^{\\prime}}f(x^{\\prime})_{i}}\\end{array}$ . This technique is further explored in the next subsection. ", "page_idx": 17}, {"type": "text", "text": "Residual Connections. We present two distinct metric decompositions of a residual network. Consider two fully-connected layers with one residual connection. This is visualized below. ", "page_idx": 17}, {"type": "image", "img_path": "v07KRLYxDX/tmp/2668baa6f0ed8cb09659f4ad49a38638c48402085e27df71b3eebe0f4bc66063.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Let us assume that $A:\\mathbb{R}^{d_{1}}\\rightarrow\\mathbb{R}^{d_{2}}$ and $\\boldsymbol{B}:\\mathbb{R}^{d_{2}}\\rightarrow\\mathbb{R}^{d_{1}}$ . Here, the input $x$ feeds back into the output layer $B$ creating a residual block (the set of layers between the input and the residual connection). ", "page_idx": 18}, {"type": "text", "text": "Trivially, we can aggregate the entire residual block as one metric decomposition. That is, let $h(x)=B(A(x))+x$ be our metric decomposition. Then, define a metric on the image of $h,\\mathbb{R}^{d_{1}}$ , analogous to the hidden layers of a fully-connected neural network. This is the approach we use throughout our practical applications section (Section 5), and it is the standard way to counter layers in computer vision [27] and natural langauge processing [16]. ", "page_idx": 18}, {"type": "text", "text": "To operate at a finer lever of granularity, we can also represent each layer within the residual block as a part of a metric decomposition. Let us redefine the residual block such that at every layer, we keep track of the input. The computational graph for this is shown below. ", "page_idx": 18}, {"type": "image", "img_path": "v07KRLYxDX/tmp/dc575ba9827a3deadb63f3594b066f300c71850ce144c8f74f8635732e339695.jpg", "img_caption": ["Define $A^{\\prime}:x\\mapsto(A(x),x),B^{\\prime}:(A(x),x)\\mapsto(B(A(x)),x)$ ) and $x^{\\prime}:(B(A(x)),x)\\mapsto(B(A(x))\\!+\\!x,x)$ . Then, it follows that $x\\rightarrow A^{\\prime}\\rightarrow B^{\\prime}\\rightarrow x^{\\prime}$ forms a metric decomposition. Here, the metric in each layer is with respect to the quotient space where $(a,a^{\\prime})\\sim(b,b^{\\prime})$ if and only if $a=b$ . Therefore, we also recover the same vector space structure. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Transformers. By chaining our metric decompositions for the residual blocks with our metric decompositions for the fully-connected networks we can easily create a metric decomposition for any transformer. Throughout the paper, we use two distinct methods to generate representations of its hidden layers: ", "page_idx": 18}, {"type": "text", "text": "\u2022 After each attention block which consists of multiheaded attention and multilayered perceptrons, we retrieve the last token.   \n\u2022 We average all of the tokens together. ", "page_idx": 18}, {"type": "text", "text": "In both of these methods, we are significantly reducing the dimension of the hidden layer. Thus, to formalize these metrics, we need to quotient out points that break the definiteness of our metric, as we have done before with the residual block. ", "page_idx": 18}, {"type": "text", "text": "A.2 Beyond Neural Networks: Inducing Metric Decompositions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We have shown that our notion of a metric decomposition can well-describe many deep learning architectures, but what about models that are not neural networks (like a decision tree)? Herein, we demonstrate that we can induce metric decompositions even when the model itself does not have explicit hidden layers. ", "page_idx": 18}, {"type": "text", "text": "Let us now consider an arbitrary function $f:\\mathcal{X}\\to\\mathcal{Y}$ . We can induce a metric decomposition on $f$ through an auxiliary function $f^{\\prime}:\\mathcal{X}\\rightarrow\\mathcal{X}$ , for a metric-decomposable $f^{\\prime}$ . If $f^{\\prime}={\\mathrm{Id}}_{\\chi}$ , then, $f=f\\circ f^{\\prime}$ and the metric decomposition of $f$ would be exactly the metric decomposition of $f^{\\prime}$ . This is visualized below. ", "page_idx": 18}, {"type": "image", "img_path": "v07KRLYxDX/tmp/ad19d2b4df0ffe23b3e2225f23a42e9d538b0ea2535a5929f020a3c86fe7c75e.jpg", "img_caption": ["Metric Decomposition of $f^{\\prime}$ "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Essentially, we have created an autoencoder for $\\scriptstyle{\\mathcal{X}}$ . This is common in many applications where a neural network or some other method is used as a feature extractor. In this way, we can simply define our metric with respect to these extracted features. However, this requires that either the autoencoder to be exact or that our function $f$ is invariant under representations that collide. Thus, this would allow models such as decision trees to also be metric decomposed. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "B Proof of Robustness ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem (See Thm. 4.1). Let $A\\subset{\\mathcal{X}}\\times{\\mathcal{Y}}$ such that $\\mathbb{P}_{D_{\\mathcal{X},\\mathcal{Y}}}[A]>0$ and $\\delta,\\eta>0$ . Let $A^{\\prime}=\\{(x^{\\prime},y^{\\prime})\\in$ $\\begin{array}{r}{\\mathcal{X}\\times\\mathcal{Y}:\\mathbb{E}_{(x,y)\\sim D_{\\mathcal{X},\\mathcal{Y}}}\\Delta\\mathcal{L}_{f}^{(x,y)}(x^{\\prime},y^{\\prime})>\\eta\\}}\\end{array}$ . If $f:\\mathcal{X}\\to\\mathcal{Y}$ is $\\epsilon$ -knowledge continuous with respect to the hidden layer indexed by $k$ and $(\\mathcal{Z}_{k},d_{k})$ is bounded by $B>0$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(x,y)\\sim D_{\\mathcal{X},\\mathcal{Y}}}[A^{\\prime}\\mid d_{k}(f^{k}(x),f^{k}(A))<\\delta]\\leq\\frac{\\epsilon\\delta}{\\eta\\left(1-\\exp\\left[-\\Omega\\left(\\frac{\\delta}{B}-\\sqrt{\\log\\frac{1}{\\mathbb{P}[A]}}\\right)^{2}\\right]\\right)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $f^{k}(A)=\\{f^{k}(a):a\\in A\\}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(x,y)\\sim{D_{\\mathcal{X},\\mathcal{Y}}}}\\left[A^{\\prime}\\mid d_{k}(f^{k}(x),f^{k}(A))<\\delta\\right]=\\frac{\\mathbb{P}_{(x,y)\\sim{D_{\\mathcal{X},\\mathcal{Y}}}}\\left[A^{\\prime}\\cap d_{k}(f^{k}(x),f^{k}(A))<\\delta\\right]}{\\mathbb{P}_{(x,y)\\sim{D_{\\mathcal{X},\\mathcal{Y}}}}[d_{k}(f^{k}(x),f^{k}(A))<\\delta]}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We bound the numerator and denominator of Eq. B.2 separately. The denominator is given by Cor. B.3. We upper-bound the numerator using Markov\u2019s inequality. Firstly, we find the expectation of $\\mathcal{L}_{f}^{(x,y)}(x^{\\prime},y^{\\prime})$ over $A^{\\prime}\\cap d_{k}(f^{k}(x),f^{k}(A))<\\delta$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{\\mathcal{X},\\mathcal{Y}}}\\sigma_{f}^{k}(x,y)=\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{\\mathcal{X},\\mathcal{Y}}}\\left(\\mathbb{E}_{(x^{\\prime},y^{\\prime})\\sim D_{\\mathcal{X},\\mathcal{Y}}}\\left[\\frac{\\Delta\\mathcal{L}_{f}^{(x,y)}(x^{\\prime},y^{\\prime})}{d_{k}(f^{k}(x),f^{k}(x^{\\prime}))}\\right]\\right),}}\\\\ &{}&{=\\mathbb{E}_{(x,y),(x^{\\prime},y^{\\prime})\\sim(D_{\\mathcal{X},\\mathcal{Y}}\\times D_{\\mathcal{X},\\mathcal{Y}})}\\left[\\frac{\\Delta\\mathcal{L}_{f}^{(x,y)}(x^{\\prime},y^{\\prime})}{d_{k}(f^{k}(x),f^{k}(x^{\\prime}))}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The previous inequality follows from Fubini\u2019s theorem, then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{X,y}}\\sigma_{f}^{k}(x,y)\\geq\\mathbb{E}_{(x,y),(x^{\\prime},y^{\\prime})\\sim(D_{X,y}\\times D_{X,y})}\\Bigg[\\frac{\\Delta C_{f}^{(x,y)}(x^{\\prime},y^{\\prime})}{d_{k}(f^{k}(x),f^{k}(x^{\\prime}))}\\Bigg],}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad a_{k}(f^{k}(x),f^{k}(a))<\\delta}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{\\delta}\\mathbb{E}_{(x,y),(x^{\\prime},y^{\\prime})\\sim(D_{X,y}\\times D_{X,y})}\\left[\\Delta C_{f}^{(x,y)}(x^{\\prime},y^{\\prime})\\right],}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad a_{k}(f^{k}(x),f^{k}(A))<\\delta}\\\\ &{\\delta\\operatorname{E}_{(x,y)\\sim\\mathcal{D}_{X,y}}\\sigma_{f}^{k}(x,y)\\geq\\mathbb{E}_{(x,y),(x^{\\prime},y^{\\prime})\\sim(D_{X,y}\\times D_{X,y})}\\left[\\Delta C_{f}^{(x,y)}(x^{\\prime},y^{\\prime})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "And by $\\epsilon$ -knowledge continuity, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\delta\\epsilon\\geq\\mathbb{E}_{(x,y),(x^{\\prime},y^{\\prime})\\sim(D_{\\mathcal{X},\\mathcal{Y}}\\times D_{\\mathcal{X},\\mathcal{Y}})}\\left[\\Delta\\mathcal{L}_{f}^{(x,y)}(x^{\\prime},y^{\\prime})\\right].}}\\\\ {{(x^{\\prime},y^{\\prime})\\epsilon A}}\\\\ {{d_{k}(f^{k}(x),f^{k}(A)){<}\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This gives us an upper-bound of expectation of $\\Delta\\mathcal{L}_{f}^{(x,y)}(x^{\\prime},y^{\\prime})$ over the set of all points that are within $\\delta$ -radius from $f^{k}(A)$ . Since $\\Delta\\mathcal{L}_{f}^{(x,y)}(x^{\\prime},y^{\\prime})\\ge0$ everywhere, by Markov\u2019s inequality, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(x,y)\\sim\\mathcal{D}_{\\mathcal{X},\\mathcal{Y}}}[A^{\\prime}\\cap d_{k}(f^{k}(x),f^{k}(A))<\\delta]\\leq\\frac{\\delta\\,\\mathbb{E}\\,\\sigma_{f}^{k}(x,y)}{\\eta},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\leq\\frac{\\delta\\epsilon}{\\eta}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The last inequality follows from $\\mathbb{E}_{(x,y)\\sim D_{\\boldsymbol{X},y}}\\,\\sigma_{f}^{k}(x,y)<\\epsilon$ , by the definition of $\\epsilon$ -knowledge continuity. Now, by applying the complement of Lem. B.2, we lower-bound the denominator and yield the following ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(x^{\\prime},y^{\\prime})\\sim{\\cal D}}\\left[A^{\\prime}\\mid d_{k}(f^{k}(x),f^{k}(x^{\\prime}))<\\delta\\right]\\leq\\frac{\\epsilon\\delta}{\\eta\\left(1-\\exp\\left(-\\frac{2}{B^{2}}\\left(\\delta-B\\sqrt{\\frac{1}{2}\\log\\frac{2}{\\mathbb{P}[A]}}\\right)^{2}\\right)\\right)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The proof is concluded by applying $\\Omega(\\cdot)$ notation to the denominator. ", "page_idx": 20}, {"type": "text", "text": "B.1 Technical Lemmas ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Definition 6. A function $f:\\mathcal{X}_{1}\\times\\ldots\\times\\mathcal{X}_{n}\\to\\mathbb{R}$ has bounded variation if there are $c_{1},\\ldots,c_{n}\\in\\mathbb{R}$ such that for all $1\\leq i\\leq n$ and $x_{1}\\in{\\mathcal{X}}_{1},\\ldots,x_{n}\\in{\\mathcal{X}}_{n},$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x_{i}^{\\prime}\\in{\\mathcal{X}}_{i}}|f(x_{1},\\dots,x_{i},\\dots,x_{n})-f(x_{1},\\dots,x_{i}^{\\prime},\\dots,x_{n})|\\leq c_{i}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma B.1 (McDiarmid\u2019s Inequality). Assume that the function $f:\\mathcal{X}_{1}\\times\\ldots\\times\\mathcal{X}_{n}\\to\\mathbb{R}$ satisfy the bounded differences property with bounds $c_{1},\\ldots,c_{n}$ . Consider the independent random variables $X_{1},\\ldots,X_{n}$ where $X_{i}\\in\\mathcal{X}_{i}$ for all $1\\leq i\\leq n$ . Then, for any $\\epsilon>0$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}[|f(X_{1},\\ldots,X_{n})-\\mathbb{E}[f(X_{1},\\ldots,X_{n})|\\geq\\epsilon]\\leq2\\exp\\left(-\\frac{2\\epsilon^{2}}{\\sum_{i=1}^{n}c_{i}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma B.2. Suppose that $(\\mathcal{X},d)$ is a bounded metric space such that $\\operatorname*{sup}_{x,x^{\\prime}\\in{\\mathcal{X}}}d(x,x^{\\prime})<B$ for some $B>0$ . Let $A\\subset X$ such that $\\mathbb{P}[A]>0$ and $\\epsilon>0$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}[d(x,A)\\ge\\epsilon]\\le\\exp\\left(-\\frac{2}{B^{2}}\\left(\\epsilon-B\\sqrt{\\frac{1}{2}\\log\\frac{2}{\\mathbb{P}[A]}}\\right)^{2}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. For brevity, denote $f_{A}(x)=d(A,x)=\\operatorname*{inf}_{a\\in A}d(x,a)$ . Since $(\\mathcal{X},d)$ is a bounded metric space, by Lem. B.1, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathbb{P}[|f_{A}(x)-\\mathbb{E}f_{A}(x)|\\ge\\epsilon]=2\\exp\\left(-\\displaystyle\\frac{2\\epsilon^{2}}{B^{2}}\\right),}}\\\\ {{\\mathbb{P}[f_{A}(x)-\\mathbb{E}f_{A}(x)\\ge\\epsilon]+\\mathbb{P}[f_{A}(x)-\\mathbb{E}f_{A}(x)\\le-\\epsilon]\\le2\\exp\\left(-\\displaystyle\\frac{2\\epsilon^{2}}{B^{2}}\\right),}}\\\\ {{\\mathbb{P}[f_{A}(x)-\\mathbb{E}f_{A}(x)\\le-\\epsilon]\\le2\\exp\\left(-\\displaystyle\\frac{2\\epsilon^{2}}{B^{2}}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\epsilon=\\mathbb{E}f_{A}(x)$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}[f_{A}(x)\\leq0]\\leq2\\exp\\left(-\\frac{2(\\mathbb{E}f_{A}(x))^{2}}{B^{2}}\\right),}\\\\ &{\\qquad\\mathbb{P}[A]\\leq2\\exp\\left(-\\frac{2(\\mathbb{E}f_{A}(x))^{2}}{B^{2}}\\right),}\\\\ &{\\qquad\\mathbb{E}f_{A}(x)\\leq\\sqrt{\\frac{B^{2}}{2}\\log\\left(\\frac{2}{\\mathbb{P}[A]}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The second inequality follows from $\\mathbb{P}[f_{A}(x)\\leq0]=\\mathbb{P}[f_{A}(x)=0]\\geq\\mathbb{P}[A]$ . By Eq. B.15, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}[f_{A}(x)-\\mathbb{E}f_{A}(x)\\geq\\epsilon]+\\mathbb{P}[f_{A}(x)-\\mathbb{E}f_{A}(x)\\leq-\\epsilon]\\leq2\\exp\\left(-\\frac{2\\epsilon^{2}}{B^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}[f_{A}(x)-\\mathbb{E}f_{A}(x)\\geq\\epsilon]\\leq2\\exp\\left(-\\frac{2\\epsilon^{2}}{B^{2}}\\right),}\\\\ {\\mathbb{P}[f_{A}(x)\\geq\\epsilon+\\mathbb{E}f_{A}(x)]\\leq2\\exp\\left(-\\frac{2\\epsilon^{2}}{B^{2}}\\right),}\\\\ {\\mathbb{P}\\left[f_{A}(x)\\geq\\epsilon+\\sqrt{\\frac{B^{2}}{2}\\log\\left(\\frac{2}{\\mathbb{P}[A]}\\right)}\\right]\\leq2\\exp\\left(-\\frac{2\\epsilon^{2}}{B^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $\\delta>0$ , let $\\begin{array}{r}{\\epsilon=\\delta-\\sqrt{\\frac{B^{2}}{2}\\log\\frac{2}{\\mathbb P[A]}}}\\end{array}$ And so, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}[f_{A}(x)\\ge\\delta]\\le2\\exp\\left(-\\frac{2}{B^{2}}\\left(\\delta-B\\sqrt{\\frac{1}{2}\\log\\left(\\frac{2}{\\mathbb{P}[A]}\\right)}\\right)^{2}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is the desired expression. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}[f_{A}(x)<\\delta]\\geq1-2\\exp{\\left(-\\frac{2}{B^{2}}\\left(\\delta-B\\sqrt{\\frac{1}{2}\\log{\\left(\\frac{2}{\\mathbb{P}[A]}\\right)}}\\right)^{2}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C Proof of Expressiveness ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proposition (See Prop. 4.4). Suppose $(\\mathcal{X},d_{\\mathcal{X}}),(\\mathcal{Y},d_{\\mathcal{Y}})\\;:=\\;(\\mathcal{X},d_{\\mathcal{X}})$ are compact metric spaces, $F\\subset\\mathcal{D}^{\\mathcal{X}}$ is the set of all continuous functions from $\\scriptstyle{\\mathcal{X}}$ to $\\boldsymbol{\\mathfrak{V}}$ such that $\\int d_{\\mathcal{X}}(x,x^{\\prime})^{-1}d\\mu_{f}<\\infty$ and $\\mathcal{L}$ be Lipschitz continuous in both coordinates. Then, there exists a universal function approximator $\\nu$ of $\\mathcal{F}$ that is knowledge continuous (i.e. \ud835\udd3c $\\sigma_{f}^{k}(x,y)<\\infty$ for some $k$ ). ", "page_idx": 21}, {"type": "text", "text": "Proof. By Lem. C.3, the set of Lipschitz continuous functions $\\mathcal{L}$ is dense in the set of all continuous functions $\\mathcal{C}$ with respect to the uniform metric. By Lem. C.1, since $|{\\mathcal{L}}(x,y)|\\leq K d(x,y)$ , if $\\operatorname*{sup}_{x\\in\\mathcal{X}}d(f(x),g(x))<\\epsilon$ , then for any probability measure $\\mathbb{P}$ over $\\scriptstyle{\\mathcal{X}}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int{\\mathcal{L}}(f(x),g(x))\\,d\\mathbb{P}\\leq\\int|{\\mathcal{L}}(f(x),g(x))|\\,d\\mathbb{P}\\leq K\\epsilon,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $K$ is the Lipschitz constant of $\\mathcal{L}$ . This implies that for any sequence $\\epsilon_{1},\\epsilon_{2},\\ldots$ we can choose Lipschitz continuous functions $f_{1},f_{2},\\ldots$ with Lipschitz constants $C_{1},C_{2},...$ such that $\\begin{array}{r}{\\int\\mathcal{L}(f_{n}(x)\\dot{,}y)\\;d\\mu_{f}\\;<\\;\\epsilon_{n}}\\end{array}$ . It remains to show that each of these functions are in fact knowledge continuous. Since $\\scriptstyle{\\mathcal{X}}$ is a metric space, we consider the trivial metric decomposition of our sequence of functions (see Remark 1). Specifically, we denote $h_{1}=\\mathrm{Id}_{\\chi}$ and proceed to bound \ud835\udd3c $\\sigma_{f}^{1}(x,y)$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\sigma_{f_{n}}^{1}(x,y)=\\displaystyle\\iint\\frac{\\Delta C_{f_{n}}^{(x,y)}(x^{\\prime},y^{\\prime})}{d x}\\,(d\\mu_{f}\\times d\\mu_{f}),}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\iint\\frac{\\frac{|C(f_{n}(x,x^{\\prime}))-C(f_{n}(x^{\\prime}),y)+C(f_{n}(x^{\\prime}),y)-C(f_{n}(x^{\\prime}),y^{\\prime})|}{d x(x,x^{\\prime})}\\,(d\\mu_{f}\\times d\\mu_{f}),}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\iint\\frac{|C(f_{n}(x),y)-C(f_{n}(x^{\\prime}),y)|}{d x(x,x^{\\prime})}\\,d(\\mu_{f}\\times\\mu_{f})}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\iint\\frac{|C(f_{n}(x^{\\prime}),y)-C(f_{n}(x^{\\prime}),y^{\\prime})|}{d(x,x^{\\prime})}\\,(d\\mu_{f}\\times d\\mu_{f}),}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\iint\\frac{K d_{X}(f(x),f(x^{\\prime}))}{d_{X}(x,x^{\\prime})}\\,d(\\mu_{f}\\times\\mu_{f})+\\displaystyle\\iint\\frac{K d_{X}(y,y^{\\prime})}{d_{X}(x,x^{\\prime})}\\,d(\\mu_{f}\\times\\mu_{f}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Lem. C.4, any compact metric space is bounded. So, let $(\\mathcal{X},d)$ be bounded by $b>0$ . It follows that $d_{\\mathcal{X}}(y,y^{\\prime})\\leq b$ and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\leq\\iint K C_{n}\\;d(\\mu_{f}\\times\\mu_{f})+K b\\int{\\frac{1}{d_{\\mathcal{X}}(x,x^{\\prime})}}\\;d\\mu_{f},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n=K C_{n}+K b\\int d_{\\mathcal{X}}(x,x^{\\prime})^{-1}d\\mu_{f},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By assumption $\\int d_{\\mathcal{X}}(x,x^{\\prime})^{-1}d\\mu_{f}<\\infty$ and the statement of the proposition follows. ", "page_idx": 22}, {"type": "text", "text": "C.1 Technical Lemmas ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma C.1. If $\\b{\\mathcal{L}}(\\cdot,\\cdot)$ is Lipschitz continuous in both coordinates, then for any $x,x^{\\prime}\\in\\mathcal{X}$ , $|{\\mathcal{L}}(x,x^{\\prime})|\\leq$ $K d({\\boldsymbol{x}},{\\boldsymbol{x}}^{\\prime}),$ , where $K$ is the Lipschitz constant of $\\mathcal{L}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. By Lipschitz continuity, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{L}(x,x^{\\prime})-\\mathcal{L}(x,x)|\\leq K d(x,x^{\\prime}),}\\\\ {|\\mathcal{L}(x,x^{\\prime})|\\leq K d(x,x^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma C.2. The set of all Lipschitz continuous functions from ${\\mathcal{X}}\\rightarrow{\\mathcal{X}}$ separates all points in $\\scriptstyle{\\mathcal{X}}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. The identity function is 1-Lipschitz continuous and it also separates all points in $\\scriptstyle{\\mathcal{X}}$ . ", "page_idx": 22}, {"type": "text", "text": "Corollary C.3. Let $\\mathcal{C}\\subset\\mathcal{X}^{\\mathcal{X}}$ be the set of all continuous functions from ${\\mathcal{X}}\\rightarrow{\\mathcal{X}}$ and $\\mathcal{L}\\subset\\mathcal{X}^{\\mathcal{X}}$ be the set of all Lipschitz continuous functions from ${\\mathcal{X}}\\rightarrow{\\mathcal{X}}$ . If $\\scriptstyle{\\mathcal{X}}$ is compact, then $\\mathcal{L}$ is dense in $\\mathcal{C}$ with respect to the uniform metric: $d^{\\prime}(f,g)=\\operatorname*{sup}_{x\\in\\mathcal{X}}d(f(x),g(x)).$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. This follows directly from Lem. C.2 and the Stone-Weierstrass theorem [65]. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.4. Any compact metric space $(\\mathcal{X},d)$ is also bounded. ", "page_idx": 22}, {"type": "text", "text": "Proof. By way of contraposition suppose that $(\\mathcal{X},d)$ is not bounded. Then, $\\operatorname*{sup}_{x,x^{\\prime}\\in{\\mathcal{X}}}d(x,x^{\\prime})=\\infty$ . Pick $x_{1}\\in\\mathcal{X}$ arbitrarily and pick $x_{n}$ for $n\\in\\mathbb{Z}^{+},n>1$ such that $d(x_{n},x_{1})>n$ . Clearly, there does not exist a convergent subsequence of the sequence $x_{1},x_{2},\\ldots$ Thus, $(\\mathcal{X},d)$ cannot be compact. \u25a0 ", "page_idx": 22}, {"type": "text", "text": "D Proof of Equivalence Between Lipschitz Continuity and Knowledge Continuity ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proposition. (See Prop. 4.6) Suppose that $(\\mathcal{X},d_{\\mathcal{X}}),(\\mathcal{Y},d_{\\mathcal{Y}})$ are metric spaces. Let the first \ud835\udc5bmetric decompositions of $f:\\mathcal{X}\\to\\mathcal{Y}$ be $K_{i}$ -Lipschitz continuous, for $i\\in[n]$ . If $f$ is $\\epsilon$ -knowledge continuous with respect to the $n^{t h}$ hidden layer and $d_{\\mathcal{Y}}(f(x),f(x^{\\prime}))\\leq\\eta\\Delta\\mathcal{L}_{f}^{(x,y)}(x^{\\prime},y)$ for all $x,x^{\\prime}\\in\\mathcal{X}$ , $y\\in\\mathcal{V}$ and some $\\eta>0$ , then $f$ is Lipschitz continuous in expectation. That is, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(x,y),(x^{\\prime},y^{\\prime})\\sim D_{\\boldsymbol{X},\\boldsymbol{y}}}\\,\\frac{d_{\\boldsymbol{y}}(\\boldsymbol{f}(\\boldsymbol{x}),\\boldsymbol{f}(\\boldsymbol{x}^{\\prime}))}{d_{\\boldsymbol{\\mathcal{X}}}(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})}\\leq\\epsilon\\eta\\prod_{j=1}^{n}K_{j}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We proceed to bound the knowledge continuity of $f$ from below. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\,\\sigma_{f}^{k}(x,y)\\geq\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{X,y}}\\,\\mathbb{E}_{(x^{\\prime},y^{\\prime})\\sim\\mathcal{D}_{X,y}}\\,\\frac{\\Delta\\mathcal{E}_{f}^{(x,y)}(x^{\\prime},y)}{\\mu_{k}(f^{k}(x),f^{k}(x^{\\prime}))},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\Delta\\mathcal{E}_{f}^{(x,y)}(x^{\\prime},y)}\\\\ &{\\qquad\\qquad\\geq\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{X,y}}\\,\\mathbb{E}_{(x,y^{\\prime})\\sim\\mathcal{D}}\\,\\frac{\\Delta\\mathcal{E}_{f}^{(x,y)}(x^{\\prime},y)}{\\prod_{j=1}^{m}K_{j}d_{X}(x^{\\prime},x)},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\frac{1}{2}d_{y}(f(x),f(x^{\\prime}))}\\\\ &{\\qquad\\qquad\\geq\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{X,y}}\\,\\mathbb{E}_{(x,y^{\\prime})\\sim\\mathcal{D}}\\,\\frac{\\overline{{1}}\\,d_{y}\\left(f(x),f(x^{\\prime})\\right)}{\\prod_{j=1}^{m}K_{j}d_{X}(x,x^{\\prime})},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\frac{1}{\\eta}d_{y}(f(x),f^{k}(x^{\\prime}))}\\\\ &{\\qquad=\\mathbb{E}_{(x,y),(x^{\\prime},y^{\\prime})\\sim\\mathcal{D}_{X,y}}\\,\\frac{\\overline{{1}}\\,d_{y}\\left(f(x),f(x^{\\prime})\\right)}{\\prod_{j=1}^{m}K_{j}d_{X}(x,x^{\\prime})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Eq. D.2 comes from the fact that we take the expectation only over pairs of points $(x,y),(x^{\\prime},y^{\\prime})$ where $y=y^{\\prime}$ and also because the summand is always nonnegative. Then, we inductively apply the definition of $K_{i}$ -Lipschitz continuity to yield Eq. D.3. Eq. D.4 follows directly from the assumption in the statement of the proposition. Since the expression in Eq. D.4 now has no dependence on the label distribution, we may expand the expectation which results in Eq. D.5. Lastly, by the definition of $\\epsilon$ -knowledge continuity, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\epsilon\\ge\\mathbb{E}_{(x,y),(x^{\\prime},y^{\\prime})\\sim{\\cal D}_{\\cal{X},{\\cal{y}}}}\\frac{1}{\\prod_{j=1}^{n}K_{j}d_{\\mathcal{X}}(x,x^{\\prime})},}\\\\ {\\epsilon\\eta\\displaystyle\\prod_{j=1}^{n}K_{j}\\ge\\mathbb{E}_{(x,y),(x^{\\prime},y^{\\prime})\\sim{\\cal D}_{\\cal{X},{\\cal{y}}}}\\,\\frac{d_{\\mathcal{Y}}(f(x),f(x^{\\prime}))}{d_{\\mathcal{X}}(x,x^{\\prime})},\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and this concludes the proof of the proposition. ", "page_idx": 23}, {"type": "text", "text": "To prove Cor. 4.7, we need the following auxiliary result from [89]. ", "page_idx": 23}, {"type": "text", "text": "Proposition D.1 (See [89]). For a neural network $f:\\mathbb{R}^{n}\\to\\mathbb{R}^{K}$ with Lipschitz constant $L$ under $\\ell_{p}$ -norm, define the resulting classifier $g$ as $g(x):=$ arg $\\operatorname*{max}_{k\\in[K]}f_{k}(x)$ for an input $x$ . Then, $g$ is provably robust under perturbations $\\begin{array}{r}{\\|\\delta\\|_{p}<\\frac{\\ell\\sqrt{2}}{2L}m a r g i n(f(x)),}\\end{array}$ , i.e. ", "page_idx": 23}, {"type": "equation", "text": "$$\ng(x+\\delta)=g(x)\\qquad f o r\\,a l l\\,\\|\\delta\\|_{p}<\\frac{\\sqrt{2}}{2L}m a r g i n(f(x)).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, margin $(f(x))$ is the difference between the largest and second largeset output logit. ", "page_idx": 23}, {"type": "text", "text": "Corollary (See Cor. 4.7). Suppose that assumptions of Prop. 4.6 are true. And also assume that $(\\mathcal{X},d_{\\mathcal{X}})=(\\mathbb{R}^{n},\\ell_{p})$ , $(\\mathfrak{H},d_{\\mathfrak{H}})=(\\mathbb{R}^{m},\\ell_{p})$ , for $1\\le p\\le\\infty$ . Define a classifier from $f:\\,\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}^{m}$ , $g$ , where $g(x)\\;:=\\;$ arg $\\operatorname*{max}_{k\\in[m]}f_{k}(x)$ for any $x\\,\\in\\,\\mathbb{R}^{n}$ . Then, with probability $\\begin{array}{r}{1\\,-\\,\\frac{\\epsilon\\eta}{t}\\prod_{j=1}^{n}K_{j},}\\end{array}$ , $g(x)=g(x+\\delta)$ for all $\\begin{array}{r}{\\|\\delta\\|_{p}<\\frac{\\bar{\\sqrt{2}}}{2t}m a r g i n(f(x))}\\end{array}$ and $t>0$ . $f_{k}(x)$ is the $k^{t h}$ coordinate of $f(x)$ and margin $(f(x))$ denotes the difference between the largest and second-largest output logits. ", "page_idx": 23}, {"type": "text", "text": "Proof. By Prop. 4.6, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(x,y),(x^{\\prime},y^{\\prime})\\sim D_{\\boldsymbol{X},\\boldsymbol{y}}}\\,\\frac{d_{\\boldsymbol{y}}(\\boldsymbol{f}(\\boldsymbol{x}),\\boldsymbol{f}(\\boldsymbol{x}^{\\prime}))}{d_{\\boldsymbol{\\mathcal{X}}}(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})}\\leq\\epsilon\\eta\\prod_{j=1}^{n}K_{j}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Markov\u2019s inequality, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(x,y),(x^{\\prime},y^{\\prime})\\sim D_{\\boldsymbol{X},\\boldsymbol{y}}}\\left[\\frac{d_{\\boldsymbol{y}}(f(\\boldsymbol{x}),f(\\boldsymbol{x}^{\\prime}))}{d_{\\boldsymbol{\\mathcal{X}}}(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})}\\geq t\\right]\\leq\\frac{\\epsilon\\eta}{t}\\prod_{j=1}^{n}K_{j}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We yield the corollary by directly applying Prop. D.1 assuming that $f$ is $t$ -Lipschitz continuous. ", "page_idx": 23}, {"type": "text", "text": "Next, we establish conditions under which Lipschitz continuity implies knowledge continuity. ", "page_idx": 23}, {"type": "text", "text": "Proposition (Prop. 4.8). Let $(\\mathcal{X},d_{\\mathcal{X}}),(\\mathcal{Y},d_{\\mathcal{Y}})$ be a metric spaces. Let $f:\\mathcal{X}\\to\\mathcal{Y}$ be $\\epsilon$ -Lipschitz continuous and $\\mathcal{L}(f(x),y)$ be $\\eta$ -Lipschitz continuous with respect to both coordinates. If the first $n$ metric decompositions of $f$ are $K_{i}$ -Lipschitz continuous, then $f$ is knowledge continuous with respect to the $n^{t h}$ hidden layer. That is, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(x,y)\\sim{\\cal D}_{\\mathcal{X},\\mathcal{Y}}}\\,\\sigma_{f}^{n}(x,y)\\le\\epsilon\\eta\\prod_{j=1}^{n}\\frac{1}{K_{j}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let us start with the definition of $\\epsilon$ -Lipschitz continuity and lower-bound it. For any $(x,y),(x^{\\prime},y^{\\prime})\\in\\mathcal{X}\\times\\mathcal{Y}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{d_{\\mathcal{Y}}(f(x),f(x^{\\prime}))}{d_{\\mathcal{X}}(x,x^{\\prime})}\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d_{\\mathcal{Y}}(f(x),f(x^{\\prime}))}{\\prod_{j=1}^{n}\\frac{1}{K_{j}}d_{k}(f^{k}(x),f^{k}(x^{\\prime}))}\\le\\epsilon,}\\\\ {\\displaystyle\\frac{\\frac{1}{\\eta}|{\\mathcal{L}}(x,y)-{\\mathcal{L}}(x^{\\prime},y^{\\prime})|}{\\prod_{j=1}^{n}\\frac{1}{K_{j}}d_{k}(f^{k}(x),f^{k}(x^{\\prime}))}\\le\\epsilon,}\\\\ {\\displaystyle\\frac{|{\\mathcal{L}}(x,y)-{\\mathcal{L}}(x^{\\prime},y^{\\prime})|}{d_{k}(f^{k}(x),f^{k}(x^{\\prime}))}\\le\\epsilon\\eta\\prod_{j=1}^{n}\\frac{1}{K_{j}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Eq. D.11 follows from inductively applying the definition of Lipschitz continuity on the metric decompositions of $f$ . Specifically, $d_{i+1}(\\dot{f}^{i+1}(\\bar{x)},f^{i+1}(x^{\\prime}))\\leq K_{i}d_{i}(\\dot{f}^{i}(x),f^{i}(x))$ . Then, by the Lipschitz continuity of $\\mathcal{L}$ in both coordinates we yield Eq. D.12. Since the Lebesgue integral preserves order, Eq. D.13 directly implies the statement of the proposition and this concludes the proof. \u25a0 ", "page_idx": 24}, {"type": "text", "text": "E Predicting Adversarial Robustness with Volatility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we detail the experimental methods and results that use knowledge continuity to predict adversarial vulnerability, briefly discussed in Section 5. We focus on langauge models of various sizes and their ability to perform sentiment classification on the IMDB dataset [48]. Before computing any statistics of the model, we finetune it against the IMDB dataset and reserve a test set on which we compute a vulnerability score and estimate the model\u2019s adversarial vulnerability. ", "page_idx": 24}, {"type": "text", "text": "Vulnerability Score. As described in the main text, given a model with $n$ hidden layers, we compute all of its $k$ -volatility scores. This is done with a naive Monto-Carlo algorithm which we present in Appendix G. This results in a list of $k$ -volatility scores $\\{\\epsilon_{1},\\hdots,\\epsilon_{n}\\}$ , one for each hidden layer. Then, we perform a simple average $n^{-1}\\sum_{k=1}^{n}\\epsilon_{k}$ . Let us denote this quantity as the vulnarability score. ", "page_idx": 24}, {"type": "text", "text": "Estimating Adversarial Robustness. It remains to estimate the adversarial vulnerability of a given model. We do this empirically by applying an out-of-the-box adversarial attack (specifically, TextFooler [35]) on the given model with respect to the reserved test set. We then measure the number of successful adversarial attacks defined as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{\\#Successful~Adversarial~Attacks}=\\frac{\\lvert\\mathcal{X}^{\\mathrm{adversarial}}\\cap\\mathcal{X}^{\\mathrm{correct}})}{\\lvert\\mathcal{X}^{\\mathrm{correct}}\\rvert},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ${\\mathcal{X}}^{\\mathrm{correct}}$ is the set of examples in the test set that are correctly classified by the model (after finetuning) without any intervention. And, $\\mathcal{X}^{\\mathrm{adversarial}}$ are the set of examples that are incorrectly classified after an adversarial attack is applied. In other words, we only consider points where a perturbation will worsen performance. In expectation, this estimate of adversarial robustness should be a $1/2$ factor of the notion of vulnerability we present in Thm. 4.1, where we also consider a point to be vulnerable if perturbation increases its performance. ", "page_idx": 24}, {"type": "text", "text": "We then perform a linear regression using vulnerability score and a host of other model properties to predict the number of successful adversarial attacks. Concretely, we seek to learn the relationship: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{H}\\mathrm{Successful~Adversarial~Attacks}=m^{T}\\Big(n^{-1}\\sum_{k=1}^{n}\\epsilon_{k}\\oplus\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $m\\in\\mathbb{R}^{d}$ and $b\\in\\mathbb{R}$ are the learnable regression parameters. We also incorporate $d-1$ size and architectural variables into our regression as we found that significantly increases its predictiveness. And so, the input variables to our regression and their types are: ", "page_idx": 24}, {"type": "table", "img_path": "v07KRLYxDX/tmp/f2007410bcfb1d6439d7ceadce12109e769eba7e7f6cabe3a687bd5f3eaad963.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "v07KRLYxDX/tmp/4a6e8ea7a5506c2c1147626e339d8a66f8272dd0178e1159212604645836c4ba.jpg", "table_caption": [], "table_footnote": ["Table 2: Regression results from our three previously described experimental settings. We regress the number of successful adversarial attacks against (1) only the vulnerability score (2) vulnerability score and model characteristics (3) only model characteristics. The coefficients for each of these regressions results are shown in the column Coefficients. We also run permutation tests for each coefficient and the change in $R^{2}$ is shown in the column $\\Delta R^{2}$ (higher the better). "], "page_idx": 25}, {"type": "text", "text": "For the regression itself, we perform a Ridge regression with $\\alpha=1$ . We test three experimental conditions where we regress the model\u2019s adversarial robustness against: (1) only vulnerability score, (2) vulnerability score and model characteristics, (3) only model characteristics. We experiment with seven models: RoBERTa (Base/Large) [44], BERT-Uncased (Base/Large) [16], GPT2, and T5 (Small/Base) [58]. Our regression results are shown in Table. 2. ", "page_idx": 25}, {"type": "text", "text": "After yielding an initial line-of-best fit (see Fig. 2(a)), we run permutation tests to determine the contribution of each feature to the explained variance. Specifically, for each feature, keeping all else constant, we permute its values. If this feature is a significant contributor to the explained variance, intuitively, we should see a large decrease in $R^{2}$ after this intervention. If $s$ is the $R^{2}$ without any intervention and $s_{\\sigma_{i}(d)}$ is the $R^{2}$ after permuting the data by $\\sigma_{i}(\\cdot):[n]\\rightarrow[n]$ (for a dataset of $n$ data points). Then, we define ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta R^{2}=s-\\frac{1}{N}\\sum_{k=1}^{N}s_{\\sigma_{k}(d)},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $N$ controls the number of permutations that we apply. For all experiments we choose $N=100$ For formal theory on permutation tests, see [8]. ", "page_idx": 25}, {"type": "text", "text": "We find that when our vulnerability score is added to the regression, it contributes significantly to the explained variance. Moreover, in (2), we see that vulnerability score has the highest feature importance among all regression variables. ", "page_idx": 25}, {"type": "text", "text": "F Localizing Volatile Hidden Representations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we localize adversarially vulnerable hidden representations in two ways. Firstly, we use $k$ -volatility to gauge which layers are vulnerable across a selection of models. Then, we focus on model-specific characterizations of robustness with respect to $k$ -volatility. We present experiments on the same selection of models in Appendix E, the same dataset (IMDB [48]), and the same adversarial attack (TextFooler [35]) to empirically measure adversarial vulnerability. ", "page_idx": 25}, {"type": "text", "text": "F.1 Layerwise Volatility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "As mentioned in the previous section (Section E), for a given model with $n$ hidden layers, we can measure its $k$ -volatility for $k\\,\\in\\,[n]$ through a Monte-Carlo algorithm. For each model, we then plot its $k$ -volatility against its relative depth which is defined as $\\lfloor k/n\\rfloor$ . These curves are shown in Fig. 2(b). We see that models which have different architectures independent of size have very different $k$ -volatility curves. ", "page_idx": 25}, {"type": "text", "text": "We have already shown in the previous section that there is a positive correlation between $k$ -volatility and adversarial vulnerability. However, this correlation is derived from the simple average of all $k$ - volatility scores. Are the $k$ -volatility scores in some layers more predictive of adversarial vulnerability than others? If the $k$ -volatility in some layers is more correlated with $k$ -volatility in others, then it should suffice to minimize $k$ -volatility in these former layers. This would also speed up regularization and training. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "We repeat the experiments in the previous settings. But, instead of collating $k$ -volatility through a simple average, we run one regression for each relative depth across all models (which we discretize into 9 bins). This result is shown in Fig. 2(c). Surprisingly, we find that the magnitude of $k$ -volatility is not necessarily predictive of adversarial vulnerability. For example, in Fig. 2(b), almost all of the models exhibit low average $k$ -volatility in the latter layers. However, the $k$ -volatility of latter layers predict adversarial vulnerability the best. ", "page_idx": 26}, {"type": "text", "text": "F.2 Model-Specific Volatility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We start by exploring the $k$ -volatility across each of our test models. We notice that $k$ -volatility cannot be predicted by surface-level features such as size or model type alone. This is shown clearly in Fig. 3. Yet, as discussed in Appendix E, it is still able to predict actual adversarial vulnerability with moderate power. Thus, we conjecture that $k$ -volatility captures a complex aspect of the model\u2019s vulnerability which cannot be solely attributed to its size or type. ", "page_idx": 26}, {"type": "image", "img_path": "v07KRLYxDX/tmp/0ce5e961f5ec363819f4eab0641dfdd8f35c2c17d86551de7768a08fcd17466c.jpg", "img_caption": ["Figure 3: Average $k$ -volatility plotted against the log of number of model parameters (left). We see that although there is a strong negative correlation, the exactly relationship is nontrivial. Moreover, this negative correlation is also consistently observed across model families (right). "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "G Regularizing Knowledge Continuity ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we provide a comprehensive overview of regulating knowledge continuity to achieve robustness. We first show a simple algorithm that estimates $k$ -volatility. Then, we demonstrate how this can incorporated into any loss function as a regularization term. We then prove guarantees that revolve around the unbiasedness of our estimation algorithm. Lastly, we present detailed discussion of the results shown in Table 1 including training details and ablation studies over the hyperparameters. ", "page_idx": 26}, {"type": "text", "text": "G.1 Estimating Knowledge Continuity Algorithmically ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We first present a method for estimating $k$ -volatility. This is shown in Alg. 1(ESTKVOL). In theory, one should choose $M=N$ , as this will lead to a most accurate estimate. This is similar to contrastive learning methods where it is desirable to make the minibatch sizes as large as possible [56]. However, if $N\\gg1$ , this can become quickly intractable. In practice, during regularization we keep $N$ to be the same as if we were doing normal finetuning (i.e. 32/64) and set $M=N$ . This works well, and, anecdotally, we find that in contrast to contrastive learning increasing $N$ or $M$ past this threshold yields marginal returns. Further work could examine this relationship in more detail. ", "page_idx": 26}, {"type": "text", "text": "As discussed in the main text, the choice of metric (or representation space) which we enforce knowledge continuity against is crucial as it determines the type of robustness we will achieve. Therefore, in Alg. 1(KCREG), we incorporate this detail by sampling a hidden layer of interest using a Beta distribution specified by hyperparameters $\\alpha,\\beta$ . Then, on that minibatch, regularize $k$ -volatility ", "page_idx": 26}, {"type": "text", "text": "Algorithm 1 A Monte-Carlo algorithm for estimating $k$ -volatility of some metric decomposable function $f$ with $n$ hidden layers (left). Augmenting any loss function to regularize $k$ -volatility (right), given some Beta distribution parameterized by $\\alpha,\\beta$ and regularization strength $\\lambda\\geq0$ . ", "page_idx": 27}, {"type": "text", "text": "procedure ESTKVOL $\\left\\{(x_{i},y_{i})\\right\\}_{i=1}^{N},M,f,k)$ Sample $\\{\\boldsymbol n_{1},\\hdots,\\boldsymbol n_{M}\\}\\subset[N]$ uniformly $\\sigma_{f}^{k}\\leftarrow0$ Losses $\\leftarrow\\{\\mathcal{L}(f(x_{n_{i}}),y_{n_{i}})\\}_{i=1}^{M}$ for $(i,j)\\in[M]\\times[M]$ do $\\mathrm{Dist}\\leftarrow d_{k}(f^{k}(x_{n_{i}}),f^{k}(x_{n_{j}}))$ \ud835\udf0e\ud835\udc53\ud835\udc58\u2190\ud835\udf0e\ud835\udc53\ud835\udc58+ |Losses\ud835\udc56\u2212Losses\ud835\udc57|\u2215DIST return \ud835\udf0e\ud835\udc58 ", "page_idx": 27}, {"type": "text", "text": "procedure $\\mathrm{KCREG}(\\alpha,\\beta,M,\\lambda)$ \ud835\udc4b\u223cBeta(\ud835\udefc, \ud835\udefd) $k\\leftarrow\\operatorname*{max}(\\lfloor X n\\rfloor,1)$ \ud835\udf0e\ud835\udc53\ud835\udc58\u2190ESTKVOL({(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc56)}\ud835\udc56\ud835\udc41=1, \ud835\udc53, \ud835\udc40, \ud835\udc58) return \ud835\udc411 \u2211\ud835\udc56\ud835\udc41=1 \ue238(\ud835\udc53(\ud835\udc65\ud835\udc56), \ud835\udc66\ud835\udc56) +\ud835\udc4012 \ud835\udf06\ud835\udf0e\ud835\udc53\ud835\udc58 ", "page_idx": 27}, {"type": "text", "text": "with respect to that sampled layer. Note that we choose the Beta distribution for simplicity, however, it can be replaced by any distribution like a mixture of Gaussians. ", "page_idx": 27}, {"type": "text", "text": "In contrast to existing adversarial training methods such as [32] and [63] which only use the embeddings, our algorithm gives the practitioner more control over which hidden layer (or distance metric) to enforce smoothness. In this way, if the practitioner has some knowledge a priori of the attacker\u2019s strategy, they may choose to optimize against the most suitable metric decomposition. We present a brief discussion of the various tradeoffs when choosing $\\alpha,\\beta$ in the following section as well as a detailed empirical analysis in the following subsections. ", "page_idx": 27}, {"type": "text", "text": "$\\lambda$ is the weight we put on the regularizer in relation to the loss function $\\mathcal{L}$ . We provide a detailed ablation study of the effects of $\\lambda$ in the following subsections. We surprisingly find that even for $\\lambda\\ll1$ we can achieve significant edge in terms of robustness over existing methods. This is in contrast to virtual adversarial training methods such as [43] which requires applying a $\\lambda$ -value magnitudes larger. Moreover, for larger $\\lambda$ , we find that the accuracy of the model is not compromised. This provides some empirical support for Theorem 4.3. ", "page_idx": 27}, {"type": "text", "text": "G.2 Theoretical Guarantees of $k$ -Volatility Estimation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this subsection, we show that our Monte-Carlo algorithm presented in Alg. 1(ESTKVOL) is an unbiased estimator. The proof is simple and follows from some bookkeeping. ", "page_idx": 27}, {"type": "text", "text": "Proposition G.1 (Alg. 1(ESTKVOL) is an Unbiased Estimator). Assuming that each data point in the batch, {(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc56)}\ud835\udc56\ud835\udc41=1 $\\{(x_{i},y_{i})\\}_{i=1}^{N}\\stackrel{<}{\\sim}{D_{\\mathcal{X},\\mathcal{Y}}}$ , is sampled i.i.d., then Alg. 1(ESTKVOL) is an unbiased estimator for \ud835\udd3c $\\sigma_{f}^{k}(x,y)$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. Let $\\hat{\\theta}$ be the random variable representing the output of Alg. 1. It suffices to show that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\hat{\\theta}]=\\mathbb{E}\\,\\sigma_{f}^{k}(x,y),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the expectation on the left-hand side is taken over the set of all batches. By the definition of Alg. 1(ESTKVOL), ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}[\\hat{\\theta}]=\\mathbb{E}\\left(\\sum_{i=1}^{M}\\sum_{j=1}^{M}\\frac{1}{M^{2}}\\frac{\\Delta\\mathcal{L}_{f}^{(x_{n_{j}},y_{n_{j}})}(x_{n_{i}},y_{n_{i}})}{d_{k}(f^{k}(x_{n_{i}}),f^{k}(x_{n_{j}}))}\\right),}}\\\\ &{}&{=\\sum_{i=1}^{M}\\sum_{j=1}^{M}\\frac{1}{M^{2}}\\operatorname{\\mathbb{E}}\\left(\\frac{\\Delta\\mathcal{L}_{f}^{(x_{n_{j}},y_{n_{j}})}(x_{n_{i}},y_{n_{i}})}{d_{k}(f^{k}(x_{n_{i}}),f^{k}(x_{n_{j}}))}\\right),}\\\\ &{}&{=\\mathbb{E}\\,\\sigma_{f}^{k}(x,y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The second equality follows from the linearity of expectation. ", "page_idx": 27}, {"type": "text", "text": "We emphasize that our estimator is very naive. Improving its efficiency could form the basis of possible future work. For example, Rao-Blackwellizing [6] Alg. 1(ESTKVOL) to yield an estimator with smaller variance, applying rejection sampling to deal with the potential sparsity of the representation space discussed in Section 4.4, or adapting the regularization weight based on some bootstrapped confidence interval (if the estimate has higher variance then decrease weight on regularization and vice versa). However, we see that even with this naive algorithm we achieve improvements in robustness as well as training speed. ", "page_idx": 27}, {"type": "image", "img_path": "v07KRLYxDX/tmp/772723d3e88cc2bffe869610d4261afc6fb0ba2f12ee4ae5985cdff800bfb7de.jpg", "img_caption": ["Figure 4: Regularization $k$ -volatility for a host of vision models. We apply two adversarial attacks FGSM [24] (top row) and SI-NI-FGSM [41] (bottom row) with various attack strengths. Attack strength is measured in terms of maximum $\\ell_{2}$ -norm of the applied perturbation to the image. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "G.3 Computer Vision Results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In addition to regulating language models, we also demonstrate that KCREG is effective for vision tasks. This provides empirical support for the equivalences we proved in Section 4.5. The exact same method of $k$ -volatility estimation and loss augmentation is applied. We finetune three models ResNet50 [28], MobileNetV2 [61], and ViT16 [17] on the MNIST dataset both with and without our regularization algorithm. We then apply two different adversarial attacks: FGSM [24] and SI-NIFGSM [41]. We find that in both cases, regularization $k$ -volatility improves/stabilizes robustness across attack strengths (see Fig. 4). ", "page_idx": 28}, {"type": "text", "text": "G.4 Ablation Studies ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Herein, we present ablation studies for the crucial hyperparameters in our regularization algorithm (across the natural language tasks that we explored in the main text), Alg. 1(KCREG): \ud835\udf06which is the weight we assign the knowledge continuity regulation loss and $(\\alpha,\\beta)$ which determines the sampling behavior of the index of the hidden representation space. ", "page_idx": 28}, {"type": "text", "text": "Ablation Study of $\\lambda$ (Fig. 5(right)). The weight given to the regularizer $(\\lambda)$ is ablated over, with the results shown in Fig. 5. For any positive $\\lambda$ , there is an immediate large improvement in adversarial robustness. Next, as $\\lambda$ is systematically increased by factors of 10, we do not see a significant change in the accuracy (not under attack). This corroborates Theorem. 4.3, as it demonstrates that regulating knowledge discontinuities (no matter how strongly) is not at odds with minimizing the empirical risk of our model. On the other hand, we also do not see a significant increase in adversarial robustness as $\\lambda$ increases. This may imply that we have reached the threshold of adversarial robustness under TextFooler [35]. Specifically, the adversarial attacks generated by TextFooler may not be valid in that they have flipped the ground-truth label. Therefore, we believe that a good $\\lambda$ for this particular application should lie somewhere between 0 and $1\\times10^{-4}$ . ", "page_idx": 28}, {"type": "image", "img_path": "v07KRLYxDX/tmp/c943b849da82509bac5f5322ce769a0c97736147b25f7c32adf805e83ad42cf3.jpg", "img_caption": ["Figure 5: Ablation over the strength of regularization and its effect on the attack strength-attack success rate curves (left). Ablation over the regularization strength (for fixed attack strength $=0.3\\$ ) and its effect on test accuracy (right). We see that moderate regularization significantly improves robustness across all attack strengths. This improvement does not come at the expense of test accuracy. The attack-strength is measured using the minimum angular similarity between the perturbed and original text. Both ablations are done with respect to GPT2 on the IMDB [48] dataset with respect to the TextFooler attack [35]. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Ablation Study of Adversarial Attack Strength (Fig. 5(left)). For every value of $\\lambda$ , we also vary the strength of the adversarial attack. The adversarial attack strength is measured through the angular similarity of the embeddings between the original text and the perturbed text. Intuitively, if this constraint is loosened the adversary is allowed to find text that is semantically very different and vice versa. We see that moderate $k$ -volatility regulation achieves the best adversarial robustness across all attack strengths. ", "page_idx": 29}, {"type": "text", "text": "Ablation Study of $(\\alpha,\\beta)$ In this subsection, we briefly discuss how the $\\alpha,\\beta$ hyperparameters which determine the shape of the Beta distribution in Alg. 1(KCREG) affect the final performance and robustness of our model on the IMDB dataset. Recall that the shape of the Beta distribution determines the index of the hidden layers we are using the compute the knowledge continuity. Thus, they are crucial in determining the behavior of our regularizer. ", "page_idx": 29}, {"type": "text", "text": "We finetune {BERT, T5, GPT2} models on the IMDB dataset with the hyperparameters described in the next subsection. The results are displayed in Table 3. Across all models we observe a decrease in robustness for $\\alpha=1,\\beta=2$ . These values correspond to a right-skewed distribution which places high sampling probability on the earlier (closer to the input) hidden layers. Intuitively, perturbations in the early layers should correspond to proportional textual perturbations in the input text. Pure textual perturbations with respect to some metric like the Levenshtein distance should be only loosely if not completely (un)correlated with the actual labels of these inputs. Therefore, enforcing knowledge continuity with respect to this metric should not see increase robustness. Moreover, we also observe a larger decrease in accuracy (not under attack) with the same parameters. This suggests that maintaining this sort of knowledge continuity in the earlier layers is harder to converge on and there may be a \u201cpush-and-pull\u201d behavior between optimizing knowledge continuity and accuracy (not under attack). Surprisingly, we observe no significant difference between the other $\\alpha,\\beta$ values shown in the table. ", "page_idx": 29}, {"type": "text", "text": "We did not formally benchmark other configurations of $\\alpha,\\beta$ such as increasing their magnitude to impose a sharper distribution. Anecdotally, during training, we noticed that using these sharper distributions both significantly slowed the model\u2019s convergence and decreased the model\u2019s accuracy (not under attack). It could be that though knowledge continuity itself is a local property and the enforcement of this local property requires change on a global scale. In other words, one cannot simply reduce the knowledge discontinuities or uniformly converge with respect to one layer without participation from other layers. The extent to which other layers are involved in the regularization of a specific one is an interesting question that we leave for future research. ", "page_idx": 29}, {"type": "table", "img_path": "v07KRLYxDX/tmp/d0da2d3313016136d0c5fda807380aa139f8ac86832e8f77e50feb74efe3eb3c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 3: We train finetune {BERT, T5, GPT2} using knowledge continuity regularization, as described in Alg. 1(KCREG). We varied the $\\alpha,\\beta$ hyperparameters for the Beta distribution as to determine the effect of these parameters on model performance and robustness. The rows of the table are labeled with the format: Mode $1{+}\\mathrm{Reg}_{(\\alpha,\\beta)}$ . The bolded entries of the table correspond to the best performing metrics out of the knowledge continuity regulated models. ", "page_idx": 30}, {"type": "table", "img_path": "v07KRLYxDX/tmp/4d280f1dc139ea0a6cee179f1791d79c6e026497da02aabd575b57441b69283b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 4: Training hyperparameters and optimizer configurations for finetuning models {BERT, GPT2, T5} on IMDB without any form of regularization or adversarial training. ", "page_idx": 30}, {"type": "text", "text": "G.5 Training Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we describe in detail the training objectives, procedures, algorithms, and hyperparmeters that we used in the main text and further experiments done in the appendix. ", "page_idx": 30}, {"type": "text", "text": "Brute-Force Adversarial Training. For all models undergoing adversarial training, we first finetune the model against the training set. Then, attack it using the TextFooler [35] algorithm with examples from the training set. After the attacks are concluded, we then incorporate the text of successful adversarial attacks back into the training set and proceed to finetune again. This procedure iteratively continues. For the sake of computational efficiency, for all models we applied this procedure once. The parameters we are using during the adversarial attack is the same hyperparameters we actually use at test-time. Specifically, we impose a query budget of 300 queries. ", "page_idx": 30}, {"type": "text", "text": "Plain Finetuning on IMDB. The IMDB dataset consist of 50,000 examples with 25,000 for training and 25,000 for testing. We split the test set $40\\%{-}60\\%$ to create a validation and test set of 10,000 and 15,000 examples, respectively. Examples were sampled uniformly at random during the splitting process. Since adversarial attacks were costly, we uniformly subsampled 5,000 examples from this 15,000 to benchmark robustness in the experiments related to the regularizer. However, for the experiments estimating the knowledge vulnerability score, we performed adversarial attacks on all 15,000 datapoints in the test set. We found no significant difference between robustness estimation on this 5,000 subsample versus and the entire 15,000 dataset. ", "page_idx": 30}, {"type": "text", "text": "We train all models using the hyperparameter and optimizer configurations shown in Table 4. ", "page_idx": 30}, {"type": "text", "text": "Knowledge Discontinuity Regulation on IMDB. To enforce the knowledge discontinuity on IMDB, we use a constant $\\lambda=1\\times10^{-2}$ for all models. As shown in Table 3, we varied $\\alpha,\\beta\\in\\{1,2\\}\\times\\{1,2\\}$ and displayed the best models in terms of robustness in Table. 1 in the main text. We train all models for 50 epochs. Other than that all the other hyperparameters and optimizer configurations are the same as regular finetuning (see Table 4). ", "page_idx": 31}, {"type": "text", "text": "Knowledge Discontinuity Regulation on ANLI. Optimizing over the ANLI dataset was significantly harder than on IMDB. As a result, for each model class {BERT, GPT2, T5} we performed a quick hyperparameter search over \ud835\udf06 $\\downarrow(1\\times10^{-4})$ , the learning rate $(5\\times10^{-5})$ , and weight decay $(1\\times\\dot{1}0^{-9})$ fixing the parameterization of the Beta distribution to be the best values on the IMDB dataset. That is, for T5: $\\alpha=2,\\beta=1$ ; BERT-Base-Uncased: $\\alpha=2,\\beta=1$ ; GPT2: $\\alpha=2,\\beta=2$ . ", "page_idx": 31}, {"type": "text", "text": "ALUM on IMDB and ANLI. We train all ALUM models for 50 epochs (the same as knowledge discontinuity regularized models). For hyperpararmeters specific to the ALUM algorithm we choose all of the same ones as its authors, [43], with the exception of $\\alpha$ (analogous to the $\\lambda$ in our algorithm, essentially the weight put on the virtual adversarial training loss term). The authors of the original paper choose $\\alpha=10$ . We, however, found that this applied to finetuning does not converge at all. Thus, with a rough grid search in the parameter space we found $\\alpha=1\\breve{\\times}10^{-3}$ to be the best with respect to both performance and robustness. ", "page_idx": 31}, {"type": "text", "text": "We keep the same hyperparameters on ANLI, however, we impose early stopping during the training process. That is, we choose the best model with respect to its performance on the dev set. ", "page_idx": 31}, {"type": "text", "text": "H Certifying Robustness at Test-Time ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Herein, we present a certification algorithm using Thm. 4.1 and our Monte-Carlo estimate of $k$ - volatility 1(ESTKVOL). Our algorithm (shown in Alg. 2) is based on the work of [12]. We upperbound the $k$ -volatility by bootstrapping a $1-\\alpha$ confidence interval. Then, directly apply Thm. 4.1 using the 0-1 loss function. Thus, Cor. H.1 follows. We emphasize here that this certification algorithm may not be directly informative, especially in the discrete/non-metrizable setting, unless we have an inverse map from the representation space back to the input space. This is discussed further in [82]. Nonetheless, it can be used as a method to verify whether or not certain intervention techniques are successful before deploying them in the wild. ", "page_idx": 31}, {"type": "text", "text": "Corollary H.1. Let $A=\\{(x_{i},y_{i})_{i=1}^{n}\\}$ and $A^{\\prime}=\\{(x^{\\prime},y^{\\prime})\\in\\mathcal{X}\\times\\mathcal{Y}:\\mathbb{E}_{(x,y)\\sim D_{\\mathcal{X},\\mathcal{Y}}}\\Delta\\mathcal{L}_{f}^{(x,y)}(x^{\\prime},y^{\\prime})>\\eta\\}.$ . $(x,y)\\!\\in\\!A$ Then, with probability $1-\\alpha_{;}$ , the output of Alg. 2 bounds $\\mathbb{P}[A^{\\prime}|d_{j}(f^{j}(x),f^{j}(A))]$ where $\\mathcal{L}$ is the 0-1 loss. ", "page_idx": 31}, {"type": "text", "text": "Algorithm 2 Certifying robustness of a metric decomposable function $f$ with respect to one hidden   \nrepresentation using Alg. 1(ESTKVOL) and Thm. 4.1. procedure CERTIFY $(f,\\{(x_{i},y_{i})\\}_{i=1}^{n},k,j,\\alpha,\\delta,\\eta)$ Let $\\mathcal{L}$ be the 0-1 loss function pro $\\begin{array}{r l}&{\\epsilon_{U}\\gets\\mathrm{UPPERCONFBOUND}(f,\\mathcal{L},\\{(x_{i},y_{i})\\}_{i=1}^{n},k,j,\\alpha)}\\\\ &{B\\gets\\operatorname*{max}_{1\\leq a,b\\leq n}d_{j}(f^{j}(x_{a}),f^{j}(x_{b}))}\\\\ &{V\\gets\\eta\\left(1-\\exp\\left(-2/B^{2}\\left(\\delta-B\\sqrt{\\frac{1}{2}\\log2n}\\right)^{2}\\right)\\right)}\\\\ &{\\mathrm{return~CLIP}(1-\\epsilon_{U}\\delta/V,0,1)}\\\\ &{\\mathrm{redure~UPPERCONFBOUND}(f,\\mathcal{L},\\{(x_{i},y_{i})\\}_{i=1}^{n},k,j,\\alpha)}\\end{array}$ $U\\gets\\mathbf{0}_{k}$ for $i\\leftarrow1\\ldots k$ do $S\\gets$ sample w/ replacement $n$ points from $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ r $\\begin{array}{r l}&{\\quad U_{i}\\gets\\mathrm{EsTKVoL}(S,\\mathcal{L},f,j)}\\\\ &{\\quad\\mathbf{eturn}\\ \\frac{1}{k}\\sum_{\\ell=1}^{k}U_{k}+\\Phi^{-1}(\\alpha)\\mathrm{std}(U)/\\sqrt{k}}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "Along these lines, we apply our certification algorithm to our regularized models to verify that the certified robustness has indeed improved. These results are shown in Fig. 6. ", "page_idx": 31}, {"type": "image", "img_path": "v07KRLYxDX/tmp/b2d063c3b3a4a0bceb2a912e619c12cfab4629f290c98b5886ff7192bf905bb4.jpg", "img_caption": ["Figure 6: Certification of robustness for GPT2, laye ${\\it=}6$ . We apply Alg. 2 to certify robustness of the model before and after regularization with Alg. 1(KDREG). Each line corresponds to the change in absolute accuracy for a set of examples to be considered non-robust. The $y_{\\mathrm{~\\,~}}$ -axis corresponds to the certified probability measure of the set of non-robust examples under this criterion and the $x$ -axis corresponds to the maximum perturbation distance in the representation space. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "I Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "This contribution is concerned with robust deep learning models. As deep learning becomes ubiquitous as the primary method for creating artificial intelligence, their applications in increasingly critical areas to the lay and corporations alike demand not only both high inferential accuracy and confidence but also safety and trustworthiness guarantees. Robustness addresses this latter point. More specifically, our contribution unifies separate robustness efforts from continuous and discrete domains. ", "page_idx": 32}, {"type": "text", "text": "J Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "All of our experiments were conducted on four NVIDIA RTX A6000 GPUs as well as four NVIDIA Quadro RTX 6000 GPUs. The rest of our codebase including implementations of the algorithms and figures described in the manuscript can be found at https://github.com/alansun17904/kc. ", "page_idx": 32}, {"type": "text", "text": "K Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The certification guarantees of our definition knowledge continuity is a probabilistic one. Specifically, this randomness is over the data distribution. However, this does not protect against out-of-distribution attacks that plague large language models such as [72, 91]. More work is needed to yield deterministic results that do not become vacuous in discrete settings. As mentioned in Section 4.4, our expressiveness bounds only apply under little restrictions to the metric decompositions of the estimator $f$ . Though we see some empirical verification for this in Appendix G, it remains unclear whether or not we can tighten these bounds. ", "page_idx": 32}, {"type": "text", "text": "L NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We present a detailed discussion of the limitations in Section K. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: For all of the theoretical results in the paper, we include all of its assumptions. We include full proofs of each theoretical result in Appendices A, B, C, G. To the best of our knowledge, the proofs are correct. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We present all of the hyperparameters in the experiments that require training in Appendix G. Additionally, our compute resources are detailed in Appendix J. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closedsource models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We attach all of the code used to generate the figures and the experimental results in the supplementary materials. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). ", "page_idx": 34}, {"type": "text", "text": "\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: For the experiments that require training, we discuss in detail the hyperparameters in Appendix G. Moreover, we also attach the code used to generate all results and figures in the supplementary materials of the submission. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "page_idx": 35}, {"type": "text", "text": "Justification: The experiments in our paper serve as a type of sanity check and demonstrate possible explorations rather than a benchmark against existing state-of-the-art methods. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide details on the compute resources we use in Appendix J. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics and to the best of our knowledge it does conform to this in every respect. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We discuss the broader impacts of the work in Appendix I. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper is concerned with training more robust deep learning models. Thus, it does not pose such risks. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: We do not introduce any new assets in the paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: We do not perform any crowdsourcing experiments nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We do not perform any experiments that involve crowdsourcing nor research with human subjects. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]