[{"figure_path": "v07KRLYxDX/figures/figures_4_1.jpg", "caption": "Figure 1: Examples of knowledge (dis)continuities. f : X \u2192 Y is a measurable map, and (Zk, dk) is one of its hidden representations. The color of the points indicates loss. \u2605 denotes knowledge continuity induced by sparsity: an isolated concept with no knowledge relations close to it. So, any perturbation moves far away with high probability. Smooth changes in loss around \u2605 implies knowledge continuity. Finally, \u25c6 is not knowledge continuous due to drastic changes in loss nearby. Notice that the classification of points is independent of input-output clustering behavior since X, Y may not be endowed with a metric.", "description": "This figure illustrates three scenarios of knowledge continuity in a model's hidden representation space.  The color of the points represents the loss. A single point (\u2605) surrounded by a large space shows sparse knowledge continuity where any small perturbation results in large changes in loss. A cluster of points (\u2605) with smooth changes in loss nearby illustrates knowledge continuity. A point (\u25c6) surrounded by a drastic change in loss exhibits knowledge discontinuity. Importantly, the classification of points is independent of the input-output space clustering since the input-output space might not have a metric.", "section": "4 Knowledge Continuity"}, {"figure_path": "v07KRLYxDX/figures/figures_8_1.jpg", "caption": "Figure 2: (a) The average percentage of successful adversarial attacks by TextFooler [35] on a host of models [58, 57, 16, 44] and the IMDB [48] dataset regressed with the average of knowledge continuity coefficients across all hidden layers (R2 = 0.35). (b) k-Volatility as k is varied across a model\u2019s relative depth. (c) Correlation between k-volatility and adversarial vulnerability (averaged across all models shown in (b)) with respect to TextFooler [35] as k varies.", "description": "This figure demonstrates the correlation between knowledge continuity and adversarial robustness.  Panel (a) shows a regression analysis indicating that the average knowledge continuity across hidden layers correlates with the percentage of successful adversarial attacks (R-squared = 0.35). Panels (b) and (c) further explore this relationship by showing how k-volatility (a measure of knowledge continuity at a specific layer) changes with the relative depth of the model and its correlation with adversarial vulnerability.", "section": "Practical Applications"}, {"figure_path": "v07KRLYxDX/figures/figures_18_1.jpg", "caption": "Figure 1: Examples of knowledge (dis)continuities. f : X \u2192 Y is a measurable map, and (Zk, dk) is one of its hidden representations. The color of the points indicates loss. \u2605 denotes knowledge continuity induced by sparsity: an isolated concept with no knowledge relations close to it. So, any perturbation moves far away with high probability. Smooth changes in loss around \u2605 implies knowledge continuity. Finally, \u25cf is not knowledge continuous due to drastic changes in loss nearby. Notice that the classification of points is independent of input/output clustering behavior since X, Y may not be endowed with a metric.", "description": "This figure illustrates three different scenarios regarding knowledge continuity in a model's representation space.  The first scenario shows sparse knowledge continuity, where a point (star) is isolated and has no nearby points with similar loss values. Any small perturbation would result in a large change in loss. The second case shows smooth knowledge continuity; points near a central point (star) have gradual changes in loss values, indicating continuity. The last case (circle) shows knowledge discontinuity; a point has a drastic change in loss value near it, indicating discontinuity. The key is that the classification (loss) is independent of the input and output spaces' metrics (or lack thereof).", "section": "4 Knowledge Continuity"}, {"figure_path": "v07KRLYxDX/figures/figures_18_2.jpg", "caption": "Figure 1: Examples of knowledge (dis)continuities. f : X \u2192 Y is a measurable map, and (Zk, dk) is one of its hidden representations. The color of the points indicates loss. \u2605 denotes knowledge continuity induced by sparsity: an isolated concept with no knowledge relations close to it. So, any perturbation moves far away with high probability. Smooth changes in loss around \u2605 implies knowledge continuity. Finally, \u25cf is not knowledge continuous due to drastic changes in loss nearby. Notice that the classification of points is independent of input/output clustering behavior since X, Y may not be endowed with a metric.", "description": "This figure illustrates three different scenarios regarding knowledge continuity in a model's hidden representation space.  The first scenario (\u2605) shows sparse knowledge continuity, where a data point is isolated and far from similar points, leading to high volatility in loss with small changes in the representation. The second scenario (\u2605) depicts knowledge continuity, where smooth changes in loss occur when the representation changes.  Finally, the third scenario (\u25cf) displays knowledge discontinuity, showing that even a minor change in the representation leads to a large change in the loss function.  The color of the points represents the loss values.  Importantly, the visualization emphasizes that this classification is independent of any metric on the input/output spaces (X,Y).", "section": "Defining Perceived Knowledge"}, {"figure_path": "v07KRLYxDX/figures/figures_18_3.jpg", "caption": "Figure 1: Examples of knowledge (dis)continuities. f : X \u2192 Y is a measurable map, and (Zk, dk) is one of its hidden representations.  denotes knowledge continuity induced by sparsity: an isolated concept with no knowledge relations close to it. So, any perturbation moves far away with high probability. Smooth changes in loss around \u2605 implies knowledge continuity. Finally,  is not knowledge continuous due to drastic changes in loss nearby. Notice that the classification of points is independent of input/output clustering behavior since X, Y may not be endowed with a metric.", "description": "This figure illustrates the concept of knowledge continuity using examples of sparse knowledge continuity, knowledge continuity, and knowledge discontinuity. It shows how the change in loss around a point in the hidden representation space relates to the concept of knowledge continuity.  The color of the points represents loss, and their arrangement shows how the change in loss varies with respect to the closeness of the points in the hidden representation space. The lack of a metric on X and Y is emphasized, indicating the model\u2019s knowledge is not dependent on input/output distance.", "section": "4 Knowledge Continuity"}, {"figure_path": "v07KRLYxDX/figures/figures_26_1.jpg", "caption": "Figure 2: (a) The average percentage of successful adversarial attacks by TextFooler [35] on a host of models [58, 57, 16, 44] and the IMDB [48] dataset regressed with the average of knowledge continuity coefficients across all hidden layers (R2 = 0.35). (b) k-Volatility as k is varied across a model\u2019s relative depth. (c) Correlation between k-volatility and adversarial vulnerability (averaged across all models shown in (b)) with respect to TextFooler [35] as k varies.", "description": "This figure shows the correlation between knowledge continuity and adversarial robustness.  Panel (a) demonstrates a regression showing that the average knowledge continuity across hidden layers correlates with the success rate of adversarial attacks (TextFooler) on several language models using the IMDB dataset.  Panel (b) illustrates how k-volatility (a measure of knowledge continuity) changes with the relative depth (layer number) of the model.  Finally, panel (c) shows the correlation between k-volatility and the success rate of adversarial attacks for different layers in the models.", "section": "Practical Applications"}, {"figure_path": "v07KRLYxDX/figures/figures_28_1.jpg", "caption": "Figure 4: Regularization k-volatility for a host of vision models. We apply two adversarial attacks FGSM [24] (top row) and SI-NI-FGSM [41] (bottom row) with various attack strengths. Attack strength is measured in terms of maximum l2-norm of the applied perturbation to the image.", "description": "This figure shows the results of applying the knowledge continuity regularization method to various vision models. Two adversarial attacks, FGSM and SI-NI-FGSM, were used with varying strengths. The x-axis represents the attack strength, and the y-axis shows the macro F1 score.  The top row shows results for the FGSM attack, while the bottom row displays results for the SI-NI-FGSM attack.  The three columns show results for three different model architectures: ResNet50, MobileNet, and ViT16. The red bars represent the performance of the models with the knowledge continuity regularization applied, while the black dots represent the performance of the baseline models without regularization.  The color scale shows the difference in macro F1 score between the regularized and baseline models.", "section": "G Computer Vision Results"}, {"figure_path": "v07KRLYxDX/figures/figures_29_1.jpg", "caption": "Figure 5: Ablation over the strength of regularization and its effect on the attack strength-attack success rate curves (left). Ablation over the regularization strength (for fixed attack strength = 0.3) and its effect on test accuracy (right). We see that moderate regularization significantly improves robustness across all attack strengths. This improvement does not come at the expense of test accuracy. The attack-strength is measured using the minimum angular similarity between the perturbed and original text. Both ablations are done with respect to GPT2 on the IMDB [48] dataset with respect to the TextFooler attack [35].", "description": "This figure shows the ablation study of the hyperparameters used in the knowledge continuity regularization algorithm. The left plot shows the attack success rate curves with different regularization strength (\u03bb) for various attack strengths. The right plot shows test accuracy against regularization strength (\u03bb) with fixed attack strength (0.3). It demonstrates that moderate regularization significantly improves robustness without sacrificing test accuracy.", "section": "G Regularizing Knowledge Continuity"}, {"figure_path": "v07KRLYxDX/figures/figures_32_1.jpg", "caption": "Figure 6: Certification of robustness for GPT2, layer=6. We apply Alg. 2 to certify robustness of the model before and after regularization with Alg. 1(KDREG). Each line corresponds to the change in absolute accuracy for a set of examples to be considered non-robust. The y-axis corresponds to the certified probability measure of the set of non-robust examples under this criterion and the x-axis corresponds to the maximum perturbation distance in the representation space.", "description": "This figure shows the results of applying a certification algorithm (Alg. 2) to a GPT-2 model, both before and after applying a knowledge continuity regularization (Alg. 1).  The algorithm assesses the model's robustness to perturbations. Each line represents a different threshold for considering examples as \"non-robust\". The y-axis indicates the certified probability that examples exceed this non-robustness threshold at a given perturbation distance (x-axis).  The color gradient likely represents the change in model accuracy from the unperturbed baseline, showing how accuracy degrades with increasing perturbation magnitude.", "section": "H Certifying Robustness at Test-Time"}]