[{"type": "text", "text": "Breaking the False Sense of Security in Backdoor Defense through Re-Activation Attack ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mingli Zhu1 Siyuan Liang2 Baoyuan $\\mathbf{W}\\mathbf{u}^{1*}$ ", "page_idx": 0}, {"type": "text", "text": "1School of Data Science, The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China 2National University of Singapore, Singapore ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks face persistent challenges in defending against backdoor attacks, leading to an ongoing battle between attacks and defenses. While existing backdoor defense strategies have shown promising performance on reducing attack success rates, can we confidently claim that the backdoor threat has truly been eliminated from the model? To address it, we re-investigate the characteristics of the backdoored models after defense (denoted as defense models). Surprisingly, we find that the original backdoors still exist in defense models derived from existing post-training defense strategies, and the backdoor existence is measured by a novel metric called backdoor existence coefficient. It implies that the backdoors just lie dormant rather than being eliminated. To further verify this finding, we empirically show that these dormant backdoors can be easily re-activated during inference stage, by manipulating the original trigger with well-designed tiny perturbation using universal adversarial attack. More practically, we extend our backdoor re-activation to black-box scenario, where the defense model can only be queried by the adversary during inference stage, and develop two effective methods, i.e., query-based and transfer-based backdoor re-activation attacks. The effectiveness of the proposed methods are verified on both image classification and multimodal contrastive learning (i.e., CLIP) tasks. In conclusion, this work uncovers a critical vulnerability that has never been explored in existing defense strategies, emphasizing the urgency of designing more robust and advanced backdoor defense mechanisms in the future. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The pervasive application of Deep Neural Networks (DNNs) across safety-critical domains like facial recognition and autonomous driving [23, 36] has underlined their significance and profound impact in industrial and academic spheres. Despite their transformative potential, DNNs are known to be vulnerable to malicious threats [5, 27], which compromise the integrity and reliability of advanced systems. One of the representative threats is backdoor attacks [18, 31], where an adversary pre-defines a \"trigger\" and embeds it within limited training data such that the backdoored model will misclassify trigger-containing inputs into specific target categories while appropriately processing benign inputs. ", "page_idx": 0}, {"type": "text", "text": "A successful backdoor attack consists of two stages: (1) the embedding of the backdoor within the model during training; and (2) its subsequent activation during inference stage [62]. To identify [14] and mitigate the harmful impacts of backdoor attacks, substantial efforts have been made ranging from dataset segmentation [7, 50], trigger inversion [53, 56], model pruning [64, 71], and fine-tuning based defenses [30, 67]. While these existing defense mechanisms aim at decreasing the attack success rates (ASR) [59] of corresponding backdoored models, a fundamental question arises: can we confidently claim that the backdoor threat has truly been eliminated from the model? In this work, we use the term defense model(s) to denote those models which have initially been poisoned to backdoored models and subsequently defended using some defensive techniques, for convenience. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To answer above question, we introduce an innovative concept, backdoor existence coefficient (BEC) to quantify the extent of backdoor presence within models. Using BEC, we can re-investigate the backdoor existence in existing defense models [30, 58, 67]. Specifically, the BEC measures the similarity of activation among backdoor-related neurons in the poisoned samples between the backdoored model and its corresponding defense model. Fig. 1 presents the relationship between BEC and backdoor activation (indicated by ASR) across three different attack and defense methods for comparison. In this figure, distinct shapes and colors denote various attack and defense methods, respectively. As depicted in the figure, even though the ASRs decline nearly to zero which implies that defense models perform comparably to clean models, the BECs in the defense models remain significantly high. This notable observation implies that the original backdoors just lie dormant rather than being eliminated in defense models. ", "page_idx": 1}, {"type": "image", "img_path": "E2odGznGim/tmp/70cf380163c081179f76f8f833d7353134f971e819b8871fa49e8601c5265116.jpg", "img_caption": ["Figure 1: Comparative analysis of backdoor existence coefficient and backdoor activation rate across different models. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Inspired by above observations, we pose a question: Since the original trigger fails to activate the original backdoor, is it possible to unearth a variant of the original trigger that is capable of re-activating the backdoor? Given that in real-world scenarios where the adversary cannot modify the defense model, our objective is to modify the original trigger, thereby facilitating backdoor re-activation in defense models during inference stage. To verify this feasibility, we formulate the backdoor re-activation task as constrained optimization problem with the goal of searching for a minimal universal adversarial perturbation on the original trigger. Consequently, this general technique can be seamlessly combined with any prevailing backdoor attacks to re-activate backdoor effect in defense models in their inference time. To demonstrate the real-world threat posed by backdoor re-activation attack, we also expand our method to black-box and transfer attack scenarios, where adversaries are limited to querying the model without access to its internal mechanisms. Nowadays, multimodal contrastive learning (MMCL) has impressed us with its performance across a range of tasks and backdoor threats in MMCL have also been broadly studied. In this work, we consider both image classification and multimodal tasks, demonstrating the universality and adaptability of our approach. Extensive experimental results on nine different attacks and eight state-of-the-art defenses across four benchmark datasets and three model architectures demonstrate the effectiveness of our method. Our work reveals a new vulnerability in existing defense strategies, emphasizing the need for more robust and advanced defense mechanisms in the future. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are threefold: 1) We re-investigate existing defense methods, and reveal that the original backdoor still exists in the model even after defense, though it cannot be activated by the original trigger. 2) We develop a novel optimization problem to re-activate the original backdoor during inference by perturbing the original trigger, under white-box, black-box, and transfer attack scenarios. 3) We demonstrate the effectiveness of the proposed method with extensive experiments on both image classification and the emerging multi-modal contrastive learning tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Backdoor attacks. Backdoor attacks [15, 16, 22, 48, 55, 59, 75] are a significant security threat in DNNs. As summarized by Wu et al. [60, 62], a successful backdoor attack consists of two components: backdoor injection during pre-training or training stage, and backdoor activation during inference stage. Backdoor injection could be divided into data poisoning attack at pre-training stage and training-controllable attack at training stage. During a data poisoning attack, an adversary releases a poisoned dataset to plant backdoors. Representative works include BadNets [18], Blended [10], LF [68], SSBA [31], and Trojan [37]. For training-controllable attack [70], an adversary takes control of the training process to optimize triggers and inject backdoors. Notable examples are Input-Aware [40] and WaNet [41]. In inference stage, the adversary uses the poisoned samples to activate backdoors in the backdoored model, thereby achieving a successful attack. ", "page_idx": 1}, {"type": "text", "text": "While backdoor attacks are prevalent in supervised learning, backdoor threats also exist in domain of multi-modal contrastive learning (MMCL) [32, 33]. Carlini et al. [6] are the pioneers to unveil backdoor threats in MMCL, demonstrating that as few as $0.0001\\%$ of images can trigger a successful attack. More recently, sophisticated approaches have been introduced [2]. For instance, TrojanVQA [52] is designed for the multi-modal visual question answering task, while BadCLIP [35] shows that their attack can persist in effectiveness against backdoor defenses. ", "page_idx": 2}, {"type": "text", "text": "While a variety of attack methods have been proposed, they primarily focus on enhancing attack success rate during backdoor injection stage and employ the same trigger to activate backdoors in inference stage. They did not consider that the model might be fine-tuned or defended by users, and the original triggers fail to activate backdoors in inference stage. Although Qi et al. [42] attempted to enhance backdoor signal during inference stage, they did not consider defensive techniques in depth, and their attack lacks universality. In this work, we focus on a general backdoor attack method during inference time, researching on how to re-activate the dormant backdoors in defense models. ", "page_idx": 2}, {"type": "text", "text": "Backdoor defenses. A range of works [21, 24, 29, 39, 69] focusing on backdoor defenses have been put forward to address the threat of backdoor attacks. Considering defense stages, four main categories emerge: pre-processing defenses, training-stage defenses, post-training defenses, and inference stage defenses [61]. Pre-processing defenses [7, 24, 76] aim to fliter out poisoned samples from poisoned dataset. Training-stage strategies [9, 21, 29, 57] consider that the defender has access to both training samples and the model, and mitigates backdoor effects during training process. They leverage discrepancies between poisoned and benign samples to fliter out suspicious instances. Post-training defenses [11, 39, 54, 69, 73] focus on removing backdoor effect from backdoored models through pruning potential backdoor neurons [64], backdoor triggers reversion and unlearning [53], or enhancing fine-tuning processes for backdoor mitigation [72]. Inference stage defenses aim at preventing backdoor activation with samples detection or samples recovery techniques [76]. In the domain of MMCL, there are a range of works [3, 34, 65]. CleanCLIP [3] is the first to defend the MMCL model using MMCL loss and self-supervised learning within each modality with clean samples. Additionally, RoCLIP [66] introduces a robust pre-training approach, which focuses on disrupting the link between poisoned image-caption pairs. In this work, we focus on backdoor re-activation attack and thus mainly consider our attack against post-training backdoor defenses. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce our threat model and methods for image classification task for clarity.   \nFor the formulation and methods for multimodal contrastive learning, please refer to Appendix A. ", "page_idx": 2}, {"type": "text", "text": "3.1 Threat model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. For the image classification task, the training dataset is $\\mathcal{D}=\\{(\\pmb{x}^{(i)},y^{(i)})\\}_{i=1}^{n}\\subseteq\\mathcal{X}\\times\\mathcal{Y},$ , where $\\mathcal{X}\\subset\\mathbb{R}^{d}$ and $\\mathcal{V}=\\{1,\\ldots,K\\}$ are input space and label set, respectively. Given an input $\\textbf{\\em x}$ , we define a deep neural network with $L$ layers as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(\\pmb{x})=f^{(L)}\\circ f^{(L-1)}\\circ\\cdot\\cdot\\cdot\\circ f^{(1)}(\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f^{(l)}$ is the function in the $l^{t h}$ layer of the network, $1\\leq l\\leq L$ . The feature map of the $l^{t h}$ layer is denoted as $m^{(l)}({\\pmb x})\\in\\mathbb{R}^{c_{l}\\times h_{l}\\times w_{l}}$ , and $f_{k}({\\pmb x})$ represents the logit of the $k^{\\mathrm{th}}$ class. ", "page_idx": 2}, {"type": "text", "text": "Before introducing our methods, we first outline the pipeline of backdoor attack and defense. As summarized in [62] and shown in Tab. 1, the whole pipeline of backdoor attack and defense involves four stages: ", "page_idx": 2}, {"type": "text", "text": "I. Pre-training stage: An adversary conducts data poisoning backdoor attack, which involves revising a small fraction of $\\mathcal{D}$ to generate poisoned dataset $\\bar{\\boldsymbol{D}}_{p}=\\{(\\mathbf{x}_{\\xi}^{(i)},t)\\}_{i=1}^{n_{p}}$ by injecting a trigger $\\xi$ into the image and changing the corresponding label into target label $t$ . ", "page_idx": 2}, {"type": "text", "text": "II. Training stage: An adversary controls the training process to inject backdoors into model $f_{\\theta_{\\mathrm{A}}}$ . ", "page_idx": 2}, {"type": "text", "text": "III. Post-training stage: A defender receives the poisoned model, and can gather some benign samples to remove the backdoor effect from the model, denoted as $f_{\\theta_{\\mathrm{D}}}$ . ", "page_idx": 2}, {"type": "table", "img_path": "E2odGznGim/tmp/e6a9c4134b0541165e3f51848285211f2cb3b4cc603e90f2eb1ab8945f924ce4.jpg", "table_caption": ["Table 1: Illustration of the pipeline of backdoor attack and defense. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "IV. Inference stage: With the defense model $f_{\\theta_{\\mathrm{D}}}$ , the original trigger fails to activate the backdoor, i.e., $f_{\\theta_{\\mathrm{D}}}({\\boldsymbol{x}}_{\\xi})\\neq{t}$ . The goal is to re-activate backdoors, i.e., $f_{\\pmb{\\theta}_{\\mathrm{D}}}(\\pmb{x}_{\\pmb{\\xi}^{\\prime}})=t$ , where $\\pmb{\\xi}^{\\prime}=\\pmb{\\xi}+\\Delta_{\\xi}$ . ", "page_idx": 3}, {"type": "text", "text": "Existing backdoor attacks primarily focus on achieving high attack success rates (ASR) in backdoor injection stages (I and II), with little consideration for the defensive impact in stage III. Given the failures of $\\pmb{x}_{\\pmb{\\xi}}$ in attacking $f_{\\theta_{D}}$ , our work focuses on the backdoor re-activation attack in stage $I V.$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Backdoor existence coefficient ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "While the model performance in Tab. 1 suggests that $f_{\\theta_{\\mathrm{D}}}$ and $f_{\\theta_{\\mathrm{C}}}$ are analogous, we argue that in terms of the backdoor effect, $f_{\\theta_{\\mathrm{D}}}$ and $f_{\\theta_{\\mathrm{A}}}$ are actually more closely aligned, which indicates the persistent existence of backdoor in model $f_{\\theta_{\\mathrm{D}}}$ . To verify this, we need a metric to measure the quantity of backdoor existence within a model. An effective indicator should be capable of quantifying the similarity of backdoor effect between backdoored model $f_{\\theta_{\\mathrm{A}}}$ and the target defense model $f_{\\theta_{\\mathrm{D}}}$ across the entire models. To achieve this, we propose a new metric, Backdoor Existence Coefficient (BEC), which is calculated through the following three steps: ", "page_idx": 3}, {"type": "text", "text": "1. Backdoor neuron identification: Firstly, we need to identify backdoor-related neurons. Zheng et al. [71] proposed Trigger-activated Change (TAC) to quantify the correlation between backdoor impact and neurons (see Appendix C for details). With this metric, backdoor-related neurons in $f_{\\theta_{\\mathrm{A}}}$ are identified for each layer. Thus, the feature maps corresponding to these neuron indices are selected for each model, denoted as $\\tilde{m}_{\\mathrm{A}}^{(l)}(x_{\\xi})$ , $\\tilde{m}_{\\mathrm{D}}^{(l)}(\\pmb{x}_{\\pmb{\\xi}})$ , and $\\tilde{m}_{\\mathrm{C}}^{(l)}({\\pmb x}_{\\xi})$ respectively. Denote the feature maps across dataset $\\mathcal{D}_{p}$ as $\\tilde{m}^{(l)}(\\mathcal{D}_{p})\\in\\mathbb{R}^{n_{p}\\times(\\tilde{c}_{l}\\times h_{l}\\times w_{l})}$ . ", "page_idx": 3}, {"type": "text", "text": "2. Backdoor effect similarity metric: In order to measure the backdoor effect similarity between models, we employ Centered Kernel Alignment (CKA) [25] (see Appendix C for details) to quantify the similarity between these matrices. The similarity in backdoor effects between $f_{\\theta_{\\mathrm{D}}}$ and $f_{\\theta_{\\mathrm{A}}}$ , calculated through the use of corresponding features, can be computed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nS_{\\mathrm{D,A}}^{(l)}(\\mathcal{D}_{p})=\\mathbf{C}\\mathbf{K}\\mathbf{A}\\left(\\tilde{m}_{\\mathrm{D}}^{(l)}(\\mathcal{D}_{p}),\\tilde{m}_{\\mathrm{A}}^{(l)}(\\mathcal{D}_{p})\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $S_{\\mathrm{C,A}}^{(l)}(D_{p})$ is computed accordingly. ", "page_idx": 3}, {"type": "text", "text": "3. Backdoor existence coefficient computation: The BEC is the average of normalized backdoor effect similarity across all layers. By assigning the BEC of $f_{\\theta_{\\mathrm{A}}}$ a value of 1 and $f_{\\theta_{\\mathrm{C}}}$ a value of 0, the computation can proceed as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho_{\\mathrm{BEC}}(f_{\\theta_{\\mathrm{D}}},f_{\\theta_{\\mathrm{A}}},f_{\\theta_{\\mathrm{C}}};\\mathcal{D}_{p})=\\frac{1}{N}\\sum_{l=1}^{N}\\frac{S_{\\mathrm{D},\\mathrm{A}}^{(l)}(\\mathcal{D}_{p})-S_{\\mathrm{C},\\mathrm{A}}^{(l)}(\\mathcal{D}_{p})}{S_{\\mathrm{A},\\mathrm{A}}^{(l)}(\\mathcal{D}_{p})-S_{\\mathrm{C},\\mathrm{A}}^{(l)}(\\mathcal{D}_{p})}\\in[0,1].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remark. $S_{\\mathrm{A,A}}^{(l)}(D_{p})=1$ . The second and third arguments in $\\rho_{\\mathrm{BEC}}$ serve as two reference models to measure the backdoor existence of the model corresponding to the first argument $f_{\\theta_{\\mathrm{D}}}$ . Denote $\\rho_{\\mathrm{BEC}}(f_{\\theta_{\\mathrm{D}}},f_{\\theta_{\\mathrm{A}}},f_{\\theta_{\\mathrm{C}}};\\mathcal{D}_{p})$ as $\\rho_{\\mathrm{BEC}}(f_{\\theta_{\\mathrm{D}}})$ for simplicity. The higher the value $\\rho_{\\mathrm{BEC}}(f_{\\theta_{\\mathrm{D}}})$ , the stronger the existence of backdoors in the model. We utilize BEC to signify backdoor existence and employ ASR to quantify the extent of backdoor activation. As shown in Fig. 1, the BEC remains consistently high across various defenses, despite backdoor activation being low. ", "page_idx": 3}, {"type": "text", "text": "3.3 Backdoor re-activation attack ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Motivated by the fact analyzed above that the original backdoor still exists in the defense model $f_{\\theta_{\\mathrm{D}}}$ , here we explore the possibility to re-activate the backdoor during inference. Since the adversary cannot modify $f_{\\theta_{\\mathrm{D}}}$ during inference, one feasible solution is to modify the original trigger $\\xi$ . Specifically, we propose to pursue a new trigger $\\xi^{\\prime}$ by perturbing $\\xi$ , i.e., $\\pmb{\\xi}^{\\prime}=\\pmb{\\xi}+\\Delta_{\\pmb{\\xi}}$ , such that $\\xi^{\\prime}$ could re-activate the original backdoor, i.e., $f_{\\pmb{\\theta}_{\\mathrm{D}}}(\\pmb{x}_{\\pmb{\\xi}^{\\prime}})=t$ . In the following, we will present how to obtain a successful trigger perturbation $\\Delta_{\\xi}$ under white-box, black-box, and transfer attack scenarios, respectively. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "White-box backdoor re-activation attack. In white-box scenario, the adversary has access to the parameters of $f$ but cannot manipulate them. In this case, we could obtain $\\Delta_{\\xi}$ by solving the constrained optimization problem $\\bar{\\operatorname*{min}_{\\|\\Delta_{\\pmb{\\xi}}\\|_{p}\\leq\\rho}{\\mathcal{L}_{t o t}(\\Delta_{\\pmb{\\xi}};\\mathcal{D}_{p},f)}}$ , where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t o t}(\\Delta_{\\xi};\\mathcal{D}_{p},f)=\\sum_{(x_{\\xi},t)\\in\\mathcal{D}_{p}}\\mathcal{L}_{\\mathrm{CE}}(f(x_{\\xi+\\Delta_{\\xi}}),t)-\\lambda\\log\\left(1-\\operatorname*{max}_{k\\neq t}\\frac{e^{f_{k}(x_{\\xi+\\Delta_{\\xi}})}}{\\sum_{i=1}^{N}e^{f_{i}(x_{\\xi+\\Delta_{\\xi}})}}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\|\\cdot\\|_{p}$ means $\\ell_{p}$ norm, $\\rho$ is the perturbation bound, $\\mathcal{L}_{\\mathrm{CE}}$ is cross-entropy loss, and $\\lambda>0$ is a hyper-parameter. This problem can be easily solved using project gradient descent (PGD) [38]. ", "page_idx": 4}, {"type": "text", "text": "Black-box backdoor re-activation attack. Although the re-activation attack under the white-box scenario is easy to implement, it may be impractical. Thus, we also consider the practical blackbox scenario, where the adversary lacks information to the defense model and can only query the model and obtain the predicted score. Consequently, the above problem (4) is no longer directly optimized by the PGD algorithm. Inspired by existing black-box adversarial attacks [1, 8], we propose a novel random search based optimization algorithm. Specifically, we extend the query-based black-box adversarial attack method Square Attack [1] that was designed for optimizing samplespecific perturbation, to solve problem (4), dubbed Universal Square Attack. Its overall procedure is summarized in Alg. 1 in Appendix. ", "page_idx": 4}, {"type": "text", "text": "Transfer-based backdoor re-activation attack. In addition to the query-based black-box attack, we also explore transfer-based attack scenario. In this scenario, the adversary trains a backdoored model $f_{\\theta_{\\mathrm{A}}}$ and releases it to downstream users. The user receives model $f_{\\theta_{\\mathrm{A}}}$ , and obtains a defense model $f_{\\theta_{\\mathrm{D}}}$ based on $f_{\\theta_{\\mathrm{A}}}$ by some post-training defense. Thus, the adversary does not know the exact defense method, but has full information about the original trigger $\\xi$ and model $f_{\\theta_{\\mathrm{A}}}$ which has same model architecture as $f_{\\theta_{\\mathrm{D}}}$ . The adversary also has restricted query limits. Consequently, leveraging transfer attacks becomes a viable strategy for attacking. The main idea is that the adversary can imitate defense process to get some defense models $f_{\\theta_{\\mathrm{D}_{i}}}$ themselves, where $i=1,\\cdot\\cdot\\cdot,M$ . Then these defense models can serve as surrogate models to generate perturbation $\\Delta_{\\xi}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta_{\\pmb{\\xi}}^{*}=\\arg\\operatorname*{min}_{\\|\\Delta_{\\pmb{\\xi}}\\|_{p}\\leq\\rho}\\sum_{i=1}^{M}\\mathcal{L}_{t o t}(\\Delta_{\\pmb{\\xi}};\\mathcal{D}_{p},f_{\\pmb{\\theta}_{\\mathrm{D}_{i}}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Overall, we propose a universal backdoor re-activation attack that aims to enhance the performance of existing backdoor attack methods during inference. We have explored three scenarios\u2014white-box attack (WBA), query-based black-box attack (BBA), and transfer attack (TA). Besides, we would like to emphasize again that the proposed attack can be naturally extended to multi-modal learning tasks, other than the classification task demonstrated above. The details are presented in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Implementation details ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Models and datasets. For image classification task, we evaluate all our attacks on three benchmark datasets CIFAR-10 [26], Tiny ImageNet [28], and GTSRB [49] over two network architectures, PreAct-ResNet18 [20] and VGG19-BN [47]. We utilize the setup in BackdoorBench [59]. For MMCL task, we use the open-sourced CLIP model from OpenAI [44] as the pre-trained model. Following the setting of CleanCLIP [3], the model is poisoned on the CC3M dataset [45] and subsequently tested through zero-shot evaluation on ImageNet-1K validation set [13]. ", "page_idx": 4}, {"type": "text", "text": "Backdoor attacks. For image classification task, we adopt seven widely used backdoor attacks including: (1) five data poisoning attack: BadNets [18], Blended [10], LF [68], SSBA [31], and ", "page_idx": 4}, {"type": "text", "text": "Trojan [37]; and (2) two training-controllable attacks: Input-Aware [40] and WaNet [41]. We follow the default attack configuration as in BackdoorBench [59] and the $0^{t h}$ label is set to be the target label. For MMCL task, we adopt four backdoor attacks including: BadNets, Blended, SIG [4], and TrojanVQA [52]. In data poisoning phase, 1500 samples out of 500K image-text pairs from CC3M dataset are poisoned and the target label is banana as in [3]. ", "page_idx": 5}, {"type": "text", "text": "Backdoor defenses. For image classification task, we adopt six state-of-the-art post-training defense methods: NC [53], NAD [30], i-BAU [67], FT-SAM [72] , SAU [58], and FST [39]. For MMCL task, we consider two defense methods: (1) FT [3]: fine-tuning the model with multimodal contrastive loss using clean dataset; and (2) CleanCLIP [3]: a fine-tuning defense method for CLIP models. All the detailed introduction about the above attack and defense methods can be found in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Implementation details. At backdoor injection phase, the poisoning ratio is set to $10\\%$ , following the configuration in BackdoorBench [59]. At defense phase, $5\\%$ clean samples are given to defend models. At backdoor re-activation phase, we consider defense models as our target model. The adversary is given $2\\%$ (i.e., 1000) poisoned samples to conduct attacks. We consider both $\\ell_{\\infty}$ and $\\ell_{2}$ norm attacks, and the perturbation bounds are set to 0.05 and 2, respectively. The loss hyper-parameter $\\lambda$ is 1 for all our experiments. For query-based black-box attack, the maximum query limit is 10,000 for each image. For transfer attack, the adversary is given $10\\%$ poisoned samples to conduct backdoor re-activation attack. The $\\ell_{2}$ norm bound is set to 1 for transfer attack. We simply assume three surrogate models can be used and we just divided these defenses into two groups: (1) NC, NAD, i-BAU; and (2) FT-SAM, SAU, FST. Specifically, we generate perturbation in each group and test the ASRs in the other group. All the ASRs are tested on testing dataset. For MMCL tasks, we assume the adversary lacks knowledge of the downstream task. Therefore, attacks are executed in the upstream task for both white-box and transfer attacks and subsequently tested in downstream zero-shot task. More details about the implementations can be found in Appendix D. We have provided the PyTorch2 implementation of our method on Github. ", "page_idx": 5}, {"type": "text", "text": "4.2 Main results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Backdoor re-activation attack. Tab. 2 shows the performance of our backdoor re-activation attack under white-box attack (WBA) and query-based black-box attack (BBA) settings in comparison with ASRs of original backdoored models (No Defense) and defense models (Defense). By observing the table, the following profound insights emerge: (1) Compared to defense models, our attacks show a striking level of efficacy. Both our WBA and BBA have exhibited an impressively absolute improvement of $76.94\\%$ and $42.95\\%$ on average, respectively when compared against defense mechanisms, which shows the effectiveness of our re-activation attack method. (2) The close performance of our WBA compared to \"No Defense\" underscores the efficacy of our backdoor re-activation mechanism, affirming the recoverability of the backdoors in defense models. By setting WBA as an upper bound for backdoor recovery, the more realistic BBA reveals substantial attack performance. Despite a gap between the two approaches, we posit that this disparity can be lessened through a sophisticated black-box attack strategy. (3) In terms of specific defenses, our attack against SAU and FST exhibits relatively poor ASRs. This suggests that SAU\u2019s backdoor removal efficiency is significant, which aligns with the subsequent analysis of Fig. 3. In contrast, FST\u2019s BBA seems comparatively subdued. It may be attributed to the reinitialized FC layers, effectively cutting backdoor activations. These insights serve as valuable pointers for crafting defense strategies in the future. ", "page_idx": 5}, {"type": "text", "text": "Backdoor re-activation attack via transfer attack. In this experiment, we group these defenses into two distinct groups: (1) weak group (NC, NAD, i-BAU) and (2) strong group (FT-SAM, SAU, FST) to better to observe the impact of defense methods on the performance of transfer-based re-activation attacks (TA). Two key findings emerged from results in Tab. 3: (1) Transfer attacks generally exhibit strong performance in comparison with results in Tab. 2. The ensemble attack strategies applied on the weak group demonstrate better attack effectiveness on strong defense models than that in BBAs. (2) Utilizing ensemble strategies on strong defense methods results in remarkably effective ASRs on weak defense models, surpassing even the efficacy of WBA in Tab. 2. This outcome raises concerns: if adversaries simulate stronger defenses to derive substitute models for launching transfer attacks, it could lead to serious security threats. ", "page_idx": 5}, {"type": "table", "img_path": "E2odGznGim/tmp/57dea1d76dab5a998cb97dd2858979a64babed304a8806cb3b817c7dc59cba20.jpg", "table_caption": ["Table 2: Performance $(\\%)$ of backdoor re-activation attack on both white-box (WBA) and blackbox (BBA) scenarios with $\\ell_{\\infty}$ -norm bound $\\rho=0.05$ against different defenses with CIFAR-10 on PreAct-ResNet18. The best results are highlighted in boldface. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "E2odGznGim/tmp/a77752be505396a8ffefb71a502b21a4f3afcf9dc5cd75290146fbfa38ad74ef.jpg", "table_caption": ["Table 3: Attack performance $(\\%)$ on target models of transfer-based re-activation attack (TA) with $\\ell_{2}$ -norm bound $\\rho=1$ against different defenses with CIFAR-10 on PreAct-ResNet18. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Effectiveness of attacks on CLIP models. Tab. 4 lists the performance of our backdoor reactivation attack under white-box attack (WBA) and transfer-based attack (TA) on the CLIP model. Our attacks yield significant improvements, with ASR enhancements of $34.87\\%$ and $43.35\\%$ on average, respectively, compared to defense models. The results for TA and WBA are very close. One possible reason is that the similarity between the FT and CleanCLIP methods leads to strong transfer performance. We advocate for the development of stronger defenses on CLIP to combat attacks. Due to space constraints, attack results and analysis on Tiny ImageNet (Tab. 12) and GTSRB (Tab. 13) datasets, and results on VGG19-BN models (Tab. 14) are provided in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "4.3 Ablation study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Influence of norm bound and norm type. We studied the impact of norm type and norm bound on the attack performance. The results are shown in (a) and (b) of Fig. 2. It can be observed that it is difficult to achieve high success rates under smaller norm bounds. However, when the norm bound is sufficiently large, the attack effectiveness converges and approaching nearly $100\\%$ for both $\\ell_{\\infty}$ -norm and $\\ell_{2}$ -norm types against all defense models. ", "page_idx": 6}, {"type": "text", "text": "Influence of the size of poisoned samples. We investigated the impact of the size of poisoned samples on attack performance for Blended attack. As shown in (c) and (d) of Fig. 2, increasing the number of training samples in WBA shows significant improvement in attack results. However, in ", "page_idx": 6}, {"type": "text", "text": "Table 4: Performance $(\\%)$ of our attack on both white-Table 5: Our attacks $(\\%)$ on defense box (WBA) and transfer-based (TA) attacks with $\\ell_{\\infty}$ -norm models in comparison with clean ones bound $\\rho=0.05$ against different defenses with ImageNet-with $\\ell_{\\infty}$ -norm bound $\\rho\\,=\\,0.05$ under 1K on CLIP. Best results are highlighted in boldface. different model structures and datasets. ", "page_idx": 6}, {"type": "table", "img_path": "E2odGznGim/tmp/86fe01c9c8262eb4f5b243f94c56ac5811d63a5a5f8ffee6df26b96596953730.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "E2odGznGim/tmp/82fc22164c9fc787ba45f1f425159792640dee506d471af2bb5f65e8729fd0da.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: (a) and (b) show attack results under different norm types $p$ and bounds $\\rho$ for WBA. (c) and (d) show attack results under different number of poisoned samples for WBA and BBA. ", "page_idx": 7}, {"type": "text", "text": "Table 6: Detection performance (TPR $\\%$ ) on dif-Table 7: Performance $(\\%)$ against test-time deferent \u27e8model, poisoned samples\u27e9pairs. fenses. ", "page_idx": 7}, {"type": "table", "img_path": "E2odGznGim/tmp/5128d9b8f47c9bb0741db773eb1b11fb7b46533923093fdcd51d93130a267f83.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "the BBA setting, the ASRs remains relatively stable and does not exhibit significant enhancements with the increase of training samples. This suggests that the difficulty in BBA lies in finding a good universal perturbation, especially when dealing with a large number of training samples. However, the successful attacks with minimal samples also highlight the significant potency of the attack method. ", "page_idx": 7}, {"type": "text", "text": "Attacks performance against clean models. To demonstrate the specific vulnerability of defense models, we contrast the performance of our attacks on the defense models in comparison with clean models. Tab. 5 provides a summary of our method\u2019s performance across all backdoor attacks and defense methods, in comparison of the ASRs on clean models. It can be observed that, although some effectiveness is achieved on the clean models, the vulnerability of defense models is significantly higher than that of the clean model, with this gap being more pronounced in particular defenses. This indicates that defense models are indeed more fragile in comparison with clean models. ", "page_idx": 7}, {"type": "text", "text": "4.4 Further analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Backdoor existence analysis. We provide more experimental demonstration on the existence of backdoors in defense models. We employ our BEC metric to quantify the existence of backdoors in all defense models and visualize the relationship between BEC and backdoor activation rates, as depicted in (a) of Fig. 3. We observe that backdoors persist across defense models, albeit with low backdoor activation rates. The BECs in SAU, SAM, and i-BAU are relatively low, while FST exhibits a notably high BEC. This contrast may stem from the former\u2019s optimization objectives resembling adversarial training, whereas the latter primarily disrupts activations through layers re-initialization. ", "page_idx": 7}, {"type": "text", "text": "Relationship between BECs and ASRs. We validate the relationship between the ASR of reactivation attack (WBA) and the residual of backdoors. We computed the Pearson Correlation Coefficients (PPC) between BECs of different defense models and their white-box ASRs among all attacks, as shown in (b) of Fig. 3. It is evident that in most cases, there is a strong correlation between the two. In other words, the more backdoors remain in models, the easier it is for attacks to succeed. Therefore, our metrics can serve as an indicator of backdoored model security. ", "page_idx": 7}, {"type": "text", "text": "Feature map visualization. Here we visualize the feature maps between different models to directly observe their similarities. Fig. 3 (c) displays the visualizations of activations from the final four convolutional layers of three models, sorted in descending order according to backdoored model\u2019s TAC value, with each subplot arranged from top to bottom. It can be observed that the defense model and backdoored model exhibit similar patterns: highlighting activations in backdoor-related neurons. This directly indicates the persistence of backdoors within defense models. ", "page_idx": 7}, {"type": "image", "img_path": "E2odGznGim/tmp/85d68b3c47a4b7a1bd6260d5083293c3339c89d576768366ca3c707f5e9f2a95.jpg", "img_caption": ["Figure 3: (a).Visualization of the correlation between backdoor activation rate and BEC. (b). Pearson correlation coefficients of ASR and BEC under different attacks. (c). Visualization of feature maps. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Attack against test-time detection and defenses. Given that our attack is conducted during the test phase, it is essential to assess whether it can evade backdoor detection and defenses at test phase. To this end, we test three test-time backdoor detection methods: SCALE-UP [19], SentiNet [12], and STRIP [17], as well as three test-time defenses: STRIP [17], ZIP [46], and SCALE-UP [19]. ", "page_idx": 8}, {"type": "text", "text": "The detection task requires two input arguments, including the model and the query datasets. We evaluate five pairs, including \u27e8the original backdoored model $f_{\\mathrm{A}}$ , the original poisoned dataset $\\mathcal{D}_{p}\\rangle$ , \u27e8the defense model with FT-SAM $f_{\\mathrm{D,FT-SAM}},D_{p}\\rangle$ , $\\langle f_{\\mathrm{D,FT-SAM}}$ , the re-activation dataset $\\mathcal{D}_{p,\\Delta\\xi}\\rangle$ , \u27e8the defense model with SAU $f_{\\mathrm{D,SAU}},\\mathcal{D}_{p})$ , $\\langle\\ensuremath{f_{\\mathrm{D,SAU}}}$ , the re-activation dataset $\\mathcal{D}_{p,\\Delta\\xi}\\rangle$ . The result in Tab. 6 shows that our attacks do not markedly increase the TPR compared to the other two pairs. More detection performance on our BBA and TA are shown in Tab. 18 in Appendix. ", "page_idx": 8}, {"type": "text", "text": "Tab. 7 shows the defense results. It shows that our attack maintains a certain level of ASR against ZIP. However, for SCALE-UP and STRIP, there is a significant drop in ASR. Meanwhile, the model\u2019s ACC is also notably low. This experiment highlights the potential for future attack method designs aimed at evading test-time defenses. Possible strategies could include techniques to better align with feature distributions of clean data and to avoid triggering excessively strong activations. ", "page_idx": 8}, {"type": "text", "text": "Attack against adaptive defense. Considering defenders are aware of adversary\u2019 strategies, they can introduce random perturbations for queries so as to disrupt the adversary\u2019s ability. We assess both adversary\u2019s ASR and the model accuracy on clean samples under varying perturbation bound. As depicted in Tab. 8, minor noise has slight impact on ASR. However, with larger noise amplitudes, despite failed attacks, the model\u2019s accuracy is significantly affected. ", "page_idx": 8}, {"type": "table", "img_path": "E2odGznGim/tmp/e4aaa3e8501bdb4d1237f42e3f0ec9b0212ad9a4ec0300b9c983878fd18d32da.jpg", "table_caption": ["Table 8: Results $(\\%)$ against adaptive defense. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Comparison among OBA, RBA, and gUAA ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To verify that our re-activation attack method finds a highly correlated backdoor with the original backdoor, and to distinguish it from general universal adversarial perturbation attack (gUAA), we systematically compare the original backdoor attack (OBA), our re-activation attack (RBA), and gUAA from three key perspectives. ", "page_idx": 8}, {"type": "text", "text": "To facilitate the understanding of our analysis, we firstly clarify the definitions and settings. OBA refers to an existing backdoor attack following the standard backdoor injection and activation pro", "page_idx": 8}, {"type": "table", "img_path": "E2odGznGim/tmp/ab068317e40e1a51f105f2fd4ae847a9331fabb01325f0d711988596d91a2bb2.jpg", "table_caption": ["Table 9: CKA scores between OBA, RBA, and gUAA. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "cess; RBA means that, given the defense model, we aim to re-activate the injected backdoor of OBA by searching for a new trigger $\\xi^{\\prime}$ , starting from original trigger $\\xi$ , based on some original poisoned samples $\\mathcal{D}_{p}$ ; gUAA refers to a targeted universal adversarial perturbation attack (same class as OBA and RBA) where, given $f_{\\theta_{\\mathrm{D}}}$ , we aim to find a perturbation starting from clean samples $\\mathcal{D}_{c}$ . The searched UAP is denoted as $\\Delta$ , and the perturbed dataset as $\\mathcal{D}_{c,\\Delta}$ . Our analyses are as follows: ", "page_idx": 8}, {"type": "table", "img_path": "E2odGznGim/tmp/0809bb725bd96cfe69c721f2f54a79cae4b0e154ccf61bafabc7ba15ceace488.jpg", "table_caption": ["Table 10: ASR $(\\%)$ of RBA and gUAA with differ-Table 11: ASR $(\\%)$ of OBA, RBA, and gUAA ent query numbers. under different $l_{\\infty}$ -norm of random noise. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "E2odGznGim/tmp/74c5095753b9145bc7a85f055c7dca713ad0684192eb3fd33f4f54cf191cd41b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "\u2022 Activation mechanism of backdoor effect: We analyze the backdoor activation mechanism in each attack. As demonstrated in Sec. 3.2, we adopt the CKA metric to measure backdoor effect similarity between models. Here we calculate the following three CKA scores: $\\begin{array}{r l r}{S_{\\mathrm{RBA,OBA}}}&{=}&{\\frac{1}{N}\\sum_{l=1}^{N}\\mathbf{CKA}(\\tilde{m}_{\\mathrm{D}}^{(l)}(\\mathcal{D}_{p,\\Delta_{\\xi}}),\\tilde{m}_{\\mathrm{A}}^{(l)}(\\mathcal{D}_{p}))}\\end{array}$ , SgUAA,OBA $=$ $\\begin{array}{r}{\\frac{1}{N}\\sum_{l=1}^{N}\\mathbf{CKA}(\\tilde{m}_{\\mathrm{D}}^{(l)}(\\mathcal{D}_{c,\\Delta}),\\tilde{m}_{\\mathrm{A}}^{(l)}(\\mathcal{D}_{p}))}\\end{array}$ , SRBA,gUAA = $\\begin{array}{r}{\\frac{1}{N}\\sum_{l=1}^{N}\\mathbf{CKA}(\\tilde{m}_{\\mathrm{D}_{-}}^{(l)}(\\mathcal{D}_{p,\\Delta_{\\xi}}),\\tilde{m}_{\\mathrm{D}}^{(l)}(\\mathcal{D}_{c,\\Delta}))}\\end{array}$ . As shown in Tab. 9, $S_{\\mathrm{RBA,OBA}}\\gg S_{\\mathrm{gUAA,OBA}}\\approx$ $S_{\\mathrm{RBA,gUAA}}$ across all attack-defense pairs. This demonstrates that the backdoor activation mechanisms between RBA and OBA are highly similar, and both differ significantly from that of gUAA. \u2022 Starting from the original trigger $\\xi$ , it is easier and faster to find a new trigger $\\xi^{\\prime}$ that achieves a high attack success rate (ASR): As shown in Tab. 10, given the same query numbers, the ASR of RBA is much higher than that of gUAA, and RBA increases in speed faster than gUAA. This indicates that RBA is much closer to OBA than gUAA. \u2022 Compared to $\\Delta$ , both the original trigger $\\xi$ and the new trigger $\\xi^{\\prime}$ are more robust to random noise: We discovered that the robustness to random noise can distinguish the trigger of an intended backdoor from the trigger of a natural backdoor (i.e., gUAP). Specifically, we perturb $\\xi,\\xi^{\\prime}$ , and $\\Delta$ with the same level of random noise and record the ASR of these attacks. As shown in Tab. 10, both OBA and RBA are more robust than gUAA. This confirms that RBA produces an intended backdoor trigger similar to OBA, rather than a gUAP. ", "page_idx": 9}, {"type": "text", "text": "In conclusion, our analyses verify that our RBA method finds a backdoor highly correlated with the original backdoor, rather than a less correlated one (new backdoor) or a general UAP (natural backdoor). Thus, we assert that our RBA effectively re-activates the original backdoor. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper illuminates the false sense of security in backdoor defenses and proposes a new threat to enhance existing backdoor attacks in inference-time. Our pioneering introduction of the backdoor existence coefficient unveils the residual presence of backdoors within defense models. Moreover, we propose a novel optimization problem to re-activate these dormant backdoors and craft distinct algorithms tailored specifically to white-box, black-box, and transfer attack scenarios. The proposed method can be integrated with existing backdoor attacks to boost their attack success rate during the inference stage. The efficacy of our method is evidenced through exhaustive evaluation on both image classification and multi-modal contrastive learning tasks. The threat revealed by this study underscores the pressing need for designing advanced defense mechanisms in the future. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work. Despite the efficacy of our proposed method, its effectiveness is limited when confronted with defenders that inject noise into each query. Promising future work is to devise more sophisticated attacks that can bypass this defenses. Another limitation is that if defenders aim to decrease both ASR and BEC, our attacks will become challenging, even though directly optimizing the BEC is not feasible. This serves as another direction for our future work. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts. As deep neural networks sourced from untrusted origins face significant risks from backdoor attacks, this study provides a meaningful exploration into the false security in backdoor defense models. This could spark further advancements in backdoor defenses. Nonetheless, the potential misuse by ill-intended entities should be cautiously considered. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by Guangdong Basic and Applied Basic Research Foundation (No. 2024B1515020095), National Natural Science Foundation of China (No. 62076213), Shenzhen Science and Technology Program under grants (No. RCYX20210609103057050), Longgang District Key Laboratory of Intelligent Digital Economy Security, the National Research Foundation, Singapore, and the CyberSG R&D Programme Office (\u201cCRPO\u201d), under the National Cybersecurity R&D Programme (\u201cNCRP\u201d), RIE2025 NCRP Funding Initiative (Award CRPO-GC1-NTU-002). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "[1] Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: a query-efficient black-box adversarial attack via random search. In European Conference on Computer Vision, 2020. 5, 17, 21   \n[2] Jiawang Bai, Kuofeng Gao, Shaobo Min, Shu-Tao Xia, Zhifeng Li, and Wei Liu. Badclip: Trigger-aware prompt learning for backdoor attacks on clip. In CVPR, 2024. 3   \n[3] Hritik Bansal, Nishad Singhi, Yu Yang, Fan Yin, Aditya Grover, and Kai-Wei Chang. Cleanclip: Mitigating data poisoning attacks in multimodal contrastive learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 112\u2013123, 2023. 3, 5, 6, 7, 16, 19, 20, 21   \n[4] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In International Conference on Image Processing, 2019. 6, 7, 20   \n[5] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, pages 2154\u20132156, 2018. 1   \n[6] Nicholas Carlini and Andreas Terzis. Poisoning and backdooring contrastive learning. In International Conference on Learning Representations, 2022. 3   \n[7] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. In Workshop on Artificial Intelligence Safety, 2019. 1, 3   \n[8] Jinghui Chen and Quanquan Gu. Rays: A ray searching method for hard-label adversarial attack. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1739\u20131747, 2020. 5   \n[9] Weixin Chen, Baoyuan Wu, and Haoqian Wang. Effective backdoor defense by exploiting sensitivity of poisoned samples. In Advances in Neural Information Processing Systems, 2022. 3   \n[10] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv e-prints, pages arXiv\u20131712, 2017. 2, 5, 7, 19, 20, 22, 23   \n[11] Zhenzhu Chen, Shang Wang, Anmin Fu, Yansong Gao, Shui Yu, and Robert H Deng. Linkbreaker: Breaking the backdoor-trigger link in dnns via neurons consistency check. IEEE Transactions on Information Forensics and Security, 17:2000\u20132014, 2022. 3   \n[12] Edward Chou, Florian Tramer, and Giancarlo Pellegrino. Sentinet: Detecting localized universal attacks against deep learning systems. In 2020 IEEE Security and Privacy Workshops (SPW), pages 48\u201354. IEEE, 2020. 9   \n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition, 2009. 5, 19   \n[14] Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, and Jun Zhu. Black-box detection of backdoor attacks with limited information and data. In International Conference on Computer Vision, 2021. 1   \n[15] Kuofeng Gao, Jiawang Bai, Bin Chen, Dongxian Wu, and Shu-Tao Xia. Backdoor attack on hash-based image retrieval via clean-label data poisoning. In BMVC, 2023. 2   \n[16] Kuofeng Gao, Jiawang Bai, Baoyuan Wu, Mengxi Ya, and Shu-Tao Xia. Imperceptible and robust backdoor attack in 3d point cloud. IEEE Transactions on Information Forensics and Security, 19:1267\u20131282, 2023. 2   \n[17] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the 35th annual computer security applications conference, pages 113\u2013125, 2019. 9   \n[18] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks. IEEE Access, 7:47230\u201347244, 2019. 1, 2, 5, 7, 19, 20, 22, 23   \n[19] Junfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, and Cong Liu. Scale-up: An efficient black-box input-level backdoor detection via analyzing scaled prediction consistency. In The Eleventh International Conference on Learning Representations. 9 ", "page_idx": 11}, {"type": "text", "text": "[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, 2016. 5 ", "page_idx": 12}, {"type": "text", "text": "[21] Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. In International Conference on Learning Representations, 2022. 3 ", "page_idx": 12}, {"type": "text", "text": "[22] Wenbo Jiang, Hongwei Li, Guowen Xu, and Tianwei Zhang. Color backdoor: A robust poisoning attack in color space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8133\u20138142, 2023. 2 ", "page_idx": 12}, {"type": "text", "text": "[23] Paramjit Kaur, Kewal Krishan, Suresh K Sharma, and Tanuj Kanchan. Facial-recognition algorithms: A literature review. Medicine, Science and the Law, 60(2):131\u2013139, 2020. 1 ", "page_idx": 12}, {"type": "text", "text": "[24] Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, and Aleksander Madry. Rethinking backdoor attacks. In International Conference on Machine Learning, pages 16216\u201316236. PMLR, 2023. 3 ", "page_idx": 12}, {"type": "text", "text": "[25] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International conference on machine learning, pages 3519\u20133529. PMLR, 2019. 4, 18, 19 ", "page_idx": 12}, {"type": "text", "text": "[26] Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009. 5, 19 [27] Ram Shankar Siva Kumar, Magnus Nystr\u00f6m, John Lambert, Andrew Marshall, Mario Goertzel, Andi Comissoneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry perspectives. In 2020 IEEE security and privacy workshops (SPW), pages 69\u201375. IEEE, 2020. 1 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[28] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 2015. 5, 19 [29] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learning: Training clean models on poisoned data. In Conference on Neural Information Processing Systems, 2021. 3 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[30] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks. In International Conference on Learning Representations, 2021. 1, 2, 6, 7, 21, 22, 23 ", "page_idx": 12}, {"type": "text", "text": "[31] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. In International Conference on Computer Vision, 2021. 1, 2, 5, 7, 20, 22, 23 ", "page_idx": 12}, {"type": "text", "text": "[32] Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, and Xiaochun Cao. Vl-trojan: Multimodal instruction backdoor attacks against autoregressive visual language models. arXiv preprint arXiv:2402.13851, 2024. 3 ", "page_idx": 12}, {"type": "text", "text": "[33] Siyuan Liang, Jiawei Liang, Tianyu Pang, Chao Du, Aishan Liu, Ee-Chien Chang, and Xiaochun Cao. Revisiting backdoor attacks against large vision-language models. arXiv preprint arXiv:2406.18844, 2024. 3 ", "page_idx": 12}, {"type": "text", "text": "[34] Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, and Xiaochun Cao. Unlearning backdoor threats: Enhancing backdoor defense in multimodal contrastive learning via local token unlearning. arXiv preprint arXiv:2403.16257, 2024. 3 ", "page_idx": 12}, {"type": "text", "text": "[35] Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, and Ee-Chien Chang. Badclip: Dualembedding guided backdoor attack on multimodal contrastive learning. arXiv preprint arXiv:2311.12075, 2023. 3 ", "page_idx": 12}, {"type": "text", "text": "[36] Liangkai Liu, Sidi Lu, Ren Zhong, Baofu Wu, Yongtao Yao, Qingyang Zhang, and Weisong Shi. Computing systems for autonomous driving: State of the art and challenges. IEEE Internet of Things Journal, 8(8):6469\u20136486, 2020. 1 ", "page_idx": 12}, {"type": "text", "text": "[37] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. In Network and Distributed System Security Symposium, 2018. 2, 6, 7, 20, 22, 23 ", "page_idx": 12}, {"type": "text", "text": "[38] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. 5, 17 ", "page_idx": 12}, {"type": "text", "text": "[39] Rui Min, Zeyu Qin, Li Shen, and Minhao Cheng. Towards stable backdoor purification through feature shift tuning. Advances in Neural Information Processing Systems, 36, 2024. 3, 6, 7, 9, 21, 22, 23   \n[40] Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. In Conference on Neural Information Processing Systems, 2020. 2, 6, 7, 20, 22, 23   \n[41] Tuan Anh Nguyen and Anh Tuan Tran. Wanet - imperceptible warping-based backdoor attack. In International Conference on Learning Representations, 2021. 2, 6, 7, 20, 22, 23   \n[42] Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Revisiting the assumption of latent separability for backdoor defenses. In The eleventh international conference on learning representations, 2022. 3   \n[43] Xiangyu Qi, Tinghao Xie, Jiachen T Wang, Tong Wu, Saeed Mahloujifar, and Prateek Mittal. Towards a proactive $\\{{\\bf M}{\\bf L}\\}$ approach for detecting backdoor poison samples. In 32nd USENIX Security Symposium (USENIX Security 23), pages 1685\u20131702, 2023. 26   \n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021. 5   \n[45] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018. 5, 19   \n[46] Yucheng Shi, Mengnan Du, Xuansheng Wu, Zihan Guan, Jin Sun, and Ninghao Liu. Black-box backdoor defense via zero-shot image purification. Advances in Neural Information Processing Systems, 36:57336\u2013 57366, 2023. 9   \n[47] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. 5   \n[48] Zhengyao Song, Yongqiang Li, Danni Yuan, Li Liu, Shaokui Wei, and Baoyuan Wu. Wpda: Frequencybased backdoor attack with wavelet packet decomposition. arXiv preprint arXiv:2401.13578, 2024. 2   \n[49] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In international joint conference on neural networks, 2011. 5, 19   \n[50] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. Advances in neural information processing systems, 31, 2018. 1   \n[51] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(11), 2008. 25   \n[52] Matthew Walmer, Karan Sikka, Indranil Sur, Abhinav Shrivastava, and Susmit Jha. Dual-key multimodal backdoors for visual question answering. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 15375\u201315385, 2022. 3, 6, 7, 21   \n[53] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In Symposium on Security and Privacy, 2019. 1, 3, 6, 7, 21, 22, 23   \n[54] Hang Wang, Zhen Xiang, David J Miller, and George Kesidis. Mm-bd: Post-training detection of backdoor attacks with arbitrary backdoor pattern types using a maximum margin statistic. In 2024 IEEE Symposium on Security and Privacy (SP), pages 15\u201315. IEEE Computer Society, 2023. 3   \n[55] Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, and Baoyuan Wu. Versatile backdoor attack with visible, semantic, sample-specific, and compatible triggers. arXiv preprint arXiv:2306.00816, 2023. 2   \n[56] Zhenting Wang, Kai Mei, Juan Zhai, and Shiqing Ma. Unicorn: A unified backdoor trigger inversion framework. In International Conference on Learning Representations, 2023. 1   \n[57] Shaokui Wei, Hongyuan Zha, and Baoyuan Wu. Mitigating backdoor attack by injecting proactive defensive backdoor. arXiv preprint arXiv:2405.16112, 2024. 3   \n[58] Shaokui Wei, Mingda Zhang, Hongyuan Zha, and Baoyuan Wu. Shared adversarial unlearning: Backdoor mitigation by unlearning shared adversarial examples. Advances in Neural Information Processing Systems, 36, 2024. 2, 6, 7, 9, 21, 22, 23   \n[59] Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, and Chao Shen. Backdoorbench: A comprehensive benchmark of backdoor learning. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 1, 2, 5, 6, 19, 20, 21   \n[60] Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, and Chao Shen. Backdoorbench: A comprehensive benchmark and analysis of backdoor learning. arXiv preprint arXiv:2401.15002, 2024. 2   \n[61] Baoyuan Wu, Shaokui Wei, Mingli Zhu, Meixi Zheng, Zihao Zhu, Mingda Zhang, Hongrui Chen, Danni Yuan, Li Liu, and Qingshan Liu. Defenses in adversarial machine learning: A survey. arXiv preprint arXiv:2312.08890, 2023. 3   \n[62] Baoyuan Wu, Zihao Zhu, Li Liu, Qingshan Liu, Zhaofeng He, and Siwei Lyu. Attacks in adversarial machine learning: A systematic survey from the life-cycle perspective. arXiv preprint arXiv:2302.09457, 2023. 1, 2, 3   \n[63] Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. Rethinking infonce: How many negative samples do you need? arXiv preprint arXiv:2105.13003, 2021. 16   \n[64] Dongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. In Conference on Neural Information Processing Systems, 2021. 1, 3   \n[65] Yuan Xun, Siyuan Liang, Xiaojun Jia, Xinwei Liu, and Xiaochun Cao. Ta-cleaner: A fine-grained text alignment backdoor defense strategy for multimodal contrastive learning. arXiv preprint arXiv:2409.17601, 2024. 3   \n[66] Wenhan Yang, Jingdong Gao, and Baharan Mirzasoleiman. Robust contrastive language-image pretraining against data poisoning and backdoor attacks. Advances in Neural Information Processing Systems, 36, 2024. 3   \n[67] Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of backdoors via implicit hypergradient. In International Conference on Learning Representations, 2022. 1, 2, 6, 7, 9, 21, 22, 23   \n[68] Yi Zeng, Won Park, Z. Morley Mao, and Ruoxi Jia. Rethinking the backdoor attacks\u2019 triggers: A frequency perspective. In International Conference on Computer Vision, 2021. 2, 5, 7, 20, 22, 23   \n[69] Xiaoyu Zhang, Yulin Jin, Tao Wang, Jian Lou, and Xiaofeng Chen. Purifier: Plug-and-play backdoor mitigation for pre-trained models via anomaly activation suppression. In Proceedings of the 30th ACM International Conference on Multimedia, pages 4291\u20134299, 2022. 3   \n[70] Zhendong Zhao, Xiaojun Chen, Yuexin Xuan, Ye Dong, Dakui Wang, and Kaitai Liang. Defeat: Deep hidden feature backdoor attacks by imperceptible perturbation and latent representation constraints. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15213\u2013 15222, 2022. 2   \n[71] Runkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Data-free backdoor removal based on channel lipschitzness. In European Conference on Computer Vision, 2022. 1, 4, 18   \n[72] Mingli Zhu, Shaokui Wei, Li Shen, Yanbo Fan, and Baoyuan Wu. Enhancing fine-tuning based backdoor defense with sharpness-aware minimization. In International Conference on Computer Vision, 2023. 3, 6, 7, 9, 21, 22, 23   \n[73] Mingli Zhu, Shaokui Wei, Hongyuan Zha, and Baoyuan Wu. Neural polarizer: A lightweight and effective backdoor defense via purifying poisoned features. Advances in Neural Information Processing Systems, 36, 2024. 3   \n[74] Rui Zhu, Di Tang, Siyuan Tang, XiaoFeng Wang, and Haixu Tang. Selective amnesia: On efficient, high-fidelity and blind suppression of backdoor effects in trojaned machine learning models. In 2023 IEEE Symposium on Security and Privacy (SP), pages 1\u201319. IEEE, 2023. 26   \n[75] Zihao Zhu, Mingda Zhang, Shaokui Wei, Li Shen, Yanbo Fan, and Baoyuan Wu. Boosting backdoor attack with a learnable poisoning sample selection strategy. arXiv preprint arXiv:2307.07328, 2023. 2   \n[76] Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, and Baoyuan Wu. Vdc: Versatile data cleanser for detecting dirty samples via visual-linguistic inconsistency. In The Twelfth International Conference on Learning Representations, 2023. 3 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Structure of Appendix. We provide more analysis and experimental results in Appendix, which includes: (1) Formulation of multi-modal contrastive learning, backdoor attacks and our re-activation attack for MMCL in Appendix A. (2) More description and algorithms details in Appendix B. (3) Introduction of TAC and CKA in Appendix C. (4) Experimental implementation details in Appendix D. (5) More experimental results in Appendix E. (6) Running time analysis in Appendix F. (7) Visualization in Appendix G. (8) Additional experimental results in Appendix H. ", "page_idx": 15}, {"type": "text", "text": "A Backdoor multi-modal contrastive learning ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Formulation of multi-modal contrastive learning task. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For multi-modal contrastive learning task, the training dataset is image-text pairs $\\mathcal{D}=$ $\\{(\\pmb{v}^{(i)},\\pmb{t}^{(i)})\\}_{i=1}^{n}\\,\\subseteq\\,\\mathcal{V}\\times\\mathcal{T}$ , where $\\mathcal{V}\\subset\\mathbb{R}^{d_{\\pmb{v}}}$ and $\\mathcal{T}\\subset\\mathbb{R}^{d_{t}}$ are image space and text space, respectively. For the network, we choose CLIP as our primary MMCL model for the attack. CLIP is composed of a visual encoder $f_{\\theta_{v}}:\\mathcal{V}\\to\\mathbb{R}^{d}$ and a textual encoder $f_{\\theta_{t}}:\\mathcal{T}\\to\\mathbb{R}^{d}$ , each with parameters $\\theta_{v}$ and $\\theta_{t}$ representing their respective encoders. Denote the image embedding and text embedding as ${\\pmb v}_{e}^{(i)}\\,=\\,\\bar{f_{{\\pmb\\theta}_{v}}}({\\pmb v}^{(i)}),\\bar{\\pmb t}_{e}^{(i)}\\,=\\,\\bar{f_{{\\pmb\\theta}_{t}}}({\\pmb t}^{(i)})$ , respectively, for convenience. Given a batch of training pairs $\\{(\\boldsymbol{v}^{(i)},\\boldsymbol{t}^{(i)})\\}_{i=1}^{n_{1}}$ , CLIP is optimized using the InfoNCE loss [63] as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\{\\pmb{\\theta}_{v},\\pmb{\\theta}_{t}\\}}-\\sum_{i=1}^{n_{1}}\\log\\frac{\\exp\\left(\\pmb{v}_{e}^{(i)}\\cdot\\pmb{t}_{e}^{(i)}/\\tau\\right)}{\\sum_{j=1}^{n_{1}}\\exp\\left(\\pmb{v}_{e}^{(i)}\\cdot\\pmb{t}_{e}^{(j)}/\\tau\\right)},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\tau$ is the temperature parameter. Given an input image $\\pmb{v}^{(i)}$ , denote the output text of the model be $h_{\\Theta}(\\pmb{v}^{(i)})$ for convenience, where $\\boldsymbol{\\Theta}=\\{\\pmb{\\theta}_{v},\\pmb{\\theta}_{t}\\}$ . ", "page_idx": 15}, {"type": "text", "text": "A.2 Backdoor attacks for multi-modal contrastive learning. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For MMCL task, backdoor attacks could also be divided into data poisoning attack and training controllable attack. For data poisoning attack, an adversary creates poisoning pairs $({\\pmb v}^{(i)}+{\\pmb\\xi},T)$ by patching a backdoor trigger $\\xi$ on the image $\\pmb{v}^{(i)}$ and revising the corresponding label into the target label $T$ (for example, \"a photo of banana\" in [3] and in our work). In training controllable backdoor attack, the adversary can control the training process to inject backdoors into the model. The goal of the adversary is to train a poisoned model such that $h_{\\Theta_{\\mathrm{A}}}\\big(\\pmb{v}^{(i)}\\big)=\\pmb{t}^{(i)}$ and $h_{\\Theta_{\\mathrm{A}}}(\\pmb{v}^{(i)}+\\pmb{\\xi})=\\bar{T}$ . And the goal of the defender is to purify the poisoned model such that the new model performs normally as: $\\bar{h}_{\\Theta_{\\mathrm{D}}}(\\pmb{v}^{(i)})=\\pmb{t}^{(i)}$ and $h_{\\Theta_{\\mathrm{D}}}({\\pmb v}^{(i)}+{\\pmb\\xi})\\neq T$ in inference time. Denote the encoder of poisoned image and the target label as $(v_{\\xi}^{(i)})_{e}$ and $T_{e}$ , respectively for convenience. Our goal is to search for a perturbation $\\Delta_{\\xi}^{*}$ onto the original trigger $\\xi$ such that $h_{\\Theta_{\\mathrm{D}}}(\\pmb{v}^{(i)}+\\pmb{\\xi}+\\Delta_{\\pmb{\\xi}}^{*})=T$ ", "page_idx": 15}, {"type": "text", "text": "Existing backdoor attacks primarily focus on achieving high attack success rates (ASR) in backdoor injection stages (I and II), with little consideration for the defensive impact in stage III. Given the failures of ${\\pmb v}^{(i)}+{\\pmb\\xi}$ in attacking $h_{\\Theta_{D}}$ , our work focuses on the re-activation attack in stage $I V$ (please refer to Sec. 3.2 for formal definition of different stages of backdoors). ", "page_idx": 15}, {"type": "text", "text": "A.3 Re-activation attacks for multi-modal contrastive learning. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we introduce our optimization formulation to learn the new trigger $\\xi^{\\prime}=\\xi+\\Delta_{\\xi}$ . Since CLIP uses multi-modal contrastive learning instead of supervised learning to train the model, we also optimize the perturbation $\\Delta_{\\xi}$ by optimizing it with multi-modal contrastive learning loss. Given a number of $n_{p}$ pairs $(v_{\\xi}^{(i)},T)$ from the poisoned dataset $\\mathcal{D}_{p}$ and $n_{c}$ pairs $(\\pmb{v}^{(j)},\\pmb{t}^{(j)})\\in\\mathcal{D}_{c}$ from the clean dataset $\\mathcal{D}_{c}$ , the optimization problem is formulated as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta_{\\xi}^{*}=\\mathop{\\mathrm{arg\\,min}}_{\\|\\Delta_{\\xi}\\|_{\\mathfrak{p}}\\leq\\rho}-\\sum_{(\\mathfrak{p}_{\\xi}^{(i)},T)\\in\\mathcal{D}_{p}}\\log\\frac{\\exp\\left((v_{\\xi}^{(i)})_{e}\\cdot T_{e}/\\tau\\right)}{\\exp\\left((v_{\\xi}^{(i)})_{e}\\cdot T_{e}/\\tau\\right)+\\sum_{(v^{(j)},t)\\in\\mathcal{D}_{c}}\\exp\\left((v_{\\xi}^{(i)})_{e}\\cdot t_{e}^{(j)}/\\tau\\right)},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "1: Input: Defense model $f$ , training dataset $\\mathcal{D}_{p}$ , image shape $c,h,w$ , norm $p$ , perturbation bound   \n$\\rho$ , target label $t\\in{1,\\ldots,K}$ , number of iterations $N$ , termination condition $\\epsilon$ .   \n2: Output: Perturbation $\\Delta_{\\xi}^{*}$ as in Eq. 4.   \n3: $\\hat{\\pmb{x}}\\leftarrow\\pmb{x}+\\operatorname*{init}(\\Delta_{\\pmb{\\xi}})$ for $\\pmb{x}^{'}\\!\\in\\mathcal{D}_{p},\\quad l^{*}\\gets\\mathcal{L}_{t o t}(\\mathcal{D}_{p},\\Delta_{\\pmb{\\xi}}).$   \n4: for $i=0,...,N-1$ do   \n5: if ASR > 1 \u2212\u03f5 then return \u2206\u03be.   \n6: else   \n7: $h^{(i)}\\leftarrow$ side length of the square to modify (according to some schedule [1]);   \n8: $\\Delta_{\\pmb{\\xi}}^{\\mathrm{new}}\\sim P\\left(\\rho,\\bar{h^{(i)}},w,c,\\Delta_{\\pmb{\\xi}}^{\\bar{\\ i}},\\hat{\\pmb{x}},\\pmb{x}\\right)$ for $\\pmb{x}\\in\\mathcal{D}_{p}$ (see Appendix B for details);   \n9: $\\hat{x}_{\\mathrm{new}}~\\gets~\\operatorname{Project}{\\hat{x}}+\\Delta_{\\pmb{\\xi}}^{\\mathrm{new}}$ onto $\\left\\{z\\in\\mathbb{R}^{d}:\\|z-x\\|_{p}\\leq\\rho\\right\\}\\cap[0,1]^{d}$ for $\\pmb{x}\\in\\mathcal{D}_{p}$ ;   \n10: $l_{\\mathrm{new}}\\,\\gets\\mathcal{L}_{t o t}(\\hat{\\mathbf{x}}_{\\mathrm{new}}\\,,t)$ for $\\pmb{x}\\in\\mathcal{D}_{p}$ ;   \n11: if $l_{\\mathrm{new}}~<l^{\\ast}$ then $\\Delta_{\\xi}\\leftarrow\\Delta_{\\xi}^{\\mathrm{new}}$ , $l^{\\bar{*}}\\leftarrow l_{\\mathrm{new}}$ , compute ASR;   \n12: $i\\gets i+1$ ;   \n13: end if   \n14: end for   \n15: return $\\Delta_{\\xi}^{*}$ . ", "page_idx": 16}, {"type": "text", "text": "where $\\|\\cdot\\|_{p}$ means $\\ell_{p}$ norm, and $\\rho$ is the perturbation bound. This problem can be solved using project gradient descent (PGD) [38] algorithm. Then the optimized $\\Delta_{\\xi}^{*}$ is attached to poisoned samples and the ASR is the probability of successful attacks out of the total number of new poisoned samples. ", "page_idx": 16}, {"type": "text", "text": "B Algorithms details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this work, we provide some description and details of our attack algorithms. ", "page_idx": 16}, {"type": "text", "text": "White-box attack setting. As shown in Eq. 4, this is a constrained optimization problem, which can be solved by using the classical project gradient descent (PGD) [38] algorithm to solve it. The main idea of PGD involves updating the perturbation using stochastic gradient descent in the initial step. Subsequently, in the following stage, the perturbation is constrained within the $\\rho$ -ball employing $\\ell_{p}$ norm projection. We provide the algorithm description in Alg. 2. For additional insights, please refer to [38] for more details. ", "page_idx": 16}, {"type": "text", "text": "Black-box attack setting. In this work, to solve our optimization problem in black-box setting, we utilize a randomized search strategy as emphasized in Square Attack [1]. Square Attack utilizes a randomized search scheme where it selects localized square-shaped updates at random positions. This approach ensures that in each iteration, the perturbation is positioned near the boundary of the feasible set. A significant difference between our attack and Square Attack lies in their objective: Square Attack searches for a perturbation for each image, terminating the query upon successful attack, while our objective is to discover a highly generalizable universal perturbation to restore the effectiveness of the backdoor utility. Therefore, we extend it to a universal Square Attack approach: ", "page_idx": 16}, {"type": "text", "text": "1. Firstly, initialize a universal perturbation.   \n2. In each iteration, we randomly update our perturbation following the strategy in Square Attack. Apply the perturbation onto the image and then query the model with the new images.   \n3. Compare the loss: if the current loss is lower than the best loss, update the perturbation; otherwise, do not update and restart the search. ", "page_idx": 16}, {"type": "text", "text": "The above three steps represent the main concept of our algorithm. Details on the specific square update technique can be found in work [1]. ", "page_idx": 16}, {"type": "text", "text": "Transfer attack setting. The main idea of the transfer attack is to compute the averaged loss across models for each mini-batch. The detailed algorithm is shown in Alg. 3. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 White-box Backdoor Re-Activation Attack (WBA) ", "page_idx": 17}, {"type": "text", "text": "1: Input: Defense model $f$ , training dataset $\\mathcal{D}_{p}$ , norm $p$ , perturbation bound $\\rho$ , target label   \n$t\\in{1,\\ldots,K}$ , number of iterations $N$ .   \n2: Output: Perturbation $\\Delta_{\\xi}^{*}$ as in Eq. 4.   \n3: initialize $(\\Delta_{\\xi})$ .   \n4: for $i=0,...,N-1$ do   \n5: for mini-batch $\\mathcal{B}=\\{(\\pmb{x}_{\\pmb{\\xi}}^{i},t)\\}_{i=1}^{b}\\subset\\mathcal{D}_{p}$ do   \n6: Given $f$ and input $\\{(\\pmb{x}_{\\xi}^{i}+\\Delta_{\\xi},t)\\}_{i=1}^{b}$ , compute the loss $l$ of Eq. 4;   \n7: Update $\\Delta_{\\xi}$ by minimizing $l$ via PGD algorithm;   \n8: end for   \n9: end for   \n10: return $\\Delta_{\\xi}^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "Algorithm 3 Re-Activation Attack via Transfer Attack (TA) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1: Input: Surrogate models $f_{m},m=1,\\cdots\\,,M$ , training dataset $\\mathcal{D}_{p}$ , norm $p$ , perturbation bound   \n$\\rho$ , target label $t\\in{1,\\ldots,K}$ , number of iterations $N$ .   \n2: Output: Perturbation $\\Delta_{\\xi}^{*}$ as in Eq. 5.   \n3: initialize $(\\Delta_{\\xi})$ .   \n4: for $i=0,...,N-1$ do   \n5: for mini-batch $\\mathcal{B}=\\{(\\pmb{x}_{\\pmb{\\xi}}^{i},t)\\}_{i=1}^{b}\\subset\\mathcal{D}_{p}$ do   \n6: $l=0$ ;   \n7: for $m=0,...,M-1$ do   \n8: Given $f_{m}$ and input $\\{(\\pmb{x}_{\\xi}^{i}+\\Delta_{\\xi},t)\\}_{i=1}^{b}$ , compute the total loss $l_{m}$ of Eq. 5;   \n9: $l\\leftarrow l+l_{m}$ ;   \n10: end for   \n11: Update $\\Delta_{\\xi}$ by minimizing $l$ via PGD algorithm;   \n12: end for   \n13: end for   \n14: return $\\Delta_{\\xi}^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "C Introduction of TAC and CKA ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We provide the detailed introduction of Trigger-activated Change (TAC) [71] and Centered Kernel Alignment (CKA) [25] in this section. ", "page_idx": 17}, {"type": "text", "text": "Trigger-activated Change. To measure the correlation of neurons with backdoors, Zheng et al. [71] proposed the TAC metric to quantify the correlation between the impact of backdoors and neurons. Given the poisoned dataset $\\mathcal{D}_{p}=\\{(\\pmb{x}_{\\xi}^{(i)},y^{(i)})\\}$ , let the original clean dataset of $\\mathcal{D}_{p}$ to be $\\mathcal{D}_{c}$ , i.e., $\\mathcal{D}_{c}=\\{(\\pmb{x}^{(i)},y^{(i)})|\\pmb{x}_{\\pmb{\\xi}}^{(i)}\\in\\mathcal{D}_{p}\\}$ . Then the TAC can be computed as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nT A C_{k}^{(l)}(\\mathcal{D}_{p},\\mathcal{D}_{c})=\\frac{1}{|\\mathcal{D}_{p}|}\\sum_{({\\boldsymbol x}_{\\xi},{\\boldsymbol x})\\in(\\mathcal{D}_{p},\\mathcal{D}_{c})}\\left\\|f_{k}^{(l)}({\\boldsymbol x})-f_{k}^{(l)}({\\boldsymbol x}_{\\xi})\\right\\|_{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $k$ is the index of channel of the $l^{t h}$ layer. A higher TAC value assigned to a neuron indicates a stronger association with backdoors. In this work, with this metric, we first assign each neuron with a TAC value. In order to select neurons relevant to backdoors and considering their sparsity nature, the top $10\\%$ of neurons based on their descending TAC values are chosen as the backdoor related neurons. Then the Backdoor Existence Coefficient can be computed accordingly. ", "page_idx": 17}, {"type": "text", "text": "Centered Kernel Alignment. The Centered Kernel Alignment (CKA) [25] measures the similarity between representations, which utilizes HSIC to measure the independence between two distributions. It quantifies how well neural networks preserve similarity relations in the data across different layers. It is a valuable tool in feature analysis and understanding DNNs especially for high-dimensional features. In this work we employ CKA to quantify the similarity between features in different networks. As the work [25] shows, the Centered Kernel Alignment (CKA) is defined as follows: Let $X\\in\\mathbb{R}^{n\\times d}$ and $Y\\in\\mathbb{R}^{n\\times d}$ be two representations from neural networks, where $n$ represent number of samples and $d$ is the feature dimension. The empirical estimator of Hilbert-Schmidt Independence Criterion (HSIC) is defined as: ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{HSIC}(K,L)=\\frac{1}{(n-1)^{2}}\\mathrm{tr}(K H L H),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $H$ is the centering matrix $H_{n}\\,=\\,I_{n}\\,-\\,{\\textstyle{\\frac{1}{n}}}\\mathbf{1}\\mathbf{1}^{\\mathrm{T}}$ . The $K$ and $H$ are linear kernels: $K_{i j}\\,=$ $k(\\mathbf{x}_{i},\\mathbf{y}_{j})=\\mathbf{x}_{i}^{\\mathrm{{T}}}\\mathbf{y}_{i},L_{i j}=l(\\mathbf{x}_{i},\\mathbf{y}_{j})=\\mathbf{x}_{i}^{\\mathrm{{T}}}\\mathbf{y}_{i}$ as defined in [25]. Then the Centered Kernel Alignment is defined as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{CKA}(K,L)={\\frac{\\mathrm{HSIC}(K,L)}{\\sqrt{\\mathrm{HSIC}(K,K)\\,\\mathrm{HSIC}(L,L)}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "More details could be found in [25]. ", "page_idx": 18}, {"type": "text", "text": "D Experimental implementation details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we delve into the implementation details, covering the evaluation datasets, specifics of the attacks and defenses compared, and implementation of our proposed methods. All experiments are executed five times with varying random seeds and the averaged results are displayed in this work. ", "page_idx": 18}, {"type": "text", "text": "D.1 Datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For image classification task, we use three benchmark datasets: CIFAR-10 [26], Tiny ImageNet [28], and GTSRB [49] to assess the performance of our approach, following the benchmarks outlined in [59]. For MMCL task, a subset of CC3M dataset [45] is selected for backdoor injection and the poisoned models are tested through zero-shot evaluation on ImageNet-1K validation set [13]. All dataset splits are aligned in our experiments. ", "page_idx": 18}, {"type": "text", "text": "\u2022 CIFAR-10: Total number of 60,000 images distributed among ten classes, with 5,000 images per class in the training set and 1,000 images per class in the testing set. Each image in CIFAR-10 is sized $32\\times32$ pixels.   \n\u2022 Tiny ImageNet: A subset of ImageNet [13] containing 200 classes, 500 training samples and 50 testing samples per class. Each image in Tiny ImageNet is sized $64\\times64$ pixels.   \n\u2022 GTSRB: A total of 39,209 training images and 12,630 testing images among 43 classes. Each image in GTSRB is sized $32\\times32$ pixels.   \n\u2022 CC3M: The CC3M dataset has about 3300K, 15K, 12K image-text pairs for the training, validation, and testing dataset, respectively. Each image in CC3M is sized $224\\times224$ pixels. Following [3], 500K image-text pairs from the CC3M are selected in backdoor injection phase.   \n\u2022 ImageNet-1K: the ImageNet-1K dataset is a subset of ImageNet dataset, which has a total of 1000 classes. Each image in ImageNet-1K is sized $224\\times224$ pixels. ", "page_idx": 18}, {"type": "text", "text": "D.2 Backdoor attack details. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We introduce the different backdoor attack methods first, followed by the experimental settings. ", "page_idx": 18}, {"type": "text", "text": "Fig. 4 and 5 show the visualization of poisoned samples in comparison with clean image for different backdoor attacks on CIFAR-10 and ImageNet-1K dataset, respectively. The attack details for image classification task are as follows: ", "page_idx": 18}, {"type": "text", "text": "\u2022 BadNets [18]: BadNets is trigger-additive attack which inserts a patch of fixed pattern (a $3\\times3$ white square patch in our work) to replace some pixels in the image. The patch size is $3\\times3$ on CIFAR-10 and GTSRB, and $6\\times6$ on Tiny ImageNet, following BackdoorBench. \u2022 Blended backdoor attack (Blended) [10]: Blended attack blends a pre-defined image (Hello Kitty in our work) with the original image. The blend coefficient $\\alpha$ is 0.2, following BackdoorBench. ", "page_idx": 18}, {"type": "image", "img_path": "E2odGznGim/tmp/ce2c8e06565ed22da4d7c66ad6392b48016732ffa9bc9109549186fe02c9d240.jpg", "img_caption": ["Figure 4: Visualization of poisoned samples for different backdoor attacks on CIFAR-10 dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "E2odGznGim/tmp/bc99780efbc5fcdb508a1d0c4f1845668775feb01c70165c28c5d0740e9e0a2e.jpg", "img_caption": ["Figure 5: Visualization of poisoned samples for different backdoor attacks on ImageNet-1K dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "\u2022 Input-aware dynamic backdoor attack (Input-Aware) [40]: Input-Aware is a trainingcontrollable attack that first learns a trigger generator by adversarial training. Then the generator is used to produce sample-specific triggers during model training. \u2022 Low frequency attack (LF) [68]: LF first learns a universal adversarial perturbation (UAP) and fliters the high-frequency artifacts. Then the flitered UAP is the trigger and patched onto the clean samples to generate poisoned samples. \u2022 Sample-specific backdoor attack (SSBA) [31]: SSBA first trains an autoencoder. Then the autoencoder is used to fuse triggers with clean samples to generate poisoned samples. \u2022 Trojan backdoor attack (Trojan) [37]: Trojan first learns a universal adversarial perturbation (UAP), and then patches it onto the clean samples to generate poisoned samples. \u2022 Warping-based poisoned networks (WaNet) [41]: WaNet first defines a warping function to perturb the clean samples to generate poisoned samples. Then the adversary controls the training process to make sure the model learns the specific warping. ", "page_idx": 19}, {"type": "text", "text": "More details can be found in BackdoorBench [59]. ", "page_idx": 19}, {"type": "text", "text": "For MMCL task, we follow CleanCLIP\u2019s settings[3]. The attack details for MMCL task are as follows: ", "page_idx": 19}, {"type": "text", "text": "\u2022 BadNets [18]: BadNets is trigger-additive attack which inserts a patch of fixed pattern (a $16\\times16$ random noise patch in our work) to replace some pixels in the image.   \n\u2022 Blended backdoor attack (Blended) [10]: Blended attack blends a pre-defined image (a global random noise patch in our work) with the original image. The blend coefficient $\\alpha$ is 0.2.   \n\u2022 SIG [4]: SIG attack designs a sine wave pattern noise as a trigger, which has a same size with the image. The blend coefficient $\\alpha$ is 0.2. ", "page_idx": 19}, {"type": "text", "text": "\u2022 TrojanVQA [52]: TrojanVQA is a training-controllable attack in which the adversary utilizes both modalities to generate triggers, which has a size of $16\\times16$ . ", "page_idx": 20}, {"type": "text", "text": "To poison image classification task, we use a poisoned dataset with $10\\%$ poisoning ratio to train the poisoned model. To poison the MMCL model, we start with the pre-trained CLIP model which is trained on 400M image-text pairs. After that, a total of 500K image-text pairs within which 1500 samples are poisoned pairs is used for backdoor injection. The model is trained for 10 epochs with a learning rate of 1e-6, and a batch size of 128. ", "page_idx": 20}, {"type": "text", "text": "D.3 Backdoor defense details. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We introduce the different backdoor defense methods in this section. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Neural cleanse (NC) [53]: NC first searches for a minimal UAP to detect backdoors. If the model is detected as a backdoor model, it purifies the model by unlearning the optimized UAP.   \n\u2022 Neural attention distillation (NAD) [30]: NAD use knowledge distillation strategy which distills the attention across the model to acquire a new clean model.   \n\u2022 Implicit backdoor Adversarial unlearning (i-BAU) [67]: I-BAU designs a implicit hypergradient method to solve the adversarial training optimization.   \n\u2022 FT-SAM [72]: It utilizes sharpness-aware minimization to fine-tune the poisoned model.   \n\u2022 Shared adversarial unlearning (SAU) [58]: SAU first generates shared adversarial examples and then unlearns these adversarial examples to purify the model.   \n\u2022 Feature shift tuning (FST) [39]: FST encourages feature shifts by re-iniltialization the linear classifier and fine-tuning the model.   \n\u2022 CleanCLIP [3]: CleanCLIP use both multi-modal contrastive loss and in-modal selfsupervised loss to fine-tune the model.   \n\u2022 FT [3]: FT uses multi-modal contrastive loss to fine-tune the model. ", "page_idx": 20}, {"type": "text", "text": "More details about the implementation of defenses can be found in BackdoorBench [59]. For CleanCLIP, we follow the work\u2019s setting [3] that the CLIP model is trained for 10 epochs with 50 steps of warm-up using a learning rate of 4.5e-6, and a batch size of 64. A total of 10K training pairs are selected from CC3M to train. ", "page_idx": 20}, {"type": "text", "text": "D.4 Backdoor re-activation attack details. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "During the inference phase, we implement our re-activation attack by searching for a global universal perturbation (same size as images) without altering model parameters. For image classification tasks, we employ the same optimized hyperparameters to learn the perturbation across various models and datasets. Specifically, the details are as follows: ", "page_idx": 20}, {"type": "text", "text": "\u2022 For white-box attack, we use the SGD optimizer with a learning rate of 0.05, update the adversarial perturbation within the inner loops for 5 steps, and train for a total of 50 epochs. The hyperparameter $\\lambda$ for the loss is fixed at 1. The training dataset is 1000 poisoned samples that are randomly selected from the original poisoned samples. The batch size is set to 256.   \n\u2022 For black-box attack, we follow the original hyperparameters as in [1]. The updated criterion is based on the decrease in loss rather than the improvement in ASR, as we have found that this approach yields better results.   \n\u2022 For transfer attack, we maintain same hyperparameters to those used in white-box attacks except for training epochs, which is set to 100. We also use a smaller norm bound 1. The training samples are set to 5000. For ensembling these surrogate models, we average their losses in each mini-batch. ", "page_idx": 20}, {"type": "text", "text": "For MMCL task, we use the SGD optimizer with a learning rate of 0.01, update the adversarial perturbation within the inner loop for one steps, and train for a total of 40 epochs. We use the $\\ell_{\\infty}$ -norm with a 0.05 norm bound. We use 1500 poisoned image-text pairs and some clean reference data for optimization. ", "page_idx": 20}, {"type": "text", "text": "E Experimental results on different datasets and models ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we showcase the performance of our attacks under various settings to demonstrate the superior performance of our method. ", "page_idx": 21}, {"type": "text", "text": "Table 12: Performance $(\\%)$ of backdoor re-activation attack on both white-box (WBA) and querybased black-box (BBA) attacks with $\\ell_{\\infty}$ -norm bound $\\rho=0.05$ against different defenses with Tiny ImageNet on PreAct-ResNet18. The best results are highlighted in boldface. ", "page_idx": 21}, {"type": "table", "img_path": "E2odGznGim/tmp/5216b5027c0a2570d5a7fd346e30b2faa3cc84d74d712da79d4f3158a2eafaa0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Attack performance on Tiny ImageNet dataset. Tab. 12 presents the performance of our reactivation attack with both WBA and BBA applied on the Tiny ImageNet dataset with PreActResNet18, compared with ASRs of original attack models (No Defense) and defense models (Defense). Careful observation and analysis of this table furnishes some important insights: ", "page_idx": 21}, {"type": "text", "text": "1. While not achieving the same high level efficacy as in previous experiments, our attacks still show reasonable effectiveness against defense models. On average, our WBA and BBA improve ASRs by $50.00\\%$ and $19.77\\%$ respectively when compared against defense mechanisms, which is actually high than that on CIFAR-10 dataset. This highlights some potential security vulnerabilities in these defense models, although the final ASRs is less severe than those exposed in the previous dataset.   \n2. The performance of our WBA establishes the viability of our backdoor recovery mechanism in a more challenging setting. It further verifies the latent recoverability of backdoors in defense models. Despite the existing gap between our WBA and the more realistic BBA, we suggest that this gap can be reduced with further optimization of our black-box attack strategy.   \n3. When it comes to defense mechanisms, similar to previous observations, attacks on three defenses FT-SAM, SAU and FST show less impressive ASRs. This can be seen as an indication of the significant efficiency of their backdoor removal mechanism. While these mechanisms are more effective in this dataset, these insights are still crucial for developing future defense strategy development.   \n4. It\u2019s worth mentioning some failed cases in our experiment on the Tiny ImageNet dataset. Although our attack methods generally show promising results, the performance in some particular instances falls short of expectations. Future work can gain valuable insights from scrutinizing these instances more closely. Such failures in specific settings serve as a stepping stone toward the development of more effective and robust attack strategies, such as increasing the diversity of random searches. ", "page_idx": 21}, {"type": "text", "text": "Attack performance on GTSRB dataset. Tab. 13 presents the performance of our re-activation attack with both WBA and BBA applied on the GTSRB dataset with PreAct-ResNet18, compared with ASRs of original attack models $\\mathbf{No}$ Defense) and defense models (Defense). A meticulous examination of the results in Tab. 13 provides pivotal insights: ", "page_idx": 21}, {"type": "text", "text": "1. Consistent with earlier experiments, our attacks display impressive effectiveness against these defense models. In these tests, our WBA and BBA average ASRs show an improvement of $50.58\\%$ and $45.25\\%$ respectively compared to the defense mechanisms. This achievement exposes vulnerabilities in the current defense models that were previously unnoticed and highlights the robustness of our attacks.   \n2. The effective performance of our WBA affirms the potency of our backdoor recovery mechanism. The backdoors\u2019 resilience and latent recoverability in defense models are ", "page_idx": 21}, {"type": "table", "img_path": "E2odGznGim/tmp/1feaf5ef8c1dc753b54d109f99186bc0925daa86645b54c948cb7c510b88f5d3.jpg", "table_caption": ["Table 13: Performance $(\\%)$ of backdoor re-activation attack on both white-box (WBA) and querybased black-box (BBA) attacks with $\\ell_{\\infty}$ -norm bound $\\rho\\:=\\:0.05$ against different defenses with GTSRB on PreAct-ResNet18. The best results are highlighted in boldface. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "further substantiated. Additionally, the performance gap between our WBA and the more realistic BBA suggests room for further enhancement of our black-box attack strategy. ", "page_idx": 22}, {"type": "text", "text": "3. Concerning the defense mechanisms, the ASRs against i-BAU and SAU continue to be relatively less impressive. The indication of these defense mechanisms\u2019 efficiency in backdoor removal remains constant. Despite the improved effectiveness witnessed in the GTSRB dataset, these results works as reference for the design of more robust defense strategies in the future. ", "page_idx": 22}, {"type": "table", "img_path": "E2odGznGim/tmp/237fd6e29bc3e16b5d76cbd4468247fa93d0ea2511d9b0986b37674e435d918f.jpg", "table_caption": ["Table 14: Performance $(\\%)$ of backdoor re-activation attack on both white-box (WBA) and querybased black-box (BBA) attacks with $\\ell_{\\infty}$ -norm bound $\\rho\\:=\\:0.05$ against different defenses with CIFAR-10 on VGG19-BN. The best results are highlighted in boldface. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Attack performance on VGG19-BN network. Tab. 14 presents the performance of our reactivation attack with both WBA and BBA applied on the CIFAR-10 dataset with VGG19-BN architecture, compared with ASRs of original attack models (No Defense) and defense models (Defense). Detailed analysis of the results provides the following key takeaways: ", "page_idx": 22}, {"type": "text", "text": "1. Our attacks display remarkable potency against the VGG19-BN network, with both our WBA and BBA demonstrating impressive average ASRs. Specifically, our WBA achieves a significantly high ASR, further emphasizing the backdoor\u2019s recoverability, even in this more complex network architecture. As for BBA, although its ASR doesn\u2019t reach the same level as WBA, it presents a commendable rate, denoting a successful real-world adversarial scenario. ", "page_idx": 22}, {"type": "text", "text": "2. The superior performance of our WBA attests to the robust and tenacious nature of our backdoor recovery mechanism, showcasing our approach\u2019s adaptability and effectiveness across different network structures. The observable gap in ASR between WBA and BBA can be an impetus for refining the black-box attack strategy. ", "page_idx": 22}, {"type": "text", "text": "Attack performance with different norm types under different backdoor attacks. In (a) and (b) of Fig. 3 in the main script, we display the ASR results under different norm types and bounds using Blended attack model. Here, we do more experiments on different attacks and the results are shown in Fig. 6. As can be seen from the figure, our re-activation attack demonstrates a consistent trend across various attack models, showing stable high ASRs when the bound approaches 2 and 0.05 for $\\ell_{2}$ -norm and $\\ell_{\\infty}$ -norm attacks, respectively. ", "page_idx": 22}, {"type": "image", "img_path": "E2odGznGim/tmp/652257d5b1b2216db6a9b2d51d5d10b57be90430e873e54309c659d41e1bbdb1.jpg", "img_caption": ["Figure 6: (a) and (b) show the ASR results under different norm type $p$ and bound $\\rho$ for BadNets. (c) and (d) show the ASR results under different norm type $p$ and bound $\\rho$ for LF. (e) and (f) show the ASR results under different norm type $p$ and bound $\\rho$ for SSBA. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "F Running time analysis ", "text_level": 1, "page_idx": 23}, {"type": "table", "img_path": "E2odGznGim/tmp/5778ab8ff5844621fddeff729762855bc04ec68bb7a72c180ec294028930d4ff.jpg", "table_caption": ["Table 15: Running time analysis. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "In this section, we conduct an analysis of algorithm complexity based on running time statistics. Except for the initial training phase for attack and defense, all our re-activation attacks are trained on a single 3090Ti GPU. We provide a comparative view of the running times. Since the running time of our attack is only related to trainning dataset and network, while independent with specific backdoor attack or defense methods, we didn\u2019t specify the particular method, as the running time is consistent across methods. As displayed in Tab. 15, our attack achieves impressive speed. This can be attributed primarily to our attack requiring a smaller number of training samples, and our approach\u2019s efficiency in computing adversarial samples, needing only a few inner-loop iterations to achieve satisfactory performance. As a result, the training speed is expedited. Unlike in traditional adversarial attacks, our attack will require no further training once the optimal UAP solution is found. This condition poses a considerable threat in reality. It is noted that \"N/A\" appears for BBA on the CLIP models and CC3M dataset in the table, as we did not conduct black-box attacks for the CLIP models. For query-based black-box attack, the attacker cannot directly access the target model (such as weights or gradients) and CLIP models only return the final matching score or ranking results. This limits the ability of query-based black-box attacks. Moreover, there are no relevant studies for reference. Thus, we marked related result as \"N/A\". Additionally, we want to emphasize that BBA requires a large number of queries to the target model to achieve satisfactory attack performance, whereas TA only needs a single query to launch an attack. Clearly, TA is both more efficient and practical compared to BBA. ", "page_idx": 23}, {"type": "text", "text": "G Visualization ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we provide two visualization techniques to showcase the existence of backdoors: feature map visualization and t-SNE visualization. ", "page_idx": 24}, {"type": "text", "text": "Feature maps visualization. We visualize the feature maps, i.e., the features after all convolutional layers, of LF attack and different defense models. We rank these features in descending order of the TAC value of the attack model, meaning that, in each image\u2019s subplot from top to bottom, it illustrates a sort from high to low of backdoor effect. From Figures7 to 12, we can make the following observations: ", "page_idx": 24}, {"type": "text", "text": "\u2022 In the attack model, the highlighted part of the feature maps (the upper sections of each subplot) indicates the existence of a backdoor in the model.   \n\u2022 The highlighted corresponding section in the defense model suggests that the defense model is still sensitive to backdoor samples. This sensitivity manifests as an ability to primarily activate neurons related to the backdoor, even when presented with such samples.   \n\u2022 We have also visualized the feature maps of the clean model and have found that no such phenomenon exists in the clean model. This comparison indicates a stark difference in behavior between the defense model and the clean model. ", "page_idx": 24}, {"type": "image", "img_path": "E2odGznGim/tmp/d79ef5cb9ae0ffc3cc8e07c3f5abc6cd0bd0cc649ddc82fdf9d831d9b4ca7112.jpg", "img_caption": ["Figure 7: Sorted feature map visualization for all convolutional layers on PreAct-ResNet18 with the features in descending order of TAC values for LF backdoor attack. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "T-SNE visualization. We attempt to observe the backdoor effect in defense models by visualizing the features of poisoned and clean samples via t-SNE visualization [51]. As illustrated in Figures 13 to 15, black dots denote poisoned samples while different colors signify various classes of clean samples. Several observations can be drawn from these figures: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Across these attacks, the backdoor samples are clustered in the feature space. \u2022 In the defense models, numerous backdoor samples still cluster together, indicating that the backdoor traits of these samples continue to dominate the network\u2019s recognition of backdoor images, even though these are no longer classified into the target class. ", "page_idx": 24}, {"type": "image", "img_path": "E2odGznGim/tmp/ab2cfda6fffda71e0865c3328bce727b6a014574b1e2b07caf088f8b5b232b0e.jpg", "img_caption": ["Figure 8: Sorted feature map visualization for all convolutional layers on PreAct-ResNet18 with the features in descending order of TAC values for NAD defense against LF backdoor attack. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Thus, viewing from the t-SNE visualization, we may also infer that backdoors still exist within these defense models. ", "page_idx": 25}, {"type": "text", "text": "H Additional experimental results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "H.1 Attack performance against recent defenses ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To test the attack performance against recent defenses, we evaluate the performance against two defense methods: SEAM [74] and CT [43], respectively. The evaluations are conducted on CIFAR-10 dataset with PreAct-ResNet18 network, and the results are shown in Table 16. It is found that both SEAM and CT are vulnerable to the proposed re-activation attack. We would like to emphasize that we have not claimed all post-training defenses are vulnerable to re-activation attacks. The primary objectives of our work are: (1) to reveal this new threat, which has been validated against several classic post-training defenses, and (2) to provide effective tools for evaluating the vulnerability of both existing and future post-training defenses. Therefore, future post-training defense strategies should take this threat into account and aim to mitigate the proposed re-activation attack. ", "page_idx": 25}, {"type": "table", "img_path": "E2odGznGim/tmp/8299df1e06e3240943c96b83b220dca25b290364d8d4396f01d28df412dac728.jpg", "table_caption": ["Table 16: ASR $(\\%)$ of our attack against SEAM and CT. "], "table_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "E2odGznGim/tmp/f40dd50470f252b5e66499e1a7b945159c4da5394b570a0b1555d7d73a1b1a6e.jpg", "img_caption": ["Figure 9: Sorted feature map visualization for all convolutional layers on PreAct-ResNet18 with the features in descending order of TAC values for NC defense against LF backdoor attack. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "H.2 Transfer attack across model architectures ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In the previously discussed transfer-based re-activation attack, we considered a scenario where the attacker is the publisher of the backdoored model. Here, we explore a more strict scenario, where the attacker is only the publisher of the poisoned dataset and has no knowledge of the defender\u2019s model architecture or training process. Therefore, when carrying out a transfer-based re-activation attack, the adversary can perform a transfer attack across different model architectures. We study this problem as follows. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Threat model: Here we present a more strict setting where the adversary can only manipulate the training dataset, while having no access to the training and post-training stages. Thus, the adversary only knows the original trigger $\\xi$ , but has no knowledge of $f_{\\theta_{\\mathrm{A}}}$ or $f_{\\theta_{\\mathrm{D}}}$ . Compared to the previous threat model, one major challenge is the unknown architecture of the target model $f_{\\mathbf{D}}$ .   \n\u2022 Main attack steps: Compared to the steps in the previous setting, there is one additional step where the adversary must first train a backdoored model $f_{\\theta_{\\mathrm{A}}}^{\\prime}$ based on $\\mathcal{D}_{p}$ , which has a different architecture than . All remaining steps are the same as those in the previous setting.   \n\u2022 Experimental results: As shown in Table 17, although the transfer attack across model architectures does not achieve as high ASR as the transfer attack with the same architecture (i.e., the results in Table 3 of the main manuscript), it still demonstrates a certain degree of backdoor transferability. This is an intriguing phenomenon worthy of further exploration. ", "page_idx": 26}, {"type": "text", "text": "H.3 More experimental results on test-time detection ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In Table 6 of the main manuscript, we analyzed the performance of our WBA attack against test-time detection. Here, we present additional experimental results, focusing on more backdoor attack methods, and evaluating the performance of our WBA, BBA, and TA attacks. As shown in Tab. 18, our three kinds of attacks do not markedly increase the TPR compared the defense models. These findings provide insights to develop more stealthy re-activation backdoor attacks in the future. ", "page_idx": 26}, {"type": "image", "img_path": "E2odGznGim/tmp/dff7a33d611ae58f2526277e3932f3588bb1dc763af6ad01b41c359aafff5281.jpg", "img_caption": ["Figure 10: Sorted feature map visualization for all convolutional layers on PreAct-ResNet18 with the features in descending order of TAC values for FST defense against LF backdoor attack. "], "img_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "E2odGznGim/tmp/513e0d3c2bf497851830ca2adf2955ec948d5f6b84a81b37ba8db7d0230b582c.jpg", "table_caption": ["Table 17: Transfer re-activation attack preformance (ASR $\\%$ ) against the target model PreActResNet18, using different architectures of source models. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "E2odGznGim/tmp/29c43e3706598cafe65f0f9be433b98ae27028f2f0875c53029ad3484488dfa7.jpg", "table_caption": ["Table 18: ASR of our three attack methods against three test-time backdoor detection methods. "], "table_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "E2odGznGim/tmp/757bbd3ab393f23dbb285d61fd769ff9ad987d8ccced061e51fa8297c8734d73.jpg", "img_caption": ["Figure 11: Sorted feature map visualization for all convolutional layers on PreAct-ResNet18 with the features in descending order of TAC values for FT-SAM defense against LF backdoor attack. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "E2odGznGim/tmp/26610118ea9defa8349d9fc6b6543809479a7540d92d703f262478c094d6e23f.jpg", "img_caption": ["Figure 12: Sorted feature map visualization for all convolutional layers on PreAct-ResNet18 with the features in descending order of TAC values for clean model. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "E2odGznGim/tmp/d61b8caf1f8bfd097c7783fa233ee05e8da4b65b2388cd9db5fc58d30969c362.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 13: Comprison of T-SNE visualization between Blended attack model and different defense models on CIFAR-10. ", "page_idx": 29}, {"type": "image", "img_path": "E2odGznGim/tmp/712bbd891bfc6c6e42e2e6ffe33a834f0778d9dced30ebd97dfb7ead3f709f4f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 14: Comprison of T-SNE visualization between LF attack model and different defense models on CIFAR-10. ", "page_idx": 29}, {"type": "image", "img_path": "E2odGznGim/tmp/0e5a0f2c391d8254fda4601abbfde8cd7a5cd951dc084b56a4bd4a6cd35d6dc8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 15: Comprison of T-SNE visualization between Trojan attack model and different defense models on CIFAR-10. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 30}, {"type": "text", "text": "Justification: The main claims presented in the abstract and introduction offer a precise overview of what was accomplished and investigated in the research, providing an accurate reflection of the paper\u2019s scope and contributions. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] . ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work. See the \"Limitations and future work\" section for details. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper provides detailed information on the experimental setup for the reproduction of experimental results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We provided the code via an anonymous website. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We outline all details related to the training and test processes, including data splits, hyperparameters, their selection process, and the types of optimizers used, thereby providing a comprehensive understanding of the results. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We have considered the randomness and run the experiments multi times for main results. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have clearly reported the resources used for experiments. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have discussed it in \"Broader Impacts\" section. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 33}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our work has no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Properly credited. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}]