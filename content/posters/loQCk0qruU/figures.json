[{"figure_path": "loQCk0qruU/figures/figures_1_1.jpg", "caption": "Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods", "description": "This figure compares the accuracy and expected calibration error (ECE) of various parameter-efficient fine-tuning (PEFT) methods on the CIFAR100 dataset for few-shot (1-20 shot) adaptation.  It demonstrates that while PEFT methods achieve high accuracy, they often suffer from poor calibration (high ECE), especially in low-shot learning scenarios.  The figure highlights the under-confidence problem where models are often too hesitant to make confident predictions, even when they are accurate.  Bayesian-PEFT, a new method proposed in the paper, aims to address this issue.", "section": "Introduction"}, {"figure_path": "loQCk0qruU/figures/figures_2_1.jpg", "caption": "Figure 2: PEFT on the 1-shot CIFAR100 dataset: all existing PEFT techniques exhibit severe under-confidence while the proposed B-PEFT reduces the ECE by almost an order of magnitude.", "description": "The figure shows reliability diagrams for four different parameter-efficient fine-tuning (PEFT) methods on the 1-shot CIFAR100 dataset.  The x-axis represents the model's confidence, and the y-axis represents the accuracy.  The ideal performance is a diagonal line.  The plots for VPT, Adapter, and Bias show that these models are severely underconfident, making predictions with low confidence even when they are relatively accurate. In contrast, the plot for B-PEFT shows that the proposed Bayesian-PEFT method significantly improves the calibration, producing much more confident predictions and reducing the expected calibration error (ECE) by nearly an order of magnitude.", "section": "1 Introduction"}, {"figure_path": "loQCk0qruU/figures/figures_5_1.jpg", "caption": "Figure 3: 1-shot Cifar10 results and evidence vacuity trends", "description": "This figure presents the results of applying the proposed method to the Cifar10 dataset using a 1-shot learning approach. It consists of four sub-figures: (a) shows the relationship between accuracy and dissonance, demonstrating the model's accuracy despite low confidence; (b) shows the AUC curve, indicating good discrimination ability; (c) visualizes the evidence distribution for a sample, highlighting the low evidence assigned to most classes; and (d) shows the vacuity distribution, confirming the model's under-confidence.  These plots illustrate the under-confident behavior of existing methods even when achieving relatively high accuracy.", "section": "3 Bayesian Parameter-Efficient Fine-Tuning of Foundation Models"}, {"figure_path": "loQCk0qruU/figures/figures_6_1.jpg", "caption": "Figure 4: (a) Schematic diagram and (b) Graphical model of the B-PEFT model", "description": "This figure shows the schematic diagram (a) and graphical model (b) of the proposed Bayesian Parameter Efficient Fine-Tuning (B-PEFT) framework. The schematic diagram illustrates the process of using an ensemble of evidential models to generate predictions, incorporating base-rate adjustment and diversity-inducing evidential ensemble techniques. The graphical model provides a visual representation of the probabilistic relationships between the input, latent variables, and the output predictions, using Dirichlet distributions and multinomial likelihoods.", "section": "4 Experiments and Results"}, {"figure_path": "loQCk0qruU/figures/figures_6_2.jpg", "caption": "Figure 4: (a) Schematic diagram and (b) Graphical model of the B-PEFT model", "description": "This figure shows a schematic diagram and a graphical model of the proposed Bayesian Parameter Efficient Fine-Tuning (B-PEFT) framework. The schematic diagram (a) illustrates the process of input, base rate adjustment for each ensemble, predictions from the ensemble, and finally, accuracy and Expected Calibration Error (ECE).  The graphical model (b) presents a more detailed representation of the Bayesian modeling, with the observed variable, latent variables, and how multiple ensembles are combined to reach a final prediction.  This framework uses evidential learning and integrates state-of-the-art parameter-efficient fine-tuning techniques with two Bayesian components to improve prediction accuracy and calibration under challenging few-shot learning settings.", "section": "3 Bayesian Parameter-Efficient Fine-Tuning of Foundation Models"}, {"figure_path": "loQCk0qruU/figures/figures_9_1.jpg", "caption": "Figure 6: (a-b): Vacuity distribution of a single model and (c-d): variance distribution of ensemble models for 1/5 shots cifar10 as In-Distribution and Cifar100 as Out-of-Distribution dataset", "description": "This figure visualizes the vacuity and variance distributions for in-distribution (CIFAR-10) and out-of-distribution (CIFAR-100) data samples, considering both 1-shot and 5-shot scenarios.  The plots illustrate how the model's uncertainty, represented by vacuity and variance, changes depending on whether the sample is in-distribution or out-of-distribution and the number of shots used for training.", "section": "Uncertainty quantification results"}, {"figure_path": "loQCk0qruU/figures/figures_9_2.jpg", "caption": "Figure 4: (a) Schematic diagram and (b) Graphical model of the B-PEFT model", "description": "This figure illustrates the Bayesian model averaging process for building a diversity-inducing evidential ensemble in the B-PEFT framework. The schematic diagram (a) shows the overall process, while the graphical model (b) visually represents the relationships between variables such as observed variables, latent variables, ensemble predictions, accuracy, and expected calibration error (ECE).", "section": "3.3 Building A Diversity Induced Evidential Ensemble"}, {"figure_path": "loQCk0qruU/figures/figures_9_3.jpg", "caption": "Figure 3: 1-shot Cifar10 results and evidence vacuity trends", "description": "This figure shows the results of a 1-shot Cifar10 experiment, demonstrating the relationship between dissonance and accuracy, the distribution of evidence for classes, and vacuity distribution. It highlights the model's under-confidence despite accurate predictions, attributing this to insufficient evidence allocation and underestimation of prior knowledge from pre-training.", "section": "3.2 Strengthening the prior belief through base rate adjustment"}, {"figure_path": "loQCk0qruU/figures/figures_21_1.jpg", "caption": "Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods", "description": "The figure shows the accuracy and Expected Calibration Error (ECE) on CIFAR100 dataset for different Parameter-Efficient Fine-Tuning (PEFT) methods in few-shot adaptation. It demonstrates that although PEFT methods achieve high accuracy, they produce under-confident predictions, especially in low-shot settings. The Bayesian-PEFT method significantly improves calibration performance.", "section": "1 Introduction"}, {"figure_path": "loQCk0qruU/figures/figures_23_1.jpg", "caption": "Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods", "description": "This figure compares the accuracy and expected calibration error (ECE) of various parameter-efficient fine-tuning (PEFT) methods on the CIFAR100 dataset for few-shot adaptation scenarios (1-20 shots).  It highlights the trade-off between accuracy and calibration, showcasing how some methods achieve high accuracy but poor calibration (high ECE), indicating under-confidence in predictions.  Bayesian-PEFT is shown to outperform other methods in both accuracy and calibration.", "section": "1 Introduction"}, {"figure_path": "loQCk0qruU/figures/figures_23_2.jpg", "caption": "Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods", "description": "This figure shows the accuracy and expected calibration error (ECE) for different parameter-efficient fine-tuning (PEFT) methods on the CIFAR-100 dataset for few-shot adaptation.  It demonstrates that while PEFT methods achieve high accuracy, they often suffer from poor calibration (high ECE), especially in low-shot learning scenarios. The figure highlights the under-confidence issue that is a significant problem with PEFT methods.", "section": "1 Introduction"}, {"figure_path": "loQCk0qruU/figures/figures_25_1.jpg", "caption": "Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods", "description": "The figure shows the accuracy and expected calibration error (ECE) for various parameter-efficient fine-tuning (PEFT) methods on the CIFAR100 dataset using few-shot adaptation.  It demonstrates that while PEFT methods achieve high accuracy, they often suffer from poor calibration (high ECE), especially in low-shot learning scenarios.  The plot highlights the under-confidence problem where the model is accurate but assigns low confidence to its predictions.  Bayesian-PEFT is shown to significantly improve calibration.", "section": "1 Introduction"}, {"figure_path": "loQCk0qruU/figures/figures_25_2.jpg", "caption": "Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods", "description": "This figure compares the accuracy and expected calibration error (ECE) of various parameter-efficient fine-tuning (PEFT) methods on the CIFAR100 dataset for few-shot adaptation.  It shows that while PEFT methods generally improve accuracy, they often result in poorly calibrated models, especially in low-shot scenarios. The Bayesian-PEFT method is highlighted for its superior performance. The x-axis represents the accuracy, while the y-axis represents the ECE. Lower ECE indicates better calibration. ", "section": "1 Introduction"}, {"figure_path": "loQCk0qruU/figures/figures_25_3.jpg", "caption": "Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods", "description": "This figure compares the accuracy and expected calibration error (ECE) of various parameter-efficient fine-tuning (PEFT) methods on the CIFAR100 dataset for few-shot adaptation.  It demonstrates that while PEFT methods achieve high accuracy, they often suffer from poor calibration, especially in low-shot scenarios.  Bayesian-PEFT is shown to significantly improve calibration while maintaining high accuracy. The x-axis represents accuracy, and the y-axis represents ECE.  Lower ECE indicates better calibration.", "section": "Introduction"}, {"figure_path": "loQCk0qruU/figures/figures_26_1.jpg", "caption": "Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods", "description": "This figure compares the accuracy and expected calibration error (ECE) of various parameter-efficient fine-tuning (PEFT) methods on the CIFAR100 dataset for few-shot adaptation. It shows that while most methods achieve high accuracy, they suffer from poor calibration (high ECE), indicating underconfidence in their predictions. The proposed Bayesian-PEFT method is highlighted for its superior calibration performance.", "section": "1 Introduction"}, {"figure_path": "loQCk0qruU/figures/figures_27_1.jpg", "caption": "Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods", "description": "The figure shows the accuracy and expected calibration error (ECE) of different parameter-efficient fine-tuning (PEFT) methods on the CIFAR100 dataset for few-shot adaptation.  It demonstrates that while PEFT methods achieve high accuracy, they also suffer from under-confidence issues, particularly in low-shot learning scenarios. The Bayesian-PEFT method is shown to significantly improve calibration.", "section": "1 Introduction"}, {"figure_path": "loQCk0qruU/figures/figures_28_1.jpg", "caption": "Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods", "description": "This figure shows a comparison of the accuracy and Expected Calibration Error (ECE) achieved by various parameter-efficient fine-tuning (PEFT) methods on the CIFAR100 dataset for few-shot adaptation.  It illustrates that while all PEFT methods improve accuracy, especially as the number of shots increases, they significantly underperform in terms of calibration.  Bayesian-PEFT is shown to greatly improve calibration.", "section": "1 Introduction"}]