[{"type": "text", "text": "In-Trajectory Inverse Reinforcement Learning: Learn Incrementally From An Ongoing Trajectory ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shicheng Liu & Minghui Zhu School of Electrical Engineering and Computer Science Pennsylvania State University University Park, PA 16802, USA {sf15539,muz16}@psu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inverse reinforcement learning (IRL) aims to learn a reward function and a corresponding policy that best fit the demonstrated trajectories of an expert. However, current IRL works cannot learn incrementally from an ongoing trajectory because they have to wait to collect at least one complete trajectory to learn. To bridge the gap, this paper considers the problem of learning a reward function and a corresponding policy while observing the initial state-action pair of an ongoing trajectory and keeping updating the learned reward and policy when new stateaction pairs of the ongoing trajectory are observed. We formulate this problem as an online bi-level optimization problem where the upper level dynamically adjusts the learned reward according to the newly observed state-action pairs with the help of a meta-regularization term, and the lower level learns the corresponding policy. We propose a novel algorithm to solve this problem and guarantee that the algorithm achieves sub-linear local regret $O(\\sqrt{T}+\\log T+\\sqrt{T}\\log T)$ .If the reward function is linear, we prove that the proposed algorithm achieves sub-linear regret ${\\cal O}(\\log T)$ . Experiments are used to validate the proposed algorithm. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inverse reinforcement learning (IRL) aims to learn a reward function and a corresponding policy that are consistent with the demonstrated trajectories of an expert. In recent years, several IRL methods are proposed to help learn the reward and policy, including maximum margin methods [1, 2], maximum entropy methods [3, 4], maximum likelihood methods [5, 6], and Bayesian methods [7, 8]. ", "page_idx": 0}, {"type": "text", "text": "The aforementioned IRL works learn from pre-collected demonstration sets and do not improve the learned model during deployment. Online IRL [9, 10, 11] instead can learn from sequentially arrived demonstrated trajectories and continuously improve the learned reward and policy from the newly observed complete trajectories. However, recent applications of IRL motivate the need to learn incrementally from an ongoing trajectory before it terminates. For example, inferring a moving shooter's intention from its ongoing movement in order to evacuate the hiding victims [12] before the shooter finds them. In this case, we need to quickly update the inference about the shooter's intention once a new movement of the shooter is observed, so that we can use the latest inference to plan a rescue strategy as soon as possible. We cannot wait until the shooter trajectory ends, in case the shooter has found the victims. Another example is learning a target customer's investment preference from its daily updated investment trajectory in a stock market [13] in order to recommend appropriate stocks [14, 15] before other competitors get this customer. However, current IRL works cannot learn from an ongoing trajectory because they have to wait to collect at least one complete trajectory to learn from. To bridge the gap, this paper proposes in-trajectory IRL, a new type of IRL that learns a reward function and a corresponding policy at the initial state-action pair of an ongoing trajectory, and keeps updating the learned reward and policy once a new state-action pair of the ongoing trajectory is observed. We summarize our contributions as follows: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Contribution statement. This paper proposes the first in-trajectory IRL framework, termed metaregularized in-trajectory inverse reinforcement learning\" (MERIT-IRL), to learn a reward function and a corresponding policy from an ongoing trajectory. Our contributions are twofold. ", "page_idx": 1}, {"type": "text", "text": "First, we formulate this in-trajectory learning problem as an online bi-level optimization problem where the upper level continuously updates the learned reward according to the newly observed state-action pairs and the lower level computes the corresponding policy. We develop a novel online learning algorithm (MERIT-IRL) to solve this problem. The major novelty of the proposed algorithm is that we propose a novel reward update mechanism specially designed for the in-trajectory learning setting. This special reward update not only aims to explain the expert trajectory observed so far, but also aims to consider for the future. Moreover, since the data is lacking as there is only one ongoing trajectory, we introduce a meta-regularization term to embed prior knowledge and avoid overfitting. ", "page_idx": 1}, {"type": "text", "text": "Second, we theoretically guarantee that MERIT-IRL achieves sub-linear local regret $O({\\sqrt{T}}+\\log T+$ $\\sqrt{T}\\log T)$ . If the reward function is linear, we prove that MERIT-IRL achieves sub-linear regret ${\\cal O}(\\log T)$ . The major novelty of the theoretical analysis is to address the difficulty that the input data is not identically independent distributed (i.i.d.) but temporally correlated, i.e., the data $(s_{t},a_{t})$ at each time $t$ is affected by the data $\\left(s_{t-1},a_{t-1}\\right)$ at last time. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Due to the space limit, we only discuss the related works on learning from incomplete trajectories here, and we include the discussion on more related works in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "Papers [16, 17] use partial trajectories to update the learned reward function by comparing the expert trajectories after the expert states and the trajectories starting from those expert states rolled out by the learned policy. However, given the current expert state, they use expert trajectory suffix (i.e., future trajectory) to compare while we can only access expert trajectory prefix (i.e., previous trajectory) from an ongoing trajectory. Papers [18, 19, 20] use imitation learning to learn from incomplete demonstrations. In specific, paper [18] uses a discounted sum along the future trajectory as the weight for weighted behavior cloning and works effectively even if only portions of trajectories are observed. Papers [19, 20] extend GAIL [21] to solve for the case where the action sequences are not complete. However, these works all require a pre-collected set of demonstrations so that they are not in-trajectory learning since the trajectory in their cases is not ongoing. ", "page_idx": 1}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we formulate the problem of in-trajectory IRL. In in-trajectory IRL, there is an   \nexpert whose decision-making is based on a Markov decision process (MDP). An MDP is a tuple   \n$(S,{\\mathcal{A}},\\gamma,r_{E},P_{0},P)$ which consists of a state set $\\boldsymbol{S}$ , an action set $\\boldsymbol{\\mathcal{A}}$ , a discount factor $\\gamma\\in[0,1)$ \uff0c   \na reward function $r_{E}:S\\times A\\to\\mathbb{R}.$ , and the initial state distribution $P_{0}(\\cdot)$ . The state transition   \nprobability (density) function is denoted by $P(\\cdot|\\cdot,\\cdot)$ such that $P(s^{\\prime}|s,a)$ denotes the probability $s^{\\prime}$ fyl $s$ $a$ $\\pi_{E}$ $\\zeta^{E}=\\bar{S}_{0}^{E},A_{0}^{E},S_{1}^{E},A_{1}^{E},\\cdot\\cdot.$ $t$   \naction par $(S_{t}^{E},A_{t}^{\\overline{{E}}})$   \nfrom the ongoing trajectory and update the learned reward function and policy at each time $t$ ", "page_idx": 1}, {"type": "text", "text": "Many IRL algorithms [5, 6, 11, 22, 23] employ a bi-level learning structure. In this structure, the upper level learns a reward function while the lower level aims to find an associated policy by solving an RL problem under the current learned reward function. Inspired by their bi-level learning structure, we formulate the in-trajectory IRL problem as an online non-convex bi-level optimization problem. In specific, we aim to learn a reward function $r_{\\theta}$ (parameterized by. $\\theta$ ) in the upper level and a policy corresponding to $r_{\\theta}$ in the lower level. The loss function at time $t$ is defined as: ", "page_idx": 1}, {"type": "equation", "text": "$$\nL_{t}(\\theta;(S_{t}^{E},A_{t}^{E}))\\triangleq-\\gamma^{t}\\log\\pi_{\\theta}(A_{t}^{E}|S_{t}^{E})+\\frac{\\lambda\\gamma^{t}}{2}||\\theta-\\bar{\\theta}||^{2},\\quad\\pi_{\\theta}=\\arg\\operatorname*{max}_{\\pi}J_{\\theta}(\\pi)+H(\\pi).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Note that $L_{t}(\\theta;(S_{t}^{E},A_{t}^{E}))$ is defined using $\\pi_{\\theta}$ and $\\theta$ is the parameter of the reward function $r_{\\theta}$ , here the policy $\\pi_{\\theta}$ is also parameterized by $\\theta$ because it is computed by solving an RL problem (in the lower level) under the reward function $r_{\\theta}$ , and thus is indirectly parameterized by $\\theta$ .Maximum likelihood IRL (ML-IRL) [5] has a similar bi-level formulation with (1), however, ML-IRL only solves an offline optimization problem and its analysis does not hold for non i.i.d. input data and continuous state-action space. We discuss our distinctions from ML-IRL in Appendix A.1. ", "page_idx": 2}, {"type": "text", "text": "The upper-level loss function $L_{t}$ has two terms. The first term $-\\gamma^{t}\\log\\pi_{\\theta}(A_{t}^{E}|S_{t}^{E})$ is the discounted negative log-likelihood of the state-action pair $(S_{t}^{E},A_{t}^{E})$ at time $t$ and the second term $\\frac{\\lambda\\gamma^{t}}{2}||\\theta-\\bar{\\theta}||^{2}$ is the discounted meta-regularization term [24] where $\\lambda$ is a hyper-parameter. The likelihood function is commonly used in IRL [5, 6] to learn a reward function. Basically, the upper-level loss function at time $t$ encourages to find a reward function $r_{\\theta}$ that makes the observed state-action pair $(S_{t}^{E},A_{t}^{E})$ most likely and meanwhile, the reward parameter $\\theta$ should not be too far from the prior experience, i.e., the meta-prior $\\bar{\\theta}$ . Note that $\\bar{\\theta}$ is a pre-trained meta-prior that embeds the information of \u201crelevant experience\". We will introduce the training of $\\bar{\\theta}$ in Subsection 4.3 and Appendix C. ", "page_idx": 2}, {"type": "text", "text": "The lower-level problem is used to compute $\\pi_{\\theta}$ using the current reward function $r_{\\theta}$ . It proposes to find a policy $\\pi_{\\theta}$ that maximizes the entropy-regularized cumulative reward $J_{\\theta}(\\pi)+H(\\pi)$ . The cumulative reward of a policy $\\pi$ under the reward function $r_{\\theta}$ .is $\\begin{array}{r}{J_{\\theta}(\\pi)\\triangleq E_{S,A}^{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{\\theta}(S_{t},A_{t})]}\\end{array}$ where the initial state is drawn from $P_{0}$ . The causal entropy of a policy $\\pi$ is defined as $H(\\pi)$ = $\\begin{array}{r}{E_{S,A}^{\\pi}[-\\sum_{t=0}^{\\infty}\\gamma^{t}\\log\\pi(A_{t}|S_{t})]}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "Since the expert demonstrates $\\{(S_{t}^{E},A_{t}^{E})\\}_{t\\geq0}$ sequentially, we have a sequence of loss functions $\\{L_{t}(\\theta;(S_{t}^{E},A_{t}^{E}))\\}_{t\\geq0}$ . We use this sequence of loss functions (1) to formulate an online learning probemAtypical onne leain problemistominmizetherege $\\begin{array}{r l}{\\lefteqn{\\sum_{t=0}^{T-1}L_{t}(\\theta_{t};(S_{t}^{E},A_{t}^{E}))-}}\\end{array}$ $\\begin{array}{r}{\\operatorname*{min}_{\\theta}\\sum_{t=0}^{T-1}L_{t}(\\theta;(S_{t}^{E},A_{t}^{E}))}\\end{array}$   \n$L_{t}$   \nwhich is widely adopted in online non-convex optimization [25, 26] and online IRL [11]. The local regret quantifies the general stationarity of a sequence of loss functions under the learned parameters. In specific, given a sequence of loss functions $\\{f_{t}(x)\\}_{t\\geq0}$ , the local regret [11, 25, 26] at time $t$ is defined as $\\begin{array}{r}{||\\frac{1}{t+1}\\sum_{i=0}^{t}\\nabla f_{i}(x_{t})||^{2}}\\end{array}$ which quantifes the gradient norms of the average of allthe previous loss functions under the current learned parameter $x_{t}$ . The total local regret is defined as the sum of the local regret at each time $t$ i.e, $\\begin{array}{r}{\\sum_{t=0}^{T-1}||\\frac{1}{t+1}\\sum_{i=0}^{t}{\\nabla f_{i}(x_{t})||^{2}}}\\end{array}$ In our case, we replace $\\{f_{t}\\}_{t\\ge0}$ with the loss function $\\{L_{t}\\}_{t\\ge0}$ defined in (1) and thus formulate the local regret (2)-(3) which has a bi-level formulation. We aim to minimize the following local regret: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\{(S_{t}^{E},A_{t}^{E})\\sim\\mathbb{P}_{t}^{\\pi_{E}}(\\cdot,\\cdot)\\}_{t\\geq0}}\\biggl[\\displaystyle\\sum_{t=0}^{T-1}||\\frac{1}{t+1}\\sum_{i=0}^{t}\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\\biggr],}\\\\ &{\\mathrm{s.t.}\\quad\\pi_{\\theta_{t}}=\\arg\\underset{\\pi}{\\operatorname*{max}}\\,J_{\\theta_{t}}(\\pi)+H(\\pi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(S_{t}^{E},A_{t}^{E})\\;\\sim\\;\\mathbb{P}_{t}^{\\pi_{E}}(\\cdot,\\cdot)$ means that $(S_{t}^{E},A_{t}^{E})$ is drawn from the state-action distribution $\\mathbb{P}_{t}^{\\pi_{E}}(\\cdot,\\cdot)$ , and $\\mathbb{P}_{t}^{\\pi}(\\cdot,\\cdot)$ is the state-action distributioninducedby $\\pi$ at time $t$ in the MDP. ", "page_idx": 2}, {"type": "text", "text": "Difficulties of solving problem (2)-(3). We want to design a fast algorithm to solve problem (2)-(3) since we need to finish the update of $\\theta$ and $\\pi$ before the next state-action pair is observed and the time between two consecutive state-action pairs can be short. However, designing and analyzing such a fast algorithm is difficult due to the following challenges: ", "page_idx": 2}, {"type": "text", "text": "(i) First and foremost, current state-of-the-arts [25, 26] on online non-convex optimization use followthe-leader-based algorithms which solve mi $\\begin{array}{r}{{1}_{\\theta}\\sum_{i=0}^{t}L_{i}\\big(\\theta;(S_{i}^{E},A_{i}^{E})\\big)}\\end{array}$ to near stationarity at each time $t$ . This is time-consuming because they require multiple gradient descent updates of $\\theta$ . One way to alleviate this problem is to use online gradient descent (OGD) which only updates $\\theta$ by one gradient descent step at each time $t$ . However, since OGD does not solve the problem to near stationarity at any time $t$ , it is extremely difficult to quantify the overall stationarity after $T$ iterations. While OGD has been well studied in online convex optimization, it is rarely studied in online non-convex optimization. The recent work [11] uses OGD to quantify the local regret, however, its analysis can only hold when the input data is i.i.d. In contrast, the input data in our problem is not i.i.d. In specific, the input data at time $t$ (i.e., $(S_{t}^{E},A_{t}^{E}))$ is actually affected by the input data at last step (i.e., $(S_{t-1}^{E},A_{t-1}^{E}))$ . This correlation between any two consecutive input data makes it dificult to analyze the growth rate of the local regret. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "(ii) Second, it is time-consuming if we fully solve the lower-level problem (3) to get $\\pi_{\\theta}$ because this requires multiple policy updates to solve an RL problem. Therefore, we use a \u201csingle-loop\" method which only requires one-step policy update for a given $r_{\\theta}$ . However, since the policy is only updated once, the updated policy can be far from $\\pi_{\\theta}$ and thus making the analysis difficult. Single-loop methods are widely adopted to solve hierarchical problems, including bi-level optimization [27, 28], game theory [29, 30], min-max problems [31, 32], etc. Recently, single-loop methods are applied to IRL [5], however, the paper [5] only solves an offine optimization problem and its analysis does not hold for non i.i.d. input data and continuous state-action space. We include a section in Appendix A.1 to discuss our distinctions from [5]. ", "page_idx": 3}, {"type": "text", "text": "4  Algorithm and Theoretical Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section has three parts. The first part presents a novel online learning algorithm that solves the problem (2)-(3) and tackles the aforementioned two difficulties. The second part proves that Algorithm 1 achieves sub-linear local regret. If the reward function is linear, we prove that Algorithm 1 achieves sub-linear regret. The third part introduces a meta-learning method to get the meta-prior $\\bar{\\theta}$ ", "page_idx": 3}, {"type": "text", "text": "4.1 The proposed algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In practice, the expert will demonstrate a specifi trajectory $s_{0}^{E},a_{0}^{E},s_{1}^{E},a_{1}^{E},\\cdots.$ For distinction, we use the capital letters (e.g.. $S$ ) to represent random variables and the lower-case letters (e.g., s) to represent specific values. To design a fast algorithm, we propose an online-gradient-descent-based single-loop algorithm. In specific, at each time $t$ , the algorithm updates both policy $\\pi$ and reward parameter $\\theta$ only once. The policy update is to solve the lower-level problem (3) and the reward update is to solve the upper-level problem (2). In the following, we elaborate the procedure of policy update and reward update. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Meta-regularized In-trajectory Inverse Reinforcement Learning (MERIT-IRL) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Input: Initialized policy $\\pi_{0}$ ,the streaming input data $\\{(s_{t}^{E},a_{t}^{E})\\}_{t\\geq0}$ Output: Learned reward parameter $\\theta_{T}$ andpolicy $\\pi_{T}$ ", "page_idx": 3}, {"type": "text", "text": "1: Compute $\\bar{\\theta}$ using the meta-regularization in Section 4.3 and Appendix C, and set $\\theta_{0}=\\bar{\\theta}$   \n2: for $t=0,1,\\cdot\\cdot\\cdot\\,,T-1$ do   \n3: Compute the soft Q-function $Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}$ (defined in Appendix B.1) under the current reward function $r_{\\theta_{t}}$ and policy $\\pi_{t}$   \n4: Update $\\pi_{t+1}(a|s)\\propto\\exp(Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s,a))$ for any $(s,a)\\in S\\times A$   \n5: Roll out policy $\\pi_{t+1}$ twice: one starting from $s_{0}^{E}$ to get $s_{0}^{E},a_{0}^{\\prime},s_{1}^{\\prime},a_{1}^{\\prime},\\cdots$ , and the other starting from $(s_{t}^{E},a_{t}^{E})$ to get $s_{t+1}^{\\prime\\prime},a_{t+1}^{\\prime\\prime},\\cdots$   \n6: Compute $\\begin{array}{r}{g_{t}=\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(s_{i}^{\\prime},a_{i}^{\\prime})-\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})+\\frac{\\lambda(1-\\gamma^{t+1})}{1-\\gamma}(\\theta_{t}-\\bar{\\theta})}\\end{array}$ where $s_{0}^{\\prime}=s_{0}^{E}$ and $\\left(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime}\\right)=\\left(s_{i}^{E},a_{i}^{E}\\right)$ for $0\\leq i\\leq t$   \n7: Update $\\theta_{t+1}=\\theta_{t}-\\alpha_{t}g_{t}$   \n8: end for ", "page_idx": 3}, {"type": "text", "text": "Policy update (lines 3-4 of Algorithm 1). At each time $t$ , we only partially solve the lower-level problem (3) via one-step soft policy iteration [5, 33]. In specific, the soft policy iteration contains $Q$ $Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}$ $r_{\\theta_{t}}$ $\\pi_{t}$ $\\pi_{t+1}(s,a)\\propto$ $\\exp(Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s,a))$ for any $(s,a)\\in S\\times A$ practicalilementaons, $\\pi_{t+1}$ can be obtainedby one-step policy update in soft Q-learning [33] or one-step actor update in soft actor-critic [34]. ", "page_idx": 3}, {"type": "text", "text": "Reward update (lines 5-7 of Algorithm 1). At each time $t$ the algorithm observes $(s_{t}^{E},a_{t}^{E})$ and aims to leverage all the previously observed data to update the reward parameter. In specific, as $\\begin{array}{r}{L_{t}(\\theta;(s_{t}^{E},a_{t}^{E}))=-\\gamma^{t}\\log\\pi_{\\theta}(a_{t}^{E}|s_{t}^{E})+\\frac{\\lambda\\gamma^{t}}{2}||\\theta-\\bar{\\theta}||^{2}}\\end{array}$ is the meta-regularized negativ log-likelihood of $(s_{t}^{E},a_{t}^{E})$ , the algorithm can formulate $\\begin{array}{r}{\\sum_{i=0}^{t}L_{i}(\\theta;(s_{i}^{E},a_{i}^{E}))}\\end{array}$ at time $t$ using all the previously collected data (i.e., $\\{(s_{i}^{E},a_{i}^{E})\\}_{i=0,\\cdots,t})$ . To update the reward parameter, the algorithm partially minimizes $\\begin{array}{r}{\\sum_{i=0}^{t}L_{i}(\\theta;(s_{i}^{E},a_{i}^{E}))}\\end{array}$ via one-step gradient descent. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Lemma 1. The gradient of $\\begin{array}{r}{\\sum_{i=0}^{t}L_{i}(\\theta;(s_{i}^{E},a_{i}^{E}))}\\end{array}$ can be calculated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\nabla\\sum_{i=0}^{t}L_{i}(\\theta;(s_{i}^{E},a_{i}^{E}))=E_{S,A}^{\\pi_{\\theta}}\\biggr[\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(S_{i},A_{i})\\biggr|S_{0}=s_{0}^{E}\\biggr]+\\frac{\\lambda(1-\\gamma^{t+1})}{1-\\gamma}(\\theta-\\bar{\\theta})}\\\\ &{\\displaystyle-\\sum_{i=0}^{t}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(s_{i}^{E},a_{i}^{E})-E_{S,A}^{\\pi_{\\theta}}\\biggr[\\sum_{i=t+1}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(S_{i},A_{i})\\biggr|S_{t}=s_{t}^{E},A_{t}=a_{t}^{E}\\biggr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the gradient (4) holds for continuous state-action space. Since the gradient (4) has expectation terms under the policy $\\pi_{\\theta}$ , we can only approximate it. In specific, we roll out the policy $\\pi_{t+1}$ twice: one starting from $s_{0}^{\\check{E}}$ to get a trajectory $\\{(\\boldsymbol{s}_{i}^{\\prime},\\boldsymbol{a}_{i}^{\\prime})\\}_{i\\ge0}$ where $s_{0}^{\\prime}=s_{0}^{E}$ , and the other one starting from $(s_{t}^{E},a_{t}^{E})$ to get a trajectory $\\{(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})\\}_{i\\geq0}$ where $\\left(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime}\\right)=\\left(s_{i}^{E},a_{i}^{E}\\right)$ for $0\\leq i\\leq t$ Then we use the empirical estimate $\\begin{array}{r}{g_{t}=\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(s_{i}^{\\prime},a_{i}^{\\prime})-\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})+\\frac{\\lambda(1-\\gamma^{\\tau+1})}{1-\\gamma}(\\theta_{t}-\\bar{\\theta})}\\end{array}$ to approximate $\\begin{array}{r l}{\\lefteqn{\\nabla\\sum_{i=0}^{t}L_{i}(\\theta_{t};(s_{i}^{E},a_{i}^{E}))}\\quad}&{{}}\\end{array}$ With the gradient approximation $g_{t}$ , we utilize stochastic online gradient descent $\\theta_{t+1}=\\theta_{t}-\\alpha_{t}g_{t}$ to update the reward parameter. ", "page_idx": 4}, {"type": "text", "text": "Discussion on our special design of the reward update. The right subfigure in Figure 1 visualizes our reward update (modulo the meta-regularization term). The green trajectory (i.e., $\\{\\overline{{(s_{t}^{E},a_{t}^{E})}}\\}_{t\\geq0})$ .s the expert trajectory, and the red trajectories (i.e., $\\{(s_{t}^{\\prime},a_{t}^{\\prime})\\}_{t\\geq0}$ and $\\{(\\bar{s_{i}^{\\prime\\prime}},a_{i}^{\\prime\\prime})\\}_{i>t})$ are the trajectories generated by the learned policy. Given the expert trajectory prefix (i.e., the incomplete trajectory $\\overline{{\\{(s_{i}^{E},a_{i}^{E})\\}_{i=0}^{t}}}$ observed so far), our method completes the expert trajectory by rolling out the learned policy starting from $(s_{t}^{E},a_{t}^{E})$ and filling the trajectory suffix $\\{(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})\\}_{i>t}$ . The combined complete trajectory includes the expert trajectory prefix $\\{(s_{i}^{E},a_{i}^{E})\\}_{i=0}^{t}$ and the learner-filled trajectory suffix $\\{(\\bar{s}_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})\\}_{i\\geq t}$ . We update the reward function by comparing this combined trajectory to a complete trajectory $\\{(s_{t}^{\\prime},a_{i}^{\\prime})\\}_{t\\geq0}$ generated by the learned policy starting from the expert's initial stat $s_{0}^{E}$ ", "page_idx": 4}, {"type": "text", "text": "A more straightforward way for the reward update is to directly compare the trajectory prefixes (visualized in the middle of Figure 1) at each time $t$ . However, this naive method can be problematic. We explain the issue of this naive method and the advantage of our method in the following context. ", "page_idx": 4}, {"type": "image", "img_path": "mJZH9w8qgu/tmp/b818d58c2530064f82647bce5886d48ee0c40267b58b6f9b68406e5f4252eef8.jpg", "img_caption": ["Figure 1: Standard IRL (left), the naive method for intrajectory learning (middle), and our method (right). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1 visualizes the reward update for standard IRL (left), the naive method (i.e., directly run standard IRL algorithms on the expert trajectory prefix) (middle), and our method (right). The standard IRL (left) updates the reward function by comparing the complete expert trajectory and the complete trajectory generated by the learned policy. This case is ideal, however, it is infeasible when the trajectory is ongoing and we can only observe an incomplete expert trajectory $\\{(s_{i}^{E},a_{i}^{E})\\}_{i=0}^{t}$ at each time $t$ . The naive method (mid", "page_idx": 4}, {"type": "text", "text": "dle) updates the reward function by simply comparing the expert trajectory prefix $\\{(s_{i}^{E},a_{i}^{E})\\}_{i=0}^{t}$ and the incomplete trajectory $\\{(s_{i}^{\\prime},a_{i}^{\\prime})\\}_{i=0}^{t}$ with the same length generated by the learned policy starting from the expert's initial state $s_{0}^{E}$ . This kind of reward update is myopic as it does not consider for the future. Since it runs standard IRL on the trajectory prefix observed so far, it will always regard the current state-action pair as the terminal state-action pair and thus has no ability to consider the state-action pairs in the future. In contrast, our special design gives the algorithm the ability to consider for the future. Instead of only comparing incomplete trajectory prefixes, our method compares complete trajectories just as the standard IRL. In specific, we compare the combined complete trajectory $\\{\\{(\\bar{s}_{i}^{E},a_{i}^{E})\\}_{i=0}^{t},\\{(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})\\}_{i>t}\\}$ and the complete trajectory $\\bar{\\{(s_{i}^{\\prime},a_{i}^{\\prime})\\}}_{i\\geq0}$ solely generated by the learned policy. The comparison between the learner prefix $\\{s_{i}^{\\prime},a_{i}^{\\prime}\\}_{i=0}^{t}$ and expert prefix $\\{s_{i}^{E},a_{i}^{E}\\}_{i=0}^{t}$ encouragesteleed rewadfctiontoexplaintexpets strad haviors so far, and the comparison between the suffixes $(\\{s_{i}^{\\prime},a_{i}^{\\prime}\\}_{i>t}$ and $\\{s_{i}^{\\prime\\prime},\\bar{a_{i}^{\\prime\\prime}}\\}_{i>t})$ encourages that we are learning a reward function that is useful for predicting the future. Note that as $t$ increases, the expert trajectory prefix weights more and more in the combined complete trajectory, and eventually we will recover the standard IRL reward update when $t$ goes to infinity. In Theorems 1 and 2, we theoretically guarantee that the proposed reward update can achieve sub-linear (local) regret. This shows the perfect consistency between the intuition and theory. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.2  Theoretical analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To quantify the local regret of Algorithm 1, we have two challenges: (i) Since we only update $\\pi$ by one step at each time $t$ we have $\\pi_{t+1}$ instead of the optimal solution $\\pi_{\\theta_{t}}$ of the lower-level problem (3). The policy $\\pi_{t+1}$ can be far away from $\\pi_{\\theta_{t}}$ and thus the empirical gradient estimate $g_{t}$ can be a bad approximation of the gradient (4). (i) Since we only update $\\theta$ once at each time $t$ instead of finding a near-stationary point $\\theta^{\\prime}$ such that $\\begin{array}{r}{\\vert\\vert\\sum_{i=0}^{t}L_{i}(\\theta^{\\prime};(s_{i}^{E},a_{i}^{E}))\\vert\\vert\\leq\\epsilon}\\end{array}$ as in [25, 26], the gradient norm $\\begin{array}{r l}{\\vert\\vert\\sum_{i=0}^{t}\\nabla L_{i}\\big(\\theta_{t};\\big(s_{i}^{E},a_{i}^{E}\\big)\\big)\\vert\\vert}&{{}}\\end{array}$ is not stabilized under the threshold $\\epsilon$ at every time $t$ . Therefore the local regret (i. $\\begin{array}{r l}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{{}\\sum_{t=0}^{T-1}||\\frac{1}{t+1}\\sum_{i=0}^{t}\\nabla L_{i}(\\theta_{t};(s_{i}^{E},a_{i}^{E}))||^{2})}\\end{array}$ is hard to quantify and may not be sub-linear in $T$ . What's worse, the input data is not i.i.d. but correlated, i.e., the input data $(s_{t}^{E},a_{t}^{E})$ at time $t$ is affected by the input data $(s_{t-1}^{E},a_{t-1}^{E})$ at last step. This correlation makes it even more difficult to quantify the local regret. ", "page_idx": 5}, {"type": "text", "text": "To solve the first challenge, we adopt the idea of two-timescale stochastic approximation [27] where the lower level updates in a faster timescale and the upper level updates in a slower timescale. The policy update is faster because it converges linearly under a fixed reward function [35] while the reward update is slower given that we choose $\\alpha_{t}\\propto\\dot{(t+1)}^{-1/2}$ Intuitively, snce the policy updat is faster than the reward update, the reward parameter is \u201crelatively fixed\" compared to the policy. It is expected that $\\pi_{t+1}$ shall stay close to $\\pi_{\\theta_{t}}$ and at last converges to $\\pi_{\\theta_{t}}$ when $t$ increases. ", "page_idx": 5}, {"type": "text", "text": "To solve the second challenge, we divide our analysis into two steps: (i) We quantify the difference of the gradient norms between the curent correlated state-action distribution $\\mathbb{P}_{t}^{\\pi_{E}}(\\cdot,\\cdot)$ and a stationary state-action distribution for any loss function $L_{i}$ \uff0c $i\\geq0$ Note that $\\mathbb{P}_{t+1}^{\\pi_{E}}(\\cdot,\\cdot)$ is affected by $\\mathbb{P}_{t}^{\\pi_{E}}(\\cdot,\\cdot)$ (ii) We quantify the local regret under the stationary distribution. The benefit of doing so is that the input data is i.i.d. under the stationary distribution, and thus we can cast the online gradient descent method as a stochastic gradient descent method and quantify its local regret. Finally, we can quantify the local regret under the current correlated distribution $\\mathbb{P}_{t}^{\\bar{\\pi_{E}}}(\\cdot,\\cdot)$ by combining (i) and (i). ", "page_idx": 5}, {"type": "text", "text": "We start our analysis with the definitions of stationary state distribution and stationary state-action distribution. For a given policy $\\pi$ , the corresponding stationary state distribution is $\\mu^{\\pi}(s)\\triangleq(1-$ $\\begin{array}{r}{\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{P}_{t}^{\\pi}(s)}\\end{array}$ and the stationary state-action distribution is $\\begin{array}{r}{\\mu^{\\pi}(s,a)\\triangleq(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{P}_{t}^{\\pi}(s,a)}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. The parameterized reward function $r_{\\theta}$ satisfies $\\left|r_{\\theta_{1}}(s,a)-r_{\\theta_{2}}(s,a)\\right|\\leq\\bar{C}_{r}||\\theta_{1}-\\theta_{2}||$ and $||\\nabla_{\\theta}r_{\\theta_{1}}(s,a)-\\nabla_{\\theta}r_{\\theta_{2}}(s,a)||\\leq\\tilde{C}_{r}||\\theta_{1}-\\theta_{2}||$ for any $(\\theta_{1},\\theta_{2})$ and any $(s,a)\\in S\\times A$ where $\\bar{C}_{r}$ and ${\\tilde{C}}_{r}$ are positive constants. ", "page_idx": 5}, {"type": "text", "text": "Assumption 2 (Ergodicity). There exist constants $C_{M}~>~0$ and $\\rho\\ \\in\\ (0,1)$ such that for any policy $\\pi$ and any $t\\,\\geq\\,0$ . the following holds for the Markov chain induced by the policy $\\pi$ and the state transition function $P$ $\\mathrm{sup}_{S_{0}\\sim P_{0}}\\,d_{T V}(\\mathbb{P}_{t}^{\\pi}(\\cdot),\\mu^{\\pi}(\\cdot))\\,\\le\\,C_{M}\\rho^{t}$ where $d_{T V}(\\mathbb{P}_{1}(\\cdot),\\mathbb{P}_{2}(\\cdot))\\ \\triangleq$ $\\begin{array}{r}{\\frac{1}{2}\\int_{s\\in\\mathcal{S}}|\\mathbb{P}_{1}(s)-\\mathbb{P}_{2}(s)|d s}\\end{array}$ is the total variation distance between the two state distributions $\\mathbb{P}_{1}$ and $\\mathbb{P}_{2},\\,S_{0}$ is the initial state,and $\\mathbb{P}_{t}^{\\pi}(\\cdot)$ is the state distribution induced by the policy $\\pi$ at time $t$ ", "page_idx": 5}, {"type": "text", "text": "Assumptions 1-2 are common in RL [36, 37, 38, 39]. Assumption 2 holds for any time-homogeneous Markov chain with finite state space or any uniformly ergodic Markov chain with general state space. Proposition1.SupposeAssumptions $I{-}2$ hold and $\\textstyle\\alpha_{t}\\in(0,{\\frac{1-\\gamma}{\\lambda}})$ , we have the following relation for any $i\\geq0$ and any $\\theta_{t},t\\geq0$ $\\begin{array}{r l}&{\\big|E_{(S_{i}^{E},A_{i}^{E})\\sim\\mathbb{P}_{i}^{\\pi_{E}}(\\cdot,\\cdot)}\\big[||\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\\big]-E_{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)}\\big[||\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\\big]\\big|,}\\\\ &{\\leq8C_{M}\\bar{C}_{r}^{2}\\big(\\displaystyle\\frac{2-\\gamma}{1-\\gamma}\\big)^{2}\\rho^{i}\\gamma^{2i},}\\end{array}$ where $(S_{i}^{E},A_{i}^{E})\\sim\\mathbb{P}_{i}^{\\pi_{E}}(\\cdot,\\cdot)$ means that $(S_{i}^{E},A_{i}^{E})$ is drawnfromthe correlated distribution $\\mathbb{P}_{i}^{\\pi_{E}}(\\cdot,\\cdot)$ and $(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)$ means that $(S_{i}^{E},A_{i}^{E})$ isdrawnfromthestationarvdistribution $\\mu^{\\pi_{E}}(\\cdot,\\cdot)$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Proposition 1 quantifies the gap of gradient norms between the current correlated distribution $\\mathbb{P}_{i}^{\\pi_{E}}$ and the stationary distribution $\\mu^{\\pi_{E}}$ . We next quantify the local regret under the stationary distribution $\\mu^{\\pi_{E}}$ with the following lemma: ", "page_idx": 6}, {"type": "text", "text": "Lemma2. Suppose Assumptions 1-2 hold and choose \u03b1\u03bc = (t/2 ,it holds that: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\{(S_{t}^{E},A_{t}^{E})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)\\}_{t\\geq0}}\\bigg[\\displaystyle\\sum_{t=0}^{T-1}||\\frac{1}{t+1}\\sum_{i=0}^{t}\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\\bigg]}\\\\ &{\\leq D_{1}(\\log T+1)+D_{2}\\sqrt{T}+D_{3}\\sqrt{T}(\\log T+1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $D_{1},D_{2}$ and $D_{3}$ are positive constants whose expressions can be found in Appendix B.4. ", "page_idx": 6}, {"type": "text", "text": "Lemma 2 quantifies the local regret under the stationary distribution $\\mu^{\\pi_{E}}$ . With Proposition 1 and Lemma 2, we can quantify the local regret under the current correlated distribution. ", "page_idx": 6}, {"type": "text", "text": "Theorem1 Supse Asptions -2 holdandchose\u03b1 =) ,we have that: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\{(S_{t}^{E},A_{t}^{E})\\sim\\mathbb{P}_{t}^{\\pi_{E}}(\\cdot,\\cdot)\\}_{t\\geq0}}\\bigg[\\displaystyle\\sum_{t=0}^{T-1}||\\displaystyle\\frac{1}{t+1}\\sum_{i=0}^{t}\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\\bigg]}\\\\ &{\\leq\\bigg(D_{1}+\\displaystyle\\frac{8C_{M}\\bar{C}_{r}^{2}(2-\\gamma)^{2}}{(1-\\rho\\gamma^{2})(1-\\gamma)^{2}}\\bigg)(\\log T+1)+D_{2}\\sqrt{T}+D_{3}\\sqrt{T}(\\log T+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 1 is based on Proposition 1 and Lemma 2. It shows that Algorithm 1 achieves sub-linear local regret. Moreover, if the reward function is linear, Algorithm 1 achieves sub-linear regret: ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Suppose the expert reward function $r_{E}$ and the parameterized reward $r_{\\theta}$ are linear, and Assmptions 1-2hold.Choose $\\begin{array}{r}{\\alpha_{t}=\\frac{1-\\gamma}{\\lambda(t+1)(1-\\gamma^{t+1})}}\\end{array}$ we have that: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\{(S_{t}^{E},A_{t}^{E})\\sim\\mathbb{P}_{t}^{\\pi_{E}}(\\cdot,\\cdot)\\}_{t\\geq0}}\\bigg[\\displaystyle\\sum_{t=0}^{T-1}L_{t}(\\theta_{t};(S_{t}^{E},A_{t}^{E}))\\bigg]}\\\\ &{-\\operatorname*{min}_{\\theta}E_{\\{(S_{t}^{E},A_{t}^{E})\\sim\\mathbb{P}_{t}^{\\pi_{E}}(\\cdot,\\cdot)\\}_{t\\geq0}}\\bigg[\\displaystyle\\sum_{t=0}^{T-1}L_{t}(\\theta;(S_{t}^{E},A_{t}^{E}))\\bigg]\\leq D_{4}+D_{5}(\\log T+1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $D_{4}$ and $D_{5}$ are positive constants whose expressions are in Appendix B.6. ", "page_idx": 6}, {"type": "text", "text": "4.3  Meta-Regularization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Since there is only one training trajectory and this trajectory is not complete during the learning process, we need to add a regularization term to avoid overfitting. Inspired by humans\u2019 using relevant experience to help do inference, we introduce the meta-regularization $\\frac{\\lambda}{2}||\\theta-\\bar{\\theta}||^{2}$ where $\\lambda$ is the hyper-parameter and the meta-prior $\\bar{\\theta}$ is learned from \u201crelevant experience\". In specific, we introduce a set of relevant tasks $\\{\\tau_{j}\\}_{j\\sim P_{T}}$ where each task $\\mathcal{T}_{j}$ is an IRL problem and $P_{7}$ is the implicit task distribution. The tasks $\\{\\tau_{j}\\}_{j\\sim P_{T}}$ are relevant in the sense that they share the components $(S,{\\mathcal{A}},\\gamma,P_{0},P)$ of the MDP with our in-trajectory learning problem. However, the expert's reward functions of different tasks are different and are drawn from an unknown reward function distribution. For example, in the stock market case mentioned in the introduction, the experts of different tasks invest in the same stock market but may have different preferences. As standard in meta-learning [40, 41, 42], we assume that the expert's reward function $r_{E}$ of our in-trajectory learning problem is also drawn from the same unknown reward function distribution. Note that the reward functions of the relevant tasks $\\{\\tau_{j}\\}_{j\\sim P_{T}}$ are different from $r_{E}$ even if they are drawn from the same unknown reward function distribution. ", "page_idx": 6}, {"type": "text", "text": "For each task $\\mathcal{T}_{j}$ , there is a batch of trajectories and we divide this batch into two sets, i.e., $\\mathcal{D}_{j}^{\\mathrm{tr}}$ and $\\mathcal{D}_{j}^{\\mathrm{eval}}$ . The training set $\\mathcal{D}_{j}^{\\mathrm{tr}}$ only has one trajectory, just as our in-trajectory learning problem, and the evaluation set $\\mathcal{D}_{j}^{\\mathrm{eval}}$ has abundant trajectories. Define the loss function on a certain data set $\\mathcal{D}\\triangleq\\{\\zeta^{v}\\}_{v=1}^{m}$ as $\\begin{array}{r}{L(\\theta,\\mathcal{D})\\,\\triangleq\\,-\\sum_{v=1}^{m}\\sum_{t=0}^{\\infty}\\gamma^{t}\\log\\pi_{\\theta}(a_{t}^{v}|s_{t}^{v})}\\end{array}$ The goal of each task $\\mathcal{T}_{j}$ isto learn a task-specific adaptation $\\phi_{j}$ using the training set $\\mathcal{D}_{j}^{\\mathrm{tr}}$ , such that $\\phi_{j}$ can minimize the test loss $L(\\phi_{j},{\\mathcal D}_{j}^{\\mathrm{eval}})$ on the evaluation set $\\mathcal{D}_{j}^{\\mathrm{eval}}$ . The goal of meta-regularization is to find a meta-prior $\\bar{\\theta}$ , from which such task-specific adaptations $\\phi_{j}$ can be adapted to all tasks $\\{\\tau_{j}\\}_{j\\sim P_{T}}$ . In specific, meta-regularization [24] proposes a bi-level optimization problem (5). The lower-level problem uses only one trajectory $\\mathcal{D}_{j}^{\\mathrm{tr}}$ to find the task-specific adaptation $\\phi_{j}$ such that the meta-regularized loss function $\\begin{array}{r l}{L(\\phi,\\mathcal{D}_{j}^{\\mathrm{tr}})+\\frac{\\lambda}{2(1-\\gamma)}||\\phi-\\bar{\\theta}||^{2}}&{{}}\\end{array}$ is minimized. The upper-level problem is to find a meta-prior $\\bar{\\theta}$ such that the corresponding task-specific adaptations $\\{\\phi_{j}\\}_{j\\sim P_{T}}$ can minimize the expected loss function $L(\\phi_{j},{\\mathcal D}_{j}^{\\mathrm{eval}})$ over the evaluation sets of all tasks $\\{\\mathcal{T}_{i}\\}_{j\\sim P_{T}}$ ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\bar{\\theta}}~E_{j\\sim P_{T}}\\big[L\\big(\\phi_{j},{\\mathcal D}_{j}^{\\mathrm{eval}}\\big)\\big],\\quad\\mathrm{s.t.}~\\phi_{j}=\\arg\\operatorname*{min}_{\\phi}L\\big(\\phi,{\\mathcal D}_{j}^{\\mathrm{tr}}\\big)+\\frac{\\lambda}{2(1-\\gamma)}\\|\\phi-\\bar{\\theta}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The lower-level loss function in (5) is the offline version of our in-trajectory loss function (1) (i.e., $\\begin{array}{r}{L(\\phi,\\mathcal{D}_{j}^{\\mathrm{tr}})\\,+\\,\\frac{\\lambda}{2(1-\\gamma)}||\\phi-\\bar{\\theta}||^{2}\\,=\\,\\sum_{t=0}^{\\infty}L_{t}\\bigl(\\phi;(s_{t}^{\\mathrm{tr}},a_{t}^{\\mathrm{tr}})\\bigr))}\\end{array}$ Where $(s_{t}^{\\mathrm{{tr}}},a_{t}^{\\mathrm{{tr}}})\\,\\in\\,\\mathcal{D}_{j}^{\\mathrm{{tr}}}$ Our in-trajectory learning problem can also be regarded as to find a task-specific adaptation. Note that the in-trajectory learning problem is online while the lower-level problem in (5) is offline because the in-trajectory problem is ongoing where we keep observing new state-action pairs. In contrast, the lower-level problem in (5) is based on \u201cexperience\" that has already happened. Due to the space limit, we include the algorithm and theoretical guarantees of solving the problem (5) in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We present three experiments to show the effectiveness of MERIT-IRL. We use four baselines for comparisons. (i) IT-IRL: this method is MERIT-IRL without meta-regularization. (i) Naive MERITIRL: this method has the meta-regularization term but uses the naive way (in the middle of Figure 1) to update reward. (i) Naive IT-IRL: this method uses the naive way to update the learned reward and does not have the meta-regularization term. (iv) Hindsight: this method is meta-regularized ML-IRL [5] which can access the complete expert trajectory and uses the standard IRL (visualized in the left of Figure 1) with meta-regularization to update the learned reward. The experiment details are in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "5.1  MuJoCo experiment ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this subsection, we consider the target velocity problem for three MuJoCo robots: HalfCheetah, Walker, and Hopper. The target velocity problem is widely used in meta-RL [43] and meta-IRL [44]. In specific, the robots aim to maintain a target velocity in each task and the target velocity of different tasks is different. To test the performance of MERIT-IRL, we use 10 test tasks whose target velocity is randomly between 1.5 and 2.0. In the test tasks, there is only one expert trajectory and the state-action pairs of this trajectory are sequentially revealed (to MERIT-IRL, IT-IRL, Naive MERIT-IRL, and Naive IT-IRL) in an online fashion. The baseline Hindsight uses the complete expert trajectory to learn a reward function. The ground truth reward is designed as $-|v-v_{\\mathrm{target}}|$ (as in [43]) where $v$ is the current robot velocity and $v_{\\mathrm{target}}$ is the target velocity. To learn the meta-prior $\\bar{\\theta}$ , we use 50 relevant tasks whose target velocity is randomly between O and 3. ", "page_idx": 7}, {"type": "text", "text": "Figures 2a-2c show the in-trajectory learning performance where the $x$ -axis is the time step $t$ of the expert trajectory and the $y$ -axis is the cumulative reward of the learned policy $\\pi_{t}$ when only the first $t$ steps of the expert trajectory are observed. The $x$ -limit is $1,000$ because the trajectory length in MuJoCo is 1, 00o. Note that the baseline \u201cHindsight\" is not in-trajectory learning since it learns from a complete expert trajectory. For comparison, we use two horizontal lines (close to each other) to show the performance of Hindsight and the expert in the figures. Figure 2a shows that MERIT-IRL achieves similar performance with the expert when only $40\\%$ of the complete expert trajectory $t=400$ ) is observed while IT-IRL can only achieve performance close to the expert after observing more than $90\\%$ of the complete expert trajectory $t=900$ ). This shows the effectiveness of the meta-regularization. Naive MERIT-IRL and Naive IT-IRL fail to imitate the expert even if the complete expert trajectory is observed $(t=1,000)$ . This shows the effectiveness of our special design of the reward update. The discussions on Figures 2b and 2c are in Appendix D.2. ", "page_idx": 7}, {"type": "text", "text": "Table 1 shows the results after observing the complete expert trajectory. MERIT-IRL performs much better than IT-IRL, Naive MERIT-IRL, and Naive IT-IRL. MERIT-IRL achieves similar performance with Hindsight and expert. Note that it is not expected that MERIT-IRL outperforms Hindsight since Hindsight uses the complete expert trajectory to learn. ", "page_idx": 7}, {"type": "table", "img_path": "mJZH9w8qgu/tmp/6c2320d0631f2c47c72a37a3a53e421f0b0f16f1394b5548d5b6ad275a71608f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "mJZH9w8qgu/tmp/5a77469374a7a81846b4032191fe1e53f93511f22a793da4d1b95ab065183058.jpg", "img_caption": ["Figure 2: In-trajectory learning performance. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.2 Stock market experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "RL to train a stock trading agent has been widely studied in AI for finance [45, 46, 47]. In this experiment, we use IRL to learn the reward function (i.e., investing preference) of the target investor in a stock market scenario. In specific, we use the real-world data of 30 constituent stocks in Dow Jones Industrial Average from 2021-01-01 to 2022-01-01. We use a benchmark called \u201cFinRL\" [47] to configure the real-world stock data into an MDP environment. The target investor (i.e., expert) has an initial asset of $\\mathbb{S}1$ , 000 and trades stocks on every stock market opening day. The stock market opens 252 days between 2021-01-01 and 2022-01-01, and thus the trajectory length is 252. The reward function of the target investor is defined as $p_{1}-p_{2}$ where $p_{1}$ is the investor's profit which is the money earned from trading stocks subtracting the transaction cost, and $p_{2}$ models the investor's preference of whether willing to take risks. In specific, $p_{2}$ is positive if the investor buys stocks whose turbulence indices are larger than a certain turbulence threshold, and zero otherwise. The value of $p_{2}$ depends on the type and amount of the trading stocks. The turbulence thresholds of different investors are different. The turbulence index measures the price fuctuation of a stock. If the turbulence index is high, the corresponding stock has a high fluctuating price and thus is risky to buy [47]. Therefore, an investor unwilling to take risks has a relatively low turbulence threshold. We include experiment details in Appendix D.3. To test performance, we use 10 test tasks whose turbulence thresholds are randomly between 45 and 50. To learn $\\bar{\\theta}$ , we use 50 relevant tasks whose turbulence thresholds are randomly between 30 and 60. ", "page_idx": 8}, {"type": "text", "text": "Figure 2d shows that MERIT-IRL achieves similar cumulative reward with the expert at $t=140$ which is less than $60\\%$ of the whole trajectory, while the three in-trajectory baselines fail to imitate the expert before the ongoing trajectory terminates. The last row in Table 1 shows that MERIT-IRL achieves similar performance with Hindsight and the expert. More discussions on the results are in Appendix D.3. ", "page_idx": 8}, {"type": "text", "text": "5.3  Learning from a shooter's ongoing trajectory ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This part presents the experiment of learning from an ongoing shooter trajectory. Following [12], we model the shooter's movement as a navigation problem. We build a simulator in Gazebo (Figure 3a) where the shooter moves from the door (lower left corner) to the red target (upper right corner). The learner observes the ongoing trajectory of the shooter and keeps updating the learned reward and policy. In our case, the complete shooter trajectory has the length of 140. Figures $3\\mathbf{b}{-}3\\mathbf{g}$ showour in-trajectory learning performance where the heat maps visualize the learned reward. We normalize the learned reward to $[0,1]$ . We can observe that as the ongoing trajectory is expanding, the learned reward function becomes more and more precise to locate the goal area. When $t=40$ wecannottell the goal area from the heat map (Figure 3b). However, as the time $t$ grows, we can almost locate the goal area when $t=60$ (Figure 3c) and precisely locate the goal area when $t=80$ (Figure 3d). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "mJZH9w8qgu/tmp/212c47be0fa156f4b5f79205900bb26843fbdf8115e188a2e3e3076b659285fc.jpg", "img_caption": ["Figure 3: Learning performance on the active shooting scenario. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 3h shows the policy learning performance. Since there is no ground truth reward in this problem, we use \u201csuccess rate\" to quantify the performance of the learned policy. The success rate is the rate that the learned policy successfully reaches the goal. From 3h, we can see that MERIT-IRL outperforms the other baselines and can achieve $100\\%$ successratewhen $t=80$ (i.e., only observing $57\\bar{\\%}$ of the complete trajectory). Note that we do not include Hindsight and Expert in Figure 3 since theyboth achieve $100\\%$ successrate. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper proposes MERIT-IRL, the first in-trajectory inverse reinforcement learning theoretical framework that learns a reward function while observing an initial portion of a trajectory and keeps updating the learned reward function when extended portions (i.e., new state-action pairs) of the trajectory are observed. Experiments show that MERIT-IRL can imitate the expert from the ongoing expert trajectory before it terminates. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is partially supported by the National Science Foundation through grants ECCS 1846706 and ECCS 2140175. We would like to thank the reviewers for their insightful and constructive sugestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] P. Abbeel and A. Y. Ng, \u201cApprenticeship learning via inverse reinforcement learning,\u201d in International Conference on Machine Learning, pp. 1-8, 2004.   \n[2] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich, \u201cMaximum margin planning,\u201d in International Conference on Machine Learning, Pp. 729-736, 2006.   \n[3] B. D. Ziebart, A. L. Mas, J. A. Bagnell, and A. K. Dey, \u201cMaximum entropy inverse reinforcement learning\" in National Conference on Artificial intelligence, pp. 1433-1438, 2008.   \n[4]  B. D. Ziebart, J. A. Bagnell, and A. K. Dey, \u201cModeling interaction via the principle of maximum causal entropy,\" in International Conference on Machine Learning, pp. 1255-1262, 2010.   \n[5]  S. Zeng, C. Li, A. Garcia, and M. Hong, \u201cMaximum-likelihood inverse reinforcement learning with finite-time guarantees,\" in Advances in Neural Information Processing Systems, 2022.   \n[6]  S. Liu and M. Zhu, \u201c\"Distributed inverse constrained reinforcement learning for multi-agent systems, Advances in Neural Information Processing Systems, vol. 35, pp. 33444-33456, 2022.   \n[7] D. Ramachandran and E. Amir, \u201cBayesian inverse reinforcement learning.)\u201d in International Joint Conference on Artificial Intelligence, pp. 2586-2591, 2007.   \n[8] A. J. Chan and M. van der Schar, \u201cScalable bayesian inverse reinforcement learning\u201d in International Conference on Learning Representations, 2021.   \n[9] N. Rhinehart and K. M. Kitani, \u201cFirst-person activity forecasting with online inverse reinforcement learning,\u201d' in IEEE International Conference on Computer Vision, pp. 3696-3705, 2017.   \n[10] S. Arora, P. Doshi, and B. Banerjee, \u201cOnline inverse reinforcement learning under occlusion\" in International Conference on Autonomous Agents and Multiagent Systems, pp. 1170-1178, 2019.   \n[11]  S. Liu and M. Zhu, \u201cLearning multi-agent behaviors from distributed and streaming demonstrations\"\u2019 Advances in Neural Information Processing Systems, vol. 36, 2024.   \n[12] A. Aghalari, N. Morshedlou, M. Marufuzzaman, and D. Carruth, \u201cInverse reinforcement learning to assess safety of a workplace under an active shooter incident,\" IISE Transactions, vol. 53, no. 12, pp. 1337-1350, 2021.   \n[13]  Q. Sun, X. Gong, and Y.-W. Si, \u201cTransaction-aware inverse reinforcement learning for trading in stock markets,\u201d Applied Intelligence, vol. 53, no. 23, pp. 28186-28206, 2023.   \n[14]  J. Chang and W. Tu, \u201cA stock-movement aware approach for discovering investors' personalized preferences in stock markets,\" in International Conference on Tools with Artificial Intelligence, pp. 275-280, 2018.   \n[15] X. Hu, Y. Chen, L. Ren, and Z. Xu, \u201cInvestor preference analysis: An online optimization approach with missing information,\" Information Sciences, vol. 633, pp. 27-40, 2023.   \n[16]  Y. Xu, W. Gao, and D. Hsu, \u201cReceding horizon inverse reinforcement learning, Advances in Neural Information Processing Systems, vol. 35, pp. 27880-27892, 2022.   \n[17] G. Swamy, D. Wu, S. Choudhury, D. Bagnell, and S. Wu, \u201cInverse reinforcement learning without reinforcement learning,\u201d in International Conference on Machine Learning, pp. 33299- 33318, 2023.   \n[18]  K. Yan, A. Schwing, and Y-X. Wang, \u201c\"A simple solution for offine imitation from observations and examples with possibly incomplete trajectories2? Advances in Neural Information Processing Systems, 2024.   \n[19] M. Sun and X. Ma, \u201cAdversarial imitation learning from incomplete demonstrations,\u2019 in International Joint Conference on Artificial Intelligence, pp. 3513-3519, 2019.   \n[20] D. Xu, F. Zhu, Q. Liu, and P. Zhao, \u201cArail: Learning to rank from incomplete demonstrations,\" Information Sciences, vol. 565, pp. 422-437, 2021.   \n[21] J. Ho and S. Ermon, \u201cGenerative adversarial imitation learning,\u201d\" in Advances in Neural Information Processing Systems, pp. 4572-4580, 2016.   \n[22] G. Qiao, G. Liu, P Poupart, and Z. Xu, \u201cMulti-modal inverse constrained reinforcement leaning from a mixture of demonstrations,\u2019 Advances in Neural Information Processing Systems, vol. 36, 2024.   \n[23] S. Liu and M. Zhu,\u201cMeta inverse constrained reinforcement learning: Convergence guarantee and generalization analysis,\" in International Conference on Learning Representations, 2023.   \n[24]  A. Rajeswaran, C. Finn, S. M. Kakade, and S. Levine, \u201cMeta-learning with implicit gradients,\u201d in Advances in Neural Information Processing Systems, Pp. 113-124, 2019.   \n[25] E. Hazan, K. Singh, and C. Zhang, \u201cEfficient regret minimization in non-convex games,\u201d in International Conference on Machine Learning, pp. 1433-1441, 2017.   \n[26] N. Hallak, P. Mertikopoulos, and V. Cevher, \u201cRegret minimization in stochastic non-convex learning via a proximal-gradient approach, in International Conference on Machine Learning, Pp. 4008-4017, 2021.   \n[27] M. Hong, H.-T. Wai, Z. Wang, and Z. Yang, \u201cA two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic,\u2019 arXiv preprint arXiv:2007.05170, 2020.   \n[28] T. Chen, Y. Sun, Q. Xiao, and W. Yin, \u201cA single-timescale method for stochastic bilevel optimization,\u201d in International Conference on Artificial Intelligence and Statistics, pp. 2466- 2488, 2022.   \n[29] F. Schafer and A. Anandkumar,\"\u201cCompetitive gradient descent,\" Advances in Neural Information Processing Systems, pp. 7625-7635, 2019.   \n[30] V. S. Varma, J. Veetaseveera, R. Postoyan, and I.-C. Morarescu, \u201cDistributed gradient methods to reach a nash equilibrium in potential games\"' in IEEE Conference on Decision and Control, Pp. 3098-3103, 2021.   \n[31]  S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge university press, 2004.   \n[32] J. Zhang, P. Xiao, R. Sun, and Z. Luo, \u201cA single-loop smoothed gradient descent-ascent algorithm for nonconvex-concave min-max problems,\u201d Advances in Neural Information Processing Systems, pp. 7377-7389, 2020.   \n[33] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, \u201cReinforcement learning with deep energy-based policies,\"' in International Conference on Machine Learning, pp. 1352-1361, 2017.   \n[34] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine,\u201c\"Soft actor-critic: Off-olicy maximum entopy deep reinforcement learning with a stochastic actor,\"' in International Conference on Machine Learning, Pp. 1861-1870, 2018.   \n[35] S. Cen, C. Cheng, Y. Chen, Y. Wei, and Y. Chi, \u201cFast global convergence of natural policy gradient methods with entropy regularization,\" Operations Research, vol. 70, no. 4, pp. 2563- 2578, 2022.   \n[36] Z. Zheng, F. Gao, L. Xue, and J. Yang, \u201cFederated Q-learning: Linear regret speedup with low communication cost, arXiv preprint arXiv:2312.15023, 2023.   \n[37] H. Gong and M. Wang, \u201c\"A duality approach for regret minimization in average-award ergodic markov decision processes,\u201d in Conference on Learning for Dynamics and Control, vol. 120, pp. 862-883, 2020.   \n[38] Z. Zheng, H. Zhang, and L. Xue, \u201cFederated Q-learning with reference-advantage decomposition: Almost optimal regret and logarithmic communication cost,\" arXiv preprint arXiv:2405.18795, 2024.   \n[39] Z. Zheng, H. Zhang, and L. Xue, \u201cGap-dependent bounds for Q-learning using referenceadvantage decomposition, arXiv preprint arXiv:2410.07574, 2024.   \n[40] S. Xu and M. Zhu, \u201cEficient gradient approximation method for constrained bilevel optimization\"' in AAAI Conference on Artijficial Intelligence, vol. 37, pp. 12509-12517, 2023.   \n[41] S. Xu and M. Zhu, \u201cMeta value learning for fast policy-centric optimal motion planning,\u201d in Robotics science and systems, 2022.   \n[42] S. Xu and M. Zhu, \u201cOnline constrained meta-learning: provable guarantees for generalization,\" Advances in Neural Information Processing Systems, vol. 36, 2024.   \n[43]  C. Finn, P. Abbeel, and S. Levine, \u201cModel-agnostic meta-learning for fast adaptation of deep networks,\u201d in International Conference on Machine Learning, pp. 1126-1135, 2017.   \n[44] S. K. Seyed Ghasemipour, S. S. Gu, and R. Zemel, \"Smile: Scalable meta inverse reinforcement learning throughcontext-conditional policies,\u201d in Advances in Neural InformationProcessing Systems, Pp. 7881-7891, 2019.   \n[45] Y. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai, \u201cDeep direct reinforcement learning for financial signal representation and trading\\*\" IEEE Transactions on Neural Networks and Learning Systems, vol. 28, no. 3, p. 653-664, 2016.   \n[46] Z. Zhang, S. Zohren, and S. Roberts, \u201cDeep reinforcement learning for rading\u201d The Journal of Financial Data Science, vol. 2, no. 2, pp. 25-40, 2020.   \n[47] X.-Y Liu, H. Yang, J. Gao, and C. D. Wang, \u201cFinrl: Deep reinforcement learning framework to automate trading in quantitative finance,\" in ACM International Conference on AI in Finance, Pp. 1-9, 2021.   \n[48] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey, \u201cHuman behavior modeling with maximum entropy inverse optimal control.\" in AAAI Spring Symposium: Human Behavior Modeling, vol. 92, 2009.   \n[49] M Monfort, . Lu, and BD.Zibart, \u201cItent predition and trajctry forecasting via preditie inverse linear-quadratic regulation,\u201d in AAAI Conference on Artificial Inteligence, pp. 3672- 3678, 2015.   \n[50] S. Gaurav and B. Ziebart, \u201cDiscriminatively learning inverse optimal control models for predicting human intentions,\" in International Conference on Autonomous Agents and MultiAgent Systems, pp. 1368-1376, 2019.   \n[51] W. Krichene, M. Balandat, C. Tomlin, and A. Bayen, \u201c\"The hedge algorithm on a continuum, in International Conference on Machine Learning, Pp. 824-832, 2015.   \n[52] N. Agarwal, A. Gonen, and E. Hazan, \u201cLearning in non-convex games with an optimization oracle,' in Conference on Learning Theory, pp. 18-29, 2019.   \n[53] D. Garg, S. Chakraborty, C. Cundy, J. Song, and S. Ermon, \u201cIq-learn: Inverse soft-q learning for imitation,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 4028-4039, 2021.   \n[54] M.-F. Balcan, M. Khodak, and A. Talwalkar, \u201cProvable guarantees for gradient-based metalearning,\" in International Conference on Machine Learning, pp. 424-433, 2019.   \n[55] K. Xu, E. Ratner, A. Dragan, S. Levine, and C. Finn, \u201cLearning a prior over intent via metainverse reinforcement learning,\u201d in International Conference on Machine Learning, pp. 6952- 6962, 2019.   \n[56] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, \u201cMeta-learning in neural networks: A survey\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 5149-5169, 2021.   \n[57] X.-Y. Liu, Z. Xia, J. Rui, J. Gao, H. Yang, M. Zhu, C. Wang, Z. Wang, and J. Guo, \u201cFinRLMeta: Market environments and benchmarks for data-driven financial reinforcement learning\\*\\* Advances in Neural Information Processing Systems, vol. 35, pp. 1835-1849, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "This appendix consists of four parts: related works, proof, meta-regularization algorithm and convergence guarantee, and Experiment details. ", "page_idx": 13}, {"type": "text", "text": "A Related works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Applying IRL to predict from ongoing trajectories. Papers [48, 49, 50] use standard IRL to predict goals of incomplete (or ongoing) trajectories. In specific, they first learn the reward function corresponding to each potential goal candidate from complete trajectories in the training phase and then use Bayesian methods to pick the most likely goal candidate of incomplete trajectories in the testing phase. However, these works are not in-trajectory learning since they do not learn a reward function from an incomplete ongoing trajectory. ", "page_idx": 13}, {"type": "text", "text": "Online non-convex optimization. This paper casts the in-trajectory learning problem as an online non-convex bi-level optimization problem where at each online iteration, a new state-action pair is input. Current literature on online non-convex optimization has two major categories. The first one is to use the regret originally defined in online convex optimization [51, 52]. However, it assumes to find a global optimal solution of a non-convex optimization problem at each online iteration. Therefore, the second category studies \u201clocal regret\" [25, 26] and uses follow-the-leader-based methods to minimize the local regret. However, the follow-the-leader-based methods need to solve a non-convex optimization problem to obtain a near-stationary point at each online iteration, which can be computationally expensive and time-consuming. If the streaming data arrives at a fast speed, the computation at each online iteration may not be finished before the next data arrives. The computational burden of each online iteration can be mitigated by online gradient descent (OGD) methods where we only partially solve the non-convex optimization problem by one-step gradient descent at each online iteration. While OGD is widely studied in online convex optimization, it is rarely studied in online non-convex optimization. [11] uses OGD to quantify the local regret, however, its analysis only holds when the input data is identically independent distributed (i.i.d.). In contrast, the input data in our problem is not i.i.d. In specific, the input data at time $t$ (i.e, $(S_{t}^{E},A_{t}^{E}))$ is affetedby theinputdata a laststep Gi.e $(S_{t-1}^{E},\\mathring{A}_{t-1}^{E}))$ This temporal corelation btwenany two consecutive input data makes it difficult to analyze the growth rate of the local regret. ", "page_idx": 13}, {"type": "text", "text": "Regularization and meta-learning in IRL. Moreover, the data of in-trajectory learning is extremely lacking since there is only one demonstrated trajectory and this trajectory is not complete during the learning process. The lack of data can easily lead to overfitting and a common way to alleviate this problem is to use regularizers [21, 53]. Inspired by humans\u2019 using relevant experience to help the inference, we introduce a novel regularization method called meta-regularization [24, 54]. Compared to the regularizers commonly used in IRL [21, 53], the meta-regularizer provides humanexperience-like prior information which helps recover the reward function from few data. Similar to the meta-initialization method [54, 43] commonly used in IRL [55], meta-regularization provides an initialization that the algorithm starts at. However, more importantly, meta-regularization also provides a regularization term to avoid overfitting. ", "page_idx": 13}, {"type": "text", "text": "A.1 Distinction from Maximum-likelihood inverse reinforcement learning (ML-IRL) [5] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We discuss our distinctions from ML-IRL from the following three aspects: problem seting, algorithm design, and theoretical analysis. ", "page_idx": 13}, {"type": "text", "text": "Distinction in problem setting. We study in-trajectory IRL and formulate an online optimization problem, while ML-IRL studies standard IRL and formulates an offline optimization problem. ", "page_idx": 13}, {"type": "text", "text": "Distinctions in algorithm design. ML-IRL and our algorithm both update policy and reward in a single loop. However, we propose a novel reward update mechanism specially designed for the in-trajectory learning case. This special design requires to use the current learned policy to complete the expert trajectory, which gives the algorithm the ability to consider for the future. This special design of reward update is novel compared to ML-IRL. ", "page_idx": 13}, {"type": "text", "text": "Distinctions in theoretical analysis. The analysis in our paper is substantially different from that in ML-IRL due to three facts: (1) The input data in our paper is not i.i.d., while the input data in ML-IRL is i.i.d. (2) We solve an online optimization problem, while ML-IRL solves an offline optimization problem. (3) Our analysis holds for continuous state-action space, while the analysis of ML-IRL is limited to finite state-action space. We now discuss the distinctions in theoretical analysis caused by the three facts in detail. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "In our case, the input data is not i.i.d. but temporally correlated, i.e., the input data $(s_{t},a_{t})$ is affected by the input data $\\left(s_{t-1},a_{t-1}\\right)$ at last time step. In contrast, the input data in ML-IRL is i.i.d. sampled from a pre-collected data set. To solve this non i.i.d. issue of the input data, we propose a novel theoretical technique that has three steps (detailed in Subsection 4.2). Step 1: We propose the stationary distribution $\\mu^{\\pi_{E}}$ and quantify the gradient norm difference between the real distribution $\\mathbb{P}_{t}^{\\pi_{E}}$ and this stationary distribution $\\mu^{\\pi_{E}}$ in Proposition 1. Step 2: We quantify the local regret over the stationary distribution in Lemma 2. The benefit of doing this is that the input data can be regarded as i.i.d. sampled from this stationary distribution. Step 3: We combine step 1 and step 2, and quantify the (local) regret over the real distribution, where the data is not i.i.d., in Theorem 1 and Theorem 2. We can see that step 1 and step 3 are to solve the non i.i.d. issue of the input data, so that the corresponding theorem statements (Proposition 1, Theorem 1, and Theorem 2) are novel compared to ML-IRL because ML-IRL does not have this non i.i.d. issue. The only theorem statement relevant to ML-IRL is Lemma 2 in step 2 where we both analyze the algorithm over a stationary distribution, and the data is i.i.d. sampled from the stationary distribution. ", "page_idx": 14}, {"type": "text", "text": "However, Lemma 2 in step 2 still has significant distinctions from ML-IRL because Lemma 2 quantifies the local regret in the context of online optimization, while ML-IRL quantifies convergence in the context of offline optimization. First, the objective function is dynamically changing in the online setting because the learner observes a new state-action pair at each online iteration, while the objective function is fixed in ML-IRL. Second, the local regret contains the term $L_{t}\\big(\\theta_{t};(s_{t}^{E},a_{t}^{E})\\big)$ \uff0c however, $\\theta_{t}$ is computed before the learner knows $(s_{t}^{E},a_{t}^{E})$ This makes it more difficult to quantify the local regret because the learner does not know $L_{t}$ when it computes $\\theta_{t}$ . These two difficulties do not appear in the offline optimization in ML-IRL. To solve these two issues, we need to additionally construct a new time-invariant function $\\bar{L}$ in Appendix B.4 and quantify the convergence of the new function $\\bar{L}$ . Then, in order to quantify the local regret of $\\{L_{t}\\}_{t\\ge0}$ , we need to quantify the difference between the real loss function $\\{L_{t}\\}_{t\\ge0}$ and the constructed loss function $\\bar{L}$ ", "page_idx": 14}, {"type": "text", "text": "Moreover, our theoretical analysis holds for continuous state-action space while the theoretical analysis in ML-IRL is limited to finite state-action space. The extension to continuous state-action space brings new difficulties and requires significant novel analysis. In general, the difficulties stem from two aspects: (1) The constants in ML-IRL, e.g., the smoothness constant of the loss function $L$ and the coefficient of convergence rate, include the term $\\vert S\\vert\\times\\vert A\\vert$ . When the state-action space is continuous, those constants are not finite because $|{\\cal S}|\\times|{\\cal A}|$ is now infinite. To address this issue, we propose new methods to bound those constants. For example, in order to show that the loss function $L$ is smooth, rather than using $\\lVert\\nabla L(\\theta_{1})-\\nabla L(\\theta_{2})\\rVert$ to find the smoothness constant as in ML-IRL, we aim to show that $\\left|\\left|\\nabla^{2}L(\\theta\\bar{\\rangle}\\right|\\right|$ is upper bounded by a constant $C_{L}$ in Lemma A.2 and this constant $C_{L}$ does not rely on $|S|\\times|A|$ . Given that $\\lvert|\\nabla^{2}L(\\theta)\\rvert|\\leq C_{L}$ , the loss function $L$ is $C_{L}$ -smooth. (2) Since the action space $\\boldsymbol{\\mathcal{A}}$ is finite in ML-IRL, their proved properties of the $Q$ -function $Q^{\\mathrm{soft}}$ (e.g., Lipschitz continuity, contraction property, monotonic improvement, and smoothness) can be easily extended to the value function $V^{\\mathrm{soft}}$ by summing over different actions $a\\in{\\mathcal{A}}$ .When the action space becomes continuous, summing over infinitely many different actions does not preserve those properties. Thus we have to propose new methods to prove those propoerties of the value function $\\bar{V}^{\\mathrm{so\\bar{f}t}}$ In specific,we prove the Lipschitz continuity, contraction property,monotonic improvement, and smoothness of the value function $V^{\\mathrm{soft}}$ in Claims 3-5 in Appendix B.4. ", "page_idx": 14}, {"type": "text", "text": "B Proof ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section provides the proof of all the proposition, lemmas, and theorems in the paper. To start with, we first introduce the expression of soft $Q$ -function and soft Bellman policy. ", "page_idx": 14}, {"type": "text", "text": "B.1 Notions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The soft $Q$ -function and soft value function are: ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ_{\\theta,\\pi}^{\\mathrm{soft}}(s,a)\\triangleq r_{\\theta}(s,a)+\\gamma\\int_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)V_{\\theta,\\pi}^{\\mathrm{soft}}(s^{\\prime})d s^{\\prime},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{\\theta,\\pi}^{\\mathrm{soft}}(s)\\triangleq E_{S,A}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\big(r_{\\theta}(S_{t},A_{t})-\\log\\pi(A_{t}|S_{t})\\big)\\bigg|S_{0}=s_{0}^{E}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The soft Bellman policy is as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\pi_{\\theta}(a|s)=\\displaystyle\\frac{\\exp(Q_{\\theta}^{\\mathrm{soft}}(s,a))}{\\exp(V_{\\theta}^{\\mathrm{soft}}(s))},}\\\\ &{Q_{\\theta}^{\\mathrm{soft}}(s,a)=r_{\\theta}(s,a)+\\gamma\\displaystyle\\int_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)V_{\\theta}^{\\mathrm{soft}}(s^{\\prime})d s^{\\prime},}\\\\ &{\\quad V_{\\theta}^{\\mathrm{soft}}(s)=\\log\\left(\\displaystyle\\int_{a\\in\\mathcal{A}}\\exp(Q_{\\theta}^{\\mathrm{soft}}(s,a))d a\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It has been proved [33] that the soft Bellman policy $\\pi_{\\theta}$ is the optimal solution of the lower-level problem (4). We define $J_{\\theta}(s)\\,\\triangleq\\,E_{S,A}^{\\pi_{\\theta}}[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{\\theta}(S_{t},A_{t})|S_{0}\\,=\\,s]$ as the expected cumulative reward of policy $\\pi_{\\theta}$ starting from state $s$ and $\\begin{array}{r}{J_{\\theta}(s,a)\\triangleq E_{S,A}^{\\pi_{\\theta}}[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{\\theta}(S_{t},A_{t})|S_{0}=s,A_{0}=a].}\\end{array}$ Lemma 3. We have the gradient $\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\,=\\,E_{S,A}^{\\pi_{\\theta}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}\\,=\\,s,A_{0}\\,=$ $\\begin{array}{r}{a]-E_{S,A}^{\\pi_{\\theta}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}=s].}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Proof.Define $Z_{\\theta}(s,a)\\triangleq\\exp(Q_{\\theta}^{\\mathrm{soft}}(s,a))$ and $Z_{\\theta}(s)\\triangleq\\exp(V_{\\theta}^{\\mathrm{soft}}(s))$ , therefore $Z_{\\theta}$ is smooth in $\\theta$ given that it is a composition of logarithmic, exponential, and linear functions of $r_{\\theta}$ and $r_{\\theta}$ is smooth in $\\theta$ (Assumption 1). ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\log Z_{\\theta}(s)=\\frac{\\displaystyle\\int_{a\\in A}\\nabla_{\\theta}Z_{\\theta}(s,a)d a}{\\displaystyle Z_{\\theta}(s)},}\\\\ &{\\quad=\\int_{a\\in A}\\frac{Z_{\\theta}(s,a)}{\\displaystyle Z_{\\theta}(s)}\\nabla_{\\theta}\\log Z_{\\theta}(s,a)d a,}\\\\ &{\\quad=\\int_{a\\in A}\\pi_{\\theta}(a|s)\\left[\\nabla_{\\theta}r_{\\theta}(s,a)+\\gamma\\int_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)\\nabla_{\\theta}\\log Z_{\\theta}(s^{\\prime})d s^{\\prime}\\right]d a,}\\\\ &{\\quad=\\int_{a\\in A}\\pi_{\\theta}(a|s)\\left[\\nabla_{\\theta}r_{\\theta}(s,a)+\\gamma\\int_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)\\int_{a^{\\prime}\\in A}\\left(\\nabla_{\\theta}r_{\\theta}(s^{\\prime},a^{\\prime})\\right.\\right.}\\\\ &{\\quad\\left.\\quad+\\left.\\gamma\\int_{s^{\\prime}\\in S}P(s^{\\prime\\prime}|s^{\\prime},a^{\\prime})\\nabla_{\\theta}\\log Z_{\\theta}(s^{\\prime\\prime})d s^{\\prime\\prime}\\right)d a^{\\prime}d s^{\\prime}\\right]d a.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Keep the expansion, we can get $\\begin{array}{r}{\\nabla_{\\theta}\\log Z_{\\theta}(s)=E_{S,A}^{\\pi_{\\theta}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}=s]}\\end{array}$ and similarly we can get $\\begin{array}{r}{\\mathrm{\\Sigma}_{\\theta}\\log Z_{\\theta}(s,a)\\,=\\,E_{S,A}^{\\pi_{\\theta}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}\\,=\\,s,A_{0}\\,=\\,a].\\mathrm{~T~}}\\end{array}$ hus we have the gradient $\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)=\\nabla_{\\theta}\\log Z_{\\theta}(s,a)-\\nabla_{\\theta}\\log Z_{\\theta}(s)$ \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B.2 Proof of Lemma 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\sum_{i=0}^{t}L_{i}(\\theta;(s_{i}^{E},a_{i}^{E}))=-\\sum_{i=0}^{t}\\gamma^{i}\\log\\pi_{\\theta}(a_{i}^{E}|s_{i}^{E})}\\end{array}$ . When the dynamics $P$ is deterministic, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla\\sum_{i=0}^{t}L_{i}(\\theta;(s_{i}^{E},a_{i}^{E}))=-\\displaystyle\\sum_{i=0}^{t}\\gamma^{i}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{i}^{E}|s_{i}^{E}),}}\\\\ &{=-\\displaystyle\\sum_{i=0}^{t}\\gamma^{i}\\left[\\nabla_{\\theta}Q_{\\theta}^{\\mathrm{sot}}(s_{i}^{E},a_{i}^{E})-\\nabla_{\\theta}V_{\\theta}^{\\mathrm{sot}}(s_{i}^{E})\\right],}\\\\ &{=-\\displaystyle\\sum_{i=0}^{t}\\gamma^{i}\\left[\\nabla_{\\theta}r_{\\theta}(s_{i}^{E},a_{i}^{E})+\\gamma\\nabla_{\\theta}V_{\\theta}^{\\mathrm{sot}}(s_{i+1}^{E})-\\nabla_{\\theta}V_{\\theta}^{\\mathrm{sot}}(s_{i}^{E})\\right],}\\\\ &{=-\\displaystyle\\sum_{i=0}^{t}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(s_{i}^{E},a_{i}^{E})-\\displaystyle\\sum_{i=1}^{t+1}\\gamma^{i}\\nabla_{\\theta}V_{\\theta}^{\\mathrm{sot}}(s_{i}^{E})+\\displaystyle\\sum_{i=0}^{t}\\gamma^{i}\\nabla_{\\theta}V_{\\theta}^{\\mathrm{sot}}(s_{i}^{E}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=-\\displaystyle\\sum_{i=0}^{t}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(s_{i}^{E},a_{i}^{E})-\\gamma^{i+1}\\nabla_{\\theta}V_{\\theta}^{\\mathrm{sd}}(s_{t+1}^{E})+\\nabla_{\\theta}V_{\\theta}^{\\mathrm{sd}}(s_{0}^{E}),}\\\\ &{\\overset{(a)}{=}-\\displaystyle\\sum_{i=0}^{t}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(s_{i}^{E},a_{i}^{E})-E_{S,A}^{\\pi}\\bigg[\\displaystyle\\sum_{i=t+1}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(S_{i},A_{i})|S_{i}=s_{t+1}^{E}\\bigg]}\\\\ &{+E_{S,A}^{\\pi}\\bigg[\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(S_{i},A_{i})|S_{0}=s_{0}^{E}\\bigg],}\\\\ &{=-\\displaystyle\\sum_{i=0}^{t}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(s_{i}^{E},a_{i}^{E})-E_{S,A}^{\\pi}\\bigg[\\displaystyle\\sum_{i=t+1}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(S_{i},A_{i})|S_{t}=s_{t}^{E},A_{t}=a_{t}^{E}\\bigg]}\\\\ &{+E_{S,A}^{\\pi}\\bigg[\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(S_{i},A_{i})|S_{0}=s_{0}^{E}\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where equality $(a)$ follows from the proof of Lemma 3. ", "page_idx": 16}, {"type": "text", "text": "When the dynamics is stochastic, we can prove that the above gradient is an unbiased estimate of ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\hat{L}_{\\mathrm{lowff}}^{(1)}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where equality $(b)$ follows from the fact that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\{(S_{i}^{E},A_{i}^{E})\\sim\\mathbb{P}_{i}^{\\pi_{E}}(\\cdot,\\cdot)(\\cdot,\\cdot)\\}_{i\\geq0}}\\biggl[E_{S,A}^{\\pi_{\\theta}}\\big[\\displaystyle\\sum_{i=t+1}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(S_{i},A_{i})\\big|S_{t}=S_{t}^{E},A_{t}=A_{t}^{E}\\big]\\biggr],}\\\\ &{=E_{\\{(S_{i}^{E},A_{i}^{E})\\sim\\mathbb{P}_{i}^{\\pi_{E}}(\\cdot,\\cdot)(\\cdot,\\cdot)\\}_{i\\geq0}}\\biggl[E_{S,A}^{\\pi_{\\theta}}\\big[\\displaystyle\\sum_{i=t+1}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta}(S_{i},A_{i})\\big|S_{t+1}=S_{t+1}^{E}\\big]\\biggr],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "because $\\mathbb{P}_{t+1}^{\\pi_{E}}(\\cdot)=\\mathbb{P}_{t}^{\\pi_{E}}(S_{t}^{E},A_{t}^{E})P(\\cdot|S_{t}^{E},A_{t}^{E})$ and $S_{t+1}^{E}\\sim P(\\cdot|S_{t}^{E},A_{t}^{E})$ ", "page_idx": 16}, {"type": "text", "text": "Since we quantify the local regret in expectation in Theorem 1, this unbiased estimate can be used when the dynamics is stochastic. ", "page_idx": 16}, {"type": "text", "text": "B.3 Proof of Proposition 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "From Assumption 2, we know that $\\begin{array}{r}{d_{\\mathrm{TV}}(\\mathbb{P}_{t}^{\\pi_{E}}(\\cdot),\\mu^{\\pi_{E}}(\\cdot))=\\frac{1}{2}\\int_{s\\in\\mathcal{S}}|\\mathbb{P}_{t}^{\\pi_{E}}(s)-\\mu^{\\pi_{E}}(s)|d s\\leq C_{M}\\rho^{t}}\\end{array}$ where the initial state is $s_{0}$ . For any state-action pair $(s,a)\\in S\\times A$ , we know that $\\mathbb{P}_{t}^{\\pi_{E}}(s,a)=$ $\\mathbb{P}_{t}^{\\pi_{E}}(s)\\pi_{E}(a|s)$ and $\\mu^{\\pi_{E}}(s,a)=\\mu^{\\pi_{E}}(s)\\pi_{E}(a|s)$ . Therefore, we have that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\mathrm{TV}}(\\mathbb{P}^{\\pi_{E}}(\\cdot,\\cdot),\\mu^{\\pi_{E}}(\\cdot,\\cdot)),}\\\\ &{=\\displaystyle\\frac{1}{2}\\int_{s\\in\\mathcal{S}}\\int_{a\\in\\mathcal{A}}\\big|\\mathbb{P}_{t}^{\\pi_{E}}(s)\\pi_{E}(a|s)-\\mu^{\\pi_{E}}(s)\\pi_{E}(a|s)|d s d a,}\\\\ &{=\\displaystyle\\frac{1}{2}\\int_{s\\in\\mathcal{S}}\\int_{a\\in\\mathcal{A}}\\big|\\mathbb{P}_{t}^{\\pi_{E}}(s)-\\mu^{\\pi_{E}}(s)|\\pi_{E}(a|s)d s d a,}\\\\ &{=\\displaystyle\\frac{1}{2}\\int_{s\\in\\mathcal{S}}\\big|\\mathbb{P}_{t}^{\\pi_{E}}(s)-\\mu^{\\pi_{E}}(s)|d s,}\\\\ &{\\leq C_{M}\\rho^{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Claim 1. The trajectory of $\\theta_{t}$ is bounded, i.e., $\\begin{array}{r}{||{\\theta}_{t}-\\bar{\\theta}||\\leq\\frac{2\\bar{C}_{r}}{\\lambda}}\\end{array}$ for any $t\\geq0$ ", "page_idx": 17}, {"type": "text", "text": "Proof. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lVert\\theta_{t+1}-\\bar{\\theta}\\rVert=\\lVert\\theta_{t}-\\alpha_{t}g_{t}-\\bar{\\theta}\\rVert,}\\\\ &{=\\left\\lVert\\theta_{t}-\\bar{\\theta}-\\alpha_{t}\\Biggl[\\frac{\\infty}{t_{*}^{3}\\gamma}\\gamma^{i_{\\nabla}}\\varphi v_{\\theta_{t}}(s_{i}^{\\prime},a_{i}^{\\prime})-\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i_{\\nabla}}\\varphi v_{\\theta_{t}}(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})+\\frac{\\lambda(1-\\gamma^{i+1})}{1-\\gamma}(\\theta_{t}-\\bar{\\theta})\\Biggr]\\right\\rVert,}\\\\ &{=\\left\\lVert(1-\\frac{\\alpha_{t}\\lambda(1-\\gamma^{i+1})}{1-\\gamma})(\\theta_{t}-\\bar{\\theta})-\\alpha_{t}\\Biggl[\\sum_{i=0}^{\\infty}\\gamma^{i_{\\nabla}}\\varphi v_{\\theta_{t}}(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})-\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i_{\\nabla}}\\varphi v_{\\theta_{t}}(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})\\Biggr]\\right\\rVert,}\\\\ &{\\overset{(a)}{\\le}(1-\\frac{\\alpha_{t}\\lambda(1-\\gamma^{i+1})}{1-\\gamma})\\lVert\\theta_{t}-\\bar{\\theta}\\rVert+\\alpha_{t}\\Biggr\\rVert\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i_{\\nabla}}\\varphi v_{\\theta_{t}}(s_{i}^{\\prime},a_{i}^{\\prime})-\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i_{\\nabla}}\\varphi r_{\\theta_{t}}(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})\\Biggr\\lVert,}\\\\ &{\\overset{(b)}{\\le}(1-\\frac{\\alpha_{t}\\lambda(1-\\gamma^{i+1})}{1-\\gamma})\\lVert\\theta_{t}-\\bar{\\theta}\\rVert+\\frac{2\\alpha_{t}\\bar{C}_{r}}{1-\\gamma},}\\\\ &{\\le(1-\\frac{\\alpha_{t}\\lambda}{1-\\gamma})\\lVert\\theta_{t}-\\bar{\\theta}\\rVert+\\frac{2\\alpha_{t}\\bar{C}_{r}}{1-\\gamma},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(a)$ follows triangle inequality and $(b)$ uses the upper bound of $\\nabla_{\\boldsymbol{\\theta}}r_{\\boldsymbol{\\theta}}$ in Assumption 1. Therefore, we have the following relation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle||\\theta_{t+1}-\\bar{\\theta}||-\\frac{2\\bar{C}_{r}}{\\lambda}\\leq(1-\\frac{\\alpha_{t}\\lambda}{1-\\gamma})\\bigg(||\\theta_{t}-\\bar{\\theta}||-\\frac{2\\bar{C}_{r}}{\\lambda}\\bigg),}\\\\ {\\displaystyle\\Rightarrow||\\theta_{t}-\\bar{\\theta}||\\leq(1-\\frac{\\alpha_{t}\\lambda}{1-\\gamma})^{t}\\bigg(||\\theta_{0}-\\bar{\\theta}||-\\frac{2\\bar{C}_{r}}{\\lambda}\\bigg)+\\frac{2\\bar{C}_{r}}{\\lambda},}\\\\ {\\displaystyle\\overset{(c)}{=}\\frac{2\\bar{C}_{r}}{\\lambda}\\bigg[1-(1-\\frac{\\alpha_{t}\\lambda}{1-\\gamma})^{t}\\bigg]\\leq\\frac{2\\bar{C}_{r}}{\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(c)$ follows the fact that $\\theta_{0}=\\bar{\\theta}$ and $(d)$ follows the fact that $\\begin{array}{r}{\\alpha_{t}\\leq\\frac{1-\\gamma}{\\lambda}}\\end{array}$ Recall that the loss function $\\begin{array}{r}{L_{i}\\big(\\theta;(s_{i}^{E},a_{i}^{E})\\big)\\ =\\ -\\gamma^{i}\\log\\pi_{\\theta}(a_{i}^{E}|s_{i}^{E})\\,+\\,\\frac{\\lambda\\gamma^{i}}{2}||\\theta\\,-\\,\\bar{\\theta}||^{2}}\\end{array}$ and thus $\\nabla L_{i}(\\theta;(s_{i}^{E},a_{i}^{E}))~=~-\\gamma^{i}[\\nabla_{\\theta}Q_{\\theta}^{\\mathrm{soft}}(s_{i}^{E},a_{i}^{E})~-~\\nabla_{\\theta}V_{\\theta}^{\\mathrm{soft}}(s_{i}^{E})]~+~\\lambda\\gamma^{i}(\\theta~-~\\bar{\\theta}).~~~\\mathrm{Fr}~~~$ om Lemma 3, we know t $\\mathrm{hat}~\\nabla_{\\theta}V_{\\theta}^{\\mathrm{soft}}(s_{i}^{E})~=~E_{S,A}^{\\pi_{\\theta}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}~=~s_{i}^{E}]$ and $\\nabla_{\\theta}Q_{\\theta}^{\\mathrm{soft}}(s_{i}^{E},a_{i}^{E})\\,\\,=$ $\\begin{array}{r l r}{E_{S.A}^{\\pi\\theta}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}}&{=}&{s_{i}^{E},A_{0}}&{=}&{a_{i}^{E}].}\\end{array}$ Then, $\\begin{array}{r l r}{||\\nabla_{\\theta}V_{\\theta}^{\\mathrm{soft}}(s_{i}^{E})||}&{\\leq}&{\\frac{\\bar{C}_{r}}{1-\\gamma}}\\end{array}$ and IIVQof(s,a)l\u2264 ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Now we can see that ", "page_idx": 17}, {"type": "equation", "text": "$$\n||\\nabla L_{i}(\\theta_{t};;;(s_{i}^{E},a_{i}^{E}))||=\\gamma^{i}||\\nabla_{\\theta}Q_{\\theta_{t}}^{\\mathrm{soft}}(s_{i}^{E},a_{i}^{E})-\\nabla_{\\theta}V_{\\theta_{t}}^{\\mathrm{soft}}(s_{i}^{E})+\\lambda(\\theta_{t}-\\bar{\\theta})||,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\leq\\gamma^{i}||\\nabla_{\\theta}Q_{\\theta_{t}}^{\\mathrm{soft}}(s_{i}^{E},a_{i}^{E})-\\nabla_{\\theta}V_{\\theta_{t}}^{\\mathrm{soft}}(s_{i}^{E})+\\lambda(\\theta_{t}-\\bar{\\theta})||\\leq\\gamma^{i}\\bigg(\\displaystyle\\frac{2\\bar{C}_{r}}{1-\\gamma}+2\\bar{C}_{r}\\bigg),}\\\\ &{\\displaystyle\\Rightarrow||\\nabla L_{i}(\\theta_{t};(s_{i}^{E},a_{i}^{E}))||^{2}\\leq4\\bar{C}_{r}^{2}\\gamma^{2i}\\bigg(\\displaystyle\\frac{2-\\gamma}{1-\\gamma}\\bigg)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|E_{(S_{i}^{E},A_{i}^{E})\\sim\\mathbb{P}_{i}^{\\pi}(\\cdot,\\cdot)}\\Big[\\|\\nabla L_{i}(\\theta_{i};(S_{i}^{E},A_{i}^{E}))\\|^{2}\\Big]-E_{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi}\\mathbb{E}_{i}^{\\pi}(\\cdot,\\cdot)}\\Big[\\|\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))\\|^{2}\\Big]\\Big|,}\\\\ &{=\\bigg|\\displaystyle\\int_{s\\in S}\\int_{a\\in A}\\!\\!\\!\\!\\!\\int_{i}^{\\pi}\\!\\!\\!\\varepsilon(s,a)\\|\\nabla L_{i}(\\theta_{t};(s,a))\\|^{2}d a d s}\\\\ &{-\\int_{s\\in S}\\int_{a\\in A}\\mu^{\\pi_{E}}(s,a)\\|\\nabla L_{i}(\\theta_{t};(s,a))\\|^{2}d a d s\\bigg|,}\\\\ &{\\le\\displaystyle\\int_{s\\in S}\\int_{a\\in A}\\!\\!\\!\\!\\int_{a}^{\\pi\\pi}\\!\\!\\!\\varepsilon(s,a)-\\mu^{\\pi_{E}}(s,a)|\\cdot||\\nabla L_{i}(\\theta_{t};(s,a))||^{2}d a d s,}\\\\ &{\\le2C_{M}\\rho^{i}\\cdot4\\bar{C}_{r}^{2}\\gamma^{2i}\\bigg(\\displaystyle\\frac{2-\\gamma}{1-\\gamma}\\bigg)^{2}=8C_{M}\\bar{C}_{r}^{2}\\bigg(\\displaystyle\\frac{2-\\gamma}{1-\\gamma}\\bigg)^{2}\\rho^{i}\\gamma^{2i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma 4. Suppose Assumptions 1-2 hold, the we have the following for any $(s,a)\\in S\\times A$ and any $\\theta_{1},\\theta_{2},t\\colon||\\nabla L_{t}(\\theta_{1},(s,a))\\!-\\!\\nabla L_{t}(\\theta_{2},(s,a))||\\le C_{L}||\\theta_{1}\\!-\\!\\theta_{2}||$ and $\\begin{array}{r}{|Q_{\\theta_{1},\\pi_{\\theta_{1}}}^{s o f t}(s,a)\\!-\\!Q_{\\theta_{2},\\pi_{\\theta_{2}}}^{s o f t}(s,a)|\\leq}\\end{array}$ $C_{Q}||\\theta_{1}-\\theta_{2}||$ where $\\begin{array}{r}{C_{L}=\\frac{2\\tilde{C}_{r}}{1-\\gamma}+\\frac{4\\bar{C}_{r}^{3}}{(1-\\gamma)^{4}}+\\lambda}\\end{array}$ and $\\begin{array}{r}{C_{Q}=\\frac{\\bar{C}_{r}}{1-\\gamma}}\\end{array}$ \uff0c ", "page_idx": 18}, {"type": "text", "text": "Proof. Note that $Q_{\\theta,\\pi_{\\theta}}^{\\mathrm{soft}}=Q_{\\theta}^{\\mathrm{soft}}$ and $\\begin{array}{r}{\\nabla_{\\theta}Q_{\\theta}^{\\mathrm{soft}}(s,a)=E_{S,A}^{\\pi_{\\theta}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}=s,A_{0}=a]}\\end{array}$ (proof of Lemma 3). Therefore, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n||\\nabla_{\\theta}Q_{\\theta}^{\\mathrm{soft}}(s,a)||\\leq\\frac{\\bar{C}_{r}}{1-\\gamma}\\triangleq C_{Q}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We know from Lemma 1 that $\\nabla L_{t}(\\theta;(s_{t}^{E},a_{t}^{E}))=-\\gamma^{t}[\\nabla_{\\theta}Q_{\\theta}^{\\mathrm{soft}}(s_{t}^{E},a_{t}^{E})-\\nabla_{\\theta}V_{\\theta}^{\\mathrm{soft}}(s_{t}^{E})]+\\lambda\\gamma^{t}(\\theta-\\bar{\\theta})$ To find the smoothness constant of $L_{t}$ , we need to compute the Hessian of $L_{t}$ . First, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}^{2}Q_{\\theta}^{\\mathrm{st}}(s,a)=\\nabla_{\\theta}E_{S,A}^{\\pi_{\\theta}}\\underset{t=0}{\\overset{\\sim}{\\sum}}\\gamma^{\\top}\\nabla_{\\theta}P_{\\theta}(S_{t},A_{t})|S_{0}=s,A_{0}=a],}\\\\ &{=\\nabla_{\\theta}^{2}\\pi_{\\theta}^{\\prime}\\boldsymbol{r}(s,a)+\\gamma\\int_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)\\nabla_{\\theta}E_{S,A}^{\\pi_{\\theta}}\\underset{t=0}{\\overset{\\sim}{\\sum}}\\underset{t=0}{\\overset{\\sim}{\\sum}}\\gamma^{\\top}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}=s^{\\prime}|d s^{\\prime},}\\\\ &{=\\nabla_{\\theta}^{2}\\pi_{\\theta}^{\\prime}\\boldsymbol{r}(s,a)}\\\\ &{+\\gamma\\int_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)\\nabla_{\\theta}\\int_{a^{\\prime}\\in A}\\pi_{\\theta}^{\\prime}(a^{\\prime}|s^{\\prime})E_{S,A}^{\\pi_{\\theta}}\\underset{t=0}{\\overset{\\sim}{\\sum}}\\underset{t=0}{\\overset{\\sim}{\\nabla}}\\gamma^{\\top}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}=s^{\\prime},A_{0}=a^{\\prime}|d s^{\\prime},}\\\\ &{=\\nabla_{\\theta}^{2}\\pi_{\\theta}^{\\prime}(s,a)+\\gamma\\int_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)\\int_{a^{\\prime}\\in A}\\underset{t=0}{\\overset{\\leftarrow}{\\sum}}\\underset{t=0}{\\overset{\\pi}{\\sum}}\\big(\\nabla_{\\theta}\\pi_{\\theta}(a^{\\prime}|s^{\\prime})\\cdot E_{S,A}^{\\pi_{\\theta}}\\big|\\underset{t=0}{\\overset{\\infty}{\\sum}}\\gamma^{\\top}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}=s^{\\prime},}\\\\ &{A_{0}=a^{\\prime}\\big)+\\pi_{\\theta}(a^{\\prime}|s^{\\prime})\\cdot\\nabla_{\\theta}E_{S,A}^{\\pi_{\\theta}}\\underset{t=0}{\\overset{\\pi}{\\sum}}\\gamma^{\\top} \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Keep the expansion, we can get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{T}_{\\theta\\theta}^{2}Q_{\\theta}^{\\mathrm{soft}}(s,a)=E_{S,A}^{\\pi_{\\theta}}[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta\\theta}^{2}r_{\\theta}(S_{t},A_{t})\\vert S_{0}=s_{0},A_{0}=a_{0}]}}\\\\ {{\\displaystyle+E_{S,A}^{\\pi_{\\theta}}\\left[\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}\\pi_{\\theta}(A_{i}\\vert S_{i})\\cdot E_{S^{\\prime},A^{\\prime}}^{\\pi_{\\theta}}[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta}r_{\\theta}(S_{t}^{\\prime},A_{t}^{\\prime})\\vert S_{0}^{\\prime}=S_{i},A_{0}^{\\prime}=A_{i}]\\Big\\vert S_{0}=s_{0},A_{0}=a_{0}\\right]}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we take a look at the second term in the above equality: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\pi_{\\theta}(a|s)\\cdot E_{\\delta,A}^{\\pi_{\\theta}}\\!\\!\\!\\prod_{t=0}^{\\infty}\\!\\gamma^{t}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}=s,A_{0}=a],}\\\\ &{=\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\cdot E_{S,A}^{\\pi_{\\theta}}\\!\\!\\!\\prod_{t=0}^{\\infty}\\!\\!\\!\\!\\prod_{\\ell=0}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell=0}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell=0}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell=0}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\!\\!\\prod_{\\ell={\\ell}}^{\\infty}\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{||\\nabla_{\\theta\\theta}^{2}Q_{\\theta}^{\\mathrm{sot}}(s,a)||\\leq\\bigg|\\bigg|E_{S,A}^{\\pi_{\\theta}}[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta\\theta}^{2}r_{\\theta}(S_{t},A_{t})|S_{0}=s_{0},A_{0}=a_{0}]\\bigg|\\bigg|}\\\\ &{\\displaystyle+\\bigg|\\bigg|E_{S^{\\prime},A^{\\prime}}^{\\pi_{\\theta}}\\bigg[\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}\\pi_{\\theta}(A_{i}^{\\prime}|S_{i}^{\\prime})\\cdot E_{S,A}^{\\pi_{\\theta}}[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}=S_{i}^{\\prime},A_{0}=A_{i}^{\\prime}]\\bigg|\\bigg|,}\\\\ &{\\displaystyle\\leq\\cfrac{\\tilde{C}_{r}}{1-\\gamma}+\\sum_{t=0}^{\\infty}\\gamma^{t}\\frac{2\\tilde{C}_{r}^{3}}{(1-\\gamma)^{3}}=\\cfrac{\\tilde{C}_{r}}{1-\\gamma}+\\frac{2\\bar{C}_{r}^{3}}{(1-\\gamma)^{4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, we can get $\\begin{array}{r}{||\\nabla_{\\theta\\theta}^{2}V_{\\theta}^{\\mathrm{soft}}(s)||\\le\\frac{\\tilde{C}_{r}}{1-\\gamma}+\\frac{2\\bar{C}_{r}^{3}}{(1-\\gamma)^{4}}}\\end{array}$ Therefore, we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lvert|\\nabla^{2}L_{t}(\\theta;(s_{t}^{E},a_{t}^{E}))\\rvert|\\leq\\gamma^{t}\\bigg(\\lvert|\\nabla_{\\theta\\theta}^{2}Q_{\\theta}^{\\mathrm{soft}}(s_{t}^{E},a_{t}^{E})\\rvert|+\\lvert|\\nabla_{\\theta\\theta}^{2}V_{\\theta}^{\\mathrm{soft}}(s_{t}^{E})\\rvert|+\\lambda\\bigg),}\\\\ &{\\leq\\gamma^{t}\\bigg(\\displaystyle\\frac{2\\tilde{C}_{r}}{1-\\gamma}+\\displaystyle\\frac{4\\bar{C}_{r}^{3}}{(1-\\gamma)^{4}}+\\lambda\\bigg)\\leq\\displaystyle\\frac{2\\tilde{C}_{r}}{1-\\gamma}+\\displaystyle\\frac{4\\bar{C}_{r}^{3}}{(1-\\gamma)^{4}}+\\lambda\\triangleq C_{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.4Proof of Lemma 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This proof is based on the proof in ML-IRL [5]. The differences are: (i) their proof only holds for finite state-action space while we extend to continuous state-action space; (ii) their analysis is for offline settings while we extend to online settings to quantify the local regret. We first introduce the following claims which serve as building blocks in this subsection. ", "page_idx": 19}, {"type": "text", "text": "Claim 2. For any given policy $\\pi$ and state-actionpair $(s,a)$ it holds that $\\vert Q_{\\theta_{1},\\pi}^{s o f t}(s,a)\\ -$ $Q_{\\theta_{2},\\pi}^{s o f t}(s,a)|\\leq C_{Q}||\\theta_{1}-\\theta_{2}||$ and $|V_{\\theta_{1},\\pi}^{s o f t}(s)-V_{\\theta_{2},\\pi}^{s o f t}(s)|\\leq C_{Q}||\\theta_{1}-\\theta_{2}||.$ ", "page_idx": 19}, {"type": "text", "text": "Proof. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\theta_{1},\\pi}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{2},\\pi}^{\\mathrm{soft}}(s,a),}\\\\ &{=E_{S,A}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\big[r_{\\theta_{1}}(S_{t},A_{t})-\\log\\pi(A_{t}|S_{t})\\big]\\Bigg|S_{0}=s,A_{0}=a\\right]}\\\\ &{-\\;E_{S,A}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\big[r_{\\theta_{2}}(S_{t},A_{t})-\\log\\pi(A_{t}|S_{t})\\big]\\Bigg|S_{0}=s,A_{0}=a\\right],}\\\\ &{=E_{S,A}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\big[r_{\\theta_{1}}(S_{t},A_{t})-r_{\\theta_{2}}(S_{t},A_{t})\\big]\\Bigg|S_{0}=s,A_{0}=a\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Rightarrow|Q_{\\theta_{1},\\pi}(s,a)-Q_{\\theta_{2},\\pi}(s,a)|\\leq\\sum_{t=0}^{\\infty}\\gamma^{t}{\\bar{C}}_{r}||\\theta_{1}-\\theta_{2}||={\\frac{{\\bar{C}}_{r}}{1-\\gamma}}||\\theta_{1}-\\theta_{2}||=C_{Q}||\\theta_{1}-\\theta_{2}||.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, we can get that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|V_{\\theta_{1},\\pi}^{\\mathrm{soft}}(s)-V_{\\theta_{2},\\pi}^{\\mathrm{soft}}(s)|\\le E_{S,A}^{\\pi}\\displaystyle\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}[r_{\\theta_{1}}(S_{t},A_{t})-r_{\\theta_{2}}(S_{t},A_{t})]\\bigg|S_{0}=s\\right],}\\\\ &{\\le\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\bar{C}_{r}||\\theta_{1}-\\theta_{2}||=\\displaystyle\\frac{\\bar{C}_{r}}{1-\\gamma}||\\theta_{1}-\\theta_{2}||=C_{Q}||\\theta_{1}-\\theta_{2}||.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Claim 3. The soft Bellman operator $T_{\\theta}^{s o f t}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathcal{T}_{\\theta}^{s o f t}Q)(s,a)\\triangleq r_{\\theta}(s,a)+\\gamma\\int_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)\\log\\left[\\displaystyle\\int_{a^{\\prime}\\in A}\\exp(Q(s^{\\prime},a^{\\prime}))d a^{\\prime}\\right]d s^{\\prime},}\\\\ &{\\quad(\\mathcal{T}_{\\theta}^{s o f t}V)(s)\\triangleq\\log\\left[\\displaystyle\\int_{a\\in A}\\exp\\left(r_{\\theta}(s,a)+\\gamma\\int_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)V(s^{\\prime})d s^{\\prime}\\right)d a\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "is a contraction map with constant $\\gamma$ ", "page_idx": 20}, {"type": "text", "text": "Proof. It has been proved that $T_{\\theta}^{\\mathrm{soft}}Q$ is a contraction map with constant $\\gamma$ (Appendix A.2 in [33]. Here we show that $T_{\\theta}^{\\mathrm{soft}}V$ is a contraction map with constant $\\gamma$ Define a norm of $V$ as $\\vert\\vert V_{1}-V_{2}\\vert\\vert=\\operatorname*{sup}_{s\\in S}\\vert V_{1}(s)-V_{2}(s)\\vert$ and suppose $||V_{1}^{\\overline{{\\ }}}-V_{2}||=\\epsilon$ . Then we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\theta}^{\\mathrm{sot}}V_{1}(s)=\\log\\biggl[\\int_{a\\in A}\\exp\\biggl(r_{\\theta}(s,a)+\\gamma\\int_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)V_{1}(s^{\\prime})d s^{\\prime}\\biggr)d a\\biggr],}\\\\ &{\\le\\log\\biggl[\\int_{a\\in A}\\exp\\biggl(r_{\\theta}(s,a)+\\gamma\\int_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)[V_{2}(s^{\\prime})+\\epsilon]d s^{\\prime}\\biggr)d a\\biggr],}\\\\ &{=\\log\\biggl[\\int_{a\\in A}\\exp\\biggl(r_{\\theta}(s,a)+\\gamma\\int_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)V_{2}(s^{\\prime})d s^{\\prime}+\\gamma\\epsilon\\biggr)d a\\biggr],}\\\\ &{=\\log\\biggl[\\int_{a\\in A}\\exp(\\gamma\\epsilon)\\exp\\biggl(r_{\\theta}(s,a)+\\gamma\\int_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)V_{2}(s^{\\prime})d s^{\\prime}\\biggr)d a\\biggr],}\\\\ &{=\\mathcal{T}_{\\theta}^{\\mathrm{sot}}V_{2}(s)+\\gamma\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly we can get $T_{\\theta}^{\\mathrm{soft}}V_{1}(s)\\;\\geq\\;T_{\\theta}^{\\mathrm{soft}}V_{2}(s)\\,-\\,\\gamma\\epsilon$ Therefore, $\\lvert\\lvert T_{\\theta}^{\\mathrm{soft}}V_{1}\\,-\\,T_{\\theta}^{\\mathrm{soft}}V_{2}\\lvert\\rvert\\,\\le\\,\\gamma\\epsilon\\,=$ $\\gamma||V_{1}-\\mathrm{\\dot{V}_{2}}||$ \u53e3 ", "page_idx": 20}, {"type": "text", "text": "$Q_{\\theta_{t},\\pi_{t+1}}^{s o f t}(s,a)\\ge T_{\\theta_{t}}^{s o f t}(Q_{\\theta_{t},\\pi_{t}}^{s o f t})(s,a)$ $V_{\\theta_{t},\\pi_{t+1}}^{s o f t}(s)\\geq T_{\\theta_{t}}^{s o f t}(V_{\\theta_{t},\\pi_{t}}^{s o f t})(s)\\,f o r$ any $(s,a)$ ", "page_idx": 20}, {"type": "text", "text": "Proof. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\theta_{t},\\pi_{t+1}}^{\\mathrm{soft}}(s,a)\\overset{(i)}{=}r_{\\theta_{t}}(s,a)+\\gamma\\int_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)E_{a^{\\prime}\\sim\\pi_{t+1}}[Q_{\\theta_{t},\\pi_{t+1}}^{\\mathrm{soft}}(s^{\\prime},a^{\\prime})-\\log\\pi_{t+1}(a^{\\prime}|s^{\\prime})]d s^{\\prime},}\\\\ &{\\overset{(i i)}{\\geq}r_{\\theta_{t}}(s,a)+\\gamma\\int_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)E_{A^{\\prime}\\sim\\pi_{t+1}(\\cdot|s^{\\prime})}[Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s^{\\prime},A^{\\prime})-\\log\\pi_{t+1}(A^{\\prime}|s^{\\prime})]d s^{\\prime},}\\\\ &{=r_{\\theta_{t}}(s,a)+\\gamma\\int_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)\\log\\left[\\int_{a^{\\prime}\\in\\mathcal{A}}\\exp(Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s^{\\prime},a^{\\prime}))d a^{\\prime}\\right]d s^{\\prime},}\\\\ &{=\\mathcal{T}_{\\theta}^{\\mathrm{soft}}(Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}})(s,a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(i)$ follow equations (2)-(3) in [34] and $(i i)$ follows policy improvement theorem (Theorem 4 in [33]). Similarly, we can get that ", "page_idx": 20}, {"type": "equation", "text": "$$\nV_{\\theta_{t},\\pi_{t+1}}^{\\mathrm{soft}}(s)=E_{A\\sim\\pi_{t+1}(\\cdot\\vert s)}[Q_{\\theta_{t},\\pi_{t+1}}^{\\mathrm{soft}}(s,A)-\\log\\pi_{t+1}(A\\vert s)],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\geq E_{A\\sim\\pi_{t+1}(\\cdot|s)}[Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s,A)-\\log\\pi_{t+1}(A|s)],}\\\\ &{=\\log\\biggl[\\displaystyle\\int_{a\\in A}\\exp(Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s,a))d a\\biggr],}\\\\ &{=\\log\\biggl[\\displaystyle\\int_{a\\in A}\\exp\\biggl(r_{\\theta_{t}}(s,a)+\\gamma\\displaystyle\\int_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)V_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s^{\\prime})d s^{\\prime}\\biggr)d a\\biggr],}\\\\ &{=\\mathcal{T}_{\\theta_{t}}^{\\mathrm{soft}}(V_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}})(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Claim 5. The following holds for any $(s,a)\\in S\\times A$ and $\\theta_{1},\\,\\theta_{2},\\,t$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n|V_{\\theta_{1}}^{s o f t}(s,a)-V_{\\theta_{2}}^{s o f t}(s,a)|\\leq C_{Q}||\\theta_{1}-\\theta_{2}||.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof Note that $\\nabla_{\\theta}V_{\\theta}^{\\mathrm{soft}}(s)=E_{S,A}^{\\pi_{\\theta}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta}r_{\\theta}(S_{t},A_{t})|S_{0}=s]$ (proof of Lemma 3), therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n||\\nabla_{\\theta}V_{\\theta}^{\\mathrm{soft}}(s)||\\leq\\frac{\\bar{C}_{r}}{1-\\gamma}=C_{Q}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We first show the convergence of $\\pi_{t}$ in Algorithm 1. For any $(s,a)\\in S\\times A$ , we know that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\log\\pi_{t+1}(a|s)-\\log\\pi_{\\theta_{t}}(a|s)|\\le|Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{t}}^{\\mathrm{soft}}(s,a)|+|V_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s)-V_{\\theta_{t}}^{\\mathrm{soft}}(s)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now we take alook at the term $|Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{t}}^{\\mathrm{soft}}(s,a)|$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\theta_{t-1}}^{\\omega_{\\theta_{t}}}(s,a)-Q_{\\theta_{t}}^{\\omega_{\\theta}}(s,a)|,}\\\\ &{\\le|Q_{\\theta_{t-1}}^{\\omega_{\\theta}}(s,a)-Q_{\\theta_{t-1}}^{\\omega_{\\theta}}(s,a)|+|Q_{\\theta_{t-1},\\pi_{t}}^{\\infty\\theta}(s,a)-Q_{\\theta_{t-1}}^{\\omega_{\\theta}}(s,a)|+|Q_{\\theta_{t-1}}^{\\omega_{\\theta}}(s,a)-Q_{\\theta_{t}}^{\\operatorname{soth}}(s,a)|,}\\\\ &{\\overset{(a)}{\\le}C_{2}||\\theta_{t}-\\theta_{t-1}||+|Q_{\\theta_{t-1},\\pi_{t}}^{\\omega_{\\theta}}(s,a)-Q_{\\theta_{t-1}}^{\\omega_{\\theta}}(s,a)|+C_{2}||\\theta_{t}-\\theta_{t-1}||,}\\\\ &{=2C_{Q}||\\theta_{t}-\\theta_{t-1}||+|Q_{\\theta_{t-1},\\pi_{t}}^{\\omega_{\\theta}}(s,a)-Q_{\\theta_{t-1}}^{\\omega_{\\theta}}(s,a)|,}\\\\ &{\\overset{(b)}{=}2C_{Q}||\\theta_{t}-\\theta_{t-1}||+Q_{\\theta_{t-1}}^{\\omega_{\\theta}}(s,a)-Q_{\\theta_{t-1},\\pi_{t}}^{\\omega_{\\theta}}(s,a),}\\\\ &{\\overset{(c)}{\\le}2C_{Q}||\\theta_{t}-\\theta_{t-1}||+Q_{\\theta_{t-1}}^{\\omega_{\\theta}}(s,a)-Q_{\\theta_{t-1}}^{\\omega_{\\theta}}(Q_{\\theta_{t-1},\\pi_{t-1}}^{\\omega_{\\theta}})(s,a),}\\\\ &{\\overset{(d)}{=}2C_{Q}||\\theta_{t}-\\theta_{t-1}||+T_{\\theta_{t-1}}^{\\omega_{\\theta}}(Q_{\\theta_{t-1}}^{\\omega_{\\theta}})(s,a)-T_{\\theta_{t-1}}^{\\omega_{\\theta}}(Q_{\\theta_{t-1},\\pi_{t-1}}^{\\omega_{\\theta}})(s,a),}\\\\ &{\\overset{\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(a)$ follows Lemma 2 and Claim 2, $(b)$ follows the fact that $\\pi_{\\theta}$ is the optimal solution, $(c)$ followsClaim4, (d fllows the fact that Q is a fixed pointof $\\mathcal{T}_{\\theta_{t-1}}^{\\mathrm{soft}}$ (Theorem n [3),and $(e)$ follows Claim 3. ", "page_idx": 21}, {"type": "text", "text": "Similarly we can bound the tem $|V_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s)-V_{\\theta_{t}}^{\\mathrm{soft}}(s)|$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{|V_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s)-V_{\\theta_{t}}^{\\mathrm{soft}}(s)|,}&\\\\ &{\\le|V_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s)-V_{\\theta_{t-1},\\pi_{t}}^{\\mathrm{soft}}(s)|+|V_{\\theta_{t-1},\\pi_{t}}^{\\mathrm{soft}}(s)-V_{\\theta_{t-1}}^{\\mathrm{soft}}(s)|+|V_{\\theta_{t-1}}^{\\mathrm{soft}}(s)-V_{\\theta_{t-1},\\pi_{t-1}}^{\\mathrm{soft}}(s)|,}&\\\\ &{\\overset{(f)}{\\le}C_{Q}||\\theta_{t}-\\theta_{t-1}||+|V_{\\theta_{t-1},\\pi_{t}}^{\\mathrm{soft}}(s)-V_{\\theta_{t-1}}^{\\mathrm{soft}}(s)|+C_{Q}||\\theta_{t}-\\theta_{t-1}||,}&\\\\ &{\\overset{(g)}{\\le}2C_{Q}||\\theta_{t}-\\theta_{t-1}||+V_{\\theta_{t-1}}^{\\mathrm{soft}}(s)-\\mathcal{T}_{\\theta_{t-1}}^{\\mathrm{soft}}(V_{\\theta_{t-1},\\pi_{t}}^{\\mathrm{soft}})(s),}&\\\\ &{\\le2C_{Q}||\\theta_{t}-\\theta_{t-1}||+\\gamma|V_{\\theta_{t-1}}^{\\mathrm{soft}}(s)-V_{\\theta_{t-1},\\pi_{t}}^{\\mathrm{soft}}(s)|,}&\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(f)$ follows Claim 2 and claim 5 and $(e)$ follows Claim 4. ", "page_idx": 21}, {"type": "text", "text": "Now we take a look at the term $\\left|\\left|\\boldsymbol{\\theta}_{t}-\\boldsymbol{\\theta}_{t-1}\\right|\\right|$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n||{\\boldsymbol{\\theta}}_{t}-{\\boldsymbol{\\theta}}_{t-1}||=\\alpha_{t-1}||{\\boldsymbol{g}}_{t-1}||,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\leq\\alpha_{t-1}\\left[\\bigg|\\displaystyle\\bigg|\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta}r_{\\theta_{t-1}}(s_{t}^{\\prime},a_{t}^{\\prime})\\bigg|\\bigg|+\\bigg|\\displaystyle\\bigg|\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\theta}r_{\\theta_{t-1}}(s_{t}^{\\prime\\prime},a_{t}^{\\prime\\prime})\\bigg|\\bigg|+\\frac{\\lambda(1-\\gamma^{t-1})}{1-\\gamma}\\bigg|\\bigg|\\theta_{t-1}-\\bar{\\theta}\\bigg|\\bigg|\\right],}&\\\\ &{\\overset{(h)}{\\leq}\\alpha_{t-1}\\bigg(\\displaystyle\\frac{\\bar{C}_{r}}{1-\\gamma}+\\displaystyle\\frac{\\bar{C}_{r}}{1-\\gamma}+\\frac{\\lambda(1-\\gamma^{t-1})}{1-\\gamma}\\cdot\\displaystyle\\frac{2\\bar{C}_{r}}{\\lambda}\\bigg),}&\\\\ &{\\leq\\frac{4\\alpha_{t-1}\\bar{C}_{r}}{1-\\gamma},\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}&{0}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $(h)$ follows claim 1. Recall that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\log\\pi_{t+1}(a|s)-\\log\\pi_{\\theta_{t}}(a|s)|\\leq|Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{t}}^{\\mathrm{soft}}(s,a)|+|V_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s)-V_{\\theta_{t}}^{\\mathrm{soft}}(s)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Summing from $i=0$ to $t$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n(n-1)}\\Bigg[\\underset{i=n}{\\overset{p}{\\prod}}\\Bigg[\\underset{s_{u=1}}{\\overset{p}{\\prod}}\\Bigg(s_{u}^{h_{u}},(s_{u})-Q_{u}^{h_{u}}(s_{u})\\Bigg)+|\\mathcal{V}_{u,v}^{h_{u}}(s_{u})-\\mathcal{V}_{u}^{h_{u}}(s_{u})|\\Bigg],}\\\\ &{\\leq\\underset{s_{u=1}}{\\overset{p}{\\sum}}\\Bigg[\\underset{s_{u=1}}{\\overset{p}{\\prod}}\\Bigg(Q_{u,v_{1}}^{h_{u}},(s_{u})+\\underset{s_{u}^{h_{u}}\\sim\\mathcal{V}_{u}^{h_{u}}(s_{u})}{\\sum}Q_{u,v_{1}}^{h_{u}},(s_{u})\\Bigg)+|\\mathcal{V}_{u,v_{1},u_{1}}^{h_{u}}(s_{u})-\\mathcal{V}_{u,v_{1}}^{h_{u}}(s_{u})|\\Bigg]\\Bigg]}\\\\ &{=(1-\\underset{s_{u=1}}{\\overset{p}{\\sum}})\\Bigg[\\underset{s_{u=1}}{\\overset{p}{\\prod}}\\Bigg(Q_{u,v_{1}}^{h_{u}},(s_{u})-Q_{u,v_{1}}^{h_{u}}(s_{u})\\Bigg)+|\\mathcal{V}_{u,v_{1}}^{h_{u}}(s_{u})-\\mathcal{V}_{u,v_{1}}^{h_{u}}(s_{u})|\\Bigg],}\\\\ &{\\overset{(i)}{\\underset{s_{u=1}}{\\leq}}\\underset{s_{u=1}}{\\overset{p}{\\underbrace{\\prod}}}\\Bigg[Q_{u,v_{1}}^{h_{u}},\\Bigg]+\\Bigg(\\underset{s_{u=1}}{\\overset{p}{\\prod}}\\Bigg(Q_{u,v_{1}}^{h_{u}},(s_{u})-Q_{u,v_{1}}^{h_{u}}(s_{u})\\Bigg)+|\\mathcal{V}_{u,v_{1}}^{h_{u}}(s_{u})-\\mathcal{V}_{u,v_{1}}^{h_{u}}(s_{u})|}\\\\ &{=(Q_{u,v_{1}}^{h_{u}},(s_{u})-Q_{u,v_{1}}^{h_{u}}(s_{u\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $(i)$ follows (8), $\\begin{array}{r}{\\bar{D}_{1}=\\frac{16C_{Q}\\bar{C}_{r}}{(1-\\gamma)^{2}}}\\end{array}$ and $\\begin{array}{r}{\\bar{D}_{2}=\\frac{1}{1-\\gamma}\\bigg(|Q_{\\theta_{0}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{0},\\pi_{0}}^{\\mathrm{soft}}(s,a)|+|V_{\\theta_{0},\\pi_{0}}^{\\mathrm{soft}}(s)-}\\end{array}$ we can se that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{V_{\\theta_{0}}^{\\mathrm{soft}}(s)\\vert-\\vert Q_{\\theta_{t}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s,a)\\vert-\\vert V_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s)-V_{\\theta_{t}}^{\\mathrm{soft}}(s)\\vert\\Bigg).\\ \\mathrm{Therefore},}\\\\ &{}&{\\displaystyle\\frac{1}{t}\\sum_{i=0}^{t-1}\\vert\\log\\pi_{i+1}(a\\vert s)-\\log\\pi_{\\theta_{i}}(a\\vert s)\\vert\\leq\\frac{\\bar{D}_{1}}{\\sqrt{t}}+\\frac{\\bar{D}_{2}}{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Define the lossfunction $\\begin{array}{r}{\\bar{L}(\\theta)\\triangleq E_{(S,A)\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)}\\left[-\\log\\pi_{\\theta}(A|S)+\\frac{\\lambda}{2}||\\theta-\\bar{\\theta}||^{2}\\right]}\\end{array}$ , then we can see that $E_{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)}[L_{i}(\\theta;(S_{i}^{E},A_{i}^{E}))]=\\gamma^{i}\\bar{L}(\\theta)$ . Moreover, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\|\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))\\|\\overset{(j)}{\\le}\\gamma^{i}\\lambda\\|\\theta_{t}-\\bar{\\theta}\\|}&\\\\ &{+\\gamma^{i}\\|E_{S,A}^{\\pi_{\\theta_{t}}}[\\displaystyle\\sum_{k=0}^{\\infty}\\gamma^{k}\\nabla_{\\theta}r_{\\theta_{t}}(S_{k},A_{k})|S_{0}=S_{i}^{E}]-E_{S,A}^{\\pi_{\\theta_{t}}}[\\displaystyle\\sum_{k=0}^{\\infty}\\gamma^{k}\\nabla_{\\theta}r_{\\theta_{t}}(S_{k},A_{k})|S_{0}=S_{i}^{E},A_{0}=A_{i}^{E}]\\|}&\\\\ &{\\overset{(k)}{\\le}2\\gamma^{i}\\bar{C}_{r}+\\displaystyle\\frac{2\\gamma^{i}\\bar{C}_{r}}{1-\\gamma},}&{(10)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $(j)$ follows Lemma 3 and $(k)$ follows Claim 1. ", "page_idx": 23}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi}E\\cdot(\\cdot,\\cdot)}[||\\frac{\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))}{\\gamma^{i}}-\\nabla\\bar{L}(\\theta_{t})||^{2}]\\overset{(i^{\\prime})}{\\leq}\\bigg[2\\bar{C}_{r}+\\frac{2\\bar{C}_{r}}{1-\\gamma}\\bigg]^{2},}\\\\ &{\\Rightarrow E_{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi}E\\cdot(\\cdot,\\cdot)}[||\\frac{t-1}{t}\\!-\\!L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))-\\gamma^{i}\\bar{L}(\\theta_{t})||^{2}],}\\\\ &{=\\bigg(\\displaystyle\\frac{1-\\gamma^{t}}{1-\\gamma}\\bigg)^{2}E_{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi}E\\cdot(\\cdot,\\cdot)}[||\\frac{1}{t}\\displaystyle\\sum_{i=0}^{t-1}\\frac{L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))}{\\gamma^{i}}-\\bar{L}(\\theta_{t})||^{2}],}\\\\ &{\\leq\\displaystyle\\frac{1}{t}\\cdot\\frac{4\\bar{C}_{r}^{2}(2-\\gamma)^{2}}{(1-\\gamma)^{4}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\left(l^{\\prime}\\right)$ follows the fact that a bounded variable $X\\in[-a,a]$ has bounded variance at most $a^{2}$ Now we take a look at the term $\\begin{array}{r}{g_{t}-\\frac{1-\\gamma^{t+1}}{1-\\gamma}\\nabla\\bar{L}(\\theta_{t})}\\end{array}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\{X_{i},\\cdots,\\theta\\}^{n},\\theta\\in\\mathcal{X}_{r}}\\left\\{\\left[\\begin{array}{l}{B_{r}-1-\\frac{\\theta^{\\star}(\\theta)}{2}\\nabla L(\\theta)\\right],}\\\\ {+\\frac{E_{\\theta}}{2}\\nabla L(\\theta)\\pi^{\\theta}(\\theta)\\right\\}}\\\\ {+\\frac{E_{\\theta}}{2}\\langle(\\theta),\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle}\\\\ {+\\frac{E_{\\theta}}{2}\\langle(\\theta),\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle,}\\\\ {-E_{\\theta}\\langle\\theta,\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle,}\\\\ {=E_{\\{X_{i},\\theta\\}^{n},\\theta\\in\\mathcal{Y}_{r}}\\langle\\theta,\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\left\\{\\frac{\\theta}{2}\\nabla L(\\theta)\\langle\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\right\\},}\\\\ &{=E_{\\{X_{i},\\theta\\}^{n},\\theta\\in\\mathcal{Y}_{r}}\\langle\\theta,\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle}\\\\ &{+E_{\\theta}\\langle\\theta,\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle\\langle\\theta,\\dot{Y}_{r}\\rangle\\langle\\theta,\\dot{X}_{r}\\rangle\\langle\\theta|\\dot{X}_{r}\\rangle}\\\\ &{=E_{\\theta}^{n},}\\\\ &{-E_{\\theta}^{n}[\\frac{1}{2}\\sqrt{\\pi}]^{-1}\\nabla^{1}\\pi E_{\\theta}\\langle\\dot{X}_{r}\\rangle\\langle\\theta \n$$EALVora,(S,Aa)S=S,A=AP]. ", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From equation (64) in [5], we know that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\biggl|\\biggl|E_{S,A}^{\\pi_{t+1}}[\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(S_{i},A_{i})|S_{0}=s_{0}]-E_{S,A}^{\\pi_{\\theta_{t}}}[\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(S_{i},A_{i})|S_{0}=s_{0}]\\biggr|\\biggr|,}\\\\ &{\\ \\leq\\displaystyle\\frac{2\\bar{C}_{r}}{1-\\gamma}\\int_{s\\in\\mathcal{S}}\\int_{a\\in\\mathcal{A}}|Q_{\\theta_{t}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s,a)|d a d s,}\\\\ &{\\ \\leq\\displaystyle\\frac{2\\bar{C}_{r}C_{d}}{1-\\gamma}\\underset{(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}{\\operatorname*{sup}}\\bigl\\{|Q_{\\theta_{t}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s,a)|\\bigr\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $C_{d}$ is the product of the area of $\\boldsymbol{S}$ and the area of $\\boldsymbol{\\mathcal{A}}$ ", "page_idx": 24}, {"type": "text", "text": "Therefore, we can get that ", "page_idx": 24}, {"type": "equation", "text": "$$\nE_{(S,A)\\sim\\mu^{\\pi_{E}}}\\bigg[\\bigg|\\bigg|g_{t}-\\frac{1-\\gamma^{t+1}}{1-\\gamma}\\nabla\\bar{L}(\\theta_{t})\\bigg|\\bigg|\\bigg]\\leq\\frac{4\\bar{C}_{r}C_{d}}{1-\\gamma}\\operatorname*{sup}_{(s,a)\\in S\\times A}\\{|Q_{\\theta_{t}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s,a)|\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "From (7) and (8), we know that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle(Q_{\\theta_{i},\\pi_{i}}^{\\infty}(s,a)-Q_{\\theta_{i}}^{\\infty}(s,a)\\big|\\le\\gamma|Q_{\\theta_{i-1},\\pi_{i-1}}^{\\infty}(s,a)-Q_{\\theta_{i-1}}^{\\infty}(s,a)|+\\frac{8\\alpha_{i-1}C_{Q}C_{r}}{1-\\gamma},}\\\\ &{\\displaystyle\\Rightarrow\\alpha_{i}|Q_{\\theta_{i-1}}^{\\infty}(s,a)-Q_{\\theta_{i}}^{\\infty}(s,a)|\\le\\alpha_{i-1}\\gamma|Q_{\\theta_{i-1},\\pi_{i-1}}^{\\infty}(s,a)-Q_{\\theta_{i-1}}^{\\infty}(s,a)|+\\frac{8\\alpha_{i-1}^{2}C_{Q}C_{r}}{1-\\gamma},}\\\\ &{\\displaystyle\\Rightarrow\\sum_{i=1}^{t}\\alpha_{i}|Q_{\\theta_{i-1}}^{\\infty}(s,a)-Q_{\\theta_{i}}^{\\infty}(s,a)|\\le\\sum_{i=0}^{t-1}\\alpha_{i}\\gamma|Q_{\\theta_{i-1}}^{\\infty}(s,a)-Q_{\\theta_{i}}^{\\infty}(s,a)|+\\sum_{i=0}^{t-1}\\frac{8\\alpha_{i}^{2}C_{Q}C_{r}}{1-\\gamma},}\\\\ &{\\displaystyle\\Rightarrow(1-\\gamma)\\sum_{i=0}^{t-1}\\alpha_{i}|Q_{\\theta_{i-1}}^{\\infty}(s,a)-Q_{\\theta_{i}}^{\\infty}(s,a)|,}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\displaystyle\\le\\alpha_{0}|Q_{\\theta_{0},\\pi_{0}}^{\\infty}(s,a)-Q_{\\theta_{0}}^{\\infty\\mu}(s,a)|-\\alpha_{i}|Q_{\\theta_{i},\\pi_{i}}^{\\infty}(s,a)-Q_{\\theta_{i}}^{\\infty\\mu}(s,a)|+\\sum_{i=0}^{t-1}\\frac{8\\alpha_{i}^{2}C_{Q}C_{r}}{1-\\gamma},\\qquad\\quad(1\\le j\\le N)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and similarly we can see that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{t}|Q_{\\theta_{i},\\pi_{i}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{i}}^{\\mathrm{soft}}(s,a)|\\le\\displaystyle\\sum_{i=0}^{t-1}\\gamma|Q_{\\theta_{i},\\pi_{i}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{i}}^{\\mathrm{soft}}(s,a)|+\\displaystyle\\frac{8\\alpha_{i}C Q\\bar{C}_{r}}{1-\\gamma},}\\\\ &{\\displaystyle\\Rightarrow(1-\\gamma)\\displaystyle\\sum_{i=1}^{t-1}|Q_{\\theta_{i},\\pi_{i}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{i}}^{\\mathrm{soft}}(s,a)|,}\\\\ &{\\displaystyle\\le|Q_{\\theta_{0},\\pi_{0}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{0}}^{\\mathrm{soft}}(s,a)|-|Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(s,a)-Q_{\\theta_{t}}^{\\mathrm{soft}}(s,a)|+\\displaystyle\\sum_{i=0}^{t-1}\\frac{8\\alpha_{i}C Q\\bar{C}_{r}}{1-\\gamma},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Telescoping from $i=0$ to $t-1$ , we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=0}^{t-1}\\alpha_{i}E_{(S,A)\\sim\\mu^{\\tau}E}\\left[\\left|\\boldsymbol{g}_{i}-\\frac{1-\\gamma^{i+1}}{1-\\gamma}\\nabla\\bar{L}(\\boldsymbol{\\theta}_{i})\\right|\\right|\\right],}\\\\ &{\\displaystyle\\overset{(l)}{\\leq}\\frac{4\\bar{C}_{r}C_{d}}{1-\\gamma}\\sum_{i=0}^{t-1}\\alpha_{i}|Q_{\\theta_{i},\\pi_{i}}^{\\mathrm{soft}}(S,A)-Q_{\\theta_{i}}^{\\mathrm{soft}}(S,A)|,}\\\\ &{\\displaystyle\\overset{(m)}{\\leq}\\frac{4\\bar{C}_{r}C_{d}}{(1-\\gamma)^{2}}\\bigg[\\alpha_{0}|Q_{\\theta_{0},\\pi_{0}}^{\\mathrm{soft}}(S,A)-Q_{\\theta_{0}}^{\\mathrm{soft}}(S,A)|-\\alpha_{t}|Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{soft}}(S,A)-Q_{\\theta_{t}}^{\\mathrm{soft}}(S,A)|+\\sum_{i=0}^{t-1}\\frac{8\\alpha_{i}^{2}C_{Q}\\bar{C}_{r}}{1-\\gamma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $(l)$ follows (12) and $(m)$ follows (13). Similarly, we can see that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=0}^{t-1}E_{(S,A)\\sim\\mu^{\\pi_{E}}}\\left[\\left|\\left|g_{i}-\\frac{1-\\gamma^{i+1}}{1-\\gamma}\\nabla\\bar{L}(\\theta_{i})\\right|\\right|\\right],}\\\\ &{\\displaystyle\\leq\\frac{4\\bar{C}_{r}C_{d}}{1-\\gamma}\\sum_{i=0}^{t-1}|Q_{\\theta_{i},\\pi_{i}}^{\\mathrm{sott}}(S,A)-Q_{\\theta_{i}}^{\\mathrm{sott}}(S,A)|,}\\\\ &{\\displaystyle\\leq\\frac{4\\bar{C}_{r}C_{d}}{(1-\\gamma)^{2}}\\left[|Q_{\\theta_{0},\\pi_{0}}^{\\mathrm{sott}}(S,A)-Q_{\\theta_{0}}^{\\mathrm{sott}}(S,A)|-|Q_{\\theta_{t},\\pi_{t}}^{\\mathrm{sott}}(S,A)-Q_{\\theta_{t}}^{\\mathrm{sott}}(S,A)|+\\sum_{i=0}^{t-1}\\frac{8\\alpha_{i}C_{Q}\\bar{C}_{r}}{1-\\gamma}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\mu_{1},\\eta_{1})\\sum L(\\mu_{1})+\\mathbb{P}(L(\\mu_{1})^{\\top}(\\mu_{1},\\eta_{2}|\\cdot\\alpha_{1})-\\tilde{L}_{1}^{-1}|\\mu_{1}|+\\mu_{1}|),}\\\\ &{\\overset{(a)}{\\leq}L(\\mu_{1})+\\eta_{1}\\sqrt{L(\\mu_{1})}^{\\top}\\nabla L(\\mu_{1})^{\\top}-\\frac{W L(\\zeta,\\eta_{1})}{L^{1}},}\\\\ &{=L(\\mu_{1})+\\eta_{1}\\frac{\\sqrt{L(\\mu_{1})}}{L^{1-\\frac{d}{r}}}\\nabla L(\\mu_{1}|)^{2}+\\alpha_{1}\\sqrt{L(\\mu_{1})}^{\\top}(\\mu_{1}-\\frac{1}{L^{\\frac{d}{r}}}\\nabla L(\\mu_{1})-\\frac{W L^{\\top}C(\\zeta)}{\\sqrt{L(\\mu_{1})}},}\\\\ &{2\\,L(\\mu_{1})+\\alpha_{1}\\frac{1-\\frac{d}{r}}{L^{1-\\frac{d}{r}}}\\nabla L(\\mu_{1}|)^{2}-\\alpha_{1}\\sqrt{L(\\mu_{1})}^{\\top}(\\mu_{1})\\big\\{L(\\mu_{1}-\\frac{1-r}{L^{\\frac{d}{r}}}+\\frac{1+1}{L^{\\frac{d}{r}}}\\nabla L(\\mu_{1})-\\frac{W L^{\\top}C(\\zeta)}{\\sqrt{L(\\mu_{1})}},}\\\\ &{\\overset{(a)}{\\leq}L(\\mu_{1})+\\alpha_{1}\\frac{1-r_{1}^{\\frac{d}{r}}+1}{L^{1-\\frac{d}{r}}}\\nabla L(\\mu_{1}|)^{2}-\\alpha_{1}\\sqrt{L(\\mu_{1})}^{\\top}(\\mu_{1})\\big\\{L(\\mu_{1}-\\frac{1-r_{1}^{\\frac{d}{r}}}{\\sqrt{L(\\mu_{1})}}+\\nabla L(\\mu_{1})\\big\\}-\\frac{W L^{\\top}C(\\zeta)}{\\sqrt{L(\\mu_{1})}},}\\\\ &{\\overset{(b)}{\\leq}L(\\mu_{1})+\\alpha_{1}\\frac{1-r_{1}^{\\frac{d}{r}}+1}{L^{1-\\frac{d}{r}}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $(n)$ follows (8), $(o)$ follows (10), $(p)$ follows (15). Therefore, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\{(S_{r}^{K},A_{*}^{K})\\sim\\mu^{R}(\\cdot,\\cdot)\\}_{1/2}}\\int_{t=0}^{T-1}\\alpha_{T-1}\\big||\\nabla\\bar{L}(\\theta_{t})||^{2}\\Big\\},}\\\\ &{\\leq E_{\\{(S_{r}^{K},A_{*}^{K})\\sim\\mu^{R}(\\cdot,\\cdot)\\}_{2/2}}\\left[\\underset{t=0}{\\overset{\\sum-1}{\\sum}}\\alpha_{t}\\frac{1-\\gamma^{t}}{1-\\gamma}||\\nabla\\bar{L}(\\theta_{t})||^{2}\\right],}\\\\ &{\\leq\\bar{L}(\\theta_{T})-\\bar{L}(\\theta_{0})+\\frac{8\\bar{C}_{r}^{2}C_{d}(2-\\gamma)}{(1-\\gamma)^{3}}E_{\\{(S_{r}^{K},A_{r}^{K})\\sim\\mu^{R}(\\cdot,\\cdot)\\}_{1/2}}\\Big[\\alpha_{0}|Q_{\\theta_{0},m}^{\\otimes{R}}(S_{0}^{K},A_{0}^{K})-Q_{\\theta_{0}}^{\\otimes{I}}(S_{0}^{K},A_{0}^{I})}\\\\ &{-\\alpha_{T}|Q_{\\theta_{T},\\pi_{T}}^{\\otimes{R}}(S_{T}^{{K}},A_{T}^{{K}})-Q_{\\theta_{T}}^{\\otimes{I}}(S_{T}^{{K}},A_{T}^{{K}})|\\Big]+\\left(\\frac{32C_{Q}\\bar{C}_{T}^{2}C_{d}}{(1-\\gamma)^{3}}+\\frac{8\\bar{C}_{r}^{2}C_{L}}{(1-\\gamma)^{2}}\\right)\\underset{i=0}{\\overset{\\sum-1}{\\sum}}\\alpha_{i}^{2},}\\\\ &{\\Rightarrow E_{\\{(S_{r}^{K},A_{t}^{K})\\sim\\mu^{R}(\\cdot,\\cdot)\\}_{1/2}}\\left[\\underset{t=0}{\\overset{\\sum-1}{\\sum}}||\\nabla\\bar{L}(\\theta_{t})||^{2}\\right]\\leq D_{2}\\sqrt{T}+D_{3}\\sqrt{T}(\\log T+1),}\\end{array}\\quad\\quad\\mathrm{(I8)}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathrm{where}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!D_{2}}&{=}&{\\bar{L}(\\theta_{T})\\;-\\;\\bar{L}(\\theta_{0})\\;+\\;\\frac{8\\bar{C}_{r}^{2}C_{d}(2-\\gamma)}{(1-\\gamma)^{3}}E_{\\{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)\\}_{i\\geq0}}\\bigg[\\alpha_{0}|Q_{\\theta_{0},\\pi_{0}}^{\\mathrm{sof}}(S_{0}^{E},A_{0}^{E})\\;-\\;}\\\\ &{}&{\\!\\!\\!\\!\\!\\!\\!Q_{\\theta_{0}}^{\\mathrm{sof}}(S_{0}^{E},A_{0}^{E})|\\!-\\!\\alpha_{T}|Q_{\\theta_{T},\\pi_{T}}^{\\mathrm{soff}}(S_{T}^{E},A_{T}^{E})\\!-\\!Q_{\\theta_{T}}^{\\mathrm{sof}}(S_{T}^{E},A_{T}^{E})|\\bigg]\\;\\mathrm{and}\\;D_{3}=\\frac{2(1-\\gamma)}{\\lambda}\\bigg(\\frac{32C_{Q}\\bar{C}_{r}^{2}C_{d}}{(1-\\gamma)^{3}}+\\frac{8\\bar{C}_{r}^{2}C_{L}}{(1-\\gamma)^{2}}\\bigg)}\\\\ &{}&{.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)\\}_{i\\geq0}}\\bigg[\\displaystyle\\sum_{t=0}^{T-1}||\\displaystyle\\frac{1}{t+1}\\sum_{i=0}^{t}\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\\bigg],}\\\\ &{\\leq2E_{\\{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)\\}_{i\\geq0}}\\bigg[\\displaystyle\\sum_{t=0}^{T-1}||\\displaystyle\\frac{1}{t+1}\\sum_{i=0}^{t}\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))-\\gamma^{i}\\nabla\\bar{L}(\\theta_{t})||^{2}\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\,2E_{\\{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi}E(\\cdot,\\cdot)\\}_{i\\geq0}}\\biggl[\\underset{t=0}{\\overset{T-1}{\\sum}}\\|\\frac{1-\\gamma^{t}}{(t+1)(1-\\gamma)}\\nabla\\bar{L}(\\theta_{t})\\|^{2}\\biggr],}\\\\ &{\\quad\\leq2E_{\\{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi}E(\\cdot,\\cdot)\\}_{i\\geq0}}\\biggl[\\underset{t=0}{\\overset{T-1}{\\sum}}\\|\\frac{1}{t+1}\\underset{t=0}{\\overset{t}{\\sum}}\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))-\\gamma^{i}\\nabla\\bar{L}(\\theta_{t})\\|^{2}\\biggr]}\\\\ &{\\quad+\\,2E_{\\{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi}E(\\cdot,\\cdot)\\}_{i\\geq0}}\\biggl[\\underset{t=0}{\\overset{T-1}{\\sum}}\\|\\nabla\\bar{L}(\\theta_{t})\\|^{2}\\biggr],}\\\\ &{\\quad\\stackrel{(i)}{\\leq}\\biggl[\\underset{t=0}{\\overset{T-1}{\\sum}}\\frac{8\\bar{C}_{i}^{2}(2-\\gamma)^{2}}{(t+1)(1-\\gamma)^{4}}+2E_{\\{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi}E(\\cdot,\\cdot)\\}_{i\\geq0}}\\biggl[\\underset{t=0}{\\overset{T-1}{\\sum}}\\|\\nabla\\bar{L}(\\theta_{t})\\|^{2}\\biggr],}\\\\ &{\\quad\\leq D_{1}(\\log T+1)+D_{2}\\sqrt{T}+D_{3}\\sqrt{T}(\\log T+1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\begin{array}{r}{D_{1}=\\frac{8\\bar{C}_{r}^{2}(2-\\gamma)^{2}}{(1-\\gamma)^{4}}}\\end{array}$ hwsas ally need to take an extra expectation over the dynamics $P$ because we need to roll out $\\pi_{t}$ to formulate $g_{t}$ Here, we omit the expectation over the dynamics. ", "page_idx": 26}, {"type": "text", "text": "B.5Proof of Theorem 1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We know that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{((s^{\\prime},a^{\\prime})^{\\prime},a^{\\prime\\prime}\\leftarrow(s^{\\prime})),\\sigma_{(a^{\\prime})}^{2}}\\left[\\sum_{a=0}^{1}\\left|\\nabla L_{\\lambda}(\\theta_{\\lambda}(s^{\\prime},a^{\\prime},a^{\\prime}))\\right|\\right],}\\\\ &{\\leq E_{((s^{\\prime},a^{\\prime})^{\\prime},a^{\\prime\\prime}\\leftarrow(s^{\\prime})),\\sigma_{(a^{\\prime})}^{2}}\\left[\\sum_{a=0}^{1}|\\gamma_{1}\\rangle\\!\\left(\\nabla L_{\\lambda}(\\theta_{\\lambda}(s^{\\prime},a^{\\prime}))\\right|\\right)\\right]}\\\\ &{+E_{((s^{\\prime},a^{\\prime})^{\\prime},a^{\\prime\\prime}\\leftarrow(s^{\\prime})),\\sigma_{(a^{\\prime})}}\\left[\\sum_{a=0}^{1}|\\nabla L_{\\lambda}(\\theta_{\\lambda}(s^{\\prime},a^{\\prime}))-\\gamma^{\\prime}\\nabla L_{(\\theta)}(a^{\\prime})|\\right],}\\\\ &{\\leq E_{((s^{\\prime},a^{\\prime})^{\\prime\\prime}\\rightarrow a^{\\prime}\\leftarrow(s^{\\prime})),\\sigma_{(a^{\\prime})}}\\left[\\sum_{a=0}^{1}\\left|\\nabla L_{\\lambda}(\\theta_{\\lambda}(s^{\\prime}))\\right|\\right]}\\\\ &{+E_{((s^{\\prime},a^{\\prime})^{\\prime\\prime}\\rightarrow a^{\\prime}\\leftarrow(s^{\\prime})),\\sigma_{(a^{\\prime})}}\\left[\\sum_{a=0}^{1}\\left|\\nabla L_{\\lambda}(\\theta_{\\lambda}(s^{\\prime},a^{\\prime\\prime}))-\\gamma^{\\prime}\\nabla L_{(\\theta)}(a^{\\prime})\\right|\\right],}\\\\ &{\\overset{(a)}{\\leq}E_{((s^{\\prime},a^{\\prime})^{\\prime\\prime}\\rightarrow a^{\\prime}\\leftarrow(s^{\\prime})),\\sigma_{(a^{\\prime})}}\\left[\\sum_{a=0}^{1}\\left|\\nabla L_{\\lambda}(\\theta_{\\lambda}(s^{\\prime},a^{\\prime}))-\\gamma^{\\prime}\\nabla L_{(\\theta)}(a^{\\prime})\\right|\\right],}\\\\ &{\\overset{(b)}{\\leq}E_{((s^{\\prime},a^{\\prime})^{\\prime}\\rightarrow a^{\\prime}\\leftarrow(s^\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $(a)$ follows (18) and (11). ", "page_idx": 26}, {"type": "text", "text": "Therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T-1}E_{\\{(S_{i}^{E},A_{i}^{E})\\sim\\mathbb{P}_{i}^{\\pi_{E}}(\\cdot,\\cdot)\\}_{i\\geq0}}\\left[||\\frac{1}{t+1}\\sum_{i=0}^{t}\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\\right],}\\\\ &{\\displaystyle\\leq\\sum_{t=0}^{T-1}E_{\\{(S_{i}^{E},A_{i}^{E})\\sim\\mathbb{P}_{i}^{\\pi_{E}}(\\cdot,\\cdot)\\}_{i\\geq0}}\\left[\\frac{1}{t+1}\\sum_{i=0}^{t}||\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\\right],}\\\\ &{\\displaystyle\\leq\\sum_{t=0}^{T-1}E_{\\{(S_{i}^{E},A_{i}^{E})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)\\}_{i\\geq0}}\\left[\\frac{1}{t+1}\\sum_{i=0}^{t}||\\nabla L_{i}(\\theta_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\displaystyle\\sum_{t=0}^{T-1}\\displaystyle\\frac{1}{t+1}\\sum_{i=0}^{t}\\biggl[E_{\\{(S_{i}^{R},A_{i}^{R})\\sim\\gamma_{i}^{\\pi}E(\\cdot,\\cdot)\\}_{i\\pm}}\\|\\nabla L_{i}(\\theta_{i};(S_{i}^{E},A_{i}^{E}))\\|^{2}\\biggr]}\\\\ &{=E_{\\{(S_{i}^{R},A_{i}^{R})\\sim\\mu^{\\pi}E(\\cdot,\\cdot)\\}_{i\\pm}}\\|\\biggl|\\nabla L_{i}(\\theta_{i};(S_{i}^{E},A_{i}^{E}))\\|^{2}\\biggr|\\biggr],}\\\\ &{\\stackrel{(s)}{\\leq}D_{1}\\log T+D_{2}\\sqrt{T}+D_{3}\\log T\\sqrt{T}+\\displaystyle\\sum_{t=0}^{T-1}\\displaystyle\\frac{1}{t+1}\\sum_{i=0}^{t}8G_{M}\\tilde{C}_{r}^{2}\\biggl(\\frac{2-\\gamma}{1-\\gamma}\\biggr)^{2}\\rho^{i}\\gamma^{2i},}\\\\ &{\\leq D_{1}\\log T+D_{2}\\sqrt{T}+D_{3}\\log T\\sqrt{T}+\\displaystyle\\sum_{t=0}^{T-1}\\displaystyle\\frac{1}{t+1}\\cdot8G_{M}\\tilde{C}_{r}^{2}\\biggl(\\frac{2-\\gamma}{1-\\gamma}\\biggr)^{2}\\displaystyle\\frac{1}{1-\\rho\\gamma^{2}},}\\\\ &{\\leq\\biggl(D_{1}+\\displaystyle\\frac{8C_{M}\\tilde{C}_{r}^{2}(2-\\gamma)^{2}}{(1-\\rho\\gamma^{2})(1-\\gamma)^{2}}\\biggr)\\log T+D_{2}\\sqrt{T}+D_{3}\\log T\\sqrt{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $(s)$ follows (19) and Proposition 1. Note that to achieve this local regret rate, we actually need to take an extra expectation over the dynamics $P$ because we need to roll out $\\pi_{t}$ to formulate $g_{t}$ . Here, we omit the expectation over the dynamics. ", "page_idx": 27}, {"type": "text", "text": "B.6Proof of Theorem 2 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Suppose the expert reward function $r_{E}$ and the parameterized reward function $r_{\\theta}$ are both linear, i.e., $r_{E}=\\theta_{E}^{\\top}\\phi$ and $r_{\\theta}=\\theta^{\\top}\\phi$ , where $\\phi:S\\times A\\to\\mathbb{R}_{+}^{n}$ is an $n$ -dimensional feature vector such that $\\left||\\phi(s,a)|\\right|\\leq\\bar{C}_{r}$ for any $(s,a)\\in S\\times A$ . The proof follows the similar idea with that of Theorem 1 where follow two-step process: step (i) quantifies the regret under the stationary distribution $\\mu^{\\pi_{E}}(\\cdot,\\cdot)$ and step (i quantifies the difference between the correlated distribution $\\mathbb{P}_{t}^{\\pi_{E}}$ and the stationary distribution $\\mu^{\\pi_{E}}$ . We start our proof with the following claim: ", "page_idx": 27}, {"type": "text", "text": "Claim 6. If the parameterized reward function $r_{\\theta}$ is linear, the function $\\bar{L}(\\theta)$ is $\\lambda$ -strongly convex for any $\\theta$ ", "page_idx": 27}, {"type": "text", "text": "Proof. Recall that $\\bar{L}(\\theta)\\,=\\,E_{(\\bar{S},\\bar{A})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)}[-\\log\\pi_{\\theta}(\\bar{A}|\\bar{S})]+\\textstyle{\\frac{\\lambda}{2}}||\\theta-\\bar{\\theta}||^{2}$ . Define $\\bar{L}(\\theta;(S,A))\\triangleq$ $-\\log\\pi_{\\theta}(\\bar{A}|\\bar{S})+\\textstyle\\frac{\\lambda}{2}||\\theta-\\bar{\\theta}||^{2}$ , from Lemma 3, we can see that ", "page_idx": 27}, {"type": "equation", "text": "$$\n7\\bar{L}(\\theta;(\\bar{S},\\bar{A}))=E_{S,A}^{\\pi_{\\theta}}\\bigg[\\sum_{i=0}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\bigg|S_{0}=\\bar{S}\\bigg]-E_{S,A}^{\\pi_{\\theta}}\\bigg[\\sum_{i=0}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\bigg|S_{0}=\\bar{S},A_{0}=\\bar{A}\\bigg]+\\lambda(\\theta-\\theta_{0})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta\\theta}^{2}\\bar{L}(\\theta;(\\bar{S},\\bar{A})),}\\\\ &{=\\nabla_{\\theta}E_{S,A}^{\\pi_{\\theta}}\\bigg[\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\bigg\\vert S_{0}=\\bar{S}\\bigg]-\\nabla_{\\theta}E_{S,A}^{\\pi_{\\theta}}\\bigg[\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\bigg\\vert S_{0}=\\bar{S},A_{0}=\\bar{A}\\bigg]+\\lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now, we take a look at ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}E_{S,A}^{\\pi_{\\theta}}\\bigg[\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\bigg|S_{0}=\\bar{S},A_{0}=\\bar{A}\\bigg]=\\nabla_{\\theta}\\bigg\\{\\phi(\\bar{S},\\bar{A})}\\\\ &{+\\displaystyle\\int_{s_{1}\\in S}P(s_{1}|\\bar{S},\\bar{A})\\displaystyle\\int_{a_{t+1}\\in A}\\pi_{\\theta}(a_{1}|s_{1})E_{S,A}^{\\pi_{\\theta}}\\bigg[\\displaystyle\\sum_{i=1}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\bigg|S_{1}=s_{1},A_{1}=a_{1}\\bigg]d a_{1}d s_{1}\\bigg\\},}\\\\ &{=\\displaystyle\\int_{s_{1}\\in S}P(s_{1}|\\bar{S},\\bar{A})\\displaystyle\\int_{a_{1}\\in A}\\Biggl\\{\\nabla_{\\theta}\\pi_{\\theta}(a_{1}|s_{1})\\cdot E_{S,A}^{\\pi_{\\theta}}\\bigg[\\displaystyle\\sum_{i=1}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\bigg|S_{1}=s_{1},A_{1}=a_{1}\\Biggr]}\\\\ &{+\\pi_{\\theta}(a_{1}|s_{1})\\cdot\\nabla_{\\theta}E_{S,A}^{\\pi_{\\theta}}\\bigg[\\displaystyle\\sum_{i=1}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\bigg|S_{1}=s_{1},A_{1}=a_{1}\\bigg]\\Biggr\\}d a_{1}d s_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Keep the expansion, we can see that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}E_{S,A}^{\\pi_{\\theta}}\\bigg[\\sum_{i=0}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\bigg|S_{0}=\\bar{S},A_{0}=\\bar{A}\\bigg],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=E_{S,A}^{\\pi_{\\theta}}\\biggl\\{\\sum_{i=1}^{\\infty}\\nabla_{\\theta}\\pi_{\\theta}(A_{i}^{\\prime}|S_{i}^{\\prime})\\cdot E_{S,A}^{\\pi_{\\theta}}\\biggl[\\sum_{i=1}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\biggr|S_{1}=S_{1}^{\\prime},A_{1}=A_{1}^{\\prime}\\biggr]\\biggl|S_{0}^{\\prime}=\\bar{S},A_{0}^{\\prime}=\\bar{A}\\biggr\\}},}}\\\\ {{\\displaystyle\\nabla_{\\theta}E_{S,A}^{\\pi_{\\theta}}\\biggl[\\sum_{i=0}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\biggr|S_{0}=\\bar{S}\\biggr],}}\\\\ {{\\displaystyle=E_{S,A}^{\\pi_{\\theta}}\\biggl\\{\\sum_{i=0}^{\\infty}\\nabla_{\\theta}\\pi_{\\theta}(A_{i}^{\\prime}|S_{i}^{\\prime})\\cdot E_{S,A}^{\\pi_{\\theta}}\\biggl[\\sum_{i=0}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\biggr|S_{0}=S_{0}^{\\prime},A_{0}=A_{0}^{\\prime}\\biggr]\\biggr|S_{0}^{\\prime}=\\bar{S}\\biggr\\}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{(\\delta,\\lambda)\\sim\\mu^{\\pi}(c,s)}\\Bigg\\{\\nabla_{\\theta}E_{\\lambda,\\lambda}^{\\pi*}\\Bigg[\\underset{=0}{\\overset{\\sum}{\\nabla}}z^{j}(\\delta(\\lambda_{i},A_{i})\\Big|S_{0}-\\delta\\bigg)-\\nabla_{\\theta}E_{\\lambda,\\lambda}^{\\pi*}\\Bigg[\\underset{=0}{\\overset{\\sum}{\\sum}}z^{j}(\\delta(S_{i},A_{i})\\Big|S_{0}-\\delta)\\bigg]S_{0}=\\delta,A_{0}=\\lambda\\Bigg\\}}\\\\ &{=E_{(\\delta,\\lambda)\\sim\\mu^{\\pi}(c,s)}E_{\\lambda^{\\pi},\\lambda^{\\pi}}^{\\pi*}\\Bigg\\{\\nabla_{\\theta}\\pi_{0}(A_{0}^{j}|S_{0}^{j})\\cdot E_{\\lambda,\\lambda}^{\\pi*}\\Bigg[\\underset{=0}{\\overset{\\sum}{\\nabla}}z^{j}(\\delta(S_{i},A_{i})\\Big|S_{0}=S_{0}^{\\prime},A_{0}=A_{0}^{j}\\Bigg]\\Bigg|S_{0}^{\\pi}=\\delta_{0}\\Bigg\\}}\\\\ &{+E_{(\\delta,\\lambda)\\sim\\mu^{\\pi}(c,s)}\\Bigg\\{E_{\\delta,\\lambda^{\\pi}}^{\\pi*}\\Bigg\\{\\underset{=0}{\\overset{\\sum}{\\nabla}}\\sum_{i=1}^{\\pi}\\nabla_{\\theta}\\pi_{i}(A_{i}^{j}|S_{i}^{\\pi})\\cdot E_{\\lambda,\\lambda}^{\\pi*}\\Bigg[\\underset{=1}{\\overset{\\sum}{\\sum}}\\gamma^{j}(\\delta(S_{i},A_{i})\\Big|S_{1}=S_{1}^{\\prime},A_{1}=A_{1}^{j}\\Bigg|\\Bigg]S_{0}^{\\pi}=}\\\\ &{-E_{\\delta,\\lambda}^{\\pi*}\\Bigg\\{\\underset{=1}{\\overset{\\sum}{\\nabla}}\\nabla_{\\theta}\\pi_{0}(A_{i}^{j}|S_{i}^{\\pi})\\cdot E_{\\lambda,\\lambda}^{\\pi*}\\Bigg[\\underset{=1}{\\overset{\\sum}{\\sum}}\\gamma^{j}(\\delta(S_{i},A_{i})\\Big|S_{1}=S_{1}^{\\prime},A_{1}=A_{1}^{j}\\Bigg]\\Bigg\\}_{0}=\\delta,A_{0}=\\bar{A}\\Bigg\\}\\Bigg\\},}\\\\ &{=E_{(\n$$= Cov(X) \u2265 0, ", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $(a)$ follows Lemma 3, $\\begin{array}{r}{X\\triangleq E_{S,A}^{\\pi_{\\theta}}[\\sum_{i=0}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})|S_{0}=\\bar{S},A_{0}=A]}\\end{array}$ is a random vector, $\\begin{array}{r}{E X\\triangleq E_{S,A}^{\\pi_{\\theta}}[\\!\\sum_{i=0}^{\\infty}\\gamma^{i}\\phi(S_{i},A_{i})\\vert S_{0}=\\bar{S}]}\\end{array}$ is te expectation of $X$ over the aton distibtion, ad $\\operatorname{Cov}(X)$ is the covariance matrix of the random vector $X$ with itself, which is always positive definite because the policy $\\pi_{\\theta}$ is always stochastic. ", "page_idx": 28}, {"type": "text", "text": "Therefore, we can see that ", "page_idx": 28}, {"type": "equation", "text": "$$\n||\\nabla_{\\theta\\theta}^{2}\\bar{L}(\\theta)||=||\\mathrm{Cov}(X)+\\lambda||\\overset{(b)}{\\geq}\\lambda,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $(b)$ follows (20). ", "page_idx": 28}, {"type": "text", "text": "Step (i). We first quantify the regret under the stationary distribution $\\mu^{\\pi_{E}}(\\cdot,\\cdot)$ .Suppose $\\theta^{*}\\ \\in$ $\\begin{array}{r}{\\arg\\operatorname*{min}\\bar{L}(\\theta)=\\arg\\operatorname*{min}\\frac{1-\\gamma^{T}}{1-\\gamma}\\bar{L}(\\theta)=\\arg\\operatorname*{min}E_{(S_{t}^{E},A_{t}^{E})\\sim\\mu^{\\pi_{E}}}[\\sum_{t=0}^{T-1}L_{t}(\\theta;(S_{t}^{E},A_{t}^{E}))].}\\end{array}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle||\\theta_{t+1}-\\theta^{*}||^{2}=||\\theta_{t}-\\alpha_{t}g_{t}-\\theta^{*}||^{2}=||\\theta_{t}-\\theta^{*}||^{2}+\\alpha_{t}^{2}||g_{t}||^{2}-2\\alpha_{t}\\langle g_{t},\\theta_{t}-\\theta^{*}\\rangle,}\\\\ {\\displaystyle\\Rightarrow E[||\\theta_{t+1}-\\theta^{*}||^{2}],}\\\\ {\\displaystyle\\stackrel{(c)}{\\le}E[||\\theta_{t}-\\theta^{*}||^{2}]+\\frac{16\\alpha_{t}^{2}\\bar{C}_{r}^{2}}{(1-\\gamma)^{2}}-2\\alpha_{t}\\langle\\frac{1-\\gamma^{t+1}}{1-\\gamma}\\nabla\\bar{L}(\\theta_{t}),\\theta_{t}-\\theta^{*}\\rangle-2\\alpha_{t}\\langle g_{t}-\\frac{1-\\gamma^{t+1}}{1-\\gamma}\\nabla\\bar{L}(\\theta_{t}),\\theta_{t}-\\theta^{*}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $(c)$ follows (8) ", "page_idx": 28}, {"type": "text", "text": "Since $\\bar{L}(\\theta)$ is $\\lambda$ -strongly convex, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\bar{L}(\\theta^{*})\\geq\\bar{L}(\\theta_{t})+\\langle\\nabla\\bar{L}(\\theta_{t}),\\theta^{*}-\\theta_{t}\\rangle+\\frac{\\lambda}{2}\\|\\theta_{t}-\\theta^{*}\\|^{2},}\\\\ {\\Rightarrow\\bar{L}(\\theta_{t})-\\bar{L}(\\theta^{*})\\leq\\langle\\nabla\\bar{L}(\\theta_{t}),\\theta_{t}-\\theta^{*}\\rangle-\\frac{\\lambda}{2}\\|\\theta_{t}-\\theta^{*}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{(\\Delta)}{\\leq}\\frac{1-\\gamma}{2\\alpha_{t}(1-\\gamma^{t+1})}\\bigg[\\Big|(\\mu_{t}-\\theta^{*})\\Big|^{2}-\\Big||\\theta_{t+1}-\\theta^{*}\\Big|\\Big|^{2}\\bigg]-\\frac{1-\\gamma}{1-\\gamma^{t+1}}\\langle\\theta_{t}-\\frac{1-\\gamma^{t+1}}{1-\\gamma}\\nabla\\tilde{L}(\\theta_{t}),\\theta_{t}-\\theta^{*}\\rangle}\\\\ &{+\\frac{8\\alpha_{t}C_{\\gamma}^{2}}{(1-\\gamma)(1-\\gamma^{t+1})}-\\frac{\\lambda_{t}}{2}\\Big|\\theta_{t}-\\theta^{*}\\Big|^{2},}\\\\ &{\\Rightarrow E_{(\\delta^{*},A^{*})\\sim\\rho^{*}(\\epsilon,\\gamma)}\\big[\\tilde{L}(\\theta_{t})-\\tilde{L}(\\theta^{*})\\big],}\\\\ &{\\leq\\frac{1-\\gamma-\\lambda\\alpha(1-\\gamma^{t+1})}{2\\alpha_{t}(1-\\gamma^{t+1})}E_{(\\delta^{*},A^{*})\\sim\\rho^{*}(\\epsilon,\\gamma)}\\Big[\\big|\\theta_{t}-\\theta^{*}\\big|^{2}\\big]-\\frac{1-\\gamma}{2\\alpha_{t}(1-\\gamma^{t+1})}E_{(\\delta^{*},A^{*})\\sim\\rho^{*}(\\epsilon,\\gamma)}\\Big[\\big|\\theta_{t+1}-\\gamma^{t+1}\\big|\\mathcal{L}(\\theta_{t})\\big|^{2}\\Big]}\\\\ &{+\\frac{8\\alpha_{t}C_{\\gamma}^{2}}{(1-\\gamma)(1-\\gamma^{t+1})}+\\frac{1-\\gamma}{1-\\gamma^{t+1}}E_{(\\delta^{*},A^{*})\\sim\\rho^{*}(\\epsilon,\\gamma)}\\bigg[\\big|\\theta_{t}-\\frac{1-\\gamma^{t+1}}{1-\\gamma}\\nabla\\tilde{L}(\\theta_{t})\\big|\\cdot\\big|\\theta_{t}-\\theta^{*}\\big|^{2}\\bigg],}\\\\ &{\\stackrel{()}{\\leq}\\frac{1-\\gamma}{1-\\gamma^{t}}\\frac{-\\lambda\\alpha(1-\\gamma^{t+1})}{(1-\\gamma^{t})}E_{(\\delta^{*},A^{*})\\sim\\rho^{*}(\\epsilon,\\gamma)}\\big[\\big|\\theta_{\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $(d)$ follows (21), and $(e)$ follows Claim 1 which shows that the trajectory of $\\theta_{t}$ is bounded and thus there is a positive constant $\\hat{C}$ such that $||\\theta_{t}-\\theta^{*}||\\leq\\hat{C}$ ", "page_idx": 29}, {"type": "text", "text": "Telescoping (22) from $t=0$ to $t=T-1$ , we can see that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{(\\delta_{t}^{k},A_{t}^{k})\\sim\\mu^{k}(\\cdot)\\sim\\nu^{\\delta}(\\cdot)}\\bigg[\\displaystyle\\sum_{t=0}^{T-1}L(\\theta_{t};(S_{t}^{k},A_{t}^{k}))-\\displaystyle\\sum_{t=1}^{T-1}L_{(t}(\\theta^{\\star};(S_{t}^{k},A_{t}^{k}))\\Big],}\\\\ &{=E_{(\\delta_{t}^{k},A_{t}^{k})\\sim\\mu^{k}(\\cdot)}\\bigg[\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{\\frac{1}{T}}(t)_{t}\\bigg]-\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{\\frac{1}{T}}(t^{\\theta})\\bigg],}\\\\ &{\\leq E_{(\\delta_{t}^{k},A_{t}^{k})\\sim\\mu^{k}(\\cdot)}\\bigg[\\displaystyle\\sum_{t=0}^{T-1}L(\\theta_{t})-\\displaystyle\\sum_{t=0}^{T-1}\\bar{L}(\\theta^{\\star})\\bigg],}\\\\ &{\\overset{(i)}{\\leq}\\displaystyle\\frac{1-\\gamma-\\lambda\\alpha\\mu}{2\\alpha_{0}}E_{(\\delta_{t}^{k},A_{t}^{k})\\sim\\mu^{k}(\\cdot)}(|\\|\\theta_{t}-\\theta^{\\star}||^{2})-\\displaystyle\\frac{1-\\gamma}{2\\alpha_{T}-1(1-\\gamma^{T})}E_{(\\delta_{t}^{k},A_{t}^{k})\\sim\\mu^{k}(\\cdot)}\\|\\theta_{T}-\\theta^{\\star}|}\\\\ &{+\\displaystyle\\frac{4\\zeta_{t}C_{0}\\zeta}{(1-\\gamma)^{2}}\\bigg[\\big(\\theta_{\\mu,\\nu_{0}}^{\\alpha}(S,A)-Q_{\\mu_{0}}^{\\alpha_{0}}(S,A)\\big)-|Q_{\\mu_{0},\\nu_{\\tau}}^{\\alpha_{0}}(S,A)-Q_{\\mu_{0}}^{\\alpha_{0}}(S,A)|\\big]+\\displaystyle\\sum_{t=0}^{t-1}\\frac{8\\alpha_{t}C_{0}\\bar{L}_{\\tau}}{1-\\gamma}\\bigg],}\\\\ &{\\leq\\bar{D}_{4}+\\displaystyle\\frac{32\\zeta_{0}Q_{\\tau}C_{0}\\zeta\\zeta_{1}^{2}}{\\lambda(1-\\gamma)}(\\log{T}+1) \n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l}&{\\mathrm{where}\\quad(f)\\quad\\mathrm{follows}\\quad(16),\\quad\\mathrm{and}\\quad\\Bar{D}_{4}}&{{}=}&{{}}&{\\frac{1-\\gamma-\\lambda\\alpha_{0}}{2\\alpha_{0}}E_{(S_{t}^{E},A_{t}^{E})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)}[\\|\\theta_{t}\\ \\ -\\ \\theta^{*}\\|^{2}]\\quad-}\\\\ &{\\frac{1-\\gamma}{2\\alpha_{T-1}(1-\\gamma^{T})}E_{(S_{t}^{E},A_{t}^{E})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)}[\\|\\theta_{T}\\ \\ -\\ \\theta^{*}\\|^{2}]\\quad+}&{{}\\frac{4\\tilde{C}_{r}C_{d}\\hat{C}}{(1-\\gamma)^{2}}[\\|Q_{\\theta_{0},\\pi_{0}}^{\\mathrm{sot}}(S,A)\\ \\ -\\ Q_{\\theta_{0}}^{\\mathrm{sot}}(S,A)\\ ]\\quad-}\\\\ &{|Q_{\\theta_{T},\\pi_{T}}^{\\mathrm{sot}}(S,A)-Q_{\\theta_{T}}^{\\mathrm{sot}}(S,A)|].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Step (ii). We now quantify the difference between the stationary distribution $\\mu^{\\pi_{E}}(\\cdot,\\cdot)$ and the correlated distribution $\\mathbb{P}_{t}^{\\pi_{E}}(\\cdot,\\cdot)$ . Note that $||\\theta_{t}||$ is bounded (proved in Clam 1), the softBellman policy $\\pi_{\\theta}$ is continuous in $\\theta$ and $(s,a)$ , and we assume that the state-action space is bounded, there is a positive constant $C_{\\pi}$ such that $||\\log\\pi_{\\theta_{t}}(a|s)||\\leq C_{\\pi}$ for any $(s,a)\\in S\\times A$ .We start with the following relation: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\biggl|\\biggl|E_{(S_{t}^{E},A_{t}^{E})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)}\\biggl[L_{t}(\\theta_{t};(S_{t}^{E},A_{t}^{E}))\\biggr]-E_{(S_{t}^{E},A_{t}^{E})\\sim\\mathbb{P}_{t}^{\\pi_{E}}(\\cdot,\\cdot)}\\biggl[L_{t}(\\theta_{t};(S_{t}^{E},A_{t}^{E}))\\biggr]\\biggr|,}\\\\ &{=\\gamma^{t}\\biggl|\\biggl|\\int_{s\\in S}\\int_{a\\in A}|\\mathbb{P}_{t}^{\\pi_{E}}(s,a)-\\mu^{\\pi_{E}}(s,a)|\\cdot||\\log\\pi_{\\theta_{t}}(a|s)||d a d s\\biggr|\\biggr|,}\\\\ &{\\leq C\\pi C_{M}\\gamma^{t}\\rho^{t},}\\\\ &{\\Rightarrow\\biggl|\\biggl|E_{(S_{t}^{E},A_{t}^{E})\\sim\\mu^{\\pi_{E}}(\\cdot,\\cdot)}\\biggl[\\underset{t=0}{\\overset{T-1}{\\sum}}L_{t}(\\theta_{t};(S_{t}^{E},A_{t}^{E}))\\biggr]-E_{(S_{t}^{E},A_{t}^{E})\\sim\\mathbb{P}_{t}^{\\pi_{E}}(\\cdot,\\cdot)}\\biggl[\\underset{t=0}{\\overset{T-1}{\\sum}}L_{t}(\\theta_{t};(S_{t}^{E},A_{t}^{E}))\\biggr]\\biggr|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\leq\\frac{C_{\\pi}C_{M}}{1-\\gamma\\rho}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{(\\mathcal{N},\\pi^{\\prime},\\pi^{\\prime})\\sim\\pi^{\\prime}}\\left[\\displaystyle\\sum_{i=0}^{T-1}L_{\\{i,0\\}}(\\varrho_{i}^{\\nu},\\xi_{i}^{\\nu},\\xi_{j}^{\\nu})\\right]-\\eta_{\\mu\\nu}^{\\star}E_{(\\mathcal{N},\\pi^{\\prime},\\mathcal{H})\\sim\\pi^{\\prime}}\\left[\\displaystyle\\sum_{i=1}^{T-1}L_{\\{i,0\\}}(\\varrho_{i}^{\\nu},\\xi_{i}^{\\nu},\\xi_{j}^{\\nu})\\right],}\\\\ &{\\le E_{(\\mathcal{N},\\pi^{\\prime},\\pi^{\\prime})\\sim\\pi^{\\prime}}\\left[\\displaystyle\\sum_{i=0}^{T-1}L_{\\{i,0\\}}(\\varrho_{i}^{\\nu},\\xi_{i}^{\\nu},\\xi_{j}^{\\nu})\\right]-E_{(\\mathcal{N},\\pi^{\\prime},\\mathcal{H})\\sim\\pi^{\\prime}}\\left[\\displaystyle\\sum_{i=1}^{T-1}L_{\\{i,0\\}}(\\varrho_{i}^{\\nu},\\xi_{i}^{\\nu},\\xi_{j}^{\\nu})\\right]}\\\\ &{+E_{(\\mathcal{N},\\pi^{\\prime},\\pi^{\\prime})\\sim\\pi^{\\prime}}\\left[\\displaystyle\\sum_{i=0}^{T-1}L_{\\{i,0\\}}(\\varrho_{i}^{\\nu},\\xi_{i}^{\\nu},\\xi_{j}^{\\nu})-\\displaystyle\\sum_{i=1}^{T-1}L_{\\{i,0\\}}(\\varrho_{i}^{\\nu},\\xi_{i}^{\\nu},\\xi_{j}^{\\nu})\\right]}\\\\ &{+E_{(\\mathcal{N},\\pi^{\\prime},\\pi^{\\prime})\\sim\\pi^{\\prime}}\\left[\\displaystyle\\sum_{i=0}^{T-1}L_{\\{i,0\\}}(\\varrho_{i}^{\\nu},\\xi_{i}^{\\nu},\\xi_{j}^{\\nu})\\right]-\\eta_{\\mu\\nu}^{\\star}E_{(\\mathcal{N},\\pi^{\\prime},\\pi^{\\prime})\\sim\\pi^{\\prime}}\\left[\\displaystyle\\sum_{i=1}^{T-1}L_{\\{i,0\\}}(\\varrho_{i}^{\\nu},\\xi_{i}^{\\nu},\\xi_{j}^{\\nu})\\right]}\\\\ &{\\le E_{(\\mathcal{N},\\pi^{\\prime},\\pi^{\\prime})\\sim \n$$= D4 + Ds(logT + 1), ", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $(f)$ follows (23) and (24), $\\begin{array}{r}{D_{4}=\\frac{2C_{\\pi}C_{M}}{1-\\gamma\\rho}+\\bar{D}_{4}}\\end{array}$ 2C\u03c0CM + D4, and D5 = $\\begin{array}{r}{D_{5}=\\frac{32C_{Q}C_{d}\\hat{C}\\bar{C}_{r}^{2}}{\\lambda(1-\\gamma)^{3}}}\\end{array}$ ", "page_idx": 30}, {"type": "text", "text": "C  Meta-Regularization Algorithm and Convergence Guarantee ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "To solve problem (5), we use a double-loop method, i.e., we first solve the lower-level problem for $K$ iterations to get an approximate $\\phi_{j,K}$ of $\\phi_{j}$ and then use the approximate $\\phi_{j,K}$ to solve the upper-level problem. We do not use a single-loop method to solve (5) because we need to get the task-specific adaptation $\\phi_{j}$ for each task $\\mathcal{T}_{j}$ . The single-loop method only partially solves the lower-level problem by one-step gradient descent and thus the obtained parameter can be far away from $\\phi_{j}$ ", "page_idx": 30}, {"type": "text", "text": "Lemma 5. The gradient of the lower-level problem in (5) is $\\begin{array}{r}{E_{S,A}^{\\pi_{\\phi}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi}(S_{t},A_{t})\\big|S_{0}=}\\end{array}$ $\\begin{array}{r}{s_{0}^{\\mathrm{tr}}\\bigr]-\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi}\\bigr(s_{t}^{\\mathrm{tr}},a_{t}^{\\mathrm{tr}}\\bigr)+\\frac{\\lambda}{1-\\gamma}\\bigl(\\phi-\\bar{\\theta}\\bigr)}\\end{array}$ where $(s_{t}^{\\mathrm{tr}},a_{t}^{\\mathrm{tr}})\\in\\ensuremath{\\mathcal{D}}_{j}^{\\mathrm{tr}}$ ", "page_idx": 30}, {"type": "text", "text": "We rllout the policy $\\pi_{\\phi}$ from $s_{t}^{\\mathrm{tr}}$ to get trajectory $s_{0}^{\\phi},a_{0}^{\\phi},\\cdots$ where $s_{0}^{\\phi}=s_{t}^{\\mathrm{tr}}$ and approximate the lower-level gradient by $\\begin{array}{r}{g_{\\phi}=\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi}\\big(s_{t}^{\\phi},a_{t}^{\\phi}\\big)-\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi}\\big(s_{t}^{\\mathrm{tr}},a_{t}^{\\mathrm{tr}}\\big)+\\frac{\\lambda}{1-\\gamma}\\big(\\phi-\\bar{\\theta}\\big)}\\end{array}$ .We update $\\phi_{j,k+1}=\\phi_{j,k}-\\beta_{k}g_{\\phi_{j,k}}$ for $K$ times to get $\\phi_{j,K}$ where $\\beta_{k}$ is the step size and $\\phi_{j,k}$ is the parameter at time $k$ , and use $\\phi_{j,K}$ to calculat the approximateof thehyper-grat $\\begin{array}{r}{\\frac{d}{d\\theta}L(\\phi_{j},D_{j}^{\\mathrm{eval}})}\\end{array}$ ", "page_idx": 30}, {"type": "text", "text": "Lemma 6. The hyper-gradient (i.e,gradient of the upper-level problem in (5)) is $\\begin{array}{r l}{\\frac{d}{d\\theta}L(\\phi_{j},D_{j}^{\\mathrm{eval}})=}\\end{array}$ $\\begin{array}{r}{\\left[I+\\frac{1-\\gamma}{\\lambda}\\nabla_{\\phi\\phi}^{2}L(\\phi_{j},{\\mathcal D}_{j}^{\\mathrm{tr}})\\right]^{-1}\\!\\nabla_{\\phi}L(\\phi_{j},{\\mathcal D}_{j}^{\\mathrm{eval}})}\\end{array}$ Where $\\begin{array}{l}{\\displaystyle\\nabla_{\\phi}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{eval}})=|\\mathcal{D}_{j}^{\\mathrm{eval}}|\\cdot E_{S,A}^{\\pi_{\\phi_{j}}}\\biggr[\\underset{t=0}{\\overset{\\infty}{\\sum}}\\gamma^{t}\\nabla_{\\phi}r_{\\phi_{j}}(S_{t},A_{t})\\biggr|S_{0}\\sim P_{0}\\biggr]-\\displaystyle\\sum_{v=1}^{|\\mathcal{D}_{j}^{\\mathrm{eval}}|}\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi_{j}}(s_{t}^{v},a_{t}^{v}),}\\\\ {\\displaystyle\\nabla_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})=E_{S,A}^{\\pi_{\\phi_{j}}}\\biggr[\\underset{t=0}{\\overset{\\infty}{\\sum}}\\gamma^{t}\\nabla_{\\phi\\phi}^{2}r_{\\phi_{j}}(S_{t},A_{t})\\biggr|S_{0}=s_{0}^{\\mathrm{tr}}\\biggr]-\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi\\phi}^{2}r_{\\phi_{j}}(s_{t}^{\\mathrm{tr}},a_{t}^{\\mathrm{tr}})+e.}\\end{array}$ ", "page_idx": 30}, {"type": "text", "text": "The $|\\mathcal{D}_{j}^{\\mathrm{eval}}|$ is the number of trajectories in $\\mathcal{D}_{j}^{\\mathrm{eval}}$ al, (s, ag) \u2208 Deval, and $e$ is an extra term whose expression can be found in Appendix C.2. ", "page_idx": 31}, {"type": "text", "text": "To approximatethe hyper-gradient, we firstrll out $\\pi_{\\phi_{j,K}}$ to get two trajectories $s_{0}^{\\phi_{j,K}},a_{0}^{\\phi_{j,K}},\\cdots$ ai,k ,... and $\\bar{s}_{0}^{\\phi_{j,K}},\\bar{a}_{0}^{\\phi_{j,K}},\\cdot\\cdot\\cdot$ ,... where so? s\\$s, is drawn from Po and s $\\bar{s}_{0}^{\\phi_{j,K}}=s_{0}^{\\mathrm{tr}}$ . We estimate $\\nabla_{\\phi}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{eval}})$ via $\\begin{array}{r}{\\bar{\\gamma}_{\\phi}L(\\phi_{j,K},{\\mathcal D}_{j}^{\\mathrm{eval}})=|{\\mathcal D}_{j}^{\\mathrm{eval}}|\\cdot\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi_{j,K}}(s_{t}^{\\phi_{j,K}},a_{t}^{\\phi_{j,K}})-\\sum_{v=1}^{|{\\mathcal D}_{j}^{\\mathrm{eval}}|}\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi_{j,K}}(s_{t}^{v},a_{t}^{v})}\\end{array}$ and estimate the term $\\nabla_{\\phi\\phi}^{2}L(\\phi_{j},D_{j}^{\\mathrm{tr}})$ via $\\begin{array}{r}{\\bar{\\nabla}_{\\phi\\phi}^{2}L(\\phi_{j,K},\\mathcal{D}_{j}^{\\mathrm{tr}})=\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi\\phi}^{2}r_{\\phi_{j,K}}\\big(\\bar{s}_{t}^{\\phi_{j,K}},\\bar{a}_{t}^{\\phi_{j,K}}\\big)-}\\end{array}$ $\\begin{array}{r l}{\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi\\phi}^{2}r_{\\phi_{j,K}}(s_{t}^{\\mathrm{tr}},a_{t}^{\\mathrm{tr}})}\\end{array}$ Thereforewe canaproximatethehypergradientermn $\\textstyle{\\frac{d}{d\\theta}}L(\\phi_{j},D_{j}^{\\mathrm{eval}})$ $\\begin{array}{r}{h_{j}=[I+\\frac{1-\\gamma}{\\lambda}\\bar{\\nabla}_{\\phi\\phi}^{2}L(\\phi_{j,K},\\mathcal{D}_{j}^{\\mathrm{tr}})]^{-1}\\bar{\\nabla}_{\\phi}L(\\phi_{j,K},\\mathcal{D}_{j}^{\\mathrm{eval}})}\\end{array}$ We omithe exratem $e$ in the aproximate $h_{j}$ and its impact on the convergence can be bounded (proved in Appendix C.4). ", "page_idx": 31}, {"type": "text", "text": "To solve the upper-level problem in (5), at each iteration $n$ , we sample a batch of $B$ tasks, and compute the task-specific adaptation $\\phi_{j,K}$ and the hyper-gradient $h_{j}$ for each task. The update law to solvetheupper-level prblem is: $\\begin{array}{r}{\\bar{\\theta}_{n+1}=\\bar{\\theta}_{n}-\\frac{\\tau_{n}}{B}\\sum_{j=1}^{B}h_{j}}\\end{array}$ where $\\tau_{n}$ is the step sze. Note that the time index $k$ is for the lower-level problem and $n$ is for the upper-level problem. ", "page_idx": 31}, {"type": "text", "text": "Algorithm 2 Meta-regularization ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Input: Initialized meta-prior ${\\bar{\\theta}}_{0}$ and task-specific adaptation $\\phi_{j,0}$   \nOutput: Learned prior ${\\bar{\\theta}}_{N}$   \n1: for $n=0,1,\\cdot\\cdot\\cdot\\,,N-1$ do   \n2: Sample a batch of $B$ tasks $\\{\\mathcal{T}_{j}\\}_{j=1}^{B}\\sim P_{\\mathcal{T}}$   \n3: for each task $\\mathcal{T}_{j}$ do   \n4: for $k=0,1,\\cdot\\cdot\\cdot\\,,K-1$ do   \n5: Compute the soft Bellman policy $\\pi_{\\phi_{j,k}}$ via soft Q-learning or soft actor-critic   \n6: Roll outhe plicy $\\pi_{\\phi_{j,k}}$ to get a trajectory $s_{0}^{\\phi_{j,k}},a_{0}^{\\phi_{j,k}},\\cdots$   \n7: Compute the gradient $\\begin{array}{r}{g_{\\phi_{j,k}}=\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi_{j,k}}(s_{t}^{\\phi_{j,k}},a_{t}^{\\phi_{j,k}})-\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi_{j,k}}(s_{t}^{\\mathrm{tr}},a_{t}^{\\mathrm{tr}})}\\end{array}$   \n$+\\textstyle{\\frac{\\lambda}{1-\\gamma}}(\\phi_{j,k}-{\\bar{\\theta}}_{n})$   \n8: Update $\\phi_{j,k+1}=\\phi_{j,k}-\\beta_{k}g_{\\phi_{j,k}}$   \n9: end for   \n10: Compute the soft Bellman policy $\\pi_{\\phi_{j,K}}$ via soft Q-learning or soft actor-critic   \n11: $\\pi_{\\phi_{j,K}}$ $s_{0}^{\\phi_{j,K}},a_{0}^{\\phi_{j,K}},\\cdots$ and $\\bar{s}_{0}^{\\phi_{j,K}},\\bar{a}_{0}^{\\phi_{j,K}},\\cdot\\cdot\\cdot$   \n12: Compute the hyper-gradient $\\begin{array}{r}{h_{j}=[I+\\frac{1-\\gamma}{\\lambda}\\bar{\\nabla}_{\\phi\\phi}^{2}L(\\phi_{j,K},\\mathcal{D}_{j}^{\\mathrm{tr}})]^{-1}\\bar{\\nabla}_{\\phi}L(\\phi_{j,K},\\mathcal{D}_{j}^{\\mathrm{eval}})}\\end{array}$   \n13: end for   \n14: Update $\\begin{array}{r}{\\bar{\\theta}_{n+1}=\\bar{\\theta}_{n}-\\frac{\\tau_{n}}{B}\\sum_{j=1}^{B}h_{j}}\\end{array}$   \n15. end for ", "page_idx": 31}, {"type": "text", "text": "Lemma7 Convergence of thlower-level problem). Suppose Assumptions 1-2hold and $\\begin{array}{r}{\\lambda\\ge\\frac{C_{L}}{2}+\\eta}\\end{array}$ Wwhere $\\begin{array}{r}{\\eta\\in(0,\\frac{C_{L}}{2})}\\end{array}$ Let $\\begin{array}{r}{\\beta_{k}=\\frac{1-\\gamma}{\\eta(k+1)}}\\end{array}$ thenwehve ", "page_idx": 31}, {"type": "equation", "text": "$$\nE[||\\phi_{j,K}-\\phi_{j}||^{2}]\\leq O(\\frac{1}{K}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Assumption 3. The parameterized reward function $r_{\\theta}$ has bounded third-order gradient, i.e., $\\lvert|\\nabla_{\\theta\\theta\\theta}^{3}r_{\\theta}(s,a)\\rvert|\\leq\\hat{C}_{r}$ for any $(s,a)$ where ${\\hat{C}}_{r}$ is a positive constant. ", "page_idx": 31}, {"type": "text", "text": "Theorem 3 (Convergence of the upper-level problem). Suppose Assumption 3 and the condition in Lemma7hold. Let $\\tau_{n}=(n+1)^{-1/2}$ andefine $F({\\bar{\\theta}})$ as $E_{j\\sim P_{T}}[L(\\phi_{j},D_{j}^{e\\nu a l})]$ underthemeta-prior $\\bar{\\theta}$ Then we have the following convergence: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{n=0}^{N-1}E[||\\nabla F(\\bar{\\theta}_{n})||^{2}]\\leq O(\\frac{1}{\\sqrt{N}}+\\frac{\\log N}{\\sqrt{N}}+\\frac{1}{K})+C_{1},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and the expression of $C_{1}$ can be found in (27). ", "page_idx": 31}, {"type": "text", "text": "C.1 Proof of Lemma 5 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The proof is similar to that of Lemma 1. We first prove the case of deterministic dynamics and the corresponding result can be an unbiased estimate in cases of stochastic dynamics (proved in SubsectionB.2). ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\nabla_{\\phi}L(\\phi,D_{j}^{u})=-\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}\\log\\pi_{\\phi}(a_{t}^{u}|s_{t}^{u})}\\\\ {\\displaystyle=-\\sum_{t=0}^{\\infty}\\gamma^{t}\\left[\\nabla_{\\phi}Q_{\\phi}^{\\mathrm{st}}(s_{t}^{u},a_{t}^{u})-\\nabla_{\\phi}V_{\\phi}^{\\mathrm{st}}(s_{t}^{u})\\right],}\\\\ {\\displaystyle=-\\sum_{t=0}^{\\infty}\\gamma^{t}\\left[\\nabla_{\\phi}r_{\\phi}(s_{t}^{u},a_{t}^{u})+\\gamma\\nabla_{\\phi}V_{\\phi}^{\\mathrm{st}}(s_{t+1}^{u})-\\nabla_{\\phi}V_{\\phi}^{\\mathrm{st}}(s_{t}^{u})\\right],}\\\\ {\\displaystyle=\\nabla_{\\phi}V_{\\phi}^{\\mathrm{st}}(s_{0}^{u})-\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi}(s_{t}^{u},a_{t}^{u}),}\\\\ {\\displaystyle\\overset{(a)}{=}E_{s,A}^{\\infty}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi}(S_{t},A_{t})\\right]S_{0}=s_{0}^{u}\\right]-\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi}(s_{t}^{u},a_{t}^{u}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $(a)$ follows the proof of Lemma 3. ", "page_idx": 32}, {"type": "text", "text": "C.2  Proof of Lemma 6 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Since the lower-level problem of (5) is unconstrained and $\\phi_{i}$ is the optimal solution, we know that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}L(\\phi_{j},{\\mathcal D}_{j}^{\\mathrm{tr}})+\\frac{\\lambda}{1-\\gamma}(\\phi_{j}-\\bar{\\theta})=0\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We further take derivative of both sides in (25) with respect to $\\bar{\\theta}$ and we get that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left[\\nabla_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})+\\frac{\\lambda}{1-\\gamma}\\right]\\nabla_{\\bar{\\theta}}\\phi_{j}-\\frac{\\lambda}{1-\\gamma}=0\\Rightarrow\\nabla_{\\bar{\\theta}}\\phi_{j}=\\left[I+\\frac{1-\\gamma}{\\lambda}\\nabla_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})\\right]^{-1}\\!\\!.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{d}{d\\bar{\\theta}}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{eval}})=(\\nabla_{\\bar{\\theta}}\\phi_{j})^{\\top}\\nabla_{\\phi}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{eval}})=\\bigg[I+\\frac{1-\\gamma}{\\lambda}\\nabla_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{r}})\\bigg]^{-1}\\nabla_{\\phi}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{eval}}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Similar to Lemma 5, we can know that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}L(\\phi_{j},{\\mathcal D}_{j}^{\\mathrm{esa}})=\\sum_{v=1}^{|\\mathcal D_{j}^{\\mathrm{esa}}|}E_{S,A}^{\\pi_{\\phi_{j}}}\\bigg[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi_{j}}(S_{t},A_{t})\\bigg|S_{0}=s_{0}^{v}\\bigg]-\\sum_{v=1}^{|\\mathcal D_{j}^{\\mathrm{esa}}|}\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi_{j}}(s_{t}^{v},a_{t}^{v}),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $(s_{t}^{v},a_{t}^{v})\\in\\zeta^{v}$ and $\\zeta^{v}\\in\\mathcal{D}_{j}^{\\mathrm{eval}}$ ", "page_idx": 32}, {"type": "text", "text": "Since there is usually abundant data in $\\mathcal{D}_{j}^{\\mathrm{eval}}$ and $S_{0}\\sim P_{0}$ , we can reformulate (26) as the following: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}L(\\phi_{j},{\\mathcal D}_{j}^{\\mathrm{eval}})\\approx|{\\mathcal D}_{j}^{\\mathrm{eval}}|\\cdot E_{S,A}^{\\pi_{\\phi_{j}}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi_{j}}(S_{t},A_{t})\\right]S_{0}\\sim P_{0}\\right]-\\sum_{v=1}^{|{\\mathcal D}_{j}^{\\mathrm{eul}}|}\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}r_{\\phi_{j}}(s_{t}^{v},a_{t}^{v}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Claim 7. The second-order information $\\begin{array}{r}{\\nabla_{\\phi\\phi}^{2}Q_{\\phi}^{s o f t}(s,a)=\\Delta(s,a)+E_{S,A}^{\\pi_{\\phi}}[\\sum_{t=0}^{\\infty}\\gamma^{t}C o\\nu(S_{t})|S_{0}=}\\end{array}$ $s,A_{0}\\ =\\ a]$ and $\\nabla_{\\phi\\phi}^{2}V_{\\phi}^{s o f t}(s)\\ =\\ \\Delta(s)\\:+\\:E_{S,A}^{\\pi_{\\phi}}[\\sum_{t=0}^{\\infty}\\gamma^{t}C o\\nu(S_{t})|S_{0}\\ =\\ s]\\:\\:w$ here $\\begin{array}{l l}{\\Delta(s,a)}&{=}\\end{array}$ $\\begin{array}{r}{E_{S,A}^{\\pi_{\\phi}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi\\phi}^{2}r_{\\phi}(S_{t},A_{t})|S_{0}\\,=\\,s,A_{0}\\,=\\,a]}\\end{array}$ $\\begin{array}{r}{\\Delta(s)\\,=\\,E_{S,A}^{\\pi_{\\phi}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi\\phi}^{2}r_{\\phi}(S_{t},A_{t})|S_{0}\\,=\\,s]}\\end{array}$ \uff0c and $\\begin{array}{r}{C o\\nu(s)\\triangleq\\int_{a\\in\\mathcal{A}}\\pi_{\\phi}(a|s)[\\Delta(s,a)-\\Delta(s)]\\Delta(s)d a}\\end{array}$ is the covariance matrix of $\\Delta(s,\\cdot)$ at state $s$ \uff0c ", "page_idx": 32}, {"type": "text", "text": "The proof of Claim 7 follows the proof of Lemma 4 ", "page_idx": 33}, {"type": "text", "text": "Now we take a look at the term $\\nabla_{\\phi\\phi}^{2}L(\\phi_{j},D_{j}^{\\mathrm{tr}})$ and consider the case of deterministic dynamics: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}^{2}\\Delta L(\\phi_{j},D_{j}^{u})=-\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}^{2}\\log\\pi_{\\phi_{j}}(a_{t}^{u}|s_{t}^{u}),}\\\\ &{=-\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\left[\\nabla_{\\phi}^{2}\\phi_{\\phi_{j}}^{\\alpha\\beta}(s_{t}^{u},a_{t}^{u})-\\nabla_{\\phi_{\\phi}}^{2}V_{\\phi_{j}}^{\\alpha\\beta}(s_{t}^{u})\\right],}\\\\ &{=-\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\left[\\nabla_{\\phi}^{2}\\rho_{\\phi_{j}}(s_{t}^{u},a_{t}^{u})+\\gamma\\nabla_{\\phi_{\\phi}}^{2}V_{\\phi_{j}}^{\\alpha\\beta}(s_{t+1}^{u})-\\nabla_{\\phi_{\\phi}}^{2}V_{\\phi_{j}}^{\\alpha\\beta}(s_{t}^{u})\\right],}\\\\ &{=\\nabla_{\\phi_{\\phi}}^{2}V_{\\phi_{j}}^{\\alpha\\beta}(a_{t}^{u})-\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi}^{2}a_{t}^{p}\\rho_{\\phi_{j}}(s_{t}^{u},a_{t}^{u}),}\\\\ &{\\overset{(a)}{=}E_{s,A}^{\\prime\\prime}\\displaystyle\\sum_{s=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi_{\\phi}}^{2}\\rho_{\\phi_{j}}(s_{t},A_{t})|S_{0}=s_{0}^{u}|-\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi_{\\phi}}^{2}\\rho_{\\phi_{j}}(s_{t}^{u},a_{t}^{u})}\\\\ &{+E_{s,A}^{\\prime\\prime}\\displaystyle\\sum_{s=0}^{\\infty}\\gamma^{t}\\mathrm{Cor}(S_{t})|S_{0}=s_{0}^{u}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $(a)$ follows Claim 7. As proved in Subsection B.2, we can still use this expression in the case ofstochastic dynamics. We defne $\\begin{array}{r}{e\\triangleq E_{S,A}^{\\pi_{\\phi_{j}}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbf{Cov}(S_{t})|S_{0}=s_{0}^{\\mathrm{tr}}]}\\end{array}$ However, $e$ is intractable to compute because it requires to compute $\\mathrm{COV}(S_{t})$ at every visited state. To approximate the covariance matrix $\\mathrm{COV}(S_{t})$ , we need to empirically roll out the policy $\\pi_{\\phi_{j}}$ from $S_{t}$ for enough times to get enough samples. We need to do these policy roll-outs at every state $S_{t}$ . For example, suppose we roll out the policy $\\pi_{\\phi_{j}}$ ten times to get ten samples at each state $S_{t}$ . Empirically, we need to do these roll-outs $10\\times\\bar{T}$ where $\\bar{T}$ is a very large integer that we regard as infinity because the trajectory horizon is infinite. This is intractable because we need to roll out the policy for too many times. Moreover, we cannot guarante that we can approximate $\\mathrm{COV}(S_{t})$ well given that we only use ten samples to approximate it. ", "page_idx": 33}, {"type": "text", "text": "C.3 Proof of Lemma 7 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We know that $\\begin{array}{r}{\\|\\nabla_{\\phi\\phi}^{2}L(\\phi,\\mathcal{D}_{j}^{\\mathrm{tr}})+\\frac{\\lambda}{1-\\gamma}||\\leq||\\nabla_{\\phi\\phi}^{2}L(\\phi,\\mathcal{D}_{j}^{\\mathrm{tr}})||+\\frac{\\lambda}{1-\\gamma}\\leq\\sum_{t=0}^{\\infty}\\biggr(||\\nabla^{2}L_{t}(\\phi)||+\\lambda\\gamma^{t}\\biggr)\\overset{(a)}{\\leq}}\\end{array}$ $\\scriptstyle{\\frac{1}{1-\\gamma}}C_{L}$ where $(a)$ follows (6). Therefore, the lower-level objective function in (8) is $\\frac{C_{L}}{1\\!-\\!\\gamma}$ smooth. Moreover snce \u5165\u2265 + n, then IVL(\u03a6,D)\u2264 . Therefore, the lower-level objective functio i 8) is $\\frac{2\\eta}{1-\\gamma}$ srongly convex fllowing thestandard resulfo strongly-convex and smoth stochastic optimization, we can reach the result in Lemma 7. ", "page_idx": 33}, {"type": "text", "text": "C.4 Proof of Theorem 3 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this proof, we frst bound the hyper-gradient approximation error (i.e., $\\begin{array}{r}{||\\frac{d}{d\\bar{\\theta}}L(\\phi_{j},D_{j}^{\\mathrm{eval}})-h_{j}||)}\\end{array}$ and then prove theconvergence, Define $\\begin{array}{r}{\\bar{h}_{j}\\,\\triangleq\\,[I+\\frac{1-\\gamma}{\\lambda}\\bar{\\nabla}_{\\phi\\phi}^{2}L(\\phi_{j},{D}_{j}^{\\mathrm{tr}})]^{-1}\\bar{\\nabla}_{\\phi}L(\\phi_{j},\\bar{\\mathcal{D}}_{j}^{\\mathrm{eval}})}\\end{array}$ Where $\\begin{array}{r}{\\bar{\\nabla}_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})\\triangleq E_{S,A}^{\\pi_{\\phi_{j}}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi\\phi}^{2}r_{\\phi_{j}}(S_{t},A_{t})|S_{0}=s_{0}^{\\mathrm{tr}}]-\\sum_{t=0}^{\\infty}\\gamma^{t}\\nabla_{\\phi\\phi}^{2}r_{\\phi_{j}}(s_{t}^{\\mathrm{tr}},a_{t}^{\\mathrm{tr}}).}\\end{array}$ Therefore, we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\bar{h}_{j}-\\displaystyle\\frac{d}{d\\bar{\\theta}}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{eud}})\\|,}\\\\ &{\\le\\left|\\left|[I+\\displaystyle\\frac{1-\\gamma}{\\lambda}\\nabla_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})]^{-1}-[I+\\displaystyle\\frac{1-\\gamma}{\\lambda}\\bar{\\nabla}_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})]^{-1}\\right|\\right|\\cdot||\\nabla_{\\phi}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{eud}})||,}\\\\ &{\\le\\left[\\left|\\left|I+\\displaystyle\\frac{1-\\gamma}{\\lambda}\\nabla_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})\\right|^{-1}\\right|\\right|+\\left|\\left|[I+\\displaystyle\\frac{1-\\gamma}{\\lambda}\\bar{\\nabla}_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})]^{-1}\\right|\\right|\\right]\\cdot||\\nabla_{\\phi}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{eud}})||,}\\\\ &{\\overset{(a)}{\\le}\\left(\\displaystyle\\frac{2\\lambda}{2\\lambda+2\\eta-C_{L}}+\\displaystyle\\frac{\\lambda}{\\lambda-2\\bar{C}_{r}}\\right)\\cdot\\displaystyle\\frac{2|\\mathcal{D}_{j}^{\\mathrm{eud}}|\\bar{C}_{r}}{1-\\gamma}\\triangleq C_{1}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $(a)$ follows the fact that $\\begin{array}{r}{||I+\\frac{1-\\gamma}{\\lambda}\\nabla_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})||\\,\\geq\\,1-||\\frac{1-\\gamma}{\\lambda}\\nabla_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})||\\,\\geq\\,1-}\\end{array}$ $\\frac{C_{L}\\!-\\!2\\eta}{2\\lambda}$ $\\begin{array}{r}{||\\nabla_{\\phi\\phi}^{2}L(\\phi,\\mathcal{D}_{j}^{\\mathrm{tr}})||\\,\\leq\\,\\frac{C_{L}-2\\eta}{2(1-\\gamma)}}\\end{array}$ $||I+$ $\\begin{array}{r}{\\frac{1-\\gamma}{\\lambda}\\nabla_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})]^{-1}||\\,\\leq\\,\\frac{2\\lambda}{2\\lambda+2\\eta-C_{L}}}\\end{array}$ $\\begin{array}{r}{||[I+\\frac{1-\\gamma}{\\lambda}\\bar{\\nabla}_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})]^{-1}||\\leq}\\end{array}$ 2 given that L(\u03a6,D)\u2264. Note that \u5165 > and CL > C (proved in (6), therefore $C_{1}$ is a positive constant. ", "page_idx": 34}, {"type": "text", "text": "Now, we bound the term $||h_{j}-\\bar{h}_{j}||$ . We define $\\begin{array}{r}{\\Delta_{\\phi_{j}}\\ =\\ I+\\frac{1-\\gamma}{\\lambda}\\bar{\\nabla}_{\\phi\\phi}^{2}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{tr}})}\\end{array}$ and $\\Delta_{\\phi_{j}}\\;=\\;$ $\\begin{array}{r}{I+\\frac{1-\\gamma}{\\lambda}\\bar{\\nabla}_{\\phi\\phi}^{2}L(\\phi_{j,K},\\mathcal{D}_{j}^{\\mathrm{tr}})}\\end{array}$ Thus we have $\\begin{array}{r}{||\\Delta_{\\phi_{j}}^{-1}||\\leq\\frac{\\lambda}{\\lambda-2\\tilde{C}_{r}}}\\end{array}$ $||\\Delta_{\\phi_{j,K}}^{-1}||\\leq$ X20. Therefore, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E[\\|h_{j}-\\bar{h}_{j}\\|]=E\\biggl[\\biggl\\|\\Delta_{\\phi,x}^{-1}\\nabla_{\\phi}E(\\phi_{j,K},\\mathcal{T}_{j}^{\\mathrm{eq}})-\\Delta_{\\phi,j}^{-1}\\nabla_{\\phi}E(\\phi_{j},\\mathcal{T}_{j}^{\\mathrm{eq}})\\biggr\\|\\biggr],}\\\\ &{\\leq E\\biggl[\\biggl\\|\\Delta_{\\phi,x}^{-1}\\nabla_{\\phi}E(\\phi_{j,K},\\mathcal{T}_{j}^{\\mathrm{eq}})-\\Delta_{\\phi,x}^{-1}\\nabla_{\\phi}E_{\\phi}(\\phi_{j,p},\\mathcal{T}_{j}^{\\mathrm{eq}})\\biggr\\|\\biggr]}\\\\ &{+\\biggl\\|\\Delta_{\\phi,x}^{-1}\\nabla_{\\phi}E(\\phi_{j,K},\\mathcal{T}_{j}^{\\mathrm{eq}})-\\Delta_{\\phi,x}^{-1}\\nabla_{\\phi}E_{\\phi}(\\phi_{j,K},\\mathcal{T}_{j}^{\\mathrm{eq}})\\biggr\\|\\biggr],}\\\\ &{\\leq E\\biggl[\\biggl\\|\\Delta_{\\phi,x}^{-1}\\nabla_{\\phi}E(\\phi_{j,K},\\mathcal{T}_{j}^{\\mathrm{eq}})-\\nabla_{\\phi}E(\\phi_{j,K},\\mathcal{T}_{j}^{\\mathrm{eq}})\\biggr\\|+\\biggl\\|\\Delta_{\\phi,x}^{-1}-\\Delta_{\\phi,j}^{-1}\\biggr\\|\\cdot\\|\\nabla_{\\phi}E(\\phi_{j,K},\\mathcal{T}_{j}^{\\mathrm{eq}})\\biggr\\|\\biggr]}\\\\ &{\\overset{(b)}{\\leq}\\frac{\\lambda}{\\lambda-2\\bar{C}_{r}}\\cdot|\\mathcal{P}_{j}^{\\mathrm{eq}}|\\cdot\\frac{C_{2}-2\\eta}{2(1-\\gamma)}|\\phi_{j,K}-\\phi_{j}|\\|+E[\\|\\Delta_{\\phi,x}^{-1}-\\Delta_{\\phi}^{-1}\\|]\\cdot|\\mathcal{D}_{j}^{\\mathrm{eq}}|\\cdot\\frac{2\\bar{C}_{r}}{1-\\gamma},}\\\\ &{\\leq O(\\frac{1}{K})+E\\|\\Delta_{\\phi,j}^{-1}|\\cdot\\|\\Delta_{\\phi,j}^{-1}\\|\\cdot\\|\\Delta_{\\phi,K}^{-1}-\\Delta_{\\phi,j}^{-1}\\|\\cdot\\|\\mathcal{D}_{r}^{\\mathrm{eq}}|\\cdot\\frac{2\\bar{\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $||\\nabla_{\\phi\\phi}^{3}L(\\phi_{j},{\\mathcal D}_{j}^{\\mathrm{tr}})||\\ \\leq\\ \\tilde{C}_{L}$ .Theexpressionof $\\tilde{C}_{L}$ can be derived following the proof of Lemma 4 by notifying Assumption 3. The $(b)$ holds because (i) $\\begin{array}{r l}{||\\nabla_{\\phi\\phi}^{2}L(\\phi,D_{j}^{\\mathrm{eval}})||}&{=}\\end{array}$ $\\begin{array}{r}{\\frac{|\\mathcal{D}_{j}^{\\mathrm{eval}}|}{|\\mathcal{D}_{j}^{\\mathrm{t}}|}||\\nabla_{\\phi\\phi}^{2}L(\\phi,\\mathcal{D}_{j}^{\\mathrm{tr}})||\\,\\leq\\,|\\mathcal{D}_{j}^{\\mathrm{eval}}|\\,\\cdot\\,\\frac{C_{l}-2\\eta}{2(1-\\gamma)}}\\end{array}$ and (i) $\\begin{array}{r}{||\\nabla_{\\phi}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{eval}})||\\,\\leq\\,|\\mathcal{D}_{j}^{\\mathrm{eval}}|\\,\\cdot\\,\\frac{2\\bar{C}_{r}}{1-\\gamma}}\\end{array}$ (see the xpression of $\\nabla_{\\phi}L(\\phi_{j},D_{j}^{\\mathrm{eval}})$ in Lemma 5). ", "page_idx": 34}, {"type": "equation", "text": "$$\nE[||h_{j}-\\frac{d}{d\\bar{\\theta}}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{esa}})||]\\le E[||h_{j}-\\bar{h}_{j}||+||\\bar{h}_{j}-\\frac{d}{d\\bar{\\theta}}L(\\phi_{j},\\mathcal{D}_{j}^{\\mathrm{esa}})|||\\overset{(c)}{\\le}C_{1}+O(\\frac{1}{K}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $(c)$ follows (27)-(28). Define $F({\\bar{\\theta}})$ as $E_{j\\sim P_{T}}[L(\\phi_{j},D_{j}^{\\mathrm{eval}})]$ under the meta-prior $\\bar{\\theta}$ . Note that $\\begin{array}{r}{||h_{j}||\\leq||\\Delta_{\\phi_{j,K}}^{-1}||\\cdot||\\nabla_{\\phi}L(\\phi_{j,K},\\mathcal{D}_{j}^{\\mathrm{eval}})||\\leq\\frac{\\lambda}{\\lambda-2\\tilde{C}_{r}}\\cdot|\\mathcal{D}_{j}^{\\mathrm{eval}}|\\cdot\\frac{2\\bar{C}_{r}}{1-\\gamma}.}\\end{array}$ Therefore, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\bar{\\theta}_{n+1})\\geq F(\\bar{\\theta}_{n})+(\\mathrm{V}F(\\bar{\\theta}_{n}))^{\\top}(\\bar{\\theta}_{n+1}-\\bar{\\theta}_{n})-\\frac{\\lvert D^{\\mathrm{out}}\\rvert(C_{L}-2\\eta)}{4(1-\\gamma)}\\lVert\\bar{\\theta}_{n+1}-\\bar{\\theta}_{n}\\rVert^{2},}\\\\ &{\\geq F(\\bar{\\theta}_{n})+\\frac{7n}{B_{f,1}^{\\top}}\\langle\\mathrm{V}F(\\bar{\\theta}_{n})\\rangle^{\\top}h_{j}-\\frac{\\lvert D^{\\mathrm{out}}\\rvert(C_{L}-2\\eta)\\gamma_{0}^{2}}{4(1-\\gamma)}\\lVert\\frac{1}{B_{f,1}^{\\top}}\\rVert^{2},}\\\\ &{\\geq F(\\bar{\\theta}_{n})+\\frac{7n}{B_{f,1}^{\\top}}\\frac{\\sqrt{(1-\\gamma)}}{2^{-1}}\\langle\\mathrm{V}F(\\bar{\\theta}_{n})\\rangle^{\\top}h_{j}-\\frac{\\lvert D^{\\mathrm{out}}\\rvert(C_{L}-2\\eta)\\gamma_{0}^{2}}{4(1-\\gamma)}\\cdot\\frac{\\lambda}{-\\lambda-2\\bar{C}_{f}}\\cdot\\lvert\\mathcal{D}_{f}^{\\mathrm{out}}\\rvert,\\frac{2\\bar{C}_{f}}{1-\\gamma},}\\\\ &{\\geq F(\\bar{\\theta}_{n})+\\frac{7n}{B_{f,1}^{\\top}}\\frac{\\sqrt{(1-\\gamma)}}{2^{-1}}\\langle\\mathrm{V}F(\\bar{\\theta}_{n})\\rangle^{\\top}h_{j}-\\frac{2\\Delta C_{f}\\gamma_{0}^{2}\\lvert\\mathcal{D}\\rvert^{2}(C_{L}-2\\eta)\\gamma_{0}^{2}}{4(1-\\gamma)^{2}(\\lambda-2\\gamma_{0})},}\\\\ &{\\Rightarrow E[F(\\bar{\\theta}_{n+1})]\\geq E[F(\\bar{\\theta}_{n})]+\\tau_{n}E[\\vert\\nabla F(\\bar{\\theta}_{n})\\vert]^{2}+\\frac{\\gamma_{n}}{B_{f,1}^{\\top}}\\frac{B}{\\int_{0}}E[h_{j}-\\frac{d}{d\\theta}E(\\phi,\\mathcal{D}_{f}^{\\mathrm{out}})]}\\\\ &{+\\frac{\\tau_{n}}{B_{f,1}^\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{\\ell}^{(d)}E[F(\\bar{\\theta}_{n})]+\\tau_{n}E[\\|\\nabla F(\\bar{\\theta}_{n})\\|^{2}]+\\tau_{n}(C_{1}+O(\\frac{1}{K}))-\\frac{2\\lambda\\bar{C}_{r}|D^{\\mathrm{eval}}|^{2}(C_{L}-2\\eta)\\tau_{n}^{2}}{4(1-\\gamma)^{2}(\\lambda-2\\tilde{C}_{r})},}\\\\ {\\displaystyle\\Rightarrow\\frac{1}{N}\\sum_{n=0}^{N-1}\\tau_{N}E[\\|\\nabla F(\\bar{\\theta}_{n})\\|^{2}]\\leq\\frac{1}{N}\\sum_{n=0}^{N-1}\\tau_{n}E[\\|\\nabla F(\\bar{\\theta}_{n})\\|^{2}],}\\\\ {\\displaystyle\\leq\\frac{1}{N}[F(\\bar{\\theta}_{N})-F(\\bar{\\theta}_{0})]+\\frac{1}{N}\\sum_{n=0}^{N-1}\\tau_{n}(C_{1}+O(\\frac{1}{K}))+\\frac{1}{N}\\sum_{n=0}^{N-1}\\frac{2\\lambda\\bar{C}_{r}|D^{\\mathrm{eval}}|^{2}(C_{L}-2\\eta)\\tau_{n}^{2}}{4(1-\\gamma)^{2}(\\lambda-2\\tilde{C}_{r})},}\\\\ {\\displaystyle\\Rightarrow\\frac{1}{N}\\sum_{n=0}^{N-1}E[\\|\\nabla F(\\bar{\\theta}_{n})\\|^{2}]\\leq O(\\frac{1}{\\sqrt{N}}+\\frac{\\log N}{\\sqrt{N}}+\\frac{1}{K})+C_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $|\\mathcal{D}^{\\mathrm{eval}}|\\triangleq\\operatorname*{sup}_{i\\sim P_{T}}\\{|\\mathcal{D}_{j}^{\\mathrm{eval}}|\\}$ and $(d)$ follows (29). ", "page_idx": 35}, {"type": "text", "text": "D Experiment details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The code was running on a laptop whose processor is AMD Ryzen 7 470oU with Radeon Graphics, $2.00\\mathrm{GHz}$ , and the installed RAM is 20.0GB. The operating system is Ubuntu 18.04. We use a neural network to parameterize the learned reward function. The neural network has two hidden layers where each hidden layer has 64 neurons. The activation functions are respectively ReLU and Tanh. ", "page_idx": 35}, {"type": "text", "text": "D.1  Baselines ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Here we provide the update rule for each baseline. Given a learned reward function $r_{\\theta}$ ,thepolicy updates of the four baselines are the same with that of MERIT-IRL, i.e., one-step policy iteration. The difference is the reward update. ", "page_idx": 35}, {"type": "text", "text": "IT-IRL: IT-IRL is MERIT-IRL without the meta-regularization term. Therefore, the reward update of IT-IRL is $\\theta_{t+1}=\\theta_{t}-\\alpha_{t}g_{t}^{\\prime}$ where $\\begin{array}{r}{g_{t}^{\\prime}=\\sum_{i=0}^{\\infty}\\gamma^{i}\\mathbf{\\bar{V}}_{\\theta}r_{\\theta_{t}}(s_{i}^{\\prime},a_{i}^{\\prime})-\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})}\\end{array}$ Recall from Subsection 4.1 that $\\{(s_{i}^{\\prime},a_{i}^{\\prime})\\}_{i\\geq0}$ is generated by the learned policy $\\pi_{\\theta_{t}}$ starting from the initial state $s_{0}^{\\prime}=s_{0}^{E}$ and $\\{(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})\\}_{i\\geq0}$ is generated by the learned policy $\\pi_{\\theta_{t}}$ starting from $(s_{t}^{\\prime\\prime},a_{t}^{\\prime\\prime})$ where $(s_{i}^{\\prime\\prime},a_{i}^{\\prime\\prime})=(s_{i}^{E},a_{i}^{E})$ for $0\\leq i\\leq t$ ", "page_idx": 35}, {"type": "text", "text": "Naive MERIT-IRL: This method has the meta-regularization term, however, it uses the naive way (depicted in the middle of Figure 1) to update the reward function. In specific, it only compares the partial expert trajectory $\\[\\check{s}_{i}^{E},a_{i}^{E}\\}_{i=0}^{t}$ and partial learner trajectory $\\{\\bar{s_{i}^{\\prime}},a_{i}^{\\prime}\\}_{i=0}^{t}$ Therefore, the reward update of Naive MERIT-IRL is $\\theta_{t+1}\\,=\\,\\theta_{t}\\,-\\,\\alpha_{t}g_{t}^{\\prime\\prime}$ where $\\begin{array}{r}{g_{t}^{\\prime\\prime}=\\sum_{i=0}^{t}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(s_{i}^{\\prime},a_{i}^{\\prime})\\nonumber\\,-}\\end{array}$ $\\begin{array}{r}{\\sum_{i=0}^{t}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(s_{i}^{E},a_{i}^{E})+\\frac{\\lambda(1-\\gamma^{t+1})}{1-\\gamma}(\\theta-\\bar{\\theta}).}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "Naive IT-IRL: This method does not have the meta-regularization term and uses the naive way to update the reward function. Therefore, the reward update of Naive IT-IRL is $\\theta_{t+1}=\\theta_{t}-\\dot{\\alpha_{t}}g_{t}^{\\prime\\prime\\prime}$ where $\\begin{array}{r}{g_{t}^{\\prime\\prime\\prime}=\\sum_{i=0}^{t}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(s_{i}^{\\prime},a_{i}^{\\prime})-\\sum_{i=0}^{t}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(s_{i}^{E},a_{i}^{E}).}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "Hindsight: This method is a standard IRL method with the meta-regularization term where the complete expert trajectory. $\\{s_{i}^{E},a_{i}^{E}\\}_{i\\geq0}$ and the complete learner trajectory $\\{s_{i}^{\\prime},a_{i}^{\\prime}\\}_{i\\geq0}$ are compared to update the reward function. Therefore, the reward update of Hindsight is $\\theta_{t+1}=\\theta_{t}-\\alpha_{t}g_{t}^{\\prime\\prime\\prime\\prime}$ where $\\begin{array}{r}{g_{t}^{\\prime\\prime\\prime\\prime}\\stackrel{}{=}\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(s_{i}^{\\prime},a_{i}^{\\prime})-\\sum_{i=0}^{\\infty}\\gamma^{i}\\nabla_{\\theta}r_{\\theta_{t}}(s_{i}^{E},a_{i}^{E})+\\frac{\\lambda(1-\\gamma^{t+1})}{1-\\gamma}(\\theta-\\bar{\\theta})}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "D.2 MuJoCo ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "D.2.1 Walker ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Figure 2b shows that MERIT can achieve similar performance with the expert after $t=600$ While the other three in-trajectory learning baselines fail to imitate the expert before the ongoing trajectory terminates. Note that the naive methods (i.e., Naive MERIT-IRL and Naive IT-IRL) have much smallerimprovementfrom $t=0$ compared to MERIT-IRL and IT-IRL. The reason is that the naive reward update method is flawed. Intuitively, the reward update mechanism of these two baselines are myopic as explained in Subsection 4.1. Theoretically, the gradients $g_{t}^{\\prime\\prime}$ of Naive MERIT-IRL and $g_{t}^{\\prime\\prime\\prime}$ of Naive IT-IRL are biased estimate of (4) even if $\\pi_{t}$ approaches $\\pi_{\\theta_{t}}$ since (4) includes the trajectory suffix $(i>t)$ terms while $g_{t}^{\\prime\\prime}$ and $g_{t}^{\\prime\\prime\\prime}$ only include the trajectory prefix $(i\\leq t)$ terms. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "MERIT-IRL performs much better than IT-IRL. The reason is that the meta-regularization term restricts the learned reward parameter within a certain neighborhood of the meta-prior $\\bar{\\theta}$ (proved in Appendix B.3). Given that $\\bar{\\theta}$ is trained over a family of relevant tasks, it is expected that the actual reward function parameter of our task shall be \u201cclose\" to $\\bar{\\theta}$ [24, 43, 56], i.e., inside this neighborhood. Therefore, MERIT-IRL can efficiently learn the expert's reward function. On the contrary, IT-IRL does not have the meta-prior $\\bar{\\theta}$ as a guidance and thus has to search over the whole parameter space, which is extremely difficult to learn the expert's reward function when the data is lacking. Note that MERIT-IRL and Naive MERIT-IRL have better initial performance than IT-IRL and Naive IT-IRL since MERIT-IRL and Naive MERIT-IRL starts at the meta-prior $\\bar{\\theta}$ while IT-IRL and Naive IT-IRL initializes randomly. ", "page_idx": 36}, {"type": "text", "text": "D.2.2 Hopper ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Figure 2c shows that MERIT can achieve similar performance with the expert after $t=500$ While the other three in-trajectory learning baselines fail to imitate the expert before the ongoing trajectory terminates. ", "page_idx": 36}, {"type": "text", "text": "D.3 Stock Market ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We use the real-world data of 30 constitute stocks in Dow Jones Industrial Average from 2021-01-01 to 2022-01-01. The 30 stocks are respectively: \u2018AXP', ^AMGN', \u201cAAPL', \u2018BA', \u201cCAT', \u201cCSCO', \u2018CVX',\u2018GS',\u201cHD', \u2018HON', \u201cIBM', \u201cINTC', \u2018JNJ',\u201cKO', \u2018JPM', \u201cMCD', \u201cMMM', \u201cMRK', \u201cMSFT', \u2018NKE', \u2018PG',\u2018TRV', \u2018UNH', \u2018CRM', \u2018VZ',\u201cV', \u201cWBA', \u201cWMT', \u2018DIS', \u2018DOW'. ", "page_idx": 36}, {"type": "text", "text": "The state of the stock market MDP is the perception of the stock market, including the open/close price of each stock, the current asset, and some technical indices [47]. The action has the same dimension as the number of stocks where each dimension represents the amount of buying/selling the corresponding stock. The detailed formulation of the MDP can be found in FinRL [47, 57]. ", "page_idx": 36}, {"type": "text", "text": "The turbulence index is a technical index of stock market and is included as a dimension of the state [47, 57]. The function $p_{2}$ is defined as the amount of buying the stocks whose turbulence index is larger than the turbulence threshold. Therefore, the more the target investor buys the stocks whose turbulence index is larger than the turbulence threshold, the larger $p_{2}$ will be and thus the smaller reward the target investor will receive. ", "page_idx": 36}, {"type": "text", "text": "Discussion on the experiment results. In Figure 2d, MERIT-IRL can achieve the similar cumulative reward with the expert when only the first $60\\bar{\\%}$ of the trajectory is observed while IT-IRL can achieve performance close to the expert after $t\\,=\\,220$ \uff1aThis shows that the meta-regularization can help imitate the expert faster. In contrast, Naive MERIT-IRL and Naive IT-IRL barely improves because the naive reward update method is fawed. Intuitively, the reward update mechanism of these two baselines are myopic as explained in Subsection 4.1. Theoretically, the gradients $g_{t}^{\\prime\\prime}$ of Naive MERITIRL and $g_{t}^{\\prime\\prime\\prime}$ of Naive IT-IRL are biased estimate of (4) even if $\\pi_{t}$ approaches $\\pi_{\\theta_{t}}$ since (4) includes the trajectory suffix $(i>t)$ terms while $g_{t}^{\\prime\\prime}$ and $g_{t}^{\\prime\\prime\\prime}$ only include the trajectory prefix ( $(i\\leq t)$ terms. ", "page_idx": 36}, {"type": "text", "text": "The last row in Table 1 shows the final results of the algorithms. We can see that MERIT-IRL achieves much better performance than the other in-trajectory learning baselines (i.e., IT-IRL, Naive MERIT-IRL, and Naive IT-IRL). MERIT-IRL achieves comparable performance with Hindsight and the expert. Note that it is not expected that MERIT-IRL outperforms Hindsight since Hindsight has the complete expert trajectory to learn. ", "page_idx": 36}, {"type": "text", "text": "E  Potential negative societal impact ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Since MERIT-IRL can infer the reward function of the expert, potential negative societal impact may occur when the learner is malicious. Take the stock market experiment as an example, private information like preferences or habits of the investors may be leaked by using MERIT-IRL. To avoid this situation, the investors needs to take additional strategies such as protecting its investment data from unsecure resources. ", "page_idx": 36}, {"type": "text", "text": "F Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "From the objective (2), we can see that the goal of MERIT-IRL is to align with the expert demonstration, i.e., finding a reward function such that its corresponding policy makes the expert trajectory most likely. An ideal case is that we can also directly quantify the reward learning performance and study the reward identifiability issue. Thus, a future work is to study the reward identifiability issue in the context of in-trajectory IRL. ", "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The abstract summarizes our contributions, and the introduction has a \u201ccontribution statement\" part which elaborates our contributions and mentions the consistency with theoretical and experiment results. The assumptions and limitations are included in \"theoretical analysis\" (Subsection 4.2) and Appendix F. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: The limitation is mentioned in Appendix F. Under the assumptions, i.e., Assumptions 1 and 2, we have a justification of either the assumption is widely used in literature or the assumption can be satisfied by real scenarios. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We clearly state our assumptions in Assumptions 1 and 2. The theoretical statements are also clearly stated in Subsection 4.2 and the correct proof is included in Appendix B. All the assumptions and theoretical statements are numbered and cross-referenced. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper provides pseudocode and elaborates each step of the algorithm in Subsection 4.1 and Appendix 4.3. We also include the experiment details in Appendix D. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 39}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The code is submitted in the supplementary materials and we include a document in the code folder to describe how to run the code. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: Appendix D includes the experiment details, and we also submit the code. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: In the experiment results (i.e., Table 1, Table ??, Table ??), we include both the mean and standard deviation. In Figure 2, we also plot the mean and standard deviation. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confdence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We include the type of operating system, CPU, and RAM used in the first paragraph in Appendix D. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper follows NeurIPS Code of Ethnics. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: We discuss the potential negative societal impact in Appendix E. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 41}, {"type": "text", "text": "\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: This paper poses no such risks because we do not have data nor model to release. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We use the SAC code from a Github repository, and we clearly state it at the top of that code script. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The submitted code can be considered as an asset, and we provide a file along with the code to document the code. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 43}, {"type": "text", "text": "\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}]