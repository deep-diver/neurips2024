{"references": [{"fullname_first_author": "K. He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-00-00", "reason": "This paper introduces the Masked Autoencoder (MAE) model, which is the primary focus of the current research and the foundation for the proposed method."}, {"fullname_first_author": "J. Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-04", "reason": "This paper introduces BERT, a foundational model for masked language modeling, which inspired the application of masked image modeling in computer vision."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-11", "reason": "This paper introduces Vision Transformers (ViT), a crucial architecture used in MAE and many related MIM models, which is directly relevant to the current work."}, {"fullname_first_author": "K. He", "paper_title": "Momentum contrast for unsupervised visual representation learning", "publication_date": "2020-00-00", "reason": "This paper introduces MoCo, a contrastive learning method, which is used as a comparison method in the paper to show the differences between MAE and contrastive learning-based MIM."}, {"fullname_first_author": "L. Kong", "paper_title": "Understanding masked autoencoders via hierarchical latent variable models", "publication_date": "2023-00-00", "reason": "This paper provides a theoretical framework to understand the internal mechanism of MAE, which is used to support the analysis and findings presented in the current research."}]}