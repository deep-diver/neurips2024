[{"figure_path": "7vXufiEzSy/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of our self-guided MAE.", "description": "This figure compares the original MAE with the proposed self-guided MAE. It shows how random masking in the original MAE leads to less distinguishable patch-level clustering, while the self-guided method generates informed masks that accelerate the training process by focusing on less easily separable patches.  The figure highlights the improved embedding space achieved with the self-guided method and the superior ability to discern patch-level clustering.", "section": "Analysis of MAE"}, {"figure_path": "7vXufiEzSy/figures/figures_2_1.jpg", "caption": "Figure 2: Relationships among the patch embeddings. (a) Pairwise similarity matrix for all 196 \u00d7 196 pairs of patches. (b) Similarity between the mean patch and all individual patches. (c) Attention score of the class token.", "description": "This figure compares the patch relationships in the learned embedding space, using the last layer embeddings for 196 (14 \u00d7 14) patches of test images.  The analysis includes a pairwise similarity matrix (M) showing the relationships between all pairs of patches, a similarity score showing the relationship between each patch and the average patch, and the attention score of the class (CLS) token. The visualizations highlight how the MAE encoder shows more polarized values indicating clear patch clustering compared to MoCo and ViT.", "section": "3 Analysis of MAE"}, {"figure_path": "7vXufiEzSy/figures/figures_4_1.jpg", "caption": "Figure 3: MAE learns patch clustering from very early stage of training process. (a) MAE widens the gap \u00b5intra \u2212 \u00b5inter. (b) Token relations drastically converge at early epochs and then gradually level off. Numbers in the legend denote the layer i. More details are provided in Appendix B.", "description": "This figure shows that the MAE model starts learning patch clustering from the very early stage of training.  The left graph (a) illustrates how the gap between the mean intra-cluster edge weights and the mean inter-cluster edge weights increases over training epochs. This indicates that the model is progressively learning to better distinguish between different clusters of patches. The right graph (b) shows how the KL divergence between the distribution of token relations at different training epochs and the final converged distribution decreases rapidly at early epochs and then gradually levels off.  This confirms that token relations converge early in the training process and that patch clustering is learned from the beginning of the training process. The numbers in the legend of graph (b) specify the layer number of MAE.  Appendix B contains more details.", "section": "3.3 When Does MAE Learn Patch Clustering?"}, {"figure_path": "7vXufiEzSy/figures/figures_5_1.jpg", "caption": "Figure 4: Exploitation rate.", "description": "This figure shows the exploitation rate of visible tokens (R<sup>(L)</sup><sub>V\u2192O</sub>) and mask tokens (R<sup>(L)</sup><sub>M\u2192O</sub>) in each decoder layer. The exploitation rate is a measure of how much the mask tokens are used to reconstruct the masked-out patches. The figure shows that the exploitation rate of mask tokens surpasses that of visible tokens after around 50 epochs. This suggests that the decoder is able to leverage the information learned by the encoder from the visible tokens to reconstruct the masked-out tokens.", "section": "3.4 Operations of the Decoder"}, {"figure_path": "7vXufiEzSy/figures/figures_6_1.jpg", "caption": "Figure 1: Illustration of our self-guided MAE.", "description": "This figure compares the original MAE with the proposed self-guided MAE.  The original MAE uses random masking, while the self-guided MAE generates informed masks covering the main object entirely. The informed masks are generated using distinguishable patch representations that emerge early in the training process. This leads to faster training and clearer embeddings. The figure shows the input image, random masking, MAE feature, bi-partitioned masking, informed masking with a hint, and the proposed method's feature.", "section": "Analysis of MAE"}, {"figure_path": "7vXufiEzSy/figures/figures_7_1.jpg", "caption": "Figure 6: MAE properties.", "description": "This figure shows two plots: attention distance and normalized mutual information (NMI) across different layers of the MAE model. The attention distance measures how globally each layer attends to the image while NMI measures the homogeneity of the attention map. The plots show that the attention distance increases and NMI increases up to a certain point, and then they decrease in the decoder layers. This indicates that the early layers of the encoder tend to attend to more local regions, while the later layers attend to the entire image. The decoder shows a similar trend, but the values are lower.", "section": "3 Analysis of MAE"}, {"figure_path": "7vXufiEzSy/figures/figures_9_1.jpg", "caption": "Figure 7: Metrics explaining our performance gain. Layers left on the red dotted line belong to the encoder, and the rest to the decoder.", "description": "This figure shows three subfigures that analyze the learned feature space of the proposed method and compares it to the vanilla MAE. Subfigure (a) displays the attention distance, showing how well the model captures global image context. Subfigure (b) presents a Fourier analysis of the learned features, highlighting the emphasis on high-frequency components (patterns). Subfigure (c) illustrates the variance of mask token embeddings in decoder layers, reflecting the diversity of learned patch clusters. The results indicate that the proposed method learns more global features, emphasizes pattern-based representation, and achieves finer patch-level clustering compared to the vanilla MAE.", "section": "5.4 Analysis on the Learned Feature Space"}, {"figure_path": "7vXufiEzSy/figures/figures_13_1.jpg", "caption": "Figure I: Hierarchical latent variable model framework [29]. Assuming high-level shared information c exists among the whole tokens, MAE encoder learns to estimate \u0109 from X to reconstruct raw pixels of Xm. Here, shared information is equivalent to statistical dependency inside X. sm and sv stand for information specific to Xm and Xu, respectively. Dotted line indicates potential dependency.", "description": "This figure illustrates the hierarchical latent variable model framework of Masked Autoencoders. It shows how the encoder estimates high-level latent variables (shared information) from visible patches to reconstruct masked-out patches.  The dotted line represents the potential statistical dependency between visible and masked patches.  It visually explains the process of reconstructing masked-out image information by leveraging the learned high-level latent variables representing the entire image.", "section": "Appendix A Method Elaboration"}, {"figure_path": "7vXufiEzSy/figures/figures_14_1.jpg", "caption": "Figure 5: Examples of Self-guided Informed masking. More examples and detailed explanations on our method are displayed in Appendix A.", "description": "This figure provides several examples to visually compare different masking strategies.  The top row shows the input images. The second row shows the images after random masking. The third row shows the images after bi-partitioned masking. The fourth row displays the images after applying the proposed self-guided informed masking method. The bottom row shows the result of the proposed method with hint tokens added. The figure demonstrates how the proposed method generates more informative masks by focusing on the main object in each image, leading to improved performance in downstream tasks.", "section": "4 Self-Guided Informed Masking"}, {"figure_path": "7vXufiEzSy/figures/figures_15_1.jpg", "caption": "Figure III: Illustrations of patch clusters learned by MAE. (a) Input images. (b) Similarity-based patch clusters. (c) t-sne plots of the patch embeddings.", "description": "This figure shows examples of patch clustering learned by the MAE model.  It contains three rows. The top row (a) shows example input images. The middle row (b) shows the images again, but with patches colored according to which cluster they belong to, illustrating that MAE groups similar patches (e.g., similar textures or colors) together. The bottom row (c) uses t-SNE to project the patch embeddings into a 2D space, visually demonstrating that patches assigned to the same cluster are located near each other in the embedding space. The figure visually demonstrates the ability of MAE to learn patch-level clustering from raw image data.", "section": "B Token Relations"}, {"figure_path": "7vXufiEzSy/figures/figures_15_2.jpg", "caption": "Figure 3: MAE learns patch clustering from very early stage of training process. (a) MAE widens the gap \u03bcintra\u2212\u03bcinter. (b) Token relations drastically converge at early epochs and then gradually level off. Numbers in the legend denote the layer i. More details are provided in Appendix B.", "description": "This figure shows that MAE starts learning patch-level clustering from the very beginning of pre-training.  Part (a) tracks the difference between the mean intra-cluster and mean inter-cluster edge weights over training epochs, showing a widening gap indicating increasing clustering. Part (b) shows the KL divergence of token relations over epochs for different layers, illustrating that relations converge rapidly in early epochs and then stabilize, indicating early establishment of patch clusters.", "section": "3.3 When Does MAE Learn Patch Clustering?"}, {"figure_path": "7vXufiEzSy/figures/figures_16_1.jpg", "caption": "Figure V: Bi-partitioning performance of various models. MAE, MoCo and ViT show different trends of bi-partitioning performance in both of (a) similarity score and (b) attention score.", "description": "This figure compares the performance of three different models (MAE, MoCo, and ViT) in terms of bi-partitioning image patches into two clusters.  It shows how the gap between the mean intra-cluster and inter-cluster similarity scores changes over training epochs using two different metrics: similarity score and attention score.  The plots illustrate that MAE consistently widens the gap, indicating effective patch clustering, whereas MoCo shows a clear gap from early stages, and ViT exhibits inconsistent behavior.  This visualization supports the paper's claim that MAE excels at learning pattern-based patch-level clustering.", "section": "5.4 Analysis on the Learned Feature Space"}, {"figure_path": "7vXufiEzSy/figures/figures_16_2.jpg", "caption": "Figure 3: MAE learns patch clustering from very early stage of training process. (a) MAE widens the gap \\(\\mu_{intra} - \\mu_{inter}\\). (b) Token relations drastically converge at early epochs and then gradually level off. Numbers in the legend denote the layer i. More details are provided in Appendix B.", "description": "This figure shows that MAE learns patch clustering from a very early stage of training.  The first graph (a) shows the difference between intra-cluster and inter-cluster edge weights over training epochs using both cosine similarity and attention scores; this gap increases over time, demonstrating the separation of clusters.  The second graph (b) displays the KL divergence of token relations across training epochs for different layers; it demonstrates rapid convergence at early stages, signifying the establishment of patch-level clustering relationships early in the training process.", "section": "3.3 When Does MAE Learn Patch Clustering?"}, {"figure_path": "7vXufiEzSy/figures/figures_17_1.jpg", "caption": "Figure VII: Qualitative comparison on ImageNet validation set. Patches are discriminated in more fine-grained manner with our method. More diverse and finer patch clusters constructed in foreground verify our hypothesis that intensive masking on specific cluster leads to establish more diverse high-level latent variables.", "description": "This figure shows a qualitative comparison of patch clustering between the proposed method and the original MAE on the ImageNet validation set.  The top row displays the input images. The middle row shows the patch clustering results obtained using the original MAE. The bottom row displays the patch clustering results obtained using the proposed self-guided masked autoencoder. The figure demonstrates that the proposed method achieves more fine-grained and diverse patch clustering compared to the original MAE, supporting the hypothesis that intensive masking on specific clusters leads to a more diverse set of high-level latent variables. The various colors represent different clusters.", "section": "C Qualitative Results"}, {"figure_path": "7vXufiEzSy/figures/figures_18_1.jpg", "caption": "Figure VIII: Comparison of the Quality of the informed masks generated from different layers. Each example is denoted by the index of the original image in Figure II. Although early layers of the encoder and the last layer of the decoder yield inappropriate bi-partitioning result, our similarity-score-based masking strategy robustly alleviates this issue, leading to minor difference in performance in the layer selection for generating informed mask.", "description": "This figure shows a comparison of the quality of informed masks generated using different layers (Layer 3, Layer 7, Layer 11, and Layer 8 (Decoder)) of the MAE model.  The results demonstrate that even though earlier layers of the encoder and the last decoder layer may produce less effective bi-partitioning of images, the similarity-score-based masking method consistently generates good informed masks, minimizing the impact of layer choice on performance.", "section": "D Analysis on Ablation Studies"}]