[{"figure_path": "apPHMfE63y/figures/figures_2_1.jpg", "caption": "Figure 1: Comparison of the strategic regret of OptGTM and LinUCB. The strategic arms adapt their strategies gradually over the course of 20 epochs. OptGTM performs similarly across all epochs, whereas LinUCB performs increasingly worse as the arms adapt to the algorithm (Figure 1a). Figure 1b and 1c provide a closer look at the regret of the algorithms across the T rounds in the initial epoch, where the arms are truthful, and the final epoch after the arms have adapted to the algorithms.", "description": "This figure compares the performance of the Optimistic Grim Trigger Mechanism (OptGTM) and the Linear Upper Confidence Bound (LinUCB) algorithm in a strategic linear contextual bandit setting.  It shows how the strategic regret of both algorithms changes over 20 epochs, where strategic arms adapt their strategies to maximize their selection frequency.  OptGTM maintains relatively consistent performance across epochs, while LinUCB's performance degrades significantly as the arms learn to game the system.  Subplots (b) and (c) provide a detailed view of the regret at the beginning (truthful arms) and end (strategic arms) of the experiment.", "section": "6 Experiments: Simulating Strategic Context Manipulation"}, {"figure_path": "apPHMfE63y/figures/figures_4_1.jpg", "caption": "Figure 1: Comparison of the strategic regret of OptGTM and LinUCB. The strategic arms adapt their strategies gradually over the course of 20 epochs. OptGTM performs similarly across all epochs, whereas LinUCB performs increasingly worse as the arms adapt to the algorithm (Figure 1a). Figure 1b and 1c provide a closer look at the regret of the algorithms across the T rounds in the initial epoch, where the arms are truthful, and the final epoch after the arms have adapted to the algorithms.", "description": "This figure compares the strategic regret of two algorithms, OptGTM and LinUCB, across 20 epochs.  The arms strategically adapt their strategies over time.  The figure shows that OptGTM's regret remains relatively consistent, while LinUCB's regret increases substantially as the arms adapt, approaching that of a uniformly random selection strategy. Subfigures 1b and 1c zoom in on the regret at the beginning (truthful arms) and end (adapted arms) of the experiment to highlight the performance difference.", "section": "6 Experiments: Simulating Strategic Context Manipulation"}, {"figure_path": "apPHMfE63y/figures/figures_9_1.jpg", "caption": "Figure 1: Comparison of the strategic regret of OptGTM and LinUCB. The strategic arms adapt their strategies gradually over the course of 20 epochs. OptGTM performs similarly across all epochs, whereas LinUCB performs increasingly worse as the arms adapt to the algorithm (Figure 1a). Figure 1b and 1c provide a closer look at the regret of the algorithms across the T rounds in the initial epoch, where the arms are truthful, and the final epoch after the arms have adapted to the algorithms.", "description": "This figure compares the performance of the Optimistic Grim Trigger Mechanism (OptGTM) and LinUCB algorithms in a strategic linear contextual bandit setting.  It shows that OptGTM maintains relatively consistent regret over 20 epochs, while LinUCB's regret significantly increases as strategic arms adapt their strategies to exploit LinUCB's weaknesses.  The figure is broken into three subplots: one showing overall regret across epochs, and two focusing on epoch 0 (where the arms are truthful) and epoch 20 (after the arms have fully adapted).", "section": "Experiments: Simulating Strategic Context Manipulation"}, {"figure_path": "apPHMfE63y/figures/figures_9_2.jpg", "caption": "Figure 1: Comparison of the strategic regret of OptGTM and LinUCB. The strategic arms adapt their strategies gradually over the course of 20 epochs. OptGTM performs similarly across all epochs, whereas LinUCB performs increasingly worse as the arms adapt to the algorithm (Figure 1a). Figure 1b and 1c provide a closer look at the regret of the algorithms across the T rounds in the initial epoch, where the arms are truthful, and the final epoch after the arms have adapted to the algorithms.", "description": "This figure compares the strategic regret of two algorithms, OptGTM and LinUCB, across 20 epochs where strategic arms adapt their strategies.  The left panel (1a) shows the overall regret; OptGTM's regret remains relatively stable, while LinUCB's regret increases significantly as the arms adapt.  The right two panels (1b and 1c) provide detailed regret comparisons for epoch 0 (truthful arms) and epoch 20 (strategic arms), respectively, illustrating the performance difference more clearly.", "section": "Experiments: Simulating Strategic Context Manipulation"}]