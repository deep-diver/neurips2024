[{"figure_path": "38UFpdt3Tr/figures/figures_1_1.jpg", "caption": "Figure 1: Key components of D2DMoE: (a) We enhance the activation sparsity in the base model. (b) We convert FFN layers in the model to MoE layers with routers that predict the contribution of each expert. (c) We introduce dynamic-k routing that selects the experts for execution based on their predicted contribution.", "description": "This figure illustrates the three main components of the Dense to Dynamic-k Mixture-of-Experts (D2DMoE) method.  (a) shows how the activation sparsity is enhanced in the base model before conversion. (b) depicts the conversion of feed-forward network (FFN) layers into Mixture-of-Experts (MoE) layers, highlighting the role of routers in predicting expert contribution. (c) illustrates the dynamic-k expert selection, a key improvement that adapts the number of experts used per token.", "section": "Contributions of this paper"}, {"figure_path": "38UFpdt3Tr/figures/figures_2_1.jpg", "caption": "Figure 2: (a) Cost-accuracy tradeoff for a MoEfied [27] GPT-2 model obtained starting from models with different levels of activation sparsity. Sparsification correlates with the model performance. (b) Distribution of non-zero activations in the FFN layers in GPT-2-base on OpenWebText, with and without the sparsity enforcement phase. Both models exhibit significant variance, and the mean-to-variance ratio increases in the sparsified model. (c) We propose to exploit the variation in activations through a dynamic-k routing procedure that adapts the number of experts allocated to a sample.", "description": "This figure demonstrates the impact of activation sparsity on MoE conversion, showing a cost-accuracy tradeoff.  It displays that higher activation sparsity improves the efficiency of MoE conversion and that a dynamic-k expert selection method, which adapts the number of experts used per sample, significantly enhances efficiency compared to a static top-k method.  The distribution of non-zero activations shows significant variance, suggesting that a dynamic approach is beneficial.", "section": "3 Method"}, {"figure_path": "38UFpdt3Tr/figures/figures_4_1.jpg", "caption": "Figure 4: FLOPs-performance tradeoff comparison of our method and MoEfication [52] on CV and NLP benchmarks. We also include early-exit (ZTW, [49]) and token dropping baselines (A-ViT, [51]) for classification. Our method outperforms these baselines across multiple computational budgets.", "description": "This figure presents the cost-performance tradeoffs of different model compression and acceleration methods applied to four different tasks.  The x-axis represents the computational cost in GFLOPS (or TFLOPS for Gemma-2B), and the y-axis represents the accuracy or loss.  Four different methods are compared: MoEfication (a baseline method), D2DMoE (the proposed method), ZTW (a zero-time waste early-exit method), and A-ViT (an adaptive token dropping method).  The results show that D2DMoE consistently outperforms other methods across a wide range of computational budgets for all four tasks. This demonstrates the efficiency of D2DMoE in achieving high performance while significantly reducing inference cost.", "section": "4 Experiments"}, {"figure_path": "38UFpdt3Tr/figures/figures_4_2.jpg", "caption": "Figure 1: Key components of D2DMoE: (a) We enhance the activation sparsity in the base model. (b) We convert FFN layers in the model to MoE layers with routers that predict the contribution of each expert. (c) We introduce dynamic-k routing that selects the experts for execution based on their predicted contribution.", "description": "This figure illustrates the three main steps of the D2DMoE method: First, activation sparsity is enhanced in the base model. Then, feed-forward network (FFN) layers are transformed into Mixture-of-Experts (MoE) layers, using routers that estimate the contribution of each expert. Finally, dynamic-k routing is applied to select which experts to activate on a per-token basis.", "section": "Contributions of this paper"}, {"figure_path": "38UFpdt3Tr/figures/figures_5_1.jpg", "caption": "Figure 5: Single D2DMoE layer execution wall-clock time.", "description": "This figure compares the execution time of a single D2DMoE layer against a standard MLP layer on an NVIDIA A100 GPU.  The x-axis represents the number of FLOPs (floating point operations), and the y-axis represents the wall-clock time in milliseconds. The figure shows that D2DMoE, especially when using the optimized Triton implementation, significantly reduces the wall-clock time compared to the standard MLP, achieving a 63% speedup at a point where D2DMoE maintains 99% of the original accuracy.  The plot demonstrates the efficiency gains of D2DMoE in terms of inference speed.", "section": "4.4 Execution latency"}, {"figure_path": "38UFpdt3Tr/figures/figures_6_1.jpg", "caption": "Figure 6: D2DMoE applied to models pruned with CoFi.", "description": "This figure shows the results of applying the D2DMoE method on top of models that have been pruned using the CoFi method.  Different sparsity levels (s) are tested, showing that D2DMoE continues to provide acceleration even when using highly pruned models.  The x-axis represents the GFLOPs and the y-axis represents the accuracy. The figure demonstrates the complementarity of D2DMoE and CoFi in achieving both reduced computational cost and maintained performance.", "section": "4.5 Compatibility with model compression techniques"}, {"figure_path": "38UFpdt3Tr/figures/figures_6_2.jpg", "caption": "Figure 4: FLOPs-performance tradeoff comparison of our method and MoEfication [52] on CV and NLP benchmarks. We also include early-exit (ZTW, [49]) and token dropping baselines (A-ViT, [51]) for classification. Our method outperforms these baselines across multiple computational budgets.", "description": "This figure shows the comparison results of the proposed method, D2DMoE, and other methods (MoEfication, ZTW, A-ViT) on various tasks (image classification, text classification, language modeling) with different computational budgets.  The x-axis represents the computational cost (GFLOPS or TFLOPS), and the y-axis represents the accuracy or loss.  The figure demonstrates that D2DMoE achieves a better cost-performance trade-off compared to existing approaches, maintaining high accuracy even with significantly reduced computational costs.", "section": "4 Experiments"}, {"figure_path": "38UFpdt3Tr/figures/figures_7_1.jpg", "caption": "Figure 8: D2DMOE allows for a dynamical allocation of computation for each layer and each input independently. a) Per-layer distribution of the number of executed experts on CARER dataset in D2DMOE with \u03c4 = 0.01 for a standard model (top) and a sparsified model (bottom). Sparsification leads to a significantly lower number of selected experts. b) Computational load maps of selected ImageNet-1k samples for our converted ViT-B model with \u03c4 = 0.0025. D2DMoE allocates its computational budget to semantically important regions of the input.", "description": "This figure shows two aspects of the D2DMoE model's dynamic computation allocation.  The left panel (a) displays histograms showing the distribution of the number of experts used per layer for both a standard and a sparsified model on the CARER dataset. It highlights how sparsification significantly reduces the number of experts needed. The right panel (b) presents computational load maps for several ImageNet-1k images processed by a converted ViT-B model. The heatmaps illustrate how the model focuses computational resources on semantically important areas of the images.", "section": "5 Analysis"}, {"figure_path": "38UFpdt3Tr/figures/figures_8_1.jpg", "caption": "Figure 9: Analysis experiments with D2DMoE. (a) Phases ablation (BERT-base) (b) Sparsification (GPT2-base) (c) ReLU vs GELU (GPT2-base) (d) Expert granularity (GPT2-base)", "description": "This figure presents the results of ablation studies and other experiments conducted to analyze the impact of different components of the proposed D2DMoE method.  Panel (a) shows the impact of each phase (sparsification, regression routing, dynamic-k, and attention projection replacement) on a BERT-base model. Panel (b) shows the correlation between activation sparsity of the base model and performance of the resulting MoE model for GPT-2-base.  Panel (c) compares the performance using ReLU and GELU activation functions for GPT-2-base models.  Panel (d) analyzes the effect of different expert sizes on the performance and computational cost for GPT2-base.", "section": "5 Analysis"}, {"figure_path": "38UFpdt3Tr/figures/figures_15_1.jpg", "caption": "Figure 10: FLOPs ratio between dynamic-k expert layer and standard two-layer MLP for different values of the total number of experts n and number of selected experts k. We assume the hidden dimension of router dh is based on model dimension dm, and set standard expansion factor e = 4. For different sizes of router, dynamic-k uses fewer FLOPs than standard MLP as long as the total number of experts is sufficiently large and the number of selected experts is not equal to the total number of experts. For the clarity of presentation, we plot discrete values of k and n as continuous.", "description": "This figure shows the FLOPs ratio between a dynamic-k expert layer and a standard two-layer MLP.  It illustrates how the dynamic-k approach, by selecting fewer experts (k) out of the total (n), can significantly reduce computational cost compared to a standard MLP, especially when the total number of experts is sufficiently large and not all experts are used.", "section": "B Comparison of FLOPs between standard FFN layer and dynamic-k MoE"}, {"figure_path": "38UFpdt3Tr/figures/figures_15_2.jpg", "caption": "Figure 5: Single D2DMoE layer execution wall-clock time.", "description": "This figure shows the wall-clock time of a single D2DMoE layer with 24 experts and an expert dimensionality of 128, measured on an NVIDIA A100 GPU.  The x-axis represents the FLOPS (floating point operations per second), and the y-axis shows the wall-clock time in milliseconds. The plot demonstrates that the execution time scales linearly with the number of executed experts, indicating that D2DMoE's efficient implementation has negligible overhead. The figure also displays a comparison with a standard MLP (Multilayer Perceptron) module, illustrating that D2DMoE achieves a speedup of up to 3 times while maintaining 99% of the original accuracy.", "section": "4.4 Execution latency"}, {"figure_path": "38UFpdt3Tr/figures/figures_16_1.jpg", "caption": "Figure 12: Performance of D2DMoE applied on a ViT-S distilled from the larger ViT-B model.", "description": "This figure shows the performance of D2DMoE when applied to a Vision Transformer (ViT) model that has been trained via knowledge distillation using a larger ViT model as the teacher. The x-axis represents the computational cost (GFLOPs), while the y-axis shows the accuracy.  The plot demonstrates that D2DMoE is effective at reducing the computational cost of even a distilled model, while maintaining a high level of accuracy.", "section": "4.5 Compatibility with model compression techniques"}, {"figure_path": "38UFpdt3Tr/figures/figures_16_2.jpg", "caption": "Figure 4: FLOPs-performance tradeoff comparison of our method and MoEfication [52] on CV and NLP benchmarks. We also include early-exit (ZTW, [49]) and token dropping baselines (A-ViT, [51]) for classification. Our method outperforms these baselines across multiple computational budgets.", "description": "This figure shows the cost-performance trade-off for various model compression techniques on different computer vision (CV) and natural language processing (NLP) benchmarks.  It compares the proposed D2DMoE method against MoEfication, an early-exit baseline (ZTW), and a token-dropping baseline (A-ViT). The x-axis represents the computational cost in FLOPs (floating-point operations per second), while the y-axis represents either accuracy or loss, depending on the specific benchmark.  The results demonstrate that D2DMoE consistently achieves superior performance across various computational budgets compared to the other methods.", "section": "4 Experiments"}, {"figure_path": "38UFpdt3Tr/figures/figures_17_1.jpg", "caption": "Figure 9: Analysis experiments with D2DMoE. (a) Phases ablation (BERT-base) (b) Sparsification (GPT2-base) (c) ReLU vs GELU (GPT2-base) (d) Expert granularity (GPT2-base)", "description": "This figure presents the ablation study of the proposed D2DMoE method, showing the impact of each component on the performance, and compares the performance of models with different activation functions (ReLU and GELU) and expert granularities. The results demonstrate that each component contributes to performance improvement, and that D2DMoE achieves better cost-accuracy trade-off compared to the baseline, especially with smaller experts.", "section": "5 Analysis"}, {"figure_path": "38UFpdt3Tr/figures/figures_17_2.jpg", "caption": "Figure 1: Key components of D2DMoE: (a) We enhance the activation sparsity in the base model. (b) We convert FFN layers in the model to MoE layers with routers that predict the contribution of each expert. (c) We introduce dynamic-k routing that selects the experts for execution based on their predicted contribution.", "description": "This figure illustrates the three main components of the Dense to Dynamic-k Mixture-of-Experts (D2DMoE) method.  (a) shows how the activation sparsity of the original dense model is improved. (b) demonstrates the conversion of Feed-Forward Networks (FFN) layers into Mixture-of-Experts (MoE) layers, highlighting the role of routers in predicting expert contribution. Finally, (c) details the dynamic-k routing mechanism which adaptively selects the number of experts to execute per token, enhancing efficiency.", "section": "Contributions of this paper"}, {"figure_path": "38UFpdt3Tr/figures/figures_21_1.jpg", "caption": "Figure 16: Per-layer distribution of the number of executed experts in D2DMoE trained on the CARER with different \u03c4 thresholds for a standard, non-sparsified model (top row) and a sparsified model (bottom row). The high variability of that number explains the computational gains from using dynamic-k.", "description": "This figure shows the distribution of the number of experts executed per layer for different threshold values (\u03c4) in the D2DMoE model trained on the CARER dataset.  The top row displays results for a standard model, while the bottom row shows results for a model with enforced sparsity.  The heatmaps illustrate the percentage of selected experts across layers for three different threshold values (\u03c4 = 0.1, \u03c4 = 0.01, and \u03c4 = 0.001). The high variability across layers and across threshold values highlights the advantage of the dynamic-k expert selection strategy; the model adapts compute allocation to the difficulty of each input.", "section": "5.1 Expert selection patterns"}, {"figure_path": "38UFpdt3Tr/figures/figures_21_2.jpg", "caption": "Figure 16: Per-layer distribution of the number of executed experts in D2DMoE trained on the CARER with different \u03c4 thresholds for a standard, non-sparsified model (top row) and a sparsified model (bottom row). The high variability of that number explains the computational gains from using dynamic-k.", "description": "This figure shows the distribution of the number of executed experts across different layers of a model trained on the CARER dataset using the proposed D2DMoE method.  It compares a standard model with a sparsified model, showing that the sparsified model consistently uses fewer experts. The variability in the number of experts across layers and different threshold values (\u03c4) highlights the effectiveness of the dynamic-k expert selection mechanism in D2DMoE, which adapts to the complexity of input data by dynamically choosing the number of experts to execute per layer. This dynamic adaptation leads to computational savings.", "section": "5 Analysis"}, {"figure_path": "38UFpdt3Tr/figures/figures_21_3.jpg", "caption": "Figure 8: D2DMOE allows for a dynamical allocation of computation for each layer and each input independently. a) Per-layer distribution of the number of executed experts on CARER dataset in D2DMoE with \u03c4 = 0.01 for a standard model (top) and a sparsified model (bottom). Sparsification leads to a significantly lower number of selected experts. b) Computational load maps of selected ImageNet-1k samples for our converted ViT-B model with \u03c4 = 0.0025. D2DMoE allocates its computational budget to semantically important regions of the input.", "description": "This figure shows the dynamic computation allocation of D2DMoE.  (a) demonstrates how the number of experts used varies across layers, and this variance is reduced by sparsification. (b) illustrates how D2DMoE focuses computation on semantically important regions of an image, showcasing its adaptability.", "section": "5 Analysis"}, {"figure_path": "38UFpdt3Tr/figures/figures_22_1.jpg", "caption": "Figure 16: Per-layer distribution of the number of executed experts in D2DMoE trained on the CARER with different \u03c4 thresholds for a standard, non-sparsified model (top row) and a sparsified model (bottom row). The high variability of that number explains the computational gains from using dynamic-k.", "description": "This figure shows the distribution of the number of experts used per layer in the D2DMoE model trained on the CARER dataset.  It compares a standard model with a sparsified model. The heatmaps show the percentage of selected experts for different thresholds (\u03c4 = 0.1, 0.01, 0.001). The high variability in the number of experts across layers and different thresholds highlights the efficiency of the dynamic-k expert selection mechanism, which adapts the number of experts to the input complexity. The sparsified model consistently uses fewer experts than the standard model.", "section": "5.1 Expert selection patterns"}, {"figure_path": "38UFpdt3Tr/figures/figures_22_2.jpg", "caption": "Figure 16: Per-layer distribution of the number of executed experts in D2DMoE trained on the CARER with different \u03c4 thresholds for a standard, non-sparsified model (top row) and a sparsified model (bottom row). The high variability of that number explains the computational gains from using dynamic-k.", "description": "This figure shows the distribution of the number of experts executed per layer for both a standard and a sparsified model trained on the CARER dataset using the D2DMoE method.  Three different thresholds (\u03c4) are used to control the number of experts selected.  The top row displays results for the standard model, and the bottom row displays results for the sparsified model. The heatmaps illustrate the percentage of selected experts across layers for each threshold value. The high variability in the number of experts used across layers highlights the efficiency of the dynamic-k expert selection method, which adapts to the input complexity.", "section": "5.1 Expert selection patterns"}]