[{"type": "text", "text": "ROBO-INSTRUCT: Simulator-Augmented Instruction Alignment For Finetuning CodeLLMs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Large language models (LLMs) have shown great promise at generating robot pro  \n2 grams from natural language given domain-specific robot application programming   \n3 interfaces (APIs). However, the performance gap between proprietary LLMs and   \n4 smaller open-weight LLMs remains wide. This raises a question: Can we fine  \n5 tune smaller open-weight LLMs for generating domain-specific robot programs to   \n6 close the performance gap with proprietary LLMs? While SELF-INSTRUCT is a   \n7 promising solution by generating a diverse set of training data, it cannot verify the   \n8 correctness of these programs. In contrast, a robot simulator with a well-defined   \n9 world can identify execution errors but limits the diversity of programs that it can   \n0 verify. In this work, we introduce ROBO-INSTRUCT, which brings the best of   \n11 both worlds \u2014 it promotes the diversity of SELF-INSTRUCT, while providing cor  \n12 rectness of simulator-based checking. ROBO-INSTRUCT introduces ROBOSIM to   \n13 synthesize a consistent world state on the fly by inferring properties relevant to the   \n14 program being checked, and simulating actions accordingly. Furthermore, the in  \n15 structions and programs generated by SELF-INSTRUCT may be subtly inconsistent   \n16 \u2014 such as the program missing a step implied by the instruction. ROBO-INSTRUCT   \n17 further addresses this with INSTALIGN, an instruction-program alignment pro  \n18 cedure that revises the task instruction to reflect actual results of the generated   \n19 program. Given a few seed task descriptions and the robot APIs, ROBO-INSTRUCT   \n20 is capable of generating a training dataset using only a small open-weight model.   \n21 This dataset is then be used to fine-tune small open-weight language models, en  \n22 abling them to even exceed the performance of several proprietary LLMs including   \n23 GPT-3.5-Turbo and Gemini-Pro. ", "page_idx": 0}, {"type": "text", "text": "24 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "25 Large language models (LLMs) have demonstrated great promise at generating robot programs from   \n26 natural language instructions [3, 10\u201312, 17, 18, 31, 39]. For example, consider an instruction for   \n27 a service mobile robot: \"Check how many conference rooms have no markers.\" The robot may   \n28 be equipped with a domain-specific robot application programming interface (API) that includes   \n29 skills such as go_to(location) for navigation and is_in_room(object) for perception. Since   \n30 such domain-specific APIs do not exist in the training dataset of general-purpose LLMs, in-context   \n31 learning (ICL) via few-shot examples is often employed to describe and use such APIs for performing   \n32 few-shot inference. However, there is a significant performance gap [10] in the correctness of   \n33 programs generated by ICL for large proprietary models and smaller open-weight models that can be   \n34 deployed locally on robots. This raises a question: can we fine-tune small open-weight LLMs for   \n35 generating domain-specific robot programs to close the performance gap with proprietary LLMs?   \n36 Since training datasets of the domain-specific robot programs are often unavailable, SELF-INSTRUCT   \n37 might seem like a promising solution [29, 36]. Consider the setting of generating programs for   \n38 service mobile robots that can perceive objects, navigate to various locations, manipulate items, and   \n39 communicate with humans. By formulating these robot skills into APIs, we can create a few seed   \n40 task examples demonstrating their use case and employ SELF-INSTRUCT to generate a diverse set of   \n41 instruction-program pairs as training data, as illustrated in Fig. 1. However, using SELF-INSTRUCT   \n42 na\u00efvely may generate infeasible instructions\u2014e.g., asking the robot to pick up multiple objects at once   \n43 when it cannot due to physical constraints. They can also violate domain-specific constraints. For   \n44 example, in Fig. 1, after line 2 confirms the absence of a key at the current location, line 3 erroneously   \n45 attempts to pick up a key. Further, these instructions may not align with the generated programs, even   \n46 if these programs are valid. For example, Fig. 1 shows an example instruction directing the robot to   \n47 verbally ask in each room if a key exists, whereas the program instructs the robot to visually check   \n48 in each room. Finally, the generated programs may have execution errors. These challenges may   \n49 appear to be solvable using a simulator, but a simulator needs an initial world state to check against   \n50 programs. A simulator using a hand-curated world state will end up rejecting the wide diversity of   \n51 programs generated by SELF-INSTRUCT, even if they are executable, just because the world state did   \n52 not capture some aspect relevant to them (e.g., the presence of a \u201ckey\u201d).   \n53 This work introduces ROBO-INSTRUCT, a new framework based on SELF-INSTRUCT, to address these   \n54 issues and improve the performance of small open-weight language models for generating domain  \n55 specific robot programs. As shown in Fig. 1, ROBO-INSTRUCT introduces two novel components:   \n56 (1) ROBOSIM, a task-agnostic simulator that encodes domain-specific constraints and validates   \n57 robot programs generated from SELF-INSTRUCT. Critically, ROBOSIM dynamically synthesizes   \n58 a consistent world state starting from arbitrary programs. (2) INSTALIGN, an instruction-program   \n59 alignment procedure that revises the generated instructions to better reflect the intent of the generated   \n60 programs. ROBO-INSTRUCT also employs a rejection-sampling mechanism that rejects invalid   \n61 programs detected by ROBOSIM and queries SELF-INSTRUCT for a new program corresponding to   \n62 the same generated instruction.   \n63 We validate ROBO-INSTRUCT by fine-tuning Codellama-Python-7B [30] and evaluate on ROBOEVAL,   \n64 a domain-specific code generation benchmark for service mobile robots. We show that ROBO  \n65 INSTRUCT is capable of improving the performance of the Codellama model by using only a small   \n66 open-weight model to generate the training dataset. Compared to the base Codellama-Python  \n67 7B model without fine-tuning, our ROBO-INSTRUCT fine-tuned models outperform by $28.75\\%$ in   \n68 average pass $@1$ scores; and, compared to SELF-INSTRUCT fine-tuned model, our model outperform   \n69 by $13.75\\%$ .; and the best pass $@1$ of ROBO-INSTRUCT fine-tuned model achieves a $68.75\\%$ match,   \n70 surpassing the performance of the proprietary GPT-3.5-Turbo and Gemini-1.0-Pro.   \n72 1. We introduce ROBO-INSTRUCT, a new framework for improving the code generation   \n73 performance of small open-weight language models for domain-specific robot programs.   \n74 This framework introduces two novel components, ROBOSIM and INSTALIGN.   \n75 2. We introduce a dynamic world synthesis and evaluation process for generating relevant   \n76 world states for automated code checking for diverse, arbitrary tasks in ROBOSIM.   \n77 3. We introduce INSTALIGN, an instruction alignment procedure to refine instruction-code   \n78 pairs to improve alignment between instructions and code generated by SELF-INSTRUCT.   \n79 4. We fine-tune a small open-weight model, Codellama-Python-7B [30], using ROBO  \n80 INSTRUCT, and improve its performance to outperform several CodeLLMs, including   \n81 Deepseek-Coder-33B [8], and Starcoder2-15B [21] and two proprietary LLMs, GPT-3.5-   \n82 Turbo [27] and Gemini-1.0-Pro [33] on the ROBOEVAL benchmark. ", "page_idx": 0}, {"type": "image", "img_path": "LnJ2EGKTXh/tmp/dda51a0e481c438a04849a15c43c5eb45516183017dfc8fb7432e38589daaf93.jpg", "img_caption": ["Figure 1: High-Level Overview of ROBO-INSTRUCT. This figure also illustrates an example of an invalid SELF-INSTRUCT-generated instruction and program, as well as pass $@1$ results of different LLMs on ROBOEVAL. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "83 Our code and data will be released at URL anonymized. ", "page_idx": 2}, {"type": "text", "text": "84 2 ROBO-INSTRUCT ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "85 In this section, we present how ROBO-INSTRUCT generates training datasets of domain-specific robot   \n86 programs. Alg. 1 shows a broad overview of the framework. To add an entry in the training dataset,   \n87 SELF-INSTRUCT first generates an instruction-program pair, $(\\mathcal{T},\\mathcal{P})$ , from the robot APIs and seed   \n88 tasks, shown in Appendix A.4. Then, ROBOSIM dynamically synthesizes a consistent world state on   \n89 the fly as it executes and validates $\\mathcal{P}$ . If $\\mathcal{P}$ is invalid, ROBO-INSTRUCT employs a rejection-sampling   \n90 method, which generates a new program $\\mathcal{P}$ given the same $\\mathcal{T}$ and evaluates the new $\\mathcal{P}$ again. This   \n91 process repeats until $\\mathcal{P}$ becomes valid or a predefined maximum resampling limit is reached. If the   \n92 limit is reached, the instruction might be invalid given the domain-specific APIs or too complex to   \n93 generate a program, so the instruction-program pair is discarded. Finally, if $\\mathcal{P}$ is valid, INSTALIGN   \n94 takes in $(\\mathcal{T},\\mathcal{P})$ to revise $\\mathcal{T}$ to better reflect the intent of $\\mathcal{P}$ and the aligned instruction and program is   \n95 saved to the training dataset. In the following subsections, we elaborate on the specific design of each   \n96 component.   \n98 We present a principled approach to design ROBOSIM for validating domain-specific robot programs.   \n99 Alg. 2 illustrates the high-level algorithm used to assess the correctness of a robot program. ROBOSIM   \n100 employs the concept of world state to simulate the robot actions directed by a program, ensuring   \n101 consistent and reliable evaluation. A world state is a symbolic representation of the environment   \n102 in which the robot operates, and it keeps track of the high-level changes in the robot state and the   \n103 surrounding environment as the robot performs actions in order. For example, consider a program   \n104 instruction that commands a robot to check if an apple is nearby. The world state queries the stored   \n105 information about the surrounding environment, identifies all objects at the robot\u2019s current location,   \n106 and informs the program whether an apple is present.   \n107 However, since SELF-INSTRUCT generates arbitrary programs based on the provided APIs, ROBOSIM   \n108 does not know what a plausible world state relevant to the program would be a priori \u2014 e.g., reasoning   \n109 about the existence of an apple in the example program. Thus, we equip ROBOSIM with the ability   \n110 to expand the world state as more robot actions are performed. Our approach is inspired by angelic   \n111 execution [4], which has previously been used for software verification of programs with partially   \n112 defined library functions. In our case, instead of partially defined library functions, we have unknown   \n113 plausible world states. ROBOSIM dynamically synthesizes and grows a world state based on domain  \n114 specific constraints (e.g., object permanence, robot skills, etc.) and the execution trace of the program,   \n115 which allows it to infer a consistent and relevant world state.   \n116 Specifically, ROBOSIM modifies the program to replace all API calls with the DYNAMICEVAL   \n117 function (Alg. 2 line 4) \u2014 when an API function is called during execution, the DYNAMICEVAL   \n118 function is invoked instead.   \n119 DYNAMICEVAL makes an important extension to the formulation of STRIPS [7] to integrate with   \n120 API functions. DYNAMICEVAL equips each API function with specific pre-conditions, effects, and   \n121 return values. The pre-conditions are composed of literals tailored to the function\u2019s requirements.   \n122 For instance, the API function is_in_room(\u2018apple\u2019), which determines if an object \u2018apple\u2019 is in   \n123 the same room as the robot, uses two literals for its pre-condition: robot_at(X) and obj_at(X,   \n124 \u2018apple\u2019). Generally, STRIPS assigns one of two possible values to each literal: True if the literal is   \n125 defined, otherwise False. However, prior to program execution, DYNAMICEVAL is unaware of the   \n126 program-relevant literals. Thus we assign a third value, undefined, to such unknown literals. Literals   \n127 must thus be explicitly defined as either True or False, or they remain undefined if not specified.   \n128 Alg. 3 demonstrates how DYNAMICEVAL executes an API function and updates the world state. First,   \n129 it calculates the precondition specified for the function. It then checks each literal in the precondition   \n130 to see if it is defined. If a literal is undefined, DYNAMICEVAL invokes GROWWORLD, a stochastic   \n131 function that assigns a random truth value to the literal and updates the world state accordingly.   \n132 Finally, DYNAMICEVAL proceeds to execute the API function using the current world state, retrieves   \n133 the return values, and applies the function\u2019s effects to update the world state.   \n134 Fig. 2 illustrates an example of ROBOSIM executing a generated program. Initially, ROBOSIM\u2019s   \n135 world state only specifies the robot\u2019s current location, and whether a pie is in the same room   \n136 as the robot remains undefined (line 2). Therefore, DYNAMICEVAL invokes GROWWORLD to ", "page_idx": 2}, {"type": "table", "img_path": "LnJ2EGKTXh/tmp/b26e54b6c4126db36d4cd564ca772ee4abebee8bb80a70758eb8846c6ee6cd25.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "table", "img_path": "LnJ2EGKTXh/tmp/fdfc29b2407cee88902782bb26654bac2deef68dde8bdaf8777b4b241bc326c3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "LnJ2EGKTXh/tmp/dee77c5f6251d2a2ea6fdd7e82530cb524c455aa0b4525e285e1d31e8adddec2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Example of ROBOSIM executing a generated program and updating the world state. Initially, ROBOSIM begins with a world state that includes only the robot\u2019s current location. As the program executes, two distinct execution paths emerge, depicted in light purple and blue. This figure demonstrates how the world state is updated along each execution path. ", "page_idx": 4}, {"type": "text", "text": "137 randomly determine a truth value for the obj_at(start_loc, \"pie\") literal, leading to two   \n138 distinct execution paths depicted in light purple and blue. Subsequently, as additional API functions   \n139 are called, more literals are introduced or updated in the world state to ensure consistent evaluations.   \n140 Finally, due to the stochastic nature of DYNAMICEVAL, ROBOSIM must execute the generated   \n141 program multiple times to validate the program. If all executions are successful, the program is   \n142 deemed correct (Alg. 2 line 5-11). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "143 2.2 INSTALIGN: Instruction-Program Alignment Procedure ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "144 Given that LLMs are extensively trained in code understanding [30], INSTALIGN is a procedure that   \n145 prompts an LLM to revise $\\mathcal{T}$ to better reflect the intent of $\\mathcal{P}$ . This procedure involves two steps: first,   \n146 given $\\mathcal{T}$ and $\\mathcal{P}$ , INSTALIGN leverages Chain-of-Thought reasoning [37] (CoT) to prompt an LLM   \n147 to generate a revised instruction, $\\mathcal{Z}_{\\mathrm{revised}}$ ; second, INSTALIGN invokes the LLM again to determine   \n148 whether $\\mathcal{T}$ or $\\mathcal{Z}_{\\mathrm{revised}}$ is more aligned with $\\mathcal{P}$ \u2019s intent and output the chosen instruction as $\\mathcal{T}_{\\mathrm{aligned}}$ .   \n149 To generate $\\mathcal{Z}_{\\mathrm{revised}}$ , the prompt to the LLM comprises the robot API function definitions, $\\mathcal{T},\\mathcal{P}$ , and   \n150 CoT instructions. The CoT asks the LLM to perform the following three steps in order: 1. write down   \n151 all the robot APIs used in the program; 2. examine these APIs and write down step by step what   \n152 the program does; 3. combine all the information above to revise the robot instruction. Similarly,   \n153 to determine $\\mathcal{T}_{\\mathrm{aligned}}$ , an LLM is prompted to think step by step about $\\mathcal{P},\\mathcal{Z}$ and $\\mathcal{Z}_{\\mathrm{revised}}$ to arrive at a   \n154 conclusion. Detailed prompt is shown in Appendix A.6. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "155 3 Analysis and Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "156 In this section, we investigate the following two research questions: ", "page_idx": 4}, {"type": "text", "text": "157 1. Is ROBO-INSTRUCT effective at generating training data to fine-tune a small language model   \n158 for generating domain-specific robot programs?   \n159 2. How do ROBOSIM and InstAlign impact the effectiveness of ROBO-INSTRUCT?   \n160 We conduct our investigation by fine-tuning the Codellama-Python-7B model [30] on the synthetic   \n161 dataset generated by ROBO-INSTRUCT and evaluate the fine-tuned model using ROBOEVAL [10], a   \n162 domain-specific code generation benchmark for service mobile robots. In the following subsections,   \n163 we first provide a brief description of ROBOEVAL. Then we present our experimental results address  \n164 ing the two main research questions. Finally, we offer more analysis of ROBOSIM, INSTALIGN, and   \n165 the synthetic dataset. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "166 3.1 ROBOEVAL: A Domain-Specific Robot Code Generation Benchmark ", "text_level": 1, "page_idx": 5}, {"type": "image", "img_path": "LnJ2EGKTXh/tmp/fad2f9835191b091143363741393c5f257a53bfb0a17d55d36521c4713e422db.jpg", "img_caption": ["Figure 3: ROBOEVAL APIs and benchmark task example. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "167 ROBOEVAL is a domain-specific code generation benchmark, featuring a suite of 16 tasks designed   \n168 to evaluate the ability of LLMs to understand custom APIs and generate programs for service robots.   \n169 In this domain, a service robot can perceive objects, navigate to various locations, manipulate items,   \n170 and communicate with humans. Furthermore, the robot should be capable of basic commonsense   \n171 reasoning and executing complex tasks that involve conditional and repetitive actions. To facilitate   \n172 these capabilities, ROBOEVAL defines a set of 8 API functions in Python as skill primitives. Fig. 3   \n173 illustrates these function signatures and definitions, alongside an example task instruction and its   \n174 canonical solution from the benchmark. In addition, unlike other popular code generation benchmark   \n175 tasks [2, 6, 9, 14, 16, 19], the order of the robot\u2019s actions is crucial for successfully completing the   \n176 specified tasks. For instance, in the task \"bring me a marker from the classroom that does not have a   \n177 whiteboard,\" the robot must check each classroom until it finds one without a whiteboard, whereas   \n178 simply bringing back a marker is insufficient. Hence, ROBOEVAL evaluates the generated program by   \n179 executing it in a simulator to capture the action traces, which are subsequently validated for sequence   \n180 correctness using temporal logic. ", "page_idx": 5}, {"type": "text", "text": "181 3.2 RQ1: Is ROBO-INSTRUCT Effective at Generating Training Data to Fine-Tune a Small 182 Language Model for Generating Domain-Specific Robot Programs? ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "183 Experiment Setup. We use the open-weight LLM, Llama3-8B-Inst, for ROBO-INSTRUCT. To   \n184 generate a diverse dataset, we employ nucleus sampling for creating instruction-program pairs,   \n185 setting the temperature $T=1$ and top $p=0.95$ . The maximum resampling limit is capped at 3 to   \n186 accommodate instructions that initially produce invalid programs. For the LLM used in INSTALIGN,   \n187 we empirically adjust the generation temperature to $T=0.3$ to optimize performance. Furthermore,   \n188 we assess the edit similarity between token sequences of each instruction pair in the dataset [15],   \n189 removing duplicates where the similarity score exceeds 0.6. We use the same setup to generate   \n190 data via SELF-INSTRUCT. Instead of discarding invalid programs, SELF-INSTRUCT includes every   \n191 generated instruction-program pair in the training dataset. Finally, we create two datasets with 5K   \n192 instruction-program pairs each using SELF-INSTRUCT and ROBO-INSTRUCT respectively. These   \n193 datasets are then used to fine-tune the Codellama-Python-7B model. The learning rate is set to be   \n194 3e-5 with a warmup ratio of $3\\%$ and a constant lr scheduler. We employ the AdamW optimizer [20]   \n195 with an effective batch size of 8, training each model for 5 epochs using a sequence length of 2048   \n196 tokens. We train all our models on a single H-100 GPU using unsloth [35].   \n197 Baselines. We divide our baseline models into 2 categories: 1) proprietary LLMs, including   \n198 GPT4 [28], GPT3.5-Turbo [27], Gemino-Pro [33], and 2) open-weight LLMs, including Codellama  \n199 Python-7B [30], Codellama-Python-34B, Starcoder2-33B [21], Deepseek-Coder-33B [8], and   \n200 Llama3-8B-Inst [1]. All the results are evaluated using ROBOEVAL and reported in Tab. 1.   \n201 Tab. 1 presents the average pass $@1$ results for different LLMs on ROBOEVAL, using two different   \n202 temperature settings for generation: greedy decoding at a temperatures of $T\\,=\\,0$ and nucleus   \n203 sampling at a temperature of $T=0.2$ . The results show that ROBO-INSTRUCT-fine-tuned Codellama   \n204 significantly improves upon the base Codellama-Python-7B and outperforms the SELF-INSTRUCT  \n205 fine-tuned variant. Notably, it surpasses all open-weight models, including larger ones like Codellama  \n206 Python-34B and Deepseek-Coder-33B. Additionally, although the training dataset was generated   \n207 using Llama3-8B-Inst, which scores less than $50\\%$ pass $@1$ on ROBOEVAL, our ROBO-INSTRUCT  \n208 fine-tuned model still achieves a significant improvement, scoring $68.75\\%$ under deterministic   \n209 temperature settings for generation. Finally, compared to proprietary models, while our ROBO  \n210 INSTRUCT-fine-tuned model trails the more powerful GPT-4, it outperforms GPT-3.5-Turbo and   \n211 Gemini-1.0-Pro in generating programs for service mobile robots. This result demonstrates the   \n212 effectiveness of our approach in generating domain-specific robot program data for fine-tuning a   \n213 small language model. It suggests that the fine-tuned model could potentially replace some proprietary   \n214 models, providing a more cost-effective and private option for local deployment.   \n216 Using the same setup as in the previous section, we investigate the effectiveness of ROBOSIM   \n217 and INSTALIGN. Since SELF-INSTRUCT may generate invalid instructions that no corresponding   \n218 valid program can pass in ROBOSIM, we propose rejecting these unsolvable instructions (we name   \n219 this process RU) to evaluate the upperbound performance of SELF-INSTRUCT. Tab. 2 shows the   \n220 average pass $@1$ results from Codellama-7B-Python fine-tuned on different datasets generated by   \n221 each method. First, findings from SELF-INSTRUCT + RU indicate that simply discarding invalid   \n222 instructions could also improve model performance. Additionally, fine-tuning with a dataset created   \n223 from SELF-INSTRUCT $^+$ RoboSim results in the smallest proportion of invalid program errors. Finally,   \n224 while incorporating either ROBOSIM or INSTALIGN individually offers some improvement over the   \n225 baseline SELF-INSTRUCT $^+$ RU results, ROBO-INSTRUCT still results in the best performance. This   \n226 indicates that the integration of these two components is important to the framework\u2019s effectiveness. ", "page_idx": 5}, {"type": "table", "img_path": "LnJ2EGKTXh/tmp/0b38aaf411d2917e15edd0a471f2f4904f03eb27c7fefd194b75011a1db08e3d.jpg", "table_caption": [], "table_footnote": ["Table 1: Pass $@1$ results of different LLMs on ROBOEVAL computed with greedy decoding $T=0$ and nucleus sampling $T=0.2$ . "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "LnJ2EGKTXh/tmp/49165d1fdc3c07aab44add664e256931405452a23b92cd8997522a0bab1eba04.jpg", "table_caption": ["215 3.3 RQ2: How Do ROBOSIM and InstAlign Impact the Effectiveness of ROBO-INSTRUCT? "], "table_footnote": ["Table 2: Pass $@1$ results of different LLMs on ROBOEVAL computed with greedy decoding $T=0$ and nucleus sampling $T=0.2$ . "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "227 3.4 Qualitative analysis of the generated program errors ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "LnJ2EGKTXh/tmp/9a9c312168b114b06df1b763fce70d1ddd795396acea10febef57bc1aecc2ced.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: SELF-INSTRUCT-Generated Program Errors: Examples 1 to 4 illustrate errors specific to the Python language, and Examples 5 to 8 highlight errors rooted in domain-specific constraints.2 ", "page_idx": 7}, {"type": "text", "text": "228 We analyze invalid programs identified by ROBOSIM, categorizing the errors into two types: language  \n229 native errors and domain-specific constraint violations. Fig. 4 displays eight examples of these   \n230 programs, with Examples 1 to 4 illustrating errors specific to the Python language, and Examples 5   \n231 to 8 highlighting errors rooted in domain-specific constraints. Language-native errors are generally   \n232 straightforward, such as syntax errors, the use of undefined variables or functions, or improper use of   \n233 provided APIs.   \n234 In contrast, errors related to domain-specific constraints tend to be more complex to detect. For   \n235 instance, Example 5 illustrates the program incorrectly trying to pick up a watering can (line 3) after   \n236 establishing that it is not present at the location (line 2). Similarly, Example 6 demonstrates an error   \n237 where the program inappropriately asks Jack (line 5) after confirming his absence from the room   \n238 (line 3). Example 7 illustrates a scenario in which ROBOSIM updates the world state by labeling   \n239 \"item storage room\" as a location after executing the go_to command (line 2). Subsequently, the   \n240 robot attempts to pick up this location (line 3), resulting in an error. Example 9 is the most intricate   \n241 scenario where the world state in the living room is updated to include a toy after the robot places it   \n242 there (line 7). When the robot returns to the living room for the second time (line 5), it does not place   \n243 down what it holds (line 7). Hence, in the third room the robot visits (line 3), when it attempts to pick   \n244 up a toy again (line 4), an error occurs because the robot can only carry one item at a time. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "245 4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "246 4.1 LLMs for Robot Code Generation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "247 LLMs have shown impressive capabilities in generating robot programs from natural language   \n248 [11, 17, 31]. One popular approach uses LLMs to generate composable costmaps for robots to   \n249 plan their motion on. In this approach, Voxposer [12] focuses on the tabletop manipulation setting   \n250 and NavCon [3] focuses on creating composable maps for navigation. Using LLM to create reward   \n251 functions is also promising. Eureka [23, 24] and Language to Rewards for Robotic Skill Synthesis [41]   \n252 both show that LLM can generate good reward functions that allows robots to acquire complex skills.   \n253 Finally, LLM can also be used to generate programs for high-level planning. $\\mathrm{LLM+p}$ [18] outputs a   \n254 robot plan in the form of the well-defined planning domain definition language (PDDL). Tidybot [39]   \n255 uses an LLM to generate a rule that captures user preferences from examples and executes a program   \n256 to sequentially complete the task in order. RoboEval [10] focuses on generating domain-specific   \n257 programs for service mobile robots. It generates a program that allows the service robot to carry out   \n258 long-horizon tasks and then validates the correctness of the program. ", "page_idx": 8}, {"type": "text", "text": "259 4.2 Generating Datasets For Fine-tuning LLMs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "260 To enhance LLMs\u2019 performance in code generation, numerous studies have explored the creation   \n261 of specialized datasets [13, 25, 26]. SELF-INSTRUCT [36] is one popular method for generating   \n262 synthetic datasets using an LLM. Following this methodology, Alpaca [32] generates 52K instruction  \n263 following demonstrations and subsequently fine-tunes the LLaMA 7B model [34] to create Alpaca 7B,   \n264 which can behave qualitatively similarly to OpenAI\u2019s text-davinci-003. Code Alpaca [5] extends this   \n265 approach to generate code instructions using 21 seed tasks, while Gorilla-LM [29] adapts the method   \n266 to focus on ML domain-specific APIs from Huggingface, TensorFlow Hub, and Torch Hub. To create   \n267 more complex instructions, Evol-Instruct [22, 40] proposes iteratively updating instructions to become   \n268 more complex through different prompting strategies. In addition to Evol-Instruct, OSS-Instruct [38]   \n269 uses open-source code snippets to generate 75K high-quality instruction data and fine-tunes the   \n270 Codelllama-Python-7B model to create Magicoder, which can match the performance of GPT-3.5-   \n271 Turbo [27] on HumanEval [6]. While these works focus on creating seed instruction sets to generate   \n272 synthetic data for effectively fine-tuning an LLM, our research investigates post-processing methods   \n273 in addition to SELF-INSTRUCT. Specifically, we concentrate on generating domain-specific programs   \n274 in robotics [10], where we can effectively leverage constraints to filter out erroneous programs. ", "page_idx": 8}, {"type": "text", "text": "275 5 Conclusion, Limitation and Future Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "276 In this work, we introduce ROBO-INSTRUCT, a novel framework to generate synthetic training data   \n277 to fine-tune small language models for domain-specific robot programs. ROBO-INSTRUCT comprises   \n278 two novel components: 1) ROBOSIM, an angelic-execution-based algorithm to effectively validate   \n279 SELF-INSTRUCT-generated programs, and 2) INSTALIGN, an instruction alignment procedure to   \n280 revise instructions to better align with the generated programs. The experimental results demonstrate   \n281 that the Codellama-Python-7B model fine-tuned on the ROBO-INSTRUCT-generated dataset can   \n282 significantly outperform many popular open-weight LLMs for generating domain-specific robot   \n283 programs. It also outperforms two proprietary LLMs, GPT-3.5-Turbo and Gemino-1.0-Pro, as well   \n284 as the SELF-INSTRUCT-fine-tuned variant. A limitation of this study is that ROBO-INSTRUCT   \n285 relies on SELF-INSTRUCT to fliter invalid programs, making the dataset quality dependent on SELF  \n286 INSTRUCT\u2019s performance. This can introduce biases if SELF-INSTRUCT consistently fails in certain   \n287 areas. Future work will explore integrating ROBO-INSTRUCT with advanced methods like Evol-Inst   \n288 and OSS-Inst to enhance dataset quality for domain-specific robot programs. ", "page_idx": 8}, {"type": "text", "text": "289 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "290 [1] Meta AI. Introducing meta llama 3: The most capable openly available llm to date. https:   \n291 //ai.meta.com/blog/meta-llama-3/, 2024. Accessed: 2024-05-21.   \n292 [2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David   \n293 Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis   \n294 with large language models, 2021.   \n295 [3] Harel Biggie, Ajay Narasimha Mopidevi, Dusty Woods, and Christoffer Heckman. Tell me   \n296 where to go: A composable framework for context-aware embodied robot navigation, 2023.   \n297 [4] Manfred Broy and Martin Wirsing. On the algebraic specification of nondeterministic program  \n298 ming languages. In Proceedings of the 6th Colloquium on Trees in Algebra and Programming,   \n299 CAAP \u201981, page 162\u2013179, Berlin, Heidelberg, 1981. Springer-Verlag. ISBN 3540108289.   \n300 [5] Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation.   \n301 https://github.com/sahil280114/codealpaca, 2023.   \n302 [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,   \n303 Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul   \n304 Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke   \n305 Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad   \n306 Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias   \n307 Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex   \n308 Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,   \n309 William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,   \n310 Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,   \n311 Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech   \n312 Zaremba. Evaluating large language models trained on code. 2021.   \n313 [7] Richard E. Fikes and Nils J. Nilsson. Strips: a new approach to the application of theorem   \n314 proving to problem solving. In Proceedings of the 2nd International Joint Conference on   \n315 Artificial Intelligence, IJCAI\u201971, page 608\u2013620, San Francisco, CA, USA, 1971. Morgan   \n316 Kaufmann Publishers Inc.   \n317 [8] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen,   \n318 Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder:   \n319 When the large language model meets programming \u2013 the rise of code intelligence, 2024. URL   \n320 https://arxiv.org/abs/2401.14196.   \n321 [9] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo,   \n322 Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding   \n323 challenge competence with apps. NeurIPS, 2021.   \n324 [10] Zichao Hu, Francesca Lucchetti, Claire Schlesinger, Yash Saxena, Anders Freeman, Sadanand   \n325 Modak, Arjun Guha, and Joydeep Biswas. Deploying and evaluating llms to program service   \n326 mobile robots. IEEE Robotics and Automation Letters, 9(3):2853\u20132860, 2024. doi: 10.1109/   \n327 LRA.2024.3360020.   \n328 [11] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps   \n329 for robot navigation. In Proceedings of the IEEE International Conference on Robotics and   \n330 Automation (ICRA), London, UK, 2023.   \n331 [12] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Vox  \n332 poser: Composable 3d value maps for robotic manipulation with language models. In 7th   \n333 Annual Conference on Robot Learning, 2023. URL https://openreview.net/forum?id=   \n334 9_8LF30mOC.   \n335 [13] Andreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi Rui Tam, Keith   \n336 Stevens, Abdullah Barhoum, Duc Minh Nguyen, Oliver Stanley, Rich\u00e1rd Nagyf,i Shahul   \n337 ES, Sameer Suri, David Alexandrovich Glushkov, Arnav Varma Dantuluri, Andrew Maguire,   \n338 Christoph Schuhmann, Huu Nguyen, and Alexander Julian Mattick. Openassistant conversations   \n339 - democratizing large language model alignment. In Thirty-seventh Conference on Neural   \n340 Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://   \n341 openreview.net/forum?id=VSJotgbPHF.   \n342 [14] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen  \n343 Tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for   \n344 data science code generation. ArXiv, abs/2211.11501, 2022.   \n345 [15] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris   \n346 Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better.   \n347 In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th   \n348 Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),   \n349 pages 8424\u20138445, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi:   \n350 10.18653/v1/2022.acl-long.577. URL https://aclanthology.org/2022.acl-long.577.   \n351 [16] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond,   \n352 Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy,   \n353 Cyprien de Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl,   \n354 Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,   \n355 Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level   \n356 code generation with alphacode. Science, 378(6624):1092\u20131097, 2022. doi: 10.1126/science.   \n357 abq1158. URL https://www.science.org/doi/abs/10.1126/science.abq1158.   \n358 [17] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence,   \n359 and Andy Zeng. Code as policies: Language model programs for embodied control. In arXiv   \n360 preprint arXiv:2209.07753, 2022.   \n361 [18] Bo Liu, Yuqian Jiang, et al. LLM $\\mathbf{\\nabla}+\\mathbf{P}$ : Empowering Large Language Models with Optimal   \n362 Planning Proficiency. arXiv:2304.11477, 2023.   \n363 [19] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated   \n364 by chatGPT really correct? rigorous evaluation of large language models for code generation.   \n365 In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:   \n366 //openreview.net/forum?id $\\cdot$ 1qvx610Cu7.   \n367 [20] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International   \n368 Conference on Learning Representations, 2019. URL https://openreview.net/forum?   \n369 id=Bkg6RiCqY7.   \n370 [21] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Noua  \n371 mane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian,   \n372 Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov,   \n373 Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo,   \n374 Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau\u00df, Naman Jain, Yixuan   \n375 Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang,   \n376 Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra,   \n377 Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu,   \n378 Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane   \n379 Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu\u00f1oz   \n380 Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and   \n381 Harm de Vries. Starcoder 2 and the stack v2: The next generation, 2024.   \n382 [22] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing   \n383 Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models   \n384 with evol-instruct. In The Twelfth International Conference on Learning Representations, 2024.   \n385 URL https://openreview.net/forum?id $\\cdot$ UnUwSIgK5W.   \n386 [23] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh   \n387 Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design   \n388 via coding large language models. arXiv preprint arXiv: Arxiv-2310.12931, 2023.   \n389 [24] Yecheng Jason Ma, William Liang, Hungju Wang, Sam Wang, Yuke Zhu, Linxi Fan, Osbert   \n390 Bastani, and Dinesh Jayaraman. Dreureka: Language model guided sim-to-real transfer. 2024.   \n391 [25] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,   \n392 Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslin  \n393 gual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.   \n394 [26] Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue   \n395 Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack:   \n396 Instruction tuning code large language models. In The Twelfth International Conference on   \n397 Learning Representations, 2024. URL https://openreview.net/forum?id $\\cdot$ mw1PWNSWZP.   \n398 [27] OpenAI. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/   \n399 chatgpt/, 2022.   \n400 [28] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren  \n401 cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red   \n402 Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham  \n403 mad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher   \n404 Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brock  \n405 man, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann,   \n406 Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis,   \n407 Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey   \n408 Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,   \n409 Thomas Degry, Noah Deutsch, Damien Deville, et al. Gpt-4 technical report, 2024.   \n410 [29] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language   \n411 model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.   \n412 [30] Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,   \n413 Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov,   \n414 Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan   \n415 Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas   \n416 Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for   \n417 code, 2024.   \n418 [31] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay,   \n419 Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task   \n420 plans using large language models. In 2023 IEEE International Conference on Robotics and   \n421 Automation (ICRA), pages 11523\u201311530, 2023. doi: 10.1109/ICRA48891.2023.10161317.   \n422 [32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy   \n423 Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.   \n424 https://github.com/tatsu-lab/stanford_alpaca, 2023.   \n425 [33] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,   \n426 Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson,   \n427 Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy   \n428 Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom   \n429 Hennigan, Benjamin Lee, Fabio Viola, et al. Gemini: A family of highly capable multimodal   \n430 models, 2024.   \n431 [34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo  \n432 th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,   \n433 Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation   \n434 language models, 2023.   \n435 [35] Unslothai. Unsloth: Finetune llama 3, mistral & gemma llms $2{-}5\\mathrm{x}$ faster with 80 URL   \n436 https://github.com/unslothai/unsloth. Accessed: 2024-05-22.   \n437 [36] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi,   \n438 and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc  \n439 tions, 2022.   \n440 [37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,   \n441 Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language   \n442 models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,   \n443 Advances in Neural Information Processing Systems, 2022.   \n444 [38] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source   \n445 code is all you need, 2023.   \n446 [39] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette   \n447 Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance   \n448 with large language models. Autonomous Robots, 2023.   \n449 [40] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qing  \n450 wei Lin, and Daxin Jiang. WizardLM: Empowering large pre-trained language models to follow   \n451 complex instructions. In The Twelfth International Conference on Learning Representations,   \n452 2024. URL https://openreview.net/forum?id $\\equiv$ CfXh93NDgH.   \n453 [41] Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonza  \n454 lez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, Brian   \n455 Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie   \n456 Tan, Yuval Tassa, and Fei Xia. Language to rewards for robotic skill synthesis. Arxiv preprint   \n457 arXiv:2306.08647, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "459 A.1 Overview ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "460 In this appendix, we first present ablation experiments to investigate the percentage of invalid   \n461 programs generated by SELF-INSTRUCT and examine how the generation temperature in INSTALIGN   \n462 affects final performance. Next, we analyze and compare the datasets generated by ROBO-INSTRUCT   \n463 and SELF-INSTRUCT. Finally, we list the seed tasks used in ROBOEVALand the CoT prompt. ", "page_idx": 13}, {"type": "image", "img_path": "LnJ2EGKTXh/tmp/1c4406578e0df542fc7285a466e54cdadfa19381c1adcd13b72d2054d3888446.jpg", "img_caption": ["464 A.2 Ablation Exmperiments ", "Figure 5: Ablation Experiments "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "465 A.2.1 effectivenss of the simulator ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "466 We analyze the percentage of instruction-program pairs discarded by ROBOSIM at various maximum   \n467 resampling limits, as shown in Fig. 5. Initially, with the maximum resampling limit set to 0, disabling   \n468 the rejection-sampling method, approximately $51\\%$ of the programs generated by SELF-INSTRUCT   \n469 contain errors. As the limit increases, fewer programs are discarded. However, there is a diminishing   \n470 return; even with the maximum resampling limit set to 10, about $15\\%$ of the instructions still result in   \n471 invalid programs. ", "page_idx": 13}, {"type": "text", "text": "472 A.2.2 Instruction Alignment model temperature ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "473 We further investigate how varying LLM temperatures for generating $\\mathcal{T}_{\\mathrm{revised}}$ in INSTALIGN impact   \n474 the performance of the fine-tuned model. Fig. 5 shows the bar chart of the pass $@1$ score of the   \n475 models fine-tuned over datasets generated using different LLM temperatures in INSTALIGN. The   \n476 model performs the best when fine-tuned on the dataset generated using LLM temperature $T=0.3$ .   \n477 As the temperature increases, we observe a decrease in performance. ", "page_idx": 13}, {"type": "text", "text": "478 A.3 Analysis of the Generated Datasets ", "page_idx": 13}, {"type": "image", "img_path": "LnJ2EGKTXh/tmp/347df1d1d7c20ee42b3282801ec2040d96d8b1fbdcd91ba9ba241778a79ae9e2.jpg", "img_caption": ["(a) Token Length Distribution for SELF-INSTRUCT(b) Cosine Similarity with ROBOEVALfor SELFvs.ROBO-INSTRUCT INSTRUCT vs.ROBO-INSTRUCT "], "img_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "LnJ2EGKTXh/tmp/58b3b8700034e960ece59728785e7c429d8b082b15a0b4e569ef2fbc8676d41a.jpg", "table_caption": ["Figure 6: Dataset Analysis "], "table_footnote": ["Table 3: Dataset Statistics "], "page_idx": 14}, {"type": "text", "text": "479 We first compute and plot the distribution of token lengths in the SELF-INSTRUCT-generated dataset   \n480 and the ROBO-INSTRUCT-generated dataset, as shown in Fig. 6(a). Next, we measure the cosine   \n481 similarity between each dataset and the ROBOEVALbenchmark tasks following the approach in   \n482 Magicoder [38], as depicted in Fig. 6(b). Finally, Tab. 3 presents the n-gram diversity score of each   \n483 dataset, along with the number of synthesized locations and objects. Our findings indicate that both   \n484 distributions and dataset statistics are very similar, suggesting that ROBO-INSTRUCT enhances the   \n485 quality of the generated data over SELF-INSTRUCT rather than merely aligning the dataset towards   \n486 the benchmark tasks. ", "page_idx": 14}, {"type": "text", "text": "487 A.4 ROBOEVALSeed Task Example ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "4881 # Instruction: Go to Arjun \u2019s office ,   \n4892 # ask him if he is ready to head out   \n4903 # and come back and tell me what he said   \n4914 def task_program ():   \n4925 start_loc $=$ get_current_location ()   \n4936 go_to(\"Arjun \u2019s office\")   \n4947 response $=$ ask(\"Arjun\",   \n4958 \"Are you ready t o go?\",   \n4969 [\"Yes\", \"No\"])   \n49170 go_to(start_loc)   \n49181 say(\"Arjun said: + response)   \nListing 1: Seed Task Example 1   \n4991 # Instruction: Ask Alice if she needs 1, 2, or 3 boxes.   \n5002 # Go to the storage room and ask if they have that many boxes.   \n5013 # If so , go place the boxes in Alice \u2019s office.   \n5024 # Otherwise , tell Alice you could not get the boxes.   \n5035 def task_program ():   \n5046 go_to(\"Alice \u2019s office\")   \n5057 num_boxes $=$ ask(\"Alice\"   \n5068 \"How many boxes do you need?\",   \n5079 [\"1\", \"2\", \"3\"])   \n50180 go_to(\"storage room\")   \n50191 response = ask(\"\",   \n51102 \"Do you have\" + num_boxes + boxes?\",   \n51113 [\"Yes\", \"No\"])   \n51124   \n51135   \n51146   \n51157   \n51168   \n51179   \n51280   \n51291   \n52202 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "if response $\\c=~\\cdots$ Yes\": for _ in range(int(num_boxes)): pick(\"box\") go_to(\"Alice \u2019s office\") place(\"box\") go_to(\"storage room\")   \nelse: go_to(\"Alice \u2019s office\") say(\"I could not get the boxes\") ", "page_idx": 15}, {"type": "text", "text": "211 # Instruction: Check if there is a red marker in the main   \n222 # office , and if so , tell Eve that there is a marker there.   \n233 # If not , go to the supply room and   \n244 # bring a red marker to the main office.   \n255 def task_program ():   \n5266 go_to(\"main office\")   \n5277 red_marker_found $=$ is_in_room(\"red marker\")   \n5288 if red_marker_found :   \n299 go_to(\"Eve\u2019s office\")   \n3100 say(\"There is a red marker in the main office\")   \n53111 else:   \n53122 go_to(\"supply room\")   \n3133 pick(\"red marker\")   \n53144 go_to(\"main office\")   \n3155 place(\"red marker\") ", "page_idx": 15}, {"type": "text", "text": "Listing 3: Seed Task Example 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "361 # Instruction: Check every classroom if there is a whiteboard.   \n372 # Go to Aiden \u2019s office to tell him which room does not   \n383 # have a whiteboard. Come back and tell me task is completed.   \n394 def task_program ():   \n405 start_loc $=$ get_current_location ()   \n416 list_of_rooms $=$ get_all_rooms ()   \n427 room_without_whiteboard $=$ []   \n438 for room in list_of_rooms :   \n449 if \"classroom\" not in room:   \n4150 continue   \n4161 go_to(room)   \n4172 if not is_in_room(\"whiteboard\"):   \n4183 room_without_whiteboard .append(room)   \n4194 go_to(\"Aiden \u2019s office\")   \n5105 if len( room_without_whiteboard ) > 0:   \n5116 message $=$ \"\"   \n5127 for room in room_without_whiteboard :   \n5138 message $+=$ room +   \n5149 message $+=$ \"do not have a whiteboard\"   \n5250 else:   \n5261 message $=$ \"all classrooms have a whiteboard\"   \n5272 say(message)   \n5283 go_to(start_loc)   \n5294 say(\"task is completed\") ", "page_idx": 15}, {"type": "text", "text": "Listing 4: Seed Task Example 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "5601 # Instruction: Go to the kitchen and wait for someone   \n5612 # to show up. When someone shows up , ask them to open   \n5623 # the fridge , then pick up a diet coke.   \n5634 # Finally , put the diet coke in the living room.   \n5645 def task_program ():   \n5656 go_to(\"kitchen\")   \n5667 while True:   \n5678 if is_in_room(\"person\"):   \n5689 response = ask(\"\",   \n56190 \"Please open the fridge\",   \n57101 [\"Yes\", \"No\"])   \n57112 if response $==$ \"Yes\":   \n57123 pick(\"diet coke\")   \n57134 break   \n57145 time.sleep (1)   \n57156 go_to(\"living room\")   \n57167 place(\"diet coke\")   \n5771 # Instruction: Take a bed sheet from the laundry room   \n5782 # and put it in each of the bedrooms.   \n5793 def task_program ():   \n5804 start_loc $=$ get_current_location ()   \n5815 list_of_rooms $=$ get_all_rooms ()   \n5826 for room in list_of_rooms :   \n5837 if \"bedroom\" not in room:   \n5848 continue   \n5859 go_to(\"laundry room\")   \n58160 pick(\"bed sheet\")   \n58171 go_to(room)   \n58182 place(\"bed sheet\")   \n58193 go_to(start_loc) ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Listing 6: Seed Task Example 6 ", "page_idx": 16}, {"type": "text", "text": "590 A.5 Prompts to Generate Synthetic Dataset Using SELF-INSTRUCT ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "LnJ2EGKTXh/tmp/452a35a0fec1276377d145fe1de54095f0191b5cc2c43d7cd29f56029c2131b4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 4: Prompts to Generate Synthetic Dataset Using SELF-INSTRUCT. ", "page_idx": 16}, {"type": "text", "text": "591 A.6 CoT Prompts for INSTALIGN ### Role: You are an expert at understanding robot programs. You will be given a task instruction and robot program pair. However, the instruction may not align with the program well. You need to correct the task instruction to match the given robot program. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "### Context: The robot only has access to the following 8 APIs and standard Python functions   \n- def get_current_location() $\\mathsf{\\Pi}_{\\mathsf{-}}>\\mathsf{s t r}$ :   \n- def get_all_rooms() $->$ list[str]:   \n- def is_in_room(object : str) $->$ bool:   \n- def go_to(location : str) $->$ None:   \n- ask(person $:$ str, question $:$ str, options: list[str]) $->$ str:   \n- say(message : str) $->$ None:   \n- def pick(obj: str) $->$ None:   \n- def place(obj: str) $->$ None: ", "page_idx": 17}, {"type": "text", "text": "### Inputs Original Instruction: This is a task instruction that may not align with the robot program Robot Program: This is a python function starting with \u2018def task_program():\u2018 ", "page_idx": 17}, {"type": "text", "text": "### Task:   \n1. Write down all the provided APIs used in the program and explain the effect of each API in this program   \n2. Examine these APIs and write down step by step what the program does   \n3. Combine all the results above and rewrite the instruction under # Final Corrected Instruction: You need to be specific and clear in your final corrected instruction. ", "page_idx": 17}, {"type": "text", "text": "Table 5: CoT Prompts for INSTALIGN. ", "page_idx": 17}, {"type": "text", "text": "592 B CheckList ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "593 1. [Claims] Yes. The research questions listed in the evaluation section are formulated so as to   \n594 directly reflect the claims of the paper.   \n595 2. [Limitations] Yes. This is discussed in Section 5.   \n596 3. [Theory, Assumptions and Proofs] N/A. We do not have any theoretical results.   \n597 4. [Experimental Result Reproducibility] Yes. We provide the training hyperparameters in   \n598 Section 4. We will also release our model upon acceptance.   \n599 5. [Open Access to Data and Code] Yes. We provide the prompts that are used to generate   \n600 the training dataset. We will also release our training dataset upon acceptance.   \n601 6. [Experimental Setting/ Details] Yes. We discuss the details of the training scheme in   \n602 Section 3.2, which follows the standard approach to fine-tuning an LLM.   \n603 7. [Experiment Statistical Significance] Yes. We performed ablation studies to validate our   \n604 methods in Section 3.3.   \n605 8. [Experiments Compute Resource] Yes. We mention that we train all our models on a   \n606 single H-100 GPU using unsloth in Section 3.2.   \n607 9. [Code Of Ethics] Yes   \n608 10. [Broader Impacts] N/A: This paper addresses an existing problem (using LLMs to synthe  \n609 size robot programs [10, 12, 17]), and does not introduce any novel concerns beyond the   \n610 existing scope.   \n611 11. [Safeguards] N/A: the programs we will generate or release are domain-specific with   \n612 respect to RoboEval [10], which has existing safeguards in place.   \n613 12. [Licenses] Yes \u2014 we build on SELF-INSTRUCT, Llamav3 [30], and RoboEval [10] with   \n614 attribution, and Table 1 refers to the licenses of the models used in the evaluation.   \n615 13. [Assets] N/A. The code we will release will include details of documentation, training,   \n616 license, and limitations. The code will be released upon acceptance.   \n617 14. [Crowdsourcing and Research with Human Subjects] N/A   \n618 15. [IRB Approvals] N/A ", "page_idx": 18}]