[{"figure_path": "iF7MnXnxRw/figures/figures_6_1.jpg", "caption": "Figure 1: Comparison of linear attention and softmax attention on two MQAR tasks {(L = 256, KV-pairs = 16), (L = 512, KV-pairs = 64)}, fixed model size d = 512, and varying state expansion n. We report the best result from a learning rate sweep in np.logspace(-4, -2, 4).", "description": "This figure compares the performance of linear and softmax attention mechanisms on two different MQAR tasks with varying sequence lengths and numbers of key-value pairs.  The model size is fixed, but the state expansion (n) is varied.  The best results from a learning rate sweep are reported, showing how linear attention's performance approaches that of softmax attention as state expansion increases.", "section": "4.1 Softmax Attention vs. Separable Attention"}, {"figure_path": "iF7MnXnxRw/figures/figures_7_1.jpg", "caption": "Figure 2: Model accuracy with increasing model size d for different models: softmax, linear, and normalized attention, S6, and SSD. The MQAR task is (L = 512, KV-pairs = 64), we fix n = 128, and report the best performance of a learning rate sweep in np.logspace(-4, -2, 4).", "description": "The figure compares the performance of different attention mechanisms (softmax, linear, normalized) and state space models (S6, SSD) on the MQAR benchmark task with sequence length L=512 and KV-pairs=64.  The model size 'd' is varied, and the best accuracy achieved during a learning rate sweep (using np.logspace(-4, -2, 4)) is reported for each model and size, with a fixed state expansion 'n' of 128.", "section": "4 Architecture Comparison: Theoretical and Experimental Results"}, {"figure_path": "iF7MnXnxRw/figures/figures_8_1.jpg", "caption": "Figure 3: Comparison of qLSTM (8) and a qLSTM variant where the original state transition A\u1d62 is replaced by (23).", "description": "This figure compares the performance of a standard qLSTM model with a modified version where the state transition matrix A\u1d62 is replaced with a reversed sigmoid function (equation 23 from the paper).  The comparison is done across three different MQAR tasks, each varying in sequence length and the number of key-value pairs, and across multiple model sizes (d). The orange lines show the performance of the modified qLSTM (using equation 23), while the blue lines show the performance of the standard qLSTM. The results demonstrate the improvement achieved by using the modified state transition, particularly noticeable in the tasks with longer sequences.", "section": "4 Architecture Comparison: Theoretical and Experimental Results"}, {"figure_path": "iF7MnXnxRw/figures/figures_14_1.jpg", "caption": "Figure 4: Visual representation of the dimensions considered in the DSF.", "description": "This figure visually represents the matrix dimensions in the Dynamical Systems Framework (DSF) described by the recurrence equation:  hi = Aihi-1 + Biui.  It highlights the hidden state (hi) with dimension N, the diagonal state transition matrix (Ai) with dimension N x N, and the input matrix (Bi) with dimension N x d.  The diagram also illustrates the concept of 'state expansion,' where the input dimension d is expanded by a factor of n, resulting in a hidden state with dimension N = nd.", "section": "3.2 Architecture Reformulation"}, {"figure_path": "iF7MnXnxRw/figures/figures_23_1.jpg", "caption": "Figure 1: Comparison of linear attention and softmax attention on two MQAR tasks {(L = 256, KV-pairs = 16), (L = 512, KV-pairs = 64)}, fixed model size d = 512, and varying state expansion n. We report the best result from a learning rate sweep in np.logspace(-4, -2, 4).", "description": "This figure compares the performance of linear and softmax attention on two different MQAR tasks with varying sequence lengths and numbers of key-value pairs.  The model size (d) is kept constant at 512, while the state expansion (n) is varied. The results show that as the state expansion increases, the accuracy of linear attention approaches that of softmax attention.", "section": "4 Architecture Comparison: Theoretical and Experimental Results"}, {"figure_path": "iF7MnXnxRw/figures/figures_23_2.jpg", "caption": "Figure 3: Comparison of qLSTM (8) and a qLSTM variant where the original state transition A\u017c is replaced by (23).", "description": "This figure compares the performance of a standard quasi LSTM (qLSTM) against a modified version where the state transition matrix is replaced by a reversed sigmoid of the projected input, inspired by the S6 model.  The results are shown for three different MQAR tasks with varying sequence lengths and numbers of key-value pairs, and for different model sizes (d). The shaded areas represent the standard deviation across multiple runs with different random seeds.", "section": "4 Architecture Comparison: Theoretical and Experimental Results"}, {"figure_path": "iF7MnXnxRw/figures/figures_24_1.jpg", "caption": "Figure 7: Results for softmax attention [2], linear attention [14], normalized attention (22), S6 [7], and SSD [8] on four different, progressively harder MQAR tasks {L = 64, KV-pairs = 4}, {L = 128, KV-pairs = 8}, {L = 256, KV-pairs = 16}, and {L = 512, KV-pairs = 64}. We sweep the model size d = [64, 128, 256, 512] and the state expansion n = [32, 64, 128, 256] for each model and task. We only report the best performance from a learning rate sweep in np.logspace(-4, -2, 4) measured as accuracy on the MQAR task. The accuracy is denoted in % in the grid in the figure.", "description": "This figure shows the accuracy of different models (softmax attention, linear attention, normalized attention, S6, and SSD) on four different MQAR tasks with varying sequence lengths and key-value pairs.  The model size (d) and state expansion (n) are also varied. The heatmap shows the accuracy for each combination of model, task, d, and n, highlighting the impact of these parameters on performance. Note that only the best accuracy from a learning rate sweep is reported for each configuration.", "section": "4 Architecture Comparison: Theoretical and Experimental Results"}]