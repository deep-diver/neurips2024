[{"figure_path": "83vxe8alV4/figures/figures_1_1.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "This figure compares the posterior predictive distribution of a Bayesian neural network (BNN) trained using the proposed FSP-LAPLACE method with different choices of Gaussian process (GP) priors. It shows how the method allows for efficient approximate Bayesian inference under interpretable function space priors, enabling the encoding of functional properties such as smoothness, length-scale, and periodicity through the GP prior. The plots also compare FSP-LAPLACE with other approximate inference methods and models, highlighting its effectiveness and ability to capture functional properties.", "section": "1 Introduction"}, {"figure_path": "83vxe8alV4/figures/figures_5_1.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "This figure demonstrates the effectiveness of FSP-LAPLACE in performing Bayesian inference using interpretable function-space priors.  It shows posterior predictive distributions obtained using different GP kernels (Matern 1/2, Matern 3/2, Matern 5/2, RBF, and Periodic), highlighting how FSP-LAPLACE successfully incorporates prior knowledge encoded in the chosen GP prior.  The figure also compares FSP-LAPLACE's performance with other approximate inference methods (Laplace, Full Variational Inference (FVI), Maximum a Posteriori (MAP), GP, and Sparse GP), showing that FSP-LAPLACE provides competitive performance while allowing for efficient and interpretable BNN inference.", "section": "1 Introduction"}, {"figure_path": "83vxe8alV4/figures/figures_7_1.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "This figure shows the posterior predictive distribution of a Bayesian neural network (BNN) trained using the proposed FSP-LAPLACE method with different Gaussian process (GP) priors.  The top row demonstrates how different GP kernels (Matern 1/2, Matern 3/2, Matern 5/2, RBF, and Periodic) shape the posterior predictive.  The bottom row compares the FSP-LAPLACE method to related methods like Full Variational Inference (FVI), Maximum A Posteriori (MAP) estimation, GP regression, and Sparse GP regression.  The gray points represent noisy observations of a periodic function, highlighting how the different priors affect uncertainty estimation and prediction accuracy.", "section": "1 Introduction"}, {"figure_path": "83vxe8alV4/figures/figures_8_1.jpg", "caption": "Figure 3: Results using our method (FSP-LAPLACE) as a surrogate model for Bayesian optimization. We find that FSP-LAPLACE performs particularly well on lower-dimensional problems, where it converges more quickly and to higher rewards than the Laplace, obtaining comparable scores as the Gaussian process (GP).", "description": "This figure compares the performance of FSP-LAPLACE against other Bayesian Optimization methods (FVI, Laplace, and GP) on six different test functions with varying dimensionality (2 to 10). The x-axis represents the number of function evaluations, while the y-axis shows the maximum reward achieved.  FSP-LAPLACE demonstrates competitive or superior performance, especially in lower-dimensional settings, converging faster and achieving higher maximum rewards than Laplace, while matching GP's performance in several cases.", "section": "Experiments"}, {"figure_path": "83vxe8alV4/figures/figures_19_1.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "This figure compares the posterior predictive of a Bayesian neural network (BNN) trained using the proposed FSP-LAPLACE method with several other approximate inference methods and models.  It highlights the ability of FSP-LAPLACE to incorporate interpretable function space priors in the form of a Gaussian process (GP) prior, demonstrating how different GP priors influence the resulting posterior distribution.  The plots illustrate that FSP-LAPLACE can capture various functional properties, such as smoothness and periodicity, directly in function space, leading to improved uncertainty estimates. The gray data points represent noisy observations of a periodic function. ", "section": "1 Introduction"}, {"figure_path": "83vxe8alV4/figures/figures_19_2.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "This figure shows the posterior predictive distribution of a Bayesian neural network (BNN) trained using the proposed FSP-LAPLACE method with different Gaussian process (GP) priors.  It demonstrates the ability of FSP-LAPLACE to incorporate prior knowledge about the function (such as smoothness or periodicity) into the BNN's predictions, resulting in more accurate and interpretable uncertainty estimates. The gray points represent noisy observations of the underlying function, while the green lines show samples from the posterior predictive distribution, and the red line is the mean prediction.", "section": "1 Introduction"}, {"figure_path": "83vxe8alV4/figures/figures_19_3.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "This figure shows the posterior predictive of a Bayesian neural network trained with FSP-LAPLACE under different choices of GP priors (RBF, periodic and Matern). The plots demonstrate how FSP-LAPLACE can incorporate prior knowledge about the function to be learned (e.g., smoothness, periodicity, lengthscale) directly into the model, leading to more accurate and informative uncertainty estimates. This is in contrast to traditional methods which often rely on less informative isotropic Gaussian priors in weight space.", "section": "1 Introduction"}, {"figure_path": "83vxe8alV4/figures/figures_19_4.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "The figure shows the results of applying the proposed FSP-LAPLACE method to a Bayesian neural network (BNN) with different Gaussian process (GP) priors.  The top row illustrates how different GP priors (e.g., Matern kernels, RBF, periodic) influence the posterior predictive distribution of the BNN. The bottom row compares the FSP-LAPLACE results to other approximate inference methods such as Full Variational Inference (FVI), standard Laplace approximation, and GP/Sparse GP methods. The plots visualize how FSP-LAPLACE efficiently incorporates prior knowledge (encoded in the GP prior) to achieve accurate and well-calibrated uncertainty estimates.", "section": "1 Introduction"}, {"figure_path": "83vxe8alV4/figures/figures_20_1.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "This figure shows the posterior predictive distribution of a Bayesian neural network (BNN) trained with the proposed FSP-LAPLACE method, compared to several related approximate inference methods. The top row displays the posterior predictive of the BNN under various choices of GP prior, demonstrating the flexibility of the method in encoding functional properties like smoothness and periodicity. The bottom row compares the performance of FSP-LAPLACE to other methods such as full variational inference, Laplace approximation, and standard Gaussian processes, illustrating the competitive performance of FSP-LAPLACE.", "section": "1 Introduction"}, {"figure_path": "83vxe8alV4/figures/figures_20_2.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "This figure compares the performance of FSP-LAPLACE with other Bayesian neural network inference methods under different choices of GP priors.  It shows posterior predictive distributions of a BNN for various choices of GP priors (different kernels), demonstrating the method's ability to encode functional properties into the model.  The plots show FSP-LAPLACE achieves competitive performance compared to other state-of-the-art methods.", "section": "1 Introduction"}, {"figure_path": "83vxe8alV4/figures/figures_20_3.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "This figure shows the posterior predictive distribution of a Bayesian neural network (BNN) trained with the proposed FSP-LAPLACE method, which uses function-space priors.  The top row displays the predictive distribution under different choices of GP priors (kernels): Matern 1/2, Matern 3/2, Matern 5/2, RBF, and Periodic. The bottom row compares FSP-LAPLACE to other approximate inference methods: Laplace approximation, full variational inference (FVI), maximum a posteriori (MAP) estimation, a full Gaussian process (GP), and a sparse Gaussian process (sparse GP). Gray points are noisy observations, and the green lines represent samples from the posterior predictive distribution.", "section": "1 Introduction"}, {"figure_path": "83vxe8alV4/figures/figures_21_1.jpg", "caption": "Figure C.5: FSP-LAPLACE with a Matern-1/2 covariance function against baselines in the two-moons classification task. Similar to the Gaussian process (GP) and sparse GP, our method shows a rough decision boundary.", "description": "This figure compares the performance of FSP-LAPLACE against several baselines (FVI, Laplace, MAP, GP, Sparse GP) on the two-moons classification task.  The key difference is that FSP-LAPLACE uses a Matern-1/2 covariance function as a prior. The figure shows that FSP-LAPLACE produces a rough decision boundary, similar to that produced by the Gaussian Process and sparse Gaussian Process, both of which also use the Matern-1/2 kernel.  This indicates that FSP-LAPLACE successfully incorporates the prior information from the chosen kernel into the model's predictions.", "section": "Additional qualitative results"}, {"figure_path": "83vxe8alV4/figures/figures_21_2.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "This figure compares the posterior predictive distribution of a Bayesian neural network (BNN) using different methods, including the proposed FSP-LAPLACE method.  It highlights the ability of FSP-LAPLACE to incorporate prior knowledge about the underlying function (like smoothness or periodicity) using a Gaussian process prior, leading to improved results. The different plots show how various methods handle noisy observations of a periodic function, demonstrating FSP-LAPLACE's superiority in capturing functional properties.", "section": "1 Introduction"}, {"figure_path": "83vxe8alV4/figures/figures_22_1.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "This figure shows the posterior predictive of a Bayesian neural network (BNN) using FSP-LAPLACE with different Gaussian process (GP) priors.  It compares FSP-LAPLACE's performance to other approximate inference methods, such as Full Variational Inference (FVI) and Laplace approximation, highlighting FSP-LAPLACE's ability to incorporate structured prior knowledge about the function into the BNN inference process. The GP priors used demonstrate different functional properties (smoothness, periodicity, length scale) showing how FSP-LAPLACE allows for incorporating such prior knowledge.", "section": "1 Introduction"}, {"figure_path": "83vxe8alV4/figures/figures_22_2.jpg", "caption": "Figure 1: FSP-LAPLACE allows for efficient approximate Bayesian neural network (BNN) inference under interpretable function space priors. Using our method, it is possible to encode functional properties like smoothness, lengthscale, or periodicity through a Gaussian process (GP) prior. The gray data points in the plots are noisy observations of a periodic function.", "description": "This figure demonstrates the effectiveness of FSP-LAPLACE in performing approximate Bayesian inference for Bayesian neural networks.  It highlights the use of interpretable function-space priors, specifically Gaussian process priors, to encode various functional properties (smoothness, length scale, periodicity).  The figure showcases how the method handles noisy observations of a periodic function, illustrating its ability to incorporate prior knowledge effectively. ", "section": "1 Introduction"}]