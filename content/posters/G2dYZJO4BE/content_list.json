[{"type": "text", "text": "Achievable distributional robustness when the robust risk is only partially identified ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Julia Kostin Department of Computer Science ETH Zurich jkostin@ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Nicola Gnecco Fanny Yang Gatsby Computational Neuroscience Unit Department of Computer Science University College London ETH Zurich nicola.gnecco@gmail.com fan.yang@inf.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In safety-critical applications, machine learning models should generalize well under worst-case distribution shifts, that is, have a small robust risk. Invariancebased algorithms can provably take advantage of structural assumptions on the shifts when the training distributions are heterogeneous enough to identify the robust risk. However, in practice, such identifiability conditions are rarely satisfied \u2013 a scenario so far underexplored in the theoretical literature. In this paper, we aim to flil the gap and propose to study the more general setting of partially identifiable robustness. In particular, we define a new risk measure, the identifiable robust risk, and its corresponding (population) minimax quantity that is an algorithmindependent measure for the best achievable robustness under partial identifiability. We introduce these concepts broadly, and then study them within the framework of linear structural causal models for concreteness of the presentation. We use the introduced minimax quantity to show how previous approaches provably achieve suboptimal robustness in the partially identifiable case. We confirm our findings through empirical simulations and real-world experiments and demonstrate how the test error of existing robustness methods grows increasingly suboptimal as the proportion of previously unseen test directions increases. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The success of machine learning methods typically relies on the assumption that the training and test data follow the same distribution. However, this assumption is often violated in practice. For instance, this can happen if the test data are collected at a later time or using a different measuring device. Without further assumptions on the test distribution, generalization under distribution shift is impossible. However, practitioners often have partial information about the set of possible \"shifts\" that may occur during test time, inducing a set of feasible test distributions that the model should generalize to. We refer to the resulting set as the robustness set. With $\\mathcal{R}(\\beta;\\mathbb{P})$ denoting the population risk of a model parameterized by $\\beta$ for distribution $\\mathbb{P}$ , the robust risk can be written as ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{\\mathrm{rob}}(\\beta):=\\displaystyle\\operatorname*{sup}_{\\mathbb{P}\\in\\mathcal{P}_{\\mathrm{rob}}(\\boldsymbol{\\theta}^{\\star})}\\mathcal{R}(\\boldsymbol{\\beta};\\mathbb{P}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\mathcal{P}_{\\mathrm{rob}}(\\theta^{\\star})$ corresponds to the robustness set that we assume to be fully characterized by some true parameter $\\theta^{\\star}$ . In safety-critical applications, the goal is often to find a minimizer of the robust risk, i.e. a robust prediction model that shows the best performance on the worst-case distribution out of the robustness set. ", "page_idx": 0}, {"type": "table", "img_path": "G2dYZJO4BE/tmp/d9e5755dfd4f8d6dd67fc10d372fd712fd53b7fc932c06d53060eca897e2eac0.jpg", "table_caption": ["Table 1: Comparison of various distributional robustness frameworks and what kind of assumptions their analysis can account for (with an incomplete list of examples for each framework). "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "A number of subfields in machine learning and optimization have addressed this problem. For example, in distributionally robust optimization (DRO) [7, 16], the parameter $\\theta^{\\star}$ may be the training distribution $\\mathbb{P}$ and the robustness set the neighborhood of $\\mathbb{P}$ in some probability distance metric [30, 35, 22, 15]. Relatedly, adversarial robustness [23, 32] studies the risk on worst-case transformations of examples drawn from some distribution $\\mathbb{P}$ and can be seen as equivalent to distribution shift robustness [53]. DRO-type methods minimize the worst-case robustness against arbitrary distribution shifts in the neighborhood without structural assumptions. Although being assumption-agnostic can be viewed as a strength, it also has its caveat: even when available, prior knowledge about the structure of expected test shifts cannot be incorporated. In such cases, the robust model\u2019s prediction might be overly conservative, resulting in suboptimal performance when the test shifts are in fact more benign. [47]. ", "page_idx": 1}, {"type": "text", "text": "In many practical scenarios, data from heterogeneous sources is available at training time \u2013 for example, data from different geographic locations or time ranges. Due to the lack of modeling assumptions, multiple environments in the DRO setting cannot, in general, be leveraged to achieve better robustness in a given robustness set \u2013 in those contexts, the presence of multiple environments is usually argued to enable robustness against a larger robustness set. Instead, domain experts can anticipate which aspects of the joint probability distribution of $(X,Y)$ are more likely to shift. Such prior structural information can, for example, be incorporated through the framework of structural causal models (SCMs), via the approach of causality-oriented robustness [34, 11]. Importantly, the literature in this area has so far focused on settings when the desired robust objective $\\mathcal{R}_{\\mathrm{rob}}$ is identifiable, i.e. computable from training data. Traditional causal learning and invariancebased methods aim to fully identify some underlying causal parameter of the SCM for robustness against all (potentially infinite) interventions [38, 42, 6, 29]. However, the training data is often not heterogeneous enough to fully identify the causal parameter. Thus, another line of work [45, 50, 28] focuses on the scenario when the causal parameter is not necessarily identifiable, but the test shifts only occur in training directions, rendering the robust risk (1) identifiable. We provide an overview in Table 1 and an additional discussion of related work in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "In practice, causality- and invariance-based methods often result in wrong representations of the data [27, 44] and end up performing similarly to empirical risk minimization (ERM) that ignores the multi-environment information [3, 24, 43]. Many possible explanations for this observation have been proposed in the literature. In our work, we focus on the non-identifiability failure scenario. In particular, we extend the discussion of invariance-based methods to include the partially identifiable setting, where not only the causal parameter, but the robust risk (1) is not determinable using training data either1. Specifically, we aim to discuss the following question: ", "page_idx": 1}, {"type": "text", "text": "What is the optimal worst-case performance any model can have for given structural relationships between test and training data and how do existing methods comparatively perform in such settings? ", "page_idx": 1}, {"type": "text", "text": "When the robust risk is not identifiable from training data, we obtain a whole set of possible objectives that includes the true robust risk. In this case, we are interested in the best achievable robustness for any algorithm that we capture in a quantity called the identifiable robust risk: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathrm{rob,ID}}(\\beta):=\\operatorname*{sup}_{\\mathrm{\\tiny\\begin{array}{c}{\\mathrm{possible}}\\\\ {\\mathrm{true\\;model}\\;\\theta^{\\star}}\\end{array}}}\\operatorname*{sup}_{\\mathbb{P}\\in\\mathcal{P}_{\\mathrm{rob}}(\\theta^{\\star})}\\mathcal{R}(\\beta;\\mathbb{P}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that $\\mathcal{R}_{\\mathrm{rob,ID}}(\\beta)$ is well-defined even when the standard robust risk is not identifiable \u2013 it takes the supremum over the robust risks induced by all possible true model parameters $\\theta^{\\star}$ that are consistent with the given set of training data distributions. Furthermore, the minimal value of the identifiable robust risk corresponds to the optimal worst-case performance in the partially identifiable setting. Spiritually, this minimax population quantity is reminiscent of the algorithm-independent limits in classical statistical learning theory [58].2 Even though our partial identifiability framework can be evaluated for arbitrary modeling assumptions on the distribution shift (such as covariate/label shift, DRO, etc.), we present it in a concrete setting for clarity of the exposition. Specifically, we discuss linear structural causal models (SCMs) with unobserved confounding (cf. Section 2), similar to the setting of IV (instrumental variables) and anchor regression [45, 46]. ", "page_idx": 2}, {"type": "text", "text": "The identifiable robust risk (2) not only represents a notion of algorithm-independent optimality for any combination of training and test shifts. In the linear SCM setting in Section 2, we also show theoretically and empirically that the ranking and optimality of different robustness methods change drastically in identifiable vs. partially identifiable settings. The same can be observed in experiments on real-world data. Our experimental results strongly indicate that evaluation and benchmarking on partially identifiable settings are important for determining the effectiveness of robustness methods. Finally, while the identifiable robust predictor is only provably optimal for the linear SCM, experiments on real-world data in Section 3.3 suggest that our estimator may significantly improve upon other invariance-based methods in more realistic scenarios. ", "page_idx": 2}, {"type": "text", "text": "2 Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first introduce the linear causal model setting and describe our structural assumptions on the training and test distributions. Then, we introduce our framework for distributional robustness that allows for partial identifiability and define the identifiable robust risk, the worst-case robust risk among the possible robust risks induced by the training distributions. ", "page_idx": 2}, {"type": "text", "text": "2.1 Data distribution and a model of additive environmental shifts ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Data distribution. We are given multiple training environments indexed by $e\\,\\in\\,\\mathcal{E}_{\\mathrm{train}}$ , where $\\mathcal{E}_{\\mathrm{train}}$ is a countable environment index set. For each training environment $e$ , we observe data $(\\overrightharpoon{X_{e}},Y_{e})\\sim\\mathbb{P}_{e}^{X,Y}$ consisting of input covariates $X_{e}\\,\\in\\,\\mathbb{R}^{d}$ and the target variable $Y_{e}\\in\\mathbb{R}$ , which are generated by the linear structural causal model (SCM) (3) and its corresponding causal graph, depicted in Figure 1. Throughout the paper, we assume that we observe the collection of training distributions $\\bar{\\mathcal{P}}_{\\theta^{\\star},\\mathcal{E}_{\\mathrm{train}}}=\\{\\mathbb{P}_{\\theta^{\\star},e}^{\\bar{X},Y}\\}_{e\\in\\mathcal{E}_{\\mathrm{train}}}$ {P\u03b8X\u22c6,,Ye }e\u2208Etrain, omitting \u03b8\u22c6when it is clear from the context. We discuss the corresponding finite sample setting in Appendix D. ", "page_idx": 2}, {"type": "text", "text": "We assume that the true unobserved parameters $\\theta^{\\star}:=(\\beta^{\\star},\\Sigma^{\\star})\\in\\Theta$ , where $\\Theta\\subset\\mathbb{R}^{d+(d+1)(d+1)}$ , are invariant across environments. The joint covariance $\\Sigma^{\\star}$ of the noise $(\\eta,\\xi)$ can be written in block form as $\\Sigma^{\\star}=\\left(\\!\\!\\begin{array}{c c}{{\\Sigma_{\\eta}^{\\star}}}&{{\\Sigma_{\\eta,\\xi}^{\\star}}}\\\\ {{\\Sigma_{\\eta,\\xi}^{\\star}}}&{{(\\sigma_{\\xi}^{\\star})^{2}}}\\end{array}\\!\\!\\right)$ We allow the presence of latent confounders between $X_{e}$ and $Y_{e}$ , and hence the case where $\\bar{\\Sigma}_{\\eta,\\xi}^{\\star}\\neq0$ . Note that the confounded noise setting is, in general, more challenging than the independent noise setting, since, given any number of environments, common estimators such as the linear regression estimator are biased away from the causal parameter $\\beta^{\\star}$ . ", "page_idx": 2}, {"type": "text", "text": "Additive distribution shift. The distribution shift between environments is modeled by the (random) additive shift $A_{e}\\,\\in\\,\\mathbb{R}^{d}$ with mean $\\mathbb{E}\\left[A_{e}\\right]=\\mu_{e}$ and covariance matrix $\\mathrm{Cov}[A_{e}]\\,=\\,\\Sigma_{e}$ . In ", "page_idx": 2}, {"type": "image", "img_path": "G2dYZJO4BE/tmp/0e0ad17b0aa6e3746c7a0018942a2521fefa50c8c54b68c35920650ccea60021.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: (Left) Causal graph corresponding to the SCM in Equation (3). Observed variables $(X_{e},Y_{e})$ are indicated by solid circles while unobserved variables, namely the additive shift $A_{e}$ and confounders $H$ , are shown in dashed circles. Note that here, bidirectional edges indicate that the relationship between two nodes can be in either direction. ", "page_idx": 3}, {"type": "text", "text": "general, the environment shifts $A_{e}$ can be degenerate, i.e. the covariances $\\Sigma_{e}$ are not assumed to be full rank. For simplicity of presentation, we further assume that $\\mathcal{E}_{\\mathrm{train}}$ contains a reference environment $e=0$ satisfying $\\mu_{0}\\,=\\,0$ and $\\Sigma_{0}\\,=\\,0$ . In Appendix B, we discuss how our results apply if this condition is not met. Our additive shift structure implies that the joint distribution $\\bar{\\mathbb{P}_{e}^{X,\\dot{Y},A}}=\\mathbb{P}_{e}^{A}\\times\\mathbb{P}^{X,Y|A}$ changes in each environment. However, we do not allow for direct interventions on $Y$ or the latent confounders, that is $\\mathbb{P}^{X,Y|A}$ remains invariant. Note that our distribution shift setting is more general than covariate shift: due to unobserved confounding, the conditional distribution $\\mathbb{P}_{e}^{Y|X}$ also varies across environments. ", "page_idx": 3}, {"type": "text", "text": "In summary, our model (3) describes a multi-environment setting where different training distributions vary by changing the distribution $\\mathbb{P}_{e}^{A}$ of the random additive shifts $A_{e}$ , but the causal model parameters $\\theta^{\\star}$ remain invariant across all training and test environments. It can model a variety of multi-environment settings in the related literature. For instance, choosing $|\\mathcal{E}_{\\mathrm{train}}|\\;=\\;1$ and $A\\sim\\mathcal{N}(0,M\\Sigma_{A}M^{\\top})$ , where $\\boldsymbol{M}\\;\\in\\;\\mathbb{R}^{d\\times q},\\boldsymbol{\\Sigma}_{A}\\;\\in\\;\\mathbb{R}^{q\\times q}$ , yields the setup of continuous anchor regression $[45]^{3}$ . Discrete anchor regression [45] corresponds to a discrete environment index set $\\mathcal{E}_{\\mathrm{train}}=[m],m\\in\\mathbb{N},$ and deterministic mean shifts $A_{e}=\\dot{\\mu}_{e}\\in\\mathbb{R}^{d}$ . The more general additive shift setting in [49] corresponds to $\\ensuremath{\\mathcal{E}}_{\\mathrm{train}}=[m]$ and $\\boldsymbol{A_{e}}\\sim\\mathcal{N}(\\boldsymbol{\\mu_{e}},\\boldsymbol{\\Sigma_{e}})$ . Note that in the above works, the environment index is modeled as a random variable $E\\sim\\mathbb{P}^{E}$ through which one assigns weights to different training environments. The results in this paper only depend on the support of $\\mathbb{P}^{E}$ , that is, whether an environment was seen or not. Thus, the population-level guarantees \u2013 the focus of this paper \u2013 are the same for any distributions with the same support on $\\mathbb{P}^{\\tilde{E}}$ . ", "page_idx": 3}, {"type": "text", "text": "Structural assumptions on test distribution shift. During test time, we expect to observe data that follows a new, previously unseen distribution PtXe,sYt . The test data are generated by the same SCM (3), but with a new additive shift $A_{\\mathrm{test}}\\sim\\mathbb{P}_{\\mathrm{test}}^{A}$ with corresponding finite mean $\\mu_{\\mathrm{test}}$ and covariance $\\Sigma_{\\mathrm{test}}$ . dEivreecnt itohnosu agnh d wsiez deso  onfo tp oosbssiebrlev ed $\\mathbb{P}_{\\mathrm{test}}^{X,Y}$ utdiuornisn go ft rtahien isnhigf,t  wvaer idaob laes $\\mathbb{P}_{\\mathrm{test}}^{A}$ ,  tphaartt iiasl knowledge about the ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[A_{\\mathrm{test}}{A_{\\mathrm{test}}}^{\\top}\\right]=\\Sigma_{\\mathrm{test}}+\\mu_{\\mathrm{test}}\\mu_{\\mathrm{test}}{}^{\\top}\\preceq M_{\\mathrm{test}};\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $M_{\\mathrm{test}}\\succeq0$ . If the test distribution of $X$ is given (as in the domain adaptation setting), one can directly estimate the shift of the test environment and set $M_{\\mathrm{test}}:=\\mathbb{E}\\,[A_{\\mathrm{test}}A_{\\mathrm{test}}{}^{\\top}]^{4}$ . In the following, we consider the distributional robustness setting in which partial knowledge about test shifts is given in form of their maximum strength $\\gamma$ and general direction $\\mathcal{M}\\subseteq\\mathbb{R}^{d}$ . We can then formalize this partial knowledge by setting $M_{\\mathrm{test}}=\\gamma\\Pi_{\\mathcal{M}}$ , where $\\gamma>0$ and $\\Pi_{\\mathcal{M}}$ is an orthogonal projection onto the subspace $\\mathcal{M}$ .5 ", "page_idx": 3}, {"type": "text", "text": "2.2 Classical distributional robustness ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given the test shift directions $\\mathcal{M}$ and strength $\\gamma$ , our goal is to find an estimator using the training data that has a small risk over the entire set of shifted test distributions, called the robustness set, that ", "page_idx": 3}, {"type": "text", "text": "we define as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}_{\\theta^{\\star}}(\\gamma\\Pi_{\\mathcal{M}}):=\\{\\mathbb{P}_{\\theta^{\\star},\\mathrm{test}}^{X,Y}:\\mathbb{E}\\left[A_{\\mathrm{test}}A_{\\mathrm{test}}{}^{\\top}\\right]\\preceq\\gamma\\Pi_{\\mathcal{M}}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For a given robustness set $\\mathcal{P}_{\\theta^{\\star}}(\\gamma\\Pi_{\\mathcal{M}})$ , we define the robust risk ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta^{\\star},\\gamma\\Pi_{\\mathcal{M}}):=\\displaystyle\\operatorname*{sup}_{\\mathbb{P}\\in\\mathcal{P}_{\\theta^{\\star}}(\\gamma\\Pi_{\\mathcal{M}})}\\mathcal{R}(\\beta;\\mathbb{P}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the corresponding robust predictor $\\begin{array}{r}{\\beta^{r o b}:=\\arg\\operatorname*{min}_{\\beta\\in\\mathbb{R}^{d}}\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta^{\\star},\\gamma\\Pi_{\\mathcal{M}})}\\end{array}$ , where $\\mathscr{R}(\\beta;\\mathbb{P}):=$ $\\mathbb{E}_{\\mathbb{P}}[(Y-\\beta^{\\top}X)^{2}]$ denotes the risk w.r.t. to the squared loss. The robust risk and the robust predictor can be explicitly computed as a function of the true model parameters $\\theta^{\\star}=(\\beta^{\\star},\\Sigma^{\\star})$ and the test shift bound $\\gamma\\Pi_{\\mathcal{M}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\beta_{\\theta^{\\star}}^{r o b}=\\underset{\\beta\\in\\mathbb{R}^{d}}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\underset{\\mathbb{P}\\in\\mathcal{P}_{\\theta^{\\star}}(\\gamma\\Pi_{M})}{\\operatorname*{sup}}\\mathbb{E}_{\\mathbb{P}}[(Y-\\beta^{\\top}X)^{2}]=\\beta^{\\star}+(\\gamma\\Pi_{M}+\\Sigma_{\\eta}^{\\star})^{-1}\\Sigma_{\\eta,\\xi}^{\\star}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In practice, neither the model parameters $\\theta^{\\star}$ nor the robust risk $\\mathcal{R}_{\\mathrm{rob}}$ itself can generally be fully identified. Instead, in the next sections we show that the robust prediction model can usually only be set-identified, except for specific combinations of the training and test shifts. ", "page_idx": 4}, {"type": "text", "text": "2.3 Partially identifiable robustness framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we formally introduce the identifiable robust risk and related notions that allow us to characterize model robustness in the case when the robust predictor cannot be identified. We start with the notion of observational equivalence [17]: ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Observational equivalence). We call model parameters $\\theta_{1}\\,=\\,(\\beta_{1},\\Sigma_{1})$ and $\\theta_{2}\\,=$ $(\\beta_{2},\\Sigma_{2})$ observationally equivalent with respect to a set of shift distributions $\\{\\Dot{\\mathbb{P}}_{e}^{A}:e\\in\\mathcal{E}_{\\mathrm{train}}\\}^{6}\\ i f$ they induce the same set $\\mathcal{P}_{\\theta,\\mathcal{E}_{\\mathrm{train}}}$ of training distributions over the observed variables $(X_{e},Y_{e})$ as described in Section 2.1, i.e. ", "page_idx": 4}, {"type": "equation", "text": "$$\nF o r\\,a l l\\,e\\in\\mathcal{E}_{\\mathrm{train}}:\\mathbb{P}_{\\theta_{1},e}^{X,Y}=\\mathbb{P}_{\\theta_{2},e}^{X,Y}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By observing $\\mathcal{P}_{\\theta^{\\star},\\mathcal{E}_{\\mathrm{train}}}$ , we can identify the model parameters up to the observationally equivalent set defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{eq}}:=\\{\\theta=(\\beta,\\Sigma)\\in\\Theta:\\mathcal{P}_{\\theta,\\mathcal{E}_{\\mathrm{train}}}=\\mathcal{P}_{\\theta^{\\star},\\mathcal{E}_{\\mathrm{train}}}\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When the observationally equivalent set is not a singleton, prior work only considers scenarios where the robustness set (5) and hence also the robust prediction model are still identifiable (see Equation (7)). This scenario is shown in Figure 2a and discussed again in Section 3.2. In general, however, the observation of multiple training environments $\\mathcal{P}_{\\theta^{\\star},\\mathcal{E}_{\\mathrm{train}}}$ neither identifies the model parameters nor the robustness set or robust risk, which is the partially identified setting that we focus on. Instead, we can only compute a superset of the robustness set ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal P_{\\Theta_{\\mathrm{eq}}}(\\gamma\\Pi_{\\mathcal M}):=\\bigcup_{\\theta\\in\\Theta_{\\mathrm{eq}}}\\mathcal P_{\\theta}(\\gamma\\Pi_{\\mathcal M})\\supset\\mathcal P_{\\theta^{\\star}}(\\gamma\\Pi_{\\mathcal M})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and correspondingly, a set of robust risks $\\{\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta,\\gamma\\Pi_{\\mathcal{M}})\\,:\\,\\theta\\,\\in\\,\\Theta_{\\mathrm{eq}}\\}$ and robust predictors $B_{\\Theta_{\\mathrm{eq}}}^{r o b}:=\\{\\beta_{\\theta}^{r o b}:\\,\\theta\\in\\Theta_{\\mathrm{eq}}\\}$ . In this case, we would still like to achieve the \u201cbest-possible\u201d robustness, which is intuitively test shift robustness for the \u201chardest-possible\u201d parameters that could have induced the observed training distributions. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Identifiable robust risk and the minimax quantity). Consider the data model (3). The identifiable robust risk is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{\\mathrm{rob,ID}}(\\beta;\\Theta_{\\mathrm{eq}},\\gamma\\Pi_{\\mathcal{M}}):=\\displaystyle\\operatorname*{sup}_{\\theta\\in\\Theta_{\\mathrm{eq}}}\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta,\\gamma\\Pi_{\\mathcal{M}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We will denote its minimizer as $\\beta^{\\mathrm{rob,ID}}$ and refer to it as the identifiable robust predictor. The optimal robustness on test shifts bounded by $\\gamma\\Pi_{\\mathcal{M}}$ given training data $\\mathcal{P}_{\\theta^{\\star},\\mathcal{E}_{\\mathrm{train}}}$ is described by the minimax quantity ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{M}(\\Theta_{\\mathrm{eq}},\\gamma\\Pi_{\\mathcal{M}})=\\displaystyle\\operatorname*{inf}_{\\beta\\in\\mathbb{R}^{d}}\\mathcal{R}_{\\mathrm{rob,ID}}(\\beta;\\Theta_{\\mathrm{eq}},\\gamma\\Pi_{\\mathcal{M}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "G2dYZJO4BE/tmp/cfc7cc2ef63843ae40fd91f15e4979876c06f72954960a91d7afc3b7dc602961.jpg", "img_caption": ["Figure 2: Relationship between identifiability of the model parameters and identifiability of the robust risk. (a) The classical scenario where the test shift directions $\\mathcal{M}_{\\mathrm{seen}}$ are contained in the span of training shifts so that the robust risk and thus its minimizer are point-identified. (b) The more general scenario of this paper, where the shift directions during test time $\\mathcal{M}_{\\mathrm{gen}}$ can contain new shift directions and the robust risk can only be set-identified. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "The definition of the identifiable robust risk reflects the absence of knowledge of the model parameters in test shift directions that were not observed during training. In words, the most robust parameter choice $\\beta^{\\mathrm{rob,ID}}$ is the one that minimizes the worst-case robust risk for parameters in the observationally equivalent set . In the next sections, we compute these quantities explicitly for the setting of Section 2. This will allow us to compare the best achievable robustness in the partially identified case with the guarantees of prior methods in this setting. ", "page_idx": 5}, {"type": "text", "text": "3 Main results for partially identified robustness ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now compute the identifiable robust risk (8) and derive a lower bound for the minimax quantity (9) in the additive shift setting. We then compare the identifiable robust risk of existing robustness methods and ordinary least squares (OLS) with the minimizer of the identifiable robust risk both theoretically and empirically. ", "page_idx": 5}, {"type": "text", "text": "3.1 Minimax robustness results for the SCM ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The degree to which the model parameters $\\theta^{\\star}$ in the linear SCM setting (3) can be identified depends on the number of environments and the total rank of the additive shifts. This is well-studied, for instance, in the instrumental variable (IV) regression literature [4, 9]. In particular, in the setting of Section 2.1, the causal parameter $\\beta^{\\star}$ can only be identified along the mean and variance shifts of the covariates across the training data. Therefore, if not enough shift directions are observed, it is merely set-identifiable. In the following, we show how set-identifiability of the model parameters translates into set-identifiability of the robust prediction model (7). More formally, we denote by $\\boldsymbol{S}$ the subspace consisting of all additive shift directions seen during training: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS:=\\mathrm{range}\\left[\\sum_{e\\in\\mathcal{E}_{\\mathrm{train}}}\\left(\\Sigma_{e}+\\mu_{e}\\mu_{e}^{\\top}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The definition of the space $\\boldsymbol{S}$ induces the following orthogonal decompositions of the causal parameter and test shift directions $\\mathcal{M}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta^{\\star}=\\beta^{S}+\\beta^{S^{\\perp}},\\quad\\mathrm{and}\\quad\\Pi_{\\mathcal{M}}\\preceq S S^{\\top}+R R^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $S$ and $R$ are matrices with orthonormal columns such that range $S\\subset S$ , range $R\\subset S^{\\perp}$ and range $S$ , range $R$ are the smallest subspaces satisfying Equation $(11)^{7}$ . The matrix $S$ corresponds to test shift directions along the model can be identified. Conversely, $R$ corresponds to test shift directions, along which the model is non-identified. The vector $\\beta^{S}$ is the identifiable part of the causal parameter. It uniquely defines a set of identified model parameters that reads ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{S}:=(\\beta^{S},\\Sigma_{\\eta}^{S},\\Sigma_{\\eta,\\xi}^{S},(\\sigma_{\\xi}^{S})^{2})=(\\beta^{S},\\Sigma_{\\eta}^{\\star},\\Sigma_{\\eta,\\xi}^{\\star}+\\Sigma_{\\eta}^{\\star}\\beta^{S^{\\perp}},(\\sigma_{\\xi}^{\\star})^{2}+2\\langle\\Sigma_{\\eta,\\xi}^{\\star},\\beta^{S^{\\perp}}\\rangle+\\langle\\beta^{S^{\\perp}},\\Sigma_{\\eta}^{\\star}\\beta^{S^{\\perp}}\\rangle)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and can be computed from the training distributions. In the next proposition, we show that the model parameters and robust predictor can be identified up to a set around $\\theta^{S}$ , which can be interpreted as the set\u2019s geometric center. From the characterization of this set, it directly follows that the robust predictor is only identifiable if the test shifts are in the direction of the training shifts, i.e. ${\\mathcal{M}}\\subset{\\mathcal{S}}$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 1 (Identifiability of model parameters and robust predictor). Suppose that the set of training and test distributions is generated according to Section 2.1, with some model parameter $\\theta\\in\\Theta$ . Then, it holds that ", "page_idx": 6}, {"type": "text", "text": "(a) the model parameters generating the training distribution (3) can be identified up to the following observationally equivalent set : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta_{\\mathrm{eq}}=\\Theta\\cap\\{\\beta^{S}+\\alpha,\\Sigma_{\\eta}^{\\star},\\Sigma_{\\eta,\\xi}^{S}-\\Sigma_{\\eta}^{\\star}\\alpha,(\\sigma_{\\xi}^{S})^{2}-2\\alpha^{\\top}\\Sigma_{\\eta,\\xi}^{S}+\\alpha^{\\top}\\Sigma_{\\eta}\\alpha\\colon\\alpha\\in\\mathcal{S}^{\\perp}\\}\\ni\\theta^{\\star};}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$(b)$ the robust predictor $\\beta^{r o b}$ as defined in Equation (7) is identified up to the set ", "page_idx": 6}, {"type": "equation", "text": "$$\nB_{\\Theta}^{r o b}\\cap\\{\\beta^{S}+(\\gamma\\Pi_{M}+\\Sigma_{\\eta}^{\\star})^{-1}\\Sigma_{\\eta,\\xi}^{S}+(\\gamma\\Pi_{M}+\\Sigma_{\\eta}^{\\star})^{-1}\\alpha\\colon\\alpha\\in\\mathrm{range}\\;R\\}\\ni\\beta^{r o b}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof of Proposition 1 is provided in Appendix F.1. Proposition 1 implies two well-known settings: If we observe a rich enough set $\\mathcal{P}_{\\mathcal{E}_{\\mathrm{train}}}$ of training environments such that $\\mathcal{S}=\\mathbb{R}^{d}$ , the model parameters are uniquely identified, corresponding to the setting of full-rank instruments [4]. However, even in the under-identified case $\\mathcal{S}\\ne\\mathbb{R}^{\\stackrel{\\lambda}{d}}$ , if the test shift directions $\\mathcal{M}$ are contained in the space $\\boldsymbol{S}$ of training-time shifts, i.e. $R=0$ , the robust prediction model is identifiable from training data regardless of the identifiability of the model parameters. This is the setting considered e.g. in anchor regression [45] and discussed again in Section 3.2 and Appendix C. ", "page_idx": 6}, {"type": "text", "text": "So far, we have described how the identifiability of the robust prediction model depends on the structure of both the training environments (via the space $\\boldsymbol{S}$ ) and the test environments (via $\\mathcal{M}$ ). We now aim to compute the smallest achievable robust loss for the general partially identifiable setting, which allows for $R\\neq0$ . In particular, we provide a lower bound on the best-possible achievable distributional robustness formalized by the minimax quantity (9). First observe that without further assumptions on the parameter space $\\Theta$ , the observationally equivalent set is unbounded, and the identifiable robust risk (8) can be infinite. The following assumption allows us to provide a finegrained analysis of robustness in a partially identified setting. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.1 (Boundedness of the causal parameter). There exists a constant $C>0$ such that any causal parameter $\\beta$ generating the SCM (3) is norm-bounded by $C$ , i.e. $\\|\\beta\\|_{2}\\leq C$ and hence \u0398 = Bd(C) \u00d7 R(d+1)\u00d7(d+1). ", "page_idx": 6}, {"type": "text", "text": "Furthermore, two key quantities that appear in the bounds are $S_{\\mathrm{tot}}=\\mathbb{R}^{d}-\\mathrm{range}\\;.$ $R$ , the space of directions which are either identified or unperturbed during test time, and $C_{\\mathrm{ker}}=\\sqrt{C^{2}-\\|\\beta^{S}\\|^{2}}$ , the maximum norm of the non-identified part of the causal parameter $\\beta^{\\star}$ . Finally, recall that the reference distribution P\u03b8X\u22c6,,Y0 is observed and hence identifiable. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1. Assume that the training and test data follow the data-generating mechanism in Section 2.1 with test time shifts decomposed as in Equation (11) for some semi-orthogonal matrices $S,R$ with range $S\\subset S$ , range $R\\subset\\bar{S}^{\\perp}$ . Further, let Assumption 3.1 hold with parameter $C$ . The identifiable robust risk (8) is then given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathrm{rob},\\mathrm{ID}}(\\beta;\\Theta_{\\mathrm{eq}},\\gamma\\Pi_{M})=\\gamma\\mathbb{I}_{R\\neq0}(C_{\\mathrm{ker}}+\\|R^{\\top}\\beta\\|_{2})^{2}+\\gamma\\|S^{\\top}(\\beta^{S}-\\beta)\\|_{2}^{2}+\\mathcal{R}(\\beta;\\mathbb{P}_{\\theta^{\\star},0}^{X,Y}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Further, we obtain the following lower bound for the minimax quantity as defined in Equation (9): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathfrak{M}(\\Theta_{\\mathrm{eq}},\\gamma\\Pi_{\\mathcal{M}})\\left\\{\\overset=\\geq\\gamma\\mathbb{I}_{R\\not=0}C_{\\mathrm{ker}}^{2}+\\operatorname*{min}_{R^{\\top}\\beta=0}\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta^{S},\\gamma S S^{\\top}),\\quad i f\\gamma\\geq\\gamma_{\\mathrm{th}};\\atop{\\romannumeral1}}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma_{\\mathrm{th}}=\\frac{(\\kappa(\\Sigma_{\\eta}^{\\star})+1)\\|R R^{\\top}\\Sigma_{\\eta,\\xi}^{S}\\|}{C_{\\mathrm{ker}}}}\\end{array}$ . Moreover, if $R\\neq0$ , for small shifts ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\gamma\\to0}\\frac{\\mathfrak{M}(\\Theta_{\\mathrm{eq}},\\gamma\\Pi_{\\mathcal{M}})}{\\gamma}=(C_{\\mathrm{ker}}+\\|R R^{\\top}\\Sigma_{\\eta}^{\\star-1}\\Sigma_{\\eta,\\xi}^{S}\\|)^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We prove Theorem 3.1 in Appendix F.2. In the case of no new test shifts, i.e., $R=0$ , is discussed in prior work [45, 49], as the strength $\\gamma$ of the shift grows, the identifiable robust risk saturates. On ", "page_idx": 6}, {"type": "text", "text": "the other hand, if $R\\neq0$ , i.e., the test shift contains new directions w.r.t. to the training data, the best achievable robustness ${\\mathfrak{M}}(\\Theta_{\\mathrm{eq}},\\gamma\\Pi_{\\mathcal{M}})$ grows linearly with $\\gamma$ , and thus no infinite robustness is possible. We highlight that the minimax quantity is attained by the identifiable robust predictor ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta^{\\mathrm{rob,ID}}=\\underset{\\beta\\in\\mathbb{R}^{d}}{\\arg\\operatorname*{min}}\\,\\mathcal{R}_{\\mathrm{rob,ID}}(\\beta;\\Theta_{\\mathrm{eq}},\\gamma\\Pi_{\\mathcal{M}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and for $\\gamma\\ge\\gamma_{\\mathrm{th}}$ , the lower bound corresponds to its identifiable robust risk and thus is tight. Moreover, $\\gamma\\ge\\gamma_{\\mathrm{th}}$ , $\\beta^{\\mathrm{rob,ID}}$ can be explicitly computed from the training distributions (cf. Appendix F.2) and is orthogonal to the space range $R$ of non-identifiable test shift directions. In other words, for large shifts $\\gamma$ in non-identified directions, the optimal robust model would \"abstain\" from prediction in those directions. For smaller $\\gamma$ , $\\beta^{\\mathrm{rob,ID}}$ gradually utilizes less information in the non-identified directions, thus interpolating between maximum predictive power (OLS) and robustness w.r.t. new directions (abstaining). Note that the model $\\beta^{\\mathrm{rob,ID}}$ is a population quantity that is identifiable from the collection of training distributions. When only finite samples are available, we discuss in Appendix D how we can still estimate the minimax quantity by minimizing an empirical loss function (17) that can be computed from multi-environment data. Additionally, in Appendix D, we provide details on the computation of the empirical identifiable robust risk and the corresponding estimator. ", "page_idx": 7}, {"type": "text", "text": "3.2 Theoretical analysis of existing finite robustness methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now evaluate existing finite robustness methods in our partial identifiability framework and discuss in which scenarios they are far from the best achievable robustness. A spiritually similar systematic comparison of domain adaptation methods is presented in [12], however, in our setting, the robust risk is not identifiable from data. We impose a probability distribution on the environment variable $E\\,\\in\\,\\mathcal{E}_{\\mathrm{train}}$ s.t. $\\mathbb{P}[E\\,=\\,e]\\,=\\,w_{e}$ , which allows us to compare to the anchor regression framework and similar, where the environment weights are required to obtain an estimate of $M_{\\mathrm{test}}$ In our discussion, we focus on discrete anchor regression [45] and pooled OLS estimators8. In discrete anchor regression, for each environment $e$ , we observe data $(X_{e},Y_{e})$ following the SCM $X_{e}=\\mu_{e}+\\eta$ ; $\\dot{Y_{e}}={\\beta^{\\star}}^{\\top}{X_{e}}+\\xi$ , where $\\mu_{e}\\in\\mathbb{R}^{d}$ are mean shifts and the noise is distributed like in Equation (3). The discrete anchor regression estimator minimizes the following robust risk: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\beta_{\\mathrm{anchor}}=\\arg\\operatorname*{min}_{\\beta\\in\\mathbb{R}^{d}}\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta^{\\star},\\gamma M_{\\mathrm{anchor}}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where Manchor = e\u2208Etrain . The pooled ordinary least squares (OLS) estimator $\\beta_{\\mathrm{OLS}}$ corresponds to $\\beta_{\\mathrm{anchor}}$ with $\\gamma=1$ . We observe that the test shifts bounded by $\\gamma M_{\\mathrm{anchor}}$ are fully contained in the space of identified directions $\\boldsymbol{S}$ , since $S={\\mathrm{range~}}\\cup_{e\\in{\\mathcal{E}}_{\\mathrm{train}}}\\mu_{e}\\mu_{e}^{\\top}={\\mathrm{range~}}M_{\\mathrm{anchor}}$ Thus, according to Proposition 1, the robust risk and robust predictor $\\beta_{\\mathrm{anchor}}$ are identifiable for all $\\gamma>0$ . We now evaluate the robustness performance of $\\beta_{\\mathrm{anchor}}$ and $\\beta_{\\mathrm{OLS}}$ with respect to the more general shifts bounded by $M_{\\mathrm{new}}:=\\gamma M_{\\mathrm{anchor}}^{\\bar{}}+\\gamma^{\\prime}R R^{\\top}$ , thus consisting of training-identified shifts $M_{\\mathrm{anchor}}$ and a possibly smaller share of previously unseen shifts in range $R\\subset S^{\\perp}$ . With respect to $M_{\\mathrm{new}}$ , the robust risk is only partially identified, and identifiable robust risk (8) given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathrm{rob,ID}}(\\beta;\\Theta_{\\mathrm{eq}},M_{\\mathrm{new}})=\\gamma^{\\prime}(C_{\\mathrm{ker}}+\\|R^{\\top}\\beta\\|_{2})^{2}+\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta^{\\star},\\gamma M_{\\mathrm{anchor}}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We evaluate how the identifiable robust risks (14) of both previous methods depend on the strength $\\gamma^{\\prime}$ of the previously unseen shift: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{R}_{\\mathrm{rob,ID}}\\big(\\beta_{\\mathrm{anchor}};\\Theta_{\\mathrm{eq}},M_{\\mathrm{new}}\\big)/\\gamma^{\\prime}=\\big(C_{\\mathrm{ker}}+\\|R R^{\\top}(\\Sigma_{\\eta}^{\\star}+\\gamma M_{\\mathrm{anchor}})^{-1}\\Sigma_{\\eta,\\xi}^{S}\\|\\big)^{2}+o(\\gamma^{\\prime});}\\\\ &{}&{\\mathcal{R}_{\\mathrm{rob,ID}}\\big(\\beta_{\\mathrm{OLS}};\\Theta_{\\mathrm{eq}},M_{\\mathrm{new}}\\big)/\\gamma^{\\prime}=\\big(C_{\\mathrm{ker}}+\\|R R^{\\top}(\\Sigma_{\\eta}^{\\star}+M_{\\mathrm{anchor}})^{-1}\\Sigma_{\\eta,\\xi}^{S}\\|\\big)^{2}+o(\\gamma^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In contrast, for the best achievable robustness in the anchor setting9 it holds ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\frac{\\mathfrak{M}(\\Theta_{\\mathrm{eq}},M_{\\mathrm{new}})}{\\gamma^{\\prime}}=C_{\\mathrm{ker}}^{2}+o(\\gamma^{\\prime}),\\,\\,\\,\\mathrm{if}\\,\\,\\gamma^{\\prime}\\geq\\gamma_{\\mathrm{th}};}\\\\ &{\\displaystyle\\operatorname*{lim}_{\\gamma^{\\prime}\\to0}\\frac{\\mathfrak{M}(\\Theta_{\\mathrm{eq}},M_{\\mathrm{new}})}{\\gamma^{\\prime}}=(C_{\\mathrm{ker}}+\\|R R^{\\mathsf{T}}(\\Sigma_{\\eta}^{\\star}+\\gamma M_{\\mathrm{anchor}})^{-1}\\Sigma_{\\eta,\\xi}^{S}\\|)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "G2dYZJO4BE/tmp/fe3bc8a3db1964532c9beb2fbde043a912c2991e044a18f10aca93527de58a36.jpg", "img_caption": ["Figure 3: Test error under a partially unidentified distribution shift $A_{\\mathrm{{test}}}$ of the baseline estimators $\\beta_{\\mathrm{OLS}}$ , $\\beta_{\\mathrm{anchor}}$ (using the \"correct\" $\\gamma$ ) for finite robustness and the identifiable robust predictor in (mean-shifted) multienvironment finite-sample experiments in the classical identified setting (left) and the partially identified robustness setting (right). The details of the experimental setting can be found in Appendix E. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Thus, the anchor regression estimator is optimal in the limit of small unseen shifts but significantly deviates from the best achievable robustness for smaller shifts. This is due to the fact that the term $\\lVert R R^{\\top}(\\Sigma_{\\eta}^{\\star}+\\gamma M_{\\mathrm{anchor}})^{-1}\\Sigma_{\\eta,\\xi}^{S}\\rVert$ only goes to zero as $\\gamma\\to\\infty$ (yielding the minimax risk) if $M_{\\mathrm{anchor}}$ is full-rank, otherwise, it is strictly bounded from below as $\\Sigma_{\\eta}^{\\star}$ is full-rank. Moreover, under some conditions on the covariance matrix (e.g., if $\\Sigma_{\\eta}^{\\star}$ is block-diagonalizable w.r.t. $\\boldsymbol{S}$ and $S^{\\perp}$ ), pooled OLS and the anchor estimator achieve the same rate in $\\gamma^{\\prime}$ , showcasing how finite robustness methods can perform similarly to empirical risk minimization if the assumptions on the robustness set are not met. We provide additional comparisons in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "3.3 Experimental results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we provide empirical evidence of our theoretical conclusions in Section 3.1 and Section 3.2. In particular, we compare the prediction performance of multiple existing robustness methods to the minimax lower bound, estimated by the identifiable robust predictor \u2013 including partially idenfitiable settings in which the test data contains shifts in previously unseen directions. We observe that both in a synthetic adversarial setting, empirical risk minimization and invariancebased robustness methods have significantly sub-optimal test loss in the partially identified setting, confirming our theoretical predictions in Section 3.2.Furthermore, we observe that even though the minimizer of identifiable robust risk is optimal only for the linear causal setting in Section 2.1, it surprisingly outperforms existing methods in a real-world experiment. ", "page_idx": 8}, {"type": "text", "text": "Experiments on synthetic Gaussian data We simulate Gaussian covariates according to Equation (3) with multiple environments differing by linearly independent randomly selected mean shifts. Given a fixed confounding model represented by a noise covariance $\\Sigma$ and fixed directions $\\boldsymbol{S}$ of training mean shifts, we \"evaluate\" the identifiable robust risk by first picking the most adversarial $\\beta^{\\star}$ for fixed $\\Sigma$ and $\\boldsymbol{S}$ , and then computing its robust risk (6). We describe the full details of the data generation and loss evaluation in Appendix E. We consider two shift scenarios: in the first one, corresponding to the identifiable case in Figure 2a, the test environment is only perturbed by bounded shifts in training directions, as considered in prior work [45, 49]. In the second scenario, corresponding to the non-identifiable case Figure 2b, the test environment is perturbed by a mixture of training shifts and shifts in previously unobserved directions. We compute the empirical minimizers $\\hat{\\beta}_{\\mathrm{OLS}},\\bar{\\hat{\\beta}}_{\\mathrm{anchor}}$ and $\\hat{\\beta}^{\\mathrm{rob,ID}}$ of the OLS, anchor regression and identifiable robust losses, respectively, and compare their test MSE (mean squared error) in Figure 3. In the first (identifiable) setting \u2013 Figure 3 (left) \u2013 the robust risk is asymptotically constant across $\\gamma$ for both robust methods, while the error for the vanilla ERM or OLS estimator increases linearly. In contrast, in the second, partially identified, setting \u2013 Figure 3 (right) \u2013 all estimators exhibit linearly increasing test errors; however the slopes of the anchor and OLS estimator are much steeper and lead to larger errors than the empirical minimizer of (14) that closely matches the analytic theoretical lower bound. ", "page_idx": 8}, {"type": "text", "text": "Real-world data experiments We consider the single-cell gene expression dataset from [41], which consists of single-cell observations over $d=622$ genes collected from both observational and several interventional environments. Following [48], we only select the 28 genes that are active in the observational environment. For each gene $j=1,\\ldots,28$ , we generate a dataset $D_{j}$ where $Y:=X_{j}$ is the target variable and the covariates are the three genes most strongly correlated with $Y$ . For each $D_{j}$ , we perform three experiments \u2013 every experiment uses a different interventional environment besides the observational data as training data (an illustration of the data structure can be found in Figure 5). We then separately compute the mean-squared error on subsets of samples from all three interventional environments (including held-out samples from the interventional environment used for training). As a proxy for shift strength $\\gamma>0$ , for each test environment, we pick the $s\\times100\\%$ of data points closest to the observational mean. More details on this process can be found in Appendix E. We describe the computation of the identifiable robust estimator in Appendix D. In Figure 4, we show the test MSE of various OOD methods and the identifiable robust estimator as a function of $s$ , presented in four different scenarios: no unseen shifts (left), some proportion of unseen shifts (middle panels) and $100\\%$ unseen shift directions (right). We compare the performance of anchor regression [45], invariant causal prediction (ICP) [38], Distributional Robustness via Invariant Gradients (DRIG) [49], and OLS with our estimated lower bound Rob-ID. We observe that the performance ranking of the robustness methods significantly varies with the proportion of new test shift directions. When no new shift information is present, anchor regression and DRIG are optimal. However, as soon as some unseen directions are present, their performance becomes inferior to OLS/ERM and the gap to the minimizer of the identifiable robust risk (in the setting in Section 2) grows with the proportion of unseen shifts. While the MSE of the previous invariant methods increases drastically with the strength of the test shift, the test loss of the identifiable robust predictor remains relatively stable. ", "page_idx": 8}, {"type": "image", "img_path": "G2dYZJO4BE/tmp/29c733419ee7ae62edf987a03d1c20f88bf3f03d5f3084e60979939738c447d7.jpg", "img_caption": ["Figure 4: The figures show the performance of the identifiable robust predictor (Rob-ID) compared to other methods as a function of perturbation strength $s$ (which is obtained by selecting the $s\\times100$ percent of data points closest to the observational mean). Different panels correspond to the proportion of unseen shift directions at test time. For each panel and perturbation strength $s$ , each point represents an average over the 28 target genes and three experiments (i.e., training environments). ", "Methods: Rob-ID $\\Bumpeq$ Anchor DRIG ICP OLS "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4 Conclusion and future directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces the identifiable robust risk that is well-defined even in settings where the robust risk is not computable from training distributions. When the robustness set is identifiable (such as anchor regression-related methods [45, 49]), the identifiable robust risk reduces to the conventional robust risk. In this paper, we instantiate our general framework for linear structural causal models with additive shifts. We compute tight lower bounds for this setting and show how existing invariancebased methods are suboptimal. Further, we demonstrate how i) the benefits of invariance-based methods strongly decrease in the partially identifiable setting; and ii) this suboptimality increases with perturbation strength and proportion of previously unobserved test shifts. ", "page_idx": 9}, {"type": "text", "text": "The main limitation of our paper is its reliance on a linear causal setting to explicitly compute the observationally equivalent set and estimate the minimax quantity. However, we expect that the results and intuition developed in this paper can be extended to linear shifts in a lower-dimensional latent space via a suitable parametric or non-linear map [55, 10]. Important future directions include extending our results to more general causal graphs, non-linear relationships between covariates, non-additive shifts and the classification setting. Further, a potential use of our work is in the field of active intervention selection (e.g, [60, 21]). By computing the most adversarial model parameter for a given estimator, e.g., OLS, we can obtain an intervention which minimizes the identifiable robust risk of the estimator on the next unseen shift. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution generalization. Advances in Neural Information Processing Systems, 34:3438\u20133450, 2021.   \n[2] Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games. In International Conference on Machine Learning, pages 145\u2013155. PMLR, 2020.   \n[3] Kartik Ahuja, Jun Wang, Amit Dhurandhar, Karthikeyan Shanmugam, and Kush R. Varshney. Empirical or invariant risk minimization? A sample complexity perspective. In International Conference on Learning Representations, 2021.   \n[4] Takeshi Amemiya. Advanced Econometrics. Harvard University Press, 1985.   \n[5] Joshua D Angrist, Guido W Imbens, and Donald B Rubin. Identification of causal effects using instrumental variables. Journal of the American Statistical Association, 91(434):444\u2013455, 1996.   \n[6] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[7] Aharon Ben-Tal, Dick den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. Management Science, 59(2):341\u2013357, 2013.   \n[8] Andrew Bennett, Nathan Kallus, and Tobias Schnabel. Deep generalized method of moments for instrumental variable analysis. Advances in Neural Information Processing Systems, 32, 2019.   \n[9] Roger J Bowden and Darrell A Turkington. Instrumental variables. Number 8. Cambridge university press, 1990.   \n[10] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Sch\u00f6lkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing. Advances in Neural Information Processing Systems, 36, 2024.   \n[11] Peter B\u00fchlmann. Invariance, causality and robustness. Statistical Science, 35(3):404\u2013426, 2020.   \n[12] Yuansi Chen and Peter B\u00fchlmann. Domain adaptation under structural causal models. Journal of Machine Learning Research, 22(261):1\u201380, 2021.   \n[13] Mathieu Chevalley, Yusuf Roohani, Arash Mehrjou, Jure Leskovec, and Patrick Schwab. Causalbench: A large-scale benchmark for network inference from single-cell perturbation data. arXiv preprint arXiv:2210.17283, 2022.   \n[14] Rune Christiansen, Niklas Pfister, Martin Emil Jakobsen, Nicola Gnecco, and Jonas Peters. A causal framework for distribution generalization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):6614\u20136630, 2021.   \n[15] John Duchi and Hongseok Namkoong. Variance-based regularization with convex objectives. Journal of Machine Learning Research, 20(68):1\u201355, 2019.   \n[16] John C Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust optimization. The Annals of Statistics, 49(3):1378\u20131406, 2021.   \n[17] Jean-Marie Dufour and Cheng Hsiao. Identification, pages 65\u201377. Palgrave Macmillan UK, London, 2010.   \n[18] Jianqing Fan, Cong Fang, Yihong Gu, and Tong Zhang. Environment invariant linear least squares. arXiv preprint arXiv:2303.03092, 2023.   \n[19] Justin Frake, Anthony Gibbs, Brent Goldfarb, Takuya Hiraiwa, Evan Starr, and Shotaro Yamaguchi. From perfect to practical: Partial identification methods for causal inference in strategic management research. Available at SSRN 4228655, 2023.   \n[20] Charlie Frogner, Sebastian Claici, Edward Chien, and Justin Solomon. Incorporating unlabeled data into distributionally robust learning. Journal of Machine Learning Research, 22(56):1\u201346, 2021.   \n[21] Juan L Gamella and Christina Heinze-Deml. Active invariant causal prediction: Experiment selection through stability. Advances in Neural Information Processing Systems, 33:15464\u2013 15475, 2020.   \n[22] Rui Gao, Xi Chen, and Anton J Kleywegt. Wasserstein distributionally robust optimization and variation regularization. Operations Research, 2022.   \n[23] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.   \n[24] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International Conference on Learning Representations, 2021.   \n[25] Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. Deep IV: A flexible approach for counterfactual prediction. In International Conference on Machine Learning, pages 1414\u20131423. PMLR, 2017.   \n[26] Martin Emil Jakobsen and Jonas Peters. Distributional robustness of k-class estimators and the pulse. The Econometrics Journal, 25(2):404\u2013432, 2022.   \n[27] Pritish Kamath, Akilesh Tangella, Danica Sutherland, and Nathan Srebro. Does invariant risk minimization capture invariance? In International Conference on Artificial Intelligence and Statistics, pages 4069\u20134077. PMLR, 2021.   \n[28] Lucas Kook, Beate Sick, and Peter B\u00fchlmann. Distributional anchor regression. Statistics and Computing, 32(3):39, 2022.   \n[29] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (REx). In International Conference on Machine Learning, pages 5815\u20135826. PMLR, 2021.   \n[30] Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh ShafieezadehAbadeh. Wasserstein distributionally robust optimization: Theory and applications in machine learning. In Operations research & management science in the age of analytics, pages 130\u2013166. Informs, 2019.   \n[31] Jiashuo Liu, Jiayun Wu, Bo Li, and Peng Cui. Distributionally robust optimization with data geometry. Advances in Neural Information Processing Systems, 35:33689\u201333701, 2022.   \n[32] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.   \n[33] Sara Magliacane, Thijs Van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, and Joris M Mooij. Domain adaptation by using causal inference to predict invariant conditional distributions. Advances in Neural Information Processing Systems, 31, 2018.   \n[34] Nicolai Meinshausen. Causality from a distributional robustness point of view. In 2018 IEEE Data Science Workshop (DSW), pages 6\u201310. IEEE, 2018.   \n[35] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the Wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming, 171(1):115\u2013166, 2018.   \n[36] Krikamol Muandet, Arash Mehrjou, Si Kai Lee, and Anant Raj. Dual instrumental variable regression. Advances in Neural Information Processing Systems, 33:2710\u20132721, 2020.   \n[37] Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, USA, 2nd edition, 2009.   \n[38] Jonas Peters, Peter B\u00fchlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(5):947\u20131012, 2016.   \n[39] Jonas Peters, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Elements of causal inference: foundations and learning algorithms. 2017.   \n[40] Lei S Qi, Matthew H Larson, Luke A Gilbert, Jennifer A Doudna, Jonathan S Weissman, Adam P Arkin, and Wendell A Lim. Repurposing CRISPR as an RNA-guided platform for sequence-specific control of gene expression. Cell, 152(5):1173\u20131183, 2013.   \n[41] Joseph M Replogle, Reuben A Saunders, Angela N Pogson, Jeffrey A Hussmann, Alexander Lenail, Alina Guna, Lauren Mascibroda, Eric J Wagner, Karen Adelman, Gila Lithwick-Yanai, et al. Mapping information-rich genotype-phenotype landscapes with genome-scale Perturb-seq. Cell, 185(14):2559\u20132575, 2022.   \n[42] Mateo Rojas-Carulla, Bernhard Sch\u00f6lkopf, Richard Turner, and Jonas Peters. Invariant models for causal transfer learning. Journal of Machine Learning Research, 19(36):1\u201334, 2018.   \n[43] Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. An online learning approach to interpolation and extrapolation in domain generalization. In International Conference on Artificial Intelligence and Statistics, pages 2641\u20132657. PMLR, 2022.   \n[44] Elan Rosenfeld, Pradeep Kumar Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. In International Conference on Learning Representations, 2021.   \n[45] Dominik Rothenh\u00e4usler, Nicolai Meinshausen, Peter B\u00fchlmann, and Jonas Peters. Anchor regression: Heterogeneous data meet causality. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83(2):215\u2013246, 2021.   \n[46] Sorawit Saengkyongam, Leonard Henckel, Niklas Pfister, and Jonas Peters. Exploiting independent instruments: Identification and distribution generalization. In International Conference on Machine Learning, pages 18935\u201318958. PMLR, 2022.   \n[47] Shiori Sagawa\\*, Pang Wei Koh\\*, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2020.   \n[48] Christoph Schultheiss and Peter B\u00fchlmann. Assessing the overall and partial causal wellspecification of nonlinear additive noise models. Journal of Machine Learning Research, 25(159):1\u201341, 2024.   \n[49] Xinwei Shen, Peter B\u00fchlmann, and Armeen Taeb. Causality-oriented robustness: exploiting general additive interventions. arXiv preprint arXiv:2307.10299, 2023.   \n[50] Wenqi Shi and Wenkai Xu. Learning nonlinear causal effect via kernel anchor regression. In Uncertainty in Artificial Intelligence, pages 1942\u20131952. PMLR, 2023.   \n[51] Yuge Shi, Jeffrey Seely, Philip Torr, Siddharth N, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In International Conference on Learning Representations, 2022.   \n[52] Rahul Singh, Maneesh Sahani, and Arthur Gretton. Kernel instrumental variable regression. Advances in Neural Information Processing Systems, 32, 2019.   \n[53] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. In International Conference on Learning Representations, 2018.   \n[54] Elie Tamer. Partial identification in econometrics. Annu. Rev. Econ., 2(1):167\u2013195, 2010.   \n[55] Nikolaj Thams, Michael Oberst, and David Sontag. Evaluating robustness to dataset shift via parametric robustness sets. Advances in Neural Information Processing Systems, 35:16877\u2013 16889, 2022.   \n[56] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge University Press, 2000.   \n[57] Chuanlong Xie, Haotian Ye, Fei Chen, Yue Liu, Rui Sun, and Zhenguo Li. Risk variance penalization. arXiv preprint arXiv:2006.07544, 2020.   \n[58] Bin Yu. Assouad, Fano, and le Cam. In Festschrift for Lucien Le Cam: Research papers in probability and statistics, pages 423\u2013435. Springer, 1997.   \n[59] Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the Davis\u2013Kahan theorem for statisticians. Biometrika, 102(2):315\u2013323, 2015.   \n[60] Jiaqi Zhang, Louis Cammarata, Chandler Squires, Themistoklis P Sapsis, and Caroline Uhler. Active learning for optimal intervention design in causal models. Nature Machine Intelligence, 5(10):1066\u20131075, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The following sections provide deferred discussions, proofs and experimental details. ", "page_idx": 14}, {"type": "text", "text": "Table of contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Extended related work 16 ", "page_idx": 14}, {"type": "text", "text": "B Extension to the general additive shift setting 16 ", "page_idx": 14}, {"type": "text", "text": "C Comparison to finite robustness methods continued 17 ", "page_idx": 14}, {"type": "text", "text": "C.1 Continuous anchor regression [45] 17   \nC.2 Distributionally robust invariant gradients (DRIG) [49] 18 ", "page_idx": 14}, {"type": "text", "text": "D Empirical estimation of the identifiable robust predictor 19 ", "page_idx": 14}, {"type": "text", "text": "D.1 Computing the identifiable robust loss 19   \nD.2 Consistency of the identifiable robust predictor 20   \nD.3 Proof of Proposition 3 . . . 21   \nD.4 Proof of auxiliary lemmas 22   \nD.4.1 Proof of Lemma D.1 22   \nD.4.2 Proof of Lemma D.2 23   \nD.4.3 Proof of Lemma D.3 23 ", "page_idx": 14}, {"type": "text", "text": "E Details on finite-sample experiments 24 ", "page_idx": 14}, {"type": "text", "text": "E.1 Synthetic experiments . . 24   \nE.2 Real-world data experiments 25   \nProofs 26   \nF.1 Proof of Proposition 1 . 26   \nF.2 Proof of Theorem 3.1 27 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Extended related work", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To put our work into context, first, we discuss relevant distributional robustness literature organized according to structural assumptions on the desired robustness set. Second, we summarize existing views on partial identifiability in the causality and econometrics literature and how our findings connect to their perspective. ", "page_idx": 15}, {"type": "text", "text": "No structural assumptions on the shift. DRO: Distributionally robust optimization (DRO) tackles the problem of domain generalization when the robustness set is a ball around the training distribution w.r.t. some probability distance measure, e.g., Wasserstein distance [53, 35] or $f$ -divergences [7, 16]. Considering all test distributions in a discrepancy ball can lead to overly conservative predictions, and therefore, alternatives have been proposed in, e.g., the Group DRO literature [47, 20, 31]. However, these methods cannot protect against perturbations larger than the ones seen during training time and do not provide a clear interpretation of the perturbations class [49]. ", "page_idx": 15}, {"type": "text", "text": "Structural assumptions on the shift. Robustness from the lens of causality takes a step further, by assuming a structural causal model [37] generating the observed data $(X,Y)$ . Infinite robustness methods: The motivation of causal methods for robustness is that the causal function is worst-case optimal to predict the response under interventions of arbitrary direction and strength on the covariates [34, 11]. For this reason, causal models achieve what we call infinite robustness. Depending on the assumptions of the SCM, there are different ways to achieve infinite robustness. When there are no latent confounders, several works [38, 18, 33, 42, 2, 6, 51, 57, 29, 1] aim to identify the causal parents and achieve infinite robustness by exploiting the heterogeneity across training environments. In the presence of latent confounders, it is possible to achieve infinite robustness by identifying the causal function with, e.g., the instrumental variable method [5, 25, 52, 8, 36]. There are different limitations to infinitely robust methods. First, the identifiability conditions of the causal parents and/or causal function are often challenging to verify in practice. Second, ERM can outperform these methods when the interventions (read shifts) at test time are not arbitrarily strong or act directly on the response or latent variable [3, 24]. Finite robustness methods: In real data, shifts of arbitrary direction and strength in the covariates are unrealistic. Thus, different methods [45, 26, 28, 49, 14] trade off robustness against predictive power to achieve what we call finite robustness. The main idea of finite robustness methods is to learn a function that is as predictive as possible while protecting against shifts up to some strength in the directions that are observed during training time. These methods, however, only provide robustness guarantees that depend on the heterogeneity of the training data and do not offer insights into the limits of algorithm-independent robustness under shifts in new directions. ", "page_idx": 15}, {"type": "text", "text": "Partial identifiability: The problem of identification is at the center of the causal and econometric literature [39, 4]. It studies the conditions under which the (population) training distribution uniquely determines the causal parameters of the underlying SCM. Often, the training distribution only offers partial information about the causal parameters and, therefore, determines a set of observational equivalent parameters. This setting is known as partial or set identification and is used in causality and econometrics to learn intervals within which the true causal parameter lies [54]. In this work, we borrow the notion of partial identification to study the problem of distributional robustness when the robustness set itself is only partially identified. ", "page_idx": 15}, {"type": "text", "text": "B Extension to the general additive shift setting ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We discuss how our setting changes when we relax the assumptions on the existence of the reference environment. We consider the data-generating process in Equation (3), where $\\ensuremath{\\mathcal{E}}_{\\mathrm{train}}=[m]$ , $m\\in\\mathbb{N}$ If no environment $e$ exists with $\\mu_{e}=0$ and $\\Sigma_{e}=0$ , we first pick an arbitrary distribution $\\mathbb{P}_{\\mathrm{ref}}^{X,Y}$ as the reference environment10 . We denote $\\Sigma_{\\eta}^{\\prime}:=\\Sigma_{\\eta}^{\\star}+\\Sigma_{\\mathrm{ref}}$ . ", "page_idx": 15}, {"type": "text", "text": "First, we show we can express the space $\\boldsymbol{S}$ of training additive shift directions defined in Equation (10) in the general case. We center all distributions by $\\mu_{\\mathrm{ref}}$ , so that $\\mathfrak{T}\\left[X_{e}\\right]=\\mu_{e}-\\mu_{\\mathrm{ref}}$ for all $e\\in\\mathcal{E}_{\\mathrm{train}}$ . ", "page_idx": 15}, {"type": "text", "text": "With respect to the arbitrary reference environment, we now define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{S}:=\\mathrm{range}\\,\\bigcup_{e\\in\\mathcal{E}_{\\mathrm{train}}}\\left(\\Sigma_{e}-\\Sigma_{\\mathrm{ref}}+(\\mu_{e}-\\mu_{\\mathrm{ref}})(\\mu_{e}-\\mu_{\\mathrm{ref}})^{\\top}\\right)\\subset\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We now consider test shifts with respect to the environment $\\mathbb{P}_{\\mathrm{ref}}^{X,Y_{11}}$ . We define the test shift upper bound $\\gamma\\Pi_{\\mathcal{M}}$ . Again, we can decompose the upper bound as $^\\gamma\\Pi_{\\mathcal{M}}\\,=\\,\\gamma S S^{\\top}+\\gamma R R^{\\top}$ , where $S S^{\\top}$ and $\\dot{R}R^{\\top}$ are orthogonal projections onto ${\\mathcal{S}}\\cap{\\mathcal{M}}$ and $s^{\\perp}\\cap\\mathcal{M}$ , respectively. Again, we can decompose the causal parameter $\\beta^{\\star}$ as $\\beta^{\\star}\\,=\\,\\beta^{S}\\,+\\,\\beta^{S^{\\perp}}$ . The projection $\\beta^{S}$ of the causal parameter onto the relative training shifts induces the following observationally equivalent parameters corresponding to the reference distribution: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{^{s}:=(\\beta^{s},\\Sigma_{\\eta}^{\\prime},\\Sigma_{\\eta,\\xi}^{s},(\\sigma_{\\xi}^{s})^{2})=(\\beta^{s},\\Sigma_{\\eta}^{\\prime},\\Sigma_{\\eta,\\xi}^{\\star}+\\Sigma_{\\eta}^{\\prime}\\beta^{{s}^{\\bot}},(\\sigma_{\\xi}^{\\star})^{2}+2\\langle\\Sigma_{\\eta,\\xi}^{\\star},\\beta^{{s}^{\\bot}}\\rangle+\\langle\\beta^{{s}^{\\bot}},\\Sigma_{\\eta}^{\\prime}\\beta^{{s}^{\\bot}}\\rangle).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Again, $\\theta^{S}$ can be identified from the training distributions and is referred to as the identified model rpoabruasmt eptreerdsi. ctTihoen  fmololdoewli insg  oandlay pitdeedn tviefriasibolen  iof ft hPer otepsots isthiiofnts  1a rseh oinw tsh teh adti raescstiuomni nofg  tshhei frtesl aotni $\\mathbb{P}_{\\mathrm{ref}}^{X,Y}$ ,n itnhge shifts: ", "page_idx": 16}, {"type": "text", "text": "Proposition 2 (Identifiability of reference distribution parameters and robust prediction model). Suppose that the set of training and test distributions is generated according to Equations (3) and (4). Then, $\\theta^{S}$ is observationally equivalent to $\\theta^{\\star}$ and computable from training distributions. Furthermore, it holds that ", "page_idx": 16}, {"type": "text", "text": "(a) the model parameters generating the reference distribution can be identified up to the following observationally equivalent set : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta_{\\mathrm{eq}}=\\{\\beta^{S}+\\alpha,\\Sigma_{\\eta}^{\\prime},\\Sigma_{\\eta,\\xi}^{S}-\\Sigma_{\\eta}^{\\prime}\\alpha,(\\sigma_{\\xi}^{S})^{2}-2\\alpha^{\\top}\\Sigma_{\\eta,\\xi}^{S}+\\alpha^{\\top}\\Sigma_{\\eta}^{\\prime}\\alpha\\colon\\alpha\\in\\mathcal{S}^{\\perp}\\}\\ni\\theta^{\\star}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$(b)$ the robust prediction model $\\beta^{r o b}$ as defined in Equation (7) is identified up to the set ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\beta^{S}+(\\gamma\\Pi_{M}+\\Sigma_{\\eta}^{\\prime})^{-1}\\Sigma_{\\eta,\\xi}^{S}+\\{(\\gamma\\Pi_{M}+\\Sigma_{\\eta}^{\\prime})^{-1}\\alpha\\colon\\alpha\\in\\mathrm{range}\\;R\\}\\ni\\beta^{r o b}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The proof is analogous to Appendix F.1. A version of Theorem 3.1 for perturbations on the reference environment follows accordingly. ", "page_idx": 16}, {"type": "text", "text": "C Comparison to finite robustness methods continued ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Continuous anchor regression [45] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the continuous anchor regression setting, during training we observe the distribution according to the SCM $X=M A+\\eta$ ; $\\boldsymbol{Y}=\\beta^{\\star\\intercal}\\boldsymbol{X}+\\boldsymbol{\\bar{\\xi}}$ , where $A\\sim\\bar{\\mathcal{N}}(0,\\Sigma_{A})$ is an observed $q_{\\mathrm{~\\,~}}$ -dimensional anchor variable and $\\boldsymbol{M}\\,\\in\\,\\mathbb{R}^{d\\times q}$ is a known matrix. Note that in this setting, we do not have a reference environment, but, since the anchor variable is observed, the distribution of the additive shift $M A$ is known. The test shifts are assumed to be bounded by $M_{\\mathrm{test}}\\,=\\,\\gamma M\\Sigma_{A}M^{\\top}$ . Since range $M_{\\mathrm{test}}\\subset\\cal{S}=\\mathrm{range}\\:M$ , no new directions are observed during test time, in other words, $R=0$ Thus, both the corresponding robust loss and the anchor regression estimator can be determined from training data. It holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\beta_{\\mathrm{anchor}}=\\underset{\\beta\\in\\mathbb{R}^{d}}{\\arg\\operatorname*{min}}\\,\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta^{\\star},\\gamma M\\Sigma_{A}M^{\\top}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Again, the pooled OLS estimator corresponds to $\\beta_{\\mathrm{anchor}}$ with $\\gamma=1$ . Similar to the discrete anchor case, in case the test shifts are given by $\\mathbf{\\bar{\\}{\\mathcal{M}}_{n e w}}=\\gamma{M}\\Sigma_{A}{M}^{\\top}+\\gamma^{\\prime}R R^{\\top}$ , the identifiable robust risk (8) is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathrm{rob,ID}}(\\beta;\\Theta_{\\mathrm{eq}},M_{\\mathrm{new}})=\\gamma^{\\prime}(C_{\\mathrm{ker}}+\\|R^{\\top}\\beta\\|_{2})^{2}+\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta^{\\star},\\gamma M\\Sigma_{A}M^{\\top})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and for the best achievable robustness of the anchor estimator it holds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{R}_{\\mathrm{rob,ID}}\\big(\\beta_{\\mathrm{anchor}},\\Theta_{\\mathrm{eq}};M_{\\mathrm{new}}\\big)/\\gamma^{\\prime}=(C_{\\mathrm{ker}}+\\|R R^{\\top}(\\Sigma_{\\eta}^{\\star}+\\gamma M\\Sigma_{A}M^{\\top})^{-1}\\Sigma_{\\eta,\\xi}^{S}\\|)^{2}+o(\\gamma^{\\prime});}\\\\ &{\\quad\\operatorname*{lim}_{\\gamma^{\\prime}\\to0}\\mathcal{R}_{\\mathrm{rob,ID}}\\big(\\beta_{\\mathrm{anchor}},\\Theta_{\\mathrm{eq}};M_{\\mathrm{new}}\\big)/\\gamma^{\\prime}=\\displaystyle\\operatorname*{lim}_{\\gamma^{\\prime}\\to0}\\frac{\\mathfrak{M}(\\Theta_{\\mathrm{eq}},M_{\\mathrm{new}})}{\\gamma^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The above results follow by plugging $M_{\\mathrm{new}}$ with $M:=M_{\\mathrm{anchor}}$ into the proof of Theorem 3.1 in Appendix F.2. ", "page_idx": 17}, {"type": "text", "text": "C.2 Distributionally robust invariant gradients (DRIG) [49] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "DRIG [49] uses the framework of Gaussian additive shifts $\\boldsymbol{A_{e}}\\sim\\mathcal{N}(\\boldsymbol{\\mu_{e}},\\boldsymbol{\\Sigma_{e}})$ . For each environment $e$ , we observe data $(X_{e},Y_{e})$ distributed according to the SCM $X_{e}=A_{e}+\\eta$ ; $Y_{e}=\\beta^{\\star}{}^{\\top}X_{e}+\\xi$ , where the noise is distributed like in Equation (3). DRIG consider more a more general intervention setting, additionally allowing additive shifts of $Y$ and hidden confounders $H$ . However, their identifiability results can only be shown for the case of interventions on $X$ , and since identifiability of the causal parameter is a crucial part of our analysis, we only consider shifts on the covariates. DRIG assumes existence of a reference environment $e=0$ with $\\mu_{0}=0$ and for which it is required that the second moment of the reference environment is dominated by the second moment of the training mixture: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Sigma_{0}\\preceq\\sum_{e\\in[m]}w_{e}(\\Sigma_{e}+\\mu_{e}\\mu_{e}^{\\top}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This assumption allows [49] to derive the DRIG estimator which is robust against test shifts upper bounded by $\\begin{array}{r}{M_{\\mathrm{DRIG}}:=\\stackrel{}{\\gamma}\\sum_{e\\in[m]}w_{e}(\\Sigma_{e}-\\Sigma_{0}+\\mu_{e}\\mu_{e}^{\\top})}\\end{array}$ . The following lemma allows us to make further statements about $M_{\\mathrm{DRIG}}$ : ", "page_idx": 17}, {"type": "text", "text": "Lemma C.1. Let $A$ and $B$ be positive semidefinite matrices such that $B\\preceq A$ . Then it holds that range $B\\subset$ range $A$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. It suffices to show that $\\ker A\\subset\\ker B$ . $\\ker A\\subset\\ker B$ implies that range $A=(\\ker A)^{\\perp}\\subset$ $(\\ker B)^{\\perp}={\\mathrm{range}}\\ B.)$ ) Consider $x\\in\\ker A$ , $x\\neq0$ . Then it holds that $x^{\\top}(A-B)x=x^{\\top}A x-$ ${\\boldsymbol{x}}^{\\top}{\\boldsymbol{B}}{\\boldsymbol{x}}=0-{\\boldsymbol{x}}^{\\top}{\\boldsymbol{B}}{\\boldsymbol{x}}\\geq0$ , from which it follows that $x^{\\top}B x=0$ and thus $x\\in\\ker B$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Because of the assumption $\\begin{array}{r}{\\Sigma_{0}\\preceq\\sum_{e\\in[m]}w_{e}(\\Sigma_{e}+\\mu_{e}\\mu_{e}^{\\top})}\\end{array}$ , by Lemma C.1 it follows that range $\\Sigma_{0}\\subset$ $\\cup_{e\\geq1}$ range $(\\Sigma_{e}+\\mu_{e}\\mu_{e}^{\\top})$ and thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{range}\\;M_{\\mathrm{DRIG}}\\subseteq\\mathrm{range}\\;\\left(\\sum_{e\\geq1}w_{e}(\\Sigma_{e}+\\mu_{e}\\mu_{e}^{\\top})\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, the robustness directions achievable by DRIG in the \"dominated reference environment\" setting are the same as the ones under the assumption $\\Sigma_{0}=0$ . ", "page_idx": 17}, {"type": "text", "text": "Again, we observe that the test shifts bounded by $\\gamma M_{\\mathrm{DRIG}}$ are fully contained in the space of identified directions $\\boldsymbol{S}$ . If the test shifts are instead bounded by $M_{\\mathrm{new}}\\;:=\\;\\gamma M_{\\mathrm{DRIG}}+\\dot{\\gamma}^{\\prime}R R^{\\top}$ , including some unseen directions range $R\\subset S^{\\perp}$ , the robust risk in the DRIG setting is only partially identified. The identifiable robust risk (8) is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathrm{rob,ID}}(\\beta;\\Theta_{\\mathrm{eq}},M_{\\mathrm{new}})=\\gamma^{\\prime}(C_{\\mathrm{ker}}+\\|R^{\\top}\\beta\\|_{2})^{2}+\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta^{\\star},\\gamma M_{\\mathrm{DRIG}}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and again, the DRIG estimator is optimal for infinitesimal shifts $\\gamma^{\\prime}$ and suboptimal for larger $\\gamma^{\\prime}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{\\mathrm{rob,ID}}(\\beta_{\\mathrm{DRIG}};\\Theta_{\\mathrm{eq}},M_{\\mathrm{new}})/\\gamma^{\\prime}=(C_{\\mathrm{ker}}+\\|R R^{\\top}(\\Sigma_{\\eta}^{\\star}+\\gamma M_{\\mathrm{DRIG}})^{-1}\\Sigma_{\\eta,\\xi}^{S}\\|)^{2}+o(\\gamma^{\\prime});}\\\\ &{\\qquad\\qquad\\qquad\\underbrace{\\mathfrak{M}(\\Theta_{\\mathrm{eq}},M_{\\mathrm{new}})}_{\\gamma^{\\prime}}=C_{\\mathrm{ker}}^{2},\\;\\;\\mathrm{if}\\;\\gamma^{\\prime}\\geq\\gamma_{\\mathrm{th}};}\\\\ &{\\qquad\\qquad\\operatorname*{lim}_{\\gamma^{\\prime}\\to0}\\frac{\\mathfrak{M}(\\Theta_{\\mathrm{eq}},M_{\\mathrm{new}})}{\\gamma^{\\prime}}=(C_{\\mathrm{ker}}+\\|R R^{\\top}(\\Sigma_{\\eta}^{\\star}+\\gamma M_{\\mathrm{DRIG}})^{-1}\\Sigma_{\\eta,\\xi}^{S}\\|)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The above results follow by plugging $M_{\\mathrm{new}}$ with $M:=M_{\\mathrm{DRIG}}$ into the proof of Theorem 3.1 in Appendix F.2. ", "page_idx": 17}, {"type": "text", "text": "D Empirical estimation of the identifiable robust predictor ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we discuss how to compute the identifiable robust loss and its minimizer from finitesample multi-environment training data. We first describe the finite-sample setting and provide a high-level algorithm. We then discuss some parts of the algorithm in more detail. Finally, we show that under certain assumptions, the empirical identifiable robust loss is consistent. ", "page_idx": 18}, {"type": "text", "text": "D.1 Computing the identifiable robust loss ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Algorithm 1 Computation of the identifiable robust loss   \n1: Input: Multi-environment data $D:=\\cup_{e\\in\\mathcal{E}_{\\mathrm{train}}}D_{e}$ , test shift strength $\\gamma>0$ , test shift directions $M\\in\\mathbb{R}^{d\\times d}$ , causal parameter upper bound $C>0$ .   \n2: Step 1: Estimate the training shift directions $\\hat{S}(D)$ , its orthogonal complement $\\hat{S}^{\\perp}(\\mathcal{D})$ , and the identified causal parameter $\\hat{\\beta}^{S}$ .   \n3: Step 2: Estimate the identified and non-identified test shift directions ${\\hat{S}},{\\hat{R}}$ and their projections $\\hat{S}\\hat{S}^{\\bar{\\top}}$ and $\\hat{R}\\hat{R}^{\\top}$ .   \n4: Step 3: Estimate the norm $\\hat{C}_{\\mathrm{ker}}$ of the non-identified causal parameter.   \n5: Step 4: Compute the identifiable robust loss function $\\begin{array}{r}{\\mathcal{L}_{n}(\\beta;\\hat{\\beta}^{\\mathcal{S}},\\hat{S}\\hat{S}^{\\top},\\hat{R}\\hat{R}^{\\top})\\leftarrow\\underbrace{\\mathcal{L}_{\\mathrm{ref}}(\\beta;\\mathcal{D}_{0})}_{\\mathrm{reference~loss}}+\\underbrace{\\mathcal{L}_{\\mathrm{inv}}(\\beta;\\hat{\\beta}^{\\mathcal{S}},\\hat{S}\\hat{S}^{\\top},\\gamma)}_{\\mathrm{invariance~penalty~term}}+\\underbrace{\\mathcal{L}_{\\mathrm{id}}(\\beta;\\hat{C}_{\\mathrm{ker}},\\hat{R}\\hat{R}^{\\top},\\gamma)}_{\\mathrm{non-identifiability~penalty~term}}.}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "6: Return: identifiable robust predictor and the estimated minimax \"hardness\" of the problem: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\hat{\\beta}^{\\mathrm{rob,ID}}\\gets\\arg\\operatorname*{min}\\mathcal{L}_{n}(\\beta;\\hat{\\beta}^{S},\\hat{S}\\hat{S}^{\\top},\\hat{R}\\hat{R}^{\\top});}\\\\ &{}&{\\hat{\\mathfrak{M}}(\\mathcal{D},\\gamma,M)\\gets\\underset{\\beta\\in\\mathbb{R}^{d}}{\\operatorname*{min}}\\mathcal{L}_{n}(\\beta;\\hat{\\beta}^{S},\\hat{S}\\hat{S}^{\\top},\\hat{R}\\hat{R}^{\\top}).\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Training data. We observe data from $m+1$ training environments indexed by $E\\,\\in\\,\\mathcal{E}_{\\mathrm{train}}\\,=$ $\\{0,...,m\\}$ , where $E=0$ represents the reference environment. We impose a discrete probability distribution $\\mathbb{P}^{E}$ on the training environment $E\\in\\mathcal{E}_{\\mathrm{train}}$ , resulting in the joint distribution $({\\bar{X}},Y,E)\\stackrel{.}{\\sim}$ $\\mathbb{P}^{X,Y|E}\\times\\mathbb{P}^{E}$ . For each environment $E\\,=\\,e$ , we observe the samples $\\mathcal{D}_{e}:=\\,\\{(X_{e,i},Y_{e,i})\\}_{i=1}^{n_{e}}$ , where $(X_{e,i},Y_{e,i})$ are independent copies of $(X_{e},Y_{e})\\sim\\mathbb{P}^{X,Y|E=e}$ . Then, the resulting dataset is $D:=\\cup_{e\\in\\mathcal{E}_{\\mathrm{train}}}D_{e}$ with $n:=n_{0}+\\cdot\\cdot+n_{m}.$ . Furthermore, for each environment $E=e$ , we define the weights ${\\widehat{w_{e}}}:=n_{e}/n$ . ", "page_idx": 18}, {"type": "text", "text": "Computation of the identifiable robust loss. In Algorithm 1, we present a high-level scheme for computing the identifiable robust loss from multi-environment data, which consists of multiple steps. First, nuisance parameters related to the training and test shift directions are estimated, which we describe in more detail below. Afterwards, the three terms of the loss are computed: the (squared) loss $\\mathcal{L}_{\\mathrm{ref}}(\\beta;\\mathcal{D}_{0})$ on the reference environment is computed as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{ref}}(\\beta;\\mathcal{D}_{0})=\\sum_{i=1}^{n_{0}}(Y_{0,i}-\\beta^{\\top}X_{0,i})^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The invariance penalty term $\\mathcal{L}_{\\mathrm{inv}}(\\beta;\\hat{\\beta}^{S},\\hat{S}\\hat{S}^{\\top},\\gamma)$ (which increasingly aligns any estimator $\\beta$ in the direction of the estimated invariant causal predictor $\\hat{\\beta}^{S}$ as $\\gamma\\rightarrow\\infty,$ ) can be computed as following in the linear SCM setting: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{inv}}(\\beta;\\hat{\\beta}^{S},\\hat{S}\\hat{S}^{\\top},\\gamma)=\\gamma\\|\\hat{S}\\hat{S}^{\\top}(\\beta-\\beta^{S})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, the non-identifiability penalty term $\\mathcal{L}_{\\mathrm{id}}(\\beta;\\hat{C}_{\\mathrm{ker}},\\hat{R}\\hat{R}^{\\top},\\gamma)$ can be computed as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{id}}(\\beta;\\hat{C}_{\\mathrm{ker}},\\hat{R}\\hat{R}^{\\top},\\gamma)\\leftarrow\\gamma(C_{\\mathrm{ker}}+\\|\\hat{R}\\hat{R}^{\\top}\\beta\\|_{2})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The non-identifiability term, with increasing $\\gamma$ , penalizes any predictor $\\beta$ towards zero on the subspace $R$ of non-identified test shift directions. In total, the identifiable robust loss (in the linear SCM setting) equals ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n}(\\beta;\\hat{\\beta}^{\\mathcal{S}},\\hat{S}\\hat{S}^{\\top},\\hat{R}\\hat{R}^{\\top})=\\sum_{i=1}^{n_{0}}(Y_{0,i}-\\beta^{\\top}X_{0,i})^{2}+\\gamma\\lVert\\hat{S}\\hat{S}^{\\top}(\\beta-\\beta^{S})\\rVert_{2}^{2}+\\gamma(C_{\\mathrm{ker}}+\\lVert\\hat{R}\\hat{R}^{\\top}\\beta\\rVert_{2})^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we suppress dependence on $C$ and $\\gamma$ and only leave the dependence on the nuisance parameters. ", "page_idx": 19}, {"type": "text", "text": "Choice/Estimation of nuisance parameters. We now provide more details on the empirical estimation of the nuisance parameters $\\hat{S},\\hat{S},\\hat{R};$ , and $\\hat{\\beta}^{S}$ . ", "page_idx": 19}, {"type": "text", "text": "\u2022 The constant $C$ corresponds to the upper bound on the norm of the true causal parameter $\\beta^{\\star}$ . Thus, the practitioner chooses $C$ in advance to ensure that (with high probability) $\\|\\beta^{\\star}\\|_{2}\\leq C$ . ", "page_idx": 19}, {"type": "text", "text": "\u2022 The training shift directions $\\hat{S}$ can be computed via ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{S}(\\mathcal{D})=\\mathrm{range}\\sum_{e=1}^{m}(\\mathrm{Cov}(X^{e})-\\mathrm{Cov}(X^{0})+\\mu_{e}\\mu_{e}^{\\top}-\\mu_{0}\\mu_{0}^{\\top}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where for $e\\,\\in\\,\\mathcal{E}_{\\mathrm{train}}$ , the matrix $\\operatorname{Cov}(X^{e})$ is the empirical covariance matrix estimated within the training environment $E=e$ , and $\\mu_{e}\\in\\mathbb{R}^{d}$ is the empirical mean of the covariates within the training environment $E\\,=\\,e$ . Additionally, we compute the orthogonal complement $\\hat{S}^{\\perp}(\\mathcal{D})$ of the space $\\hat{S}(D)^{12}$ . ", "page_idx": 19}, {"type": "text", "text": "\u2022 The decomposition of the test shift directions $M$ into identified and non-identified shift directions (and their corresponding projection matrices) can be computed as follows. Let $\\Pi_{\\hat{S}}$ and $\\Pi_{\\hat{S}^{\\perp}}$ denote the projection matrices on $\\hat{S}(D)$ and $\\hat{S}^{\\perp}(\\mathcal{D})$ , respectively. Consider the singular value decompositions \u03a0 S\u02c6M = U S\u02c6\u03a3 S\u02c6V S\u02c6\u22a4 and $\\Pi_{\\hat{S}^{\\perp}}M=U_{\\hat{S}^{\\perp}}\\Sigma_{\\hat{S}^{\\perp}}V_{\\hat{S}^{\\perp}}^{\\top}$ Then, define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{S}=U_{\\hat{\\cal S}},\\quad\\hat{R}=U_{\\hat{\\cal S}^{\\perp}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The subspaces range $(\\Pi_{\\hat{S}}M)$ and range $(\\Pi_{\\hat{S}^{\\perp}}M)$ are minimal subspaces contained in $\\hat{S}$ and $\\hat{S}^{\\perp}$ , respectively, such that $\\mathrm{range}(M)\\,\\subset\\,\\mathrm{range}(\\Pi_{\\hat{\\cal S}}M)\\oplus\\mathrm{range}(\\Pi_{\\hat{\\cal S}^{\\perp}}M)$ . The matrices $\\hat{S}\\hat{S}^{\\top}$ and $\\hat{R}\\hat{R}^{\\top}$ are their corresponding projection matrices. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The identified causal parameter $\\hat{\\beta}^{S}$ (approximately) equals the true causal parameter $\\beta^{\\star}$ on the space of training shift directions $\\hat{S}$ . As conjectured in the anchor regression literature [45, 49, 26] (see, for example, the discussion right after Theorem 3.4 in [26] and Appendix H.3 therein) for $\\gamma\\to\\infty$ , the estimators $\\beta_{\\mathrm{anchor}}^{\\gamma}$ and $\\beta_{\\mathrm{DRIG}}^{\\gamma}$ converge to the causal parameter $\\beta^{\\star}$ on $\\boldsymbol{S}$ . Thus, the identified causal parameter can be estimated as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\beta}^{S}:=\\Pi_{\\hat{S}}\\beta_{\\mathrm{anchor}}^{\\infty}\\quad\\mathrm{or}\\quad\\hat{\\beta}^{S}:=\\Pi_{\\hat{S}}\\beta_{\\mathrm{DRIG}}^{\\infty}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for the setting of mean or mean $^+$ variance shifts, respectively. ", "page_idx": 19}, {"type": "text", "text": "D.2 Consistency of the identifiable robust predictor ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For any estimator $\\beta\\in\\mathbb{R}^{d}$ and given the estimated nuisance parameters $\\hat{\\varphi}:=(\\hat{S}\\hat{S}^{\\top},\\hat{R}\\hat{R}^{\\top},\\hat{\\beta}^{S})$ , we define the sample identifiable robust risk as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\dot{\\boldsymbol{z}}}_{n}(\\beta,\\hat{\\boldsymbol{\\varphi}}):=\\frac{1}{n_{0}}\\sum_{i\\in\\mathcal{D}_{0}}\\left(Y_{0,i}-\\beta^{\\top}X_{0,i}\\right)^{2}+\\gamma\\|\\hat{S}\\hat{S}^{\\top}(\\hat{\\beta}^{S}-\\beta)\\|_{2}^{2}+\\gamma\\left(\\sqrt{C-\\|\\hat{\\beta}^{S}\\|_{2}^{2}}+\\|\\hat{R}\\hat{R}^{\\top}\\beta\\|_{2}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Correspondingly, we define the estimator of the identifiable robust predictor by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\beta}^{\\mathrm{rob,ID}}:=\\underset{\\beta\\in\\mathcal{B}}{\\arg\\operatorname*{min}}\\,\\mathcal{L}_{n}(\\beta,\\hat{\\varphi}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $B\\subseteq\\mathbb{R}^{d}$ is some compact set whose interior contains the true parameter $\\beta^{\\mathrm{rob,ID}}$ . ", "page_idx": 20}, {"type": "text", "text": "To show the consistency of (18), we first require consistency of the nuisance parameter estimators, which we state as an assumption. ", "page_idx": 20}, {"type": "text", "text": "Assumption D.1. The estimated nuisance parameters $\\hat{\\varphi}:=(\\hat{S}\\hat{S}^{\\top},\\hat{R}\\hat{R}^{\\top},\\hat{\\beta}^{S})$ are consistent, that is, for $n\\rightarrow\\infty,$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{S}\\hat{S}^{\\top}-S S^{\\top}\\|_{F}\\xrightarrow{\\mathbb{P}}0,\\quad\\|\\hat{R}\\hat{R}^{\\top}-R R^{\\top}\\|_{F}\\xrightarrow{\\mathbb{P}}0,\\quad\\hat{\\beta}^{S}\\xrightarrow{\\mathbb{P}}\\beta^{S}:=\\Pi_{S}\\beta^{\\star},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where for any matrix $A\\,\\in\\,\\mathbb{R}^{m\\times q}$ , $\\|A\\|_{F}\\,=\\,{\\sqrt{\\operatorname{tr}\\left(A^{\\top}A\\right)}}$ denotes the Frobenius norm, and $S S^{\\top}$ , $R R^{\\top}$ are the corresponding population projection matrices onto $\\Pi_{S}\\mathcal{M},\\Pi_{S^{\\perp}}\\mathcal{M}$ respectively. ", "page_idx": 20}, {"type": "text", "text": "Depending on the assumptions of the data-generating process, Assumption D.1 can be shown to hold. For example, in the anchor regression setting [45], the consistency of the projection matrices $\\hat{S}\\hat{S}^{\\top}$ , $\\hat{R}\\hat{R}^{\\top}$ , and $\\Pi_{\\hat{S}}$ holds if the dimension of $\\boldsymbol{S}$ is known (due to the mean shift structure). The proof relies on the Davis\u2013Kahan theorem (see, for example, [59]) and the consistency of the covariance matrix estimator. Moreover, in the anchor regression setting, it is conjectured that the estimator $\\beta_{\\mathrm{anchor}}^{\\infty}$ converges to its population counterpart (as discussed right after Theorem 3.4 in [26] and Appendix H.3 therein) which implies that $\\hat{\\beta}^{S}:=\\Pi_{\\hat{S}}\\beta_{\\mathrm{anchor}}^{\\infty}$ consistently estimates $\\beta^{S}=\\Pi_{S}\\beta^{\\star}$ . ", "page_idx": 20}, {"type": "text", "text": "Under the assumption of the consistency of the nuisance parameter estimators, we can now show that (18) is a consistent estimator of the identifiable robust predictor. ", "page_idx": 20}, {"type": "text", "text": "Proposition 3. Consider the estimator $\\hat{\\beta}^{\\mathrm{rob,ID}}$ of the identifiable robust predictor defined in (18). Suppose the optimization problem is over a compact set $B\\ {\\stackrel{\\triangledown}{\\subseteq}}\\ \\mathbb{R}^{d}$ whose interior contains the true minimizer $\\beta^{\\mathrm{rob,ID}}$ . Moreover, suppose Assumption $D.I$ holds. Finally, assume that the covariance matrix E $;[X_{0}X_{0}^{\\top}]\\succ0$ with bounded eigenvalues and $\\mathbb{E}\\left[Y_{0}^{2}\\right]<\\infty$ . Then, $\\hat{\\beta}^{\\mathrm{rob,ID}}$ is consistent, i.e., as $:n,n_{0}\\to\\infty$ it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\beta}^{\\mathrm{rob,ID}}\\overset{\\mathbb{P}}{\\rightarrow}\\beta^{\\mathrm{rob,ID}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D.3 Proof of Proposition 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For ease of notation define $\\beta_{0}:=\\beta^{\\mathrm{rob,ID}}$ and $\\hat{\\beta}:=\\hat{\\beta}^{\\mathrm{rob,ID}}$ . For any parameter of interest $\\beta\\in\\mathcal B$ and nuisance parameters $\\boldsymbol{\\varphi}=(P_{S},P_{R},b)$ , define the function ", "page_idx": 20}, {"type": "equation", "text": "$$\n(x,y)\\mapsto g_{\\beta,\\varphi}(x,y):=(y-\\beta^{\\top}x)^{2}+\\gamma\\|P_{S}(b-\\beta)\\|_{2}^{2}+\\gamma\\left(\\sqrt{C-\\|b\\|_{2}^{2}}+\\|P_{R}\\beta\\|_{2}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using (19), the robust identifiable risk and its sample version defined in (17) can be written, respectively as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\beta,\\varphi)=\\mathbb{E}\\left[g_{\\beta,\\varphi}(X_{0},Y_{0})\\right],\\quad\\mathcal{L}_{n}(\\beta,\\varphi)=\\frac{1}{n_{0}}\\sum_{i\\in\\mathcal{D}_{0}}g_{\\beta,\\varphi}(X_{0,i},Y_{0,i}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Our goal is to show that $\\hat{\\beta}\\stackrel{\\mathbb{P}}{\\rightarrow}\\beta_{0}$ . First, we show that the minimum of the loss is well-separated. ", "page_idx": 20}, {"type": "text", "text": "Lemma D.1. Suppose that $\\mathbb{E}\\left[X_{0}X_{0}^{\\top}\\right]\\succ0$ . Then, for all $\\delta>0$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\left\\{{\\mathcal{L}}(\\beta,\\varphi_{0})\\colon\\|\\beta-\\beta_{0}\\|_{2}>\\delta\\right\\}>{\\mathcal{L}}(\\beta_{0},\\varphi_{0}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Fix $\\delta>0$ . From the well-separation of the minimum from Lemma D.1, there exists $\\varepsilon>0$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\{\\|\\hat{\\beta}-\\beta_{0}\\|_{2}>\\delta\\right\\}\\subseteq\\left\\{\\mathcal{L}(\\hat{\\beta},\\varphi_{0})-\\mathcal{L}(\\beta_{0},\\varphi_{0})>\\varepsilon\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Big(\\|\\hat{\\beta}-\\beta_{0}\\|_{2}>\\delta\\Big)\\leq\\mathbb{P}\\Big(\\mathcal{L}(\\hat{\\beta},\\varphi_{0})-\\mathcal{L}(\\beta_{0},\\varphi_{0})>\\varepsilon\\Big)}\\\\ &{\\qquad=\\mathbb{P}\\Big(\\mathcal{L}(\\hat{\\beta},\\varphi_{0})-\\mathcal{L}_{n}(\\hat{\\beta},\\varphi_{0})+\\mathcal{L}_{n}(\\hat{\\beta},\\varphi_{0})-\\mathcal{L}_{n}(\\hat{\\beta},\\hat{\\varphi})}\\\\ &{\\qquad\\qquad\\qquad+\\mathcal{L}_{n}(\\hat{\\beta},\\hat{\\varphi})-\\mathcal{L}_{n}(\\beta_{0},\\hat{\\varphi})+\\mathcal{L}_{n}(\\beta_{0},\\hat{\\varphi})-\\mathcal{L}(\\beta_{0},\\varphi_{0})>\\varepsilon\\Big)}\\\\ &{\\qquad\\leq\\mathbb{P}\\Big(\\mathcal{L}(\\hat{\\beta},\\varphi_{0})-\\mathcal{L}_{n}(\\hat{\\beta},\\varphi_{0})>\\varepsilon/4\\Big)+\\mathbb{P}\\Big(\\mathcal{L}_{n}(\\hat{\\beta},\\varphi_{0})-\\mathcal{L}_{n}(\\hat{\\beta},\\hat{\\varphi})>\\varepsilon/4\\Big)}\\\\ &{\\qquad+\\mathbb{P}\\Big(\\mathcal{L}_{n}(\\hat{\\beta},\\hat{\\varphi})-\\mathcal{L}_{n}(\\beta_{0},\\hat{\\varphi})>\\varepsilon/4\\Big)+\\mathbb{P}\\big(\\mathcal{L}_{n}(\\beta_{0},\\hat{\\varphi})-\\mathcal{L}(\\beta_{0},\\varphi_{0})>\\varepsilon/4\\big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We now want to prove convergence the four terms in (21) and (22). For this, we use the following statements proved in Appendix D.4. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.2. Suppose $B\\subseteq\\mathbb{R}^{d}$ is a compact set. Moreover, assume that the covariance matrix $\\mathbb{E}\\left[X_{0}X_{0}^{\\top}\\right]\\succ0$ with bounded eigenvalues and $\\mathbb{E}\\left[Y_{0}^{2}\\right]<\\infty$ . Then, as $n,n_{0}\\to\\infty$ it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\beta\\in\\mathcal{B}}\\left|\\mathcal{L}_{n}(\\beta,\\varphi_{0})-\\mathcal{L}(\\beta,\\varphi_{0})\\right|\\overset{\\mathbb{P}}{\\rightarrow}0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma D.3. As $n\\to\\infty$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\beta\\in\\mathcal{B}}\\left|\\mathcal{L}_{n}(\\beta,\\hat{\\varphi})-\\mathcal{L}_{n}(\\beta,\\varphi_{0})\\right|\\overset{\\mathbb{P}}{\\rightarrow}0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The two terms in (21) converge to 0 by Lemma D.2 and Lemma D.3, respectively. The first term in (22) equals 0 since $\\hat{\\beta}$ minimizes $\\beta\\mapsto\\mathcal{L}_{n}(\\beta,\\hat{\\varphi})$ . Finally, we observe that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\beta\\in\\mathcal{B}}\\left|\\mathcal{L}_{n}(\\beta,\\hat{\\varphi})-\\mathcal{L}(\\beta,\\varphi_{0})\\right|\\overset{\\mathbb{P}}{\\rightarrow}0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\beta\\in B}|\\mathcal{L}_{n}(\\beta,\\hat{\\varphi})-\\mathcal{L}(\\beta,\\varphi_{0})|\\leq\\operatorname*{sup}_{\\beta\\in B}|\\mathcal{L}_{n}(\\beta,\\hat{\\varphi})-\\mathcal{L}_{n}(\\beta,\\varphi_{0})|+\\operatorname*{sup}_{\\beta\\in B}|\\mathcal{L}_{n}(\\beta,\\varphi_{0})-\\mathcal{L}(\\beta,\\varphi_{0})|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first term converges in probability by Lemma D.3, and the second term converges in probability by Lemma D.2. This implies that the second term in (22) converges to zero. Since $\\delta>0$ was arbitrary, it follows that $\\hat{\\beta}\\stackrel{\\mathbb{P}}{\\rightarrow}\\beta_{0}$ . ", "page_idx": 21}, {"type": "text", "text": "D.4 Proof of auxiliary lemmas ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.4.1 Proof of Lemma D.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "By definition, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\beta,\\varphi_{0})=\\mathbb{E}\\left[(Y_{0}-\\beta^{\\top}X_{0})^{2}\\right]+\\gamma\\Vert S S^{\\top}(\\beta^{S}-\\beta)\\Vert_{2}^{2}+\\gamma\\left(\\sqrt{C-\\Vert\\beta^{S}\\Vert_{2}^{2}}+\\Vert R R^{\\top}\\beta\\Vert_{2}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\mathbb{E}\\left[X_{0}X_{0}^{\\top}\\right]\\succ0$ , the first term is strongly convex in $\\beta$ . Moreover, the second and third terms are convex in $\\beta$ . Therefore, $\\mathcal{L}(\\beta,\\varphi_{0})$ is strongly convex in $\\beta$ . Since $\\mathcal{L}(\\beta,\\varphi_{0})$ is also continuous in $\\beta$ , it follows that there exists a unique global minimum. Let $\\beta_{0}$ denote the global minimizer of $\\mathcal{L}(\\beta,\\varphi_{0})$ . By the fact that $\\mathcal{L}(\\beta_{0},\\varphi_{0})$ is a global minimum, and by definition of strong convexity, there exists a positive constant $m>0$ such that, for all $\\beta\\in\\mathcal B$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\beta,\\varphi_{0})\\geq\\mathcal{L}(\\beta_{0},\\varphi_{0})+\\frac{m}{2}\\|\\beta-\\beta_{0}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Fix $\\delta>0$ . Then, by (26), for all $\\beta\\in\\mathcal B$ such that $\\|\\beta-\\beta_{0}\\|_{2}>\\delta$ it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\beta,\\varphi_{0})\\geq\\mathcal{L}(\\beta_{0},\\varphi_{0})+\\frac{m\\delta^{2}}{2}>\\mathcal{L}(\\beta_{0},\\varphi_{0}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since the inequality holds for all $\\beta\\in\\mathcal B$ such that $\\|\\beta-\\beta_{0}\\|_{2}>\\delta$ , we conclude that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\{{\\mathcal{L}}(\\beta,\\varphi_{0})\\colon\\|\\beta-\\beta_{0}\\|_{2}>\\delta\\}>{\\mathcal{L}}(\\beta_{0},\\varphi_{0}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\delta>0$ was arbitrary, the claim follows. ", "page_idx": 21}, {"type": "text", "text": "D.4.2 Proof of Lemma D.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Recall that for any $\\beta\\in\\mathcal B$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\beta,\\varphi_{0})=\\mathbb{E}\\left[g_{\\beta,\\varphi_{0}}(X_{0},Y_{0})\\right],\\quad\\mathcal{L}_{n}(\\beta,\\varphi_{0})=\\frac{1}{n_{0}}\\sum_{i\\in\\mathcal{D}_{0}}g_{\\beta,\\varphi_{0}}(X_{0,i},Y_{0,i}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To show the result, we must establish that the class of functions $\\{g_{\\beta,\\varphi_{0}}\\colon\\beta\\in B\\}$ is Glivenko\u2013Cantelli. From [56], a set of sufficient conditions for being a Glivenko\u2013Cantelli class is that (i) $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is compact, (ii) $\\beta\\mapsto g_{\\beta,\\varphi_{0}}(x,y)$ is continuous for every $(x,y)$ , and (iii) $\\beta\\mapsto g_{\\beta,\\varphi_{0}}$ is dominated by an integrable function. By assumption, (i) holds. Moreover, by (19), it follows that $\\beta\\mapsto g_{\\beta,\\varphi_{0}}$ is continuous for all $(x,y)$ and thus (ii) holds. We now show that (iii) holds. Since $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is compact we have that $\\begin{array}{r}{\\operatorname*{sup}_{\\beta\\in B}\\|\\beta\\|_{2}=C_{1}<\\infty}\\end{array}$ . For fixed $\\gamma>0$ , and all $(x,y)$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{\\beta,\\varphi_{0}}(x,y)\\leq\\underset{\\beta\\in B}{\\operatorname*{sup}}|g_{\\beta,\\varphi_{0}}(x,y)|}\\\\ &{\\leq\\underset{\\beta\\in B}{\\operatorname*{sup}}(y-\\beta^{\\top}x)^{2}+2\\gamma\\|S S^{\\top}\\|_{F}^{2}\\left(\\|\\beta^{S}\\|_{2}^{2}+\\underset{\\beta\\in B}{\\operatorname*{sup}}\\|\\beta\\|_{2}^{2}\\right)}\\\\ &{\\quad\\quad+\\gamma\\left(\\sqrt{C-\\|\\beta^{S}\\|_{2}^{2}}+\\|R R^{\\top}\\|_{F}\\underset{\\beta\\in B}{\\operatorname*{sup}}\\|\\beta\\|_{2}\\right)^{2}}\\\\ &{\\leq2y^{2}+2C_{1}^{2}\\|x\\|_{2}^{2}+K=:G(x,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $K<\\infty$ is a finite constant not depending on $(x,y)$ . Furthermore, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[G(X_{0},Y_{0})\\right]=2\\mathbb{E}\\left[Y_{0}^{2}\\right]+2C_{1}^{2}\\;\\mathrm{tr}\\left(\\mathbb{E}\\left[X_{0}X_{0}^{\\top}\\right]\\right)+K<\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "since $\\mathbb{E}\\left[Y^{2}\\right]<\\infty$ and $\\mathbb{E}\\left[X_{0}X_{0}^{\\top}\\right]$ has bounded eigenvalues by assumption. From (27) and (28), it follows that (iii) holds. ", "page_idx": 22}, {"type": "text", "text": "D.4.3 Proof of Lemma D.3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For fixed $\\gamma>0$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\gamma}\\underset{\\beta\\in B}{\\operatorname*{sup}}|\\mathcal{L}_{n}(\\beta,\\hat{\\varphi})-\\mathcal{L}_{n}(\\beta,\\varphi_{0})|\\leq\\underset{\\beta\\in B}{\\operatorname*{sup}}\\left|\\|\\hat{S}\\hat{S}^{\\top}(\\hat{\\beta}^{S}-\\beta)\\|_{2}^{2}-\\|S S^{\\top}(\\beta^{S}-\\beta)\\|_{2}^{2}\\right|}\\\\ {+\\underset{\\beta\\in B}{\\operatorname*{sup}}\\left|\\left(\\sqrt{C-\\|\\hat{\\beta}^{S}\\|_{2}^{2}}+\\|\\hat{R}\\hat{R}^{\\top}\\beta\\|_{2}\\right)^{2}-\\left(\\sqrt{C-\\|\\beta^{S}\\|_{2}^{2}}+\\|R R^{\\top}\\beta\\|_{2}\\right)^{2}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We can upper bound (29) as follows, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\beta\\in\\mathcal{B}}{\\operatorname*{sup}}\\left||\\hat{S}\\hat{S}^{\\top}(\\hat{\\beta}^{s}-\\beta)||_{2}^{2}-\\|S S^{\\top}(\\beta^{s}-\\beta)\\|_{2}^{2}\\right|}\\\\ &{=\\underset{\\beta\\in\\mathcal{B}}{\\operatorname*{sup}}\\left|(\\hat{\\beta}^{s}-\\beta)^{\\top}\\hat{S}\\hat{S}^{\\top}(\\hat{\\beta}^{s}-\\beta)-(\\beta^{s}-\\beta)^{\\top}S S^{\\top}(\\beta^{s}-\\beta)\\right|}\\\\ &{=\\underset{\\beta\\in\\mathcal{B}}{\\operatorname*{sup}}\\left|(\\hat{\\beta}^{s}-\\beta)^{\\top}\\hat{S}\\hat{S}^{\\top}(\\hat{\\beta}^{s}-\\beta^{s})+(\\hat{\\beta}^{s}-\\beta^{s})^{\\top}\\hat{S}\\hat{S}^{\\top}(\\beta^{s}-\\beta)\\right.}\\\\ &{\\qquad\\quad\\left.+(\\beta^{s}-\\beta)^{\\top}(\\hat{S}\\hat{S}^{\\top}-S S^{\\top})(\\beta^{s}-\\beta)\\right|}\\\\ &{\\leq2\\underset{\\beta\\in\\mathcal{B}}{\\operatorname*{sup}}\\|\\hat{\\beta}^{s}-\\beta\\|_{2}\\,\\|\\hat{S}\\hat{S}^{\\top}\\|_{F}\\,\\|\\hat{\\beta}^{s}-\\beta^{s}\\|_{2}+\\underset{\\beta\\in B}{\\operatorname*{sup}}\\|\\beta^{s}-\\beta\\|_{2}^{2}\\,\\|\\hat{S}\\hat{S}^{\\top}-S S^{\\top}\\|_{F}}\\\\ &{\\leq C_{1}\\|\\hat{\\beta}^{s}-\\beta^{s}\\|_{2}+C_{2}\\|\\hat{S}\\hat{S}^{\\top}-S S^{\\top}\\|_{F}\\xrightarrow{\\beta}0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (31) follows from the Cauchy\u2013Schwarz inequality and that $\\|A\\|_{2}\\,\\leq\\,\\|A\\|_{F}$ , the constants $C_{1},C_{2}<\\infty$ in (32) follow from compactness of $\\boldsymbol{\\beta}$ , and the convergence in probability follows from ", "page_idx": 22}, {"type": "text", "text": "Assumption D.1. Furthermore, we can upper bound (30) as follows, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{sup}_{\\beta\\in B}\\Bigg|\\Bigg(\\sqrt{C-\\|\\hat{\\beta}^{s}\\|_{2}^{2}}+\\|\\hat{R}\\hat{R}^{\\top}\\beta\\|_{2}\\Bigg)^{2}-\\bigg(\\sqrt{C-\\|\\beta^{s}\\|_{2}^{2}}+\\|R R^{\\top}\\beta\\|_{2}\\bigg)^{2}\\Bigg|}\\\\ &{\\quad=\\operatorname*{sup}_{\\beta\\in B}\\Bigg|C-\\|\\hat{\\beta}^{s}\\|_{2}^{2}+\\|\\hat{R}\\hat{R}^{\\top}\\beta\\|_{2}^{2}+2\\sqrt{C-\\|\\hat{\\beta}^{s}\\|_{2}^{2}}\\,\\|\\hat{R}\\hat{R}^{\\top}\\beta\\|_{2}}\\\\ &{\\qquad\\qquad-C+\\|\\beta^{s}\\|_{2}^{2}-\\|R R^{\\top}\\beta\\|_{2}^{2}-2\\sqrt{C-\\|\\beta^{s}\\|_{2}^{2}}\\,\\|R R^{\\top}\\beta\\|_{2}\\Bigg|}\\\\ &{\\quad\\leq\\operatorname*{sup}_{\\beta\\in B}\\bigg|\\|\\hat{\\beta}^{s}\\|_{2}^{2}-\\|\\beta^{s}\\|_{2}^{2}\\bigg|+\\operatorname*{sup}_{\\beta}\\bigg|\\beta^{\\top}(\\hat{R}\\hat{R}^{\\top}-R R^{\\top})\\beta\\bigg|}\\\\ &{\\qquad+2\\operatorname*{sup}\\bigg|\\sqrt{C-\\|\\hat{\\beta}^{s}\\|_{2}^{2}}\\,\\|\\hat{R}\\hat{R}^{\\top}\\beta\\|_{2}-\\sqrt{C-\\|\\beta^{s}\\|_{2}^{2}}\\,\\|R R^{\\top}\\beta\\|_{2}\\bigg|}\\\\ &{\\quad=(I)+(I I I).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Assumption D.1, $(I)$ converges in probability to zero. Regarding $(I I)$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\beta\\in{\\mathcal{B}}}\\left|\\beta^{\\top}(\\hat{R}\\hat{R}^{\\top}-R R^{\\top})\\beta\\right|\\leq\\operatorname*{sup}_{\\beta\\in{\\mathcal{B}}}\\lVert\\beta\\rVert_{2}^{2}\\,\\lVert\\hat{R}\\hat{R}^{\\top}-R R^{\\top}\\rVert_{F}\\stackrel{\\mathbb{P}}{\\rightarrow}0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the inequality follows from Cauchy\u2013Schwarz and that $\\|A\\|_{2}\\leq\\|A\\|_{F}$ , and the convergence in probability follows from Assumption D.1 along with the compactness of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . It remains to upper bound $(I I I)$ . We have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{(I I I)}{2}\\leq\\underset{\\beta\\in\\mathbb{R}}{\\operatorname*{sup}}\\left|\\sqrt{C-\\|\\hat{\\beta}^{\\delta}\\|_{2}^{2}}\\,\\|\\hat{R}\\hat{R}^{\\gamma}\\beta\\|_{2}-\\sqrt{C-\\|\\beta^{\\delta}\\|_{2}^{2}}\\,\\|\\hat{R}\\hat{R}^{\\gamma}\\beta\\|_{2}\\right|}&{}\\\\ &{\\quad+\\underset{\\beta\\in\\mathbb{R}}{\\operatorname*{sup}}\\left|\\sqrt{C-\\|\\beta^{\\delta}\\|_{2}^{2}}\\,\\|\\hat{R}\\hat{R}^{\\gamma}\\beta\\|_{2}-\\sqrt{C-\\|\\beta^{\\delta}\\|_{2}^{2}}\\,\\|R R^{\\tau}\\beta\\|_{2}\\right|}\\\\ &{\\leq\\bigg(\\underset{\\beta\\in\\mathbb{R}}{\\operatorname*{sup}}\\big\\|\\beta\\|_{2}\\,\\|\\hat{R}\\hat{R}^{\\tau}\\|_{F}\\bigg)\\,\\left|\\sqrt{C-\\|\\hat{\\beta}^{\\delta}\\|_{2}^{2}}-\\sqrt{C-\\|\\beta^{\\delta}\\|_{2}^{2}}\\right|}\\\\ &{\\quad+\\underset{\\beta\\in\\mathbb{R}}{\\operatorname*{sup}}\\left|\\sqrt{\\beta^{\\tau}\\hat{R}\\hat{R}^{\\tau}\\hat{\\beta}}-\\sqrt{\\beta^{\\gamma}R R^{\\tau}\\hat{\\beta}}\\right|\\bigg(\\sqrt{C-\\|\\beta^{\\delta}\\|_{2}^{2}}\\bigg)}\\\\ &{\\leq C_{3}\\left|\\|\\beta^{\\delta}\\|_{2}^{2}+\\|\\hat{\\beta}^{\\delta}\\|_{2}^{2}\\right|^{1/2}+\\sqrt{C}\\underset{\\beta\\in\\mathbb{R}}{\\operatorname*{sup}}\\left|\\beta^{\\tau}(\\hat{R}\\hat{R}^{\\tau}-R R^{\\tau})\\beta\\right|^{1/2}}\\\\ &{\\leq C_{3}\\left|\\|\\beta^{\\delta}\\|_{2}^{2}+\\|\\hat{\\beta}^{\\delta}\\|_{2}^{2}\\right|^{1/2}+\\sqrt{C}\\,\\underset{\\beta\\in\\mathbb{R}}{\\operatorname*{sup}}\\big\\|\\beta\\|_{2}^{2}\\,\\|\\hat{R}\\hat{R}^{\\tau}-R R^{\\tau}\\|_{F}\\bigg)^{1/2}\\overset{\\delta}{\\geq}0\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The inequality in (33) follows from the compactness of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , the fact that $\\hat{R}\\hat{R}^{\\top}$ has bounded eigenvalues, and that $|{\\sqrt{x}}-{\\sqrt{y}}|\\leq|x-y|^{1/2}$ for all $x,y\\geq0$ . The inequality in (34) follows from Cauchy\u2013 Schwarz and that $\\|A\\|_{2}\\leq\\|A\\|_{F}$ . The convergence in probability follows form Assumption D.1 and the compactness of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ . ", "page_idx": 23}, {"type": "text", "text": "E Details on finite-sample experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we provide more details of the data generation for our synthetic finite-sample experiments as well as data processing for the real-world data experiments. ", "page_idx": 23}, {"type": "text", "text": "E.1 Synthetic experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For the synthetic experiments, we generate a random SCM which satisfies our assumptions. For $d\\ =\\ 15$ , we randomly sample the joint covariance $\\Sigma^{\\star}$ of $(X,Y)$ , fixing its total variance and the eigenvalues. We consider 7 environments including the reference environment, and for each environment except the reference, we randomly generate mean shifts of fixed norm. Since we have 6 non-zero random Gaussian mean shifts, it holds a.s. that $\\dim S=6$ . We then randomly generate an \"initial guess\" for $\\beta^{\\star}\\in\\mathbb{R}^{d}$ of fixed norm $C=10$ . Now, with respect to the space $\\boldsymbol{S}$ of the identifiable directions induced by the mean shifts, we choose the most \"adversarial\" causal parameter $\\beta_{\\mathrm{adv}}^{\\star}$ which is equal to $\\beta^{\\star}$ on $\\boldsymbol{S}$ , but on $S^{\\perp}$ has the opposite direction of the noise OLS estimator $\\bar{\\Sigma_{\\eta}^{\\star}}^{-1}\\Sigma_{\\eta,\\xi}^{\\star}$ . We ensure that $\\lVert\\beta_{\\mathrm{adv}}^{\\star}\\rVert_{2}=C$ . Note that under the observed shifts, $\\beta^{\\star}$ and $\\beta_{\\mathrm{adv}}^{\\star}$ are observationally equivalent. We complete $\\beta_{\\mathrm{adv}}^{\\star}$ to the set $\\theta_{\\mathrm{adv}}$ of observationally equivalent model parameters and generate the multi-environment training data according to $\\theta_{\\mathrm{adv}}$ and the collection of mean shifts. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "For Figure 3 (left), we define the test shift upper bound as $M_{\\mathrm{anchor}}\\,=\\,\\gamma S S^{\\top}$ , where $S$ is taken to be a two-dimensional subspace of $\\boldsymbol{S}$ . We vary $\\gamma$ from 0 to 10, and for each $\\gamma$ , we compute the oracle anchor regression estimator by minimizing the discrete anchor regression loss. Additionally, we compute the pooled OLS estimator and the identifiable robust predictor $\\beta^{\\mathrm{rob,ID}}$ as described in Appendix D. Finally, we generate test data with a Gaussian additive shift $A_{\\mathrm{test}}\\sim\\mathcal{N}(0,M_{\\mathrm{anchor}})$ . We evaluate the loss of $\\beta_{\\mathrm{OLS}}$ , $\\beta_{\\mathrm{anchor}}$ and $\\beta^{\\mathrm{rob,ID}}$ on this test environment and include the population lower bound. ", "page_idx": 24}, {"type": "text", "text": "For Figure 3 (right), we define the test shift upper bound as $M_{\\mathrm{new}}=\\gamma S S^{\\top}+\\gamma^{\\prime}R R^{\\top}$ , where $R$ is a 2-dimensional subspace of the space $S^{\\perp}$ and we set $\\gamma^{\\prime}=0.05\\gamma$ to showcase the effect of small unseen shifts compared to large identified shifts. We vary $\\gamma$ from 0 to 40 and for each $\\gamma$ , compute the oracle anchor regression estimator by minimizing the discrete anchor regression loss. Additionally, we compute the pooled OLS estimator and the identifiable robust predictor $\\beta^{\\mathrm{rob,ID}}$ as described in Appendix D, for which we use knowledge of spaces $S$ and $R$ and prior knowledge of $M_{\\mathrm{new}}$ . Finally, we generate test data with a Gaussian additive shift $A_{\\mathrm{test}}\\,{\\widetilde{\\mathbf{\\Gamma}}}\\sim{\\mathcal{N}}(0,M_{\\mathrm{new}}{\\widetilde{\\mathbf{\\Gamma}}})$ . We evaluate the loss of $\\beta_{\\mathrm{OLS}}$ , $\\beta_{\\mathrm{anchor}}$ and $\\beta^{\\mathrm{rob,ID}}$ on this test environment, plot the resulting test losses for different estimators and include the population lower bound. ", "page_idx": 24}, {"type": "text", "text": "E.2 Real-world data experiments ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "G2dYZJO4BE/tmp/90a5d84a2cbbe150bd90c17dab6ea4c45e5e591f3237efc43c57b5c722d3e05e.jpg", "img_caption": ["Figure 5: The figures illustrate the structure of the (a) training-time shifts and (b-c) test-time shifts for different perturbation strengths on the example of two covariates. Panel (a) shows the training data containing two environments\u2013observational (blue) and shifted (orange) corresponding to the knockout of the gene ENSG00000089009. Panels (b) and (c) show the training data in grey and test data from a previously unseen environment (green). Panel (b) depicts the top $10\\%$ test data points closest to the training support (perturbation strength $=0.1$ ). Panel (c) illustrates the full test data (perturbation strength $=1.0$ ). "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "We consider the K562 dataset from [41] and perform the preprocessing as done in [13]. The resulting dataset consists of $n=162,751$ single-cell observations over $d=622$ genes collected from observational and several interventional environments. The interventional environments arise by knocking down a single gene at a time using the CRISPR interference method [40]. Following [48], we select only always-active genes in the observational setting, resulting in a smaller dataset of 28 genes. For each gene $j=1,\\ldots,28$ , we set $Y:=X_{j}$ as the target variable and select the three genes $X_{k_{1}},\\ldots,X_{k_{3}}$ most strongly correlated with $Y$ (using Lasso), resulting in a dataset with columns $Y,X_{k_{1}},\\ldots,X_{k_{3}}$ . Given this dataset, we construct the training and test environments as follows. Let $\\scriptscriptstyle\\mathcal{O}$ denote the 10,691 observations collected from the observational environment, and let $\\mathcal{T}_{i}$ denote the observations collected from the interventional environment where the gene $k_{i}$ was knocked down. We will denote by $\\mathcal{T}_{i,s}$ the $s$ -th quantile of datapoints in ${\\mathcal{T}}_{i}$ w.r.t. to the expression value of the gene $k_{i}$ . These are the $s\\times100\\%$ of datapoints with the weakest shift compared to the observational mean of the gene $k_{i}$ , and thus the parameter $s\\in[0,1]$ is a proxy for the strength of the shift. Furthermore, denote by ${\\mathcal{T}}_{i,s}^{*}$ a random sample of $\\mathcal{T}_{i,s}$ of a certain size. For each $i\\in\\{1,2,3\\}$ , we train the methods on $D_{i}^{\\mathrm{train}}:=O\\cup T_{i,1}^{*}$ , with $|\\mathcal{T}_{i,1}^{*}|=20$ . An illustration of the training data $\\mathcal{D}_{i}^{\\mathrm{train}}$ is shown in panel (a) of Figure 5. For each shift strength $s\\in\\{0.1,\\ldots,1\\}$ we evaluate the models on the test samples from the three interventional environments. An example of the test data for different shift strengths $s$ and a previously unseen direction is shown in Figure $5(\\mathsf{b-c})$ . Figure 4 shows the test MSE performance as a function of perturbation strength. We compare our method Rob-ID, defined as the minimizer of the empirical identifiable robust risk (17), with anchor regression [45], invariant causal prediction (ICP) [38], Distributional Robustness via Invariant Gradients (DRIG) [49], and OLS (corresponding to vanilla ERM). We use the following parameters for Rob-ID: $\\gamma=50$ , $C_{\\mathrm{ker}}=1.0$ , and $M=\\mathrm{Id}$ . For anchor regression and DRIG, we select $\\gamma=50$ . For ICP, we set the significance level for the invariance tests to $\\alpha=0.05$ . ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "These numerical experiments are computationally light and can be run in $\\approx5$ minutes on a personal laptop.13 ", "page_idx": 25}, {"type": "text", "text": "F Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "F.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For every environment $e\\,\\in\\,\\mathcal{E}_{\\mathrm{train}}$ , we observe the first moments $\\mathbb{E}\\left(X_{e}\\right)$ and $\\mathbb{E}\\left(Y_{e}\\right)$ , and second moments $\\mathbb{E}\\,(X_{e}X_{e}^{\\top}),\\mathbb{E}\\,(Y_{e}^{2})$ and $\\mathbb{E}\\left(X_{e}Y_{e}\\right)$ . Since it holds by assumption that $\\mu_{0}=0$ and $\\Sigma_{0}=0$ , we have that $\\mathbb{E}\\left(X_{0}X_{0}^{\\top}\\right)=\\Sigma_{\\eta}^{\\star}$ , and so we can identify $\\Sigma_{\\eta}^{\\star}$ uniquely. Furthermore, it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left(X_{0}Y_{0}\\right)=\\Sigma_{\\eta}^{\\star}\\beta^{\\star}+\\Sigma_{\\eta,\\xi}^{\\star},}\\\\ &{\\mathbb{E}\\left(X_{e}Y_{e}\\right)=(\\Sigma_{e}+\\mu_{e}\\mu_{e}^{\\top}+\\Sigma_{\\eta}^{\\star})\\beta^{\\star}+\\Sigma_{\\eta,\\xi}^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By taking the difference between Equation (36) and Equation (35), we can identify $(\\Sigma_{e}+\\mu_{e}\\mu_{e}^{\\top})\\beta^{\\star}$ Thus, the causal parameter $\\beta^{\\star}$ is identifiable on the subspace $\\boldsymbol{S}$ defined in Equation (10) and is not identifiable on its orthogonal complement $S^{\\perp}$ . Thus, for any for vector $\\alpha\\in S^{\\perp}$ , the vector $\\beta=\\beta^{\\star}+\\alpha$ is consistent with the data-generating process. It remains to compute the covariance parameters induced by an arbitrary $\\tilde{\\beta}:=\\beta^{\\star}+\\alpha$ , for $\\alpha\\in S^{\\perp}$ . For every environment $e\\in\\mathcal{E}_{\\mathrm{train}}$ , the second mixed moment between $X_{e}$ and $Y_{e}$ has to satisfy the following equality ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left(X_{e}Y_{e}\\right)=(\\Sigma_{e}+\\mu_{e}\\mu_{e}^{\\top}+\\Sigma_{\\eta}^{\\star})\\beta^{\\star}+\\Sigma_{\\eta,\\xi}^{\\star}=\\left(\\Sigma_{e}+\\mu_{e}\\mu_{e}^{\\top}+\\Sigma_{\\eta}^{\\star}\\right)\\tilde{\\beta}+\\tilde{\\Sigma}_{\\eta,\\xi},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "from which it follows that $\\tilde{\\Sigma}_{\\eta,\\xi}:=\\Sigma_{\\eta,\\xi}^{\\star}-\\Sigma_{\\eta}^{\\star}\\alpha$ . By computing $\\mathbb{E}\\left(Y_{e}^{2}\\right)$ and inserting $\\tilde{\\beta}=\\beta^{\\star}+\\alpha$ and $\\tilde{\\Sigma}_{\\eta,\\xi}$ , we similarly obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{\\sigma}_{\\xi}^{2}:=(\\sigma_{\\xi}^{\\star})^{2}-2\\alpha^{\\top}\\Sigma_{\\eta,\\xi}^{\\star}+\\alpha^{\\top}\\Sigma_{\\eta}^{\\star}\\alpha.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, we obtain the following set of observationally equivalent model parameters consistent with P\u03b8\u22c6,Etrain: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta_{\\mathrm{eq}}=\\{\\beta^{\\star}+\\alpha,\\Sigma_{\\eta}^{\\star},\\Sigma_{\\eta,\\xi}^{\\star}-\\Sigma_{\\eta}^{\\star}\\alpha,(\\sigma_{\\xi}^{\\star})^{2}-2\\alpha^{\\top}\\Sigma_{\\eta,\\xi}^{\\star}+\\alpha^{\\top}\\Sigma_{\\eta}^{\\star}\\alpha\\colon\\alpha\\in S^{\\perp}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since the observationally equivalent set is identifiable from the training distribution, but model parameters $\\beta^{\\star},\\Sigma_{\\eta,\\xi}^{\\star},(\\sigma_{\\xi}^{\\star})^{2}$ are not, it is helpful to re-express the observationally equivalent set through identifiable quantities. For this, we note that the \"identified causal predictor\" $\\beta^{S}=\\beta^{\\star}-\\beta^{S^{\\perp}}$ induces an observationally equivalent model given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\quad:\\mathcal{S}:=(\\beta^{S},\\Sigma_{\\eta}^{S},\\Sigma_{\\eta,\\xi}^{S},(\\sigma_{\\xi}^{S})^{2})=(\\beta^{S},\\Sigma_{\\eta}^{\\star},\\Sigma_{\\eta,\\xi}^{\\star}+\\Sigma_{\\eta}^{\\star}\\beta^{S^{\\perp}},(\\sigma_{\\xi}^{\\star})^{2}+2\\langle\\Sigma_{\\eta,\\xi}^{\\star},\\beta^{S^{\\perp}}\\rangle+\\langle\\beta^{S^{\\perp}},\\Sigma_{\\eta}^{\\star}\\beta^{S^{\\perp}}\\rangle).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From this reparameterization, we infer the final form of the observationally equivalent set : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta_{\\mathrm{eq}}=\\{\\beta^{S}+\\alpha,\\Sigma_{\\eta}^{\\prime},\\Sigma_{\\eta,\\xi}^{S}-\\Sigma_{\\eta}^{\\prime}\\alpha,(\\sigma_{\\xi}^{S})^{2}-2\\alpha^{\\top}\\Sigma_{\\eta,\\xi}^{S}+\\alpha^{\\top}\\Sigma_{\\eta}^{\\prime}\\alpha\\colon\\alpha\\in\\mathcal{S}^{\\perp}\\}\\ni\\theta^{\\star}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, Equation (12) follows. To find the robust predictor $\\beta^{r o b}$ , we write down the robust loss with respect to $M_{\\mathrm{test}}$ and any $\\theta_{\\alpha}$ from the observationally equivalent set : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta_{\\alpha},M_{\\mathrm{test}})=(\\beta^{S}+\\alpha-\\beta)^{\\top}\\big(M_{\\mathrm{test}}+\\Sigma_{\\eta}^{\\star}\\big)(\\beta^{S}+\\alpha-\\beta)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,2(\\beta^{S}+\\alpha-\\beta)^{\\top}(\\Sigma_{\\eta,\\xi}^{\\star}-\\Sigma_{\\eta}^{\\star}\\alpha)+(\\sigma_{\\xi}^{S})^{2}-2\\alpha^{\\top}\\Sigma_{\\eta,\\xi}^{S}+\\alpha^{\\top}\\Sigma_{\\eta}^{\\star}\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "inserting $\\alpha\\in S^{\\perp}$ and rearranging, Equation (13) follows. ", "page_idx": 26}, {"type": "text", "text": "F.2 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We structure the proof as follows: first, we quantify the non-identifiability of the robust risk by explicitly computing its supremum over the observationally equivalent set of the observationally equivalent model parameters (referred to as the identifiable robust risk). Second, we derive a lower bound for the identifiable robust risk by considering two cases depending on how a predictor $\\overline{{\\beta}}$ interacts with the possible test shifts $M_{\\mathrm{test}}$ . In this proof, we use more general notation, with the test shifts bounded by a PSD matrix $M_{\\mathrm{test}}\\preceq\\gamma M+\\dot{\\gamma^{\\prime}}R R^{\\top}$ , which range $M\\subset S$ and rang $:R\\subset S^{\\perp}$ . The statement of the theorem follows by setting $\\gamma=\\gamma^{\\prime}$ . However, we believe that the more refined statement is useful, e.g., when one expects strong shifts in training directions and only weak \"new\" shifts. ", "page_idx": 26}, {"type": "text", "text": "Computation of the identifiable robust risk. For any model-generating parameter $\\theta=(\\beta,\\Sigma)$ it holds that the robust risk of the model Equation (3) under test shifts $M_{\\mathrm{test}}\\succeq0$ is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{\\mathrm{rob}}(\\overline{{\\beta}};\\theta,M_{\\mathrm{test}})=(\\beta-\\overline{{\\beta}})^{\\top}(M_{\\mathrm{test}}+\\Sigma_{\\eta}^{\\star})(\\beta-\\overline{{\\beta}})+2(\\beta-\\overline{{\\beta}})^{\\top}\\Sigma_{\\eta,\\xi}+(\\sigma_{\\xi})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We recall that the observationally equivalent set of model parameters after observing the multienvironment training data Equation (3) is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta_{\\mathrm{eq}}=\\{\\beta^{\\mathcal{S}}+\\alpha,\\Sigma_{\\eta}^{\\star},\\Sigma_{\\eta,\\xi}^{\\mathcal{S}}-\\Sigma_{\\eta}^{\\star}\\alpha,(\\sigma_{\\xi}^{\\mathcal{S}})^{2}-2\\alpha^{\\top}\\Sigma_{\\eta,\\xi}^{\\mathcal{S}}+\\alpha^{\\top}\\Sigma_{\\eta}\\alpha:\\alpha\\in\\mathcal{S}^{\\perp}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\boldsymbol{S}$ is the span of identified directions defined in Equation (10). Moreover, we recall that by Assumption 3.1, for any causal parameter $\\beta$ it should hold that $\\lVert\\beta\\rVert_{2}=\\lVert\\beta^{S}+\\alpha\\rVert_{2}\\leq C$ , which translates into the following constraint for the parameter $\\alpha$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\alpha\\|_{2}\\leq\\sqrt{C^{2}-\\|\\beta^{S}\\|_{2}^{2}}=:C_{\\mathrm{ker}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Inserting Equation (37) in Equation (8), we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathrm{rob,ID}}(\\overline{\\beta};\\Theta_{\\mathrm{eq}},M_{\\mathrm{test}})=\\operatorname*{sup}_{\\alpha\\in\\mathcal{S}^{\\perp}\\atop\\|\\alpha\\|_{2}\\leq C_{\\mathrm{ker}}}\\mathcal{R}_{\\mathrm{rob}}(\\overline{\\beta};\\theta_{\\alpha},M_{\\mathrm{test}}),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\theta_{\\alpha}$ is a short notation for $(\\beta^{S}+\\alpha,\\Sigma_{\\eta}^{\\star},\\Sigma_{\\eta,\\xi}^{S}-\\Sigma_{\\eta}^{\\star}\\alpha,(\\sigma_{\\xi}^{S})^{2}-2\\alpha^{\\top}\\Sigma_{\\eta,\\xi}^{S}+\\alpha^{\\top}\\Sigma_{\\eta}^{\\star}\\alpha)$ . We now compute the supremum explicitly in case $M_{\\mathrm{test}}$ has the form $M_{\\mathrm{test}}=\\gamma M+\\gamma^{\\prime}R R^{\\top}$ , where $M$ is a PSD matrix with range $M\\subseteq S$ and $R$ is a semi-orthogonal matrix with range $R\\subseteq S^{\\perp}$ . Note that this assumption includes both the setting of Theorem 3.1 and the setting of finite robustness methods in Section 3.2. For any $\\alpha\\in S^{\\perp}$ , we write down the robust loss as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{\\mathrm{rob}}(\\overline{{\\beta}};\\theta_{\\alpha},M_{\\mathrm{test}})=(\\beta^{S}-\\overline{{\\beta}})^{\\top}\\big(M_{\\mathrm{test}}+\\Sigma_{\\eta}^{\\star}\\big)(\\beta^{S}-\\overline{{\\beta}})+2(\\beta^{S}-\\overline{{\\beta}})^{\\top}\\Sigma_{\\eta,\\xi}^{S}+(\\sigma_{\\xi}^{S})^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\alpha^{\\top}M_{\\mathrm{test}}\\alpha+2\\alpha^{\\top}M_{\\mathrm{test}}(\\beta^{S}-\\overline{{\\beta}})}\\\\ &{\\qquad\\qquad\\qquad=\\mathcal{R}_{\\mathrm{rob}}(\\overline{{\\beta}};\\theta^{S},M_{\\mathrm{test}})+\\alpha^{\\top}M_{\\mathrm{test}}\\alpha+2\\alpha^{\\top}M_{\\mathrm{test}}(\\beta^{S}-\\overline{{\\beta}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The first term is the robust loss of $\\overline{{\\beta}}$ under test shift $M_{\\mathrm{test}}$ and the identified model-generating parameter $\\theta^{S}$ , thus it does not depend on $\\alpha$ . By the structure of $M_{\\mathrm{test}}$ , we obtain that ", "page_idx": 26}, {"type": "equation", "text": "$$\nf(\\alpha):=\\alpha^{\\top}M_{\\mathrm{test}}\\alpha+2\\alpha^{\\top}M_{\\mathrm{test}}(\\beta^{S}-\\overline{{\\beta}})=\\gamma^{\\prime}\\alpha^{\\top}R R^{\\top}\\alpha-\\alpha^{\\top}R R^{\\top}\\overline{{\\beta}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "If $R=0$ , i.e., the test shifts consist only of the identified directions, we have $f(\\alpha)=0$ , independently of $\\alpha$ , and thus ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{\\mathrm{rob,ID}}(\\overline{{\\beta}};\\Theta_{\\mathrm{eq}},M_{\\mathrm{test}})=\\mathcal{R}_{\\mathrm{rob}}(\\overline{{\\beta}};\\theta^{S},M_{\\mathrm{test}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This implies the first statement of the theorem. ", "page_idx": 27}, {"type": "text", "text": "We now consider the case where $R\\neq0$ , i.e., $R R^{\\top}$ is a non-degenerate projection. Our goal is to maximize $f(\\alpha)$ subject to constraints $\\alpha\\in S^{\\perp}$ , $\\lVert\\alpha\\rVert_{2}\\leq C_{\\mathrm{ker}}$ . Let $\\tilde{R}$ be an orthonormal extension of $R$ such that range $(R|\\tilde{R})=S^{\\perp}$ . Then, we can parameterize $\\alpha\\in S^{\\perp}$ as $\\begin{array}{r}{\\alpha=(R|\\tilde{R})(\\frac{w}{\\tilde{w}})}\\end{array}$ and the corresponding Lagrangian reads ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\alpha,\\lambda)=\\gamma^{\\prime}\\alpha^{\\top}R R^{\\top}\\alpha-\\alpha^{\\top}R R^{\\top}\\overline{{\\beta}}+\\lambda(C_{\\mathrm{ker}}^{2}-\\|\\alpha\\|_{2}^{2})}\\\\ {=\\gamma^{\\prime}\\|w\\|_{2}^{2}-w^{\\top}R^{\\top}\\overline{{\\beta}}+\\lambda(C_{\\mathrm{ker}}^{2}-\\|(w,\\tilde{w})\\|_{2}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Differentiating with respect to $w,\\tilde{w}$ yields ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle w=\\frac{\\gamma^{\\prime}}{\\gamma^{\\prime}-\\lambda}R^{\\top}\\overline{{{\\beta}}};}}\\\\ {{\\displaystyle\\tilde{w}=0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "After differentiating w.r.t. $\\lambda$ , we obtain $\\begin{array}{r}{\\frac{\\gamma^{\\prime}}{\\gamma^{\\prime}-\\lambda}=\\pm\\frac{C_{\\mathrm{ker}}}{\\|R^{\\top}\\overline{{\\beta}}\\|_{2}}}\\end{array}$ . By inserting in the objective function and comparing, we obtain the value of the identifiable robust risk: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{\\mathrm{rob,ID}}(\\overline{\\beta};\\Theta_{\\mathrm{eq}},M_{\\mathrm{test}})=\\gamma^{\\prime}C_{\\mathrm{ker}}^{2}+2\\gamma^{\\prime}\\|R^{\\top}\\overline{\\beta}\\|_{2}+\\mathcal{R}_{\\mathrm{rob}}(\\overline{\\beta};\\theta^{S},M_{\\mathrm{test}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Putting together the cases $R=0$ and $R\\neq0$ , and plugging in $M_{\\mathrm{test}}=\\gamma S S^{\\top}+\\gamma^{\\prime}R R^{\\top}$ , we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{\\mathrm{rob},\\mathrm{ID}}(\\overline{\\beta};\\Theta_{\\mathrm{eq}},M_{\\mathrm{test}})=\\gamma/\\mathbb{I}_{R\\neq0}(C_{\\mathrm{ker}}+\\|R^{\\top}\\overline{\\beta}\\|_{2})^{2}+\\mathcal{R}_{\\mathrm{rob}}(\\overline{\\beta};\\theta^{S},\\gamma M)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\gamma/\\mathbb{I}_{R\\neq0}(C_{\\mathrm{ker}}+\\|R^{\\top}\\overline{\\beta}\\|_{2})^{2}+\\gamma\\|S^{\\top}(\\beta^{S}-\\overline{\\beta})\\|_{2}^{2}+\\mathcal{R}_{0}(\\beta^{S},\\overline{\\beta}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\mathcal{R}_{\\mathrm{rob}}(\\overline{{\\beta}};\\theta^{S},\\gamma M)$ is the robust risk of the estimator $\\overline{{\\beta}}$ w.r.t. the \"identified\" test shift $\\gamma M$ and the identified model parameter $\\theta^{S}$ , whereas $\\mathcal{R}_{0}(\\theta^{S},\\overline{{\\beta}})$ is the risk of $\\overline{{\\beta}}$ on the reference environment $e=0$ . ", "page_idx": 27}, {"type": "text", "text": "Derivation of the lower bound for the identifiable robust risk. Now that we have explicitly computed the identifiable robust risk, we devote ourselves to the computation of the lower bound for its best possible value ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\overline{{\\beta}}\\in\\mathbb{R}^{d}}\\mathcal{R}_{\\mathrm{rob,ID}}\\big(\\overline{{\\beta}};\\Theta_{\\mathrm{eq}},M_{\\mathrm{test}}\\big).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In this part, we will only consider the case $R\\neq0$ , since the case $R=0$ corresponds to the (discrete) anchor regression-like setting, where both the robust risk and its minimizer are uniquely identifiable, and computable from training data. We will distinguish between two cases. ", "page_idx": 27}, {"type": "text", "text": "Case 1: $\\|\\boldsymbol{R}^{\\top}\\overline{{\\boldsymbol{\\beta}}}\\|_{2}=0$ . In this case, $\\overline{{\\beta}}$ is fully located in the orthogonal complement of $R$ , which consists of $\\boldsymbol{S}$ and $\\tilde{R}$ . We will denote (the basis of) this subspace by $S_{\\mathrm{tot}}={\\cal S}\\oplus\\tilde{\\cal R}$ . Thus, $S_{\\mathrm{tot}}$ is the \"total\" stable subspace consisting of identified directions in $\\boldsymbol{S}$ and non-identified, but unperturbed directions $\\tilde{R}$ . We will parameterize $\\overline{{\\beta}}$ as $\\overline{{\\beta}}=S_{\\mathrm{tot}}w$ . Thus, we are looking to solve the optimization problem ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\beta^{\\mathrm{rob,ID}}=\\operatorname*{arg\\,min}_{w}\\big(\\beta^{\\mathcal{S}}-S_{\\mathrm{tot}}w\\big)^{\\top}\\big(\\gamma S S^{\\top}+\\Sigma_{\\eta}^{\\star}\\big)\\big(\\beta^{\\mathcal{S}}-S_{\\mathrm{tot}}w\\big)+2\\big(\\beta^{\\mathcal{S}}-S_{\\mathrm{tot}}w\\big)^{\\top}\\Sigma_{\\eta,\\xi}^{\\mathcal{S}}+\\big(\\sigma_{\\xi}^{\\mathcal{S}}\\big)^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Setting the gradient to zero yields the asymptotic identifiable robust estimator ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta^{\\mathrm{rob,ID}}=\\beta^{\\mathcal{S}}+S_{\\mathrm{tot}}[S_{\\mathrm{tot}}^{\\top}(\\gamma S S^{\\top}+\\Sigma_{\\eta})S_{\\mathrm{tot}}]^{-1}S_{\\mathrm{tot}}^{\\top}\\Sigma_{\\eta,\\xi}^{S}}\\\\ {=\\beta^{\\mathcal{S}}+S_{\\mathrm{tot}}[\\gamma\\mathrm{Id}_{S_{\\mathrm{tot}}}+S_{\\mathrm{tot}}^{\\top}\\Sigma_{\\eta}S_{\\mathrm{tot}}]^{-1}S_{\\mathrm{tot}}^{\\top}\\Sigma_{\\eta,\\xi}^{S},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which corresponds to the loss value of ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{I}_{\\mathrm{rob},\\mathrm{ID}}(\\beta^{\\mathrm{rob},\\mathrm{ID}};\\Theta_{\\mathrm{eq}},M_{\\mathrm{test}})=\\gamma^{\\prime}C_{\\mathrm{ker}}^{2}+(\\sigma_{\\xi}^{S})^{2}-2\\Sigma_{\\eta,\\xi}^{S}{^\\top}S_{\\mathrm{tot}}[S_{\\mathrm{tot}}^{\\top}(\\gamma S S^{\\top}+\\Sigma_{\\eta})S_{\\mathrm{tot}}]^{-1}S_{\\mathrm{tot}}^{\\top}\\Sigma_{\\eta,\\xi}^{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Case 2: $\\|\\boldsymbol{R}^{\\top}\\overline{{\\boldsymbol{\\beta}}}\\|_{2}\\neq0$ . Since for $\\|\\boldsymbol{R}^{\\top}\\overline{{\\boldsymbol{\\beta}}}\\|_{2}\\neq0$ , the objective function is differentiable, we compute its gradient to be ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}\\mathcal{R}_{\\mathrm{rob,ID}}(\\beta;\\Theta_{\\mathrm{eq}},M_{\\mathrm{test}})=2\\gamma^{\\prime}R R^{\\top}\\beta/\\|R R^{\\top}\\beta\\|+2\\gamma^{\\prime}R R^{\\top}\\beta+\\nabla\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta^{\\mathcal{S}},\\gamma M)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=2\\gamma^{\\prime}R R^{\\top}\\beta/\\|R R^{\\top}\\beta\\|+2\\gamma^{\\prime}R R^{\\top}\\beta+2(\\Sigma_{\\eta}^{\\star}+\\gamma M)(\\beta-\\beta^{S})-2\\Sigma_{\\eta,\\xi}^{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This equation is, in general, not solvable w.r.t. $\\beta$ in closed form. Instead, we provide the limit of the optimal value of the function when the strength of the unseen shifts is small, i.e. $\\gamma^{\\prime}\\to0$ . We know that for $\\gamma^{\\prime}=0$ , the minimizer of the identifiable robust risk is given by the anchor estimator ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\beta_{\\mathrm{anchor}}=\\beta^{S}+(\\Sigma_{\\eta}^{\\star}+\\gamma M)^{-1}\\Sigma_{\\eta,\\xi}^{S}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, we lower bound the term $2\\gamma^{\\prime}C_{\\mathrm{ker}}\\|R^{\\top}\\beta\\|$ by the scalar product $2\\gamma^{\\prime}C_{\\mathrm{ker}}\\langle{R^{\\top}\\beta,}~~{R^{\\top}\\beta_{\\mathrm{anchor}}}\\rangle/\\|\\beta_{\\mathrm{anchor}}\\|$ and expect it to be tight for small $\\gamma^{\\prime}$ . After inserting this lower bound in Equation (38) we obtain the minimizer of the lower bound of form ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta_{L B}=\\beta^{S}+(\\Sigma_{\\eta}^{\\star}+\\gamma M+\\gamma^{\\prime}R R^{\\top})^{-1}(\\Sigma_{\\eta,\\xi}^{S}-\\gamma^{\\prime}C_{\\mathrm{ker}}R R^{\\top}(\\Sigma_{\\eta}^{\\star}+\\gamma M)^{-1}\\Sigma_{\\eta,\\xi}^{S}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We can now lower bound $\\|\\boldsymbol{R}\\boldsymbol{R}^{\\top}\\beta_{L B}\\|$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|R R^{\\top}\\beta_{L B}\\|\\geq\\|R R^{\\top}(\\Sigma_{\\eta}^{\\star}+\\gamma M)^{-1}\\Sigma_{\\eta,\\xi}^{\\star}\\|-o(\\gamma^{\\prime}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "from which the rate for small $\\gamma^{\\prime}$ follows. If we set $\\gamma=\\gamma^{\\prime}$ and $M=S S^{\\top}$ , the claim (15) of the theorem follows. For Section 3.2, the lower bound directly implies optimality of the identifiable robust risk of the anchor estimator when the strength of the unseen shifts $\\gamma^{\\prime}$ is small. Additionally. if $\\gamma=0$ , i.e. only unseen test shifts occur, we conclude that the OLS and anchor estimators have the same rates. ", "page_idx": 28}, {"type": "text", "text": "Lower bound $\\gamma_{\\mathrm{th}}$ for $\\gamma^{\\prime}$ . Finally, we want to derive a lower bound on the shift strength $\\gamma^{\\prime}$ such that for all $\\gamma^{\\prime}\\geq\\gamma_{\\mathrm{th}}$ Case 1 of our proof is valid, i.e. it holds that $\\beta^{\\mathrm{rob,ID}}$ is given by the closed form \"abstaining\" estimator (39). For this, we find $\\gamma_{\\mathrm{th}}$ such that for all $\\gamma^{\\prime}\\ge\\gamma_{\\mathrm{th}}$ zero is contained in the subdifferential of $\\mathcal{R}_{\\mathrm{rob,ID}}\\big(\\beta^{\\mathrm{rob,ID}};\\Theta_{\\mathrm{eq}},M_{\\mathrm{test}}\\big)$ at $\\beta^{\\mathrm{rob,ID}}$ . Then the KKT conditions are met, and $\\beta^{\\mathrm{rob,ID}}$ is the unique minimizer of the identifiable robust risk due to strong convexity of the objective. We compute the subdifferential to be ", "page_idx": 28}, {"type": "equation", "text": "$$\nS=\\gamma^{\\prime}C_{\\mathrm{ker}}\\{R R^{\\top}\\beta:\\|\\beta\\|_{2}\\leq1\\}+\\nabla\\mathcal{R}_{\\mathrm{rob}}(\\beta^{\\mathrm{rob},\\mathrm{ID}};\\theta^{S},\\gamma M).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $\\beta^{\\mathrm{rob,ID}}$ is the minimizer of $\\mathcal{R}_{\\mathrm{rob}}(\\beta;\\theta^{S},\\gamma M)$ under the constraint $R^{\\top}\\beta=0$ , the gradient is zero in $R^{\\perp}$ and it remains to show that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|R R^{\\top}\\nabla\\mathcal{R}_{\\mathrm{rob}}(\\beta^{\\mathrm{rob,ID}};\\theta^{S},\\gamma M)\\|\\leq\\gamma^{\\prime}C_{\\mathrm{ker}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "or ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\gamma^{\\prime}\\geq\\|R R^{\\top}\\nabla\\mathcal{R}_{\\mathrm{rob}}(\\beta^{\\mathrm{rob,ID}};\\theta^{S},\\gamma M)\\|/C_{\\mathrm{ker}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Via an upper bound on the projected gradient, we derive the stricter condition ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\gamma^{\\prime}\\geq\\frac{\\Vert R R^{\\top}\\Sigma_{\\eta,\\xi}^{S}\\Vert(1+\\kappa(\\Sigma_{\\eta}^{\\star}))}{C_{\\mathrm{ker}}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\kappa(\\Sigma_{\\eta}^{\\star})$ is the condition number of the covariance matrix. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We state our contributions relative to prior work in the abstract, in Section 1, and in Appendix A. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: In the abstract and in Section 1, we highlight the setting that we consider. We explicitly describe the assumptions in Section 2 and summarize the limitations in Section 4. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Appendix F contains proofs of all results appearing in the main paper. Appendix B, Appendix C, and Appendix $\\mathrm{D}$ are self-contained and contain derivations and proof of the results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Appendix E provides all the necessary information to reproduce the experimental results presented in Section 3.2. We provide details on empirical estimation of the proposed loss function in Appendix D. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: While we do not provide the code, the paper provides all necessary information on reproducing the experiment in Appendices D and E. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We specify all details to understand the experimental results in Section 3.2 and Appendix E. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: ", "page_idx": 31}, {"type": "text", "text": "In the numerical experiment, shown in Figure 3, we provide the average test MSE and its $5\\%$ and $95\\%$ -quantiles over 100 repetitions for each method. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The numerical experiment described in Section 3.2 is computationally very light and can be run on a personal laptop in a few minutes. We describe this in Appendix E. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and confirm that our work conforms to it in all aspects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Even if our work addresses the theoretical limits of distributional robustness, we mention in the abstract and in Section 1 that the topic of distributional robustness is central to safety-critical applications. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The work develops a theoretical framework and considers synthetic experiments. Therefore, explicit safeguards do not seem applicable at this stage. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: In the numerical experiment in Section 3.3, we cite the existing work that we compare to our framework and the dataset used. In running the numerical experiment, we reimplemented all the methods (including existing ones) for ease of comparison. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: At this stage, the paper does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing experiments or research with human subjects. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing experiments or research with human subjects. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]