[{"figure_path": "pJlFURyTG5/figures/figures_6_1.jpg", "caption": "Figure 1: Performance comparisons in terms of cost and reward on three Safe ManyAgent Ant tasks. Each column subfigure represents a different task, and we plot the cost curves (the lower the better) in the upper row and the reward curves (the higher the better) in the bottom row for each task.", "description": "This figure compares the performance of four different algorithms (IPPO, HAPPO, MAPPO-L, and Scal-MAPPO-L) on three variations of the Safe ManyAgent Ant task.  Each task varies the number of agents and their arrangement.  The top row shows the average episode cost for each algorithm, with lower values indicating better performance. The bottom row shows the average episode reward, where higher values are better.  The shaded regions represent the standard deviation across multiple runs.  The figure demonstrates that Scal-MAPPO-L achieves comparable performance to MAPPO-L while being more computationally efficient, particularly given that it does not access the global state information unlike MAPPO-L. ", "section": "4 Experiments"}, {"figure_path": "pJlFURyTG5/figures/figures_7_1.jpg", "caption": "Figure 2: Performance comparisons in terms of cost and reward on Safe ManyAgent Ant task, Safe Ant task, and Safe Coupled HalfCheetah task. In each task, the performance of Scal-MAPPO-L with different K and MAPPO-L are demonstrated.", "description": "This figure compares the performance of Scal-MAPPO-L with different values of K (the number of neighboring agents considered) against the performance of MAPPO-L (which considers the global state) on three different safe multi-agent reinforcement learning tasks.  The plots show the average episode cost and reward over time for each algorithm and different K values. This helps to demonstrate the impact of the local information constraint and the effectiveness of the proposed Scal-MAPPO-L algorithm in achieving both high reward and satisfying safety constraints.", "section": "4.2 Results"}, {"figure_path": "pJlFURyTG5/figures/figures_24_1.jpg", "caption": "Figure 1: Performance comparisons in terms of cost and reward on three Safe ManyAgent Ant tasks. Each column subfigure represents a different task, and we plot the cost curves (the lower the better) in the upper row and the reward curves (the higher the better) in the bottom row for each task.", "description": "This figure compares the performance of four different multi-agent reinforcement learning algorithms (IPPO, HAPPO, MAPPO-L, and Scal-MAPPO-L) on three variations of the Safe ManyAgent Ant task.  The plots show both the average episode cost and the average episode reward for each algorithm over the course of training.  Lower cost is better and higher reward is better. The results demonstrate that the Scal-MAPPO-L algorithm, which is proposed by the authors, achieves comparable or better performance while only accessing local state information, unlike the other algorithms which access more global information.", "section": "4 Experiments"}, {"figure_path": "pJlFURyTG5/figures/figures_24_2.jpg", "caption": "Figure 1: Performance comparisons in terms of cost and reward on three Safe ManyAgent Ant tasks. Each column subfigure represents a different task, and we plot the cost curves (the lower the better) in the upper row and the reward curves (the higher the better) in the bottom row for each task.", "description": "This figure compares the performance of four different multi-agent reinforcement learning algorithms (IPPO, HAPPO, MAPPO-L, and Scal-MAPPO-L) on three variations of the Safe ManyAgent Ant task.  Each task varies in the number of agents and their arrangement. The top row displays the average cost incurred during training, with lower values indicating better performance. The bottom row showcases the average reward obtained, where higher values signify better performance.  The results visually demonstrate Scal-MAPPO-L's comparable or superior performance in terms of both cost and reward compared to the other algorithms, especially considering its decentralized nature.", "section": "4 Experiments"}]