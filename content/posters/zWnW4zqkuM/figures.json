[{"figure_path": "zWnW4zqkuM/figures/figures_1_1.jpg", "caption": "Figure 1: We propose a new task Graph2Image featuring image synthesis by conditioning on graph information and introduce a novel graph-conditioned diffusion model called INSTRUCTG2I to tackle this problem. (a) Graph2Image is supported by prevalent multimodal attributed graphs and is grounded in real-world applications, e.g., virtual artistry. (b) INSTRUCTG2I outperforms baseline image generation techniques, demonstrating the usefulness of graph information. (c) To accommodate realistic user queries, INSTRUCTG2I exhibits smooth controllability in utilizing text/graph information and managing the strength of multiple graph edges.", "description": "This figure demonstrates the core concept and capabilities of the INSTRUCTG2I model.  Panel (a) shows a multimodal attributed graph (MMAG) example, illustrating how artworks are interconnected based on style, artist, etc.  Panel (b) compares INSTRUCTG2I's image generation performance against existing methods (Stable Diffusion, InstructPix2Pix, and ControlNet), highlighting the improvement achieved by incorporating graph information.  Finally, panel (c) showcases INSTRUCTG2I's ability to controllably generate images by adjusting the weighting of text and graph-based prompts, with examples of artwork generation influenced by the style of Monet and Kandinsky.", "section": "1 Introduction"}, {"figure_path": "zWnW4zqkuM/figures/figures_3_1.jpg", "caption": "Figure 2: The overall framework of INSTRUCTG2I. (a) Given a target node with a text prompt (e.g., House in Snow) in a Multimodal Attributed Graph (MMAG) for which we want to generate an image, (b) we first perform semantic PPR-based neighbor sampling, which involves structure-aware personalized PageRank and semantic-aware similarity-based reranking to sample informative neighboring nodes in the graph. (c) These neighboring nodes are then inputted into a Graph-QFormer, encoded by multiple self-attention and cross-attention layers, represented as graph tokens and used to guide the denoising process of the diffusion model, together with text prompt tokens.", "description": "This figure illustrates the overall framework of the INSTRUCTG2I model, showing the process of generating images from multimodal attributed graphs (MMAGs).  It is broken down into three parts: (a) Training setup, where the input consists of a target node within the graph and its associated text prompt; (b) Semantic PPR-based neighbor sampling, illustrating the process of selecting relevant neighbor nodes using both structural (personalized PageRank) and semantic (similarity-based reranking) criteria; and (c) The InstructG2I model, which uses a Graph-QFormer to encode the selected neighbors into graph tokens. These graph tokens, along with text prompt tokens, are then used to guide the denoising process of a diffusion model to generate the final image.", "section": "3 Methodology"}, {"figure_path": "zWnW4zqkuM/figures/figures_6_1.jpg", "caption": "Figure 1: We propose a new task Graph2Image featuring image synthesis by conditioning on graph information and introduce a novel graph-conditioned diffusion model called INSTRUCTG2I to tackle this problem. (a) Graph2Image is supported by prevalent multimodal attributed graphs and is grounded in real-world applications, e.g., virtual artistry. (b) INSTRUCTG2I outperforms baseline image generation techniques, demonstrating the usefulness of graph information. (c) To accommodate realistic user queries, INSTRUCTG2I exhibits smooth controllability in utilizing text/graph information and managing the strength of multiple graph edges.", "description": "This figure presents an overview of the proposed Graph2Image task and the INSTRUCTG2I model.  (a) illustrates the concept of multimodal attributed graphs (MMAGs) used as input, showing how interconnected nodes with image and text information can represent complex relationships (in this case, virtual art). (b) compares image generation results of INSTRUCTG2I against other methods, highlighting its superior performance due to the incorporation of graph context. (c) showcases INSTRUCTG2I's ability to control image generation through text and graph parameters, providing smooth transitions between different styles and levels of detail.", "section": "1 Introduction"}, {"figure_path": "zWnW4zqkuM/figures/figures_6_2.jpg", "caption": "Figure 3: INSTRUCTG2I achieves the best trade-off between DINOV2 (\u2191) and FID (\u2193) scores.", "description": "This figure shows the quantitative comparison of INSTRUCTG2I against various baselines across three datasets (ART500K, Amazon, and Goodreads) using two metrics: DINOV2 score (representing image-image similarity) and FID score (representing image quality and consistency).  The plot visually demonstrates that INSTRUCTG2I outperforms other methods, achieving the optimal balance between high DINOV2 scores (indicating strong similarity to ground truth images) and low FID scores (indicating high image quality).", "section": "4.2 Main results"}, {"figure_path": "zWnW4zqkuM/figures/figures_7_1.jpg", "caption": "Figure 5: Ablation study on semantic PPR-based neighbor sampling. The results indicate that both structural and semantic relevance proposed by our method effectively improve the image generation quality and consistency with the graph context.", "description": "This figure shows the results of an ablation study on the semantic PPR-based neighbor sampling method used in INSTRUCTG2I.  The study compares different neighbor sampling techniques: random sampling, PPR-based sampling, semantics-based sampling, and the proposed semantic PPR-based sampling. For each method, the figure shows example sampled neighbors, and the generated images based on those neighbors.  The results demonstrate that incorporating both structural and semantic relevance in neighbor selection significantly improves the quality and consistency of the generated images with the ground truth image.", "section": "4.3 Ablation Study"}, {"figure_path": "zWnW4zqkuM/figures/figures_8_1.jpg", "caption": "Figure 1: We propose a new task Graph2Image featuring image synthesis by conditioning on graph information and introduce a novel graph-conditioned diffusion model called INSTRUCTG2I to tackle this problem. (a) Graph2Image is supported by prevalent multimodal attributed graphs and is grounded in real-world applications, e.g., virtual artistry. (b) INSTRUCTG2I outperforms baseline image generation techniques, demonstrating the usefulness of graph information. (c) To accommodate realistic user queries, INSTRUCTG2I exhibits smooth controllability in utilizing text/graph information and managing the strength of multiple graph edges.", "description": "This figure demonstrates the core idea and capabilities of the INSTRUCTG2I model. (a) shows an example of a Multimodal Attributed Graph (MMAG) used for image generation, highlighting the task of Graph2Image. (b) compares the image synthesis results of INSTRUCTG2I with baseline methods, showcasing its superior performance due to graph conditioning. (c) illustrates the model's controllable generation capabilities, allowing for smooth transitions in image style by adjusting the influence of text and graph information.", "section": "1 Introduction"}, {"figure_path": "zWnW4zqkuM/figures/figures_9_1.jpg", "caption": "Figure 2: The overall framework of INSTRUCTG2I. (a) Given a target node with a text prompt (e.g., House in Snow) in a Multimodal Attributed Graph (MMAG) for which we want to generate an image, (b) we first perform semantic PPR-based neighbor sampling, which involves structure-aware personalized PageRank and semantic-aware similarity-based reranking to sample informative neighboring nodes in the graph. (c) These neighboring nodes are then inputted into a Graph-QFormer, encoded by multiple self-attention and cross-attention layers, represented as graph tokens and used to guide the denoising process of the diffusion model, together with text prompt tokens.", "description": "This figure illustrates the overall framework of the INSTRUCTG2I model for image generation from multimodal attributed graphs. It is broken down into three parts:\n(a) Shows the initial setup with a target node and its text prompt in an MMAG.\n(b) Details the semantic PPR-based neighbor sampling process, which uses personalized PageRank and similarity-based reranking to select relevant neighbor nodes.\n(c) Illustrates the INSTRUCTG2I model architecture, where selected neighbors are processed by a Graph-QFormer and combined with text prompt tokens to guide the diffusion model during the image denoising process.", "section": "3 Methodology"}]