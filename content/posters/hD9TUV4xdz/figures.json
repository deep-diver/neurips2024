[{"figure_path": "hD9TUV4xdz/figures/figures_1_1.jpg", "caption": "Figure 1: The relationship between the optimal learning rate and the batch size is different between Adam and SGD. The orange line represents the tendency of the optimal learning rate to converge to a non-zero value when the batch size is large enough.", "description": "This figure illustrates the contrasting relationships between optimal learning rate and batch size for Adam-style and SGD-style optimizers.  The SGD curve shows a linear increase in optimal learning rate with batch size. Conversely, the Adam curve demonstrates a surge, initially increasing, then decreasing, and finally leveling off at a non-zero value as the batch size grows large enough. This highlights a key difference in how these optimizer types respond to scaling.", "section": "1 Introduction"}, {"figure_path": "hD9TUV4xdz/figures/figures_7_1.jpg", "caption": "Figure 2: Batch size versus optimal learning rate within the context of CNN trained on FashionMNIST.", "description": "This figure shows the relationship between batch size and optimal learning rate for a Convolutional Neural Network (CNN) trained on the Fashion-MNIST dataset.  It includes three subfigures: (a) a statistical histogram showing the distribution of the term (\u03c0\u03c3\u00b2/2\u03bc\u00b2)\u00b2; (b) illustrating the relationship for small batch sizes, aligning with Theorem 3 from the paper; and (c) illustrating the relationship for larger batch sizes, according to Theorem 4. The plots visually represent how the optimal learning rate changes as batch size increases, demonstrating the 'surge' phenomenon described in the paper.  The plots also include curves generated using existing SGD scaling laws for comparison, illustrating the difference in scaling behavior between Adam-style and SGD optimizers.", "section": "3.3 Results"}, {"figure_path": "hD9TUV4xdz/figures/figures_8_1.jpg", "caption": "Figure 3: The relationship between batch sizes and optimal learning rates within the context of ResNet-18 trained on TinyImageNet. The red dashed line accurately predicts the peak value, and as the training loss decreases, the peak value gradually shifts to the right.", "description": "This figure shows the relationship between batch size and optimal learning rate for the ResNet-18 model trained on the TinyImageNet dataset.  The figure is a grid plot showing multiple trials for each batch size, showing a surge (increase and then decrease) in optimal learning rate as batch size increases. The peak of the optimal learning rate moves to the right (larger batch sizes) as the training loss decreases. A red dashed line shows the model fit which the authors propose in the paper. This illustrates the non-linear relationship between batch size and optimal learning rate in Adam-style optimizers.", "section": "3.3 Results"}, {"figure_path": "hD9TUV4xdz/figures/figures_8_2.jpg", "caption": "Figure 4: The relationship between batch sizes and optimal learning rates within the context of DistilGPT2 trained on Eli5Category.", "description": "This figure visualizes the relationship between batch size and optimal learning rate for the DistilGPT2 model trained on the ELI5-Category dataset. It shows two scenarios: one with Adam optimizer's hyperparameters set to \u03b2\u2081 = 0.0, \u03b2\u2082 = 0.0 (sign of gradient), and another with \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.999 (default Adam).  The plots illustrate how the optimal learning rate initially increases and then decreases as the batch size increases, resembling a surge.  The figure also shows the estimated values of Bnoise and Emax for each configuration, and compares the results with fitted curves from previous research for SGD optimizers.", "section": "3.3 Results"}, {"figure_path": "hD9TUV4xdz/figures/figures_9_1.jpg", "caption": "Figure 2: Batch size versus optimal learning rate within the context of CNN trained on FashionMNIST.", "description": "This figure visualizes the relationship between batch size and optimal learning rate for a Convolutional Neural Network (CNN) trained on the Fashion-MNIST dataset.  The left subplot shows the results of a grid search, plotting various learning rates against different batch sizes, color-coded by the final training loss achieved. The right subplot presents fitted curves based on theoretical analysis, comparing the proposed scaling law (orange line) with linear and square root scaling laws commonly used for SGD-type optimizers. This comparison highlights the difference between Adam-style optimizers and SGD-style optimizers in terms of how optimal learning rate scales with batch size.", "section": "3.3 Results"}, {"figure_path": "hD9TUV4xdz/figures/figures_18_1.jpg", "caption": "Figure 6: Finer-grained grid search results for the experiments shown in Figure 4(b).", "description": "This figure presents a more detailed grid search focusing on the relationship between batch size and optimal learning rate for the DistilGPT2 model trained on the ELI5-Category dataset, specifically expanding upon the results shown in Figure 4(b).  The heatmap shows the training loss (color coded) across various batch size and learning rate combinations, while the yellow line highlights the optimal learning rate for each batch size. The finer granularity of this grid search provides a more precise visualization of the optimal learning rate's behavior.", "section": "3.3 Results"}, {"figure_path": "hD9TUV4xdz/figures/figures_18_2.jpg", "caption": "Figure 2: Batch size versus optimal learning rate within the context of CNN trained on FashionMNIST.", "description": "This figure visualizes the relationship between batch size and optimal learning rate for a Convolutional Neural Network (CNN) trained on the Fashion-MNIST dataset.  The left panel shows a heatmap illustrating the training loss achieved with different combinations of batch size and learning rate. The right panel displays the optimal learning rate (y-axis) plotted against the batch size (x-axis).  The orange line represents the theoretical prediction of the optimal learning rate based on the authors' proposed model, showcasing how the optimal learning rate initially increases, then decreases, and finally plateaus as the batch size grows larger. This behavior contradicts previous findings that learning rate increases monotonically with batch size. The figure demonstrates the \"surge phenomenon\" described in the paper, where the optimal learning rate peaks at a specific batch size before decreasing, a phenomenon not captured by previous models.", "section": "3 Results"}, {"figure_path": "hD9TUV4xdz/figures/figures_18_3.jpg", "caption": "Figure 2: Batch size versus optimal learning rate within the context of CNN trained on FashionMNIST.", "description": "This figure shows the relationship between batch size and optimal learning rate for a Convolutional Neural Network (CNN) trained on the Fashion-MNIST dataset.  The left panel displays the results of a grid search showing training loss as a heatmap with different batch sizes and learning rates.  The right panel displays a curve showing the optimal learning rate across various batch sizes, along with theoretical curves for comparison.  This illustrates the \"surge phenomenon\" described in the paper where the optimal learning rate initially rises, then falls before eventually plateauing. ", "section": "3.3 Results"}, {"figure_path": "hD9TUV4xdz/figures/figures_18_4.jpg", "caption": "Figure 2: Batch size versus optimal learning rate within the context of CNN trained on FashionMNIST.", "description": "This figure shows the relationship between batch size and optimal learning rate for a Convolutional Neural Network (CNN) trained on the Fashion-MNIST dataset.  The left subplot displays the results of a grid search, showing the optimal learning rates for various batch sizes. The right subplot displays curves fitting to the data from the left subplot which depicts the theoretical relationship between optimal learning rate and batch size according to Theorem 3 and Theorem 4 from the paper.  The figure demonstrates that the optimal learning rate initially increases, then decreases (a surge), and eventually plateaus as the batch size increases, as predicted by the theoretical analysis.", "section": "3.3 Results"}]