{"references": [{"fullname_first_author": "Carlo Acerbi", "paper_title": "Spectral measures of risk: A coherent representation of subjective risk aversion", "publication_date": "2002-01-01", "reason": "This paper introduces spectral risk measures, which are central to the proposed SRCPO algorithm and its theoretical analysis."}, {"fullname_first_author": "Joshua Achiam", "paper_title": "Constrained policy optimization", "publication_date": "2017-01-01", "reason": "This paper introduces constrained policy optimization (CPO), a foundational method for safe RL that SRCPO builds upon and improves upon."}, {"fullname_first_author": "Alekh Agarwal", "paper_title": "On the theory of policy gradient methods: Optimality, approximation, and distribution shift", "publication_date": "2021-01-01", "reason": "This paper provides important theoretical foundations for policy gradient methods, which are crucial for understanding the convergence properties of SRCPO."}, {"fullname_first_author": "Nicole B\u00e4uerle", "paper_title": "Minimizing spectral risk measures applied to Markov decision processes", "publication_date": "2021-01-01", "reason": "This paper presents a bilevel optimization approach for spectral risk measures in MDPs, which is the core methodology used in SRCPO."}, {"fullname_first_author": "Yinlam Chow", "paper_title": "Risk-constrained reinforcement learning with percentile risk criteria", "publication_date": "2017-01-01", "reason": "This paper is a seminal work in risk-constrained reinforcement learning (RCRL), providing a comprehensive overview of the field and motivating the need for improved algorithms like SRCPO."}]}