[{"type": "text", "text": "Dissecting the Failure of Invariant Learning on Graphs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qixun Wang1 Yifei Wang 2 Yisen Wang $^{1,3*}$ Xianghua Ying 1 ", "page_idx": 0}, {"type": "text", "text": "1 State Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University 2 CSAIL, MIT 3 Institute for Artificial Intelligence, Peking University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Enhancing node-level Out-Of-Distribution (OOD) generalization on graphs remains a crucial area of research. In this paper, we develop a Structural Causal Model (SCM) to theoretically dissect the performance of two prominent invariant learning methods\u2014Invariant Risk Minimization (IRM) and Variance-Risk Extrapolation (VREx)\u2014in node-level OOD settings. Our analysis reveals a critical limitation: due to the lack of class-conditional invariance constraints, these methods may struggle to accurately identify the structure of the predictive invariant ego-graph and consequently rely on spurious features. To address this, we propose Crossenvironment Intra-class Alignment (CIA), which explicitly eliminates spurious features by aligning cross-environment representations conditioned on the same class, bypassing the need for explicit knowledge of the causal pattern structure. To adapt CIA to node-level OOD scenarios where environment labels are hard to obtain, we further propose CIA-LRA (Localized Reweighting Alignment) that leverages the distribution of neighboring labels to selectively align node representations, effectively distinguishing and preserving invariant features while removing spurious ones, all without relying on environment labels. We theoretically prove CIA-LRA\u2019s effectiveness by deriving an OOD generalization error bound based on PAC-Bayesian analysis. Experiments on graph OOD benchmarks validate the superiority of CIA and CIA-LRA, marking a significant advancement in nodelevel OOD generalization. The codes are available at https://github.com/ NOVAglow646/NeurIPS24-Invariant-Learning-on-Graphs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generalizing to unseen testing distributions that differ from the training distributions, known as OutOf-Distribution (OOD) generalization, is one of the key challenges in machine learning. Invariant learning, which aims to capture predictive features that remain consistent under distributional shifts, is a crucial strategy for addressing OOD generalization. Numerous invariant learning methods have been proposed to tackle OOD problems in computer vision (CV) tasks [Arjovsky et al., 2020, Krueger et al., 2021, Bui et al., 2021, Rame et al., 2022, Shi et al., 2021, Mahajan et al., 2021, Zhang et al., 2021a, Wang et al., 2022a, Yi et al., 2022, Wang et al., 2022b, Xin et al., 2023]. While in recent years, enhancing OOD generalization on graph data is an emerging research direction attracting increasing attention. In this work, we focus on the challenge of node-level OOD generalization on graphs. ", "page_idx": 0}, {"type": "text", "text": "Straightforwardly adapting the above methods to node-level graph OOD scenarios presents several challenges: 1) the prediction of a node\u2019s label depends on its neighbored samples in an ego-subgraph, causing a gap from conventional CV OOD scenarios where samples are independently predicted; and 2) environment labels in node-level tasks are often unavailable [Wu et al., 2021, Li et al., 2023a, ", "page_idx": 0}, {"type": "text", "text": "Liu et al., 2023], rendering invariant learning methods based on environment partitioning infeasible. To illustrate the failure of directly adapting traditional invariant learning to graphs, we evaluate two representative OOD approaches, Invariant Risk Minimization (IRM [Arjovsky et al., 2020]) and Variance-Risk Extrapolation (VREx [Krueger et al., 2021]), in OOD node classification scenarios. We choose IRM and VREx for two reasons: 1) Numerous node-level graph OOD strategies [Zhang et al., 2021b, Wu et al., 2021, Liu et al., 2023, Tian et al., 2024] utilize VREx as invariance regularization (details in Appendix A.2). Therefore, analyzing VREx can cover a significant portion of graph-OOD methods; and 2) IRM and VREx are two prominent OOD methods that we can theoretically prove to be effective on non-graph data (Proposition 2.2). By testing their performance on graph data, we can better reveal the differences between graph and non-graph data. We choose real-world graph datasets: Arxiv, Cora, and WebKB; synthetic datasets: CBAS and a toy graph OOD dataset with spurious correlations between node features and labels for evaluation. From Table 1, we observe that IRM and VREx offer marginal or no improvement over ERM on both real-world and synthetic node-level graph OOD datasets. This naturally raises some questions here: ", "page_idx": 1}, {"type": "text", "text": "On graphs, why do traditional invariant learning methods fail? How to make them work again? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To theoretically analyze their failure modes, we build a Structural Causal Model (SCM) to model the data generation process under two types of distributional shifts: concept shift and covariate shift, and gain a high-level understanding of the challenges in node-level OOD generalization: To correctly predict a node\u2019s label, the structure of a predictive invariant neighboring egograph (which we call it a causal pattern) and their invariant node features must be learned. However, identifying the correct structure of the causal pattern presents additional optimization requirements (compared to CV scenarios) for Graph Neural Networks (GNNs) since they must adjust the aggregation parameters (such as the attention weights in GAT [Velic\u02c7kovic\u00b4 et al., 2018]) to achieve this. IRM and VREx lack class-conditional ", "page_idx": 1}, {"type": "table", "img_path": "7eFS8aZHAM/tmp/8901764285f74c54335a62a4ec9b1f8c05f95926424c2b4b1880284a02e24671.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Table 1: Real-Cov./Con. are average OOD accuracy on the covariate/concept shift of Arxiv, Cora, CBAS, and WebKB. Toy denotes results on our toy synthetic graph OOD dataset. ", "page_idx": 1}, {"type": "text", "text": "invariance constraints, which causes insufficient supervision for regularizing the training of these aggregation parameters, leading to non-unique solutions of GNN parameters and potentially resulting in the learning of spurious features. (detailed analysis is in Section 2). To overcome this, we propose Cross-environment Intra-class Alignment (CIA) that further considers class-conditional invariance to identify causal patterns2. We theoretically demonstrate that by aligning node representations of the same class and different environments, CIA can eliminate spurious features and learn the correct causal pattern, as same-class different-environment samples share similar causal patterns while exhibiting different spurious features. Table 1 shows CIA\u2019s empirical gains. To leverage the advantage of CIA and adapt it to scenarios without environment labels, we further propose CIA-LRA (Localized Reweighting Alignment), utilizing localized label distribution to find node pairs with significant differences in spurious features and small differences in causal ones for alignment, to eliminate the spurious features while alleviating the collapse of the causal ones. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. By constructing an SCM, we provide a theoretical analysis revealing that VREx and IRM could rely on spurious features when using a GAT-like GNN (Section 2.2) in node-level OOD scenarios, revealing a key challenge of invariant learning on graphs.   \n2. We propose CIA and theoretically prove its effectiveness in learning invariant representations on graphs (Section 3.1). To adapt CIA to node-level OOD scenarios where environment labels are unavailable, we further propose CIA-LRA that requires no environment labels or complex environmental partitioning processes to achieve invariant learning (Section 3.2), with theoretical guarantees on its generalization performance (Section 4).   \n3. We evaluate CIA and CIA-LRA on the Graph OOD benchmark (GOOD) [Gui et al., 2022] on GAT and GCN [Kipf and Welling, 2016]. The results demonstrate CIA\u2019s superiority over ", "page_idx": 1}, {"type": "text", "text": "non-graph invariant learning methods, and CIA-LRA achieves state-of-the-art performance (Section 5.2). ", "page_idx": 2}, {"type": "text", "text": "We leave comparisons of our method and existing node-level OOD works in Appendix A.3. ", "page_idx": 2}, {"type": "text", "text": "2 Dissecting Invariant Learning on Graphs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For OOD node classification, we are given a single training graph ${\\mathcal{G}}=(A,X,Y)$ containing $N$ nodes $\\mathcal{V}=\\{v_{i}\\}_{i=1}^{N}$ from multiple training environments $e\\in\\mathcal{E}_{\\mathrm{tr}}$ . $A\\in\\left\\{0,1\\right\\}^{N\\times N}$ is the adjacency matrix, $A_{i,j}=1$ iff there is an edge between $v_{i}$ and $v_{j}$ . $X\\in\\mathbb{R}^{N\\times D}$ are node features. The $i$ -th row $X_{i}\\in\\mathbb{R}^{D}$ represents the original node feature of $v_{i}$ . $Y^{'}\\in\\{0,1,...,C-1\\}^{N}$ are the labels, $C$ is the number of the classes. Denote the subgraph containing nodes of environment $e$ as $\\mathcal{G}^{e}=(A^{e},X^{e},Y^{e})$ , which follows the distribution $p_{e}$ . Let $A^{e}\\in\\left\\{0,1\\right\\}^{N^{e}\\times N^{e}}$ and $D^{e}$ be the adjacency matrix and the diagonal degree matrix for nodes from environment $e$ respectively, where $\\begin{array}{r}{D_{i i}^{e}=\\sum_{j=1}{A^{e}}_{i j}}\\end{array}$ , $N^{e}$ is the number of samples in $e$ . Denote the normalized adjacency matrix as $\\bar{A}^{e}=(D^{e}+\\stackrel{\\circ}{I}_{N^{e}})^{-\\frac12}A^{e}(D^{e}+I_{N^{e}})^{-\\frac12}$ , $I_{N^{e}}$ is the identity matrix. Let $\\tilde{A}^{e}=\\bar{A}+(D^{e}+I_{N^{e}})^{-\\frac{1}{2}}I_{N^{e}}(D^{e}+I_{N^{e}})^{-\\frac{1}{2}}$ . Suppose the unseen test environments are $e^{\\prime}\\in\\mathcal{E}_{\\mathrm{te}}$ . The test distribution $p_{e^{\\prime}}\\neq p_{e}\\forall e^{\\prime}\\in\\mathcal{E}_{\\mathrm{te}}$ , $\\forall e\\in\\mathcal{E}_{\\mathrm{tr}}$ . OOD generalization aims to minimize the prediction error over test distributions. ", "page_idx": 2}, {"type": "text", "text": "To understand the obstacles in invariant learning on graphs, we start by examining whether IRMv1 (practical implementation of the original challenging IRM objective, proposed by Arjovsky et al. [2020]) and VREx can be successfully transferred to node-level graph OOD tasks. Their objectives are as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathrm{IRMv1})\\underset{w,\\phi}{\\mathrm{min}}\\mathbb{E}_{e}[\\mathcal{L}\\left(w\\circ\\phi(X^{e}),Y^{e}\\right)+\\beta\\|\\nabla_{w|w=1.0}\\mathcal{L}\\left(w\\circ\\phi(X^{e}),Y^{e}\\right)\\|_{2}^{2}],}\\\\ &{(\\mathrm{VREx})\\underset{w,\\phi}{\\mathrm{min}}\\mathbb{E}_{e}[\\mathcal{L}\\left(w\\circ\\phi(X^{e}),Y^{e}\\right)]+\\beta\\mathrm{Var}_{e}[\\mathcal{L}\\left(w\\circ\\phi(X^{e}),Y^{e}\\right)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $w$ and $\\phi$ denote the classifier and feature extractor, respectively. $\\mathcal{L}$ is the cross-entropy loss. $\\beta$   \nis some hyperparameter. ", "page_idx": 2}, {"type": "text", "text": "2.1 A Causal Data Model on Graphs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Data generation process. We construct an SCM to characterize two kinds of distribution shifts: concept shift (Figure 1a) and covariate shift (Figure 1b). $C$ and $S$ denote unobservable causal/spurious latent variables that affect the generation of the graph $G$ , and dashed $E$ are environmental variables usually unobservable. We consider a simple case that each node $v$ in environment $e$ has a 2-dim feature $[x_{v}^{1},x_{v}^{2}]^{\\top},\\,x_{v}^{1},\\;x_{v}^{2}\\in\\mathbb{R}$ Denote the concatenated node features of all nodes in $e$ as $X_{1}\\in\\mathbb{R}^{N^{e}\\times1}$ and $X_{2}^{e}\\in\\mathbb{R}^{N^{e}\\times1}$ corresponding to $x_{v}^{1}$ and $x_{v}^{2}$ , respectively. For the SCM in Figure 1a3, the data generation process of environment $e$ is ", "page_idx": 2}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/c1c4afa39c01a90cbefe73b9a9c0058572beb35ea4edfb48e00f7377857007cf.jpg", "img_caption": ["shift shift ", "Figure 1: Causal graphs of the SCMs considered in our work. "], "img_footnote": [], "page_idx": 2}, {"type": "equation", "text": "$$\nY^{e}=(\\tilde{A}^{e})^{k}X_{1}+n_{1},\\;X_{2}^{e}=(\\tilde{A}^{e})^{m}Y^{e}+n_{2}+\\epsilon^{e},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $k\\in\\mathbb{N}^{+}$ represents the \"depth\" (number of hops) of the causal pattern, and $m\\in\\mathbb{N}^{+}$ is the depth of the ego-graph determining the spurious node features. $n_{1}\\,\\in\\,\\mathbb{R}^{N^{e}\\times1}$ and $n_{2}\\,\\in\\,\\mathbb{R}^{N^{e}\\times1}$ represent random Gaussian noise. $\\epsilon^{e}$ stands for an environmental variable, causing the spurious correlations between $X_{2}^{e}$ and $Y$ . A detailed description of the model is in Appendix F.1. ", "page_idx": 2}, {"type": "text", "text": "How the proposed model considers both node feature shifts and structural shifts? $X_{1}$ represent invariant node features causing $Y^{e}$ . $X_{2}^{e}$ denotes spurious node features that vary with environments. As for structural shifts, we consider an environmental A\u02dce in Equation (2), which means the structure can vary with environments. For example, there could be a spurious correlation between certain structures and the label; or, the graph connectivity or size may shift [Buffelli et al., 2022, Xia et al., ", "page_idx": 2}, {"type": "text", "text": "2023]. We model the invariant structural feature as the structure of a node\u2019s $k$ -layer neighboring ego-graph. See Appendix F.2 for more discussions of the structural shifts. ", "page_idx": 3}, {"type": "text", "text": "We also have the following assumption about the stability of the causal patterns across environments: Assumption 2.1. (Stability of the causal patterns) The $k$ -layer causal pattern in Equation (2) is invariant across environments for every class $c$ . ", "page_idx": 3}, {"type": "text", "text": "A simple multi-layer GNN. Consider a $L$ -layer GNN $f$ parameterized by $\\Theta\\quad=$ $\\left\\{\\theta_{1},\\bar{\\theta_{2}},\\bar{\\theta_{1}^{1}}^{(l)},\\theta_{1}^{2}{}^{(l)},\\bar{\\theta_{2}^{\\bar{1}}}^{(l)},\\theta_{2}^{2}{}^{(l)}\\right\\},l=1,2,...,L-1;$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(H_{1}^{(l)}\\quad H_{2}^{(l)}\\right)=\\left({\\bar{A}}\\quad{\\bar{I}}\\right)\\left(\\theta_{1}^{1(l-1)}\\quad\\theta_{2}^{1(l-1)}\\right)\\left(H_{1}^{(l-1)}\\quad H_{2}^{(l-1)}\\right),\\;l=2,...,L,\\;H_{1}^{(1)}=X_{1},\\;H_{2}^{(1)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where \u03b8i and \u03b8ji(l) are scalars for $i,j\\in\\{1,2\\},\\forall l.\\;H_{i}^{(l)}\\in\\mathbb{R}^{N\\times D}$ are GNN representations. ", "page_idx": 3}, {"type": "text", "text": "Remark. In this GNN, we keep only the top-layer weight matrix $[\\theta_{1}\\,\\theta_{2}]^{\\top}$ , and let the weight matrix of lower layers $1,...,L-1$ be an identity matrix. This architecture resembles an SGC [Wu et al., 2019]. $\\theta_{1}$ and $\\theta_{2}$ are for invariant/spurious features, respectively. $\\theta_{1}^{1}{}^{(l)},\\,\\theta_{2}^{1}{}^{(l)}$ are weights for aggregating features from neighboring nodes and \u03b812(l), \u03b822(l) are weights for features of a centered node, this setup can be seen as a GAT. When all lower-layer parameters equal 1, the GNN degenerates to a GCN (see Appendix F.2 for justification of the choice of the GNN). ", "page_idx": 3}, {"type": "text", "text": "We consider a regression problem that we aim to minimize the MSE loss over all environments $\\mathbb{E}_{e}[R(e)]=\\mathbb{E}_{e}\\left[\\breve{\\mathbb{E}}_{n_{1},n_{2}}\\left[\\|f_{\\Theta}(A^{e},X^{e})-Y^{e}\\|_{2}^{2}\\right]\\right]$ . The optimal invariant parameter set $\\Theta^{*}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\theta_{1}=1}\\\\ {\\theta_{2}=0\\quad\\mathrm{or}\\quad\\exists l\\in\\{1,...,L-1\\}\\,\\mathrm{s.t.}\\,\\theta_{2}^{1(l)}=\\theta_{2}^{2(l)}=0}\\\\ {\\theta_{1}^{1(l)}=1,\\theta_{1}^{2(l)}=1,\\quad l=L-1,...,L-k+1}\\\\ {\\theta_{1}^{1(l)}=0,\\theta_{1}^{2(l)}=1,\\quad l=L-k,L-k-1,...,1}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In Equation (4), the GNN parameters for spurious features (line 2) is zero, which means it removes spurious node features. Also, it learns the correct depth $k$ of the causal pattern $\\tilde{A}^{k}X_{1}$ (line 3-4). ", "page_idx": 3}, {"type": "text", "text": "2.2 Intriguing Failure of VREx and IRM on Graphs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Now we are ready to present the failure cases in this node-level OOD task: optimizing IRMv1 and VREx induces a model that relies on spurious features $X_{2}^{e}$ to predict, leading to poor OOD generalization. To illustrate that this failure arises from the graph data, we first prove that IRMv1 and VREx can learn invariant features under the non-graph version of SCM of Equation (2). ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.2. (IRMv1 and VREx can learn invariant features for non-graph tasks, proof is in Appendix G.1.1.) For the non-graph version of the SCM in Equation (2), ", "page_idx": 3}, {"type": "equation", "text": "$$\nY^{e}=X_{1}+n_{1},\\;X_{2}^{e}=Y^{e}+n_{2}+\\epsilon^{e},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "VREx and IRMv1 can learn invariant features when using a linear network: $f(X)=\\theta_{1}X_{1}+\\theta_{2}X_{2}$ . ", "page_idx": 3}, {"type": "text", "text": "Now we will give the main theorem revealing the failure of VREx and IRMv1 on graphs. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.3. (IRMv1 and VREx will use spurious features on graphs, informal) Under the SCM of Equation (2), the IRMv1 and VREx objectives have non-unique solutions for parameters of the GNN (3), and there exist solutions that use spurious features, i.e. $\\theta_{2}\\neq0$ . ", "page_idx": 3}, {"type": "text", "text": "Intuitive illustration of the failure. From Theorem 2.3, we find that the main reason for the failure lies in the message-passing mechanism in representation learning. Let\u2019s provide some key steps in the proof of the IRMv1 case as an illustration. For the non-graph OOD task Equation (5), we can verify that when IRMv1 objective is solved, i.e. $\\nabla_{w}R(e)=\\bar{0}$ for all $e$ , the invariant solution $\\theta_{2}=0$ leads to $(\\theta_{1})^{2}X_{1}^{\\top}X_{1}-\\theta_{1}X_{1}^{\\top}X_{1}=0$ , which can be satisfied when $\\theta=1$ . However, in the graph case, $\\theta_{2}=0$ leads to ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\theta_{1})^{2}((\\tilde{A^{e}})^{s}X_{1})^{\\top}(\\tilde{A^{e}})^{s}X_{1}-\\theta_{1}((\\tilde{A^{e}})^{k}X_{1})^{\\top}(\\tilde{A^{e}})^{s}X_{1}=0,\\,\\forall e,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(\\tilde{A}^{e})^{s}X_{1}$ is the learned representation of the GNN, $0<s\\leq L$ . $k$ is the depth of the causal pattern. Now we explain why the invariant solution may not hold on graphs. When the depth of the learned aggregation pattern $s\\neq k$ , Equation (6) cannot hold for a fixed $\\theta_{1}$ (since $\\theta_{1}$ will depend on $e$ then). This means that identifying the underlying structure of the causal pattern imposes additional difficulty for invariant learning. Moreover, even if the GNN can learn representations of different depths (e.g. GAT)4, the proof in Appendix G.1.3 shows that IRM failed to provide sufficient supervision to optimize the aggregation parameters $\\theta_{j}^{i}$ , $i,j\\,\\in\\,\\{1,2\\}$ such that $s\\,=\\,k$ . A similar analysis holds for VREx. In general, successful invariant learning on graphs requires capturing both invariant node features and the structure of the causal pattern, while methods like IRM and VREx that solely enforce a cross-environment invariance at the loss level5 may not be able to achieve these goals. The formal versions and proof are in Appendix G.1.3 (IRM) and G.1.2 (VREx). ", "page_idx": 4}, {"type": "text", "text": "3 The Proposed Methods ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Cross-environment Intra-class Alignment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Inspired by the examples of VREx and IRMv1, we aim to introduce additional invariance regularization to guide the model in identifying the underlying invariant node features and structures. We propose CIA (Cross-environment Intra-class Alignment), which aligns the representations from the same class across different environments. Intuitively, since such node pairs share similar invariant features and causal pattern structures while differ in spurious features, aligning their representations will help achieve our targets. Denote the representation of node $i$ as $\\phi_{\\Theta}(i)$ and the classifier parameterized by $\\theta_{h}$ as $h_{\\theta_{h}}$ CIA\u2019s objective is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta_{h},\\Theta}\\mathbb{E}_{e}\\left[\\mathcal{L}\\big(h_{\\theta_{h}}\\circ\\phi_{\\Theta}(A^{e},X^{e}),Y^{e}\\big)\\right]\\quad\\mathrm{s.t.}\\quad\\operatorname*{min}_{\\Theta}\\,\\mathcal{L}_{\\mathrm{CIA}}=\\mathbb{E}_{e,e^{\\prime}}\\mathbb{E}_{c}\\mathbb{E}_{\\qquad i,j}\\qquad\\big[\\mathcal{D}\\big(\\phi_{\\Theta}(i),\\phi_{\\Theta}(j)\\big)\\big]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Omega_{c}^{e,e^{\\prime}}=\\{(i,j)|i\\neq j\\land Y_{i}^{e}=Y_{j}^{e^{\\prime}}=c\\land E_{i}=e,$ , $E_{j}=e^{\\prime}\\}$ is the set of nodes with same label $c$ and from two different environments. $\\mathcal{L}$ is the cross-entropy loss. $\\mathcal{D}$ is some distance metric and we adopt L-2 distance. Now we prove that CIA can learn invariant representations regardless of the unknown causal patterns: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Under the SCM of Equation (2) and Assumption 2.1, optimizing the CIA objective will lead to the optimal invariant solution $\\Theta^{*}$ in Equation (4) for parameters of the GNN (3). ", "page_idx": 4}, {"type": "text", "text": "The proof is in Appendix G.1.4. By enforcing class-conditional invariance, which is not considered in VREx and IRMv1, CIA overcomes the above obstacles and eliminates spurious features. As long as a GNN has the capacity to adaptively learn the true depth of the causal pattern (such as the one considered in Equation (3)) or a GAT), CIA can identify the invariant causal pattern. ", "page_idx": 4}, {"type": "text", "text": "Remark. One might note that the objective of CIA is analogous to MatchDG [Mahajan et al., 2021]. However, we are the first to adapt such an idea to node-level OOD tasks and theoretically reveal its advantage. In Appendix A.1, we compare our extension and the original MatchDG in detail. ", "page_idx": 4}, {"type": "text", "text": "3.2 Localized Reweighting Alignment: an Adaptation to Graphs without Environment Labels ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "So far, we have theoretically and empirically validated CIA\u2019s advantage on graphs, but it still requires environmental labels that are challenging to obtain in most node classification tasks [Wu et al., 2021, Liu et al., 2023, Li et al., 2023a]. In this section, we propose CIA-LRA (Localized Reweighting Alignment) that realizes CIA\u2019s objective without using environment labels by identifying node pairs with significant/minor differences in spurious/invariant features and then aligning their representations. As illustrated in Figure 2, CIA-LRA mainly incorporates three components: ", "page_idx": 4}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/76498c8512d47e8c95deeb54bef0797bd65e7e8a0651675890415db2673d6816.jpg", "img_caption": ["Figure 2: The overall framework of our proposed CIA-LRA. The invariant subgraph extractor $\\phi_{\\theta_{m}}$ identifies the invariant subgraph for each node. Then the GNN encoder $\\phi_{\\Theta}$ aggregates information from the estimated invariant subgraphs to output node representations. CIA-LRA mainly contains two strategies: localized alignment and reweighting alignment. Localized alignment: we restrict the alignment to a local range to avoid overalignment that may cause the collapse of invariant features (shown in Appendix D.1). Reweighting alignment: to better eliminate spurious features and preserve invariant features without using environment labels, we assign large weights to node pairs with significant discrepancies in heterophilic Neighborhood Label Distribution (NLD) and minor discrepancies in homophilic NLD. See Section 3.2 for a detailed analysis of CIA-LRA. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Localized alignment. To avoid learning a collapsed representation of invariant features, it is crucial to align node pairs that share similar invariant features. To achieve this, we align nodes close to each other (about 2 to 6 hops). This is based on two observations. First, we observe that spurious features tend to exhibit larger changes within local graph areas than invariant ones, and nodes from the same class that are too distant may differ more in their invariant features than the closer ones (evidence in Appendix D.4). This is because invariant features are generally more stable than spurious ones, according to Chen et al. [2022], Sch\u00f6lkopf et al. [2021]. Second, we empirically find that alignment over an extensive range or too many nodes yields only marginal performance improvements, or even leads to performance degradation (see Appendix D.1), while increasing computational costs. This may also be attributed to the feature collapse caused by excessive alignment of too many node pairs. Formally, the local pairs are defined as $\\Omega_{c}(t)=\\{(i,j)|i\\neq j\\land Y_{i}=Y_{j}=c\\land d(i,j)\\leq t\\}$ , where $d(i,j)$ represents the the shortest path length from node $v_{i}$ to $v_{j}$ , $t\\in\\mathbb{N}^{\\mathfrak{\\top}}$ is a hyperparameter. Also, we propose to assign smaller weights to pairs more distant away from each other. ", "page_idx": 5}, {"type": "text", "text": "Reweighting Alignment. Since environment labels are unavailable, we need a metric to reflect the distribution of the spurious and invariant representations so that node pairs with significant/small differences in spurious/invariant features can be identified. Since we assume the causal patterns of the same class are similar (Assumption 2.1), the label distribution of homophilic (i.e., sameclass) neighbors directly affects the invariant features aggregated to the centered node (empirical evidence in Appendix D.5.2). Therefore, pairs with smaller differences in the ratio of homophilic neighbors should be assigned larger weights for alignment. The ratio discrepancy can be calculated as follows: $r_{.}^{\\mathrm{same}}(c)_{i,j}\\,=\\,\\bar{|r_{i}^{c}-\\bar{r}_{j}^{c}|}$ , where $r_{i}^{c}$ is the ratio of the neighbors of $v_{i}$ of class $c$ within $L$ hops $L$ is the number of layers of the GNN). As for spurious features, we utilize Heterophilic (i.e., different-class) Neighborhood Label Distribution (HeteNLD) as a measurement, as it affects the two kinds of main distributional shifts on graphs: 1) environmental node feature shifts, and 2) Neighborhood Label Distribution (NLD) shift (both empirically verified in Appendix D.5.1). HeteNLD determines the first kind of shift when correlations exist between labels and spurious node features, e.g., concept shift. The second kind, NLD shift, which is affected by HeteNLD, can be regarded as a structural shift as the discrepancy in neighborhood distribution will induce a gap in the aggregated representations (Theorem 4.4 shows this shift increases OOD error). Although aligning the representations significantly differing in homophilic neighbor ratio mitigates these two kinds of shifts, it also leads to the collapsed invariant representations and suboptimal performance (Table 4 shows this effect). Therefore we assign larger weights to the pair with a larger discrepancy in HeteNLD when alignment. The discrepancy in HeteNLD is calculated as follows: $\\begin{array}{r}{r^{\\mathrm{diff}}(c)_{i,j}=\\dot{\\sum_{c^{\\prime}\\neq c}}\\left|r_{i}^{c^{\\prime}}-r_{j}^{c^{\\prime}}\\right|\\!.}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Invariant Subgraph Extractor. In Section 2.1, the structural invariant features are defined as the $k$ -hop neighboring ego-graph for ease of analysis. However, in practice, the invariant structure may merely be a subgraph of the neighborhood nodes. To better capture the invariant ego subgraph, we train an invariant subgraph extractor inspired by Li et al. [2023a]. Concretely, we learn an auxiliary GNN encoder $\\phi_{\\theta_{m}}$ (parameterized by $\\theta_{m}$ ) to predict an soft edge mask $M\\in\\mathbf{\\dot{R}}^{N\\times N}$ , and then apply it during training and test: ", "page_idx": 6}, {"type": "text", "text": "Now we are ready to present the formal objective of CIA-LRA: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta_{h},\\Theta,\\theta_{m}}\\ \\mathcal{L}(h_{\\theta_{h}}\\circ\\phi_{\\Theta}(A_{m},X),Y)\\mathrm{~s.t.~}\\operatorname*{min}_{\\Theta,\\theta_{m}}\\ \\mathbb{E}_{c}\\mathbb{E}_{(i,j)\\in\\Omega_{c}(t)}\\left[w_{i,j}\\mathcal{D}(\\phi_{\\Theta}(i),\\phi_{\\Theta}(j))\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In Equation (9), $\\mathcal{L}$ is the cross-entropy loss, $\\Omega_{c}(t),\\boldsymbol{r}^{\\mathrm{diff}}(\\boldsymbol{c})_{i,j},\\boldsymbol{r}^{\\mathrm{same}}(\\boldsymbol{c})_{i,j}$ and $d(i,j)$ are defined in the above analysis. In practice, we use CIA-LRA as a regularization term added to the cross-entropy loss with a weight $\\lambda$ as a hyperparameter. The detailed implementation of CIA-LRA is in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "4 Theoretical Justification: an OOD Generalization Error Bound ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now will derive an OOD generalization error bound to show that optimizing CIA-LRA can minimize OOD error. To achieve this, we adopt a PAC-Bayesian framework following Ma et al. [2021] and establish a Contextual Stochastic Block Model (CSBM, [Deshpande et al., 2018]) for OOD multi-classification. The proposed CSBM-OOD is as follows (more discussions are in Appendix F.4): ", "page_idx": 6}, {"type": "text", "text": "Definition 4.1. (CSBM-OOD). For node $i$ of class $c$ from environment $e$ , its node feature $x_{i}\\in\\mathbb{R}^{D}$ consists of two parts, $x_{i}=[x_{\\mathrm{inv}}^{\\top};x_{\\mathrm{sp}}^{\\top}]^{\\top}$ , where $x_{\\mathrm{inv}}\\in\\mathbb{R}^{\\frac{D}{2}}$ sampled from the Gaussian distribution $\\mathcal{N}(\\mu_{c},\\sigma^{2}I)$ is the invariant feature and $\\boldsymbol{x}_{\\mathrm{sp}}\\in\\mathbb{R}^{\\frac{D}{2}}$ sampled from the $\\mathcal{N}(\\mu_{c}^{e},\\sigma^{2}I)$ is the spurious feature. 6 Suppose $\\{\\mu_{c}\\}$ and $\\{\\mu_{c}^{e}\\}$ for all $c$ and $e$ form sets of orthonormal basis. We use $p_{i}^{\\mathrm{hm}}$ to denote the homophilic ratio of node $i$ \u2019s one-hop neighbors and use $p_{i}^{\\mathrm{ht}}(c^{\\prime})$ to denote the heterophilic ratio of node $i$ \u2019s one-hop neighbors of class $c^{\\prime}$ ( $\\stackrel{\\cdot}{c}\\ne c^{\\prime},$ . We assume $\\Pr(y_{i}=c)$ are the same for all classes $c$ . ", "page_idx": 6}, {"type": "text", "text": "The GNN model used for deriving the error bound (following Ma et al. [2021]): The GNN model has a 1-layer mean aggregation $g$ that outputs the aggregated feature $g_{i}\\in\\mathbb{R}^{D}$ for node $i$ . The GNN classifier $h$ on top of $g$ is a ReLU-activated $L$ -layer MLP with $W_{1},...,W_{L}$ as parameters for each layer. $h$ is from a function family $\\mathcal{H}$ . The prediction for node $i$ is $h_{i}\\in\\mathbb{R}^{C}$ with $h_{i}[c]$ representing the predicted logit for class $c$ . Denote the largest width of all the hidden layers as $b$ . ", "page_idx": 6}, {"type": "text", "text": "Notations. Denote nodes of environment $e$ as $V_{e}$ . We consider the error of generalizing from a mixed training environment $e^{\\mathrm{tr}}$ to any test environment $e^{\\mathrm{te}}\\in\\mathcal{E}_{\\mathrm{te}}$ , where $V_{e^{\\mathrm{tr}}}:=\\cup_{e\\in\\mathcal{E}_{\\mathrm{tr}}}V_{e}$ represents all training nodes. To guarantee the generalization, we need to characterize the distance between $V_{e^{\\mathrm{{te}}}}$ and $V_{e^{\\mathrm{tr}}}$ : define $\\begin{array}{r}{\\epsilon_{e^{\\mathrm{te}},e^{\\mathrm{tr}}}\\,=\\,\\operatorname*{max}_{j\\in V_{e^{\\mathrm{te}}}}\\operatorname*{min}_{i\\in V_{e^{\\mathrm{tr}}}}\\|g_{i}\\,-\\,g_{j}\\|_{2}}\\end{array}$ as the aggregated feature distance between the training and test subgroup. Define the number of nodes in environment $e$ as $N_{e}$ . We consider the margin loss of environment $e$ that is used by Ma et al. [2021], Mao et al. [2023]: $\\begin{array}{r}{\\widehat{\\mathcal{L}}_{e}^{\\gamma}(h):=\\frac{1}{N^{e}}\\sum_{v_{i}\\in V_{e}}\\mathbf{1}\\left[h_{i}\\left[y_{i}\\right]\\leq\\gamma+\\operatorname*{max}_{c\\neq y_{i}}h_{i}[c]\\right]}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Now we introduce some assumptions adapted from Ma et al. [2021] that are used in our proof. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.2. (Equal-sized and disjoint near sets, adapted from Assumption 2 of Ma et al. [2021]) For each node $v_{i}\\in V_{e}\\mathrm{u}$ , define $\\begin{array}{r}{V_{e^{\\mathrm{te}}}^{(i)}:=\\left\\{j\\in V_{e^{\\mathrm{te}}}\\mid\\|g_{i}-g_{j}\\|_{2}\\leq\\epsilon_{e^{\\mathrm{tr}},e^{\\mathrm{te}}}\\right\\}\\!.}\\end{array}$ . For any test environment $e^{\\mathrm{te}}$ , assume $V_{e^{\\mathrm{te}}}^{(i)}$ of each $v_{i}\\in V_{e}\\mathrm{u}$ are disjoint and have the same size $N_{e^{\\mathrm{te}}}\\in\\mathbb{N}^{+}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.3. (concentrated expected loss difference, adapted from Assumption 3 of Ma et al. [2021]) Let $P$ be a distribution on $\\mathcal{H}$ , defined by sampling the vectorized MLP parameters from ${\\mathcal{N}}\\left(0,\\sigma^{2}I\\right)$ for some $\\begin{array}{r}{\\sigma^{2}\\,\\le\\,\\frac{\\big(\\gamma/8\\epsilon_{e^{\\mathrm{te}},e^{\\mathrm{tr}}}\\big)^{2/L}}{2b\\big(\\lambda N_{e^{\\mathrm{tr}}}^{-\\alpha}+\\ln2b L\\big)}}\\end{array}$ . For any $L$ layer GNN classifier $h\\in\\mathcal H$ with model parameters $W_{1}^{h},\\dots,W_{L}^{h}$ , define $\\begin{array}{r}{T_{h}:=\\operatorname*{max}_{l=1,\\ldots,L}\\left\\|W_{l}\\right\\|_{2}}\\end{array}$ . Assume that there exists some $0<\\alpha<$ $\\frac{1}{4}$ satisfying ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{Pr}_{h\\sim P}\\left(\\mathcal{L}_{e^{\\mathrm{t}}}^{\\gamma/4}(h)-\\mathcal{L}_{e^{\\mathrm{t}}}^{\\gamma/2}(h)>N_{e^{\\mathrm{t}}}^{-\\alpha}+H C\\epsilon^{\\mathrm{e}},\\mathrm{e}^{\\mathrm{t}}\\,\\Big|\\,T_{h}^{L}\\epsilon_{e^{\\mathrm{t}},e^{\\mathrm{t}}}>\\frac{\\gamma}{8}\\right)\\le e^{-N_{e^{\\mathrm{t}}}^{2\\alpha}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Now we present the node-level OOD generalization bound (proof in Appendix G.3): ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.4. (Subgroup OOD Generalization Bound for GNNs, informal). Let $\\tilde{h}$ be any classifier in a function family $\\mathcal{H}$ with parameters $\\left\\{\\widetilde{W}_{l}\\right\\}_{l=1}^{L}$ . Under Assumption 4.2 and 4.3, for any $e^{t e}\\,\\in$ , and large enough , there exist $\\begin{array}{r}{0<\\alpha<\\frac{1}{4}}\\end{array}$ , we have ", "page_idx": 7}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/d07bb7a9157abffc3c228b46ea52b502e9fdcc702ce53d00bee6471aa1f6da74.jpg", "img_caption": ["where $p_{i}^{h t}(c^{\\prime}|c)$ is the ratio of heterophilic neighbors of class $c^{\\prime}$ when $\\begin{array}{r c l c l}{y_{i}}&{=}&{c,}&{B_{e^{t e}}}&{=}\\end{array}$ $\\operatorname*{max}_{i\\in V_{e^{t r}}\\cup V_{e^{t e}}}\\left|\\left|g_{i}\\right|\\right|_{2}$ is the maximum feature norm, $V_{e^{t e}}^{(i)}:=\\left\\{j\\in V_{e^{t e}}\\mid\\|g_{i}-g_{j}\\|_{2}\\leq\\epsilon_{e^{t r},e^{t e}}\\right\\}$ . const is a constant depending on $\\alpha,\\,\\delta,$ , and $\\gamma$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "The observations from Theorem 4.4 is summarized as follows: Term (a) reflects the separability of the original features of different classes $|[(\\mu_{c}-\\mu_{c^{\\prime}})^{\\top};(\\mu_{c}^{e^{\\mathrm{te}}}-\\mu_{c^{\\prime}}^{e^{\\mathrm{te}}})^{\\top}]|$ and the distance of the aggregated features between the training and test set $\\epsilon_{e^{\\mathrm{te}},e^{\\mathrm{tr}}}$ . The former factor is the nature of the dataset itself. Term ${\\bf(b)}$ is the distributional discrepancy between the training and test subgroups, caused by the distribution shifts in spurious features. When there exist correlations between labels and spurious features, CIA-LRA can minimize this term by minimizing the representation distance of node pairs with large discrepancy in the label distribution of heterophilic neighbors7. Term (c) measures the shift in HeteNLD between the training and test subgroups, representing the OOD error caused by the shift in the aggregated features of the same class. CIA-LRA minimizes this term by enforcing stronger alignment on pairs with greater HeteNLD differences. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Experiment Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We run experiments using 3-layer GAT and GCN on GOOD [Gui et al., 2022], a graph OOD benchmark. We reported the results on both covariate shift and concept shift. The detailed experimental setup and hyperparameter settings are in Appendix C. We compare our methods with the following algorithms: ERM [Vapnik, 1999]; traditional invariant learning methods: IRM, VREx, GroupDRO [Sagawa et al., 2019], Deep Coral [Sun and Saenko, 2016], IGA [Koyama and Yamaguchi, 2020]; graph OOD methods: EERM, SRGNN, CIT [Xia et al., 2023], CaNet [Wu et al., 2024]; graph data augmentation: Mixup [Wang et al., 2021a], GTrans [Jin et al., 2022]. ", "page_idx": 7}, {"type": "text", "text": "5.2 Main Results of OOD Generalization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 2 reports the main OOD generalization results. The observations are summarized as follows: 1) CIA-LRA improves the best baseline methods by $2.44\\%$ and $3.23\\%$ on GAT and GCN, respectively, ", "page_idx": 7}, {"type": "text", "text": "Table 2: OOD test accuracy $(\\%)$ . Our methods are marked in bold. The best and second-best results are shown in bold and underlined, respectively. The values in parentheses are standard deviations. \u2019OOM\u2019 denotes Out Of Memory. Results of EERM on Cora GCN marked by \u2019\\*\u2019 are from Gui et al. [2022] since we got OOM during our running. ", "page_idx": 8}, {"type": "table", "img_path": "7eFS8aZHAM/tmp/38ed2eb1f3b111acecdfbb6a054c69f77f8114e2474aa06b2b7899b9bce745d0.jpg", "table_caption": ["(a) Results on GAT "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "achieving state-of-the-art performance. 2) CIA outperforms IRM and VREx on all splits, which validates our theoretical findings in Section 2. Notably, it performs best among the non-graph-specific methods. 3) CIA-LRA improves CIA in most cases. This suggests that our reweighting strategy can enhance generalization on graphs even without environment labels. 4) MatchDG outperforms IRM and VREx on 12 out of 16 splits but underperforms CIA on average (averaged over 16 splits except on Arxiv, CIA: 57.56, MatchDG: 56.73). ", "page_idx": 8}, {"type": "text", "text": "5.3 CIA can be Integrated into and Improve other Graph-OOD Methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We replace VREx with CIA in the loss function of EERM to show that CIA can improve generalization in a plug-andplay manner. Table 3 shows that this improves original EERM by a large margin or has comparable performances, indicating the performance of node-level OOD algorithms can be limited by VREx. ", "page_idx": 8}, {"type": "table", "img_path": "7eFS8aZHAM/tmp/b3ab9ca029d9b8ff131b0a06cc6b325b12b6cce1544ee919de78dc21789a3441.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: By replacing VREx in EERM with CIA (marked as EERM-CIA), the performance is significantly improved. ", "page_idx": 8}, {"type": "text", "text": "5.4 Empirical Understanding of the Role of CIA-LRA ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "A synthetic dataset. We construct a synthetic dataset (mentioned in Section 1) to validate the role of each module in CIA-LRA in eliminating spurious features and preventing the collapse of invariant representations. We generate a random graph and create a 4-class OOD classification task. Each node has a 4-dim feature, with the first/last two dimensions representing invariant/spurious features (details in Appendix C.3), so we can disentangle the learned invariant and spurious representations. ", "page_idx": 8}, {"type": "text", "text": "Figure 3 depicts the OOD accuracy, the variance of the invariant representation, and the norm of the spurious representation across training epochs for CIA and CIA-LRA. The observations are summarized below. ", "page_idx": 9}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/ebe27a5ac75b1ed7c7a9abdd4e751a0cf496725ff1699d282ade186df2f53b89.jpg", "img_caption": ["Figure 3: Left: OOD test accuracy. Mid: the variance of the invariantaveraged on the four representation. Right: the norm of the spurious representation. CIAsplits of Cora. and CIA-LRA use $\\lambda=0.5$ in this figure. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "1) Aligning the large discrepancy in HeteNLD helps to eliminate spurious features on concept shift and improves generalization. As evident from the right column, incorporating $r^{\\mathrm{diff}}$ diminishes the norm of spurious features under concept shift. For covariate shift, while $r^{\\mathrm{diff}}$ will not remove environmental spurious features due to their independence from labels, it still helps generalization since it reduces the error caused by shifts in HeteNLD as predicted by Theorem 4.4. 2) CIA-LRA alleviates collapse of causal representation that CIA may suffer when adopting a substantial $\\lambda$ . When using a large $\\lambda$ $\\mathrm{\\Delta(=0.5)}$ , the performance of CIA deteriorates to the level of random guessing $(25\\%)$ after approximately 50 epochs. In contrast, CIA-LRA sustains its accuracy at a high level because it avoids excessive alignment by aligning only local pairs and reweighting (further evidence in Appendix D.2). The mid column shows that the invariant features learned by CIA progressively collapse, even if CIA removes most spurious features (right column). 3) Maintaining the discrepancy in homophilic neighboring label distribution $r^{\\mathrm{same}}$ helps keep the variance of the invariant representation, slightly improving performance. ", "page_idx": 9}, {"type": "text", "text": "Ablation study. We also conduct ablation studies on CIA-LRA. Table 4 shows that removing any module causes a significant performance drop, demonstrating the effectiveness of each module. ", "page_idx": 9}, {"type": "text", "text": "5.5 Effects of the Hyperparameters of CIA-LRA ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This section analyzes the effect of $\\lambda$ and $t$ of CIA-LRA. Figure 4 shows that the test accuracy increases with $\\lambda$ when $\\lambda\\,\\leq\\,0.5$ . Too small $t$ leads to a sub-optimal performance due to insufficient regularization from aligning only a few pairs. Also, most parameter combinations outperform the baseline methods, indicating that CIA-LRA leads to consistently superior performance. Additional studies of the effects of $\\lambda$ and $t$ are in Appendix D.2 and D.3, respectively. ", "page_idx": 9}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/52a76948cc4f736945389fc3cd3493445e545d32657b52aab72952e5a106d8be.jpg", "img_caption": ["(a) Cora degree concept(b) Cora degree covariate shift. ERM: 61.36, VREx:shift. ERM: 55.30, VREx: 61.42, IRM: 61.77. 55.07, IRM: 55.34. ", "Figure 4: Effect of $\\lambda$ and the number of hops $t$ on OOD test accuracy $(\\%)$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, by theoretically dissecting the failure of IRM and VREx in node-level graph OOD tasks, we attribute it to the difficulty in identifying the graph-specific causal pattern structures. To address this, we propose CIA with additional class-conditional invariance constraints and its environmentlabel-free variant CIA-LRA tailored for graph OOD scenarios. Further theoretical and experimental results validate their efficacy. Notably, CIA can be incorporated in other graph OOD frameworks, serving as a better invariant learning objective than the widely-used VREx on graphs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Yisen Wang was supported by National Key R&D Program of China (2022ZD0160300), National Natural Science Foundation of China (92370129, 62376010), and Beijing Nova Program (20230484344, 20240484642). Xianghua Ying was supported by the National Natural Science Foundation of China (62371009). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. In ICML, 2020. ", "page_idx": 10}, {"type": "text", "text": "David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In ICML. PMLR, 2021.   \nManh-Ha Bui, Toan Tran, Anh Tran, and Dinh Phung. Exploiting domain-specific features to enhance domain generalization. In NeurIPS, 2021.   \nAlexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution generalization. In ICML. PMLR, 2022.   \nYuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. arXiv preprint arXiv:2104.09937, 2021.   \nDivyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In ICML. PMLR, 2021.   \nDinghuai Zhang, Kartik Ahuja, Yilun Xu, Yisen Wang, and Aaron Courville. Can subnetwork structure be the key to out-of-distribution generalization? In ICML, 2021a.   \nRuoyu Wang, Mingyang Yi, Zhitang Chen, and Shengyu Zhu. Out-of-distribution generalization with causal invariant transformations. In CVPR, 2022a.   \nMingyang Yi, Ruoyu Wang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming Ma. Breaking correlation shift via conditional invariant regularizer. In ICLR, 2022.   \nQixun Wang, Yifei Wang, Hong Zhu, and Yisen Wang. Improving out-of-distribution generalization by adversarial training with structured priors. In NeurIPS, 2022b.   \nShiji Xin, Yifei Wang, Jingtong Su, and Yisen Wang. On the connection between invariant learning and adversarial training for out-of-distribution generalization. In AAAI, 2023.   \nQitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. Handling distribution shifts on graphs: An invariance perspective. In ICLR, 2021.   \nHaoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Invariant node representation learning under distribution shifts with multiple latent environments. ACM Transactions on Information Systems, (1):1\u201330, 2023a.   \nYang Liu, Xiang Ao, Fuli Feng, Yunshan Ma, Kuan Li, Tat-Seng Chua, and Qing He. Flood: A flexible invariant learning framework for out-of-distribution generalization on graphs. In SIGKDD, 2023.   \nShengyu Zhang, Kun Kuang, Jiezhong Qiu, Jin Yu, Zhou Zhao, Hongxia Yang, Zhongfei Zhang, and Fei Wu. Stable prediction on graphs with agnostic distribution shift. arXiv preprint arXiv:2110.03865, 2021b.   \nQin Tian, Wenjun Wang, Chen Zhao, Minglai Shao, Wang Zhang, and Dong Li. Graphs generalization under distribution shifts. arXiv preprint arXiv:2403.16334, 2024.   \nPetar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In ICLR, 2018.   \nQitian Wu, Fan Nie, Chenxiao Yang, Tianyi Bao, and Junchi Yan. Graph out-of-distribution generalization via causal intervention. arXiv preprint arXiv:2402.11494, 2024.   \nXiner Li, Shurui Gui, Youzhi Luo, and Shuiwang Ji. Graph structure and feature extrapolation for out-of-distribution generalization. arXiv preprint arXiv:2306.08076, 2023b.   \nShurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. Good: A graph out-of-distribution benchmark. In NeurIPS, 2022.   \nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \nDavide Buffelli, Pietro Li\u00f2, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.   \nDonglin Xia, Xiao Wang, Nian Liu, and Chuan Shi. Learning invariant representations of graph neural networks via cluster generalization. In NeurIPS, 2023.   \nFelix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, ICML, 2019.   \nYongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, MA Kaili, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of-distribution generalization on graphs. In NeurIPS, 2022.   \nBernhard Sch\u00f6lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning, 2021.   \nJiaqi Ma, Junwei Deng, and Qiaozhu Mei. Subgroup generalization and fairness of graph neural networks. In NeurIPS, 2021.   \nYash Deshpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. Contextual stochastic block models. In NeurIPS, 2018.   \nHaitao Mao, Zhikai Chen, Wei Jin, Haoyu Han, Yao Ma, Tong Zhao, Neil Shah, and Jiliang Tang. Demystifying structural disparity in graph neural networks: Can one size fti all? In NeurIPS, 2023.   \nVladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks, (5):988\u2013999, 1999.   \nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.   \nBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV Workshop. Springer, 2016.   \nMasanori Koyama and Shoichiro Yamaguchi. Out-of-distribution generalization with maximal invariant predictor. 2020.   \nYiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph classification. In WWW, 2021a.   \nWei Jin, Tong Zhao, Jiayuan Ding, Yozen Liu, Jiliang Tang, and Neil Shah. Empowering graph representation learning with test-time graph transformation. arXiv preprint arXiv:2210.03561, 2022.   \nJunchi Yu, Jian Liang, and Ran He. Mind the label shift of augmentation-based graph ood generalization. In CVPR, 2023.   \nHaoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Learning invariant graph representations for out-of-distribution generalization. In NeurIPS, 2022a.   \nQi Zhu, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. Shift-robust gnns: Overcoming the limitations of localized graph training data. In NeurIPS, 2021.   \nYongqiang Chen, Yatao Bian, Kaiwen Zhou, Binghui Xie, Bo Han, and James Cheng. Rethinking invariant graph representation learning without environment partitions. In ICLR Workshop, 2023a.   \nFredrik D Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in domaininvariant representations. In AISTATS. PMLR, 2019.   \nLing Yang, Jiayi Zheng, Heyuan Wang, Zhongyi Liu, Zhilin Huang, Shenda Hong, Wentao Zhang, and Bin Cui. Individual and structural graph information bottlenecks for out-of-distribution generalization. IEEE Transactions on Knowledge and Data Engineering, 2023.   \nYing-Xin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. arXiv preprint arXiv:2201.12872, 2022a.   \nNianzu Yang, Kaipeng Zeng, Qitian Wu, Xiaosong Jia, and Junchi Yan. Learning substructure invariance for out-of-distribution molecular representations. In NeurIPS, 2022.   \nTianrui Jia, Haoyang Li, Cheng Yang, Tao Tao, and Chuan Shi. Graph invariant learning with subgraph co-mixup for out-of-distribution generalization. arXiv preprint arXiv:2312.10988, 2023.   \nHaoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Ood-gnn: Out-of-distribution generalized graph neural network. IEEE Transactions on Knowledge and Data Engineering, 2022b.   \nYongqiang Chen, Yatao Bian, Kaiwen Zhou, Binghui Xie, Bo Han, and James Cheng. Does invariant graph learning via environment augmentation learn invariance? In NeurIPS, 2023b.   \nXuexin Chen, Ruichu Cai, Kaitao Zheng, Zhifan Jiang, Zhengting Huang, Zhifeng Hao, and Zijian Li. Unifying invariance and spuriousity for graph out-of-distribution via probability of necessity and sufficiency. arXiv preprint arXiv:2402.09165, 2024.   \nShurui Gui, Meng Liu, Xiner Li, Youzhi Luo, and Shuiwang Ji. Joint learning of label and environment causal independence for graph out-of-distribution generalization. In NeurIPS, 2024.   \nDavid Burshtein, Vincent Della Pietra, Dimitri Kanevsky, and Arthur Nadas. Minimum impurity partitions. The Annals of Statistics, pages 1637\u20131646, 1992.   \nBernhard Sch\u00f6lkopf. Causality for machine learning. In Probabilistic and Causal Inference: The Works of Judea Pearl, pages 765\u2013804. 2022.   \nYao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is homophily a necessity for graph neural networks? In ICLR, 2022.   \nJincheng Huang, Ping Li, Rui Huang, Na Chen, and Acong Zhang. Revisiting the role of heterophily in graph representation learning: An edge classification perspective. ACM Transactions on Knowledge Discovery from Data, 2023.   \nXinyi Wu, Zhengdao Chen, William Wei Wang, and Ali Jadbabaie. A non-asymptotic analysis of oversmoothing in graph neural networks. In ICLR, 2022b.   \nHuayi Tang and Yong Liu. Towards understanding the generalization of graph neural networks. In ICML. PMLR, 2023.   \nHao Zhu and Piotr Koniusz. Simple spectral graph convolution. In ICLR, 2021.   \nYifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Dissecting the diffusion process in linear graph convolutional networks. In NeurIPS, 2021b.   \nYong Lin, Lu Tan, Yifan Hao, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, and Tong Zhang. Spurious feature diversification improves out-of-distribution generalization. arXiv preprint arXiv:2309.17230, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Additional Related Works 15 ", "page_idx": 13}, {"type": "text", "text": "A.1 Invariant Learning for OOD Generalization . . 15   \nA.2 Graph-OOD Works Using VREx and Similar Variants . 15   \nA.3 Comparison with Existing Node-level OOD Generalization Works . 16   \nA.4 Graph-level OOD Generalization . . 17 ", "page_idx": 13}, {"type": "text", "text": "B Additional Theoretical Results of the Covariate Shift Case 17 ", "page_idx": 13}, {"type": "text", "text": "B.1 Theoretical Model Setup of the Covariate Shift Case 17   \nB.2 Theoretical Results of the Covariate Shift Case . 17 ", "page_idx": 13}, {"type": "text", "text": "C Detailed Experimental Setup 18 ", "page_idx": 13}, {"type": "text", "text": "C.1 Basic Settings . . 18   \nC.2 Hyperparameter Settings of the Main OOD Generalization Results . 18   \nC.3 Details of the Toy Dataset . . 18 ", "page_idx": 13}, {"type": "text", "text": "D Additional Experimental Results 19 ", "page_idx": 13}, {"type": "text", "text": "D.1 Excessive Alignment Leads to the Collapse of the Invariant Features. . . 19   \nD.2 CIA-LRA Alleviates Representation Collapse Caused by Excessive Alignment 19   \nD.3 Effect of the Number of Hops for Localized Alignment . . . . . . . . . 20   \nD.4 Discussion and Validation of the Assumption on the Rate of Change of Causal and Spurious Features w.r.t Spatial Position . . . 20   \nD.5 Discussion and Validation of the Assumption on the Feature Distance and Neighborhood Label Distribution Discrepancy . . . 21   \nD.6 Validation of the True Feature Generation Depth . . . . 26   \nD.7 Time Cost of CIA-LRA . . 29 ", "page_idx": 13}, {"type": "text", "text": "E Detailed training procedure 30 ", "page_idx": 13}, {"type": "text", "text": "F Additional Discussion of Theoretical Settings and Results 30 ", "page_idx": 13}, {"type": "text", "text": "F.1 Detailed Setup of the Theoretical Model in Section 2 . . . 30   \nF.2 Discussion of the Structural Feature Considered in the Theoretical Model and   \nJustification for the Choice of the Analyzed GNN . . . . . 31   \nF.3 Discussion of the Failure Solution for GNNs of VREx and IRMv1 . . 31   \nF.4 The Superiority of the Proposed CSBM-OOD . . . . . 32   \nF.5 Tightness of the Error Bound of Theorem 4.4 . . 32 ", "page_idx": 13}, {"type": "text", "text": "G Proofs of the Theoretical Results 32 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "G.1 Proofs of the Concept Shift Case Presented in the Main Text . 32   \nG.2 Proof of the Covariate Shift Case . . 39   \nG.3 Proof of Theorem 4.4 . 45 ", "page_idx": 13}, {"type": "text", "text": "H Limitations 48 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "I Broader Impacts 48 ", "page_idx": 13}, {"type": "text", "text": "J Compute Resources 49 ", "page_idx": 13}, {"type": "text", "text": "A Additional Related Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Invariant Learning for OOD Generalization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Invariant learning seeks to find stable features across multiple training environments to achieve OOD generalization [Arjovsky et al., 2020, Krueger et al., 2021, Bui et al., 2021, Rame et al., 2022, Shi et al., 2021, Mahajan et al., 2021, Wang et al., 2022a, Yi et al., 2022, Wang et al., 2022b]. IRM [Arjovsky et al., 2020] and VREx are two of the most well-known methods. The goal of IRM is to learn a representation that elicits a classifier to achieve optimality in all training environments. VREx [Krueger et al., 2021] reduces the variance of risks across training environments to improve robustness against distribution shifts. Mahajan et al. [2021] argues that the invariant features can also change across environments. Hence, they proposed MatchDG to align the representations of the so-called same \"object\" of different environments. To the best of our knowledge, we are the first work to theoretically analysis the limitations of IRM and VREx in OOD node classification problems. ", "page_idx": 14}, {"type": "text", "text": "It\u2019s worth emphasizing that although the CIA objective is similar to MatchDG [Mahajan et al., 2021], our extensions of MatchDG on graphs are non-trivial: ", "page_idx": 14}, {"type": "text", "text": "1) The extensions from MatchDG to CIA are mainly theoretical. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 We extend the idea of MatchDG to the node-level OOD task by providing a theoretical characterization of CIA\u2019s working mechanism on graphs (Theorem 3.1), revealing its superiority in node-level OOD scenarios for the first time.   \n\u2022 We establish the connection between node-level OOD error and two kinds of distributional shifts: (1) environmental node feature shifts and (2) the heterophilic neighborhood label distribution shifts (see Theorem 4.4), giving further explanation of CIA and CIA-LRA\u2019s performance gain in node-level OOD generalization. ", "page_idx": 14}, {"type": "text", "text": "2) The methodological extensions of CIA-LRA: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 It identifies the node pairs with significant differences in spurious features without using environment labels, providing a new perspective that the widely adopted environment partition paradigm Wu et al. [2021], Yu et al. [2023], Li et al. [2023b,a, 2022a], Liu et al. [2023] may not be necessary for node-level OOD generalization. One can remove spurious features by leveraging neighboring label distributions (analysis in Section 3.2), shedding light on the role of neighborhood label distribution as compensation for the absence of environment labels.   \n\u2022 It\u2019s the first node-level OOD method explicitly considering the OOD error caused by shifts in heterophilic neighborhood label distribution, (pointed out by term (c) in Theorem 4.4). Such shifts can be regarded as a kind of structural shift that has been first observed in Mao et al. [2023].   \n\u2022 It\u2019s the first node-level OOD method using homophilic neighborhood label distribution to reflect the find-grained distribution of the invariant features, avoiding the collapse of invariant features. ", "page_idx": 14}, {"type": "text", "text": "A.2 Graph-OOD Works Using VREx and Similar Variants ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We summarize the graph-OOD works that include VREx or IGA as part of the training objectives below. Node-level works, which can be covered by our analysis: ", "page_idx": 14}, {"type": "text", "text": "\u2022 EERM [Wu et al., 2021], Equation (5): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathrm{Var}\\left(\\left\\{L\\left(g_{w_{k}^{*}}(G),Y;\\theta\\right):1\\leq k\\leq K\\right\\}\\right)+\\frac{\\beta}{K}\\sum_{k=1}^{K}L\\left(g_{w_{k}^{*}}(G),Y;\\theta\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $g_{w_{k}^{*}}(G)$ is the generated $k$ -th environment. ", "page_idx": 14}, {"type": "text", "text": "\u2022 INL [Li et al., 2023a], Equation (8): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathrm{e}{\\mathrm{~supp}}(\\mathcal{E}_{\\mathrm{infer}})}\\mathcal{R}^{e}\\left(f\\left(\\mathrm{G}_{\\mathrm{v}}\\right),\\mathrm{y};\\theta\\right)+\\lambda\\operatorname{trace}{\\mathrm{~}\\big(}\\mathrm{Var}_{\\mathcal{E}_{\\mathrm{infer}}}\\left(\\nabla_{\\theta}\\mathcal{R}^{e}\\right){\\big)}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathcal{E}_{\\mathrm{infer}}$ is the inferred environment label. ", "page_idx": 14}, {"type": "text", "text": "\u2022 FLOOD [Liu et al., 2023], Equation (12): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta,\\omega,\\psi}\\mathcal{L}_{\\mathrm{train}}\\,=\\mathcal{L}(\\theta,\\omega)+\\alpha\\mathcal{R}_{\\mathrm{V-REx}}(\\omega,\\psi)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u2022 Zhang et al. [2021b] Equation (10): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{global}}\\,=\\sum_{e,e^{\\prime}\\in\\mathcal{E}}\\left(\\mathcal{L}_{\\mathrm{pred}}^{0,e}\\,-\\mathcal{L}_{\\mathrm{pred}}^{0,e^{\\prime}}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as they claimed: \"We note that minimizing the pair-wise distance of losses as defined in Equation 10 is equivalent to minimizing the variance of losses.\" ", "page_idx": 15}, {"type": "text", "text": "\u2022 GLIDER [Tian et al., 2024], Equation (9): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\mathbb{V}_{e}\\left[\\ell\\left(f_{c}\\left(g\\left(G_{v}^{e}\\right)\\right),y_{v}^{e}\\right)\\right]+\\alpha\\mathbb{E}_{e}\\left[\\ell\\left(f_{c}\\left(g\\left(G_{v}^{e}\\right)\\right),y_{v}^{e}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Graph-level works, which are not covered in our analysis: ", "page_idx": 15}, {"type": "text", "text": "\u2022 LiSA [Yu et al., 2023], Equation (15): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f}\\mathcal{L}_{c l s}\\left(f,\\left\\{g_{i}^{*}\\right\\}_{i=1}^{n}\\right)+\\operatorname{Var}_{e}\\left(\\mathcal{L}_{c l s}\\left(f,g_{i}^{*}\\right)\\right),i=1\\sim n,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $g_{i}^{*}$ are the augmented training subgraphs of representing different environments. ", "page_idx": 15}, {"type": "text", "text": "\u2022 G-splice [Li et al., 2023b], Equation (6): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\boldsymbol{\\psi}^{*}:=\\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\psi}}[\\mathbf{\\mathcal{E}}_{(G,\\boldsymbol{y})\\sim\\cup_{\\boldsymbol{\\varepsilon}\\in\\left\\{\\varepsilon\\cup\\varepsilon_{A}\\right\\}}}\\boldsymbol{P}_{\\varepsilon}\\left[\\ell\\left(f_{\\boldsymbol{\\psi}}(G),\\boldsymbol{y}\\right)\\right]+\\gamma\\operatorname{Var}_{\\varepsilon\\in\\left\\{\\varepsilon\\cup\\varepsilon_{A}\\right\\}}\\left[\\mathbb{E}_{(G,\\boldsymbol{y})\\sim P_{\\varepsilon}}\\ell\\left(f_{\\boldsymbol{\\psi}}(G),\\boldsymbol{y}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathcal{E}_{A}$ are the augmented environments. ", "page_idx": 15}, {"type": "text", "text": "\u2022 GIL [Li et al., 2022a], Equation (8): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{e\\in\\mathrm{supp}(\\mathcal{E}_{i n f e r})}\\mathcal{R}^{e}(f(\\mathrm{G}),\\mathrm{Y};\\theta)+\\lambda\\operatorname{trace}\\left(\\mathrm{Var}_{\\mathcal{E}_{\\mathrm{infer}}}\\left(\\nabla_{\\theta}\\mathcal{R}^{e}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathcal{E}_{\\mathrm{infer}}$ is the inferred environment label. ", "page_idx": 15}, {"type": "text", "text": "A.3 Comparison with Existing Node-level OOD Generalization Works ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Among the graph OOD methods, one line of research focuses on the node-level OOD generalization [Wu et al., 2021, Liu et al., 2023, Zhu et al., 2021, Li et al., 2023a, Xia et al., 2023]. We summarize the drawbacks of previous node-level OOD methods as follows. 1) it is hard for environmentinference-based methods to generate reliable environments. To generate environments, FLOOD [Liu et al., 2023] uses random data augmentation that lacks sufficient prior. EERM [Wu et al., 2021] generates environments by maximizing loss variance, which may not necessarily enlarge differences in spurious features across environments that have been proven to be crucial for invariant learning [Chen et al., 2023a]. Also, the adversarial learning of its environment generation process may lead to unstable performance (Table 2) and high training costs. INL [Li et al., 2023a] relies on an estimated invariant ego-graph of each node, whose quality could significantly affect performance. Moreover, all these methods need to manually specify the number of environments, which could be inaccurate. 2) previous node-level invariant learning objectives also have some limitations. For instance, Zhang et al. [2021b], Wu et al. [2021], Liu et al. [2023], Li et al. [2023a], Tian et al. [2024] use VREx Krueger et al. [2021] as their invariance regularization. However, we theoretically prove its potential failure on node-level OOD tasks in Section 2.2. SRGNN [Zhu et al., 2021] only aligns the marginal distribution $p(X)$ between the biased training distribution and the given unbiased distribution, which has been proved to have failure cases [Johansson et al., 2019]. 3) some work are based on intuitive guidelines and lacks theoretical guarantees on the OOD generalization performance [Liu et al., 2023, Zhu et al., 2021, Yang et al., 2023]. Our proposed CIA-LRA achieves invariant learning without complex environment inference that could be unstable through a representation alignment manner. Additionally, we provide theoretical guarantees for our invariant learning objective and empirically validate its working mechanism. ", "page_idx": 15}, {"type": "text", "text": "Comparison between our Theorem 2.3 and Theorem 1 of Wu et al. [2021] It\u2019s worth mentioning that Theorem 1 of $\\mathrm{Wu}$ et al. [2021] proves min\u0398 $\\mathbb{V}_{e}[R(e)]$ will min $\\iota\\,I(y,e|z)$ , where $q(z|x)$ is the induced distribution by encoder $\\phi$ . This seems to confilct with Theorem 2.3. This is because the upper bound derived in $\\mathrm{Wu}$ et al. [2021] $I(y,e|z)\\,\\leq\\,D_{\\mathrm{KL}}(q(y|z)\\|\\mathbb{E}_{e}[q(y|z)])\\,\\leq\\,\\mathbb{V}_{e}[R(e)]$ is not tight. Thus, minimizing the variance of loss across training environments does not necessarily minimize mutual information between the label and the environment given the learned representation. ", "page_idx": 15}, {"type": "text", "text": "A.4 Graph-level OOD Generalization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "There has been a substantial amount of work focusing on the OOD generalization problem on graphs. The vast majority have centered on graph classification tasks[Chen et al., 2022, Wu et al., 2022a, Li et al., 2022a, 2023b, Yang et al., 2022, Yu et al., 2023, Chen et al., 2023a, Buffelli et al., 2022, Jia et al., 2023, Li et al., 2022b, Chen et al., 2023b, 2024, Gui et al., 2024]. Most works aim at identifying the invariant subgraph of a whole graph through specific regularization so that the model can use it when inference. Compared to node-level OOD generalization, the graph-level one is more akin to traditional OOD generalization, as the individual samples (graphs) are independently distributed. We focus on the more challenging node-level OOD generalization in this work. ", "page_idx": 16}, {"type": "text", "text": "B Additional Theoretical Results of the Covariate Shift Case ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Theoretical Model Setup of the Covariate Shift Case ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we will extend our theoretical model in the main text to the covariate shift setting. The causal graph of the covariate shift is shown in Figure 1b. For the covariate shift setting, spurious features are independent of $Y$ , while $X$ changes with environment $e$ . Thus we can model the data generation process for environment $e$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\nY^{e}=(\\tilde{A}^{e})^{k}X_{1}+n_{1},\\quad X_{2}^{e}=n_{2}+\\epsilon^{e},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the definition of $n_{1}$ and $n_{2}$ are the same as Section 2, $\\epsilon^{e}$ represents environmental spurious features. $\\epsilon_{i}^{e}$ (each dimension of $\\epsilon^{e}$ ) is a random variable that are independent for $i\\,=\\,1,...,N^{e}$ . We assume the intra-environment expectation of the environment spurious variable is $\\mathbb{E}_{\\epsilon_{i}\\sim p_{e}}[\\epsilon_{i}]=$ $\\mu^{e}\\in\\mathbb{R}$ since spurious features are consistent in a certain environment. We further assume the crossenvironment expectation $\\mathbb{E}_{e}[\\pmb{\\epsilon}^{e}]=\\mathbf{0}$ and cross-environment variance $\\mathbb{V}_{e}[\\epsilon_{i}^{e}]=\\sigma^{2}$ , $i=1,...,N^{e}$ for simplicity. This is consistent with the covariate shift case that $p(X)$ can arbitrarily change across different domains, and the support set of $X$ may vary. Also, we require $L\\geq k$ to ensure the GNN has enough capacity to learn the causal representations. ", "page_idx": 16}, {"type": "text", "text": "B.2 Theoretical Results of the Covariate Shift Case ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Now we will present similar conclusions as the concept shift case. Even if VREx and IRMv1 can successfully capture invariant features in the non-graph task, they induce a model that uses spurious features. Still, CIA can learn invariant representations under covariate shift. ", "page_idx": 16}, {"type": "text", "text": "Proposition B.1. (VREx and IRMv1 learn invariant features for non-graph tasks under covariate shift, proof is in Appendix G.2.1) For the non-graph version of the SCM in Equation (19), ", "page_idx": 16}, {"type": "equation", "text": "$$\nY^{e}=X_{1}+n_{1},\\;X_{2}^{e}=n_{2}+\\epsilon^{e},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Optimizing $V R E x$ min\u0398 $\\mathcal{L}_{V R E x}=\\mathbb{V}_{e}[R(e)]$ and IRMv1 $\\begin{array}{r}{\\operatorname*{min}_{\\Theta}\\mathcal{L}_{I R M\\nu I}=\\mathbb{E}_{e}[||\\nabla_{w|w=1.0}R(e)||^{2}]\\;\\nu}\\end{array}$ will learn invariant features when using a $^{\\,l}$ -layer linear network: $f(X)=\\theta_{1}X_{1}+\\theta_{2}X_{2}$ . ", "page_idx": 16}, {"type": "text", "text": "Proposition B.2. (VREx will use spurious features on graphs under covariate shift) Under the SCM of Equation $(l9)$ , the objective min\u0398 $\\mathbb{V}_{e}[R(e)]$ has non-unique solutions for parameters of the GNN (3) when part of the model parameters $\\bar{\\{\\theta_{1}^{1}}^{(l)},\\bar{\\theta_{1}^{2}}^{(l)},\\theta_{2}^{1}}^{(l)},\\theta_{2}^{2}{}^{(l)}\\}$ take the values ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Theta_{0}=\\left\\{\\begin{array}{l l}{\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,}&{l=L-1,...,L-s+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,}&{l=L-s,L-s-1,...,1}\\\\ {\\theta_{2}^{1}{}^{(l)}=0,\\theta_{2}^{2}{}^{(l)}=1,}&{l=L-1,...,1}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$0\\,<\\,s\\,<\\,L$ is some positive integer. Specifically, the VREx solutions of $\\theta_{1}$ and $\\theta_{2}$ are the sets of solutions of the cubic equation, some of which are spurious solutions that $\\theta_{2}\\neq0$ (although $\\theta_{2}=0$ is indeed one of the solutions, $V R E x$ is not guaranteed to reach this solution): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{c_{1}\\sigma^{2}(2\\theta_{1}\\theta_{2}+(\\theta_{2})^{2}-2c_{2}\\sigma^{2}\\theta_{2})+c_{3}\\theta_{2}-\\mathbb{E}_{e}[N^{e}]c_{1}\\sigma^{2}\\theta_{1}\\theta_{2}+\\mathbb{E}_{e}[N^{e}]c_{2}\\sigma^{2}\\theta_{2}=0}\\\\ {\\left[c_{3}\\theta_{2}-\\mathbb{E}_{e}[N^{e}]c_{1}\\sigma^{2}\\theta_{1}\\theta_{2}+\\mathbb{E}_{e}[N^{e}]c_{2}\\sigma^{2}\\theta_{2}\\right]c_{4}-c_{5}(\\theta_{2})^{2}=0}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nc_{4}=\\mathbb{E}_{e}\\left[((\\tilde{A}^{e})^{k}X_{1})^{\\top}\\mathbf{1}_{\\mathbf{N}^{e}}\\right]\\sigma^{2},\\,c_{5}=\\mathbb{E}_{e}\\left[N^{e}\\left(t r((\\tilde{A}^{e})^{\\top}\\tilde{A}^{e}{}^{k})+N^{e}(1+\\sigma^{2})\\right)\\right].\\qquad\\qquad\\qquad\\qquad\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proposition B.3. (IRMv1 will use spurious features on graphs under covariate shift) Under the SCM of Equation (19), there exists $s\\in\\mathbb{N}^{+}$ that satisfies $0<s<L$ and $s\\neq k$ such that optimizing the IRMv1 objective $\\mathrm{min}_{\\Theta}\\mathbb{E}_{e}[||\\nabla_{w|w=1.0}R(e)||^{2}]$ will not lead to the invariant solution $\\theta_{2}=0$ for parameters of the GNN (3) when $\\{\\theta_{1}^{1}{}^{(l)},\\theta_{1}^{2}{}^{(l)},\\theta_{2}^{1}{}^{(l)},\\theta_{2}^{2}{}^{(l)}\\}$ take the special solution: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Theta_{0}=\\left\\{\\begin{array}{l l}{\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,}&{l=L-1,...,L-s+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,}&{l=L-s,L-s-1,...,1}\\\\ {\\theta_{2}^{1}{}^{(l)}=0,\\theta_{2}^{2}{}^{(l)}=1,}&{l=L-1,...,1}\\end{array}\\right.\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proposition B.4. Optimizing the CIA objective will lead to the optimal solution $\\Theta^{*}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\theta_{1}=1}\\\\ {\\theta_{2}=0}\\\\ {\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-1,...,L-k+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-k,L-k-1,...,1}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C Detailed Experimental Setup ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Basic Settings ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "All experimental results were averaged over three runs with different random seeds. Following Gui et al. [2022], we use an OOD validation set for model selection. ", "page_idx": 17}, {"type": "text", "text": "GAT experiments. For experiments on GAT, we adopt the learning rate of 0.01 for Arxiv, 0.001 for Cora, 0.003 for CBAS, and 0.1 for WebKB. The reason why we didn\u2019t use the default learning rate in Gui et al. [2022] is that since the original GOOD benchmark didn\u2019t implement GAT, so we chose to tune a learning rate for adapting GAT to reach a decent performance. The settings of batch size, training epochs, weight decay, and dropout follow Gui et al. [2022]. ", "page_idx": 17}, {"type": "text", "text": "GCN experiments. For experiments on GCN, we follow the default settings of batch size, training epochs, learning rate, weight decay, and dropout provided by Gui et al. [2022]. ", "page_idx": 17}, {"type": "text", "text": "It\u2019s worth mentioning that we choose different normalization strategies for the invariant edge mask of CIA-LRA to achieve better performance for GCN. In Equation (8), we use Sigmoid as normalization. However, we find it is better to use a Min-Max normalization for GCN on some of the datasets. Specifically, for experiments on GCN, we use Sigmoid normalization for CBAS, Arxiv (time concept); Sigmoid for training and Min-Max for testing for WebKB (covariate); Min-Max normalization for the other dataset splits. For GAT, we use Sigmoid during training and testing for all datasets. ", "page_idx": 17}, {"type": "text", "text": "C.2 Hyperparameter Settings of the Main OOD Generalization Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Most hyperparameter settings are adopted from Gui et al. [2022], except that for EERM we reduce the number of generated environments from 10 to 7 and reduce the number of adversarial steps from 5 to 1 for memory and time complexity concerns. For each method, we conduct a grid search for about $3{\\sim}7$ values of each hyperparameter. The hyperparameter search space is presented in Table 5. ", "page_idx": 17}, {"type": "text", "text": "C.3 Details of the Toy Dataset ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Now we introduce the setting of the toy synthetic task of Figure 3. The synthetic dataset consists of four classes. Each node has a 4-dim node feature. The first/last two dimensions correspond to the invariant/spurious feature for each of the four classes, as shown in Table 6a and 6b. We artificially create both concept shift and covariate shift in this dataset. ", "page_idx": 17}, {"type": "text", "text": "To explicitly disentangle the learned invariant and the spurious components for quantitative analysis, we employ a 1-layer GCN. We take the output of the first/last two dimensions of the weight matrix as the invariant/spurious representation. ", "page_idx": 17}, {"type": "table", "img_path": "7eFS8aZHAM/tmp/727e9ec7157a698c33c566e6f14d94bd0e6985f5b1ccd42a6c34a39d56a52475.jpg", "table_caption": ["Table 5: Hyperparameter setting of the experiments. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "7eFS8aZHAM/tmp/e94df71e782eb865b40efbce673d0d2bc8a8fda8e66bcce1ad1760eb014af9a2.jpg", "table_caption": ["(a) The invariant part (the first two dimensions) of the(b) The spurious part (the last two dimensions) of the node features in the synthetic dataset. nodes features in the synthetic dataset. ", "Table 6: Information of the toy dataset of Figure 3. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Additional Experimental Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Excessive Alignment Leads to the Collapse of the Invariant Features. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "One may wonder why CIA underperforms CIA-LRA even if CIA uses the ground truth environment labels. In this section, we will show that CIA may suffer excessive alignment, which will lead to the collapse of the learned invariant features and consequently hurt generalization. We use the intraclass variance of the representation corresponding to invariant features (averaged over all classes) to measure the degree of collapse of invariant features. Base on this measurement, the excessive alignment can be caused by: ", "page_idx": 18}, {"type": "text", "text": "1) Using a that is too large. Evidence: on the toy dataset of Figure 3, a larger $\\lambda$ leads to smaller intra-class variance of invariant representations. We also compute the intra-class variance of invariant representations at epoch 50 on the toy dataset, ", "page_idx": 18}, {"type": "text", "text": "2) Aligning the representations of too many nodes. Evidence: we show that aligning fewer node pairs can alleviate the collapse of invariant representation. By modifying CIA to align local pairs (same-class, different-environment nodes within 1 hop), termed \"CIA-local\", the results in Table 7b show that when by aligning local pairs instead of all pairs, CIA-local avoids the collapse that CIA suffers and achieves better performance. ", "page_idx": 18}, {"type": "text", "text": "D.2 CIA-LRA Alleviates Representation Collapse Caused by Excessive Alignment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 5, we illustrate the impact of $\\lambda$ on OOD accuracy. Both CIA and CIA-LRA experience a performance decline at $\\lambda=0.5$ , indicating that excessive alignment can hinder generalization. Furthermore, CIA shows an earlier and more pronounced performance drop than CIA-LRA. This suggests that the CIA-LRA method mitigates representation collapse by aligning fewer pairs and selectively focusing on pairs with smaller differences in invariant features. ", "page_idx": 18}, {"type": "text", "text": "Table 7: Experimental evidence of the factors that can cause the collapse of the learned invariant representations. ", "page_idx": 19}, {"type": "text", "text": "(a) Using a that is too large can cause the collapse.(b) Aligning the representations of too many nodes The intra-class variance of invariant representations oncan cause the collapse. Accuracy and the variance the toy dataset of Figure 3 at epoch 50. of the invariant representations on the toy dataset of Figure 3 at epoch 200. ", "page_idx": 19}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/71db0dc423b42d287c0094cd55db3bcc5c3071fcd1a34a349c218701bf7f315a.jpg", "img_caption": ["(a) Cora degree concept (b) Cora degree covariate (c) CBAS color concept (d) CBAS color covariate "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 5: The effect of $\\lambda$ on OOD accuracy. CIA exhibits an earlier and more severe performance drop than CIA-LRA, demonstrating that CIA-LRA can alleviate the feature collapse caused by excessive alignment. ", "page_idx": 19}, {"type": "text", "text": "The role of CIA-LRA in alleviating the collapse of the invariant features can also be reflected in Figure 6, in which the representation learned by CIA collapsed to a compact region. However, CIA-LRA does not exhibit such collapse, maintaining the diversity of the causal representation. ", "page_idx": 19}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/370c68aba50be32d95bcf27acb0676d0836927fa86816181f655d984ee3e26cd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 6: Visualization of the learned representations at epoch 100 on the toy dataset (concept shift).   \nClasses are distinguished by color. $\\lambda=0.5$ for CIA and CIA-LRA. ", "page_idx": 19}, {"type": "text", "text": "D.3 Effect of the Number of Hops for Localized Alignment ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Figure 7, we plot the OOD accuracy curve of CIA-LRA against the number of hops $t$ for localized alignment (with $\\lambda=0.05)$ ). CIA-LRA achieves optimal performance within a local range of 6 to 10 hops. Performance is notably lower at smaller hops $(t=2)$ ), due to limited regularization from aligning only a few pairs of representations. As $t$ increases, performance gains diminish and can even degrade, particularly on the CBAS color covariate. This underscores the importance of localized alignment: optimal OOD performance is attained by aligning nodes within about 10 hops. Extending the alignment range further does not enhance performance significantly and may lead to performance drops and higher computational costs. These findings support the hypothesis in Appendix D.4 that invariant features distant on the graph differ substantially, and their alignment could induce invariant feature collapse, leading to a suboptimal generalization performance. ", "page_idx": 19}, {"type": "text", "text": "D.4 Discussion and Validation of the Assumption on the Rate of Change of Causal and Spurious Features w.r.t Spatial Position ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To verify the intuition presented in Section 3.2 that spurious features exhibit larger changes within a local range (about 5 to 10 hops) on a graph compared to invariant features, we conduct experiments on real-world datasets Arxiv and Cora. To extract invariant features, we use a pre-trained VREx model and take the output of the last layer as invariant features8. To obtain spurious features, we train an ERM model to predict the environment label and take the output of the last layer as spurious features. For each class, we randomly sample 10 nodes and generate corresponding 10 paths using Breadth-First Search (BFS). We extract invariant and spurious features of the nodes on each path and plot the L-2 distances between the node representations on the paths and the starting node. The results of Cora are in Figure 8 and 9, and the results of Arxiv are in Figure 10 and 11. We chose some of the classes to avoid excessive paper length; the results for the other classes are similar. ", "page_idx": 19}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/4b226b17dbca8f03e35d0c1aeec7296a6c036f6de112530848827be2824f802a.jpg", "img_caption": ["Figure 7: The effect of the number of hops $t$ for localized alignment on OOD accuracy. Too small $t$ will lead to suboptimal performance. Too large $t$ brings limited performance gain or even deteriorates the performance. "], "img_footnote": ["(a) Cora degree concept (b) Cora degree covariate (c) CBAS color concept (d) CBAS color covariate "], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "We observe that: despite the curve\u2019s slight fluctuations, the invariant feature difference shows a clear positive correlation with the distance from the starting point. Specifically, within about $5\\!\\sim\\!10$ hops, the changes of spurious features grow more rapidly than those in invariant ones. This insight led us to align the representations of adjacent nodes to better eliminate spurious features and avoid the collapse of the invariant features. This also explains why we add a weighting term $d(i,j)$ in our loss function to assign smaller weight node pairs farther apart. Additional experimental evidence supporting the importance of localized alignment is in Appendix D.3, which shows that alignment over a large range may lead to suboptimal performance and increasing computational costs. ", "page_idx": 20}, {"type": "text", "text": "This assumption aligns with those adopted in a series of previous works on causality and invariant learning [Chen et al., 2022, Burshtein et al., 1992, Sch\u00f6lkopf, 2022, Sch\u00f6lkopf et al., 2021]. These works assume that invariant features are better clustered than spurious features. In the node-level graph OOD scenario, we observe this phenomenon primarily within local parts of a graph. In some cases, when two nodes are too far apart, their invariant features can vary more than the spurious features, as seen in Figure 11 (a) path 1,2,4,6,9 and 10. Therefore, matching the representations in a local region helps alleviate the invariant feature collapse problem. ", "page_idx": 20}, {"type": "text", "text": "D.5 Discussion and Validation of the Assumption on the Feature Distance and Neighborhood Label Distribution Discrepancy ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.5.1 Heterophilic Neighborhood Labels Distribution Reflect Spurious Feature Distribution ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we will empirically validate the key intuition of CIA-LRA: the label distribution of the neighbors from different classes (which we call Heterophilic Neighborhood Label Distribution, HeteNLD) reflects the spurious representation of the centered node. In node-level OOD scenarios, the distributional shifts of spurious features originate from two main sources: (1) the shifts in spurious node features associated with environments, and (2) the shifts in Neighborhood Label Distribution (NLD), which affects the aggregated representation of the centered node. The first type of spurious feature is analogous to those defined in Computer Vision (CV) OOD domains, while the second type is specific to graph structures. The NLD shift is a more general instance of the graph heterophily problem [Ma et al., 2022, Huang et al., 2023, Mao et al., 2023], where changes in the ratio of homophilic neighbors from training to test graphs can degrade performance. This occurs because the changes in the homophilic ratio lead to the distributional shift in the aggregated representation of the same-class nodes. Most previous methods [Ma et al., 2022, Huang et al., 2023, Mao et al., 2023] ", "page_idx": 20}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/0e2d61dcea1c2dadccf6148b43ecdc627ea5e34e7be8317079dad607c0054816.jpg", "img_caption": ["Figure 8: Visualization of the rate of change of invariant features and spurious features on Cora (part 1). ", "(b) class 17 of Cora "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/1789f950c8093f6a1e7cd8d7cdca649bd1bedc5e54bed9133f2b203d3cc67378.jpg", "img_caption": ["Figure 9: Visualization of the rate of change of invariant features and spurious features on Cora (part 2). ", "(b) class 41 of Cora "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/6807b0db8c813794b9e2bfbce5cb442a21dd5e2ff34560953ce3b6835a266db1.jpg", "img_caption": ["Figure 10: Visualization of the rate of change of invariant features and spurious features on Arxiv (part 1). ", "(b) class 29 of Arxiv "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/0dd3df208a52a914b677363ed6203aed574ef05699386cb1474ec23a9cabd2e1.jpg", "img_caption": ["(b) class 17 of Arxiv "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 11: Visualization of the rate of change of invariant features and spurious features on Arxiv (part 2). ", "page_idx": 24}, {"type": "text", "text": "only focus on the binary-classification setting, where changes in the homophilic neighbor ratio are equivalent to changes in the heterophilic neighbor ratio. However, we consider the more general multiclassification tasks. Therefore, we propose to use HeteNLD as a measurement, considering every class different from the central class and using their distribution to reflect shifts in the aggregated representation. Although the ratio of homophilic neighbors also affects environmental spurious features and NLD, it affects the invariant representation as well. Assigning larger weights to the pair with significant differences in the ratio of homophilic neighbors will simultaneously eliminate environmental spurious features and learn a collapsed invariant representation. As evidenced in Table 4, moving the $\\bar{r}^{\\mathrm{same}}(c)_{i,j}$ to the numerator of Equation (9) will lead to a significant performance decrease. Hence we usersame(1c)i,j instead of $r^{\\mathrm{same}}(c)_{i,j}$ in $w_{i j}$ . ", "page_idx": 25}, {"type": "text", "text": "In the following part, we will empirically validate our intuition that HeteNLD can reflect the two spurious representation distributions on concept shift, where $p(Y|X)$ varies across environments, and covariate shift, where $p(X)$ changes with environments, respectively. We will show that HeteNLD affects the spurious features of the centered node in different manners under concept shift and covariate shift. ", "page_idx": 25}, {"type": "text", "text": "Covariate shift. For covariate shifts on graphs, since spurious features are not necessarily correlated with labels, the environmental spurious features cannot be reflected by HeteNLD. However, we can still measure how HeteNLD affects the aggregated neighborhood representation. To obtain neighborhood representation, we train a 1-layer GCN that aggregates neighboring features and discards the features of the centered node. We hope to observe whether the gap of HeteNLD accurately reflects the distance of neighborhood representation. To ensure that the discrepancy in the aggregated neighboring feature is caused solely by heterophilic neighbors, we only use point pairs with the same number of homophilic neighbors. Specifically, we compute the L-2 distance between the neighborhood representations of two nodes with the same number of class-same neighbors, and plot its trend w.r.t. the distance of HeteNLD (according to the definition of $r_{i,j}^{\\mathrm{diff}}$ in Equation (9), except that we didn\u2019t normalize by the node degree here). We run experiments on Cora to verify this. We evaluate on both word shifts (node feature shifts) and degree (graph structure shifts) for a comprehensive understanding. We show the results of the first 30 classes of Cora. The results in Figure 12 and 13 show a clear positive correlation between the neighborhood representation distance and HeteNLD discrepancy under covariate shifts, indicating HeteNLD discrepancy can reflect the distance of the aggregated representation. ", "page_idx": 25}, {"type": "text", "text": "Concept shift. As for concept shift, spurious features are correlated with labels, thus the label of a node contains information about spurious features correlated with this class. Hence, by observing HeteNLD, we can measure the distribution of the spurious feature. For concept shift, we train a GNN to predict environment labels to obtain spurious representations. Table 14 and 15 also show a clear positive correlation between spurious feature distance and HeteNLD discrepancy on concept shift, indicating that HeteNLD discrepancy can reflect the distance of the environmental spurious features. ", "page_idx": 25}, {"type": "text", "text": "D.5.2 Homophilic Neighboring Labels Reflect Invariant Feature Distribution ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Now will validate that the ratio of the same-class neighbors reflects the aggregated invariant representation. We use VREx to approximately extract invariant features and compute their distance w.r.t. the discrepancy of the ratio of the same-class neighbors. We evaluate on 4 splits of Cora: word+covariate, word+concept, degree+covariate and degree+concept. For each data split, we randomly choose 5 classes with sufficiently large differences in homophilic neighbor ratios for visualization. The results in Figure 16 also show a positive correlation trend between the distance of the invariant representations and the difference in the ratio of same-class neighbors, indicating the latter can reflect the former. ", "page_idx": 25}, {"type": "text", "text": "D.6 Validation of the True Feature Generation Depth ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For the theoretical model in Section 2, we assume that the number of layers of the GNN $L$ is greater than the depth of the causal pattern $k$ . In this section, we empirically verify how large $k$ really is on real-world datasets. Specifically, we use GCN with different layers to predict the ground-truth label $Y$ on Cora and Arxiv datasets respectively (results are in Table 9a and 9b). As mentioned above, since a GCN with $L$ layers will aggregate features from $L$ -hop neighbors for prediction, if the depth ", "page_idx": 25}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/b2eb01f8bc30f8e1db7fc4869146eba1db7e67f1dd9e4a58ba24ad50ada8fb70.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 12: The relationship between the distance of the aggregated neighborhood representation and distance of HeteNLD on Cora word domain, covariate shift. Each sub-figure is a class, and each dot in the figure represents a node pair in the graph. The red line is obtained by linear regression. The positive correlation is clear. ", "page_idx": 26}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/49f7a696c8716fe77d63fa9378c60cf035c5d404270bd47d79c2e8f2de22b2de.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 13: The relationship between the distance of the aggregated neighborhood representation and distance of HeteNLD on Cora degree domain, covariate shift. The positive correlation is clear. ", "page_idx": 26}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/1551dbd93941c8eb53954ab473db789d3adce5a55ca6be54d3770e8e39410ec5.jpg", "img_caption": ["Figure 14: The relationship between the distance of environmental spurious features and distance of HeteNLD on Cora word, concept shift. The positive correlation holds for most classes. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/24b4e49cdb2405424608cb3554be863585196450154426a67aa5aa92b7cf89c8.jpg", "img_caption": ["Figure 15: The relationship between the distance of environmental spurious features and distance of HeteNLD on Cora degree, concept shift. The positive correlation holds for most classes. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "7eFS8aZHAM/tmp/2979b9fd9c3b356482b4f7fcec96bdc2392877a0a86373f13d55211ef278ab73.jpg", "img_caption": ["Figure 16: The relationship between the distance of invariant representations and discrepancy in the same-class neighbor ratio (the ratios in the figure are multiplied by node degree) on Cora degree, concept shift. Line 1 to 4 are results of Cora word+covariate, word+concept, degree+covariate and degree $^+$ concept, respectively. Each subgraph marks a class, and each point in the subfigure represents a node pair. There is a positive correlation between the invariant feature distance and the difference in neighboring label ratio of the same class as the centered node. "], "img_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "7eFS8aZHAM/tmp/6360b94c6525b6342822e0db2035bb26d2f78ec18b3fb15dfd2fafe28f68bd15.jpg", "table_caption": ["Table 8: Time cost (seconds) to achieve optimal test performance on Arxiv using GAT on a single RTX 3090 GPU. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "of the GCN is equal to the true generation depth, then the performance should be close to optimal. Therefore, we use the layer number that yields the optimal empirical performance (denoted as $L^{*}$ ) to approximate $k$ . We find that the $L^{*}\\leq4$ in most cases. This indicates that our assumptions $L\\leq k$ hold easily. ", "page_idx": 28}, {"type": "text", "text": "D.7 Time Cost of CIA-LRA ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "To show the running time of CIA-LRA, we show the time cost to reach the best test accuracy on our largest dataset Arxiv (with 50k 60k nodes). The results are in Table 8 below. The time cost of CIA-LRA is comparable to baseline methods. ", "page_idx": 28}, {"type": "table", "img_path": "7eFS8aZHAM/tmp/d358a763859334b2818bfcbc985690439386e7182c6e5b61fa14010cc0e763f3.jpg", "table_caption": ["(a) OOD accuracy on causal prediction $(\\%)$ of GCN with(b) OOD accuracy $(\\%)$ of GCN with different numbers different numbers of layers on Arxiv. of layers on Cora. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 1 shows the detailed training procedure (pseudo code) of CIA-LRA. We use the same GNN encoder for the invariant subgraph extractor. Empirically, we add CIA or CIA-LRA after one epoch of ERM training. ", "page_idx": 29}, {"type": "text", "text": "Algorithm 1 Detailed Training Procedure of CIA-LRA   \nRequire: A labeled training graph ${\\mathcal{G}}=(A,X,Y)$ , a GNN $f_{\\Theta}$ , and invariant subgraph generator GNN $f_{\\theta_{m}}$ . The number of hops $t$ , CIA-LRA weight $\\lambda$ , the number of classes $C$ , total iterations $T$ , model learning rate $r_{1}$ , invariant subgraph generator learning rate $r_{2}$ , the number of $f_{\\Theta}$ \u2019s layers $L$   \nEnsure: Updated model $f_{\\Theta}$ with parameter $\\Theta$ .   \n1: for iterations in $[1,2,...,T]$ do   \n2: Randomly sample a subgraph $A^{\\prime}\\in\\mathbb{R}^{N\\times N}$ from $A\\in\\mathbb{R}^{N_{0}\\times N_{0}}$ .   \n3: Compute and apply the edge mask according to Equation (8) to obtain the masked adjacency matrix $A_{m}\\gets\\dot{A^{\\prime}}\\odot M\\in\\check{\\mathbb{R}}^{N\\times N}$ .   \n4: Initialize $\\mathcal{L}_{\\mathrm{CIA-LRA}}\\leftarrow0$ .   \n5: Calculate the node representations $\\phi(A_{m},X)\\in\\mathbb{R}^{N\\times D}$ .   \n6: ### Calculate $A^{t}$ , where the $(i,j)$ -th element of $A^{t}$ equals the length of the shortest path from node $i$ to $j$ if the length is less than $t$ else infinity:   \n7: Initialize $\\bar{A}^{t}(i,j)\\leftarrow\\operatorname{Inf}$ if $i\\neq j$ , $A^{t}(i,i)\\gets1$ , $A_{t e m}\\leftarrow A_{m}$   \n8: for hop $h$ in $[1,2,...,t]$ do   \n9: $A_{t e m}\\gets A_{m}A_{t e m}$   \n10: if $A_{t e m}(i,j)>0$ and $A_{t e m}(i,j)<A^{t}(i,j)$ then   \n11: $A^{t}(i,j)\\gets h$   \n12: end if   \n13: end for   \n14: ### Compute the ratio of neighbored nodes of each class:   \n15: Compute the normalized adjacency matrix $\\bar{A}$ , where $\\bar{A}$ \u2019s $i$ -th row $\\bar{A}_{i}\\leftarrow A_{m i}/D_{i}$ , $A_{m i}$ is the $i$ -th row of $A_{m}$ and $D_{i}\\in\\mathbb{R}$ is the degree of node $i$ .   \n16: Initialize the neighbored label ratio $\\bar{R}\\leftarrow Y\\in\\mathbb{R}^{N\\times C}$ , where $R(i,c)$ is the ratio of node $i$ \u2019s neighbors of class $c$ within a $L$ -hop range, $Y$ are the one hop labels.   \n17: for $l$ in $[1,2,...,L]$ do   \n18: $R\\leftarrow\\bar{A}R$   \n19: end for   \n20: for $c$ in $[1,2,...,C]$ do   \n21: Sample the nodes of class $c$ from $A^{t}$ and form $A_{c}^{t}$ . Use $A_{c}^{t}$ to screen for pairs of nodes not exceeding a distance of $t$ hops $\\Omega_{c}(t)$ .   \n22: Compute CIA-LRA loss of class $c$ : $\\mathcal{L}_{\\mathrm{CLA-LRA}}^{c}$ according to Equation (9) using $\\Omega_{c}(t),R,A_{c}^{t}$ and $\\phi(A_{m},X)$ :   \n23: $\\mathcal{L}_{\\mathrm{CIA-LRA}}\\leftarrow\\mathcal{L}_{\\mathrm{CIA-LRA}}+\\mathcal{L}_{\\mathrm{CIA-LRA}}^{c}$   \n24: end for   \n25: Compute final loss $\\mathcal{L}\\gets\\mathcal{L}_{\\mathrm{ce}}(f_{\\Theta}(A,X),Y)+\\lambda\\mathcal{L}_{\\mathrm{CIA-LRA}},$ $\\mathcal{L}_{\\mathrm{ce}}$ is the cross-entropy loss   \n26: Update model parameters $\\Theta\\leftarrow\\Theta-r\\nabla_{\\Theta}\\mathcal{L}$ , $\\theta_{m}\\gets\\Theta_{m}-r\\nabla_{\\theta_{m}}\\mathcal{L}$   \n27: end for ", "page_idx": 29}, {"type": "text", "text": "F Additional Discussion of Theoretical Settings and Results ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "F.1 Detailed Setup of the Theoretical Model in Section 2 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The proposed data generation process. In the theoretical model of Equation (2), each dimension of $\\boldsymbol{n_{1}}\\in\\mathbb{R}^{\\bar{N}^{e}\\times1}$ and $\\bar{n_{2}}\\in\\mathbb{R}^{N^{e}\\times1}$ are i.i.d, following a standard Gaussian distribution. $\\epsilon^{e}\\in\\mathbb{R}^{N^{e}\\times1}$ is an environment spurious variable. $\\epsilon_{i}^{e}$ (each dimension of $\\epsilon^{e}$ ) are independent random variables, $i=$ $1,...,N^{e}$ . We further assume the cross-environment expectation $\\mathbb{E}_{e}[\\pmb{\\epsilon}^{e}]=\\mathbf{0}$ and cross-environment variance $\\mathbb{E}_{e}[\\epsilon_{i}^{e}]=\\sigma^{2}$ , $i=1,...,N^{e}$ for brevity. ", "page_idx": 29}, {"type": "text", "text": "The considered multi-layer GNN. In the analyzed GNN of Equation (3), we simplify the classifier to an identity mapping. Such simplification has been adopted by various previous theoretical works on graphs [Wu et al., 2022b, Tang and Liu, 2023]. We assume $L\\geq k$ to ensure the model has enough capacity to learn invariant features. We verify this assumption by using GCNs with different numbers of layers to predict the ground-truth labels (see Appendix D.6). ", "page_idx": 30}, {"type": "text", "text": "F.2 Discussion of the Structural Feature Considered in the Theoretical Model and Justification for the Choice of the Analyzed GNN ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Structural Features and Structural Shifts Considered in Section 2.1. To reflect reality as much as possible, it is necessary to consider both nodal and structural invariant and spurious features in the theoretical model. As mentioned in Section 2.1, we model the invariant structural feature as the structure of the $k$ -hop ego-subgraph. A natural question is raised here: ", "page_idx": 30}, {"type": "text", "text": "Can we find other ways to define the invariant/spurious structural features? ", "page_idx": 30}, {"type": "text", "text": "The answer is yes. For example, the invariant structure can be modeled as the subgraph of the ego-graph of a node, following Li et al. [2023a]. However, it is fundamentally impossible for GNNs using mean aggregation (like GCN) to learn such causal structures. This is because such GNNs will assign fixed weights to each neighboring node feature, and they can\u2019t split the causal substructure from the neighbored ego-sgraph. Therefore, we make the causal structure feasible for GCN-like GNNs to learn by defining the causal structure as the whole $k$ -hop neighboring ego-graph, rather than a subgraph, and show that OOD failure can still happen (Theorem 2.3). Then, under this setting, the remaining challenge becomes identifying the true $k$ by optimizing the shallow layer GNN parameters. However, in real practice, the invariant causal pattern may still be an ego-subgraph. This can be reflected in the performance gain of the invariant subgraph extractor used in CIA-LRA. ", "page_idx": 30}, {"type": "text", "text": "Why Do We Choose Such a GNN in Section 2.1? From the above analysis, we show that such a choice is a compromise solution between the case of GCN (that can only extract a whole ego-graph) and GAT-like GNNs (that can extract a subgraph from an ego-graph). Although in this GNN each neighbored node is solely assigned the same weight, the shallow layer parameters can be optimized to realize the aggregation of different depths to capture the causal structures of different depths. ", "page_idx": 30}, {"type": "text", "text": "F.3 Discussion of the Failure Solution for GNNs of VREx and IRMv1 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In Theorem G.2 and G.3 (the formal version Theorem 2.3), we show that VREx and IRMv1 could induce a model that uses spurious features. Now we\u2019ll give an intuitive explanation of this failure mode. When the lower-layer parameters of the $\\mathbf{\\Delta}\\mathbf{\\mathfrak{INN}}\\,\\theta_{1}^{1}{}^{(l)},\\theta_{2}^{1}{}^{(l)},\\theta_{1}^{2}{}^{(l)},\\theta_{2}^{2}{}^{(l)}$ take the specific solution $\\Theta_{0}$ in Equation (21), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nH_{1}^{(L)}=\\frac{\\partial H_{1}^{(L)}}{\\partial{\\theta_{1}^{i}}^{(l)}}=\\tilde{A}^{e^{s}}X_{1},\\;H_{2}^{(L)}=\\frac{\\partial H_{2}^{(L)}}{\\partial{\\theta_{2}^{i}}^{(l)}}=\\tilde{A}^{e^{k+m}}X_{1},\\;\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "holds for $i=1,2,\\;l=1,...,L-1$ and every environment $e$ . Thus, we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{1}}=\\frac{\\partial\\mathcal{L}}{\\partial(H_{1}^{(L)}\\theta_{1})}\\frac{\\partial(H_{1}^{(L)}\\theta_{1})}{\\partial\\theta_{1}}=\\frac{\\partial\\mathcal{L}}{\\partial(H_{1}^{(L)}\\theta_{1})}H_{1}^{(L)}}\\\\ {\\displaystyle\\overset{(*)}{=}\\frac{\\partial\\mathcal{L}}{\\partial(H_{1}^{(L)}\\theta_{1})}\\frac{\\partial(H_{1}^{(L)})}{\\partial\\theta_{1}^{i}}=\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{1}^{i}}\\frac{1}{\\theta_{1}},}\\\\ {\\displaystyle i=1,2,\\quad l=1,...,L-1}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "(\u2217) is because of Equation (25). Therefore, \u2202\u2202\u03b8L1 $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{1}}\\,=\\,0\\,\\Rightarrow\\,\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{1}^{\\,i}\\,^{(l)}}\\,=\\,0}\\end{array}$ . The same is true for $\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{2}}$ and $\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{2}^{i\\;(l)}}$ . This means the solution of the top-level parameters $\\theta_{1}$ and $\\theta_{2}$ of the GNN will only be constrained by two equations, $\\begin{array}{r}{\\frac{{\\partial\\mathcal{L}}}{{\\partial\\theta_{1}}}=0}\\end{array}$ and $\\begin{array}{r}{\\frac{{\\partial\\mathcal{L}}}{{\\partial{\\theta_{2}}}}=0}\\end{array}$ , rather than be constrained by all gradient functions \u2202Lj $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{i}^{j}}=0,\\;i=1,2.}\\end{array}$ . By analyzing the specific loss of VREx and IRMv1, we conclude that they will induce a non-zero $\\theta_{2}$ . ", "page_idx": 30}, {"type": "text", "text": "Note that the failure solution $\\Theta_{0}$ here is not the unique one, we choose $\\Theta_{0}$ just for the elegant expression and to better convey the intuition. In effect, the conclusion $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{1}}=\\dot{0}\\Rightarrow\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{1}^{i}^{\\left(l\\right)}}=0}\\end{array}$ and $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{2}}=0\\Rightarrow\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{2}^{i}^{\\left(l\\right)}}=0}\\end{array}$ holds as long as the lower-layer aggregation parameters satisfy $\\begin{array}{r}{\\gamma_{0}^{\\prime}=\\left\\{\\begin{array}{l l}{\\theta_{1}^{1(l)}=1,\\theta_{1}^{2(l)}=1,}&{l=L-1,...,L-s_{1}+1}\\\\ {\\theta_{1}^{1(l)}=0,\\theta_{1}^{2(l)}=1,}&{l=L-s_{1},L-s_{1}-1,...,1}\\\\ {\\theta_{2}^{1(l)}=1,\\theta_{2}^{2(l)}=1,}&{l=L-1,...,L-s_{2}+1}\\\\ {\\theta_{2}^{1(l)}=0,\\theta_{2}^{2(l)}=1,}&{l=L-s_{2},L-s_{2}-1,...,1}\\end{array}\\right.,\\mathrm{for~some~}s_{1},\\ s_{2}\\in\\mathbb{N}^{+},\\ 1<s_{1}\\leq L-s_{2}\\in\\mathbb{N}^{+}.}\\end{array}$ , 1 < s2 \u2264L, (27) ", "page_idx": 31}, {"type": "text", "text": "i.e., we don\u2019t require the spurious branch of the GNN to be identity mapping $I$ as equation (36) does.   \nThis failure mode can happen to GCN (all $\\theta_{j}^{i}=1$ , $i=1,2$ , $j=1,2$ ) and also for GAT. ", "page_idx": 31}, {"type": "text", "text": "F.4 The Superiority of the Proposed CSBM-OOD ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Our CSBM-OOD in Section introduces several advancements over the conventional CSBMs [Ma et al., 2021, Mao et al., 2023]: 1) It supports multi-class classification, extending beyond the binary classification framework of traditional CSBMs; 2) It accommodates unique neighboring label distributions for each node, in contrast to the traditional models that assume a uniform class-shared homophily/heterophily ratio across all nodes; 3) our model integrates OOD shifts, while traditional ones don\u2019t. ", "page_idx": 31}, {"type": "text", "text": "F.5 Tightness of the Error Bound of Theorem 4.4 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "When there are no distributional shifts in spurious node features and heterophilic neighborhood distribution between training and test environments, the terms (a)-(d) in Eq. (109) becomes zero, and the upper bound becomes $\\begin{array}{r l}&{\\widehat{\\mathcal{L}}_{e^{\\mathrm{tr}}}^{\\gamma}(\\widetilde{h})+c o n s t=\\widehat{\\mathcal{L}}_{e^{\\mathrm{tr}}}^{\\gamma}(\\widetilde{h})+\\frac{1}{N_{e^{\\mathrm{tr}}}^{1-2\\alpha}}+\\frac{1}{N_{e^{\\mathrm{tr}}}^{2\\alpha}}\\ln\\frac{L C(2B_{e^{\\mathrm{tr}}})^{1/L}}{\\gamma^{1/L}\\delta}}\\end{array}$ , i.e., our bound only larger than the ideal error $\\widehat{\\mathcal{L}}_{e^{\\mathrm{tr}}}^{\\gamma}(\\widetilde{h})$ by a constant const. When the number of training samples $N_{e^{\\mathrm{tr}}}$ is large, const will be smal l enough and can be negligible. Hence, the tightness of our bound is guaranteed. ", "page_idx": 31}, {"type": "text", "text": "G Proofs of the Theoretical Results ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "G.1 Proofs of the Concept Shift Case Presented in the Main Text ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we give proof of the propositions of the concept shift model presented in the main text. ", "page_idx": 31}, {"type": "text", "text": "G.1.1 Proof of the non-Graph Success Case of VREx and IRMv1 under Concept Shift ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We restate Proposition 2.2 as Proposition G.1 below. ", "page_idx": 31}, {"type": "text", "text": "Proposition G.1. For the non-graph version of the SCM in Equation (2), ", "page_idx": 31}, {"type": "equation", "text": "$$\nY^{e}=X_{1}+n_{1},\\;X_{2}^{e}=Y^{e}+n_{2}+\\epsilon^{e},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "VREx and IRMv1 will learn invariant features when using a $^{\\,l}$ -layer linear network: $f(X)\\ =$ $\\theta_{1}X_{1}+\\theta_{2}X_{2}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. VREx. Denote $X_{1}\\theta_{1}+X_{2}^{e}\\theta_{2}-X_{1}-n_{1}$ as $l_{e}$ . The variance of loss across environments is: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{e}[R(e)]=\\mathbb{E}_{e}[R^{2}(e)]-\\mathbb{E}_{e}^{2}[R(e)]}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\mathbb{E}_{e}\\left[\\left(\\mathbb{E}_{n_{1},n_{2}}\\left\\|X_{1}\\theta_{1}+(X_{1}+n_{1}+n_{2}+\\epsilon^{e})\\theta_{2}-X_{1}-n_{1}\\right\\|_{2}^{2}\\right)^{2}\\right]}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Take the derivative of $\\mathbb{V}_{e}[R(e)]$ with respect to $\\theta_{1}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}}=\\mathbb{E}_{e}\\left[2\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}X_{1}\\right]\\right]}\\\\ &{\\qquad\\qquad-\\ 2\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\right]\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}X_{1}\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using the fact that $\\mathbb{E}_{n_{1},n_{2}}[n_{1}]~=~\\mathbb{E}_{n_{1},n_{2}}[n_{2}]~=~\\mathbf{0}$ , $\\mathbb{E}_{n_{1},n_{2}}[n_{1}^{\\top}n_{2}]\\ =\\ \\mathbb{E}_{n_{1},n_{2}}[n_{2}^{\\top}n_{1}]\\ =\\ 0$ and $\\mathbb{E}_{n_{1},n_{2}}[n_{1}^{\\top}n_{1}]=\\mathbb{E}_{n_{1},n_{2}}[n_{2}^{\\top}n_{2}]=N^{e}$ if it is the noise from $e$ , $\\mathbb{E}_{n_{1},n_{2}}[n_{1}^{\\top}\\epsilon^{e}]=\\mathbb{E}_{n_{1},n_{2}}[n_{2}^{\\top}\\epsilon^{e}]=0$ and using the assumption that $\\mathbb{E}_{e}[(\\epsilon_{i}^{e})^{2}]=\\sigma^{2}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{e}\\left[2\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}X_{1}\\right]\\right]}\\\\ &{=\\!\\theta_{1}^{3}X_{1}^{\\4}+3\\theta_{1}^{1}\\!^{2}\\theta_{2}X_{1}^{4}+\\theta_{1}\\theta_{2}{}^{2}[3X_{1}^{2}(X_{1}^{2}+\\sigma^{2})+2X_{1}^{2}\\mathbb{E}_{e}[N^{e}]]}\\\\ &{+\\theta_{2}^{3}X_{1}(X_{1}^{3}+X_{1}\\sigma^{2}+\\mathbb{E}_{e}[\\epsilon^{e}^{\\top}\\epsilon^{e}\\epsilon^{e}]+2X_{1}\\mathbb{E}_{e}[N^{e}])}\\\\ &{-3\\theta_{1}^{2}X_{1}^{4}-\\theta_{1}\\theta_{2}(6X_{1}^{4}+2X_{1}^{2}\\mathbb{E}_{e}[N^{e}])-\\theta_{2}^{2}(3X_{1}^{2}(X_{1}^{2}+\\sigma^{2})+4X_{1}^{2}\\mathbb{E}_{e}[N^{e}])}\\\\ &{+\\theta_{1}(3X_{1}^{4}+X_{1}^{2}\\mathbb{E}_{e}[N^{e}]0)+3\\theta_{2}(X_{1}^{4}+X_{1}^{2}\\mathbb{E}_{e}[N^{e}])-X_{1}^{2}(X_{1}^{2}+\\mathbb{E}_{e}[N^{e}])}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\right]\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}X_{1}\\right]\\right]}\\\\ &{=\\!\\theta_{1}\\!^{3}X_{1}\\!^{4}+\\!3\\theta_{1}\\!^{2}\\theta_{2}X_{1}\\!^{4}+\\!\\theta_{1}\\theta_{2}\\!^{2}[3X_{1}^{2}(X_{1}^{2}+\\sigma^{2})+2X_{1}^{2}\\mathbb{E}_{e}[N^{e}]]}\\\\ &{+\\theta_{2}^{3}X_{1}(X_{1}^{3}+X_{1}\\sigma^{2}+2X_{1}\\mathbb{E}_{e}[N^{e}])}\\\\ &{-3\\theta_{1}^{2}X_{1}^{4}-\\theta_{1}\\theta_{2}(6X_{1}^{4}+2X_{1}^{2}\\mathbb{E}_{e}[N^{e}])-\\theta_{2}^{2}(3X_{1}^{2}(X_{1}^{2}+\\sigma^{2})+4X_{1}^{2}\\mathbb{E}_{e}[N^{e}])}\\\\ &{+\\theta_{1}(3X_{1}^{4}+X_{1}^{2}\\mathbb{E}_{e}[N^{e}]0)+3\\theta_{2}(X_{1}^{4}+X_{1}^{2}\\mathbb{E}_{e}[N^{e}])-X_{1}^{2}(X_{1}^{2}+\\mathbb{E}_{e}[N^{e}])}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Plug Equation (31) and (32) back into Equation (30), we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}}=\\theta_{2}^{3}X_{1}^{\\top}\\mathbb{E}_{e}[\\epsilon^{e\\top}\\epsilon^{e}\\epsilon^{e}]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let $\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}}=0$ )]= 0, we have \u03b82 = 0. ", "page_idx": 32}, {"type": "text", "text": "Now we need to validate $\\theta_{2}=0$ is also a solution to $\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{2}}=0$ . Let\u2019s calculate $\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{2}}$ when $\\theta_{2}=0$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{2}}=\\mathbb{E}_{e}\\left[2\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}(X_{1}+n_{1}+n_{2}+\\epsilon^{e})\\right]\\right]}\\\\ &{\\qquad\\qquad-\\,2\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\right]\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}(X_{1}+n_{1}+n_{2}+\\epsilon^{e})\\right]\\right]}\\\\ &{\\qquad=(\\theta_{1}^{2}X_{1}^{2}-2\\theta_{1}X_{1}^{2}+X_{1}^{2}+\\mathbb{E}_{e}N^{e})(\\theta_{1}X_{1}^{2}-X_{1}^{2}-\\mathbb{E}_{e}N^{e})}\\\\ &{\\qquad-(\\theta_{1}^{2}X_{1}^{2}-2\\theta_{1}X_{1}^{2}+X_{1}^{2}+\\mathbb{E}_{e}N^{e})(\\theta_{1}X_{1}^{2}-X_{1}^{2}-\\mathbb{E}_{e}N^{e})}\\\\ &{\\qquad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "So far, we have proved $\\theta_{2}=0$ is the solution for VREx, hence it will learn invariant features. We finish the proof for VREx. ", "page_idx": 32}, {"type": "text", "text": "IRMv1. The objective of IRMv1 is $\\mathbb{E}_{e}\\|\\nabla_{w}R(e)\\|_{2}^{2}$ . When IRMv1 loss is optimized to zero, we have $\\nabla_{w}R(e)=0$ for all environments $e$ . ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{w}R(e)=\\mathbb{E}_{n_{1},n_{2}}[2(\\theta_{1}X_{1}+\\theta_{2}X_{2}-(X_{1}+n_{1}))^{\\top}(\\theta_{1}X_{1}+\\theta_{2}X_{2})]}\\\\ &{\\qquad\\qquad=2((\\theta_{1})^{2}X_{1}^{\\top}X_{1}+(\\theta_{2})^{2}(X_{1}^{\\top}X_{1}+\\epsilon^{\\top}\\epsilon^{e}+2N^{e})+2\\theta_{1}\\theta_{2}X_{1}^{\\top}(X_{1}+\\epsilon^{e})}\\\\ &{\\qquad\\qquad-\\theta_{1}X_{1}^{\\top}X_{1}-\\theta_{2}(X_{1}^{\\top}X_{1}+X_{1}^{\\top}\\epsilon^{e}+N^{e}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To realize $\\nabla_{w}R(e)\\,=\\,0$ for all $e$ , we must let $\\theta_{2}\\,=\\,0$ (and consequently $\\theta_{1}=1$ ), otherwise the solution of $\\theta_{2}$ will include terms related to $\\epsilon^{e}$ and $N^{e}$ that vary with environments, and a single value $\\theta_{2}$ cannot fit all these values. Thus we finish the proof for IRMv1. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "G.1.2 Proof of the Failure Case on Graphs of VREx under Concept Shift ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We present the formal version of the VREx part in Theorem 2.3 as Theorem G.2 below. ", "page_idx": 33}, {"type": "text", "text": "Theorem G.2. (VREx will use spurious features on graphs under concept shift, formal) Under the SCM of Equation (2), the objective min\u0398 $\\mathbb{V}_{e}[R(e)]$ has non-unique solutions for parameters of the GNN (3) when part of the model parameters $\\overline{{\\{\\theta_{1}^{1}}^{(l)}}},\\theta_{1}^{2}{}^{(l)},\\theta_{2}^{1}{}^{(l)},\\theta_{2}^{2}{}^{(l)}\\}$ take the values ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Theta_{0}=\\left\\{\\begin{array}{l l}{\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,}&{l=L-1,...,L-s+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,}&{l=L-s,L-s-1,...,1}\\\\ {\\theta_{2}^{1}{}^{(l)}=0,\\theta_{2}^{2}{}^{(l)}=1,}&{l=L-1,...,1}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for some $0<s<L.$ . Specifically, the VREx solutions of $\\theta_{1}$ and $\\theta_{2}$ are the sets of solutions of the cubic equation, some of which are spurious solutions that $\\theta_{2}\\neq0$ (although $\\theta_{2}=0$ is indeed one of the solutions, VREx is not guaranteed to reach this solution): ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{(3c_{1}\\theta_{1}\\theta_{2}+c_{1}(\\theta_{2})^{2}-2c_{6}\\theta_{2})\\sigma^{2}-\\mathbb{E}_{e}[N^{e}(2c_{1}(\\theta_{1}+\\theta_{2})-c_{6})]\\sigma^{2}\\theta_{2}+c_{7}\\theta_{2}=0}\\\\ {(\\mathbb{E}_{e}[N^{e}(2c_{1}(\\theta_{1}+\\theta_{2})-c_{6})]\\sigma^{2}\\theta_{2}-c_{7})(c_{3}-c_{4})\\theta_{2}-[c_{2}(\\theta_{1}+\\theta_{2})-c_{5}](\\theta_{2})^{2}=0}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w h e r e\\ c_{1}\\ =\\ \\mathbb{E}_{e}[(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e^{s}}X_{1})],\\ c_{2}\\ =\\ \\mathbb{E}_{e}[N^{e}(\\tilde{A}^{e^{s}}X_{1})^{\\top}\\tilde{A}^{e^{s}}X_{1}],\\ c_{3}\\ =\\ \\mathbb{E}_{e}[(\\tilde{A}^{e^{s}}X_{1})^{\\top}{\\mathbf{1}}\\times{\\mathbf{1}}],}\\\\ &{c_{4}\\ =\\ \\mathbb{E}_{e}[((\\tilde{A}^{e^{k}}X_{1})^{\\top}{\\mathbf{1}}],\\ c_{5}\\ =\\ \\mathbb{E}_{e}[N^{e}((\\tilde{A}^{e^{k}}X_{1})^{\\top}\\tilde{A}^{e^{s}}X_{1}\\ +\\ t r((\\tilde{A}^{e^{k}})^{\\top}\\tilde{A}^{e^{k}})\\ +\\ N^{e}(1+\\sigma^{2}))}\\\\ &{c_{6}=\\mathbb{E}_{e}[(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e^{k}}X_{1})],\\,c_{7}=\\mathbb{E}_{e}[\\epsilon^{e^{\\top}}\\epsilon^{e}\\epsilon^{e^{\\top}}(\\tilde{A}^{e^{s}}X_{1})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. We will use some symbols to simplify the expression of the toy GNN. Denote $\\tilde{A}^{m}n_{1}+n_{2}+\\epsilon$ as $\\eta$ . Use the following notations to represent the components of the $L$ -layer GNN model: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\bar{t}}_{\\Theta}(A,X)=H_{1}^{(L)}\\theta_{1}+H_{2}^{(L)}\\theta_{2}}\\\\ &{\\qquad\\qquad=\\underbrace{\\biggl[\\theta_{1}^{1(L-1)}\\bar{A}\\left(\\cdots\\theta_{1}^{1(3)}\\left(\\theta_{1}^{1(2)}\\bar{A}(\\theta_{1}^{1(1)}\\bar{A}+\\theta_{1}^{2(1)}\\bar{I})X_{1}+\\theta_{1}^{2(2)}(\\theta_{1}^{1(1)}\\bar{A}+\\theta_{1}^{2(1)}\\bar{I})X_{1}\\right)+\\ldots\\right)}_{{\\cal C}_{1}}\\biggr]}_{\\mathcal{C}_{1}}}\\\\ &{\\qquad\\qquad+\\underbrace{\\biggl[\\theta_{2}^{1(L-1)}\\bar{A}\\left(\\cdots\\theta_{2}^{1(3)}\\left(\\theta_{2}^{1(2)}\\bar{A}(\\theta_{2}^{1(1)}\\bar{A}+\\theta_{2}^{2(1)}\\bar{I})\\bar{A}^{k+m}X_{1}+\\theta_{2}^{2(2)}(\\theta_{2}^{1(1)}\\bar{A}+\\theta_{2}^{2(1)}\\bar{I})\\bar{A}^{k+m}\\right.}_{{\\cal C}_{2}}}\\\\ &{\\qquad\\qquad+\\underbrace{\\biggl[\\theta_{2}^{1(L-1)}\\bar{A}\\left(\\cdots\\theta_{2}^{1(3)}\\left(\\theta_{2}^{1(2)}\\bar{A}(\\theta_{2}^{1(1)}\\bar{A}+\\theta_{2}^{2(1)}\\bar{I})\\eta+\\theta_{2}^{2(2)}(\\theta_{2}^{1(1)}\\bar{A}+\\theta_{2}^{2(1)}\\bar{I})\\eta\\right)+\\ldots\\right)\\biggr]\\theta_{2}}_{\\mathcal{Z}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~,~}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~oro~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "$C_{1},C_{2},Z\\ \\in\\ \\mathbb{R}^{N\\times1}$ . We use $C_{1}^{e},\\,C_{2}^{e}$ , and $Z^{e}$ to denote the variables from the corresponding environment $e$ . We further denote $C_{2}^{e}\\stackrel{^{\\textstyle-}}{=}C_{2}^{e^{\\prime}}\\tilde{A}^{e^{s}}X_{1},Z^{e}=C_{2}^{e^{\\prime}}\\eta.$ . ", "page_idx": 33}, {"type": "text", "text": "Using these notations, the loss of environment $e$ is ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(e)=\\mathbb{E}_{n_{1},n_{2}}\\left[\\|f_{\\Theta}(A^{e},X^{e})-Y^{e}\\|_{2}^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}_{n_{1},n_{2}}\\left[\\left\\|C_{1}^{e}\\theta_{1}+(C_{2}^{e}+Z^{e})\\theta_{2}-\\tilde{A}^{e}^{k}X_{1}-n_{1}\\right\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Denote the inner term $C_{1}^{e}\\theta_{1}+(C_{2}^{e}+Z^{e})\\theta_{2}-{\\tilde{A}^{e}}^{k}X_{1}-n_{1}$ as $l_{e}$ . ", "page_idx": 33}, {"type": "text", "text": "The variance of loss across environments is: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{e}[R(e)]=\\mathbb{E}_{e}[R^{2}(e)]-\\mathbb{E}_{e}^{2}[R(e)]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{e}\\left[\\left(\\mathbb{E}_{n_{1},n_{2}}\\left\\|C_{1}^{e}\\theta_{1}+(C_{2}^{e}+Z^{e})\\theta_{2}-\\tilde{A}^{e}^{k}X_{1}-n_{1}\\right\\|_{2}^{2}\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad-\\,\\mathbb{E}_{e}^{2}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left\\|C_{1}^{e}\\theta_{1}+(C_{2}^{e}+Z^{e})\\theta_{2}-\\tilde{A}^{e}^{k}X_{1}-n_{1}\\right\\|_{2}^{2}\\right].}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[(l_{e}^{\\top}l_{e})^{2}\\right]\\right]-\\mathbb{E}_{e}^{2}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Take the derivative of $\\mathbb{V}_{e}[R(e)]$ with respect to $\\theta_{1}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}}=\\mathbb{E}_{e}\\left[2\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}C_{1}^{e}\\right]\\right]}\\\\ &{\\qquad\\qquad-\\ 2\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\right]\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}C_{1}^{e}\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Calculate the derivative by terms: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi_{n_{1},n_{2}}[l_{c}^{\\top}l_{e}]=\\mathbb{E}_{n_{1},n_{2}}[C_{1}^{e}{^\\top}C_{1}^{e}(\\theta_{1})^{2}+C_{1}^{e}{^\\top}C_{2}^{e}\\theta_{1}\\theta_{2}+C_{1}^{e}{^\\top}Z^{e}\\theta_{1}\\theta_{2}-C_{1}^{e}{^\\top}(\\tilde{A}^{e})^{k}X_{1}\\theta_{1}-C_{1}^{e}{^\\top}n_{1}\\theta_{1}}\\\\ &{\\qquad\\qquad+\\,C_{2}^{e}{^\\top}C_{1}^{e}\\theta_{1}\\theta_{2}+C_{2}^{e}{^\\top}C_{2}^{e}(\\theta_{2})^{2}+C_{2}^{e}{^\\top}Z^{e}\\theta_{1}\\theta_{2}-C_{2}^{e}{^\\top}(\\tilde{A}^{e})^{k}X_{1}\\theta_{2}-C_{2}^{e}{^\\top}n_{1}\\theta_{2}}\\\\ &{\\qquad\\qquad+\\,Z^{e}{^\\top}C_{1}^{e}\\theta_{1}\\theta_{2}+Z^{e}{^\\top}C_{2}^{e}(\\theta_{2})^{2}+Z^{e}{^\\top}Z^{e}(\\theta_{2})^{2}-Z^{e}{^\\top}(\\tilde{A}^{e})^{k}X_{1}\\theta_{2}-Z^{e}{^\\top}n_{1}\\theta_{2}}\\\\ &{\\qquad\\qquad-\\,((\\tilde{A}^{e})^{k}X_{1})^{\\top}(C_{1}^{e}\\theta_{1}+C_{2}^{e}\\theta_{2})-((\\tilde{A}^{e})^{k}X_{1})^{\\top}Z^{e}\\theta_{2}+((\\tilde{A}^{e})^{k}X_{1})^{\\top}(\\tilde{A}^{e})^{k}X_{1}}\\\\ &{\\qquad\\qquad+\\,((\\tilde{A}^{e})^{k}X_{1})^{\\top}n_{1}-n_{1}^{\\top}(C_{1}^{e}\\theta_{1}+C_{2}^{e}\\theta_{2})-n_{1}^{\\top}Z^{e}\\theta_{2}+n_{1}^{\\top}(\\tilde{A}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since $n_{1}$ and $n_{2}$ are independent standard Gaussian noise, we have $\\mathbb{E}_{n_{1},n_{2}}[n_{1}]=\\mathbb{E}_{n_{1},n_{2}}[n_{2}]=\\mathbf{0}$ , $\\mathbb{E}_{n_{1},n_{2}}[n_{1}^{\\top}n_{2}]=\\mathbb{E}_{n_{1},n_{2}}[n_{2}^{\\top}n_{1}]=0$ and $\\mathbb{E}_{n_{1},n_{2}}[n_{1}^{\\top}n_{1}]=\\mathbb{E}_{n_{1},n_{2}}[n_{2}^{\\top}n_{2}]=N^{e}$ if it is the noise from $e$ . Also, since $\\epsilon^{e}$ and $n_{1},n_{2}$ are independent, we have $\\mathbb{E}_{n_{1},n_{2}}[n_{1}^{\\top}\\epsilon^{e}]=\\mathbb{E}_{n_{1},n_{2}}[n_{2}^{\\top}\\epsilon^{e}]=0$ . ", "page_idx": 34}, {"type": "text", "text": "When ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-1,...,L-s+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-s,L-s-1,...,1}\\\\ {\\theta_{2}^{1}{}^{(l)}=0,\\theta_{2}^{2}{}^{(l)}=1,\\quad l=L-1,...,1}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "we have $C_{2}^{e^{\\prime}}\\,=\\,I_{N^{e}}\\,\\,\\in\\,\\,\\mathbb{R}^{N^{e}\\times N^{e}}$ and $C_{1}^{e}\\;=\\;\\tilde{A}^{e^{s}}X_{1}$ . Consequently, we get $\\mathbb{E}_{n_{1},n_{2}}[Z^{e^{\\top}}n_{1}]\\;=$ $\\mathrm{tr}(C_{2}^{e^{\\prime}}\\tilde{A}^{e^{k}})=\\mathrm{tr}(\\tilde{A}^{e^{k}}),\\mathbb{E}_{n_{1},n_{2}}[Z^{e^{\\top}}Z^{e}]=\\mathrm{tr}\\left((\\tilde{A}^{e^{k}})^{\\top}(\\tilde{A}^{e^{k}})\\right)+N^{e}+\\epsilon^{e^{\\top}}\\epsilon^{e}.$ ", "page_idx": 34}, {"type": "text", "text": "Use the above conclusions and rewrite Equation (42) as (here we only plug in the value of $C_{2}^{e\\prime}$ ): ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi_{n_{1},n_{2}}[l_{e}^{\\top}l_{e}]=}\\\\ &{C_{1}^{e}\\tau_{1}^{\\top}C_{1}^{e}(\\theta_{1})^{2}+C_{1}^{e^{\\top}}C_{2}^{e}\\theta_{1}\\theta_{2}-C_{1}^{e}\\tau_{1}^{\\top}(\\tilde{A}^{e})^{k}X_{1}\\theta_{1}+C_{2}^{e^{\\top}}C_{1}^{e}\\theta_{1}\\theta_{2}+C_{2}^{e^{\\top}}C_{2}^{e}(\\theta_{2})^{2}-C_{2}^{e^{\\top}}(\\tilde{A}^{e})^{k}X_{1}\\theta_{2}}\\\\ &{+\\operatorname{tr}\\left((\\tilde{A}^{e})^{\\top}(\\tilde{A}^{e})\\right)(\\theta_{2})^{2}-((\\tilde{A}^{e})^{k}X_{1})^{\\top}(C_{1}^{e}\\theta_{1}+C_{2}^{e}\\theta_{2})+((\\tilde{A}^{e})^{k}X_{1})^{\\top}(\\tilde{A}^{e})^{k}X_{1}+N^{e}\\left(1+(\\theta_{2})^{2}\\right.}\\\\ &{\\left.-2\\mathrm{tr}(\\tilde{A}^{e})\\right)}\\\\ &{+[C_{1}^{e}\\tau_{1}^{\\top}\\epsilon^{e}+C_{2}^{e^{\\top}}\\epsilon^{e}+\\epsilon^{e^{\\top}}C_{1}^{e}]\\theta_{1}\\theta_{2}+\\epsilon^{e^{\\top}}\\epsilon^{e}(\\theta_{2})^{2}-2((\\tilde{A}^{e})^{k}X_{1})^{\\top}\\epsilon^{e}\\theta_{2}\\right\\}(\\ast\\ast),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "$(*)$ and $(**)$ represent terms that are independent and associated with $\\epsilon^{e}$ , respectively. Additionally, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{n_{1},n_{2}}[2l_{e}^{\\top}C_{1}^{e}]=2\\left[C_{1}^{e^{\\top}}C_{1}^{e}\\theta_{1}+C_{2}^{e^{\\top}}C_{1}^{e}\\theta_{2}+(C_{2}^{e^{\\prime}}\\epsilon^{e})^{\\top}C_{1}^{e}\\theta_{2}-((\\tilde{A^{e}})^{k}X_{1})^{\\top}C_{1}^{e}\\right]}\\\\ &{\\qquad\\qquad\\qquad=2\\left[C_{1}^{e^{\\top}}C_{1}^{e}\\theta_{1}+C_{2}^{e^{\\top}}C_{1}^{e}\\theta_{2}+\\epsilon^{e^{\\top}}C_{1}^{e}\\theta_{2}-((\\tilde{A^{e}})^{k}X_{1})^{\\top}C_{1}^{e}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Multiplying Equation (44) and (45) and take the expectation on $e$ , using the assumption that $\\mathbb{E}_{e}[(\\epsilon^{\\stackrel{\\bullet}{e}}_{i})^{2}]\\stackrel{\\smile}{=}\\sigma^{2}$ ( $\\epsilon^{e}{_i}$ is the $i$ -th element of $\\epsilon^{e}$ ): ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{z}_{e}\\left[2\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}l_{e}]\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}C_{1}\\right]\\right]=4\\mathbb{E}_{e}\\left[(\\mathfrak{x})\\left(C_{1}^{e^{\\top}}C_{1}^{e}\\theta_{1}+C_{2}^{e^{\\top}}C_{2}^{e^{\\prime}}\\theta_{2}-((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{1}^{e}\\right)\\right]}\\\\ &{\\phantom{=4\\mathbb{E}_{e}\\left[(\\tilde{A}^{e^{s}}X_{1})^{\\top}\\tilde{A}^{e^{s}}X_{1}(3\\theta_{1}\\theta_{2}+(\\theta_{2})^{2})-2(\\tilde{A}^{e^{s}}X_{1})^{\\top}((\\tilde{A}^{e})^{k}X_{1})^{\\top}(\\tilde{A}^{e})\\theta_{2}-2(\\tilde{A}^{e^{s}}X_{1})^{\\top}\\theta_{1}\\theta_{2}\\right]}\\\\ &{\\phantom{=4\\mathbb{E}_{e}\\left[N^{e}\\epsilon^{e^{\\top}}\\epsilon^{e}{}^{\\epsilon}{}^{\\tau}(\\tilde{A}^{e^{s}}X_{1})\\right]\\theta_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Next target is to compute $2\\mathbb{E}_{e}[\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}l_{e}]]$ and $\\mathbb{E}_{e}[\\mathbb{E}_{n_{1},n_{2}}[2l_{e}^{\\top}C_{1}]]$ . Since $\\epsilon^{e}$ has zero mean, we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n2\\mathbb{E}_{e}[\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}l_{e}]]=2\\mathbb{E}_{e}[(*)]+2\\mathbb{E}_{e}[N^{e}](\\theta_{2})^{2}\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{e}[\\mathbb{E}_{n_{1},n_{2}}[2l_{e}^{\\top}C_{1}^{e}]]=2\\mathbb{E}_{e}\\left[C_{1}^{e^{\\top}}C_{1}^{e}\\theta_{1}+C_{2}^{e^{\\top}}C_{1}^{e}\\theta_{2}-((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{1}^{e}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Use Equation (46) (47) and (48) and let \u2202Ve[R(e)]= 0, we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{e}\\left[3(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e^{s}}X_{1})(\\theta_{1}\\theta_{2}+\\frac{1}{3}(\\theta_{2})^{2})-2(\\tilde{A}^{e^{s}}X_{1})^{\\top}((\\tilde{A}^{e})^{k}X_{1})\\theta_{2}\\right]\\sigma^{2}+\\mathbb{E}_{e}[{\\epsilon^{e}}^{\\top}{\\epsilon^{e}}{\\epsilon^{e}}^{\\top}(\\tilde{A}^{e}{}^{s}X_{1})\\theta_{2}+\\theta_{2}{\\epsilon^{e}}^{\\top}]\\mathbb{E}_{e}[\\left(\\tilde{A}^{e}{}^{s}X_{1}\\right)^{\\top}(\\tilde{A}^{e}{}^{s}X_{1})\\theta_{2},}\\\\ &{-\\mathbb{E}_{e}[N^{e}]\\mathbb{E}_{e}\\left[2(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e}{}^{s}X_{1})(\\theta_{1}+\\theta_{2})-((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{1}^{e}\\right]\\theta_{2}\\sigma^{2}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now we start calculating the expression of \u2202Ve\u2202[\u03b8R(e)]: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{2}}=\\mathbb{E}_{e}\\left[2\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}(C_{2}+Z^{e})\\right]\\right]}\\\\ &{\\qquad\\qquad-\\left.2\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\right]\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}(C_{2}^{e}+Z^{e})\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Let \u2202Ve[R(e)] = 0: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{c}_{e}\\left[(C_{1}^{e^{\\tau}}C_{2}^{e^{\\prime}}+C_{2}^{e^{\\tau}}C_{2}^{e^{\\prime}}+C_{2}^{e^{\\prime}}{}^{\\top}C_{1}^{e^{\\tau}})\\theta_{1}\\theta_{2}+(C_{2}^{e^{\\prime}})^{\\top}C_{2}^{e^{\\prime}}(\\theta_{2})^{2}-2((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{2}^{e^{\\prime}}\\theta_{2}\\right]}\\\\ &{\\mathfrak{c}_{e}\\left[(C_{2}^{e^{\\tau^{\\prime}}}C_{2}^{e}\\theta_{2}-((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{2}^{e^{\\prime}})\\right]\\sigma^{2}}\\\\ &{-\\mathbb{E}_{e}\\left[N^{e}\\sigma^{2}\\left(C_{1}^{e^{\\tau}}C_{2}^{e}\\theta_{1}+C_{2}^{e^{\\tau}}C_{2}^{e}\\theta_{2}-((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{2}^{e}+\\mathfrak{w}((\\tilde{A}^{e^{k}})^{\\top}\\tilde{A}^{e^{k}})+N^{e}+C_{2}^{e^{\\tau^{\\prime}}}C_{2}^{e^{\\prime}}\\sigma^{2}\\right)(\\theta_{2},\\theta_{1})\\right]}\\\\ &{\\underset{\\circ}{\\sim}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Plug Equation (49) in (51), we reach: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\mathbb{E}_{e}\\left[N^{e}\\big(\\tilde{A}^{e^{s}}X_{1}\\big)^{\\top}(\\tilde{A}^{e^{s}}X_{1})(\\theta_{1}+\\theta_{2})-((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{1}^{e}\\right]\\sigma^{2}-\\mathbb{E}_{e}[\\epsilon^{e^{\\top}}\\epsilon^{e}\\epsilon^{e^{\\top}}(\\tilde{A}^{e^{s}}X_{1})]\\right]\\theta_{2}}\\\\ &{\\mathrm{~}\\mathfrak{c}_{e}\\left((\\tilde{A}^{e^{s}}X_{1})^{\\top}\\mathbf{1}_{\\mathbb{N}^{e}}\\theta_{2}-(\\tilde{A}^{e^{k}}X_{1})^{\\top}\\mathbf{1}_{\\mathbb{N}^{e}}\\right)}\\\\ &{-\\mathbb{E}_{e}\\left[N^{e}\\left((\\tilde{A}^{e^{s}}X_{1})^{\\top}\\tilde{A}^{e^{s}}X_{1}(\\theta_{1}+\\theta_{2})-(\\tilde{A}^{e^{k}}X_{1})^{\\top}\\tilde{A}^{e^{s}}X_{1}+\\mathrm{tr}((\\tilde{A}^{e^{k}})^{\\top}\\tilde{A}^{e^{k}})+N^{e}(1+\\sigma^{2})\\right)\\right](\\theta_{2},\\theta_{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Let~}\\;c_{1}\\;=\\;\\;\\mathbb{E}_{e}[(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e^{s}}X_{1})],\\;\\;c_{2}\\;\\;=\\;\\;\\mathbb{E}_{e}[N^{e}(\\tilde{A}^{e^{s}}X_{1})^{\\top}\\tilde{A}^{e^{s}}X_{1}],\\;\\;c_{3}\\;\\;=\\;\\;\\mathbb{E}_{e}[(\\tilde{A}^{e^{s}}X_{1})^{\\top}{\\mathbf{1}}],}\\\\ &{c_{4}\\;=\\;\\;\\mathbb{E}_{e}[((\\tilde{A}^{e^{k}}X_{1})^{\\top}{\\mathbf{1}}],\\;c_{5}\\;\\;=\\;\\;\\mathbb{E}_{e}[N^{e}((\\tilde{A}^{e^{k}}X_{1})^{\\top}\\tilde{A}^{e^{s}}X_{1}\\,+\\,\\mathrm{tr}((\\tilde{A}^{e^{k}})^{\\top}\\tilde{A}^{e^{k}})\\,+\\,N^{e}(1\\,+\\,\\sigma^{2}))]}\\\\ &{c_{6}=\\mathbb{E}_{e}[(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e^{k}}X_{1})],\\;c_{7}=\\mathbb{E}_{e}[\\epsilon^{e^{\\top}}\\epsilon^{e}\\epsilon^{e^{\\top}}(\\tilde{A}^{e^{s}}X_{1})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "we conclude that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{(3c_{1}\\theta_{1}\\theta_{2}+c_{1}(\\theta_{2})^{2}-2c_{6}\\theta_{2})\\sigma^{2}-\\mathbb{E}_{e}[N^{e}(2c_{1}(\\theta_{1}+\\theta_{2})-c_{6})]\\sigma^{2}\\theta_{2}+c_{7}\\theta_{2}=0}\\\\ {(\\mathbb{E}_{e}[N^{e}(2c_{1}(\\theta_{1}+\\theta_{2})-c_{6})]\\sigma^{2}\\theta_{2}-c_{7})(c_{3}-c_{4})\\theta_{2}-[c_{2}(\\theta_{1}+\\theta_{2})-c_{5}](\\theta_{2})^{2}=0}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "As for the derivative respect to $\\theta_{1}^{1}{}^{(l)},\\theta_{1}^{2}{}^{(l)},\\theta_{2}^{1}{}^{(l)},\\theta_{2}^{2}{}^{(l)}.$ , when they take the special value in Equation (43), we have $\\begin{array}{r}{\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}}\\,=\\,0\\,\\Rightarrow\\,\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}^{1(l)}}\\,=\\,\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}^{2(l)}}\\,=\\,0}\\end{array}$ )] = 0 and \u2202Ve[R(e)] $\\begin{array}{r}{\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{2}}\\,=\\,0\\,\\Rightarrow\\,\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{2}^{1(l)}}\\,=\\,}\\end{array}$ \u2202V\u2202e\u03b8[2R(l()e)] = 0, l = 1, ..., L. So we conclude the solution induced by Equation (53) is the solution of the objective, and $\\theta_{2}=0$ is not a valid solution. ", "page_idx": 35}, {"type": "text", "text": "G.1.3 Proof of the Failure Case on Graphs of IRMv1 under Concept Shift", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We present the formal version of the IRM part in Theorem 2.3 as Theorem G.3 below. ", "page_idx": 36}, {"type": "text", "text": "Theorem G.3. (IRMv1 will use spurious features on graphs under concept shift, formal) Under the SCM of Equation (2), there exists $s\\in\\mathbb{N}^{+}$ that satisfies $0<s<L$ and $s\\neq k$ such that optimizing the IRMv1 objective $\\mathrm{min}_{\\Theta}\\mathbb{E}_{e}[||\\nabla_{w|w=1.0}R(e)||^{2}]$ will not lead to the invariant solution $\\theta_{2}=0$ for parameters of the GNN (3) when $\\{\\theta_{1}^{\\mathrm{1}}{}^{(l)},\\theta_{1}^{2}{}^{(l)},\\theta_{2}^{1}{}^{(l)},\\theta_{2}^{2}{}^{(l)}\\}$ take the special solution: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Theta_{0}=\\left\\{\\begin{array}{l l}{\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,}&{l=L-1,...,L-s+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,}&{l=L-s,L-s-1,...,1}\\\\ {\\theta_{2}^{1}{}^{(l)}=0,\\theta_{2}^{2}{}^{(l)}=1,}&{l=L-1,...,1}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. From the proof of non-graph IRMv1 case Appendix G.1.1 we know that when IRMv1 objective is optimized, we have $\\bar{\\nabla_{w}}R({e})\\,=\\,0$ for all $e$ . For the graph case, the expected risk of environment $e$ is ", "page_idx": 36}, {"type": "equation", "text": "$$\nR(e)=\\mathbb{E}_{n_{1},n_{2}}[\\|\\theta_{1}C_{1}^{e}+\\theta_{2}(C_{2}^{e}+Z^{e})-(\\tilde{A}^{e})^{k}X_{1}-n_{1}\\|_{2}^{2}],\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the definition of $C_{1}^{e},\\,C_{2}^{e}$ and $Z^{e}$ follows Equation (38). Now let\u2019s check if the invariant solution $\\theta_{2}=0$ is a valid solution. If $\\theta_{2}=0$ holds, then the following equation must hold for every environment $e$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{w}R(e)=\\mathbb{E}_{n_{1},n_{2}}[(\\theta_{1}C_{1}^{e}+\\theta_{2}(C_{2}^{e}+Z^{e})-(\\tilde{A}^{e})^{k}X_{1}-n_{1})^{\\top}(\\theta_{1}C_{1}^{e}+\\theta_{2}(C_{2}^{e}+Z^{e}))]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{n_{1},n_{2}}[(\\theta_{1})^{2}{C_{1}^{e}}^{\\top}C_{1}^{e}+(\\theta_{2})^{2}(C_{2}^{e}+Z^{e})^{\\top}(C_{2}^{e}+Z^{e})+2\\theta_{1}\\theta_{2}{C_{1}^{e}}^{\\top}(C_{2}^{e}+Z^{e})}\\\\ &{\\qquad\\qquad-\\theta_{1}{C_{1}^{e}}^{\\top}((\\tilde{A}^{e})^{k}X_{1}+n_{1})-\\theta_{2}(C_{2}^{e}+Z^{e})((\\tilde{A}^{e})^{k}X_{1}+n_{1})]}\\\\ &{\\qquad\\qquad=(\\theta_{1})^{2}((\\tilde{A}^{e})^{s}X_{1})^{\\top}((\\tilde{A}^{e})^{s}X_{1})-\\theta_{1}((\\tilde{A}^{e})^{k}X_{1})^{\\top}((\\tilde{A}^{e})^{s}X_{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "When $s\\neq k$ , we have = ((((  AA\u02dc\u02dcee))ksXX1))\u22a4\u22a4((((  AA\u02dc\u02dcee))ssXX1)). The value of this solution of \u03b81 varies with environment $e$ , and thus is not a valid solution. ", "page_idx": 36}, {"type": "text", "text": "However, now we will show that optimizing IRMv1 does not necessarily lead to lower-layer parameters such that $s=k$ . To reveal this, by taking the derivative of $\\mathcal{L}_{\\mathrm{IRMv1}}$ w.r.t. $\\theta_{1}$ and $\\theta_{2}$ and let them $=0$ , we can get two cubic equations: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMvl}}}{\\partial\\theta_{1}}=\\!\\mathbb{E}_{e}[(C_{1}^{e^{\\top}}C_{1}^{e}(\\theta_{1})^{2}+(C_{2}^{e^{\\top}}C_{2}^{e}+2C_{2}^{e^{\\top}}\\epsilon^{e}+2N^{e}+\\epsilon^{\\top}\\epsilon)(\\theta_{2})^{2}}\\\\ &{\\qquad\\qquad+({C_{1}^{e^{\\top}}C_{2}^{e}+C_{1}^{e^{\\top}}\\epsilon^{e}})\\theta_{1}\\theta_{2}-(\\tilde{A}^{e^{k}}X_{1})^{\\top}C_{1}^{e}\\theta_{1}-[(\\tilde{A}^{e^{k}}X_{1})^{\\top}+N^{e}+n_{1}^{\\top}\\epsilon^{e}]\\theta_{2})}\\\\ &{\\qquad\\qquad\\quad(2C_{1}^{e^{\\top}}C_{1}^{e^{\\top}}\\theta_{1}+(C_{1}^{e^{\\top}}C_{2}^{e}+C_{1}^{e^{\\top}}\\epsilon)\\theta_{2}-(\\tilde{A}^{e^{k}}X_{1})^{\\top}C_{1}^{e})]=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}}=\\!\\mathbb{E}_{e}[(C_{1}^{e}\\top C_{1}^{e}(\\theta_{1})^{2}+(C_{2}^{e}\\top C_{2}^{e}+2C_{2}^{e}\\top_{\\epsilon}^{e}+2N^{e}+\\epsilon^{\\top}\\epsilon)(\\theta_{2})^{2}}\\\\ &{\\qquad\\qquad+({C_{1}^{e}}^{\\top}C_{2}^{e}+{C_{1}^{e}}^{\\top}\\epsilon^{e})\\theta_{1}\\theta_{2}-(\\tilde{A}^{e}^{k}X_{1})^{\\top}C_{1}^{e}\\theta_{1}-[(\\tilde{A}^{e}^{k}X_{1})^{\\top}+N^{e}+n_{1}^{\\top}\\epsilon^{e}]\\theta_{2})}\\\\ &{\\qquad\\quad(2(C_{2}^{e}\\top_{\\epsilon}^{e}+2C_{2}^{e}\\top_{\\epsilon}^{e}+2N^{e}+\\epsilon^{e}\\top_{\\epsilon}^{e})\\theta_{2}+(C_{1}^{e}\\top_{\\epsilon_{2}}^{e}+C_{1}^{e}\\top_{\\epsilon})\\theta_{1}}\\\\ &{\\qquad\\quad-(\\tilde{A}^{e}^{k}X_{1})^{\\top}(C_{2}^{e}+\\epsilon^{e})+N^{e}+n_{1}^{\\top}\\epsilon^{e})]=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "From the analysis in Appendix F.3, we know that as long as the lower-layer parameters take any value that satisfies the form in Equation (27), even if $s\\neq k$ , we can get $\\begin{array}{r}{\\overline{{\\partial\\ell_{\\mathrm{IRMv1}}}}^{\\,\\^{\\prime}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{1}^{1}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{1}^{2}}}\\end{array}$ and \u2202LIRMv1 $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}^{1}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}^{2}}}\\end{array}$ \u2202L\u2202I\u03b8R2Mv1. Thus, IRM cannot necessarily learn a s = k. At this time (when $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{1}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{1}^{1}}\\stackrel{^{-}}{=}\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{1}^{2}}\\stackrel{^{-}}{=}0}\\end{array}$ and $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}^{1}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}^{2}}=0}\\end{array}$ but $s\\neq k$ ), from the form of Equations (57) and (58) we know that there exist solutions that $\\theta_{2}\\neq0$ , and the solution of $\\theta_{1}$ and $\\theta_{2}$ both depend on $\\mathbb{E}_{e}(F(e))$ , where $F(e)$ is some random variable associated with $e$ . ", "page_idx": 36}, {"type": "text", "text": "G.1.4 Proof of the Successful on Graphs Case of CIA under Concept Shift ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Theorem G.4. Optimizing the CIA objective will lead to the optimal solution $\\Theta^{*}$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\theta_{1}=1}\\\\ {\\theta_{2}=0\\quad a n d\\quad\\exists l\\in\\{1,...,L-1\\}\\;s.t.\\;\\theta_{2}^{1(l)}=\\theta_{2}^{2(l)}=0}\\\\ {\\theta_{1}^{1(l)}=1,\\theta_{1}^{2(l)}=1,\\quad l=L-1,...,L-k+1}\\\\ {\\theta_{1}^{1(l)}=0,\\theta_{1}^{2(l)}=1,\\quad l=L-k,L-k-1,...,1}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. For brevity, denote a node representation of $C_{1c}^{e}$ as $C_{1}^{i}$ and the one of $C_{1\\mathrm{~\\it~c~}}^{e^{\\prime}}$ as $C_{1}^{j}$ . The same is true for $C_{2}^{i}$ and $C_{2}^{j}$ . In this toy model, we need to consider the expectation of the noise, while in real cases such noise is included in the node features so taking expectation on $e$ will handle this. Therefore, we add $\\mathbb{E}_{n_{1},n_{2}}$ in this proof, and this expectation is excluded in the formal description of the objective in the main paper. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{CIA}}=\\mathbb{E}_{\\mathbf{\\Phi}_{e},e^{\\prime}}\\mathbb{E}_{n_{1},n_{2}}\\mathbb{E}_{c}\\mathbb{E}_{\\delta}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{1}}=0}\\end{array}$ , we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{\\phi}_{e^{\\neq}}}\\mathbb{E}_{c}\\mathbb{E}_{(i,j)\\in\\Omega^{e,\\epsilon^{\\prime}}}\\left[(C_{1}^{i}-C_{1}^{j})^{\\top}(C_{1}^{i}-C_{1}^{j})\\theta_{1}+(C_{2}^{i}-C_{2}^{k})^{\\top}(C_{1}^{i}-C_{1}^{j})\\theta_{2}\\right]=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Also, we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\mathcal{D}\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{2}}=\\mathbb{E}_{\\mathit{e},\\epsilon^{\\prime}}\\mathbb{E}_{\\mathit{c}}\\mathbb{E}_{\\mathit{\\iota},j)\\in\\Omega^{*}\\times\\mathit{e}^{\\prime}}\\left[(C_{1}^{i}-C_{1}^{j})^{\\top}(C_{2}^{i}-C_{2}^{j})\\theta_{1}+\\left[(C_{2}^{i}-C_{2}^{k})^{\\top}(C_{2}^{i}-C_{2}^{j})+(Z^{e}-Z^{e^{\\prime}})^{\\top}(C_{2}^{i}-C_{2}^{j})^{\\top}(C_{2}^{i}-C_{2}^{j})\\right]\\right].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Further let $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{2}}=0}\\end{array}$ , combining Equation (62) and using Assumption 2.1 we get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\theta_{1}=0\\quad\\mathrm{or}\\quad\\exists l\\in\\{1,...,L-1\\}\\,\\mathrm{s.t.}\\,\\theta_{1}^{1(l)}=\\theta_{1}^{2(l)}=0}\\\\ {\\theta_{2}=0}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "or, if $\\theta_{1}\\neq0$ and $\\forall l\\in\\{1,...,L-1\\}$ , the parameters of that layer $l$ of the invariant branch of the GNN are not all zero: $\\theta_{1}^{1}{}^{(l)}\\neq0$ or $\\theta_{1}^{2^{(l)}}\\neq0$ , then we get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\mathrm{\\i}_{2}\\mathbb{E}_{e,e^{\\prime}}\\mathbb{E}_{c}\\mathbb{E}}\\\\ &{\\begin{array}{r l}{\\nu_{+}\\mathbb{E}_{c}^{j}\\!\\exp\\!\\!\\!^{\\prime}\\!}&{{}_{i,j}}\\\\ {e\\neq\\!e^{\\prime}}&{(i,j)\\!\\exp\\!\\!\\!^{\\prime}\\!e^{\\tau^{\\prime}}}\\end{array}\\!\\left[-\\frac{[(C_{1}^{i}-C_{1}^{j})^{\\top}(C_{2}^{i}-C_{2}^{j})]^{2}}{(C_{1}^{i}-C_{1}^{j})^{\\top}(C_{1}^{i}-C_{1}^{j})}+(C_{2}^{i}-C_{2}^{j})^{\\top}(C_{2}^{i}-C_{2}^{j})+(Z^{e}-Z^{e^{\\prime}})^{\\top}(Z^{e}-\\tau^{\\prime})\\right]}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "According to Cauchy\u2013Schwarz inequality, F > 0 unless \u2203l \u2208{1, ..., L \u22121} s.t. \u03b821(l) $\\theta_{2}^{1}{}^{(l)}=\\theta_{2}^{2}{}^{(l)}=0$ .   \nTo ensure $\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{2}}$ , we conclude that $\\theta_{2}=0$ or $\\exists l\\in\\{1,...,L-1\\}$ s.t. $\\theta_{2}^{1}{}^{(l)}=\\theta_{2}^{2}{}^{(l)}=0$ . ", "page_idx": 37}, {"type": "text", "text": "In conclusion, to satisfy the constraint of CIA, no matter whether the invariant branch has zero output, the spurious branch must have zero parameters, i.e., ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\theta_{2}=0\\quad\\mathrm{or}\\quad\\exists l\\in\\{1,...,L-1\\}\\;\\mathrm{s.t.}\\;\\theta_{2}^{1}{}^{(l)}=\\theta_{2}^{2}{}^{(l)}=0\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus, CIA will remove spurious features. ", "page_idx": 37}, {"type": "text", "text": "Now we show that when CIA objective has been reached (the spurious branch has zero outputs), the objective of min\u0398 $\\mathbb{E}_{e}[\\mathcal{L}(f_{\\Theta}(A^{e};X^{e}),Y^{e})]$ will help to learn predictive paramters of the invariant branch $\\theta_{1},\\theta_{1}^{1}{}^{(}l)$ and $\\theta_{1}^{2}{}^{(}l_{}^{}$ ). When Equation (66) holds, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathbb{E}_{e}\\left[\\mathcal{L}\\left(f_{\\Theta}(A^{e},X^{e}),Y^{e}\\right)\\right]}{\\partial\\theta_{1}}=2\\mathbb{E}_{e}\\mathbb{E}_{n_{1},n_{2}}\\left[\\left(C_{1}^{e}\\theta_{1}-(\\tilde{A}^{e})^{k}X_{1}-n_{1}\\right)^{\\top}C_{1}^{e}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2\\mathbb{E}_{e}\\left[\\left(C_{1}^{e}\\theta_{1}-(\\tilde{A}^{e})^{k}X_{1}\\right)^{\\top}C_{1}^{e}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let $\\begin{array}{r}{\\frac{\\partial\\mathbb{E}_{e}[\\mathcal{L}(f_{\\Theta}(A^{e},X^{e}),Y^{e})]}{\\partial\\theta_{1}}=0.}\\end{array}$ , we get the predictive parameters ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\theta_{1}=1}\\\\ {\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-1,...,L-k+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-k,L-k-1,...,1}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Plug the final solution back in $\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{1}^{1\\,(l)}}$ $\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{1}^{2\\,(l)}}$ \u2202LCIA \u2202LCIA $\\frac{\\partial\\mathbb{E}_{e}[\\mathcal{L}(f_{\\Theta}(A^{e},X^{e}),Y^{e})]}{\\partial\\theta_{1}^{1}{}^{(l)}},$ \u2202\u03b81(l) \u2202\u03b82(l) $\\frac{\\partial\\mathbb{E}_{e}[\\mathcal{L}(f_{\\Theta}(A^{e},X^{e}),Y^{e})]}{\\partial\\theta_{1}^{2(l)}}$ $\\frac{\\partial\\mathbb{E}_{e}[\\mathcal{L}(f_{\\Theta}(A^{e},X^{e}),Y^{e})]}{\\partial\\theta_{2}^{1}{}^{(l)}}$ $\\frac{\\partial\\mathbb{E}_{e}[\\mathcal{L}(f_{\\Theta}(A^{e},X^{e}),Y^{e})]}{\\partial\\theta_{2}^{2(l)}}$ we can verify that these terms are all when further letting $\\exists l\\in\\{1,...,L-1\\}$ s.t. $\\bar{\\theta}_{2}^{1}{}^{(l)}=\\theta_{2}^{2}{}^{(l)}=0$ . \u53e3 ", "page_idx": 38}, {"type": "text", "text": "G.2 Proof of the Covariate Shift Case ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "G.2.1 Proof of the non-Graph Success Case of VREx and IRMv1 under Covariate Shift ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We restate Proposition B.1 as Proposition G.5 below. Proposition G.5. (VREx and IRMv1 learn invariant features for non-graph tasks under covariate shift, proof is in ) For the non-graph version of the SCM in Equation $(l9)$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\nY^{e}=X_{1}+n_{1},\\;X_{2}^{e}=n_{2}+\\epsilon^{e},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Optimizing VREx min\u0398 $\\mathcal{L}_{V R E x}=\\mathbb{V}_{e}[R(e)]$ and IRMv1 min\u0398 $\\mathcal{L}_{I R M\\nu I}=\\mathbb{E}_{e}[||\\nabla_{w|w=1.0}R(e)||^{2}]\\;w i l l$ learn invariant features when using a $^{\\,l}$ -layer linear network: $f(X)=\\theta_{1}X_{1}+\\theta_{2}X_{2}$ . ", "page_idx": 38}, {"type": "text", "text": "Proof. VREx. For covariate shift, denote $X_{1}\\theta_{1}+X_{2}^{e}\\theta_{2}-X_{1}-n_{1}$ as $l_{e}$ , the expected risk in environment $e$ is $R(e)=\\mathbb{E}_{n_{1},n_{2}}\\|\\theta_{1}X_{1}+\\theta_{2}(n_{2}+\\epsilon^{e})-(X_{1}+n_{1})\\|_{2}^{2}$ . Take the derivative of the VREx objective $\\mathbb{V}_{e}[R(e)]$ with respect to $\\theta_{1}$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}}=\\mathbb{E}_{e}\\left[2\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}X_{1}\\right]\\right]}\\\\ &{\\qquad\\qquad-\\ 2\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\right]\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}X_{1}\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Calculate these terms: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\mathbb{E}_{e}[\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}l_{e}]\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}X_{1}]]}\\\\ &{{=}(\\theta_{1})^{3}(X_{1}^{\\top}X_{1})^{2}+\\theta_{1}(\\theta_{2})^{2}(3X_{1}^{\\top}X_{1}\\sigma^{2}+X_{1}^{\\top}X_{1}\\mathbb{E}_{e}[N^{e}])+(\\theta_{2})^{3}\\mathbb{E}_{e}[(\\epsilon^{e})^{\\top}\\epsilon(\\epsilon^{e})^{\\top}]X_{1}}\\\\ &{{+}(\\theta_{1})^{2}(X_{1}^{\\top}X_{1}-2(X_{1}^{\\top}X_{1})^{2})-(\\theta_{2})^{2}(X_{1}^{\\top}X_{1})^{2}(3\\sigma^{2}+\\mathbb{E}_{e}[N^{e}])}\\\\ &{{+}\\theta_{1}(X_{1}^{\\top}X_{1})^{2}(3(X_{1}^{\\top}X_{1})^{2}+\\mathbb{E}_{e}[N^{e}])-(X_{1}^{\\top}X_{1})^{2}((X_{1}^{\\top}X_{1})^{2}+\\mathbb{E}_{e}[N^{e}])}\\\\ &{~~\\mathbb{E}_{e}[\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}l_{e}]\\mathbb{E}_{e}[\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}X_{1}]]}\\\\ &{{=}(\\theta_{1})^{3}(X_{1}^{\\top}X_{1})^{2}+\\theta_{1}(\\theta_{2})^{2}(X_{1}^{\\top}X_{1}\\sigma^{2}+X_{1}^{\\top}X_{1}\\mathbb{E}_{e}[N^{e}])}\\\\ &{{+}(\\theta_{1})^{2}(X_{1}^{\\top}X_{1}-2(X_{1}^{\\top}X_{1})^{2})-(\\theta_{2})^{2}(X_{1}^{\\top}X_{1})^{2}(\\sigma^{2}+\\mathbb{E}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Hence, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}}=\\theta_{1}(\\theta_{2})^{2}(2X_{1}^{\\top}X_{1}\\sigma^{2})+(\\theta_{2})^{3}X_{1}-2(\\theta_{2})^{2}X_{1}\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let $\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}}=0$ , we have $\\theta_{2}=0$ . ", "page_idx": 38}, {"type": "text", "text": "When $\\theta_{2}=0$ and when $\\theta_{1}=1$ , we get ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}(n_{2}+\\epsilon^{e})]=\\theta_{1}X_{1}^{\\top}\\epsilon-X_{1}^{\\top}\\epsilon^{e}=0.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "As a result, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{2}}=\\mathbb{E}_{e}\\left[2\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}(n_{2}+\\epsilon^{e})\\right]\\right]}\\\\ &{\\phantom{\\frac{(0)^{*}}{\\partial\\theta_{2}}}-2\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\right]\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}(n_{2}+\\epsilon^{e})\\right]\\right]}\\\\ &{\\phantom{\\frac{(0)^{*}}{\\partial\\theta_{2}}}=0-0}\\\\ &{=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In conclusion, $\\theta_{1}=1$ and $\\theta_{2}=0$ is the solution of the VREx objective in this non-graph covariate shift task. ", "page_idx": 39}, {"type": "text", "text": "IRMv1. The objective of IRMv1 is $\\mathbb{E}_{e}\\|\\nabla_{w}R(e)\\|_{2}^{2}$ . When IRMv1 loss is optimized to zero, we have $\\nabla_{w}R(e)=0$ for all environments $e$ . ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{w}R(e)=\\mathbb{E}_{n_{1},n_{2}}[2\\big(\\theta_{1}X_{1}+\\theta_{2}X_{2}-(X_{1}+n_{1})\\big)(\\theta_{1}X_{1}+\\theta_{2}X_{2})]}\\\\ &{\\qquad\\qquad=2\\big((\\theta_{1})^{2}X_{1}^{\\top}X_{1}+(\\theta_{2})^{2}\\big((\\epsilon^{e})^{\\top}\\epsilon^{e}+N^{e}\\big)+2\\theta_{1}\\theta_{2}X_{1}^{\\top}\\epsilon^{e}}\\\\ &{\\qquad\\qquad-\\theta_{1}X_{1}^{\\top}X_{1}-\\theta_{2}X_{1}^{\\top}\\epsilon^{e}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "To realize $\\nabla_{w}R(e)\\,=\\,0$ for all $e$ , we must let $\\theta_{2}\\,=\\,0$ (and consequently $\\theta_{1}=1$ ), otherwise the solution of $\\theta_{2}$ will include terms related to $\\epsilon^{e}$ and $N^{e}$ that vary with environments, and a single value $\\theta_{2}$ cannot fit all these values. Thus we finish the proof for IRMv1. \u53e3 ", "page_idx": 39}, {"type": "text", "text": "G.2.2 Proof of the Failure Case on Graphs of VREx under Covariate Shift ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We restate Theorem B.2 as Theorem G.6 below: ", "page_idx": 39}, {"type": "text", "text": "Theorem G.6. (VREx will use spurious features on graphs under covariate shift) Under the SCM of Equation (19), the objective min\u0398 $\\mathbb{V}_{e}[R(e)]$ has non-unique solutions for parameters of the GNN (3) when part of the model parameters $\\{\\theta_{1}^{1}{}^{(l)},\\theta_{1}^{2}{}^{(l)},\\theta_{2}^{1}{}^{(l)},\\theta_{2}^{2}{}^{(l)}\\}$ take the values ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\Theta_{0}=\\left\\{\\begin{array}{l l}{\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,}&{l=L-1,...,L-s+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,}&{l=L-s,L-s-1,...,1}\\\\ {\\theta_{2}^{1}{}^{(l)}=0,\\theta_{2}^{2}{}^{(l)}=1,}&{l=L-1,...,1}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "$0<s<L$ is some positive integer, $\\theta_{1}$ and $\\theta_{2}$ have four sets of solutions of the quadratic equation, some of which are spurious solutions that $\\theta_{2}\\neq0$ (although $\\theta_{2}=0$ is indeed one of the solutions, VREx is not guaranteed to reach this solution): ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{c_{1}\\sigma^{2}(2\\theta_{1}\\theta_{2}+(\\theta_{2})^{2}-2c_{2}\\sigma^{2}\\theta_{2})+c_{3}\\theta_{2}-\\mathbb{E}_{e}[N^{e}]c_{1}\\sigma^{2}\\theta_{1}\\theta_{2}+\\mathbb{E}_{e}[N^{e}]c_{2}\\sigma^{2}\\theta_{2}=0}\\\\ {\\left[c_{3}\\theta_{2}-\\mathbb{E}_{e}[N^{e}]c_{1}\\sigma^{2}\\theta_{1}\\theta_{2}+\\mathbb{E}_{e}[N^{e}]c_{2}\\sigma^{2}\\theta_{2}\\right]c_{4}-c_{5}(\\theta_{2})^{2}=0}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nu h e r e\\;c_{1}=\\mathbb{E}[(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e^{s}}X_{1})],\\;c_{2}=\\mathbb{E}[(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e^{k}}X_{1})],\\;c_{3}=\\mathbb{E}_{e}[\\epsilon^{e^{\\top}}\\epsilon^{e}\\epsilon^{e\\top}(\\tilde{A}^{e^{s}}X_{1})]\\sigma^{2},}\\\\ &{4=\\mathbb{E}_{e}\\left[((\\tilde{A}^{e})^{k}X_{1})^{\\top}{\\mathbf{1}}_{\\mathbf{N}^{e}}\\right]\\sigma^{2},\\;c_{5}=\\mathbb{E}_{e}\\left[N^{e}\\left(t r((\\tilde{A}^{e^{k}})^{\\top}\\tilde{A}^{e^{k}})+N^{e}(1+\\sigma^{2})\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. We will use some symbols to simplify the expression of the toy GNN. Denote $n_{2}+\\epsilon^{e}$ as $\\eta$ . Use the following notations to represent the components of the $L$ -layer GNN model: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\bar{e}}(A,X)=H_{1}^{(L)}\\theta_{1}+H_{2}^{(L)}\\theta_{2}}\\\\ &{\\quad\\quad\\quad=\\underbrace{\\Big[\\theta_{1}^{1(L-1)}\\bar{A}\\left(\\ldots\\theta_{1}^{1(3)}\\left(\\theta_{1}^{1(2)}\\bar{A}(\\theta_{1}^{1(1)}\\bar{A}+\\theta_{1}^{2(1)}\\bar{I})X_{1}+\\theta_{1}^{2(2)}(\\theta_{1}^{1(1)}\\bar{A}+\\theta_{1}^{2(1)}\\bar{I})X_{1}\\right)+\\ldots\\right)_{\\bar{C}_{1}}\\Big]}_{C_{1}}+\\underbrace{\\theta_{1}^{2(3)}(\\theta_{1}^{1(3)}\\bar{A}+\\theta_{1}^{2(1)}\\bar{I})X_{1}}_{=:\\ \\ \\big[\\theta_{2}^{1(2)}\\bar{A}\\left(\\ldots\\theta_{2}^{1(3)}\\left(\\theta_{2}^{1(3)}\\bar{A}(\\theta_{2}^{1(1)}\\bar{A}+\\theta_{2}^{2(1)}\\bar{I})\\eta+\\theta_{2}^{2(2)}(\\theta_{2}^{1(1)}\\bar{A}+\\theta_{2}^{2(1)}\\bar{I})\\eta\\right)+\\ldots\\right)\\Big]}_{\\mathcal{Z}}\\theta_{2}}\\\\ &{\\quad\\quad\\quad=C_{1}\\theta_{1}+Z\\theta_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "$C_{1},Z\\in\\mathbb{R}^{N\\times1}$ . We u se $C_{1}^{e}$ \u2032and $Z^{e}$ to denote the variables from the corresponding environment $e$ .   \nWe further denote $Z^{e}=C_{2}^{\\bar{e}\\prime}\\eta$ . ", "page_idx": 39}, {"type": "text", "text": "Using these notations, the loss of environment $e$ is ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{R}(e)=\\mathbb{E}_{n_{1},n_{2}}\\left[\\|f_{\\Theta}(A^{e},X^{e})-Y^{e}\\|_{2}^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}_{n_{1},n_{2}}\\left[\\left\\|{C_{1}^{e}\\theta_{1}+Z^{e}\\theta_{2}-\\tilde{A}^{e}}^{k}X_{1}-n_{1}\\right\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Denote the inner term $C_{1}^{e}\\theta_{1}+Z^{e}\\theta_{2}-\\tilde{A}^{e}{}^{k}X_{1}-n_{1}$ as $l_{e}$ . The variance of loss across environments is: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{e}[R(e)]=\\mathbb{E}_{e}[R^{2}(e)]-\\mathbb{E}_{e}^{2}[R(e)]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{e}\\left[\\left(\\mathbb{E}_{n_{1},n_{2}}\\left\\|C_{1}^{e}\\theta_{1}+Z^{e}\\theta_{2}-\\tilde{A}^{e}^{k}X_{1}-n_{1}\\right\\|_{2}^{2}\\right)^{2}\\right]}\\\\ &{\\quad\\quad\\quad-\\mathbb{E}_{e}^{2}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left\\|C_{1}^{e}\\theta_{1}+Z^{e}\\theta_{2}-\\tilde{A}^{e}^{k}X_{1}-n_{1}\\right\\|_{2}^{2}\\right].}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[(l_{e}^{\\top}l_{e})^{2}\\right]\\right]-\\mathbb{E}_{e}^{2}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Take the derivative of $\\mathbb{V}_{e}[R(e)]$ with respect to $\\theta_{1}$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}}=\\mathbb{E}_{e}\\left[2\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}C_{1}^{e}\\right]\\right]}\\\\ &{\\qquad\\qquad-\\ 2\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\right]\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}C_{1}^{e}\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Calculate the derivative by terms: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}l_{e}]=\\mathbb{E}_{n_{1},n_{2}}[C_{1}^{e^{\\top}}C_{1}^{e}(\\theta_{1})^{2}+{C_{1}^{e^{\\top}}}Z^{e}\\theta_{1}\\theta_{2}-{C_{1}^{e^{\\top}}}(\\tilde{A^{e}})^{k}X_{1}\\theta_{1}-{C_{1}^{e}}^{\\top}n_{1}\\theta_{1}}\\\\ &{\\qquad\\qquad\\qquad+\\,Z^{e^{\\top}}C_{1}^{e}\\theta_{1}\\theta_{2}+{Z^{e}}^{\\top}Z^{e}(\\theta_{2})^{2}-{Z^{e}}^{\\top}(\\tilde{A^{e}})^{k}X_{1}\\theta_{2}-{Z^{e}}^{\\top}n_{1}\\theta_{2}}\\\\ &{\\qquad\\qquad\\qquad-\\,((\\tilde{A^{e}})^{k}X_{1})^{\\top}C_{1}^{e}\\theta_{1}-((\\tilde{A^{e}})^{k}X_{1})^{\\top}Z^{e}\\theta_{2}+((\\tilde{A^{e}})^{k}X_{1})^{\\top}(\\tilde{A^{e}})^{k}X_{1}}\\\\ &{\\qquad\\qquad\\qquad+\\,((\\tilde{A^{e}})^{k}X_{1})^{\\top}n_{1}-n_{1}^{\\top}C_{1}^{e}\\theta_{1}-n_{1}^{\\top}Z^{e}\\theta_{2}+n_{1}^{\\top}(\\tilde{A^{e}})^{k}X_{1}+n_{1}^{\\top}n_{1}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Since $n_{1}$ and $n_{2}$ are independent standard Gaussian noise, we have $\\mathbb{E}_{n_{1},n_{2}}[n_{1}]=\\mathbb{E}_{n_{1},n_{2}}[n_{2}]=\\mathbf{0}$ , $\\mathbb{E}_{n_{1},n_{2}}[n_{1}^{\\top}n_{2}]=\\mathbb{E}_{n_{1},n_{2}}[n_{2}^{\\top}n_{1}]=0$ and $\\mathbb{E}_{n_{1},n_{2}}[n_{1}^{\\top}n_{1}]=\\mathbb{E}_{n_{1},n_{2}}[n_{2}^{\\top}n_{2}]=N^{e}$ if it is the noise from $e$ . Also, since $\\epsilon^{e}$ and $n_{1},n_{2}$ are independent, we have $\\mathbb{E}_{n_{1},n_{2}}[n_{1}^{\\top}\\epsilon^{e}]=\\mathbb{E}_{n_{1},n_{2}}[n_{2}^{\\top}\\epsilon^{e}]=0$ . ", "page_idx": 40}, {"type": "text", "text": "When ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-1,...,L-s+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-s,L-s-1,...,1}\\\\ {\\theta_{2}^{1}{}^{(l)}=0,\\theta_{2}^{2}{}^{(l)}=1,\\quad l=L-1,...,1}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "we have $C_{2}^{e^{\\prime}}=I_{N^{e}}\\,\\in\\,\\mathbb{R}^{N^{e}\\times N^{e}}$ and $C_{1}^{e}\\,=\\,\\tilde{A}^{e}\\,^{s}X_{1}$ . Consequently, we get $\\mathbb{E}_{n_{1},n_{2}}[Z^{e^{\\top}}n_{1}]\\,=\\,0$ , $\\mathbb{E}_{n_{1},n_{2}}[Z^{e^{\\top}}Z^{e}]=N^{e}+\\epsilon^{e^{\\top}}\\epsilon^{e}$ . ", "page_idx": 40}, {"type": "text", "text": "Use the above conclusions and rewrite Equation (82) as (here we only plug in the value of $C_{2}^{e^{\\prime}}$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}l_{e}]=}\\\\ &{~C_{1}^{e^{\\top}}C_{1}^{e}(\\theta_{1})^{2}-C_{1}^{e^{\\top}}(\\tilde{A^{e}})^{k}X_{1}\\theta_{1}}\\\\ &{~-((\\tilde{A^{e}})^{k}X_{1})^{\\top}C_{1}^{e}\\theta_{1}+((\\tilde{A^{e}})^{k}X_{1})^{\\top}(\\tilde{A^{e}})^{k}X_{1}+N^{e}\\left(1+(\\theta_{2})^{2}\\right)\\right\\}(*)}\\\\ &{~+[C_{1}^{e^{\\top}}\\epsilon^{e}+\\epsilon^{e^{\\top}}C_{1}^{e}]\\theta_{1}\\theta_{2}+\\epsilon^{e^{\\top}}\\epsilon^{e}(\\theta_{2})^{2}-2((\\tilde{A^{e}})^{k}X_{1})^{\\top}\\epsilon^{e}\\theta_{2}\\right\\}(**),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "$(*)$ and $(**)$ represent terms that are independent and associated with $\\epsilon^{e}$ , respectively. Additionally, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{n_{1},n_{2}}[2l_{e}^{\\top}C_{1}^{e}]=2\\left[C_{1}^{e\\top}C_{1}^{e}\\theta_{1}+\\epsilon^{e\\top}C_{1}^{e}\\theta_{2}-((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{1}^{e}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Multiplying Equation (84) and (85) and take the expectation on $e$ , using the assumption that $\\mathbb{E}_{e}[(\\epsilon^{\\stackrel{\\bullet}{e}}{_{i}})^{2}]\\stackrel{\\smile}{=}\\sigma^{\\dot{2}}$ $\\boldsymbol{\\epsilon}^{e}{}_{i}$ is the $i$ -th element of $\\epsilon^{e}$ ): ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{z}}_{e}\\left[2\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}l_{e}]\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}C_{1}\\right]\\right]=4\\mathbb{E}_{e}\\left[(*)\\left(C_{1}^{e^{\\top}}C_{1}^{e}\\theta_{1}-((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{1}^{e}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,4\\mathbb{E}_{e}\\left[(\\tilde{A}^{e^{s}}X_{1})^{\\top}\\tilde{A}^{e^{s}}X_{1}(2\\theta_{1}\\theta_{2}+(\\theta_{2})^{2})-2(\\tilde{A}^{e^{s}}X_{1})^{\\top}((\\tilde{A}^{e})^{k}X_{1})^{\\top}(\\theta_{2}-\\theta_{1})^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,4\\mathbb{E}_{e}[N^{e}\\epsilon^{e^{\\top}}\\epsilon^{e}\\epsilon^{e^{\\top}}(\\tilde{A}^{e^{s}}X_{1})]\\theta_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Next target is to compute $2\\mathbb{E}_{e}[\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}l_{e}]]$ and $\\mathbb{E}_{e}[\\mathbb{E}_{n_{1},n_{2}}[2l_{e}^{\\top}C_{1}]]$ Since $\\epsilon^{e}$ has zero mean, we have: ", "page_idx": 41}, {"type": "equation", "text": "$$\n2\\mathbb{E}_{e}[\\mathbb{E}_{n_{1},n_{2}}[l_{e}^{\\top}l_{e}]]=\\mathbb{E}[(*)]+\\mathbb{E}[2N^{e}]\\sigma^{2}(\\theta_{2})^{2}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{e}[\\mathbb{E}_{n_{1},n_{2}}[2l_{e}^{\\top}C_{1}^{e}]]=2\\mathbb{E}_{e}\\left[C_{1}^{e^{\\top}}C_{1}^{e}\\theta_{1}-((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{1}^{e}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Use Equation (86) (87) and (88) and let $\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}}=0$ , we have: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{e}\\left[2(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e^{s}}X_{1})(\\theta_{1}\\theta_{2}+\\frac{1}{2}(\\theta_{2})^{2})-2(\\tilde{A}^{e^{s}}X_{1})^{\\top}((\\tilde{A}^{e})^{k}X_{1})\\theta_{2}\\right]\\sigma^{2}+\\mathbb{E}_{e}[\\epsilon^{e^{\\top}}\\epsilon^{e}\\epsilon^{e^{\\top}}(\\tilde{A}^{e^{s}}X_{1})\\theta_{2}+\\epsilon^{e}\\epsilon^{e}]\\mathbb{E}_{e}[\\epsilon^{e}\\theta^{2}]\\mathbb{E}_{e}[\\epsilon^{e}\\theta^{2}]\\mathbb{E}_{e}[\\epsilon^{e}\\theta^{2}]\\mathbb{E}_{e}[\\epsilon^{e}\\theta^{2}]\\mathbb{E}_{e}[\\epsilon^{e}\\theta^{2}]\\mathbb{E}_{e}[\\epsilon^{e}\\theta^{2}]\\mathbb{E}_{e}[\\epsilon^{e}\\theta^{2}]\\mathbb{E}_{e}[\\epsilon^{e}\\theta^{2}]\\Big]}\\\\ &{-\\mathbb{E}_{e}[N^{e}]\\mathbb{E}_{e}\\left[(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e^{s}}X_{1})\\theta_{1}-((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{1}^{e}\\right]\\theta_{2}\\sigma^{2}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Now we start calculating the expression of \u2202Ve\u2202[\u03b8R(e)]: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathbb{V}_{e}\\left[R\\left(e\\right)\\right]}{\\partial\\theta_{2}}=\\mathbb{E}_{e}\\left[2\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}Z^{e}\\right]\\right]}\\\\ &{\\qquad\\qquad-\\,2\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[l_{e}^{\\top}l_{e}\\right]\\right]\\mathbb{E}_{e}\\left[\\mathbb{E}_{n_{1},n_{2}}\\left[2l_{e}^{\\top}Z^{e}\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Let $\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{2}}=0$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{z}_{e}\\left[(C_{1}^{e^{\\top}}C_{2}^{e^{\\prime}}+C_{2}^{e^{\\prime}}^{\\top}C_{1}^{e^{\\top}})\\theta_{1}\\theta_{2}+(C_{2}^{e^{\\prime}})^{\\top}C_{2}^{e^{\\prime}}(\\theta_{2})^{2}-2((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{2}^{e^{\\prime}}\\theta_{2}\\right]\\mathbb{E}_{e}\\left[(-((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{2}^{e^{\\prime}})\\right.}\\\\ &{-\\left.\\mathbb{E}_{e}\\left[N^{e}\\sigma^{2}\\left(\\operatorname{tr}((\\tilde{A}^{e})^{\\top}\\tilde{A}^{e^{k}})+N^{e}+C_{2}^{e^{\\prime}\\top}C_{2}^{e^{\\prime}}\\sigma^{2}\\right)(\\theta_{2})^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "$=0$ ", "page_idx": 41}, {"type": "text", "text": "Plug Equation (89) in (91), we reach: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\mathbb{E}_{e}\\left[N^{e}(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e^{s}}X_{1})\\theta_{1}-N^{e}((\\tilde{A}^{e})^{k}X_{1})^{\\top}C_{1}^{e}\\right]\\theta_{2}\\sigma^{2}-\\mathbb{E}_{e}[\\epsilon^{e^{\\top}}\\epsilon^{e}\\epsilon^{e^{\\top}}(\\tilde{A}^{e^{s}}X_{1})]\\theta_{2}\\right]\\mathbb{E}_{e}\\left(-(\\tilde{A}^{e^{k}}X_{1})^{\\top}\\theta_{1}\\right)}\\\\ &{-\\mathbb{E}_{e}\\left[N^{e}\\left(\\mathrm{tr}((\\tilde{A}^{e^{k}})^{\\top}\\tilde{A}^{e^{k}})+N^{e}(1+\\sigma^{2})\\right)\\right](\\theta_{2})^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "$=0$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~et~}c_{1}\\,=\\,\\mathbb{E}[(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e^{s}}X_{1})],\\,c_{2}\\,=\\,\\mathbb{E}[(\\tilde{A}^{e^{s}}X_{1})^{\\top}(\\tilde{A}^{e^{k}}X_{1})],\\,c_{3}\\,=\\,\\mathbb{E}_{e}[\\epsilon^{e^{\\top}}\\epsilon^{e}\\epsilon^{e^{\\top}}(\\tilde{A}^{e^{s}}X_{1})]\\sigma^{2},}\\\\ &{\\,+=\\mathbb{E}_{e}\\left[((\\tilde{A}^{e})^{k}X_{1})^{\\top}\\mathbf{1}_{\\mathrm{N}^{e}}\\right]\\sigma^{2},\\,c_{5}=\\mathbb{E}_{e}\\left[N^{e}\\left(\\mathrm{tr}((\\tilde{A}^{e^{k}})^{\\top}\\tilde{A}^{e^{k}})+N^{e}(1+\\sigma^{2})\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "we conclude that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{c_{1}\\sigma^{2}(2\\theta_{1}\\theta_{2}+(\\theta_{2})^{2}-2c_{2}\\sigma^{2}\\theta_{2})+c_{3}-\\mathbb{E}_{e}[N^{e}]c_{1}\\sigma^{2}\\theta_{1}\\theta_{2}+\\mathbb{E}_{e}[N^{e}]c_{2}\\sigma^{2}\\theta_{2}=0}\\\\ {\\left[c_{3}\\theta_{2}-\\mathbb{E}_{e}[N^{e}]c_{1}\\sigma^{2}\\theta_{1}\\theta_{2}+\\mathbb{E}_{e}[N^{e}]c_{2}\\sigma^{2}\\theta_{2}\\right]c_{4}-c_{5}(\\theta_{2})^{2}=0}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "As for the derivative respect to $\\theta_{1}^{1}{}^{(l)},\\theta_{1}^{2}{}^{(l)},\\theta_{2}^{1}{}^{(l)},\\theta_{2}^{2}{}^{(l)}.$ , when they take the special value in Equation (83), we have $\\begin{array}{r}{\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}}\\,=\\,0\\,\\Rightarrow\\,\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}^{1(l)}}\\,=\\,\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{1}^{2(l)}}\\,=\\,0}\\end{array}$ )] = 0 and \u2202Ve[R(e)] $\\begin{array}{r}{\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{2}}\\,=\\,0\\,\\Rightarrow\\,\\frac{\\partial\\mathbb{V}_{e}[R(e)]}{\\partial\\theta_{2}^{1(l)}}\\,=\\,}\\end{array}$ \u2202V\u2202e\u03b8[2R(l()e)] = 0, l = 1, ..., L. So we conclude the solution induced by Equation (93) is the solution of the objective, and $\\theta_{2}=0$ is not a valid solution. ", "page_idx": 41}, {"type": "text", "text": "G.2.3 Proof of the Failure Case on Graphs of IRMv1 under Covariate Shift ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We restate Theorem B.3 as Theorem G.7 below: ", "page_idx": 42}, {"type": "text", "text": "Theorem G.7. (IRMv1 will use spurious features on graphs under covariate shift) Under the SCM of Equation (19), there exists $s\\,\\in\\,\\mathbb{N}^{+}$ that satisfies $0\\,<\\,s\\,<\\,L$ and $s\\neq k$ such that optimizing the IRMv1 objective $\\mathrm{min}_{\\Theta}\\mathbb{E}_{e}[||\\nabla_{w|w=1.0}R(e)||^{2}]$ will not lead to the invariant solution $\\theta_{2}=0$ for parameters of the GNN (3) when $\\{\\theta_{1}^{1}{}^{(l)},\\theta_{1}^{2}{}^{(l)},\\theta_{2}^{1}{}^{(l)},\\theta_{2}^{2}{}^{(l)}\\}$ take the special solution: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\Theta_{0}=\\left\\{\\begin{array}{l l}{\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,}&{l=L-1,...,L-s+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,}&{l=L-s,L-s-1,...,1}\\\\ {\\theta_{2}^{1}{}^{(l)}=0,\\theta_{2}^{2}{}^{(l)}=1,}&{l=L-1,...,1}\\end{array}\\right.\\,.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. From the proof of non-graph IRMv1 case Appendix G.1.1 we know that when the IRMv1 objective is optimized, we have $\\bar{\\nabla}_{w}R(e)\\,=\\,0$ for all $e$ . For the graph case, the expected risk of environment $e$ is ", "page_idx": 42}, {"type": "equation", "text": "$$\nR(e)=\\mathbb{E}_{n_{1},n_{2}}[\\|\\theta_{1}C_{1}^{e}+\\theta_{2}Z^{e}-\\tilde{A}_{e}^{k}X_{1}-n_{1}\\|_{2}^{2}],\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the definition of $C_{1}^{e}$ and $Z^{e}$ follows Equation (78). Now let\u2019s check if the invariant solution $\\theta_{2}=0$ is a valid solution. Let $\\theta_{2}=0$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{w}R(e)=\\mathbb{E}_{n_{1},n_{2}}[(\\theta_{1}C_{1}^{e}+\\theta_{2}Z^{e}-(\\tilde{A}^{e})^{k}X_{1}-n_{1})^{\\top}(\\theta_{1}C_{1}^{e}+\\theta_{2}Z^{e})]}\\\\ &{\\qquad\\qquad=(\\theta_{1})^{2}((\\tilde{A}^{e})^{s}X_{1})^{\\top}((\\tilde{A}^{e})^{s}X_{1})+(\\theta_{2})^{2}[N^{e}+(\\epsilon^{e})^{\\top}\\epsilon^{e}]+2\\theta_{1}\\theta_{2}((\\tilde{A}^{e})^{s}X_{1})^{\\top}\\epsilon}\\\\ &{\\qquad\\qquad-\\theta_{1}((\\tilde{A}^{e})^{s}X_{1})^{\\top}((\\tilde{A}^{e})^{k}X_{1})-\\theta_{2}(N^{e}+(\\epsilon^{e})^{\\top})((\\tilde{A}^{e})^{k}X_{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "If $\\theta_{2}=0$ is a feasible solution, then the following equation must hold for every environment $e$ : ", "page_idx": 42}, {"type": "equation", "text": "$$\n(\\theta_{1})^{2}((\\tilde{A}^{e})^{s}X_{1})^{\\top}((\\tilde{A}^{e})^{s}X_{1})-\\theta_{1}((\\tilde{A}^{e})^{s}X_{1})^{\\top}((\\tilde{A}^{e})^{k}X_{1})=0.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "When $s\\neq k$ , we get $\\begin{array}{r}{\\theta_{1}=\\frac{((\\tilde{A}^{e})^{s}X_{1})^{\\top}((\\tilde{A}^{e})^{s}X_{1})}{((\\tilde{A^{e}})^{s}X_{1})^{\\top}((\\tilde{A^{e}})^{k}X_{1})}}\\end{array}$ (we discard the trivial solution of $\\theta_{1}=0$ ). The value of this solution of $\\theta_{1}$ varies with environment $e$ , and thus is not a valid solution. However, now we will show that optimizing IRMv1 does not necessarily lead to lower-layer parameters such that $s=k$ To reveal this, by taking the derivative of $\\mathcal{L}_{\\mathrm{IRMv1}}$ w.r.t. $\\theta_{1}$ and $\\theta_{2}$ and let them $=0$ , we can get two cubic equations: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{1}}=\\!\\mathbb{E}_{e}[(\\boldsymbol{C}_{1}^{e}^{\\top}\\boldsymbol{C}_{1}^{e}(\\theta_{1})^{2}+(\\boldsymbol{N}^{e}+\\boldsymbol{\\epsilon}^{\\top}\\boldsymbol{\\epsilon})(\\theta_{2})^{2}+\\boldsymbol{C}_{1}^{e}{\\top}\\boldsymbol{\\epsilon}^{e}\\theta_{1}\\theta_{2}-(\\tilde{\\boldsymbol{A}}^{e}^{k}\\boldsymbol{X}_{1})^{\\top}\\boldsymbol{C}_{1}^{e}\\theta_{1})}\\\\ &{\\qquad\\qquad\\quad(2\\boldsymbol{C}_{1}^{e}\\top\\boldsymbol{C}_{1}^{e}\\top\\theta_{1}+\\boldsymbol{C}_{1}^{e}\\top\\boldsymbol{\\epsilon}\\theta_{2}-(\\tilde{\\boldsymbol{A}}^{e}^{k}\\boldsymbol{X}_{1})^{\\top}\\boldsymbol{C}_{1}^{e})]=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}}=\\!\\mathbb{E}_{e}[(\\boldsymbol{C}_{1}^{e^{\\top}}\\boldsymbol{C}_{1}^{e}(\\theta_{1})^{2}+(\\boldsymbol{N}^{e}+\\boldsymbol{\\epsilon}^{\\top}\\boldsymbol{\\epsilon})(\\theta_{2})^{2}+\\boldsymbol{C}_{1}^{e^{\\top}}\\boldsymbol{\\epsilon}^{e}\\theta_{1}\\theta_{2}-(\\tilde{\\boldsymbol{A}}^{e^{k}}\\boldsymbol{X}_{1})^{\\top}\\boldsymbol{C}_{1}^{e}\\theta_{1})}\\\\ &{\\qquad\\qquad\\quad(2(\\boldsymbol{N}^{e}+\\boldsymbol{\\epsilon}^{e^{\\top}}\\boldsymbol{\\epsilon}^{e})\\theta_{2}+(\\boldsymbol{C}_{1}^{e^{\\top}}\\boldsymbol{C}_{2}^{e}+\\boldsymbol{C}_{1}^{e^{\\top}}\\boldsymbol{\\epsilon})\\theta_{1})]=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "From the analysis in Appendix F.3, we know that as long as the lower-layer parameters take any value that satisfies the form in Equation (27), even if $s\\neq k$ , we can get $\\begin{array}{r}{\\overline{{\\partial\\ell_{\\mathrm{IRMv1}}}}^{\\,\\^{\\prime}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{1}^{1}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{1}^{2}}}\\end{array}$ and \u2202LIRMv1 $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}^{1}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}^{2}}}\\end{array}$ \u2202LIR2Mv1. At this time (when $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMvl}}}{\\partial\\theta_{1}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMvl}}}{\\partial\\theta_{1}^{1}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMvl}}}{\\partial\\theta_{1}^{2}}=0}\\end{array}$ and $\\begin{array}{r l}{\\lefteqn{\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}}=}}\\end{array}$ $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}^{1}}=\\frac{\\partial\\mathcal{L}_{\\mathrm{IRMv1}}}{\\partial\\theta_{2}^{2}}=0}\\end{array}$ \u2202L\u2202I\u03b8R2Mv1 = 0 but s \u0338= k), from the form of Equations (98) and (99) we know that there exist solutions that $\\theta_{2}\\neq0$ , and the solution of $\\theta_{1}$ and $\\theta_{2}$ both depend on $\\mathbb{E}_{e}(F(e))$ , where $F(e)$ is some random variable associated with $e$ . ", "page_idx": 42}, {"type": "text", "text": "G.2.4 Proof of the Successful Case on Graphs of CIA under Covariate Shift ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Theorem G.8. Optimizing the CIA objective will lead to the optimal solution $\\Theta^{*}$ : ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\theta_{1}=1}\\\\ {\\theta_{2}=0}\\\\ {\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-1,...,L-k+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-k,L-k-1,...,1}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. For brevity, denote a node representation of $C_{1c}^{e}$ as $C_{1}^{i}$ and the one of $C_{1\\,\\,\\,c}^{e^{\\prime}}$ as $C_{1}^{j}$ . The same is true for $C_{2}^{i}$ and $C_{2}^{j}$ . In this toy model, we need to consider the expectation of the noise, while in real cases such noise is included in the node features so taking expectation on $e$ will handle this. Therefore, we add $\\mathbb{E}_{n_{1},n_{2}}$ in this proof, and this expectation is excluded in the formal description of the objective in the main paper. ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{CIA}}c=\\mathbb{E}_{\\underset{e\\neq e^{\\prime}}{\\sum}}\\mathbb{E}_{n_{1},n_{2}}\\mathbb{E}_{c}\\mathbb{E}_{\\underset{(i,j)\\in\\Omega^{e,e^{\\prime}}}{\\sum}}\\left[\\mathcal{D}(\\phi_{\\Theta}(A^{e},X^{e})_{[c][v_{i}]},\\phi_{\\Theta}(A^{e},X^{e^{\\prime}})_{[c][v_{j}]})\\right]}\\\\ &{\\qquad=\\mathbb{E}_{\\underset{e\\neq e^{\\prime}}{\\sum}}\\mathbb{E}_{n_{1},n_{2}}\\mathbb{E}_{c}\\mathbb{E}_{\\underset{(i,j)\\in\\Omega^{e,e^{\\prime}}}{i,j}}\\lVert C_{1}^{i}+Z^{e}-C_{1}^{j}-Z^{e^{\\prime}}\\rVert_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{1}}=\\mathbb{E}_{\\underset{e\\neq e^{\\prime}}{e,e^{\\prime}}}\\mathbb{E}_{n_{1},n_{2}}\\mathbb{E}_{\\underset{(i,j)\\in\\Omega^{e,e^{\\prime}}}{\\mathbb{E}}}\\left[C_{1}^{i}\\theta_{1}+Z^{e}\\theta_{2}-C_{1}^{j}\\theta_{1}-Z^{e^{\\prime}}\\theta_{2}\\right]^{\\top}\\left(C_{1}^{i}-C_{1}^{j}\\right)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Let $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{1}}=0}\\end{array}$ , we have: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{\\phi}_{e,e^{\\prime}}}\\mathbb{E}_{c}\\mathbb{E}_{\\mathbf{\\phi}_{(i,j)\\in\\Omega^{e,e^{\\prime}}}}\\left[(C_{1}^{i}-C_{1}^{j})^{\\top}(C_{1}^{i}-C_{1}^{j})\\theta_{1}\\right]=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thus, we get two possible solutions of the invariant branch. The first valid solution is the optimal one: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\theta_{1}=1}\\\\ {\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-1,...,L-k+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-k,L-k-1,...,1}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The second valid solution is a trivial one: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\theta_{1}=0\\quad\\mathrm{or}\\quad\\exists l\\in\\{1,...,L-1\\}\\;\\mathrm{s.t.}\\;\\theta_{1}^{1}{}^{(l)}=\\theta_{1}^{2}{}^{(l)}=0\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Take the derivative of the objective w.r.t. $\\theta_{2}$ : ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{2}}=\\mathbb{E}_{\\underset{e\\neq e^{\\prime}}{e,e^{\\prime}}}\\mathbb{E}_{c}\\mathbb{E}_{\\underset{(i,j)\\in\\Omega^{e,e^{\\prime}}}{i,j}}\\left[\\left[(Z^{e}-Z^{e^{\\prime}})^{\\top}(Z^{e}-Z^{e^{\\prime}})\\right]\\theta_{2}\\right]=2\\sigma^{2}\\theta_{2}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Let $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{2}}=0}\\end{array}$ , we get $\\theta_{2}=0$ . Thus, CIA will remove spurious features. ", "page_idx": 43}, {"type": "text", "text": "Now we show that when CIA objective has been reached (the spurious branch has zero outputs), the objective of min\u0398 $\\mathbb{E}_{e}[\\mathcal{L}(f_{\\Theta}(A^{e},X^{e}),Y^{e})]$ will help to learn predictive parameters of the invariant branch $\\theta_{1},\\theta_{1}^{1}(\\l_{l})$ and $\\theta_{1}^{2}{}^{(}l$ ). When $\\theta_{2}=0$ : ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathbb{E}_{e}\\left[\\mathcal{L}(f_{\\Theta}(A^{e},X^{e}),Y^{e})\\right]}{\\partial\\theta_{1}}=2\\mathbb{E}_{e}\\mathbb{E}_{n_{1},n_{2}}\\left[\\left(C_{1}^{e}\\theta_{1}-(\\tilde{A}^{e})^{k}X_{1}-n_{1}\\right)^{\\top}C_{1}^{e}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2\\mathbb{E}_{e}\\left[\\left(C_{1}^{e}\\theta_{1}-(\\tilde{A}^{e})^{k}X_{1}\\right)^{\\top}C_{1}^{e}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Let $\\begin{array}{r}{\\frac{\\partial\\mathbb{E}_{e}[\\mathcal{L}(f_{\\Theta}(A^{e},X^{e}),Y^{e})]}{\\partial\\theta_{1}}=0.}\\end{array}$ , we get the predictive parameters ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\theta_{1}=1}\\\\ {\\theta_{1}^{1}{}^{(l)}=1,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-1,...,L-k+1}\\\\ {\\theta_{1}^{1}{}^{(l)}=0,\\theta_{1}^{2}{}^{(l)}=1,\\quad l=L-k,L-k-1,...,1}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Plug the final solution back in $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{1}^{1}(l)},\\quad\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{1}^{2}(l)},\\quad\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{2}^{1}(l)},\\quad\\frac{\\partial\\mathcal{L}_{\\mathrm{CIA}}}{\\partial\\theta_{2}^{2}(l)},\\quad\\frac{\\partial\\mathbb{E}_{e}\\left[\\mathcal{L}\\left(f_{\\Theta}(A^{e},X^{e}),Y^{e}\\right)\\right]}{\\partial\\theta_{1}^{1(l)}},}\\end{array}$ $\\frac{\\partial\\mathbb{E}_{e}[\\mathcal{L}(f_{\\Theta}(A^{e},X^{e}),Y^{e})]}{\\partial\\theta_{1}^{2(l)}}$ $\\frac{\\partial\\mathbb{E}_{e}[\\mathcal{L}(f_{\\Theta}(A^{e},X^{e}),Y^{e})]}{\\partial\\theta_{2}^{1}{}^{(l)}}$ $\\frac{\\partial\\mathbb{E}_{e}[\\mathcal{L}(f_{\\Theta}(A^{e},X^{e}),Y^{e})]}{\\partial\\theta_{2}^{2(l)}}$ we can verify that these terms are all 0. ", "page_idx": 43}, {"type": "text", "text": "G.3 Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "The following proof of Theorem 4.4 is adapted from Ma et al. [2021] and Mao et al. [2023]. We restate Theorem 4.4 as Theorem G.9 below. ", "page_idx": 44}, {"type": "text", "text": "Theorem G.9. (Subgroup Generalization Bound for GNNs). Let \u02dch be any classifier in a function family $\\mathcal{H}$ with parameters $\\left\\{\\widetilde{W}_{l}\\right\\}_{l=1}^{L}$ . Under Assumption 4.2 and 4.3, for any $e^{t e}\\in\\mathcal{E}_{t e},\\gamma\\ge0,$ , and large enough $N_{e^{t r}}$ , with probability at least $1-\\delta$ , we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{e e^{(\\bar{h})}}^{0}(\\bar{h})\\leq\\hat{\\mathcal{L}}_{e e^{(\\bar{h})}}^{\\gamma}(\\bar{h})}\\\\ &{\\qquad\\qquad+O(\\underbrace{1}_{Q^{2}}\\underbrace{\\sum_{c=1}^{C}C\\sqrt{|[(\\mu_{c}-\\mu_{c^{\\prime}})^{\\top};\\left(\\mu_{c}^{e^{\\mu}}-\\mu_{c^{\\prime}}^{e^{\\mu}}\\right)^{\\top}]|}}_{(a)}+2\\sqrt{2})\\epsilon\\epsilon_{c^{\\mu},e^{\\eta}}}\\\\ &{\\qquad\\qquad+\\underbrace{2\\sum_{c=1}^{C}(C-1)B_{e^{\\epsilon}}|\\mu_{c}^{e^{\\mu}}-\\mu_{c}^{e^{\\eta}}|}_{(b)}}\\\\ &{\\qquad\\qquad+\\underbrace{\\frac{1}{2\\sigma^{2}}\\sum_{c=1}^{\\infty}\\sum_{c\\in\\mathcal{V}_{e^{\\epsilon}}}\\sum_{\\bar{h}\\in\\mathcal{V}_{e^{\\epsilon}}}\\sum_{j\\in\\mathcal{V}_{e^{(\\bar{h})}}}\\sum_{c=1}^{C}\\sum_{c^{\\prime}\\not=e^{\\epsilon}}|p_{j}^{\\mu}(e^{\\prime}|c)-p_{i}^{\\mu}(c^{\\prime}|c)|}_{(c)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n+\\underbrace{\\frac{b\\sum_{l=1}^{L}\\left\\|\\widetilde{W}_{l}\\right\\|_{F}^{2}}{(\\gamma/8)^{2/L}N_{e^{t r}}^{\\alpha}}\\left(\\epsilon_{e^{t e},e^{t r}}\\right)^{2/L}}_{(d)}+c o n s t)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $p_{i}^{h t}(c^{\\prime}|c)$ is the ratio of heterophilic neighbors of class $c^{\\prime}$ when $y_{i}=c,$ , $\\begin{array}{r}{c o n s t\\,=\\,\\frac{1}{N^{1-2\\alpha}}\\,+}\\end{array}$ $\\begin{array}{r}{\\frac{1}{N_{e^{t r}}^{2\\alpha}}\\ln{\\frac{L C(2B_{e^{t e}})^{1/L}}{\\gamma^{1/L}\\delta}}}\\end{array}$ is a term independent of aggregated feature distance and the difference in neighboring heterophilic label distribution, where $B_{e^{t e}}\\,=\\,\\operatorname*{max}_{i\\in V_{e^{t r}}\\cup V_{e^{t e}}}\\,\\|g_{i}\\|_{2}$ is the maximum feature norm. ", "page_idx": 44}, {"type": "text", "text": "To prove Theorem G.9, we need the following lemma that bounds the expected loss discrepancy between the train and the test node subgroups. ", "page_idx": 44}, {"type": "text", "text": "Lemma G.10. Given a distribution $P$ over $\\mathcal{H}$ , for $\\lambda\\,>\\,0,$ , define the expected loss discrepancy between $V_{e}$ and $V_{e^{\\prime}}$ with respect to $(P,\\gamma,\\lambda)$ as $\\bar{D}_{e,e^{\\prime}}^{\\gamma}(P;\\lambda):=\\ln\\mathbb{E}_{h\\sim P}e^{\\bar{\\lambda}\\left(\\mathcal{L}_{e}^{\\gamma/2}(h)-\\mathcal{L}_{e^{\\prime}}^{\\gamma}(h)\\right)}$ . Under Assumption 4.2, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle?_{e^{\\kappa},e^{\\kappa}}^{\\gamma/2}\\le\\frac{1}{\\sigma^{2}}(\\sum_{c=1}^{C}\\sum_{c^{\\prime}\\neq c}(\\sqrt{|[(\\mu_{c}-\\mu_{c^{\\prime}})^{\\top};\\left(\\mu_{c}^{e^{\\kappa}}-\\mu_{c^{\\prime}}^{e^{\\kappa}}\\right)^{\\top}]|}+2\\sqrt{2})\\epsilon_{e^{\\kappa},e^{\\kappa}}+2\\sum_{c=1}^{C}(C-1)B_{e^{\\kappa}}|\\mu_{c}^{e^{\\kappa}}-\\mu_{c}^{e^{\\kappa}}|)}\\\\ &{\\displaystyle\\qquad+\\frac{1}{2\\sigma^{2}}\\frac{1}{N_{e^{\\kappa}}}\\sum_{i\\in V_{e^{\\kappa}}}\\frac{1}{N_{e^{\\kappa}}}\\sum_{j\\in V_{e^{\\kappa}}^{(i)}}\\sum_{c^{\\prime}\\neq c}\\sum_{c^{\\prime}\\neq c}|p_{j}^{h}(c^{\\prime}|c)-p_{i}^{h t}(c^{\\prime}|c)|}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. According to Eq. (14) of Lemma 5 in Ma et al. [2021], under 4.2, we already have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\mathcal{L}_{e^{\\alpha}}^{\\mathrm{u}}(h)-\\mathcal{L}_{e^{\\alpha}}^{\\mathrm{r}}(h)}\\\\ &{\\le\\displaystyle\\frac{1}{N_{e^{\\alpha}}}\\sum_{i\\in V_{e^{\\alpha}}}\\frac{1}{N_{e^{\\alpha}}}\\sum_{j\\in V_{e^{\\alpha}}^{(i)}}\\sum_{c=1}^{C}\\left(1\\cdot\\left(\\mathcal{L}^{\\frac{\\gamma}{2}}(h_{j},c)-\\mathcal{L}^{\\gamma}(h_{i},k)\\right)+\\left(\\mathrm{Pr}(y_{j}=c|g_{j})-\\mathrm{Pr}(y_{i}=c|g_{i})\\right)\\cdot1\\right)}\\\\ &{\\le\\displaystyle\\frac{1}{N_{e^{\\alpha}}}\\sum_{i\\in V_{e^{\\alpha}}}\\frac{1}{N_{e^{\\alpha}}}\\sum_{j\\in V_{e^{\\alpha}}^{(i)}}\\sum_{c=1}^{C}(\\mathrm{Pr}(y_{j}=c|g_{j})-\\mathrm{Pr}(y_{i}=c|g_{i}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "In the following proof, the main goal is to bound the term $(\\operatorname*{Pr}(y_{j}=c|g_{j})-\\operatorname*{Pr}(y_{i}=c|g_{i}))$ under the multi-classification OOD generalization setting. Using the Bayes theorem, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{N_{e^{\\alpha}}}\\displaystyle\\sum_{i\\in V_{e^{\\alpha}}}\\frac{1}{N_{e^{\\epsilon}}}\\displaystyle\\sum_{j\\in V_{e^{(i)}}^{(i)}}\\sum_{c=1}^{C}(\\mathbf{Pr}(y_{j}=c|g_{j})-\\mathbf{Pr}(y_{i}=c|g_{i}))}\\\\ &{=\\frac{1}{N_{e^{\\alpha}}}\\displaystyle\\sum_{i\\in V_{e^{\\alpha}}}\\frac{1}{N_{e^{\\epsilon}}}\\displaystyle\\sum_{j\\in V_{e^{(i)}}^{(i)}}^{C}\\sum_{c=1}^{C}\\frac{\\mathbf{Pr}(g_{j}|y_{j}=c)\\sum_{c^{\\prime}\\neq c}\\mathbf{Pr}(g_{i}|y_{j}=c^{\\prime})-\\mathbf{Pr}(g_{i}|y_{i}=c)\\sum_{c^{\\prime}\\neq c}\\mathbf{Pr}(g_{j}|y_{j}=c^{\\prime})}{(\\sum_{c^{\\prime}=1}^{C}\\mathbf{Pr}(g_{j}|y_{j}=c^{\\prime}))(\\sum_{c^{\\prime}=1}^{C}\\mathbf{Pr}(g_{i}|y_{i}=c^{\\prime}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Let consider the term Pr(gj|yjC=c)Pr(gi|yj=c\u2032)\u2212Pr(Cgi|yi=c)Pr(gj|yj=c\u2032)for each c\u2032 \u0338= c, and bound all these ( c\u2032=1 Pr(gj|yj=c\u2032))( c\u2032=1 Pr(gi|yi=c\u2032))   \nterms respectively. When $y_{i}=c$ , denote $\\mu_{i}(c)\\in\\mathcal{R}^{D\\times(C-1)}$ as the matrix composed of all the class means of node $v_{i}$ \u2019s heterophilic neighbors, i.e. the columns of $\\mu_{i}(c)$ are $\\mu_{c}^{\\prime}$ , $c^{\\prime}\\in\\{1,2,...,C\\}/\\{c\\}$ . The elements of $p_{i}^{\\mathrm{ht}}\\,\\in\\,\\dot{\\mathcal{R}}^{C-1}$ are corresponding $p_{i}^{\\mathrm{ht}}(c^{\\prime})$ , $c^{\\prime}\\,\\in\\,\\{1,2,...,C\\}/\\{c\\}$ . According to Definition 4.1, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(g_{i}|y_{i}=c)=\\frac{1}{\\sqrt{(2\\pi\\sigma^{2})^{D}}}\\exp(-\\frac{1}{2\\sigma^{2}}\\|g_{i}-[p_{i}^{\\mathrm{hm}}\\mu_{c}^{\\top}+(\\mu_{i}(c)p_{i}^{\\mathrm{hl}})^{\\top};p_{i}^{\\mathrm{hm}}\\mu_{c}^{e^{\\mathrm{e^{c}\\top}}}+(\\mu_{i}(c)^{e^{\\mathrm{iv}}}p_{i}^{\\mathrm{hl}})^{\\top}]^{\\top}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Denote $\\begin{array}{r}{(\\sum_{c^{\\prime}=1}^{C}\\operatorname*{Pr}(g_{j}|y_{j}\\,=\\,c^{\\prime}))(\\sum_{c^{\\prime}=1}^{C}\\operatorname*{Pr}(g_{i}|y_{i}\\,=\\,c^{\\prime}))}\\end{array}$ as $\\exp(M)\\,\\in\\,[0,C]$ , where $M\\,>\\,0$ is a constant, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~|\\frac{\\mathrm{Pr}(g_{j}|y_{j}=c)\\mathrm{Pr}(g_{i}|y_{j}=c^{\\prime})-\\mathrm{Pr}(g_{i}|y_{j}=c)\\mathrm{Pr}(g_{i}|y_{j}=c^{\\prime})}{(\\sum_{c^{\\prime}=1}^{C}\\mathrm{Pr}(g_{j}|y_{j}=c^{\\prime}))(\\sum_{c^{\\prime}=1}^{C}\\mathrm{Pr}(g_{i}|y_{i}=c^{\\prime}))}|}\\\\ &{=|\\exp(-\\frac{1}{2\\sigma^{2}}\\|g_{j}-[p_{j}^{\\mathrm{bm}}\\mu_{c}^{\\top}+(\\mu_{i}(c)p_{j}^{\\mathrm{bu}})^{\\top};p_{j}^{\\mathrm{bm}}\\mu_{c}^{e^{\\mathrm{v}\\top}}+(\\mu_{j}(c)^{e^{\\mathrm{se}}}p_{j}^{\\mathrm{bu}})^{\\top}]^{\\top}\\|_{2}^{2})}\\\\ &{~~\\exp(-\\frac{1}{2\\sigma^{2}}\\|g_{i}-[p_{i}^{\\mathrm{bm}}\\mu_{c^{\\prime}}^{\\top}+(\\mu_{i}(c^{\\prime})p_{i}^{\\mathrm{bu}})^{\\top};p_{i}^{\\mathrm{bm}}\\mu_{c^{\\prime}}^{e^{\\mathrm{v}\\top}}+(\\mu_{i}(c^{\\prime})^{e^{\\mathrm{i}\\pi}}p_{i}^{\\mathrm{bu}})^{\\top}]^{\\top}\\|_{2}^{2})}\\\\ &{-\\exp(-\\frac{1}{2\\sigma^{2}}\\|g_{j}-[p_{j}^{\\mathrm{bm}}\\mu_{c^{\\prime}}^{\\top}+(\\mu_{i}(c^{\\prime})p_{j}^{\\mathrm{bu}})^{\\top};p_{j}^{\\mathrm{bm}}\\mu_{c^{\\prime}}^{e^{\\mathrm{v}\\top}}+(\\mu_{j}(c)^{e^{\\mathrm{i}\\pi}}p_{j}^{\\mathrm{bu}})^{\\top}]^{\\top}\\|_{2}^{2})}\\\\ &{~~\\exp(-\\frac{1}{2\\sigma^{2}}\\|g_{i}-[p_{i}^{\\mathrm{bm}}\\mu_{c}^{\\top}+(\\mu_{i}(c)p_{i}^{\\mathrm{bu}})^{\\top};p_{i \n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Further arranging the formula, we obtain: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\frac{\\mathrm{Pr}(g_{j}|y_{j}=c)\\mathrm{Pr}(g_{i}|y_{j}=c^{\\prime})-\\mathrm{Pr}(g_{i}|y_{i}=c)\\mathrm{Pr}(g_{i}|y_{j}=c^{\\prime})|}{(\\sum_{e^{\\prime}=1}^{C}\\mathrm{Pr}(g_{j}|y_{j}=c^{\\prime}))(\\sum_{e^{\\prime}=1}^{C}\\mathrm{Pr}(g_{i}|y_{i}=c^{\\prime}))}|}\\\\ &{=|\\exp(-\\frac{\\|\\boldsymbol{g}_{j}-[y_{j}^{\\mathrm{bm}}\\boldsymbol{\\mu}_{C}^{\\top}+(\\mu_{i}(c)p_{i}^{\\mathrm{b}\\dagger})^{\\top};p_{j}^{\\mathrm{bm}}\\boldsymbol{\\mu}_{C}^{\\top}\\mathrm{\\Lambda}(\\mu_{j}(c)\\!\\!\\cdot\\!\\!\\delta^{\\prime}_{p_{j}^{\\mathrm{b}\\dagger}}\\!\\!\\!\\mid^{\\top}\\,\\!\\!\\Vert_{2}^{2}}{2\\sigma^{2}}}\\\\ &{\\quad-\\,\\frac{\\|\\boldsymbol{g}_{i}-[p_{i}^{\\mathrm{bm}}\\boldsymbol{\\mu}_{C}^{\\top}+(\\mu_{i}(c^{\\prime})p_{i}^{\\mathrm{b}\\dagger})^{\\top};p_{i}^{\\mathrm{bm}}\\boldsymbol{\\mu}_{C}^{\\mathrm{a}\\top}+(\\mu_{i}(c)^{\\ c}\\!\\!\\cdot\\!\\!\\delta^{\\prime}_{p_{i}^{\\mathrm{b}\\dagger}}\\!\\!\\!\\Vert_{2}^{\\top}-M)}{2\\sigma^{2}}}\\\\ &{-\\,\\exp(-\\frac{\\|\\boldsymbol{g}_{j}-[p_{j}^{\\mathrm{bm}}\\boldsymbol{\\mu}_{C}^{\\top}+(\\mu_{i}(c^{\\prime})p_{j}^{\\mathrm{ba}})^{\\top};p_{j}^{\\mathrm{ba}}\\boldsymbol{\\mu}_{C}^{\\mathrm{a}\\top}}{2\\sigma^{2}}+(\\mu_{j}(c)^{c}\\!\\!\\cdot\\!\\!\\delta^{\\prime}_{p_{j}^{\\mathrm{ba}}}\\!\\!\\!\\)^{\\top}]^{\\top}\\Vert_{2}^{2}}\\\\ &{\\quad-\\,\\frac{\\|\\boldsymbol{g}_{i}-[p_{i}^{\\mathrm{bm}}\\boldsymbol{\\mu}_{C}^{\\top}+(\\mu_{i}(c)p\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n|\\exp(-x)-\\exp(-y)|=\\exp(-\\xi)|y-x|\\leq|x-y|\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "(116) ", "page_idx": 46}, {"type": "text", "text": "for $x,\\;y>0$ and $\\xi\\in[x,y]$ (or $\\xi\\in[y,x];$ ). Using Equation (116), we get ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Big|\\frac{\\mathrm{P}(\\phi_{1}|y_{2})}{\\mathrm{C}}-\\mathrm{P}(\\phi_{1}|y_{2})-\\mathrm{c}^{*}\\Big)-\\mathrm{P}(\\phi_{1}|y_{2})-\\mathrm{c}^{*}\\Big|(y_{2}|y_{2})-\\mathrm{c}^{*}\\Big|}\\\\ &{\\quad\\Big|\\frac{\\mathrm{P}(\\phi_{1}|y_{2})}{\\mathrm{C}}-\\mathrm{P}(\\phi_{1}|y_{2})\\Big|y_{2}-\\mathrm{c}^{*}\\Big|(y_{1}^{\\top}\\phi_{1}\\Big|y_{1}-\\mathrm{c}^{*}\\Big)\\Big|}\\\\ &{\\leq\\frac{1}{2^{|\\mathcal{D}|}}(\\|\\phi_{1}-\\|_{\\mathcal{D}}^{\\top}\\phi_{1}^{*}\\|^{2}+\\Big(\\mu_{1}(\\phi_{1}^{\\top})\\beta_{1}^{\\top}\\gamma^{\\top}\\beta_{1}^{\\top}\\beta_{1}^{\\top}\\beta_{1}^{\\top}\\sigma^{*}+(\\mu_{1}\\phi_{1}^{\\top})\\beta_{1}^{\\top}\\gamma^{\\top}\\beta_{1}^{\\top}\\Big)^{\\top}\\Big|\\frac{1}{2}}\\\\ &{\\quad+\\|\\beta_{1}-\\|_{\\mathcal{D}}^{\\top}\\mu_{1}^{\\top}\\mu_{1}^{\\top}+\\Big(\\mu_{1}(\\gamma)\\beta_{1}^{\\top}\\gamma^{\\top}\\beta_{1}^{\\top}\\mu_{1}^{\\top}\\sigma^{*}+(\\mu_{1}(\\phi_{1}^{\\top})^{\\top}\\beta_{1}^{\\top})\\tau\\Big|\\mathbb{I}\\Big]^{2}}\\\\ &{\\quad-\\|y_{2}-\\mathrm{i}^{\\top}\\beta_{1}^{\\top}\\mu_{1}^{\\top}+\\Big(\\mu_{1}(\\gamma)\\beta_{1}^{\\top}\\gamma^{\\top}\\beta_{1}^{\\top}\\mu_{1}^{\\top}\\sigma^{*}+(\\mu_{1}(\\phi_{1}^{\\top})^{\\top}\\beta_{1}^{\\top})\\gamma^{\\top}\\Big|\\mathbb{I}\\Big]^{2}}\\\\ &{\\quad-\\|\\beta_{1}-\\|_{\\mathcal{D}}^{\\top}\\mu_{1}^{\\top}\\sigma^{*}+(\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Let\u2019s bound (A), (B) and (C) respectively. ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle|\\frac{\\alpha}{\\le}\\frac{1}{\\sigma^{2}}(|[(\\mu_{c}-\\mu_{c^{\\prime}})^{\\top};(\\mu_{c}^{\\mathrm{e}^{\\mathrm{e}}}-\\mu_{c^{\\prime}}^{\\mathrm{e}})^{\\top}]||g_{i}-g_{j}|]_{2}+(|\\mu_{c}^{\\mathrm{e}^{\\mathrm{e}}}-\\mu_{c}^{\\mathrm{e}^{\\mathrm{e}}}|+|\\mu_{c^{\\prime}}^{\\mathrm{e}}-\\mu_{c^{\\prime}}^{\\mathrm{e}}|)|g_{i}|}\\\\ &{\\quad+|[\\mu_{c}^{\\top};\\mu_{c}^{\\mathrm{e}^{\\mathrm{e}}^{\\top}}]||g_{i}-g_{j}|_{2}+|[\\mu_{c^{\\prime}}^{\\top};\\mu_{c^{\\prime}}^{\\mathrm{e}^{\\mathrm{e}}\\top}]||g_{i}-g_{j}|_{2}}\\\\ &{\\quad+(|\\mu_{c}^{\\mathrm{e}^{\\mathrm{e}}}-\\mu_{c}^{\\mathrm{e}^{\\mathrm{e}}}|+|\\mu_{c}^{\\mathrm{e}^{\\mathrm{e}}}-\\mu_{c^{\\prime}}^{\\mathrm{e}}|)|g_{j}|}\\\\ &{\\displaystyle\\overset{(b)}{=}\\frac{1}{\\sigma^{2}}\\left((\\sqrt{|[(\\mu_{c}-\\mu_{c^{\\prime}})^{\\top};(\\mu_{c}^{\\mathrm{e}^{\\mathrm{e}}}-\\mu_{c^{\\prime}}^{\\mathrm{e}})^{\\top}]|}+2\\sqrt{2})\\|g_{i}-g_{j}\\|_{2}+(|\\mu_{c}^{\\mathrm{e}^{\\mathrm{e}}}-\\mu_{c}^{\\mathrm{e}^{\\mathrm{e}}}|+|\\mu_{c^{\\prime}}^{\\mathrm{e}}-\\mu_{c^{\\prime}}^{\\mathrm{e}^{\\mathrm{e}}}|)(|g_{i}|\\right.}\\\\ &{\\left.\\le\\frac{1}{\\sigma^{2}}\\left((\\sqrt{|[(\\mu_{c}-\\mu_{c^{\\prime}})^{\\top};(\\mu_{c}^{\\mathrm{e}^{\\mathrm{e}}}-\\mu_{c^{\\prime}}^{\\mathrm{e}})^{\\top}]|}+2\\sqrt{2})\\|g_{i}-g_{j}\\|_{2}+2B_{e\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "(118) $(a)$ uses the fact that $|g_{j}p_{j}^{\\mathrm{hm}}-g_{i}p_{i}^{\\mathrm{hm}}|\\leq\\|g_{i}-g_{j}\\|_{2}$ and $p_{i}^{\\mathrm{hm}},~p_{i}^{\\mathrm{ht}}(c),~p_{i}^{\\mathrm{ht}}(c^{\\prime}),~p_{j}^{\\mathrm{hm}},~p_{j}^{\\mathrm{ht}}(c),~p_{j}^{\\mathrm{ht}}(c^{\\prime})$ are all $<1$ , and $(b)$ is because of the assumption that $|[\\mu_{c}^{\\top};{\\mu_{c}^{\\mathrm{e}^{\\mathrm{t}}}}^{\\top}]|=|[\\mu_{c^{\\prime}}^{\\top};{\\mu_{c^{\\prime}}^{\\mathrm{e}^{\\mathrm{t}}}}^{\\top}]|=\\bar{\\sqrt{2}}$ . Also, since the class means are normalized, we have $({\\bf B})=0$ . Similarly, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(\\mathbf{C})=\\frac{1}{2\\sigma^{2}}(\\bigl(p_{j}^{\\mathrm{ht}}(c)^{2}-p_{i}^{\\mathrm{ht}}(c)^{2}\\bigr)-\\bigl(p_{j}^{\\mathrm{ht}}(c^{\\prime})^{2}-p_{i}^{\\mathrm{ht}}(c^{\\prime})^{2}\\bigr))}\\\\ {\\displaystyle\\qquad\\leq\\frac{1}{2\\sigma^{2}}\\,\\bigl(\\vert p_{j}^{\\mathrm{ht}}(c)-p_{i}^{\\mathrm{ht}}(c)\\vert+\\vert p_{j}^{\\mathrm{ht}}(c^{\\prime})-p_{i}^{\\mathrm{ht}}(c^{\\prime})\\vert\\bigr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Using these results, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\mathcal{L}_{\\infty}^{\\infty}(h)-\\mathcal{L}_{\\infty}^{\\infty}(h)}\\\\ &{\\leq\\displaystyle{\\frac{1}{2\\pi}\\sum_{i=1}^{\\infty}\\frac{1}{\\omega_{i}\\omega_{i}\\omega_{j}}\\sum_{\\mu\\in[\\infty,h]}^{\\infty}\\frac{\\sum_{i=1}^{\\infty}\\mathbb{P}_{\\alpha}(g_{i}|\\mu_{i}=\\epsilon)\\sum_{\\ell\\in[\\infty,h]}\\mathbb{P}_{\\alpha}(g_{i}|\\mu_{i}=\\epsilon)-\\mathbb{P}_{\\alpha}(g_{i}||\\mu_{i}=\\epsilon)\\sum_{\\ell\\in[\\infty,h]}\\mathbb{P}_{\\alpha}(g_{i}|\\mu_{i}=\\epsilon)}{\\sum_{i=1}^{\\infty}\\mathbb{P}_{\\alpha}(g_{i}|\\mu_{i}=\\epsilon)\\prod_{\\ell=1}^{\\infty}\\mathbb{P}_{\\alpha}(g_{i}|\\mu_{i}=\\epsilon)}}}\\\\ &{=\\displaystyle{\\frac{1}{\\lambda_{\\mathrm{e}}}\\sum_{i=1}^{\\infty}\\sum_{\\omega_{i}\\in[\\infty,h]}\\sum_{\\mu\\in[\\infty,h]}\\sum_{\\ell\\in[\\infty,h]}\\sum_{\\ell\\in[\\infty]}\\sum_{\\ell\\in[\\infty,h]}^{\\infty}((\\sqrt{1[\\mu_{(\\mu\\mu-\\mu)^{\\prime}}^{\\infty}]}\\cdot(\\mu_{\\epsilon}^{\\epsilon}-\\mu_{\\epsilon}^{\\epsilon})^{\\epsilon})\\mathbb{P}_{1}(g_{i}<\\mu_{\\epsilon}^{\\epsilon})|)}}\\\\ &{+\\frac{2}{\\omega^{2}}\\sum_{i=1}^{\\infty}(C-1)\\alpha_{i}\\omega_{i}\\omega_{i}^{\\epsilon}-\\mu_{\\epsilon}^{\\epsilon}|}\\\\ &{+\\frac{1}{2\\pi}\\sum_{i=1}^{\\infty}\\sum_{\\nu=1}^{\\infty}\\frac{1}{N_{i}\\omega_{i}\\omega_{j}}\\sum_{\\mu\\in[\\infty,h]}^{\\infty}\\sum_{\\ell\\in[\\infty,h]}\\big[\\nu_{i}(\\ell|\\epsilon_{i})-p_{i}^{\\nu}(\\epsilon)|}\\\\ &{+\\frac{1}{2\\pi}\\frac{C}{\\omega_{i}\\omega_{j}}\\sum_{\\mu\\in[\\infty,h]}\\sum_{\\ell=1}^{\\infty}\\sum_{\\nu\\in[\\infty,h]}\\big[\\nu_{i}( \n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Now we can proceed with the proof of Theorem G.9. ", "page_idx": 47}, {"type": "text", "text": "Proof. Directly replace the result of Lemma 5 in Ma et al. [2021] with that of Lemma G.10 and following the proof of Lemma 6 and Theorem 3 in Ma et al. [2021], under Assumption 4.3, we finish the proof of Theorem G.9. \u53e3 ", "page_idx": 47}, {"type": "text", "text": "H Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "The theoretical analysis is limited to linear GNNs. However, there some justification for using a linear GNN. 1) Some recent works [Zhu and Koniusz, 2021, Wang et al., 2021b] observed that linear GNNs achieve comparable performance to nonlinear GNNs. Tang and Liu [2023] also theoretically proved that SGC can outperform GCN under some mild assumptions. 2) many recent works on the theoretical analysis of graphs/OOD generalization adopt linear networks [Lin et al., 2023, Wu et al., 2022b, Mao et al., 2023]. 3) Our theory matches the experimental results on the nonlinear GCN and GAT that CIA outperforms IRM and VREx. ", "page_idx": 47}, {"type": "text", "text": "We didn\u2019t implement some of the node-level OOD methods in our main experiments including Li et al. [2023a], Liu et al. [2023] because they didn\u2019t release the code. Another limitation is that we didn\u2019t provide an in-depth theoretical comparison between our method and existing node-level OOD methods, but only revealed the difficulty of invariant learning on graphs through the examples of VREx and IRM. However, this theoretical finding is sufficient to inspire the CIA solution and its enhanced version, CIA-LRA. We reserve a more comprehensive analysis of the failure of OOD methods on graphs and broader guidance for designing graph-OOD work for future research. ", "page_idx": 47}, {"type": "text", "text": "I Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Our work contributes to improving the node-level OOD generalization of GNN models. We believe our work could positively impact various fields by improving predictive accuracy in areas like healthcare, social networking, etc., potentially leading to better-personalized services and enhanced safety. ", "page_idx": 47}, {"type": "text", "text": "J Compute Resources ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "We use one NVIDIA GeForce RTX 3090 or 4090 GPU for each single experiments. All algorithms except EERM and GTrans can be executed on a single 24GB GPU when processing the largest dataset, Arxiv, without encountering out-of-memory. ", "page_idx": 48}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: The abstract and introduction concisely state how we dissect the challenges of invariant learning on graphs and out strategies to tackle it. In the introduction, we list all our contributions and indicate the sections in which they are made. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 49}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: We point out the limitations in Appendix H: we didn\u2019t compare the empirical results with some of the node-level OOD works because their codes are unavailable; we didn\u2019t theoretically analyze all invariant learning methods. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 49}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: All supplementary assumptions, formal versions of the theorems, and full proofs can be found in Appendix G. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 50}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We have provided all basic experimental settings and the hyperparameter search space in Appendix C to reproduce all our results. The detailed training procedure of CIA-LRA is provided in Appendix 1. We will release our implementation of CIA and CIA-LRA following the acceptance of our work. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 50}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 51}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 51}, {"type": "text", "text": "Answer: [No] ", "page_idx": 51}, {"type": "text", "text": "Justification: The datasets utilized in our paper are publicly accessible and remain unmodified. We will release our code following the acceptance of our work, complete with detailed instructions to ensure the faithful reproduction of the main experimental results. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 51}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Justification: We adopt the default training details open-source GOOD benchmark, and we have stated some modifications of the setting in the main text and the appendix. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 51}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: The improvements offered by CIA-LRA exceed the error bars of the best existing methods. Also, CIA significantly outperforms VREx and IRM across most dataset splits. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 52}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: In Appendix J, we report the type of GPU we use and the memory costs. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 52}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: Our research conforms, in every respect, to the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 52}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We discuss the potential impacts of out work in Appendix I. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 53}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 53}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: Yes. All assets used in our research, including code, data, and models, are properly credited. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 53}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 54}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: [TODO] ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 54}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 54}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 54}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 55}]