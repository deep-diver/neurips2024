[{"type": "text", "text": "Task-recency bias strikes back: Adapting covariances in Exemplar-Free Class Incremental Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Grzegorz Rypes\u00b4c\u00b4\u2217 IDEAS NCBR Warsaw University of Technology grzegorz.rypesc@ideas-ncbr.pl ", "page_idx": 0}, {"type": "text", "text": "Sebastian Cygert IDEAS NCBR Gdan\u00b4sk University of Technology sebastian.cygert@ideas-ncbr.pl ", "page_idx": 0}, {"type": "text", "text": "Tomasz Trzcin\u00b4ski IDEAS NCBR Warsaw University of Technology Tooploox ", "page_idx": 0}, {"type": "text", "text": "Bart\u0142omiej Twardowski IDEAS NCBR Autonomous University of Barcelona Computer Vision Center ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Exemplar-Free Class Incremental Learning (EFCIL) tackles the problem of training a model on a sequence of tasks without access to past data. Existing state-of-the-art methods represent classes as Gaussian distributions in the feature extractor\u2019s latent space, enabling Bayes classification or training the classifier by replaying pseudo features. However, we identify two critical issues that compromise their efficacy when the feature extractor is updated on incremental tasks. First, they do not consider that classes\u2019 covariance matrices change and must be adapted after each task. Second, they are susceptible to a task-recency bias caused by dimensionality collapse occurring during training. In this work, we propose AdaGauss \u2013 a novel method that adapts covariance matrices from task to task and mitigates the taskrecency bias owing to the additional anti-collapse loss function. AdaGauss yields state-of-the-art results on popular EFCIL benchmarks and datasets when training from scratch or starting from a pre-trained backbone. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continual learning (CL), an essential area of machine learning, focuses on developing algorithms that can learn progressively from a continuous stream of data and adapt to new tasks while retaining previously acquired knowledge. This paradigm is paramount for creating systems capable of lifelong learning, much like humans, and robust in dynamic environments where data distribution evolves over time. A significant challenge within CL is exemplar-free class incremental learning (EFCIL) [41, 26], which requires the model to incorporate new classes without storing previous data samples (exemplars). This approach is especially relevant in scenarios with privacy constraints or limited storage capacity, as it compels the model to retain knowledge and prevent catastrophic forgetting [9, 27] solely through internal mechanisms, such as knowledge distillation [12, 21, 45, 51, 24], parameter regularization [17, 2, 6], expanding neural architecture [44, 53, 32, 3] or generative replay [40, 14, 28]. ", "page_idx": 0}, {"type": "text", "text": "Recent state-of-the-art methods designed for EFCIL often represent classes as Gaussian distributions in the latent space of the feature extractor. That enables an inference using Bayes classifier [10, 35] or training a linear classifier using pseudo-prototypes sampled from these distributions [24, 31, 51, 37]. However, we present in this work that these methods have multiple shortcomings and can be improved. First, they assume that covariance matrices of past classes are constant across incremental training. ", "page_idx": 0}, {"type": "image", "img_path": "5H4l37IsZ8/tmp/eca348c0cf6608121b3d18561bac8617f111e4b222d544e6ad924fef736191ff.jpg", "img_caption": ["Figure 1: Latent space visualization, average accuracy after the last task, and symmetrical KL divergence between memorized and ground truth distributions for ResNet18 trained sequentially on ImagenetSubset dataset split into ten tasks. Freezing the feature extractor prevents changes in data distribution but results in inseparable classes. When the network is trained on incremental tasks (unfrozen), the ground truth distributions change and do not match the memorized ones. A suitable CL method should adapt the mean and covariance of distributions to retain valid decision boundaries. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, as presented in Fig. 1, when the feature extractor is updated on incremental tasks (it is unfrozen), distributions of previous classes change and no longer match the memorized ones. Suitable methods must adapt both means and covariances. EFC [24] predicts drift (change) only of the distribution mean and points out that adapting covariances is an open question. Second, the methods suffer from a dimensionality collapse [30, 16], which is more significant in early tasks. That makes old classes\u2019 covariances to be of lower rank than those from recent tasks, which introduces errors while inverting the matrices for the classification, leading to increased task-recency bias. We explain this in detail in Sec. 3.2. ", "page_idx": 1}, {"type": "text", "text": "This work focuses on the challenging problems of adapting classes\u2019 covariances and overcoming dimensional collapse in EFCIL. We are the first to introduce a method that adapts the mean and covariance of memorized distributions, significantly reducing the error between memorized and ground truth distributions. We also overcome the dimensionality collapse of feature representations by introducing a novel anti-collapse loss, which alleviates the problem of task-recency bias. We dub the resulting method AdaGauss - Adapting Gaussians. Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We analyze dimensionality collapse in EFCIL settings and explain that it leads to taskrecency bias. We introduce a novel anti-collapse loss to prevent it.   \n\u2022 We show that knowledge distillation techniques in EFCIL provide different representation strengths of the feature extractor. We are the first to utilize knowledge distillation through a learnable projector network in EFCIL.   \n\u2022 Based on these findings, we propose AdaGauss, a novel method to adapt both means and covariances of memorized class distributions, which results in state-of-the-art results when the model is trained from scratch or starting from a pre-trained weights. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Semantic drift. We investigate offline, EFCIL setting [26] focusing on keeping the network size constant, where no task information is available at test time. Regularization-based approaches penalize changes to important neural network parameters [17, 6, 47, 22] or use distillation techniques to regularize neuron activations [21, 45, 51, 24]. However, even with knowledge distillation, the features from old classes will change, causing catastrophic forgetting [9, 27]. Therefore, few works tried to predict these changes by approximating their semantic drift [45, 24, 15, 36]. However, those strategies\u2019 limitations are that they adapt only prototypes, ignoring changes in covariance matrices, which we experimentally show is suboptimal. As predicting the drift is challenging, many methods focus on scenarios where the backbone is frozen after the first task [4, 31, 23, 29, 10]. However, this prevents the feature extractor to adapt to new tasks [24]. We show that it is possible to change the feature extractor and adapt the covariance matrices of classes. ", "page_idx": 1}, {"type": "text", "text": "Task-recency bias. Another challenge in CL is a task-recency bias, where the model is biased towards classifying classes from new tasks [13, 26, 50]. While some works approached this problem using exemplars [1, 43, 13, 48] the problem is amplified in an exemplar-free setting. Some works considered prototype replay, which maintains the decision boundary between classes [31, 51, 37, 39, 38, 53]. To improve this strategy, PASS [52] included prototype augmentation, and EFC [24] updates their prototypes after each task. In this work, we point out that the cause for task-recency bias in the EFCIL scenario is the dimensionality collapse of the feature extractor, leading to numerical instabilities when inverting covariance matrices. ", "page_idx": 2}, {"type": "text", "text": "Dimensionality collapse. Recent works revealed that supervised learning exhibits signs of neural collapse [30, 16], where a large fraction of features\u2019 variance is described only by a small fraction of their dimensions. Since then, several studies [5, 7, 46, 16] showed that utilizing additional MLP projector is a crucial component to alleviate the collapse of the representations and improve their transferability. Another implication of the neural collapse in CL is that it becomes challenging to invert covariance matrices. Existing methods add a constant value to the diagonal [35, 24, 51] of the covariance matrices or utilize shrinking [10] to prevent that. On the contrary, we propose an anti-collapse loss, which is more elegant and does not artificially alter covariance matrices. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Exemplar-Free Class-Incremental Learning (EFCIL) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Class-Incremental Learning (CIL) scenario considers a dataset split into $T$ tasks, each corresponding to the non-overlapping set of classes $C_{1}\\cup C_{2}\\cup\\cdot\\cdot\\cdot\\cup C_{T}=C$ such that $C_{t}\\cap C_{s}=\\emptyset$ for $t\\neq s$ . In Exemplar-Free CIL (EFCIL), during a training step $t$ , we have only access to current task data $D_{t}=\\{(\\dot{x},y)|y\\in C_{t}\\}$ and we cannot store any exemplars from the previous steps. The objective is to train a model that discriminates between old $(<t)$ and new classes combined. We assume a task-agnostic evaluation [41, 26], where the method does not know the task id during the inference. ", "page_idx": 2}, {"type": "text", "text": "3.2 The three observations that motivate towards AdaGauss ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we provide an insight into problems with current EFCIL methods. We train the standard ResNet18 [11] network on the ImagenetSubset dataset divided into ten equal tasks. We point out that: 1. covariance of class distributions during CL sessions change and must be adapted; 2. the task recency bias comes from the differences in representational strength of the model; 3. when training from scratch in EFCIL, the models are susceptible to dimensionality collapse. ", "page_idx": 2}, {"type": "text", "text": "Observation 1. As illustrated in Fig. 1, training the feature extractor on incremental tasks makes memorized distribution not match the ground truth (GT) ones. More specifically, the mean and covariance of GT change, and to keep valid decision boundaries, both memorized means and covariances must be adapted. That decreases symmetrical KL divergence between memorized and GT distributions, thus increasing average accuracy after the last task. However, existing state-of-the-art methods [36, 24, 51, 52] do not adapt covariance matrices, while others [31, 10, 55, 54] freeze the feature extractor after the initial task, which does not guarantee separability of classes from new tasks (first image in Fig. 1). ", "page_idx": 2}, {"type": "text", "text": "Observation 2. When training the feature extractor with different knowledge distillation methods (feature [53, 52, 45], logit [21, 33], projected [18]), representational strength of the feature extractor increases with each task, as presented in Fig. 2. That makes memorized covariance matrices of late tasks have a higher rank than those from early tasks, as presented in Fig. 3. When these matrices are inverted, the opposite happens - due to numerical instabilities, norms of inverted covariance matrices of early tasks will be greater. That causes task-recency bias as presented in Fig. 4. In the case of Bayes classification [35, 10], the Mahalanobis distance is much higher for early tasks, whereas in the case of sampling pseudo-prototypes [24, 51] the logits for recent tasks are higher, what skews classification towards recent tasks. This bias differs from already well-studied linear head bias [13, 43, 48], as it occurs at the level of the representations, where no linear head and no exemplars are utilized. ", "page_idx": 2}, {"type": "text", "text": "Observation 3. Fig. 3 also presents that feature extractor suffers from dimensionality collapse [30, 16] as ranks of covariance matrices are much lower than the latent space size (512 for ResNet18). That makes classes covariance matrices non-invertible. That, in turn, disallows the calculation of Mahalanobis distance, likelihood, and sampling from such collapsed distribution. In order to overcome this issue, the existing methods utilize shrinking [10] or add a constant value to the diagonal [35, 24, 51] of the covariance matrices to prevent that. However, these techniques artificially alter classes\u2019 distributions, introducing additional hyperparameters and a new source of errors accumulating during long CIL sessions. A more elegant solution would directly prevent the dimensionality collapse of the feature extractor during training while preserving the class separability provided by cross-entropy. ", "page_idx": 2}, {"type": "image", "img_path": "5H4l37IsZ8/tmp/45abc3084350e55435bf24c9ac84884bec07ece348af8d78cdba56bbae5b1af9.jpg", "img_caption": ["Figure 2: The representational Figure 3: Average rank of mem-Figure 4: Average Mahalanobis strength of ResNet18 trained orized covariance matrices of distance between memorized on 10 tasks of ImagenetSubset classes after each task (black) on distributions and joint dataset dataset split into 10 tasks for ImagenetSubset for logit distil-per each task after the last task different knowledge distillation lation. Norm of these matrices (black) and average logit value methods. After each task, we when inverted (green). Lower on linear head trained by sammeasure how many eigenvalues rank leads to larger values in pling prototypes from memosum to $95\\%$ variance of all fea-inverses of covariance matrices rized distributions. There is a tures provided. due to numerical instabilities. visible task-recency bias. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.3 AdaGauss method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Motivated by these three observations, we made the following decisions about AdaGauss. Based on the first observation, after training the feature extractor $F$ on an incremental task, we train an auxiliary network (adapter), which we utilize to adapt the means and covariances of old classes to the latent space of the new feature extractor. To perform knowledge distillation and improve the representation strength of the feature extractor (second observation), we utilize feature distillation through a learnable projector. In order to overcome the dimensionality collapse and task-recency bias, showcased by the second and third observations, we utilize a novel anti-collapse loss that regularizes the features\u2019 covariance matrix and prevents dimensional collapse. AdaGauss memorizes each class as a mean and covariance and performs Bayes classification as in [10, 35]. We provide a pseudo-code of our method in Alg. 1. Below, we explain the motivation and details of the method. ", "page_idx": 3}, {"type": "text", "text": "3.3.1 Feature distillation through a learnable projector ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Inspired by representational-learning [18], we utilize a feature distillation through a learnable projector to mitigate forgetting, which we refer to as projected distillation. As presented in Fig. 2, this distillation technique provides representations with a better eigenvalues distribution, thus decreasing the problem of task-recency bias compared to standard logit [21, 33] and feature [45, 53, 52] distillation techniques. As the projector, we utilize a 2-layer MLP network $\\phi^{t\\to t-1}:\\mathbb{R}^{S}\\to\\mathbb{R}^{\\bar{S}}$ with hidden size $d$ times bigger than the latent space $S$ . Following existing continual learning works [21, 45, 33], when training $F_{t}$ on minibatch $B$ , we freeze $F_{t-1}$ trained on the previous task. Finally, we calculate our knowledge distillation loss as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{P K D}=\\sum_{i\\in B}\\|\\phi^{t\\rightarrow t-1}\\left(F_{t}\\left(x_{i}\\right)\\right)-F_{t-1}\\left(x_{i}\\right)\\|^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.3.2 Overcoming dimensionality collapse ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As described in Sec. 3.2, existing methods for EFCIL that represent classes as Gaussian distributions suffer from dimensionality collapse, which leads to task-recency bias caused by the fact that ranks of covariance matrices are different amongst the tasks. To overcome the collapse, we encourage the feature extractor to produce features whose dimensions are linearly independent. Therefore, in each task, we directly optimize the covariance matrices of features produced by $F_{t}$ to be positive-definitive by the diagonal of the Cholesky decomposition of covariance of each training minibatch to be positive. More precisely, let $S$ be the size of the feature vectors and $a_{i}$ be $i$ -th element of the diagonal of a Cholesky decomposition of minibatch\u2019s covariance matrix. We formulate the anti-collapse loss $L_{A C}$ in the form: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{A C}=-\\frac{1}{S}\\sum_{i=1}^{S}\\operatorname*{min}(a_{i},1)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This loss forces Cholesky\u2019s decomposition of covariance of each minibatch to have diagonal entries greater than 1. Therefore, they are positive, and the covariance matrix is positive-definite due to the property of Cholesky decomposition. More on the definition of $L_{A C}$ in Appendix, Sec. A.2). ", "page_idx": 4}, {"type": "text", "text": "3.3.3 Training the feature extractor ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In each task $t$ , we train all parameters of the feature extractor $F_{t}$ together with additional projector $\\phi$ used for knowledge distillation. Following most works [10, 35, 51, 31, 21], we utilize popular cross-entropy loss $L_{C E}$ to discriminate between classes. The final loss function is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\cal L}={\\cal L}_{C E}+{\\cal L}_{A C}+\\lambda{\\cal L}_{P K D},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda\\in{\\mathcal{R}}$ is a plasticity-stability trade-off hyperparameter, similar to [21]. ", "page_idx": 4}, {"type": "text", "text": "After training the feature extractor, we represent classes $C_{t}$ as multivariate Gaussian distributions in the latent space. More precisely, we represent any class $c\\in C_{t}$ as $\\mathcal{N}(\\mu_{c},\\Sigma_{c})$ . ", "page_idx": 4}, {"type": "text", "text": "3.3.4 Adapting Gaussian distributions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After training of $F_{t}$ is completed, representations of old classes drifted [45] (changed) and no longer match memorized Gaussians. Therefore, we update memorized Gaussians representing past classes to recover ground truth representations. To do that, we train an auxiliary adaptation network $\\psi^{t-1\\rightarrow t}:\\mathbb{R}^{S}\\rightarrow\\mathbb{R}^{S}$ (called adapter), which maps features from the old latent space to the new one. We use only the current data from task $t$ for that. Training loss is: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\psi}=\\sum_{i\\in B}||\\psi^{t-1\\to t}(F_{t-1}(x_{i}))-F_{t}(x_{i})||^{2}+L_{A C}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$L_{A C}$ is the same anti-collapse loss as used during the training of the feature extractor. After training the adapter, for each old class $c$ , we sample from $\\mathcal{N}(\\mu_{c},\\bar{\\Sigma}_{c})$ a set of $N$ points: $n_{1},n_{2},\\ldots,n_{N}$ , where $N\\gg|S|$ and transform them through $\\psi$ obtaining new set: $\\{\\psi(n_{1}),\\psi(n_{2}),\\ldots,\\psi(n_{N})\\}$ . We calculate adopted mean $\\mu_{c}^{n e w}$ and covariance $\\Sigma_{c}^{n e w}$ using new sets of data and update the old distribution as follows: $(\\mu_{c},\\Sigma_{c})\\,=\\,(\\mu_{c}^{n e w},\\Sigma_{c}^{n e w})$ A pseudocode of the full AdaGauss method is presented in Alg. 1. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Datasets and metrics. We evaluate our method on several well-established benchmark datasets. CIFAR100 [19] consists of $50\\mathrm{k}$ training and 10k testing images in resolution $32\\mathrm{x}32$ . TinyImageNet [20], a subset of ImageNet [8], has $100\\mathrm{k}$ training and $10\\mathbf{k}$ testing images in 64x64 resolution. ImagenetSubset contains 100 classes from ImageNet (ILSVRC 2012) [34]. We split these datasets into 10 and 20 equal tasks. Thus, each task contains the same number of classes, a standard practice in EFCIL [21, 45, 35, 24]. We also evaluate our method on fine-grained datasets: CUB200 [42] represents 11, 788 images of bird species, and FGVCAircraft [25] dataset consists of 10, 200 images of planes. We split fine-grained datasets into 5, 10, and 20 tasks. As the evaluation metric, we utilize commonly used average accuracy $A_{l a s t}$ , which is the accuracy after the last task, and average incremental accuracy $A_{i n c}$ , which is the average of accuracies after each task [26, 24, 10]. ", "page_idx": 4}, {"type": "text", "text": "Baselines and hyperparameters. We compare our method to multiple EFCIL baselines. Wellestablished ones, like EWC [17], LwF [21], PASS [52], IL2A [51], SSRE [53], and the most recent and strong EFCIL baselines: FeTrIL [31], FeCAM [10], DS-AL [54] and EFC [24]. For the baseline ", "page_idx": 4}, {"type": "text", "text": "1: Initialize: Training data $(D_{1},D_{2},...,D_{T}),I$ $F_{1}$ (feature extractor), $\\lambda_{\\cdot}$ , N   \n2: Train $F_{1}$ on $D_{1}$ using $L_{C E}+L_{A C}$   \n3: for $c\\in C_{1}$ do   \n4: Obtain set of features: $O=\\{F_{1}(x):x,c\\in D_{1}\\}$   \n5: Store $\\mu_{c}=\\mathrm{mean}(O)$ and $\\Sigma_{c}=$ covariance $(O)$   \n6: end for   \n7: for $t=2,3,4,\\dots$ do   \n8: Initialize $\\phi^{t\\to t-1}$ (distiller), $\\psi^{t-1\\rightarrow t}$ (adapter)   \n9: Train $F_{t}$ on $D_{t}$ using $L=L_{C E}+L_{A C}+\\lambda L_{P K D}$   \n10: for $c\\in C_{t}$ do   \n11: Obtain set of features: $O=\\{F_{t}(x):x,c\\in D_{t}\\}$   \n12: Store $\\mu_{c}=\\mathrm{mean}(O)$ and $\\Sigma_{c}=\\cdot$ covariance $(O)$   \n13: end for   \n14: Train adapter $\\psi^{t-1\\to t}$ on $D_{t}$ using $L_{\\psi}+L_{A C}$   \n15: for $c\\in\\cup_{i=1}^{t-1}C_{i}$ do   \n16: Sample $n_{1}$ $_1,n_{2},\\dots,n_{N}$ from $\\mathcal{N}(\\mu_{c},\\Sigma_{c})$   \n17: Calculate \u00b5cne $\\Sigma_{c}^{n e w}$ of set $\\{\\psi^{t-1\\to t}(n_{1}),\\psi^{t-1\\to t}(n_{2}),\\ldots,\\psi^{t-1\\to t}(n_{N})\\}$   \n18: $\\mu_{c}=\\mu_{c}^{n e w}$ ; $\\Sigma_{c}=\\Sigma_{c}^{n e w}$   \n19: end for   \n20: end for ", "page_idx": 5}, {"type": "text", "text": "results on CIFAR100, TinyImageNet, and ImagenetSubset, we take the results reported in [24], while for FeCAM, we run its original implementation. For fine-grained datasets (CUB200, FGVCAircrafts), we run implementations provided in FACIL [26] and PyCIL [49] frameworks (if provided) or from the authors\u2019 repositories. We set default hyperparameters proposed in the original works. We utilize random crops and horizontal flips as data augmentation. ", "page_idx": 5}, {"type": "text", "text": "Implementation details and reproducibility. We utilize standard ResNet18 [11] as a feature extractor $F$ for all methods. We train it from scratch on CIFAR100, TinyImagenetSubset, and ImagenetSubset, while for experiments on fine-grained datasets, we utilize weights pre-trained on ImageNet. We implement our method in FACIL[26] benchmark2. We set $\\lambda=10,N=10000,d=$ 32 and add a single linear bottleneck layer at the end of the $F$ with $S$ output dimensions, which define the latent space. When training from scratch, we set $S=64$ , while for fine-grained datasets, we decrease it to 32, as there are fewer examples per class. We use an SGD optimizer running for 200 epochs with a weight decay equal to 0.0005. When training from scratch, we utilize a starting learning rate (lr) of 0.1, decreased by ten times after 60, 120, and 180 epochs. We train the adapter using an SGD optimizer with weight decay of 0.0005, running for 100 epochs with a starting lr of 0.01; we decrease it ten times after 45 and 90 epochs. ", "page_idx": 5}, {"type": "text", "text": "We utilize a single machine with an NVIDIA RTX4080 graphics card to run experiments. The time for execution of a single experiment varied depending on the dataset type, but it was at most ten hours. We attach details of utilized hyperparameters in scripts in the code repository. We report all results as the mean and variance of five runs. ", "page_idx": 5}, {"type": "text", "text": "4.1 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Training from scratch. We present the baseline results and AdaGauss method when training from scratch in Tab. 1. We consider $T\\,=\\,10$ and $T\\,=\\,20$ equal tasks. We can see an improvement over the most recent state-of-the-art method - EFC [24]. We improve its results by $3.7\\%$ and $6.8\\%$ points in terms of average accuracy on ImagenetSubset split into 10 and 20 tasks, respectively. This improvement is also consistent in terms of average incremental accuracy - $5.1\\%$ and $7.5\\%$ points and on the other datasets. This increase can be attributed to the fact that EFC does not adapt covariance matrices from task to task (just means), which, as we showed in Sec. 3.2, is required to improve the results. Older method - IL2A [51], which does not adapt their classes representations (means and covariance matrices) method at all, achieves much lower results than our approach - $23.4\\%$ and $25.1\\%$ points lower average accuracy on ImagenetSubset. ", "page_idx": 5}, {"type": "table", "img_path": "5H4l37IsZ8/tmp/ffd8aa73ca4ea340c1991cce6a28299d318897672b4a9e00bc427e8df9f01735.jpg", "table_caption": ["Table 1: Average incremental and last accuracy in EFCIL when training the feature extractor from scratch. The mean of 5 runs is reported. Full results are in Tab. 5. We denote the best results in bold. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "5H4l37IsZ8/tmp/21f3ac44a13bd046c0fa68834747b0b4b46bbb8b7e8c6a971bb9152e65f1933d.jpg", "table_caption": ["Table 2: Average incremental and last accuracy in EFCIL fine-grained scenarios when utilizing a pre-trained feature extractor. We report the mean of 5 runs, while variances are reported in Tab. 6. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Methods such as FeTrIL [31], FeCAM [10] and DS-AL [54] overcome the problem of distribution drift by freezing the feature extractor on the first task. However, it cannot adapt well to the new incremental tasks, resulting in poor plasticity and worse results than AdaGauss and EFC [24]. FeTrIL achieves $14.9\\%$ and $16.0\\%$ points lower average accuracy on ImagenetSubset, while FeCAM - $12.4\\%$ and $13.6\\%$ . ", "page_idx": 6}, {"type": "text", "text": "Training from pre-trained model. We provide the baseline results and our method when training from a ImageNet pre-trained model in Tab. 2. Despite having a strong feature extractor from the very beginning, it still needs to be adapted to discriminate better between fine-grained classes. We report results for 5, 10, and 20 equal tasks. AdaGauss achieves state-of-the-art results. It improves the average accuracy of the second-best method EFC [24] by $4.8\\%$ and $4.4\\%$ points on CUB200 and StanfordCars for $T=10$ , respectively. The results are consistent for other number of tasks. ", "page_idx": 6}, {"type": "text", "text": "Ablation study. We perform ablation of our method on CIFAR100 and ImagenetSubset datasets split into ten equal tasks in Table 3. First, we test our method with the nearest mean classifier (NMC) instead of the Bayes classifier to verify whether considering covariance improves the results. Without covariance matrices and with NMC [33] (1st row), we get worse results: $9.7\\%$ and $9.6\\%$ points lower average accuracy on CIFAR100 and ImagenetSubset, respectively. Memorizing covariances and sampling pseudo-prototypes to adapt means (2nd row) improves NMC results only slightly. Next, we utilize the Bayes classifier instead of NMC but assume that class distributions have diagonal covariance matrices (3rd row). That decreases the average accuracy of our method by $5.0\\%$ and $3.9\\%$ , respectively, proving that ground truth test distributions have non-zero off-diagonal. Then, we test our method without adapting means (5th row) like in IL2A [51] method. That severely hurts the performance - average accuracy decreases by $21.5\\%$ and $27.2~\\%$ . On the contrary, if we adapt means but not covariances like in EFC [24], we lose far less, $3.2\\%$ and $3.1\\%$ , respectively. Lastly, we check the performance of our method without the $L_{A C}$ component. To allow covariance matrices to be invertible, we add a shrink value of 0.5, similarly to [10]. This results in an average accuracy drop of $5.9\\%$ and $4.0\\%$ . The results are also consistent with the average incremental accuracies. This ablation proves our design choices and that all components are necessary to get the best results. ", "page_idx": 6}, {"type": "text", "text": "Table 3: Ablation of AdaGauss indicating the contribution from the different components. \u2217signifies that we utilized covariance matrix shrinking with the value of 0.5 (chosen on the validation set) instead of anti-collapse loss to overcome the covariance matrix singularity problem. ", "page_idx": 7}, {"type": "table", "img_path": "5H4l37IsZ8/tmp/a383856707d147cf9b33a8f92d6c34d1d11e8c62cc5f56c285d2fd0175775e83.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Adaptation results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We verify how our adaptation method improves the quality of memorized class distributions on ImagenetSubset split into ten equal tasks. For this purpose, we measure the average distances between memorized and real classes after each task. More precisely, we measure the L2 distance between means and covariances as well as symmetrical Kulbach-Leibler divergence $(D_{K L})$ between memorized and real distributions. We utilize projected distillation $\\langle\\lambda=10\\rangle$ ) and compare our method to a baseline that does not adapt distributions like in [51, 52] (No adapt) and to the prototype drift compensation introduced in EFC [24] that adapts only means. We provide results in Fig. 5. We can see that our approach allows us to better approximate ground truth distributions. More precisely, compared to EFC, it decreases the distance to real-mean by ${\\approx}29\\%$ , to real-covariance by ${\\approx}39\\%$ and $D_{K L}$ distance by ${\\approx}72\\%$ . We can also see that the EFC approach does not improve distance to real-covariance compared to no adaptation, which is a drawback of this method. ", "page_idx": 7}, {"type": "image", "img_path": "5H4l37IsZ8/tmp/a51ebfcfed0f67e3a2058dda4a433355b4980379a62d3a23d39128847b99c6c1.jpg", "img_caption": ["Figure 5: Distances from memorized distributions to the real ones in terms of distributions\u2019 mean, covariance and KL divergence across 10 tasks on ImagenetSubset dataset. AdaGauss greatly reduces errors and allows for better adaptation than prototype drift compensation (EFC). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Analysis of anti-collapse loss ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We analyze the impact of anti-collapse $L_{A C}$ regularization term on ImagenetSubset split to 10 equal tasks. After the last task, we verify how much $L_{A C}$ improves the distribution of classes\u2019 covariance eigenvalues. We report results in Fig. 6. Without utilizing $L_{A C}$ , the largest eigenvalue is $\\approx1.2*10^{5}$ times greater than the lowest, showcasing the dimensionality collapse. However, with $L_{A C}$ , this difference equals ${\\approx}84$ , proving that more eigenvectors contribute towards representations, and the collapse is greatly diminished. ", "page_idx": 7}, {"type": "text", "text": "Next, we measure the average rank of covariance matrices memorized in each task for different knowledge distillation methods and projected distillation with $L_{A C}$ . Here, we set $S=64$ . In Fig. 7 can see that without $L_{A C}$ , all distillation methods present in existing methods struggle to achieve class covariance equal to latent size $S$ , which according to Sec. 3.2 results in task-recency bias. Interestingly, when combining projected distillation with $L_{A C}$ , the rank of covariance matrices equals 64 for each task, proving that $L_{A C}$ is a promising approach for combating dimensionality collapse when training from scratch. ", "page_idx": 7}, {"type": "text", "text": "An alternative method for overcoming singularity in covariance matrices is shrinking [10]. In Fig. 8, we present results for our method with different values of shrink performed when calculating covariance matrices on CIFAR100. Intuitively, increasing the shrink value decreases the method\u2019s efficacy, as it artificially alters the covariance to be different from the ground truth representation. Without using $L_{A C}$ and without shrink, it is impossible to invert the matrices, resulting in the crash of the method. Nevertheless, the results are the highest when utilizing $L_{A C}$ without shrink $(60.2\\%)$ . ", "page_idx": 8}, {"type": "text", "text": "Figure 6: Distribution of eigen-Figure 7: Ranks of classes\u2019 values of class representations covariance matrices with diffor our method with and without ferent distillation methods and $L_{A C}$ (anti-collapse loss) term.projected distillation with anti$L_{A C}$ greatly reduces the differ-collapse term (red) for latent ence between the most and least space size $S=64$ . $L_{A C}$ makes significant eigenvalues, thus pre-covariance ranks to be equal to venting dimensional collapse. $S$ in every task. ", "page_idx": 8}, {"type": "image", "img_path": "5H4l37IsZ8/tmp/ecb49f98bc57497766e7f678f4e9ea061ebe47d21e07e9e5430bb4b37efdb6f3.jpg", "img_caption": ["Figure 8: Average incremental accuracy for AdaGauss for different values of covariance shrinking, with and without anticollapse regularization. Results without $L_{A C}$ and shrink were not included due to the inability to invert covariance matrices after the first task. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Different distillation techniques ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We test the performance of the projected distillation against other distillation techniques in AdaGauss. We train from scratch on CIFAR100, ImagenetSubset and utilize the pre-trained model on CUB200. We split datasets into ten equal tasks and use hyperparameters from experiments in Tab. 1 and Tab. 2. We present the results in Fig. 9. Projected distillation achieves better average accuracy than logit distillation by $1.4\\%$ , $0.9\\%$ , and $4.0\\%$ points on CIFAR100, ImagenetSubset, and CUB200, respectively. Interestingly, the gap between projected distillation and not using knowledge distillation is much lower on CUB200, which we contribute to using a strong pre-trained model. ", "page_idx": 8}, {"type": "text", "text": "4.5 Memory requirements ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our method does not increase the number of feature extractor\u2019s parameters. In addition, the adapter and distiller are discarded after the training, thus not increasing memory during the long CIL sessions and evaluations. AdaGauss requires S + S(S2\u22121) parameters to memorize the mean and covariance of a class, where $S$ is the latent space size. Therefore, the method requires the same number of parameters as FeCAM [10] and fewer weights than EFC [24] as we do not expand the linear classifier. Additionally, $S$ can be decreased using linear bottleneck layer before the latent space. ", "page_idx": 8}, {"type": "image", "img_path": "5H4l37IsZ8/tmp/1cdc2ddbfc55e65ea0dff10c8699cad762d3bf156b90b70b82812dc033656224.jpg", "img_caption": ["Figure 9: Average last task acc. of our method for different knowledge distillation techniques. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.6 Time complexity of AdaGauss ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We measure the training and inference time of popular EFCIL methods using their original implementations on a single machine with NVIDIA GeForce RTX 4060 and AMD Ryzen 5 5600X CPU. We repeat each experiment 5 times, train all methods for 200 epochs, use four workers, and have a batch size equal to 128. We test vanilla AdaGauss and AdaGauss, where the Bayes classifier is replaced with a trained linear head, where the classifier is trained on samples from class distributions (mean and cov. matrix). We utilize the FeTrIL version with a linear classification head. ", "page_idx": 8}, {"type": "text", "text": "We present results in Tab. 4. The inference of our method takes a similar amount of time as in FeCAM, as the feature extraction step is followed by performing Bayes classification. The inference time of AdaGauss is slightly higher than that of methods with linear classification head (LwF, FeTrIL, AdaGauss with linear head) because Bayes classification requires an additional matrix multiplication when calculating the Mahalanobis distance. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "The training time of AdaGauss is longer than for LwF, EFC, FeCAM, and FeTriL as we do not freeze the backbone after the initial task and additionally train the auxiliary adaptation network. Still, AdaGauss takes less time to train than its main competitor - EFC, and is much faster than SSRE. Our method does not increase the number of networks\u2019 parameters because the distiller and the adapter are disposed after training steps. ", "page_idx": 9}, {"type": "table", "img_path": "5H4l37IsZ8/tmp/9bfcb02a2eb6e41ea04a8dff20983eda2286ccaa80e7ef24ef6b7263449d64e9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusions and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we analyze the impact of dimensionality collapse in EFCIL. We explain that it leads to differences across tasks in ranks of classes\u2019 covariance matrices, which in turn causes task-recency bias. We also present that due to distribution drift, means and covariance of classes change, and they should be adapted from task to task. Based on these findings, we propose the first EFCIL method to adapt both means and covariances, dubbed AdaGauss. It utilizes feature distillation through a learnable projector and a novel anti-collapse regularization term during training that prevents having degenerated, non-invertible features covariance matrices as class representations. That, in turn, alleviates the task-recency bias of the classifier in continual learning. With the series of experiments, we show that AdaGauss achieves state-of-the-art results in common EFCIL scenarios, both when trained from scratch and when initialized from a pre-trained model. ", "page_idx": 9}, {"type": "text", "text": "The limitation of our method is that the cross-entropy separates classes only from the current task. However, when training the feature extractor, old classes can begin overlapping with each other and with new classes int he latent space causing forgetting. This problem is an open question in EFCIL. We speculate it can be alleviated wit a contrastive loss. Another problem arises when there is very little data representing a single class, making high-dimensional covariance matrix impossible to calculate. We tackle it by introducing a bottleneck layer at the very end of the feature extractor. However, it can limit its representational strength. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was partially funded by National Science Centre, Poland, grant no: 2020/39/B/ST6/01511, 2022/45/B/ST6/02817, and 2023/51/D/ST6/02846. Bart\u0142omiej Twardowski acknowledges the grant RYC2021-032765-I. This paper has been supported by the Horizon Europe Programme (HORIZON-CL4-2022-HUMAN-02) under the project \"ELIAS: European Lighthouse of AI for Sustainability\", GA no. 101120237. We gratefully acknowledge Polish high-performance computing infrastructure PLGrid (HPC Center: ACK Cyfronet AGH) for providing computer facilities and support within computational grant no. PLG/2023/017431. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax for incremental learning. In ICCV, 2021.   \n[2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European conference on computer vision (ECCV), pages 139\u2013154, 2018.   \n[3] Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3366\u20133375, 2017.   \n[4] Eden Belouadah and Adrian Popescu. Deesil: Deep-shallow incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pages 0\u20130, 2018.   \n[5] Florian Bordes, Randall Balestriero, Quentin Garrido, Adrien Bardes, and Pascal Vincent. Guillotine regularization: Why removing layers is needed to improve generalization in self-supervised learning. Transactions on Machine Learning Research, 2023. [6] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of the European conference on computer vision (ECCV), pages 532\u2013547, 2018.   \n[7] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15750\u201315758, 2021.   \n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[9] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128\u2013135, 1999.   \n[10] Dipam Goswami, Yuyang Liu, Bart\u0142omiej Twardowski, and Joost van de Weijer. Fecam: Exploiting the heterogeneity of class distributions in exemplar-free continual learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[12] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[13] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 831\u2013839, 2019.   \n[14] Wenpeng Hu, Zhou Lin, Bing Liu, Chongyang Tao, Zhengwei Tao Tao, Dongyan Zhao, Jinwen Ma, and Rui Yan. Overcoming catastrophic forgetting for continual learning via model adaptation. In International conference on learning representations, 2019.   \n[15] Ahmet Iscen, Jeffrey Zhang, Svetlana Lazebnik, and Cordelia Schmid. Memory-efficient incremental learning through feature adaptation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVI 16, pages 699\u2013715. Springer, 2020.   \n[16] Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional collapse in contrastive self-supervised learning. arXiv preprint arXiv:2110.09348, 2021.   \n[17] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences (PNAS), 2017.   \n[18] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International conference on machine learning, pages 3519\u20133529. PMLR, 2019.   \n[19] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.   \n[20] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[21] Zhizhong Li and Derek Hoiem. Learning without forgetting. Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2017.   \n[22] Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M Lopez, and Andrew D Bagdanov. Rotate your networks: Better weight consolidation and less catastrophic forgetting. In 2018 24th International Conference on Pattern Recognition (ICPR), pages 2262\u20132268. IEEE, 2018.   \n[23] Chunwei Ma, Zhanghexuan Ji, Ziyun Huang, Yan Shen, Mingchen Gao, and Jinhui Xu. Progressive voronoi diagram subdivision enables accurate data-free class-incremental learning. In In The Eleventh International Conference on Learning Representations, 2023.   \n[24] Simone Magistri, Tomaso Trinci, Albin Soutif-Cormerais, Joost van de Weijer, and Andrew D Bagdanov. Elastic feature consolidation for cold start exemplar-free incremental learning. arXiv preprint arXiv:2402.03917, 2024.   \n[25] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.   \n[26] Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost van de Weijer. Class-incremental learning: Survey and performance evaluation on image classification. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u201320, 2022.   \n[27] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109\u2013165. Elsevier, 1989.   \n[28] Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi. Learning to remember: A synaptic plasticity driven framework for continual learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11321\u201311329, 2019.   \n[29] Aristeidis Panos, Yuriko Kobe, Daniel Olmeda Reino, Rahaf Aljundi, and Richard E Turner. First session adaptation: A strong replay-free baseline for class-incremental learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 18820\u201318830, 2023.   \n[30] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 2020.   \n[31] Gr\u00e9goire Petit, Adrian Popescu, Hugo Schindler, David Picard, and Bertrand Delezoide. Fetril: Feature translation for exemplar-free class-incremental learning. In Winter Conference on Applications of Computer Vision (WACV), 2023.   \n[32] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Continual learning, fast and slow. Advances in Neural Information Processing Systems, 34:16131\u201316144, 2021.   \n[33] Sylvestre-Alvise Rebuff,i Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001\u20132010, 2017.   \n[34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \n[35] Grzegorz Rype\u00b4s\u00b4c, Sebastian Cygert, Valeriya Khan, Tomasz Trzcinski, Bartosz Micha\u0142 Zieli\u00b4nski, and Bart\u0142omiej Twardowski. Divide and not forget: Ensemble of selectively trained experts in continual learning. In The Twelfth International Conference on Learning Representations, 2023.   \n[36] Grzegorz Rype\u00b4s\u00b4c, Daniel Marczak, Sebastian Cygert, Tomasz Trzci\u00b4nski, and Bart\u0142omiej Twardowski. Category adaptation meets projected distillation in generalized continual category discovery. In European Conference on Computer Vision (ECCV), 2024.   \n[37] Wuxuan Shi and Mang Ye. Prototype reminiscence and augmented asymmetric knowledge aggregation for non-exemplar class-incremental learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1772\u20131781, 2023.   \n[38] James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen, Hongxia Jin, and Zsolt Kira. Always be dreaming: A new approach for data-free class-incremental learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9374\u20139384, 2021.   \n[39] Marco Toldo and Mete Ozay. Bring evanescent representations to life in lifelong class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16732\u201316741, 2022.   \n[40] Gido M Van de Ven, Hava T Siegelmann, and Andreas S Tolias. Brain-inspired replay for continual learning with artificial neural networks. Nature communications, 11(1):4069, 2020.   \n[41] Gido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019.   \n[42] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset, 2011.   \n[43] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 374\u2013382, 2019.   \n[44] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3014\u20133023, 2021.   \n[45] Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui, and Joost van de Weijer. Semantic drift compensation for class-incremental learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6982\u20136991, 2020.   \n[46] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. International Conference on Machine Learning (ICML), 2021.   \n[47] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International conference on machine learning, pages 3987\u20133995. PMLR, 2017.   \n[48] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13208\u201313217, 2020.   \n[49] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, and De-Chuan Zhan. Pycil: A python toolbox for classincremental learning, 2021.   \n[50] Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Deep classincremental learning: A survey. arXiv preprint arXiv:2302.03648, 2023.   \n[51] Fei Zhu, Zhen Cheng, Xu-Yao Zhang, and Cheng-lin Liu. Class-incremental learning via dual augmentation. Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[52] Fei Zhu, $\\mathrm{Xu}$ -Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu. Prototype augmentation and selfsupervision for incremental learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.   \n[53] Kai Zhu, Wei Zhai, Yang Cao, Jiebo Luo, and Zheng-Jun Zha. Self-sustaining representation expansion for non-exemplar class-incremental learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[54] Huiping Zhuang, Run He, Kai Tong, Ziqian Zeng, Cen Chen, and Zhiping Lin. Ds-al: A dual-stream analytic learning for exemplar-free class-incremental learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17237\u201317244, 2024.   \n[55] Huiping Zhuang, Zhenyu Weng, Hongxin Wei, Renchunzi Xie, Kar-Ann Toh, and Zhiping Lin. Acil: Analytic class-incremental learning with absolute memorization and privacy protection. Advances in Neural Information Processing Systems, 35:11602\u201311614, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Additional experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Adaptation results when starting from a pretrained model ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We evaluate how our adaptation method improves the quality of memorized class distributions on CUB200 [42] split into ten equal tasks when starting from a model pre-trained on ImageNet. For this purpose, we measure the average distances between memorized and real classes after each task. More precisely, we measure the L2 distance between means and covariances as well as symmetrical Kulbach-Leibler divergence $(D_{K L})$ between memorized and real distributions. We utilize projected distillation $\\langle\\lambda=10\\rangle$ ) and compare our method to a baseline that does not adapt distributions like in [51, 52] (No adapt) and to the prototype drift compensation introduced in EFC [24] that adapts means only (EFC). We provide results in Fig. 10. Results are consistent with Fig. 10 - AdaGauss improves memorized distributions after every task. ", "page_idx": 13}, {"type": "image", "img_path": "5H4l37IsZ8/tmp/974640032016b2060ebccb94ed55731cd4752a20c0cb4a17c60d752610392f79.jpg", "img_caption": ["Figure 10: L2 distances from memorized distributions to the real ones in terms of distributions\u2019 mean, covariance and KL divergence across 10 tasks on CUB-200 dataset. The feature extractor was pre-trained on ImageNet. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Impact of anti-collapse loss on optimization ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Training the feature extractor of AdaGauss combines three loss functions: cross-entropy $L_{C E}$ , knowledge distillation through a learnable projector $L_{P K D}$ , and anti-collapse term to prevent features from dimensional collapse $L_{A C}$ . We analyze average values of these losses during training of our method on ImagenetSubset (we set hyperparameters as in Tab. 1). Additionally, we modify Eq. 2 to incorporate strength of covariance regularization ( $\\beta$ hyperparameter): ", "page_idx": 13}, {"type": "equation", "text": "$$\nL_{A C}=-\\frac{1}{S}\\sum_{i=1}^{S}\\operatorname*{min}(a_{i},\\beta)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In all of our experiments, we utilized $\\beta=1$ as this was sufficient to prevent dimensional collapse.   \nHere, we test AdaGauss for $\\beta=\\{0.1,1,10,100\\}$ . ", "page_idx": 13}, {"type": "text", "text": "We present results in Fig. 11. We can see that for $\\beta=1$ , all losses are stable and consistently decrease. $L_{A C}$ decreases to -1.0, a value for which it is clipped. However, when increasing $\\beta$ , $L_{C E}$ and $L_{K D}$ become bigger, underfitting our approach. This results in much lower average and incremental accuracies. On the other hand, decreasing $\\beta$ to 0.1 is not sufficient to prevent task-recency bias resulting in decreased accuracies. ", "page_idx": 13}, {"type": "image", "img_path": "5H4l37IsZ8/tmp/aeb9ab49a348f193c5faa267535ff7cf8bd0aada7a02575e1871cf679d3c359b.jpg", "img_caption": ["Figure 11: Value of $L_{C E}$ , $L_{P K D}$ , $L_{A C}$ losses for different $\\beta$ parameters, last task average accuracy and average incremental accuracy. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.3 Tab. 1 and Tab. 2 with variance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We report the mean and variance of results reported in Tab. 1 when training from scratch in Tab. 5. Also, we report results from Tab. 2 when training from a pre-trained model in Tab. 6. Although we utilize additional anti-collapse loss compared to other methods, variance of accuracies achieved by AdaGauss is simillar to EFC. ", "page_idx": 14}, {"type": "table", "img_path": "5H4l37IsZ8/tmp/5098ec86e3a5b50c3507f00b6acc3674efbae707b71f9a0e403ff5b5c99d3afd.jpg", "table_caption": ["Table 5: Average incremental and last accuracy in EFCIL scenarios when training the feature extractor from scratch. We report means and variances of 5 runs. We denote the best results in bold. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 6: Average incremental and last accuracy in EFCIL fine-grained scenarios when utilizing a pre-trained feature extractor on ImageNet. The means and variance of 5 runs are reported with the best results in bold. ", "page_idx": 14}, {"type": "table", "img_path": "5H4l37IsZ8/tmp/0af6178db82b095ee0922a6d62b4cc6cafe557dcbd4ba8286c0731dfe5e470a0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.4 Different architecture of pretrained backbone ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We test AdaGauss with different feautre extractors, namely ViT small and ConvNext. The results, presented in Tab. 7, are for EFCIL setting with 10 and 20 equal tasks and weights pretrained on ImageNet (as in Tab. 2). Using more modern feature extractors architectures further improves the results of AdaGauss. ", "page_idx": 14}, {"type": "table", "img_path": "5H4l37IsZ8/tmp/2ea9a2432049dfbadb0cdbd7453f455129f91343ed9af8ea918dc3c44d8aa68e.jpg", "table_caption": ["Table 7: AdaGauss results with different backbone architectures. We report last accuracy | average accuracy. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.5 Half dataset results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Learning from scratch is more challenging than half-dataset setting as it requires to incrementally train feature extractor, not just the classifier. However, using the pre-trained model (or learning from half) can be considered a more practical and real-life setting. Thus, we evaluated our method with a pre-trained model in Table 2. However, we have additionally compared our method to the mentioned baselines in a half-dataset setting using the original implementations under the same data augmentations as AdaGauss. Please note that we did not have enough time to perform hyperparameter search for our method - we utilized these from the equal task setting, whereas the results for other methods were optimized by their authors. We provide results in the Tab. 8. ", "page_idx": 14}, {"type": "text", "text": "AdaGauss performs better than PASS, SSRE, and FeTrIL (5 tasks) in the half-dataset setting. However, it is slightly worse than most recent baselines when using default hyperparameters. FeCAM, ACIL, and DS-AL freeze the feature extractor after the initial task, which can explain their good results in the half-dataset training. ", "page_idx": 15}, {"type": "table", "img_path": "5H4l37IsZ8/tmp/77390e92c116375004c89639794ca5a70a54f6ea3b9416c73e607f8103dcfef0.jpg", "table_caption": ["Table 8: Half dataset in the first task results. We report last accuracy | average accuracy. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.6 Predicted semantic shift for classes ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we verify whether our adaptation network can predict different shift for different classes in experiments from Tab.1. We test on CIFAR100 for $T=10$ and the answer is positive, as shown in Fig. 12. ", "page_idx": 15}, {"type": "image", "img_path": "5H4l37IsZ8/tmp/6ffc97a7b9fa9506f97f7ba64278714313999c63937d4e449f5a23047e97716a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 12: Predicted shift for different classes on CIFAR100 (10 equal tasks) by AdaGauss. The Euclidean distance is measured between old and new position in each task. ", "page_idx": 15}, {"type": "text", "text": "A.7 Batch norm influence on AdaGauss ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We measure the accuracy achieved by AdaGauss in the EFCIL scenario from scratch (CIFAR100, ImageNetSubset) and pretrained (CUB200). We train for 10 tasks without batch norm layers and with frozen batch norm layers. Results are provided in Tab. 9. We can see that possesing batch-norm layers in Resnet18 is beneficial. ", "page_idx": 15}, {"type": "table", "img_path": "5H4l37IsZ8/tmp/b84e134f2404e70a53a299e6228cfe80e126f36994b836e6a173f1ae8dfe8ea8.jpg", "table_caption": ["Table 9: AdaGauss results without or with frozen batch-norm. We report last accuracy and average accuracy separated by |. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Yes, main claims accurately reflect contributions and scope of our paper. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: Yes, we discuss limitations is Section 5. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: We do not include theoretical results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We discuss reproducibility in Sec. 4. We enclose code and scripts to reproduce results. We will also publish them upon acceptance of the manuscript. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We enclose code and scripts to reproduce results. We will also publish them on Github upon acceptance of the manuscript. We utilize open source datasets and libraries. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We discuss it in Sec. 4 and enclose scripts in the supplementary material. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We report error bars on our plots. We repeat experiments 5 times. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We discuss it in Sec. 4 ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Yes, we have read and applied NeurIPS Code of Ethics. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our work considers fundamental research in EFCIL, we do not see its connection to societal impact. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We do not release data nor models. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Yes, we cite creators or original owners of assets we utilize in our work. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we provide documentation of the code we submit. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: We do not perform crowdsourcing experiments and research with human subjects. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: We do not involve crowdsourcing nor research with human subjects. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]