[{"figure_path": "5H4l37IsZ8/figures/figures_1_1.jpg", "caption": "Figure 1: Latent space visualization, average accuracy after the last task, and symmetrical KL divergence between memorized and ground truth distributions for ResNet18 trained sequentially on ImagenetSubset dataset split into ten tasks. Freezing the feature extractor prevents changes in data distribution but results in inseparable classes. When the network is trained on incremental tasks (unfrozen), the ground truth distributions change and do not match the memorized ones. A suitable CL method should adapt the mean and covariance of distributions to retain valid decision boundaries.", "description": "This figure visualizes the latent space representation of classes in a class incremental learning setting using ResNet18 on the ImagenetSubset dataset.  It compares four scenarios: a frozen feature extractor (no adaptation), an unfrozen feature extractor with only mean adaptation, an unfrozen feature extractor with mean and covariance adaptation, and the ground truth distributions. It highlights the impact of adapting covariances on class separability and overall accuracy.  The frozen model demonstrates significant class overlap. The unfrozen models show a drift in distributions between the ground truth and the model's learned distributions, underscoring the need for methods which adapt both mean and covariance.", "section": "3.2 The three observations that motivate towards AdaGauss"}, {"figure_path": "5H4l37IsZ8/figures/figures_3_1.jpg", "caption": "Figure 1: Latent space visualization, average accuracy after the last task, and symmetrical KL divergence between memorized and ground truth distributions for ResNet18 trained sequentially on ImagenetSubset dataset split into ten tasks. Freezing the feature extractor prevents changes in data distribution but results in inseparable classes. When the network is trained on incremental tasks (unfrozen), the ground truth distributions change and do not match the memorized ones. A suitable CL method should adapt the mean and covariance of distributions to retain valid decision boundaries.", "description": "The figure visualizes the effect of training on incremental tasks on the distribution of data points in latent space.  When the feature extractor is frozen, the distributions of classes do not change, but they become inseparable. When it's unfrozen, the distributions change, but existing methods do not adapt to those changes leading to classification errors. Therefore, a suitable continual learning method should adapt the mean and covariance of distributions to maintain valid decision boundaries.", "section": "3.2 The three observations that motivate towards AdaGauss"}, {"figure_path": "5H4l37IsZ8/figures/figures_7_1.jpg", "caption": "Figure 5: Distances from memorized distributions to the real ones in terms of distributions' mean, covariance and KL divergence across 10 tasks on ImagenetSubset dataset. AdaGauss greatly reduces errors and allows for better adaptation than prototype drift compensation (EFC).", "description": "The figure visualizes the performance of three different methods (No adapt, EFC, and AdaGauss) in adapting Gaussian distributions across ten incremental tasks on the ImagenetSubset dataset.  The three plots show the L2 distance between the memorized and real means, the L2 distance between the memorized and real covariance matrices, and the Kullback-Leibler (KL) divergence between the memorized and real distributions, respectively. AdaGauss consistently outperforms the other two methods, showcasing its effectiveness in adapting both the means and covariance matrices of Gaussian distributions, leading to more accurate representation of the learned classes and improved performance in continual learning.", "section": "4.2 Adaptation results"}, {"figure_path": "5H4l37IsZ8/figures/figures_8_1.jpg", "caption": "Figure 6: Distribution of eigenvalues of class representations for our method with and without LAC (anti-collapse loss) term. LAC greatly reduces the difference between the most and least significant eigenvalues, thus preventing dimensional collapse.", "description": "This figure visualizes the impact of the anti-collapse loss (LAC) on the distribution of eigenvalues of class representations.  The left plot shows the eigenvalue distribution for the method with and without LAC.  The plot shows that without LAC, there's a large gap between the largest and smallest eigenvalues, indicating a dimensionality collapse, meaning the representations are concentrated in a small number of dimensions.  With LAC, the distribution is significantly more uniform across all dimensions, demonstrating that LAC successfully prevents this collapse by ensuring more eigenvector contribute to representations.", "section": "3.2 The three observations that motivate towards AdaGauss"}, {"figure_path": "5H4l37IsZ8/figures/figures_8_2.jpg", "caption": "Figure 2: The representational strength of ResNet18 trained on 10 tasks of ImagenetSubset dataset split into 10 tasks for different knowledge distillation methods. After each task, we measure how many eigenvalues sum to 95% variance of all features provided.", "description": "This figure visualizes how the representation strength of a ResNet18 model changes across ten incremental tasks of the ImageNetSubset dataset when trained with different knowledge distillation methods (none, feature, logit, projected).  The x-axis represents the task number, and the y-axis shows the number of eigenvalues required to capture 95% of the total variance in the feature extractor's output.  The graph demonstrates that different knowledge distillation techniques lead to varying representational strengths throughout the training process, which has implications for the effectiveness of the method.", "section": "3.2 The three observations that motivate towards AdaGauss"}, {"figure_path": "5H4l37IsZ8/figures/figures_13_1.jpg", "caption": "Figure 5: Distances from memorized distributions to the real ones in terms of distributions' mean, covariance and KL divergence across 10 tasks on ImagenetSubset dataset. AdaGauss greatly reduces errors and allows for better adaptation than prototype drift compensation (EFC).", "description": "This figure visualizes the performance of AdaGauss in adapting class distributions compared to two other methods: one without adaptation and EFC (which only adapts means).  The three graphs show the L2 distance between memorized and real means, the L2 distance between memorized and real covariance matrices, and the Kullback-Leibler (KL) divergence between memorized and real distributions, respectively, across ten incremental tasks on the ImagenetSubset dataset.  The results demonstrate that AdaGauss significantly reduces the discrepancy between memorized and actual distributions compared to the other methods, highlighting its superior ability to adapt class distributions in continual learning.", "section": "4.2 Adaptation results"}, {"figure_path": "5H4l37IsZ8/figures/figures_13_2.jpg", "caption": "Figure 1: Latent space visualization, average accuracy after the last task, and symmetrical KL divergence between memorized and ground truth distributions for ResNet18 trained sequentially on ImagenetSubset dataset split into ten tasks. Freezing the feature extractor prevents changes in data distribution but results in inseparable classes. When the network is trained on incremental tasks (unfrozen), the ground truth distributions change and do not match the memorized ones. A suitable CL method should adapt the mean and covariance of distributions to retain valid decision boundaries.", "description": "This figure visualizes the effect of training a ResNet18 model sequentially on the ImagenetSubset dataset, divided into 10 tasks.  The leftmost column shows the actual data points from past classes. The rest shows memorized distributions (how the model remembers the distributions) in the feature extractor's latent space with different training approaches.  Freezing the feature extractor maintains past distributions unchanged but results in class inseparability. Unfreezing it with no adaptation shows the shift in ground truth distributions, highlighting the need for adaptation of means and covariances (shown in the rightmost columns) to maintain accurate class boundaries.", "section": "3.2 The three observations that motivate towards AdaGauss"}, {"figure_path": "5H4l37IsZ8/figures/figures_15_1.jpg", "caption": "Figure 5: Distances from memorized distributions to the real ones in terms of distributions' mean, covariance and KL divergence across 10 tasks on ImagenetSubset dataset. AdaGauss greatly reduces errors and allows for better adaptation than prototype drift compensation (EFC).", "description": "The figure visualizes the effectiveness of AdaGauss in adapting class distributions compared to methods that don't adapt (No adapt) and only adapt means (EFC).  It shows the L2 distance between memorized and real class means, the L2 distance between memorized and real class covariances, and the Kullback-Leibler (KL) divergence between the distributions.  AdaGauss consistently shows smaller distances, indicating better adaptation of class distributions across multiple tasks, demonstrating its superiority in handling distribution drift.", "section": "4.2 Adaptation results"}]