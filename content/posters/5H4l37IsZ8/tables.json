[{"figure_path": "5H4l37IsZ8/tables/tables_6_1.jpg", "caption": "Table 1: Average incremental and last accuracy in EFCIL when training the feature extractor from scratch. The mean of 5 runs is reported. Full results are in Tab. 5. We denote the best results in bold.", "description": "This table presents the average incremental and last accuracy achieved by different continual learning methods on three benchmark datasets (CIFAR-100, TinyImageNet, and ImagenetSubset).  The results are obtained when training the model's feature extractor from scratch, not using pre-trained weights.  The table shows the performance for both 10 and 20 incremental tasks, reporting the average accuracy across all tasks (Ainc) and the accuracy after the last task (Alast). The best results for each dataset and task setting are highlighted in bold, providing a direct comparison of the AdaGauss method against several baselines.", "section": "4 Experiments"}, {"figure_path": "5H4l37IsZ8/tables/tables_6_2.jpg", "caption": "Table 2: Average incremental and last accuracy in EFCIL fine-grained scenarios when utilizing a pre-trained feature extractor. We report the mean of 5 runs, while variances are reported in Tab. 6.", "description": "This table presents the average incremental and last accuracy results for different continual learning methods on two fine-grained image classification datasets (CUB200 and FGVCAircraft).  The experiments use a pre-trained feature extractor, highlighting the performance of each method when adapting to new tasks without access to previous data. The table shows results for different numbers of tasks (T=5, T=10, T=20), comparing the average accuracy (Ainc) and the accuracy on the last task (Alast).", "section": "4 Experiments"}, {"figure_path": "5H4l37IsZ8/tables/tables_7_1.jpg", "caption": "Table 3: Ablation of AdaGauss indicating the contribution from the different components. * signifies that we utilized covariance matrix shrinking with the value of 0.5 (chosen on the validation set) instead of anti-collapse loss to overcome the covariance matrix singularity problem.", "description": "This table presents the ablation study of the AdaGauss method, showing the impact of different components on the performance. It demonstrates the necessity of adapting means and covariances, utilizing the Bayes classifier, and employing the anti-collapse loss for optimal results. The results are presented for CIFAR-100 and ImagenetSubset datasets with 10 tasks each.", "section": "4 Experiments"}, {"figure_path": "5H4l37IsZ8/tables/tables_9_1.jpg", "caption": "Table 1: Average incremental and last accuracy in EFCIL when training the feature extractor from scratch. The mean of 5 runs is reported. Full results are in Tab. 5. We denote the best results in bold.", "description": "This table presents the average incremental and last accuracy for different continual learning methods on CIFAR-100, TinyImageNet, and ImagenetSubset datasets when the feature extractor is trained from scratch.  The results are shown for 10 and 20 tasks, each containing an equal number of classes. The best-performing methods are highlighted in bold, indicating the superior performance of AdaGauss, the proposed method, compared to other state-of-the-art approaches.", "section": "4 Experiments"}, {"figure_path": "5H4l37IsZ8/tables/tables_14_1.jpg", "caption": "Table 1: Average incremental and last accuracy in EFCIL when training the feature extractor from scratch. The mean of 5 runs is reported. Full results are in Tab. 5. We denote the best results in bold.", "description": "This table presents the average incremental and last accuracy achieved by different continual learning methods on three benchmark datasets (CIFAR-100, TinyImageNet, and ImageNetSubset) when training the feature extractor from scratch.  The results are shown for two different numbers of tasks (T=10 and T=20).  The best performing method for each metric and dataset is highlighted in bold.  The table provides a summary of the performance, with more detailed results presented in Table 5.", "section": "4 Experiments"}, {"figure_path": "5H4l37IsZ8/tables/tables_14_2.jpg", "caption": "Table 2: Average incremental and last accuracy in EFCIL fine-grained scenarios when utilizing a pre-trained feature extractor. We report the mean of 5 runs, while variances are reported in Tab. 6.", "description": "This table presents the average incremental and last accuracy results for different continual learning methods on two fine-grained datasets (CUB200 and FGVCAircraft) when using a pre-trained feature extractor.  The results are broken down by the number of tasks (T=5, T=10, T=20) and show the average performance across five runs.  Variances for these results are provided in a separate table (Table 6).  The table compares the performance of AdaGauss to several other state-of-the-art continual learning methods.", "section": "4 Experiments"}, {"figure_path": "5H4l37IsZ8/tables/tables_14_3.jpg", "caption": "Table 7: AdaGauss results with different backbone architectures. We report last accuracy | average accuracy.", "description": "This table presents the results of the AdaGauss method using different backbone architectures (ResNet18, ConvNext (small), and ViT (small)) for the CUB200 and FGVCAircraft datasets.  The experiments were conducted with 10 and 20 equal tasks, and the models used weights pre-trained on ImageNet.  The table shows the last accuracy and average accuracy achieved by AdaGauss for each combination of dataset, task number, and architecture. This allows for a comparison of the performance of AdaGauss across different architectural choices.", "section": "A.4 Different architecture of pretrained backbone"}, {"figure_path": "5H4l37IsZ8/tables/tables_15_1.jpg", "caption": "Table 8: Half dataset in the first task results. We report last accuracy | average accuracy.", "description": "This table presents the last accuracy and average accuracy for different continual learning methods on CIFAR100 and ImageNetSubset datasets when training is performed only on half of the data.  It compares the performance of AdaGauss against other state-of-the-art methods in this specific setting.", "section": "4.1 Results"}, {"figure_path": "5H4l37IsZ8/tables/tables_15_2.jpg", "caption": "Table 9: AdaGauss results without or with frozen batch-norm. We report last accuracy and average accuracy separated by |.", "description": "This table presents the ablation study results on the impact of batch normalization on AdaGauss performance. It compares the last accuracy and average accuracy of AdaGauss when trained with no batch normalization, with frozen batch normalization, and with standard batch normalization (Resnet18). The results are shown for CIFAR100, ImageNetSubset, and CUB200 datasets.", "section": "4. Results"}]