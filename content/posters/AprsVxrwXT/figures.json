[{"figure_path": "AprsVxrwXT/figures/figures_0_1.jpg", "caption": "Figure 1: MVGamba is a unified 3D generation framework build on Gaussian Splatting, which can generate high-quality 3D contents in a feed-forward manner in sub-seconds.", "description": "This figure shows the overall architecture of MVGamba, a unified 3D content generation framework.  It highlights the framework's ability to generate high-quality 3D models from various input types: single images, multiple views (sparse-view reconstruction), and text prompts.  The core of the framework is built upon Gaussian splatting, which allows for fast and efficient 3D model generation.", "section": "Abstract"}, {"figure_path": "AprsVxrwXT/figures/figures_2_1.jpg", "caption": "Figure 2: (a) Previous Gaussian reconstruction models sacrifice the integrity of multi-view information for computationally intensive architectures, resulting in multi-view inconsistency and blurred textures. (b) Comparison of FLOPs between self-attention in Transformers and SSM in Mamba.", "description": "The figure illustrates the limitations of previous Gaussian reconstruction models and introduces the efficiency of Mamba. In (a), the comparison shows that prior models compromised multi-view information integrity for computational efficiency, leading to inconsistent and blurry results. In contrast, (b) demonstrates that Mamba achieves linear complexity, significantly reducing computational costs compared to the quadratic complexity of Transformers.", "section": "2 Related Work"}, {"figure_path": "AprsVxrwXT/figures/figures_3_1.jpg", "caption": "Figure 3: (a) Multi-view Gaussian Reconstructor (Sec. 3.2): Multi-view inputs with ray embedding are used for causal sequence modeling, predicting Gaussians rendered at novel views and supervised with ground truth images. (b) Unified inference pipeline (Sec. 3.4): MVGamba combines multi-view diffusion models and Gaussian reconstructor to generate high-quality 3D content in sub-seconds.", "description": "This figure illustrates the architecture of the MVGamba model.  (a) shows the multi-view Gaussian reconstructor, which takes multi-view images as input, processes them using a causal sequence modeling approach based on Mamba blocks, and predicts the parameters of 3D Gaussians.  Novel view supervision and differentiable rendering are used during training. (b) shows the unified inference pipeline, which combines multi-view diffusion models (MVDream and ImageDream) with the multi-view Gaussian reconstructor to generate 3D content from single images or text prompts. The output is then converted into a mesh via TSDF.", "section": "3 Method"}, {"figure_path": "AprsVxrwXT/figures/figures_6_1.jpg", "caption": "Figure 4: Qualitative comparison in image-to-3D and text-to-3D generation. Please refer to Appendix C for more generation results.", "description": "This figure shows a qualitative comparison of the 3D model generation results of four different methods: DreamGaussian, Triplane-Gaussian, LGM, and MVGamba.  Each row represents a different input (either a single image or a text prompt).  The figure demonstrates that MVGamba produces 3D models with higher visual fidelity and better overall quality than the other methods.  The Appendix C contains additional results.", "section": "4.2 Comparison against Baselines"}, {"figure_path": "AprsVxrwXT/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative results in sparse-view reconstruction. Given four views as input, MVGamba effectively reconstructs both the geometric structure and detailed textures.", "description": "This figure shows the results of sparse-view reconstruction using MVGamba.  The top row displays four input views of a garden gnome. The bottom row shows four input views of a mango juice box. In each case, MVGamba successfully reconstructs the 3D model from the limited input views, generating novel views that accurately represent the object's geometry and texture. This demonstrates the model's ability to reconstruct detailed 3D models from a sparse set of input views, showcasing its effectiveness in handling incomplete visual information.", "section": "4 Experiment"}, {"figure_path": "AprsVxrwXT/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative comparison in image-to-3D and text-to-3D generation. Please refer to Appendix C for more generation results.", "description": "This figure compares the image-to-3D and text-to-3D generation results of MVGamba against several other state-of-the-art methods.  Different input prompts (single image or text) and the corresponding 3D model outputs are shown for each method. This allows a visual comparison of the quality and fidelity of the 3D models generated by each approach.  More detailed results can be found in Appendix C of the paper.", "section": "4 Experiment"}, {"figure_path": "AprsVxrwXT/figures/figures_8_2.jpg", "caption": "Figure 7: (a) Worst-case simulation of the inconsistency introduced by the multi-view diffusion model. (b) The effect of sequence length on 3D reconstruction.", "description": "The figure shows two experiments conducted to diagnose the progress of MVGamba. (a) shows a worst-case simulation to test the robustness of MVGamba to multi-view input inconsistency. It compares the results of MVGamba's causal sequence prediction with a merge operation method, demonstrating that the causal model is more robust to noise. (b) illustrates the effect of varying the length of the Gaussian sequence on 3D reconstruction performance. It shows that the model's performance improves with increasing sequence length. These findings support the claims made in the paper about MVGamba's effectiveness.", "section": "Ablation and Discussion"}, {"figure_path": "AprsVxrwXT/figures/figures_9_1.jpg", "caption": "Figure 8: Ablation study on input order. Top: MVGamba may fail if the depth of the front-view is estimated incorrectly and the front-view is given first. Bottom: Manually changing the input order to provide the side-view first allows MVGamba to generate satisfactory 3D content, as the side-view contains sufficient depth information.", "description": "This figure shows an ablation study on the input order for MVGamba. The top row demonstrates that if the depth of the front view is incorrectly estimated and the front view is input first, the model may fail to generate satisfactory 3D content.  The bottom row shows that changing the input order to prioritize the side view (which contains sufficient depth information) enables MVGamba to generate satisfactory 3D content. This highlights the importance of input order and depth estimation accuracy for optimal performance.", "section": "5 Ablation and Discussion"}, {"figure_path": "AprsVxrwXT/figures/figures_17_1.jpg", "caption": "Figure 4: Qualitative comparison in image-to-3D and text-to-3D generation. Please refer to Appendix C for more generation results.", "description": "This figure displays a qualitative comparison of the image-to-3D and text-to-3D generation capabilities of MVGamba against several state-of-the-art baselines.  Each row represents a different input (an image or a text prompt), and the columns show the results of different methods. The results demonstrate that MVGamba produces higher-quality results, especially in terms of detail and consistency across different viewpoints.  Appendix C contains further results.", "section": "4.2 Comparison against Baselines"}, {"figure_path": "AprsVxrwXT/figures/figures_18_1.jpg", "caption": "Figure 4: Qualitative comparison in image-to-3D and text-to-3D generation. Please refer to Appendix C for more generation results.", "description": "This figure shows a qualitative comparison of the 3D models generated by MVGamba and several other state-of-the-art methods, including DreamGaussian, Triplane-Gaussian, and LGM. For each method, the figure shows several examples of 3D models generated from either a single image or text prompt. The figure demonstrates that MVGamba is able to generate high-quality 3D models that are comparable in quality to the other methods, and in some cases, even superior in quality. The figure also shows the variety of different 3D models that MVGamba can generate, demonstrating the versatility of the approach.", "section": "4.2 Comparison against Baselines"}, {"figure_path": "AprsVxrwXT/figures/figures_18_2.jpg", "caption": "Figure 4: Qualitative comparison in image-to-3D and text-to-3D generation. Please refer to Appendix C for more generation results.", "description": "This figure presents a qualitative comparison of the 3D models generated by MVGamba and other state-of-the-art methods. The comparison is done for both image-to-3D and text-to-3D generation tasks, showing the results for various inputs such as single images and text prompts.  The figure showcases the visual differences in the quality and details of the generated 3D models, allowing for a visual assessment of the performance of MVGamba compared to its counterparts.", "section": "4.2 Comparison against Baselines"}, {"figure_path": "AprsVxrwXT/figures/figures_19_1.jpg", "caption": "Figure 4: Qualitative comparison in image-to-3D and text-to-3D generation. Please refer to Appendix C for more generation results.", "description": "This figure presents a qualitative comparison of 3D model generation results from different methods (DreamGaussian, Triplane-Gaussian, LGM, and MVGamba).  It showcases the visual quality of generated 3D models from both single images and text prompts. The figure highlights that MVGamba achieves better results compared to other methods in terms of generating high fidelity and detailed 3D content from different input types. More detailed results can be found in Appendix C. ", "section": "4.2 Comparison against Baselines"}, {"figure_path": "AprsVxrwXT/figures/figures_19_2.jpg", "caption": "Figure 3: (a) Multi-view Gaussian Reconstructor (Sec. 3.2): Multi-view inputs with ray embedding are used for causal sequence modeling, predicting Gaussians rendered at novel views and supervised with ground truth images. (b) Unified inference pipeline (Sec. 3.4): MVGamba combines multi-view diffusion models and Gaussian reconstructor to generate high-quality 3D content in sub-seconds.", "description": "This figure shows the architecture of the MVGamba model. (a) illustrates the multi-view Gaussian reconstructor which takes multi-view images as input, processes them through a causal sequence modeling approach, and predicts Gaussians for novel views.  (b) shows the unified inference pipeline where multi-view diffusion models are used to generate initial multi-view images which are fed to the multi-view Gaussian reconstructor to produce final high-quality 3D content in a short timeframe.", "section": "3 Method"}]