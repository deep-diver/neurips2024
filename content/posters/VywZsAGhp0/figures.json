[{"figure_path": "VywZsAGhp0/figures/figures_2_1.jpg", "caption": "Figure 1: SMV for node groups of different degrees. More results are shown in Appendix C.", "description": "This figure displays the Smoothness Metric Value (SMV) for nodes grouped by their degrees, using the GAT model with varying numbers of layers (2, 4, 8, 16, 32, 64).  The SMV measures how similar node representations become as the number of layers increase in a GNN.  The results show the relationship between node degree and over-smoothing.  Nodes with higher degrees tend to show higher SMV values, indicating higher over-smoothing, as their neighborhood subgraphs have more overlap. This observation is used to support the paper's argument on over-smoothing from the perspective of overlapping neighborhood subgraphs.  Appendix C contains additional results for other models and datasets.", "section": "3.1 Revisit over-smoothing from the perspective of neighborhood subgraphs overlapping"}, {"figure_path": "VywZsAGhp0/figures/figures_4_1.jpg", "caption": "Figure 2: The framework of PSNR Module.", "description": "This figure shows the architecture of the Posteriori-Sampling-based Node-Adaptive Residual module (PSNR).  The PSNR module takes the output of a previous GNN layer (Hk-1) as input. A GNN layer processes this input to produce H\u2019k-1. These are then fed into an encoder along with the adjacency matrix (A) and positional embedding (LayerEmb(k)) to obtain the posterior distribution of the residual coefficients (\u03b7k). This distribution is then sampled to obtain node-adaptive residual coefficients which are then used to compute the final output (Hk) via a residual connection with the initial input.", "section": "4 The Proposed Method PSNR"}, {"figure_path": "VywZsAGhp0/figures/figures_7_1.jpg", "caption": "Figure 3: Different residual methods' effectiveness in mitigating over-smoothing.", "description": "The figure shows the effectiveness of different residual methods in mitigating over-smoothing in deep GNNs. The x-axis represents the number of layers and the y-axis represents the accuracy.  The results demonstrate that the PSNR module consistently outperforms other methods in maintaining stable performance even at significantly deeper layers (64 layers). This is because PSNR effectively mitigates the loss of high-order neighborhood subgraph information, which is crucial for maintaining performance in deep GNNs.", "section": "5.3 Effectiveness in mitigating over-smoothing (RQ1)"}, {"figure_path": "VywZsAGhp0/figures/figures_15_1.jpg", "caption": "Figure 1: SMV for node groups of different degrees. More results are shown in Appendix C.", "description": "This figure shows the Smoothness Metric Value (SMV) for different node degree groups across various layers of GCN and GAT models.  Higher SMV indicates more over-smoothing.  It visually demonstrates that nodes with higher degrees exhibit more over-smoothing, supporting the paper's claim that over-smoothing is related to overlapping high-order neighborhood subgraphs.", "section": "3.1 Revisit over-smoothing from the perspective of neighborhood subgraphs overlapping"}]