[{"type": "text", "text": "Inversion-based Latent Bayesian Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jaewon Chu,\u2217 Jinyoung Park,\u2217 Seunghun Lee, Hyunwoo J. Kim\u2020 Computer Science & Engineering Korea University   \n{allonsy07, lpmn678, llsshh319, hyunwoojkim}@korea.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Latent Bayesian optimization (LBO) approaches have successfully adopted Bayesian optimization over a continuous latent space by employing an encoderdecoder architecture to address the challenge of optimization in a high dimensional or discrete input space. LBO learns a surrogate model to approximate the black-box objective function in the latent space. However, we observed that most LBO methods suffer from the \u2018misalignment problem\u2019, which is induced by the reconstruction error of the encoder-decoder architecture. It hinders learning an accurate surrogate model and generating high-quality solutions. In addition, several trust region-based LBO methods select the anchor, the center of the trust region, based solely on the objective function value without considering the trust region\u2019s potential to enhance the optimization process. To address these issues, we propose Inversion-based Latent Bayesian Optimization (InvBO), a plug-and-play module for LBO. InvBO consists of two components: an inversion method and a potential-aware trust region anchor selection. The inversion method searches the latent code that completely reconstructs the given target data. The potential-aware trust region anchor selection considers the potential capability of the trust region for better local optimization. Experimental results demonstrate the effectiveness of InvBO on nine real-world benchmarks, such as molecule design and arithmetic expression ftiting tasks. Code is available at https://github.com/mlvlab/InvBO. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian optimization (BO) has been used in a wide range of applications such as material science [1], chemical design [2, 3], ,and hyperparameter optimization [4, 5]. The main idea of BO is probabilistically estimating the expensive black-box objective function using a surrogate model to find the optimal solution with minimum objective function evaluation. While BO has shown its success on continuous domains, applying BO over discrete input space is challenging [6, 7]. To address it, Latent Bayesian Optimization (LBO) has been proposed [8\u201314]. LBO performs BO over a latent space by mapping the discrete input space into the continual latent space with generative models such as Variational Auto Encoders (VAE) [15], consisting of an encoder $q_{\\phi}$ and a decoder $p_{\\theta}$ . Unlike the standard BO, the surrogate model in LBO associates a latent vector $\\mathbf{z}$ with an objective function value by emulating the composition of the objective function and the decoder of VAE. ", "page_idx": 0}, {"type": "text", "text": "In LBO, however, the reconstruction error of the VAE often leads to one latent vector ${\\bf z}$ being associated with two different objective function values as explained in Figure 1. We observe that the discrepancy between $y$ and $y^{\\prime}$ (or $\\mathbf{x}$ and $\\mathbf{x}^{\\prime}$ ) hinders learning an accurate surrogate model $g$ and generating high-quality solutions. We name this the \u2018misalignment problem\u2019. Most prior works [9, 11, 12] use the surrogate model $g^{\\mathrm{enc}}$ , which is trained with the encoder triplet $(\\mathbf{x},\\mathbf{z},y)$ , and generate solutions $\\mathbf{x}^{\\prime}$ via decoder $p_{\\theta}$ . Since $g^{\\mathrm{enc}}$ fails to estimate the composite function of $p_{\\theta}$ and $f$ , this approach often results in suboptimal outcomes. Some works [13, 14] handle the misalignment problem by employing the surrogate model $g^{\\mathrm{dec}}$ trained with the decoder triplet $(\\mathbf{x}^{\\prime},\\mathbf{z},y^{\\prime})$ . However, they request a huge amount of additional oracle calls to obtain the decoder triplet, which leads to inefficient optimization. In addition, several existing LBO methods [13, 14, 16] adopt the trust region method to restrict the search space and have shown performance gain. Most prior works select the anchor, the center of the trust region, as the current optimal point. This objective function value-based anchor selection overlooks the potential to benefti the optimization performance of the latent vectors within the trust region. ", "page_idx": 0}, {"type": "image", "img_path": "TrN5TcWY87/tmp/aea8d8c1221c9247016223e97241d439255ee99a72ae9c839f40cc199f904fe1.jpg", "img_caption": ["Figure 1: Misalignment problem. In LBO, a latent vector z can be associated with two function values $y$ and $y^{\\prime}$ due to the reconstruction error of the VAE, i.e., $\\mathbf{x}\\neq\\mathbf{x}^{\\prime}$ . (a) In Encoder triplet $(\\mathbf{x},\\mathbf{z},y)$ , latent vector $\\mathbf{z}$ is associated with $f(\\mathbf{x})$ , where $\\mathbf{x}$ is the original input to the encoder, i.e., $\\mathbf{z}=q_{\\phi}(\\mathbf{x})$ . (b) In Decoder triplet $(\\mathbf{x}^{\\prime},\\mathbf{z},y^{\\prime})$ , ${\\bf z}$ is associated with $y^{\\prime}=f(\\mathbf{x}^{\\prime})$ , which is the objective function value of reconstructed input value $\\mathbf{x}^{\\prime}$ using the decoder, i.e., $\\mathbf{x}^{\\prime}=p_{\\theta}(\\mathbf{z})$ . The discrepancy between $y$ and $y^{\\prime}$ hinders learning the accurate surrogate model $g$ . We name this the \u2018misalignment problem\u2019. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we propose an Inversion-based Latent Bayesian Optimization (InvBO), a plug-and-play module for VAE-based LBO methods. InvBO consists of two components: the inversion method and a potential-aware trust region anchor selection. The inversion method addresses the misalignment problem by inverting decoder $p_{\\theta}$ to find the latent code that yields $\\mathbf{x}$ without any additional oracle call. We theoretically analyze that our inversion method decreases the upper bound of the error between the surrogate model and the objective function within the trust region. The potential-aware trust region anchor selection method selects the anchor considering not only the observed objective function value but also the potential to enhance the optimization process of the latent vectors that the trust region contains. We provide the experimental evaluation on nine different tasks, Guacamol, DRD3, and arithmetic expression ftiting task to show the general effectiveness of InvBO. Specifically, plug-and-play results of InvBO over diverse prior LBO works show a large performance gain and achieved state-of-the-art performance. ", "page_idx": 1}, {"type": "text", "text": "The contributions of our paper are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose the inversion method to address the misalignment problem in LBO by generating the decoder triplet without using any additional oracle calls.   \n\u2022 We propose the potential-aware trust region anchor selection, aiming to select the centers of trust regions considering the latent vectors expected to benefit the optimization process within the trust regions.   \n\u2022 By combining the inversion method and potential-aware trust region anchor selection, we propose Inversion-based Latent Bayesian Optimization (InvBO), a novel plug-and-play module for LBO, and achieve state-of-the-art performance on the nine different tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Latent Bayesian Optimization ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The goal of Latent Bayesian Optimization (LBO) [8, 9, 11\u201314, 16\u201319] is to learn a latent space to enable optimization over a continuous space from discrete or structured input (e.g., graph or image). LBO consists of a Variational AutoEncoder (VAE) to generate data from the latent representation and a surrogate model (e.g., Gaussian process) to map the latent representation into the objective score. Some works on the LBO have designed new decoder architectures [8, 20\u201323] to perform the reconstruction better, while other works have proposed learning mechanisms [9\u201314, 17] to alleviate the discrepancy between the latent space and input space. LOL-BO [13] adapts the concept of trust-region to the latent space and jointly learns a VAE and a surrogate model to search the data point in the local region. CoBO [14] designs new loss to encourage the correlation between the distances in the latent space and objective function. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Inversion in Generative Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Inversion has widely been applied to a variety of generative models such as Generative Adversarial Networks (GANs) [24, 25] and Diffusion models [26\u201329]. Inversion is the process of finding the latent code $\\mathbf{z}_{\\mathrm{inv}}$ of a given image to manipulate images with generative models. Formally, given an image $\\mathbf{x}$ and the well-trained generator $G$ , the inversion can be written as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{z}_{\\mathrm{inv}}=\\underset{\\mathbf{z}\\in\\mathcal{Z}}{\\arg\\operatorname*{min}}\\;d_{\\mathcal{X}}(G(\\mathbf{z}),\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $d_{\\mathcal{X}}(\\cdot,\\cdot)$ denotes the distance metric in the image space $\\mathcal{X}$ , and $\\mathcal{Z}$ is the latent space. To solve Eq. (1), most inversion-based works can be generally classified as two approaches: optimizationbased and learning-based methods. The optimization-based inversion [30\u201333] iteratively finds a latent vector to reconstruct the target image $\\mathbf{x}$ through the fixed generator. The learning-based inversion [25, 34, 35] trains the encoder for mapping the image $\\mathbf{x}$ to the latent code ${\\bf z}$ while fixing the decoder. In this work, we introduce the concept of inversion to find the latent vector that can generate a desired sample for constructing an aligned triplet and we use the optimization-based inversion. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Bayesian optimization. Bayesian optimization (BO) is a powerful and sample-efficient optimization algorithm that aims at searching the input $\\mathbf{x}$ with a maximum objective value $f(\\mathbf{x})$ , which is formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}^{*}=\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\arg\\operatorname*{max}}\\,f(\\mathbf{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the black-box objective function $f:\\,x\\mapsto\\,y$ is assumed expensive to evaluate, and $\\mathcal{X}$ is a feasible set. Since the objective function $f$ is unknown or cost-expensive, BO methods probabilistically emulate the objective function by a surrogate model $g$ with observed dataset $\\dot{\\mathcal{D}}\\,=\\,\\{(\\mathbf{x}^{i},y^{i})\\vert\\dot{y}^{i}\\,=\\,f(\\mathbf{x}^{i})\\}_{i=1}^{n}$ . With the surrogate model $g$ , the acquisition function $\\alpha$ selects the most promising point $\\mathbf{x}^{n+1}$ as the next evaluation point while balancing exploration and exploitation. BO repeats this process until the oracle budget is exhausted. ", "page_idx": 2}, {"type": "text", "text": "Trust region-based local Bayesian optimization. Classical Bayesian optimization methods often suffer from the difficulty of the optimization in a high dimensional space [36]. To address this problem, TuRBO [36] adopts trust regions to limit the search space to small regions. The anchor (center) of trust region $\\tau$ is selected as a current optimal point, and the size of the trust region is scheduled during the optimization process. At the beginning of the optimization, the side length of all trust regions is set to $L_{\\mathrm{init}}$ . When the trust region $\\tau$ updates the best score $\\tau_{\\mathrm{succ}}$ times in a row, the side length becomes twice until it reaches $L_{\\mathrm{max}}$ . Similarly, when it fails to update the best score $\\tau_{\\mathrm{fail}}$ times in a row, the side length becomes half. When $L$ falls below a $L_{\\mathrm{min}}$ , the side length of the trust region is set to $L_{\\mathrm{init}}$ and restart the scheduling. Recently, LOL-BO [13] adapted trust region-based local optimization to LBO, and has shown performance gain. ", "page_idx": 2}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present an Inversion-based Latent Bayesian Optimization (InvBO) consisting of an inversion and a novel trust region anchor selection method for effective and efficient optimization. We first describe latent Bayesian optimization and the misalignment problem of it (Section 4.1). Then, we introduce the inversion method to address the misalignment problem without using any additional oracle budgets (Section 4.2). Lastly, we present a potential-aware trust region anchor selection for better local search space (Section 4.3). ", "page_idx": 2}, {"type": "image", "img_path": "TrN5TcWY87/tmp/ef385bc29c4d045d73f0088f4bf537748553fedee02fd8347fbc361b27ff37dc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Comparison of solutions to the misalignment problem. (a) Some works [13, 14] solve the misalignment problem by the recentering technique that generates the aligned triplet $(\\mathbf{x}^{\\prime},\\mathbf{z},y^{\\prime})$ . However, it requests additional oracle calls as $y^{\\prime}=f(\\mathbf{x}^{\\prime})$ is unevaluated, and does not fully use the evaluated function value $y=f(\\mathbf{x})$ . (b) The inversion method (ours) aims to find $\\mathbf{z}_{\\mathrm{inv}}$ that generates the evaluated data $\\mathbf{x}$ to get the aligned triplet $(\\mathbf{x},\\mathbf{z}_{\\mathrm{inv}},y)$ without any additional oracle calls. ", "page_idx": 3}, {"type": "text", "text": "4.1 Misalignment in Latent Bayesian Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "BO has proven its effectiveness in various areas where input space $\\mathcal{X}$ is continuous, however, BO over the discrete domain, such as chemical design, is a challenging problem. To handle this problem, VAE-based latent Bayesian optimization (LBO) has been proposed [8, 11, 13, 14] that leverages BO over a continuous space by mapping the discrete input space $\\mathcal{X}$ to a continuous latent space $\\mathcal{Z}$ . Variational autoencoder (VAE) is composed of encoder $q_{\\phi}\\,:\\,\\mathcal{X}\\,\\mapsto\\,\\mathcal{Z}$ to compute the latent representation $\\mathbf{z}$ of the input data $\\mathbf{x}$ and decoder $p_{\\theta}:\\mathcal{Z}\\mapsto\\mathcal{X}$ to generate the data $\\mathbf{x}$ from the latent $\\mathbf{z}$ . ", "page_idx": 3}, {"type": "text", "text": "Given the objective function $f$ , latent Bayesian optimization can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{z}^{*}=\\underset{\\mathbf{z}\\in\\mathcal{Z}}{\\arg\\operatorname*{max}}\\,f(p_{\\theta}(\\mathbf{z})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p_{\\theta}(\\mathbf{z})$ is a generated data with the decoder $p_{\\theta}$ and $\\mathcal{Z}$ is a latent space. Unlike the standard BO, the surrogate model $g$ aims to emulate the function $f\\circ p_{\\theta}:\\mathcal{Z}\\mapsto\\mathcal{Y}$ . To the end, the surrogate model is trained with aligned dataset $\\mathcal{D}_{.}=\\{(\\mathbf{x}^{i},\\mathbf{z}^{i},y^{i})\\}_{i=1}^{n}$ , where $\\mathbf{x}^{i}\\,=\\,p_{\\theta}(\\mathbf{z}^{i})$ is generated by the decoder $p_{\\theta}:\\mathcal{Z}\\mapsto\\mathcal{X}$ and $y^{i}=f(\\mathbf{x}^{i})$ is the objective value of $\\mathbf{x}^{i}$ evaluated via the black box objective function $f:\\mathcal X\\mapsto\\mathcal Y$ . In the rest of our paper, we define that the dataset is aligned when all triplets satisfy the above conditions (i.e., all triplets are the decoder triplets explained in Figure 1), and the dataset is misaligned otherwise. We define the \u2018misalignment problem\u2019 as the misaligned dataset hinders the accurate learning of the surrogate model $g$ . ", "page_idx": 3}, {"type": "text", "text": "Most existing LBO works [11\u201314] overlook the misalignment problem 1, which originates from two processes: (i) construction of initial dataset $\\mathcal{D}^{0}$ and (ii) update of VAE. ", "page_idx": 3}, {"type": "text", "text": "Construction of initial dataset $\\mathcal{D}^{0}$ . Since initial dataset $\\mathcal{D}^{0}$ is composed of pairs of input data and its corresponding objective value $\\{(\\mathbf{\\dot{x}}^{i},y^{i})|y^{i}=$ $f(\\mathbf{x}^{i})\\}_{i=1}^{n}$ , LBO requires latent vectors $\\{\\mathbf{z}^{i}\\}_{i=1}^{n}$ to train the surrogate model. Most works compute a latent vector $\\mathbf{z}^{i}$ as $\\mathbf{z}^{i}\\,=\\,q_{\\phi}(\\mathbf{x}^{i})$ under the assumption that VAE completely reconstructs every data points (i.e., $p_{\\theta}\\big(q_{\\phi}\\big(\\mathbf{x}^{i}\\big)\\big)=\\mathbf{x}^{i})$ ), which is difficult to be satisfied in every case. This results in the data misalignment $(\\mathbf{x}^{i}\\neq p_{\\theta}(\\mathbf{z}^{i}))$ during the construction of initial dataset $\\mathcal{D}^{0}$ . ", "page_idx": 3}, {"type": "image", "img_path": "TrN5TcWY87/tmp/0547f67918447ed9a7f213a31544b843e4e2a73b450698f94fba2e5d1188724d.jpg", "img_caption": ["Figure 3: (Left) The number of oracle calls to evaluate the queries selected by the acquisition function (blue) and during the recentering (Red). (Right) The number of objective function evaluation that updates the best score. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Update of VAE. In LBO works, updating VAE during the optimization plays a crucial role in adapting well to newly generated samples. However, due to the update of VAE, the previously generated triplet $(\\mathbf{x},\\mathbf{z},y)$ cannot ensure the alignment since z was computed by the VAE before the update. Previous LBO works [13, 14] solve the misalignment problem originating from the VAE update with a recentering technique that requests additional oracle calls to generate the aligned dataset $\\mathcal{D}=\\{(\\mathbf{x}^{\\prime}{}^{i},q_{\\phi}(\\mathbf{x}^{i}),f(\\mathbf{x}^{\\prime}{}^{i}))\\}_{i=1}^{n}$ where $\\mathbf{x}^{\\prime}{}^{i}=p_{\\theta}\\bigl(q_{\\phi}(\\mathbf{x}^{i})\\bigr)$ as shown in Figure 2 (Left). But, it has a limitation to consuming a huge amount of additional oracle calls while they do not update the best score (Figure 3). Note that prior works do not explicitly mention the additional oracle calls during the recentering technique, but it can be verified by the official GitHub code. Further details about oracle consumption of the recentering are provided in the supplement Section H. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.2 Inversion-based Latent Bayesian Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our primary goal is training the surrogate model $g$ to correctly emulate the composite function $f\\circ p_{\\theta}:\\mathcal{Z}\\mapsto\\mathcal{V}$ via constructing an aligned dataset without consuming additional oracle calls. To the end, we propose an inversion method that inverts the target discrete data $\\mathbf{x}$ into the latent vector ${\\bf z}$ that satisfies $\\mathbf{x}=p_{\\theta}(\\mathbf{z})$ for dataset alignment as shown in Figure 2 (Right). With a pre-trained frozen decoder $p_{\\theta}$ , the latent vector $\\mathbf{z}$ can be optimized by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{z}_{\\mathrm{inv}}=\\underset{\\mathbf{z}\\in\\mathcal{Z}}{\\arg\\operatorname*{min}}\\;d_{\\mathcal{X}}(\\mathbf{x},p_{\\theta}(\\mathbf{z})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{x}$ is a target data and $d_{\\mathcal{X}}$ is a distance function in the input space $\\mathcal{X}$ . We use the normalized Levenshtein distance [37] as our distance function, $d_{\\mathcal{X}}$ , which can be applied to any string-form data. Our inversion method, however, is flexible and can utilize any task-specific distance functions, such as Tanimoto similarity [38] for molecule design tasks. To solve the Eq. (4), we iteratively update a latent vector ${\\bf z}$ to find $\\mathbf{z}_{\\mathrm{inv}}$ that reconstructs the target data $\\mathbf{x}$ . We provide the overall pseudocode of the inversion method in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "The initialization strategy of latent vector z plays a key role in the optimization-based inversion process. We set the initialization point of latent vector $\\mathbf{z}$ as an output of a pre-trained encoder $q_{\\phi}(\\mathbf{x})$ given target discrete data x as in line 1. We iteratively update the latent vector ${\\bf z}$ with the cross-entropy loss used in VAE training in line 3 until it reaches the maximum number of iterations $T$ . Before the iteration budget is exhausted, we finish the inversion process when the distance between the generated data $p_{\\theta}(\\mathbf{z})$ and target data $\\mathbf{x}$ is less than $\\epsilon$ as our goal is finding the latent vector $\\mathbf{z}_{\\mathrm{inv}}$ that satisfies Eq. (4), which is denoted in line 4. The inversion method ", "page_idx": 4}, {"type": "table", "img_path": "TrN5TcWY87/tmp/cf3378e2f696d811710d3dee2da97ad9474025fc9b2cdc2db8cc022f0d22b075.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "generates the aligned dataset during the construction of the initial dataset $\\mathcal{D}^{0}$ and the update of VAE to handle the misalignment problem. ", "page_idx": 4}, {"type": "text", "text": "We theoretically show that optimizing the latent vector $\\mathbf{z}$ to satisfy $d_{\\mathcal{X}}(\\mathbf{x},p_{\\theta}(\\mathbf{z}))\\approx0$ with inversion plays a crucial role in minimizing the upper bound of the error between the posterior mean of the surrogate model and the objective function value within the trust region centered at $\\mathbf{z}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Let $f$ be a black-box objective function and m be a posterior mean of Gaussian process, p\u03b8 be a decoder of the variational autoencoder, c be an arbitrarily small constant, $d_{\\mathcal{X}}$ and $d_{\\mathcal{Z}}$ be the distance function on input $\\mathcal{X}$ and latent $\\mathcal{Z}$ spaces, respectively. The distance function $d_{\\mathcal{X}}$ is bounded between $\\boldsymbol{O}$ and $^{\\,I}$ , inclusive. We assume that $f$ , m and the composite function of $f$ and $p_{\\theta}$ are $L_{1},L_{2}$ , and $L_{3}$ -Lipschitz continuous functions, respectively. Suppose the following assumptions are satisfied: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f(\\mathbf{x})-m(\\mathbf{z})|\\leq c,}\\\\ {d_{\\mathcal{X}}(\\mathbf{x},p_{\\theta}(\\mathbf{z}))\\leq\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then the difference between the posterior mean of the arbitrary point $\\mathbf{z}^{\\prime}$ in the trust region centered at z with trust radius $\\delta$ and the black box objective value is upper bounded as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n|f(p_{\\theta}(\\mathbf{z}^{\\prime}))-m(\\mathbf{z}^{\\prime})|\\leq c+\\gamma\\cdot L_{1}+\\delta\\cdot(L_{2}+L_{3}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $d_{\\mathcal{Z}}(\\mathbf{z},\\mathbf{z}^{\\prime})\\leq\\delta$ . ", "page_idx": 4}, {"type": "text", "text": "The proof is available in Section A. We assume that the black box function $f$ , the posterior mean of Gaussian process $m$ , and the objective function $f\\circ p_{\\theta}$ are Lipschitz continuous functions, which is a common assumption in Bayesian optimization [39, 40] or global optimization [41]. Proposition 1 implies that the upper bound of the Gaussian process prediction error within the trust region can be minimized by reducing the distance between $\\mathbf{x}$ and $p_{\\theta}(\\mathbf{z})$ , denoted as $\\gamma$ . Our inversion method reduces $\\gamma$ by generating an aligned dataset without additional oracle calls. In Eq. (6), the constant $c$ represents the accuracy of the surrogate model, which can be improved during training. Some LBO works [12, 13] introduce regularization terms to learn a smooth latent space, implicitly reducing $L_{3}$ , the Lipschitz constant of the composite function $f\\circ p_{\\theta}$ . CoBO [14] further proposes regularization losses that explicitly reduce $L_{3}$ with theoretical grounding. Since the surrogate model $m$ emulates the composite function $f\\circ p_{\\theta}$ , its Lipschitz constant $L_{2}$ can also be reduced along with $L_{3}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.3 Potential-aware Trust Region Anchor Selection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Here, we propose a potential-aware trust region anchor selection to consider both the objective function value $y$ and the potential ability of the trust region to benefit the optimization. Previous trust region-based BO works [13, 14, 36, 42] select the anchor based on the corresponding objective function value only. However, this objective score-based anchor selection does not consider that the trust region contains latent points expected to enhance the optimization performance. ", "page_idx": 5}, {"type": "text", "text": "We design a potential score to measure the potential ability to enhance the optimization process of the trust region. To compute it, we use the acquisition function value, which is generally employed to measure the potential ability to improve the optimization process of a given data point. Specifically, we employ Thompson Sampling, a well-established acquisition function used in previous trust regionbased methods [13, 14, 36]. Formally, the potential score of each trust region $\\mathcal{T}_{i}$ is computed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{pot}}^{i}=\\operatorname*{max}_{\\mathbf{z}\\in Z_{\\mathrm{cand}}^{i}}f(\\mathbf{z})\\;\\mathrm{where}\\;\\hat{f}\\sim\\mathcal{G P}\\left(\\mu(\\mathbf{z}),k(\\mathbf{z},\\mathbf{z}^{\\prime})\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $i$ is an index of the candidate anchor point, $Z_{\\mathrm{cand}}^{i}$ is the candidate set sampled from the trust region $\\mathcal{T}_{i}$ and $\\hat{f}$ is a sampled function from the surrogate model (e.g., GP) posterior. ", "page_idx": 5}, {"type": "text", "text": "As the scale of objective function values $Y=\\{y^{1},y^{2},...,y^{n}\\}$ and that of the potential ability of each trust region A = \u03b1p1ot, \u03b1p2ot, .. ., \u03b1n is changed dynamically during the optimization process, we calculate a scaled potential score $\\alpha_{\\mathrm{pot}}^{i}$ by adjusting the scale of $\\alpha_{\\mathrm{max}}^{i}$ according to the $Y$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{scaled}}^{i}=\\frac{\\alpha_{\\mathrm{pot}}^{i}-A_{\\mathrm{min}}}{A_{\\mathrm{max}}-A_{\\mathrm{min}}}\\times(Y_{\\mathrm{max}}-Y_{\\mathrm{min}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $A_{\\mathrm{max}}=\\mathrm{max}_{i}\\left[\\alpha_{\\mathrm{pot}}^{i}\\right]$ and $A_{\\mathrm{min}}=\\mathrm{min}_{i}\\left[\\alpha_{\\mathrm{pot}}^{i}\\right]$ denote the maximum and minimum value of $A$ , respectively. $Y_{\\mathrm{max}}=\\operatorname*{max}_{i}\\left[y^{i}\\right]$ and $Y_{\\mathrm{min}}=\\operatorname*{min}_{i}\\left[y^{i}\\right]$ indicate the maximum and minimum value of $Y$ , respectively. Based on the scaled potential score $\\alpha_{\\mathrm{scaled}}^{i}$ , the final score $s^{i}$ of each anchor is calculated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\ns^{i}=y^{i}+\\alpha_{\\mathrm{scaled}}^{i}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Our final score takes into account the objective function value of the anchor (observed value) and the potential score (model\u2019s prediction) for the better local search space. Finally, we select the anchors with the highest final score $s^{i}$ . We summarize our potential-aware trust region anchor selection schema in Algorithm 2 of Section $_\\mathrm{N}$ . ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We measure the performance of the proposed method named InvBO on nine different tasks with three Bayesian optimization benchmarks: Guacamol [43], DRD3, and arithmetic expression fitting tasks [6, 8, 11\u201313, 44]. Guacamol and the DRD3 benchmark tasks aim to find molecules with the most necessary properties. For Guacamol benchmarks, we use seven challenging tasks, Median molecules 2 (med2), Zaleplon MPO (zale), Perindopril MPO (pdop), Amlodipine MPO (adip), Osimertinib MPO (osmb), Ranolazine MPO (rano), and Valsartan SMARTS (valt). To show the effectiveness of our InvBO in various settings, we also conducted experiments in a large budget setting, which is used in previous works [13, 14]. The goal of the arithmetic expression fitting task is to generate single-variable expressions that minimize the distance from a target expression (e.g., $1/3\\;\\bar{+}\\;x+\\sin{(x\\;\\tilde{\\times}\\;x)})$ . More details of each benchmark are provided in Section K. ", "page_idx": 5}, {"type": "image", "img_path": "TrN5TcWY87/tmp/ac49d68e61e80231053f11d0713b657568272c17b2ceadb2d49154c55b1427a6.jpg", "img_caption": ["Figure 4: Optimization results on Guacamol benchmark tasks. The lines and ranges indicate the average and standard error of ten runs under the same settings. A higher score is a better score. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.2 Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare to six latent Bayesian Optimization methods: LS-BO, TuRBO [36], W-LBO [11], LOLBO [13], CoBO [14], and PG-LBO [17]. We also compare with GB-GA [45], a widely used genetic algorithm for graph structure data. LOL-BO and CoBO employ the decoder triplet generated by the recentering technique, and the other baselines employ the encoder triplet during the optimization process. In the case of LOL-BO and CoBO, we substitute the recentering technique with the inversion method. We provide the details of each baseline in Section L. ", "page_idx": 6}, {"type": "text", "text": "5.3 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For the arithmetic expression ftiting task, we follow other works [8, 11, 13, 14] to employ GrammarVAE model [8]. For the de novo molecule design tasks such as Guacamol benchmark and DRD3 tasks, we use SELFIES VAE [13] following recent works [13, 14]. In all tasks, we adopt sparse variational GP [46] with deep kernel [47] as our surrogate model. In the inversion method, the learning rate is set to 0.1 in all experiments. The further implementation details are provided in Section M. ", "page_idx": 6}, {"type": "text", "text": "5.4 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We apply our InvBO to several trust region-based LBOs, TuRBO- $L$ , LOL-BO [13] and CoBO [14], and provide optimization results on two Guacamol benchmark tasks, med2 and valt. All results are average scores of ten runs under the identical settings. Table 1 demonstrates that our InvBO consistently improves all LBO models in two tasks by a large margin. In particular, in the valt task, all baseline models with InvBO demonstrate significant performance improvements and CoBO with InvBO achieves a 0.348 score gain while the baseline models without InvBO fail in optimization. More results of other baselines with InvBO on other tasks are provided in Section E. ", "page_idx": 6}, {"type": "text", "text": "Figure 4 provides the optimization results on four Guacamol benchmark tasks including med2, zale, osmb, and valt. Each subfigure shows the number of evaluations of the black box objective function (Number of Oracle) and the corresponding average and standard error of the objective score (Best Score). Our InvBO built on CoBO achieves the best performance in all four tasks. Further results of other tasks are provided in Section F. ", "page_idx": 6}, {"type": "text", "text": "We also conduct experiments to demonstrate the effectiveness of our InvBO on DRD3 and arithmetic expression fitting tasks and large-budget settings. The experimental results are illustrated in Figure 5(a) and Figure 5(b), respectively. Please note that both DRD3 and arithmetic expression tasks aim to minimize the score while the goal of Guacamol tasks is increasing the score. Figure 5 shows that our InvBO applied to CoBO achieves the best performance. We provide further optimization results on other tasks under the large budget settings in Section G. These results demonstrate that InvBO is effective in diverse tasks and settings. ", "page_idx": 6}, {"type": "image", "img_path": "TrN5TcWY87/tmp/1403af926e442d5991ea90936d3ac0502453475757f108e99cb40208afbcd3f0.jpg", "img_caption": ["Figure 5: Optimization results on various tasks and settings. Note that: (a) A lower score is a better score. (b) A higher score is a better score. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "TrN5TcWY87/tmp/fea1ebd01758d260f07743cb06cc2a522675907b42e1b45a744cfaaa64650768.jpg", "img_caption": ["Figure 6: Optimization results of component ablation on zale, med2, osmb, and valt. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "6 Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct additional experiments to verify the contribution of each component in our InvBO: the inversion method (INV), and the potential-aware trust region anchor selection method (PAS). Figure 6 shows the optimization results of the ablation study on med2, zale, osmb, and valt tasks. From the figure, models with the inversion method (i.e., CoBO with INV and InvBO) replace the recentering technique as the inversion method, while models without the inversion method (i.e., vanilla CoBO and CoBO with PAS) employ the recentering technique. Notably, both components of our method contribute to the optimization process, and the combination work, InvBO, consistently obtains better objective scores compared to other models. Specifically, in osmb task, the average best score achieved by the methods with the PAS, INV and both (i.e., InvBO) shows 0.784, 0.792, and 0.804 score gains compared to vanilla CoBO, respectively. ", "page_idx": 7}, {"type": "text", "text": "6.2 Analysis on Misalignment Problem and Inversion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To further prove that the misaligned dataset hinders the accurate learning of the surrogate model (i.e., misalignment problem), we compare the performance of the surrogate model trained with aligned and misaligned datasets on the med2 task. Figure 7 shows the fitting results of the surrogate model trained with encoder triplets $(g^{\\mathrm{enc}}$ , left) and decoder triplets $(g^{\\mathrm{dec}}$ , right), respectively. Figure 7 demonstrates that the surrogate model $g^{\\mathrm{dec}}$ approximates the objective function accurately, while $g^{\\mathrm{enc}}$ fails to emulate the objective function. Further details of an experiment are provided in Section I. ", "page_idx": 7}, {"type": "text", "text": "In Figure 8, we compare the optimization results of CoBO using decoder triplets and encoder triplets on the valt and med2 tasks. Both models use the potential-aware anchor selection, and the decoder triplets are made by our inversion method. CoBO using the decoder triplets shows superior optimization performance over the CoBO using the encoder triplets on both tasks. This implies that the misaligned dataset hinders the accurate learning of the surrogate model, which leads to suboptimal optimization results, and the inversion method handles the problem by generating the decoder triplet. ", "page_idx": 7}, {"type": "image", "img_path": "TrN5TcWY87/tmp/253a9aa6b660aee527242546802df4eefb4e3bb5f8035322436ce4b2c661706c.jpg", "img_caption": ["Figure 7: Gaussian process fitting results trained with encoder triplets and decoder triplets. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "TrN5TcWY87/tmp/efeb9d5bf42769c8fb54d16726b812a2f67329be036215ebf011df6b3322dcca.jpg", "img_caption": ["Figure 8: Optimization results using encoder triplet and decoder triplet on valt and med2 tasks. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "TrN5TcWY87/tmp/0b9a7b89fe077995a2aea972946cc4d1d24f2e369de35031be5201315d52702b.jpg", "img_caption": ["Figure 9: (Left) Gaussian process prediction error Figure 10: Optimization results with diverse trust within the trust region. (Right) Optimization re- region anchor selections on valt and med2 tasks. sults on med2 tasks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.3 Effects of Inversion on Proposition 1 ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Section 4.2, we provide the upper bound of the error between the predicted and ground-truth objective value. In Eq. (6), the Lipschitz constant of the objective function $L_{1}$ and the trust region radius $\\delta$ is fixed or the hyper-parameter and the constant $c$ can be improved by learning the surrogate model. In the end, we can reduce the upper bound of the objective value prediction error by reducing the three components: $\\gamma,L_{2}$ , and $L_{3}$ , which implies the distance between $\\mathbf{x}$ and $p_{\\theta}(\\mathbf{z})$ , the Lipschitz constant of function $m$ and $f\\circ p_{\\theta}$ , respectively. Our inversion method reduce $\\gamma$ by searching the latent vector $\\mathbf{z}_{\\mathrm{inv}}$ that satisfies $\\mathbf{x}=p_{\\theta}(\\mathbf{z}_{\\mathrm{inv}})$ . Previously, CoBO [14] proposed regularization losses that reduce $L_{3}$ . Since the surrogate model emulates the composite function $f\\circ p_{\\theta}$ , these regularization losses can reduce $L_{2}$ along with $L_{3}$ . ", "page_idx": 8}, {"type": "text", "text": "Figure 9 (left) shows the regularization losses of CoBO and our inversion method reduces the objective value prediction error. CoBO-based models (i.e. $,\\mathrm{{CoBO}\\mathrm{{+PAS}}}$ and CoBO+InvBO) employ the CoBO regularization losses, and models with InvBO (i.e., TuRBO- $\\mathbf{\\delta}.L{+}\\mathrm{InvBO}$ and $_{\\mathrm{CoBO+InvBO)}}$ employ our inversion method. TuRBO- $L$ does not use the CoBO regularization losses nor our inversion method, and models with PAS (i.e., TuRBO- $J{+}\\mathrm{PAS}$ and ${\\mathrm{CoBO+PAS}}$ ) employ encoder triplets. Applying the regularization losses and our inversion method reduces the objective value prediction error, respectively, but our inversion method shows a larger error reduction. The combination of regularization losses and our inversion shows the smallest prediction error, which implies our inversion method robustly complements existing methods. We provide the optimization results of each model on the med2 task in Figure 9 (right). These results demonstrate that reducing the objective function prediction error plays a crucial role in optimization performance. ", "page_idx": 8}, {"type": "text", "text": "6.4 Comparing Diverse Anchor Selection Methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To further prove the importance of our potential-aware anchor selection method, we perform BO with the diverse anchor selection methods: random, acquisition $\\alpha$ , and objective score $y$ . Random indicates randomly selected anchors, and acquisition and objective indicate anchors are selected based on the max acquisition function value and objective score, respectively. All models use the inversion method, and the optimization results on valt and med2 tasks are in Figure 10. Ours and objective score-based anchor selection rapidly find high-quality data compared to the random and acquisitionbased selections. However, objective score-based anchor selection shows inferior performance than our potential-aware anchor selection. This indicates that both the uncertainty of the surrogate model and objective function value need to be considered for exploration and exploitation. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose Inversion-based Latent Bayesian Optimization (InvBO), a plug-and-play module for LBO. We introduce the inversion method that inverts the decoder to find the latent vector for generating the aligned dataset. Additionally, we present the potential-aware trust region anchor selection that considers not only the corresponding objective function value of the anchor but also the potential ability of the trust region. From our experimental results, InvBO achieves state-of-the-art performance on nine LBO benchmark tasks. We also theoretically demonstrate the effectiveness of our inversion method and provide a comprehensive analysis to show the effectiveness of our InvBO. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "One of the contributions of this paper is molecular design optimization, which requires careful consideration due to its unintentional applications such as the generation of toxic. We believe that our work has a lot of positive aspects to accelerate the development of chemical and drug discovery with an inversion-based latent Bayesian optimization method. ", "page_idx": 9}, {"type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The performance of the methods proposed in the paper depends on the quality of the generative model. For example, if the objective function is related to the molecule property, the generated model such as the VAE should have the ability to generate the proper molecule to have good performance of optimization. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partly supported by ICT Creative Consilience Program through the Institute of Information & Communications Technology Planning & Evaluation (IITP) (IITP-2024-RS-2020- II201819, $10\\%$ ) and the National Research Foundation of Korea (NRF) (NRF-2023R1A2C2005373, $45\\%$ ) grant funded by the Korea government (MSIT), and the Virtual Engineering Platform Project (Grant No. P0022336, $45\\%$ ), funded by the Ministry of Trade, Industry & Energy (MoTIE, South Korea). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Sebastian Ament, Maximilian Amsler, Duncan R Sutherland, Ming-Chiang Chang, Dan Guevarra, Aine B Connolly, John M Gregoire, Michael O Thompson, Carla P Gomes, and R Bruce van Dover. Autonomous materials synthesis via hierarchical active learning of nonequilibrium phase diagrams. Sci. Adv., 7(51):eabg4930, 2021.   \n[2] Ke Wang and Alexander W Dowling. Bayesian optimization for chemical products and functional materials. Curr. Opin. Chem. Eng., 36:100728, 2022.   \n[3] Ryan-Rhys Grifftihs and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Constrained bayesian optimization for automatic chemical design using variational autoencoders. Chem. Sci., 11(2):577\u2013586, 2020.   \n[4] Jiazhuo Wang, Jason Xu, and Xuejun Wang. Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning, 2018.   \n[5] Jia Wu, Xiu-Yun Chen, Hao Zhang, Li-Dong Xiong, Hang Lei, and Si-Hao Deng. Hyperparameter optimization for machine learning models based on bayesian optimization. J. Electron. Sci. Technol., 17(1):26\u201340, 2019. [6] Aryan Deshwal and Jana Doppa. Combining latent space and structured kernels for bayesian optimization over combinatorial spaces. In NeurIPS, 2021. [7] Changyong Oh, Jakub Tomczak, Efstratios Gavves, and Max Welling. Combinatorial bayesian optimization using the graph cartesian product. NeurIPS, 2019. [8] Matt J Kusner, Brooks Paige, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Grammar variational autoencoder. In ICML, 2017. [9] Rafael G\u00f3mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Benjam\u00edn S\u00e1nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al\u00e1n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS Cent. Sci., 4(2):268\u2013276, 2018.   \n[10] Stephan Eissman, Daniel Levy, Rui Shu, Stefan Bartzsch, and Stefano Ermon. Bayesian optimization and attribute adjustment. In UAI, 2018.   \n[11] Austin Tripp, Erik Daxberger, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Sample-efficient optimization in the latent space of deep generative models via weighted retraining. In NeurIPS, 2020.   \n[12] Antoine Grosnit, Rasul Tutunov, Alexandre Max Maraval, Ryan-Rhys Griffiths, Alexander I Cowen-Rivers, Lin Yang, Lin Zhu, Wenlong Lyu, Zhitang Chen, Jun Wang, et al. Highdimensional bayesian optimisation with variational autoencoders and deep metric learning, 2021.   \n[13] Natalie Maus, Haydn Jones, Juston Moore, Matt J Kusner, John Bradshaw, and Jacob Gardner. Local latent space bayesian optimization over structured inputs. In NeurIPS, 2022.   \n[14] Seunghun Lee, Jaewon Chu, Sihyeon Kim, Juyeon Ko, and Hyunwoo J Kim. Advancing bayesian optimization via learning correlated latent space. In NeurIPS, 2023.   \n[15] Diederik P Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[16] Natalie Maus, Kaiwen Wu, David Eriksson, and Jacob Gardner. Discovering many diverse solutions with bayesian optimization. In AISTATS, 2023.   \n[17] Taicai Chen, Yue Duan, Dong Li, Lei Qi, Yinghuan Shi, and Yang Gao. Pg-lbo: Enhancing high-dimensional bayesian optimization with pseudo-label and gaussian process guidance. In AAAI, 2024.   \n[18] Pascal Notin, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, and Yarin Gal. Improving black-box optimization in vae latent space using decoder uncertainty. NeurIPS, 2021.   \n[19] Xiaoyu Lu, Javier Gonzalez, Zhenwen Dai, and Neil D Lawrence. Structured variationally auto-encoded optimization. ICML, 2018.   \n[20] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In ICML, 2018.   \n[21] Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational autoencoder for structured data. In ICLR, 2018.   \n[22] Hiroshi Kajino. Molecular hypergraph grammar with its application to molecular optimization. In ICML, 2019.   \n[23] Bidisha Samanta, Abir De, Gourhari Jana, Vicen\u00e7 G\u00f3mez, Pratim Chattaraj, Niloy Ganguly, and Manuel Gomez-Rodriguez. Nevae: A deep generative model for molecular graphs. JMLR, 21(114):1\u201333, 2020.   \n[24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.   \n[25] Jun-Yan Zhu, Philipp Kr\u00e4henb\u00fchl, Eli Shechtman, and Alexei A Efros. Generative visual manipulation on the natural image manifold. In ECCV, 2016.   \n[26] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015.   \n[27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.   \n[28] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. In ICCV, 2021.   \n[29] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In ICLR, 2023.   \n[30] Antonia Creswell and Anil Anthony Bharath. Inverting the generator of a generative adversarial network. TNNLS, 30(7):1967\u20131974, 2018.   \n[31] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In ICCV, 2019.   \n[32] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan $^{++}$ : How to edit the embedded images? In CVPR, 2020.   \n[33] Minyoung Huh, Richard Zhang, Jun-Yan Zhu, Sylvain Paris, and Aaron Hertzmann. Transforming and projecting images into class-conditional generative networks. In ECCV, 2020.   \n[34] Guim Perarnau, Joost Van De Weijer, Bogdan Raducanu, and Jose M \u00c1lvarez. Invertible conditional gans for image editing. In NeurIPSW, 2016.   \n[35] David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, and Antonio Torralba. Inverting layers of a large generator. In ICLRW, 2019.   \n[36] David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable global optimization via local bayesian optimization. In NeurIPS, 2019.   \n[37] Vladimir I Levenshtein et al. Binary codes capable of correcting deletions, insertions, and reversals. Dokl. Phys., 10(8):707\u2013710, 1966.   \n[38] D\u00e1vid Bajusz, Anita R\u00e1cz, and K\u00e1roly H\u00e9berger. Why is tanimoto index an appropriate choice for fingerprint-based similarity calculations? Journal of cheminformatics, 2015.   \n[39] Javier Gonz\u00e1lez, Zhenwen Dai, Philipp Hennig, and Neil Lawrence. Batch bayesian optimization via local penalization. In Artificial intelligence and statistics, 2016.   \n[40] Trong Nghia Hoang, Quang Minh Hoang, Ruofei Ouyang, and Kian Hsiang Low. Decentralized high-dimensional bayesian optimization with factor graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.   \n[41] Christodoulos A Floudas and Panos M Pardalos. Encyclopedia of optimization. Springer Science & Business Media, 2008.   \n[42] Samuel Daulton, David Eriksson, Maximilian Balandat, and Eytan Bakshy. Multi-objective bayesian optimization over high-dimensional search spaces. In UAI, 2022.   \n[43] Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking models for de novo molecular design. J. Chem. Inf. Model., 59(3):1096\u20131108, 2019.   \n[44] Sungsoo Ahn, Junsu Kim, Hankook Lee, and Jinwoo Shin. Guiding deep molecular optimization with genetic exploration. In NeurIPS, 2020.   \n[45] Jan H Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Chemical science, 2019.   \n[46] Edward Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs. NeurIPS, 2005.   \n[47] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In AISTATS, 2016.   \n[48] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. NeurIPS, 2020.   \n[49] Christopher Williams and Carl Rasmussen. Gaussian processes for regression. NIPS, 1995.   \n[50] Miguel Garc\u00eda-Orteg\u00f3n, Gregor NC Simm, Austin J Tripp, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Andreas Bender, and Sergio Bacallado. Dockstring: easy molecular docking yields better benchmarks for ligand design. J. Chem. Inf. Model., 62(15):3486\u20133502, 2022.   \n[51] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 2019.   \n[52] Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. NeurIPS, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A A Proof of Proposition 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide the proof of Proposition 1. ", "page_idx": 13}, {"type": "text", "text": "Proposition 1. Let $f$ be a black-box objective function and m be a posterior mean of Gaussian process, p\u03b8 be a decoder of the variational autoencoder, c be an arbitrarily small constant, $d_{\\mathcal{X}}$ and $d_{\\mathcal{Z}}$ be the distance function on input $\\mathcal{X}$ and latent $\\mathcal{Z}$ spaces, respectively. The distance function $d_{\\mathcal{X}}$ is bounded between $\\boldsymbol{O}$ and $^{\\,I}$ , inclusive. We assume that $f$ , $m$ and the composite function of $f$ and $p_{\\theta}$ are $L_{1},L_{2}$ , and $L_{3}$ -Lipschitz continuous functions, respectively. Suppose the following assumptions are satisfied: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f(\\mathbf{x})-m(\\mathbf{z})|\\leq c,}\\\\ {d_{\\mathcal{X}}(\\mathbf{x},p_{\\theta}(\\mathbf{z}))\\leq\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then the difference between the posterior mean of the arbitrary point $\\mathbf{z}^{\\prime}$ in the trust region centered at z with trust radius $\\delta$ and the black box objective value is upper bounded as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n|f(p_{\\theta}(\\mathbf{z}^{\\prime}))-m(\\mathbf{z}^{\\prime})|\\leq c+\\gamma\\cdot L_{1}+\\delta\\cdot(L_{2}+L_{3}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $d_{\\mathcal{Z}}(\\mathbf{z},\\mathbf{z}^{\\prime})\\leq\\delta$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. With $L$ -Lipschitz continuity, we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|f({\\bf x})-f(p_{\\theta}({\\bf z}))|\\le L_{1}\\cdot d_{\\mathcal{X}}({\\bf x},p_{\\theta}({\\bf z}))\\le\\gamma\\cdot L_{1}}\\\\ &{\\quad\\quad\\quad|m({\\bf z})-m({\\bf z}^{\\prime})|\\le L_{2}\\cdot d_{\\mathcal{Z}}({\\bf z},{\\bf z}^{\\prime})\\le\\delta\\cdot L_{2},}\\\\ &{\\quad\\quad\\quad|f(p_{\\theta}({\\bf z}))-f(p_{\\theta}({\\bf z}^{\\prime}))|\\le L_{3}\\cdot d_{\\mathcal{Z}}({\\bf z},{\\bf z}^{\\prime})\\le\\delta\\cdot L_{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, the difference between the posterior mean of point within the trust region of length $\\delta$ centered at ${\\bf z}$ and the black-box objective value is upper bounded as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f(p_{\\theta}(\\mathbf{z}^{\\prime}))-m(\\mathbf{z}^{\\prime})|}\\\\ &{=|(f(\\mathbf{x})-m(\\mathbf{z}))+(f(p_{\\theta}(\\mathbf{z}))-f(\\mathbf{x}))+(f(p_{\\theta}(\\mathbf{z}^{\\prime}))-f(p_{\\theta}(\\mathbf{z})))+(m(\\mathbf{z})-m(\\mathbf{z}^{\\prime}))|}\\\\ &{\\le|f(\\mathbf{x})-m(\\mathbf{z})|+|f(p_{\\theta}(\\mathbf{z}))-f(\\mathbf{x})|+|f(p_{\\theta}(\\mathbf{z}^{\\prime}))-f(p_{\\theta}(\\mathbf{z}))|+|m(\\mathbf{z})-m(\\mathbf{z}^{\\prime})|}\\\\ &{\\le c+\\gamma\\cdot L_{1}+\\delta\\cdot(L_{2}+L_{3})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B Exploration of Potential-aware Trust Region Anchor Selection ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "TrN5TcWY87/tmp/4485d131e68170bf663613e2e6d54633452a9e49b2704a6381df1ffaf677742e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 11: Exploration effectiveness ablation study of the potential aware anchor selection (PAS) on CoBO+InvBO (INV, PAS). ", "page_idx": 13}, {"type": "text", "text": "We measure the number of searched unique data samples for each iteration to validate the exploration ability of our potential-aware trust region anchor selection in 4.3. We compare it with the objective score-based anchor selection, which selects the best anchors based on their objective score, used in most prior works [13, 14, 36]. From the figure, our potential-aware anchor selection searches more diverse data compared to the objective score-based anchor selection on both tasks. These results demonstrate that our anchor selection method improves the exploration ability without the loss of the exploitation ability. ", "page_idx": 13}, {"type": "text", "text": "C Dissimilarity in Latent Bayesian Optimization ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "TrN5TcWY87/tmp/43b0b14f06a6fb6fed7d09d8cb7aed27aa7ef184d5a5e764d38d60b1ed4bcd21.jpg", "img_caption": ["Figure 12: Dissimilarity between $\\mathbf{x}^{i}$ and $p_{\\theta}(\\mathbf{z}^{i})$ with and without inversion on med2 and valt tasks. The measurement of dissimilarity is the normalized Levenshtein distance between the SELFIES token. $\\mathbf{X}$ -axis: number of iterations, y-axis: normalized Levenshtein distance.) "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "We here provide the comparison of with and without inversion to empirically prove that the inversion generates aligned data. Given a pair of data sample $\\mathbf{x}^{i}$ and its corresponding latent vector $\\mathbf{z}^{i}$ , we measure the dissimilarity between the input $\\mathbf{x}^{i}$ and the data $p_{\\theta}(\\mathbf{z}^{i})$ generated from the decoder. The latent vector generated with the inversion method $\\mathbf{z}^{i}$ is defined as $\\begin{array}{r}{\\check{\\mathbf{z}}_{\\cdot}^{i}=\\arg\\operatorname*{min}_{\\mathbf{z}\\in\\mathcal{Z}}d_{\\mathcal{X}}(\\mathbf{x}^{i},p_{\\theta}(\\mathbf{z}))}\\end{array}$ (Section 4.2) The latent vector without inversion method $\\mathbf{z}^{i}=q_{\\phi}(\\mathbf{x}^{i})$ is constructed by feeding the input $\\mathbf{x}^{i}$ into the encoder $q_{\\phi}$ similar to prior LBO works [11, 17]. ", "page_idx": 14}, {"type": "text", "text": "Figure 12 shows the dissimilarity comparison results without and with the inversion on med2 and valt tasks. We measure the dissimilarity as the normalized Levenshtein distance between two SELFIES tokens. The ${\\bf X}$ -axis indicates the iteration, and the y-axis indicates the dissimilarity between $\\mathbf{x}^{i}$ and $p_{\\theta}(\\mathbf{z}^{i})$ . From the figure, the inversion achieves zero dissimilarity for every iteration on both tasks, which indicates that the inversion always generates aligned data. On the other hand, BO without the inversion mostly generates misaligned data. These results demonstrate the necessity of the inversion in LBO. ", "page_idx": 14}, {"type": "text", "text": "D Applying PAS to TuRBO on standard BO benchmark ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "TrN5TcWY87/tmp/5d4fb19b221badd3b520c5852897571c6b49bcb3024faed48227aaad9d2e7d6a.jpg", "img_caption": ["Figure 13: Optimization results of TuRBO and applying PAS to TuRBO on the synthetic Ackley function with 40 dimensions. The lines and ranges indicate the mean and a standard deviation of ten runs with different seeds. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "We analyze the effectiveness of PAS along with standard BO approaches. We provide the optimization performance of TuRBO and TuRBO with PAS in the Ackley benchmark function with 40 dimension, with input ranges from [-32.768, 32.768]. Our implementation is based on the codebase of TuRBO [36] provided in the BoTorch [48] tutorial, and we use the RBF kernel with Automatic Relevance Determination (ARD) lengthscale [49]. The number of initial data is 80 and the number of total oracle calls is 1,000. Figure 13 shows that applying PAS on TuRBO consistently improves the optimization performance during the iteration. This result demonstrates that PAS is also effective in standard BO benchmark tasks. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "E Experiments of Plug-and-Play ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide further results of applying our InvBO to previous LBO works, LS-BO, TuRBO- $L$ , WLBO [11], LOL-BO [13], PG-LBO [17] and CoBO [14] on Guacamol and arithmetic ftiting tasks. All results are average scores of ten runs under identical settings. Since LS-BO, W-LBO, and PG-LBO are not trust region-based LBO, we report the optimization results applying inversion (INV) only on these baselines. The experimental results are illustrated in Table 2. The table shows that applying our InvBO to all previous LBO works consistently improve the optimization performance in all Guacamol and arithmetic expression fitting tasks. ", "page_idx": 15}, {"type": "text", "text": "F Experiments on Guacamol Benchmark with Small Budget ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "TrN5TcWY87/tmp/1b9377500d1cca8aa28a1b92593e4a38113042c7281a91fb8b0b913bf2391cc5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 14: Optimization results on Guacamol benchmark tasks, excluding the tasks in Figure 4. The lines and ranges indicate the average and standard error of ten runs under the same settings. A higher score is a better score. ", "page_idx": 15}, {"type": "text", "text": "Figure 14 provides the optimization results on three Guacamol benchmark tasks including pdop, adip, and rano. The y-axis of each subfigure denotes the best-found score, and the x-axis denotes the number of the objective function evaluation. Our InvBO built on the CoBO shows the state-of-the-art performance on all three tasks. In particular, CoBO with our InvBO achieves 0.56 best score in the pdop task while the best scores of all other baselines are under 0.48. ", "page_idx": 15}, {"type": "text", "text": "G Experiments on Guacamol Benchmark with Large Budget ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide further optimization results on five Guacamol benchmark tasks, adip, osmb, pdop, rano, and zale under the large budget setting to demonstrate the effectiveness of our InvBO in diverse budget settings, which are used in previous works [13, 14]. The experimental results are illustrated in Figure 15. In the figure, our InvBO to CoBO achieves superior performance on large-budget settings. Specifically, InvBO built on the CoBO showed a large margin from the baseline and achieved a more than 0.9 best score in adip tasks. These results imply that our InvBO is effective in diverse settings and tasks. ", "page_idx": 15}, {"type": "text", "text": "H Analysis on Recentering Technique ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We here provide the details about additional oracle calls in the recentering technique [13, 14]. Firstly, CoBO explicitly mentioned the oracle call for recentering in L8-9 of Algorithm 1 in the paper. Secondly, LOL-BO\u2019s oracle calls for recentering can be verified by its official GitHub code. See, L188- 228 of lolbo/lolbo/lolbo.py and L54 of lolbo/lolbo/latent_space_objective.py. ", "page_idx": 15}, {"type": "text", "text": "Table 2: Optimization results of applying InvBO or inversion method (INV) to several LBOs on Guacamol benchmark tasks and arithmetic expression task, including the task in Table 1. A higher score is better for all tasks except the arithmetic expression task. ", "page_idx": 16}, {"type": "table", "img_path": "TrN5TcWY87/tmp/7e22e2eb668a824176127fffbf6d9b8c97c2f9a083c2dd11af92c39d1e326b20.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "TrN5TcWY87/tmp/77631db2417d1af5068f299e89c9459eaf0120601ebe121fa15d71a2caac4bd7.jpg", "img_caption": ["Figure 15: Optimization results on Guacamol task under the large budget setting, excluding the tasks in Figure 5(b). The lines and ranges indicate the average and standard error of five runs under the same settings. A higher score is a better score. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "We present the experiments on additional oracle calls in recentering in Figure 16. The figure shows the optimization results of CoBO that use recentering with additional oracle calls and recentering without additional oracle calls on med2 and valt tasks. In both tasks, CoBO that uses the recentering technique without additional oracle calls fails in optimization while consuming additional oracle calls shows progress. These results show that the additional oracle calls are essential in the recentering technique, while the previous works do not explicitly mention it. ", "page_idx": 17}, {"type": "text", "text": "I Analysis on Misalignment Problem ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We here provide the experimental details about the Gaussian process ftiting with the encoder triplets and the decoder triplets. We train the surrogate model on 300 train points and predict on 100 test points, all points are randomly sampled. The encoder triplets $\\{(\\mathbf{x}^{i},\\mathbf{\\dot{q}}_{\\phi}(\\mathbf{x}^{i}),f(\\mathbf{x}^{\\acute{i}}))\\}_{i=1}^{n}$ and the decoder triplets $\\{(\\mathbf{x}^{i},\\mathbf{z}_{\\mathrm{inv}}^{i},f(\\mathbf{x}^{i}))\\}_{i=1}^{n}$ share the same discrete data $\\mathbf{x}$ , where $\\mathbf{x}^{i}=p_{\\theta}(\\mathbf{z}_{\\mathrm{inv}}^{i})$ . The dots in the figure denote the mean predictions of the surrogate model and bars denote $95\\%$ confidence intervals. Figure 17 provides the Gaussian process fitting results trained with the encoder triplets (Left) and decoder triplets (Right) that predict the training points on the med2 task. The figure implies that the Gaussian process trained with encoder triplet fails to fti the composite function $f\\circ p_{\\theta}$ , while trained with decoder triplet emulates the function accurately. ", "page_idx": 17}, {"type": "text", "text": "J Efficiency Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Although Bayesian optimization assumes the objective function is cost-expensive, it is still important to consider the efficiency of the algorithm. We present an efficiency analysis comparing with baseline methods [11, 13, 14, 17], and TuRBO- $L$ , LS-BO. We conducted experiments under the same condition for fair comparison: a single NVIDIA RTX 2080 TI with the CPU of AMD EPYC 7742. Wall-clock time comparison with the baseline model is in Table 3. The table shows that applying our InvBO to CoBO achieves state-of-the-art performance not only under the same oracle calls but also under the same wall-clock time. ", "page_idx": 17}, {"type": "image", "img_path": "TrN5TcWY87/tmp/891d663a36e6026c3dc75c884498b028beeba5655821d3b7ffcd8417a957a852.jpg", "img_caption": ["Figure 16: Optimizaiton results on Guacamol tasks, med2 and valt. We compare the CoBO applying InvBO that uses the recentering technique with and without additional oracle calls. The lines and ranges indicate the average and standard error of ten runs under the same settings. A higher score is a better score. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "TrN5TcWY87/tmp/d9883281eec48542d7cd552d958ffdea0b4152d9521dd54a04c007a63e1feedc.jpg", "img_caption": ["Figure 17: Gaussian process fitting results trained with encoder triplets and decoder triplets. Note that the datapoints are in the training set. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "K Details of Benchmark Tasks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We demonstrate the effectiveness of InvBO through three optimization benchmarks: Guacamol [43], DRD3, and arithmetic expression fitting task [8, 11, 13, 14]. Guacamol benchmark tasks aim to design molecules with the desired properties, which are scored with a range between 0 to 1. The number of initial points is 100 and the number of oracle calls is 500 in a small-budget setting, and the number of initial points is 10,000 and the number of oracle calls is 70,000 in a large-budget setting. In the DRD3 benchmark task, we aim to design ligands (molecules that specifically bind to a protein) that bind to dopamine receptor D3 (DRD3). We evaluate the docking score of ligands with the Dockstring library [50]. The number of the observed data points is 100 and the number of max oracle calls is 3,000. Lastly, the arithmetic expression ftiting task is to generate single-variable expressions that are close to a target expression (e.g., $1/3+x+\\sin\\left(x\\times x\\right))$ . The number of initial access to oracle is 100 and the number of max oracle calls is 500. All tasks are assumed to be noiseless settings, and we select the initial points randomly from the dataset provided in [14] and the same initial data was used in all tasks. ", "page_idx": 18}, {"type": "text", "text": "L Details of Baselines ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We compare our InvBO with six latent Bayesian Optimization methods: LS-BO, TuRBO [36], WLBO [11], LOL-BO [13], CoBO [14], and PG-LBO [17], and one graph-based genetic algorithm: ", "page_idx": 18}, {"type": "table", "img_path": "TrN5TcWY87/tmp/1b80d1b271bab957bfd7c032bf9650ecc9215c09c7f05427c6907be8452ddfb6.jpg", "table_caption": ["Table 3: Wall-clock time and corresponding best score for each model. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "GB-GA [45]. We adopt the standard LBO for LS-BO approach following other work [14] with VAE update during the optimization process. TuRBO leverages trust regions to alleviate the overexploration. To adapt TuRBO to the latent space, we employ TuRBO- $L$ , LS-BO with trust region, following other latent Bayesian optimization baselines [13, 14]. W-LBO weights the data based on importance in order to focus on samples with high objective values. LOL-BO adapts the concept of the trust region to the latent space and proposes VAE learning methods to inject the prior of the sparse GP models for better optimization. CoBO designs novel regularization losses based on the Lipschitz condition to boost the correlation between the distance in the latent space and the distance within the objective function. PG-LBO utilizes unobserved data with a pseudo-labeling technique and integrates Gaussian Process guidance into VAE training to learn a latent space for better optimization. In PG-LBO, we sample the pseudo data by adding the Gaussian noise, and dynamic thresholding. ", "page_idx": 19}, {"type": "text", "text": "M Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our implementation is based on the codebase of [13]. We use PyTorch3, BoTorch4 [48], GPyTorch5 [52], and Guacamol6 software packages. In the Guacamol tasks and DRD3 task [50], we use the SELFIES VAE [13] which is pretrained in an unsupervised manner with 1.27M molecules from Guacamol benchmark. The Grammar VAE [8] is pre-trained with 40K expression data in an unsupervised manner. The dimension of latent space is 256 in the SELFIES VAE and 25 in the Grammar VAE. The size of the data obtained from the acquisition function and $k$ are hyperparameters, which are presented in Table 4. ", "page_idx": 19}, {"type": "text", "text": "M.1 Hyperparameters Setting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The learning rate used in the inversion method is 0.1 in all tasks, as we empirically observe that it always finds the latent vector that generates the target discrete data. The maximum iteration number of the inversion method is 1,000 in all tasks. The rest of the hyperparameters follow the setting used in [14]. Since arithmetic fitting tasks and Guacamol with the small budget tasks use different initial data numbers used in [14], we set the number of top- $\\cdot k$ data and the number of query points $N_{q}$ same as DRD3 task, as they use the same number of initial data points. ", "page_idx": 19}, {"type": "text", "text": "N Pseudocode of Potential-aware Trust Region Anchor Selection ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here, we provide the pseudocode of the potential-aware trust region anchor selection method. We get the max acquisition function value, which is a Thompson Sampling, of each trust region $\\mathcal{T}^{i}$ given a ", "page_idx": 19}, {"type": "table", "img_path": "TrN5TcWY87/tmp/2b5df820151f08317440f97794d442cd75b1b4f6b3dc3c30b846de11f752d518.jpg", "table_caption": ["Table 4: Other hyperparameters for various benchmarks. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "TrN5TcWY87/tmp/165ce35174caf1d6bc38c07741d513f13858c8eda33b9924511e305b4d901c36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "candidate set $Z_{\\mathrm{cand}}^{i}$ in lines 2-3. After scaling the max acquisition function values, we calculate the final score of each trust region $s^{i}$ in lines 8-9. Finally, we sort the anchors in the descending order of the final score and select the anchor. ", "page_idx": 20}, {"type": "text", "text": "O Pseudocode of InvBO Applied to CoBO ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We provide the pseudocode of CoBO applying our method, InvBO, which consists of the inversion method and the potential-aware anchor selection. The inversion method is used in lines 1 and 8, and the potential-aware anchor selection method is used in line 11. As in CoBO, we use the subset of data history, which is composed of the top $k$ scored data and the most recently evaluated data during the optimization process in line 4. When we fail $N_{\\mathrm{fail}}$ times to update the best score, we train the VAE with CoBO loss $\\mathcal{L}_{\\mathrm{CoBO}}$ and update the dataset to be aligned using the inversion method. After that, we train the surrogate model with $\\mathcal{D}^{t}$ and negative log-likelihood loss ${\\mathcal{L}}_{\\mathrm{surr}}$ except for the first iteration and select the trust region anchor with the proposed potential-aware trust region anchor selection in line 11. We select $N_{q}$ next query points from the trust region in line 12. Then, we evaluate the next query points with the objective function and update the data history in lines 17, and 18. ", "page_idx": 20}, {"type": "text", "text": "Algorithm 3 InvBO built on CoBO ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Input: Pretrained encoder $q_{\\phi}$ , decoder $p_{\\theta}$ , black-box function $f$ , surrogate model $g$ , acquisition   \nfunction $\\alpha$ , oracle budget $T$ , latent update interval $N_{\\mathrm{fail}}$ , number of query point $N_{q}$ , initial data   \n$\\mathcal{D}^{0}=\\left\\{(\\mathbf{x}^{i},y^{i})\\right\\}_{i=1}^{n}$ , learning rate for inversion $\\eta$ , distance function $d_{\\mathcal{X}}$   \n1: $\\begin{array}{r l}{\\mathcal{D}^{0}\\gets\\left\\{\\left(\\mathbf{x}^{i},\\mathbf{z}_{\\mathrm{inv}}^{i},y^{i}\\right)|\\left(\\mathbf{x}^{i},y^{i}\\right)\\in\\mathcal{D}^{0},\\mathbf{z}_{\\mathrm{inv}}^{i}\\gets\\mathrm{Inversion}\\left(\\mathbf{x}^{i},q_{\\phi},p_{\\theta},\\eta,d_{\\mathcal{X}}\\right)\\right\\}_{i=1}^{n}}&{\\triangleright A l g o r i t h m\\;I}\\end{array}$   \n2: $n_{\\mathrm{fail}}\\leftarrow0$   \n3: for $t=1,2,...,T$ do   \n4: Dt \u2190CONCAT $\\left({\\cal D}^{t-1}[-{\\cal N}_{q}:],t o p k({\\cal D}^{t-1})\\right)$   \n5: if $n_{\\mathrm{fail}}\\leq N_{\\mathrm{fail}}$ then   \n6: $n_{\\mathrm{fail}}\\leftarrow0$   \n7: Train $q_{\\phi}$ and $p_{\\theta}$ with $\\mathcal{L}_{\\mathrm{CoBO}},\\mathcal{D}^{t}$   \n8: $\\mathcal{D}^{t}\\xleftarrow{}\\left\\{\\left(\\mathbf{x}^{i},\\mathbf{z}_{\\mathrm{inv}}^{i},y^{i}\\right)|\\left(\\mathbf{x}^{i},\\mathbf{z}^{i},y^{i}\\right)\\in\\mathcal{D}^{t},\\mathbf{z}_{\\mathrm{inv}}^{i}\\gets\\mathrm{Inversion}\\left(\\mathbf{x}^{i},q_{\\phi},p_{\\theta},\\eta,d_{X}\\right)\\right\\}_{i=1}^{|\\mathcal{D}^{t}|}$ \u25b7   \nAlgorithm 1   \n9: end if   \n10: Train $g$ with $\\mathcal{L}_{\\mathrm{surr}},\\mathcal{D}^{t}$ if $t\\neq1$ else $\\mathcal{D}^{0}$   \n11: $Z_{\\mathrm{anchor}}\\leftarrow$ Trust Region Anchor Selection $(\\mathcal{D}^{t},g)$ \u25b7Algorithm 2   \n12: $Z_{\\mathrm{next}}\\gets$ Select $N_{q}$ query points in trust region centered on $Z_{\\mathrm{anchor}}$   \n13: y\u2217\u2190max(x,z,y)\u2208Dt y   \n14: if $f(p_{\\theta}(\\mathbf{z}^{i}))\\leq y^{*},\\ \\forall\\mathbf{z}^{i}\\in Z_{\\mathrm{next}}$ then   \n15: $n_{\\mathrm{fail}}\\leftarrow n_{\\mathrm{fail}}+1$   \n1167:: $\\mathcal{D}_{\\mathrm{new}}\\leftarrow\\left\\{\\left(p_{\\theta}(\\mathbf{z}^{i}),\\mathbf{z}^{i},f(p_{\\theta}(\\mathbf{z}^{i}))\\right)|\\mathbf{z}^{i}\\in Z_{\\mathrm{next}}\\right\\}_{i=1}^{|Z_{\\mathrm{next}}|}$   \n18: $\\mathcal{D}^{t}\\leftarrow\\mathrm{CONCAT}\\left(\\mathcal{D}^{t},\\mathcal{D}_{\\mathrm{new}}\\right)$   \n19: end for   \n20: $(\\mathbf{x}^{*},\\mathbf{z}^{*},y^{*})\\gets\\arg\\operatorname*{max}_{(\\mathbf{x},\\mathbf{z},y)\\in\\mathcal{D}^{T}}y$   \n21: return x\u2217 ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The main claims reflect the paper\u2019s contributions and scope sufficiently. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The paper provide the limitation after the Section 7. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper provide the theory and The full set of assumption on Proposition 1.   \nA complete proof of the proposition in Section A. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper provide the implementation details including the hyperparameter settings in Section 5.3, M and the details of baselines in Section L, and the details of benchmark tasks in Section K. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper provide anonymized code that satisfied the submission guidelines. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper provide the implementation details including the hyperparameter settings in Section 5.3, M and the details of baselines in Section L, and the details of benchmark tasks in Section K. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The paper depict the errorbar and the error statistic on every performance results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper provide the computer resources including the time of excution in Section J. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper conform the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper provide the broader impacts after the Section 7. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not include the safeguards for misusing. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper provide the license in Section M and used the assets properly. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper provide the code and documentation of it. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]