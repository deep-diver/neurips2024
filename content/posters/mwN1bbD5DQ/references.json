{"references": [{"fullname_first_author": "Yezhen Cong", "paper_title": "SatMAE: Pre-training transformers for temporal and multi-spectral satellite imagery", "publication_date": "2022-12-01", "reason": "This paper introduces SatMAE, a foundational model for remote sensing, which is directly relevant to the current paper's approach of adapting pre-trained models to RS tasks."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "This paper introduces LoRA, a data-efficient transfer learning method, which is the foundation of the proposed debLoRA method in the current paper."}, {"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-06-01", "reason": "This paper introduces masked autoencoders, a powerful self-supervised learning approach, which is relevant to the paper's discussion on foundation models and related works."}, {"fullname_first_author": "Jian Ding", "paper_title": "Object detection in aerial images: A large-scale benchmark and challenges", "publication_date": "2021-11-01", "reason": "This paper introduces the DOTA dataset, a crucial benchmark used in the paper to evaluate the performance of different methods."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision model pre-trained on natural images, which is used in the paper for transfer learning experiments."}]}