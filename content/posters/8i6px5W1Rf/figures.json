[{"figure_path": "8i6px5W1Rf/figures/figures_1_1.jpg", "caption": "Figure 1: Task descriptions. (A) An example trial from the category learning task, where an incorrect decision is made. (B) An example trial from the reward learning task where the best option is chosen and highlighted in orange. (C) Example images from the THINGS database [30]. The database has a low dimensional semantically interpretable embedding [27], which is derived from human similarity judgements. The example images are placed in the most three prominent dimensions of this embedding. In both tasks, participants were randomly assigned to one of these three dimensions. The associated category membership and rewards for the two tasks are displayed.", "description": "This figure illustrates the two experimental tasks used in the study: category learning and reward learning.  Panel A shows a sample trial from the category learning task, where participants had to assign images to one of two categories represented by dinosaurs.  Panel B shows a sample trial from the reward learning task where participants select the image associated with the higher reward. Panel C displays example images from the THINGS database, showing how images are categorized along three main dimensions (metal/artificial/hard, food-related, animal-related/organic) based on human similarity judgments. These dimensions were used to define the category boundaries in the category learning task and the reward scale in the reward learning task.", "section": "1 Introduction"}, {"figure_path": "8i6px5W1Rf/figures/figures_2_1.jpg", "caption": "Figure 2: Learning trajectories of human participants and neural networks. Neural networks can perform as well as humans. (A & B) Accuracy of human participants across trials for the category and the reward learning tasks respectively. Shaded lines indicate 95% confidence intervals. (C & D) Example learning curves for the neural network representations in the category and the reward learning tasks respectively. The best-performing models from each model type are shown.", "description": "This figure displays the learning curves for both human participants and neural networks across two tasks: category learning and reward learning.  Panels A and B show human performance, illustrating that humans quickly learn the tasks. Panels C and D present the learning curves for neural networks, demonstrating that several models achieve accuracy comparable to humans.  The figure highlights the performance of the best-performing models within different model categories (supervised, self-supervised, multimodal, and language).", "section": "Experiments"}, {"figure_path": "8i6px5W1Rf/figures/figures_3_1.jpg", "caption": "Figure 3: Model fits to human choice data. In both category learning (A) and reward learning tasks (B), several CLIP models predict human choices the best, even better than the generative features of the tasks. How well the models fitted human choice was more heterogeneously distributed for supervised, self-supervised, and language models. Plotted are the cross-validated McFadden's R2 of each representation for the category learning and the reward learning tasks respectively. Higher values indicate better fits to human behaviour. 0 marks the alignment of a random model.", "description": "This figure displays the goodness of fit of various neural network models in predicting human choices in two tasks: category learning and reward learning.  The McFadden's R-squared statistic is used to quantify the fit, with higher values indicating better alignment between model predictions and human choices. CLIP models consistently show the best fit, outperforming even the generative features used to create the tasks, while supervised, self-supervised and language models show more variability in their performance.", "section": "Model-based analyses"}, {"figure_path": "8i6px5W1Rf/figures/figures_5_1.jpg", "caption": "Figure 4: Several factors contribute to alignment. Models trained on more data and with more trainable parameters predict human choices with higher accuracy. Turning to representations, those that better separate image classes and are more similar to the generative task features exhibit stronger alignment with human choices.", "description": "This figure displays the correlation between several factors and the alignment of neural network models with human choices in two image-based learning tasks.  The factors examined include task accuracy, the number of model parameters, the number of training images, class separation in representations, the similarity between representations and the generative task embedding (measured by Kernel Alignment), and intrinsic dimensionality.  The results indicate a positive correlation between model size, amount of training data, class separation, and similarity to the task embedding, and alignment with human choices.  Conversely, lower intrinsic dimensionality is associated with higher alignment, particularly for multimodal models.", "section": "4 Model-based analyses"}, {"figure_path": "8i6px5W1Rf/figures/figures_5_2.jpg", "caption": "Figure 5: Lower intrinsic dimensionality is linked with higher alignment only for the multimodal models.", "description": "This figure displays the relationship between intrinsic dimensionality and alignment (measured by Pearson correlation) for different model types (supervised, self-supervised, and multimodal).  It reveals that lower intrinsic dimensionality correlates positively with higher human alignment, but only for the multimodal models. This finding suggests that the capability of multimodal models to compress input effectively contributes to their better alignment with human behaviour in the cognitive tasks described in the paper.", "section": "4 Model-based analyses"}, {"figure_path": "8i6px5W1Rf/figures/figures_6_1.jpg", "caption": "Figure 6: The effect of CLIP loss while controlling for model size and data. We observed that CLIP loss increases alignment when data size and architecture are controlled. Here plotted are (A) McFadden's R2, (B) task accuracy, (C) class-separation, (D) similarity with the task embedding, and (E) intrinsic dimensionality across model sizes and loss functions.", "description": "This figure analyzes the impact of CLIP loss on model alignment with human choices, while controlling for model size and dataset size.  It displays five key metrics across three model sizes (small, base, large) trained with three loss functions: CLIP, SimCLR, and CLIP+SimCLR.  The metrics visualized are McFadden's R2 (model fit to human choices), task accuracy, class separation, similarity to the task embedding (CKA), and intrinsic dimensionality.  The figure helps determine if the benefits of CLIP are due to its specific loss function or other factors such as increased model capacity and data.", "section": "4 Model-based analyses"}, {"figure_path": "8i6px5W1Rf/figures/figures_7_1.jpg", "caption": "Figure 7: We compared models aligned to humans through three different methods against baselines that had the same architecture and that were pretrained on the same data. Only two of the gLocal models showed increased alignment in our tasks. Here plotted are (A) McFadden's R2, (B) task accuracy, (C) class-separation, (D) similarity with the task embedding, and (D) intrinsic dimensionality across model sizes and loss functions.", "description": "This figure compares the performance of three different human-aligned models (Harmonization, DreamSim, and gLocal) against their baselines across various metrics.  The metrics include McFadden's R2 (a measure of model fit), task accuracy, class separation (how well the model separates different classes in its representations), CKA similarity with the task embedding (how similar the model's representation is to the task's features), and intrinsic dimensionality (a measure of the complexity of the model's representation).  The results show that only some gLocal models exhibit improved alignment compared to their baselines, suggesting the effectiveness of human alignment techniques is highly dependent on the method and model used.", "section": "4 Model-based analyses"}, {"figure_path": "8i6px5W1Rf/figures/figures_8_1.jpg", "caption": "Figure 8: How do our tasks compare to other alignment methods? Our tasks offer similar (but not identical) results with similarity judgement tasks. There is a strong negative relationship with the ClickMe dataset, which focuses on localised pixel-level alignment.", "description": "This figure shows the correlation between the alignment scores obtained from the authors' proposed tasks and those obtained from other established alignment methods in the literature. The methods used for comparison include: Muttenthaler et al. (2023) odd-one-out zero-shot and probing accuracy, Peterson et al. (2018) pairwise similarity correlation, Fu et al. (2023) two-alternative forced-choice accuracy, and Fel et al. (2022) ClickMe-harmonizer alignment. The results indicate that the authors' tasks show stronger correlations with similarity judgment tasks and a negative correlation with pixel-level alignment measures.", "section": "How do our tasks compare to other alignment measures?"}, {"figure_path": "8i6px5W1Rf/figures/figures_18_1.jpg", "caption": "Figure 3: Model fits to human choice data. In both category learning (A) and reward learning tasks (B), several CLIP models predict human choices the best, even better than the generative features of the tasks. How well the models fitted human choice was more heterogeneously distributed for supervised, self-supervised, and language models. Plotted are the cross-validated McFadden's R2 of each representation for the category learning and the reward learning tasks respectively. Higher values indicate better fits to human behaviour. 0 marks the alignment of a random model.", "description": "This figure shows the performance of various neural network models in predicting human choices in two tasks: category learning and reward learning.  The McFadden's R-squared (R2) values represent the goodness of fit, with higher values indicating better alignment with human behavior.  The figure highlights that CLIP models generally outperform other models, even exceeding the performance of the generative features used to create the tasks.  The performance of supervised, self-supervised, and language models is more varied.", "section": "4 Model-based analyses"}, {"figure_path": "8i6px5W1Rf/figures/figures_19_1.jpg", "caption": "Figure 3: Model fits to human choice data. In both category learning (A) and reward learning tasks (B), several CLIP models predict human choices the best, even better than the generative features of the tasks. How well the models fitted human choice was more heterogeneously distributed for supervised, self-supervised, and language models. Plotted are the cross-validated McFadden's R2 of each representation for the category learning and the reward learning tasks respectively. Higher values indicate better fits to human behaviour. 0 marks the alignment of a random model.", "description": "This figure shows the performance of various neural network models in predicting human choices in two tasks: category learning and reward learning. The model performance is measured using McFadden's R-squared, which indicates the goodness of fit. CLIP models consistently outperformed other model types, suggesting their representations are highly aligned with human decision-making processes. The figure also reveals that alignment varied significantly across different model training approaches.", "section": "4 Model-based analyses"}, {"figure_path": "8i6px5W1Rf/figures/figures_19_2.jpg", "caption": "Figure 7: We compared models aligned to humans through three different methods against baselines that had the same architecture and that were pretrained on the same data. Only two of the gLocal models showed increased alignment in our tasks. Here plotted are (A) McFadden\u2019s R2, (B) task accuracy, (C) class-separation, (D) similarity with the task embedding, and (D) intrinsic dimensionality across model sizes and loss functions.", "description": "This figure compares the performance of three human-aligned models (Harmonization, DreamSim, and gLocal) against their respective baselines across various metrics (McFadden's R2, task accuracy, class separation, similarity with the task embedding, and intrinsic dimensionality) for two naturalistic learning tasks (category and reward learning). The results show that only some gLocal models exhibited improved alignment compared to their baselines, while others showed either no improvement or even a decrease in alignment.", "section": "4 Model-based analyses"}, {"figure_path": "8i6px5W1Rf/figures/figures_20_1.jpg", "caption": "Figure 12: Participant Performance Against Chance Level at Each Trial. Trial-by-trial p-values from 1 sample t-tests testing accuracy against chance level for (A) category learning task and the (B) reward learning task.", "description": "The figure shows the results of 1-sample t-tests comparing participants' performance to chance level for each trial in both the category learning and reward learning tasks.  The x-axis represents the trial number, and the y-axis shows the p-value. The dashed horizontal line indicates the significance threshold (p=0.05). Points above the line indicate that the participants' performance was not significantly different from chance in that trial, while those below represent trials where the participants performed significantly above chance. The plot illustrates how quickly participants learned to perform above chance in both tasks.", "section": "3 Behavioural analyses"}, {"figure_path": "8i6px5W1Rf/figures/figures_20_2.jpg", "caption": "Figure 13: Example trials showing the similarity between CLIP and human decisions that show disagreement with the task embedding. Each row shows three trials from a different condition. Orange highlighted text shows the option chosen by all CLIP models and the human participant, whereas grey text shows the decision made by the task embedding. As the tasks were generated using the task embedding, all the choices shown here made by CLIP and humans are suboptimal. Shown examples are from the second half of the task, as to eliminate the learning process as a confound. The original images are replaced with copyright-free alternatives from the THINGSplus database [89].", "description": "This figure displays example trials from the category and reward learning tasks, highlighting instances where both CLIP models and human participants made the same incorrect choices. These examples illustrate scenarios where the choices made deviate from those predicted by the task's generative embedding, demonstrating the complexities involved in aligning human decisions with model predictions.", "section": "3 Behavioural analyses"}, {"figure_path": "8i6px5W1Rf/figures/figures_21_1.jpg", "caption": "Figure 4: Several factors contribute to alignment. Models trained on more data and with more trainable parameters predict human choices with higher accuracy. Turning to representations, those that better separate image classes and are more similar to the generative task features exhibit stronger alignment with human choices.", "description": "This figure shows the results of an analysis investigating factors that contribute to the alignment between human choices and neural network model predictions.  Panel A shows the relationship between task accuracy and model parameters, training images, class separation (R2), CKA similarity to task embeddings, and intrinsic dimensionality. Panels B-F depict these relationships individually, with statistical significance (p-values) indicated for each correlation.  The results suggest that models with more parameters, trained on larger datasets, with better class separation, and greater similarity to human representations achieve better alignment.", "section": "Which factors contribute to alignment?"}]