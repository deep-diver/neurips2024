[{"figure_path": "8i6px5W1Rf/tables/tables_22_1.jpg", "caption": "Table 1: Details on Models and Model Performance. Here, we report the model size, the type, and the negative log-likelihoods (NLL) for different tasks and different learning models. For a given representation and task, the best-performing learning model\u2019s NLL is highlighted in bold. The models are ordered such that the ones reported higher have lower negative log likelihoods across the two asks, as measured through the performance of the linear models reported in the main text.", "description": "This table presents detailed information on 86 pretrained neural network models used in the study.  For each model, it lists its type (e.g., supervised, self-supervised, multimodal), the number of parameters in millions, and the negative log-likelihood (NLL) scores achieved in two tasks: category learning and reward learning.  The NLL is a measure of how well the model's representations fit the human choices in each task.  Lower NLL indicates better fit to human data. The table is crucial for comparing the performance of different models, aiding in the analysis of factors contributing to alignment between human and model representation.", "section": "A Methods"}, {"figure_path": "8i6px5W1Rf/tables/tables_23_1.jpg", "caption": "Table 1: Details on Models and Model Performance. Here, we report the model size, the type, and the negative log-likelihoods (NLL) for different tasks and different learning models. For a given representation and task, the best-performing learning model's NLL is highlighted in bold. The models are ordered such that the ones reported higher have lower negative log likelihoods across the two asks, as measured through the performance of the linear models reported in the main text.", "description": "This table presents detailed information about the 86 pretrained neural network models used in the study.  For each model, it lists the type (Supervised, Self-Supervised, Multimodal, Language), the number of parameters (in millions), and the negative log-likelihood (NLL) scores for both the category and reward learning tasks.  The NLL is a measure of how well the model's predictions fit the human data.  Lower NLL values indicate better fits. The table also shows results from three variations of the model training processes (Main, Sparse, PCA) to explore the impact of different training methods on model performance and alignment with human behaviour.", "section": "A Methods"}, {"figure_path": "8i6px5W1Rf/tables/tables_24_1.jpg", "caption": "Table 1: Details on Models and Model Performance. Here, we report the model size, the type, and the negative log-likelihoods (NLL) for different tasks and different learning models. For a given representation and task, the best-performing learning model\u2019s NLL is highlighted in bold. The models are ordered such that the ones reported higher have lower negative log likelihoods across the two asks, as measured through the performance of the linear models reported in the main text.", "description": "This table presents detailed information about the 86 pretrained neural network models used in the study. It includes the model name, type (supervised, self-supervised, multimodal, or language), number of parameters (in millions), and the negative log-likelihood (NLL) for each model on both the category learning and reward learning tasks.  The NLL measures how well each model predicts human choices in the respective tasks. Lower NLL values indicate better predictive performance. The models are ranked according to their overall performance (combined NLL across both tasks).", "section": "A Methods"}, {"figure_path": "8i6px5W1Rf/tables/tables_25_1.jpg", "caption": "Table 2: Investigated factors that contribute to alignment for each model", "description": "This table presents the results of an analysis investigating the factors that contribute to the alignment of neural network representations with human choices.  For a range of neural network models, the table shows task accuracy, training image size, class separation (R2), centered kernel alignment (CKA) similarity with task embedding, and intrinsic dimensionality. This allows for a comparison of the various factors' influence on the level of alignment between human choices and model predictions.", "section": "Additional results"}, {"figure_path": "8i6px5W1Rf/tables/tables_26_1.jpg", "caption": "Table 2: Investigated factors that contribute to alignment for each model", "description": "This table presents a detailed breakdown of the factors contributing to the alignment of various neural network models with human performance in the image-based learning tasks.  For each model, it lists its type (supervised, self-supervised, multimodal, or language), the number of parameters (in millions), the task accuracy, the number of training images (in millions), the class separation (R2), the Centered Kernel Alignment (CKA) similarity with the task embedding, and the intrinsic dimensionality. This allows for a comparison of different model characteristics and their relative impact on alignment.", "section": "Additional results"}]