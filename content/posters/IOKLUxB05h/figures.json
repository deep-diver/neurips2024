[{"figure_path": "IOKLUxB05h/figures/figures_1_1.jpg", "caption": "Figure 1: Our LE-SINR model takes as input free-form text describing aspects of a species' preferred habitat or range and geospatially grounds it to generate a plausible range map for that species.", "description": "This figure illustrates the overall process of the LE-SINR model.  The input is a text description of a species' habitat or range (e.g., \"Fairly common in Andean cloud forest from Colombia to Peru, typically from around 800\u20131800m\"). This text is then processed by the LE-SINR model, which combines language processing with geospatial information to produce a predicted species range map visualized on a world map.", "section": "1 Introduction"}, {"figure_path": "IOKLUxB05h/figures/figures_2_1.jpg", "caption": "Figure 2: LE-SINR learns to align location and text representations at training time using presence-only observation data and habitat or range descriptions for a set of species. Optionally, we can also include a learnable species token Ey allowing range estimation for seen species y from the training set. The model is trained on millions of observations from iNaturalist and language data from Wikipedia articles across thousands of species. LE-SINR supports zero-shot range estimation based on text descriptions for novel species and can also be used as a prior for few-shot range estimation.", "description": "This figure illustrates the LE-SINR model architecture.  The model takes both location data (latitude and longitude) and text descriptions of a species as input.  These inputs are processed separately through location and language encoders (using an LLM), which map them into a shared embedding space.  The model can then use these embeddings to predict the species' range.  Optionally, a learnable species token can be included to enhance predictions for species observed in the training data. The entire architecture is designed for both zero-shot (predicting species ranges with only text descriptions) and few-shot (combining text and a limited number of observations) range estimation.", "section": "3.3 Language Enhanced SINR (LE-SINR) Architecture"}, {"figure_path": "IOKLUxB05h/figures/figures_7_1.jpg", "caption": "Figure 3: Range Estimation from Text and Observations. (Left) IUCN and (Middle) S&T results for zero-shot range estimation based on text, and few-shot estimation based on the text-driven prior. Both range and habitat texts improve few-shot performance over baseline SINR. (Right) Comparison of the position branch of SINR and LE-SINR for range estimation using a few examples. Language-driven covariates learned by LE-SINR lead to better generalization when observations are limited. We report the MAP for range estimation for species in the S&T and IUCN test sets.", "description": "This figure compares the performance of the proposed LE-SINR model with the baseline SINR model in both zero-shot and few-shot range estimation settings using the IUCN and S&T datasets. The left and middle panels illustrate the zero-shot and few-shot results, demonstrating that using textual descriptions of species (habitat or range) enhances few-shot performance. The right panel displays the comparison of position branch features from the two models, highlighting the superior generalization capability of LE-SINR in data-scarce situations.", "section": "4 Results"}, {"figure_path": "IOKLUxB05h/figures/figures_8_1.jpg", "caption": "Figure 4: Zero-Shot Range Estimation. Here we show the 'Habitat' and 'Range' text descriptions and corresponding zero-shot range maps for the Hyacinth Macaw (top) and the Yellow Baboon (bottom), with expert derived range maps inset.", "description": "This figure demonstrates the zero-shot range estimation capabilities of the LE-SINR model.  It shows predicted range maps generated solely from text descriptions of the Hyacinth Macaw's habitat and range, and the Yellow Baboon's habitat and range. The predictions are compared to expert-derived range maps to evaluate the accuracy of the zero-shot approach.  The figure highlights the model's ability to translate textual information into geographically relevant predictions. The inset shows the expert range.", "section": "4 Results"}, {"figure_path": "IOKLUxB05h/figures/figures_8_2.jpg", "caption": "Figure 5: Geospatial Grounding of Non-Species Concepts. LE-SINR is able to geographically ground text prompts to locations on the earth. Here we display the inner product between the location encoder's features and the language model's encoding of the text displayed in the bottom left of each panel. This includes coarse concepts such as continents and countries (top row), geographic features such as specific lakes and mountain ranges (second row), in addition to concepts that do not appear in our species text training data but are likely already represented in the language model (third and fourth row). We do, however, observe some limitations resulting from the biases in our training data which favors North America, Europe, and Australasia (final row). Please zoom in to see more detail.", "description": "This figure demonstrates LE-SINR's ability to geospatially ground various text prompts, even those unrelated to species.  The heatmaps show the model's predicted probability of a concept's presence at each location, based solely on the textual input.  The results reveal that the model learns geospatial relationships from both coarse (continents, countries) and finer-grained (lakes, mountain ranges) concepts. While generally successful, it exhibits biases reflecting its training data's geographic distribution.", "section": "4 Results"}, {"figure_path": "IOKLUxB05h/figures/figures_8_3.jpg", "caption": "Figure 4: Zero-Shot Range Estimation. Here we show the 'Habitat' and 'Range' text descriptions and corresponding zero-shot range maps for the Hyacinth Macaw (top) and the Yellow Baboon (bottom), with expert derived range maps inset.", "description": "This figure demonstrates the results of zero-shot range estimation using only text descriptions of a species' habitat and range.  It shows two examples: the Hyacinth Macaw and the Yellow Baboon. For each species, the figure presents three maps: a map generated from a text description of the species' habitat preferences, a map generated from a text description of the species' known range, and a map representing expert-derived range data. Comparing the model's range predictions to the expert data allows for an assessment of the model's accuracy.", "section": "4 Results"}, {"figure_path": "IOKLUxB05h/figures/figures_8_4.jpg", "caption": "Figure 5: Geospatial Grounding of Non-Species Concepts. LE-SINR is able to geographically ground text prompts to locations on the earth. Here we display the inner product between the location encoder's features and the language model's encoding of the text displayed in the bottom left of each panel. This includes coarse concepts such as continents and countries (top row), geographic features such as specific lakes and mountain ranges (second row), in addition to concepts that do not appear in our species text training data but are likely already represented in the language model (third and fourth row). We do, however, observe some limitations resulting from the biases in our training data which favors North America, Europe, and Australasia (final row). Please zoom in to see more detail.", "description": "This figure shows the ability of the LE-SINR model to geographically ground various text prompts, even those not directly related to species.  The heatmaps illustrate the model's spatial understanding, highlighting how it associates concepts like continents, countries, and specific geographical features with their corresponding locations. While showing strong performance overall, biases in the training data (favoring North America, Europe, and Australasia) are apparent.", "section": "4 Results"}, {"figure_path": "IOKLUxB05h/figures/figures_9_1.jpg", "caption": "Figure 5: Geospatial Grounding of Non-Species Concepts. LE-SINR is able to geographically ground text prompts to locations on the earth. Here we display the inner product between the location encoder's features and the language model's encoding of the text displayed in the bottom left of each panel. This includes coarse concepts such as continents and countries (top row), geographic features such as specific lakes and mountain ranges (second row), in addition to concepts that do not appear in our species text training data but are likely already represented in the language model (third and fourth row). We do, however, observe some limitations resulting from the biases in our training data which favors North America, Europe, and Australasia (final row). Please zoom in to see more detail.", "description": "This figure demonstrates the ability of the LE-SINR model to geospatially ground various text prompts, including geographical concepts (continents, countries, lakes, mountain ranges), and non-geographical concepts (historical events, pop culture figures). The heatmaps show the model's predicted probability of a concept's presence at different locations. The results showcase the model's capacity to learn and extrapolate from both species-related and non-species-related data, but also highlight limitations due to dataset bias.", "section": "4 Results"}, {"figure_path": "IOKLUxB05h/figures/figures_13_1.jpg", "caption": "Figure A1: Visualization of the intermediate position representation learned by the network projected to three dimensions using Independent Component Analysis. (Top) Standard SINR model. (Bottom) Our LE-SINR model. Both models were trained with position features as the only input.", "description": "This figure compares the intermediate position representations learned by SINR and LE-SINR models.  Both models were trained using only position features. The visualization uses Independent Component Analysis (ICA) to project the high-dimensional embeddings into three dimensions (for visualization purposes), represented by color in RGB space. The differences in the visualizations highlight how LE-SINR learns richer spatial information than SINR. This richer representation is a contributing factor to the improved performance of LE-SINR, especially in few-shot scenarios.", "section": "A Additional Results"}, {"figure_path": "IOKLUxB05h/figures/figures_14_1.jpg", "caption": "Figure 3: Range Estimation from Text and Observations. (Left) IUCN and (Middle) S&T results for zero-shot range estimation based on text, and few-shot estimation based on the text-driven prior. Both range and habitat texts improve few-shot performance over baseline SINR. (Right) Comparison of the position branch of SINR and LE-SINR for range estimation using a few examples. Language-driven covariates learned by LE-SINR lead to better generalization when observations are limited. We report the MAP for range estimation for species in the S&T and IUCN test sets.", "description": "This figure compares the performance of the proposed LE-SINR model against the baseline SINR model for range estimation, under zero-shot and few-shot learning settings using IUCN and S&T datasets.  The left and middle panels show the mean average precision (MAP) for zero-shot range estimations based solely on text descriptions (habitat and range), and for few-shot estimations incorporating the text-based priors alongside limited observational data. The right panel illustrates the advantage of LE-SINR's learned positional embeddings in the few-shot setting; LE-SINR generalizes better with fewer observations.", "section": "4 Results"}, {"figure_path": "IOKLUxB05h/figures/figures_15_1.jpg", "caption": "Figure A3: Additional Zero-Shot Range Estimation. Here we show the 'Habitat' (left) and 'Range' (center) text zero-shot range maps with associated expert derived range maps (right), for a variety of species. While the 'Range' text provides a strong prior in all cases with high probability assigned to regions within the expert derived range, the 'Habitat' text does not always do this, with the Striated Babbler, Striped Sticky Frog, and the Raucous Toad providing no strong prior. Zoom in to see details.", "description": "This figure in the appendix visualizes the results of zero-shot range estimation experiments for six different species. The maps compare the predicted range maps using habitat descriptions (left column), range descriptions (middle column), and expert-derived range maps (right column).  It demonstrates that range descriptions provide more accurate predictions aligned with the expert maps, while habitat descriptions are sometimes less precise, highlighting the different information each description type provides for accurate range prediction.  The figure emphasizes the value of precise, location-specific information contained within range descriptions for successful zero-shot range estimation, versus the sometimes less accurate estimation when using habitat descriptions alone.", "section": "A Additional Results"}, {"figure_path": "IOKLUxB05h/figures/figures_16_1.jpg", "caption": "Figure A1: Visualization of the intermediate position representation learned by the network projected to three dimensions using Independent Component Analysis. (Top) Standard SINR model. (Bottom) Our LE-SINR model. Both models were trained with position features as the only input.", "description": "This figure compares the learned position embeddings of the standard SINR model and the LE-SINR model, both trained using only position features. The LE-SINR model incorporates language data, which is reflected in the visualizations.  By projecting the embeddings into three dimensions using Independent Component Analysis (ICA), the figure allows for visualization of these learned representations.  The visualization highlights differences in the spatial structure captured by the two models, suggesting that the LE-SINR model learns a richer and potentially more informative representation of geographic space.", "section": "A Additional Results"}, {"figure_path": "IOKLUxB05h/figures/figures_16_2.jpg", "caption": "Figure 5: Geospatial Grounding of Non-Species Concepts. LE-SINR is able to geographically ground text prompts to locations on the earth. Here we display the inner product between the location encoder's features and the language model's encoding of the text displayed in the bottom left of each panel. This includes coarse concepts such as continents and countries (top row), geographic features such as specific lakes and mountain ranges (second row), in addition to concepts that do not appear in our species text training data but are likely already represented in the language model (third and fourth row). We do, however, observe some limitations resulting from the biases in our training data which favors North America, Europe, and Australasia (final row). Please zoom in to see more detail.", "description": "This figure demonstrates the model's ability to geospatially ground both species-related and non-species-related concepts.  The heatmaps show the inner product between location embeddings and text embeddings for various terms, illustrating that the model effectively maps textual concepts to their corresponding geographical locations. Although generally successful, the visualizations also highlight biases present in the training data, which is skewed towards North America, Europe, and Australasia.", "section": "4 Results"}, {"figure_path": "IOKLUxB05h/figures/figures_16_3.jpg", "caption": "Figure A1: Visualization of the intermediate position representation learned by the network projected to three dimensions using Independent Component Analysis. (Top) Standard SINR model. (Bottom) Our LE-SINR model. Both models were trained with position features as the only input.", "description": "This figure visualizes the learned positional embeddings from both the standard SINR model and the LE-SINR model, projected into three dimensions using Independent Component Analysis (ICA).  The visualization helps to understand how the models represent spatial information.  The top panel shows the representation learned by SINR (trained only on location data), while the bottom panel shows the representation learned by LE-SINR (trained on location and language data). The difference highlights how incorporating language data enriches the model's spatial understanding.", "section": "A Additional Results"}, {"figure_path": "IOKLUxB05h/figures/figures_16_4.jpg", "caption": "Figure A1: Visualization of the intermediate position representation learned by the network projected to three dimensions using Independent Component Analysis. (Top) Standard SINR model. (Bottom) Our LE-SINR model. Both models were trained with position features as the only input.", "description": "This figure compares the learned position embeddings from two different models: the standard SINR model and the LE-SINR model. Both models were trained only using position features. The visualization is done in 3D using Independent Component Analysis. The visualization shows that LE-SINR learns a richer spatial structure than SINR.", "section": "A Additional Results"}, {"figure_path": "IOKLUxB05h/figures/figures_16_5.jpg", "caption": "Figure A1: Visualization of the intermediate position representation learned by the network projected to three dimensions using Independent Component Analysis. (Top) Standard SINR model. (Bottom) Our LE-SINR model. Both models were trained with position features as the only input.", "description": "This figure compares the position embeddings learned by the standard SINR model and the LE-SINR model, projected into three dimensions using Independent Component Analysis.  It visualizes how the learned representations differ when only position features are used for training.  The color intensity likely represents a magnitude of some feature, possibly indicating a higher concentration of certain characteristics or signals in specific geographic areas.  The difference is intended to show how LE-SINR's integration of language data contributes to richer spatial representations.", "section": "A Additional Results"}, {"figure_path": "IOKLUxB05h/figures/figures_16_6.jpg", "caption": "Figure A1: Visualization of the intermediate position representation learned by the network projected to three dimensions using Independent Component Analysis. (Top) Standard SINR model. (Bottom) Our LE-SINR model. Both models were trained with position features as the only input.", "description": "This figure compares the learned position embeddings of the standard SINR model and the LE-SINR model by projecting them into three dimensions using Independent Component Analysis (ICA). The visualization helps understand the differences in the spatial representations learned by each model.  Both models were trained using only position features. LE-SINR's spatial representation shows a richer structure compared to the standard SINR model, indicating that the incorporation of language data results in more detailed and informative spatial features.", "section": "A Additional Results"}]