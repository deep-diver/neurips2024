[{"figure_path": "GqefKjw1OR/figures/figures_5_1.jpg", "caption": "Figure 1: A schematic of the sparsity inducing CSVAE.", "description": "This figure shows a schematic of the sparsity-inducing CSVAE (Compressive Sensing Variational Autoencoder).  The input is a compressed observation y, which is fed into an encoder that outputs the mean (\u03bc\u03c6) and standard deviation (\u03c3\u03c6) of a latent variable z.  This latent variable is then sampled from a standard normal distribution N(0,I) and passed to a decoder that outputs the square root of the diagonal elements of the covariance matrix (\u221a\u03b3\u03b8) for the compressible representation s. This s is also sampled from a normal distribution N(0, I) and multiplied by the dictionary D and the measurement matrix A to generate a reconstruction of y which is corrupted by additive white Gaussian noise N(0,\u03c32I). The CSVAE is trained using variational inference to learn the parameters of the encoder and decoder such that the reconstruction closely matches the input y while promoting sparsity in s.", "section": "3.2 Compressive Sensing VAE"}, {"figure_path": "GqefKjw1OR/figures/figures_8_1.jpg", "caption": "Figure 2: a) and b) nMSE and SSIM over M (Nt = 20000, MNIST), c) and d) nMSE and SSIM over Nt (M = 160, MNIST), e) exemplary reconstructed MNIST images (M = 200, Nt = 20000), f) nMSE over M (SNRdB = 10dB, Nt = 10000, piece-wise smooth fct.), g) nMSE over Nt (SNRdB = 10dB, M = 100, piece-wise smooth fct.), h) exemplary reconstructed piece-wise smooth fct. (M = 100, Nt = 1000), i) nMSE comparison of dictionaries (MNIST, M = 160, Nt = 20000).", "description": "This figure presents the results of the experiments conducted to evaluate the performance of different algorithms on the MNIST dataset and a dataset of piecewise smooth functions.  Subfigures (a) and (b) show the normalized mean squared error (nMSE) and structural similarity index (SSIM) for varying numbers of measurements (M) while keeping the number of training samples constant.  Subfigures (c) and (d) show the same metrics but with a fixed number of measurements (M) and varying numbers of training samples (Nt). Subfigure (e) displays example reconstructions of MNIST digits. Subfigures (f) and (g) illustrate the performance on the piecewise smooth function dataset, with varying M and Nt respectively, and show nMSE results for signal-to-noise ratio (SNR) of 10dB. Subfigure (h) provides example reconstructions of the piecewise smooth functions.  Finally, subfigure (i) compares the performance of the algorithms using different types of dictionaries.", "section": "4.2 Results"}, {"figure_path": "GqefKjw1OR/figures/figures_9_1.jpg", "caption": "Figure 3: a) and b) nMSE and SSIM over M (Nt = 5000), c) and d) nMSE and SSIM over Nt (M = 1800), e) exemplary reconstructed celebA images (M = 2700, Nt = 5000), f) histogram of h(z|y) for compressed test MNIST images of digits 0, 1 and 7, where the CSVAE is trained on compressed zeros, g) training and reconstruction time for MNIST (M = 200, Nt = 20000).", "description": "Figure 3 presents the performance evaluation results of different models on the CelebA dataset.  The plots illustrate the nMSE and SSIM metrics against varying observation dimensions (M) and training sample sizes (Nt).  Exemplar reconstructed CelebA images are shown to visualize the models' performance. A histogram shows uncertainty quantification by displaying the differential entropy h(z|y) for the CSVAE trained only on compressed MNIST zeros. Finally, training and reconstruction times for the MNIST dataset are provided.", "section": "4.2 Results"}, {"figure_path": "GqefKjw1OR/figures/figures_18_1.jpg", "caption": "Figure 4: Exemplary signals within the 1D dataset of piecewise smooth functions.", "description": "This figure displays six example signals from a 1D dataset of piecewise smooth functions.  These signals are used in the paper's experiments to evaluate the proposed algorithm's performance.  The piecewise smooth nature of the functions is evident in the plots, exhibiting regions of relative smoothness interspersed with sharp transitions or discontinuities.", "section": "4 Experiments"}, {"figure_path": "GqefKjw1OR/figures/figures_19_1.jpg", "caption": "Figure 2: a) and b) nMSE and SSIM over M (N\u2081 = 20000, MNIST), c) and d) nMSE and SSIM over Nt (M = 160, MNIST), e) exemplary reconstructed MNIST images (M = 200, Nt = 20000), f) nMSE over M (SNRdB = 10dB, Nt = 10000, piece-wise smooth fct.), g) nMSE over Nt (SNRdB = 10dB, M = 100, piece-wise smooth fct.), h) exemplary reconstructed piece-wise smooth fct. (M = 100, Nt = 1000), i) nMSE comparison of dictionaries (MNIST, M = 160, Nt = 20000).", "description": "This figure displays the normalized mean squared error (nMSE) and structural similarity index (SSIM) for different models on the MNIST dataset and a dataset of piecewise smooth functions.  Subfigures (a) and (b) show the performance with a varying number of measurements (M) and a fixed number of training samples (Nt), while subfigures (c) and (d) show the opposite, keeping M fixed and varying Nt. Subfigure (e) provides example reconstructions of MNIST digits. Subfigures (f) and (g) show the results on the piecewise smooth functions, and subfigure (h) provides example reconstructions. Finally, subfigure (i) compares the performance with various dictionaries.", "section": "4.2 Results"}, {"figure_path": "GqefKjw1OR/figures/figures_20_1.jpg", "caption": "Figure 7: Exemplary reconstructed MNIST images for M = 200, N<sub>t</sub> = 20000 from a) models, which are solely trained on compressed data (with observations of dimension M), and b) models, which are trained on ground truth data.", "description": "This figure compares the reconstruction results of MNIST images using different models.  Subfigure (a) shows reconstructions from models trained only on compressed data without ground truth. Subfigure (b) shows reconstructions from models trained with ground truth data (either the compressible representation or the original image).  The comparison highlights the impact of having ground truth during model training on the accuracy of the reconstruction.  Each row represents a different model (SBL, CSGAN, CSGMM, and CSVAE), and each column shows a different image.", "section": "4.2 Results"}, {"figure_path": "GqefKjw1OR/figures/figures_21_1.jpg", "caption": "Figure 8: Exemplary reconstructed FashionMNIST images (M = 200, Nt = 20000, Fig. 6 g), h)).", "description": "This figure displays exemplary reconstructed images from the FashionMNIST dataset using various methods. The original images are shown for comparison alongside reconstructions generated by Lasso, SBL, CSGAN, CSVAE, and CSGMM.  The purpose is to visually demonstrate the comparative performance of the different algorithms in reconstructing detailed features of the clothing items.", "section": "Additional results"}, {"figure_path": "GqefKjw1OR/figures/figures_21_2.jpg", "caption": "Figure 4: Exemplary signals within the 1D dataset of piecewise smooth functions.", "description": "This figure shows ten example signals from the one-dimensional dataset of piecewise smooth functions used in the paper's experiments.  Each signal is a curve that demonstrates piecewise smoothness, meaning it consists of smooth segments connected with discontinuities. The signals are designed to be compressible, with statistical structure in their wavelet domain. This dataset is used to evaluate the performance of the proposed model and compare it with baseline methods for compressive sensing.", "section": "4 Experiments"}, {"figure_path": "GqefKjw1OR/figures/figures_21_3.jpg", "caption": "Figure 10: Exemplary reconstructed MNIST images (M = 160, Nt = 20000, Fig. 2 a))", "description": "This figure displays example images reconstructed using different methods: Original, Lasso, SBL, CSGAN, CSVAE (ours), and CSGMM (ours). The parameters used for reconstruction were M = 160 and Nt = 20000.  The figure visually demonstrates the quality of reconstruction achieved by each method, allowing for a qualitative comparison of their performance on MNIST image data. The quality of the reconstructed images from the original and the proposed method is similar, whereas the other reconstruction methods provide lower-quality outputs.", "section": "4.2 Results"}, {"figure_path": "GqefKjw1OR/figures/figures_21_4.jpg", "caption": "Figure 2: a) and b) nMSE and SSIM over M (Nt = 20000, MNIST), c) and d) nMSE and SSIM over Nt (M = 160, MNIST), e) exemplary reconstructed MNIST images (M = 200, Nt = 20000), f) nMSE over M (SNRdB = 10dB, Nt = 10000, piece-wise smooth fct.), g) nMSE over Nt (SNRdB = 10dB, M = 100, piece-wise smooth fct.), h) exemplary reconstructed piece-wise smooth fct. (M = 100, Nt = 1000), i) nMSE comparison of dictionaries (MNIST, M = 160, Nt = 20000).", "description": "The figure shows the performance of different compressive sensing methods on MNIST and piecewise smooth function datasets.  Subfigures (a) and (b) illustrate nMSE and SSIM versus the number of measurements (M) for a fixed number of training samples (Nt = 20000) on the MNIST dataset. Subfigures (c) and (d) show the same metrics but with a fixed M and varying Nt. Subfigure (e) displays example reconstructed MNIST images. Subfigures (f) and (g) show the results for the piecewise smooth function dataset with varying M and Nt, respectively, and include added noise. Subfigure (h) provides example reconstructions. Finally, subfigure (i) compares the performance of different dictionaries.", "section": "4.2 Results"}]