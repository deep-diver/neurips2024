[{"type": "text", "text": "NeCGS: Neural Compression for 3D Geometry Sets ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 This paper explores the problem of effectively compressing 3D geometry sets   \n2 containing diverse categories. We make the first attempt to tackle this fundamental   \n3 and challenging problem and propose NeCGS, a neural compression paradigm,   \n4 which can compress hundreds of detailed and diverse 3D mesh models $({\\sim}684\\,\\mathrm{MB})$ )   \n5 by about 900 times (0.76 MB) with high accuracy and preservation of detailed   \n6 geometric details. Specifically, we first represent each irregular mesh model/shape   \n7 in a regular representation that implicitly describes the geometry structure of the   \n8 model using a 4D regular volume, called TSDF-Def volume. Such a regular rep  \n9 resentation can not only capture local surfaces more effectively but also facilitate   \n10 the subsequent process. Then we construct a quantization-aware auto-decoder   \n11 network architecture to regress these 4D volumes, which can summarize the sim  \n12 ilarity of local geometric structures within a model and across different models   \n13 for redundancy elimination, resulting in more compact representations, including   \n14 an embedded feature of a smaller size associated with each model and a network   \n15 parameter set shared by all models. We finally encode the resulting features and   \n6 network parameters into bitstreams through entropy coding. After decompressing   \n17 the features and network parameters, we can reconstruct the TSDF-Def volumes,   \n18 where the 3D surfaces can be extracted through the deformable marching cubes.   \n19 Extensive experiments and ablation studies demonstrate the significant advantages   \n20 of our NeCGS over state-of-the-art methods both quantitatively and qualitatively.   \n21 We have included the source code in the Supplemental Material. ", "page_idx": 0}, {"type": "text", "text": "22 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 3D mesh models/shapes are widely used in various fields, such as computer graphics, virtual reality,   \n24 robotics, and autonomous driving. As geometric data becomes increasingly complex and voluminous,   \n25 effective compression techniques have become critical for efficient storage and transmission. More  \n26 over, current geometry compression methods primarily focus on individual 3D models or sequences   \n27 of 3D models that are temporally correlated, but struggle to handle more general data sets, such as   \n28 compressing large numbers of unrelated 3D shapes.   \n29 Unlike images and videos represented as regular 2D or 3D volumes, mesh models are commonly   \n30 represented as triangle meshes, which are irregular and challenging to compress. Thus, a natural   \n31 idea is to structure the mesh models and then leverage image or video compression techniques to   \n32 compress them.Converting mesh models into voxelized point clouds is a common practice, and the   \n33 mesh models can be recovered from the point clouds via surface reconstruction methods [22, 24].   \n34 Based on this, in recent years, MPEG has developed two types of 3D point cloud compression (PCC)   \n35 standards [46, 28]: geometry-based PCC (GPCC) for static models and video-based PCC (VPCC) for   \n36 sequential models. And with advancements in deep learning, numerous learning-based PCC methods   \n37 [41, 14, 55, 19, 54] have emerged, enhancing compression efficiency. However, the voxelized point   \n38 clouds require a high resolution (typically $2^{10}$ or more) to accurately represent geometry data, which   \n39 is redundancy, limiting the compression efficiency.   \n40 Another regular representation involves utilizing implicit fields of mesh models, such as signed   \n41 distance fields (SDF) and truncated signed distance fields (TSDF). This is achieved by calculating   \n42 the value of the implicit field at each uniformly distributed grid point, resulting in a regular volume.   \n43 And the mesh models can be recovered from the implicit fields through Matching Cubes [32] or its   \n44 variants [15, 45]. Compared with point clouds, the implicit volume could represent the mesh models   \n45 in a relatively small resolution. Recently proposed methods, such as DeepSDF [36], utilize multilayer   \n46 perceptrons (MLPs) to regress the SDFs of any given query points. While this representation achieves   \n47 high accuracy for single or similar models (e.g., chairs, tables), the limited receptive field of MLPs   \n48 makes it challenging to represent large numbers of models in different categories, which is a more   \n49 common scenario in practice.   \n50 We propose NeCGS, a novel framework for compressing large sets of geometric models. Our NeCGS   \n51 framework consists of two stages: regular geometry representation and compact neural compression.   \n52 In the first stage, each model is converted into a regular 4D volumetric format, called the TSDF-Def   \n53 volume, which can be considered a 3D \u2018image\u2019. In the second stage, we use an auto-decoder to   \n54 regress these 4D volumes. The embedded features and decoder parameters represent these models,   \n55 and compressing these components allows us to compress the entire geometry set. We conducted   \n56 extensive experiments on various datasets, demonstrating that our NeCGS framework achieves higher   \n5 compression efficiency compared to existing geometry compression methods when handling large   \n58 numbers of models. Our NeCGS can achieve a compression ratio of nearly 900 on some datasets,   \n59 compressing hundreds or even thousands of different models into $1\\!\\sim\\!2$ MB while preserving detailed   \n60 structures. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "lizRmKCnBp/tmp/b10fe6518cd49a7d084c791553250da3c74c79f8e2f69ec571fd6cd43ebbbab7.jpg", "img_caption": ["Figure 1: Our NeCGeS can compress geometry data with hundreds or even thousands of shapes into $1{\\sim}2\\,\\mathrm{MB}$ while preserving details. Left: Original Geometry Data. Right: Decompressed Geometry Data. $\\mathtt{a}$ Zoom in for details. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "61 2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "62 2.1 Geometry Representation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "63 In general, the representation of geometry data is divided into two main categories, explicit represen  \n64 tation and implicit representation, and they could be transformed into another.   \n65 Explicit Representation. Among the explicit representations, voxelization [7] is the most intuitive.   \n66 In this method, geometry models are represented by regularly distributed grids, effectively converting   \n67 them into 3D \u2018images\u2019. While this approach simplifies the processing of geometry models using   \n68 image processing techniques, it requires a high resolution to accurately represent the models, which   \n69 demands substantial memory and limits its application. Another widely used geometry representation   \n70 method is the point cloud, which consists of discrete points sampled from the surfaces of models.   \n71 This method has become a predominant approach for surface representation [2, 39, 40]. However, the   \n72 discrete nature of the points imposes constraints on its use in downstream tasks such as rendering and   \n73 editing. Triangle meshes offer a more precise and efficient geometry representation. By approximating   \n74 surfaces with numerous triangles, they achieve higher accuracy and efficiency for certain downstream   \n75 tasks.   \n76 Implicit Representation. Implicit representations use the isosurface of a function or field to represent   \n77 surfaces. The most widely used implicit representations include Binary Occupancy Field (BOF)   \n78 [22, 35], Signed Distance Field (SDF) [36, 29], and Truncated Signed Distance Field (TSDF) [11],   \n79 from which the model\u2019s surface can be easily extracted. However, these methods are limited to   \n80 representing watertight models. The Unsigned Distance Field (UDF) [8], which is the absolute value   \n81 of the SDF, can represent more general models, not just watertight ones. Despite this advantage,   \n82 extracting surfaces from UDF is challenging, which limits its application.   \n83 Conversion between Geometry Representations. Geometry representations can be converted   \n84 between explicit and implicit forms. Various methods [21, 22, 24, 6, 35, 29, 45] are available for   \n85 calculating the implicit field from given models. Conversely, when converting from implicit to   \n86 explicit forms, Marching Cubes [32] and its derivatives [48, 49, 15, 45] can reconstruct continuous   \n87 surfaces from various implicit fields. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "88 2.2 3D Geometry Data Compression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "89 Single 3D Geometric Model Compression. In recent decades, compression techniques for images   \n90 and videos have rapidly advanced [51, 34, 59, 5, 4]. However, the irregular nature of geometry   \n91 data makes it more challenging to compress compared to images and video, which are represented   \n92 as volumetric data. A natural approach is to convert geometry data into voxelized point clouds,   \n93 treating them as 3D \u2018images\u2019, and then applying image and video compression techniques to them.   \n94 Following this intuition, MPEG developed the GPCC standards [13, 28, 47], where triangle meshes or   \n95 triangle soup approximates the surfaces of 3D models, enabling the compression of models with more   \n96 complex structures. Subsequently, several improved methods [37, 60, 53, 62] and learning-based   \n97 methods [18, 43, 10, 9, 3, 42, 54] have been proposed to further enhance compression performance.   \n98 However, these methods rely on voxelized point clouds to represent geometry models, which is   \n99 inefficient and memory-intensive, limiting their compression efficiency. In contrast to the previously   \n00 mentioned methods, Draco [12] uses a kd-tree-based coding method to compress vertices and employs   \n01 the EdgeBreaker algorithm to encode the topological relationships of the geometry data. Draco   \n02 utilizes uniform quantization to control the compression ratio, but its performance decreases at higher   \n03 compression ratios.   \n104 Multiple Model Compression. Compared to compressing single 3D geometric models, compressing   \n105 multiple objects is significantly more challenging. SLRMA [17] addresses this by using a low-rank   \n106 matrix to approximate vertex matrices, thus compressing sequential models. Mekuria et al. [33]   \n107 proposed the first codec for compressing sequential point clouds, where each frame is coded using   \n108 Octree subdivision through an 8-bit occupancy code. Building on this concept, MPEG developed the   \n109 VPCC standards [13, 28, 47], which utilize 3D-to-2D projection and encode time-varying projected   \n110 planes, depth maps, and other data using video codecs. Several improved methods [57, 26, 1, 44]   \n111 have been proposed to enhance the compression of sequential models. Recently, shape priors like   \n112 SMPL [31] and SMAL [63] have been introduced, allowing the pose and shape of a template frame   \n113 to be altered using only a few parameters. Pose-driven geometry compression methods [16, 58, 56]   \n114 leverage this approach to achieve high compression efficiency. However, these methods are limited to   \n115 sequences of corresponding geometry data and cannot handle sets of unrelated geometry data, which   \n116 is more common in practice. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "lizRmKCnBp/tmp/0761d06a223edf5079336ad757c63761362b7f240dd0527657f92f961c3032aa.jpg", "img_caption": ["Figure 2: The pipeline of NeCGS. It first represents original meshes regularly into TSDF-Def volumes, and an auto-decoder network is utilized to regress these volume. Then the embedded features and decoder parameters are compressed into bitstreams through entropy coding. When decompressing the models, the decompressed embedded features are fed into the decoder with the decompressed parameters from the bitstreams, reconstructing the TSDF-Def volumes, and the models can be extracted from them. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "117 3 Proposed Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "118 Overview. Given a set of $N$ 3D mesh models containing diverse categories, denoted as $\\boldsymbol{S}=\\{\\mathbf{S}_{i}\\}_{i=1}^{N}$   \n119 we aim to compress them into a bitstream while maintaining the quality of the decompressed models   \n120 as much as possible. To this end, we propose a neural compression paradigm called NeCGS. As   \n121 shown in Fig. 2, NeCGS consists of two main modules, i.e., Regular Geometry Representation (RGR)   \n122 and Compact Neural Representation (CNR). Specifically, RGR first represents each irregular mesh   \n123 model within $\\boldsymbol{S}$ into a regular 4D volume, namely TSDF-Def volume that mplicitly describes the   \n124 geometry structure of the model, via a rendering-based optimization, thus leading to a set of 4D   \n125 volumes $\\boldsymbol{\\mathcal{V}}:=\\{\\mathbf{V}_{i}\\}_{i=1}^{N}$ with $\\mathbf{V}_{i}$ corresponding to $\\mathbf{S}_{i}$ . Then CNR further obtains a more compact   \n126 neural representation of $\\mathcal{V}$ , where a quantization-aware auto-decoder-based network is constructed   \n127 to regress these volumes, producing an embedded feature for each volume. Finally, the embedded   \n128 features along with the network parameters are encoded into a bitstream through a typical entropy   \n129 coding method to achieve compression. We also want to note that NeCGS can also be applied to   \n130 compress 3D geometry sets represented in $3D$ point clouds, where one can either reconstruct from the   \n131 given point clouds 3D surfaces through a typical surface reconstruction method or adopt a pre-trained   \n132 network for SDF estimation from point clouds, e.g., SPSR [22] or IMLS [24], to bridge the gap   \n133 between 3D mesh and point cloud models. In what follows, we will detail NeCGS. ", "page_idx": 3}, {"type": "text", "text": "134 3.1 Regular Geometry Representation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "135 Unlike 2D images and videos, where pixels are uniformly   \n136 distributed on 2D regular girds, the irregular characteristic   \n137 of 3D mesh models makes it challenging to compress them   \n138 efficiently and effectively. We propose to convert each   \n139 3D mesh model to a 4D regular volume called TSDF  \n140 Def volume, which implicitly represents the geometry   \n141 structure of the model. Such a regular representation can   \n142 describe the model precisely, and its regular nature proves   \n143 beneficial for compression in the subsequent stage.   \n144 TSDF-Def Volume. Although 3D regular SDF or TSDF   \n145 volumes are widely used for representing 3D geometry   \n146 models, they may introduce distortions when the volume ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "lizRmKCnBp/tmp/3fe5040ef168baee4272419f726c8c032a4f7ced188f2c99f8c90a3d8b146551.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: 2D visual illustration of DMC. The blue points refer to the deformable grid points, the green points refer to the vertices of the extracted surfaces, and the orange lines refer to the faces of the extracted surfaces. Left: The original grid points. Right: The surface extraction. ", "page_idx": 3}, {"type": "text", "text": "147 resolution is relatively limited. Inspired by recent shape extracting methods [48, 49], we propose   \n148 TSDF-Def, which extends the regular TSDF volume by introducing an additional deformation for   \n149 each grid point to adjust the detailed structure during the extraction of models, as shown in Fig.   \n150 3. Accordingly, we develop the differentiable Deformable Marching Cubes (DMC), the variant of   \n151 the Marching Cubes method [32], for surface extraction from a TSDF-Def volume. Consequently,   \n152 each shape S is represented as a 4D TSDF-Def volume, denoted as $\\mathbf{V}\\,\\in\\,\\mathbb{R}^{K\\times K\\times K\\times4}$ , where $K$   \n153 is the volume resolution. More specifically, the value of the grid point located at $(u,v,w)$ is   \n154 $\\mathbf{V}(u,v,w):=[\\mathrm{TSDF}(u,v,w),\\Delta u,\\Delta v,\\Delta w]$ , where $(\\Delta u,\\Delta v,\\Delta w)$ are the deformation for the grid   \n155 point and $1\\leq u,v,w\\leq K$ . TSDF-Def enhances representation accuracy, particularly when the grid   \n156 resolution is relatively low.   \n157 Optimization of TSDF-Def Volumes. To obtain the optimal TSDF-Def volume $\\mathbf{V}$ for a given model   \n158 S, after initializing the deformations of each grid to zero and computing the TSDF value for each   \n159 grid we optimize the following problem: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{V}}\\mathcal{E}_{\\mathrm{Rec}}\\big(\\mathtt{D M C}(\\mathbf{V}),\\mathbf{S}\\big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "160 where $\\mathtt{D M C}(\\cdot)$ refers to the differentiable DMC process for extracting surfaces from TSDF-Def   \n161 volumes, and the $\\mathcal{E}_{\\mathrm{{Reg}}}(\\cdot,~\\cdot)$ measures the differences between the rendered depth and silhouette   \n162 images of two mesh models through the differentiable rasterization [25]. Algorithm 1 summarizes   \n163 the whole optimization process. More details can be found in Sec. A.2 of the subsequent Appendix. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: Optimization of TSDF-Def Volumes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: 3D mesh model S; the maximum number of iterations maxIter. Output: The optimal TSDF-Def volume $\\mathbf{V}\\in\\mathbb{R}^{K\\times K\\times K\\times4}$ . 1 Place uniformly distributed grids in the cube of S, denoted as $\\mathbf{G}\\in\\mathbb{R}^{K\\times K\\times K\\times3}$ ; 2 Initialize $\\mathbf{V}[...,0]$ as the ground truth TSDF of $\\mathbf{S}$ at the location of $\\mathbf{G}$ , the deformation $\\mathbf{V}[...,1$ : $]{=}0$ , and the current iteration $\\mathtt{I t e r}=0$ ; 3 while Iter $<$ maxIter do 4 Recover shape from $\\mathbf{V}$ according to DMC, $\\mathtt{D M C}(\\mathbf{V})$ ; 5 Calculate the reconstruction error, $\\mathcal{E}_{\\mathrm{Rec}}\\big(\\mathrm{DMC}(\\mathbf{V}),\\mathbf{S}\\big)$ ; 6 Optimize V using ADAM optimizer based on the reconstruction error; 7 Iter:=Iter+1; 8 end 9 return V; ", "page_idx": 4}, {"type": "text", "text": "164 3.2 Compact Neural Representation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "165 Observing the similarity of local geometric structures within a typical 3D model and across different   \n166 models, i.e., redundancy, we further propose a quantization-aware neural representation process   \n167 to summarize the similarity within $\\mathcal{V}$ , leading to more compact representations with redundancy   \n168 removed.   \n169 Network Architecture. We construct an auto-decoder network architecture to regress these 4D   \n170 TSDF-Def volumes. Specifically, it is composed of a head layer, which increases the channel of its   \n171 input, and $L$ cascaded upsampling modules, which progressively upscale the feature volume. We   \n172 also utilize the PixelShuffle technique [50] between the convolution and activation layers to achieve   \n173 upscaling. We refer reviewers to Sec. B of Appendix for more details. For TSDF-Def volume $\\mathbf{V}_{i}$ ,   \n174 the corresponding input to the auto-decoder is the embedded feature, denoted as Fi \u2208RK\u2032\u00d7K\u2032\u00d7K\u2032\u00d7C,   \n175 where $K^{\\prime}$ is the resolution satisfying $K^{\\prime}\\ll K$ and $C$ is the number of channels. Moreover, we   \n176 integrate differentiable quantization to the embedded features and network parameters in the process,   \n177 which can efficiently reduce the quantization error. In all, the compact neural representation process   \n178 can be written as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{V}}_{i}=\\mathcal{D}_{\\mathcal{Q}(\\mathbf{\\Theta}\\mathbf{)}}(\\mathcal{Q}(\\mathbf{F}_{i})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "179 where $\\mathcal{Q}(\\cdot)$ stands for the differentiable quantization operator, and $\\widehat{\\mathbf{V}}_{i}$ is the regressed TSDF-Def. ", "page_idx": 4}, {"type": "text", "text": "180 Loss Function. We employ a joint loss function comprising Mean Absolute Error (MAE) and   \n181 Structural Similarity Index (SSIM) to simultaneously optimize the embedded features $\\{\\mathbf{F}_{i}\\}$ and   \n182 the network parameters $\\Theta$ . In computing the MAE between the predicted and ground truth TSDF  \n183 Def volumes, we concentrate more on the grids close to the surface. These surface grids crucially   \n184 determine the surfaces through their TSDFs and deformations; hence we assign them higher weights   \n185 during optimization than the grids farther away from the surface. The overall loss function for the   \n186 $i$ -th model is written as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\widehat{\\mathbf{V}}_{i},\\mathbf{V}_{i})=\\|\\widehat{\\mathbf{V}}_{i}-\\mathbf{V}_{i}\\|_{1}+\\lambda_{1}\\|\\mathbf{M}_{i}\\odot(\\widehat{\\mathbf{V}}_{i}-\\mathbf{V}_{i})\\|_{1}+\\lambda_{2}(1-\\mathtt{S S I M}(\\widehat{\\mathbf{V}}_{i},\\mathbf{V}_{i})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "187 where $\\mathbf{M}_{i}=\\mathbb{1}(|\\mathbf{V}_{i}[...,0])|<\\tau)$ is the mask, indicating whether a grid is near the surface, i.e., its   \n188 TSDF is less than the threshold $\\tau$ , while $\\lambda_{1}$ and $\\lambda_{2}$ are the weights to balance each term of the loss   \n189 function.   \n190 Entropy Coding. After obtaining the quantized features $\\{\\widetilde{\\mathbf{F}}_{i}\\,=\\,\\mathcal{Q}(\\mathbf{F}_{i})\\}$ and quantized network   \n191 parameters $\\widetilde{\\Theta}\\,=\\,\\mathcal{Q}(\\boldsymbol{\\Theta})$ , we adopt the Huffman Codec [20] to further compress them into a bit  \n192 stream. More advanced entropy coding methods can be employed to further improve compression   \n193 performance. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "194 3.3 Decompression ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "195 To obtain the 3D mesh models from the bitstream, we first decompress the bitstream to derive the   \n196 embedded features, $\\{\\widetilde{\\mathbf{F}}_{i}\\}$ and the decoder parameter, $\\widetilde{\\Theta}$ . Then, for each $\\widetilde{\\mathbf{F}}_{i}$ , we feed it to the decoder   \n197 $\\mathcal{D}_{\\widetilde{\\Theta}}(\\cdot)$ to generate its corresponding TSDF-Def volu me ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{V}}_{i}=\\mathcal{D}_{\\widetilde{\\Theta}}(\\widetilde{\\mathbf{F}}_{i}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "198 Finally, we utilize DMC to recover each shape from $\\widehat{\\mathbf{V}}_{i}$ , $\\widehat{\\mathbf{S}}_{i}=\\mathtt{D M C}(\\widehat{\\mathbf{V}}_{i})$ , forming the set of decom  \n199 pressed geometry data, $\\widehat{S}=\\{\\widehat{\\mathbf{S}}_{i}\\}_{i=1}^{N}$ . ", "page_idx": 5}, {"type": "text", "text": "200 4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "201 4.1 Experimental Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "202 Implementation details. In the process of optimizing TSDF-Def volumes, we employed the ADAM   \n203 optimizer [23] for 500 iterations per shape, using a learning rate of 0.01. The resolution of TSDF-Def   \n204 volumes was $K=128$ . The resolution and the number of channels of the embedded features were   \n205 $K^{\\prime}=4$ and $C=16$ , respectively. And the decoder is composed of $L=5$ upsampling modules with   \n206 an up-scaling factor of 2. During the optimization, we set $\\lambda_{1}=5$ and $\\lambda_{2}=10$ , and the embedded   \n207 features and decoder parameters were optimized by the ADAM optimizer for 400 epochs, with a   \n208 learning rate of 1e-3. We achieved different compression efficiencies by adjusting decoder sizes. We   \n209 conducted all experiments on an NVIDIA RTX 3090 GPU with Intel(R) Xeon(R) CPU.   \n211 Datasets. We tested our NeCGS on various types   \n212 of datasets, including humans, animals, and CAD   \n213 models. For human models, we randomly selected   \n214 500 shapes from the AMA dataset [52]. For animal   \n215 models, we randomly selected 500 shapes from   \n216 the DT4D dataset [27]. For the CAD models, we   \n217 randomly selected 1000 shapes from the Thingi10K   \n218 dataset [61]. Besides, we randomly selected 200   \n219 models from each dataset, forming a more challenging dataset, denoted as Mixed. The details about   \n220 the selected datasets are shown in Table 1. In all experiments, we scaled all models in a cube with a   \n221 range of $[-1,1]^{3}$ to ensure they are in the same scale.   \n222 Methods under Comparison. In terms of traditional geometry codecs, we chose the three most   \n223 impactful geometry coding standards with released codes, G-PCC2 and $\\mathsf{V-P C C}^{3}$ from MPEG (see   \n224 more details about them in [13, 28, 47]), and Draco 4from Google as the baseline methods. Addi  \n225 tionally, we compared our approach with state-of-the-art deep learning-based compression methods,   \n226 specifically PCGCv2 [54]. Furthermore, we adapted DeepSDF [36] with quantization to serve as   \n227 another baseline method, denoted as QuantDeepSDF. It is worth noting that while some of the chosen   \n228 baseline methods were originally designed for point cloud compression, we utilized voxel sampling   \n229 and SPSR [22] to convert them between the forms of point cloud and surface. More details can be   \n230 found in Sec. C.2 appendix.   \n231 Evaluation Metrics. Following previous reconstruction methods [35, 38], we utilize Chamfer   \n232 Distance (CD), Normal Consistency (NC), F-Score with the thresholds of 0.005 and 0.01 (F1-0.005   \n233 and F1-0.01) as the evaluation metrics. Furthermore, to comprehensively compare the compression   \n234 efficiency of different methods, we use Rate-Distortion (RD) curves. These curves illustrate the   \n235 distortions at various compression ratios, with CD and F1-0.005 specifically describing the distortion   \n236 of the decompressed models. Our goal is to minimize distortion, indicated by a low CD and a high   \n237 F1-Score, while maximizing the compression ratio. Therefore, for the RD curve representing CD,   \n238 optimal compression performance is achieved when the curve is closest to the lower right corner.   \n239 Similarly, for the RD curve representing the F1-Score, the ideal compression performance is when   \n240 the curve is nearest to the upper right corner. Their detailed definition can be found in Sec. C.1 of   \n241 appendix. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "lizRmKCnBp/tmp/0cf884dc1ac516a0572b7949833048c194531107d8f972eec3e986ce6a97bfa7.jpg", "table_caption": ["Table 1: Details of the selected datasets1. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "lizRmKCnBp/tmp/42a01563e644708ce3d6c48b6f063494860eae2403c57546207287560ccd9ced.jpg", "img_caption": ["Figure 4: Quantitative comparisons of different methods on four 3D geometry sets. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "242 4.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "243 The RD curves of different compression methods under different datasets are shown in Fig. 4. As   \n244 the compression ratio increases, the distortion also becomes larger. It is obvious that our NeCGS   \n245 can achieve much better compression performance than the baseline methods when the compression   \n246 ratio is high, even in the challenging Mixed dataset. In particular, our NeCGS achieves a minimum   \n247 compression ratio of 300, and on the DT4D dataset, the compression ratio even reaches nearly 900,   \n248 with minimal distortion. Due to the larger model differences within the Thingi10K and Mixed datasets   \n249 compared to the other two datasets, the compression performance on these two datasets is inferior.   \n250 The visual results of different compression meth  \n251 ods are shown in Fig. 5. Compared to other   \n252 methods, models compressed using our ap  \n253 proach occupy a larger compression ratio and   \n254 retain more details after decompression. Fig.   \n255 illustrates the decompressed models under different compression ratio. Even when the compression   \n256 ratio reaches nearly 900, our method can still retain the details of the models. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "lizRmKCnBp/tmp/d084b4e72102a1cc954afd26935db784d18ef450767a29053dc03f9272e83f8d.jpg", "img_caption": ["Figure 6: Decompressed models under different compression ratios. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "lizRmKCnBp/tmp/bd97ba724be308c701b3700f0de28a16d581e1a8ea6800ae49243c5994be83d3.jpg", "img_caption": ["(a) GPCC (b) VPCC (c) PCGCv2 (d) Draco (e) QuantDeepSDF (f) Ours (g) Ori. Figure 5: Visual comparisons of different compression methods. All numbers in corners represent the compression ratio. $\\triangledown\\alpha$ Zoom in for details. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "257 4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "258 In order to illustrate the efficiency of each design of our NeCGS, we conducted extensive ablation   \n259 study about them on the Mixed dataset.   \n260 Necessity of the Deformation of   \n261 Grids. We utilize TSDF-Def volumes   \n262 to as the regular geometry representa  \n263 tion, instead of TSDF volumes like   \n264 previous methods. Compared with   \n265 models recovered from TSDF vol  \n266 umes through MC, the models recov  \n267 ered from TSDF-Def volumes through   \n268 DMC preserve more details of the thin ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "lizRmKCnBp/tmp/415cf1cecc0cdee0424422de3f72bd8d034993f4ad61a21731870f6435dcd7ae.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 7: Models recovered from different regular geometry representations under various volume resolutions. From Left to Right: Original, TSDF with $K=64$ , TSDF with $K=128$ , TSDF-Def with $K=64$ , and TSDF-Def with $K=128$ . ", "page_idx": 7}, {"type": "text", "text": "269 structures, especially when the volume resolutions are relatively small, as shown in Fig. 7. We also   \n270 conducted a numerical comparison of the decompressed models on the AMA dataset under these two   \n271 settings, and the results are shown in Table. 2, demonstrating its advantages.   \n272 Neural Representation Structure. To illustrate the superiority of auto-decoder framework, we   \n273 utilize an auto-encoder to regress the TSDF-Def volume. Technically, we used a ConvNeXt block   \n274 [30] as the encoder by replacing 2D convolutions with 3D convolutions. Under the auto-encoder   \n275 framework, we optimize the parameters of the encoder to change the embedded features. The RD   \n276 curves about these two structures are shown in Fig. 8(a), demonstrating rationality of our decoder   \n277 structure.   \n278   \n279   \n280   \n281   \n282   \n283   \n284   \n285   \n286   \n287   \n288 ", "page_idx": 7}, {"type": "table", "img_path": "lizRmKCnBp/tmp/f0a52ae89d7f2156e3ffc5820c9324454f6027d7f1a3b6c52d1768ea93c2941d.jpg", "table_caption": ["Table 2: Quantitative comparisons of different RGRs. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "lizRmKCnBp/tmp/8c70973e4141ab62e8ef39f11d13c3b080c2196599a82474c533fd94cb142510.jpg", "img_caption": ["Figure 8: (a) RD curves of different neural representation structures. (b) RD curves of different regression losses. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "SSIM Loss. Compared to MAE, which focuses on one-to-one errors between predicted and ground truth volumes, the SSIM item in Eq. 3 emphasizes more on the local similarity between volumes, increasing the regression accuracy. To verify this, we removed the SSIM item and kept others unchanged. Their RD curves are shown in Fig. 8(b), and it is obvious that the SSIM item in the regression loss increases the compression performance. The visual comparison is shown in Fig. 9, and without SSIM, there are floating parts around the decompressed models. ", "page_idx": 8}, {"type": "text", "text": "289 Resolution of TSDF-Def Volumes. We tested the com  \n290 pression performance at different resolutions of TSDF  \n291 Def volumes by adjusting the decoder layers accordingly.   \n292 Specifically, we removed the last layer for a resolution   \n293 of 64 and added an extra layer for a resolution of 256.   \n294 The quantitative and numerical comparisons are shown in   \n295 Table 3 and Fig. 10, respectively. Obviously, increasing   \n296 the volume resolution can enhance the compression effec  \n297 tiveness, resulting in more detailed structures preserved   \n298 after decompression. However, the optimization and in  \n299 ference time also increase accordingly due to more layers   \n300 involved. ", "page_idx": 8}, {"type": "image", "img_path": "lizRmKCnBp/tmp/bdaeae6a41b438c473e580b4cbf3fffcafb95da23ed9386a7ff8e31c22a3ca6c.jpg", "img_caption": ["", "Figure 9: Visual comparison of regression loss w/ and w/o SSIM item. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "lizRmKCnBp/tmp/f31bbd35f179450b9d2aa12de20cfdcaec8b749ac0d0b8ed5f91273fdb15df5a.jpg", "img_caption": ["(a) Ori. (b) 64 (c) 128 (d) 256 Figure 10: Visual comparison under different resolutions of TSDF-Def volume. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "lizRmKCnBp/tmp/7fb2bae22564ee7edd3d2d0d0a33f06f1b1a6fbfd687a258605db29d6b92dfa9.jpg", "table_caption": ["Table 3: Quantitative comparisons of different resolutions of TSDF-Def volumes. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "301 5 Conclusion and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "302 We have presented NeCGS, a highly effective neural compression scheme for 3D geometry sets.   \n303 NeCGS has achieved remarkable compression performance on various datasets with diverse and   \n304 detailed shapes, outperforming state-of-the-art compression methods to a large extent. These advan  \n305 tages are attributed to our regular geometry representation and the compression accomplished by a   \n306 convolution-based auto-decoder. We believe our NeCGS framework will inspire further advancements   \n307 in the field of geometry compression.   \n308 However, our method still suffers from the following two limitations. One is that it requires more   \n309 than 15 hours to regress the TSDF-Def volumes, and the other one is that the usage of 3D convolution   \n310 layers limits the inference speed. Our future work will focus on addressing these challenges by   \n311 accelerating the optimization process and incorporating more efficient network modules. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "312 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "313 [1] A. Ahmmed, M. Paul, M. Murshed, and D. Taubman. Dynamic point cloud geometry compression using   \n314 cuboid based commonality modeling framework. In 2021 IEEE International Conference on Image   \n315 Processing (ICIP), pages 2159\u20132163. IEEE, 2021. 3   \n316 [2] P. J. Besl and N. D. McKay. Method for registration of 3-d shapes. In Sensor Fusion IV: Control Paradigms   \n317 and Data Structures, volume 1611, pages 586\u2013606. Spie, 1992. 3   \n318 [3] S. Biswas, J. Liu, K. Wong, S. Wang, and R. Urtasun. Muscle: Multi sweep compression of lidar using   \n319 deep entropy models. Advances in Neural Information Processing Systems, 33:22170\u201322181, 2020. 3   \n320 [4] H. Chen, M. Gwilliam, S.-N. Lim, and A. Shrivastava. Hnerv: A hybrid neural representation for   \n321 videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages   \n322 10270\u201310279, 2023. 3   \n323 [5] H. Chen, B. He, H. Wang, Y. Ren, S. N. Lim, and A. Shrivastava. Nerv: Neural representations for videos.   \n324 Advances in Neural Information Processing Systems, 34:21557\u201321568, 2021. 3   \n325 [6] Z.-Q. Cheng, Y.-Z. Wang, B. Li, K. Xu, G. Dang, and S.-Y. Jin. A survey of methods for moving least   \n326 squares surfaces. In Proceedings of the Fifth Eurographics/IEEE VGTC conference on Point-Based   \n327 Graphics, pages 9\u201323, 2008. 3   \n328 [7] J. Chibane, T. Alldieck, and G. Pons-Moll. Implicit functions in feature space for 3d shape reconstruction   \n329 and completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,   \n330 pages 6970\u20136981, June 2020. 3   \n331 [8] J. Chibane, G. Pons-Moll, et al. Neural unsigned distance fields for implicit function learning. Advances in   \n332 Neural Information Processing Systems, 33:21638\u201321652, 2020. 3   \n333 [9] T. Fan, L. Gao, Y. Xu, D. Wang, and Z. Li. Multiscale latent-guided entropy model for lidar point cloud   \n334 compression. IEEE Transactions on Circuits and Systems for Video Technology, 33(12):7857\u20137869, 2023.   \n335 3   \n336 [10] C. Fu, G. Li, R. Song, W. Gao, and S. Liu. Octattention: Octree-based large-scale contexts model for point   \n337 cloud compression. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages   \n338 625\u2013633, 2022. 3   \n339 [11] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, and H. Li. Dynamic fusion with intra-and inter-modality   \n340 attention flow for visual question answering. In Proceedings of the IEEE/CVF conference on computer   \n341 vision and pattern recognition, pages 6639\u20136648, 2019. 3   \n342 [12] Google. Point cloud compression reference software. Website. https://github. com/google/draco. 3   \n343 [13] D. Graziosi, O. Nakagami, S. Kuma, A. Zaghetto, T. Suzuki, and A. Tabatabai. An overview of ongoing   \n344 point cloud compression standardization activities: Video-based (v-pcc) and geometry-based (g-pcc).   \n345 APSIPA Transactions on Signal and Information Processing, 9:e13, 2020. 3, 7   \n346 [14] A. F. Guarda, N. M. Rodrigues, and F. Pereira. Point cloud coding: Adopting a deep learning-based   \n347 approach. In 2019 Picture Coding Symposium (PCS), pages 1\u20135. IEEE, 2019. 1   \n348 [15] B. Guillard, F. Stella, and P. Fua. Meshudf: Fast and differentiable meshing of unsigned distance field   \n349 networks. In European Conference on Computer Vision, pages 576\u2013592, 2022. 2, 3   \n350 [16] J. Hou, L.-P. Chau, N. Magnenat-Thalmann, and Y. He. Compressing 3-d human motions via keyframe  \n351 based geometry videos. IEEE Transactions on Circuits and Systems for Video Technology, 25(1):51\u201362,   \n352 2014. 3   \n353 [17] J. Hou, L.-P. Chau, N. Magnenat-Thalmann, and Y. He. Sparse low-rank matrix approximation for data   \n354 compression. IEEE Transactions on Circuits and Systems for Video Technology, 27(5):1043\u20131054, 2015. 3   \n355 [18] L. Huang, S. Wang, K. Wong, J. Liu, and R. Urtasun. Octsqueeze: Octree-structured entropy model for   \n356 lidar compression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,   \n357 pages 1313\u20131323, 2020. 3   \n358 [19] T. Huang and Y. Liu. 3d point cloud geometry compression on deep learning. In Proceedings of the 27th   \n359 ACM international conference on multimedia, pages 890\u2013898, 2019. 1   \n360 [20] D. A. Huffman. A method for the construction of minimum-redundancy codes. Proceedings of the IRE,   \n361 40(9):1098\u20131101, 1952. 6   \n362 [21] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface reconstruction. In Proceedings of the fourth   \n363 Eurographics symposium on Geometry processing, pages 61\u201370, 2006. 3   \n364 [22] M. Kazhdan and H. Hoppe. Screened poisson surface reconstruction. ACM Transactions on Graphics   \n365 (ToG), 32(3):1\u201313, 2013. 1, 3, 4, 7   \n366 [23] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,   \n367 2014. 6   \n368 [24] R. Kolluri. Provably good moving least squares. ACM Transactions on Algorithms, 4(2):1\u201325, 2008. 1, 3,   \n369 4   \n370 [25] S. Laine, J. Hellsten, T. Karras, Y. Seol, J. Lehtinen, and T. Aila. Modular primitives for high-performance   \n371 differentiable rendering. ACM Transactions on Graphics (ToG), 39(6):1\u201314, 2020. 5   \n372 [26] L. Li, Z. Li, V. Zakharchenko, J. Chen, and H. Li. Advanced 3d motion prediction for video-based dynamic   \n373 point cloud compression. IEEE Transactions on Image Processing, 29:289\u2013302, 2019. 3   \n374 [27] Y. Li, H. Takehara, T. Taketomi, B. Zheng, and M. Nie\u00dfner. 4dcomplete: Non-rigid motion estimation   \n375 beyond the observable surface. In Proceedings of the IEEE/CVF International Conference on Computer   \n376 Vision, pages 12706\u201312716, 2021. 6   \n377 [28] H. Liu, H. Yuan, Q. Liu, J. Hou, and J. Liu. A comprehensive study and comparison of core technologies   \n378 for mpeg 3-d point cloud compression. IEEE Transactions on Broadcasting, 66(3):701\u2013717, 2019. 1, 3, 7   \n379 [29] S.-L. Liu, H.-X. Guo, H. Pan, P.-S. Wang, X. Tong, and Y. Liu. Deep implicit moving least-squares   \n380 functions for 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and   \n381 Pattern Recognition, pages 1788\u20131797, June 2021. 3   \n382 [30] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In Proceedings   \n383 of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976\u201311986, 2022. 8   \n384 [31] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. Smpl: A skinned multi-person linear   \n385 model. ACM Trans. Graph., 34(6), oct 2015. 3   \n386 [32] W. E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3d surface construction algorithm.   \n387 ACM siggraph computer graphics, 21(4):163\u2013169, 1987. 2, 3, 5   \n388 [33] R. Mekuria, K. Blom, and P. Cesar. Design, implementation, and evaluation of a point cloud codec for   \n389 tele-immersive video. IEEE Transactions on Circuits and Systems for Video Technology, 27(4):828\u2013842,   \n390 2016. 3   \n391 [34] F. Mentzer, E. Agustsson, M. Tschannen, R. Timofte, and L. V. Gool. Practical full resolution learned   \n392 lossless image compression. In Proceedings of the IEEE/CVF conference on computer vision and pattern   \n393 recognition, pages 10629\u201310638, 2019. 3   \n394 [35] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks: Learning 3d   \n395 reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and   \n396 Pattern Recognition, pages 4460\u20134470, June 2019. 3, 7   \n397 [36] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove. Deepsdf: Learning continuous signed   \n398 distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer   \n399 Vision and Pattern Recognition, pages 165\u2013174, June 2019. 2, 3, 7   \n400 [37] E. Peixoto. Intra-frame compression of point cloud geometry using dyadic decomposition. IEEE Signal   \n401 Processing Letters, 27:246\u2013250, 2020. 3   \n402 [38] S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, and A. Geiger. Convolutional occupancy networks. In   \n403 European Conference on Computer Vision, pages 523\u2013540. Springer, 2020. 7   \n404 [39] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and   \n405 segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages   \n406 652\u2013660, 2017. 3   \n407 [40] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet+ $^+$ : Deep hierarchical feature learning on point sets in a   \n408 metric space. Advances in neural information processing systems, 30:1\u2013xxx, 2017. 3   \n409 [41] M. Quach, G. Valenzise, and F. Dufaux. Learning convolutional transforms for lossy point cloud geometry   \n410 compression. In 2019 IEEE international conference on image processing (ICIP), pages 4320\u20134324. IEEE,   \n411 2019. 1   \n412 [42] M. Quach, G. Valenzise, and F. Dufaux. Learning convolutional transforms for lossy point cloud geometry   \n413 compression. In 2019 IEEE international conference on image processing (ICIP), pages 4320\u20134324. IEEE,   \n414 2019. 3   \n415 [43] Z. Que, G. Lu, and D. Xu. Voxelcontext-net: An octree based framework for point cloud compression. In   \n416 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6042\u20136051,   \n417 2021. 3   \n418 [44] E. Ramalho, E. Peixoto, and E. Medeiros. Silhouette 4d with context selection: Lossless geometry   \n419 compression of dynamic point clouds. IEEE Signal Processing Letters, 28:1660\u20131664, 2021. 3   \n420 [45] S. Ren, J. Hou, X. Chen, Y. He, and W. Wang. Geoudf: Surface reconstruction from 3d point clouds via   \n421 geometry-guided distance representation. In Proceedings of the IEEE/CVF Internation Conference on   \n422 Computer Vision, pages 14214\u201314224, 2023. 2, 3   \n423 [46] S. Schwarz, M. Preda, V. Baroncini, M. Budagavi, P. Cesar, P. A. Chou, R. A. Cohen, M. Krivoku\u00b4ca,   \n424 S. Lasserre, Z. Li, et al. Emerging mpeg standards for point cloud compression. IEEE Journal on Emerging   \n425 and Selected Topics in Circuits and Systems, 9(1):133\u2013148, 2018. 1   \n426 [47] S. Schwarz, M. Preda, V. Baroncini, M. Budagavi, P. Cesar, P. A. Chou, R. A. Cohen, M. Krivoku\u00b4ca,   \n427 S. Lasserre, Z. Li, et al. Emerging mpeg standards for point cloud compression. IEEE Journal on Emerging   \n428 and Selected Topics in Circuits and Systems, 9(1):133\u2013148, 2018. 3, 7   \n429 [48] T. Shen, J. Gao, K. Yin, M.-Y. Liu, and S. Fidler. Deep marching tetrahedra: a hybrid representation for   \n430 high-resolution 3d shape synthesis. Advances in Neural Information Processing Systems, 34:6087\u20136101,   \n431 2021. 3, 5   \n432 [49] T. Shen, J. Munkberg, J. Hasselgren, K. Yin, Z. Wang, W. Chen, Z. Gojcic, S. Fidler, N. Sharp, and J. Gao.   \n433 Flexible isosurface extraction for gradient-based mesh optimization. ACM Transactions on Graphics   \n434 (TOG), 42(4):1\u201316, 2023. 3, 5   \n435 [50] W. Shi, J. Caballero, F. Husz\u00e1r, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time   \n436 single image and video super-resolution using an efficient sub-pixel convolutional neural network. In   \n437 Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1874\u20131883, 2016.   \n438 5   \n439 [51] Y. Str\u00fcmpler, J. Postels, R. Yang, L. V. Gool, and F. Tombari. Implicit neural representations for image   \n440 compression. In European Conference on Computer Vision, pages 74\u201391. Springer, 2022. 3   \n441 [52] D. Vlasic, I. Baran, W. Matusik, and J. Popovic\u00b4. Articulated mesh animation from multi-view silhouettes.   \n442 ACM Transactions on Graphics, 27(3):1\u20139, 2008. 6   \n443 [53] C. Wang, W. Zhu, Y. Xu, Y. Xu, and L. Yang. Point-voting based point cloud geometry compression. In   \n444 2021 IEEE 23rd International Workshop on Multimedia Signal Processing (MMSP), pages 1\u20135. IEEE,   \n445 2021. 3   \n446 [54] J. Wang, D. Ding, Z. Li, and Z. Ma. Multiscale point cloud geometry compression. In 2021 Data   \n447 Compression Conference (DCC), pages 73\u201382. IEEE, 2021. 1, 3, 7   \n448 [55] J. Wang, H. Zhu, H. Liu, and Z. Ma. Lossy point cloud geometry compression via end-to-end learning.   \n449 IEEE Transactions on Circuits and Systems for Video Technology, 31(12):4909\u20134923, 2021. 1   \n450 [56] X. Wu, P. Zhang, M. Wang, P. Chen, S. Wang, and S. Kwong. Geometric prior based deep human point   \n451 cloud geometry compression. IEEE Transactions on Circuits and Systems for Video Technology, 2024. 3   \n452 [57] J. Xiong, H. Gao, M. Wang, H. Li, K. N. Ngan, and W. Lin. Efficient geometry surface coding in v-pcc.   \n453 IEEE Transactions on Multimedia, 25:3329\u20133342, 2022. 3   \n454 [58] R. Yan, Q. Yin, X. Zhang, Q. Zhang, G. Zhang, and S. Ma. Pose-driven compression for dynamic 3d   \n455 human via human prior models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 3   \n456 [59] Y. Yang, R. Bamler, and S. Mandt. Improving inference for neural image compression. Advances in Neural   \n457 Information Processing Systems, 33:573\u2013584, 2020. 3   \n458 [60] X. Zhang, W. Gao, and S. Liu. Implicit geometry partition for point cloud compression. In 2020 Data   \n459 Compression Conference (DCC), pages 73\u201382. IEEE, 2020. 3   \n460 [61] Q. Zhou and A. Jacobson. Thingi10k: A dataset of 10,000 3d-printing models. arXiv preprint   \n461 arXiv:1605.04797, 2016. 6   \n462 [62] W. Zhu, Y. Xu, D. Ding, Z. Ma, and M. Nilsson. Lossy point cloud geometry compression via region-wise   \n463 processing. IEEE Transactions on Circuits and Systems for Video Technology, 31(12):4575\u20134589, 2021. 3   \n464 [63] S. Zuff,i A. Kanazawa, D. Jacobs, and M. J. Black. 3D menagerie: Modeling the 3D shape and pose of   \n465 animals. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), July 2017. 3 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "466 Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "467 A Regular Geometry Representation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "468 A.1 Tensor Quantization ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "469 Denoted $\\mathbf{x}$ is a tensor, we quantize it in a fixed interval, $[a,b]$ , at $(2^{N}+1)$ levels5 by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{Q}(\\mathbf{x})=\\mathrm{Round}\\left(\\frac{\\mathrm{C1amp}(\\mathbf{x},a,b)-a}{s}\\right)\\times s+a,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "470 where $s=(b-a)/2^{N}$ . In our experiment, we set $a=-1$ and $b=1$ . ", "page_idx": 13}, {"type": "text", "text": "471 A.2 Optimization of TSDF-deformation Volumes ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "472 We set a series of camera pose, $\\mathcal{T}=\\{\\mathbf{T}_{i}\\}_{i=1}^{E}$ , around the meshes. Let $\\mathbf{I}_{1}^{\\mathrm{{D}}}(\\mathbf{T}_{i})$ and $\\mathbf{I}_{2}^{\\mathrm{D}}(\\mathbf{T}_{i})$ represent   \n473 the depth images obtained from the reconstructed mesh $\\mathtt{D M C(V)}$ and the given mesh $\\mathbf{S}$ at the pose $\\mathbf{T}_{i}$   \n474 respectively. Similarly, let $\\mathbf{I}_{1}^{\\mathrm{M}}(\\mathbf{T}_{i})$ and $\\mathbf{I}_{2}^{\\mathrm{M}}(\\mathbf{T}_{i})$ denote their respective silhouette images at pose $\\mathbf{T}_{i}$ .   \n475 The reconstruction error produced by silhouette and depth images at all pose are ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{M}}(\\sf D\\mathrm{MC}(\\mathbf{V}),\\mathbf{S})=\\sum_{\\mathcal{T}_{i}\\in\\mathcal{T}}\\|\\mathbf{I}_{1}^{\\mathrm{M}}(\\mathbf{T}_{i})-\\mathbf{I}_{2}^{\\mathrm{M}}(\\mathbf{T}_{i})\\|_{1}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "476 and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{D}}(\\mathrm{DMC}(\\mathbf{V}),\\mathbf{S})=\\sum_{T_{i}\\in T}\\|\\mathbf{\\Gamma}(\\mathbf{I}_{1}^{\\mathrm{D}}(\\mathbf{T}_{i})-\\mathbf{I}_{2}^{\\mathrm{D}}(\\mathbf{T}_{i}))*\\mathbf{I}_{2}^{\\mathrm{M}}(\\mathbf{T}_{i})\\|_{1}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "477 Then the reconstruction error is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{Rec}}(\\mathsf{D M C}(\\mathbf{V}),\\mathbf{S})=\\mathcal{E}_{\\mathrm{M}}(\\mathsf{D M C}(\\mathbf{V}),\\mathbf{S})+\\lambda_{\\mathrm{rec}}\\mathcal{E}_{\\mathrm{D}}(\\mathsf{D M C}(\\mathbf{V}),\\mathbf{S}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "478 where $E=4$ and $\\lambda_{\\mathrm{{rec}}}=10$ in our experiment. ", "page_idx": 13}, {"type": "text", "text": "479 B Auto-decoder-based Neural Compression ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "480 B.1 Upsampling Module ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "481 In each upsampling module, we utilize a PixelShuffle layer between the convolution and activa  \n482 tion layers to upscale the input, as shown in Fig. 11. The input feature volume has dimensions   \n483 $(N_{\\mathrm{in}},N_{\\mathrm{in}},N_{\\mathrm{in}},C_{\\mathrm{in}})$ , with an upsampling scale of $s$ and an output channel count of $C_{\\mathrm{out}}$ . ", "page_idx": 13}, {"type": "text", "text": "484 C Experiment ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "485 C.1 Evaluation Metric ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "486 Let $\\mathbf{S}_{\\mathrm{Rec}}$ and $\\mathbf{S}_{\\mathrm{GT}}$ denote the reconstructed and ground-truth 3D shapes, respectively. We then   \n487 randomly sample $N_{\\mathrm{eval}}=10^{5}$ points on them, obtaining two point clouds, $\\mathbf{P}_{\\mathrm{Rec}}$ and $\\mathbf{P}_{\\mathrm{GT}}$ . For each   \n488 point of $\\mathbf{P}_{\\mathrm{Rec}}$ and $\\mathbf{P}_{\\mathrm{GT}}$ , the normal of the triangle face where it is sampled is considered to be its   \n489 normal vector, and the normal sets of $\\mathbf{P}_{\\mathrm{Rec}}$ and $\\mathbf{P}_{\\mathrm{GT}}$ are denoted as $\\mathbf{N}_{\\mathrm{Rec}}$ and $\\mathbf{N}_{\\mathrm{GT}}$ , respectively.   \n490 Let $\\mathtt{N N\\_P o i n t}(\\mathbf{x},\\mathbf{P})$ be the operator that returns the nearest point of $\\mathbf{x}$ in the point cloud $\\mathbf{P}$ . The CD   \n491 between them is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{CD}(\\mathbf{S}_{\\mathrm{Rec}},\\mathbf{S}_{\\mathrm{GT}})=\\!\\frac{1}{2N_{\\mathrm{eval}}}\\displaystyle\\sum_{\\mathbf{x}\\in\\mathbf{P}_{\\mathrm{Rec}}}\\|\\mathbf{x}-\\mathtt{N N\\_P o i n t}(\\mathbf{x},\\mathbf{P}_{\\mathrm{GT}})\\|_{2}}\\\\ {+\\frac{1}{2N_{\\mathrm{eval}}}\\displaystyle\\sum_{\\mathbf{x}\\in\\mathbf{P}_{\\mathrm{GT}}}\\|\\mathbf{x}-\\mathtt{N N\\_P o i n t}(\\mathbf{x},\\mathbf{P}_{\\mathrm{Rec}})\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "image", "img_path": "lizRmKCnBp/tmp/76877c8a41311e3401d28564f58b41b2f8793df9753591936a34ef19006910a8.jpg", "img_caption": ["Figure 11: Upsampling Module. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "492 Let NN_Normal $(\\mathbf{x},\\mathbf{P})$ be the operator that returns the normal vector of the point $\\mathbf{x}$ \u2019s nearest point in   \n493 the point cloud $\\mathbf{P}$ . The NC is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\scriptstyle\\mathrm{{NC}}\\displaystyle(\\mathbf{S}_{\\mathrm{Rec}},\\mathbf{S}_{\\mathrm{GT}})\\,=\\,\\!\\frac{1}{2N_{\\mathrm{eval}}}\\sum_{\\mathbf{x}\\in\\mathbf{P}_{\\mathrm{Rec}}}|\\mathbf{N}_{\\mathrm{Rec}}(\\mathbf{x})\\cdot\\boldsymbol{\\mathrm{NI}}_{-}\\boldsymbol{\\mathrm{Normal}}(\\mathbf{x},\\mathbf{P}_{\\mathrm{GT}})|}\\\\ {\\scriptstyle+\\frac{1}{2N_{\\mathrm{eval}}}\\sum_{\\mathbf{x}\\in\\mathbf{P}_{\\mathrm{GT}}}|\\mathbf{N}_{\\mathrm{GT}}(\\mathbf{x})\\cdot\\boldsymbol{\\mathrm{NI}}_{-}\\boldsymbol{\\mathrm{Normal}}(\\mathbf{x},\\mathbf{P}_{\\mathrm{Rec}})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "494 F-Score is defined as the harmonic mean between the precision and the recall of points that lie within   \n495 a certain distance threshold $\\epsilon$ between $\\mathbf{S}_{\\mathrm{Rec}}$ and $\\mathbf{S}_{\\mathrm{GT}}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{F-Score}(\\mathbf{S}_{\\mathrm{Rec}},\\mathbf{S}_{\\mathrm{GT}},\\epsilon)=\\frac{2\\cdot\\mathrm{Recal1\\cdotPrecision}}{\\mathrm{Recal1}+\\mathrm{Precision}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "496 where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Recal1}(\\mathbf{S}_{\\mathrm{Rec}},\\mathbf{S}_{\\mathrm{GT}},\\epsilon)=\\left|\\left\\{\\mathbf{x}_{1}\\in\\mathbf{P}_{\\mathrm{Rec}},\\mathrm{s.t.}\\underset{\\mathbf{x}_{2}\\in\\mathbf{P}_{\\mathrm{GT}}}{\\mathrm{min}}\\|\\mathbf{x}_{1}-\\mathbf{x}_{2}\\|_{2}<\\epsilon\\right\\}\\right|,}\\\\ {\\mathrm{Precision}(\\mathbf{S}_{\\mathrm{Rec}},\\mathbf{S}_{\\mathrm{GT}},\\epsilon)=\\left|\\left\\{\\mathbf{x}_{2}\\in\\mathbf{P}_{\\mathrm{GT}},\\mathrm{s.t.}\\underset{\\mathbf{x}_{1}\\in\\mathbf{P}_{\\mathrm{Rec}}}{\\mathrm{min}}\\|\\mathbf{x}_{1}-\\mathbf{x}_{2}\\|_{2}<\\epsilon\\right\\}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "image", "img_path": "lizRmKCnBp/tmp/f67170440ebf5ecb522778c965ac7b6da2b64785133e964d8d9b02830ad38d96.jpg", "img_caption": ["Figure 12: Pipeline of QuantDeepSDF. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "497 C.2 QuantDeepSDF ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "498 Compared to DeepSDF, our QuantDeepSDF incorporates the following two modifications: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The decoder parameters are quantized to enhance compression efficiency. ", "page_idx": 14}, {"type": "text", "text": "\u2022 To maintain consistency with our NeCGS, the points sampled during training are drawn from TSDF-Def volumes. ", "page_idx": 15}, {"type": "text", "text": "502 The pipeline of QuantDeepSDF is shown in Fig. 12. Specifically, the decoder is an MLP, where the   \n503 input is the concatenated vector of coordinate $\\mathbf{x}\\in\\mathbb{R}^{3}$ and the $i$ -th embedded feature vector $\\mathbf{F}_{i}\\in\\mathbb{R}^{C}$ ,   \n504 and the output is the corresponding TSDF-Def value. In our experiment, the decoder consists of 8   \n505 layers, and the compression ratio is controled by changing the width of each layer. ", "page_idx": 15}, {"type": "text", "text": "506 C.3 Auto-Encoder in Ablation Study ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "507 Different from the auto-encoder used in our framework, where the embed features are directly   \n508 optimized, auto-encoder utilizes an encoder to produce the embedded features, where the inputs are   \n509 the TSDF-Def volumes. And the decoder is kept the same as our framework. During the optimization,   \n510 the parameters of encoder and decoder are optimized. Once optimized, the embedded features   \n511 produced by the encoder and decoder parameters are compressed into bitstreams. ", "page_idx": 15}, {"type": "text", "text": "512 C.4 More Visual Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "513 Fig. 13 depicts the visual results of the decompresed models from the AMA dataset, DT4D dataset,   \n514 and Thingi10K dataset under various compression ratios, respectively. With the compression ratio   \n515 increasing, the decompressed models still preserve the detailed structures, without large distortion. ", "page_idx": 15}, {"type": "image", "img_path": "lizRmKCnBp/tmp/8383fd3f31877ab3c306c95f4c79f201778919b8d92d2f74bd617688f6db1423.jpg", "img_caption": ["Figure 13: Visual results of the decompressed models under different compression ratios. From Top to Bottom: AMA, DT4D, and Thingi10K. $\\triangledown\\alpha$ Zoom in for details. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "516 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "517 1. Claims   \n518 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n519 paper\u2019s contributions and scope?   \n520 Answer: [Yes]   \n521 Justification: Abstract.   \n522 Guidelines:   \n523 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n524 made in the paper.   \n525 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n526 contributions made in the paper and important assumptions and limitations. A No or   \n527 NA answer to this question will not be perceived well by the reviewers.   \n528 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n529 much the results can be expected to generalize to other settings.   \n530 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n531 are not attained by the paper.   \n532 2. Limitations   \n533 Question: Does the paper discuss the limitations of the work performed by the authors?   \n534 Answer: [Yes]   \n535 Justification: Sec. 5.   \n536 Guidelines:   \n537 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n538 the paper has limitations, but those are not discussed in the paper.   \n539 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n540 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n541 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n542 model well-specification, asymptotic approximations only holding locally). The authors   \n543 should reflect on how these assumptions might be violated in practice and what the   \n544 implications would be.   \n545 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n546 only tested on a few datasets or with a few runs. In general, empirical results often   \n547 depend on implicit assumptions, which should be articulated.   \n548 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n549 For example, a facial recognition algorithm may perform poorly when image resolution   \n550 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n551 used reliably to provide closed captions for online lectures because it fails to handle   \n552 technical jargon.   \n553 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n554 and how they scale with dataset size.   \n555 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n556 address problems of privacy and fairness.   \n557 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n558 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n559 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n560 judgment and recognize that individual actions in favor of transparency play an impor  \n561 tant role in developing norms that preserve the integrity of the community. Reviewers   \n562 will be specifically instructed to not penalize honesty concerning limitations.   \n563 3. Theory Assumptions and Proofs   \n564 Question: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 16}, {"type": "text", "text": "65 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "69 \u2022 The answer NA means that the paper does not include theoretical results.   \n70 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n1 referenced.   \n72 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n73 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n74 they appear in the supplemental material, the authors are encouraged to provide a short   \n75 proof sketch to provide intuition.   \n76 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n7 by formal proofs provided in appendix or supplemental material.   \n78 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "579 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "580 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n581 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n582 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "583   \n584 Justification: Sec. 4.1.   \n585 Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "18 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n19 tions to faithfully reproduce the main experimental results, as described in supplemental   \n20 material? ", "page_idx": 17}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "621   \n622   \n623   \n624   \n625   \n626   \n627   \n628   \n629   \n630   \n631   \n632   \n633   \n634   \n635   \n636   \n637   \n638   \n639   \n640   \n641   \n642   \n643   \n644   \n645   \n646   \n647   \n648   \n649   \n650   \n651   \n652   \n653   \n654   \n655   \n656   \n657   \n658   \n659   \n660   \n661   \n662   \n663   \n664   \n665   \n666   \n667   \n668   \n669   \n670   \n671   \n672 ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Sec. 4.1 Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Sec. 4.2 and 4.3. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "681 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "82 Question: For each experiment, does the paper provide sufficient information on the com  \n83 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n84 the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "686 Justification: Sec. 4.1 and 4.3.   \n687 Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "707 10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "708 Question: Does the paper discuss both potential positive societal impacts and negative   \n709 societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 19}, {"type": "text", "text": "724 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n725 that a generic algorithm for optimizing neural networks could enable people to train   \n726 models that generate Deepfakes faster.   \n727 \u2022 The authors should consider possible harms that could arise when the technology is   \n728 being used as intended and functioning correctly, harms that could arise when the   \n729 technology is being used as intended but gives incorrect results, and harms following   \n730 from (intentional or unintentional) misuse of the technology.   \n731 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n732 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n733 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n734 feedback over time, improving the efficiency and accessibility of ML).   \n735 11. Safeguards   \n736 Question: Does the paper describe safeguards that have been put in place for responsible   \n737 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n738 image generators, or scraped datasets)?   \n739 Answer: [NA]   \n740 Justification: [NA]   \n741 Guidelines:   \n742 \u2022 The answer NA means that the paper poses no such risks.   \n743 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n744 necessary safeguards to allow for controlled use of the model, for example by requiring   \n745 that users adhere to usage guidelines or restrictions to access the model or implementing   \n746 safety filters.   \n747 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n748 should describe how they avoided releasing unsafe images.   \n749 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n750 not require this, but we encourage authors to take this into account and make a best   \n751 faith effort.   \n752 12. Licenses for existing assets   \n753 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n754 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n755 properly respected?   \n756 Answer: [Yes]   \n757 Justification: [NA]   \n758 Guidelines:   \n759 \u2022 The answer NA means that the paper does not use existing assets.   \n760 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n761 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n762 URL.   \n763 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n764 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n765 service of that source should be provided.   \n766 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n767 package should be provided. For popular datasets, paperswithcode.com/datasets   \n768 has curated licenses for some datasets. Their licensing guide can help determine the   \n769 license of a dataset.   \n770 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n771 the derived asset (if it has changed) should be provided.   \n772 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n773 the asset\u2019s creators.   \n774 13. New Assets   \n775 Question: Are new assets introduced in the paper well documented and is the documentation   \n776 provided alongside the assets?   \n78 Justification: [NA]   \n79 Guidelines:   \n80 \u2022 The answer NA means that the paper does not release new assets.   \n781 \u2022 Researchers should communicate the details of the dataset/code/model as part of   \n82 their submissions via regular templates. This includes details about training, license,   \n783 limitations, etc.   \n84 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n785 asset is used.   \n86 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n87 create an anonymized URL or include an anonymized zip file.   \n788 14. Crowdsourcing and Research with Human Subjects   \n89 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n90 include the full text of instructions given to participants and screenshots, if applicable, as   \n91 well as details about compensation (if any)?   \n92 Answer: [NA]   \n93 Justification: [NA]   \n94 Guidelines:   \n95 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n796 human subjects.   \n97 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n798 tion of the paper involves human subjects, then as much detail as possible should be   \n99 included in the main paper.   \n00 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n801 or other labor should be paid at least the minimum wage in the country of the data   \n02 collector. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "05 Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]   \n10 Justification: [NA]   \n11 Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n14 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you   \n16 should clearly state this in the paper.   \n17 \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]