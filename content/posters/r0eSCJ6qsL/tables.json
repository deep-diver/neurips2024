[{"figure_path": "r0eSCJ6qsL/tables/tables_5_1.jpg", "caption": "Table 1: Analysis of the Configuration of Convolution and Transformer Blocks. We analyze various convolutional and transformer blocks by training hybrid architectures with different options on the ImageNet-1K dataset. We provide the inference latency (as throughput) for different GPUs. Results show that FusedMBConv and Vanilla Transformer blocks provide a better trade-off over accuracy and latency than others.", "description": "This table presents a comparison of different convolution and transformer blocks used in the AsCAN architecture. The comparison is based on inference latency (throughput) on A100 and V100 GPUs, along with the model's top-1 accuracy on the ImageNet-1K dataset. The results highlight that FusedMBConv and Vanilla Transformer blocks provide the best balance between accuracy and latency.", "section": "3.1 Architecture Design: Asymmetric Convolution-Attention Networks (ASCAN)"}, {"figure_path": "r0eSCJ6qsL/tables/tables_5_2.jpg", "caption": "Table 2: Analysis of Architecture Configuration. We analyze the distribution of convolution and transformer blocks by training hybrid architecture with different distributions of blocks on ImageNet-1K dataset [72]. It demonstrates that the design in Fig. 2 provides a better trade-off over accuracy and latency. Symbol M denotes MaxViT block [31] composed of MBConv[73, 75] and Multi-Axial Attention blocks, which is equivalent to CT. Note that CoAtNet [11] uses MBConv blocks compared to the FusedMBConv blocks in our design.", "description": "This table presents an ablation study on different configurations of convolutional and transformer blocks in a hybrid architecture for ImageNet-1K classification.  It compares various arrangements of these blocks (asymmetric vs symmetric, number of blocks per stage) in terms of their performance (top-1 accuracy), inference throughput on A100 and V100 GPUs, and model size (parameters).  The results demonstrate that the asymmetric configuration proposed in the paper offers superior performance and latency trade-offs.", "section": "3.1 Architecture Design: Asymmetric Convolution-Attention Networks (ASCAN)"}, {"figure_path": "r0eSCJ6qsL/tables/tables_7_1.jpg", "caption": "Table 3: GenEval Scores. We use this benchmark to compare T2I models on various aspects of generation, including counting, color attribution, position, etc. It clearly shows that our model achieves better overall score, by convincingly outperforming pipelines such as PixArt-a and SDXL.", "description": "This table presents the GenEval scores for various text-to-image (T2I) models, including the proposed AsCAN model and several baselines.  GenEval is a benchmark that evaluates different aspects of image generation, such as the accuracy of object counting, color and position recognition, and color attribution. The table shows that the AsCAN model outperforms the other methods across all metrics, indicating its superior performance in generating images that align well with the textual descriptions.", "section": "4.3 Text-to-Image Generation"}, {"figure_path": "r0eSCJ6qsL/tables/tables_9_1.jpg", "caption": "Table 4: ImageNet-1K Class Conditional Generation. We train a smaller variant of our T2I UNet architecture to perform class conditional generation on ImageNet-1K. We train this model at 256 \u00d7 256 and 512 \u00d7 512. Our asymmetric architecture achieves similar FID as the state-of-the-art models with less than half the floating point operations (e.g., Ours vs. DiT-XL/2-G), and better FID than the existing work with similar computation (e.g., Ours vs. U-ViT-L/2).", "description": "This table compares the performance of the proposed AsCAN architecture against other state-of-the-art models for class-conditional image generation on the ImageNet-1K dataset.  The comparison is made at two different resolutions (256x256 and 512x512).  Key metrics include the number of floating-point operations (FLOPs), throughput (samples per second on an A100 GPU with a batch size of 64), and the Fr\u00e9chet Inception Distance (FID), which measures the quality of the generated images.", "section": "4.2 Class Conditional Generation"}, {"figure_path": "r0eSCJ6qsL/tables/tables_16_1.jpg", "caption": "Table 7: Results on ImageNet-1K [72] for Classification Task. We compare the performance of our asymmetric architectures, AsCAN, against baselines. We report the inference latency as the throughput (images per second) that is measured by inferring images with batch size as B in half-precision, i.e., fp16, on an A100 GPU using torch-compile and benchmark utility from timm library [100]. A similar procedure (without torch-compile) is followed to obtain the throughput on the V100 GPU.", "description": "This table compares the performance of the proposed AsCAN architecture against various state-of-the-art baselines on the ImageNet-1K classification task.  It shows the inference throughput (images per second) for different batch sizes (B=1, 16, 64) on both A100 and V100 GPUs. The metrics reported include model parameters, multiply-accumulate operations (MACs), and top-1 accuracy.  The results highlight the superior throughput and accuracy trade-offs achieved by the AsCAN architecture compared to other convolutional, transformer, and hybrid architectures.", "section": "4.1 Image Recognition"}, {"figure_path": "r0eSCJ6qsL/tables/tables_23_1.jpg", "caption": "Table 7: Results on ImageNet-1K [72] for Classification Task. We compare the performance of our asymmetric architectures, AsCAN, against baselines. We report the inference latency as the throughput (images per second) that is measured by inferring images with batch size as B in half-precision, i.e., fp16, on an A100 GPU using torch-compile and benchmark utility from timm library [100]. A similar procedure (without torch-compile) is followed to obtain the throughput on the V100 GPU.", "description": "This table compares the performance of different CNN architectures on the ImageNet-1K image classification task.  The table shows the throughput (images per second) on A100 and V100 GPUs for various batch sizes (1, 16, 64) for each architecture, and their top-1 accuracy.  It highlights the performance and speed of the proposed AsCAN architecture against a variety of state-of-the-art convolutional and transformer-based networks.", "section": "4.1 Image Recognition"}, {"figure_path": "r0eSCJ6qsL/tables/tables_24_1.jpg", "caption": "Table 7: Results on ImageNet-1K [72] for Classification Task. We compare the performance of our asymmetric architectures, AsCAN, against baselines. We report the inference latency as the throughput (images per second) that is measured by inferring images with batch size as B in half-precision, i.e., fp16, on an A100 GPU using torch-compile and benchmark utility from timm library [100]. A similar procedure (without torch-compile) is followed to obtain the throughput on the V100 GPU.", "description": "This table compares the performance of the proposed AsCAN architecture with several state-of-the-art baselines on the ImageNet-1K image classification task.  It presents inference latency (measured as throughput in images per second) on both A100 and V100 GPUs for various batch sizes (B=1, B=16, B=64), along with the top-1 accuracy and model parameters (Params) and Multiply-Accumulate operations (MACs) for each model. The table highlights the superior performance and latency trade-offs achieved by AsCAN compared to other models.", "section": "4.1 Image Recognition"}, {"figure_path": "r0eSCJ6qsL/tables/tables_24_2.jpg", "caption": "Table 9: Analysis of Architecture Configuration (with T before C). We extend Tab. 2 by ablating over the preference of C and T blocks in a stage. Using T block in stem / early layers result in lower throughput. Further, the performance of configurations with T before C yields lower accuracy vs throughput trade-off.", "description": "This table shows the ablation study on the architecture design by changing the order of convolutional and transformer blocks within stages.  The results demonstrate that using convolutional blocks (C) before transformer blocks (T) leads to better performance in terms of accuracy and throughput.", "section": "Architecture Design: Asymmetric Convolution-Attention Networks (ASCAN)"}, {"figure_path": "r0eSCJ6qsL/tables/tables_24_3.jpg", "caption": "Table 7: Results on ImageNet-1K [72] for Classification Task. We compare the performance of our asymmetric architectures, AsCAN, against baselines. We report the inference latency as the throughput (images per second) that is measured by inferring images with batch size as B in half-precision, i.e., fp16, on an A100 GPU using torch-compile and benchmark utility from timm library [100]. A similar procedure (without torch-compile) is followed to obtain the throughput on the V100 GPU.", "description": "This table compares the performance of the proposed AsCAN architectures with various other state-of-the-art models on the ImageNet-1K image classification task.  It shows the throughput (images per second) achieved by each model on both A100 and V100 GPUs with batch sizes of 1, 16, and 64, along with their Top-1 accuracy.  The results highlight the superior performance and latency trade-offs of the AsCAN models.", "section": "4.1 Image Recognition"}, {"figure_path": "r0eSCJ6qsL/tables/tables_25_1.jpg", "caption": "Table 11: Semantic Segmentation Results on ADE20K [104]. We compare different backbones on the ADE20K [104] semantic segmentation benchmark with UPerNet [106] as the detection architecture. Computational and storage statistics are computed using the input resolution of 512 \u00d7 512.", "description": "This table compares the performance of different backbones (Swin-T, FasterViT-2, ASCAN-T, Swin-S, FasterViT-3, ASCAN-B, Swin-B, FasterViT-4, and ASCAN-L) on the ADE20K semantic segmentation dataset using the UPerNet architecture.  It presents the latency (in frames per second) on A100 and V100 GPUs, the number of parameters, the number of multiply-accumulate operations (MACs), and the mean Intersection over Union (mIoU) achieved by each backbone.", "section": "A.3 ADE20K Semantic Segmentation"}, {"figure_path": "r0eSCJ6qsL/tables/tables_25_2.jpg", "caption": "Table 12: Resource Usage Comparison. We compare the training time required to learn various diffusion models in the literature to our proposal. We also include parameter count and inference latency of these models.", "description": "This table compares the training time (in A100 GPU days), the number of parameters, and the inference time per image (at 512x512 and 1024x1024 resolutions) for several text-to-image generation models, including the authors' model.  It highlights the computational resource requirements of different approaches, showcasing the relative efficiency of the proposed model.", "section": "A.5 T2I Evaluation Details"}]