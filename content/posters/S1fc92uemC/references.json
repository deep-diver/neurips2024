{"references": [{"fullname_first_author": "Lewis", "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks", "publication_date": "2020-12-01", "reason": "This paper introduces the foundational Retrieval-Augmented Generation (RAG) framework, which is central to the current work."}, {"fullname_first_author": "Izacard", "paper_title": "Leveraging passage retrieval with generative models for open domain question answering", "publication_date": "2021-04-01", "reason": "This paper is highly influential in showing the potential of combining retrieval with generative models for open-domain question answering, a key motivation for the current work."}, {"fullname_first_author": "Liu", "paper_title": "ChatQA: Surpassing GPT-4 on conversational QA and RAG", "publication_date": "2024-01-01", "reason": "This paper proposes a strong baseline, ChatQA, that serves as a major comparison point for the proposed RankRAG model in terms of performance on various RAG benchmarks."}, {"fullname_first_author": "Karpukhin", "paper_title": "Dense passage retrieval for open-domain question answering", "publication_date": "2020-12-01", "reason": "This paper introduces the Dense Passage Retrieval (DPR) technique, a commonly used retriever that is compared to RankRAG and serves as an important part of the experimental setup."}, {"fullname_first_author": "Wang", "paper_title": "Aligning language models with self-generated instructions", "publication_date": "2023-07-01", "reason": "This paper explores instruction tuning for language models, a technique that is adapted and extended in RankRAG to combine context ranking and answer generation within a single model."}]}