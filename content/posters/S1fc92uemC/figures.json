[{"figure_path": "S1fc92uemC/figures/figures_2_1.jpg", "caption": "Figure 1: Performance of ChatQA-1.5, one of the strongest RAG models, on different context size k. We observe a trade-off of selecting top-k contexts: a smaller k compromises the recall, while a larger k could introduce irrelevant or noisy context and mislead the LLM generation.", "description": "This figure shows the performance of the ChatQA-1.5 model (a strong RAG model) on four different datasets (NQ, TriviaQA, PopQA, FEVER) when varying the number of top-k contexts used for answer generation.  The results illustrate a trade-off. Using a small number of top contexts (e.g., k=5) results in lower recall (fewer relevant documents retrieved), while using a large number of contexts (e.g., k=20) introduces noisy or irrelevant information that negatively impacts the model's ability to generate accurate answers. The optimal k value appears to be around 10 for the various datasets shown.", "section": "3.2 Limitation of Current RAG Pipelines"}, {"figure_path": "S1fc92uemC/figures/figures_3_1.jpg", "caption": "Figure 2: Two-stage instruction tuning framework for RankRAG.", "description": "This figure illustrates the two-stage instruction tuning framework used for RankRAG. Stage-I involves supervised fine-tuning (SFT) on various instruction-following datasets such as conversational datasets (SODA, Dolly, OpenAssistant), long-form QA datasets (ELI5), and synthetic instruction datasets (Self-Instruct, Unnatural Instructions). This stage enhances the LLM's instruction-following abilities. Stage-II focuses on RankRAG instruction tuning, where the LLM is trained on multiple tasks such as context-rich QA, retrieval-augmented QA, retrieval-augmented ranking, and context ranking. This is done using datasets like MS MARCO, conversational QA datasets (Synthetic Conversation, Human AnnotatedConvQA), and reading comprehension datasets (NarrativeQA, DROP, Quoref, NewsQA, TAT-QA, ROPES). This stage aims to enhance the LLM's capability for both context ranking and answer generation.  The inference stage is also depicted, showing a retriever extracting top-N documents, which are then reranked by RankRAG to select the top-K documents for answer generation.", "section": "4 RankRAG"}, {"figure_path": "S1fc92uemC/figures/figures_7_1.jpg", "caption": "Figure 3: Performance with different retrievers. The performance of Recall is in Appendix E.1.", "description": "This figure compares the performance of RankRAG and ChatQA-1.5 using two different retrievers: DPR and Contriever, across three question answering datasets (NQ, TriviaQA, and PopQA).  The x-axis represents the datasets, and the y-axis shows the Exact Match accuracy.  The bars in each group represent the performance of ChatQA-1.5 and RankRAG. The caption indicates that a more detailed breakdown of Recall performance for each model and retrieval method is available in Appendix E.1.", "section": "5.3 Ablation Studies"}, {"figure_path": "S1fc92uemC/figures/figures_9_1.jpg", "caption": "Figure 1: Performance of ChatQA-1.5, one of the strongest RAG models, on different context size k. We observe a trade-off of selecting top-k contexts: a smaller k compromises the recall, while a larger k could introduce irrelevant or noisy context and mislead the LLM generation.", "description": "This figure shows the performance of the ChatQA-1.5 model on four different question answering datasets (NQ, TriviaQA, PopQA, FEVER) with varying numbers of top-k retrieved contexts.  The results illustrate a trade-off; using a small number of contexts (k) reduces the recall of relevant information, while using a larger number of contexts increases the likelihood of including irrelevant or noisy information which negatively impacts the quality of the LLM's generated answers.", "section": "3 Preliminaries"}, {"figure_path": "S1fc92uemC/figures/figures_21_1.jpg", "caption": "Figure 1: Performance of ChatQA-1.5, one of the strongest RAG models, on different context size k. We observe a trade-off of selecting top-k contexts: a smaller k compromises the recall, while a larger k could introduce irrelevant or noisy context and mislead the LLM generation.", "description": "This figure shows the performance of the ChatQA-1.5 model on four different question answering datasets (NQ, TriviaQA, PopQA, FEVER) with varying numbers of top-k retrieved contexts.  It demonstrates a clear trade-off: using a small number of contexts (k=5) limits recall (the ability to find relevant information), while using too many contexts (k=20) introduces irrelevant or noisy information that negatively impacts the LLM's ability to generate accurate answers.  The optimal k value appears to be around 10, showing a balance between recall and the ability to filter out irrelevant information.", "section": "3 Preliminaries"}]