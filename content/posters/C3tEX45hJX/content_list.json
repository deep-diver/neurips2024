[{"type": "text", "text": "Diffusion Spectral Representation for Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dmitry Shribak\u2217 Chen-Xiao Gao\u2217 Yitong Li Georgia Tech Nanjing University Georgia Tech shribak@gatech.edu gaocx@lamda.nju.edu.cn yli3277@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Chenjun Xiao Bo DaiCUHK(SZ) Georgia Techchenjunx@cuhk.edu.cn bodai@cc.gatech.edu", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion-based models have achieved notable empirical successes in reinforcement learning (RL) due to their expressiveness in modeling complex distributions. Despite existing methods being promising, the key challenge of extending existing methods for broader real-world applications lies in the computational cost at inference time, i.e., sampling from a diffusion model is considerably slow as it often requires tens to hundreds of iterations to generate even one sample. To circumvent this issue, we propose to leverage the flexibility of diffusion models for RL from a representation learning perspective. In particular, by exploiting the connection between diffusion models and energy-based models, we develop Diffusion Spectral Representation (Diff-SR), a coherent algorithm framework that enables extracting sufficient representations for value functions in Markov decision processes (MDP) and partially observable Markov decision processes (POMDP). We further demonstrate how Diff-SR facilitates efficient policy optimization and practical algorithms while explicitly bypassing the difficulty and inference cost of sampling from the diffusion model. Finally, we provide comprehensive empirical studies to verify the benefits of Diff-SR in delivering robust and advantageous performance across various benchmarks with both fully and partially observable settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have demonstrated remarkable generative modeling capabilities, achieving significant success in producing high-quality samples across various domains such as images and videos [Ramesh et al., 2021, Saharia et al., 2022, Brooks et al., 2024]. In comparison to other generative approaches, diffusion models stand out for their ability to represent complex, multimodal data distributions, a strength that can be attributed to two primary factors. First, diffusion models progressively denoise data by reversing a diffusion process. This iterative refinement process empowers them to capture complicated patterns and structures within the data distribution, thus enabling the generation of samples with unprecedented accuracy [Ho et al., 2020]. Second, diffusion models exhibit impressive mode coverage, effectively addressing a common issue of mode collapse encountered in other generative approaches [Song et al., 2020]. ", "page_idx": 0}, {"type": "text", "text": "The potential of diffusion models is increasingly being investigated for sequential decision-making tasks. The inherent flexibility of diffusion models to accurately capture complex data distributions makes them exceptionally suitable for both model-free and model-based methods in reinforcement learning (RL). There are three main approaches that attempts to apply diffusion models, including diffusion policy [Wang et al., 2022, Chi et al., 2023], diffusion-based planning [Janner et al., 2022, Jackson et al., 2024, Du et al., 2024], and diffusion world model [Ding et al., 2024, Rigter et al., 2023]. Empirical results indicate that diffusion-based approaches can stabilize the training process and enhance empirical performance compared with their conventional counterparts, especially in environments with high-dimensional inputs. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The flexibility of diffusion models, however, also comes with a substantial inference cost: generating even a single sample from a diffusion model is notably slow, typically requiring tens to thousands of iterations [Ho et al., 2020, Lu et al., 2022, Zhang and Chen, 2022, Song et al., 2023]. Furthermore, prior works also consider generating multiple samples to enhance quality, which further exacerbates this issue [Rigter et al., 2023]. The computational demands are particularly problematic for RL, since whether employing a diffusion-based policy or diffusion world model, the learning agent must frequently query the model for interactions with the environment during the learning phase or when deployed in the environment. This becomes the key challenge when extending diffusion-based methods for broader applications with more complex state spaces. Meanwhile, the planning with exploration issue has not been explicitly considered in the existing diffusion-based RL algorithms. The flexibility of diffusion models in fact induces extra difficulty in the implementation of the principle of optimism in the face of uncertainty in the planning step to balance the inherent trade-off between exploration vs. exploitation, which is indispensable in the online setting to avoid suboptimal policies [Lattimore and Szepesv\u00e1ri, 2020, Amin et al., 2021]. ", "page_idx": 1}, {"type": "text", "text": "In conclusion, there has been insufficient work considering both efficiency and computation tractability for planning and exploration in a unified and coherent perspective, when applying diffusion model for sequential decision-making. This raises a very natural question, i.e., ", "page_idx": 1}, {"type": "text", "text": "Can we exploit the flexibility of diffusion models with efficient planning and exploration for RL? ", "page_idx": 1}, {"type": "text", "text": "In this paper, we provide an affirmative answer to this question, based on our key observation that diffusion models, beyond their conventional role as generative tools, can play a crucial role in learning sufficient representations for RL. Specifically, ", "page_idx": 1}, {"type": "text", "text": "\u2022 By exploiting the energy-based model view of the diffusion model, we develop a coherent algorithmic framework Diffusion Spectral Representation (Diff-SR), designed to learn representations that capture the latent structure of the transition function in Section 3.2;   \n\u2022 We then show that such diffusion-based representations are sufficiently expressive to represent the value function of any policy, which paves the way for efficient planning and exploration, circumventing the need for sample generation from the diffusion model, and thus, avoiding the inference costs associated with prior diffusion-based methods in Section 3.3;   \n\u2022 We conduct comprehensive empirical studies to validate the beneftis of Diff-SR, in both fully and partially observable RL settings, demonstrating its robust, superior performance and efficiency across various benchmarks in Section 5. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we briefly introduce the Markov Decision Processes, as the standard mathematical abstraction for RL, and diffusion models, as the building block for our algorithm design. ", "page_idx": 1}, {"type": "text", "text": "Markov Decision Processes (MDPs). We consider Markov Decision Processes [Puterman, 2014] specified by the tuple $\\mathcal{M}\\,=\\,\\langle S,\\mathcal{A},\\mathbb{P},r,\\gamma,\\mu_{0}\\rangle$ , where $\\boldsymbol{S}$ is the state space, $\\boldsymbol{\\mathcal{A}}$ is the action space, $\\bar{\\mathbb{P}^{\\prime}}:S\\times\\bar{A^{\\prime}}\\!\\to\\Delta(\\bar{S})$ is the transition function, $r:S\\times A\\to\\mathbb{R}$ is the reward function, $\\gamma\\in[0,1)$ is the discount factor, $\\mu_{0}\\,\\in\\,\\Delta(S)$ is the initial state distribution2. The value function specifies the discounted cumulative rewards obtained by following a policy $\\pi\\,:\\,{\\mathcal{S}}\\,\\rightarrow\\,\\Delta(A),\\,V^{{\\dagger}}(s)\\,=$ $\\begin{array}{r}{\\mathbb{E}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s\\right]}\\end{array}$ , where $\\mathbb{E}^{\\pi}$ denotes the expectation under the distribution induced by the interconnection of $\\pi$ and the environment. The state-action value function is defined by ", "page_idx": 1}, {"type": "text", "text": "The goal of RL is to find an optimal policy that maximizes the policy value, i.e., $\\pi^{*}=$ $\\mathrm{argmax}_{\\pi}$ $\\mathbb{E}_{s\\sim\\mu_{0}}[V^{\\pi}(s)]$ . ", "page_idx": 1}, {"type": "text", "text": "For any MDP, one can always factorize the transition operator through the singular value decomposition (SVD), i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}(s^{\\prime}|s,a)=\\langle\\phi^{*}(s,a),\\mu^{*}(s^{\\prime})\\rangle\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with $\\langle\\cdot,\\cdot\\rangle$ defined as the inner product. Yao et al. [2014], Jin et al. [2020], Agarwal et al. [2020a] considered a subset of MDPs, in which $\\phi^{\\ast}\\left(s,a\\right)\\in\\mathbb{R}^{d}$ with finite $d$ , which is known as Linear/Lowrank MDP. The subclass is then generalized for infinite spectrum with fast decay eigenvalues [Ren et al., 2022a]. Leveraging this specific structure of the function class, this spectral view serves as an instrumental framework for examining the statistical and computational attributes of RL algorithms in the context of function approximation. In fact, the most significant advantage of exploiting said spectral structure is that we can represent its state-value function $Q^{\\pi}(s,a)$ as a linear function with respect to $[r(s,a),\\phi^{*}(s,a)]$ for any policy $\\pi$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim\\mathbb{P}(\\cdot\\mid s,a)}\\left[V^{\\pi}(s^{\\prime})\\right]=r(s,a)+\\left\\langle\\phi^{*}(s,a),\\int_{S}\\mu^{*}(s^{\\prime})V^{\\pi}(s^{\\prime})d s^{\\prime}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It\u2019s important to highlight that in many practical scenarios, the feature mapping $\\phi^{*}$ is often unknown. In the meantime, the learning of the $\\phi^{*}$ is essentially equivalent to un-normalized conditional density estimation, which is notoriously difficult [LeCun et al., 2006, Song and Kingma, 2021, Dai et al., 2019]. Besides the optimization intractability in learning, the coupling of exploration to learning also compounds the difficulty: learning $\\phi^{*}$ requires full-coverage data to capture $\\mathbb{P}(s^{\\prime}|s,a)$ , while the design of exploration strategy often relies on an accurate $\\phi^{*}$ [Jin et al., 2020, Yang et al., 2020]. Recently, a range of spectral representation learning algorithms has emerged to address these challenges and provide an estimate of $\\phi^{*}$ in both online and offline settings [Uehara et al., 2021]. However, existing methods either require designs of negative samples [Ren et al., 2022b, Qiu et al., 2022, Zhang et al., 2022], or rely on additional assumptions on $\\phi^{*}$ [Ren et al., 2022c,a]. ", "page_idx": 2}, {"type": "text", "text": "Diffusion Models. Diffusion models [Sohl-Dickstein et al., 2015, Ho et al., 2020, Song et al., 2020] are composed of a forward Markov process gradually perturbing the observations $x_{0}\\sim p_{0}\\left(x\\right)$ to a target distribution $x_{T}\\sim q_{T}\\left(x\\right)$ with corruption kernel $q_{t+1|t}$ , and a backward Markov process recovering the original observations distribution from the noisy $x_{T}$ . After $T$ steps, the forward process forms a joint distribution, ", "page_idx": 2}, {"type": "equation", "text": "$$\nq_{0:T}\\left(x_{0:T}\\right)=p_{0}\\left(x_{0}\\right)\\prod_{t=0}^{T-1}q_{t+1|t}\\left(x_{t+1}|x_{t}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The reverse process can be derived by Bayes\u2019 rule from the joint distribution, i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\nq_{t\\mid t+1}\\left(x_{t}|x_{t+1}\\right)={\\frac{q_{t+1\\mid t}\\left(x_{t+1}|x_{t}\\right)q_{t}\\left(x_{t}\\right)}{q_{t+1}\\left(x_{t+1}\\right)}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with $q_{t}\\left(x_{t}\\right)$ as the marginal distribution at $t$ -step. Although we can obtain the expression of reverse kernel $q_{t|t+1}(\\cdot|\\cdot)$ from Bayes\u2019s rule, it is usually intractable. Therefore, the reverse kernel is usually parameterized with a neural network, denoted as $p^{\\theta}(x_{t}|x_{t+1})$ . Recognizing that the diffusion models are a special class of latent variable models [Sohl-Dickstein et al., 2015, Ho et al., 2020], maximizing the ELBO emerges as a natural choice for learning, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{\\Sigma}_{e l b o}\\left(\\theta\\right)=\\mathbb{E}_{p_{0}\\left(x\\right)}\\left[D_{K L}\\left(q_{T\\mid0}\\middle|q_{T}\\right)+\\sum_{t=1}^{T-1}\\mathbb{E}_{q_{t\\mid0}}\\left[D_{K L}\\left(q_{t\\mid t+1,0}\\middle||p_{t\\mid t+1}^{\\theta}\\right)\\right]-\\mathbb{E}_{q_{1\\mid0}}\\left[\\log p_{0\\mid1}^{\\theta}\\left(x_{0}\\middle|x_{1}\\right)\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For continuous domain, the forward process of corruption usually employs Gaussian noise, i.e., $q_{t+1|t}(x_{t+1}|x_{t})\\;\\;=\\;\\;{\\mathcal N}(x_{t+1};{\\sqrt{1-\\beta_{t+1}}}x_{t},\\beta_{t+1}I)$ , where $t\\;\\;\\in\\;\\;\\{0,\\ldots,T-1\\}$ . The kernel for backward process is also Gaussian and can be parametrized as $p^{\\theta}\\left(x_{t}|x_{t+1}\\right)\\;=$ $\\begin{array}{r}{\\mathcal{N}\\left(x_{t};\\frac{1}{\\sqrt{1-\\beta_{t+1}}}\\left(x_{t+1}+\\beta_{t+1}s^{\\theta}\\left(x_{t+1},t+1\\right)\\right),\\beta_{t+1}I\\right)\\right)}\\end{array}$ , where $s^{\\theta}\\left(x_{t+1},t+1\\right)$ denotes the score network with parameters $\\theta$ . The ELBO can be specified as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\overline{{\\ell}}_{s m}\\left(\\theta\\right)=\\sum_{t=1}^{T}\\left(1-\\alpha_{t}\\right)\\mathbb{E}_{p_{0}}\\mathbb{E}_{q_{t|0}}\\left[\\left|\\left|s^{\\theta}\\left(x_{t},t\\right)-\\nabla_{x_{t}}\\log q_{t|0}\\left(x_{t}|x_{0}\\right)\\right|\\right|^{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\alpha_{t}=\\prod_{i=1}^{t}(1-\\beta_{i})}\\end{array}$ . With the learned $s^{\\theta}\\left(x,t\\right)$ , the samples can be generated by sampling $x_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , and then following the estimated reverse Markov chain with $p^{\\theta}\\left(x_{t}|x_{t+1}\\right)$ iteratively. To ensure the quality of samples from diffusion models, the reverse Markov chain requires tens to thousands of iterations, which induces high computational costs for diffusion model applications. ", "page_idx": 2}, {"type": "text", "text": "Random Fourier Features. Random Fourier features [Rahimi and Recht, 2007, Dai et al., 2014] allow us to approximate infinite-dimensional kernels using finite-dimensional feature vectors. Bochner\u2019s theorem states that a continuous function of the form $k(x,y)=k(x-y)$ can be represented by a Fourier transform of a bounded positive measure [Bochner, 1932]. For Gaussian kernel $k(x-y)$ , consider the feature $z_{\\omega}(x)=\\exp\\!{\\left(-\\mathbf{i}\\omega^{\\top}x\\right)}$ , with $\\omega\\sim{\\mathcal{N}}(0,I)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k(x-y)=\\left[\\int p(\\omega)\\exp(-\\mathbf{i}\\omega^{\\top}(x-y))\\mathrm{d}\\omega\\right]=\\mathbb{E}_{\\omega}[-\\exp(\\mathbf{i}\\omega^{\\top}(x-y))]]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\omega}\\left[z_{\\omega}(x)z_{\\omega}(y)^{*}\\right]=\\langle z_{\\omega}(x),z_{\\omega}(y)\\rangle_{\\mathcal{N}(\\omega)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot\\rangle_{\\mathscr{N}(\\omega)}$ is a shorthand for $\\mathbb{E}_{\\omega\\sim\\mathcal{N}(0,I)}[\\left<\\cdot,\\cdot\\right>]$ . By sampling $\\omega_{1},\\omega_{2},\\ldots,\\omega_{N}\\;\\sim\\;{\\mathcal N}(0,I)$ , we can approximate $k(x\\,-\\,y)$ with the inner product of finite-dimentional vectors $\\hat{z}_{\\omega}(x)~=~$ $\\begin{array}{r}{\\frac{1}{\\sqrt{N}}\\big(z_{\\omega_{1}}(\\hat{x}),z_{\\omega_{2}}(x),\\ldots,z_{\\omega_{N}}(\\acute{x})\\big)}\\end{array}$ and $\\begin{array}{r}{\\hat{z}_{\\omega}(y)=\\frac{\\textbf{\\hat{l}}}{\\sqrt{N}}(z_{\\omega_{1}}(y),z_{\\omega_{2}}(y),\\dots,z_{\\omega_{N}}(y))}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Diffusion Spectral Representation for Efficient Reinforcement Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "It is well known that the generation procedure of diffusion models becomes the major barrier for real-world application, especially in RL. Moreover, the complicated generation procedure makes the uncertainty estimation for exploration intractable. The spectral representation $\\phi^{*}(s,a)$ provides an efficient way for planning and exploration, as illustrated in Section 2, which inspires our Diffusion Spectral Representation $(D i f f.S R)$ for RL, as our answer to the motivational question. As we will demonstrate below, the representation view of diffusion model enjoys the flexibility and also enables efficient planning and exploration, while directly bypasses the cost of sampling. For simplicity, we introduce Diff-SR in MDP setting in the main text. However, the proposed Diff-SR is also applicable for POMDPs as shown in Appendix B. We first illustrate the inherent challenges of applying diffusion models for representation learning in RL. ", "page_idx": 3}, {"type": "text", "text": "3.1 An Impossible Reduction to Latent Variable Representation [Ren et al., 2022a] ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the latent variable representation (LV-Rep) [Ren et al., 2022a], the latent variable model is exploited for spectral representation $\\phi^{*}\\left(s,a\\right)$ in (1), by which an arbitrary state-value function can be linearly represented, and thus, efficient planning and exploration is possible. Specifically, in the LV-Rep, one considers the factorization of dynamics as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}(s^{\\prime}|s,a)=\\int p(z|s,a)p(s^{\\prime}|z)d z=\\langle p(z|s,a),p(s^{\\prime}|z)\\rangle_{L_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By recognizing the connection between (5) and SVD of transition operator (1), the learned latent variable model $p(z|s,a)$ can be used as $\\phi^{*}(s,a)$ for linearly representing the $Q^{\\pi}$ -function for an arbitrary policy $\\pi$ . ", "page_idx": 3}, {"type": "text", "text": "Since the diffusion model can be recast as a special type of latent variable model [Ho et al., 2020], the first straightforward idea is to extend LV-Rep with diffusion models for $p(z|s,a)$ . We consider the following forward process that perturbs each $z_{i-1}$ with Gaussian noises: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(z_{i}|z_{i-1},s,a),\\quad\\forall i=1,\\ldots,k,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $z_{0}=s^{\\prime}$ . Then, following the definition of the diffusion model, the backward process can be set as ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(z_{i-1}|z_{i},s,a),\\quad\\forall i=0,\\ldots,k,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which are Gaussian distributions that denoise the perturbed latent variables. Thus, the dynamics model can be formulated as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}(s^{\\prime}|s,a)=\\int\\prod_{i=2}^{k}q(z_{i-1}|z_{i},s,a)q(s^{\\prime}|z_{1},s,a)d\\left\\{z_{i}\\right\\}_{i=1}^{k}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Indeed, Equation (7) converts the diffusion model to a latent variable model for dynamics modeling. However, the dependency of $(s,a)$ in $q(s^{\\prime}|z_{1},s,a)$ correspondingly induces the undesirable dependency of $(s,a)$ into $\\mu(s^{\\bar{\\prime}})$ in (1), and therefore the factorization provided by the diffusion model cannot linearly represent the $Q^{\\pi}$ -function \u2014 the vanilla LV-Rep reduction from diffusion models is impossible. ", "page_idx": 3}, {"type": "text", "text": "3.2 Diffusion Spectral Representation from Energy-based View ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Instead of reducing to LV-Rep, in this work we extract the spectral representations by exploiting the relationship between diffusion models and energy-based models (EBMs). ", "page_idx": 3}, {"type": "text", "text": "Spectral Representation from EBMs. We parameterize the transition operator $\\mathbb{P}(s^{\\prime}|s,a)$ using an EBM, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}(s^{\\prime}|s,a)=\\exp\\left(\\psi(s,a)^{\\top}\\nu\\left(s^{\\prime}\\right)-\\log Z\\left(s,a\\right)\\right),\\,Z\\left(s,a\\right)=\\int\\exp\\left(\\psi(s,a)^{\\top}\\nu\\left(s^{\\prime}\\right)\\right)d s^{\\prime}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By simple algebra manipulation, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\stackrel{.}{\\psi}(s,a)^{\\top}\\dot{\\boldsymbol\\nu^{\\prime}}(s^{\\prime})=-\\frac{1}{2}\\left(\\left\\|\\psi\\left(s,a\\right)-\\boldsymbol\\nu\\left(s^{\\prime}\\right)\\right\\|^{2}-\\left\\|\\psi\\left(s,a\\right)\\right\\|^{2}-\\left\\|\\boldsymbol\\nu\\left(s^{\\prime}\\right)\\right\\|^{2}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "we obtain the quadratic potential function, leading to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}\\!\\left(s^{\\prime}|s,a\\right)\\propto\\exp\\!\\left(\\Vert\\psi\\left(s,a\\right)\\Vert^{2}/2\\right)\\exp\\left(-\\Vert\\psi\\left(s,a\\right)-\\nu\\left(s^{\\prime}\\right)\\Vert^{2}/2\\right)\\exp\\left(\\Vert\\nu\\left(s^{\\prime}\\right)\\Vert^{2}/2\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The term $\\begin{array}{r}{\\exp\\left(-\\frac{\\left\\|\\psi(s,a)-\\nu\\left(s^{\\prime}\\right)\\right\\|^{2}}{2}\\right)}\\end{array}$ is the Gaussian kernel, for which we apply the random Fourier feature [Rahimi and Recht, 2007, Dai et al., 2014] and obtain the spectral decomposition of (8), (10) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}(s^{\\prime}|s,a)=\\langle\\phi_{\\omega}\\left(s,a\\right),\\mu_{\\omega}\\left(s^{\\prime}\\right)\\rangle_{\\mathcal{N}(\\omega)}\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\omega\\sim{\\mathcal{N}}(0,I)$ , and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi_{\\omega}\\left(s,a\\right)=\\exp\\left(-\\mathbf{i}\\omega^{\\top}\\boldsymbol{\\psi}\\left(s,a\\right)\\right)\\exp\\left(\\Vert\\boldsymbol{\\psi}\\left(s,a\\right)\\Vert^{2}/2-\\log Z\\left(s,a\\right)\\right),}\\\\ {\\mu_{\\omega}\\left(s^{\\prime}\\right)=\\exp\\left(-\\mathbf{i}\\omega^{\\top}\\boldsymbol{\\nu}\\left(s^{\\prime}\\right)\\right)\\exp\\left(\\Vert\\boldsymbol{\\nu}\\left(s^{\\prime}\\right)\\Vert^{2}/2\\right).\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This bridges the factorized EBMs (8) to SVD, offering a spectral representation for efficient planning and exploration, as will be shown subsequently. ", "page_idx": 4}, {"type": "text", "text": "Exploiting the random feature to connect EBMs to spectral representation for RL was first proposed by Nachum and Yang [2021] and Ren et al. [2022c] , but only Gaussian dynamics $p(s^{\\prime}|s,a)$ has been considered for its closed-form $Z\\left(s,a\\right)$ and tractable MLE. Equation (8) is also discussed in [Zhang et al., 2023b, Ouhamma et al., 2023, Zheng et al., 2022] for exploration with UCB-style bonuses. Due to the notorious difficulty in MLE of EBMs caused by the intractability of $Z\\left(s,a\\right)$ [Zhang et al., 2022], only special cases of (8) have been practically implemented. How to efficiently exploit the flexibility of general EBMs in practice still remains an open problem. ", "page_idx": 4}, {"type": "text", "text": "Representation Learning via Diffusion. We revisit the EBM understanding of diffusion models, which not only justifies the flexibility of diffusion models, but more importantly, paves the way for efficient learning of spectral representations (11) through Tweedie\u2019s identity for diffusion models. ", "page_idx": 4}, {"type": "text", "text": "Given $(s,a)$ , we consid\u221aer perturbing the samples from dynamics $s^{\\prime}\\sim\\mathbb{P}(s^{\\prime}|s,a)$ with Gaussian noise, i.e., $\\mathbb{P}(\\stackrel{.}{s}^{\\prime}|s^{\\prime};\\beta)=\\mathcal{N}\\left(\\sqrt{1-\\beta}s^{\\prime},\\beta\\stackrel{.}{I}\\right)$ . Then, we parametrize the corrupted dynamics as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\tilde{s}^{\\prime}|s,a;\\beta)=\\int\\mathbb{P}(\\tilde{s}^{\\prime}|s^{\\prime};\\beta)\\mathbb{P}(s^{\\prime}|s,a)d s^{\\prime}\\propto\\exp\\left(\\psi\\left(s,a\\right)^{\\top}\\nu(\\tilde{s}^{\\prime},\\beta)\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\psi\\left(s,a\\right)$ is shared across all noise levels $\\beta$ , and $\\mathbb{P}(\\tilde{s}^{\\prime}|s,a;\\alpha)\\rightarrow\\mathbb{P}(s^{\\prime}|s,a)$ with $\\tilde{s}^{\\prime}\\,\\rightarrow\\,s^{\\prime}$ , as $\\beta\\rightarrow0$ , there is no noise corruption on $s^{\\prime}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 1 (Tweedie\u2019s Identity [Efron, 2011]). For arbitrary corruption $\\mathbb{P}\\left(\\tilde{s}^{\\prime}|s^{\\prime};\\beta\\right)$ and $\\beta$ in $\\mathbb{P}\\left(\\tilde{s^{\\prime}}|s,a;\\beta\\right)$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\tilde{s}^{\\prime}}\\log\\mathbb{P}(\\tilde{s}^{\\prime}|s,a;\\beta)=\\mathbb{E}_{\\mathbb{P}(s^{\\prime}|\\tilde{s}^{\\prime},s,a;\\beta)}\\left[\\nabla_{\\tilde{s}^{\\prime}}\\log\\mathbb{P}(\\tilde{s}^{\\prime}|s^{\\prime};\\beta)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This can be easily verified by simple calculation, $i.e.$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\tilde{s}^{\\prime}}\\log\\mathbb{P}(\\tilde{s}^{\\prime}|s,a;\\beta)=\\frac{\\overleftarrow{\\nabla}_{\\tilde{s}^{\\prime}}\\mathbb{P}(\\tilde{s}^{\\prime}|s,a;\\beta)}{\\mathbb{P}(\\tilde{s}^{\\prime}|s,a;\\beta)}=\\frac{\\nabla_{\\tilde{s}^{\\prime}}\\int\\mathbb{P}(\\tilde{s}^{\\prime}|s^{\\prime};\\beta)\\mathbb{P}(s^{\\prime}|s,a)d s^{\\prime}}{\\mathbb{P}(\\tilde{s}^{\\prime}|s,a;\\beta)}}\\\\ &{=\\int\\frac{\\nabla_{\\tilde{s}^{\\prime}}\\log\\mathbb{P}(\\tilde{s}^{\\prime}|s^{\\prime};\\beta)\\mathbb{P}(\\tilde{s}^{\\prime}|s^{\\prime};\\beta)\\mathbb{P}(s^{\\prime}|s,a)}{\\mathbb{P}(\\tilde{s}^{\\prime}|s,a;\\beta)}d s^{\\prime}=\\mathbb{E}_{\\mathbb{P}(s^{\\prime}|\\tilde{s}^{\\prime},s,a;\\beta)}\\left[\\nabla_{\\tilde{s}^{\\prime}}\\log\\mathbb{P}(\\tilde{s}^{\\prime}|s^{\\prime};\\beta)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For Gaussian perturbation with (13), Tweedie\u2019s identity (14) is applied as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\psi\\left(s,a\\right)^{\\top}\\nabla_{\\tilde{s}^{\\prime}}\\nu\\left(\\tilde{s}^{\\prime},\\beta\\right)=\\mathbb{E}_{\\mathbb{P}\\left(s^{\\prime}\\mid\\tilde{s}^{\\prime},s,a;\\beta\\right)}\\left[\\frac{\\sqrt{1-\\beta}s^{\\prime}-\\tilde{s}^{\\prime}}{\\beta}\\right]}\\\\ {\\Rightarrow\\tilde{s}^{\\prime}+\\beta\\psi\\left(s,a\\right)^{\\top}\\nabla_{\\tilde{s}^{\\prime}}\\nu\\left(\\tilde{s}^{\\prime},\\beta\\right)=\\sqrt{1-\\beta}\\mathbb{E}_{\\mathbb{P}\\left(s^{\\prime}\\mid\\tilde{s}^{\\prime},s,a;\\beta\\right)}\\left[s^{\\prime}\\right].\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $\\zeta\\left(\\tilde{s}^{\\prime};\\beta\\right)=\\nabla_{\\tilde{s}^{\\prime}}\\nu\\left(\\tilde{s}^{\\prime},\\beta\\right)$ , we can learn $\\psi\\left(s,a\\right)$ and $\\zeta\\left(\\tilde{s}^{\\prime};\\beta\\right)$ by matching both sides of (16), ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\psi,\\zeta}\\;\\mathbb{E}_{\\beta}\\mathbb{E}_{(s,a,\\tilde{s}^{\\prime})}\\left[\\left\\|\\tilde{s}^{\\prime}+\\beta\\psi\\left(s,a\\right)^{\\top}\\zeta\\left(\\tilde{s}^{\\prime},\\beta\\right)-\\sqrt{1-\\beta}\\mathbb{E}_{\\mathbb{P}(s^{\\prime}|\\tilde{s}^{\\prime},s,a;\\beta)}\\left[s^{\\prime}\\right]\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "1: Input: representation networks $\\psi,\\zeta$ , noise levels $\\{\\beta^{k}\\}_{k=1}^{T}$ , replay buffer $\\mathcal{D}$   \n2: for update step $=1,2,...,N_{\\mathrm{rep}}$ do   \n3: Sample a batch of $n$ transitions $\\{(s_{i},a_{i},s_{i}^{\\prime})\\}_{i=1}^{n}\\sim\\mathcal{D}$   \n4: Sample noise schedules for each transition $\\{\\beta_{i}\\}_{i=1}^{n}\\sim\\operatorname{Uniform}(\\beta^{1},\\beta^{2},\\dots,\\beta^{T})$   \n5: Corrupt the next states $\\widetilde{s}_{i}^{\\prime}\\leftarrow\\sqrt{1-\\beta_{i}}s_{i}^{\\prime}+\\dot{\\sqrt{\\beta_{i}}}\\epsilon_{i}$ , where $\\epsilon_{i}\\sim\\mathcal{N}(0,I)$   \n6: Optimize $\\psi,\\zeta$ via gradient descent by minimizing Eq (18)   \n7: end for ", "page_idx": 5}, {"type": "text", "text": "which shares the same optimum of ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\psi,\\zeta}\\;\\ell_{\\mathrm{diff}}\\left(\\psi,\\dot{\\zeta}\\right):=\\mathbb{E}_{\\beta}\\mathbb{E}_{\\left(s,a,\\tilde{s}^{\\prime},s^{\\prime}\\right)}\\left[\\left\\|\\tilde{s}^{\\prime}+\\beta\\psi\\left(s,a\\right)^{\\top}\\zeta\\left(\\tilde{s}^{\\prime},\\beta\\right)-\\sqrt{1-\\beta}s^{\\prime}\\right\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The equivalence of (17) and (18) is provided in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Diffusion Spectral Representation for $Q$ -function. The loss described by (18) estimates the score function $\\psi\\left(s,a\\right)^{\\top}\\zeta\\left(\\tilde{s}^{\\prime},\\beta\\right)$ for diffusion models. In the context of generating samples, the score function suffices to drive the reverse Markov chain process. However, when deriving the random feature $\\phi_{\\omega}$ defined in (11), the partition function $Z(s,a)$ is indispensable. Furthermore, the random feature $\\phi_{\\omega}(s,a)$ is only conceptual with infinite dimensions where $\\omega\\sim\\mathcal{N}\\left(0,I\\right)$ . Next, we will proceed to analyze the structure of $Z(s,a)$ and finally construct the spectral representation with the learned $\\psi\\left(s,a\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "We first illustrate $Z(s,a)$ is also linearly representable by random features of $\\psi\\left(s,a\\right)$ , ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. Denote $\\rho_{\\omega}\\left(s,a\\right):=\\exp\\left(-\\mathbf{i}\\omega^{\\top}\\boldsymbol{\\psi}\\left(s,a\\right)+\\left\\|\\boldsymbol{\\psi}\\left(s,a\\right)\\right\\|^{2}/2\\right)$ , the partition function is linearly representable by $\\rho_{\\omega}\\left(s,a\\right)$ , i.e., $\\Tilde{Z^{\\prime}}(s,a)=\\langle\\rho_{\\omega}\\left(s,a\\right),u\\rangle_{\\mathcal{N}(\\omega)}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. We have $\\begin{array}{r}{\\phi_{\\omega}\\left(s,a\\right)=\\frac{\\rho_{\\omega}\\left(s,a\\right)}{Z\\left(s,a\\right)}}\\end{array}$ , and $\\begin{array}{r}{\\mathbb{P}\\left(s^{\\prime}|s,a\\right)=\\left\\langle\\frac{\\rho_{\\omega}\\left(s,a\\right)}{Z\\left(s,a\\right)},\\mu_{\\omega}\\left(s^{\\prime}\\right)\\right\\rangle_{\\mathcal{N}\\left(\\omega\\right)}}\\end{array}$ , which implies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int\\mathbb{P}\\left(s^{\\prime}|s,a\\right)d s^{\\prime}=1\\Rightarrow\\left\\langle\\frac{\\rho_{\\omega}\\left(s,a\\right)}{Z\\left(s,a\\right)},\\underbrace{\\int\\mu_{\\omega}\\left(s^{\\prime}\\right)d s^{\\prime}}_{u}\\right\\rangle=1\\Rightarrow\\left\\langle\\rho_{\\omega}\\left(s,a\\right),u\\right\\rangle_{\\mathcal{N}\\left(\\omega\\right)}=Z\\left(s,a\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Plug this into (11) and (2), we can represent the $Q^{\\pi}$ -function for arbitrary $\\pi$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q^{\\pi}\\left(s,a\\right)=\\left\\langle\\frac{\\rho_{\\omega}\\left(s,a\\right)}{\\langle\\rho_{\\omega}\\left(s,a\\right),u\\rangle},\\xi^{\\pi}\\right\\rangle_{\\mathcal{N}(\\omega)}=\\left\\langle\\underbrace{\\exp\\left(-\\mathrm{i}\\omega^{\\top}\\psi\\left(s,a\\right)-\\log\\left\\langle\\exp\\left(-\\mathrm{i}\\omega^{\\top}\\psi\\left(s,a\\right)\\right),u\\right\\rangle_{\\mathcal{N}(\\omega)}\\right)}_{\\varphi_{\\omega,u}\\left(s,a\\right)},\\xi^{\\pi}\\right\\rangle_{\\mathcal{N}(\\omega)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which eliminates the explicit partition function calculation. ", "page_idx": 5}, {"type": "text", "text": "We thereby construct $D$ iffusion Spectral Representation $(D i f\\!\\!f\\!\\!-\\!S R)$ , a tractable finite-dimensional representation, by approximating $\\varphi_{\\omega,u}(s,a)$ with some neural network upon $\\psi\\left(s,a\\right)$ . Specifically, in the definition of $\\varphi_{\\omega,u}\\left(s,a\\right)$ in (19), it contains Fourier basis $\\exp\\left(-\\mathbf{i}\\omega^{\\top}\\psi\\left(s,a\\right)\\right)$ , suggesting the use of trigonometry functions [Rahimi and Recht, 2007] upon $\\psi\\left(s,a\\right)$ , i.e., si $\\mathrm{n}\\left(W_{1}^{\\top}\\psi\\left(s,a\\right)\\right)$ . Meanwhile, it also contains a product wi th\u27e8exp(\u2212i\u03c9\u22a4\u03c81(s,a)),u\u27e9N (\u03c9) , suggesting the additional nonlinearity over $\\sin\\left(W_{1}^{\\top}\\psi\\left(s,a\\right)\\right)$ . Therefore, we consider the finite-dimensional neural network $\\phi_{\\theta}\\left(s,a\\right)=\\mathtt{e l u}\\left(W_{2}\\sin\\left(W_{1}^{\\top}\\psi\\left(s,a\\right)\\right)\\right)\\in\\mathbb{R}^{d}$ with learnable parameters $\\theta=(W_{1},W_{2})$ , to approximate the infinite-dimensional $\\varphi_{\\omega}\\left(s,a\\right)$ with $\\omega\\sim{\\mathcal{N}}\\left(0,I\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "Remark (Connection to sufficient dimension reduction [Sasaki and Hyv\u00e4rinen, 2018]): The theoretical properties of the factorized potential function in EBMs (8) have been investigated in [Sasaki and Hyv\u00e4rinen, 2018]. Specifically, the factorization is actually capable of universal approximation. Moreover, $\\psi\\left(s,a\\right)$ also constructs an implementation of sufficient dimension reduction [Fukumizu et al., 2009], i.e., given $\\psi\\left(s,a\\right)$ , we have $\\mathbb{P}(s^{\\prime}|s,a)=\\mathbb{P}\\left(s^{\\prime}|\\psi\\left(s,a\\right)\\right)$ , or equivalently $s^{\\prime}\\perp(s,a)|\\psi\\left(s,a\\right)$ . These properties justify the expressiveness and sufficiency of the learned $\\psi\\left(s,a\\right)$ . However, $\\psi\\left(s,a\\right)$ in [Sasaki and Hyv\u00e4rinen, 2018] is only estimated up to the partition function $Z(s,a)$ , which makes it not directly applicable for planning and exploration in RL as we discussed. ", "page_idx": 5}, {"type": "text", "text": "1: Initialize networks $\\pi,(\\xi_{1},\\theta_{1}),(\\xi_{2},\\theta_{2})$ , and $\\mathcal{D}=\\emptyset$   \n2: for timestep $t=1$ to $T$ do   \n3: $a_{t}\\sim\\pi(\\cdot|s_{t})$   \n4: $r_{t}=r(s_{t},a_{t})$ , $s_{t}^{\\prime}\\sim\\mathbb{P}(\\cdot|s_{t},a_{t})$   \n5: Compute bonus $b(s_{t},a_{t})$ using (23) (Optional)   \n6: $D\\gets D\\cup(s_{t},a_{t},r_{t},s_{t}^{\\prime})$   \n7: Update $\\psi$ with $\\mathcal{D}$ by Algorithm 1   \n8: Update the critic $(\\xi_{1},\\theta_{1})$ , $(\\xi_{2},\\theta_{2})$ by (21)   \n9: Update the policy $\\pi$ by $\\begin{array}{r}{\\operatorname*{max}_{\\pi}\\mathbb{E}_{s\\sim\\mathcal{D},a\\sim\\pi}[\\operatorname*{min}_{i\\in\\{1,2\\}}Q_{\\xi_{i},\\theta}(s,a)]}\\end{array}$   \n10: end for   \n11: Return $\\pi$ . ", "page_idx": 6}, {"type": "text", "text": "3.3 Policy Optimization with Diffusion Spectral Representation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "With the approximated finite-dimensional representation $\\phi_{\\theta}$ , the $Q$ -function can be represented as a linear function of $\\phi_{\\theta}$ and a weight vector $\\boldsymbol{\\xi}\\in\\mathbb{R}^{d}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\nQ_{\\xi,\\theta}(s,a)=\\phi_{\\theta}(s,a)^{\\top}\\xi\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This approximated value function can be integrated into any model-free algorithm for policy optimization. In particular, we update $(\\xi,\\theta)$ with the standard TD learning objective ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{critic}}(\\xi,\\theta)=\\mathbb{E}_{s,a,r,s^{\\prime}\\sim\\mathcal{D}}\\left[\\left(r+\\gamma\\mathbb{E}_{a^{\\prime}\\sim\\pi}[Q_{\\bar{\\xi},\\bar{\\theta}}(s^{\\prime},a^{\\prime})]-Q_{\\xi,\\theta}(s,a)\\right)^{2}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\pi$ is the learning algorithm\u2019s current policy, $(\\bar{\\xi},\\bar{\\theta})$ is the target network, $\\mathcal{D}$ is the replay buffer. We apply the double $Q$ -network trick to stabilize training [Fujimoto et al., 2018]. In particular, two weights $(\\xi_{1},\\theta_{1}),(\\xi_{2},\\theta_{2})$ are initialized and updated independently according to (21). Then the policy is updated by considering $\\scriptstyle\\operatorname*{max}_{\\pi}\\mathbb{E}_{s\\sim\\mathcal{D},a\\sim\\pi}[\\operatorname*{min}_{i\\in\\{1,2\\}}Q_{\\xi_{i},\\theta_{i}}(s,a)]$ . Algorithm 2 presents the pseudocode of online RL with Diff-SR. This learning framework is largely consistent with standard online reinforcement learning (RL) approaches, with the primary distinction being the incorporation of diffusion representations. As more data is collected, Line 7 specifies the update of the diffusion representation $\\phi_{\\theta}$ using Algorithm 1 and the latest buffer $\\mathcal{D}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark (Exploration with Diffusion Spectral Representation): Following [Guo et al., 2023], even $\\exp\\left(-\\mathbf{\\bar{i}}\\omega^{\\top}\\psi\\left(s,a\\right)\\right)$ is not enough for linearly representing $Q^{\\pi}$ , we still can exploit $\\exp\\left(-\\mathbf{i}\\omega^{\\top}\\psi\\left(s,a\\right)\\right)$ for bonus calculation to implement the principle of optimism in the face of uncertainty for exploration in RL, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\nb(s,a)=1-K^{\\top}\\left(s,a\\right)\\left(K+\\lambda I\\right)^{-1}K\\left(s,a\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$K(s,a):=[k((s,a),(s,a)_{i})]_{(s,a)_{i}\\in\\mathcal{D}}$ , and $K=[K((s,a)_{j})]_{(s,a)_{j}\\in\\mathcal{D}}.$ . The bonus (22) derivation is straightforward by applying the connection between random feature and kernel, similar to [Zheng et al., 2022]. In practice, we can also approximate UCB bonus with Diff-SR as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b(s,a)=\\phi_{\\bar{\\theta}}(s,a)^{\\top}\\left(\\sum_{s^{\\prime},a^{\\prime}\\in\\mathcal{D}}\\phi_{\\bar{\\theta}}(s,a)\\phi_{\\bar{\\theta}}(s,a)^{\\top}+\\lambda I\\right)^{-1}\\phi_{\\bar{\\theta}}(s,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "These bonuses are subsequently added to the reward in (21) to facilitate exploration. ", "page_idx": 6}, {"type": "text", "text": "Remark (Comparison to Spectral Representation): Both the proposed Diff-SR and the existing spectral representation for RL [Zhang et al., 2022, Ren et al., 2022b,a, Zhang et al., 2023b] are extracting the representation for $Q$ -function. However, we emphasize that the major difference lies in that existing spectral representations seek low-rank linear representations, while our representation is sufficient for representing $Q$ -function, but in a nonlinear form, as we revealed in (19). This nonlinearity in fact comes from the partition function $Z\\left(s,a\\right)$ , which is constrained to be 1 in [Zhang et al., 2022, 2023b], and thus, less flexible for representation. Meanwhile, even with nonlinear representation, it has been shown that the corresponding bonus is still enough for exploration [Guo et al., 2023], without additional computational cost. ", "page_idx": 6}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Representation learning in RL. Learning good abstractions for the raw states and actions based on the structure of the environment dynamics is thought to facilitate policy optimization. To effectively capture the information in said dynamics, existing representation learning methods employ various techniques, such as reconstruction [Watter et al., 2015, Hafner et al., 2019b, Fujimoto et al., 2023], successor features [Dayan, 1993, Kulkarni et al., 2016, Barreto et al., 2017], bisimulation [Ferns et al., 2004, Gelada et al., 2019, Zhang et al., 2021], contrastive learning [Oord et al., 2018, Nachum and Yang, 2021], and spectral decomposition [Mahadevan and Maggioni, 2007, Wu et al., 2018, Duan et al., 2019, Ren et al., 2022b]. Previous works also leverage the assumption that the transition kernel possesses a low-rank spectral structure, which permits linear representations for the state-action value function and provably sample-efficient reinforcement learning [Jin et al., 2020, Yang and Wang, 2020, Agarwal et al., 2020b, Uehara et al., 2022]. Based on this, there have been several attempts towards both practical and theoretically grounded reinforcement learning algorithms by extracting the spectral representations from the transition kernel [Ren et al., 2022c, Zhang et al., 2022, Ren et al., 2022a, Zhang et al., 2023a]. Our approach aligns with this paradigm but distinguishes itself by learning these representations via diffusion and enjoying the flexibility of energy-based modeling. ", "page_idx": 7}, {"type": "text", "text": "Diffusion model for RL. By virtue of their ability to model complex and multimodal distributions, diffusion models present themselves as well-suited candidates for specific components in reinforcement learning. For example, diffusion models can be utilized to synthesize complex behaviors [Janner et al., 2022, Ajay et al., 2022, Chi et al., 2023, Du et al., 2024], represent multimodal policies [Wang et al., 2022, Hansen-Estruch et al., 2023, Chen et al., 2023b], or provide behavior regularizations [Chen et al., 2023a]. Another line of research utilizes the diffusion model as the world model. Among them, DWM [Ding et al., 2024] and SynthER [Lu et al., 2023] train a diffusion model with off-policy dataset and augment the training dataset with synthesized data, while PolyGRAD [Rigter et al., 2023] and PGD [Jackson et al., 2024] sample from the diffusion model with policy guidance to generate near on-policy trajectory. On a larger scale, UniSim [Yang et al., 2023] employs a video diffusion model to learn a real-world simulator that accommodates instructions in various modalities. All of these methods incur great computational costs because they all involve iteratively sampling from the diffusion model to generate actions or trajectories. In contrast, our method leverages the capabilities of diffusion models from the perspective of representation learning, thus circumventing the generation costs. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate our method with state-based MDP tasks (Gym-MuJoCo locomotion [Todorov et al., 2012]) and image-based POMDP tasks (Meta-World Benchmark [Yu et al., 2020]) in this section. Besides, we also provide experiments with state-based POMDP tasks in Appendix E. Our code is publicly released at the project website. ", "page_idx": 7}, {"type": "image", "img_path": "C3tEX45hJX/tmp/77f789c5c9f26688ceea660248cf295aa7478beccd0d97e22d96977f2c327be7.jpg", "img_caption": ["Figure 1: The performance curves of the Diff-SR and baseline methods on MBBL tasks. We report the mean (solid line) and one standard deviation (shaded area) across 4 random seeds. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.1 Results of Gym-MuJoCo Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this first set of experiments, we compare Diff-SR against both model-based and model-free baseline algorithms, using the implementations provided by MBBL [Wang et al., 2019]. For representationbased RL algorithms, we include LV-Rep [Ren et al., 2022a], SPEDE [Ren et al., 2022c] and Deep ", "page_idx": 7}, {"type": "table", "img_path": "C3tEX45hJX/tmp/f6e29cf84d2d6a425cb1030a07e61a4a9bbc3998505a59a9cfe74fafa6910acf.jpg", "table_caption": ["Table 1: Performances of Diff-SR and baseline RL algorithms after 200K environment steps. Results are averaged across 4 random seeds and a window size of 10K steps. Results marked with \\* are taken from MBBL [Wang et al., 2019] and $\\dagger$ are taken from LV-Rep [Ren et al., 2022a]. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Successor Feature (DeepSF) [Barreto et al., 2017] as baselines. Note that the SPEDE is a special case of Gaussian EBM. We also include PolyGRAD [Rigter et al., 2023], a recent method that utilizes diffusion models for RL as an additional baseline. All algorithms are executed for 200K environment steps and we report the mean and standard deviation of performances across 4 random seeds. More implementation details including the hyper-parameters can be found in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "Table 1 presents the results, demonstrating that Diff-SR achieves significantly better or comparable performance to all baseline methods in most tasks except for Humanoid-ET. Specifically, in Ant and Walker, Diff-SR outperforms the second highest baseline LV-Rep by $90\\%$ and $48\\%$ . Moreover, Diff-SR consistently surpasses PolyGRAD in nearly all environments. We also provide the learning curves of Diff-SR and baseline methods in Figure 1 for an illustrative interpretation of the sample efficiency Diff-SR brings. ", "page_idx": 8}, {"type": "text", "text": "Computational Efficiency and Runtime Comparison Compared to other diffusion-based RL algorithms, DiffSR harnesses diffusion\u2019s flexibility while circumventing the time-consuming sampling process. To showcase this, we record the runtime of Diff-SR and PolyGRAD on MBBL tasks using workstations equipped with Quadro RTX 6000 cards. Results in Figure 2 illustrate that Diff-SR is about $4\\times$ faster than PolyGRAD, and such advantage is consistent across all environments. We provide a per-task breakdown of the runtime results in Appendix 4 due to space constraints. ", "page_idx": 8}, {"type": "image", "img_path": "C3tEX45hJX/tmp/61453f87bd8b46e862995bf516fa4ed4ee781af8f9e52ab8435c53af8b8d0e0b.jpg", "img_caption": ["Figure 2: Runtime comparison between Diff-SR vs. LV-Rep vs. diffusion-based RL (PolyGRAD). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Results of Meta-World Tasks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As the most difficult setting, we evaluate Diff-SR with 8 visual-input tasks selected from the MetaWorld Benchmark. Rather than directly diffuse over the space of raw pixels, we resort to techniques similar to the Latent Diffusion Model (LDM) [Rombach et al., 2022] which first encodes the raw observation image to a 1-D compact latent vector and afterward performs the diffusion process over the latent space. To deal with the partial observability, we truncate the history frames with length $L$ , encode these $L$ frames into their latent embeddings, concatenate them together, and treat them as the state of the agent. The learning of diffusion representation thus translates into predicting the next frame\u2019s latent embedding with the action and $L$ -step concatenated embedding. Following existing practices [Zhang et al., 2023a], we set $L=3$ for all Meta-World tasks. We use DrQ-V2 [Yarats et al., 2021] as the base RL algorithm, and more details of the implementations are deferred to Appendix F. ", "page_idx": 8}, {"type": "image", "img_path": "C3tEX45hJX/tmp/6211d6d3473ff50d27fabd80e171c6d4e97c2a5bcdaf314a100237f00cf1ae71.jpg", "img_caption": ["Figure 3: Performance curves for image-based POMDP tasks from Meta-World. We report the mean (solid line) and the standard deviation (shaded area) of performances across 5 random seeds. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "The performance curves of Diff-SR and the baseline algorithms are presented in Figure 3. We see that Diff-SR achieves a greater than $90\\%$ success rate for seven of the tasks, 4 more tasks than the second best baseline $\\mu\\mathrm{LV}$ -Rep. Overall, Diff-SR exhibits superior performance, faster convergence speed, and stable optimization in most of the tasks compared to the baseline methods. Finally, although Diff-SR does not require sample generation, we present the reconstruction results to validate the efficacy of the score function $\\bar{\\psi}\\left(s,a\\right)^{\\top}\\zeta\\left(\\tilde{s}^{\\prime},\\beta\\right)$ in Figure 6. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions and Discussions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce Diff-SR, a novel algorithmic framework designed to leverage diffusion models for reinforcement learning from a representation learning perspective. The primary contribution of our work lies in exploiting the connection between diffusion models and energy-based models, thereby enabling the extraction of spectral representations of the transition function. We demonstrate that such diffusion-based representations can sufficiently express the value function of any policy, facilitating efficient planning and exploration while mitigating the high inference costs typically associated with diffusion-based methods. Empirically, we conduct comprehensive studies to validate the effectiveness of Diff-SR in both fully and partially observable sequential decision-making problems. Our results underscore the robustness and advantages of Diff-SR across various benchmarks. However, the main limitation of this study is that Diff-SR has not yet been evaluated with real-world and multi-task data. Future work will focus on testing Diff-SR on real-world applications, such as robotic control. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and representation learning of low rank mdps. Advances in neural information processing systems, 33:20095\u201320107, 2020a. ", "page_idx": 10}, {"type": "text", "text": "Alekh Agarwal, Sham M. Kakade, Akshay Krishnamurthy, and Wen Sun. FLAMBE: structural complexity and representation learning of low rank mdps. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020b. ", "page_idx": 10}, {"type": "text", "text": "Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision-making? arXiv preprint arXiv:2211.15657, 2022. ", "page_idx": 10}, {"type": "text", "text": "Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, and Doina Precup. A survey of exploration methods in reinforcement learning. arXiv preprint arXiv:2109.00157, 2021. ", "page_idx": 10}, {"type": "text", "text": "Andr\u00e9 Barreto, Will Dabney, R\u00e9mi Munos, Jonathan J. Hunt, Tom Schaul, David Silver, and Hado van Hasselt. Successor features for transfer in reinforcement learning. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 4055\u20134065, 2017. ", "page_idx": 10}, {"type": "text", "text": "Salomon Bochner. Vorlesungen uber fouriersche integrale. In Akademische Verlagsgesellschaf, 1932.   \nTim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/video-generationmodels-as-world-simulators.   \nHuayu Chen, Cheng Lu, Zhengyi Wang, Hang Su, and Jun Zhu. Score regularized policy optimization through diffusion behavior. In The Twelfth International Conference on Learning Representations, 2023a.   \nHuayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu. Offilne reinforcement learning via high-fidelity generative behavior modeling. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b.   \nCheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023.   \nKurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \nIgnasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-based reinforcement learning via meta-policy optimization. In Conference on Robot Learning, pages 617\u2013629. PMLR, 2018.   \nBo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, and Le Song. Scalable kernel methods via doubly stochastic gradients. Advances in neural information processing systems, 27, 2014.   \nBo Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le Song, and Dale Schuurmans. Exponential family estimation via adversarial dynamics embedding. Advances in Neural Information Processing Systems, 32, 2019.   \nPeter Dayan. Improving generalization for temporal difference learning: The successor representation. ", "page_idx": 10}, {"type": "text", "text": "Neural Comput., 5(4):613\u2013624, 1993. ", "page_idx": 10}, {"type": "text", "text": "Marc Peter Deisenroth and Carl Edward Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pages 465\u2013472. Omnipress, 2011.   \nZihan Ding, Amy Zhang, Yuandong Tian, and Qinqing Zheng. Diffusion world model. arXiv preprint arXiv:2402.03570, 2024.   \nYilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in Neural Information Processing Systems, 36, 2024.   \nYaqi Duan, Zheng Tracy Ke, and Mengdi Wang. State aggregation learning from markov transition data. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 4488\u20134497, 2019.   \nBradley Efron. Tweedie\u2019s formula and selection bias. Journal of the American Statistical Association, 106(496):1602\u20131614, 2011.   \nYonathan Efroni, Chi Jin, Akshay Krishnamurthy, and Sobhan Miryoosef.i Provable reinforcement learning with a short-term memory. In International Conference on Machine Learning, pages 5832\u20135850. PMLR, 2022.   \nNorm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. In UAI \u201904, Proceedings of the 20th Conference in Uncertainty in Artificial Intelligence, Banff, Canada, July 7-11, 2004, pages 162\u2013169. AUAI Press, 2004.   \nScott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actorcritic methods. In International conference on machine learning, pages 1587\u20131596. PMLR, 2018.   \nScott Fujimoto, Wei-Di Chang, Edward J. Smith, Shixiang Gu, Doina Precup, and David Meger. For SALE: state-action representation learning for deep reinforcement learning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \nKenji Fukumizu, Francis R Bach, and Michael I Jordan. Kernel dimension reduction in regression. 2009.   \nTanmay Gangwani, Joel Lehman, Qiang Liu, and Jian Peng. Learning belief representations for imitation learning in pomdps. In uncertainty in artificial intelligence, pages 1061\u20131071. PMLR, 2020.   \nCarles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G. Bellemare. Deepmdp: Learning continuous latent space models for representation learning. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2170\u20132179. PMLR, 2019.   \nJiacheng Guo, Zihao Li, Huazheng Wang, Mengdi Wang, Zhuoran Yang, and Xuezhou Zhang. Provably efficient representation learning with tractable planning in low-rank pomdp. In International Conference on Machine Learning, pages 11967\u201311997. PMLR, 2023.   \nZhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Bernardo A Pires, and R\u00e9mi Munos. Neural predictive belief representations. 2018.   \nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1861\u20131870. PMLR, 10\u201315 Jul 2018.   \nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. 2019a.   \nDanijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2555\u20132565. PMLR, 2019b.   \nDanijar Hafner, Timothy P. Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.   \nPhilippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine. Idql: Implicit q-learning as an actor-critic method with diffusion policies. arXiv preprint arXiv:2304.10573, 2023.   \nNicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \nMatthew Thomas Jackson, Michael Tryfan Matthews, Cong Lu, Benjamin Ellis, Shimon Whiteson, and Jakob Foerster. Policy-guided diffusion. arXiv preprint arXiv:2404.06356, 2024.   \nMichael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. arXiv preprint arXiv:2205.09991, 2022.   \nChi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on learning theory, pages 2137\u20132143. PMLR, 2020.   \nTejas D Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J Gershman. Deep successor reinforcement learning. arXiv preprint arXiv:1606.02396, 2016.   \nThanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble trust-region policy optimization, 2018.   \nTor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \nYann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. Predicting structured data, 1(0), 2006.   \nAlex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. volume 33, pages 741\u2013752, 2020.   \nSergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.   \nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022.   \nCong Lu, Philip J. Ball, Yee Whye Teh, and Jack Parker-Holder. Synthetic experience replay. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \nYuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees, 2021.   \nSridhar Mahadevan and Mauro Maggioni. Proto-value functions: A laplacian framework for learning representation and control in markov decision processes. J. Mach. Learn. Res., 8:2169\u20132231, 2007.   \nOfir Nachum and Mengjiao Yang. Provable representation learning for imitation with contrastive fourier features. Advances in Neural Information Processing Systems, 34:30100\u201330112, 2021.   \nAnusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 7559\u20137566, 2018. doi: 10.1109/ICRA.2018.8463189.   \nTianwei Ni, Benjamin Eysenbach, and Ruslan Salakhutdinov. Recurrent model-free rl can be a strong baseline for many pomdps. 2021.   \nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \nReda Ouhamma, Debabrota Basu, and Odalric Maillard. Bilinear exponential family of mdps: frequentist regret bound with tractable exploration & planning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 9336\u20139344, 2023.   \nMartin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.   \nShuang Qiu, Lingxiao Wang, Chenjia Bai, Zhuoran Yang, and Zhaoran Wang. Contrastive ucb: Provably efficient contrastive self-supervised learning in online reinforcement learning. In International Conference on Machine Learning, pages 18168\u201318210. PMLR, 2022.   \nAli Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in neural information processing systems, 20, 2007.   \nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821\u20138831. Pmlr, 2021.   \nTongzheng Ren, Chenjun Xiao, Tianjun Zhang, Na Li, Zhaoran Wang, Sujay Sanghavi, Dale Schuurmans, and Bo Dai. Latent variable representation for reinforcement learning. arXiv preprint arXiv:2212.08765, 2022a.   \nTongzheng Ren, Tianjun Zhang, Lisa Lee, Joseph E Gonzalez, Dale Schuurmans, and Bo Dai. Spectral decomposition representation for reinforcement learning. arXiv preprint arXiv:2208.09515, 2022b.   \nTongzheng Ren, Tianjun Zhang, Csaba Szepesv\u00e1ri, and Bo Dai. A free lunch from the noise: Provable and practical exploration for representation learning. In Uncertainty in Artificial Intelligence, pages 1686\u20131696. PMLR, 2022c.   \nMarc Rigter, Jun Yamada, and Ingmar Posner. World models via policy-guided trajectory diffusion. arXiv preprint arXiv:2312.08533, 2023.   \nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 10674\u201310685. IEEE, 2022.   \nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479\u201336494, 2022.   \nHiroaki Sasaki and Aapo Hyv\u00e4rinen. Neural-kernelized conditional density estimation. arXiv preprint arXiv:1806.01754, 2018.   \nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1889\u20131897, Lille, France, 07\u201309 Jul 2015. PMLR.   \nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.   \nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \nYang Song and Diederik P Kingma. How to train your energy-based models. arXiv preprint arXiv:2101.03288, 2021.   \nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023.   \nYuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex behaviors through online trajectory optimization. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 4906\u20134913, 2012. doi: 10.1109/IROS.2012.6386025.   \nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.   \nMasatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and offline rl in low-rank mdps. arXiv preprint arXiv:2110.04652, 2021.   \nMasatoshi Uehara, Ayush Sekhari, Jason D. Lee, Nathan Kallus, and Wen Sun. Provably efficient reinforcement learning in partially observable dynamical systems. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.   \nTingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement learning, 2019.   \nZhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022.   \nManuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin A. Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2746\u20132754, 2015.   \nStephan Weigand, Pascal Klink, Jan Peters, and Joni Pajarinen. Reinforcement learning using guided observability. 2021.   \nYifan Wu, George Tucker, and Ofir Nachum. The laplacian in rl: Learning representations with efficient approximations. arXiv preprint arXiv:1810.04586, 2018.   \nLin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10746\u201310756. PMLR, 2020.   \nMengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 2023.   \nZhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael Jordan. Provably efficient reinforcement learning with kernel and neural function approximations. Advances in Neural Information Processing Systems, 33:13903\u201313916, 2020.   \nHengshuai Yao, Csaba Szepesv\u00e1ri, Bernardo Avila Pires, and Xinhua Zhang. Pseudo-mdps and factored linear action models. In 2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), pages 1\u20139. IEEE, 2014.   \nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning, 2021.   \nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 1094\u20131100. PMLR, 2020.   \nAmy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.   \nHongming Zhang, Tongzheng Ren, Chenjun Xiao, Dale Schuurmans, and Bo Dai. Provable representation with efficient planning for partially observable reinforcement learning. arXiv preprint arXiv:2311.12244, 2023a.   \nQinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022.   \nTianjun Zhang, Tongzheng Ren, Mengjiao Yang, Joseph Gonzalez, Dale Schuurmans, and Bo Dai. Making linear mdps practical via contrastive representation learning. In International Conference on Machine Learning, pages 26447\u201326466. PMLR, 2022.   \nTianjun Zhang, Tongzheng Ren, Chenjun Xiao, Wenli Xiao, Joseph E Gonzalez, Dale Schuurmans, and Bo Dai. Energy-based predictive representations for partially observed reinforcement learning. In Uncertainty in Artificial Intelligence, pages 2477\u20132487. PMLR, 2023b.   \nSirui Zheng, Lingxiao Wang, Shuang Qiu, Zuyue Fu, Zhuoran Yang, Csaba Szepesvari, and Zhaoran Wang. Optimistic exploration with learned features provably solves markov decision processes with neural dynamics. In The Eleventh International Conference on Learning Representations, 2022. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Detailed Derivations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition 3. Equation (17) shares the same optimal solutions with Equation (18). ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\beta}\\mathbb{E}_{(s,a,\\bar{s}^{\\prime},s^{\\prime})}\\left[\\left\\|\\tilde{s}^{\\prime}+\\beta\\psi\\left(s,a\\right)^{\\top}\\zeta\\left(\\bar{s}^{\\prime},\\beta\\right)-\\sqrt{1-\\beta}s^{\\prime}\\right\\|^{2}\\right]}\\\\ {=}&{\\mathbb{E}_{\\beta}\\mathbb{E}_{(s,a,\\bar{s}^{\\prime},s^{\\prime})}\\left[\\left\\|\\tilde{s}^{\\prime}+\\beta\\psi\\left(s,a\\right)^{\\top}\\zeta\\left(\\bar{s}^{\\prime},\\beta\\right)-b+b-\\sqrt{1-\\beta}s^{\\prime}\\right\\|^{2}\\right]}\\\\ {=}&{\\mathbb{E}_{\\beta}\\mathbb{E}_{(s,a,\\bar{s}^{\\prime},s^{\\prime})}\\left[\\left\\|\\tilde{s}^{\\prime}+\\beta\\psi\\left(s,a\\right)^{\\top}\\zeta\\left(\\bar{s}^{\\prime},\\beta\\right)-b\\right\\|^{2}\\right]}\\\\ {+}&{2\\underbrace{\\mathbb{E}_{\\beta}\\mathbb{E}_{(s,a,\\bar{s}^{\\prime},s^{\\prime})}\\left[\\left(\\tilde{s}^{\\prime}+\\beta\\psi\\left(s,a\\right)^{\\top}\\zeta\\left(\\bar{s}^{\\prime},\\beta\\right)-b\\right)^{\\top}\\left(b-\\sqrt{1-\\beta}s^{\\prime}\\right)\\right]}_{0}}\\\\ {+}&{\\underbrace{\\left(1-\\beta\\right)\\mathbb{E}_{\\beta}\\mathbb{E}_{(s,a,\\bar{s}^{\\prime},s^{\\prime})}\\left[\\left\\|s^{\\prime}-\\mathbb{E}_{\\mathbb{F}(s^{\\prime}|\\bar{s}^{\\prime},s,a,\\beta)}\\left[s^{\\prime}\\right]\\right\\|^{2}\\right].}_{0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The second term equals to 0 comes from the definition of $b$ . Moreover, since $b$ is independent w.r.t. $s^{\\prime}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{\\bar{z}}_{\\beta}\\mathbb{E}_{(s,a,\\bar{s}^{\\prime},s^{\\prime})}\\left[\\left\\|\\tilde{s}^{\\prime}+\\beta\\psi\\left(s,a\\right)^{\\top}\\zeta\\left(\\tilde{s}^{\\prime},\\beta\\right)-b\\right\\|^{2}\\right]=\\mathbb{E}_{\\beta}\\mathbb{E}_{(s,a,\\tilde{s}^{\\prime})}\\left[\\left\\|\\tilde{s}^{\\prime}+\\beta\\psi\\left(s,a\\right)^{\\top}\\zeta\\left(\\tilde{s}^{\\prime},\\beta\\right)-b\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{\\beta}\\mathbb{E}_{\\left(s,a,\\tilde{s}^{\\prime},s^{\\prime}\\right)}\\left[\\left\\|\\tilde{s}^{\\prime}+\\beta\\psi\\left(s,a\\right)^{\\top}\\zeta\\left(\\tilde{s}^{\\prime},\\beta\\right)-\\sqrt{1-\\beta}s^{\\prime}\\right\\|^{2}\\right]}\\\\ &{}&{=\\mathbb{E}_{\\beta}\\mathbb{E}_{\\left(s,a,\\tilde{s}^{\\prime}\\right)}\\left[\\left\\|\\tilde{s}^{\\prime}+\\beta\\psi\\left(s,a\\right)^{\\top}\\zeta\\left(\\tilde{s}^{\\prime},\\beta\\right)-b\\right\\|^{2}\\right]+\\mathsf{c o n s t a n t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we obtain the claim. ", "page_idx": 16}, {"type": "text", "text": "B Generalization for Partially Observable RL ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we discuss how to generalize Diff-SR to Partially Observable MDP (POMDP). We follow the definition of a POMDP given in [Efroni et al., 2022], which is formally denoted as a tuple $\\mathcal{P}\\,=\\,(\\mathcal{S},\\mathcal{A},\\mathcal{O},r,H,\\rho_{0},\\mathbb{P},\\mathbb{O})$ , where $\\boldsymbol{S}$ is the state space, $\\boldsymbol{\\mathcal{A}}$ is the action space, and $\\scriptscriptstyle\\mathcal{O}$ is the observation space. The positive integer $H$ denotes the horizon length, $\\rho_{0}$ is the initial state distribution, $r:\\mathcal{O}\\times\\mathcal{A}\\to[0,1]$ is the reward function, $\\mathbb{P}(\\cdot|s,a):S\\times A\\rightarrow\\Delta(S)$ is the transition kernel capturing dynamics over latent states, and $\\mathbb{O}(\\cdot|s):S\\to\\Delta({\\mathcal{O}})$ is the emission kernel, which induces an observation from a given state. ", "page_idx": 16}, {"type": "text", "text": "The agent starts at a state $s_{0}$ drawn from $\\rho_{0}(s)$ . At each step $h$ , the agent selects an action $a$ from $\\boldsymbol{\\mathcal{A}}$ This leads to the generation of a new state $s_{h+1}$ following the distribution $\\mathbb{P}\\!\\left(\\cdot|s_{h},a_{h}\\right)$ , from which the agent observes $o_{h+1}$ according to $\\mathbb{O}(\\cdot|s_{h+1})$ . The agent also receives a reward $r(o_{h+1},a_{h+1})$ Observing $o$ instead of the true state $s$ leads to a non-Markovian transition between observations, which means we need to consider policies $\\pi_{h}:{\\mathcal{O}}\\times(A\\times{\\mathcal{O}})^{h}\\to\\Delta(A)$ that depend on the entire history, denoted by $\\tau_{h}=\\{o_{0},a_{0},\\cdot\\cdot\\cdot,o_{h}\\}$ . Let $[H]:=\\{0,\\dots,H\\}$ . ", "page_idx": 16}, {"type": "text", "text": "The value functions are defined by ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{h}^{\\pi}(b_{h})=\\mathbb{E}\\left[\\sum_{t=h}^{H}r(o_{t},a_{t})|b_{h}\\right],\\,Q_{h}^{\\pi}(b_{h},a_{h})=r(o_{h},a_{h})+\\mathbb{E}_{\\mathbb{P}_{b}}\\left[V_{h+1}^{\\pi}(b_{h+1})\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Definition 1 ( $L$ -decodability [Efroni et al., 2022]). $\\forall h\\in[H]$ , define ", "page_idx": 16}, {"type": "text", "text": "A POMDP is $L$ -decodable if there exists a decoder $p^{*}:\\mathcal{X}\\to\\Delta(S)$ such that $p^{*}(x_{h})=b(\\tau_{h})$ . ", "page_idx": 16}, {"type": "text", "text": "That is, under $L$ -decodability assumption, it is sufficient to recover the belief state by an $L$ -step memory $x_{h}$ rather than the entire history $\\tau_{h}$ . This implies that we can parameterize the Q-value as a function of the observation history $Q_{h}^{\\pi}(x_{h},a_{h})$ , rather that the unknown belief state $Q_{h}^{\\pi}(b_{h}(x_{h}),a_{h})$ . By exploiting this $L$ -decodability, Zhang et al. [2023a] propose a probable efficient linear function approximation of $Q_{h}^{\\pi}(x_{h},a_{h})$ by considering a $L$ -step prediction $\\bar{\\mathbb{P}}^{\\pi}(x_{h+L}|x_{h},a_{h})$ . ", "page_idx": 17}, {"type": "text", "text": "Inspired by this, we apply EBM for $\\mathbb{P}^{\\pi}(x_{h+L}|x_{h},a_{h})$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{\\pi}(x_{h+L}|x_{h},a_{h})=\\exp\\left(\\psi(x_{h},a_{h})^{\\top}\\nu\\left(x_{h+L}\\right)-\\log Z\\left(x_{h},a_{h}\\right)\\right),}\\\\ &{\\qquad\\quad Z\\left(x_{h},a_{h}\\right)=\\displaystyle\\int\\exp\\left(\\psi(x_{h},a_{h})^{\\top}\\nu\\left(x_{h+L}\\right)\\right)\\mathrm{d}x_{h+L}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We then apply the techniques presented in Section 3.2 to learn diffusion representation $\\phi(x_{h},a_{h})\\in$ $\\mathbb{R}^{d}$ as an approximation to the random features ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi_{\\omega}\\left(x_{h},a_{h}\\right)=\\exp\\left(-\\mathbf{i}\\omega^{\\top}\\psi\\left(x_{h},a_{h}\\right)\\right)\\exp\\left(\\Vert\\psi\\left(x_{h},a_{h}\\right)\\Vert^{2}/2-\\log Z\\left(x_{h},a_{h}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The learned representation is subsequently utilized to parameterize the value function $Q_{h}^{\\pi}(x_{h},a_{h})$ for policy optimization as demonstrated by [Zhang et al., 2023a]. In particular, we consider value function approximation $Q_{\\xi,\\theta}(x_{h},a_{h})=\\phi\\dot{\\theta}(\\bar{x}_{h},a_{h}\\widetilde{)}^{\\top}\\xi$ and update it by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{critic}}(\\xi)=\\mathbb{E}_{x,a,r,x^{\\prime}\\sim\\mathcal{D}}\\left[\\left(r+\\gamma\\mathbb{E}_{a^{\\prime}\\sim\\pi}[Q_{\\bar{\\xi},\\theta}(x^{\\prime},a^{\\prime})]-Q_{\\xi,\\theta}(x,a)\\right)^{2}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The policy, now conditioned on the $L$ -step history $x$ , is updated by $\\begin{array}{r}{\\operatorname*{max}_{\\pi}\\bar{\\mathbb{E}}_{x\\sim\\mathcal{D},a\\sim\\pi}[\\operatorname*{min}_{i\\in\\{1,2\\}}Q_{\\xi_{i},\\theta}(x,a)]}\\end{array}$ . We refer interested readers to [Zhang et al., 2023a] for more details on representation learning in POMDPs. ", "page_idx": 17}, {"type": "text", "text": "C Details and Analysis for Fully Observable MDP Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 2: Hyperparameters used for Diff-SR in state-based MDP environments. ", "page_idx": 17}, {"type": "table", "img_path": "C3tEX45hJX/tmp/b14f3597070e78aeaf5bf4abce8469ed1761eda702ede94153fea9f669babe9d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.1 Baseline Methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For baseline methods, we include ME-TRPO [Kurutach et al., 2018], PETS [Chua et al., 2018], and the best model-based results among Luo et al. [2021], Deisenroth and Rasmussen [2011], Heess et al. [2015], Clavera et al. [2018], Nagabandi et al. [2018], Tassa et al. [2012] and Levine and Abbeel [2014] from MBBL [Wang et al., 2019]. For model-free algorithms, we include PPO [Schulman et al., 2017], TRPO [Schulman et al., 2015] and SAC [Haarnoja et al., 2018]. ", "page_idx": 17}, {"type": "text", "text": "C.2 Experiment Setups ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We implemented our algorithm based on Soft Actor-Critic (SAC). We use feature update ratio to denote the frequency of updating the diffusion representations as compared to critic updates. We ", "page_idx": 17}, {"type": "table", "img_path": "C3tEX45hJX/tmp/c0bcc77e74744848e03ab11ca80e58bd4cb62ba1c25b1ebf6b330e22791eae83.jpg", "table_caption": ["Table 3: Performance on various continuous control problems with partial observation. We average results across 4 random seeds and a window size of 10K. Diff-SR achieves a similar or better performance compared to the baselines. Here, Best-FO denotes the performance of Diff-SR using full observations as inputs, providing a reference on how well an algorithm can achieve most in our tests. "], "table_footnote": ["sweep the value of this parameter within [1, 3, 5, 10, 20] for all MuJoCo experiments and report the configuration that achieved the best results. Other hyper-parameters are listed in Table 2. For evaluation, we test all methods every 5,000 environment steps by simulating 10 episodes and recording the cumulative return. The reported results are the average return over the last four evaluations and four random seeds. "], "page_idx": 18}, {"type": "text", "text": "D Computational Efficiency and Runtime Comparison ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Full results are presented in Figure 4. We use exactly the same experimental setups as used in Table 1, which has been described in 5.1. We observe that in all cases, Diff-SR is about $4\\times$ faster than PolyGRAD. Such efficiency can be attributed to the fact that while we utilize diffusion to train the representations, we do not have to sample from the diffusion model iteratively, which is the main computational bottleneck for SOTA diffusion-based RL algorithms. ", "page_idx": 18}, {"type": "image", "img_path": "C3tEX45hJX/tmp/e3b4c4b8c850c3ca1cc14a1f190581fb94fc9aae170ebf7b8602c60c8d82154e.jpg", "img_caption": ["Figure 4: Per-task runtime of Diff-SR, LV-Rep and PolyGRAD on tasks from MBBL. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "E Details and Analysis for State-based Partially Observable MDP Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We also experiment Diff-SR with state-based Partially Observable MDP (POMDP) tasks. We contruct a partially observable variant based on OpenAI gym MuJoco [Todorov et al., 2012]. Adhereing to standard methodology, we mask the velocity components within the observations presented to the agent, effectively rendering the tasks partially observable [Ni et al., 2021, Weigand et al., 2021, Gangwani et al., 2020]. Under this masking scheme, a single observation is insufficient for decisionmaking. Therefore, the agent must aggregate past observations to infer the missing information and select appropriate actions. The reconstruction of missing information can be done by aggregating a past history of length $L$ , as demonstrated in Definition 1. ", "page_idx": 18}, {"type": "text", "text": "The POMDP experiment follows a similar setup to the fully observable one described in Appendix C. In the partially observable setting, velocity information is masked, and we concatenate the past $L=3$ observations follow [Zhang et al., 2023a]. The universal hyperparameters used for POMDP are listed in Table 4. For each individual environment, we explored an array of parameters and chose the highest-performing configuration. Specifically, the critic and actor learning rates were varied across $\\{0.0{\\bar{0}}15,0.000{\\bar{1}}5\\}$ , the model learning rate was varied across $\\{0.0001,0.{\\dot{0}}003,0.0008\\}$ and the feature update ratio was varied across $\\{1,3,5,10,20\\}$ . ", "page_idx": 19}, {"type": "text", "text": "We evaluate six baselines in our experiments: a diffusion approach, PolyGRAD [Rigter et al., 2023], two model-based approaches, Dreamer [Hafner et al., 2019a, 2021] and Stochastic Latent Actor-Critic (SLAC) [Lee et al., 2020], a model-free baseline, SAC-MLP, that concatenates history sequences (past four observations) as input to an MLP layer for both the critic and policy, and the neural PSR [Guo et al., 2018]. We also compared to a representation-based baseline, $\\mu\\mathrm{LV}.$ -Rep [Zhang et al., 2023a]. All methods are evaluated using the same procedure as in the fully observable setting. As a reference, we also provide the best performance achieved in the fully observable setting (without velocity masking), denoted as Best-FO, which serves as a benchmark for the optimal result an algorithm can achieve in our tests. ", "page_idx": 19}, {"type": "text", "text": "E.2 Results and Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 3 presents all experiment results, showing effectiveness of Diff-SR in partially observable continuous control tasks. The proposed method delivers superior results in 4 out of 6 tasks (HalfCheetah, Walker, Ant, Hopper). It significantly outperforms other algorithm in Walker and Ant, and achieved a comparable result with the lowest standard deviation on Pendulum, indicating consistent performance. ", "page_idx": 19}, {"type": "text", "text": "The wall time comparison between Diff-SR and PolyGRAD is shown in Figure 5. In the Humanoid task, Diff-SR is approximately 3 times faster than PolyGRAD, whereas in the other tasks, it is about 4 times faster. ", "page_idx": 19}, {"type": "image", "img_path": "C3tEX45hJX/tmp/72e2b34475efd85600a3dcbc12bf10617b80a8de6de2e778142df4097c37589b.jpg", "img_caption": ["Figure 5: Per-task running time of Diff-SR and PolyGRAD on tasks from MBBL with partial observation. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E.3 Ablations And Modifications ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Masking Observations Observations consist of both velocities and positions. Masking velocities, rather than positions, more accurately reflects real-world scenarios where positions are typically directly observed, whereas velocities must be inferred. In the humanoid environment, we specifically mask only the q-velocity component. ", "page_idx": 19}, {"type": "table", "img_path": "C3tEX45hJX/tmp/34e20995014194e562250113dccc9add193c8f2877ea131aeb402f6fde7391ef.jpg", "table_caption": ["Table 4: Hyperparameters used for Diff-SR in state-based POMDP experiments. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "F Image-based Partially Observable MDP Experiment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "F.1 Implementation Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Instead of directly diffusing over the raw pixel space, we used the VAE structure from Zhang et al. [2023a] to first encode the pixel observation $o$ into a 1-D latent embedding $e$ . To deal with the partial observability, we also follow Zhang et al. [2023a] to concatenate the embeddings of the past three observations (denoted as $o^{3}$ ) together as the state of the agent, denoted as $e^{3}$ . Let the next frame be $o^{\\prime}$ and its embedding be $e^{\\prime}$ , the representation learning objective thus translates to fitting the score function $\\psi(e^{3},a)$ and $\\zeta(\\tilde{e}^{\\prime},\\beta)$ , i.e. performing the diffusion in the latent space. In the following paragraphs, we will detail the architectures for Diff-SR. ", "page_idx": 20}, {"type": "text", "text": "Diffusion Representation Learning. Unlike previous state-based experiments, we formalize the network $\\psi$ and $\\zeta$ using the LN_ResNet architecture proposed by IDQL [Hansen-Estruch et al., 2023]. Compared to standard MLP networks, LN_ResNet is equipped with layer normalization and skip connections, making it expressive enough for diffusion modeling. For representation learning, it is worthwhile to note that, apart from the loss objectives defined in Eq (18), we also preserve the gradients of the representation networks and train them with the critic\u2019s loss defined in (21). This design choice aligns with previous works [Zhang et al., 2023a] and encourages the representations to contain task-relevant information. ", "page_idx": 20}, {"type": "text", "text": "RL optimization. We develop our code based on DrQ-V2 [Yarats et al., 2021], a model-free RL algorithm designed for tasks with visual inputs. We preserved most of the design choices from $\\mathrm{DrQ}{-}\\mathrm{V}2$ , except for the architectures of the actor and the critic. For the actor network, it receives the concatenated embeddings e3and outputs an action. The critic networks receive the diffusion representation $\\phi_{\\theta}$ as defined in the main text and predict the Q-values. ", "page_idx": 20}, {"type": "text", "text": "The hyper-parameters for the score functions $\\psi,\\zeta$ , the actor and the critic networks are listed in Table 5. ", "page_idx": 20}, {"type": "text", "text": "F.2 Generation Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In addition to the learning curves, we also present the qualitative generation results using the learned score functions. Figure 6 illustrates the progression of the diffusion process over time. In the figure, we sample a batch of data $(o^{3},a,o^{\\prime})$ from the replay buffer, and the first row depicts the ground-truth target image $o^{\\prime}$ . Starting from the second row, we sample a random Gaussian noise $\\bar{e}_{1000}^{\\prime}\\sim\\mathcal{N}(0,I)$ and iteratively apply the learned reverse diffusion process using the guidance of $(e^{3},a)$ to generate the latent embeddings at various stages of the diffusion. For every 200 steps, we pass the latent embedding to the decoder to obtain reconstructed images $\\tilde{o}_{t}^{\\prime}$ and visualize them in the figure. ", "page_idx": 20}, {"type": "text", "text": "Table 5: Hyperparameters used for Diff-SR in image-based POMDP experiments. ", "page_idx": 21}, {"type": "text", "text": "Hyper-parameters Value   \nActor Learning Rate 0.0001   \nCritic Learning Rate 0.0001   \nLearning Rate for \u03c8, \u03b6, \u03b8 0.0003   \nActor Hidden Layer Dimensions (1024, 1024)   \nDiff-SR Representation Dimension 1024   \nDiscount Factor $\\gamma$ 0.99   \nCritic Soft Update Factor $\\tau$ 0.01   \nBatch Size 1024   \nNumber of Noise Levels 1000   \nLN_ResNet Layer Width for \u03c8 512   \nLN_ResNet Layer Width for \u03b6 512   \n# of LN_ResNet Layers for \u03c8 4   \n# of LN_ResNet Layers for \u03b6 2 ", "page_idx": 21}, {"type": "image", "img_path": "C3tEX45hJX/tmp/0a84eb2fd9cab82ad6e4fabf5ecb9fb9e0b244e0cb4699c2517d78ba13aa87e1.jpg", "img_caption": ["Figure 6: The generation results of Diff-SR. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "The second row corresponds to the initial noisy embeddings and exhibits significant distortion and noise. As the denoising steps progress, we observe a gradual reduction in noise and an increasing clarity in the generated images. By the time we reach $\\tilde{o}_{600}^{\\prime}$ , the overall structure of the scene becomes more discernible, although some artifacts remain. Further along the denoising process, the images exhibit substantial improvements in terms of detail and realism. The final output, $\\tilde{o}_{0}^{\\prime}$ , closely resembles the original observations, indicating the effectiveness of our score functions in capturing the underlying information about the data. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The main claims are that (1) by exploiting the energy-based model view of the diffusion model, we develop an algorithm to learn a latent variable representation of the transition function (Section 3.2) and (2) we demonstrate that said representation is sufficiently expressive to linearly represent the value function, paving the way for efficient planning and exploration, circumventing the need for sample generation from the diffusion model, and thus, avoiding the inference costs associated with prior diffusion-based methods (Section 3.3). ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the assumptions in the derivations and experiments. We also address limitations in the conclusion. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide full sets of assumptions in the main text body and if an additional proof is required, we supplement in the Appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide a representation training algorithm implementation description in Algorithm 2 and full online pipeline implementation in Algorithm 1. We provide experimental setup and procedures in 5. We also provide additional implementation details and hyperparameter set in C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 23}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have included all experimental details; code is released on our website. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including h ow to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide full algorithm description in Algorithm 1 and 2, and experiment details in Appendix C. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: For all experimental results, we run over multiple random seeds and provide error bars for each result. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We run our experiments on a Quadro RTX 6000 and provide run time results. Our experiments can be done on standard graphics cards and do not require excessive computational capabilities. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We acknowledge and affirm the importance of the NeurIPS Code of Ethics. We have carefully considered the ethical implications of our work throughout the research and development process. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We develop reinforcement learning algorithms with broad applicability. These algorithms can be used for diverse applications, some with positive outcomes and others with potential risks. Our contribution lies in the general method, not its specific use case. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: This project does not use data or models that have a high risk for misuse. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have credited creators and original owners of assets (e.g., code, data, models) used in the paper. The license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The assets are not released at this time. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve IRB approval, crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]