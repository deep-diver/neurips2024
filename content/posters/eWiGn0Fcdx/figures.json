[{"figure_path": "eWiGn0Fcdx/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of the cross-scan in ViM models before and after token pruning.", "description": "This figure illustrates the difference between standard ViT token pruning and its application to ViM (Vision-based State Space Models).  The left side shows ViT token pruning where patches are simply removed. The middle shows the ViM scan process, illustrating how tokens are processed sequentially across various directions. The right displays the result of applying ViT-style token pruning to ViM.  The condensed token matrix and the actual ViM scan after pruning show how the naive application of ViT pruning disrupts the sequential pattern of the ViM scan, leading to performance degradation. This disruption is a key point the paper makes about why existing ViT pruning techniques fail when applied directly to ViMs.", "section": "Failure of Applying ViT Token Pruning for ViMs"}, {"figure_path": "eWiGn0Fcdx/figures/figures_3_2.jpg", "caption": "Figure 1: Illustration of the cross-scan in ViM models before and after token pruning.", "description": "This figure shows the cross-scan mechanism in Vision State Space Models (ViMs) before and after applying token pruning.  The left side illustrates a standard ViM-S model, showing the input tokens arranged in a grid, processed in the pattern of a 'ViM scan'. The middle panel shows the result of applying a naive token pruning strategy (as is commonly used in Vision Transformers), randomly removing tokens from the input.  The right side shows how the ViM scan is disrupted after naive token pruning, resulting in an uneven distribution of remaining tokens. This disruption of the sequential order is a key reason why traditional token pruning methods designed for ViTs are ineffective on ViMs.", "section": "3.2 Failure of Applying ViT Token Pruning for ViMs"}, {"figure_path": "eWiGn0Fcdx/figures/figures_9_1.jpg", "caption": "Figure 3: Visual representation on ImageNet-1K. We present the original images, attention visualizations from ViM-S, and zero-shot results of w/o and w/ our alignment method after the final layer.", "description": "This figure visualizes the attention maps of the ViM-S model on ImageNet-1K.  It compares the attention maps of the original model, a model with token pruning without the proposed hidden state alignment, and a model with token pruning using the proposed alignment. Each row represents a different example image, showing how the attention is distributed across different parts of the image. The results demonstrate the effect of the proposed alignment in maintaining similar attention patterns to the original model despite token pruning, unlike the model without alignment.", "section": "5.5 Visualization and Interpretability"}, {"figure_path": "eWiGn0Fcdx/figures/figures_9_2.jpg", "caption": "Figure 1: Illustration of the cross-scan in ViM models before and after token pruning.", "description": "This figure illustrates the concept of cross-scan in Vision State Space Models (ViMs) and how it is affected by token pruning.  The top row shows the original ViM scan process, where image patches are processed sequentially along traversal paths. The bottom row shows the effect of token pruning.  Some tokens (patches) are removed, resulting in a 'condensed token matrix'. The key point is that the naive application of token pruning disrupts the original sequential pattern of the scan, which is a crucial difference from the independent patch processing in Vision Transformers (ViTs).", "section": "3.2 Failure of Applying ViT Token Pruning for ViMs"}]