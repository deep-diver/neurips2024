[{"figure_path": "4xDxVQHsbZ/figures/figures_3_1.jpg", "caption": "Figure 1: A comparison of memory layouts of the key cache of LLM attention (left) and the key-code cache of NoMAD-Attention (middle), and an illustration of how attention scores are computed through in-register lookups in NoMAD-Attention (right).", "description": "This figure compares the memory layout of the key cache in standard LLM attention with the key-code cache in NOMAD attention.  It highlights how NOMAD attention uses transposed and blocked key codes, enabling efficient lookups using SIMD registers. The right side shows how the SIMD registers, LUTs (lookup tables), and memory hierarchy interact to calculate attention scores with ultra-low latency.", "section": "2 LLM Inference on CPUs"}, {"figure_path": "4xDxVQHsbZ/figures/figures_7_1.jpg", "caption": "Figure 2: Latency and throughput of decoding for CodeLlama-7b (4-bit and 16-bit weights) with Attention and NOMAD-Attention. NOMAD-Attention achieves 1.78\u20132.07\u00d7 higher throughput than Attention with 4-bit CodeLlama-7b, and 1.46-1.56\u00d7 higher throughput with 16-bit CodeLlama-7b.", "description": "This figure shows the latency and throughput for decoding 16,384 tokens using both Attention and NOMAD-Attention methods on the CodeLlama-7b model, with both 4-bit and 16-bit quantized weights.  The results clearly demonstrate that NOMAD-Attention significantly improves the throughput over the traditional Attention mechanism. The improvement is more pronounced with 4-bit quantized weights.", "section": "4 Results"}, {"figure_path": "4xDxVQHsbZ/figures/figures_7_2.jpg", "caption": "Figure 3: Time for processing prompts of different lengths for CodeLlama-7b (4-bit and 16-bit weights) with Attention and NoMAD-Attention. NoMAD-Attention achieves 1.63\u20131.79\u00d7 speedup over Attention to process a prompt of 16k tokens.", "description": "This figure compares the time taken to process prompts of varying lengths (4K, 8K, and 16K tokens) for CodeLlama-7b using both standard Attention and the proposed NoMAD-Attention method.  The results show that NoMAD-Attention consistently achieves a significant speedup, particularly noticeable at longer prompt lengths (1.63-1.79x speedup for 16K tokens).  Both 4-bit and 16-bit quantized weight versions of the model are shown.", "section": "4.1 Results"}, {"figure_path": "4xDxVQHsbZ/figures/figures_8_1.jpg", "caption": "Figure 4: The breakdown of decoding latency of 4-bit CodeLlama-7b for Attention and NOMAD-Attention. NOMAD-Attention effectively reduces the latency of computing query-key dot-products by 5.24-14.77\u00d7 over Attention.", "description": "This figure breaks down the latency of decoding for a 4-bit CodeLlama-7b model, comparing the standard Attention mechanism with the proposed NOMAD-Attention method for different levels of sub-quantization (dsub). It visually represents the time spent on various components of the decoding process: \n- Others (other operations)\n- Key Caching\n- Query x Key (Q x K) dot products\n- Attention Scores x Value (Attention Score x V) multiplication\n- Attention Linears\n- MLP Linears\nThe figure demonstrates that NOMAD-Attention significantly reduces the latency of computing the query-key dot products compared to the traditional Attention mechanism. This reduction becomes more significant with higher levels of sub-quantization.", "section": "4.2 Ablation Study"}, {"figure_path": "4xDxVQHsbZ/figures/figures_15_1.jpg", "caption": "Figure 5: Illustration demonstrating the mapping of an input key k<sup>t</sup> to its s-th sub-quantizer using \u03c0<sub>s</sub>(k<sup>t</sup>), where s \u2208 1 . . . S. Subsequently, each sub-quantizer maps to its closest centroid c<sup>t</sup><sub>s</sub>, where i \u2208 1 . . . S, and the results are stored in the key cache K<sup>t</sup><sub>cache</sub>.", "description": "This figure illustrates how a key vector (k<sup>t</sup>) is processed in the NOMAD-Attention method.  First, the key vector is split into sub-vectors using the function \u03c0<sub>s</sub>, one for each sub-quantizer (s = 1...S). Each sub-vector is then mapped to its nearest centroid (b<sub>s</sub>) within the corresponding codebook. The index of the closest centroid (c<sup>t</sup><sub>s</sub>) is then stored in the key cache (K<sup>t</sup><sub>cache</sub>), which is used for fast lookups later in the process. This product quantization technique converts the computationally expensive dot-product calculation into a simple lookup operation.", "section": "3.1 Transforming Dot-products into Lookups"}, {"figure_path": "4xDxVQHsbZ/figures/figures_15_2.jpg", "caption": "Figure 6: Illustration depicting the mapping of a query vector q to its s-th sub-quantizer using \u03c0\u03c2(q), where s \u2208 1 . . . S. Subsequently, the distance between \u03c0\u03c2(q) and 16 centroids is computed. This distance is quantized to a value within the range of 0-255, and the resulting quantized distance is further converted into 8-bit codes, which are stored in LUTs.", "description": "This figure illustrates the process of Product Quantization (PQ) for query vectors.  The query vector is split into sub-vectors, and each sub-vector's distance to 16 centroids (cluster centers) in a codebook is calculated. These distances are quantized to 8-bit codes and stored in a lookup table (LUT) for efficient retrieval during the attention computation.  The LUT contains pre-computed dot products, allowing for a faster lookup than traditional multiply-add operations. This method is crucial for NoMAD-Attention's efficiency.", "section": "3.1 Transforming Dot-products into Lookups"}, {"figure_path": "4xDxVQHsbZ/figures/figures_16_1.jpg", "caption": "Figure 4: The breakdown of decoding latency of 4-bit CodeLlama-7b for Attention and NOMAD-Attention. NoMAD-Attention effectively reduces the latency of computing query-key dot-products by 5.24-14.77\u00d7 over Attention.", "description": "This figure presents a breakdown of the decoding latency for a 4-bit CodeLlama-7b model using both the standard Attention mechanism and the proposed NoMAD-Attention. The breakdown categorizes latency into different components: Others, Key Caching, Query-Key (QxK) dot products, Attention Score x Value (Attention Score x V), Attention Linears, and MLP Linears.  The key observation is that NoMAD-Attention significantly reduces the latency associated with computing query-key dot products, leading to a substantial overall reduction in decoding latency.  The reduction factor varies depending on the value of `dsub` in the NoMAD-Attention algorithm.", "section": "4.2 Ablation Study"}]