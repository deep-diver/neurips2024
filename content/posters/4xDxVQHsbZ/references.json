{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is fundamental to the design of modern LLMs and the focus of optimization in this paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduced the LLaMA model, which is used as a baseline and primary subject for performance evaluation in this paper."}, {"fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with IO-awareness", "publication_date": "2022-12-01", "reason": "This paper introduced FlashAttention, a fast attention mechanism that addresses memory limitations which is related to the optimization in this paper."}, {"fullname_first_author": "Herve Jegou", "paper_title": "Product quantization for nearest neighbor search", "publication_date": "2010-01-01", "reason": "This paper introduced product quantization, a technique used in NOMAD-Attention to approximate dot products efficiently, which is a core component of the proposed method."}, {"fullname_first_author": "Tianyi Zhang", "paper_title": "Kv cache is 1 bit per channel: Efficient large language model inference with coupled quantization", "publication_date": "2024-05-03", "reason": "This is a related work by the authors themselves, showing their prior work on efficient large language model inference, which is directly relevant to the main contribution."}]}