[{"figure_path": "Nb5xlelV0C/figures/figures_1_1.jpg", "caption": "Figure 1: Our approach enables text-to-image diffusion models to generate nuanced spatial and conceptual interpolations between different conditions including text (a, c-e) and image (b), with seamless transitions in layout, conceptual blending, and user-specified prompts to guide the interpolation paths (f).", "description": "This figure demonstrates the capability of the proposed Attention Interpolation via Diffusion (AID) method for generating smooth and conceptually consistent interpolations between different image conditions.  Subfigures (a), (c), (d), and (e) showcase text-to-text interpolation, while (b) shows image-to-image interpolation.  Subfigure (f) highlights the method's ability to incorporate user-specified prompts to guide the interpolation process, allowing for more nuanced control over the generated images.", "section": "1 Introduction"}, {"figure_path": "Nb5xlelV0C/figures/figures_3_1.jpg", "caption": "Figure 2: Results comparison between AID (the 1st row) and text embedding interpolation (the 2nd row). AID increases smoothness, consistency, and fidelity significantly.", "description": "This figure shows a comparison of image interpolation results between two methods: AID (Attention Interpolation via Diffusion) and text embedding interpolation.  The top row displays the results obtained using AID, demonstrating smoother transitions and higher fidelity images. The bottom row shows the results from text embedding interpolation, which suffers from inconsistent images and lower fidelity. The figure highlights the significant improvement in image quality and consistency achieved by AID.", "section": "3.2 Text Embedding Interpolation"}, {"figure_path": "Nb5xlelV0C/figures/figures_5_1.jpg", "caption": "Figure 3: An overview of PAID: Prompt-guided Attention Interpolation of Diffusion. The main components include: (1) Replacing both cross-attention and self-attention when generating interpolated image by fused interpolated attention; (2) Selecting interpolation coefficients with Beta prior; (3) Inject prompt guidance in the fused interpolated cross-attention.", "description": "This figure illustrates the PAID (Prompt-guided Attention Interpolation of Diffusion) framework.  It shows how the model uses fused interpolated attention (combining cross and self-attention) to generate interpolated images. The interpolation coefficients are selected using a Beta distribution for smoothness. Finally, prompt guidance is integrated into the cross-attention to further direct the interpolation process.", "section": "4 AID: Attention Interpolation of Text-to-Image Diffusion"}, {"figure_path": "Nb5xlelV0C/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative comparison of different ablation setting of AID. (a) Qualitative comparison between AID without fusion (1st row), AID with fusion (2nd row), and AID with fusion and beta prior (3rd row). Fusing interpolation with self-attention alleviates the artifacts of the interpolated image significantly, while beta prior increases smoothness based on AID with fusion. (b) CLIP score of different methods on composition generation.", "description": "This figure shows the ablation study results of the proposed AID method for conditional interpolation.  Subfigure (a) presents a qualitative comparison of three variations of AID: one without self-attention fusion, one with self-attention fusion, and one with both self-attention fusion and Beta prior for interpolation coefficient selection.  The results demonstrate improved image quality and smoothness with the addition of these components. Subfigure (b) provides a quantitative comparison of CLIP scores for different methods (Stable Diffusion, CEBM, CEBM-MCMC, and PAID) on a compositional generation task.  The results indicate that PAID outperforms other methods, suggesting its effectiveness in generating high-quality images.", "section": "5 Experiments"}, {"figure_path": "Nb5xlelV0C/figures/figures_9_1.jpg", "caption": "Figure 5: Results of image editing control. Our method boosts the controlling ability over editing. The first row of (a) and (b) is generated by P2P + AID while the second row is P2P + TEI.", "description": "This figure shows the results of image editing control experiments comparing the proposed method (P2P + AID) with a baseline method (P2P + TEI). The top row displays images generated using P2P + AID, demonstrating improved control over the editing process compared to the bottom row which shows images generated with P2P + TEI.  The results suggest that the proposed AID method enhances the ability to precisely control the editing level in image editing tasks.", "section": "5.2 Application"}, {"figure_path": "Nb5xlelV0C/figures/figures_9_2.jpg", "caption": "Figure 6: Results of compositional generation. Images on the left are generated with \"a deer\" and \"a plane\" based on SD 1.4 [35] and images on the right are generated with \"a robot\" and \"a sea of flowers\" based on SDXL [30]. Compared to other methods, PAID-O properly captures both conditions with higher fidelity.", "description": "This figure compares the results of compositional generation using different methods: Vanilla Stable Diffusion, CEBM, RRR, and PAID.  The results show that PAID is superior in generating images that accurately reflect both input conditions (\"a deer\" and \"a plane\", \"a robot\" and \"a sea of flowers\") with significantly higher fidelity than other methods.", "section": "Compositional Text-to-Image Generation"}, {"figure_path": "Nb5xlelV0C/figures/figures_9_3.jpg", "caption": "Figure 7: Results of AID with image conditions. Our method is compatible with IP-Adapter for image-conditioned generation (a). In both global image prompt (b) and composition image prompt (c), from left to right the scale of additional image prompt slowly increases. The first row illustrates results controlled by AID, while the second row shows results achieved using the scale setting provided by IP-Adapter.", "description": "This figure shows the results of applying the AID method to image-conditioned generation using IP-Adapter.  The first row in each part (a, b, and c) shows results using AID, while the second row displays results when using IP-Adapter's scaling setting. Part (a) demonstrates image morphing between real images.  Part (b) displays results for a global image prompt (\"A statue is running\"). Part (c) shows results using a composition image prompt (\"A boy is smiling\"). In all cases, the gradual increase in the additional image prompt's scale is visible from left to right.", "section": "5.2 Application"}, {"figure_path": "Nb5xlelV0C/figures/figures_14_1.jpg", "caption": "Figure 8: Difference between smoothness and consistency in measurement of discrete sequence.", "description": "This figure illustrates the difference between smoothness and consistency when evaluating discrete sequences versus continuous paths in image interpolation.  It highlights that a perceptually smooth continuous path may not always translate to a smooth sequence of discrete samples, and that consistency in image progression is a separate factor.", "section": "3.3 Measuring the Quality of Conditional Interpolation"}, {"figure_path": "Nb5xlelV0C/figures/figures_15_1.jpg", "caption": "Figure 9: Diagnosis of text embedding interpolation on spatial layout (a - e) and adjacent distance (f). (a) Image generated by \u201ca cat wearing sunglasses\u201d; (b) Image generated by \u201ca dog wearing sunglasses\u201d; (c) Replacing the cross-attention during generation of (b) by (a); (d) Replacing the self-attention during generation of (b) by (a); (e) Box plot of Dst(I, I'cross) and Dst(I, I'self). When fixing a query, the key and value in self-attention mostly determine the output of pixel space compared to cross-attention. (f) The maximum adjacent distance and the average of other adjacent pairs.", "description": "This figure presents an experimental analysis of text embedding interpolation.  It compares the impact of replacing either cross-attention or self-attention mechanisms with those from a different image. The results show self-attention has a stronger influence on the spatial layout of the generated image than cross-attention.  Additionally, it demonstrates the non-uniformity of visual transitions in text embedding interpolation, highlighting the need for a more sophisticated interpolation approach.", "section": "3.4 Diagnosing Text Embedding Interpolation"}, {"figure_path": "Nb5xlelV0C/figures/figures_16_1.jpg", "caption": "Figure 1: Our approach enables text-to-image diffusion models to generate nuanced spatial and conceptual interpolations between different conditions including text (a, c-e) and image (b), with seamless transitions in layout, conceptual blending, and user-specified prompts to guide the interpolation paths (f).", "description": "This figure shows examples of attention interpolation applied to different tasks.  The top row demonstrates interpolation between two text prompts, resulting in a smooth transition between the concepts. The second row shows image-to-image interpolation, smoothly changing one image into another.  The third and fourth rows show additional text-to-text interpolations. The bottom row shows how user-specified prompts can guide the interpolation process, creating even more nuanced transitions.", "section": "1 Introduction"}, {"figure_path": "Nb5xlelV0C/figures/figures_16_2.jpg", "caption": "Figure 1: Our approach enables text-to-image diffusion models to generate nuanced spatial and conceptual interpolations between different conditions including text (a, c-e) and image (b), with seamless transitions in layout, conceptual blending, and user-specified prompts to guide the interpolation paths (f).", "description": "This figure demonstrates the AID (Attention Interpolation via Diffusion) method's ability to generate smooth and coherent image interpolations between different conditions.  Subfigure (a) shows text-to-text interpolation, (b) image-to-image, (c-e) other text-to-text examples, and (f) text-guided interpolation.  The results highlight improved spatial and conceptual consistency compared to baseline methods.", "section": "1 Introduction"}, {"figure_path": "Nb5xlelV0C/figures/figures_16_3.jpg", "caption": "Figure 1: Our approach enables text-to-image diffusion models to generate nuanced spatial and conceptual interpolations between different conditions including text (a, c-e) and image (b), with seamless transitions in layout, conceptual blending, and user-specified prompts to guide the interpolation paths (f).", "description": "This figure shows examples of attention interpolation applied to text-to-image diffusion models.  It showcases the ability to generate smooth transitions between various conditions, including interpolations between different text prompts (a, c-e), images (b), and even with user-specified prompts guiding the interpolation path (f). The figure highlights the model's capacity for nuanced spatial and conceptual blending during the interpolation process.", "section": "1 Introduction"}, {"figure_path": "Nb5xlelV0C/figures/figures_19_1.jpg", "caption": "Figure 1: Our approach enables text-to-image diffusion models to generate nuanced spatial and conceptual interpolations between different conditions including text (a, c-e) and image (b), with seamless transitions in layout, conceptual blending, and user-specified prompts to guide the interpolation paths (f).", "description": "This figure demonstrates the capability of the proposed Attention Interpolation via Diffusion (AID) method in generating smooth and coherent image interpolations across various conditions, including both text and image prompts.  Subfigures (a, c-e) showcase text-to-text interpolations, while (b) shows image-to-image interpolation. Subfigure (f) highlights the use of prompt guidance to further control the interpolation pathway.", "section": "1 Introduction"}, {"figure_path": "Nb5xlelV0C/figures/figures_19_2.jpg", "caption": "Figure 1: Our approach enables text-to-image diffusion models to generate nuanced spatial and conceptual interpolations between different conditions including text (a, c-e) and image (b), with seamless transitions in layout, conceptual blending, and user-specified prompts to guide the interpolation paths (f).", "description": "This figure shows six examples of attention interpolation. The first five examples show different types of interpolation between different text prompts and one image-to-image interpolation.  The last example demonstrates the use of prompt guidance for more fine-grained control over the interpolation path.", "section": "1 Introduction"}, {"figure_path": "Nb5xlelV0C/figures/figures_20_1.jpg", "caption": "Figure 1: Our approach enables text-to-image diffusion models to generate nuanced spatial and conceptual interpolations between different conditions including text (a, c-e) and image (b), with seamless transitions in layout, conceptual blending, and user-specified prompts to guide the interpolation paths (f).", "description": "This figure showcases the capability of the proposed Attention Interpolation via Diffusion (AID) method in generating smooth and meaningful interpolations between different image conditions.  It presents examples of text-to-text, image-to-image, and text-guided interpolations, demonstrating the ability to create nuanced transitions in both visual style and semantic content. The seamless transitions highlight the method's effectiveness in generating high-quality interpolated images.", "section": "1 Introduction"}, {"figure_path": "Nb5xlelV0C/figures/figures_21_1.jpg", "caption": "Figure 1: Our approach enables text-to-image diffusion models to generate nuanced spatial and conceptual interpolations between different conditions including text (a, c-e) and image (b), with seamless transitions in layout, conceptual blending, and user-specified prompts to guide the interpolation paths (f).", "description": "This figure demonstrates the results of attention interpolation on various image generation tasks.  It shows examples of interpolating between different text prompts (a, c-e), image prompts (b), and guided interpolation paths using additional text prompts (f). The results showcase the method's ability to generate smooth transitions, maintain conceptual coherence, and allow for precise control over the interpolation process.", "section": "1 Introduction"}, {"figure_path": "Nb5xlelV0C/figures/figures_22_1.jpg", "caption": "Figure 5: Results of image editing control. Our method boosts the controlling ability over editing. The first row of (a) and (b) is generated by P2P + AID while the second row is P2P + TEI.", "description": "This figure demonstrates the results of image editing control using two different methods: P2P+AID and P2P+TEI. The top row shows the results obtained with the P2P+AID method, while the bottom row displays the results from the P2P+TEI method.  The results illustrate how AID improves the ability to control the level of editing applied to an image. ", "section": "5.2 Application"}, {"figure_path": "Nb5xlelV0C/figures/figures_24_1.jpg", "caption": "Figure 17: Qualitative results of interpolation between animal concepts. For an animal, we use \"A photo of {animal_name}, high quality, extremely detailed\" to generate the corresponding source images. The guidance prompt is formulated as \u201cA photo of an animal called {animal_name_A}-{animal_name_B}, high quality, extremely detailed\u201d. PAID enables a strong ability to create compositional objects.", "description": "This figure shows the qualitative results of using Prompt-guided Attention Interpolation via Diffusion (PAID) to interpolate between different animal concepts.  The interpolation smoothly transitions between the source images, demonstrating the method's ability to create high-quality, detailed images that combine features from both input concepts.", "section": "5.2 Application"}, {"figure_path": "Nb5xlelV0C/figures/figures_24_2.jpg", "caption": "Figure 1: Our approach enables text-to-image diffusion models to generate nuanced spatial and conceptual interpolations between different conditions including text (a, c-e) and image (b), with seamless transitions in layout, conceptual blending, and user-specified prompts to guide the interpolation paths (f).", "description": "This figure shows examples of attention interpolation of diffusion model applied to different conditions. (a) shows text-to-text interpolation, (b) image-to-image interpolation, (c-e) text-to-text interpolation with different concepts, and (f) text-to-text interpolation with prompt guidance.  The results demonstrate the model's ability to generate smooth and consistent transitions between different conditions, even with complex and abstract concepts. This showcases the improvement in quality compared to other methods.", "section": "1 Introduction"}, {"figure_path": "Nb5xlelV0C/figures/figures_25_1.jpg", "caption": "Figure 1: Our approach enables text-to-image diffusion models to generate nuanced spatial and conceptual interpolations between different conditions including text (a, c-e) and image (b), with seamless transitions in layout, conceptual blending, and user-specified prompts to guide the interpolation paths (f).", "description": "This figure showcases the capabilities of the proposed Attention Interpolation via Diffusion (AID) method for generating smooth transitions between different image conditions.  It demonstrates successful interpolation between various text prompts (e.g., 'a lady in the sea of flowers' to 'Mobile Suit Gundam'), image-to-image morphing (Mona Lisa to Taylor Swift), and text prompts guided with additional prompts ('photo of a dog' to 'photo of a car' with intermediate guidance prompts). The figure highlights the method's ability to maintain consistency and smoothness in image transitions, as well as its potential for advanced image editing and controlled generation.", "section": "1 Introduction"}, {"figure_path": "Nb5xlelV0C/figures/figures_26_1.jpg", "caption": "Figure 20: More qualitative results generated by Animagine 3.0 [23] (the 1st row) and SDXL (from 2nd to 9th rows).", "description": "This figure displays multiple image interpolation sequences generated using two different models: Animagine 3.0 and Stable Diffusion XL (SDXL).  Each sequence shows a smooth transition between two different images, demonstrating the models' ability to generate realistic and visually appealing interpolations. The variety of subject matter, artistic styles, and image qualities showcases the models' versatility and effectiveness.", "section": "H Auxiliary Qualitative Results"}]