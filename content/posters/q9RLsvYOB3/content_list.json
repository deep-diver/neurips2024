[{"type": "text", "text": "FlexPlanner: Flexible 3D Floorplanning via Deep Reinforcement Learning in Hybrid Action Space with Multi-Modality Representation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ruizhe Zhong1, Xingbo $\\mathbf{D}\\mathbf{u}^{1}$ , Shixiong $\\mathbf{Kai^{2}}$ , Zhentao Tang2, Siyuan $\\mathbf{X}\\mathbf{u}^{2}$ , Jianye Hao2,3, Mingxuan Yuan2, Junchi Yan1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Dept. of CSE & School of AI & MoE Key Lab of AI, Shanghai Jiao Tong University 2Noah\u2019s Ark Lab, Huawei 3College of Intelligence and Computing, Tianjin University {zerzerzerz271828, duxingbo, yanjunchi}@sjtu.edu.cn   \n{kaishixiong, tangzhentao1, xusiyuan520, yuan.mingxuan}@huawei.com jianye.hao@tju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the Integrated Circuit (IC) design flow, floorplanning (FP) determines the position and shape of each block. Serving as a prototype for downstream tasks, it is critical and establishes the upper bound of the final PPA (Power, Performance, Area). However, with the emergence of 3D IC with stacked layers, existing methods are not flexible enough to handle the versatile constraints. Besides, they typically face difficulties in aligning the cross-die modules in 3D ICs due to their heuristic representations, which could potentially result in severe data transfer failures. To address these issues, we propose FlexPlanner, a flexible learning-based method in hybrid action space with multi-modality representation to simultaneously handle position, aspect ratio, and alignment of blocks. To our best knowledge, FlexPlanner is the first learning-based approach to discard heuristic-based search in the 3D FP task. Thus, the solution space is not limited by the heuristic floorplanning representation, allowing for significant improvements in both wirelength and alignment scores. Specifically, FlexPlanner models 3D FP based on multi-modalities, including vision, graph, and sequence. To address the non-trivial heuristic-dependent issue, we design a sophisticated policy network with hybrid action space and asynchronous layer decision mechanism that allow for determining the versatile properties of each block. Experiments on public benchmarks MCNC and GSRC show the effectiveness. We significantly improve the alignment score from 0.474 to 0.940 and achieve an average reduction of $16\\%$ in wirelength. Moreover, our method also demonstrates zero-shot transferability on unseen circuits. Code is publicly available at: https://github.com/Thinklab-SJTU/EDA-AI. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the very beginning stage of physical design in Electronic Design Automation (EDA), floorplanning (FP) plays a critical role. As a subsequent stage of hardware design [1] and logic synthesis [2], floorplanning provides a prototype for downstream tasks [3], ranging from power delivery network (PDN) design [4] to placement [5, 6] & routing [7, 8] (P&R), hence determining the upper bound of final PPA (Power, Performance, Area). Recognized as an NP-hard problem [9], FP establishes the chip\u2019s physical layout by optimizing the position and shape of the major blocks to minimize interconnect lengths and ensure efficient silicon area utilization. As floorplanning technologies evolve, 3D FP with stacked layers emerges with more challenges. In particular, the cross-die module alignment becomes another pivotal factor in 3D FP [10, 11, 12]. For instance, vertical buses [11] for cross-die communication connect the aligned blocks spread among multiple dies [11]. Another example is the Memory-on-Logic technology [13, 14, 12], partitioning the processor [15] into two tiers: memory tier and logic tier. The memory tier consists of memory blocks and customized intellectual property cores (IP cores) [16], while the logic tier contains other components, such as logic blocks. Blocks on different dies should be aligned together, enabling the communication established by bonding bumps or pads [14]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Existing works can be categorized into heuristic-based methods, analytical approaches, and learningbased methods. Heuristics-based methods [17, 18, 19, 20, 21] model the FP with a certain heuristic representation. To refine the current FP, they modify the heuristic representation and convert it to the corresponding FP through a decoding scheme. However, this implementation limits the flexibility to directly adjust the position of blocks. After a single modification, the entire FP result needs to be regenerated, incapable of making fine-grained adjustments. Moreover, the alignment constraints cannot be satisfied by simply incorporating alignment metrics into its heuristics. Analytical approaches [9, 22] compute the gradient of objectives w.r.t. block position and utilize gradient descent technique to optimize FP. However, the calculation of alignment is non-differentiable, making them inapplicable in 3D FP. Recently, with the emergence of Reinforcement Learning (RL), learning-based methods are promising in FP [23, 24, 25] or macro placement [26, 27, 28, 29], which is a task similar to FP and determines the position of major blocks. Among these approaches, [23, 24, 25] still retain traditional FP representation, thereby leading to the same issues encountered by heuristic methods. Conversely, [26, 27, 28, 29] focus on determining block positions in 2D scenarios. However, when directly implemented in 3D scenarios, these methods could result in 1) overlooking alignment requirements and 2) multi-die property. Specifically, in 2D FP, blocks are arranged exclusively on a single die and are organized in a queue that represents the placing order of all blocks. However, in 3D scenarios, multiple queues exist due to the multi-die property, necessitating a layer decision mechanism to merge these queues and determine the comprehensive placing order. This is a critical issue, yet it remains insufficiently explored in current research. Additionally, current learning-based methods are 3) incapable of addressing the variable aspect ratio of modules. Characteristics of typical approaches are summarized in Table 1. ", "page_idx": 1}, {"type": "text", "text": "To address the aforementioned challenges, we propose FlexPlanner, a flexible deep-learningbased approach in hybrid action space with multi-modality representation for 3D FP. FlexPlanner directly outputs the final FP result, without the reliance on any heuristic representation. Under the Actor-Critic framework, the policy network consists of three sub-modules, responsible for determining the position, layer, and aspect ratio of blocks, spanning a hybrid action ", "page_idx": 1}, {"type": "text", "text": "space. Empirical results demonstrate the effectiveness and significance of FlexPlanner. The main contributions are highlighted as follows: ", "page_idx": 1}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/511773c18a9146c02a781bd3fcc220bd570e3ae97161de157e4f54216e2097d4.jpg", "table_caption": ["Table 1: Characteristics of typical methods. "], "table_footnote": ["AR: aspect ratio of blocks. Aln: cross-die block alignment. Mod: Modality. $*\\mathrm{H}=$ Heuristics, $\\mathrm{V}=$ Vision, ${\\mathrm{G}}=$ Graph, ${\\boldsymbol{S}}=$ Sequence. "], "page_idx": 1}, {"type": "text", "text": "\u2022 First learning-based method to discard heuristic-based search in the 3D FP task. We propose a novel learning-based method with flexible hybrid action space for 3D FP, simultaneously handling the position, aspect ratio, and cross-die alignment of blocks. Without relying on the heuristic-based search, FlexPlanner allows the position and aspect ratio of each block to be explored across a comprehensive spectrum, rather than be limited by the constraints of heuristic FP representation, thereby breaking through the upper bound of performance. And we propose an innovative strategy for more effectively addressing the alignment issue. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Tackle the non-trivial issue of dependency on heuristics by incorporating hybrid action space and multi-modality representation. It is non-trivial to avoid the dependency on heuristics-based search in 3D FP due to the difficulty of modeling the complex solution space. Heuristics can only represent a subset of the entire solution space, resulting in limitations on the upper bound performance. To address this issue, we initially introduce three modalities, including vision, graph, and sequence, to comprehensively represent the state space. Additionally, we design a sophisticated policy network with hybrid action space and asynchronous layer decision mechanisms, enabling learning versatile properties such as position, aspect ratio, layer for each block in a 3D FP setting. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 Zero-shot transferability. Leveraging the advantage of the learning scheme and multi-modalities, FlexPlanner demonstrates the ability to exhibit zero-shot transferability on previously unseen circuits. This capability shows strengths in its efficiency, as it conserves substantial training resources when confronted with new IC cases. ", "page_idx": 2}, {"type": "text", "text": "\u2022 SOTA experimental results with significant alignment improvement. Within the learning framework, FlexPlanner achieves state-of-the-art performance on wirelength and alignment in 3D FP. Specifically, the average reduction in wirelength arrives at $16\\%$ , compared to previous works. Moreover, by effectively incorporating the alignment constraint, we achieve 0.940 on the alignment score, significantly surpassing the previous SOTA score of 0.474. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminary and Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Floorplan. The 2D floorplan task aims to determine the positions and shapes of each block given the block list, I/O port list, and netlist. Based on this, the 3D floorplan task is further required to place all blocks across multiple dies/layers. Each die/layer $d\\,\\in\\,{\\mathcal{D}}$ is a rectangular region with width $W$ and height $H$ , and all dies are of the same shape. Specifically, the block list is denoted as $\\bar{B}\\,=\\,\\{b_{1},b_{2},\\bar{\\,}.\\bar{\\,}.\\,,b_{n}\\}$ with $n$ blocks, where each block $b_{i}$ is a rectangle with width $w_{i}$ , height $h_{i}$ , and area $a_{i}\\,=\\,w_{i}\\cdot h_{i}$ . The bottom-left coordinate of block $b_{i}$ is denoted as $\\left({x_{i},y_{i}}\\right)$ , and the layer where the $b_{i}$ is located is denoted as $z_{i}$ . There are two types of blocks: hard block and soft block. For a hard block, its aspect ratio $\\begin{array}{r}{\\mathrm{AR}_{i}=\\frac{w_{i}}{h_{i}}}\\end{array}$ is fixed. For a soft block, its ratio $\\mathrm{AR}_{i}$ can vary between $[\\mathrm{AR}_{\\mathrm{min}},\\mathrm{AR}_{\\mathrm{max}}]$ , while the area must always satisfy $a_{i}=w_{i}\\cdot h_{i}$ . Additionally, we denote the $_\\mathrm{I/O}$ port list as $\\mathcal{T}=\\{t_{1},t_{2},\\dots,t_{m}\\}$ with m ports 2. Each port $t_{j}$ can be viewed as a point with a pre-determined position $(x_{j},y_{j},z_{j})$ , where $(x_{j},y_{j})$ is the coordinate, and $z_{j}$ is the layer index. netlist is defined as a set consisting of all nets, where each net is a set of blocks and ports, representing the interconnections. Since the layer $z_{i}$ of block $b_{i}$ is pre-assigned, we only need to determine the coordinate $(x_{i},y_{i})$ and the aspect ratio $\\mathrm{AR}_{i}$ for all blocks to optimize the following objectives: ", "page_idx": 2}, {"type": "text", "text": "1) Alignment. Given two blocks $b_{i},b_{j}$ on different layers, alignment evaluates the overlap/intersection area between them on the common projected 2D plane. We define the alignment score $\\operatorname{aln}(i,j)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{aln}_{x}(i,j)=\\mathrm{max}\\left(0,\\mathrm{min}(x_{i}+w_{i},x_{j}+w_{j})-\\mathrm{max}(x_{i},x_{j})\\right),}\\\\ &{\\mathrm{aln}_{y}(i,j)=\\mathrm{max}\\left(0,\\mathrm{min}(y_{i}+h_{i},y_{j}+h_{j})-\\mathrm{max}(y_{i},y_{j})\\right),}\\\\ &{\\mathrm{aln}(i,j)=\\mathrm{min}\\left(1,\\frac{\\mathrm{aln}_{x}(i,j)\\cdot\\mathrm{aln}_{y}(i,j)}{\\mathrm{aln}_{m}(i,j)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathrm{aln}_{m}(i,j)$ is the required minimum alignment area between block $b_{i}$ and $b_{j}$ . The total alignment score should be maximized to satisfy the alignment requirement. $\\left(b_{i},\\bar{b}_{j}\\right)$ forms an alignment pair if $\\mathrm{aln}_{m}(i,j)>0$ , and $b_{i},b_{j}$ are mutually alignment partners. ", "page_idx": 2}, {"type": "text", "text": "Cross-die block alignment is common in 3D FP [11, 13, 14]. Taking two blocks in different dies as an example, they employ vertical buses [11] or bonding bumps/pads [14] for communication, which requires capabilities for cross-die block alignment. That is, considering their projection onto a 2D plane, the related blocks must exhibit some minimum intersecting region, denoted as $\\mathrm{aln}_{m}(i,j)$ . ", "page_idx": 2}, {"type": "text", "text": "2) HPWL and 3) Overlap. Half Perimeter Wire Length (HPWL) is an approximate metric of wirelength. It can be computed much more efficiently, as accurate wirelength can be accessed only after the time-consuming routing stage. The summation of HPWL is minimized: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{\\mathrm{net}\\in\\mathrm{netlist}}}\\left(\\operatorname*{max}_{m_{i}\\in\\mathrm{net}}x_{i}^{c}-\\operatorname*{min}_{m_{i}\\in\\mathrm{net}}x_{i}^{c}+\\operatorname*{max}_{m_{i}\\in\\mathrm{net}}y_{i}^{c}-\\operatorname*{min}_{m_{i}\\in\\mathrm{net}}y_{i}^{c}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $m_{i}$ is either a block or port in net and $x_{i}^{c}$ is the center $\\mathbf{X}$ -coordinate. For a block, $\\begin{array}{r}{x_{i}^{c}=x_{i}+\\frac{w_{i}}{2}}\\end{array}$ , and for a port, $x_{i}^{c}=x_{i}$ . For overlap, given two blocks $b_{i},b_{j}$ on the same die, the overlap area between them should be minimized (detailed calculation is given in Alg. 4 in Appendix G.1). And all blocks should be placed within the fixed outline. ", "page_idx": 2}, {"type": "image", "img_path": "q9RLsvYOB3/tmp/5af895839258543f5ab85afcf212e9973ba1ccb9a58b2b836533d41c61d32344.jpg", "img_caption": ["Figure 1: Pipeline of FlexPlanner. Under the Actor-Critic framework, taking the multi-modality representation as input, the policy network consists of three sub-modules, responsible for determining the position, layer, and aspect ratio of blocks. Alignment mask and position mask are incorporated to filter out invalid positions where constraints (alignment, non-overlap, etc.) are not satisfied. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "q9RLsvYOB3/tmp/4fe96c8f1236efeeea73da31d942ef5d7ec971e8e1c7fbfeebc7c18f54c52ef0.jpg", "img_caption": ["Figure 2: Demonstration of alignment. In (a), the green region will be occupied by the block to place, and the yellow region is occupied by its alignment partner block which has already been placed. $\\mathrm{X}-$ , Y-dimensional and full alignment are shown in (b)(c)(d). Only $(x,y)$ satisfying $\\arcsin_{x}\\cdot\\arcsin_{y}\\geq\\arcsin_{m}$ are valid positions shown in (e), and this binary mask can be incorporated to fliter out invalid positions. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Overview. The 3D floorplanning task can be formulated as an episodic Markov Decision Process. As shown in Fig. 1, pipeline of our approach mainly includes state, policy network, critic network, and hybrid action space. State $s_{t}$ consists of three modalities, including vision, graph, and sequence. Action $a_{t}$ is represented as $\\left(x,y,z\\right)$ , AR), where position $\\left(x,y,z\\right)$ are discrete variables and aspect ratio AR is a continuous variable. These properties form a hybrid action space. Under the Actor-Critic framework, critic $V_{\\phi}\\big(s_{t}\\big)$ evaluates the current state. Policy network $\\pi_{\\theta}\\big(a_{t}|s_{t}\\big)$ determines the 2D coordinate $\\left({{x}_{t}},{{y}_{t}}\\right)$ of current block $b_{t}$ , the layer $z_{t+1}$ to access next block $b_{t+1}$ , and the aspect ratio $\\mathrm{AR}_{t+1}$ of $b_{t+1}$ . Masks are applied to the probability matrix of block positions to filter out invalid coordinates that do not meet constraints, such as non-alignment, overlap, or out-of-boundary. We respectively introduce the multi-modalities, layer decision, and reward function design in Sec. 3.1, Sec. 3.2 and Sec. 3.3. The overall algorithm is detailed in Alg. 5 in Appendix G.2. ", "page_idx": 3}, {"type": "text", "text": "3.1 Multi-Modality Representation of 3D Floorplanning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The state space contains three modalities: FP vision, netlist graph, and block placing sequence. ", "page_idx": 3}, {"type": "text", "text": "3.1.1 Vision Modality ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "It represents current floorplanning through images. We utilize four vision masks to depict chip layout. ", "page_idx": 3}, {"type": "text", "text": "Alignment Mask. $f_{a}\\in\\mathbb{N}^{W\\times H}$ $\\mathbb{N}$ is the field of natural number) is a matrix to evaluate the alignment area between blocks $b_{i}$ and its alignment partner $b_{j}$ . If $b_{j}$ has already been placed, $f_{a}[x,\\bar{y}]$ is the intersection area on a 2D projected plane if $b_{i}$ is placed at $(x,y)$ . If $b_{j}$ has not been placed yet, the alignment mask of $b_{i}$ will be set to a matrix full-filled with $\\mathrm{aln}_{m}(i,j)$ . The native approach has the complexity $\\mathcal{O}(W H)$ . However, when $W$ or $H$ is large, it has a low efficiency. We design an efficient alignment mask generation algorithm with meshgrid, which only iterates in regions causing projection overlap between these blocks and harnessing the power of parallel computing, thus reducing the complexity to ${\\mathcal{O}}(w h)$ namely $O(1)$ . Demonstration of alignment and details of the algorithm are shown in Fig. 2 and Alg. 1. Alignment mask is also utilized to filter out the positions that do not satisfy the alignment constraint. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Canvas Mask. Canvas mask fc \u2208N|D|\u00d7W \u00d7H is a global observation of current chip layout. The canvas mask is initialized to all zeros. After placing a block $b$ at each step, we modify the canvas mask with $f_{c}[z,x:x\\bar{+}w,y:y+h]\\!+\\!=$ 1. Based on the canvas mask, we can implement a fast calculation of the total overlap of chip layout, shown in Alg. 4 in Appendix G.1. ", "page_idx": 4}, {"type": "text", "text": "Wire Mask & Position Mask [28]. Wire mask $f_{w}\\,\\in\\,\\mathbb{N}^{W\\times H}$ is a matrix for how HPWL will increase if a block is placed at the position. It records the increase of HPWL by placing the current block to all candidate positions. Position mask $f_{p}\\in\\{0,1\\}^{W\\times H}$ indicates available positions for current block to place without overlap or out-of-boundary. 1 implies this position is feasible to place the block. ", "page_idx": 4}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/b25f713948fcee899b8ddd8e889276deb4cb2eb8790c099472d9cc024ded9fc8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Overall, in step $t$ , given current block $b_{t}$ , we concatenate $f_{a}^{(t)},f_{c}^{(t)},f_{w}^{(t)},f_{p}^{(t)}$ . We also incorporate masks f (wt+1), f p(t+1)of header block in FIFO queue q (introduced in sequence modality in Sec. 3.1.2) in each die, which represent all possible choices for next block in step $t+1$ , providing policy with future horizon. All of these input masks constitute the floorplanning vision modality. ", "page_idx": 4}, {"type": "text", "text": "3.1.2 Graph and Sequence Modality ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Graph Modality. Given a netlist, we convert it to a graph $G(V,E)$ , where $V$ is the set of vertices and $E$ is the set of edges. For two blocks $b_{i},b_{j}$ within a net, we add edges $e_{i j}$ , $e_{j i}$ between vertex pair $(v_{i},v_{j})$ . For the vertex feature, We select $(b i{\\bar{d}},x,y,z,w,h,a,p)$ , where $b i d$ is the block index and $p$ indicates whether this block has already been placed or not. Considering each block has a placing order showing the property as a sequence, it is natural to utilize positional encoding [31] to model this feature. Furthermore, we employ a Graph Attention Network [32] to produce embeddings of graph and nodes, representing the logical connection among blocks. ", "page_idx": 4}, {"type": "text", "text": "Sequence Modality. Given a die $d_{i}$ , blocks in $d_{i}$ are sorted by their area in descending order, forming a FIFO (first in, first out) queue $q_{i}$ . Thus, the block placing order for each die is pre-determined, and each queue can be viewed as a sequence. Combined with block features, we enhance and re-organize the multi-die sequence $\\mathbf{S}\\in\\mathbb{R}^{|\\mathcal{D}|\\bar{\\times}L\\times C}$ , where $L$ is the maximum number of blocks in a single die, $C$ is the number of features. We select $(b i d,x,y,z,w,h,a,p)$ as sequence features. The sequence modality provides actor and critic with global observation of entire block placing order, and is employed in the asynchronous layer decision discussed in Sec. 3.2. ", "page_idx": 4}, {"type": "text", "text": "3.2 Asynchronous Layer Decision ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In 2D FP, blocks are arranged on a single die and are organized in a FIFO queue representing the placing order. However, in 3D scenarios, multiple queues exist due to the multi-die property, necessitating a layer decision mechanism to merge these queues and determine the comprehensive placing order. A native approach is synchronous block placing, in which we initially place all blocks on the first die, followed by blocks on the second die, and continue in this manner until the last die. In [26, 27, 28, 30] with synchronous placing, the entire placing order keeps fixed. However, this synchronous die-by-die placing order neglects the cross-die connections and alignment requirements, leading to a relatively poor FP layout. To address this issue, we propose an asynchronous layer decision mechanism, which determines the layer for accessing next block. If the policy selects die $d_{i}$ for next step, the header block of the FIFO queue $q_{i}$ will be popped as the next block to place. ", "page_idx": 4}, {"type": "text", "text": "During the training process under asynchronous block placing, on one hand, the entire block placing order could vary hugely and become unstable due to the sparsity of the reward, making it non-trivial for position decider to determine the positions of blocks. On the other hand, the layer decision module could converge too fast to mode collapse (a fixed placing order leading to poor quality), empirically degenerating to synchronous die-by-die block placing. In our analysis, it is the short forward horizon leading to this problem. Since policy is only able to sense current and next states, it lacks the global receptive field of entire placing order. ", "page_idx": 5}, {"type": "text", "text": "To address these issues, we enhance the representation with the sequence modality, and employ Transformer [31] to extract global placing order feature. Given the multi-die sequence feature $\\mathbf{S}\\in\\mathbb{R}^{|\\mathcal{D}|\\times L\\times C}$ , self-attention is computed. S is viewed as the source sequence input with length $L$ . The memory $\\mathbf{M}\\in\\mathbb{R}^{|\\mathcal{D}|\\times L\\times L}$ , output of self-attention of S is given by [31]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{M}=\\mathrm{LN}(\\mathrm{MSA}(\\mathbf{S})+\\mathbf{S}),\\mathbf{M}=\\mathrm{LN}(\\mathrm{FFN}(\\mathbf{M})+\\mathbf{M}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where MSA is the multi-head self-attention, LN is layer normalization, and FFN is the feed-forward network. Next, cross-attention is applied on $\\mathbf{M}$ and current block feature $\\mathbf{b}_{k}$ . The single block feature $\\mathbf{b}_{k}\\in\\mathbb{R}^{1\\times C}$ is treated as the target sequence with length 1, serving as the query vector. The output of cross-attention $\\mathbf{O}\\in\\mathbb{R}^{|\\mathcal{D}|\\times1\\times C}$ is [31]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{O}=\\mathrm{LN}(\\mathrm{MHA}(\\mathbf{b}_{k},\\mathbf{M},\\mathbf{M})+\\mathbf{b}_{k}),\\mathbf{O}=\\mathrm{LN}(\\mathrm{FFN}(\\mathbf{O})+\\mathbf{O}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathrm{MHA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})$ denotes multi-head attention. Finally, the next layer decision probability vector can be further computed through linear projection and softmax operation. ", "page_idx": 5}, {"type": "text", "text": "3.3 Reward Function with Local Advantage and Global Baseline ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In 3D floorplanning task, final wirelength, overlap and alignment can only be accessed at the end of each episode, leading to a sparse reward. GraphPlace [26] uses this sparse reward design, where rewards at intermediate steps are all zeros. DeepPlace [27] adopts it with additional intrinsic reward via Random Network Distillation [33]. MaskPlace [28] introduces a dense reward scheme based on partial HPWL, which is calculated only on the currently placed blocks at each step. However, in two former methods [26, 27], they fail to accurately sense the intermediate quality through reward, and in MaskPlace [28], only difference between local ", "page_idx": 5}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/7447b6dea26107f10ed604719e3c7110bae56cb3f1357c980e8ced6749b17d87.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "adjacent steps is involved, lacking the global view of the whole episode. As a result, all these reward designs demonstrate relatively poor performances, especially with the complicated hybrid action space consisting of position, layer and aspect ratio. ", "page_idx": 5}, {"type": "text", "text": "To alleviate this problem, we design a novel reward function with local advantage and global baseline. We define local advantage as the difference of metric between two adjacent steps, and global baseline as the overall metric at the end of an episode. With local advantage, our model has the ability to acquire current state is whether better or worse than the previous. Global baseline depicts the overall quality of the final floorplanning result. It is also essential for asynchronous layer decision module to avoid early convergence and degeneration to poor die-by-die synchronous block placing order, shown in Sec. 4.4 and Fig. 6b. Details of reward design is shown in Alg. 2. ", "page_idx": 5}, {"type": "text", "text": "3.4 Flexible RL with Hybrid Action Space for 3D Floorplanning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We employ RL with a hybrid action space to address the 3D FP task. The state space consists of three modalities, including floorplanning vision, netlist graph and sequence of block placing order. The hybrid action space is formulated as $\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{Z}\\times\\mathcal{R}$ , with $\\mathcal{X},\\mathcal{Y},\\mathcal{Z}$ as discrete sets and $\\mathcal{R}$ as a continuous set. In each step $t$ , the policy outputs three distributions: 1) a discrete probability distribution for 2D position $(x_{t},y_{t})$ of current block $b_{t}$ , 2) a discrete probability distribution to determine the layer $z_{t+1}$ for accessing next block $b_{t+1}$ , and finally 3) the mean value and standard deviation of a contiguous Gaussian distribution to determine the aspect ratio $\\mathrm{AR}_{t+1}$ for $b_{t+1}$ . The reason for action $a_{t}$ containing the layer and aspect ratio for next step $t+1$ instead of $t$ is that: after the execution of $a_{t}$ , next state $s_{t+1}$ can be generated only after block $b_{t+1}$ to place at step $t+1$ and its shape have already been determined, since $b_{t+1}$ and its shape $(w_{t+1},h_{t+1})$ are involved in the calculation of alignment mask and wire mask. To guarantee adherence to the specified non-overlap and alignment constraints, position mask $f_{p}$ and alignment mask $f_{a}$ are incorporated in 2D position decision. Only $(x,y)$ satisfying $f_{p}[x,y]=0$ and $\\bar{f_{a}}[x,y]\\geq\\mathrm{aln}_{m}$ are considered as valid positions, where $\\mathrm{aln}_{m}$ is the required minimum alignment area between current block and its alignment partner. Finally, without the reliance on conventional heuristic FP representation, our approach exhibits more flexibility to directly solve position, aspect ratio and cross-die alignment for blocks. ", "page_idx": 5}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/b189cb31acb9691fc89cd873b4cf7df64f78f30c09309a18c7cd3c543259d72c.jpg", "table_caption": ["Table 2: Alignment score comparison among baselines and our method. The higher the alignment score, the better, and the optimal results are shown in bold. C/M means Circuit/Method. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/3884d0423710517b84026235e8d65db41d028b45a3b60324423f6588dfe62520.jpg", "table_caption": ["Table 3: HPWL (the lower the better) comparison. The optimal results are shown in bold. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We select the Actor-Critic [34] framework and Hybrid Proximal Policy Optimization [35, 36] algorithm. The objective function of our hybrid policy $\\pi_{\\theta}\\big(a_{t}|s_{t}\\big)$ can be formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\boldsymbol{L}(\\boldsymbol{\\theta})=\\sum_{k=1}^{3}\\boldsymbol{\\lambda}_{k}\\cdot\\hat{\\mathbb{E}}_{t}\\left[\\operatorname*{min}\\left(r_{t}^{(k)}(\\boldsymbol{\\theta})\\boldsymbol{\\hat{A}}_{t},\\operatorname{clip}\\left(r_{t}^{(k)}(\\boldsymbol{\\theta}),1-\\varepsilon,1+\\varepsilon\\right)\\boldsymbol{\\hat{A}}_{t}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $k=1,2,3$ represents position, layer and aspect ratio decision. $\\lambda_{k}$ is the weight for each clip loss. $r_{t}^{(k)}(\\theta)$ is the probability ratio $\\frac{\\pi_{\\theta}\\left(a_{t}^{(k)}|s_{t}\\right)}{\\pi_{\\theta_{\\mathrm{old}}}\\left(a_{t}^{(k)}|s_{t}\\right)}$ . $\\hat{A}_{t}$ denotes the generalized advantage estimation (GAE) [37], and $G_{t}=\\hat{A}_{t}+V_{t}$ is the cumulative discounted reward [37, 38]. $V_{t}$ is the estimated state value from critic network $V_{\\phi}\\big(\\boldsymbol s_{t}\\big)$ , and critic network is updated with minimizing Mean Squared Error (MSE) $L(\\phi)=\\lambda_{\\phi}\\cdot\\hat{\\mathbb{E}}_{t}\\left[(\\dot{G}_{t}-V_{\\phi}(s_{t}))^{2}\\right]$ . Entropy of each action distribution is also added as a regularization term for exploration encouragement. Training algorithm is shown in Appendix G.2, and details of model architecture are shown in Appendix $\\mathrm{D}$ . ", "page_idx": 6}, {"type": "text", "text": "4 Experiment and Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Evaluation Protocol and Benchmark ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the performance of FlexPlanner and other typical methods on public benchmark MCNC 3 and GSRC 4 shown in Table 7 in Appendix B. The floorplanning region is set to square region. I/O ports are projected to the fixed outline, and their positions are kept unchanged. Aspect ratio of each block can vary in range $\\left[{\\frac{1}{2}},2\\right]$ . Each experiment is run for five times with different seeds. More implementation details and hyper-parameter settings can be found in Appendix C. ", "page_idx": 6}, {"type": "image", "img_path": "q9RLsvYOB3/tmp/df1178d96976e327dbe5c727aaa731a064b6c9b27193a343ef838f57ba2cd15d.jpg", "img_caption": ["Figure 3: Our 3D floorplan result. Two blocks with the same index and the same color on different dies form an alignment pair, which roughly locate on the same positions and share a 2D common projected area. Gray blocks mean they do not have alignment partners. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Comparison with Baselines ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Methods including heuristic-based and learning-based approaches are selected as baselines, and the implementation details are shown in Appendix E. Average alignment score and total HPWL are employed as evaluation metrics. Results are shown in Table 2 and 3. For alignment score, our approach achieves 0.940, significantly surpassing the second-best method which scores 0.474. For HPWL, our approach achieves an average reduction of $16\\%$ . Our method reduces HPWL to 249,053, and the second-best method (Wiremask-BBO) reduces it to 296,791. However, it only achieves 0.222 for alignment score, failing to effectively tackle with the alignment constraint, empirically showing that our approach is capable of address alignment and HPWL simultaneously. Demonstration of 3D floorplanning results by our method are also shown in Fig. 3 and Fig. 4. We also compare performances on overlap, out-of-bound and runtime, shown in Appendix F. Capability and flexibility of our approach to address pre-placed modules (PPMs) are also demonstrated in Appendix F.5. ", "page_idx": 7}, {"type": "image", "img_path": "q9RLsvYOB3/tmp/9e5ad14016bfd087eadbac676f716de3c581e2286055ee13b198ba314a7a1e46.jpg", "img_caption": ["(c) Wiremask-BBO, alignment score $=0.35$ "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "q9RLsvYOB3/tmp/b0955668241ea48509b11c2f75165f18bd26affa146f2c9628f6e4443fc2c02a.jpg", "img_caption": ["(d) Ours, alignment score $=0.98$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Visualization of cross-die block alignment on circuit n50. Two blocks with the same index forms an alignment pair. For a pair with block $i,j$ , we calculate individual alignment score $\\operatorname{aln}(i,j)$ according to Eq. 1. Green means these two blocks are aligned $(\\mathrm{aln}(i,j)\\geq0.5)$ while red means not aligned $(\\mathrm{aln}(i,j)<0.5)$ . Total alignment score is calculated according to Alg. 3 in Appendix G.1. It demonstrates that our method achieves much better alignment score than other baselines. ", "page_idx": 7}, {"type": "image", "img_path": "q9RLsvYOB3/tmp/d815f51b7ff7a8f5846e733f9e65051c4603c00d0d56cfaeb7c6bd27d656a074.jpg", "img_caption": ["(a) Training curve on circuit $\\mathfrak{n}200$ . (b) Training curve on circuit $\\mathfrak{n}300$ . Figure 5: Training curve between fine-tune (based on circuit n100) and training from scratch. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/abfb09c7004b1c70b666d153da09f9acb0e9cca2629eed32a3fb7435f9bb95e2.jpg", "table_caption": ["Table 4: Zero-shot transferability evaluation (training on circuit n100). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 5: Ablation study on n100. sync: synchronous die-by-die placing. w/o aln: remove alignment mask in vision modality. w/o seq: remove sequence modality. w/o graph: remove graph modality. sparse rew: the same reward as GraphPlace [26]. diff rew: the same reward as MaskPlace [28]. ", "page_idx": 8}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/b93bf90cf1f8db3b993185ae9f54716caf73569d8d65c55fe768967ed05872dc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Transferable 3D Floorplanning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The zero-shot transferability of our method is also evaluated. We firstly train the model on circuit $\\mathfrak{n}100$ , and directly test the performance without any fine-tuning. In Table 4, ratio is calculated between running inference and training on corresponding circuit. It demonstrates that our model exhibits a good zero-shot transferability, on either smaller or larger cases. The transferability is also demonstrated through the training curves between fine-tuning (based on pre-trained weights of circuit n100) and training from scratch. In Fig. 5, through fine-tune technique, better or similar performance can be achieved, conserving substantial training resources. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We ablates effectiveness of each component in our approach, including asynchronous layer decision, multi-modality representation and reward function. Experiments on circuit n100 are shown in Table 5. Removing the alignment mask leads to a huge drop of alignment score, from 0.961 to 0.349. We also evaluate the influence of incorporating the alignment mask as an input feature and a constraint. In Fig. 6a, \u2018input\u2019 refers to the scenario where we feed the alignment mask as input to the model, while \u2018constr.\u2019 indicates that the alignment mask is utilized to fliter out invalid positions that do not satisfy the alignment constraint. As the input, alignment mask plays a critical role to effectively capture the alignment information. As the constraint, it reduces the action space and accelerates training process. ", "page_idx": 8}, {"type": "text", "text": "Shown in the column \u2018sync\u2019 in Table 5, fixed synchronous die-by-die block placing order performs worse than asynchronous layer decision, since in the latter, more flexibility is provided to plan the entire placing order. Our reward design, incorporating the local advantage and global baseline, is crucial for fully leveraging the asynchronous layer decision mechanism. In Fig. 6b, with the guidance of our reward, the layer decision module keeps active to learn an optimal placing order. However, with other reward design schemes, it is faced with either rapid convergence to the degeneration of synchronous die-by-die decision-making, or unstable oscillation. Under reward function \u2018diff\u2019 in MaskPlace [28], only the difference between two consecutive steps is focused, while the global reward is disregarded, leading to rapid convergence to the degeneration of synchronous die-by-die layer decision. Under reward function \u2018sparse\u2019 in GraphPlace [26], rewards for intermediate steps are all zeros, and only the reward of the final step is non-zero. The policy lacks the modeling of local information, resulting in its inability to accurately assess the influence of current action on the entire decision-making process. ", "page_idx": 8}, {"type": "image", "img_path": "q9RLsvYOB3/tmp/5eef4d2e75f3b88145df1e87439d454590dd153a0da5b7250fba8098d7d0784e.jpg", "img_caption": ["(a) Influence of the alignment mask. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "q9RLsvYOB3/tmp/b83a04d9a486219500d76351795a00eeba696534915eab603578dfe55d8f4f93.jpg", "img_caption": ["(b) Influence of rewards on layer decision. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 6: (a) Influence of the alignment mask. As the input feature, it plays a critical role in capturing the alignment information. As a constraint, it reduces the action space and accelerates the training process. (b) Influence of rewards on layer decision, shown in circuit $\\mathfrak{n}100$ with episode length $L=100$ ma nodf $|\\mathcal \u1e0a D \u1e0c |=2$ dhiaelsf. $z_{t}$ itsh et hpel adceitnerg msienqeude lnacyee.r t osrt $t$ $\\begin{array}{r}{Z=\\sum_{t=1}^{L/2}z_{t}}\\end{array}$ inse trhatei olany teor $Z\\rightarrow0$ $Z\\to L/2$ die-by-die synchronous layer decision (almost all die 0 or 1 in the first half episode). ", "page_idx": 9}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Typical floorplanning approaches. Classical methods in FP can be roughly categorized into heuristics and analytical approaches. The former typically models FP with a certain representation, such as $\\mathbf{B}^{\\ast}$ -tree [17], Corner Block List [18] and Sequence Pair [39], based on which heuristic algorithm (especially Simulated Annealing [40]) is utilized to search for an optimal solution. Finally, the representation is converted to corresponding FP result via a decoding scheme. Apart from the heuristics, analytical approaches [9, 22, 41] regard the FP problem as an electrostatic system [42, 43]. They compute the gradient of objective functions w.r.t. block coordinates, and utilize gradient descent-based algorithm to optimize the solution. ", "page_idx": 9}, {"type": "text", "text": "RL-based floorplanning approaches. Recently, floorplanning also attracted multiple research from the reinforcement learning communities. For instance, [23, 24, 25] incorporate traditional FP representation and RL. In each step, the policy network either 1) decides whether to accept the randomly perturbed state or not [23, 25] or 2) determines how to perturb the current state [24]. [26, 27, 28] primarily focus on making decisions about block position in 2D scenarios with RL. GraphPlace [26] and DeepPlace [27] incorporate graph and vision as representation. MaskPlace [28] employs visual representation, and designs a dense reward function based on partial HPWL. ", "page_idx": 9}, {"type": "text", "text": "RL with hybrid action space. Common RL can be categorized into discrete and continuous action spaces. However, in certain scenarios, besides the discrete action, we need to make decisions regarding its parameters, typically residing in continuous spaces [36]. Consequently, it gives rise to a hybrid action space. [44] proposes to discretize the continuous portion, which creates a large discrete set and sacrifices fine-grained control. Alternatively, [45, 46, 47, 48] convert the discrete action selection into a continuous space with an actor network and employ a DQN-based [49, 50] algorithm for training, but found to be unstable and inefficient. [36] suggests to use multiple policy heads consisting of one for discrete actions and the others for corresponding continuous parameters separately. [51] proposes to encode the hybrid action space to a continuous space with GAE [37]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose FlexPlanner, a flexible learning-based method in hybrid action space with multi-modality representation for 3D floorplanning task. It involves no reliance on heuristicbased search, thus achieves better flexibility to tackle position, aspect ratio and cross-die alignment for blocks under complex constraints. FlexPlanner outperforms baselines in alignment score and wirelength, and it also demonstrates zero-shot transferability on unseen circuits. This paper also has some limitations for future work: Further optimization in 3D FP could be involved such as thermal optimization. Our method has no potential harm to the public society at the moment. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yao Lai, Sungyoung Lee, Guojin Chen, Souradip Poddar, Mengkang Hu, David Z Pan, and Ping Luo. Analogcoder: Analog circuit design via training-free code generation. arXiv preprint arXiv:2405.14918, 2024.   \n[2] Yao Lai, Jinxin Liu, David Z Pan, and Ping Luo. Scalable and effective arithmetic tree generation for adder and multiplier designs. arXiv preprint arXiv:2405.06758, 2024.   \n[3] Lei Chen, Yiqi Chen, Zhufei Chu, Wenji Fang, Tsung-Yi Ho, Ru Huang, Yu Huang, Sadaf Khan, Min Li, Xingquan Li, et al. Large circuit models: opportunities and challenges. Science China Information Sciences, 2024.   \n[4] Chen Wang, Jingkun Mao, Giuseppe Selli, Shaofeng Luan, Lin Zhang, Jun Fan, David J Pommerenke, Richard E DuBroff, and James L Drewniak. An efficient approach for power delivery network design with closed-form expressions for parasitic interconnect inductances. IEEE Transactions on Advanced Packaging, 2006.   \n[5] Chung-Kuan Cheng, Andrew B Kahng, Ilgweon Kang, and Lutong Wang. Replace: Advancing solution quality and routability validation in global placement. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018.   \n[6] Ruizhe Zhong, Junjie Ye, Zhentao Tang, Shixiong Kai, Mingxuan Yuan, Jianye Hao, and Junchi Yan. Preroutgnn for timing prediction with order preserving partition: Global circuit pre-training, local delay learning and attentional cell modeling. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024.   \n[7] Jinwei Liu, Chak-Wa Pui, Fangzhou Wang, and Evangeline FY Young. Cugr: Detailed-routability-driven 3d global routing with probabilistic resource model. In 2020 57th ACM/IEEE Design Automation Conference (DAC), 2020.   \n[8] Xingbo Du, Chonghua Wang, Ruizhe Zhong, and Junchi Yan. Hubrouter: Learning global routing via hub generation and pin-hub connection. Advances in Neural Information Processing Systems, 36, 2023.   \n[9] Ximeng Li, Keyu Peng, Fuxing Huang, and Wenxing Zhu. Pef: Poisson\u2019s equation based large-scale fixed-outline floorplanning. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2022.   \n[10] Jill HY Law, Evangeline FY Young, and Royce LS Ching. Block alignment in 3d floorplan using layered tcg. In Proceedings of the 16th ACM Great Lakes symposium on VLSI, 2006.   \n[11] Johann Knechtel, Evangeline FY Young, and Jens Lienig. Planning massive interconnects in 3-d chips. TCAD, 2015.   \n[12] Anthony Agnesina, Moritz Brunion, Jinwoo Kim, Alberto Garcia-Ortiz, Dragomir Milojevic, Francky Catthoor, Gioele Mirabelli, Manu Komalan, and Sung Kyu Lim. Power, performance, area, and cost analysis of face-to-face bonded 3d ics. IEEE Transactions on Components, Packaging and Manufacturing Technology, 2023.   \n[13] Lennart Bamberg, Alberto Garc\u00eda-Ortiz, Lingjun Zhu, Sai Pentapati, Sung Kyu Lim, et al. Macro-3d: A physical design methodology for face-to-face-stacked heterogeneous 3d ics. In 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE), 2020.   \n[14] Sai Pentapati, Anthony Agnesina, Moritz Brunion, Yen-Hsiang Huang, and Sung Kyu Lim. On legalization of die bonding bumps and pads for 3d ics. In Proceedings of the 2023 International Symposium on Physical Design, 2023.   \n[15] Sankatali Venkateswarlu, Subrat Mishra, Herman Oprins, Bjorn Vermeersch, Moritz Brunion, Jun-Han Han, Mircea R Stan, Dwaipayan Biswas, Pieter Weckx, and Francky Catthoor. Impact of 3-d integration on thermal performance of risc-v mempool multicore soc. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2023.   \n[16] David Koblah, Rabin Acharya, Daniel Capecci, Olivia Dizon-Paradis, Shahin Tajik, Fatemeh Ganji, Damon Woodard, and Domenic Forte. A survey and perspective on artificial intelligence for security-aware electronic design automation. ACM Transactions on Design Automation of Electronic Systems, 2023.   \n[17] Yun-Chih Chang, Yao-Wen Chang, Guang-Ming Wu, and Shu-Wei Wu. $\\mathbf{B}^{\\ast}$ -trees: A new representation for non-slicing floorplans. In Design Automation Conference (DAC), 2000.   \n[18] Jai-Ming Lin, Yao-Wen Chang, and Shih-Ping Lin. Corner sequence-a p-admissible floorplan representation with a worst case linear-time packing scheme. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2003.   \n[19] Xianlong Hong, Sheqin Dong, Gang Huang, Yici Cai, Chung-Kuan Cheng, and Jun Gu. Corner block list representation and its application to floorplan optimization. IEEE Transactions on Circuits and Systems II: Express Briefs, 2004.   \n[20] Hai Zhou and Jia Wang. Acg-adjacent constraint graph for general floorplans. In IEEE International Conference on Computer Design: VLSI in Computers and Processors, 2004.   \n[21] Paul Falkenstern, Yuan Xie, Yao-Wen Chang, and Yu Wang. Three-dimensional integrated circuits (3d ic) floorplan and power/ground network co-synthesis. In Asia and South Pacific Design Automation Conference (ASP-DAC), 2010.   \n[22] Fuxing Huang, Duanxiang Liu, Xingquan Li, Bei Yu, and Wenxing Zhu. Handling orientation and aspect ratio of modules in electrostatics-based large scale fixed-outline floorplanning. In IEEE/ACM International Conference on Computer Aided Design (ICCAD), 2023.   \n[23] Qi Xu, Hao Geng, Song Chen, Bo Yuan, Cheng Zhuo, Yi Kang, and Xiaoqing Wen. Goodfloorplan: Graph convolutional network and reinforcement learning-based floorplanning. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2021.   \n[24] Mohammad Amini, Zhanguang Zhang, Surya Penmetsa, Yingxue Zhang, Jianye Hao, and Wulong Liu. Generalizable floorplanner through corner block list representation and hypergraph embedding. In SIGKDD, 2022.   \n[25] Wenbo Guan, Xiaoyan Tang, Hongliang Lu, Yuming Zhang, and Yimen Zhang. Thermal-aware fixedoutline 3-d ic floorplanning: An end-to-end learning-based approach. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2023.   \n[26] Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Azade Nazi, et al. A graph placement methodology for fast chip design. Nature, 2021.   \n[27] Ruoyu Cheng and Junchi Yan. On joint learning for solving placement and routing in chip design. NeurIPS, 2021.   \n[28] Yao Lai, Yao Mu, and Ping Luo. Maskplace: Fast chip placement via reinforced visual representation learning. NeurIPS, 2022.   \n[29] Yao Lai, Jinxin Liu, Zhentao Tang, Bin Wang, Jianye Hao, and Ping Luo. Chipformer: Transferable chip placement via offline decision transformer. In ICML, 2023.   \n[30] Yunqi Shi, Ke Xue, Song Lei, and Chao Qian. Macro placement by wire-mask-guided black-box optimization. NeurIPS, 2023.   \n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.   \n[32] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In ICLR, 2018.   \n[33] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.   \n[34] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. NeurIPS, 1999.   \n[35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[36] Zhou Fan, Rui Su, Weinan Zhang, and Yong Yu. Hybrid actor-critic reinforcement learning in parameterized action space. In IJCAI, 2019.   \n[37] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.   \n[38] Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. Journal of Machine Learning Research, 2022.   \n[39] Hiroshi Murata, Kunihiro Fujiyoshi, Shigetoshi Nakatake, and Yoji Kajitani. Vlsi module placement based on rectangle-packing by the sequence-pair. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 1996.   \n[40] Dimitris Bertsimas and John Tsitsiklis. Simulated annealing. Statistical science, 1993.   \n[41] Xingbo Du, Ruizhe Zhong, Shixiong Kai, Zhentao Tang, Siyuan Xu, Jianye Hao, Mingxuan Yuan, and Junchi Yan. Jigsawplanner: Jigsaw-like floorplanner for eliminating whitespace and overlap among complex rectilinear modules. In 2024 IEEE/ACM International Conference on Computer Aided Design (ICCAD), pages 1\u20139. ACM, 2024.   \n[42] Jingwei Lu, Pengwen Chen, Chin-Chih Chang, Lu Sha, Dennis Jen-Hsin Huang, Chin-Chi Teng, and Chung-Kuan Cheng. eplace: Electrostatics-based placement using fast fourier transform and nesterov\u2019s method. ACM Transactions on Design Automation of Electronic Systems (TODAES), 2015.   \n[43] Yibo Lin, Shounak Dhar, Wuxi Li, Haoxing Ren, Brucek Khailany, and David Z Pan. Dreamplace: Deep learning toolkit-enabled gpu acceleration for modern vlsi placement. In Proceedings of the 56th Annual Design Automation Conference 2019, 2019.   \n[44] Alexander A Sherstov and Peter Stone. Function approximation via tile coding: Automating parameter choice. In International symposium on abstraction, reformulation, and approximation, 2005.   \n[45] Matthew Hausknecht and Peter Stone. Deep reinforcement learning in parameterized action space. arXiv preprint arXiv:1511.04143, 2015.   \n[46] Warwick Masson, Pravesh Ranchod, and George Konidaris. Reinforcement learning with parameterized actions. In Proceedings of the AAAI conference on artificial intelligence, 2016.   \n[47] Jiechao Xiong, Qing Wang, Zhuoran Yang, Peng Sun, Lei Han, Yang Zheng, Haobo Fu, Tong Zhang, Ji Liu, and Han Liu. Parametrized deep q-networks learning: Reinforcement learning with discrete-continuous hybrid action space. arXiv preprint arXiv:1810.06394, 2018.   \n[48] Craig J Bester, Steven D James, and George D Konidaris. Multi-pass q-networks for deep reinforcement learning with parameterised action spaces. arXiv preprint arXiv:1905.04388, 2019.   \n[49] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.   \n[50] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR, 2016.   \n[51] Boyan Li, Hongyao Tang, YAN ZHENG, HAO Jianye, Pengyi Li, Zhen Wang, Zhaopeng Meng, and LI Wang. Hyar: Addressing discrete-continuous action reinforcement learning via hybrid action representation. In ICLR, 2021.   \n[52] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 2019.   \n[53] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[54] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. NeurIPS, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Notation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "All notations in this paper are shown in Table 6. ", "page_idx": 13}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/79526c044919323e4b79843e53a7d6766b1184846cd6a515dc7d61b4de82a0cc.jpg", "table_caption": ["Table 6: Notation. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Statistics of Benchmark ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We evaluate the performance of FlexPlanner and other typical methods on public benchmark MCNC 5 and GSRC 6 shown in Table 7. The \u2018alignment\u2019 means the number of blocks with alignment partner. ", "page_idx": 13}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/943a868925acf0d80d97173984862ed221a06348f4ffa3fac3826e629f0006da.jpg", "table_caption": ["Table 7: MCNC and GSRC benchmark "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Computational Resources ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We use PyTorch [52] deep learning framework and tianshou [38] Reinforcement Learning framework. We select Adam [53] optimizer with learning rate 0.0005. We train and test our model on a Linux server with one NVIDIA GeForce RTX 3090 GPU with 24 GB CUDA memory, two AMD Ryzen Threadripper 3970X 32-Core Processors at $3.70\\:\\mathrm{GHz}$ and 128 GB RAM. ", "page_idx": 13}, {"type": "text", "text": "C.2 Minimum Alignment Requirement Configuration ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the alignment setting, given an alignment pair $\\left(b_{i},b_{j}\\right)$ , the minimum alignment requirement $\\mathrm{aln}_{m}(i,j)$ is set to $\\mathrm{aln}_{m}(i,j)=\\alpha_{i j}\\cdot\\mathrm{min}\\{a_{i},a_{j}\\}$ , where $a_{i},a_{j}$ are the area of block $b_{i},b_{j}$ . The coefficient $\\alpha_{i j}$ controls the minimum alignment requirement between block $b_{i},b_{j}$ , and can be adjusted according to the specific circumstances for each alignment pair. In our experiments, we set all $\\alpha_{i j}=1.0$ , which is the most challenging scenario to evaluate the effectiveness of our approach. ", "page_idx": 14}, {"type": "text", "text": "C.3 Hyper-Parameters ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Other hyper-parameters are shown in Table 8. ", "page_idx": 14}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/39226fedbacc264dbf40483e409be46773a866e8edf3e66b77ca0a3bd1c61ff6.jpg", "table_caption": ["Table 8: Hyper-parameters. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "D Model Architecture ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We introduce the networks in our pipeline shown in Fig. 1 as follows: ", "page_idx": 14}, {"type": "text", "text": "Shared Encoder. The shared encoder $E_{\\phi}$ mainly consists of two parts: 1) a CNN-based backbone ResNet18 with pre-trained weights from \u2018IMAGENET1K_V1\u2019 for floorplanning vision modality input, and 2) a GNN (Graph Neural Network)-based backbone Graph Attention Network [32] with two layers for graph modality. ", "page_idx": 14}, {"type": "text", "text": "In step $t$ , given current block $b_{t}$ , we concatenate the alignment mask $f_{a}^{(t)}$ , canvas mask $f_{c}^{(t)}$ , wire mask $f_{w}^{(t)}$ , and position mask $f_{p}^{(t)}$ together. We also incorporate f (wt+1), f p(t+1)of header block in FIFO queue q of each die. These masks of queue header blocks represent all possible choices in step $t+1$ , providing policy with future horizon. All of these input masks constitute the floorplanning vision modality. ", "page_idx": 14}, {"type": "text", "text": "Taking the netlist graph as input, the GNN backbone outputs nodes embeddings, representing the local recetive field information. Besides, a global average pooling layer is applied on these node embeddings to calculate the graph embedding, which captures the global view of the whole netlist graph. ", "page_idx": 14}, {"type": "text", "text": "Critic Network. The critic network $V_{\\phi}$ mainly consists of three parts: 1) the shared encoder $E_{\\phi}$ , 2) a sequence Transformer [31] with two encoder layers and two decoder layers for block placing order sequence, and 3) a die/layer embedding model to represent each die/layer with a learnable feature vector. The shared encoder $E_{\\phi}$ is responsible for processing the floorplanning vision input, and the Transformer is for the input sequence modality. Serving as the source sequence in Transformer, the multi-die sequence feature $\\mathbf{S}\\in\\bar{\\mathbb{R}}^{|\\mathcal{D}|\\times\\bar{L}\\times C}$ is employed in the calculation of memory, which is the output of self-attention. And the single block feature $\\mathbf{b}_{k}\\in\\dot{\\mathbb{R}}^{1\\times C}$ is treated as the target sequence with length 1, serving as the query in cross-attention. To Stabilize training, the shared encoder $E_{\\phi}$ is only updated during the updating process of critic network. As a result, the critic network is responsible to take input as the state $s_{t}$ and output corresponding state value $v_{t}$ . ", "page_idx": 14}, {"type": "text", "text": "Policy Network. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 Position Decider. The position decider network $\\pi_{\\theta}^{(1)}$ mainly consists of a generator network. It takes input as the output of the shared encoder $E_{\\phi}$ , and outputs a 2D feature map as the probability matrix to determine the 2D position for current block. We borrow the generator architecture from InfoGAN [54], which utilizes up-sampling layer instead of transposed convolutional layer to realize scaling-up. It contains three blocks and ", "page_idx": 14}, {"type": "text", "text": "each block is with convolutional layer, batch normalization, leaky ReLU activation and up-sampling layer.   \nFinally, it outputs a probability matrix $\\mathbf{M}_{p}^{(t)}\\in\\mathbb{R}^{W\\times H}$ to determine the position $\\left({{x}_{t}},{{y}_{t}}\\right)$ for current block $b_{t}$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 Layer Decider. The layer decider network $\\pi_{\\boldsymbol{\\theta}}^{(2)}$ mainly consists of three parts: 1) a CNN-based model for FP vision modality, 2) a die embedding model to represent current input die, and 3) a Transformer network to process the input block placing order sequence for each die. The CNN model consists of three blocks, and each block has one convolutional layer, one max pooling layer, with ReLU as activation function. The Transformer model consists of two encoder layers and two decoder layers, and is designed to process the input multi-die block placing order sequence. The layer decider network takes input as the state $s_{t}$ and outputs a probability vector $\\mathbf{p}_{z}^{(t)}\\in|\\mathcal{D}|$ to determine the layer $z_{t+1}$ for accessing next block $b_{t+1}$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 Aspect Ratio Decider. The aspect ratio decider network $\\pi_{\\boldsymbol{\\theta}}^{(3)}$ mainly has a CNN model, consisting of three 2D convolutional layers with ReLU activation function and max pooling layer. It takes input as the floorplanning vision modality in state $s_{t}$ and the feature of next block $b_{t+1}$ to determine the aspect ratio $\\mathrm{AR}_{t+1}$ for $b_{t+1}$ . Since the aspect ratio is an action in contiguous space, we select Gaussian distribution to depict it, and the network outputs corresponding mean value and standard deviation. ", "page_idx": 15}, {"type": "text", "text": "E Baselines ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The baselines referred in Sec. 4 are introduced as follows: ", "page_idx": 15}, {"type": "text", "text": "3D- $\\mathbf{\\deltaB^{*}}$ -SA [21] is a heuristic-based approach, which represents a 3D floorplanning by $\\mathbf{B}^{\\ast}$ -tree [17]. Simulated Annealing (SA) [40] is selected to search an optimal result. At each iteration, current $\\mathbf{B}^{\\ast}$ -tree is randomly perturbed to a new state, which will be accepted based on a certain probability. Finally, the $\\mathbf{B}^{\\ast}$ -tree is converted to corresponding floorplannning result via a decoding scheme. In previous scenarios, only HPWL and out-of-bound penalty are incorporated into heuristics (cost/energy function). To optimize the alignment in 3D FP, we also combine the alignment score into its heuristics to guide the solution searching. ", "page_idx": 15}, {"type": "text", "text": "RL-CBL [24] combines heuristic-based search Corner Block List (CBL) [18, 19] and Reinforcement Learning together, utilizing policy network to determine how to perturb current FP at each step. It utilizes intermediate out-of-bound penalty and final HPWL as reward function. In 3D FP with multiple stacked layers, we extend CBL to 3D scenario. In order to realize the optimization of cross-die block alignment, we also incorporate alignment score into reward function. However, it is incapable of addressing the variable aspect ratio of soft blocks. ", "page_idx": 15}, {"type": "text", "text": "Wiremask-BBO [30] is a black-box optimization (BBO) framework, by using a wire-mask-guided [28] greedy procedure for objective evaluation. At each step, it utilize the position mask to fliter out invalid positions causing overlap or out-of-bound. Within the valid positions, the location with the minimum increment of HPWL is selected to place current block. Considering the alignment requirement in 3D FP, we incorporate the alignment mask to further fliter out invalid positions which do not satisfy the alignment constraint. However, it is incapable of addressing the variable aspect ratio of soft blocks. ", "page_idx": 15}, {"type": "text", "text": "GraphPlace [26], DeepPlace [27], MaskPlace [28] are three learning-based approaches, utilizing Reinforcement Learning to decide the position for each block on 2D chip layout. Variable aspect ratios of soft blocks are not taken into consideration. In this task, we extend them to 3D scenarios, and incorporate the alignment mask to filter out invalid positions where alignment constraint is not satisfied. Besides, alignment score is also involved into the calculation of reward function, with the purpose of assisting them to realize the optimization of cross-die alignment. ", "page_idx": 15}, {"type": "text", "text": "F Addtional Experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "F.1 Runtime Comparison ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Comparison of runtime among our approach and other baselines are shown in Table 9. The runtime of our method is either lower or comparable to other learn-based methods [26, 27, 28], and is better than the heuristic-based method [21], where tens of thousands of iterations are required. Pipeline of Wiremask-BBO [30] is the same as MaskPlace [28], while neural network is not involved in it, contributing to the shortest inference time. For the approach RL-CBL [24], it is faster than other learning-based methods since its environment is relatively simple, only responsible for decoding the heuristic representation Corner Block List (CBL) [18, 19] into corresponding FP result. However, the performance is also limited by CBL, and the final FP result is further worse than other methods without the reliance of heuristic representation. ", "page_idx": 15}, {"type": "text", "text": "F.2 Out-of-Bound ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For heuristics-based methods, such as 3D- $\\mathbf{B}^{\\ast}$ -SA [21] and RL-CBL [24], the heuristic representation will be converted to corresponding floorplan result through a decoding scheme. Although non-overlap can be guaranteed, blocks in the final FP could be out of the fixed outline, leading to the low area utilization of chip die and out-of-bound. In our method FlexPlanner, each block can be naturally placed within the boundary due to the alignment mask and position mask. Consequently, the occurrence of out-of-bound floorplan is effectively prevented. We compare the out-of-bound in Table 10, and the outbound is calculated as follows: ", "page_idx": 15}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/a4b288c31b9466496b07c1bd7a696dbef264d9a565a658fbec630ce6ece206f3.jpg", "table_caption": ["Table 9: Runtime (second) comparison among baselines and our method. The lower the runtime, the better. The unit of runtime is in seconds. C/M means Circuit/Method. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{x_{m}=\\displaystyle\\operatorname*{max}_{b_{i}\\in B}\\left\\lbrace x_{i}+w_{i}\\right\\rbrace}\\\\ {y_{m}=\\displaystyle\\operatorname*{max}_{b_{i}\\in B}\\left\\lbrace y_{i}+h_{i}\\right\\rbrace}\\\\ {\\mathrm{outbound}=\\displaystyle\\frac{\\operatorname*{max}\\left\\lbrace0,x_{m}-W\\right\\rbrace}{2W}+\\displaystyle\\frac{\\operatorname*{max}\\left\\lbrace0,y_{m}-H\\right\\rbrace}{2H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/3eb041e33cabe87029471506baf2bd8d1c09645792fb8943ea630d15b68053c2.jpg", "table_caption": ["Table 10: Comparison of out-of-bound, the lower the better. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "F.3 Overlap ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We also compare performance on overlap among our method and baselines, shown in Table 11. Our method reduces the overlap to 0.001, which comparable with 3D- $\\cdot\\mathbf{B}^{\\ast}$ -SA [21], RL-CBL [24], and better than other methods. In ${}^{3}\\mathrm{D}\\!-\\!\\mathrm{B}^{*}$ -SA [21] and RL-CBL [24], although non-overlap can be guaranteed, blocks in corresponding FP could be out of the fixed outline, leading to the low area utilization of chip die and out-of-bound as discussed in Appendix F.2. ", "page_idx": 16}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/23b62632a9df960cdfbbe3180bb664d1b25c167c7320f1b3c15d6c98ccb81082.jpg", "table_caption": ["Table 11: Overlap comparison among baselines and our method. The lower the overlap, the better. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "F.4 Influence of Grid Size ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Due to the different size of circuits, we discretize the sizes into uniform region. Given a chip die with original shape $W_{o}\\times H_{o}$ , the region is projected into $W\\times H$ . For each block $b$ with shape $w\\times h$ , its shape is also ", "page_idx": 16}, {"type": "text", "text": "projected into max $\\left\\{1,\\operatorname{round}(w\\cdot\\frac{W}{W_{o}})\\right\\}\\times\\operatorname*{max}\\left\\{1,\\operatorname{round}(h\\cdot\\frac{H}{H_{o}})\\right\\}$ . We also empirically investigate the influence of different grid size $W,H$ , shown in Table 12. Metric \u2018Error\u2019 evaluates the area error between the projected region and the original region as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Error}=\\frac{1}{|\\boldsymbol{\\mathcal{B}}|}\\sum_{\\boldsymbol{b}\\in\\boldsymbol{B}}\\frac{|\\boldsymbol{w}\\frac{\\boldsymbol{W_{o}}}{\\boldsymbol{W}}\\times\\boldsymbol{h}\\frac{\\boldsymbol{H_{o}}}{H}-\\boldsymbol{w_{o}}\\times\\boldsymbol{h_{o}}|}{\\boldsymbol{w_{o}}\\times\\boldsymbol{h_{o}}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\boldsymbol{\\mathbf{\\rho}}_{\\perp}$ is the set of all blocks. Similar performances in terms of alignment, HPWL, and overlap are achieved across different grid number settings. Lower grid numbers offer faster execution speeds but may result in relatively higher area errors that may not meet precision requirements. Higher grid numbers produce more fine-grained floorplanning results but lead to a larger action space and require additional computational resources. As a trade-off between runtime and precision, a grid size of 128 is selected. ", "page_idx": 17}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/34ed4bc7fc144a40643cf2b1f744973b8f523aee908ac4712280c51001baefa5.jpg", "table_caption": ["Table 12: The influence of different grid size on circuit n100. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "F.5 Floorplanning with Pre-Placed Modules ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our approach is also capable of addressing the circuits with pre-placed modules (PPMs). PPMs are blocks whose position and aspect ratio are pre-determined and fixed during the FP process. Heuristics-based methods [21, 24] are incapable of address the existence of PPMs. In these heuristics approaches, after the perturbation is applied on current FP representation, the entire FP layout result may undergo changes. During this process, it cannot be guaranteed that the positions and shapes of pre-placed modules will remain unchanged. Consequently, they lack the flexibility to address circuits with PPMs. Our method FlexPlanner is capable to solve this issue. At the beginning of FP process (the beginning of an episode), all PPMs are placed on their pre-determined positions, and their aspect ratios keep unchanged. After that, we modify the canvas mask and position mask, indicating that these positions have been occupied by PPMs. In each step $t$ to place a movable block $b_{t}$ , the overlap among $b_{t}$ and other blocks (PPMs and other placed blocks) can be avoided. The floorplanning result with PPMs is shown in Fig. 7. ", "page_idx": 17}, {"type": "text", "text": "G Algorithms ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "G.1 Algorithm for Calculation of Alignment Rate and Overlap ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The calculation of alignment score $\\mathrm{aln}_{t}$ and overlap $o_{t}$ at step $t$ are shown in Alg. 3 and Alg. 4. These values are involved in the calculation of reward function with local advantage and global baseline shown in Alg. 2. ", "page_idx": 17}, {"type": "table", "img_path": "q9RLsvYOB3/tmp/74e3ccffad36fbcf33e8a229594b3f49ecf291e932b16dcedfe5eadcd73f90d4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "G.2 Training Algorithm ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The overall training pipeline is shown in Alg. 5, based on PPO [35] algorithm with RL framework tianshou [38]. During a training epoch, the policy network collects training data into replay buffer by interacting with the environment. To accelerate the training process, the policy network simultaneously interacts with $n_{e}$ environments ", "page_idx": 17}, {"type": "image", "img_path": "q9RLsvYOB3/tmp/e3ca6a3d1b5929b99f21afc3a81b5cf3a4db07171222b5f6bafa86625a51fc43.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 7: Our 3D floorplan result. Blue blocks are PPMs whose position and aspect ratio are fixed during FP. Two blocks with the same index on different dies form an alignment pair, which roughly locate on the same positions and share a 2D common projected area. ", "page_idx": 18}, {"type": "text", "text": "Algorithm 4: Overlap calculation. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Input: Current block $b_{t}$ , canvas mask $f_{c}$ , chip die width $W$ and height $H$   \nOutput: Overlap $o_{t}$ at step $t$   \n$f_{c}[z_{i},x_{i}:x_{i}+w_{i},y_{i}:y_{i}+h_{i}]\\mathrel{+}=1$   \ncalculate overlap $o_{t}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\no_{t}=\\frac{\\sum_{z,x,y}\\operatorname*{max}\\{0,f_{c}[z,x,y]-1\\}}{W H}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "in parallel. We set the buffer size to satisfy ", "page_idx": 18}, {"type": "equation", "text": "$$\nL_{b u f}\\equiv0\\bmod\\left(n_{e}\\times\\mathrm{len}\\,(e p i s o d e)\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "ensuring that each episode in the replay buffer is completed and terminated with its final step, where $L_{b u f}$ is the size of replay buffer. After data collection, we further compute the reward, cumulative discounted reward and advantage. Since each episode is completed in the replay buffer, each step has the corresponding terminated step (end step) within the same episode to calculate the reward with local advantage and global baseline. Finally, both critic network and policy network are updated. ", "page_idx": 18}, {"type": "text", "text": "r epoch from 1 to n_epochs do // data collection while replay buffer is not full do // action probability calculation calculate action probability for position of current block $b_{t}$ : $\\pi_{\\theta}\\left(a_{t}^{(1)}|s_{t}\\right)$ sample $(x_{t},y_{t})\\sim\\pi_{\\theta}\\left(a_{t}^{(1)}|s_{t}\\right)$ calculate action probability for layer to next block $b_{t+1}\\colon\\pi_{\\theta}\\left(a_{t}^{(2)}|s_{t}\\right)$ sample $z_{t+1}\\sim\\pi_{\\theta}\\left(a_{t}^{(2)}|s_{t}\\right)$ calculate action probability for aspect ratio of next block $b_{t+1}\\colon\\pi_{\\theta}\\left(a_{t}^{(3)}|s_{t}\\right)$ sample $\\mathrm{AR}_{t+1}\\sim\\pi_{\\theta}\\left(a_{t}^{(3)}|s_{t}\\right)$ // action execution and next state observation execute the action $a_{t}$ : 1. place block $b_{t}$ at position $\\left({{x}_{t}},{{y}_{t}}\\right)$ 2. access next block $b_{t+1}$ from the FIFO queue $q_{z_{t+1}}$ of layer $z_{t+1}$ 3. set the aspect ratio of $b_{t+1}$ to $\\mathrm{AR}_{t+1}$ observe next state $s_{t+1}$ from environment get alignment score $\\mathrm{aln}_{t}$ , wirelength $\\mathrm{HPWL}_{t}$ , overlap $o_{t}$ from environment add sample $\\left(s,s^{\\prime},(x,y,z,\\mathrm{AR}),(\\mathrm{aln},\\mathrm{HPWL},o),(\\pi_{\\theta_{o l d}}^{(1)},\\pi_{\\theta_{o l d}}^{(2)},\\pi_{\\theta_{o l d}}^{(3)})\\right)$ into buffer end // data processing for each sample in buffer do re-compute reward $r$ with local advantage and global baseline according to Alg. 2 calculate state value for current state $:v=V_{\\phi}(s)$ ", "page_idx": 19}, {"type": "text", "text": "calculate state value for next state $s^{\\prime}\\colon v^{\\prime}=V_{\\phi}\\big(s^{\\prime}\\big)$ modify current sample with adding $(r,v,v^{\\prime})$ ", "page_idx": 19}, {"type": "text", "text": "for each sample in buffer do compute cumulative discounted reward $G$ and advantage $\\hat{A}$ for current sample according to generalized advantage estimation (GAE) [37, 35] $\\begin{array}{r}{\\hat{A}_{t}=\\sum_{i=0}^{T-t-1}(\\gamma\\lambda)^{i}\\delta_{t+i}}\\end{array}$ , where $\\delta_{t}=r_{t}+\\gamma v_{t+1}-v_{t}$ $G_{t}=\\hat{A}_{t}+v_{t}$ modify current sample with adding $(\\hat{A},G)$ ", "page_idx": 19}, {"type": "text", "text": "// network update   \nfor update_epoch from 1 to n_update_epochs do for minibatch in buffer do // policy/actor network update compute action probability \u03c0\u03b8 , \u03c0\u03b8 , \u03c0\u03b8 compute action distribution entropy $h_{\\theta}^{(1)},h_{\\theta}^{(2)},h_{\\theta}^{(3)}$ update policy/actor network according to Eq. 5 // critic network update compute state value $v=V_{\\phi}(s)$ update critic network according to Sec. 3.4 end ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We make clear claims about contributions and scope of our paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We discuss the liminations in Sec. 6. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have provided detailed hyper-parameters settings in Appendix C and the full training pipeline in Alg. 5 in Appendix G.1. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We have provided detailed hyper-parameters settings in Appendix C and the full training pipeline in Alg. 5 in Appendix G.1. Code and data will be public available once our paper is accepted. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have provided detailed hyper-parameters settings in Appendix C and the full training pipeline in Alg. 5 in Appendix G.1. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Experiments in Sec. 4 are run five times with different seeds. Both mean value and standard deviation are provided. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide the information about compute resources in Appendix C. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, is with the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Broader impacts including both positive and negative societal impacts are discussed in Sec. 6. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The license for both datasets GSRC and MCNC is MIT which can be found in http: //vlsicad.eecs.umich.edu/BK/copyright.html ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]