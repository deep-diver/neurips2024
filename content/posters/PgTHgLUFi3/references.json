{"references": [{"fullname_first_author": "S. Caldas", "paper_title": "Expanding the reach of federated learning by reducing client resource requirements", "publication_date": "2019-00-00", "reason": "This paper is foundational to the work presented, introducing the concept of federated learning and the challenges of heterogeneous clients which directly motivates the research in this paper."}, {"fullname_first_author": "E. Diao", "paper_title": "HeteroFL: Computation and communication efficient federated learning for heterogeneous clients", "publication_date": "2021-00-00", "reason": "This paper directly addresses the heterogeneity issue, proposing a solution that is compared against in this paper's experimental evaluation."}, {"fullname_first_author": "M. Khodak", "paper_title": "Initialization and regularization of factorized neural layers", "publication_date": "2021-00-00", "reason": "This paper introduces a key technique used in the proposed method: factorized layers, making it a crucial foundational piece for the proposed work."}, {"fullname_first_author": "Y. Niu", "paper_title": "Federated learning of large models at the edge via principal sub-model training", "publication_date": "2022-00-00", "reason": "This paper is highly relevant as it proposes a similar approach to spectral model sharding, providing a strong baseline for comparison and context for this work."}, {"fullname_first_author": "D. Yao", "paper_title": "FedHM: Efficient federated learning for heterogeneous models via low-rank factorization", "publication_date": "2022-00-00", "reason": "This paper directly addresses the heterogeneity issue, proposing a solution that is compared against in this paper's experimental evaluation, also using low-rank factorization as a core component."}]}