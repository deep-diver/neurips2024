[{"type": "text", "text": "UMB: Understanding Model Behavior for Open-World Object Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xing Xi Yangyang Huang Zhijie Zhong Ronghua Luo ", "page_idx": 0}, {"type": "text", "text": "School of Computer Science and Engineering South China University of Technology GuangZhou, China 510006 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Open-World Object Detection (OWOD) is a challenging task that requires the detector to identify unlabeled objects and continuously demands the detector to learn new knowledge based on existing ones. Existing methods primarily focus on recalling unknown objects, neglecting to explore the reasons behind them. This paper aims to understand the model\u2019s behavior in predicting the unknown category. First, we model the text attribute and the positive sample probability, obtaining their empirical probability, which can be seen as the detector\u2019s estimation of the likelihood of the target with certain known attributes being predicted as the foreground. Then, we jointly decide whether the current object should be categorized in the unknown category based on the empirical, the in-distribution, and the out-of-distribution probability. Finally, based on the decision-making process, we can infer the similarity of an unknown object to known classes and identify the attribute with the most significant impact on the decision-making process. This additional information can help us understand the behavior of the model\u2019s prediction in the unknown class. The evaluation results on the Real-World Object Detection (RWD) benchmark, which consists of five real-world application datasets, show that we surpassed the previous state-of-the-art (SOTA) with an absolute gain of $5.3\\;\\mathrm{mAP}$ for unknown classes, reaching $20.5\\;\\mathrm{mAP}.$ . Our code is available at https://github.com/xxyzll/UMB. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As a fundamental task in computer vision, object detection has always been the focus of extensive attention[1, 2, 3]. Traditional object detection methods are trained on closed datasets, assuming all detected objects have already been annotated in the training set. However, the real-world environment\u2019s complexity means it is impossible to annotate all objects. As a result, the application of traditional detection methods is limited. Open World Object Detection (OWOD) has been introduced to address the issue. OWOD can be divided into two subtasks: mining potential objects and incremental learning. The former requires the model to detect categories in the test set that have not been annotated in the training set. These newly discovered objects are then handed over to annotators, who select the categories of interest. Subsequently, the model is required to fine-tune its existing knowledge to detect these newly added categories (incremental learning). ", "page_idx": 0}, {"type": "text", "text": "Existing works primarily focus on generating pseudo-labels for potential objects in the training set, treating these pseudo-labels as annotations for unknown categories. For instance, ORE[4] labels samples with high objectness predicted as background as potential objects. CAT[5] and RE-OWOD[6] utilize selective search to provide annotations for unknown categories. OW-DETR[7] proposes an attention-driven pseudo-label strategy to mine potential positive samples. However, despite these heuristic methods being able to recall potential objects, they share a common flaw. As shown in Figure 1 (left), existing methods can only detect unknown objects and then provide these labels to annotators. However, the reason the model would predict these objects remains unknown to the annotators. Therefore, this paper attempts to understand the model\u2019s prediction of potential objects, establish connections between unknown and known categories, and then provide this additional information to annotators. ", "page_idx": 0}, {"type": "image", "img_path": "9Pa6cCB3gL/tmp/f0a654257a665991e5a4c38a1be0725c9a8a3510b1a0834fc5ac898e3e7acc94.jpg", "img_caption": ["Figure 1: An illustration of our UMB and other methods. Previous OWOD methods only detected unknown objects (left), while our method further understands the model\u2019s behaviour (right). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To achieve this, we propose a novel model (UMB) that uses textual attributes to mine potential unknown objects. Specifically, we first define targets that share similar attributes with known categories but are predicted as background as potential objects. Then, to find these potential objects, we build a distribution that associates attribute similarity with the probability of positive samples, which can be seen as the empirical probability of an object possessing a particular attribute being classified as a positive sample. If a sample predicted as background has a high empirical probability and attribute similarity, we regard it as an unknown object. Finally, based on the decision-making process, we infer the most similar known classes with the unknown object and calculate the most significant impact attribute. As shown in Figure 1 (right), our method can identify the unknown and provide information about their connections with known categories and the influence of attributes on decision-making. ", "page_idx": 1}, {"type": "text", "text": "We evaluated our method on the Real-World Object Detection (RWD) benchmark composed of datasets from five practical applications, and our method achieved significant improvements. We improved almost all datasets, surpassing the OVC (GT) that uses real class names. Significantly, in the Surgery[36] dataset, we achieved the ${\\bf213\\%}$ performance in unknown category. The main contributions of this paper are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To the best of our knowledge, we are the first to notice the limitations of models on unknown predictions and attempt to understand the predictive behaviour of models. \u2022 To achieve this, we propose a new model framework (UMB) that can detect unknown categories and understand model behavior utilizing the textual description of known categories. \u2022 We model the textual attributes and the probability of positive samples to obtain the empirical probability. By combining the empirical probability, the in-distribution probability, and the out-of-distribution probability, we are able to discover unknown categories. \u2022 The evaluation results on the RWD benchmark show that our method achieved significant performance improvements, establishing a new state-of-the-art (SOTA) with $\\bar{5}.3\\ \\mathrm{mAP}$ advantages in both known and unknown category performance. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Open Vocabulary Object Detection ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Open Vocabulary Object Detection (OVD), as a subset of open-world perception, was initially introduced by OVR-CNN[8]. OVD employs the text encoder to transform classes needing detection into text embeddings, determining the current object\u2019s class by calculating the similarity between all text and visual embeddings. Subsequent works further have enhanced OVD\u2019s performance, including knowledge transfer from pre-trained models through distillation[9, 10, 11], the addition of highquality object candidates[12, 13], and alignment of text-visual regions[14, 15, 16, 17, 18]. However, in the setting of OWOD, these methods fail to detect the unknown class due to the uncertainty of object categories. Our approach, based on OWL-ViT[19], broadens OVD to OWOD by modelling the correlation between text attributes and the probability of positive samples. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Open-World Object Detection ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Open World Object Detection (OWOD), distinct from OVD, presents stricter settings and is a more challenging task, as proposed by ORE[4]. OWOD requires the detector not only to detect potential unknown objects without any information of unknown classes (including category names) but also to fine-tune the detector on newly introduced classes for continuous learning of new knowledge. Existing research focuses on heuristic assumptions for potential targets. ORE considers background samples with high objectness in RPN as potential unknowns, OW-DETR[7] calculates the average score of feature regions to determine positive samples, and PROB[20] proposes the use of Mahalanobis distance to discover the potential positive samples. Some other methods use additional pseudo-label generation mechanisms to generate annotations for potential objects, including selective search[5, 21, 22], random sample generation[23], and large model knowledge transfer [24, 25, 26]. However, these OWOD methods focus on detecting potential objects and ignore investigation into underlying reasons. Our method attempts to understand the behaviour of the model\u2019s unknown prediction, establishing a relationship between unknown objects and known classes. ", "page_idx": 2}, {"type": "text", "text": "3 Our Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our method, named UMB, is built upon OWL-ViT[19], with the overall process illustrated in Figure2. First, what characteristics should of an unknown target possess? We posit that if an object is predicted as background but exhibits attributes of known classes, it should be considered an unknown target. Therefore, we model the attributes of known classes and the probability of positive samples to build distribution of the empirical probability (Sec. 3.2). The distribution represents the detector\u2019s empirical confidence in predicting objects with known class attributes as positive samples. If a background sample\u2019s empirical confidence and similarity to known class attributes (In-Distribution Probability Sec. 3.3) are both high, we consider it a potential object. ", "page_idx": 2}, {"type": "text", "text": "Then, since predictions for known and unknown classes are based on text attributes, we can infer the most similar known class based on the attribute similarity of the unknown object (eqn. 16). Finally, we can calculate the contribution of each attribute based on the decision-making process of unknown predictions, thereby identifying the attributes that have the greatest impact on decision-making (eqn. 17). This additional information can aid in understanding the model\u2019s behaviour in unknown classes. ", "page_idx": 2}, {"type": "text", "text": "3.1 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To obtain text that describe objects, we use the following template[28, 29] to request the Large Language Model (LLM) to list all attributes related to known classes: ", "page_idx": 2}, {"type": "text", "text": "Where $C$ and $Z$ denote the class name and predefined attribute type (e.g., shape), respectively. These attributes are filled into the prompt template[27]: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP r o m p t(Z,A)\\ =\\,o b j e c t\\ w h i c h\\ (i s/h a s/e t c)\\ <Z>\\ i s\\ <A>,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $A$ is the attribute text generated in eqn. 1, e.g., blue. Then, those prompts are fed into the trained text encoder to generate text attribute embeddings $E_{a t t}=[e_{a t t_{1}},e_{a t t_{2}},...,\\stackrel{\\cdot}{e}_{a t t_{n}}]^{\\intercal}\\in\\mathbb{R}^{n\\times d}$ , where d is the hidden dimension, e.g, 512. $n$ denotes the number of the text attributes. The image is fed into the trained visual encoder to generate visual embeddings $E_{v i s}=[e_{v i s_{1}},e_{v i s_{2}},...,e_{v i s_{m}}]^{\\top}\\in\\mathbb{R}^{m\\times d}$ , where $m$ represents the number of patch. In order to establish a connection between these classagnostic attributes and known categories, we use additional weights $W\\in\\mathbb{R}^{m\\times n}$ trained in known ", "page_idx": 2}, {"type": "table", "img_path": "9Pa6cCB3gL/tmp/0492ad1f4acad21470d073db322e5409b07c635d5c8a18382fc0f843b53b83cf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overall structure of our UMB. It begins by populating prompt template with known class names and employing large language model (LLM) to generate attributes (Sec. 3.1). These attributes are then filled into template and encoded by text encoder to generate attribute embeddings $(E_{a t t})$ . We model the attributes and their corresponding positive sample probabilities to build empirical probability (Sec. 3.2). We utilize the empirical, in-distribution and out-of-distribution probability to ascertain whether an object pertains to an unknown category (Sec. 3.3). ", "page_idx": 3}, {"type": "text", "text": "categories to linearly combine similarities. Therefore, given a visual embedding $e_{v i s_{i}}$ , the probability of its corresponding known category $j$ is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p\\left({C_{j}|e_{v i s_{i}}}\\right)=S i g m o i d\\big(\\boldsymbol{w}_{j,1}\\cdot s i m\\left(\\boldsymbol{e}_{v i s_{i}},\\boldsymbol{e}_{a t t_{1}}\\right)+\\ldots+\\boldsymbol{w}_{j,n}\\cdot s i m\\left(\\boldsymbol{e}_{v i s_{i}},\\boldsymbol{e}_{a t t_{n}}\\right)\\big)}\\\\ &{\\qquad\\qquad=S i g m o i d(\\sum_{k=1}^{n}\\boldsymbol{w}_{j,k}\\cdot s i m(\\boldsymbol{e}_{v i s_{i}},\\boldsymbol{e}_{a t t_{k}})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where Sigmoid is the Sigmoid activation function, and sim denotes the Cosine Similarity. The pseudocode for known class prediction and attribute generation can be found in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "3.2 Text Attribute Modeling (TAM) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.2.1 Attribute Modeling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We model the attribute similarities in the training set with category confidence as the positive sample probability to build an empirical probability distribution. However, as shown in eqn. 3, the score is the linear combination of all attribute similarities, so it is influenced by all attributes simultaneously. Thus, we weigh confidence with linear combination weights $(W\\in\\mathbb{R}^{m\\times n})$ to balance the contributions of different attributes. Specifically, given visual embedding $e_{v i s_{k}}$ , positive sample probability of attribute $i$ for category $j$ can be represented as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{p}\\left(e_{a t t_{i}},C_{j}|e_{v i s_{k}}\\right)=w_{j,i}^{1-\\beta}\\cdot p\\left(C_{j}|e_{v i s_{k}}\\right)^{\\beta},\\enspace w_{j,i}=W[j,i]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\beta$ is a hyperparameter used to balance the contributions of weights and scores. For simplicity, we use the geometric weighted average. We incorporate all similarities in the training set and their corresponding probabilities of positive samples, establishing a mapping $f_{i,j}:s i m(e_{a t t_{i}},e_{v i s_{k}})\\rightarrow$ $\\tilde{p}\\left(e_{a t t_{i}},C_{j}|e_{v i s_{k}}\\right)$ . However, during training, the model cannot utilize the annotations of any unknown classes. Therefore, we define the positive sample probability of attribute $i$ for the unknown class as the maximum of its probability to all known classes. Specifically, the probability of attribute $i$ for the unknown class $C_{u}$ can be represented as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Tilde{p}\\left(e_{a t t_{i}},C_{u}|e_{v i s_{k}}\\right)=m a x\\left(\\Tilde{p}\\left(e_{a t t_{i}},C_{1}|e_{v i s_{k}}\\right),...,\\Tilde{p}\\left(e_{a t t_{i}},C_{m}|e_{v i s_{k}}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=a r g m a x\\left(\\Tilde{p}\\left(e_{a t t_{i}},C_{j}|e_{v i s_{k}}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad j\\in[1,m]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "9Pa6cCB3gL/tmp/97cccdf3793da5c7cc8c0bfb37391248b30b6528a96cc2b6664f1b6d58ecd1cb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: An illustration of the Probability Mixture Model. To establish a continuous probability distribution, we use linear interpolation on the original distribution (left) to estimate missing points and employ the sliding window to eliminate noise within the distribution (middle). Finally, we use the probabilistic mixture model to fit the optimized distribution (right). ", "page_idx": 4}, {"type": "text", "text": "3.2.2 Distribution Optimization and Fitting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To establish a continuous probability distribution, we need to optimize and fti the original distribution. First, contrary to the OWOD benchmark that heavily relies on extensive COCO[30] and VOC[31] data, RWD pays more attention to real-world application and is specifically designed for the few-shot setting. This results in the model not having sufficient samples to establish the probability distribution. Consequently, there are some undefined points in the mapping function $f_{i,j}$ . To mitigate this, we employ the linear interpolation to estimate the values of these missing points $\\underline{{x}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{i,j}(\\underline{{x}})=k(\\underline{{x}}-\\underline{{x}}_{l})+f_{i,j}(\\underline{{x}}_{l}),\\;\\;k=(f_{i,j}(\\underline{{x}}_{r})-f_{i,j}(\\underline{{x}}_{l}))/(\\underline{{x}}_{r}-\\underline{{x}}_{l}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where, $\\underline{{x}}_{l}$ and $\\underline{{x}}_{r}$ respectively represent the points to the left and right of $\\underline{{x}}$ that are closest in the mapping $f_{i,j}$ . ", "page_idx": 4}, {"type": "text", "text": "Then, we utilize the sliding window to fliter the noise present in the distribution. With a predetermined window size, we calculate the maximum positive sample probability across the entire window to substitute the current value: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{i,j}(s i m(e_{v i s_{k}},e_{a t t_{i}}))=\\underset{a\\in[0,W_{s z}-1]}{a r g m a x}\\ f_{i,j}(s i m(e_{v i s_{k}},e_{a t t_{i}})+a),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $W_{s z}$ denotes the window size. As depicted in Figure 3, the employment of linear interpolation and the sliding window ensures the original shape of the probability distribution remains intact, concurrently minimizing the noise inherent in the distribution. ", "page_idx": 4}, {"type": "text", "text": "Finally, as shown in Figure 3 (middle), the optimized probability distribution $f_{i,u}$ demonstrates the multi-peak characteristic. Consequently, we postulate that the original distribution is composed of multiple basic probability distributions (e.g., Gaussian Distribution). As a result, we employ the mixture probability distribution to fit the initial distribution. Specifically, we construct the model using the linear combination of multiple Gaussian distributions: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{i,u}(s i m(e_{v i s_{k}},e_{a t t_{i}}))=\\displaystyle\\sum_{a=1}^{A}G m(s i m(e_{v i s_{k}},e_{a t t_{i}})|w_{a},\\sigma_{a},\\mu_{a}),}\\\\ &{\\qquad\\quad G m(x|w,\\sigma,\\mu)=w\\cdot\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $A$ is the number of the Gaussian distribution. Additionally, we observed that certain attributes demonstrate the skewed distribution, suggesting that ftiting with the Gaussian model may not be the optimal choice. Consequently, we utilize the asymmetric Weibull distribution as a substitute for the Gaussian distribution: ", "page_idx": 4}, {"type": "equation", "text": "$$\nW b(x|w,\\lambda,k)=w\\cdot\\frac{k}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{(k-1)}e^{-(\\frac{x}{\\lambda})^{k}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In order to ascertain the parameters of these distributions, we designate them as learnable parameters and employ Mean Squared Error (MSE) as the loss function for optimization. The pseudocode for text attribute modeling can be found in Algorithm 2. ", "page_idx": 4}, {"type": "text", "text": "3.3 Unknown Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Following FOMO[27], we calculate the weighted mean of all attribute embeddings as the embedding for the unknown class: ", "page_idx": 5}, {"type": "equation", "text": "$$\ne_{a t t_{u}}=\\frac{1}{m}\\sum_{j=1}^{m}\\left(\\sum_{i=1}^{n}e_{a t t_{i}}\\cdot w_{j,i}\\right)\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Following this, we utilize the pre-trained scale layer $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{1}$ ) and $s h i f t$ layer $\\mathbb{R}^{d}\\to\\mathbb{R}^{1}$ ) for the purpose of scaling the similarity[19]: ", "page_idx": 5}, {"type": "equation", "text": "$$\nT(s i m(e_{v i s_{k}},e_{a t t_{u}}))=(s i m(e_{v i s_{k}},e_{a t t_{u}})+s h i f t(e_{v i s_{k}}))\\cdot s c a l e(e_{v i s_{k}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, we adjust the similarity of the average embedding. This adjustment is segmented into three components: empirical probability, in-distribution probability, and out-of-distribution probability. ", "page_idx": 5}, {"type": "text", "text": "Empirical Probability (Empirical Prob). For known categories, each attribute contributes unevenly to the category score (eqn. 6). Hence, for the unknown class, merely using the summation of the empirical probability to ascertain category confidence is suboptimal. We utilize the maximum weight from the known class to balance the contributions from various attributes of the unknown class. Specifically, for the visual embedding $e_{v i s_{k}}$ , its corresponding empirical probability is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{f}_{u}(e_{v i s_{k}})=\\sum_{i=1}^{n}f_{i,u}(s i m(e_{v i s_{k}},e_{a t t_{i}}))\\cdot\\bar{w}_{i},\\ \\bar{w}_{i}=a r g m a x\\ w_{j,i}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Herein, $f_{i,u}$ denotes the positive sample probability of attribute $i$ towards the unknown class, as established earlier. ", "page_idx": 5}, {"type": "text", "text": "In-Distribution Probability (ID Prob). We aspire for the model to observe the known attribute of the current object. Consequently, we incorporate the weighted sum of the scaled attribute similarities: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{\\mathrm{ID}}(e_{v i s_{k}})=\\sum_{i=1}^{n}S i g m o i d(T(e_{v i s_{k}},e_{a t t_{i}}))\\cdot\\bar{w}_{i}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Out-of-Distribution Probability (OOD Prob). Both empirical probability and in-distribution probability are based on the model\u2019s prediction on known classes. Therefore, inevitably, the model predicts high empirical probabilities and in-distribution probabilities for known categories. To counteract this, we employ out-of-distribution probability to offset their influence: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{O O D}(e_{v i s_{k}})=a r g m a x(1-S o f t m a x\\left(T(s i m(e_{v i s_{k}},e_{a t t_{i}}))\\cdot w_{j,i}\\right)).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Ultimately, given the visual embedding $e_{v i s_{k}}$ , the corresponding confidence for the unknown class is denoted as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(C_{u}|e_{v i s_{k}})=S i g m o i d((\\underbrace{(f_{u}(e_{v i s_{k}})\\cdot(1-\\alpha)}_{E m p i r i c a l\\;P r o b}+\\underbrace{f_{\\mathrm{ID}}(e_{v i s_{k}})\\cdot\\alpha}_{I D\\;P r o b})\\cdot\\underbrace{f_{O O D}(e_{v i s_{k}})}_{O O D\\;P r o b})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\cdot S i g m o i d(\\underbrace{T(s i m(e_{v i s_{k}},e_{a t t_{u}}))}_{A v e r a g e\\;S i m i l a r i t y}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha$ is used to balance the contribution form in-distribution and empirical probability. ", "page_idx": 5}, {"type": "text", "text": "3.4 Additional Information ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Similarity between known and unknown classes. Predictions for both unknown and known classes are determined by attribute similarity. Hence, we can compute its similarity with known classes based on the visual embedding of objects classified as unknown. Similar to unknown inference, we take into account the empirical probability of the current object and its confidence in being predicted as a known class. Specifically, for the visual embedding $e_{v i s_{k}}$ of objects predicted as unknown, the corresponding similarity to known classes is: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS_{u}(e_{v i s_{k}})=\\underset{j\\in[1,m]}{o o f t m a x}\\left(\\sum_{i=1}^{n}f_{i,j}(s i m(e_{v i s_{k}},e_{a t t_{i}}))+p(C_{j}|e_{v i s_{k}})\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "9Pa6cCB3gL/tmp/d6f0e139abdd2cf3d493c5d88305fc8c9896adfa8ab3d1f7332f100f84f4e334.jpg", "table_caption": [], "table_footnote": ["UMB-Gm-B 13.3 43.8 43.0 39.7 1.5 18.8 19.0 5.9 15.2 4.1 4.7 4.3 2.3 5.4 3.5 11.8 10.1 13.9 14.3 11.1 8.5 17.2 16.9 14.6 UMB-Wb-B 13.5 43.8 43.0 39.7 1.4 18.8 19.0 5.9 16.3 4.1 4.7 4.3 2.3 5.4 3.5 11.8 14.5 13.9 14.3 11.1 9.6 17.2 16.9 14.6 UMB-Gm-L 18.6 50.7 50.5 50.4 11.2 42.7 40.4 22.6 35.1 11.1 10.7 10.5 13.2 22.2 19.1 34.5 24.5 36.6 39.0 17.4 20.5 32.7 31.9 27.1 UMB-Wb-L 18.6 50.8 50.5 50.4 11.1 42.8 40.4 22.5 32.7 11.1 10.7 10.5 8.6 22.3 17.3 33.2 25.6 36.6 39.0 17.4 19.3 32.7 31.6 26.8 "], "page_idx": 6}, {"type": "text", "text": "Table 1: Comparison with previous SOTA methods on the RWD benchmark. Base+GT represents the standard OVC setting using all class names including unknown label. Base-FS indicates the baseline of fine-tuning the benchmark model with the same supervision received[27]. B and L respectively represent two different sizes of the OWL-ViT model, B/14 and L/14. U, K, PK, and CK respectively represent unknown categories, known categories, previously known categories, and currently introduced categories. Overall indicates the average performance of the model on 5 datasets. Wb and Gm respectively represent use of Weibull and Gaussian distribution during the fitting stage. ", "page_idx": 6}, {"type": "text", "text": "Maximal attribute contribution. Attributes are used to compute the similarity with visual embeddings, and then the model makes predictions based on this similarity. Therefore, the contribution of each similarity can be calculated to determine the impact of a particular attribute in the decisionmaking process. For a visual embedding $e_{v i s_{k}}$ that is predicted as an unknown class, the influence of attribute $i$ on the current decision is denoted as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nC t r(e_{a t t_{i}})=\\overline{{w}}_{i}\\cdot(S i g m o i d(T(e_{v i s_{k}},e_{a t t_{i}}))\\cdot\\alpha+f_{i,u}(s i m(e_{v i s_{k}},e_{a t t_{i}}))\\cdot(1-\\alpha))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 More Details and Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In our supplemental material, we provide detailed information about our experiments, including: comprehensive descriptions of the datasets (sec A.2), definition of OWOD (sec A.1), evaluation metrics (sec A.3), details (sec A.4), more extensive ablation studies (sec A.5), analysis and visualization of PMM training (sec A.6), similarity evaluation(sec A.7), attribute study (sec A.8), discussion of the limitations (sec A.10) and broad impact(sec A.9). ", "page_idx": 6}, {"type": "text", "text": "4.2 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The OWOD benchmark is established on the VOC[31] and COCO[30] datasets. In the era of foundation models, the zero-shot capability of detectors on such datasets has reached its limit, for instance, OWL-ViT[19] unknown recall is 79.0. Therefore, following FOMO, we have shifted the benchmark for evaluating detector performance to the more practically applicable RWD benchmark. ", "page_idx": 6}, {"type": "text", "text": "4.3 Comparison with Other State-of-the-art Models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 presents the comparison of our UMB method and previous SOTA methods established on the RWD benchmark. Overall, our method achieved comprehensive leadership, surpassing previous methods with the unknown performance advantage of $\\mathsf{4.4\\ m A P}$ (Wb-B) and $5.3\\ \\mathrm{mAP}$ (Gm-L), demonstrating the effectiveness of our method. In addition, although $\\scriptstyle{\\mathrm{Base+GT}}$ uses the name of unknown categories, it performs poorly in the Aerial, Game, Medical, and Surgery datasets. Our method does not rely on unknown class names and significantly outperforms $\\scriptstyle{\\mathrm{Base+GT}}$ (e.g., Surgery: 2.4 (Base $^{\\cdot+}$ GT-L) vs 25.6 (UMB-Wb-L)). When compared with Base-FS, which received the same supervision, FOMO did not achieve comprehensive leadership and even lagged behind by $3.6\\;\\mathrm{mAP}$ in Aquatic. Our method leads whether compared with Base-FS or FOMO, and in the Surgery dataset, we doubled the performance of FOMO (12.0 (FOMO-L) vs 25.6 (UMB-Wb-L)). Gm and Wb exhibit different strengths in various OWL-ViT models. UMB-Wb shows an advantage on the B/16 model $(+1.1\\mathrm{{\\mAP})}$ , while the trend is reversed on the L/14 base $(-1.2\\;\\mathrm{mAP})$ ). Therefore, we provide two different types of probability distributions (Gm and Wb) as interchangeable options. ", "page_idx": 6}, {"type": "table", "img_path": "9Pa6cCB3gL/tmp/1f9c7ec6b3cd79c81372f514d217b97c6f0622121a2ef4c5bb34c191266e8667.jpg", "table_caption": [], "table_footnote": ["Table 2: Ablation study of UMB on RWD. We provide incremental results of model performance. $\\mathrm{U}_{A P}$ and $\\mathrm{U}_{R E}$ represent the model\u2019s mean Average Precision (mAP) and corresponding recall rate on unknown category. Avg and Std denote the mean and variance of the recall rate distribution for unknown classes under different $\\alpha$ settings (eqn. 15). In A.5, we provide more analysis. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 2 provides the incremental results of our UMB. The initial performance uses the average category embedding (eqn. 10) and out-of-distribution probability (eqn. 14). When the in-distribution probability is introduced, which is used to capture the known attributes of the current target, the performance improves by $5.8\\ \\mathrm{mAP}.$ . However, in the Aerial dataset, $\\mathrm{U}_{A P}$ only reaches $3.8\\ \\mathrm{mAP},$ reducing by 1 mAP, replaced by a significant increase in recall rate $(+44.1)$ , which means that the detector erroneously treats many background samples as unknown objects. Such a result also proves the limitations of the unknown recall as the detection metric previously used in the OWOD benchmark. Finally, by adding the empirical distribution, our UMB achieves a comprehensive lead $\\mathrm{Gm}+5.0\\$ , Wb $+3.8)$ . In addition, the effect of balance parameter $\\alpha$ on the recall rate is not obvious. In fact, in the Aquatic and Surgery datasets, the variance of the recall rate distribution reaches 0, which means that alpha correctly suppresses the background samples erroneously predicted by the detector. Overall, our UMB can provide a higher unknown recall rate (UMB-Gm 73.4, UMB-Wb 73.8) while ensuring detection accuracy. ", "page_idx": 7}, {"type": "text", "text": "4.5 Visualization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In figure 4, we provide the qualitative analysis of FOMO and our UMB. The visual analysis is divided into three parts: the recall ability of the unknown category, recall precision, and analysis of additional information. UMB shows superior performance in recalling unknown objects. UMB successfully recalled tools in the Surgery dataset (fifth row, Wb ID 3) and accurately recalled playgrounds and roofs in the Aerial dataset (second row, Wb ID 1, 2), while FOMO failed to recall these objects. Regarding recall precision, FOMO predicts multiple results to an object and incorrectly classifies unknown classified objects as known classes, such as the hero characters in the Game dataset (third row) and the misclassification of four objects in the Aquatic dataset (first row). In contrast, our UMB shows higher precision. Regarding additional information, the misclassification of FOMO in the Aquatic dataset reflects the defects of OWL-ViT in classifying these objects. With the help of formula n, UMB can infer the category most similar to the current object, and these categories correspond to the categories misclassified by FOMO. In addition, for the same object (ID 3,4), UMB identifies that the attribute with the greatest impact on the entire decision is consistent. These results prove the accuracy of our method in inferring the connection between unknown and known and discovering the attributes that have the greatest impact on decision-making. ", "page_idx": 7}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This paper attempts to understand the detector\u2019s behaviour in predicting unknown objects. To achieve this, we propose a novel detection framework, UMB, which employs class-agnostic textual attributes to unearth potential objects in the background. Given that the model\u2019s detection process for known and unknown classes hinges on textual attributes, our UMB can use the textual attributes of unknown objects to infer the most similar known category. In addition, we can calculate the attributes that have the most significant impact on the entire decision-making process. This supplementary information aids annotators in understanding the model\u2019s behaviour in predicting unknowns. We hope that UMB can promote the application of Open-World Object Detection in real-world scenarios. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "9Pa6cCB3gL/tmp/673c9fb3d0df991ddbcf39d7cb1b6b65946a4040229a103c66254b9b123db9e6.jpg", "img_caption": ["Figure 4: Qualitative Analysis. Each row, from left to right, represents: FOMO, UMB-Wb, and UMB-Gm, respectively. From top to bottom, the results are given for Aquatic, Aerial, Game, Medical, and Surgery. For fairness and clarity, we only display the TOP-K unknown predictions with a confidence level greater than 0.5. Unknown predictions are marked in Red, while known classes are marked in yellow. Each table provides the most similar known category (Category) for each unknown prediction, and the attribute (Attribute Text) that has the greatest impact on the decision-making process. In section 7, we provide an evaluation of the accuracy rate of similarity prediction. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by TCL Science and Technology Innovation Fund (Project No.   \n20231752). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: A simple and strong anchor-free object detector. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(4):1922\u20131933, 2020.   \n[2] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using programmable gradient information. arXiv preprint arXiv:2402.13616, 2024.   \n[3] Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang, Jinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, and Yi Liu. Detrs beat yolos on real-time object detection. arXiv preprint arXiv:2304.08069, 2023.   \n[4] KJ Joseph, Salman Khan, Fahad Shahbaz Khan, and Vineeth N Balasubramanian. Towards open world object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5830\u20135840, 2021.   \n[5] Shuailei Ma, Yuefeng Wang, Ying Wei, Jiaqi Fan, Thomas H Li, Hongli Liu, and Fanbing Lv. Cat: Localization and identification cascade detection transformer for open-world object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19681\u201319690, 2023.   \n[6] Xiaowei Zhao, Yuqing Ma, Duorui Wang, Yifan Shen, Yixuan Qiao, and Xianglong Liu. Revisiting open world object detection. IEEE Transactions on Circuits and Systems for Video Technology, 2023.   \n[7] Akshita Gupta, Sanath Narayan, KJ Joseph, Salman Khan, Fahad Shahbaz Khan, and Mubarak Shah. Ow-detr: Open-world detection transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9235\u20139244, 2022.   \n[8] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14393\u201314402, 2021.   \n[9] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.   \n[10] Luting Wang, Yi Liu, Penghui Du, Zihan Ding, Yue Liao, Qiaosong Qi, Biaolong Chen, and Si Liu. Object-aware distillation pyramid for open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11186\u201311196, 2023.   \n[11] Zongyang Ma, Guan Luo, Jin Gao, Liang Li, Yuxin Chen, Shaoru Wang, Congxuan Zhang, and Weiming Hu. Open-vocabulary one-stage detection with hierarchical visual-language knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14074\u201314083, 2022.   \n[12] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. Aligning bag of regions for open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15254\u201315264, 2023.   \n[13] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Open-vocabulary detr with conditional matching. In European Conference on Computer Vision, pages 106\u2013122. Springer, 2022.   \n[14] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li. Learning to prompt for open-vocabulary object detection with vision-language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14084\u201314093, 2022.   \n[15] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16793\u201316803, 2022.   \n[16] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, and Hang Xu. Detclipv2: Scalable open-vocabulary object detection pre-training via word-region alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23497\u201323506, 2023.   \n[17] Dahun Kim, Anelia Angelova, and Weicheng Kuo. Region-aware pretraining for openvocabulary object detection with vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11144\u201311154, 2023.   \n[18] Xiaoshi Wu, Feng Zhu, Rui Zhao, and Hongsheng Li. Cora: Adapting clip for open-vocabulary detection with region prompting and anchor pre-matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7031\u20137040, 2023.   \n[19] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection. In European Conference on Computer Vision, pages 728\u2013755. Springer, 2022.   \n[20] Orr Zohar, Kuan-Chieh Wang, and Serena Yeung. Prob: Probabilistic objectness for open world object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11444\u201311453, 2023.   \n[21] Zhiheng Wu, Yue Lu, Xingyu Chen, Zhengxing Wu, Liwen Kang, and Junzhi Yu. Uc-owod: Unknown-classified open world object detection. In European Conference on Computer Vision, pages 193\u2013210. Springer, 2022.   \n[22] Xiaowei Zhao, Yuqing Ma, Duorui Wang, Yifan Shen, Yixuan Qiao, and Xianglong Liu. Revisiting open world object detection. IEEE Transactions on Circuits and Systems for Video Technology, 2023.   \n[23] Yanghao Wang, Zhongqi Yue, Xian-Sheng Hua, and Hanwang Zhang. Random boxes are open-world object detectors. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6233\u20136243, 2023.   \n[24] Yulin He, Wei Chen, Yusong Tan, and Siqi Wang. Usd: Unknown sensitive detector empowered by decoupled objectness and segment anything model. arXiv preprint arXiv:2306.02275, 2023.   \n[25] Ruohuan Fang, Guansong Pang, Lei Zhou, Xiao Bai, and Jin Zheng. Unsupervised recognition of unknown objects for open-world object detection. arXiv preprint arXiv:2308.16527, 2023.   \n[26] Shuailei Ma, Yuefeng Wang, Ying Wei, Peihao Chen, Zhixiang Ye, Jiaqi Fan, Enming Zhang, and Thomas H Li. Detecting the open-world objects with the help of the brain. arXiv preprint arXiv:2303.11623, 2023.   \n[27] Orr Zohar, Alejandro Lozano, Shelly Goel, Serena Yeung, and Kuan-Chieh Wang. Open world object detection in the era of foundation models. arXiv preprint arXiv:2312.05745, 2023.   \n[28] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. arXiv preprint arXiv:2210.07183, 2022.   \n[29] Orr Zohar, Shih-Cheng Huang, Kuan-Chieh Wang, and Serena Yeung. Lovm: Language-only vision model selection. Advances in Neural Information Processing Systems, 36, 2024.   \n[30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[31] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes challenge: A retrospective. International journal of computer vision, 111:98\u2013136, 2015.   \n[32] Roboflow 100. aquarium dataset. https://universe.roboflow.com/roboflow-100/ aquarium-qlnqy, may 2023. visited on 2024-04-09.   \n[33] Ke Li, Gang Wan, Gong Cheng, Liqiu Meng, and Junwei Han. Object detection in optical remote sensing images: A survey and a new benchmark. ISPRS journal of photogrammetry and remote sensing, 159:296\u2013307, 2020.   \n[34] Roboflow 100. team fight tactics dataset. https://universe.roboflow.com/ roboflow-100/team-fight-tactics, may 2023. visited on 2024-04-09.   \n[35] Roboflow 100. x-ray rheumatology dataset. https://universe.roboflow.com/ roboflow-100/x-ray-rheumatology, may 2023. visited on 2024-04-09.   \n[36] David Bouget, Rodrigo Benenson, Mohamed Omran, Laurent Riffaud, Bernt Schiele, and Pierre Jannin. Detecting surgical tools by modelling local appearance and global shape. IEEE transactions on medical imaging, 34(12):2603\u20132617, 2015.   \n[37] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8430\u20138439, 2019.   \n[38] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32\u201373, 2017.   \nAlgorithm 1: Textual Attribute Generation   \nand Known Class Prediction   \n$/*$ Predefined attribute types \\*/   \n$A=\\mathrm{[Size}$ , Shape, Behavior, . . , Appearance]   \n$/*$ known class names (Aquatic) $^{*}\\!/$   \n$C=$ [Jellyfish, Penguin, ..., Shark, Starfish]   \nAttributes $=[]$   \n$/*$ attribute generation guided by LLM \\*/   \nfor $c_{i}$ in $C$ do for $a_{i}$ in $A$ do $1\\ast$ fill in the template (eqn. 1) \\*/ $/*$ e.g. Penguin Size, Template(Penguin, $\\mathrm{Size})=\\mathrm{I}$ am using a language-vision model to identify {Penguin}. List the {Size} attributes of {Penguin}, which will be used for detection. $^{\\ast}/$ Templat $?(c_{i},a_{j})\\to L L M$ $/*$ generate attributes, e.g, blue $^{*}\\!/$ $L L M\\rightarrow a t t r i b u t e$ $/*$ collect attributes and predictions corresponding to $\\mathrm{LLM^{*}}/$ $(a_{j},a t t r i b u t e)\\rightarrow A t t r i b u t e s$ end   \nend   \n$/*$ encoding textual attribute to embedding \\*/   \n$E_{a t t}=[]$   \nfor $a_{i}$ , attributei in Attributes do $/*$ fill in the template (eqn. 2) $^{\\ast}/$ $/*$ e.g. blue, Prompt(color, blue) $=$ object which (is/has/etc) <color> is <blue>\\*/ $P r o m p t(a_{i},a t t r i b u t e_{i})\\rightarrow$ Text_encoder $\\rightarrow E_{a t t}$   \nend   \n$/*$ encoding image to visual embedding $^{\\ast}/$   \n$E_{v i s}=[]$   \nfor patch_img in image do V ision_encoder(patch_img) \u2192Evis   \nend   \n$/*$ attribute similarity $^{*}\\!/$   \n$S i m s=[]$   \nfor $e_{v i s_{k}}$ in $E_{v i s}$ do $s i m\\ddot{s}_{i}=[]$ for $e_{a t t_{i}}$ in $E_{a t t}$ do $/*$ sim denotes the cosine similarity $^{*}\\!/$ $s i m(e_{v i s_{k}},e_{a t t_{i}})\\to s i m s_{i}$ end $s i m s_{i}\\to S i m s$   \nend   \n$/*$ known class prediction $^{\\ast}/$   \noutput $=[]$   \nfor $i d x$ , $s i m s_{i}$ in Sims do $/*$ trained attribute weight $W^{*}/$ $W[i d x]\\cdot s i m s_{i}\\underset{e q n.3}{\\rightarrow}\\overset{\\cdot}{o u t p u t}$   \nend   \nreturn output   \nend   \n/\\*eqn. $5\\!\\ast\\!/$   \nfor $x$ in $R a n g e(-1,1,g a p)$ do for $e_{a t t_{i}}$ in $E_{a t t}$ do $M a x\\_v a l=0$ for $C_{j}$ in Known_classes do for $v a l_{i}$ in $f[C_{j}][e_{a t t_{i}}]$ do if $v a l_{i}$ in $[x,x+g a p]$ then $m a x(v a l_{i},M a x\\_v a l)\\rightarrow$ Max_val end end end $(x,M a x\\textunderscore v a l)\\to f[C_{u}][e_{a t t_{i}}]$ end   \nend   \n$/*$ eqn. 6, here we set gap to 0.0001 \\*/   \nfor $x$ in Range $:(-1,1,g a p)$ do for $e_{a t t_{i}}$ in $E_{a t t}$ do if $(x,0)$ in $f[C_{u}][e_{a t t_{i}}]$ then $f[C_{u}][e_{a t t_{i}}]$ del $(x,0)$ Linear Interpolation \u2192 $(x,e s t i m a t e)\\to f[C_{u}][e_{a t t_{i}}]$ end end   \n$/*$ eqn. $7\\ast1$   \nfor $x$ in $R a n g e(-1,1,g a p)$ do for $e_{a t t_{i}}$ in $E_{a t t}$ do f[Cu][eatti] \u2190 Sliding Window filter ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "9Pa6cCB3gL/tmp/9c29193ba92a244bac578b168e72bf9e60300e90d5c74d17ebb721a5f7090537.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A.1 Task Formulation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the context of OWOD, the detection task is divided into a series of subtasks $T=\\{T_{1},T_{2},...,T_{|T|}\\}$ and their corresponding categories $K=\\{K_{1},K_{2},...,K_{|T|}\\}$ . $T_{i}$ includes all known categories from previous tasks and introduces new categories on this basis: $K_{i}=(\\bigcup_{j=1}^{i-1}K_{j})\\cup K_{n e w}$ , where $K_{n e w}$ denotes introduced new categories. When the model is trained on $T_{i}$ , our expectation is that the model should be able to detect all categories it has encountered so far (i.e., $K_{i}$ ), as well as discover those unlabelled but interesting categories. For the purpose of evaluation, the interest object is defined as those that belong to $K$ but not to $K_{i}$ (i.e., $K-K_{i})$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 Datasets ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The OWOD benchmark is a combination of COCO[30] and VOC[31] datasets. In the era of foundation models, the zero-shot capabilities of detectors on OWOD benchmark have even reached their limits. Therefore, consistent with FOMO[27], we will switch the evaluation benchmark to RWD. The RWD benchmark consists of five typical application scenarios for object detection, including underwater scenes, representing visual blurring caused by the environment (Aquatic[32]); aerial scenes, where the targets are small and difficult to distinguish (Aerial[33]); scenarios using synthetic data when data is lacking (Game[34]); medical $\\Chi$ -ray scenes, where it is difficult to distinguish between categories and professional knowledge is required(Medical[35]); and human surgery scenes, where the field of view is blurred by blood (Surgery[36]). The detailed division of the RWD benchmark is shown in Table 3. We divide RWD into two subtasks according to a $50\\%$ category ratio. When training in Task 1, all categories in the test set that belong to Task 2 are treated as unknown classes, and when training in Task 2, the categories of Task 1 are considered as previously seen classes. ", "page_idx": 13}, {"type": "text", "text": "A.3 Metric ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For known categories, we adopt the widely used mean Average Precision (mAP) as the evaluation metric for object detection. For unknown classes, previous OWOD methods[5, 4] used the recall rate of unknown classes as the evaluation metric. However, such a metric leads to models greedily treating all background objects as potential samples. Therefore, we adopt mAP, consistent with the evaluation metric for known classes, which simultaneously assesses the detector\u2019s recall ability for unknown classes and the precision of the predictions. ", "page_idx": 13}, {"type": "text", "text": "A.4 Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "All experiments were conducted using a single NVIDIA GeForce RTX 4090 GPU. Following FOMO, we initialized with the frozen OWL-ViT[19] (L/14 and B/16), which was trained on a mixed dataset composed of Object 365[37] and Visual Genome[38], demonstrating strong generalization capabilities. The large language model used for attribute generation is GPT-3.5. These attributes were matched with all predictions in the dataset, and the corresponding visual embeddings were collected if the IOU exceeded the threshold (0.8). The average of these visual embeddings was calculated to obtain the average embedding of the attributes. Following FOMO, we adopted attribute selection, attribute adaptation, and attribute refinement to train the linear combination weight. ", "page_idx": 13}, {"type": "text", "text": "All optimizers used AdamW. During the attribute selection phase, BCE was the loss function, and the learning rate remained constant without decreasing with iterations. The attribute selection phase reduced the number of attributes. Based on the ranking of weights after training, only the top 25 attributes per attribute type were retained. Attribute adaptation was used to narrow the distance between the text attributes and the average embedding of the dataset. This phase used MSE as the loss function, with a maximum of 1000 iterations. Attribute refinement took the text embedding as the parameter to be optimized, with BCE as the loss function. Attribute refinement narrowed the distance between the text embedding and the visual embedding. During the attribute selection and attribute refinement phases, the learning rate and maximum number of iterations for training were set to three values ([1e-5, 5e-5, 1e-4], [1, 10, 100]), iterating over these settings during each training to select the optimal setting. ", "page_idx": 13}, {"type": "table", "img_path": "9Pa6cCB3gL/tmp/f43bdbea7c7682c29e20bee5aab2ec1be19fd3eefc5b88019ba1a5a94b838901.jpg", "table_caption": [], "table_footnote": ["Table 3: Detailed explanation of the dataset split. Each dataset is split into two subtasks, each maintaining a category proportion of $50\\%$ . When training on Task 1, the categories of Task 2 are treated as unknown classes. During training in Task 2, the classes from Task 1 are labeled as previously seen categories, and new classes divided into Task 2 are introduced. The numbers (num1, num2, num3) following each category in Task 1 represent the number of training instances in Task 1, Task 2, and the test set, respectively. The numbers (num1, num2) in Task 2 represent the number of training and test instances for this category in Task 2. "], "page_idx": 14}, {"type": "text", "text": "Upon completion of training, the detector could detect known classes. To detect unknowns, we established the empirical probability for each attribute. In the distribution optimization phase, we set the window value to 10. In the distribution fitting phase, we used two different probability models (Gaussian and Weibull). During training, Adam was used as the optimizer, the learning rate was set to 0.01, the maximum number of iterations was 10000, and the maximum number of probability models was set to 5. Since the purpose of ftiting was to capture the shape of the empirical probability distribution and establish a continuous probability distribution, we set an interval of 0.0001 between -1 and 1. This setting was used to sparsify the data of the original distribution. Then, distribution optimization and fitting were performed in the sparsified distribution. ", "page_idx": 15}, {"type": "text", "text": "A.5 Comprehensive Ablation Experiments ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "9Pa6cCB3gL/tmp/d51f4949a5353a2f88ecc0e6a21fd079db20d796e79ebcb0a6601903a024a995.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 5: Subfigures 1 through 5 present the performance variations of the model under different settings of $\\alpha$ (eqn. 15). For clarity, we only display the performance changes corresponding to the optimal $\\beta$ (eqn. 4). Subfigure 6 provides the statistics of the recall rates corresponding to these five figures as $\\alpha$ varies. Here, Mean represents the average, while SE denotes the standard deviation. ", "page_idx": 15}, {"type": "text", "text": "Figure 5 illustrates the model\u2019s performance under different $\\alpha$ settings with the optimal $\\beta$ . In addition, subfigure 6 provides the statistics of the recall rates corresponding to subfigures 1 through 5. In the Aquatic and Game datasets, the performance of UMB is not sensitive to the changes in $\\alpha$ , showing minor performance differences. This is because Aquatic is consistent with the training data of OWL-ViT, and Game is synthetic data, neither of which poses additional challenges (such as target size). However, in the remaining datasets, the performance of UMB shows significant changes. For example, the performance of the Aerial dataset drops from $11.1\\;\\mathrm{mAP}$ (wb) to $3.6\\;\\mathrm{mAP}.$ These datasets have significant differences from the OWL-ViT training dataset, and their environmental characteristics (such as small objects in Aerial, similar objects in Medical, and blood-contaminated backgrounds in Surgery) pose additional challenges to the model. These constraints make UMB sensitive to changes in $\\alpha$ . Nevertheless, a reasonable $\\alpha$ can balance in-distribution and empirical probability contributions to achieve better detection performance. ", "page_idx": 15}, {"type": "text", "text": "For the recall rate of unknown categories, except for the Aerial and Medical datasets, UMB maintains a high value ${}_{93.3\\%}$ in Aquatic) and remains stable in the remaining datasets. This indicates that our method does not predict more objects under different $\\alpha$ settings but predicts potential objects more accurately. In the Aerial and Medical datasets, the model\u2019s recall rate is almost half that of other datasets, and these recall rates show significant fluctuations with the change of $\\alpha$ . Therefore, we infer that when the model\u2019s recall rate for a specific dataset is high, the balance between intra-division ", "page_idx": 15}, {"type": "image", "img_path": "9Pa6cCB3gL/tmp/1410544f82e9db960958d593fba891ac0070303bf1f649be97117490abfb81c6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 6: Training result for the Probabilistic Mixture Model. The training results for Aquatic, Aerial, Game, Medical, and Surgery are represented from top to bottom. We only display the ftiting process corresponding to the optimal $\\beta$ setting for clarity. The settings for $\\beta$ can be found in figure 5. Each original distribution is first filtered for noise using distribution optimization (linear interpolation and sliding window) and then fitted with the basic probability model (Weibull or Gaussian) to the optimized distribution. The first and third images in each row show the MSE loss distribution corresponding to convergence, while the second and fourth images present the best-fitting results across all attributes. In the best-ftiting result, the yellow line represents the original distribution, and the green line represents the result after fitting the probability model. ", "page_idx": 16}, {"type": "text", "text": "probability and empirical probability has a relatively small impact on the recall rate; when the recall rate is low, this balance has a more significant impact on the recall rate. ", "page_idx": 16}, {"type": "text", "text": "A.6 Probabilistic Mixture Model ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure 6 presents the ftiting results of the probabilistic mixture model. When the original probability distribution exhibits multiple peaks, the MSE loss of the probability ftiting may stabilize at a relatively high value. For instance, in the first column of the second row, the original probability distribution is composed of more than three elemental probability distributions. Upon completion of training, the MSE value stabilizes around 7.3, indicating that the model faces challenges when ftiting distributions with multiple peaks. However, when the original probability distribution exhibits a single peak, the MSE loss approaches zero, demonstrating the model\u2019s advantage in ftiting unimodal distributions. For example, in the Game dataset, the MSE value of the Gaussian model stabilizes around 0.015. Despite the possibility of multiple peaks in the original distribution, both the Weibull and Gaussian distribution can capture its essential shape characteristics. For instance, in the Aerial dataset, the Gaussian model can fit two smooth peaks to represent the original distribution. Similarly, the Weibull model can fit a smooth curve when facing distributions with multiple peak features. However, in the case of unimodal distribution, both the Gaussian and Weibull models can capture the original distribution\u2019s unimodal feature and fti a smooth curve with a single peak. For example, in the Surgery dataset, the Weibull and Gaussian models in the Game datasets demonstrate this ability. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A.7 Similarity Evaluation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In order to understand the behavior of the model, we provide the known class most similar to the unknown object and the attribute with the most significant impact on decisionmaking. Through visualization, we observe that many known predictions overlap with unknown objects in the detector\u2019s predictions. These known predictions represent that the detector considers the current object to belong to a known category. Therefore, we estimate the model\u2019s accuracy in similar inference by ", "page_idx": 17}, {"type": "image", "img_path": "9Pa6cCB3gL/tmp/a83fab8b377d8af30151e06a7e6d695a7e9183c8e510e3c791703cbf395e8af0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 7: Evaluation of similarity. The left figure represents the number of predictions where unknown and known categories overlap. The right figure shows the accuracy of the model\u2019s inference among these numbers. ", "page_idx": 17}, {"type": "text", "text": "whether the inferred known class is consistent with these overlapping known predictions. Specifically, if their IoU exceeds 0.95, we consider these two predictions overlapping. Then, if the known class inferred by the detector is consistent with the known prediction, we consider the model inference correct. Otherwise, it indicates an inference error. As shown in Figure 7 (left), the detector has many duplicate predictions for unknown and known classes. Among these duplicate predictions, the proportion of correctly inferred predictions is large, more than $80\\%$ (Figure 7). Compared with Wb, Gm\u2019s accuracy rate is always lower. In the Aquatic dataset, Gm lags behind Wb by 0.1 percentage points in accuracy, and in Medical, it lags by 0.6 percentage points. Overall, both Gm and Wb show high inference accuracy (about $90\\%$ ), and compared with $\\mathrm{Gm}$ , Wb shows a higher accuracy rate (about $2\\%$ ). ", "page_idx": 17}, {"type": "text", "text": "A.8 Attribute Study ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Figure 8 and Figure 9 present the results of attribute analysis. For clarity, we have selected only the top three categories with the highest prediction counts from the detector and the top five attributes that have the most significant impact on them. In each row, the top part shows the prediction counts for these categories. For instance, Puffin (252) in Aquatic indicates that the detector gave 252 predictions for the Puffin category. The left side shows the number of times these attributes have been identified as having the most significant impact. Moreover, the right side represents the attributes with the highest average inference scores from the model. ", "page_idx": 17}, {"type": "text", "text": "UMB demonstrates a strong ability to capture object attributes. In the Surgery dataset, the attribute (Behavior is repositioning) dominated UMB-Wb\u2019s 127 predictions (total 274) for the Hook category, and this number rose to 134 (total 273) in UMB-Gm, accounting for nearly half. This implies that when an object exhibits such an attribute, the detector will likely predict it as the Hook category. UMB exhibits precise attribute discrimination capabilities. Since the right side of each row presents the average score of the attribute\u2019s impact on all detector predictions, their differences are insignificant. Nevertheless, in the Aquatic dataset, UMB-Gm still distinguished these attributes. For instance, the ", "page_idx": 17}, {"type": "image", "img_path": "9Pa6cCB3gL/tmp/b94df50f3b101bf33ddbbf703dc703731f0a4b917f78048049cf0c983c6b9344.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "9Pa6cCB3gL/tmp/5b378bafa927c7892949109853866eef934ec19f354e1b1eafc19a897e969225.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 8: Cross-category attribute analysis (Gm). Each row from top to bottom presents the results for Aquatic, Aerial, Game, Surgery, and Medical. The left in each row represents the number of times the attribute influences the decision, while the right side indicates the average score of the attribute. ", "page_idx": 18}, {"type": "text", "text": "", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "9Pa6cCB3gL/tmp/6ac5eee73d58f954bc3794d5de053fd233eb11acbdf5ca7ec5fe5af96c079937.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 9: Cross-category attribute analysis (Wb). Each row from top to bottom presents the results for Aquatic, Aerial, Game, Surgery, and Medical. The left in each row represents the number of times the attribute influences the decision, while the right side indicates the average score of the attribute. ", "page_idx": 19}, {"type": "text", "text": "attribute (Context is the deep sea) had an average score of 0.2 in Puff but dropped to 0 in Starfish. A similar scenario occurred in UMB-Wb, where the attribute (Shape is flexible) had an impact of 0.16 on Starfish, but it dropped to 0 for Puffin. Overall, both UMB-Gm and UMB-Wb can capture and distinguish object attributes and make corresponding predictions based on these attributes. ", "page_idx": 20}, {"type": "text", "text": "We conducted additional comparative experiments on Linear Interpolation (LI) and Sliding Window (SW), as detailed in Table 4 of the attached document. The results indicate that employing LI or SW independently does not lead to significant performance improvements. Both methods exhibit only marginal enhancements compared to the original approach, suggesting that LI and SW, when used in isolation, are insufficient for accurately modeling the data distribution. ", "page_idx": 20}, {"type": "text", "text": "A.9 Broad Impact ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this paper, we focus on the performance of detectors in open-world object detection and attempt to under", "page_idx": 20}, {"type": "table", "img_path": "9Pa6cCB3gL/tmp/4748c5795bc28a862079c6c385fefb97a99916b27543cb60464a928d082e9fcd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 4: Ablation Studies. ME is Mean Embedding, OOD is Out-of-Distribution. PMM(OD) uses original distribution, LI, SW use Linear Interpolation, Sliding Window. Gm, Wb are flitered models, denoting Gaussian, Weibull distribution. ", "page_idx": 20}, {"type": "text", "text": "stand the model\u2019s behavior when predicting unknown categories. Our approach can help annotators gain a deep understanding of the model\u2019s decision-making process, thereby guiding subsequent optimization work and improving the overall performance of the detector. At the same time, understanding the model\u2019s behavior may expose potential flaws malicious actors could exploit for illegal activities. For this reason, we choose to open-source our code, both to promote the development of the current field and to identify and prevent these potential issues through the power of the community. ", "page_idx": 20}, {"type": "text", "text": "A.10 Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our focus is on understanding the behaviour of model predictions. Hence, we attempt to migrate the OVC detector to the OWOD task. As our method does not directly train the weights of the OVC detector but merely processes its output, the performance ceiling of our method will be constrained by the inherent performance of the OVC detector itself. Furthermore, due to the visual-text alignment relationships of the OVC requiring extensive data training, fine-tuning on actual application datasets could lead to additional annotation costs. ", "page_idx": 20}, {"type": "text", "text": "A.11 Failure Cases ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We present typical cases of detection failures from each dataset, focusing on recall capability and detection accuracy. For instance, in the Aquatic dataset, UMB failed to detect small orange fish, while in the Aerial dataset, it did not successfully recall vehicles. These instances reveal the detector\u2019s shortcomings in recall capability. Moreover, in the Game and Surgery datasets, UMB displayed occurrences of repeated predictions. Nevertheless, UMB still outperformed FOMO in overall performance. Specifically, in the Aquatic dataset, UMB accurately located the contours of the fish, whereas FOMO showed deviations in contour localization and even missed similar objects. Furthermore, FOMO incorrectly identified the reflection of the photographer\u2019s shoes in the glass as an unknown object, demonstrating lower precision. Similar issues were observed in the Aerial and Game datasets, where FOMO often confused objects with the background, resulting in new erroneous predictions, such as misidentifying rooftops as a single object in the Aerial dataset. However, UMB did not commit the same errors in these cases. In summary, although UMB also exhibited some false detections in certain scenarios, it outperformed FOMO in both detection accuracy and the ability to recall potential objects. ", "page_idx": 20}, {"type": "image", "img_path": "9Pa6cCB3gL/tmp/e01b6c046aede6d8be41ed4d4c90fd6c2500cacbe08487994983275572717409.jpg", "img_caption": ["Figure 10: Qualitative Results (Failure Cases). From top to bottom, the datasets are Aquatic, Aerial, Game, Medical, and Surgery. To ensure clarity and fairness in comparison, we only display predictions for unknown, selecting those with a confidence score greater than 0.5 and ranked within the top K unknown category predictions. The results indicate that the UMB method demonstrates higher precision and recall in addressing the FOMO problem. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In the introduction, we discuss the problems that existing research focuses on (first paragraph of the introduction), as well as the emphasis and shortcomings of current studies (second paragraph of the introduction). We also detail our methods and the results we have achieved (third and fourth paragraphs of the introduction). ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: In section A.10, we provide the limitation. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In section 3.2, we provide the full set of assumption. In section 4.4, we discuss the impact of empirical probability on model performance. Additionally, in section A.6, we present the detailed analysis and visualization. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In the supplemental material, we provide detailed experimental configuration (A.4) and pseudocode (1, 2), environments, and all other settings. Furthermore, we will release all the training code and weights. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our code is available at https://anonymous.4open.science/r/UMB-B61C/. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In the supplemental material, we provide all the training and test details, including: experimental configuration (A.4), data split (A.2) and hyperparameters (A.5). ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We provide the experiments that support the main claims of the paper (sec 4.3) and detailed ablation study (sec A.5). ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: In the supplemental material (sec A.4), we provide the experiment compute resource consumption. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We affirm that all research presented in this paper adheres to the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: In section A.9, we provide the societal impacts of the work performed. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All assets in the paper have been properly credited, and we adhere to the corresponding licenses and terms of use. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our code is available at https://github.com/xxyzll/UMB. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]