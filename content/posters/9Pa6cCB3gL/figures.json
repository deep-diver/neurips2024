[{"figure_path": "9Pa6cCB3gL/figures/figures_1_1.jpg", "caption": "Figure 1: An illustration of our UMB and other methods. Previous OWOD methods only detected unknown objects (left), while our method further understands the model's behaviour (right).", "description": "This figure compares the proposed method UMB with other existing Open-World Object Detection (OWOD) methods. The left side shows that traditional OWOD methods only identify unknown objects without explaining the model's reasoning behind the classification.  The right side illustrates UMB, which not only detects unknown objects but also provides insights into why the model classified them as unknown,  showing the model's decision-making process and connecting the unknown object to known classes based on attribute similarity.  This additional information helps annotators understand the model's predictions better.", "section": "1 Introduction"}, {"figure_path": "9Pa6cCB3gL/figures/figures_4_1.jpg", "caption": "Figure 3: An illustration of the Probability Mixture Model. To establish a continuous probability distribution, we use linear interpolation on the original distribution (left) to estimate missing points and employ the sliding window to eliminate noise within the distribution (middle). Finally, we use the probabilistic mixture model to fit the optimized distribution (right).", "description": "This figure illustrates the process of creating a continuous probability distribution using three methods: 1. Linear Interpolation on the original distribution to estimate missing points. 2. Sliding window to smooth out noise in the distribution. 3. Probabilistic Mixture Model to fit the optimized distribution.  The result is a more accurate and continuous probability distribution, suitable for use in the model.", "section": "3.2 Text Attribute Modeling (TAM)"}, {"figure_path": "9Pa6cCB3gL/figures/figures_8_1.jpg", "caption": "Figure 2: Overall structure of our UMB. It begins by populating prompt template with known class names and employing large language model (LLM) to generate attributes (Sec. 3.1). These attributes are then filled into template and encoded by text encoder to generate attribute embeddings (Eatt). We model the attributes and their corresponding positive sample probabilities to build empirical probability (Sec. 3.2). We utilize the empirical, in-distribution and out-of-distribution probability to ascertain whether an object pertains to an unknown category (Sec. 3.3).", "description": "This figure shows the overall architecture of the proposed model, UMB, illustrating the different stages involved in open-world object detection. It starts with generating attributes using an LLM, then building an empirical probability distribution, and finally using a multimodal probabilistic distribution mixture model to infer whether an object belongs to an unknown category or not.", "section": "3 Our Approach"}, {"figure_path": "9Pa6cCB3gL/figures/figures_12_1.jpg", "caption": "Figure 2: Overall structure of our UMB. It begins by populating prompt template with known class names and employing large language model (LLM) to generate attributes (Sec. 3.1). These attributes are then filled into template and encoded by text encoder to generate attribute embeddings (Eatt). We model the attributes and their corresponding positive sample probabilities to build empirical probability (Sec. 3.2). We utilize the empirical, in-distribution and out-of-distribution probability to ascertain whether an object pertains to an unknown category (Sec. 3.3).", "description": "This figure illustrates the overall architecture of the proposed model UMB, highlighting the different stages of processing involved in detecting unknown objects and understanding the model behavior. The process starts with generating textual attributes using an LLM, followed by modeling these attributes and probabilities to identify potential unknown objects. Finally, a decision-making process determines the category of the object.", "section": "3 Our Approach"}, {"figure_path": "9Pa6cCB3gL/figures/figures_15_1.jpg", "caption": "Figure 2: Overall structure of our UMB. It begins by populating prompt template with known class names and employing large language model (LLM) to generate attributes (Sec. 3.1). These attributes are then filled into template and encoded by text encoder to generate attribute embeddings (Eatt). We model the attributes and their corresponding positive sample probabilities to build empirical probability (Sec. 3.2). We utilize the empirical, in-distribution and out-of-distribution probability to ascertain whether an object pertains to an unknown category (Sec. 3.3).", "description": "This figure illustrates the overall architecture of the proposed UMB model.  It shows the flow of information, starting with attribute generation using an LLM, through attribute and visual embedding, empirical probability calculation, and finally, the inference process for determining whether an object belongs to a known or unknown category.  Each stage is clearly depicted in the diagram, highlighting the model's key components and their interactions.", "section": "3 Our Approach"}, {"figure_path": "9Pa6cCB3gL/figures/figures_16_1.jpg", "caption": "Figure 3: An illustration of the Probability Mixture Model. To establish a continuous probability distribution, we use linear interpolation on the original distribution (left) to estimate missing points and employ the sliding window to eliminate noise within the distribution (middle). Finally, we use the probabilistic mixture model to fit the optimized distribution (right).", "description": "This figure illustrates the process of creating a continuous probability distribution using three steps: 1. Linear interpolation to estimate missing values; 2. Sliding window to smooth the distribution and remove noise; 3. Probabilistic mixture model to fit the optimized distribution, showing how it starts with an original distribution and through several steps (linear interpolation, sliding window, and mixture model fitting) becomes a continuous and optimized probability distribution. ", "section": "3.2 Text Attribute Modeling (TAM)"}, {"figure_path": "9Pa6cCB3gL/figures/figures_17_1.jpg", "caption": "Figure 2: Overall structure of our UMB. It begins by populating prompt template with known class names and employing large language model (LLM) to generate attributes (Sec. 3.1). These attributes are then filled into template and encoded by text encoder to generate attribute embeddings (Eatt). We model the attributes and their corresponding positive sample probabilities to build empirical probability (Sec. 3.2). We utilize the empirical, in-distribution and out-of-distribution probability to ascertain whether an object pertains to an unknown category (Sec. 3.3).", "description": "This figure shows the overall architecture of the UMB model proposed in the paper. It illustrates the process of generating attributes using LLM, building empirical probability distribution, and finally inferring unknown objects based on the combined probabilities. The figure shows the flow of information and the different components of the model, including the text and visual encoders, the probabilistic mixture model, and the unknown inference module.", "section": "3 Our Approach"}, {"figure_path": "9Pa6cCB3gL/figures/figures_18_1.jpg", "caption": "Figure 2: Overall structure of our UMB. It begins by populating prompt template with known class names and employing large language model (LLM) to generate attributes (Sec. 3.1). These attributes are then filled into template and encoded by text encoder to generate attribute embeddings (Eatt). We model the attributes and their corresponding positive sample probabilities to build empirical probability (Sec. 3.2). We utilize the empirical, in-distribution and out-of-distribution probability to ascertain whether an object pertains to an unknown category (Sec. 3.3).", "description": "This figure illustrates the overall architecture of the proposed model, UMB, which consists of three main components: Attribute Generation, Text Attribute Modeling (TAM), and Unknown Inference.  The Attribute Generation component uses a Large Language Model (LLM) to generate textual attributes for known classes. TAM models these attributes along with their positive sample probabilities to construct an empirical probability distribution. Finally, the Unknown Inference component uses this distribution, along with in-distribution and out-of-distribution probabilities to identify whether an object belongs to an unknown category. ", "section": "3 Our Approach"}, {"figure_path": "9Pa6cCB3gL/figures/figures_18_2.jpg", "caption": "Figure 2: Overall structure of our UMB. It begins by populating prompt template with known class names and employing large language model (LLM) to generate attributes (Sec. 3.1). These attributes are then filled into template and encoded by text encoder to generate attribute embeddings (Eatt). We model the attributes and their corresponding positive sample probabilities to build empirical probability (Sec. 3.2). We utilize the empirical, in-distribution and out-of-distribution probability to ascertain whether an object pertains to an unknown category (Sec. 3.3).", "description": "This figure presents a detailed overview of the proposed UMB model's architecture. It illustrates the workflow, starting from attribute generation using an LLM, followed by text and visual encoding, to the final unknown object inference using a multimodal probabilistic distribution mixture model. The model incorporates empirical probability, in-distribution probability, and out-of-distribution probability to make a decision on whether an object belongs to an unknown category.", "section": "3 Our Approach"}, {"figure_path": "9Pa6cCB3gL/figures/figures_19_1.jpg", "caption": "Figure 2: Overall structure of our UMB. It begins by populating prompt template with known class names and employing large language model (LLM) to generate attributes (Sec. 3.1). These attributes are then filled into template and encoded by text encoder to generate attribute embeddings (Eatt). We model the attributes and their corresponding positive sample probabilities to build empirical probability (Sec. 3.2). We utilize the empirical, in-distribution and out-of-distribution probability to ascertain whether an object pertains to an unknown category (Sec. 3.3).", "description": "This figure illustrates the overall architecture of the proposed model UMB. It shows the flow of processing from attribute generation using an LLM to the final prediction of whether an object belongs to a known or unknown category. The key components are attribute modeling, multimodal probabilistic distribution mixture model, and unknown inference.", "section": "3 Our Approach"}, {"figure_path": "9Pa6cCB3gL/figures/figures_21_1.jpg", "caption": "Figure 2: Overall structure of our UMB. It begins by populating prompt template with known class names and employing large language model (LLM) to generate attributes (Sec. 3.1). These attributes are then filled into template and encoded by text encoder to generate attribute embeddings (Eatt). We model the attributes and their corresponding positive sample probabilities to build empirical probability (Sec. 3.2). We utilize the empirical, in-distribution and out-of-distribution probability to ascertain whether an object pertains to an unknown category (Sec. 3.3).", "description": "This figure illustrates the overall architecture of the UMB model.  It shows the flow of data through different modules, starting with attribute generation using an LLM, through text and visual encodings, to the final decision of whether an object is known or unknown based on probability distributions. The figure highlights the key components and their interactions in the model.", "section": "3 Our Approach"}]