[{"figure_path": "GnAfyR8AhC/tables/tables_6_1.jpg", "caption": "Table 1: The best case values of two terms of RHS (ID Omin and ID ECE) and LHS \u2013 OOD errors (MSE and ECE) in the bounds of Theorem 3. Reported values are an average of three repeated runs.", "description": "This table presents the best values obtained for the two components of the RHS (right-hand side) of the inequality derived in Theorem 3 of the paper: the ID calibration error (ECE) and the reciprocal of the smallest singular value of the ID input covariance matrix (\u03c3min).  It also displays the corresponding LHS (left-hand side) values representing the OOD (out-of-distribution) errors in terms of Mean Squared Error (MSE) and Expected Calibration Error (ECE). The results showcase the improvements in OOD performance by reducing the upper bound as defined in the theorem.  Note that higher \u03c3min and lower ECE values are desirable.  The results are averages over three repetitions of the experiments.", "section": "5.1 Numerical analysis on error bounds"}, {"figure_path": "GnAfyR8AhC/tables/tables_6_2.jpg", "caption": "Table 2: ImageNet accuracy. We report the accuracy on ImageNet and its distribution shift variants by fine-tuning CLIP ViT-B/16 with five methods. The best and the second-best in each column are underlined.", "description": "This table shows the accuracy results on ImageNet and five of its distribution shift variants (ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch, and ObjectNet).  The accuracy is reported for five different fine-tuning methods: Zero-Shot (ZS), standard Fine-Tuning (FT), LP-FT, FLYP, and Lipsum-FT.  The best and second-best performing methods for each dataset are underlined.", "section": "5.2 Evaluation on distribution shift benchmarks"}, {"figure_path": "GnAfyR8AhC/tables/tables_6_3.jpg", "caption": "Table 3: ImageNet ECE. Along with Table 2, we report the ECE on ImageNet and its distribution shifts to compare with other fine-tuning methods, which demonstrates our out-of-distribution (OOD) calibration performance. The best and the second-best in each column are underlined (See Figure B for details).", "description": "This table compares different fine-tuning methods on their expected calibration error (ECE) across various ImageNet datasets (including the original ImageNet-1K and several out-of-distribution datasets). Lower ECE values indicate better calibration.  The best and second-best performing methods are highlighted for each dataset.", "section": "5.2 Evaluation on distribution shift benchmarks"}, {"figure_path": "GnAfyR8AhC/tables/tables_8_1.jpg", "caption": "Table 4: Ablation study on CaRot components. We report accuracy and ECE on ImageNet (ID) and its distribution shifts (OOD). OOD values are averaged over five shifts. Values in brackets indicate the performance difference compared to the first row of each sub-table, and the dark green highlights the positive improvement.", "description": "This table presents the ablation study results for the CaRot model. It shows the impact of each component (LMCL, LOC, and LSD) on the model's performance in terms of accuracy and expected calibration error (ECE) on both in-distribution (ImageNet) and out-of-distribution datasets.  The results are presented to highlight the contribution of each component to the overall improvement in OOD generalization and calibration.", "section": "5 Experiments"}, {"figure_path": "GnAfyR8AhC/tables/tables_8_2.jpg", "caption": "Table 5. Analysis on coefficient terms of CaRot objective. Along with Table 4, we report fine-grained analysis results on each term. We set \u03bb<sub>oc</sub> as 0.2 and \u03bb<sub>SD</sub> as 1.5 when ablating each other and for all experiments throughout the paper. We select the final values of \u03bb<sub>oc</sub> and \u03bb<sub>SD</sub> based on ID ECE and \u03c3<sub>min</sub>(D<sub>ID</sub>), respectively.", "description": "This table presents the ablation study results of the hyperparameters associated with the CaRot objective function.  It shows how varying the strength coefficients \u03bb<sub>oc</sub> (for the orthogonality constraint) and \u03bb<sub>SD</sub> (for self-distillation) impact the performance in terms of accuracy (Acc.) and expected calibration error (ECE) on both the in-distribution (ID) and out-of-distribution (OOD) datasets. The final values of these hyperparameters (\u03bb<sub>oc</sub> = 0.2 and \u03bb<sub>SD</sub> = 1.5) were chosen based on the ID ECE and the minimum singular value of the ID data covariance matrix.", "section": "5.3 Further empirical studies"}, {"figure_path": "GnAfyR8AhC/tables/tables_8_3.jpg", "caption": "Table 6: ImageNet Acc. (except ObjectNet) with additional baselines.", "description": "This table presents the ImageNet classification accuracy results for various methods, including zero-shot (ZS), fine-tuning (FT), and several robust fine-tuning approaches (LP-FT, FLYP, Lipsum-FT, CAR-FT, Model Stock, ARF), and the proposed CaRot method. The accuracy is evaluated on the ImageNet dataset and its five distribution shift variants (IN-V2, IN-R, IN-A, IN-S, ObjectNet). The average accuracy across all six datasets is also reported.  The table highlights CaRot's superior performance compared to existing methods in achieving robust generalization under distribution shifts.", "section": "5.2 Evaluation on distribution shift benchmarks"}, {"figure_path": "GnAfyR8AhC/tables/tables_9_1.jpg", "caption": "Table 7: ImageNet accuracy and ECE on different backbones. We provide summarized results on CLIP RN50 and ViT-L/14. The best and the second-best in each column are underlined. (See Table H and I for details.)", "description": "This table presents a summary of the ImageNet accuracy and ECE (Expected Calibration Error) results obtained using two different backbones: ResNet50 and ViT-L/14.  It compares the performance of several fine-tuning methods (Zero-Shot, Fine-tuning, LP-FT, FLYP, and CaRot) across various metrics, highlighting the best and second-best results for each metric on both backbones.", "section": "5.2 Evaluation on distribution shift benchmarks"}, {"figure_path": "GnAfyR8AhC/tables/tables_18_1.jpg", "caption": "Table A: Comparison between SVD-based regularization and the orthogonality constraint term. Both terms are effective in terms of OOD generalization and calibration, but SVD requires a much heavier computation.", "description": "This table compares the performance of two different regularization techniques: SVD-based regularization and orthogonality constraint. Both methods aim to improve out-of-distribution (OOD) generalization and calibration. The table shows that while both methods achieve similar improvements, the SVD-based approach is computationally much more expensive.  The comparison is made across several metrics, including accuracy and expected calibration error (ECE), for both in-distribution (ID) and OOD data.  The ID-OOD Gap columns highlight the differences between ID and OOD performance.", "section": "B.1 Comparing approaches"}, {"figure_path": "GnAfyR8AhC/tables/tables_18_2.jpg", "caption": "Table 2: ImageNet accuracy. We report the accuracy on ImageNet and its distribution shift variants by fine-tuning CLIP ViT-B/16 with five methods. The best and the second-best in each column are underlined.", "description": "This table presents the ImageNet top-1 accuracy results for different fine-tuning methods across various distribution shift benchmarks. The methods compared are Zero-Shot (ZS), Fine-tuning (FT), LP-FT, FLYP, and the proposed CaRot method.  The benchmarks include the original ImageNet dataset (IN) and five distribution shifts: IN-V2, IN-R, IN-A, IN-S, and ObjectNet. The table highlights the best and second-best performing methods for each benchmark, offering a clear comparison of the methods' generalization capabilities under different distribution shifts.", "section": "5.2 Evaluation on distribution shift benchmarks"}, {"figure_path": "GnAfyR8AhC/tables/tables_19_1.jpg", "caption": "Table 3: ImageNet ECE. Along with Table 2, we report the ECE on ImageNet and its distribution shifts to compare with other fine-tuning methods, which demonstrates our out-of-distribution (OOD) calibration performance. The best and the second-best in each column are underlined (See Figure B for details).", "description": "This table compares the Expected Calibration Error (ECE) of different fine-tuning methods on ImageNet and five of its distribution shift variants (ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch, and ObjectNet).  Lower ECE values indicate better calibration, meaning the model's confidence scores are closer to its actual accuracy. The table highlights the superior calibration performance of the proposed CaRot method, especially in out-of-distribution settings.", "section": "5.2 Evaluation on distribution shift benchmarks"}, {"figure_path": "GnAfyR8AhC/tables/tables_19_2.jpg", "caption": "Table 3: ImageNet ECE. Along with Table 2, we report the ECE on ImageNet and its distribution shifts to compare with other fine-tuning methods, which demonstrates our out-of-distribution (OOD) calibration performance. The best and the second-best in each column are underlined (See Figure B for details).", "description": "This table presents the Expected Calibration Error (ECE) for different fine-tuning methods on ImageNet and five of its distribution shift variants (ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch, and ObjectNet).  Lower ECE values indicate better calibration.  The table allows a comparison of the calibration performance of the proposed method (CaRot) against existing state-of-the-art methods.", "section": "5.2 Evaluation on distribution shift benchmarks"}, {"figure_path": "GnAfyR8AhC/tables/tables_19_3.jpg", "caption": "Table 5: Analysis on coefficient terms of CaRot objective. Along with Table 4, we report fine-grained analysis results on each term. We set \u03bb<sub>loc</sub> as 0.2 and \u03bb<sub>SD</sub> as 1.5 when ablating each other and for all experiments throughout the paper. We select the final values of \u03bb<sub>loc</sub> and \u03bb<sub>SD</sub> based on ID ECE and \u03c3<sub>min</sub>(D<sub>ID</sub>), respectively.", "description": "This table presents a fine-grained ablation study on the hyperparameters \u03bb<sub>loc</sub> and \u03bb<sub>SD</sub> of the CaRot objective function.  It shows the impact of varying these parameters on the model's performance in terms of accuracy and expected calibration error (ECE) on both in-distribution (ID) and out-of-distribution (OOD) datasets. The final values of \u03bb<sub>loc</sub> and \u03bb<sub>SD</sub> used in the main experiments were determined based on the ID ECE and the minimum singular value (\u03c3<sub>min</sub>(D<sub>ID</sub>)) of the ID data.", "section": "5.2 Evaluation on distribution shift benchmarks"}, {"figure_path": "GnAfyR8AhC/tables/tables_19_4.jpg", "caption": "Table 4: Ablation study on CaRot components. We report accuracy and ECE on ImageNet (ID) and its distribution shifts (OOD). OOD values are averaged over five shifts. Values in brackets indicate the performance difference compared to the first row of each sub-table, and the dark green highlights the positive improvement.", "description": "This table presents the ablation study results for the CaRot model.  It shows the impact of each component (LMCL, LOC, LSD) on the model's performance in terms of accuracy and Expected Calibration Error (ECE) on both in-distribution (ID) ImageNet and out-of-distribution (OOD) datasets. The results demonstrate the contribution of each component to improving the overall performance.", "section": "5.3 Further empirical studies"}, {"figure_path": "GnAfyR8AhC/tables/tables_20_1.jpg", "caption": "Table 4: Ablation study on CaRot components. We report accuracy and ECE on ImageNet (ID) and its distribution shifts (OOD). OOD values are averaged over five shifts. Values in brackets indicate the performance difference compared to the first row of each sub-table, and the dark green highlights the positive improvement.", "description": "This ablation study analyzes the impact of each component of the proposed CaRot model on ImageNet and five out-of-distribution (OOD) datasets.  It shows the individual contributions of the constrained multimodal contrastive loss (LMCL-con), the orthogonality constraint (LOC), and the exponential moving average self-distillation (LSD).  The table presents accuracy and expected calibration error (ECE) for both ID and OOD data, highlighting the positive effects of each component and their combined impact on model performance.", "section": "5.3 Further empirical studies"}, {"figure_path": "GnAfyR8AhC/tables/tables_20_2.jpg", "caption": "Table 2: ImageNet accuracy. We report the accuracy on ImageNet and its distribution shift variants by fine-tuning CLIP ViT-B/16 with five methods. The best and the second-best in each column are underlined.", "description": "This table presents the ImageNet top-1 accuracy and average accuracy across five ImageNet distribution shift benchmarks (ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch, and ObjectNet) for five different fine-tuning methods: zero-shot (ZS), standard fine-tuning (FT), LP-FT, FLYP, and Lipsum-FT.  The best and second-best performing methods for each dataset are highlighted.", "section": "5.2 Evaluation on distribution shift benchmarks"}, {"figure_path": "GnAfyR8AhC/tables/tables_20_3.jpg", "caption": "Table 2: ImageNet accuracy. We report the accuracy on ImageNet and its distribution shift variants by fine-tuning CLIP ViT-B/16 with five methods. The best and the second-best in each column are underlined.", "description": "This table presents the ImageNet top-1 accuracy and average accuracy across five distribution shift benchmarks (ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch, and ObjectNet) for five different methods: zero-shot (ZS), fine-tuning (FT), LP-FT, FLYP, and CaRot (the proposed method).  The best performing method for each dataset is underlined.", "section": "5.2 Evaluation on distribution shift benchmarks"}]