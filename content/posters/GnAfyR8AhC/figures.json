[{"figure_path": "GnAfyR8AhC/figures/figures_1_1.jpg", "caption": "Figure 1: OOD accuracy vs. ID accuracy (left) and negative OOD ECE (right). To maintain consistency in the plots, where desired values are shown on the right side of the x-axis, we report negative OOD ECE. ID ACC refers to ImageNet-1K top-1 accuracy; OOD ACC and ECE refer to the averaged accuracy and ECE of the five ImageNet distribution shifts (ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch, and ObjectNet), respectively. Detailed numbers are reported in Table 2 and 3. Note that the competing methods \u2013 FLYP [17], LP-FT [30], and Lipsum-FT [42] \u2013 improve OOD accuracy over the zero-shot baseline (ZS) and naive fine-tuning (FT) but suffer from OOD miscalibration, presumably due to concerning generalization solely during fine-tuning. Our CaRot outperforms existing methods on both OOD accuracy and calibration by large margins.", "description": "This figure compares various fine-tuning methods (FLYP, LP-FT, Lipsum-FT, and the proposed CaRot) against zero-shot (ZS) and standard fine-tuning (FT) baselines.  The left panel shows OOD accuracy plotted against ID accuracy. The right panel shows negative OOD expected calibration error (ECE).  The results demonstrate that while existing methods improve OOD accuracy, they suffer from poor calibration.  In contrast, CaRot achieves superior performance in both OOD accuracy and calibration.", "section": "1 Introduction"}, {"figure_path": "GnAfyR8AhC/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of CaRot. We fine-tune a VLM using a multimodal contrastive loss with an orthogonality constraint on visual projection layer (eq.(4)) and self-distillation LSD (eq.(5)) that takes predictions of EMA teacher \u03c8 as soft target labels to train the student model \u03b8. The darker and the lighter elements denote values closer to 1 and 0, respectively. Both teacher and student models share identical VLM architecture consisting of image f\u03b8v := [f\u03b8\u0302; Wv] and text g\u03b8t := [g\u03b8\u0302\u2081; Wt] encoders, where W is the last projection layer. Given (image, text) pair data, the model outputs the pair-wise similarity score for in-batch image-text representations.", "description": "This figure illustrates the proposed CaRot framework, which fine-tunes a Vision-Language Model (VLM) using a novel multimodal contrastive loss with an orthogonality constraint and self-distillation.  The diagram shows the interaction between the student and teacher models, emphasizing the use of soft labels derived from the teacher's predictions for self-distillation, and a constraint on the visual projection matrix. The overall process aims to enhance confidence calibration and accuracy by increasing the smallest singular value of the ID input covariance matrix while improving ID calibration.", "section": "4 Method"}, {"figure_path": "GnAfyR8AhC/figures/figures_6_1.jpg", "caption": "Figure 3: Analysis of error bounds on synthetic data. Plots on the left side show RHS (x-axis) and LHS (y-axis; MSE for ineq.(2) and ECE for ineq.(1)) of the inequalities in \u00a73. We denote MSE for the mean squared error, Loc for the singular value regularization, and LSD for the calibration regularization.", "description": "This figure empirically validates the theoretical error bounds presented in section 3 of the paper.  The left plot shows the relationship between the right-hand side (RHS) of inequality (2) (OOD MSE) and the average of the reciprocal of the minimum singular value of the ID data's covariance matrix and the ID ECE.  The right plot shows the same for inequality (1) (OOD ECE). The strong negative correlation supports the theory that minimizing the RHS can reduce the OOD errors.  The plots show that reducing ID calibration error and increasing the minimum singular value of the ID input covariance matrix (which is a measure of the diversity of the input features) results in lower OOD classification and calibration errors.", "section": "5.1 Numerical analysis on error bounds"}, {"figure_path": "GnAfyR8AhC/figures/figures_7_1.jpg", "caption": "Figure 4: IN-C corruption-wise accuracy (top) and ECE (bottom). We evaluate accuracy and ECE over 15 types of image corruption with five corruption severity and report the average performance per corruption. CaRot consistently outperforms baseline methods across diverse corruptions.", "description": "This figure shows the performance comparison of different fine-tuning methods (ZS, FT, LP-FT, FLYP, CaRot) on ImageNet-C dataset. ImageNet-C is a corrupted version of ImageNet dataset with 15 types of corruptions and 5 severity levels.  The top part shows the accuracy for each corruption type, while the bottom shows the Expected Calibration Error (ECE). The results demonstrate that CaRot consistently outperforms other methods across various corruptions.", "section": "5.2 Evaluation on distribution shift benchmarks"}, {"figure_path": "GnAfyR8AhC/figures/figures_7_2.jpg", "caption": "Figure 5: Closer look at the effectiveness of CaRot on different corruptions. We provide IN-C accuracy on brightness (left) and elastic transform (right) corruptions. CaRot excels on the coarser corruption such as brightness whereas its effectiveness is weakened on the finer corruption such as elastic transform.", "description": "This figure shows box plots of accuracy for different fine-tuning methods (ZS, FT, LP-FT, FLYP, CaRot) on two specific corruptions from the ImageNet-C dataset: brightness and elastic_transform.  It highlights that CaRot's performance improvement over baselines is more pronounced for coarser corruptions (brightness) compared to finer-grained corruptions (elastic_transform).", "section": "5.2 Evaluation on distribution shift benchmarks"}, {"figure_path": "GnAfyR8AhC/figures/figures_8_1.jpg", "caption": "Figure 6: Impact of LMCL-con. Analysis on singular values. Figure 6 illustrates the last 20 singular values of the covariance matrix \\(\\mathbf{I}^T \\mathbf{I}\\) where \\(\\mathbf{I}\\) is a standardized image representations over \\(N\\) samples. Our proposed constrained contrastive loss \\(\\mathcal{L}_{\\text{MCL-con}}\\) increases the small singular values compared to the vanilla contrastive loss \\(\\mathcal{L}_{\\text{MCL}}\\). This result verifies that adding the orthogonality constraint successfully reduces \\(1/\\sigma_{\\text{min}}(\\mathbf{D}_{\\text{ID}})\\), the component of the shared upper bound we derived in \u00a73, following our intention.", "description": "This figure shows the impact of using the constrained multi-modal contrastive loss (LMCL-con) on the smallest 20 singular values of the image representation covariance matrix.  The LMCL-con loss increases these singular values compared to the unconstrained LMCL loss, which supports the theoretical finding that increasing the smallest singular value helps to improve out-of-distribution generalization.", "section": "5. Experiments"}, {"figure_path": "GnAfyR8AhC/figures/figures_15_1.jpg", "caption": "Figure A. Two-dimensional illustration of the experimental setup for numerical analyses. Note that the actual number of dimensions used for the experiments is set to 1000.", "description": "This figure is a 2D illustration of how the synthetic datasets are created for the numerical analysis in section 5.1.  The ID datasets have two features (x1 and x2) correlated with the labels, while OOD data is generated by manipulating these features to simulate a shift in distribution. Specifically, for x1, the mean is shifted, and for x2, the scale is reduced in the OOD data.  The figure helps visualize the covariate shift involved in the experiments.", "section": "A.1 Details for numerical analysis on error bounds"}, {"figure_path": "GnAfyR8AhC/figures/figures_18_1.jpg", "caption": "Figure 1: OOD accuracy vs. ID accuracy (left) and negative OOD ECE (right). To maintain consistency in the plots, where desired values are shown on the right side of the x-axis, we report negative OOD ECE. ID ACC refers to ImageNet-1K top-1 accuracy; OOD ACC and ECE refer to the averaged accuracy and ECE of the five ImageNet distribution shifts (ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch, and ObjectNet), respectively. Detailed numbers are reported in Table 2 and 3. Note that the competing methods \u2013 FLYP [17], LP-FT [30], and Lipsum-FT [42] \u2013 improve OOD accuracy over the zero-shot baseline (ZS) and naive fine-tuning (FT) but suffer from OOD miscalibration, presumably due to concerning generalization solely during fine-tuning. Our CaRot outperforms existing methods on both OOD accuracy and calibration by large margins.", "description": "The figure compares different robust fine-tuning methods on their out-of-distribution (OOD) accuracy and expected calibration error (ECE).  It shows that while some methods improve OOD accuracy, they often suffer from poor calibration. The proposed method, CaRot, achieves both high OOD accuracy and good calibration.", "section": "1 Introduction"}]