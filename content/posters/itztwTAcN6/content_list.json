[{"type": "text", "text": "A Universal Growth Rate for Learning with Smooth Surrogate Losses ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anqi Mao Mehryar Mohri Yutao Zhong Courant Institute Google Research & CIMS Courant Institute New York, NY 10012 New York, NY 10011 New York, NY 10012 aqmao@cims.nyu.edu mohri@google.com yutao@cims.nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper presents a comprehensive analysis of the growth rate of $\\mathcal{H}$ -consistency bounds (and excess error bounds) for various surrogate losses used in classification. We prove a square-root growth rate near zero for smooth margin-based surrogate losses in binary classification, providing both upper and lower bounds under mild assumptions. This result also translates to excess error bounds. Our lower bound requires weaker conditions than those in previous work for excess error bounds, and our upper bound is entirely novel. Moreover, we extend this analysis to multi-class classification with a series of novel results, demonstrating a universal square-root growth rate for smooth comp-sum and constrained losses, covering common choices for training neural networks in multi-class classification. Given this universal rate, we turn to the question of choosing among different surrogate losses. We first examine how $\\mathcal{H}$ -consistency bounds vary across surrogates based on the number of classes. Next, ignoring constants and focusing on behavior near zero, we identify minimizability gaps as the key differentiating factor in these bounds. Thus, we thoroughly analyze these gaps, to guide surrogate loss selection, covering: comparisons across different comp-sum losses, conditions where gaps become zero, and general conditions leading to small gaps. Additionally, we demonstrate the key role of minimizability gaps in comparing excess error bounds and $\\mathcal{H}$ -consistency bounds. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning algorithms frequently optimize surrogate loss functions like the logistic loss, in lieu of the task\u2019s true objective, commonly the zero-one loss. This is necessary when the original loss function is computationally intractable to optimize or lacks essential mathematical properties such as differentiability. But, what guarantees can we rely on when minimizing a surrogate loss? This is a fundamental question with significant implications for learning. ", "page_idx": 0}, {"type": "text", "text": "The related property of Bayes-consistency of surrogate losses has been extensively studied in the context of binary classification. Zhang [2004a], Bartlett et al. [2006] and Steinwart [2007] established Bayes-consistency for various convex loss functions, including margin-based surrogates. They also introduced excess error bounds (or surrogate regret bounds) for margin-based surrogates. Reid and Williamson [2009] extended these results to proper losses in binary classification. ", "page_idx": 0}, {"type": "text", "text": "The Bayes-consistency of several surrogate loss function families in the context of multi-class classification has also been studied by Zhang [2004b] and Tewari and Bartlett [2007]. Zhang [2004b] established a series of results for various multi-class classification formulations, including negative results for multi-class hinge loss functions [Crammer and Singer, 2001], as well as positive results for the sum exponential loss [Weston and Watkins, 1999, Awasthi et al., 2022a], the (multinomial) logistic loss [Verhulst, 1838, 1845, Berkson, 1944, 1951], and the constrained losses [Lee et al., 2004]. ", "page_idx": 0}, {"type": "text", "text": "Later, Tewari and Bartlett [2007] adopted a different geometric method to analyze Bayes-consistency, yielding similar results for these loss function families. Steinwart [2007] developed general tools to characterize Bayes consistency for both binary and multi-class classification. Additionally, excess error bounds have been derived by Pires et al. [2013] for a family of constrained losses and by Duchi et al. [2018] for loss functions related to generalized entropies. ", "page_idx": 1}, {"type": "text", "text": "For a surrogate loss $\\ell$ , an excess error bound holds for any predictor $h$ and has the form $\\mathcal{E}_{\\ell_{0-1}}(h)-$ $\\mathcal{E}_{\\ell_{0-1}}^{*}\\!\\leq\\Psi\\big(\\bar{\\mathcal{E}_{\\ell}}(h)-\\mathcal{E}_{\\ell}^{*}\\big)$ , where $\\mathcal{E}_{\\ell_{0-1}}(h)$ and $\\mathcal{E}_{\\ell}(h)$ represent the expected losses of $h$ for the zero-one loss and surrogate loss respectively, $\\mathcal{E}_{\\ell_{0-1}}^{*}$ and $\\mathcal{E}_{\\ell}^{*}$ the Bayes errors for the zero-one and surrogate loss respectively, and $\\Psi$ a non-decreasing function. The growth rate of excess error bounds, that is the behavior of function $\\Psi$ near zero, has gained attention in recent research [Mahdavi et al., 2014, Zhang et al., 2021, Frongillo and Waggoner, 2021, Bao, 2023]. Mahdavi et al. [2014] examined the growth rate for smoothed hinge losses in binary classification, demonstrating that smoother losses result in worse growth rates. The optimal rate is achieved with the standard hinge loss, which exhibits linear growth. Zhang et al. [2021] tied the growth rate of excess error bounds in binary classification to two properties of the surrogate loss function: consistency intensity and conductivity. These metrics enable comparisons of growth rates across different surrogates. This prompts a natural question: can we establish rigorous lower and upper bounds for excess error growth rates under specific regularity conditions? ", "page_idx": 1}, {"type": "text", "text": "Frongillo and Waggoner [2021] pioneered research on this question in binary classification settings. They established a critical square-root lower bound for excess error bounds when a surrogate loss is locally strongly convex and has a locally Lipschitz gradient. Additionally, they demonstrated a linear excess error bound for Bayes-consistent polyhedral loss functions (convex and piecewise-linear) [Finocchiaro et al., 2019] (see also [Lapin et al., 2016, Ramaswamy et al., 2018, Yu and Blaschko, 2018, Yang and Koyejo, 2020]). More recently, Bao [2023] complemented these results by showing that proper losses associated with Shannon entropy, exponential entropy, spherical entropy, squared $\\alpha$ -norm entropies and $\\alpha$ -polynomial entropies, with $\\alpha>1$ , also exhibit a square-root lower bound for excess error bounds relative to the $\\ell_{1}$ -distance. ", "page_idx": 1}, {"type": "text", "text": "However, while Bayes-consistency and excess error bounds are valuable, they are not sufficiently informative, as they are established for the family of all measurable functions and disregard the crucial role played by restricted hypothesis sets in learning. As pointed out by Long and Servedio [2013], in some cases, minimizing Bayes-consistent losses can result in constant expected error, while minimizing inconsistent losses can yield an expected loss approaching zero. To address this limitation, the authors introduced the concept of realizable $\\mathcal{H}$ -consistency, further explored by Kuznetsov et al. [2014] and Zhang and Agarwal [2020]. Nonetheless, these guarantees are only asymptotic and rely on a strong realizability assumption that typically does not hold in practice. ", "page_idx": 1}, {"type": "text", "text": "Recent research by Awasthi, Mao, Mohri, and Zhong [2022b,a] and Mao, Mohri, and Zhong [2023f,c,e,b] has instead introduced and analyzed $\\mathcal{H}$ -consistency bounds. These bounds are more informative than Bayes-consistency since they are hypothesis set-specific and non-asymptotic. Their work covers broad families of surrogate losses in binary classication, multi-class classification, structured prediction, and abstention [Mao, Mohri, Mohri, and Zhong, 2023a]. Crucially, they provide upper bounds on the estimation error of the target loss, for example, the zero-one loss in classification, that hold for any predictor $h$ within a hypothesis set $\\mathcal{H}$ . These bounds relate this estimation error to the surrogate loss estimation error. ", "page_idx": 1}, {"type": "text", "text": "Their general form is: $\\mathcal{E}_{\\ell_{0-1}}(h)-\\mathcal{E}_{\\ell_{0-1}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{0-1}}(\\mathcal{H})\\,\\le\\,\\Gamma\\bigl(\\mathcal{E}_{\\ell}(h)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell}(\\mathcal{H})\\bigr)$ , where $\\mathcal{E}_{\\ell_{0-1}}^{*}(\\mathcal{H})$ and $\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})$ represent the best-in-class expected losses for the zero-one and surrogate loss respectively, $\\Gamma$ is a non-negative concave function and $\\mathcal{M}_{\\ell_{0-1}}(\\mathcal{H})$ and $\\mathcal{M}_{\\ell}(\\mathcal{H})$ are minimizability $g a p s$ . The exact definition of these gaps will be detailed later. For now, let us mention that they are non-negative quantities, upper-bounded by the approximation error of their respective loss functions. $\\mathcal{H}$ -consistency bounds subsume excess error bounds as a special case when the hypothesis set is expanded to include all measurable functions, in which case the minimizability gaps vanish. More generally, an $\\mathcal{H}$ -consistency bound with a $\\Gamma$ function implies $\\mathcal{E}_{\\ell_{0-1}}(h)-\\mathcal{E}_{\\ell_{0-1}}^{*}(\\bar{\\mathcal{H}})+\\mathcal{M}_{\\ell_{0-1}}(\\mathcal{H})\\leq$ $\\Gamma(\\mathcal{E}_{\\ell}(h)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{H}))+\\Gamma(\\mathcal{M}_{\\ell}(\\mathcal{H}))$ since a concave function $\\Gamma$ with $\\Gamma(0)\\geq0$ is sub-additive over $\\mathbb{R}_{+}$ . Thus, when the surrogate estimation loss $\\mathcal{E}_{\\ell}(h)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})$ is minimized to $\\epsilon$ , the zero-one estimation error $\\mathcal{E}_{\\ell_{0-1}}(h)-\\mathcal{E}_{\\ell_{0-1}}^{*}(\\mathcal{H})$ is bounded by $\\Gamma(\\epsilon)+\\Gamma(\\mathcal{M}_{\\ell}(\\mathcal{H}))-\\mathcal{M}_{\\ell_{0-1}}(\\mathcal{H})$ . Can we characterize the growth rate of $\\mathcal{H}$ -consistency bounds, that is how quickly the functions $\\Gamma$ increase near zero? ", "page_idx": 1}, {"type": "text", "text": "Our results. This paper presents a comprehensive analysis of the growth rate of $\\mathcal{H}$ -consistency bounds for all margin-based surrogate losses in binary classification, as well as for comp-sum losses and constrained losses in multi-class classification. We establish a square-root growth rate near zero for margin-based surrogate losses $\\ell$ defined by $\\ell(h,x,y)\\,=\\,\\Phi(-y h(x))$ , assuming only that $\\Phi$ is convex and twice continuously differentiable with $\\Phi^{\\prime}(0)\\!>\\!0$ and $\\Phi^{\\prime\\prime}(0)\\!>\\!0$ (Section 4). This includes both upper and lower bounds (Theorem 4.2). These results directly apply to excess error bounds as well. Importantly, our lower bound requires weaker conditions than [Frongillo and Waggoner, 2021, Theorem 4], and our upper bound is entirely novel. This work demonstrates that the $\\mathcal{H}$ -consistency bound growth rate for these loss functions is precisely square-root, refining the \u201cat least square-root\u201d finding of these authors (for excess error bounds). It is known that polyhedral losses admit a linear grow rate [Frongillo and Waggoner, 2021]. Thus, a striking dichotomy emerges that reflects previous observations by these authors: $\\mathcal{H}$ -consistency bounds for polyhedral losses exhibit a linear growth rate in binary classification, while they follow a square-root rate for smooth loss functions. ", "page_idx": 2}, {"type": "text", "text": "Moreover, we significantly extend our findings to key multi-class surrogate loss families, including comp-sum losses [Mao et al., 2023f] (e.g., logistic loss or cross-entropy with softmax [Berkson, 1944], sum-losses [Weston and Watkins, 1999], generalized cross entropy loss [Zhang and Sabuncu, 2018]), and constrained losses [Lee et al., 2004, Awasthi et al., 2022a] (Section 5). In Section 5.1, we prove that the growth rate of $\\mathcal{H}$ -consistency bounds for comp-sum losses is exactly square-root. This applies when the auxiliary function $\\Phi$ they are based upon is convex and twice continuously differentiable with $\\Phi^{\\prime}(u)\\!<\\!0$ and $\\Phi^{\\prime\\prime}(u)\\!>\\!0$ for all $u$ in $\\left(0,{\\frac{1}{2}}\\right]$ . These conditions hold for all common loss functions used in practice. Further, in Section 5.2, we demonstrate that the square-root growth rate also extends to $\\mathcal{H}$ -consistency bounds for constrained losses. This requires the auxiliary function $\\Phi$ to be convex and twice continuously differentiable with $\\Phi^{\\prime}(u)\\!>\\!0$ and $\\Phi^{\\prime\\prime}(u)\\!>\\!0$ for any $u\\geq0$ , alongside an additional technical condition. These are satisfied by all constrained losses typically encountered in practice. ", "page_idx": 2}, {"type": "text", "text": "These results reveal a universal square-root growth rate for smooth surrogate losses, the predominant choice in neural network training (over polyhedral losses) for both binary and multi-class classification in applications. Given this universal growth rate, how do we choose between different surrogate losses? Section 6 addresses this question in detail. To start, we examine how $\\mathcal{H}$ -consistency bounds vary across surrogates based on the number of classes. Then, focusing on behavior near zero (ignoring constants), we isolate minimizability gaps as the key differentiating factor in these bounds. These gaps depend solely on the chosen surrogate loss and hypothesis set. We provide a detailed analysis of minimizability gaps, covering: comparisons across different comp-sum losses, conditions where gaps become zero, and general conditions leading to small gaps. These findings help guide surrogate loss selection. Additionally, we demonstrate the key role of minimizability gaps in comparing excess error bounds and $\\mathcal{H}$ -consistency bounds (Appendix F). Importantly, combining $\\mathcal{H}$ -consistency bounds with surrogate loss Rademacher complexity bounds allows us to derive zero-one loss (estimation) learning bounds for surrogate loss minimizers (Appendix O). ", "page_idx": 2}, {"type": "text", "text": "For a more comprehensive discussion of related work, please refer to Appendix A. We start with the introduction of necessary concepts and definitions. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation and definitions. We denote the input space by $\\mathcal{X}$ and the label space by $\\mathcal{Y}$ , a finite set of cardinality $n$ with elements $\\{1,\\ldots,n\\}$ . $\\mathcal{D}$ denotes a distribution over $\\mathcal{X}\\times\\mathcal{Y}$ . ", "page_idx": 2}, {"type": "text", "text": "We write $\\mathcal{H}_{\\mathrm{all}}$ to denote the family of all real-valued measurable functions defined over $\\mathcal{X}\\times\\mathcal{Y}$ and denote by $\\mathcal{H}$ a subset, $\\mathcal{H}\\subseteq\\mathcal{H}_{\\mathrm{all}}$ . The label assigned by $h\\in\\mathcal{H}$ to an input $x\\in\\mathcal X$ is denoted by $\\mathsf{h}(x)$ and defined by $\\mathsf{h}(x)=\\operatorname{argmax}_{y\\in\\mathcal{Y}}\\,h(x,y)$ , with an arbitrary but fixed deterministic strategy used for breaking the ties. For simplicity, we fix that strategy to be the one selecting the label with the highest index under the natural ordering of labels. ", "page_idx": 2}, {"type": "text", "text": "We will consider general loss functions $\\ell\\colon\\mathcal{H}\\times\\mathcal{X}\\times\\mathcal{Y}\\ \\to\\ \\mathbb{R}_{+}$ . For many loss functions used in practice, the loss value at $(x,y),\\,\\ell(h,x,y)$ , only depends on the value $h$ takes at $x$ and not on its values on other points. That is, there exists a measurable function $\\hat{\\ell}\\!:\\!\\mathbb{R}^{n}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}_{+}$ such that $\\ell(h,x,y)\\,=\\,\\hat{\\ell}(h(x),y)$ , where $h(x)\\,=\\,\\bigl[h(x,1),\\cdot\\cdot\\cdot,h(x,n)\\bigr]$ is the score vector of the predictor $h$ . We will then say that $\\ell$ is a pointwise loss function. We denote by $\\mathcal{E}_{\\ell}(h)$ the generalization error or expected loss of a hypothesis $h\\ \\in\\ \\mathcal{H}$ and by $\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})$ the best-in class error: $\\mathcal{E}_{\\ell}(h)\\;=\\;$ ", "page_idx": 2}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[\\ell(h,x,y)],\\mathcal{E}_{\\ell}^{*}(\\mathcal{K})=\\operatorname*{inf}_{h\\in\\mathcal{K}}\\mathcal{E}_{\\ell}(h).\\;\\mathcal{E}_{\\ell}^{*}(\\mathcal{K}_{\\mathrm{all}})}\\end{array}$ is also known as the Bayes error. We write $\\mathsf{p}(y\\mid x)\\,=\\,\\mathsf{\\mathcal{D}}(Y\\,=\\,y\\mid X\\,=\\,x)$ to denote the conditional probability of $Y\\,=\\,y$ given $X\\,=\\,x$ and $p(x)\\,=\\,\\bigl(\\mathsf{p}(1\\,|\\,x),\\dotsc,\\mathsf{p}(n\\,|\\,x)\\bigr)$ for the conditional probability vector for any $x\\in\\mathcal X$ . We denote by $\\mathcal{C}_{\\ell}(h,x)$ the conditional error of $h\\in\\mathcal{H}$ at a point $x\\in\\mathcal X$ and by $\\mathcal{C}_{\\ell}^{*}(\\mathcal{H},x)$ the best-in-class conditional error: $\\begin{array}{r}{\\mathfrak{C}_{\\ell}(h,x)=\\mathbb{E}_{y}\\big[\\ell(h,x,y)\\mid x\\big]=\\sum_{y\\in\\mathfrak{Y}}\\mathfrak{p}(y|x)\\,\\ell(h,x,y),\\mathfrak{C}_{\\ell}^{*}(\\mathcal{K},x)=\\operatorname*{inf}_{h\\in\\mathcal{K}}\\mathfrak{C}_{\\ell}(h,x)}\\end{array}$ , and use the shorthand $\\begin{array}{r}{\\Delta\\mathfrak{C}_{\\ell,\\mathcal{K}}(h,x)=\\mathfrak{C}_{\\ell}(h,x)-\\mathfrak{C}_{\\ell}^{*}(\\mathcal{K},x)}\\end{array}$ for the calibration gap or conditional regret for $\\ell$ . The generalization error of $h$ can be written as $\\mathcal{E}_{\\ell}(h)=\\mathbb{E}_{x}[\\mathcal{C}_{\\ell}(h,x)]$ . For convenience, we also define, for any vector $p=\\left(p_{1},...\\,,p_{n}\\right)\\in\\Delta^{n}$ , where $\\Delta^{n}$ is the probability simplex of $\\mathbb{R}^{n}$ , $\\mathcal{C}_{\\ell}(h,x,p)=$ $\\begin{array}{r}{\\sum_{y\\in\\mathbb{Y}}p_{y}\\,\\ell(h,x,y)}\\end{array}$ , $\\mathcal{\\ C}_{\\ell,\\mathcal{H}}^{*}(x,p)=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathcal{\\ C}_{\\ell}(h,x,p)$ and $\\begin{array}{r}{\\Delta\\mathfrak{C}_{\\ell,\\mathcal{K}}(h,x,p)=\\mathfrak{C}_{\\ell}(h,x,p)-\\mathfrak{C}_{\\ell,\\mathcal{K}}^{*}(x,p)}\\end{array}$ . Thus, we have $\\begin{array}{r}{\\Delta\\mathfrak{C}_{\\ell,\\mathcal{K}}(h,x,p(x))=\\Delta\\mathfrak{C}_{\\ell,\\mathcal{K}}(h,x)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "We will study the properties of a surrogate loss function $\\ell_{1}$ for a target loss function $\\ell_{2}$ . In multi-class classification, $\\ell_{2}$ is typically the zero-one multi-class classification loss function $\\ell_{0-1}$ defined by $\\ell_{0-1}\\!\\left(h,x,y\\right)\\,=\\,1_{\\mathsf{h}(x)\\neq y}$ . Some surrogate loss functions $\\ell_{1}$ include the max losses [Crammer and Singer, 2001], comp-sum losses [Mao et al., 2023f] and constrained losses [Lee et al., 2004]. ", "page_idx": 3}, {"type": "text", "text": "Binary classification. The definitions just presented were given for the general multi-class classification setting. In the special case of binary classification (two classes), the standard formulation and definitions are slightly different. For convenience, the label space is typically defined as $\\mathcal{Y}=\\left\\{-1,+1\\right\\}$ . Instead of two scoring functions, one for each label, a single real-valued function is used whose sign determines the predicted class. Thus, here, a hypothesis set $\\mathcal{H}$ is a family of measurable real-valued functions defined over $\\mathcal{X}$ and $\\mathcal{H}_{\\mathrm{all}}$ is the family of all such functions. $\\ell$ is pointwise if there exists a measurable function $\\hat{\\ell}\\!:\\!\\mathbb{R}\\times\\mathcal{Y}\\,\\to\\,\\mathbb{R}_{+}$ such that $\\ell(h,x,y)\\;=\\;\\hat{\\ell}(h(x),y)$ . The target loss function is typically the binary loss $\\ell_{0-1}$ , defined by $\\ell_{0-1}\\!\\left(h,x,y\\right)\\,=\\,1_{\\mathrm{sign}\\left(h\\left(x\\right)\\right)\\neq y}$ , where $\\mathrm{sign}(h(x))=1_{h(x)\\geq0}-1_{h(x)<0}$ . Some widely used surrogate losses $\\ell_{1}$ for $\\ell_{0-1}$ are margin-based losses, which are defined by $\\ell_{1}(h,x,y)\\,=\\,\\Phi(-y h(x))$ , for some non-decreasing convex function $\\Phi\\colon\\mathbb{R}\\to\\mathbb{R}_{+}$ . Instead of two conditional probabilities, one for each label, a single conditional probability corresponding to the positive class $+1$ is used. That is, let $\\eta(x)=\\mathcal{D}(Y=+1\\mid X=x)$ denote the conditional probability of $Y=+1$ given $X=x$ . The conditional error can then be expressed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathfrak{C}_{\\ell}(h,x)=\\mathop{\\mathbb{E}}_{y}\\!\\left[\\ell(h,x,y)\\mid x\\right]=\\eta(x)\\ell(h,x,+1)+(1-\\eta(x))\\ell(h,x,-1).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For convenience, we also define, for any $p\\in[0,1]$ , $\\begin{array}{r}{\\mathfrak{C}_{\\ell}(h,x,p)=p\\ell(h,x,+1)+(1-p)\\ell(h,x,-1)}\\end{array}$ , $\\begin{array}{r}{\\mathcal{\\ C}_{\\ell,\\mathcal{H}}^{*}(x,p)\\,=\\,\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathcal{\\ C}_{\\ell}(h,x,p)}\\end{array}$ and $\\begin{array}{r}{\\Delta\\mathfrak{C}_{\\ell,\\mathcal{K}}(h,x,p)=\\mathfrak{C}_{\\ell}(h,x,p)-\\operatorname*{inf}_{h\\in\\mathcal{K}}\\mathfrak{C}_{\\ell}(h,x,p)}\\end{array}$ . Thus, we have $\\begin{array}{r}{\\Delta\\mathfrak{C}_{\\ell,\\mathcal{K}}\\big(h,x,\\eta(x)\\big)=\\Delta\\mathfrak{C}_{\\ell,\\mathcal{K}}\\big(h,x\\big)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "To simplify matters, we will use the same notation for binary and multi-class classification, such as $\\mathcal{Y}$ for the label space or $\\mathcal{H}$ for a hypothesis set. We rely on the reader to adapt to the appropriate definitions based on the context. ", "page_idx": 3}, {"type": "text", "text": "Estimation, approximation, and excess errors. For a hypothesis $h$ , the difference $\\mathcal{E}_{\\ell}(h){-}\\mathcal{E}_{\\ell}^{*}(\\mathcal{H}_{\\mathrm{all}})$ is known as the excess error. It can be decomposed into the sum of two terms, the estimation error, $\\left(\\mathscr{E}_{\\ell}(h)-\\mathscr{E}_{\\ell}^{*}(\\mathcal{H})\\right)$ and the approximation error $\\mathcal{A}_{\\ell}\\big(\\mathcal{H}\\big)=\\big(\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}\\big)-\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}}\\big)\\big)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}\\big(h\\big)-\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}}\\big)=\\big(\\mathcal{E}_{\\ell}\\big(h\\big)-\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}\\big)\\big)+\\big(\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}\\big)-\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}}\\big)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A fundamental result for a pointwise loss function $\\ell$ is that the Bayes error and the approximation error admit the following simpler expressions. We give a concise proof of this lemma in Appendix B, where we establish the measurability of the function $x\\mapsto\\mathcal{C}_{\\ell}^{*}(\\mathcal{H}_{\\mathrm{all}},x)$ . ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.1. Let $\\ell$ be a pointwise loss function. Then, the Bayes error and the approximation error can be expressed as follows: $\\mathcal{E}_{\\ell}^{\\,*}(\\mathcal{H}_{\\mathrm{all}})\\overset{\\,^{\\cdot}}{=}\\mathbb{E}_{x}\\big[\\mathcal{C}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}},x\\big)\\big]$ and $\\mathcal{A}_{\\ell}\\big(\\mathcal{H}\\big)=\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}\\big)-\\mathbb{E}_{x}\\big[\\mathcal{\\mathrm{C}}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}},x\\big)\\big]$ . ", "page_idx": 3}, {"type": "text", "text": "For restricted hypothesis sets $(\\mathcal{H}\\,\\neq\\,\\mathcal{H}_{\\mathrm{all}})$ , the infimum\u2019s super-additivity implies that $\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})\\ \\geq$ $\\mathbb{E}_{x}[\\mathcal{C}_{\\ell}^{*}(\\mathcal{H},x)]$ . This inequality is generally strict, and the difference, $\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})-\\bar{\\mathbb{E}_{x}}\\big[\\mathcal{\\mathrm{C}}_{\\ell}^{*}(\\mathcal{H},x)\\big]$ , plays a crucial role in our analysis. ", "page_idx": 3}, {"type": "text", "text": "3 $\\mathcal{H}$ -consistency bounds ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A widely used notion of consistency is that of Bayes-consistency given below [Steinwart, 2007]. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Bayes-consistency). A loss function $\\ell_{1}$ is Bayes-consistent with respect to a loss function $\\ell_{2}$ , if for any distribution $\\mathcal{D}$ and any sequence $\\begin{array}{r}{\\{h_{n}\\}_{n\\in\\mathbb{N}}\\subset\\mathcal{H}_{\\mathrm{all}},\\mathrm{lim}_{n\\rightarrow+\\infty}\\,\\mathcal{E}_{\\ell_{1}}\\big(h_{n}^{-}\\big)\\!-\\!\\mathcal{E}_{\\ell_{1}}^{*}\\big(\\mathcal{H}_{\\mathrm{all}}\\big)=0}\\end{array}$ implies $\\begin{array}{r}{\\operatorname*{lim}_{n\\to+\\infty}\\mathcal{E}_{\\ell_{2}}\\big(h_{n}\\big)-\\mathcal{E}_{\\ell_{2}}^{*}\\big(\\mathcal{H}_{\\mathrm{all}}\\big)=0.}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Thus, when this property holds, asymptotically, a nearly optimal minimizer of $\\ell_{1}$ over the family of all measurable functions is also a nearly optimal optimizer of $\\ell_{2}$ . But, Bayes-consistency does not supply any information about a hypothesis set $\\mathcal{H}$ not containing the full family $\\mathcal{H}_{\\mathrm{all}}$ , that is a typical hypothesis set used for learning. Furthermore, it is only an asymptotic property and provides no convergence guarantee. In particular, it does not give any guarantee for approximate minimizers. Instead, we will consider upper bounds on the target estimation error expressed in terms of the surrogate estimation error, $\\mathcal{H}$ -consistency bounds [Awasthi et al., 2022b,a, Mao et al., 2023f], which account for the hypothesis set $\\mathcal{H}$ adopted. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.2 ( $\\mathcal{H}$ -consistency bounds). Given a hypothesis set $\\mathcal{H}$ , an $\\mathcal{H}$ -consistency bound relating the loss function $\\ell_{1}$ to the loss function $\\ell_{2}$ for a hypothesis set $\\mathcal{H}$ is an inequality of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{K},\\quad\\mathcal{E}_{\\ell_{2}}(h)-\\mathcal{E}_{\\ell_{2}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\ell_{2}}(\\mathcal{K})\\leq\\Gamma\\big(\\mathcal{E}_{\\ell_{1}}(h)-\\mathcal{E}_{\\ell_{1}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\ell_{1}}(\\mathcal{K})\\big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "that holds for any distribution $\\mathcal{D}$ , where $\\Gamma\\!:\\!\\mathbb{R}_{+}\\to\\mathbb{R}_{+}$ is a non-decreasing concave function with $\\Gamma\\geq0$ [Awasthi et al., 2022b,a]. Here, $\\mathcal{M}_{\\ell_{1}}(\\mathcal{H})$ and $\\mathcal{M}_{\\ell_{2}}(\\mathcal{H})$ are minimizability gaps for the respective loss functions. The minimizability gap for a hypothesis set $\\mathcal{H}$ and loss function $\\ell$ is denoted by $\\mathcal{M}_{\\ell}(\\mathcal{H})$ and defined as: $\\mathbb{M}_{\\ell}\\big(\\mathcal{H}\\big)=\\mathcal{E}_{\\ell}^{*}\\big(\\bar{\\mathcal{H}}\\big)-\\mathbb{E}_{x}\\big[\\bar{\\mathcal{C}}_{\\ell}^{*}\\big(\\mathcal{H},x\\big)\\big]$ . It quantifies the discrepancy between the best possible expected loss within a hypothesis class and the expected infimum of pointwise expected losses. This gap is always non-negative: $\\begin{array}{r}{\\mathcal{\\mathrm{M}}_{\\ell}(\\mathcal{H})=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}_{x}\\big[\\mathcal{\\mathrm{C}}_{\\ell}(h,x)\\big]-\\mathbb{E}_{x}\\big[\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathcal{\\mathrm{C}}_{\\ell}(\\mathcal{H},x)\\big]\\geq0}\\end{array}$ , by the infimum\u2019s super-additivity, and is bounded above by the approximation error $\\mathbf{\\mathcal{A}}_{\\ell}(\\mathbf{\\mathcal{H}})\\mathbf{\\Sigma}=$ $\\begin{array}{r}{\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}_{x}[\\mathcal{\\mathrm{C}}_{\\ell}(h,x)]^{\\top}\\!\\!-\\!\\mathbb{E}_{x}[\\operatorname*{inf}_{h\\in\\mathcal{H}_{\\mathrm{all}}}\\mathcal{\\mathrm{C}}_{\\ell}(\\mathcal{H},x)]}\\end{array}$ . We further study the key role of minimizability gaps in $\\mathcal{H}$ -consistency bounds and their properties in Section 6 and Appendix D. As shown in Appendix C, under general assumptions, minimizability gaps are essential quantities required in any bound that relates the estimation errors of two loss functions with an arbitrary hypothesis set $\\mathcal{H}$ . ", "page_idx": 4}, {"type": "text", "text": "Thus, an $\\mathcal{H}$ -consistency bound provides the guarantee that when the surrogate estimation loss $\\mathcal{E}_{\\ell}(h)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})$ is minimized to $\\epsilon$ , the following upper bound holds for the zero-one estimation error: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(h)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{K})\\leq\\Gamma\\big(\\epsilon+\\mathcal{M}_{\\ell}(\\mathcal{K})\\big)-\\mathcal{M}_{\\ell_{0-1}}\\big(\\mathcal{K}\\big)\\leq\\Gamma\\big(\\epsilon\\big)+\\Gamma\\big(\\mathcal{M}_{\\ell}(\\mathcal{K})\\big)-\\mathcal{M}_{\\ell_{0-1}}\\big(\\mathcal{K}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the second inequality follows from the sub-additivity of a co\u221ancave function $\\Gamma$ over $\\mathbb{R}_{+}$ . We will demonstrate that, for smooth surrogate losses, $\\Gamma(\\epsilon)$ scales as $\\sqrt{\\epsilon}$ . Note, however, that, while $\\Gamma(\\epsilon)$ tends to zero when $\\epsilon\\to0$ for functions $\\Gamma$ derived in $\\mathcal{H}$ -consistency bounds, the remaining terms in the bound are constant. This is not surprising as, in general, minimizing the surrogate estimation error to zero cannot guarantee that the zero-one estimation error will also converge to zero. This is well-known, for example, in the case of linear models [Ben-David et al., 2012]. Instead, an $\\mathcal{H}$ -consistency bound provides the tightest possible upper bound on the estimation error for the zero-one loss when the surrogate estimation error is minimized. ", "page_idx": 4}, {"type": "text", "text": "The upper bound simplifies to $\\Gamma(\\epsilon)$ when the minimizability gaps are zero, which occurs when either $\\mathcal{H}=\\mathcal{H}_{\\mathrm{all}}$ (the set of all measurable functions) or in realizable cases, which are particularly relevant to the practical use of complex neural networks in applications. In Appendix I, we examine more general cases of small minimizability gaps, taking into account the complexity of $\\mathcal{H}$ and the distribution. ", "page_idx": 4}, {"type": "text", "text": "Our results cover in particular the special case of excess bounds $\\mathcal{H}=\\mathcal{H}_{\\mathrm{all}})$ ). Let us emphasize that, for $\\mathcal{H}\\neq\\mathcal{H}_{\\mathrm{all}}$ , $\\mathcal{H}$ -consistency bounds offer tighter and more favorable guarantees on the estimation error compared to those derived from excess bounds analysis alone (see Appendix F). ", "page_idx": 4}, {"type": "text", "text": "When $\\ell_{2}=\\ell_{0-1}$ , the zero-one loss, we say that $\\upsigma$ is the $\\mathcal{H}$ -estimation error transformation function of a surrogate loss $\\ell$ if the following holds: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{K},\\quad\\mathcal{T}\\big(\\mathcal{E}_{\\ell_{0-1}}(h)-\\mathcal{E}_{\\ell_{0-1}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\ell_{0-1}}(\\mathcal{K})\\big)\\leq\\mathcal{E}_{\\ell}(h)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{K})+\\mathcal{M}_{\\ell}(\\mathcal{K}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the bound is tight. That is, for any $t\\in[0,1]$ , there exists a hypothesis $h\\in\\mathcal{H}$ and a distribution such that $\\mathcal{E}_{\\ell_{0-1}}(h)\\!-\\!\\mathcal{E}_{\\ell_{0-1}}^{*}(\\mathcal{H})\\!+\\!\\mathcal{M}_{\\ell_{0-1}}(\\mathcal{H})\\,\\bar{=}\\,t$ and $\\mathfrak{E}_{\\ell}(h)\\!-\\!\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})\\!+\\!\\mathcal{M}_{\\ell}(\\mathcal{H})=\\mathcal{T}(t)$ . An explicit form of $\\upsigma$ has been characterized for binary margin-based losses [Awasthi et al., 2022b], as well as compsum losses and constrained losses in multi-class classification [Mao et al., 2023b]. In the following sections, we will prove the property $\\mathcal{T}(t)=\\Theta(t^{2})$ (under mild assumptions), demonstrating a squareroot growth rate for $\\mathcal{H}$ -consistency bounds. Appendix $\\mathrm{E}$ provides examples of $\\mathcal{H}$ -consistency bounds for both binary and multi-class classification. Our analysis also suggests choosing appropriately $\\mathcal{H}$ and the function $\\Gamma$ to ensure a small minimizability gap and to take into account the number of classes and other properties, as discussed in Section 6. ", "page_idx": 4}, {"type": "text", "text": "4 Binary classification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We consider the broad family of margin-based loss functions $\\ell$ defined for any $h\\in\\mathcal{H}$ , and $(x,y)\\in$ $\\mathcal{X}\\times\\mathcal{Y}$ by $\\ell(h,x,y)=\\Phi(-y h(x))$ , where $\\Phi$ is a non-decreasing convex function upper-bounding the zero-one loss. Margin-based loss functions include most loss functions used in binary classification. As an example, $\\Phi(\\bar{u})=\\log(1+e^{u})$ for the logistic loss or $\\Phi(u)=\\exp(u)$ for the exponential loss. We say that a hypothesis set $\\mathcal{H}$ is complete, if for all $x\\in\\mathcal X$ , we have $\\{h(x)\\!:\\!h\\in\\mathcal{H}\\}=\\mathbb{R}$ . As shown by Awasthi et al. [2022b], the transformation $\\upsigma$ has the following form for complete hypothesis sets: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{\\mathcal{T}}(t){:}=\\operatorname*{inf}_{u\\leq0}f_{t}(u)-\\operatorname*{inf}_{u\\in\\mathbb{R}}f_{t}(u).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, for any $t\\in[0,1]$ , $f_{t}$ is defined by: $\\forall u\\in\\mathbb{R}$ , $\\begin{array}{r}{f_{t}(u)\\,=\\,\\frac{1-t}{2}\\Phi(u)+\\frac{1+t}{2}\\Phi(-u)}\\end{array}$ . The following result is useful for proving the growth rate in binary classification. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Let $\\mathcal{H}$ be a complete hypothesis set. Assume that $\\Phi$ is convex and differentiable at zero and satisfies the inequality $\\Phi^{\\prime}(0)>0$ . Then, the transformation $\\upsigma$ can be expressed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\forall t\\in[0,1],\\quad\\mathcal{T}(t)=f_{t}(0)-\\operatorname*{inf}_{u\\in\\mathbb{R}}f_{t}(u).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. By the convexity of $\\Phi$ , for any $t\\in[0,1]$ and $u\\in\\mathbb{R}_{-}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{t}(u)=\\frac{1-t}{2}\\Phi(u)+\\frac{1+t}{2}\\Phi(-u)\\geq\\Phi(0)-t u\\Phi^{\\prime}(0)\\geq\\Phi(0).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus, we can write $\\begin{array}{r}{\\mathcal{T}(t)=\\operatorname*{inf}_{u\\le0}f_{t}(u)-\\operatorname*{inf}_{u\\in\\mathbb{R}}f_{t}(u)\\ge\\Phi(0)-\\operatorname*{inf}_{u\\in\\mathbb{R}}f_{t}(u)=f_{t}(0)-\\operatorname*{inf}_{u\\in\\mathbb{R}}f_{t}(u).}\\end{array}$ , where equality is achieved when $u=0$ . \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2 (Upper and lower bound for binary margin-based losses). Let $\\mathcal{H}$ be a complete hypothesis set. Assume that $\\Phi$ is convex, twice continuously differentiable, and satisfies the inequalities $\\Phi^{\\prime}(0)>0$ and $\\Phi^{\\prime\\prime}(0)>0$ . Then, the following property holds: $\\mathbf{\\mathcal{T}}(t)\\mathbf{\\,=\\,}\\Theta(t^{2}),$ ; that is, there exist positive constants $C>0$ , $c>0$ , and $T>0$ such that $C t^{2}\\geq\\mathcal{T}(t)\\geq c t^{2}$ , for all $0<t\\leq T$ . ", "page_idx": 5}, {"type": "text", "text": "Proof sketch First, we demonstrate that, by applying the implicit function theorem, $\\operatorname{inf}_{u\\in\\mathbb{R}}f_{t}(u)$ is attained uniquely by $a_{t}^{*}$ , and that $a_{t}^{*}$ is continuously differentiable over $[0,\\epsilon]$ for some $\\epsilon>0$ . The minimizer $a_{t}^{*}$ satisfies the following condition: $\\begin{array}{r}{f_{t}^{\\prime}\\!\\left(a_{t}^{*}\\right)=\\frac{1-t}{2}\\Phi^{\\prime}\\!\\left(a_{t}^{*}\\right)-\\frac{1+t}{2}\\bar{\\Phi}^{\\prime}\\!\\left(-a_{t}^{*}\\right)=0}\\end{array}$ . Specifically, at $t=0$ , we have $\\Phi^{\\prime}(a_{0}^{*})=\\Phi^{\\prime}(-a_{0}^{*})$ . Then, by the convexity of $\\Phi$ and monotonicity of the derivative $\\Phi^{\\prime}$ , we must have $a_{0}^{*}=0$ and since $\\Phi^{\\prime}$ is non-decreasing and $\\Phi^{\\prime\\prime}(0)>0$ , we have $a_{t}^{*}>0$ for all $t\\in(0,\\epsilon]$ . Furthermore, since $a_{t}^{*}$ is a function of class $C^{\\tilde{1}}$ , we can differentiate this condition with respect to $t$ and take the limit $t\\rightarrow0$ , which gives the following equality: $\\begin{array}{r}{\\frac{d a_{t}^{*}}{d t}(0)=\\frac{\\Phi^{\\prime}(0)}{\\Phi^{\\prime\\prime}(0)}>0.}\\end{array}$ . Since $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow0}\\frac{a_{t}^{*}}{t}=\\frac{d a_{t}^{*}}{d t}\\big(0\\big)=\\frac{\\Phi^{\\prime}(0)}{\\Phi^{\\prime\\prime}(0)}>0}\\end{array}$ , we have $a_{t}^{*}=\\Theta(t)$ . By Theorem 4.1 and Taylor\u2019s theorem with an integral remainder, $\\upsigma$ can be expressed as follows: for any $t\\in[0,\\epsilon],\\mathsf{T}(t)=f_{t}(0)-\\operatorname*{inf}_{u\\in\\mathbb{R}}f_{t}(u)=$ $\\begin{array}{r}{\\int_{0}^{a_{t}^{*}}u f_{t}^{\\prime\\prime}(u)\\,d u=\\int_{0}^{a_{t}^{*}}u\\Big[\\frac{1-t}{2}\\Phi^{\\prime\\prime}(u)+\\frac{1+t}{2}\\Phi^{\\prime\\prime}(-u)\\Big]\\,d u}\\end{array}$ . Since $\\Phi^{\\prime\\prime}(0)>0$ and $\\Phi^{\\prime\\prime}$ is continuous, there is a non-empty interval $[-\\alpha,+\\alpha]$ over which $\\Phi^{\\bar{\\prime\\prime}}$ is positive. Since $a_{0}^{*}\\,=\\,0$ and $a_{t}^{*}$ is continuous, there exists a sub-interval $[0,\\epsilon^{\\prime}]\\bar{\\bf\\Delta}\\subseteq[0,\\epsilon]$ over which $a_{t}^{*}\\,\\leq\\,\\alpha$ . Since $\\Phi^{\\prime\\prime}$ is continuous, it admits a minimum and a maximum over any compact set and we can define $\\begin{array}{r}{c=\\operatorname*{min}_{u\\in[-\\alpha,\\alpha]}\\Phi^{\\prime\\prime}(u)}\\end{array}$ and $C=\\mathrm{max}_{u\\in[-\\alpha,\\alpha]}\\,\\Phi^{\\prime\\prime}(u)$ . $c$ and $C$ are both positive since we have $\\Phi^{\\prime\\prime}(0)>0$ . Thus, for $t$ in $[0,\\epsilon^{\\prime}]$ , the following inequality holds: C (at2 ) $\\begin{array}{r}{C\\frac{(a_{t}^{*})^{2}}{2}=\\int_{0}^{a_{t}^{*}}u C\\,d u\\geq\\mathcal{T}(t)=\\int_{0}^{a_{t}^{*}}u\\Big[\\frac{1-t}{2}\\Phi^{\\prime\\prime}(u)+\\frac{1+t}{2}\\Phi^{\\prime\\prime}(-u)\\Big]\\,d u\\geq\\mathcal{T}(t).}\\end{array}$ $\\begin{array}{r}{\\int_{0}^{a_{t}^{*}}u c\\,d u=c\\frac{(a_{t}^{*})^{2}}{2}}\\end{array}$ . This implies that $\\mathcal{T}(t)=\\Theta(t^{2})$ . The full proof is included in Appendix K. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2 directly applies to excess error bounds as well, when $\\mathcal{H}\\,=\\,\\mathcal{H}_{\\mathrm{all}}$ . Importantly, our lower bound requires weaker conditions than [Frongillo and Waggoner, 2021, Theorem 4], and our upper bound is entirely novel. This result demonstrates that the growth rate for these loss functions is precisely square-root, refining the \u201cat least square-root\u201d finding of these authors. It is known that polyhedral losses admit a linear grow rate [Frongillo and Waggoner, 2021]. Thus, a striking dichotomy emerges: $\\mathcal{H}$ -consistency bounds for polyhedral losses exhibit a linear growth rate, while they follow a square-root rate for smooth loss functions (see Appendix G for a detailed comparison). ", "page_idx": 5}, {"type": "text", "text": "5 Multi-class classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we will study two families of surrogate losses in multi-class classification: comp-sum losses and constrained losses, defined in Section 5.1 and Section 5.2 respectively. Comp-sum losses and constrained losses are general and cover all loss functions commonly used in practice. We will consider any hypothesis set $\\mathcal{H}$ that is symmetric and complete. We say that a hypothesis set is symmetric when it does not depend on a specific ordering of the classes, that is, when there exists a family $\\mathcal{F}$ of functions $f$ mapping from $\\mathcal{X}$ to $\\mathbb{R}$ such that $\\{[h(x,1),\\ldots,h(x,n)]{:}h\\in{\\mathcal{H}}\\}=$ $\\left\\{[f_{1}(x),\\ldots,{\\bar{f}}_{n}(x)];f_{1},\\ldots,f_{n}\\in{\\mathcal{F}}\\right\\}$ , for any $x\\in\\mathcal X$ . We say that a hypothesis set $\\mathcal{H}$ is complete if the set of scores it generates spans $\\mathbb{R}$ , that is, $\\left\\{h(x,y);h\\in{\\mathcal{H}}\\right\\}=\\mathbb{R}$ , for any $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ . ", "page_idx": 6}, {"type": "text", "text": "5.1 Comp-sum losses ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Here, we consider comp-sum losses [Mao, Mohri, and Zhong, 2023f], defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{H},\\,\\forall(x,y)\\times\\mathcal{X}\\times\\mathcal{Y},\\quad\\ell^{\\mathrm{comp}}(h,x,y)=\\Phi\\bigg(\\frac{e^{h(x,y)}}{\\sum_{y^{\\prime}\\in\\mathcal{Y}}e^{h(x,y^{\\prime})}}\\bigg),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Phi\\colon\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}_{+}$ is a non-increasing function. For example, $\\Phi$ can be chosen as the negative log function $u\\mapsto-\\log(u)$ for the comp-sum losses, which leads to the multinomial logistic loss. As shown by Mao, Mohri, and Zhong [2023b], for symmetric and complete hypothesis sets, the transformation $\\upsigma$ for the family of comp-sum losses can be characterized as follows. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.1 (Mao et al. [2023b, Theorem 3]). Let $\\mathcal{H}$ be a symmetric and complete hypothesis set. Assume that $\\Phi$ is convex, differentiable at $\\frac{1}{2}$ and satisfies the inequality $\\Phi^{\\prime}\\big(\\frac{1}{2}\\big)<0$ . Then, the transformation $\\upsigma$ can be expressed as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{T}(t)=\\operatorname*{inf}_{\\tau\\in\\left[\\frac1n,\\frac12\\right]}\\operatorname*{sup}_{|u|\\le\\tau}\\Biggl\\{\\Phi(\\tau)-\\frac{1-t}{2}\\Phi\\bigl(\\tau+u\\bigr)-\\frac{1+t}{2}\\Phi\\bigl(\\tau-u\\bigr)\\Biggr\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Next, we will show that as with the binary case, for the comp-sum losses, the properties $\\mathcal{T}(t)=\\Omega(t^{2})$ and $\\mathcal{T}(t)=O(t^{2})$ hold. We first introduce a generalization of the classical implicit function theorem where the function takes the value zero over a set of points parameterized by a compact set. We treat the special case of a function $F$ defined over $\\mathbb{R}^{3}$ and denote by $(t,a,\\tau)\\overset{.}{\\in}\\mathbb{R}^{3}$ its arguments. The theorem holds more generally for the arguments being in $\\mathbb{R}^{n_{1}}\\times\\mathbb{R}^{n_{2}}\\times\\mathbb{R}^{n_{3}}$ and with the condition on the partial derivative being non-zero replaced with a partial Jacobian being non-singular. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.2 (Implicit function theorem with a compact set). Let $F\\!:\\!\\mathbb{R}\\!\\times\\!\\mathbb{R}\\!\\times\\!\\mathbb{R}\\!\\rightarrow\\mathbb{R}$ be a continuously differentiable function in a neighborhood of $(0,0,\\tau)$ , for any $\\tau$ in a non-empty compact set $\\mathcal{C}_{\\mathrm{i}}$ , with $F(0,0,\\tau)=0$ . Then, $\\begin{array}{r}{i f\\frac{\\partial F}{\\partial a}(0,0,\\tau)}\\end{array}$ is ero for all $\\tau$ in ${\\mathcal{C}},$ , then, there exist a neighborhood O of 0 and a unique function $\\bar{a}$ defined over $\\mathcal{O}\\times\\mathcal{C}$ that is continuously differentiable and satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\forall(t,\\tau)\\in\\mathcal{O}\\times\\mathcal{C},\\quad F(t,\\bar{a}(t,\\tau),\\tau)=0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof. By the implicit function theorem (see for example [Dontchev and Rockafellar, 2009]), for any $\\tau\\in{\\mathcal{C}}$ , there exists an open set $\\mathcal{U}_{\\tau}=\\left(-t_{\\tau},+t_{\\tau}\\right)\\times\\left(\\tau-\\epsilon_{\\tau},\\tau+\\epsilon_{\\tau}\\right)$ , $(t_{\\tau}>0$ and $\\epsilon_{\\tau}>0_{,}$ ), and a unique function $\\bar{a}_{\\tau}\\{\\mathcal{U}_{\\tau}\\rightarrow\\mathbb{R}$ that is in $C^{1}$ and such that for all $\\begin{array}{r}{\\left(t,\\tau\\right)\\in\\mathcal{U}_{\\tau},F(t,\\bar{a}_{\\tau}(t),\\tau)=0.}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "By the uniqueness of $\\bar{a}_{\\tau}$ , for any $\\tau\\ne\\tau^{\\prime}$ and $\\left(t_{1},\\tau_{1}\\right)\\in\\mathcal{U}_{\\tau}\\cap\\mathcal{U}_{\\tau^{\\prime}}$ , we have $\\bar{a}_{\\tau}\\big(t_{1},\\tau_{1}\\big)\\,=\\,\\bar{a}_{\\tau^{\\prime}}\\big(t_{1},\\tau_{1}\\big)$ . Thus, we can define a function $\\bar{a}$ over $\\begin{array}{r}{\\mathcal{U}=\\bigcup_{\\tau\\in\\mathcal{C}}\\mathcal{U}_{\\tau}}\\end{array}$ that is of class $C^{1}$ and such that for any $(t,\\tau)\\in\\mathcal{U}$ , $F(t,\\bar{a}(t,\\tau),\\tau)=0$ . ", "page_idx": 6}, {"type": "text", "text": "Now, $\\cup_{\\tau\\in\\mathcal{C}}\\bigl(\\tau-\\epsilon_{\\tau},\\tau+\\epsilon_{\\tau}\\bigr)$ is a cover of the compact set $\\mathcal{C}$ via open sets. Thus, we can extract from it a finite cover $\\cup_{\\tau\\in I}\\bigl(\\tau\\!-\\!\\epsilon_{\\tau},\\tau\\!+\\!\\epsilon_{\\tau}\\bigr)$ , for some finite cardinality set $I$ . Define $\\begin{array}{r}{\\left(-t_{0},+t_{0}\\right)=\\bigcap_{\\tau\\in I}\\bigl(-t_{\\tau},+t_{\\tau}\\bigr)}\\end{array}$ , which is a non-empty open interval as an intersection of (embedded) open intervals containing zero. Then, $\\bar{a}$ is continously differentiable over $(-t_{0},+t_{0})\\times\\mathcal{C}$ and for any $(t,\\tau)\\in(-t_{0},+t_{0})\\times\\mathcal{C}$ , we have $F(t,\\bar{a}(t,\\tau),\\tau)=0$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.3 (Upper and lower bound for comp-sum losses). Assume that $\\Phi$ is convex, twice continuously differentiable, and satisfies the properties $\\Phi^{\\prime}(u)<0$ and $\\Phi^{\\prime\\prime}(u)>0$ for any $u\\in(0,\\frac{1}{2}]$ . Then, the following property holds: $\\mathcal{T}(t)=\\Theta(t^{2})$ . ", "page_idx": 6}, {"type": "text", "text": "Proof sketch For any $\\tau\\in\\left[{\\frac{1}{n}},{\\frac{1}{2}}\\right]$ , define the function $\\Phi_{\\tau}$ by $\\mathcal{T}_{\\tau}(t)=f_{t,\\tau}(0)-\\operatorname*{inf}_{|u|\\leq\\tau}f_{t,\\tau}(u)$ , where $\\begin{array}{r}{f_{t,\\tau}(u)=\\frac{1-t}{2}\\Phi_{\\tau}(u)+\\frac{1+t}{2}\\Phi_{\\tau}(-u),t\\in[0,1]}\\end{array}$ and $\\Phi_{\\tau}(u)=\\Phi(\\tau+u)$ . ", "page_idx": 7}, {"type": "text", "text": "We aim to establish a lower and upper bound for $\\operatorname*{inf}_{\\tau\\in[\\frac{1}{n},\\frac{1}{2}]}\\mathcal{T}_{\\tau}(t)$ . For any fixed $\\tau\\in\\left[{\\frac{1}{n}},{\\frac{1}{2}}\\right]$ , this situation is parallel to that of binary classification (Theorem 4.1 and Theorem 4.2), since we have $\\Phi_{\\tau}^{\\prime}(0)\\,=\\,\\Phi^{\\bar{\\prime}}(\\tau)\\,<\\,0$ and $\\Phi_{\\tau}^{\\prime\\prime}(0)\\,=\\,\\bar{\\Phi}^{\\prime\\prime}(\\tau)\\,>\\,0$ . By Theorem 5.2 and the proof of Theorem 4.2, adopting a similar notation, while incorporating the $\\tau$ subscript to distinguish different functions $\\Phi_{\\tau}$ and $f_{t,\\tau}$ , we can write $\\begin{array}{r}{\\forall t\\in[0,t_{0}],\\,\\mathcal{T}_{\\tau}(t)\\,=\\,\\int_{0}^{-a_{t,\\tau}^{*}}u\\Big[\\frac{1-t}{2}\\Phi_{\\tau}^{\\prime\\prime}\\big(-u\\big)+\\frac{1+t}{2}\\Phi_{\\tau}^{\\prime\\prime}\\big(u\\big)\\Big]\\,d u}\\end{array}$ , where $a_{t,\\tau}^{*}$ verifies $a_{0,\\tau}^{*}=0$ and $\\begin{array}{r}{\\frac{\\partial a_{t,\\tau}^{*}}{\\partial t}(0)=\\frac{\\Phi_{\\tau}^{\\prime}(0)}{\\Phi_{\\tau}^{\\prime\\prime}(0)}=c_{\\tau}<0}\\end{array}$ . Then, by further analyzing this equality, we can show the lower bound $\\operatorname*{inf}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}-a_{t,\\tau}^{*}=\\Omega(t)$ and the upper bound $\\operatorname*{sup}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}-a_{t,\\tau}^{*}=O(t)$ for some $t\\in[0,t_{1}]$ , $t_{1}>0$ . Finally, using the fact that $\\Phi^{\\prime\\prime}$ reaches its maximum and minimum over a compact set, we obtain that $\\mathcal{T}(t)\\stackrel{}{=}\\operatorname*{inf}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}\\mathcal{T}_{\\tau}(t)=\\Theta(t^{2})$ . The full proof is included in Appendix $\\mathrm{L}$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.3 significantly extends Theorem 4.2 to multi-class comp-sum losses, which include the logistic loss or cross-entropy used with a softmax activation function. It shows that the growth rate of $\\mathcal{H}$ -consistency bounds for comp-sum losses is exactly square-root, provided that the auxiliary function $\\Phi$ they are based upon is convex, twice continuously differentiable, and satisfies $\\Phi^{\\prime}(u)<\\mathrm{\\bar{0}}$ and $\\Phi^{\\prime\\prime}(u)>0$ for any $u\\in(0,\\frac{1}{2}]$ , which holds for most loss functions used in practice. ", "page_idx": 7}, {"type": "text", "text": "5.2 Constrained losses ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here, we consider constrained losses (see [Lee et al., 2004]), defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{K},\\,\\forall(x,y)\\times\\mathcal{X}\\times\\mathcal{Y},\\quad\\ell^{\\mathrm{cstnd}}(h,x,y)=\\sum_{y^{\\prime}\\neq y}\\Phi(h(x,y^{\\prime}))\\mathrm{~subject~to~}\\sum_{y\\in\\mathcal{Y}}h(x,y)=0,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\Phi\\colon\\mathbb{R}\\to\\mathbb{R}_{+}$ is a non-decreasing function. On possible choice for $\\Phi$ is the exponential function. As shown by Mao et al. [2023b], for symmetric and complete hypothesis sets, the transformation $\\upsigma$ for the family of constrained losses can be characterized as follows. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.4 (Mao et al. [2023b, Theorem 11]). Let $\\mathcal{H}$ be a symmetric and complete hypothesis set. Assume that $\\Phi$ is convex, differentiable at zero and satisfies the inequality $\\Phi^{\\prime}(0)>0$ . Then, the transformation $\\upsigma$ can be expressed as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{T}(t)=\\operatorname*{inf}_{\\tau\\ge0}\\operatorname*{sup}_{u\\in\\mathbb{R}}\\biggl\\{\\biggl(2-\\frac{1}{n-1}\\biggr)\\Phi(\\tau)-\\frac{2-\\frac{1}{n-1}-t}{2}\\Phi\\bigl(\\tau+u\\bigr)-\\frac{2-\\frac{1}{n-1}+t}{2}\\Phi\\bigl(\\tau-u\\bigr)\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Next, we will show that for the constrained losses, the properties $\\mathcal{T}(t)=\\Omega(t)$ and $\\mathcal{T}(t)=O(t)$ hold as well. Note that by Theorem 5.4, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Im\\biggl(\\Bigl(2-\\frac{1}{n-1}\\Bigr)t\\biggr)=\\biggl(2-\\frac{1}{n-1}\\biggr)\\operatorname*{inf}_{\\tau\\ge0}\\operatorname*{sup}_{u\\in\\mathbb{R}}\\biggl\\{\\Phi(\\tau)-\\frac{1-t}{2}\\Phi(\\tau+u)-\\frac{1+t}{2}\\Phi(\\tau-u)\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Therefore, to prove $\\mathcal{T}(t)=\\Theta(t^{2})$ , we only need to show ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\tau\\geq0}\\operatorname*{sup}_{u\\in\\mathbb{R}}\\biggl\\{\\Phi(\\tau)-\\frac{1-t}{2}\\Phi(\\tau+u)-\\frac{1+t}{2}\\Phi(\\tau-u)\\biggr\\}=\\Theta(t^{2}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For simplicity, we assume that the infimum over $\\tau\\geq0$ can be reached within some finite interval $[0,A]$ , $A>0$ . This assumption holds for common choices of $\\Phi$ , as discussed in [Mao et al., 2023b]. Furthermore, as demonstrated in Appendix $\\mathbf{N}$ , under certain conditions on $\\Phi^{\\prime\\prime}$ , the infimum over $\\tau\\in[0,A]$ is reached at zero for sufficiently small values of $t$ . For specific examples, see [Mao et al., 2023b, Appendix D.3], where $\\Phi(t)=e^{t}$ is considered. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.5 (Upper and lower bound for constrained losses). Assume that $\\Phi$ is convex, twice continuously differentiable, and satisfies the properties $\\Phi^{\\prime}(u)>0$ and $\\Phi^{\\prime\\prime}(u)>0$ for any $u\\geq0$ . Then, for any $A>0$ , the following property holds: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\tau\\in[0,A]}\\operatorname*{sup}_{u\\in\\mathbb{R}}\\biggl\\{\\Phi(\\tau)-\\frac{1-t}{2}\\Phi(\\tau+u)-\\frac{1+t}{2}\\Phi(\\tau-u)\\biggr\\}=\\Theta(t^{2}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof sketch For any $\\tau\\in[0,A]$ , define the function $\\Phi_{\\tau}$ by $\\mathcal{T}_{\\tau}(t)=f_{t,\\tau}(0)-\\operatorname*{inf}_{u\\in\\mathbb{R}}f_{t,\\tau}(u)$ , where $\\begin{array}{r}{f_{t,\\tau}(u)=\\frac{1-t}{2}\\Phi_{\\tau}(u)+\\frac{1+t^{-}}{2}\\Phi_{\\tau}(\\bar{-u})}\\end{array}$ , $t\\in[0,1]$ and $\\Phi_{\\tau}(u)=\\Phi(\\tau+u)$ . We aim to establish a lower and upper bound for $\\operatorname{inf}_{\\tau\\in[0,A]}\\mathcal{T}_{\\tau}(t)$ . For any fixed $\\tau\\in[0,A]$ , this situation is parallel to that of binary classification (Theorem 4.1 and Theorem 4.2), since we also have $\\Phi_{\\tau}^{\\prime}(0)=\\Phi^{\\prime}(\\tau)>0$ and $\\Phi_{\\tau}^{\\prime\\prime}(0\\dot{)}=\\Phi^{\\prime\\prime}(\\tau)>0$ . By applying Theorem 5.2 and leveraging the proof of Theorem 4.2, adopting a similar notation, while incorporating the $\\tau$ subscript to distinguish different functions $\\Phi_{\\tau}$ and $f_{t,\\tau}$ , we can write $\\forall t\\in[0,t_{0}]$ , $\\begin{array}{r}{\\mathcal{T}_{\\tau}(t)=\\int_{0}^{a_{t,\\tau}^{*}}u\\Big[\\frac{1-t}{2}\\Phi_{\\tau}^{\\prime\\prime}(u)+\\frac{1+t}{2}\\Phi_{\\tau}^{\\prime\\prime}(-u)\\Big]\\,d u}\\end{array}$ , where $a_{t,\\tau}^{*}$ verifies $a_{0,\\tau}^{*}=0$ and \u2202a\u2202t\u2217t,\u03c4 (0) = $\\begin{array}{r}{\\frac{\\partial a_{t,\\tau}^{*}}{\\partial t}(0)\\,=\\,\\frac{\\Phi_{\\tau}^{\\prime}(0)}{\\Phi_{\\tau}^{\\prime\\prime}(0)}\\,=\\,c_{\\tau}\\,>\\,0}\\end{array}$ . Then, by further analyzing this equality, we can show the lower bound $\\operatorname*{inf}_{\\tau\\in[0,A]}a_{t,\\tau}^{*}=\\Omega(t)$ and the upper bound $\\operatorname*{sup}_{\\tau\\in[0,A]}a_{t,\\tau}^{*}=O(t)$ for some $t\\in[0,t_{1}]$ , $t_{1}>0$ . Finally, using the fact that $\\Phi^{\\prime\\prime}$ reaches its maximum and minimum over some compact set, we obtain that $\\bar{\\mathcal{T}}(t)=\\operatorname*{inf}_{\\tau\\in[0,A]}\\mathcal{T}_{\\tau}(t)=\\Theta(t^{2})$ . The full proof is included in Appendix M. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.5 significantly expands our findings to multi-class constrained losses. It demonstrates that, under some assumptions, which are commonly satisfied by smooth constrained losses used in practice, constrained loss $\\mathcal{H}$ -consistency bounds also exhibit a square-root growth rate. ", "page_idx": 8}, {"type": "text", "text": "6 Minimizability gaps ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As shown in Sections 4 and 5, $\\mathcal{H}$ -consistency bounds for smooth loss functions in both binary and multi-class classification all admit a square-root growth rate near zero. In this section, we start by examining how the number of classes impacts these bounds. We then turn our attention to the minimizability gaps, which are the only distinguishing factors between the bounds. ", "page_idx": 8}, {"type": "text", "text": "6.1 Dependency on number of classes ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Even with identical growth rates, surrogate losses can vary in their $\\mathcal{H}$ -consistency bounds due to the number of classes. This factor becomes crucial to consider when the class count is large. Consider the family of comp-sum loss functions $\\ell_{\\tau}^{\\mathrm{comp}}$ with $\\tau\\in[0,2)$ , defined as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\ell_{\\tau}^{\\mathrm{{comp}}}(h,x,y)=\\Phi^{\\tau}\\!\\left(\\frac{e^{h(x,y)}}{\\sum_{y^{\\prime}\\in\\mathcal{Y}}e^{h(x,y^{\\prime})}}\\right)=\\left\\{\\frac{1}{1-\\tau}\\!\\left(\\left[\\sum_{y^{\\prime}\\in\\mathcal{Y}}e^{h(x,y^{\\prime})-h(x,y)}\\right]^{1-\\tau}-1\\right)\\quad\\tau\\neq1,\\tau\\in[0,2)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\begin{array}{r}{\\Phi^{\\tau}(u)=-\\log(u)1_{\\tau=1}+\\frac{1}{1-\\tau}\\big(u^{\\tau-1}-1\\big)1_{\\tau\\neq1}}\\end{array}$ , for any $\\tau\\in[0,2)$ . Mao et al. [2023f, Eq. (7) & Theorem 3.1], established the following bound for any $h\\in\\mathcal{H}$ and $\\tau\\in[1,2)$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{\\ell_{0-1}}(h)-\\mathcal{R}_{\\ell_{0-1}}^{*}(\\mathcal{H})\\leq\\widetilde{\\Gamma}_{\\tau}\\big(\\mathcal{R}_{\\ell_{\\tau}^{\\mathrm{comp}}}(h)-\\mathcal{R}_{\\ell_{\\tau}^{\\mathrm{comp}}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{\\tau}^{\\mathrm{comp}}}(\\mathcal{H})\\big)-\\mathcal{M}_{\\ell_{0-1}}(\\mathcal{H}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\widetilde{\\Gamma}_{\\tau}(t)=\\sqrt{2n^{\\tau-1}t}$ . Thus, while all these loss functions show square-root growth, the number of classes acts as a critical scaling factor. ", "page_idx": 8}, {"type": "text", "text": "6.2 Comparison across comp-sum losses ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Appendix $\\mathrm{H}$ , we compare minimizability gaps cross comp-sum losses. We will see that minimizability gaps decrease as $\\tau$ increases. This might suggest favoring $\\tau$ close to 2. But when accounting for $n$ , $\\ell_{\\tau}^{\\mathrm{comp}}$ with $\\tau=1$ (logistic loss) is optimal since $n$ then vanishes. Thus, both class count and minimizability gaps are essential in loss selection. In Appendix J, we will show that the minimizability gaps can become zero or relatively small under certain conditions in multi-class classification. In such scenarios, the logistic loss is favored, which can partly explain its widespread practical application. ", "page_idx": 8}, {"type": "text", "text": "6.3 Small surrogate minimizability gaps ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While minimizability gaps vanish in special scenarios (e.g., unrestricted hypothesis sets, best-inclass error matching Bayes error), we now seek broader conditions for zero or small surrogate minimizability gaps to make our bounds more meaningful. ", "page_idx": 8}, {"type": "text", "text": "Due to space constraints, we focus on binary classification here, with multi-class results given in Appendix J. We address pointwise surrogate losses which take the form $\\ell(h(x),y)$ for a labeled point $(x,y)$ . We write $A=\\{h(x){:}h\\in\\mathcal{H}\\}$ to denote the set of predictor values at $x$ , which we assume to be independent of $x$ . All proofs for this section are presented in Appendix I. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Deterministic scenario. We first consider the deterministic scenario, where the conditional probability $p(\\boldsymbol{y}\\left|\\boldsymbol{x}\\right)$ is either zero or one. For a deterministic distribution, we denote by $\\mathfrak{X}_{+}$ the subset of $\\mathcal{X}$ over which the label is $+1$ and by $\\mathcal{X}_{-}$ the subset of $\\mathcal{X}$ over which the label is $-1$ . For convenience, let $\\ell_{+}=\\operatorname*{inf}_{\\alpha\\in A}\\ell(\\alpha,+1)$ and $\\ell_{-}=\\operatorname*{inf}_{\\alpha\\in A}\\ell(\\alpha,-1)$ . ", "page_idx": 9}, {"type": "text", "text": "Theorem 6.1. Assume that $\\mathcal{D}$ is deterministic and that the best-in-class error is achieved by some $h^{*}\\in\\mathcal{H}$ . Then, the minimizability gap is null, $\\mathbf{\\mathcal{M}}(\\mathbf{\\mathcal{H}})=0,$ , iff ", "page_idx": 9}, {"type": "text", "text": "If further $\\alpha\\mapsto\\ell(\\alpha,+1)$ and $\\alpha\\mapsto\\ell(\\alpha,-1)$ are injective and $\\ell_{+}\\,=\\,\\ell(\\alpha_{+},+1)$ , $\\ell_{-}\\,=\\,\\ell(\\alpha_{-},-1)$ , then, the condition is equivalent to $h^{*}(x)=\\alpha_{+}\\boldsymbol{1}_{x\\in\\mathcal{X}_{+}}+\\alpha_{-}\\boldsymbol{1}_{x\\in\\mathcal{X}_{-}}$ Furthermore, the minimizability gap is bounded by $\\epsilon$ iff $p\\big(\\mathbb{E}\\big[\\ell(h^{*}(x),+1)\\mid y=+1\\big]-\\bar{\\ell}_{+}\\big)+\\big(1-p\\big)\\big(\\mathbb{E}\\big[\\ell(h^{*}(x),-1)\\mid y=-1\\big]-\\ell_{-}\\big)\\le\\epsilon\\big)-1\\,,$ . In particular, the condition implies: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(h^{*}(x),+1)\\mid y=+1]-\\ell_{+}\\le\\frac{\\epsilon}{p}\\quad a n d\\quad\\mathbb{E}[\\ell(h^{*}(x),-1)\\mid y=-1]-\\ell_{-}\\le\\frac{\\epsilon}{1-p}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The theorem suggests that, under those assumptions, for the surrogate minimizability gap to be zero, the best-in-class hypothesis must be piecewise constant with specific values on $\\mathfrak{X}_{+}$ and $\\mathcal{X}_{-}$ . The existence of such a hypothesis in $\\mathcal{H}$ depends both on the complexity of the decision surface separating $\\mathfrak{X}_{+}$ and $\\mathcal{X}_{-}$ and on that of the hypothesis set $\\mathcal{H}$ . More generally, when the best-in-class classifier $\\epsilon$ -approximates $\\alpha_{+}$ over $\\mathcal{X}_{+}$ and $\\alpha_{-}$ over $\\mathcal{X}_{-}$ , then the minimizability gap is bounded by $\\epsilon$ . As an example, when the decision surface is a hyperplane, a hypothesis set of linear functions combined with a sigmoid activation function can provide such a good approximation (see Figure 1 for an illustration in a simple case). ", "page_idx": 9}, {"type": "image", "img_path": "itztwTAcN6/tmp/c4c89b3d7d010eec979903a9f34d5d2eebfe91a6c4d59971c27730396dd7e018.jpg", "img_caption": ["Figure 1: Approximation provided by sigmoid activation function. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Stochastic scenario. Here, we present a general result that is a direct extension of that of the deterministic scenario. We show that the minimizability gap is zero when there exists $h^{*}\\in\\mathcal{H}$ that matches $\\alpha^{*}(x)$ for all $x$ , where $\\alpha^{*}(x)$ is the minimizer of the conditional error. We also show that the minimizability gap is bounded by $\\epsilon$ when there exists $h^{*}\\in\\mathcal{H}$ whose conditional error $\\epsilon$ -approximates best-in-class conditional error for all $x$ . ", "page_idx": 9}, {"type": "text", "text": "Theorem 6.2. The best-in-class error is achieved by some $h^{*}\\in\\mathcal{H}$ and the minimizability gap is null, $\\mathcal{M}(\\mathcal{H})=0,$ , iff there exists $h^{*}\\in\\mathcal{H}$ such that for all $x$ , ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(h^{*}(x),y)\\mid x]=\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\;a.s.\\;o\\nu e r\\,\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "If further $\\alpha\\mapsto\\mathbb{E}_{y}[\\ell(\\alpha,y)\\mid x]$ is injective and $\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}_{y}[\\ell(\\alpha,y)\\mid x]=\\mathbb{E}_{y}[\\ell(\\alpha^{*}(x),y)\\mid x]$ , then, the condition is equivalent to $\\bar{h}^{*}(x)=\\alpha^{*}(x)$ a.s. for $x\\in\\mathcal X$ . Furthermore, the minimizability gap is bounded by \u03f5, $\\begin{array}{r}{\\mathcal{M}(\\mathcal{H})\\leq\\epsilon,}\\end{array}$ , iff there exists $h^{*}\\in\\mathcal{H}$ such that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[\\mathbb{E}\\!\\left[\\ell(h^{*}(x),y)\\mid x\\right]-\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}\\!\\left[\\ell(\\alpha,y)\\mid x\\right]\\right]\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In deterministic settings, condition (4) coincides with that of Theorem 6.1. However, in stochastic scenarios, the existence of such a hypothesis depends on both decision surface complexity and the conditional distribution\u2019s properties. For illustration, see Appendix J.3 where we analyze the exponential, logistic (binary), and multi-class logistic losses. ", "page_idx": 9}, {"type": "text", "text": "We thoroughly analyzed minimizability gaps, comparing them across comp-sum losses, and identifying conditions for zero or small gaps, which help inform surrogate loss selection. In Appendix F, we show the crucial role of minimizability gaps in comparing excess bounds with $\\mathcal{H}$ -consistency bounds. Importantly, combining $\\mathcal{H}$ -consistency bounds with surrogate loss Rademacher complexity bounds yields zero-one loss (estimation) learning bounds for surrogate loss minimizers (see Appendix O). ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We established a universal square-root growth rate for the widely-used class of smooth surrogate losses in both binary and multi-class classification. This underscores the minimizability gap as a crucial discriminator among surrogate losses. Our detailed analysis of these gaps can provide guidance for loss selection. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "S. Agarwal. Surrogate regret bounds for bipartite ranking via strongly proper losses. The Journal of Machine Learning Research, 15(1):1653\u20131674, 2014.   \nP. Awasthi, N. Frank, A. Mao, M. Mohri, and Y. Zhong. Calibration and consistency of adversarial surrogate losses. Advances in Neural Information Processing Systems, pages 9804\u20139815, 2021a.   \nP. Awasthi, A. Mao, M. Mohri, and Y. Zhong. A finer calibration analysis for adversarial robustness. arXiv preprint arXiv:2105.01550, 2021b.   \nP. Awasthi, A. Mao, M. Mohri, and Y. Zhong. Multi-class $H$ -consistency bounds. In Advances in neural information processing systems, pages 782\u2013795, 2022a.   \nP. Awasthi, A. Mao, M. Mohri, and Y. Zhong. $H$ -consistency bounds for surrogate loss minimizers. In International Conference on Machine Learning, pages 1117\u20131174, 2022b.   \nP. Awasthi, A. Mao, M. Mohri, and Y. Zhong. Theoretically grounded loss functions and algorithms for adversarial robustness. In International Conference on Artificial Intelligence and Statistics, pages 10077\u201310094, 2023a.   \nP. Awasthi, A. Mao, M. Mohri, and Y. Zhong. DC-programming for neural network optimizations. Journal of Global Optimization, 2023b.   \nH. Bao. Proper losses, moduli of convexity, and surrogate regret bounds. In Conference on Learning Theory, pages 525\u2013547, 2023.   \nP. L. Bartlett and M. H. Wegkamp. Classification with a reject option using a hinge loss. Journal of Machine Learning Research, 9(8), 2008.   \nP. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138\u2013156, 2006.   \nS. Ben-David, D. Loker, N. Srebro, and K. Sridharan. Minimizing the misclassification error rate using a surrogate convex loss. In International Coference on International Conference on Machine Learning, pages 83\u201390, 2012.   \nJ. Berkson. Application of the logistic function to bio-assay. Journal of the American Statistical Association, 39:357\u2014-365, 1944.   \nJ. Berkson. Why I prefer logits to probits. Biometrics, 7(4):327\u2014-339, 1951.   \nM. Blondel. Structured prediction with projection oracles. In Advances in neural information processing systems, 2019.   \nY. Cao, T. Cai, L. Feng, L. Gu, J. Gu, B. An, G. Niu, and M. Sugiyama. Generalizing consistent multi-class classification with rejection to be compatible with arbitrary losses. In Advances in neural information processing systems, 2022.   \nN. Charoenphakdee, Z. Cui, Y. Zhang, and M. Sugiyama. Classification with rejection based on cost-sensitive classification. In International Conference on Machine Learning, pages 1507\u20131517, 2021.   \nD.-R. Chen, Q. Wu, Y. Ying, and D.-X. Zhou. Support vector machine soft margin classifiers: error analysis. The Journal of Machine Learning Research, 5:1143\u20131175, 2004.   \nC. Ciliberto, L. Rosasco, and A. Rudi. A consistent regularization approach for structured prediction. In Advances in neural information processing systems, 2016.   \nC. Ciliberto, L. Rosasco, and A. Rudi. A general framework for consistent structured prediction with implicit loss embeddings. The Journal of Machine Learning Research, 21(1):3852\u20133918, 2020.   \nC. Cortes, G. DeSalvo, and M. Mohri. Learning with rejection. In International Conference on Algorithmic Learning Theory, pages 67\u201382, 2016a.   \nC. Cortes, G. DeSalvo, and M. Mohri. Boosting with abstention. In Advances in Neural Information Processing Systems, pages 1660\u20131668, 2016b.   \nC. Cortes, G. DeSalvo, and M. Mohri. Theory and algorithms for learning with rejection in binary classification. Annals of Mathematics and Artificial Intelligence, 2023.   \nC. Cortes, A. Mao, C. Mohri, M. Mohri, and Y. Zhong. Cardinality-aware set prediction and top- $k$ classification. In Advances in neural information processing systems, 2024.   \nK. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of machine learning research, 2(Dec):265\u2013292, 2001.   \nA. L. Dontchev and R. T. Rockafellar. Robinson\u2019s implicit function theorem and its extensions. Math. Program., 117(1-2):129\u2013147, 2009.   \nJ. Duchi, K. Khosravi, and F. Ruan. Multiclass classification, information, divergence and surrogate risk. The Annals of Statistics, 46(6B):3246\u20133275, 2018.   \nJ. Finocchiaro, R. Frongillo, and B. Waggoner. An embedding framework for consistent polyhedral surrogates. In Advances in neural information processing systems, 2019.   \nR. Frongillo and B. Waggoner. Surrogate regret bounds for polyhedral losses. In Advances in Neural Information Processing Systems, volume 34, pages 21569\u201321580, 2021.   \nW. Gao and Z.-H. Zhou. On the consistency of AUC pairwise optimization. In International Joint Conference on Artificial Intelligence, 2015.   \nW. Kotlowski, K. J. Dembczynski, and E. Huellermeier. Bipartite ranking through minimization of univariate loss. In International Conference on Machine Learning, pages 1113\u20131120, 2011.   \nK. Kuratowski and C. Ryll-Nardzewski. A general theorem on selectors. Bull. Acad. Pol. Sci., S\u00e9r. Sci. Math. Astron. Phys., 13(8):397\u2013403, 1965.   \nV. Kuznetsov, M. Mohri, and U. Syed. Multi-class deep boosting. In Advances in Neural Information Processing Systems, pages 2501\u20132509, 2014.   \nM. Lapin, M. Hein, and B. Schiele. Loss functions for top-k error: Analysis and insights. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1468\u20131477, 2016.   \nY. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines: Theory and application to the classification of microarray data and satellite radiance data. Journal of the American Statistical Association, 99(465):67\u201381, 2004.   \nP. Long and R. Servedio. Consistency versus realizable H-consistency for multiclass classification. In International Conference on Machine Learning, pages 801\u2013809, 2013.   \nM. Mahdavi, L. Zhang, and R. Jin. Binary excess risk for smooth convex surrogates. arXiv preprint arXiv:1402.1792, 2014.   \nA. Mao, C. Mohri, M. Mohri, and Y. Zhong. Two-stage learning to defer with multiple experts. In Advances in neural information processing systems, 2023a.   \nA. Mao, M. Mohri, and Y. Zhong. H-consistency bounds: Characterization and extensions. In Advances in Neural Information Processing Systems, 2023b.   \nA. Mao, M. Mohri, and Y. Zhong. H-consistency bounds for pairwise misranking loss surrogates. In International conference on Machine learning, 2023c.   \nA. Mao, M. Mohri, and Y. Zhong. Ranking with abstention. In ICML 2023 Workshop The Many Facets of Preference-Based Learning, 2023d.   \nA. Mao, M. Mohri, and Y. Zhong. Structured prediction with stronger consistency guarantees. In Advances in Neural Information Processing Systems, 2023e.   \nA. Mao, M. Mohri, and Y. Zhong. Cross-entropy loss functions: Theoretical analysis and applications. In International Conference on Machine Learning, 2023f.   \nA. Mao, M. Mohri, and Y. Zhong. Principled approaches for learning to defer with multiple experts. In International Symposium on Artificial Intelligence and Mathematics, 2024a.   \nA. Mao, M. Mohri, and Y. Zhong. Predictor-rejector multi-class abstention: Theoretical analysis and algorithms. In International Conference on Algorithmic Learning Theory, 2024b.   \nA. Mao, M. Mohri, and Y. Zhong. Theoretically grounded loss functions and algorithms for scorebased multi-class abstention. In International Conference on Artificial Intelligence and Statistics, 2024c.   \nA. Mao, M. Mohri, and Y. Zhong. Enhanced $H$ -consistency bounds. arXiv preprint arXiv:2407.13722, 2024d.   \nA. Mao, M. Mohri, and Y. Zhong. $H$ -consistency guarantees for regression. In International Conference on Machine Learning, pages 34712\u201334737, 2024e.   \nA. Mao, M. Mohri, and Y. Zhong. Multi-label learning with stronger consistency guarantees. In Advances in neural information processing systems, 2024f.   \nA. Mao, M. Mohri, and Y. Zhong. Realizable $H$ -consistent and Bayes-consistent loss functions for learning to defer. In Advances in neural information processing systems, $2024\\mathrm{g}$ .   \nA. Mao, M. Mohri, and Y. Zhong. Regression with multi-expert deferral. In International Conference on Machine Learning, pages 34738\u201334759, 2024h.   \nA. K. Menon and R. C. Williamson. Bayes-optimal scorers for bipartite ranking. In Conference on Learning Theory, pages 68\u2013106, 2014.   \nC. Mohri, D. Andor, E. Choi, M. Collins, A. Mao, and Y. Zhong. Learning to reject with a fixed predictor: Application to decontextualization. In International Conference on Learning Representations, 2024.   \nM. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. MIT Press, second edition, 2018.   \nH. Mozannar and D. Sontag. Consistent estimators for learning to defer to an expert. In International Conference on Machine Learning, pages 7076\u20137087, 2020.   \nH. Mozannar, H. Lang, D. Wei, P. Sattigeri, S. Das, and D. Sontag. Who should predict? exact algorithms for learning to defer to humans. In International Conference on Artificial Intelligence and Statistics, pages 10520\u201310545, 2023.   \nC. Ni, N. Charoenphakdee, J. Honda, and M. Sugiyama. On the calibration of multiclass classification with rejection. In Advances in Neural Information Processing Systems, pages 2582\u20132592, 2019.   \nA. Nowak, F. Bach, and A. Rudi. Sharp analysis of learning with discrete losses. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1920\u20131929, 2019.   \nA. Nowak, F. Bach, and A. Rudi. Consistent structured prediction with max-min margin markov networks. In International Conference on Machine Learning, pages 7381\u20137391, 2020.   \nA. Nowak, A. Rudi, and F. Bach. On the consistency of max-margin losses. In International Conference on Artificial Intelligence and Statistics, pages 4612\u20134633, 2022.   \nA. Nowak-Vila, F. Bach, and A. Rudi. A general theory for structured prediction with smooth convex surrogates. arXiv preprint arXiv:1902.01958, 2019.   \nA. Osokin, F. Bach, and S. Lacoste-Julien. On structured prediction theory with calibrated convex surrogate losses. In Advances in Neural Information Processing Systems, 2017.   \nB. A. Pires, C. Szepesvari, and M. Ghavamzadeh. Cost-sensitive multiclass classification risk bounds. In International Conference on Machine Learning, pages 1391\u20131399, 2013.   \nH. G. Ramaswamy, A. Tewari, and S. Agarwal. Consistent algorithms for multiclass classification with an abstain option. Electronic Journal of Statistics, 12(1):530\u2013554, 2018.   \nM. D. Reid and R. C. Williamson. Surrogate regret bounds for proper losses. In International Conference on Machine Learning, pages 897\u2013904, 2009.   \nI. Steinwart. How to compare different loss functions and their risks. Constructive Approximation, 26(2):225\u2013287, 2007.   \nA. Tewari and P. L. Bartlett. On the consistency of multiclass classification methods. Journal of Machine Learning Research, 8(36):1007\u20131025, 2007.   \nK. Uematsu and Y. Lee. On theoretically optimal ranking functions in bipartite ranking. Journal of the American Statistical Association, 112(519):1311\u20131322, 2017.   \nP. F. Verhulst. Notice sur la loi que la population suit dans son accroissement. Correspondance math\u00e9matique et physique, 10:113\u2014-121, 1838.   \nP. F. Verhulst. Recherches math\u00e9matiques sur la loi d\u2019accroissement de la population. Nouveaux M\u00e9moires de l\u2019Acad\u00e9mie Royale des Sciences et Belles-Lettres de Bruxelles, 18:1\u2014-42, 1845.   \nR. Verma and E. Nalisnick. Calibrated learning to defer with one-vs-all classifiers. In International Conference on Machine Learning, pages 22184\u201322202, 2022.   \nR. Verma, D. Barrej\u00f3n, and E. Nalisnick. Learning to defer to multiple experts: Consistent surrogate losses, confidence calibration, and conformal ensembles. In International Conference on Artificial Intelligence and Statistics, pages 11415\u201311434, 2023.   \nJ. Weston and C. Watkins. Support vector machines for multi-class pattern recognition. European Symposium on Artificial Neural Networks, 4(6), 1999.   \nF. Yang and S. Koyejo. On the consistency of top-k surrogate losses. In International Conference on Machine Learning, pages 10727\u201310735, 2020.   \nJ. Yu and M. B. Blaschko. The lov\u00e1sz hinge: A novel convex surrogate for submodular losses. IEEE transactions on pattern analysis and machine intelligence, 42(3):735\u2013748, 2018.   \nM. Yuan and M. Wegkamp. Classification methods with reject option based on convex risk minimization. Journal of Machine Learning Research, 11(1), 2010.   \nJ. Zhang, T. Liu, and D. Tao. On the rates of convergence from surrogate risk minimizers to the Bayes optimal classifier. IEEE Transactions on Neural Networks and Learning Systems, 33(10): 5766\u20135774, 2021.   \nM. Zhang and S. Agarwal. Bayes consistency vs. H-consistency: The interplay between surrogate loss functions and the scoring function class. In Advances in Neural Information Processing Systems, pages 16927\u201316936, 2020.   \nT. Zhang. Statistical behavior and consistency of classification methods based on convex risk minimization. The Annals of Statistics, 32(1):56\u201385, 2004a.   \nT. Zhang. Statistical analysis of some multi-category large margin classification methods. Journal of Machine Learning Research, 5(Oct):1225\u20131251, 2004b.   \nZ. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In Advances in neural information processing systems, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents of Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Related work 16   \nB Pointwise loss functions - Proof of Lemma 2.1 17   \nC General form of $\\mathcal{H}$ -consistency bounds 18   \nD Properties of minimizability gaps 18   \nE Examples of $\\mathcal{H}$ -consistency bounds 19   \nF Comparison with excess error bounds 20   \nG Polyhedral losses versus smooth losses 21   \nH Comparison of minimizability gaps across comp-sum losses 22   \nSmall surrogate minimizability gaps: proof for binary classification 22   \nJ Small surrogate minimizability gaps: multi-class classification 24   \nJ.1 Deterministic scenario 24   \nJ.2 Stochastic scenario 25   \nJ.3 Examples 26   \nK Proof for binary margin-based losses (Theorem 4.2) 27   \nL Proof for comp-sum losses (Theorem 5.3) 28   \nM Proof for constrained losses (Theorem 5.5) 30   \nN Analysis of the function of $\\tau$ 32   \nO Generalization bounds 32   \nFuture work 33 ", "page_idx": 14}, {"type": "text", "text": "A Related work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The Bayes-consistency of surrogate losses has been extensively studied in the context of binary classification. Zhang [2004a], Bartlett et al. [2006] and Steinwart [2007] established Bayes-consistency for various convex loss functions, including margin-based surrogates. They also introduced excess error bounds (or surrogate regret bounds) for margin-based surrogates. Other works include Chen et al. [2004], which specifically studied the SVM $q$ -norm soft margin loss and established a square-root excess error bound with an optimal constant for that specific family, and Reid and Williamson [2009], which established tight excess error bounds for proper losses in binary classification. ", "page_idx": 15}, {"type": "text", "text": "The Bayes-consistency of several surrogate loss function families in the context of multi-class classification has also been studied by Zhang [2004b] and Tewari and Bartlett [2007]. Zhang [2004b] established a series of results for various multi-class classification formulations, including negative results for multi-class hinge loss functions [Crammer and Singer, 2001], as well as positive results for the sum exponential loss [Weston and Watkins, 1999, Awasthi, Mao, Mohri, and Zhong, 2022a], the (multinomial) logistic loss [Verhulst, 1838, 1845, Berkson, 1944, 1951], and the constrained losses [Lee et al., 2004]. Later, Tewari and Bartlett [2007] adopted a different geometric method to analyze Bayes-consistency, yielding similar results for these loss function families. Steinwart [2007] developed general tools to characterize Bayes consistency for both binary and multi-class classification. Additionally, excess error bounds have been derived by Pires et al. [2013] for a family of constrained losses and by Duchi et al. [2018] for loss functions related to generalized entropies. ", "page_idx": 15}, {"type": "text", "text": "For a surrogate loss $\\ell$ , an excess error bound holds for any predictor $h$ and has the form $\\mathcal{E}_{\\ell_{0-1}}(h)-$ $\\mathcal{E}_{\\ell_{0-1}}^{*}\\!\\leq\\Psi\\big(\\bar{\\mathcal{E}_{\\ell}}(h)-\\mathcal{E}_{\\ell}^{*}\\big)$ , where $\\mathcal{E}_{\\ell_{0-1}}(h)$ and $\\mathcal{E}_{\\ell}(h)$ represent the expected losses of $h$ for the zero-one loss and surrogate loss respectively, $\\mathcal{E}_{\\ell_{9^{-1}}}^{*}$ and $\\mathcal{E}_{\\ell}^{*}$ the Bayes errors for the zero-one and surrogate loss respectively, and $\\Psi$ a non-decreasing function. ", "page_idx": 15}, {"type": "text", "text": "The growth rate of excess error bounds, that is the behavior of function $\\Psi$ near zero, has gained attention in recent research [Mahdavi et al., 2014, Zhang et al., 2021, Frongillo and Waggoner, 2021, Bao, 2023]. Mahdavi et al. [2014] examined the growth rate for smoothed hinge losses in binary classification, demonstrating that smoother losses result in worse growth rates. The optimal rate is achieved with the standard hinge loss, which exhibits linear growth. Zhang et al. [2021] tied the growth rate of excess error bounds in binary classification to two properties of the surrogate loss function: consistency intensity and conductivity. These metrics enable comparisons of growth rates across different surrogates. But, can we establish lower and upper bounds for the growth rate of excess error bounds under specific regularity conditions? ", "page_idx": 15}, {"type": "text", "text": "Frongillo and Waggoner [2021] pioneered research on this question in binary classification settings. They established a critical square-root lower bound for excess error bounds when a surrogate loss is locally strongly convex and has a locally Lipschitz gradient. Additionally, they demonstrated a linear excess error bound for Bayes-consistent polyhedral loss functions (convex and piecewise-linear) [Finocchiaro et al., 2019] (see also [Lapin et al., 2016, Ramaswamy et al., 2018, Yu and Blaschko, 2018, Yang and Koyejo, 2020]). More recently, Bao [2023] complemented these results by showing that proper losses associated with Shannon entropy, exponential entropy, spherical entropy, squared $\\alpha$ -norm entropies and $\\alpha$ -polynomial entropies, with $\\alpha>1$ , also exhibit a square-root lower bound for excess error bounds relative to the $\\ell_{1}$ -distance. ", "page_idx": 15}, {"type": "text", "text": "However, while Bayes-consistency and excess error bounds are valuable, they are not sufficiently informative, as they are established for the family of all measurable functions and disregard the crucial role played by restricted hypothesis sets in learning. As pointed out by Long and Servedio [2013], in some cases, minimizing Bayes-consistent losses can result in constant expected error, while minimizing inconsistent losses can yield an expected loss approaching zero. To address this limitation, the authors introduced the concept of realizable $\\mathcal{H}$ -consistency, further explored by Kuznetsov et al. [2014] and Zhang and Agarwal [2020]. Nonetheless, these guarantees are only asymptotic and rely on a strong realizability assumption that typically does not hold in practice. ", "page_idx": 15}, {"type": "text", "text": "Recent research by Awasthi, Mao, Mohri, and Zhong [2022b,a] and Mao, Mohri, and Zhong [2023f,c,e,b] has instead introduced and analyzed $\\mathcal{H}$ -consistency bounds. These bounds are more informative than Bayes-consistency since they are hypothesis set-specific and non-asymptotic. Their work covers broad families of surrogate losses in binary classication, multi-class classification, structured prediction, and abstention [Mao, Mohri, Mohri, and Zhong, 2023a]. Crucially, they provide upper bounds on the estimation error of the target loss, for example, the zero-one loss in classification, that hold for any predictor $h$ within a hypothesis set $\\mathcal{H}$ . These bounds relate this estimation error to the surrogate loss estimation error. Their general form is: $\\mathcal{E}_{\\ell_{0-1}}(h)-\\mathcal{E}_{\\ell_{0-1}}^{*}(\\mathcal{H})\\leq f(\\mathcal{E}_{\\ell}(h)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{H}))$ , where $\\mathcal{E}_{\\ell_{0-1}}^{*}(\\mathcal{H})$ and $\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})$ represent the best-in-class expected losses for the zero-one and surrogate loss respectively, and $f$ is a non-decreasing function continuous at zero. $\\mathcal{H}$ -consistency bounds imply in particular excess error bounds, when the hypothesis set is taken to be the family of all measurable functions. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "The authors have further analyzed $\\mathcal{H}$ -consistency bounds in structured prediction, ranking, and abstention. ", "page_idx": 16}, {"type": "text", "text": "Mao, Mohri, and Zhong [2023e] revealed limitations of existing structured prediction loss functions, demonstrating they lack Bayes-consistency. They introduced new surrogate loss families proven to benefit form $\\mathcal{H}$ -consistency bounds, thus also establishing Bayes-consistency. This complements earlier negative finding about the Bayes-consistency of Struct-SVM and positive results for quadratic surrogate (QS) losses or some non-smooth polyhedral-type loss functions [Osokin et al., 2017, Ciliberto et al., 2016, Blondel, 2019, Nowak-Vila et al., 2019, Nowak et al., 2019, 2020, Ciliberto et al., 2020, Nowak et al., 2022]. ", "page_idx": 16}, {"type": "text", "text": "Mao et al. [2023c] showed that there are no meaningful $\\mathcal{H}$ -consistency bounds for general pairwise ranking and bipartite ranking surrogate losses with equicontinuous hypothesis sets, including linear models and neural networks. They proposed ranking with abstention as a solution. These results demonstrated that although these surrogate loss functions have been shown to be Bayes-consistent in various studies [Kotlowski et al., 2011, Menon and Williamson, 2014, Agarwal, 2014, Gao and Zhou, 2015, Uematsu and Lee, 2017], they are, in fact, not $\\mathcal{H}$ -consistent. ", "page_idx": 16}, {"type": "text", "text": "Mao et al. [2023a] applied $\\mathcal{H}$ -consistency bounds to two-stage learning to defer scenarios, designing new surrogate losses. This complemented the Bayes-consistent surrogate losses in the single-stage scenario of learning to defer [Mozannar and Sontag, 2020, Verma and Nalisnick, 2022, Verma et al., 2023, Mozannar et al., 2023] or learning with abstention [Bartlett and Wegkamp, 2008, Yuan and Wegkamp, 2010, Cortes et al., 2016a,b, Ramaswamy et al., 2018, Ni et al., 2019, Charoenphakdee et al., 2021, Cao et al., 2022, Cortes et al., 2023]. ", "page_idx": 16}, {"type": "text", "text": "Mao, Mohri, and Zhong [2024d] recently established enhanced $\\mathcal{H}$ -consistency bounds based on more general inequalities relating conditional regrets. They derived more favorable distribution- and predictor-dependent bounds in various scenarios including standard multi-class classification, binary and multi-class classification under Tsybakov noise conditions, and bipartite ranking. $\\mathcal{H}$ -consistency bounds have also been studied in the scenario of adversarial robustness [Awasthi et al., 2021a,b, 2023a,b], bounded regression [Mao et al., 2024e], regression with multi-expert deferral [Mao et al., 2024h], top- $k$ classification [Cortes et al., 2024], multi-label learning [Mao et al., 2024f], score-based abstention [Mao et al., 2024c], predictor-rejector abstention [Mao et al., 2024b], learning to abstain with a fixed predictor with application in decontextualization [Mohri et al., 2024], ranking with abstention [Mao et al., 2023d], realizable learning to defer [Mao et al., $2024\\mathrm{g}]$ , and learning to defer with multiple experts [Mao et al., 2024a]. ", "page_idx": 16}, {"type": "text", "text": "This papers presents a characterization of the growth rate of $\\mathcal{H}$ -consistency bounds, that is how quickly the functions $f$ increase near zero, both in binary and multi-class classification. ", "page_idx": 16}, {"type": "text", "text": "B Pointwise loss functions - Proof of Lemma 2.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 2.1. Let \u2113be a pointwise loss function. Then, the Bayes error and the approximation error can be expressed as follows: $\\mathcal{E}_{\\ell}^{\\,*}\\big(\\mathcal{H}_{\\mathrm{all}}\\big)\\,=\\mathbb{E}_{\\boldsymbol{x}}\\big[\\mathcal{C}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}},\\boldsymbol{x}\\big)\\big]$ and $\\mathcal{A}_{\\ell}\\big(\\mathcal{H}\\big)=\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}\\big)-\\mathbb{E}_{x}\\big[\\mathcal{\\mathrm{C}}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}},x\\big)\\big]$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. By definition, for a pointwise loss function $\\ell$ , there exists a measurable function $\\hat{\\ell}\\colon\\mathbb{R}^{n}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}_{+}$ such that $\\ell(h,x,y)\\,=\\,{\\hat{\\ell}}(h(x),y)$ , where $h(x)\\,=\\,\\bigl[h(x,1),\\cdot\\cdot\\cdot,h(x,n)\\bigr]$ is the score vector of the predictor $h$ . Thus, the following inequality holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{C}_{\\ell}^{\\ast}\\big(\\mathcal{H}_{\\mathrm{all}},x\\big)=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}[\\ell(h,x,y)\\mid x]=\\operatorname*{inf}_{\\alpha\\in\\mathbb{R}^{n}}\\mathbb{E}\\big[\\widehat{\\ell}(\\alpha,y)\\mid x\\big].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\hat{\\ell}\\colon(\\alpha,y)\\mapsto\\mathbb{R}_{+}$ is measurable, the function $\\left(\\alpha,x\\right)\\mapsto\\mathbb{E}_{y}\\bigl[\\hat{\\ell}(\\alpha,y)\\mid x\\bigr]$ is also measurable. We now show that the function $x\\mapsto\\mathcal{C}_{\\ell}^{*}(\\mathcal{H}_{\\mathrm{all}},x)=\\operatorname*{inf}_{\\alpha\\in\\mathbb{R}}\\mathbb{E}_{y}\\big[\\hat{\\ell}(\\alpha,y)\\mid x\\big]$ is also measurable. ", "page_idx": 16}, {"type": "text", "text": "To do this, we consider for any $\\beta>0$ , the set $\\left\\{x{\\because}\\operatorname*{inf}_{\\alpha\\in\\mathbb{R}}\\mathbb{E}_{y}\\bigl[\\hat{\\ell}(\\alpha,y)\\mid x\\bigr]<\\beta\\right\\}$ which can be expressed as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{x\\colon\\underset{\\alpha\\in\\mathbb{R}^{n}}{\\mathrm{inf}}\\;\\underline{{\\mathbb{E}}}\\big[\\widehat{\\ell}(\\alpha,y)\\mid x\\big]<\\beta\\right\\}=\\left\\{x{\\colon}\\exists\\alpha\\in\\mathbb{R}^{n}\\mathrm{~such~that~}\\underline{{\\mathbb{E}}}\\big[\\widehat{\\ell}(\\alpha,y)\\mid x\\big]<\\beta\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\Pi_{\\mathcal{X}}\\bigg\\{(\\alpha,x){\\colon}\\frac{\\mathbb{E}}{y}\\big[\\widehat{\\ell}(\\alpha,y)\\mid x\\big]<\\beta\\bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Pi_{\\mathcal{X}}$ is the projection onto $\\mathcal{X}$ . By the measurable projection theorem, $x\\quad\\mapsto$ $\\operatorname*{inf}_{\\alpha\\in\\mathbb{R}^{n}}\\mathbb{E}_{y}[{\\hat{\\ell}}(\\alpha,y)\\mid x]$ is measurable. Then, since a pointwise difference of measurable functions is measurable, for all $n\\in\\mathbb{N}$ , the set $\\begin{array}{r}{\\big\\{(\\alpha,x){\\colon}\\mathbb{E}_{y}\\big[\\widehat{\\ell}(\\alpha,y)\\mid x\\big]<\\operatorname*{inf}_{\\alpha\\in\\mathbb{R}^{n}}\\mathbb{E}_{y}\\big[\\widehat{\\ell}(\\alpha,y)\\mid x\\big]+\\frac{1}{n}\\big\\}.}\\end{array}$ is measurable. Thus, by Kuratowski and Ryll-Nardzewski [1965]\u2019s measurable selection theorem, for all $n\\in\\mathbb{N}$ , there exists a measurable function $h_{n}\\colon x\\mapsto\\alpha\\in\\mathbb{R}^{n}$ such that the following holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{\\mathcal{C}}_{\\ell}(h_{n},x)=\\mathbb{E}_{y}\\big[\\hat{\\ell}(\\alpha,y)\\mid x\\big]<\\operatorname*{inf}_{\\alpha\\in\\mathbb{R}^{n}}\\mathbb{E}\\big[\\hat{\\ell}(\\alpha,y)\\mid x\\big]+\\frac{1}{n}=\\mathcal{C}_{\\ell}^{*}(\\mathcal{H}_{\\mathrm{all}},x)+\\frac{1}{n}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\ell}^{*}(\\mathcal{H}_{\\mathrm{all}})\\leq\\mathbb{E}\\big[\\mathcal{C}_{\\ell}\\big(h_{n},x\\big)\\big]\\leq\\mathbb{E}\\big[\\mathcal{C}_{\\ell}^{*}(\\mathcal{H}_{\\mathrm{all}},x)\\big]+\\frac{1}{n}\\leq\\mathcal{E}_{\\ell}^{*}(\\mathcal{H}_{\\mathrm{all}})+\\frac{1}{n}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By taking the limit $n\\,\\rightarrow\\,+\\infty$ , we obtain $\\mathcal{E}_{\\ell}^{\\,*}\\!\\left(\\mathcal{H}_{\\mathrm{all}}\\right)\\;=\\;\\mathbb{E}_{\\boldsymbol{x}}\\!\\left[\\mathcal{C}_{\\ell}^{*}\\!\\left(\\mathcal{H}_{\\mathrm{all}},\\boldsymbol{x}\\right)\\right]$ . By definition, $\\mathbf{\\mathcal{A}}_{\\ell}(\\mathbf{\\mathcal{H}})\\;=\\;$ $\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}\\big)-\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}}\\big)=\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}\\big)-\\mathbb{E}_{x}\\big[\\mathcal{\\mathrm{C}}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}},\\tilde{x}\\big)\\big]$ . This completes the proof. ", "page_idx": 17}, {"type": "text", "text": "C General form of $\\mathcal{H}$ -consistency bounds ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Fix a target loss function $\\ell_{2}$ and a surrogate loss $\\ell_{1}$ . Given a hypothesis set $\\mathcal{H}$ , a bound relating the estimation errors of these loss functions admits the following form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{K},\\quad\\mathcal{E}_{\\ell_{2}}\\big(h\\big)-\\mathcal{E}_{\\ell_{2}}^{*}\\big(\\mathcal{K}\\big)\\leq\\Gamma_{\\mathcal{D}}\\big(\\mathcal{E}_{\\ell_{1}}\\big(h\\big)-\\mathcal{E}_{\\ell_{1}}^{*}\\big(\\mathcal{K}\\big)\\big),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where, for any distribution $\\mathcal{D}$ , $\\Gamma_{\\mathrm{\\mathcal{D}}}\\colon\\mathbb{R}_{+}\\rightarrow\\mathbb{R}_{+}$ is a non-decreasing function on $\\mathbb{R}_{+}$ . We will assume that $\\Gamma_{\\mathcal{D}}$ is concave. In particular, the bound should hold for any point mass distribution $\\delta_{x}$ , $x\\in\\mathcal{X}$ . We will operate under the assumption that the same bound holds uniformly over $\\mathcal{X}$ and thus that there exists a fixed concave function $\\Gamma$ such that $\\Gamma_{\\delta_{x}}=\\Gamma$ for all $x$ . ", "page_idx": 17}, {"type": "text", "text": "Observe that for any point mass distribution $\\delta_{x}$ , the conditional loss and the expected loss coincide and therefore that we have $\\mathcal{E}_{\\ell_{2}}(h)-\\mathcal{E}_{\\ell_{2}}^{*}(\\mathcal{H})=\\Delta\\mathcal{C}_{\\ell_{2},\\mathcal{H}}(\\mathcal{H},x)$ , and similarly with $\\ell_{1}$ . Thus, we can write: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{H},\\forall x\\in\\mathcal{X},\\quad\\Delta\\mathcal{C}_{\\ell_{2},\\mathcal{K}}(h,x)\\leq\\Gamma\\big(\\Delta\\mathcal{C}_{\\ell_{1},\\mathcal{K}}(h,x)\\big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, by Jensen\u2019s inequality, for any distribution $\\mathcal{D}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{K},\\forall x\\in\\mathcal{X},\\quad\\mathbb{E}\\big[\\Delta\\mathcal{C}_{\\ell_{2},\\mathcal{K}}(h,x)\\big]\\leq\\underline{{\\mathbb{E}}}\\big[\\Gamma\\big(\\Delta\\mathcal{C}_{\\ell_{1},\\mathcal{K}}(h,x)\\big)\\big]\\leq\\Gamma\\bigg(\\underline{{\\mathbb{E}}}\\big[\\Delta\\mathcal{C}_{\\ell_{1},\\mathcal{K}}(h,x)\\big]\\bigg).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\begin{array}{r}{\\mathbb{E}_{x}[\\Delta\\mathcal{C}_{\\ell_{2},\\mathcal{K}}(h,x)]=\\mathcal{E}_{\\ell_{2}}(h)\\!-\\!\\mathcal{E}_{\\ell_{2}}^{*}(\\mathcal{K})\\!+\\!\\mathcal{M}_{\\ell_{2}}(\\mathcal{K})}\\end{array}$ and similarly with $\\ell_{1}$ , we obtain the following bound for all distributions $\\mathcal{D}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{K},\\quad\\mathcal{E}_{\\ell_{2}}(h)-\\mathcal{E}_{\\ell_{2}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\ell_{2}}(\\mathcal{K})\\leq\\Gamma\\big(\\mathcal{E}_{\\ell_{1}}(h)-\\mathcal{E}_{\\ell_{1}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\ell_{1}}(\\mathcal{K})\\big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This leads to the general form of $\\mathcal{H}$ -consistency bounds that we will be considering, which includes the key role of the minimizability gaps. ", "page_idx": 17}, {"type": "text", "text": "D Properties of minimizability gaps ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "By Lemma 2.1, for a pointwise loss function, we have $\\mathcal{E}_{\\ell}^{\\,*}\\!\\left(\\mathcal{H}_{\\mathrm{all}}\\right)\\;=\\;\\mathbb{E}_{\\boldsymbol{x}}\\!\\left[\\mathcal{C}_{\\ell}^{*}\\!\\left(\\mathcal{H}_{\\mathrm{all}},\\boldsymbol{x}\\right)\\right]$ , thus the minimizability gap vanishes for the family of all measurable functions. ", "page_idx": 17}, {"type": "text", "text": "Lemma D.1. Let \u2113be a pointwise loss function. Then, we have $\\mathfrak{M}_{\\ell}(\\mathcal{H}_{\\mathrm{all}})=0$ . ", "page_idx": 17}, {"type": "table", "img_path": "itztwTAcN6/tmp/1a05bab32a12dcbd1c6eacd6997d1777dec8f69d9ac3d4a1f5d8b8e6031c6bdc.jpg", "table_caption": ["Table 1: Examples of $\\mathcal{H}$ -consistency bounds for binary margin-based losses. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Thus, in that case, (6) takes the following simpler form: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{H},\\quad\\mathcal{E}_{\\ell_{2}}(h)-\\mathcal{E}_{\\ell_{2}}^{*}(\\mathcal{H}_{\\mathrm{all}})\\leq\\Gamma\\big(\\mathcal{E}_{\\ell_{1}}(h)-\\mathcal{E}_{\\ell_{1}}^{*}(\\mathcal{H}_{\\mathrm{all}})\\big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In general, however, the minimizabiliy gap is non-zero for a restricted hypothesis set $\\mathcal{H}$ and is therefore important to analyze. Let $\\mathcal{I}_{\\ell}(\\mathcal{H})$ be the difference of pointwise infima $\\mathcal{I}_{\\ell}(\\mathcal{H})\\;=\\;$ $\\mathbb{E}_{x}\\big[\\mathcal{\\mathrm{C}}_{\\ell}^{*}\\big(\\mathcal{H},x\\big)-\\bar{\\mathcal{\\mathrm{C}}}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}},x\\big)\\big]$ , which is non-negative. Note that, for a pointwise loss function, the minimizability gap can be decomposed as follows in terms of the approximation error and the difference of pointwise infima: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{M}_{\\ell}\\big(\\mathcal{H}\\big)=\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}\\big)-\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}}\\big)+\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}}\\big)-\\underline{{\\mathbb{E}}}\\big[\\mathcal{C}_{\\ell}^{*}(\\mathcal{H},x)\\big]}\\\\ &{\\qquad\\qquad=\\mathcal{A}_{\\ell}\\big(\\mathcal{H}\\big)+\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}}\\big)-\\underline{{\\mathbb{E}}}\\big[\\mathcal{C}_{\\ell}^{*}\\big(\\mathcal{H},x\\big)\\big]}\\\\ &{\\qquad\\qquad=\\mathcal{A}_{\\ell}\\big(\\mathcal{H}\\big)-\\mathcal{I}_{\\ell}\\big(\\mathcal{H}\\big)\\leq\\mathcal{A}_{\\ell}\\big(\\mathcal{H}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, the minimizabiliy gap can be upper bounded by the approximation error. It is however a finer quantity than the approximation error and can lead to more favorable guarantees. When the difference of pointwise infima can be evaluated or bounded, this decomposition can provide a convenient way to analyze the minimizability gap in terms of the approximation error. ", "page_idx": 18}, {"type": "text", "text": "Note that $\\mathcal{I}_{\\ell}(\\mathcal{H})$ can be non-zero for families of bounded functions. Let $\\mathcal{Y}\\,=\\,\\left\\{-1,+1\\right\\}$ and $\\mathcal{H}$ be a family of functions $h$ with $|h(x)|\\,\\leq\\,\\Lambda$ for all $x\\,\\in\\,\\mathfrak{X}$ and such that all values in $[-\\Lambda,+\\Lambda]$ can be reached. Consider for example the exponential-based margin loss: $\\ell(h,x,y)\\,=\\,e^{-y h(x)}$ . Let $\\eta(x)=\\mathfrak{p}\\big(+1\\,|\\,x\\big)\\,=\\,\\mathrm{\\mathcal{D}}\\big(Y\\,=\\,+1\\,|\\,X\\,=\\,x\\big)$ \u221a. Thus, $\\;\\mathcal{\\ C}_{\\ell}(h,x)=\\eta(x)e^{-h(x)}+(1-\\eta(x))e^{h(x)}$ . Then, it is not hard to see that $\\mathcal{\\ C}_{\\ell}^{*}\\big(\\mathcal{H}_{\\mathrm{all}},x\\big)=2\\sqrt{\\eta(x)\\big(1-\\eta(x)\\big)}$ for all $x$ but $\\mathcal{C}_{\\ell}^{*}(\\mathcal{H},x)$ depends on $\\Lambda$ with the minimizing value for $h(x)$ being: $\\begin{array}{r}{\\operatorname*{min}\\Bigl\\{\\frac{1}{2}\\log\\frac{\\eta(x)}{1-\\eta(x)},\\Lambda\\Bigr\\}}\\end{array}$ if \u03b7 $\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\dot{x})\\geq1/2,\\operatorname*{max}\\!\\left\\{\\frac{1}{2}\\log\\frac{\\eta(x)}{1-\\eta(x)},-\\Lambda\\right\\}}\\end{array}$ otherwise. Thus, in the deterministic case, $\\mathcal{I}_{\\ell}(\\mathcal{H})=e^{-\\Lambda}$ . ", "page_idx": 18}, {"type": "text", "text": "When the best-in-class error coincides with the Bayes error, $\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})=\\mathcal{E}_{\\ell}^{*}(\\mathcal{H}_{\\mathrm{all}})$ , both the approximation error and minimizability gaps vanish. ", "page_idx": 18}, {"type": "text", "text": "Lemma D.2. For any loss function $\\ell$ such that $\\mathcal{E}_{\\ell}^{\\ast}\\big(\\mathcal{H}\\big)\\,=\\,\\mathcal{E}_{\\ell}^{\\ast}\\big(\\mathcal{H}_{\\mathrm{all}}\\big)\\,=\\,\\mathbb{E}_{x}\\big[\\mathcal{C}_{\\ell}^{\\ast}\\big(\\mathcal{H}_{\\mathrm{all}},x\\big)\\big]$ , we have $\\mathfrak{M}_{\\ell}\\big(\\mathcal{H}\\big)=\\mathcal{A}_{\\ell}\\big(\\mathcal{H}\\big)=0$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. By definition, $A_{\\ell}(\\mathcal{H})\\;=\\;\\mathcal{E}_{\\ell}^{\\,*}(\\mathcal{H})\\,-\\,\\mathcal{E}_{\\ell}^{\\,*}(\\mathcal{H}_{\\mathrm{all}})\\;=\\;0$ . Since we have $\\mathfrak{M}_{\\ell}(\\mathcal{H})\\,\\le\\,\\mathcal{A}_{\\ell}(\\mathcal{H})$ , this implies $\\begin{array}{r}{\\dot{\\mathcal{M}}_{\\ell}(\\mathcal{H})=0}\\end{array}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "E Examples of $\\mathcal{H}$ -consistency bounds ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here, we compile some common examples of $\\mathcal{H}$ -consistency bounds for both binary and multi-class classification. Table 1, 2 and 3 include the examples of $\\mathcal{H}$ -consistency bounds for binary margin-based losses, comp-sum losses and constrained losses, respectively. ", "page_idx": 18}, {"type": "text", "text": "These bounds are due to previous work by Awasthi et al. [2022b] for binary margin-based losses, by Mao et al. [2023f] for multi-class comp-sum losses, and by Awasthi et al. [2022a] and Mao et al. [2023b] for multi-class constrained losses, respectively. We consider complete hypothesis sets for binary classification (see Section 4), and symmetric and complete hypothesis sets for multi-class classification (see Section 5). ", "page_idx": 18}, {"type": "table", "img_path": "itztwTAcN6/tmp/b210af2aee26b76180a3fe031c0e90e4be380bd300edffdec8ed985ee12a5998.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "itztwTAcN6/tmp/fbf4998af29d126f81a6ad92c3349d05a7f2362a8c50852dae5c4b40c87f0b97.jpg", "table_caption": ["Table 2: Examples of $\\mathcal{H}$ -consistency bounds for comp-sum losses. ", "Table 3: Examples of $\\mathcal{H}$ -consistency bounds for constrained losses with $\\textstyle\\sum_{y\\in{\\mathcal{Y}}}h(x,y)=0$ . "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Comparison with excess error bounds ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Excess error bounds can be used to derive bounds for a hypothesis set $\\mathcal{H}$ expressed in terms of the approximation error. Here, we show, however, that, the resulting bounds are looser than $\\mathcal{H}$ -consistency bounds. ", "page_idx": 19}, {"type": "text", "text": "Fix a target loss function $\\ell_{2}$ and a surrogate loss $\\ell_{1}$ . Excess error bounds, also known as surrogate regret bounds, are bounds relating the excess errors of these loss functions of the following form: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{K}_{\\mathrm{all}},\\quad\\psi\\big(\\mathcal{E}_{\\ell_{2}}(h)-\\mathcal{E}_{\\ell_{2}}^{*}(\\mathcal{H}_{\\mathrm{all}})\\big)\\leq\\mathcal{E}_{\\ell_{1}}(h)-\\mathcal{E}_{\\ell_{1}}^{*}\\big(\\mathcal{H}_{\\mathrm{all}}\\big),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\psi\\colon\\mathbb{R}_{+}\\to\\mathbb{R}_{+}$ is a non-decreasing and convex function on $\\mathbb{R}_{+}$ . Recall that as shown in (1), the excess error can be written as the sum of the estimation error and the approximation error. Thus, the excess error bound can be equivalently expressed as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{K}_{\\mathrm{all}},\\quad\\psi\\big(\\mathcal{E}_{\\ell_{2}}(h)-\\mathcal{E}_{\\ell_{2}}^{*}(\\mathcal{K})+\\mathcal{A}_{\\ell_{2}}(\\mathcal{K})\\big)\\leq\\mathcal{E}_{\\ell_{1}}(h)-\\mathcal{E}_{\\ell_{1}}^{*}(\\mathcal{K})+\\mathcal{A}_{\\ell_{1}}(\\mathcal{K}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In Section 3, we have shown that the minimizabiliy gap can be upper bounded by the approximation error $\\mathfrak{M}_{\\ell}(\\mathcal{H})\\le\\mathcal{A}(\\mathcal{H})$ and is in general a finer quantity for a surrogate loss $\\ell_{1}$ . However, we will show that for a target loss $\\ell_{2}$ that is discrete, the minimizabiliy gap in general coincides with the approximation error. ", "page_idx": 19}, {"type": "text", "text": "Definition F.1. We say that a target loss $\\ell_{2}$ is discrete if we can write $\\ell_{2}(h,x,y)=\\mathsf{L}(\\mathsf{h}(x),y)$ for some binary function $\\mathsf{L}\\colon\\ Y\\times\\ Y\\to\\mathbb{R}_{+}$ . ", "page_idx": 19}, {"type": "text", "text": "In other words, a discrete target loss $\\ell_{2}$ is explicitly a function of both the prediction $\\mathsf{h}(x)$ and the true label $y$ , where both belong to the label space $\\mathcal{Y}$ . Consequently, it can assume at most $n^{2}$ distinct discrete values. ", "page_idx": 19}, {"type": "text", "text": "Next, we demonstrate that for such discrete target loss functions, if for any instance, the set of predictions generated by the hypothesis set completely spans the label space, then the minimizability gap is precisely equal to the approximation error. For convenience, we denote by $\\mathsf{H}(x)$ the set of predictions generated by the hypothesis set on input $x\\in\\mathcal X$ , defined as $\\mathsf{H}(x)=\\left\\{\\mathsf{h}(x){\\mathrm{:}}\\,h\\in\\mathcal{H}\\right\\}$ ", "page_idx": 19}, {"type": "text", "text": "Theorem F.2. Given a discrete target loss function $\\ell_{2}$ . Assume that the hypothesis set $\\mathcal{H}$ satisfies, for any $x\\in\\mathcal X$ , $\\mathsf{H}(x)=\\mathcal{Y}$ . Then, we have $\\mathcal{I}_{\\ell_{2}}(\\mathcal{H})=0$ and $\\mathcal{M}_{\\ell_{2}}(\\mathcal{H})=\\mathcal{A}_{\\ell_{2}}(\\mathcal{H})$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. As shown in Section 3, the minimizability gap can be decomposed in terms of the approximation error and the difference of pointwise infima: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{M}_{\\ell_{2}}\\big(\\mathcal{H}\\big)=\\mathcal{A}_{\\ell_{2}}\\big(\\mathcal{H}\\big)-\\mathcal{I}_{\\ell_{2}}\\big(\\mathcal{H}\\big)}\\\\ &{\\qquad\\qquad\\qquad=\\mathcal{A}_{\\ell_{2}}\\big(\\mathcal{H}\\big)-\\mathbb{E}\\Big[\\mathbb{C}_{\\ell_{2}}^{*}\\big(\\mathcal{H},x\\big)-\\mathbb{C}_{\\ell_{2}}^{*}\\big(\\mathcal{H}_{\\mathrm{all}},x\\big)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By definition and the fact that $\\ell_{2}$ is discrete, the conditional error can be written as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathfrak{C_{\\ell_{2}}}(h,x)=\\sum_{y\\in\\mathfrak{Y}}\\mathsf{p}(y|x)\\ell_{2}(h,x,y)=\\sum_{y\\in\\mathfrak{Y}}\\mathsf{p}(y|x)\\mathsf{L}(\\mathsf{h}(x),y).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, for any $x\\in\\mathcal X$ , the best-in-class conditional error can be expressed as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{C}_{\\ell_{2}}^{*}(\\mathcal{K},x)=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\sum_{y\\in\\mathfrak{Y}}\\mathsf{p}(y|x)\\mathsf{L}(\\mathsf{h}(x),y)=\\operatorname*{inf}_{y^{\\prime}\\in\\mathsf{H}(x)}\\sum_{y\\in\\mathfrak{Y}}\\mathsf{p}(y|x)\\mathsf{L}(y^{\\prime},y).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By the assumption that $\\mathsf{H}(x)=\\mathbb{y}$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathcal{X},\\quad\\mathcal{\\complement}_{\\ell_{2}}^{*}(\\mathcal{H},x)=\\operatorname*{inf}_{y^{\\prime}\\in\\mathsf{H}(x)}\\sum_{y\\in\\mathfrak{H}}\\mathsf{p}(y|x)\\mathsf{L}(y^{\\prime},y)=\\operatorname*{inf}_{y^{\\prime}\\in\\mathfrak{H}}\\sum_{y\\in\\mathfrak{H}}\\mathsf{p}(y|x)\\mathsf{L}(y^{\\prime},y)=\\mathcal{\\complement}_{\\ell_{2}}^{*}(\\mathcal{H}_{\\mathrm{all}},x).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Theorem F.2, for a target loss $\\ell_{2}$ that is discrete and hypothesis sets $\\mathcal{H}$ modulo mild assumptions, the minimizabiliy gap coincides with the approximation error. In such cases, by comparing an excess error bound (9) with the $\\mathcal{H}$ -consistency bound (6): ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi\\big(\\pounds_{\\ell_{2}}(h)-\\pounds_{\\ell_{2}}^{*}(\\mathcal{H})+\\mathscr{A}_{\\ell_{2}}(\\mathcal{H})\\big)\\leq\\pounds_{\\ell_{1}}(h)-\\pounds_{\\ell_{1}}^{*}(\\mathcal{H})+\\mathscr{A}_{\\ell_{1}}(\\mathcal{H})}\\\\ &{\\quad\\psi\\big(\\pounds_{\\ell_{2}}(h)-\\pounds_{\\ell_{2}}^{*}(\\mathcal{H})+\\mathfrak{M}_{\\ell_{2}}(\\mathcal{H})\\big)\\leq\\pounds_{\\ell_{1}}(h)-\\pounds_{\\ell_{1}}^{*}(\\mathcal{H})+\\mathfrak{M}_{\\ell_{1}}(\\mathcal{H}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we obtain that the left-hand side of both bounds are equal (since $\\mathfrak{M}_{\\ell_{2}}(\\mathcal{H})=\\mathcal{A}_{\\ell_{2}}(\\mathcal{H}))$ , while the right-hand side of the $\\mathcal{H}$ -consistency bound is always upper bounded by and can be finer than the right-hand side of the excess error bound (since $\\mathfrak{M}_{\\ell_{1}}(\\mathcal{H})\\le\\mathcal{A}_{\\ell_{1}}(\\mathcal{H}))$ , which implies that excess error bounds (or surrogate regret bounds) are in general inferior to $\\mathcal{H}$ -consistency bounds. ", "page_idx": 20}, {"type": "text", "text": "G Polyhedral losses versus smooth losses ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Since $\\mathcal{H}$ -consistency bounds subsume excess error bounds as a special case (Appendix F), the linear growth rate of polyhedral loss excess error bounds (Finocchiaro et al. [2019]) also dictates a linear growth rate for polyhedral $\\mathcal{H}$ -consistency bounds, if they exist. This is illustrated by the hinge loss or $\\rho$ -margin loss which have been shown to benefti from $\\mathcal{H}$ -consistency bounds [Awasthi et al., 2022b]. ", "page_idx": 20}, {"type": "text", "text": "Here, we compare in more detail polyhedral losses and the smooth losses. Assume that a hypothesis set $\\mathcal{H}$ is complete and thus $\\mathsf{H}(x)=\\mathsf{y}$ for any $x\\in\\mathcal X$ . By Theorem F.2, we have $\\mathcal{A}_{\\ell_{0-1}}(\\mathcal{H})=\\bar{\\mathcal{M}}_{\\ell_{0-1}}^{-}(\\mathcal{H})$ As shown by Frongillo and Waggoner [2021, Theorem 3], a Bayes-consistent polyhedral loss $\\Phi_{\\mathrm{poly}}$ admits the following linear excess error bound, for some $\\beta_{1}>0$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{K},\\,\\beta_{1}\\big(\\mathcal{E}_{\\ell_{0-1}}(h)-\\mathcal{E}_{\\ell_{0-1}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\ell_{0-1}}(\\mathcal{K})\\big)\\leq\\mathcal{E}_{\\Phi_{\\mathrm{poly}}}(h)-\\mathcal{E}_{\\Phi_{\\mathrm{poly}}}^{*}(\\mathcal{K})+\\mathcal{A}_{\\Phi_{\\mathrm{poly}}}(\\mathcal{K}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "However, for a smooth loss $\\Phi_{\\mathrm{smooth}}$ , if it satisfies the condition of Theorem 4.2, $\\Phi_{\\mathrm{smooth}}$ admits the following $\\mathcal{H}$ -consistency bound: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{K},\\,\\mathcal{T}\\big(\\mathcal{E}_{\\ell_{0-1}}(h)-\\mathcal{E}_{\\ell_{0-1}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\ell_{0-1}}(\\mathcal{K})\\big)\\leq\\mathcal{E}_{\\Phi_{\\mathrm{smoth}}}(h)-\\mathcal{E}_{\\Phi_{\\mathrm{smoth}}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\Phi_{\\mathrm{smoth}}}(\\mathcal{K}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathcal{T}(t)=\\Theta(t^{2})$ . Therefore, our theory offers a principled basis for comparing polyhedral losses (10) and smooth losses (11), which depends on the following factors: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The growth rate: linear for polyhedral losses, while square-root for smooth losses. \u2022 The optimization property: smooth losses are more favorable for optimization compared to polyhedral losses, in particular with deep neural networks. \u2022 The approximation theory: the approximation error $\\mathcal{A}_{\\Phi_{\\mathrm{poly}}}(\\mathcal{H})$ appears on the right-hand side of the bound for polyhedral losses, whereas a finer quantity, the minimizability gap $\\mathscr{M}_{\\Phi_{\\mathrm{smooth}}}(\\mathcal{H})$ , is present on the right-hand side of the bound for smooth losses. ", "page_idx": 20}, {"type": "text", "text": "H Comparison of minimizability gaps across comp-sum losses ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For $\\ell_{\\tau}^{\\mathrm{comp}}$ loss functions, $\\tau\\in[0,2)$ , we can characterize minimizability gaps as follows. ", "page_idx": 21}, {"type": "text", "text": "Theorem H.1. Assume that for any $x\\in\\mathcal X$ , we have $\\{(h(x,1),\\ldots,h(x,n))\\colon h\\in{\\mathcal{H}}\\}=[-\\Lambda,+\\Lambda]^{n}$ . Then, for comp-sum losses $\\ell_{\\tau}^{\\mathrm{comp}}$ and any deterministic distribution, the minimizability gaps can be expressed as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}_{\\ell_{\\tau}^{\\mathrm{comp}}}(\\mathcal{H})\\leq\\widetilde{\\mathcal{M}}_{\\ell_{\\tau}^{\\mathrm{comp}}}\\big(\\mathcal{H}\\big)=f_{\\tau}\\Big(\\mathfrak{R}_{\\ell_{\\tau=0}^{\\mathrm{comp}}}^{*}\\big(\\mathcal{H}\\big)\\Big)-f_{\\tau}\\Big(\\mathcal{\\Theta}_{\\ell_{\\tau=0}^{\\mathrm{comp}}}^{*}\\big(\\mathcal{H},x\\big)\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{f_{\\tau}(u)=\\log(1{+}u)1_{\\tau=1}{+}\\frac{1}{1-\\tau}\\big((1+u)^{1-\\tau}-1\\big)1_{\\tau\\neq1}}\\end{array}$ and $\\mathcal{C}_{\\ell_{\\tau=0}^{\\mathrm{comp}}}^{*}(\\mathcal{H},x)=e^{-2\\Lambda}(n{-}1)$ . Moreover, $\\widetilde{\\mathcal{M}}_{\\ell_{\\tau}^{\\mathrm{comp}}}(\\mathcal{H})$ is a non-increasing function of $\\tau$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Since $f_{\\tau}$ is concave and non-decreasing, and the equality $\\ell_{\\tau}=f_{\\tau}(\\ell_{\\tau=0})$ holds, the minimizability gaps can be upper bounded as follows, for any $\\tau\\geq\\!0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{\\sf M}_{\\ell_{\\tau}^{\\mathrm{comp}}}(\\mathcal{H})\\leq f_{\\tau}\\Big(\\mathfrak{R}_{\\ell_{\\tau=0}^{\\mathrm{comp}}}^{*}(\\mathcal{H})\\Big)-\\underline{{\\mathbb E}}\\big[\\mathcal{\\Theta}_{\\ell_{\\tau}^{\\mathrm{comp}}}^{*}\\big(\\mathcal{H},x\\big)\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since the distribution is deterministic, the conditional error can be expressed as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Theta_{\\ell_{\\tau}^{\\mathrm{comp}}}\\bigl(h,x\\bigr)=\\!f_{\\tau}\\!\\left(\\sum_{y^{\\prime}\\neq y_{\\mathrm{max}}}\\exp(h(x,y^{\\prime})-h(x,y_{\\mathrm{max}}))\\right)\\!\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $y_{\\mathrm{max}}\\,=\\,\\mathrm{argmax}\\,\\mathsf{p}(y\\,|\\,x)$ . Using the fact that $f_{\\tau}$ is increasing for any $\\tau>0$ , the hypothesis $h^{\\ast}{\\div}\\left(x,y\\right)\\mapsto\\Lambda1_{y=y_{\\operatorname*{max}}}-\\Lambda1_{y\\neq y_{\\operatorname*{max}}}$ achieves the best-in-class conditional error. Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}_{\\ell_{\\tau}^{\\mathrm{comp}}}^{*}\\left(\\mathcal{K},x\\right)=\\mathcal{C}_{\\ell_{\\tau}^{\\mathrm{comp}}}\\left(h^{*},x\\right)=f_{\\tau}\\Big(\\mathcal{C}_{\\ell_{\\tau=0}^{\\mathrm{comp}}}^{*}\\left(\\mathcal{K},x\\right)\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathcal{C}_{\\ell_{\\tau=0}^{\\mathrm{comp}}}^{*}(\\mathcal{H},x)=e^{-2\\Lambda}(n-1)$ . Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}_{\\ell_{\\tau}^{\\mathrm{comp}}}\\left(\\mathcal{H}\\right)\\leq f_{\\tau}\\Big(\\mathcal{R}_{\\ell_{\\tau=0}^{\\mathrm{comp}}}^{*}\\left(\\mathcal{H}\\right)\\Big)-f_{\\tau}\\Big(\\mathcal{\\mathcal{C}}_{\\ell_{\\tau=0}^{\\mathrm{comp}}}^{*}\\left(\\mathcal{H},x\\right)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This completes the first part of the proof. Using the fact that $\\tau\\mapsto f_{\\tau}{\\big(}u_{1}{\\big)}-f_{\\tau}{\\big(}u_{2}{\\big)}$ is a non-increasing function of $\\tau$ for any $u_{1}\\geq u_{2}\\geq0$ , the second proof is completed as well. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "The theorem shows that for comp-sum loss functions $\\ell_{\\tau}^{\\mathrm{comp}}$ , the minimizability gaps are nonincreasing with respect to $\\tau$ . Note that $\\Phi^{\\tau}$ satisfies the conditions of Theorem 5.3 for any $\\tau\\in[0,2)$ . Therefore, focusing on behavior near zero (ignoring constants), the theorem provides a principled comparison of minimizability gaps and $\\mathcal{H}$ -consistency bounds across different comp-sum losses. ", "page_idx": 21}, {"type": "text", "text": "I Small surrogate minimizability gaps: proof for binary classification ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Theorem 6.1. Assume that $\\mathcal{D}$ is deterministic and that the best-in-class error is achieved by some $h^{*}\\in\\mathcal{H}$ . Then, the minimizability gap is null, $\\mathcal{M}(\\mathcal{H})=0,$ , iff ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell(h^{*}(x),+1)=\\ell_{+}\\;a.s.\\;o\\nu e r\\,\\mathcal{X}_{+},\\quad\\ell(h^{*}(x),-1)=\\ell_{-}\\;a.s.\\;o\\nu e r\\,\\mathcal{X}_{-}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "If further $\\alpha\\mapsto\\ell(\\alpha,+1)$ and $\\alpha\\mapsto\\ell(\\alpha,-1)$ are injective and $\\ell_{+}=\\ell(\\alpha_{+},+1)$ , $\\ell_{-}\\,=\\,\\ell(\\alpha_{-},-1)$ , then, the condition is equivalent to $h^{*}(x)=\\alpha_{+}\\mathbf{1}_{x\\in\\mathcal{X}_{+}}+\\alpha_{-}\\mathbf{1}_{x\\in\\mathcal{X}_{-}}$ Furthermore, the minimizability gap is bounded by $\\epsilon$ iff $\\cdot p\\big(\\mathbb{E}[\\ell(h^{*}(x),+1)\\mid y=+1]-\\dot{\\ell}_{+}\\big)+\\big(1-p\\big)\\big(\\mathbb{E}[\\ell(h^{*}(x),-1)\\mid y=-1]-\\ell_{-}\\big)\\leq\\epsilon\\big)-p\\big(\\mathbb{E}[\\ell(h^{*}(x),-1)\\mid x=-1]-\\ell_{-}\\big)\\leq\\epsilon\\big)\\,.$ . In particular, the condition implies: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(h^{*}(x),+1)\\mid y=+1]-\\ell_{+}\\le\\frac{\\epsilon}{p}\\quad a n d\\quad\\mathbb{E}[\\ell(h^{*}(x),-1)\\mid y=-1]-\\ell_{-}\\le\\frac{\\epsilon}{1-p}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. By definition of $h^{*}$ , using the shorthand $p=\\mathbb{P}[y=+1]$ , we can write ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{h\\in\\mathcal{H}}{\\operatorname*{inf}}\\mathbb{E}[\\ell(h(x),y)]=\\mathbb{E}[\\ell(h^{*}(x),y)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=p\\,\\mathbb{E}\\big[\\ell(h^{*}(x),+1)\\mid y=+1\\big]+(1-p)\\,\\mathbb{E}\\big[\\ell(h^{*}(x),-1)\\mid y=-1\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since the distribution is deterministic, the expected pointwise infimum can be rewritten as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\!\\left[\\underset{h\\in\\mathcal{H}(\\mathcal{Y})}{\\operatorname*{inf}}\\;\\mathbb{E}[\\ell(h(x),y)\\mid x]\\right]=\\mathbb{E}\\!\\left[\\underset{\\alpha\\in A}{\\operatorname*{inf}}\\;\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\right]=p\\underset{\\alpha\\in A}{\\operatorname*{inf}}\\;\\ell(\\alpha,+1)+(1-p)\\underset{\\alpha\\in A}{\\operatorname*{inf}}\\;\\ell(\\alpha,-1)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=p\\ell_{+}+(1-p)\\ell_{-},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\ell_{+}=\\operatorname*{inf}_{\\alpha\\in A}\\ell(\\alpha,+1)$ and $\\ell_{-}=\\operatorname*{inf}_{\\alpha\\in A}\\ell(\\alpha,-1)$ . Thus, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}(\\mathcal{K})=p\\,\\mathbb{E}\\big[\\ell(h^{*}(x),+1)-\\ell_{+}\\;\\big|\\;y=+1\\big]+(1-p)\\,\\mathbb{E}\\big[\\ell(h^{*}(x),-1)-\\ell_{-}\\;\\big|\\;y=-1\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In view of that, since, by definition of $\\ell_{+}$ and $\\ell_{-}$ , the expressions within the conditional expectations are non-negative, the equality $\\mathcal{M}(\\mathcal{H})=0$ holds iff $\\ell(\\bar{h}^{*}(x),+1)-\\ell_{+}=0$ almost surely for any $x$ in $\\mathfrak{X}_{+}$ and $\\bar{\\ell}(h^{\\ast}(x),-1)^{-}\\ell_{-}=0$ almost surely for any $x$ in $\\mathcal{X}_{-}$ . This completes the first part of the proof. Furthermore, $\\mathcal{\\mathrm{M}}(\\mathcal{H})\\leq\\epsilon$ is equivalent to ", "page_idx": 22}, {"type": "equation", "text": "$$\np\\,\\mathbb{E}\\big[\\ell(h^{*}(x),+1)-\\ell_{+}\\mid y=+1\\big]+\\left(1-p\\right)\\mathbb{E}\\big[\\ell(h^{*}(x),-1)-\\ell_{-}\\mid y=-1\\big]\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "that is ", "page_idx": 22}, {"type": "equation", "text": "$$\np\\big(\\mathbb{E}\\big[\\ell(h^{*}(x),+1)\\mid y=+1\\big]-\\ell_{+}\\big)+\\big(1-p\\big)\\big(\\mathbb{E}\\big[\\ell(h^{*}(x),-1)\\mid y=-1\\big]-\\ell_{-}\\big)\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In light of the non-negativity of the expressions, this implies in particular: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(h^{*}(x),+1)\\mid y=+1]-\\ell_{+}\\le\\frac{\\epsilon}{p}\\quad\\mathrm{and}\\quad\\mathbb{E}[\\ell(h^{*}(x),-1)\\mid y=-1]-\\ell_{-}\\le\\frac{\\epsilon}{1-p}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This completes the second part of the proof. ", "page_idx": 22}, {"type": "text", "text": "Theorem 6.2. The best-in-class error is achieved by some $h^{*}\\in\\mathcal{H}$ and the minimizability gap is null, $\\mathcal{M}(\\mathcal{H})=0,$ , iff there exists $h^{*}\\in\\mathcal{H}$ such that for all $x$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(h^{*}(x),y)\\mid x]=\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\;a.s.\\;o\\nu e r\\,\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If further $\\alpha\\mapsto\\mathbb{E}_{y}[\\ell(\\alpha,y)\\mid x]$ is injective and $\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}_{y}[\\ell(\\alpha,y)\\mid x]=\\mathbb{E}_{y}[\\ell(\\alpha^{*}(x),y)\\mid x]$ , then, the condition is equivalent to $\\bar{h}^{*}(x)=\\alpha^{*}(x)$ a.s. for $x\\in\\mathcal X$ . Furthermore, the minimizability gap is bounded by \u03f5, $\\begin{array}{r}{\\mathcal{M}(\\mathcal{H})\\leq\\epsilon,}\\end{array}$ , iff there exists $h^{*}\\in\\mathcal{H}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[\\mathbb{E}\\!\\left[\\ell(h^{*}(x),y)\\mid x\\right]-\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}\\!\\left[\\ell(\\alpha,y)\\mid x\\right]\\right]\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Assume that the best-in-class error is achieved by some $h^{*}\\in\\mathcal{H}$ . Then, we can write ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in\\mathcal{K}}\\mathbb{E}\\big[\\ell(h(x),y)\\big]=\\mathbb{E}\\big[\\ell(h^{*}(x),y)\\big]=\\mathbb{E}\\Big[\\mathbb{E}\\big[\\ell(h^{*}(x),y)\\mid x\\big]\\Big].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The expected pointwise infimum can be rewritten as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}[\\ell(h(x),y)\\mid x]\\right]=\\mathbb{E}\\!\\left[\\operatorname*{inf}_{\\boldsymbol{\\alpha}\\in A}\\mathbb{E}[\\ell(\\boldsymbol{\\alpha},y)\\mid x]\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{M}(\\mathcal{H})=\\underset{x}{\\mathbb{E}}\\biggl[\\underset{y}{\\mathbb{E}}[\\ell(h^{*}(x),y)\\mid x]-\\underset{\\alpha\\in A}{\\operatorname*{inf}}\\ \\underset{y}{\\mathbb{E}}[\\ell(\\alpha,y)\\mid x]\\biggr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In view of that, since, by the definition of infimum, the expressions within the marginal expectations are non-negative, the condition that $\\mathcal{M}(\\mathcal{H})=0$ implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(h^{*}(x),y)\\mid x]=\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\;{\\mathrm{a.s.\\;over}}\\;{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "On the other hand, if there exists $h^{*}\\in\\mathcal{H}$ such that the condition (3) holds, then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall(\\mathcal{H})=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}[\\ell(h(x),y)]-\\mathbb{E}\\!\\left[\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\right]\\leq\\mathbb{E}\\!\\left[\\mathbb{E}[\\ell(h^{*}(x),y)\\mid x]-\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\right]=0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\mathcal{M}(\\mathcal{H})$ is non-negative, the inequality is achieved. Thus, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{M}(\\mathcal{H})=0\\mathrm{~and~}\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}[\\ell(h(x),y)]=\\mathbb{E}[\\ell(h^{*}(x),y)].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If there exists $h^{*}\\in\\mathcal{H}$ such that the condition (4) holds, then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\forall(\\mathcal{H})=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}[\\ell(h(x),y)]-\\mathbb{E}\\!\\left[\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\right]\\leq\\mathbb{E}\\!\\left[\\mathbb{E}[\\ell(h^{*}(x),y)\\mid x]-\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\right]=\\epsilon\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the other hand, since we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{\\sf M}(\\mathcal{H})=\\underset{x}{\\mathbb{E}}\\biggl[\\underset{y}{\\mathbb{E}}[\\ell(h^{*}(x),y)\\mid x]-\\underset{\\alpha\\in A}{\\operatorname*{inf}}\\ \\underset{y}{\\mathbb{E}}[\\ell(\\alpha,y)\\mid x]\\biggr],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$\\mathcal{\\mathrm{M}}(\\mathcal{H})\\leq\\epsilon$ implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[\\mathbb{E}\\!\\left[\\ell(h^{*}(x),y)\\mid x\\right]-\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}\\!\\left[\\ell(\\alpha,y)\\mid x\\right]\\right]\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This completes the proof. ", "page_idx": 23}, {"type": "text", "text": "J Small surrogate minimizability gaps: multi-class classification ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We consider the multi-class setting with label space $[n]=\\{1,2,\\ldots,n\\}$ . In this setting, the surrogate loss incurred by a predictor $h$ at a labeled point $(x,y)$ can be expressed by $\\ell(h\\bar{(}x),y)$ , where $h(x)=[h(x,1),\\ldots,h(x,n)]$ is the score vector of the predictor $h$ . We denote by $A$ the set of values in $\\mathbb{R}^{n}$ taken by the score vector of predictors in $\\mathcal{H}$ at $x$ , which we assume to be independent of $x$ : $A=\\{h(x){:}h\\stackrel{.}{\\in}\\mathcal{H}\\}$ , for all $x\\in\\mathcal{X}$ . ", "page_idx": 23}, {"type": "text", "text": "J.1 Deterministic scenario ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We first consider the deterministic scenario, where the conditional probability $p(\\boldsymbol{y}|\\boldsymbol{x})$ is either zero or one. For a deterministic distribution, we denote by $\\mathcal{X}_{k}$ the subset of $\\mathcal{X}$ over which the label is $k$ . For convenience, let $\\ell_{k}=\\operatorname*{inf}_{\\alpha\\in A}\\ell(\\alpha,k)$ , for any $k\\in[n]$ . ", "page_idx": 23}, {"type": "text", "text": "Theorem J.1. Assume that $\\mathcal{D}$ is deterministic and that the best-in-class error is achieved by some $h^{*}\\in\\mathcal{H}$ . Then, the minimizability gap is null, $\\mathcal{M}(\\mathcal{H})=0,$ , iff ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\forall k\\in[n],\\,\\ell(h^{*}(x),k)=\\ell_{k}\\;a.s.\\;o\\nu e r\\;\\mathcal{X}_{k}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If further $\\alpha\\mapsto\\ell(\\alpha,k)$ is injective and $\\ell_{k}=\\ell(\\alpha_{k},k)$ for all $k\\in[n].$ , then, the condition is equivalent t $o\\ \\forall k\\ \\in\\ [n],\\,h^{*}(x)\\;=\\;\\alpha_{k}$ a.s. for $x\\in\\mathcal{X}_{k}$ . Furthermore, the minimizability gap is bounded by $\\epsilon$ , $\\begin{array}{r}{\\mathcal{M}(\\mathcal{H})\\leq\\bar{\\epsilon},}\\end{array}$ , iff ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{k\\in[n]}p_{k}(\\mathbb{E}[\\ell(h^{*}(x),k)\\mid y=k]-\\ell_{k})\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In particular, the condition implies: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(h^{*}(x),k)\\mid y=k]-\\ell_{k}\\le\\frac{\\epsilon}{p_{k}},\\,\\forall k\\in[n],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. By definition of $h^{*}$ , using the shorthand $p_{k}=\\mathbb{P}[y=k]$ for any $k\\in[n]$ , we can write ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}[\\ell(h(x),y)]=\\mathbb{E}[\\ell(h^{*}(x),y)]=\\sum_{k\\in[n]}p_{k}\\,\\mathbb{E}\\big[\\ell(h^{*}(x),k)\\mid y=k\\big].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since the distribution is deterministic, the expected pointwise infimum can be rewritten as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}[\\ell(h(x),y)\\mid x]\\Big]=\\mathbb{E}\\Big[\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\Big]=\\sum_{k\\in[n]}p_{k}\\operatorname*{inf}_{\\alpha\\in A}\\ell(\\alpha,k)=\\sum_{k\\in[n]}p_{k}\\ell_{k},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\ell_{k}=\\operatorname*{inf}_{\\alpha\\in A}\\ell(\\alpha,k)$ , for any $k\\in[n]$ . Thus, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathfrak{M}(\\mathcal{H})=\\sum_{k\\in[n]}p_{k}\\,\\mathbb{E}[\\ell(h^{*}(x),k)-\\ell_{k}\\mid y=k].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In view of that, since, by definition of $\\ell_{k}$ , the expressions within the conditional expectations are non-negative, the equality $\\mathcal{M}(\\mathcal{H})=0$ holds iff $\\ell(h^{*}(x),k)-\\ell_{k}=0$ almost surely for any $x$ in $\\mathcal{X}_{k}$ , $\\forall k\\in[n]$ . Furthermore, $\\mathcal{\\mathrm{M}}(\\mathcal{H})\\leq\\epsilon$ is equivalent to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{k\\in[n]}p_{k}\\operatorname{\\mathbb{E}}[\\ell(h^{*}(x),k)-\\ell_{k}\\mid y=k]\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "that is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{k\\in[n]}p_{k}(\\mathbb{E}[\\ell(h^{*}(x),k)\\mid y=k]-\\ell_{k})\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In light of the non-negativity of the expressions, this implies in particular: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(h^{*}(x),k)\\mid y=k]-\\ell_{k}\\le\\frac{\\epsilon}{p_{k}},\\,\\forall k\\in[n].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This completes the proof. ", "page_idx": 24}, {"type": "text", "text": "The theorem suggests that, under those assumptions, for the surrogate minimizability gap to be zero, the score vector of best-in-class hypothesis must be piecewise constant with specific values on $\\mathcal{X}_{k}$ . The existence of such a hypothesis in $\\mathcal{H}$ depends both on the complexity of the decision surface separating $\\mathfrak{X}_{k}$ and on that of the hypothesis set $\\mathcal{H}$ . The theorem also suggests that when the score vector of best-in-class classifier $\\epsilon$ -approximates $\\alpha_{k}$ over $\\mathcal{X}_{k}$ for any $k\\in[\\bar{n}]$ , then the minimizability gap is bounded by $\\epsilon$ . The existence of such a hypothesis in $\\mathcal{H}$ depends on the complexity of the decision surface. ", "page_idx": 24}, {"type": "text", "text": "J.2 Stochastic scenario ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In the previous sections, we analyze instances featuring small minimizability gaps in a deterministic setting. Moving forward, we aim to extend this analysis to the stochastic scenario. We first provide two general results, which are the direct extensions of that in the deterministic scenario. The following result shows that the minimizability gap is zero when there exists $h^{*}\\in\\mathcal{H}$ that matches $\\alpha^{*}(x)$ for all $x$ , where $\\alpha^{*}(x)$ is the minimizer of the conditional error. It also shows that the minimizability gap is bounded by $\\epsilon$ when there exists $h^{*}\\in\\mathcal{H}$ whose conditional error $\\epsilon_{}$ -approximates best-in-class conditional error for all $x$ . ", "page_idx": 24}, {"type": "text", "text": "Theorem J.2. The best-in-class error is achieved by some $h^{*}\\in\\mathcal{H}$ and the minimizability gap is null, $\\mathcal{M}(\\mathcal{H})=0,$ , iff there exists $h^{*}\\in\\mathcal{H}$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(h^{*}(x),y)\\mid x]=\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\;a.s.\\;o\\nu e r\\,\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "If further $\\alpha\\mapsto\\mathbb{E}_{y}[\\ell(\\alpha,y)\\mid x]$ is injective and $\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}_{y}[\\ell(\\alpha,y)\\mid x]=\\mathbb{E}_{y}[\\ell(\\alpha^{*}(x),y)\\mid x]$ , then, the condition is equivalent to $\\bar{h}^{*}(x)=\\alpha^{*}(x)$ a.s. for $x\\in\\mathcal X$ . Furthermore, the minimizability gap is bounded by \u03f5, $\\begin{array}{r}{\\mathcal{M}(\\mathcal{H})\\leq\\epsilon_{:}}\\end{array}$ , iff there exists $h^{*}\\in\\mathcal{H}$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[\\mathbb{E}\\!\\left[\\ell(h^{*}(x),y)\\mid x\\right]-\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}\\!\\left[\\ell(\\alpha,y)\\mid x\\right]\\right]\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Assume that the best-in-class error is achieved by some $h^{*}\\in\\mathcal{H}$ . Then, we can write ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in\\mathcal{K}}\\mathbb{E}\\big[\\ell(h(x),y)\\big]=\\mathbb{E}\\big[\\ell(h^{*}(x),y)\\big]=\\mathbb{E}\\Big[\\mathbb{E}\\big[\\ell(h^{*}(x),y)\\mid x\\big]\\Big].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The expected pointwise infimum can be rewritten as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}[\\ell(h(x),y)\\mid x]\\right]=\\mathbb{E}\\!\\left[\\operatorname*{inf}_{\\boldsymbol{\\alpha}\\in A}\\mathbb{E}[\\ell(\\boldsymbol{\\alpha},y)\\mid x]\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{M}(\\mathcal{H})=\\underset{x}{\\mathbb{E}}\\biggl[\\underset{y}{\\mathbb{E}}[\\ell(h^{*}(x),y)\\mid x]-\\underset{\\alpha\\in A}{\\operatorname*{inf}}\\ \\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\biggr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In view of that, since, by the definition of infimum, the expressions within the marginal expectations are non-negative, the condition that $\\mathcal{M}(\\mathcal{H})=0$ implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(h^{*}(x),y)\\mid x]=\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\;{\\mathrm{a.s.\\;over}}\\;{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "On the other hand, if there exists $h^{*}\\in\\mathcal{H}$ such that the condition (3) holds, then, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall(\\mathcal{H})=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}[\\ell(h(x),y)]-\\mathbb{E}\\!\\left[\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\right]\\leq\\mathbb{E}\\!\\left[\\mathbb{E}[\\ell(h^{*}(x),y)\\mid x]-\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\right]=0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\mathcal{M}(\\mathcal{H})$ is non-negative, the inequality is achieved. Thus, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{M}(\\mathcal{H})=0\\mathrm{~and~}\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}[\\ell(h(x),y)]=\\mathbb{E}[\\ell(h^{*}(x),y)].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If there exists $h^{*}\\in\\mathcal{H}$ such that the condition (4) holds, then, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\forall(\\mathcal{H})=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}[\\ell(h(x),y)]-\\mathbb{E}\\!\\left[\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\right]\\leq\\mathbb{E}\\!\\left[\\mathbb{E}[\\ell(h^{*}(x),y)\\mid x]-\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\right]=\\epsilon\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "On the other hand, since we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{\\sf M}(\\mathcal{H})=\\underset{x}{\\mathbb{E}}\\biggl[\\underset{y}{\\mathbb{E}}[\\ell(h^{*}(x),y)\\mid x]-\\underset{\\alpha\\in A}{\\operatorname*{inf}}\\ \\mathbb{E}[\\ell(\\alpha,y)\\mid x]\\biggr],}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$\\mathcal{\\mathrm{M}}(\\mathcal{H})\\leq\\epsilon$ implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[\\mathbb{E}\\!\\left[\\ell(h^{*}(x),y)\\mid x\\right]-\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}\\!\\left[\\ell(\\alpha,y)\\mid x\\right]\\right]\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This completes the proof. ", "page_idx": 25}, {"type": "text", "text": "J.3 Examples ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Note that when the distribution is assumed to be deterministic, the condition (4) and condition (15) are reduced to the condition of Theorem 6.1 in binary classification and that of Theorem J.1 in multi-class classification, respectively. In the stochastic scenario, the existence of such a hypothesis not only depends on the complexity of the decision surface, but also depends on the distributional assumption on the conditional distribution $p(x)=\\big(\\mathsf{p}(y|x)\\big)_{y\\in\\mathcal{Y}}$ , where $\\bar{\\mathsf{p}}(y\\vert x)=\\mathcal{D}(Y=y\\vert X=x)$ is the conditional probability of $Y=y$ given $X=x$ . In the binary classification, we have $p(x)=$ $({\\mathsf{p}}(+1|x),{\\mathsf{p}}(-1|x{\\mathsf{\\ot{\\tau}}}))$ , where $\\mathsf{p}(+1\\mid x)+\\mathsf{p}(-1\\mid x)\\,=\\,1$ . For simplicity, we use the notation $\\eta(x)$ and $1-\\eta(x)$ to represent $\\mathsf{p}(+1\\,|\\,x)$ and $\\mathsf{p}(-1|x)$ respectively. In the multi-class classification with $y\\,=\\,\\{1,\\cdot\\cdot\\,,n\\}$ , we have $p(x)\\,=\\,{\\big(}{\\mathsf{p}}(1|x),{\\mathsf{p}}(2|x),\\dotsc,{\\mathsf{p}}(n|x){\\big)}$ where $n$ is the number of classes. As examples, here too, we examine exponential loss and logistic loss in binary classification and multi-class logistic loss in multi-class classification. ", "page_idx": 25}, {"type": "text", "text": "A. Example: binary classification. Let $\\epsilon\\in[0,\\frac{1}{2}]$ . We denote by $\\mathcal{X}_{+}$ the subset of $\\mathcal{X}$ over which $\\eta(x)=1-\\epsilon$ and by $\\mathcal{X}_{-}$ the subset of $\\mathcal{X}$ over which $\\eta(x)=\\epsilon$ . Let $\\mathcal{H}$ be a family of functions $h$ with $|h(x)|\\leq\\Lambda$ for all $x\\in\\mathcal{X}$ and such that all values in $[-\\Lambda,+\\Lambda]$ can be reached. Thus, $A=[-\\Lambda,\\Lambda]$ for any $x\\in\\mathcal X$ . Consider the exponential loss: $\\ell(h,x,y)=e^{-y h(x)}$ . Then, for any $x\\in\\mathcal X$ and $\\alpha\\in A$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{y}[\\ell(\\alpha,y)\\mid x]=\\left\\{{\\alpha}^{-\\alpha}+\\epsilon e^{\\alpha}\\quad x\\in{\\mathcal{X}}_{+}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, it is not hard to see that for any $\\epsilon\\le\\frac{1}{e^{2\\Lambda}+1}$ , the infimum $\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}_{y}[\\ell(\\alpha,y)\\mid x]$ can be achieved by $\\alpha^{*}(x)=\\left\\{\\!\\!\\!\\begin{array}{l l}{{\\Lambda}}&{{x\\in\\mathcal{X}_{+}}}\\\\ {{-\\Lambda}}&{{x\\in\\mathcal{X}_{-}}}\\end{array}\\!\\!\\!\\in A$ . Similarly, for the logistic loss $\\ell(h,x,y)=\\log\\bigl(1+e^{-y h(x)}\\bigr)$ , we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\ell(\\alpha,y)\\mid x]=\\left\\{\\begin{array}{l l}{\\big(1-\\epsilon\\big)\\log\\big(1+e^{-\\alpha}\\big)+\\epsilon\\log\\big(1+e^{\\alpha}\\big)}&{x\\in\\mathcal{X}_{+}}\\\\ {\\epsilon\\log\\big(1+e^{-\\alpha}\\big)+\\big(1-\\epsilon\\big)\\log\\big(1+e^{\\alpha}\\big)}&{x\\in\\mathcal{X}_{-}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and for $\\epsilon\\,\\le\\,{\\frac{1}{e^{\\Lambda}+1}}$ , the infimum $\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}_{y}[\\ell(\\alpha,y)\\mid x]$ can be achieved by $\\alpha^{*}(x)=\\left\\{\\!\\!{\\begin{array}{l l}{\\Lambda}&{x\\in{\\mathcal{X}}_{+}}\\\\ {-\\Lambda}&{x\\in{\\mathcal{X}}_{-}}\\end{array}}\\!\\!\\right..$ ", "page_idx": 25}, {"type": "text", "text": "Therefore, by Theorem 6.2, for these distributions and loss functions, when the best-in-class classifier $h^{*}$ \u03f5-approximates $\\alpha_{+}=\\Lambda$ over $\\mathcal{X}_{+}$ and $\\alpha_{-}=-\\Lambda$ over $\\mathcal{X}_{-}$ , then the minimizability gap is bounded by $\\epsilon$ . The existence of such a hypothesis in $\\mathcal{H}$ depends on the complexity of the decision surface. For example, as previously noted, when the decision surface is characterized by a hyperplane, a hypothesis set of linear functions, coupled with a sigmoid activation function, can offer a highly effective approximation (see Figure 1 for illustration). ", "page_idx": 25}, {"type": "text", "text": "B. Example: multi-class classification. Let $\\epsilon\\,\\in\\,[0,\\frac12]$ . We denote by $\\mathfrak{X}_{k}$ the subset of $\\mathcal{X}$ over which $\\mathsf{p}(k\\mid x)\\;=\\;1\\,-\\,\\epsilon$ and $\\mathsf{p}(j\\mid x)~=~\\frac{\\epsilon}{n-1}$ for $j~\\neq~k$ . Let $\\mathcal{H}$ be a family of functions $h$ with $\\left|h(x,\\cdot)\\right|\\leq\\Lambda$ for all $x\\in\\mathcal X$ and such that all values in $[-\\Lambda,+\\Lambda]$ can be reached. Thus, $A=\\left[-\\Lambda,\\Lambda\\right]^{n}$ for any $x\\,\\in\\,\\mathcal X$ . Consider the multi-class logistic loss: $\\begin{array}{r}{\\ell(h,x,y)\\,=\\,-\\log\\!\\Bigg[\\frac{e^{h(x,y)}}{\\sum_{y^{\\prime}\\in\\mathcal{Y}}e^{h(x,y^{\\prime})}}\\Bigg].}\\end{array}$ For any $\\alpha=\\left[\\alpha^{1},...,\\alpha^{n}\\right]\\in A$ , we denote by =\u2211k\u2032\u2208e[n\u03b1] e\u03b1k\u2032 . Then, for any x \u2208X and \u03b1 \u2208A, ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(\\alpha,y)\\mid x]=-(1-\\epsilon)\\log(S_{k})-{\\frac{\\epsilon}{n-1}}\\sum_{k^{\\prime}\\neq k}\\log(S_{k^{\\prime}}){\\mathrm{~if~}}x\\in{\\mathfrak{X}}_{k}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, it is not hard to see that for any $\\epsilon\\,\\leq\\,\\frac{n-1}{e^{2\\lambda}+n-1}$ , the infimum $\\operatorname*{inf}_{\\alpha\\in A}\\mathbb{E}_{y}[\\ell(\\alpha,y)\\mid x]$ can be achieved by $\\alpha^{*}(x)=[-\\Lambda,\\ldots,\\Lambda,\\ldots,-\\Lambda]$ , where $\\Lambda$ occupies the $k$ -th position for $x\\in\\mathcal X_{k}$ . Therefore, by Theorem 6.2, for these distributions and loss functions, when the best-in-class classifier $h^{*}~\\epsilon-$ approximates $\\alpha_{k}=\\left[-\\Lambda,...\\,,\\Lambda_{k-\\mathrm{th}}^{\\Lambda},...\\,,-\\Lambda\\right]$ over $\\mathfrak{X}_{k}$ , then the minimizability gap is bounded by $\\epsilon$ . The existence of such a hypothesis in $\\mathcal{H}$ depends on the complexity of the decision surface. ", "page_idx": 26}, {"type": "text", "text": "K Proof for binary margin-based losses (Theorem 4.2) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Theorem 4.2 (Upper and lower bound for binary margin-based losses). Let $\\mathcal{H}$ be a complete hypothesis set. Assume that $\\Phi$ is convex, twice continuously differentiable, and satisfies the inequalities $\\Phi^{\\prime}(0)>0$ and $\\Phi^{\\prime\\prime}(0)>0$ . Then, the following property holds: $\\mathcal{T}(t)\\,=\\,\\Theta(t^{2}),$ ; that is, there exist positive constants $C>0,\\,c>0,$ , and $T>0$ such that $C t^{2}\\geq\\mathcal{T}(t)\\geq c t^{2}$ , for all $0<t\\leq T$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. Since $\\Phi$ is convex and in $C^{2}$ , $f_{t}$ is also convex and differentiable with respect to $u$ . For any $t\\in[0,1]$ , differentiate $f_{t}$ with respect to $u$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nf_{t}^{\\prime}(u)=\\frac{1-t}{2}\\Phi^{\\prime}(u)-\\frac{1+t}{2}\\Phi^{\\prime}(-u).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Consider the function $F$ defined over $\\mathbb{R}^{2}$ by $\\begin{array}{l}{\\displaystyle{F(t,a)=\\frac{1-t}{2}\\Phi^{\\prime}(a)\\!-\\!\\frac{1+t}{2}\\Phi^{\\prime}(-a)}}\\end{array}$ . Observe that $F(0,0)=$ 0 and that the partial derivative of $F$ with respect to $a$ at $(0,0)$ is $\\Phi^{\\prime\\prime}(0)>0$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\partial F}{\\partial a}(t,a)=\\frac{1-t}{2}\\Phi^{\\prime\\prime}(a)+\\frac{1+t}{2}\\Phi^{\\prime\\prime}(-a),\\quad\\frac{\\partial F}{\\partial a}(0,0)=\\Phi^{\\prime\\prime}(0)>0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Consequently, by the implicit function theorem, there exists a continuously differentiable function $\\overline{a}$ such that $F(t,\\overline{{a}}(t))=0$ in a neighborhood $[-\\epsilon,\\epsilon]$ around zero. Thus, by the convexity of $f_{t}$ and the definition of $F$ , for $t\\in[0,\\epsilon],\\operatorname*{inf}_{u\\in\\mathbb{R}}f_{t}(u)$ is reached by $\\overline{{a}}(t)$ and we can denote it by $a_{t}^{*}$ . Then, $a_{t}^{*}$ is continuously differentiable over $[0,\\epsilon]$ . The minimizer $a_{t}^{*}$ satisfies the following equality: ", "page_idx": 26}, {"type": "equation", "text": "$$\nf_{t}^{\\prime}\\big(a_{t}^{*}\\big)=\\frac{1-t}{2}\\Phi^{\\prime}\\big(a_{t}^{*}\\big)-\\frac{1+t}{2}\\Phi^{\\prime}\\big(-a_{t}^{*}\\big)=0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Specifically, at $t\\,=\\,0$ , we have $\\Phi^{\\prime}(a_{0}^{*})\\;=\\;\\Phi^{\\prime}(-a_{0}^{*})$ . Since $\\Phi$ is convex, its derivative $\\Phi^{\\prime}$ is nondecreasing. Therefore, if $a_{0}^{*}$ were non-zero, then $\\Phi^{\\prime}$ would be constant over the segment $[-|a_{0}^{*}|,|a_{0}^{*}|]$ . This would contradict the condition $\\Phi^{\\prime\\prime}(0)>0$ , as a constant function cannot have a positive second derivative at any point. Thus, we must have $a_{0}^{*}=0$ and since $\\Phi^{\\prime}$ is non-decreasing and $\\Phi^{\\prime\\prime}(0)>0$ , we have $a_{t}^{*}>0$ for all $t\\in(0,\\epsilon]$ . By Theorem 4.1 and Taylor\u2019s theorem with an integral remainder, $\\upsigma$ can be expressed as follows: for any $t\\in[0,\\epsilon]$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Upsilon(t)=f_{t}(0)-\\operatorname*{inf}_{u\\in\\mathbb{R}}f_{t}(u)}\\\\ {\\displaystyle=f_{t}(0)-f_{t}(a_{t}^{*})}\\\\ {\\displaystyle=f_{t}^{\\prime}(a_{t}^{*})(0-a_{t}^{*})+\\int_{a_{t}^{*}}^{0}(0-u)f_{t}^{\\prime\\prime}(u)\\,d u}\\\\ {\\displaystyle=\\int_{0}^{a_{t}^{*}}u f_{t}^{\\prime\\prime}(u)\\,d u}\\\\ {\\displaystyle=\\int_{0}^{a_{t}^{*}}u\\Big[\\frac{1-t}{2}\\Phi^{\\prime\\prime}(u)+\\frac{1+t}{2}\\Phi^{\\prime\\prime}(-u)\\Big]\\,d u.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n(f_{t}^{\\prime}(a_{t}^{*})=0)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $a_{t}^{*}$ is a function of class $C^{1}$ , we can differentiate (16) with respect to $t$ , which gives the following equality for any $t$ in $(0,\\epsilon]$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n-\\frac{1}{2}\\Phi^{\\prime}(a_{t}^{*})+\\frac{1-t}{2}\\Phi^{\\prime\\prime}(a_{t}^{*})\\frac{d a_{t}^{*}}{d t}(t)-\\frac{1}{2}\\Phi^{\\prime}(-a_{t}^{*})+\\frac{1+t}{2}\\Phi^{\\prime\\prime}(-a_{t}^{*})\\frac{d a_{t}^{*}}{d t}(t)=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Taking the limit $t\\rightarrow0$ yields ", "page_idx": 27}, {"type": "equation", "text": "$$\n-\\frac{1}{2}\\Phi^{\\prime}(0)+\\frac{1}{2}\\Phi^{\\prime\\prime}(0)\\frac{d a_{t}^{*}}{d t}(0)-\\frac{1}{2}\\Phi^{\\prime}(0)+\\frac{1}{2}\\Phi^{\\prime\\prime}(0)\\frac{d a_{t}^{*}}{d t}(0)=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This implies that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{d a_{t}^{*}}{d t}(0)=\\frac{\\Phi^{\\prime}(0)}{\\Phi^{\\prime\\prime}(0)}>0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since limt\u21920a $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow0}\\frac{a_{t}^{*}}{t}=\\frac{d a_{t}^{*}}{d t}\\big(0\\big)=\\frac{\\Phi^{\\prime}(0)}{\\Phi^{\\prime\\prime}(0)}>0}\\end{array}$ , we have $a_{t}^{*}=\\Theta(t)$ . ", "page_idx": 27}, {"type": "text", "text": "Since $\\Phi^{\\prime\\prime}(0)>0$ and $\\Phi^{\\prime\\prime}$ is continuous, there is a non-empty interval $[-\\alpha,+\\alpha]$ over which $\\Phi^{\\prime\\prime}$ is positive. Since $a_{0}^{*}=0$ and $a_{t}^{*}$ is continuous, there exists a sub-interval $[0,\\epsilon^{\\prime}]\\bar{\\subseteq}\\left[0,\\epsilon\\right]$ over which $a_{t}^{*}\\leq\\alpha$ . Since $\\Phi^{\\prime\\prime}$ is continuous, it admits a minimum and a maximum over any compact set and we can define $c=\\mathrm{min}_{u\\in[-\\alpha,\\alpha]}\\;\\Phi^{\\prime\\prime}(u)$ and $C=\\mathrm{max}_{u\\in[-\\alpha,\\alpha]}\\,\\Phi^{\\prime\\prime}(u)$ . $c$ and $C$ are both positive since we have $\\Phi^{\\prime\\prime}(0)>0$ . Thus, for $t$ in $[0,\\epsilon^{\\prime}]$ , by (17), the following inequality holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{D}\\frac{(a_{t}^{*})^{2}}{2}=\\int_{0}^{a_{t}^{*}}u C\\,d u\\geq\\mathcal{T}(t)=\\int_{0}^{a_{t}^{*}}u\\bigg[\\frac{1-t}{2}\\Phi^{\\prime\\prime}(u)+\\frac{1+t}{2}\\Phi^{\\prime\\prime}(-u)\\bigg]\\,d u\\geq\\int_{0}^{a_{t}^{*}}u C\\,d u=c\\frac{(a_{t}^{*})^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This implies that $\\mathcal{T}(t)=\\Theta(t^{2})$ . ", "page_idx": 27}, {"type": "text", "text": "L Proof for comp-sum losses (Theorem 5.3) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Theorem 5.3 (Upper and lower bound for comp-sum losses). Assume that $\\Phi$ is convex, twice continuously differentiable, and satisfies the properties $\\Phi^{\\prime}(u)<0$ and $\\Phi^{\\prime\\prime}(u)>0$ for any $u\\in(0,\\frac{1}{2}]$ . Then, the following property holds: $\\mathcal{T}(t)=\\Theta(t^{2})$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. For any $\\scriptstyle{\\mathcal{T}}\\,\\in\\,\\left[{\\frac{1}{n}},\\,{\\frac{1}{2}}\\right]$ , define the function $\\mathfrak{T}_{\\tau}$ by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall t\\in[0,1],\\quad\\daleth_{\\tau}(t)=\\displaystyle\\operatorname*{sup}_{|u|\\leq\\tau}\\biggl\\{\\Phi(\\tau)-\\frac{1-t}{2}\\Phi(\\tau+u)-\\frac{1+t}{2}\\Phi(\\tau-u)\\biggr\\}}\\\\ &{\\qquad\\qquad=f_{t,\\tau}(0)-\\displaystyle\\operatorname*{inf}_{|u|\\leq\\tau}f_{t,\\tau}(u),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\nf_{t,\\tau}(u)=\\frac{1-t}{2}\\Phi_{\\tau}(u)+\\frac{1+t}{2}\\Phi_{\\tau}(-u)\\quad\\mathrm{and}\\quad\\Phi_{\\tau}(u)=\\Phi\\big(\\tau+u\\big).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We aim to establish a lower bound for $\\operatorname*{inf}_{\\tau\\in[\\frac{1}{n},\\frac{1}{2}]}\\mathcal{T}_{\\tau}(t)$ . For any fixed $\\tau\\,\\in\\,\\left[\\frac{1}{n},\\frac{1}{2}\\right]$ , this situation is parallel to that of binary classification (Theorem 4.1 and Theorem 4.2), since we have $\\Phi_{\\tau}^{\\prime}(0)=$ $\\bar{\\Phi^{\\prime}}(\\tau)<0$ and $\\Phi_{\\tau}^{\\prime\\prime}(0)\\,=\\,\\dot{\\Phi}^{\\prime\\prime}(\\tau)\\,>\\,0$ . Let $a_{t,\\tau}^{*}$ denotes the minimizer of $f_{t,\\tau}$ over $\\mathbb{R}$ . By applying Theorem 5.2 to the function $\\begin{array}{r}{F\\colon(t,u,\\tau)\\mapsto f_{t,\\tau}^{\\prime}(u)=\\frac{1-t}{2}\\Phi_{\\tau}^{\\prime}(u)-\\frac{1+t}{2}\\Phi_{\\tau}^{\\prime}\\bigl(-u\\bigr)}\\end{array}$ and the convexity of $f_{t,\\tau}$ with respect to $u,a_{t,\\tau}^{*}$ exists, is unique and is continuously differentiable over $\\left[0,t_{0}^{\\prime}\\right]\\times\\left[\\frac{1}{n},\\frac{1}{2}\\right]$ , for some $t_{0}^{\\prime}>0$ . Moreover, by using the fact that $f_{0,\\tau}^{\\prime}(\\tau)>0$ and $f_{0,\\tau}^{\\prime}(-\\tau)<0$ , and the convexity of $f_{0,\\tau}$ with respect to $u$ , we have $\\begin{array}{r}{\\left|a_{0,\\tau}^{*}\\right|\\leq\\tau,\\,\\forall\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}\\end{array}$ . By the continuity of $a_{t,\\tau}^{*}$ , we have $\\left|a_{t,\\tau}^{*}\\right|\\leq\\tau$ over $\\textstyle\\left[0,t_{0}\\right]\\times\\left[{\\frac{1}{n}},{\\frac{1}{2}}\\right]$ , for some $t_{0}>0$ and $t_{0}\\le t_{0}^{\\prime}$ . ", "page_idx": 27}, {"type": "text", "text": "Next, we will leverage the proof of Theorem 4.2. Adopting a similar notation, while incorporating the $\\tau$ subscript to distinguish different functions $\\Phi_{\\tau}$ and $f_{t,\\tau}$ , we can write ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall t\\in[0,t_{0}],\\quad\\mathcal{T}_{\\tau}(t)=\\int_{0}^{-a_{t,\\tau}^{*}}u\\Bigg[\\frac{1-t}{2}\\Phi_{\\tau}^{\\prime\\prime}(-u)+\\frac{1+t}{2}\\Phi_{\\tau}^{\\prime\\prime}(u)\\Bigg]\\,d u.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $a_{t,\\tau}^{*}$ verifies ", "page_idx": 27}, {"type": "equation", "text": "$$\na_{0,\\tau}^{*}=0\\quad\\mathrm{and}\\quad\\frac{\\partial a_{t,\\tau}^{*}}{\\partial t}(0)=\\frac{\\Phi_{\\tau}^{\\prime}(0)}{\\Phi_{\\tau}^{\\prime\\prime}(0)}=c_{\\tau}<0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We first show the lower bound $\\operatorname*{inf}_{\\tau\\in[\\frac{1}{n},\\frac{1}{2}]}-a_{t,\\tau}^{*}=\\Omega(t)$ . Given the equalities (18), it follows that for any $\\tau$ , the following holds: $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow0}\\Bigl(-a_{t,\\tau}^{\\ast}+c_{\\tau}t\\Bigr)=0}\\end{array}$ . For any $\\tau\\in\\left[{\\frac{1}{n}},{\\frac{1}{2}}\\right]$ , $t\\mapsto\\left(-a_{t,\\tau}^{*}+c_{\\tau}t\\right)$ is a continuous function over $[0,t_{0}]$ since $a_{t,\\tau}^{*}$ is a function of class $C^{1}$ . Since the infimum over a fixed compact set of a family of continuous functions is continuous, $t\\mapsto\\operatorname*{inf}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}\\left\\{-a_{t,\\tau}^{*}+c_{\\tau}t\\right\\}$ is continuous. Thus, for any $\\epsilon>0$ , there exists $t_{1}>0$ , $t_{1}\\leq t_{0}$ , such that for any $t\\in[0,t_{1}]$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{inf}_{\\tau\\in\\left[\\frac1n,\\frac12\\right]}\\left\\{-a_{t,\\tau}^{*}+c_{\\tau}t\\right\\}\\right|\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\forall\\tau\\in\\Big[\\frac{1}{n},\\frac{1}{2}\\Big],\\quad-a_{t,\\tau}^{*}\\geq-c_{\\tau}t-\\epsilon\\geq c t-\\epsilon,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $c=\\operatorname*{inf}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}-c_{\\tau}$ . Since $\\Phi_{\\tau}^{\\prime}(0)$ and $\\Phi_{\\tau}^{\\prime\\prime}(0)$ are positive and continuous functions of $\\tau$ , this infimum is attained over the compact set $\\textstyle\\left[{\\frac{1}{n}},{\\frac{1}{2}}\\right]$ , leading to $c>0$ . Since the lower bound holds uniformly over $\\tau$ , this shows that for $t\\in[0,t_{1}]$ , we have $\\operatorname*{inf}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}-a_{t,\\tau}^{*}=\\Omega(t)$ . ", "page_idx": 28}, {"type": "text", "text": "Now, since for any $\\begin{array}{r}{\\tau\\in\\Big[\\frac{1}{n},\\frac{1}{2}\\Big],-a_{t,\\tau}^{*}}\\end{array}$ is a function of class $C^{1}$ and thus continuous, its supremum over a compact set, $\\begin{array}{r l}{\\operatorname*{sup}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}-a_{t,\\tau}^{*}}\\end{array}$ , is also continuous and is bounded over $[0,t_{1}]$ by some $a>0$ For $|u|\\leq a$ and $\\scriptstyle{\\mathcal{T}}\\;\\in\\;\\left[{\\frac{1}{n}},\\,{\\frac{1}{2}}\\right]$ , we have $\\begin{array}{r}{\\frac{1}{2}-a\\leq\\tau+u\\leq\\frac{1}{2}+a}\\end{array}$ and $\\begin{array}{r}{\\frac{1}{2}-a\\leq\\tau-u\\leq\\frac{1}{2}+a}\\end{array}$ . Since $\\Phi^{\\prime\\prime}$ is positive and continuous, it reaches its minimum $C>0$ over the compact set $\\textstyle{\\left[{\\frac{1}{2}}-a,{\\frac{1}{2}}+a\\right]}$ . Thus, we can write ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall t\\in[0,t_{1}],\\forall\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right],\\quad\\mathcal{T}_{\\tau}(t)=\\int_{0}^{-a_{t,\\tau}^{*}}u\\Big[\\frac{1-t}{2}\\Phi_{\\tau}^{\\prime\\prime}(-u)+\\frac{1+t}{2}\\Phi_{\\tau}^{\\prime\\prime}(u)\\Big]\\,d u}\\\\ {\\geq\\int_{0}^{-a_{t,\\tau}^{*}}u\\Big[\\frac{1-t}{2}C+\\frac{1+t}{2}C\\Big]\\,d u}\\\\ {=\\int_{0}^{-a_{t,\\tau}^{*}}C u\\,d u=C\\frac{(-a_{t,\\tau}^{*})^{2}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, for $t\\leq t_{1}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\tau\\in\\left[\\frac1n,\\frac12\\right]}\\mathcal{T}_{\\tau}(t)\\geq C\\frac{(\\operatorname*{inf}_{\\tau\\in\\left[\\frac1n,\\frac12\\right]}-a_{t,\\tau}^{*})^{2}}{2}\\geq\\Omega(t^{2}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Similarly, we aim to establish an upper bound for $\\operatorname*{inf}_{\\tau\\in[\\frac{1}{n},\\frac{1}{2}]}\\mathcal{T}_{\\tau}(t)$ . We first show the upper bound $\\operatorname*{sup}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}-a_{t,\\tau}^{*}=O(t)$ . Given the equalities (18), it follows that for any $\\tau$ , the following holds: $\\begin{array}{r}{\\operatorname*{lim}_{t\\to0}\\Bigl(-a_{t,\\tau}^{*}+c_{\\tau}t\\Bigr)=0}\\end{array}$ . For any $\\tau\\in\\left[{\\frac{1}{n}},{\\frac{1}{2}}\\right]$ , $t\\mapsto\\left(-a_{t,\\tau}^{*}+c_{\\tau}t\\right)$ is a continuous function over $[0,t_{0}]$ since $a_{t,\\tau}^{*}$ is a function of class $C^{1}$ . Since the supremum over a fixed compact set of a family of continuous functions is continuous, $\\begin{array}{r}{t\\mapsto\\operatorname*{sup}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}\\Bigl\\{-a_{t,\\tau}^{*}+c_{\\tau}t\\Bigr\\}}\\end{array}$ is continuous. Thus, for any $\\epsilon>0$ , there exists $t_{1}>0,t_{1}\\leq t_{0}$ , such that for any $t\\in[0,t_{1}]$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{sup}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}\\left\\{-a_{t,\\tau}^{*}+c_{\\tau}t\\right\\}\\right|\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\forall\\tau\\in\\Big[\\frac{1}{n},\\frac{1}{2}\\Big],\\quad-a_{t,\\tau}^{*}\\leq-c_{\\tau}t+\\epsilon\\leq c t+\\epsilon,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r}{c=\\operatorname*{sup}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}-c_{\\tau}}\\end{array}$ . Since $\\Phi_{\\tau}^{\\prime}(0)$ and $\\Phi_{\\tau}^{\\prime\\prime}(0)$ are positive and continuous functions of $\\tau$ , this supremum is attained over the compact set $\\textstyle\\left[{\\frac{1}{n}},{\\frac{1}{2}}\\right]$ , leading to $c>0$ . Since the upper bound holds uniformly over $\\tau$ , this shows that for $t\\in[0,t_{1}]$ , we have $\\operatorname*{sup}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}-a_{t,\\tau}^{*}=O(t)$ . ", "page_idx": 28}, {"type": "text", "text": "Now, since for any $\\begin{array}{r}{\\tau\\in\\Big[\\frac{1}{n},\\frac{1}{2}\\Big],-a_{t,\\tau}^{*}}\\end{array}$ is a function of class $C^{1}$ and thus continuous, its supremum over a compact set, $\\mathrm{sup}_{\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right]}-a_{t,\\tau}^{*}$ , is also continuous and is bounded over $[0,t_{1}]$ by some $a>0$ For $|u|\\leq a$ and $\\scriptstyle{\\mathcal{T}}\\,\\in\\,\\left[{\\frac{1}{n}},\\,{\\frac{1}{2}}\\right]$ , we have $\\begin{array}{r}{\\frac{1}{2}-a\\leq\\tau+u\\leq\\frac{1}{2}+a}\\end{array}$ and $\\begin{array}{r}{\\frac{1}{2}-a\\leq\\tau-u\\leq\\frac{1}{2}+a}\\end{array}$ . Since $\\Phi^{\\prime\\prime}$ is ", "page_idx": 28}, {"type": "text", "text": "positive and continuous, it reaches its maximum $C>0$ over the compact set $\\textstyle{\\left[{\\frac{1}{2}}-a,{\\frac{1}{2}}+a\\right]}$ . Thus, we can write ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall t\\in[0,t_{1}],\\forall\\tau\\in\\left[\\frac{1}{n},\\frac{1}{2}\\right],\\quad\\mathcal{T}_{\\tau}(t)=\\int_{0}^{-a_{t,\\tau}^{*}}u\\Big[\\frac{1-t}{2}\\Phi_{\\tau}^{\\prime\\prime}(-u)+\\frac{1+t}{2}\\Phi_{\\tau}^{\\prime\\prime}(u)\\Big]\\,d u}\\\\ &{\\qquad\\qquad\\leq\\int_{0}^{-a_{t,\\tau}^{*}}u\\Big[\\frac{1-t}{2}C+\\frac{1+t}{2}C\\Big]\\,d u}\\\\ &{\\qquad\\qquad=\\int_{0}^{-a_{t,\\tau}^{*}}C u\\,d u=C\\frac{(-a_{t,\\tau}^{*})^{2}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, for $t\\leq t_{1}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\tau\\in\\left[\\frac1n,\\frac12\\right]}\\mathcal{T}_{\\tau}(t)\\leq C\\frac{(\\operatorname*{sup}_{\\tau\\in\\left[\\frac1n,\\frac12\\right]}-a_{t,\\tau}^{*})^{2}}{2}\\leq O(t^{2}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This completes the proof. ", "page_idx": 29}, {"type": "text", "text": "M Proof for constrained losses (Theorem 5.5) ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Theorem 5.5 (Upper and lower bound for constrained losses). Assume that $\\Phi$ is convex, twice continuously differentiable, and satisfies the properties $\\Phi^{\\prime}(u)>0$ and $\\Phi^{\\prime\\prime}(u)>0$ for any $u\\geq0$ . Then, for any $A>0$ , the following property holds: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\tau\\in[0,A]}\\operatorname*{sup}_{u\\in\\mathbb{R}}\\biggl\\{\\Phi(\\tau)-\\frac{1-t}{2}\\Phi(\\tau+u)-\\frac{1+t}{2}\\Phi(\\tau-u)\\biggr\\}=\\Theta(t^{2}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. For any $\\tau\\in[0,A]$ , define the function $\\Phi_{\\tau}$ by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\forall t\\in[0,1],\\quad\\mathcal{T}_{\\tau}(t)=\\operatorname*{sup}_{u\\in\\mathbb{R}}\\biggl\\{\\Phi(\\tau)-\\frac{1-t}{2}\\Phi(\\tau+u)-\\frac{1+t}{2}\\Phi(\\tau-u)\\biggr\\}}\\\\ {\\displaystyle\\qquad\\qquad=f_{t,\\tau}(0)-\\operatorname*{inf}_{u\\in\\mathbb{R}}f_{t,\\tau}(u),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{t,\\tau}(u)=\\frac{1-t}{2}\\Phi_{\\tau}(u)+\\frac{1+t}{2}\\Phi_{\\tau}(-u)\\quad\\mathrm{and}\\quad\\Phi_{\\tau}(u)=\\Phi\\big(\\tau+u\\big).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We aim to establish a lower bound for $\\operatorname{inf}_{\\tau\\in[0,A]}\\mathcal{T}_{\\tau}(t)$ . For any fixed $\\tau\\,\\in\\,[0,A]$ , this situation is parallel to that of binary classification (Theorem 4.1 and Theorem 4.2), since we also have $\\Phi_{\\tau}^{\\prime}(0)\\,=\\,\\Phi^{\\prime}(\\tau)\\,>\\,0$ and $\\Phi_{\\tau}^{\\bar{\\prime\\prime}}(0)\\,=\\,\\Phi^{\\prime\\prime}(\\tau)\\,>\\,0$ . Let $a_{t,\\tau}^{*}$ denotes the minimizer of $f_{t,\\tau}$ over $\\mathbb{R}$ . By applying Theorem 5.2 to the function $\\begin{array}{r}{F\\colon(t,u,\\tau)\\,\\mapsto\\,f_{t,\\tau}^{\\prime}(u)\\;=\\;\\frac{1-t}{2}\\Phi_{\\tau}^{\\prime}(u)\\,-\\,\\frac{1+t}{2}\\Phi_{\\tau}^{\\prime}\\bigl(-u\\bigr)}\\end{array}$ and the convexity of $f_{t,\\tau}$ with respect to $u$ , $a_{t,\\tau}^{*}$ exists, is unique and is continuously differentiable over $[0,t_{0}]\\times[0,A]$ , for some $t_{0}>0$ . ", "page_idx": 29}, {"type": "text", "text": "Next, we will leverage the proof of Theorem 4.2. Adopting a similar notation, while incorporating the $\\tau$ subscript to distinguish different functions $\\Phi_{\\tau}$ and $f_{t,\\tau}$ , we can write ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\forall t\\in[0,t_{0}],\\quad\\mathcal{T}_{\\tau}(t)=\\int_{0}^{a_{t,\\tau}^{*}}u\\Bigg[\\frac{1-t}{2}\\Phi_{\\tau}^{\\prime\\prime}(u)+\\frac{1+t}{2}\\Phi_{\\tau}^{\\prime\\prime}(-u)\\Bigg]\\,d u.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $a_{t,\\tau}^{*}$ verifies ", "page_idx": 29}, {"type": "equation", "text": "$$\na_{0,\\tau}^{*}=0\\quad\\mathrm{and}\\quad\\frac{\\partial a_{t,\\tau}^{*}}{\\partial t}(0)=\\frac{\\Phi_{\\tau}^{\\prime}(0)}{\\Phi_{\\tau}^{\\prime\\prime}(0)}=c_{\\tau}>0.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We first show the lower bound $\\operatorname*{inf}_{\\tau\\in[0,A]}a_{t,\\tau}^{*}=\\Omega(t)$ . Given the equalities (19), it follows that for any $\\tau$ , the following holds: $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow0}\\Big({a}_{t,\\tau}^{\\ast}-c_{\\tau}t\\Big)=0}\\end{array}$ . For any $\\tau\\in[0,A]$ , $t\\mapsto\\big(a_{t,\\tau}^{*}-c_{\\tau}t\\big)$ is a continuous function over $[0,t_{0}]$ since $a_{t,\\tau}^{*}$ is a function of class $C^{1}$ . Since the infimum over a fixed compact set of a family of continuous functions is continuous, $t\\mapsto\\operatorname*{inf}_{\\tau\\in[0,A]}\\Bigl\\{a_{t,\\tau}^{*}-c_{\\tau}t\\Bigr\\}$ is continuous. Thus, for any $\\epsilon>0$ , there exists $t_{1}>0,t_{1}\\leq t_{0}$ , such that for any $t\\in[0,t_{1}]$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{inf}_{\\tau\\in[0,A]}\\left\\{a_{t,\\tau}^{*}-c_{\\tau}t\\right\\}\\right|\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which implies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\forall\\tau\\in[0,A],\\quad a_{t,\\tau}^{*}\\geq c_{\\tau}t-\\epsilon\\geq c t-\\epsilon,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $c=\\operatorname*{inf}_{\\tau\\in[0,A]}c_{\\tau}$ . Since $\\Phi_{\\tau}^{\\prime}(0)$ and $\\Phi_{\\tau}^{\\prime\\prime}(0)$ are positive and continuous functions of $\\tau$ , this infimum is attained over the compact set $[0,A]$ , leading to $c>0$ . Since the lower bound holds uniformly over $\\tau$ , this shows that for $t\\in[0,t_{1}]$ , we have $\\operatorname*{inf}_{\\tau\\in[0,A]}a_{t,\\tau}^{*}=\\Omega(t)$ . ", "page_idx": 30}, {"type": "text", "text": "Now, since for any $\\tau\\in[0,A]$ , $a_{t,\\tau}^{*}$ is a function of class $C^{1}$ and thus continuous, its supremum over a compact set, $\\operatorname{sup}_{\\tau\\in[0,A]}a_{t,\\tau}^{*}$ , is also continuous and is bounded over $[0,t_{1}]$ by some $a>0$ . For $|u|\\leq a$ and $\\tau\\in[0,A]$ , we have $A-a\\leq\\tau+u\\leq A+a$ and $A-a\\leq\\tau-u\\leq A+a$ . Since $\\Phi^{\\prime\\prime}$ is positive and continuous, it reaches its minimum $C>0$ over the compact set $[A-a,A+a]$ . Thus, we can write ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall t\\in[0,t_{1}],\\forall\\tau\\in[0,A],\\quad\\mathcal{T}_{\\tau}(t)=\\displaystyle\\int_{0}^{a_{t,\\tau}^{*}}u\\Big[\\frac{1-t}{2}\\Phi_{\\tau}^{\\prime\\prime}(u)+\\frac{1+t}{2}\\Phi_{\\tau}^{\\prime\\prime}(-u)\\Big]\\,d u}\\\\ {\\geq\\displaystyle\\int_{0}^{a_{t,\\tau}^{*}}u\\Big[\\frac{1-t}{2}C+\\frac{1+t}{2}C\\Big]\\,d u}\\\\ {=\\displaystyle\\int_{0}^{a_{t,\\tau}^{*}}C u\\,d u=C\\frac{(a_{t,\\tau}^{*})^{2}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, for $t\\leq t_{1}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\tau\\in[0,A]}\\mathcal{T}_{\\tau}(t)\\geq C\\frac{(\\operatorname*{inf}_{\\tau\\in[0,A]}a_{t,\\tau}^{*})^{2}}{2}\\geq\\Omega(t^{2}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Similarly, we aim to establish an upper bound for $\\operatorname{inf}_{\\tau\\in[0,A]}\\mathcal{T}_{\\tau}(t)$ . We first show the upper bound $\\operatorname*{sup}_{\\tau\\in[0,A]}a_{t,\\tau}^{*}\\,=\\,O(t)$ . Given the equalities (19), it follows that for any $\\tau$ , the following holds: $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow0}\\Bigl(a_{t,\\tau}^{\\ast}-c_{\\tau}t\\Bigr)\\,=\\,0}\\end{array}$ . For any $\\tau\\in[0,A]$ , $t\\mapsto\\left(a_{t,\\tau}^{*}-c_{\\tau}t\\right)$ is a continuous function over $[0,t_{0}]$ since $a_{t,\\tau}^{*}$ is a function of class $C^{1}$ . Since the supremum over a fixed compact set of a family of continuous functions is continuous, t \u21a6sup\u03c4\u2208[0,A]{a $t\\mapsto\\operatorname*{sup}_{\\tau\\in[0,A]}\\left\\{a_{t,\\tau}^{*}-c_{\\tau}t\\right\\}$ is continuous. Thus, for any $\\epsilon>0$ , there exists $t_{1}>0,t_{1}\\leq t_{0}$ , such that for any $t\\in[0,t_{1}]$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{sup}_{\\tau\\in[0,A]}\\left\\{a_{t,\\tau}^{*}-c_{\\tau}t\\right\\}\\right|\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which implies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\forall\\tau\\in[0,A],\\quad a_{t,\\tau}^{*}\\leq c_{\\tau}t+\\epsilon\\leq c t+\\epsilon,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $c=\\operatorname*{sup}_{\\tau\\in[0,A]}c_{\\tau}$ . Since $\\Phi_{\\tau}^{\\prime}(0)$ and $\\Phi_{\\tau}^{\\prime\\prime}(0)$ are positive and continuous functions of $\\tau$ , this supremum is attained over the compact set $[0,A]$ , leading to $c>0$ . Since the upper bound holds uniformly over $\\tau$ , this shows that for $t\\in[0,t_{1}\\bar{]}$ , we have $\\operatorname*{sup}_{\\tau\\in[0,A]}a_{t,\\tau}^{*}=O(t)$ . ", "page_idx": 30}, {"type": "text", "text": "Now, since for any $\\tau\\in[0,A]$ , $a_{t,\\tau}^{*}$ is a function of class $C^{1}$ and thus continuous, its supremum over a compact set, $\\operatorname{sup}_{\\tau\\in[0,A]}a_{t,\\tau}^{*}$ , is also continuous and is bounded over $[0,t_{1}]$ by some $a>0$ . For $|u|\\leq a$ and $\\tau\\in[0,A]$ , we have $A-a\\leq\\tau+u\\leq A+a$ and $A-a\\leq\\tau-u\\leq A+a$ . Since $\\Phi^{\\prime\\prime}$ is positive and continuous, it reaches its maximum $C>0$ over the compact set $[A-a,A+a]$ . Thus, we can write ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall t\\in[0,t_{1}],\\forall\\tau\\in[0,A],\\quad\\mathcal{T}_{\\tau}(t)=\\displaystyle\\int_{0}^{a_{t,\\tau}^{*}}u\\Big[\\frac{1-t}{2}\\Phi_{\\tau}^{\\prime\\prime}(u)+\\frac{1+t}{2}\\Phi_{\\tau}^{\\prime\\prime}(-u)\\Big]\\,d u}\\\\ {\\leq\\displaystyle\\int_{0}^{a_{t,\\tau}^{*}}u\\Big[\\frac{1-t}{2}C+\\frac{1+t}{2}C\\Big]\\,d u}\\\\ {=\\displaystyle\\int_{0}^{a_{t,\\tau}^{*}}C u\\,d u=C\\frac{(a_{t,\\tau}^{*})^{2}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, for $t\\leq t_{1}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\tau\\in[0,A]}\\mathcal{T}_{\\tau}(t)\\leq C\\frac{(\\operatorname*{sup}_{\\tau\\in[0,A]}a_{t,\\tau}^{*})^{2}}{2}\\leq O(t^{2}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This completes the proof. ", "page_idx": 30}, {"type": "text", "text": "N Analysis of the function of $\\tau$ ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Let $F$ be the function defined by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\forall t\\in\\big[0,\\frac{1}{2}\\big],\\tau\\in\\mathbb{R},\\quad F(t,\\tau)=\\operatorname*{sup}_{u\\in\\mathbb{R}}\\biggl\\{\\Phi(\\tau)-\\frac{1-t}{2}\\Phi(\\tau+u)-\\frac{1+t}{2}\\Phi(\\tau-u)\\biggr\\},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\Phi$ is a convex function in $C^{2}$ with $\\Phi^{\\prime},\\Phi^{\\prime\\prime}\\,>\\,0$ . In light of the analysis of the previous sections, for any $(\\tau,t)$ , there exists a unique function $a_{t,\\tau}$ solution of the maximization (supremum in $F$ ), a $C^{1}$ function over a neighborhood $U$ of $(\\tau,0)$ with $a_{0,\\tau}\\;=\\;0$ , $a_{t,\\tau}\\,>\\,0$ for $t\\,>\\,0$ , and \u2202a\u2202tt,\u03c4 (0,\u03c4) = $\\begin{array}{r}{\\frac{\\partial a_{t,\\tau}}{\\partial t}\\big(0,\\tau\\big)=\\frac{\\Phi^{\\prime}(\\tau)}{\\Phi^{\\prime\\prime}(\\tau)}=c_{\\tau}}\\end{array}$ . Thus, we have $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow0}\\frac{a_{t,\\tau}}{t c_{\\tau}}=1}\\end{array}$ . The optimality of $a_{t,\\tau}$ implies ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{1-t}{2}\\Phi^{\\prime}(\\tau+a_{t,\\tau})=\\frac{1+t}{2}\\Phi^{\\prime}(\\tau-a_{t,\\tau}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, the partial derivative of $F$ over the appropriate neighborhood $U$ is given by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial F}{\\partial\\tau}(t,\\tau)=\\Phi^{\\prime}(\\tau)-\\frac{1-t}{2}\\Phi^{\\prime}(\\tau+a_{t,\\tau})\\biggl(\\frac{\\partial a_{t,\\tau}}{\\partial\\tau}(t,\\tau)+1\\biggr)-\\frac{1+t}{2}\\Phi^{\\prime}(\\tau-a_{t,\\tau})\\biggl(-\\frac{\\partial a_{t,\\tau}}{\\partial\\tau}(t,\\tau)+1\\biggr)}\\\\ {\\displaystyle=\\Phi^{\\prime}(\\tau)-\\frac{1-t}{2}\\Phi^{\\prime}(\\tau+a_{t,\\tau})\\biggl(\\frac{\\partial a_{t,\\tau}}{\\partial\\tau}(t,\\tau)+1-\\frac{\\partial a_{t,\\tau}}{\\partial\\tau}(t,\\tau)+1\\biggr)}\\\\ {\\displaystyle=\\Phi^{\\prime}(\\tau)-(1-t)\\Phi^{\\prime}(\\tau+a_{t,\\tau}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $\\Phi^{\\prime}$ is continuous, by the mean value theorem, there exists $\\xi\\in\\big(\\tau,\\tau+a_{t,\\tau}\\big)$ such that $\\Phi^{\\prime}(\\tau+$ $\\boldsymbol{a}_{t,\\tau}\\big)-\\Phi^{\\prime}(\\tau)=\\boldsymbol{a}_{t,\\tau}\\Phi^{\\prime\\prime}(\\xi)$ . Thus, we can write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\partial F}{\\partial\\tau}(t,\\tau)=\\Phi^{\\prime}(\\tau)-(1-t)\\Phi^{\\prime}(\\tau)-(1-t)a_{t,\\tau}\\Phi^{\\prime\\prime}(\\xi)}}\\\\ &{}&{\\qquad=t\\Phi^{\\prime}(\\tau)-(1-t)a_{t,\\tau}\\Phi^{\\prime\\prime}(\\xi)\\qquad\\qquad}\\\\ &{}&{\\qquad=t\\Phi^{\\prime}(\\tau)\\Bigg[1-(1-t)\\frac{a_{t,\\tau}}{t c_{\\tau}}\\frac{\\Phi^{\\prime\\prime}(\\xi)}{\\Phi^{\\prime\\prime}(\\tau)}\\Bigg].\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that if $\\Phi^{\\prime\\prime}$ is locally non-increasing, then we have $\\Phi^{\\prime\\prime}(\\xi)\\le\\Phi^{\\prime\\prime}(\\tau)$ and for $t$ sufficiently small, since $\\Phi^{\\prime}$ is increasing and $\\frac{a_{t,\\tau}}{t c_{\\tau}}\\sim1$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\partial F}{\\partial\\tau}(t,\\tau)\\geq t\\Phi^{\\prime}(\\tau)\\biggl[1-(1-t)\\frac{a_{t,\\tau}}{t c_{\\tau}}\\biggr]\\geq0.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In that case, for any $A>0$ , we can find a neighborhood O of $t$ around zero over which $\\begin{array}{r}{\\frac{\\partial F}{\\partial\\tau}(t,\\tau)}\\end{array}$ is defined for all $(t,\\tau)\\in\\mathcal{O}\\times[0,A]$ and $\\begin{array}{r}{\\frac{\\partial F}{\\partial\\tau}(t,\\tau)\\geq0}\\end{array}$ . From this, we can conclude that the infimum of $F$ over $\\tau\\in[0,A]$ is reached at zero for $t$ sufficiently small $(t\\in\\mathcal{O})$ . ", "page_idx": 31}, {"type": "text", "text": "O Generalization bounds ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Let $S=\\left((x_{1},y_{1}),\\dots,(x_{m},y_{m})\\right)$ be a sample drawn from $\\Phi^{m}$ . Denote by $\\widehat{h}_{S}$ an empirical minimizer within $\\mathcal{H}$ for the surrogate loss $\\ell$ : $\\begin{array}{r}{\\widehat{h}_{S}\\in\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\frac{1}{m}\\sum_{i=1}^{m}\\ell(h,x_{i},y_{i})}\\end{array}$ . Let $\\mathcal{H}_{\\ell}$ denote the hypothesis set $\\{(x,y)\\mapsto\\ell(h,x,y){:}h\\in\\mathcal{H}\\}$ and $\\Re_{m}^{\\ell}(\\mathcal{H})$ its Rademacher complexity. We also write $B_{\\ell}$ to denote an upper bound for $\\ell$ . Then, given the following $\\mathcal{H}$ -consistency bound: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall h\\in\\mathcal{K},\\quad\\mathcal{E}_{\\ell_{0-1}}\\big(h\\big)-\\mathcal{E}_{\\ell_{0-1}}^{*}\\big(\\mathcal{K}\\big)+\\mathcal{M}_{\\ell_{0-1}}\\big(\\mathcal{K}\\big)\\leq\\Gamma\\big(\\mathcal{E}_{\\ell}\\big(h\\big)-\\mathcal{E}_{\\ell}^{*}\\big(\\mathcal{K}\\big)+\\mathcal{M}_{\\ell}\\big(\\mathcal{K}\\big)\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for any $\\delta>0$ , with probability at least $1\\!-\\!\\delta$ over the draw of an i.i.d. sample $S$ of size $m$ , the following estimation bound holds for $\\dot{\\widehat{h}}_{S}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall h\\in\\mathcal{K},\\quad\\mathcal{E}_{\\ell_{0-1}}(h)-\\mathcal{E}_{\\ell_{0-1}}^{*}(\\mathcal{K})\\leq\\Gamma\\biggl(4\\mathfrak{R}_{m}^{\\perp}(\\mathcal{K})+2B_{\\mathsf{L}}\\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2m}}+\\mathcal{M}_{\\ell}(\\mathcal{K})\\biggr)-\\mathcal{M}_{\\ell_{0-1}}(\\mathcal{K}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. By the standard Rademacher complexity bounds [Mohri et al., 2018], for any $\\delta>0$ , with probability at least $1-\\delta$ , the following holds for all $h\\in\\mathcal{H}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathcal{E}_{\\ell}(h)-\\widehat{\\mathcal{E}}_{\\ell,S}(h)\\right|\\leq2\\mathfrak{R}_{m}^{\\ell}(\\mathcal{H})+B_{\\ell}\\sqrt{\\frac{\\log(2/\\delta)}{2m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For any $\\epsilon>0$ , by definition of the infimum, there exists $h^{*}\\in\\mathcal{H}$ such that $\\begin{array}{r}{\\mathcal{E}_{\\ell}(h^{*})\\leq\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})+\\epsilon}\\end{array}$ . By the definition of $\\widehat{h}_{S}$ , we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{\\ell}(\\widehat{h}_{S})-\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})=\\mathcal{E}_{\\ell}(\\widehat{h}_{S})-\\widehat{\\mathcal{E}}_{\\ell,S}(\\widehat{h}_{S})+\\widehat{\\mathcal{E}}_{\\ell,S}(\\widehat{h}_{S})-\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathcal{E}_{\\ell}(\\widehat{h}_{S})-\\widehat{\\mathcal{E}}_{\\ell,S}(\\widehat{h}_{S})+\\widehat{\\mathcal{E}}_{\\ell,S}(h^{*})-\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathcal{E}_{\\ell}(\\widehat{h}_{S})-\\widehat{\\mathcal{E}}_{\\ell,S}(\\widehat{h}_{S})+\\widehat{\\mathcal{E}}_{\\ell,S}(h^{*})-\\mathcal{E}_{\\ell}^{*}(h^{*})+\\epsilon}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\Bigg[2\\Re_{m}^{\\ell}(\\mathcal{H})+B_{\\ell}\\sqrt{\\frac{\\log(2/\\delta)}{2m}}\\Bigg]+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since the inequality holds for all $\\epsilon>0$ , it implies the following: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}\\big(\\widehat{h}_{S}\\big)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})\\leq4\\mathfrak{R}_{m}^{\\ell}(\\mathcal{H})+2B_{\\ell}\\sqrt{\\frac{\\log(2/\\delta)}{2m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Plugging in this inequality in the $\\mathcal{H}$ -consistency bound (21) completes the proof. ", "page_idx": 32}, {"type": "text", "text": "These bounds for surrogate loss minimizers, expressed in terms of minimizability gaps, offer more detailed and informative insights compared to existing bounds based solely on approximation errors. Our analysis of growth rates suggests that for commonly used smooth loss functions, $\\Gamma$ varies near zero with a square-root dependency. Furthermore, this dependency cannot be generally improved for arbitrary distributions. ", "page_idx": 32}, {"type": "text", "text": "P Future work ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We demonstrated a universal square-root growth rate for smooth surrogate losses commonly used in binary and multi-class classification. This result holds across all data distributions. A promising direction for future research would be to further investigate how incorporating specific distributional assumptions could refine these results. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: See the section \u201cOur results\u201d in the introduction. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: See Appendix P. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: See Section 3, Section 4, Section 5, Section 6, and Appendix. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not include experiments requiring code. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work is theoretical in nature and we do not anticipate any immediate negative societal impact. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 37}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]