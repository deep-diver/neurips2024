[{"figure_path": "7G362fgJFd/figures/figures_1_1.jpg", "caption": "Figure 1: Unifying image generation and segmentation. (a) We design a denoising diffusion model with a specific architecture that couples region prediction with spatially-masked diffusion over predicted regions, thereby generating both simultaneously. (b) An additional byproduct of running our trained denoising model on an arbitrary input image is a segmentation of that image. Using a model trained on FFHQ [31], we achieve both high quality synthesis of images and corresponding semantic segmentations (c-d), as well as the ability to accurately segment images of real faces (e-f). Segmenting a real image is fast, requiring only one forward pass (one denoising step).", "description": "This figure demonstrates the core concept of the paper: a unified model for image generation and segmentation.  It shows how the model simultaneously generates images and their corresponding segmentations (a).  It also highlights the model's ability to segment novel images using a single denoising step, making it efficient for real-world applications (b). The bottom row shows examples of generated images and segmentations, along with real images and their segmentations from the FFHQ dataset.", "section": "1 Introduction"}, {"figure_path": "7G362fgJFd/figures/figures_2_1.jpg", "caption": "Figure 2: Factorized diffusion architecture. Our framework restructures the architecture of the neural network within a DDPM [27] so as to decompose the image denoising task into parallel subtasks. All modules are end-to-end trainable and optimized according to the same denoising objective as DDPM. Left: Component factorization. An Encoder, equivalent to the first half of a standard DDPM U-Net architecture, extracts features henc. A common Middle Block processes Encoder output into shared latent features hmid. Note that Middle Block and hmid exist in the standard denoising DDPM U-Net by default. We draw it as a standalone module for a better illustration of the detailed architectural design. A Mask Generator, structured as the second half of a standard U-Net receives hmid as input, alongside all encoder features henc injected via skip connections to layers of corresponding resolution. This later network produces a soft classification of every pixel into one of K region masks, mo, m1, ..., mk. Right: Parallel decoding. A Decoder, also structured as the second half of a standard U-Net, runs separately for each region. Each instance of the Decoder receives shared features hmid and a masked view of encoder features henc mi injected via skip connections to corresponding layers. Decoder outputs are masked prior to combination. Though not pictured, we inject timestep embedding t into the Encoder, Mask Generator, and Decoder.", "description": "This figure illustrates the factorized diffusion architecture, showing how the image denoising task is decomposed into parallel subtasks. The left side depicts the component factorization, where an encoder extracts features, a middle block processes them, and a mask generator creates region masks. The right side shows the parallel decoding, where multiple decoders process the features and masks to generate the final image and segmentation.", "section": "3 Factorized Diffusion Models"}, {"figure_path": "7G362fgJFd/figures/figures_6_1.jpg", "caption": "Figure 3: Segmentation on Flower.", "description": "This figure shows the results of applying the proposed method for image segmentation on the Flower dataset.  The left panel displays the input images of various flowers, while the right panel shows the corresponding segmentation masks generated by the model. The masks effectively delineate the boundaries of the flowers, separating them from the background.", "section": "4 Experiments"}, {"figure_path": "7G362fgJFd/figures/figures_6_2.jpg", "caption": "Figure 3: Segmentation on Flower.", "description": "This figure shows the results of applying the proposed method to the Flower dataset.  The left side displays example images from the dataset, while the right shows the corresponding segmentations produced by the model. The segmentations highlight different regions within the images, effectively separating the flowers from their backgrounds and other elements.", "section": "4 Experiments"}, {"figure_path": "7G362fgJFd/figures/figures_7_1.jpg", "caption": "Figure 1: Unifying image generation and segmentation. (a) We design a denoising diffusion model with a specific architecture that couples region prediction with spatially-masked diffusion over predicted regions, thereby generating both simultaneously. (b) An additional byproduct of running our trained denoising model on an arbitrary input image is a segmentation of that image. Using a model trained on FFHQ [31], we achieve both high quality synthesis of images and corresponding semantic segmentations (c-d), as well as the ability to accurately segment images of real faces (e-f). Segmenting a real image is fast, requiring only one forward pass (one denoising step).", "description": "This figure demonstrates the unified approach of the proposed model for simultaneous image generation and semantic segmentation.  Part (a) shows the architecture: a denoising diffusion model that predicts regions and then performs spatially masked denoising within each region. Part (b) illustrates how the model segments a novel image by applying a single denoising step. Parts (c) through (f) present qualitative results, showcasing generated images and their corresponding segmentations, alongside real images and their segmented counterparts. The results highlight the model's ability to generate high-quality images and accurate semantic segmentations.", "section": "1 Introduction"}, {"figure_path": "7G362fgJFd/figures/figures_7_2.jpg", "caption": "Figure 6: Segmentation on ImageNet.", "description": "This figure shows example results for image segmentation on the ImageNet dataset. The left panel displays the input images and the right panel shows the corresponding segmentation masks generated by the proposed model. The masks accurately delineate various objects in the images, demonstrating the model's ability to perform accurate unsupervised image segmentation.", "section": "4 Experiments"}, {"figure_path": "7G362fgJFd/figures/figures_7_3.jpg", "caption": "Figure 3: Segmentation on Flower.", "description": "This figure shows the results of the proposed method on the Flower dataset.  The left panel displays the original flower images, and the right panel shows the corresponding segmentation masks generated by the model.  Each mask highlights the main flower region in each image, demonstrating the model's ability to accurately segment the foreground flower object from the background. The results visually illustrate the model\u2019s ability to perform accurate unsupervised segmentation.", "section": "4 Experiments"}, {"figure_path": "7G362fgJFd/figures/figures_7_4.jpg", "caption": "Figure 4: Segmentation on CUB.", "description": "This figure shows the results of applying the proposed method for image segmentation on the CUB dataset.  The left panel shows example images from the CUB dataset and the right panel presents the corresponding segmentations produced by the model.  The segmentations highlight different regions within the images, indicating the model's ability to successfully delineate object boundaries and separate foreground from background.", "section": "4 Experiments"}, {"figure_path": "7G362fgJFd/figures/figures_8_1.jpg", "caption": "Figure 6: Segmentation on ImageNet.", "description": "This figure shows the results of applying the proposed unsupervised image segmentation method to the ImageNet dataset.  It displays a grid of real images from ImageNet (a) alongside their corresponding generated segmentations (b). The segmentations highlight different image regions, demonstrating the model's ability to accurately segment various object categories without any explicit supervision during training.", "section": "4 Experiments"}, {"figure_path": "7G362fgJFd/figures/figures_8_2.jpg", "caption": "Figure 1: Unifying image generation and segmentation. (a) We design a denoising diffusion model with a specific architecture that couples region prediction with spatially-masked diffusion over predicted regions, thereby generating both simultaneously. (b) An additional byproduct of running our trained denoising model on an arbitrary input image is a segmentation of that image. Using a model trained on FFHQ [31], we achieve both high quality synthesis of images and corresponding semantic segmentations (c-d), as well as the ability to accurately segment images of real faces (e-f). Segmenting a real image is fast, requiring only one forward pass (one denoising step).", "description": "This figure shows the core idea of the paper: unifying image generation and segmentation using a denoising diffusion model.  It demonstrates the model's ability to simultaneously generate images and their corresponding semantic segmentations.  Panel (a) illustrates the architecture, (b) shows how segmentation is achieved on a new image, while (c-f) display example outputs: generated images and segmentations, and real images and their corresponding segmentations. The efficiency of segmenting real images (one forward pass) is highlighted.", "section": "1 Introduction"}, {"figure_path": "7G362fgJFd/figures/figures_8_3.jpg", "caption": "Figure 11: Gen. refinement along diffusion.", "description": "This figure visualizes the gradual refinement of both image and mask generation as denoising steps approach t=0 in the reverse diffusion process. It demonstrates the progressive improvement of the generated image and its corresponding mask from noisy initial state to a clear and well-defined final output, highlighting the DDPM\u2019s ability to refine both aspects simultaneously during denoising.", "section": "4.3 Ablation Study and Analysis"}, {"figure_path": "7G362fgJFd/figures/figures_9_1.jpg", "caption": "Figure 12: Interpolations on FFHQ with 250 timesteps of diffusion.", "description": "This figure shows interpolations of images from the FFHQ dataset using the model trained in the paper. The interpolations are generated by taking two images from the dataset as starting points and generating a sequence of intermediate images. The interpolations show a smooth transition between the two input images. The bottom row of images shows the corresponding heatmaps. These interpolations demonstrate the model's ability to generate realistic and varied images.", "section": "4.3 Ablation Study and Analysis"}, {"figure_path": "7G362fgJFd/figures/figures_9_2.jpg", "caption": "Figure 13: Segmentation on VOC-2012.", "description": "This figure shows the results of applying the proposed model on the PASCAL VOC 2012 dataset. The left panel displays example images from the dataset, and the right panel shows the corresponding segmentation masks generated by the model.  The model accurately segments various objects such as airplanes, boats, laptops and people demonstrating its ability to perform zero-shot object segmentation on unseen data.", "section": "4 Experiments"}, {"figure_path": "7G362fgJFd/figures/figures_13_1.jpg", "caption": "Figure 15: Hierarchical factorized diffusion architecture.", "description": "This figure illustrates a hierarchical factorized diffusion architecture.  Instead of a single level of region factorization, it uses multiple levels. The first level performs initial region segmentation, and each subsequent level refines the segmentation.  Each level employs the factorized diffusion architecture shown in Figure 2, using the previous level's output as input. The final segmentation is a combination of results from all levels.", "section": "A.1 Hierarchical Factorized Diffusion"}, {"figure_path": "7G362fgJFd/figures/figures_14_1.jpg", "caption": "Figure 15: Hierarchical factorized diffusion architecture.", "description": "This figure illustrates a hierarchical extension of the factorized diffusion model.  Instead of a single level of region mask generation and parallel decoding, it shows a multi-level approach. The first level generates coarse region masks, which are then used as input for the subsequent level. Each level refines the segmentation, leading to increasingly detailed region masks.  The diagram shows three levels, though the architecture could support more. Each level uses a factorized diffusion process, with separate decoding branches for each region. The final output is a combination of the region masks from all levels, representing a hierarchical segmentation.", "section": "A.1 Hierarchical Factorized Diffusion"}, {"figure_path": "7G362fgJFd/figures/figures_15_1.jpg", "caption": "Figure 15: Hierarchical factorized diffusion architecture.", "description": "This figure illustrates the proposed hierarchical factorized diffusion architecture.  It expands on the single-level architecture from Figure 2 by adding a second level of factorized diffusion. The first level (Level 1) generates coarse region masks, which are then used as input for the second level (Level 2). The second level further refines the segmentation into finer-grained regions.  Each branch at each level processes different segments in parallel, leading to a multi-level segmentation. This hierarchical approach allows for capturing both global context and finer details in the segmentation task.", "section": "A.1 Hierarchical Factorized Diffusion"}, {"figure_path": "7G362fgJFd/figures/figures_15_2.jpg", "caption": "Figure 18: Segmentation results on CUB with t \u2208 {0, 10, 20, 30, 40, 50, 60}.", "description": "This figure shows the segmentation results on the CUB dataset with varying noise levels (t). The x-axis represents the noise level, and the y-axis represents the performance metrics: Accuracy (Acc.), Intersection over Union (IOU), and Dice score (DICE).  Each line graph plots the trend of one of these metrics as the noise level increases. This allows for observation of how segmentation accuracy changes in relation to the amount of noise added to the image before denoising.", "section": "4 Experiments"}, {"figure_path": "7G362fgJFd/figures/figures_16_1.jpg", "caption": "Figure 3: Segmentation on Flower.", "description": "This figure shows the results of applying the proposed method for image segmentation on the Flower dataset.  The left panel displays a grid of real images from the dataset, and the right panel shows the corresponding segmentations generated by the model. Each segmentation mask highlights different regions within the flower images, demonstrating the model's ability to identify and separate various components of the flowers.", "section": "4 Experiments"}, {"figure_path": "7G362fgJFd/figures/figures_17_1.jpg", "caption": "Figure 4: Segmentation on CUB.", "description": "This figure shows the results of applying the proposed method to the CUB dataset.  The left panel shows the original images of birds, and the right panel displays the corresponding segmentations generated by the model. The segmentations highlight the different regions of the birds, such as the body, head, beak, etc. This demonstrates the model's ability to accurately segment objects within images.", "section": "4 Experiments"}, {"figure_path": "7G362fgJFd/figures/figures_17_2.jpg", "caption": "Figure 1: Unifying image generation and segmentation. (a) We design a denoising diffusion model with a specific architecture that couples region prediction with spatially-masked diffusion over predicted regions, thereby generating both simultaneously. (b) An additional byproduct of running our trained denoising model on an arbitrary input image is a segmentation of that image. Using a model trained on FFHQ [31], we achieve both high quality synthesis of images and corresponding semantic segmentations (c-d), as well as the ability to accurately segment images of real faces (e-f). Segmenting a real image is fast, requiring only one forward pass (one denoising step).", "description": "This figure shows the core idea of the paper, which is to unify image generation and segmentation using a denoising diffusion model.  The model is designed with a specific architecture that learns to both generate images and segment them simultaneously, without any explicit annotation during training.  Panel (a) illustrates the simultaneous generation of images and their corresponding regions. Panel (b) demonstrates how the model can segment a new input image by simply performing a single denoising step. Panels (c) through (f) show examples of image generation, region generation (segmentation), real images, and their corresponding segmentations, respectively.", "section": "1 Introduction"}, {"figure_path": "7G362fgJFd/figures/figures_18_1.jpg", "caption": "Figure 1: Unifying image generation and segmentation. (a) We design a denoising diffusion model with a specific architecture that couples region prediction with spatially-masked diffusion over predicted regions, thereby generating both simultaneously. (b) An additional byproduct of running our trained denoising model on an arbitrary input image is a segmentation of that image. Using a model trained on FFHQ [31], we achieve both high quality synthesis of images and corresponding semantic segmentations (c-d), as well as the ability to accurately segment images of real faces (e-f). Segmenting a real image is fast, requiring only one forward pass (one denoising step).", "description": "This figure demonstrates the model's ability to perform both image generation and segmentation simultaneously and independently.  (a) shows the architecture, where a denoising diffusion model predicts regions and then denoises them in parallel. (b) shows how the model segments a new image with a single forward pass. (c) and (d) showcase generated images and their corresponding segmentations. (e) and (f) illustrate the model's performance on real images.", "section": "1 Introduction"}, {"figure_path": "7G362fgJFd/figures/figures_18_2.jpg", "caption": "Figure 3: Segmentation on Flower.", "description": "This figure shows the results of applying the proposed unsupervised image segmentation method on the Flower dataset.  Subfigure (a) displays a grid of example images from the Flower dataset, while subfigure (b) presents the corresponding segmentation masks generated by the model.  The masks visually delineate the different regions within each image, demonstrating the model's ability to perform accurate unsupervised segmentation of natural images. The segmentation masks are monochromatic, effectively showing the segmentation's boundaries.", "section": "4 Experiments"}, {"figure_path": "7G362fgJFd/figures/figures_18_3.jpg", "caption": "Figure 7: Generation on Flower.", "description": "This figure shows samples of generated images and their corresponding generated masks from the Flower dataset.  The left panel displays a grid of generated images of flowers, demonstrating the model's ability to synthesize diverse and realistic flower images. The right panel shows the corresponding generated masks, which segment the images into regions representing different parts of the flowers, such as petals, leaves, and stems.", "section": "4.2 Image and Mask Generation"}, {"figure_path": "7G362fgJFd/figures/figures_19_1.jpg", "caption": "Figure 7: Generation on Flower.", "description": "This figure shows the results of image and mask generation on the Flower dataset.  The left panel displays a grid of generated images, demonstrating the model's ability to synthesize realistic flower images with varying compositions and backgrounds. The right panel presents corresponding generated masks, highlighting the model's capacity to simultaneously segment these images into meaningful regions. The masks effectively delineate different components within each flower image, showcasing the model's understanding of visual structure and object boundaries.", "section": "4.2 Image and Mask Generation"}, {"figure_path": "7G362fgJFd/figures/figures_19_2.jpg", "caption": "Figure 1: Unifying image generation and segmentation. (a) We design a denoising diffusion model with a specific architecture that couples region prediction with spatially-masked diffusion over predicted regions, thereby generating both simultaneously. (b) An additional byproduct of running our trained denoising model on an arbitrary input image is a segmentation of that image. Using a model trained on FFHQ [31], we achieve both high quality synthesis of images and corresponding semantic segmentations (c-d), as well as the ability to accurately segment images of real faces (e-f). Segmenting a real image is fast, requiring only one forward pass (one denoising step).", "description": "This figure demonstrates the model's ability to perform both image generation and segmentation simultaneously.  Panel (a) shows the model architecture, illustrating how region prediction and masked diffusion are coupled. Panel (b) shows how the model segments a novel input image, using only one forward pass. Panels (c) and (d) showcase generated images and their corresponding segmentations. Panels (e) and (f) demonstrate accurate segmentation of real images.", "section": "1 Introduction"}, {"figure_path": "7G362fgJFd/figures/figures_19_3.jpg", "caption": "Figure 27: Conditional ImageNet generation.", "description": "This figure shows the results of conditional image generation on the ImageNet dataset.  The left panel (a) displays a grid of generated images, showcasing the model's ability to synthesize various ImageNet classes. The right panel (b) presents the corresponding generated masks for each image, demonstrating the model's simultaneous segmentation capabilities. The figure visually exemplifies the model's proficiency in generating diverse and realistic images while simultaneously outputting accurate semantic segmentations. This is a key finding of the paper, showcasing the unified approach to image generation and segmentation.", "section": "4.2 Image and Mask Generation"}, {"figure_path": "7G362fgJFd/figures/figures_20_1.jpg", "caption": "Figure 28: Segmentation on VOC-2012.", "description": "This figure shows the results of zero-shot object segmentation on the PASCAL VOC 2012 dataset. The left panel displays example images from the dataset, while the right panel shows the corresponding segmentations produced by the proposed model.  The model successfully identifies and segments various objects such as bicycles, chairs, potted plants, and trains, demonstrating its ability to generalize to unseen data.", "section": "A.4 Additional Zero-shot Results on VOC"}, {"figure_path": "7G362fgJFd/figures/figures_20_2.jpg", "caption": "Figure 29: Segmentation on DAVIS-2017.", "description": "This figure shows the results of applying the proposed method to the DAVIS-2017 dataset.  It displays frames from two videos, \"classic-car\" and \"dance-jump\",  alongside their corresponding segmentations generated by the model.  The segmentations demonstrate the model's ability to accurately identify and delineate objects in video sequences, even without any video-specific training.", "section": "A.5 Additional Zero-shot Results on DAVIS"}, {"figure_path": "7G362fgJFd/figures/figures_20_3.jpg", "caption": "Figure 1: Unifying image generation and segmentation. (a) We design a denoising diffusion model with a specific architecture that couples region prediction with spatially-masked diffusion over predicted regions, thereby generating both simultaneously. (b) An additional byproduct of running our trained denoising model on an arbitrary input image is a segmentation of that image. Using a model trained on FFHQ [31], we achieve both high quality synthesis of images and corresponding semantic segmentations (c-d), as well as the ability to accurately segment images of real faces (e-f). Segmenting a real image is fast, requiring only one forward pass (one denoising step).", "description": "This figure demonstrates the model's ability to perform both image generation and image segmentation simultaneously in an unsupervised manner.  It shows the architecture (a) where region prediction is coupled with diffusion, the process of segmenting a new image (b), examples of generated images and their corresponding segmentations (c, d), and finally examples of real images and their segmentations (e, f). The key takeaway is that the model achieves both tasks without any annotations or prior knowledge.", "section": "1 Introduction"}]