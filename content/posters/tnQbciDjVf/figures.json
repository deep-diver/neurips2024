[{"figure_path": "tnQbciDjVf/figures/figures_1_1.jpg", "caption": "Figure 1: An overview of our TransAgent. (a) TransAgent transfers multi-source knowledge from heterogeneous agents to enhance the generalization ability of vision-language foundation models. It demonstrates knowledge versatility, transfer flexibility and deployment efficiency through elaborate agent collaboration and knowledge ensemble strategy. (b) SOTA comparison for base-to-novel generalization on 11 visual recognition benchmarks. Our method outperforms previous SOTA, especially on the more diversified target domains.", "description": "This figure provides a high-level overview of the TransAgent framework. (a) illustrates the framework's architecture, showing how it integrates knowledge from various heterogeneous agents (vision, language, and multi-modal) to improve the performance of vision-language foundation models.  It highlights the framework's key features: knowledge versatility, transfer flexibility, and deployment efficiency. (b) presents a comparison of TransAgent's performance against state-of-the-art (SOTA) methods on 11 visual recognition benchmarks, demonstrating its superior generalization ability, especially in diverse target domains.", "section": "1 Introduction"}, {"figure_path": "tnQbciDjVf/figures/figures_3_1.jpg", "caption": "Figure 2: Vision Agent Collaboration and Language Agent Collaboration. (a) VAC integrates visual knowledge via MoA gating and transfers the knowledge through layer-wise feature distillation. (b) LAC enhances the textual representations through class-specific feature distillation between the prompted textual feature and the gated textual feature.", "description": "This figure illustrates the methods used for vision and language agent collaboration in the TransAgent framework.  (a) shows how visual knowledge from various vision agents (DINO, MAE, ViTDet, SAM) is combined using a Mixture-of-Agents (MoA) gating mechanism to create gated visual tokens, which are then used in layer-wise feature distillation to enhance CLIP's vision encoder. (b) shows how textual knowledge from language agents (GPT-3, Vicuna, BERT) is integrated using MoA gating, generating gated textual tokens that undergo class-specific feature distillation with CLIP's textual features to improve CLIP's textual representation.", "section": "3 Method"}, {"figure_path": "tnQbciDjVf/figures/figures_5_1.jpg", "caption": "Figure 3: Multi-modal Agent Collaboration. Top left: We first extract the cross attention maps from the T2I agents and then obtain the score vectors through LSE pooling. Top right: We compute the score vectors from the I2T agents as the cosine similarity between the projected visual feature and the LLM's textual feature. Finally, we perform score distillation between the learned score vectors and the gated score vectors to further align the learnable prompts.", "description": "This figure illustrates the Multi-modal Agent Collaboration (MAC) part of the TransAgent framework.  It shows how knowledge is extracted from both Text-to-Image (T2I) and Image-to-Text (I2T) models. For T2I models, cross-attention maps are extracted and processed via LogSumExp (LSE) pooling to get score vectors. For I2T models, cosine similarity between projected visual features and LLM textual features generates score vectors.  These score vectors from different multi-modal agents are then gated and used in score distillation with CLIP's learned score vectors to better align the learnable prompts.", "section": "3 Method"}, {"figure_path": "tnQbciDjVf/figures/figures_7_1.jpg", "caption": "Figure 1: An overview of our TransAgent. (a) TransAgent transfers multi-source knowledge from heterogeneous agents to enhance the generalization ability of vision-language foundation models. It demonstrates knowledge versatility, transfer flexibility and deployment efficiency through elaborate agent collaboration and knowledge ensemble strategy. (b) SOTA comparison for base-to-novel generalization on 11 visual recognition benchmarks. Our method outperforms previous SOTA, especially on the more diversified target domains.", "description": "This figure provides a comprehensive overview of the TransAgent framework.  Panel (a) illustrates the architecture, showing how multiple heterogeneous agents (vision, language, and multi-modal) contribute their knowledge to a vision-language foundation model (like CLIP).  Panel (b) presents a comparison of TransAgent's performance against state-of-the-art (SOTA) methods on eleven visual recognition datasets, highlighting its superior generalization capabilities, particularly in scenarios with significant domain shifts.", "section": "1 Introduction"}, {"figure_path": "tnQbciDjVf/figures/figures_8_1.jpg", "caption": "Figure 5: Averaged gating weights of each agent on different datasets. Deeper color indicates more contributions to the gated feature(s) or score vectors.", "description": "This figure visualizes the contribution of each agent (Vision, Language, and Multimodal agents) in the TransAgent framework across different datasets.  The heatmap shows the average gating weights, where darker colors represent a stronger influence of a particular agent on the gated features or score vectors within each dataset. This highlights the adaptive nature of TransAgent, which automatically selects agents based on their relevance to the specific dataset.", "section": "4.3 Visualization"}, {"figure_path": "tnQbciDjVf/figures/figures_17_1.jpg", "caption": "Figure 6: Variance and performance of TransAgent compared with CoOp. TransAgent demonstrates better robustness and outperforms CoOp on most low-shot cases.", "description": "This figure compares the performance of TransAgent and CoOp on eleven visual recognition datasets under low-shot learning scenarios.  Each subplot represents a dataset, showing the accuracy with varying numbers of training samples (shots) per class. The shaded areas represent the variance across multiple runs.  The results demonstrate that TransAgent consistently outperforms CoOp across different datasets and exhibits greater robustness (smaller variance) in most cases, highlighting its superior performance under low-shot learning conditions.", "section": "4.1 Comparison with State-of-the-Art"}]