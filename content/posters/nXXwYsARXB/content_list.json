[{"type": "text", "text": "A hierarchical decomposition for explaining ML performance discrepancies ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Harvineet Singh1 Fan Xia1 Adarsh Subbaswamy2 Alexej Gossmann2 Jean Feng1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1University of California, San Francisco 2U.S. Food and Drug Administration, Center for Devices and Radiological Health ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning (ML) algorithms can often differ in performance across domains. Understanding why their performance differs is crucial for determining what types of interventions (e.g., algorithmic or operational) are most effective at closing the performance gaps. Aggregate decompositions express the total performance gap as the gap due to a shift in the feature distribution $p(X)$ plus the gap due to a shift in the outcome\u2019s conditional distribution $p(Y|X)$ . While this coarse explanation is helpful for guiding root cause analyses, it provides limited details and can only suggest coarse fixes involving all variables in an ML system. $D e$ - tailed decompositions quantify the importance of each variable to each term in the aggregate decomposition, which can provide a deeper understanding and suggest more targeted interventions. Although parametric methods exist for conducting a full hierarchical decomposition of an algorithm\u2019s performance gap at the aggregate and detailed levels, current nonparametric methods only cover parts of the hierarchy; many also require knowledge of the entire causal graph. We introduce a nonparametric hierarchical framework for explaining why the performance of an ML algorithm differs across domains, without requiring causal knowledge. Furthermore, we derive debiased, computationally-efficient estimators and statistical inference procedures to construct confidence intervals for the explanations. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The performance of an ML algorithm can differ across domains due to shifts in the data distribution. Understanding what contributed to this performance gap can help teams choose the most effective corrective action(s), ranging from algorithmic modifications (e.g. model retraining) to operational fixes (e.g. updating data pipelines). Prior works have focused primarily on aggregate decompositions, which decompose the performance gap into that due to a shift in the marginal distribution of the input features $p(X)$ (covariate shift [37]) and that due to a shift in the conditional distribution of the outcome $p(Y|X)$ (concept shift or conditional outcome shift) [5, 50, 30, 36, 16]. However, coarse decompositions can only suggest coarse corrective actions, such as investigating data pipelines for all features. The goal of this work is to provide a hierarchical nonparametric framework that first decomposes a performance gap into aggregate terms and then each aggregate term into detailed terms. This helps narrow down the features to investigate and understand how they affect the gap. ", "page_idx": 0}, {"type": "text", "text": "If one is willing to make the strong assumption that the expected loss of a model is a linear function of some feature set $X$ , the problem of obtaining aggregate and detailed decompositions drastically simplifies. This is the key assumption underlying the Oaxaca-Blinder (OB) decomposition, one of the most widely used frameworks in the (income and health) disparities literature [32, 3]. Given an ML model with average loss $\\mathbb{E}_{D}[\\ell]$ in domains $D=0$ and 1 and assuming the linear loss relationship $\\mathbb{E}_{D}[\\ell|X]=\\beta_{D}^{\\top}X$ , the OB framework decomposes the performance gap at the aggregate level into that due to a covariate shift $(\\beta_{0}^{\\top}(\\mathbb{E}_{1}[X]\\,-\\,\\mathbb{E}_{0}[X]))$ and that due to a conditional outcome shift $((\\beta_{1}-\\beta_{0})^{\\top}\\mathbb{E}_{1}[X])$ . That is, the former is due to a shift in the feature means and the latter is due to a shift in the coefficients. At the detailed level, the aggregate terms corresponding to covariate and conditional outcome shifts are further broken down into the contributions from each feature, i.e. $\\beta_{0,j}\\left(\\mathbb{E}_{1}[X_{j}]-\\mathbb{E}_{0}[X_{j}]\\right)$ and $(\\beta_{1,j}-\\beta_{0,j})\\mathbb{E}_{1}[X_{j}]$ , respectively. Although the highly intuitive nature of the OB framework has led to its widespread popularity, the terms are difficult to interpret under model misspecification. As such, this work aims to define a similar hierarchical decomposition framework for explaining ML performance disparities, without making strong parametric assumptions. ", "page_idx": 0}, {"type": "image", "img_path": "nXXwYsARXB/tmp/27ac17bade66b431022870e0b3788329ce533dbe76999dad3f5f506331ebf2a1.jpg", "img_caption": ["Figure 1: (left) Proposed framework called Hierarchical Decomposition of Performance Differences (namely, HDPD) helps to understand performance gaps of an ML algorithm between two domains. It decomposes the overall gap (say, in classification accuracy) into gaps due to shifts in the covariate versus outcome distribution (Aggregate). Then, it quantifies the importance of each feature to the two components (Detailed). (right) In terms of directed acyclic graphs, aggregate decompositions describe the effect of shift interventions, for instance, on the outcome $Y$ distribution while keeping all else fixed between domains. Detailed decompositions quantify how well can we explain those shift interventions by more targeted shifts with respect to feature subsets $Z_{s}$ alone. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "There is currently no unified, nonparametric framework that obtains aggregate and detailed decompositions. Instead, solutions have been proposed for parts of the hierarchy (see Table 1): nonparametric methods exist for the aggregate decomposition [5, 30, 48] and, assuming the causal graph is known, detailed decompositions of the covariate shift [40, 44, 50, 38, 4, 23]. However, the causal graph is unlikely to be known in high-dimensional settings and, more importantly, there are no methods for simultaneously obtaining a detailed decomposition of the conditional outcome shift. There are also methods that do not decompose the performance gap and instead describe distribution shifts in the variables [29] or model explanations [12, 31, 23]. However, such approaches do not quantify how such shifts ultimately contribute to an ML performance gap. We make the following contributions. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a unified hierarchical nonparametric framework for decomposing the performance gap of an ML algorithm (Fig 1 left). Using the concept of partial distribution shifts, we generalize shifts with respect to variable subgroups to encompass not only covariate shifts but also conditional outcome shifts. We then introduce a unified scoring rule for (candidate) partial shifts, which can be used even when the causal graph is not known.   \n\u2022 We derive novel debiased and asymptotically normal estimators for terms in the decomposition, which allow us to construct confidence intervals (CIs) with asymptotically valid coverage rates.   \n\u2022 We demonstrate the utility of our framework in real-world examples of prediction models for hospital readmission and insurance coverage. Code for reproducing experiments is available at https://github.com/jjfeng/HDPD. ", "page_idx": 1}, {"type": "text", "text": "2 A unifying framework for explaining performance gaps ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation. Consider a prediction algorithm $f:\\mathcal{X}\\subseteq\\mathbb{R}^{m}\\to[0,1]$ for binary outcomes $Y$ across source and target domains, denoted by $D=0$ and $D=1$ , respectively. Let the performance of $f$ be quantified in terms of a loss function $\\ell:\\mathcal{X}\\times\\{0,1\\}\\to\\mathbb{R}$ , such as the 0-1 misclassification loss $\\bar{\\mathbb{1}^{\\prime}}\\{f(X)\\neq Y\\}$ . Suppose variables $X$ can be partitioned into disjoint sets $W\\,\\in\\,\\mathbb{R}^{m_{1}}$ and $Z=X\\setminus W\\in\\mathbb{R}^{m_{2}}$ , where $m=m_{1}+m_{2}$ . Although our framework does not require knowing the causal ordering between variables, the interpretation is more intuitive when $W$ is causally upstream of $Z$ and $Y$ $\\mathrm{Fig}~1$ right). Variables $Z$ can be chosen to be mediators or modifiers of the effect of the domain shift $D$ on $Y$ . For instance, if $Z$ are treatment variables and $W$ are baseline variables, one can interpret a covariate shift as a change in the treatment policy and an outcome shift as a change in the treatment effect across the two environments. In absence of any causal knowledge, another option is to choose $W$ as the variables for which one would like the expected loss given $W$ to be invariant across the two environments; this can be useful to promote fairness of ML algorithms across environments. When this invariance does not hold, the framework explains how variables $Z$ contribute to these differences. We refer to $W$ as baseline variables and $Z$ as conditional covariates. Please refer to Appendix C for more discussion on choosing $W$ and $Z$ as well as a summary of notation (Table 2). ", "page_idx": 1}, {"type": "table", "img_path": "nXXwYsARXB/tmp/67282a661e1c1a2290a1d5378f1e8f7aa9047b1173f98e212625419c31d31617.jpg", "table_caption": ["Table 1: Comparison of HDPD to prior works that decompose ML performance gaps. The distinguishing contribution of this work is that it unifies aggregate and detailed decompositions under a nonparametric framework with uncertainty quantification. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our proposed hierarchical decomposition of an ML performance gap is based on a stratification of distribution shifts into aggregate and partial shifts. At the aggregate level, the joint distribution of $(W,Z,Y)$ can be factorized with respect to the aggregate variable groups $W,Z$ , and $Y$ , i.e. ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{D_{\\mathbb{V}}}(W)p_{D_{\\mathbb{Z}}}(Z|W)p_{D_{\\mathbb{Y}}}(Y|W,Z),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where subscripts $D_{\\mathsf{W}},D_{\\mathsf{Z}}$ and $D_{\\tt Y}$ indicate the domain of that factor. An aggregate shift substitutes a factor from the source domain in (1) with that from the target domain, i.e. we swap the factor from $p_{0}$ to $p_{1}$ . A partial shift with respect to variable subset $s$ (or an $s$ -partial shift) shifts a factor from the source domain in (1) only with respect to variable subset $s$ ; we denote this by swapping a factor from $p_{0}$ to $p_{s}$ . (We keep the precise definition of $s$ -partial shifts purposely vague until Section 2.2.) We denote expectations with respect to the joint distribution (1) as $\\mathbb{E}_{D_{\\mathrm{v}}D_{\\mathrm{z}}D_{\\mathrm{y}}}$ . ", "page_idx": 2}, {"type": "text", "text": "The overall performance gap between domains, $\\Lambda=\\mathbb{E}_{111}\\left[\\ell(W,Z,Y)\\right]-\\mathbb{E}_{000}\\left[\\ell(W,Z,Y)\\right].$ , can be decomposed hierarchically as follows. In the Appendix $\\mathrm{D}$ , we also discuss how this decomposition can be interpreted causally under certain conditions. ", "page_idx": 2}, {"type": "text", "text": "Aggregate. At the first level of the hierarchy, the framework quantifies how aggregate shifts contribute to the performance gap individually. This leads to the decomposition $\\Lambda=\\Lambda_{\\mathsf{W}}+\\Lambda_{\\mathsf{Z}}+\\Lambda_{\\mathsf{Y}}$ , where $\\Lambda_{\\mathsf{V}}$ quantifies the impact of a shift in the baseline distribution $p(\\bar{W})$ , $\\Lambda_{z}$ quantifies the impact of a shift in the conditional covariate distribution $p(Z|W)$ , and $\\Lambda_{\\Upsilon}$ quantifies the impact of a shift in the outcome distribution $p(Y|W,Z)$ . More concretely, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Lambda_{\\mathsf{M}}=\\mathbb{E}_{100}\\left[\\ell\\right]-\\mathbb{E}_{000}\\left[\\ell\\right]}\\\\ &{\\Lambda_{\\mathsf{Z}}=\\mathbb{E}_{110}\\left[\\ell\\right]-\\mathbb{E}_{100}[\\ell]=\\mathbb{E}_{1\\cdot}\\Big[\\underbrace{\\mathbb{E}_{\\cdot10}\\left[\\ell\\mid W\\right]-\\mathbb{E}_{\\cdot00}\\left[\\ell\\mid W\\right]}_{\\Delta_{\\cdot10}\\left(W\\right)}\\Big]}\\\\ &{\\Lambda_{\\mathsf{Y}}=\\mathbb{E}_{111}[\\ell]-\\mathbb{E}_{110}[\\ell]=\\mathbb{E}_{11\\cdot}\\Big[\\underbrace{\\mathbb{E}_{\\cdot1}\\left[\\ell\\mid W,Z\\right]-\\mathbb{E}_{\\cdot0}\\left[\\ell\\mid W,Z\\right]}_{\\Delta_{\\cdot0}\\left(W,Z\\right)}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The same (or similar) aggregate decompositions have also appeared in prior works [5, 30, 15, 50, 38]. ", "page_idx": 2}, {"type": "text", "text": "Detailed. At the detailed level, each aggregate term is further broken down into variable-level attributions. The effect of each variable can be isolated using partial shifts. However, because variables can interact to induce complex partial distribution shifts, we define variable importance (VI) using the Shapley attribution framework [41, 6, 31, 17], which has the benefits of satisfying axiomatic properties such as fairness, monotonicity, and full attribution. Thus, given a real-valued value function $v$ that quantifies the contribution of an $s$ -partial shift to an aggregate shift for all $s\\subseteq\\{1,\\cdots\\,,m\\}$ , the attribution to variable $j$ is the average gain in value when additionally shifting with respect to $j$ , i.e. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi_{j}:=\\ \\frac{1}{m}\\sum_{s\\subseteq\\{1,\\cdots,m\\}\\setminus j}{\\binom{m-1}{|s|}}^{-1}\\{v(s\\cup j)-v(s)\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Interpretation. Such VI values can help ML teams identify the underlying cause(s) for a performance gap and design targeted operational and/or algorithmic interventions. For instance, a variable with high importance to the conditional covariate shift term $\\Lambda_{\\mathrm{Z}}$ may indicate differences in the variable\u2019s missingness rates, prevalence, or selection bias across domains. If instead the variable is highly important to the conditional outcome shift term $\\Lambda_{\\Upsilon}$ , it may indicate inherent differences in the conditional distribution (i.e. effect modification), differences in measurement error or the way outcome is defined between domains, or omission of variables predictive of the outcome. Finally, note that variable importances should be viewed as relative to the variables included in the framework rather than absolute importances, as one cannot include all possible explanatory variables. ", "page_idx": 3}, {"type": "text", "text": "To define VI values, the key question is how to define a value function $v$ that is applicable to different types of $s$ -partial shifts, even when the causal graph is not known. It turns out that the answer is far from straightforward. The next section discusses how the value function and candidate $s$ -partial shifts must be defined with care. ", "page_idx": 3}, {"type": "text", "text": "2.1 Value of partial distribution shifts ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "When the true causal graph is known, prior works define an $s$ -partial covariate shift as the substitution of nodes $s$ with mechanisms from the target domain and its value $v(s)$ as the difference in the average loss, e.g. $\\mathbb{E}_{1s0}[\\ell]-\\mathbb{E}_{100}[\\ell]$ [50, 38]. However, this has a number of limitations: (i) knowing the entire causal graph is often impractical, (ii) in the absence of such a graph, this value function is not a proper scoring rule and can assign high values to partial shifts that contradict the true causal graph (see Example E.1 for details), and (iii) $v(s)$ can be high even if the shift does not induce similar shifts in the loss as the aggregate shift. ", "page_idx": 3}, {"type": "text", "text": "Instead, we propose to evaluate candidate $s$ -partial shifts by how closely they approximate aggregate shifts, using a nonparametric extension of the traditional $R^{2}$ measure. In the case of conditional covariate shifts, an aggregate shift induces a performance difference of $\\Delta._{10}(W)$ in strata $W$ while a candidate $s$ -partial shift induces a performance difference of $\\Delta_{\\cdot s0}(W)=\\mathbb{E}_{\\cdot s0}[\\ell|W]-\\mathbb{E}_{\\cdot00}[\\ell|W]$ . The value of this $s$ -partial shift is then the percent variation of $\\Delta_{\\cdot10}$ explained by $\\Delta_{\\cdot s0}$ , i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\nv_{\\mathrm{Z}}(s):=1-\\frac{\\mathbb{E}_{1\\cdots}\\left[\\left(\\Delta_{\\cdot s0}(W)-\\Delta_{\\cdot10}(W)\\right)^{2}\\right]}{\\mathbb{E}_{1\\cdots}\\left[\\Delta_{\\cdot10}^{2}(W)\\right]}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Likewise, for conditional outcome shifts, an aggregate shift induces a performance difference of $\\Delta.._{1}(W,Z)$ in strata $(W,Z)$ while a candidate $s$ -partial shift induces a performance difference of $\\Delta.._{s}(W,Z)=\\mathbb{E}.._{s}[\\ell|W,Z]-\\mathbb{E}._{0}[\\ell|W,Z]$ . The value of this $s$ -partial conditional outcome shift is then defined as the percent variation of $\\Delta_{\\cdots1}$ explained by $\\Delta.._{s}$ , i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\nv_{\\mathtt{Y}}(s):=1-\\frac{\\mathbb{E}_{11\\cdot}\\left[(\\Delta...s(W,Z)-\\Delta..1(W,Z))^{2}\\right]}{\\mathbb{E}_{11\\cdot}\\left[\\Delta_{\\cdot\\cdot1}^{2}(W,Z)\\right]}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This formulation of the value function in terms of $R^{2}$ provides a unified way to score partial conditional covariate and outcome shifts, does not require knowledge of the true causal graph, and is a strictly proper scoring rule under certain conditions (see Appendix E). In general, we expect the highest scoring candidate $s$ -partial shifts to be those that are close to the true causal graph and induce large shifts in the ML algorithm\u2019s loss. Finally, we acknowledge one caveat with this framework: because some variables must be held out to define the $R^{2}$ measure, we cannot score partial shifts in the baseline variables $W$ . We hope to close this gap in future work. ", "page_idx": 3}, {"type": "text", "text": "2.2 Candidate partial distribution shifts ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now present the set of candidate partial shifts considered in this work. High-level illustrations for the candidate partial shifts are given in Fig 1 right top; more detailed illustrations are given in ", "page_idx": 3}, {"type": "text", "text": "Fig 4 of the Appendix. We emphasize that these are candidates, as the true causal graph is not known.   \nWhile there are certainly other partial shifts that one may consider, many have various disadvantages.   \nAs such, we leave the investigation of other partial shifts to future work. ", "page_idx": 4}, {"type": "text", "text": "$s$ -partial conditional covariate shift: Suppose $Z_{-s}$ is downstream of $Z_{s}$ . Then $p_{s}(z|w)\\;:=\\;$ $\\bar{p_{1}}\\bar{(z_{s}|w)}p_{0}(z_{-s}|z_{s},w)$ . Wu et al. [48] considered a similar proposal. ", "page_idx": 4}, {"type": "text", "text": "$s$ -partial conditional outcome shift: Shifting the conditional distribution of $Y$ only with respect to a variable subset $Z_{s}$ but not $Z_{-s}$ requires care. We cannot simply define $p_{s}(Y|W,Z)$ as a function of only $W$ and $Z_{s}$ . Such a definition would imply that an $s$ -partial shift has a non-zero effect, even in settings with no shift in the conditional outcome distribution (i.e. $p_{1}(Y|W,Z)\\equiv p_{0}(Y|W,Z))$ . ", "page_idx": 4}, {"type": "text", "text": "Instead, we define an $s$ -partial outcome shift based on models commonly used in model recalibration/revision [42, 34], where the modified risk (conditional probability of $Y$ ) is a function of the risk in the source domain $Q:=q(W,Z):=p_{0}(Y=1|W,Z),W$ , and $Z_{s}$ . That is, we define the shift as ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{s}(y|z,r,w):=p_{1}(y|z_{s},r,w)=\\int p_{1}(y|\\tilde{z}_{-s},z_{s},w)p_{1}(\\tilde{z}_{-s}|z_{s},q(w,z_{s},\\tilde{z}_{-s})=r,w)d\\tilde{z}_{-s}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By defining the shifted outcome distribution solely as a function of $Q,W$ , and $Z_{s}$ , any direct effect from $Z_{-s}$ to $Y$ is eliminated and $p_{s}$ has the desired behavior in the setting where there is no conditional outcome shift. ", "page_idx": 4}, {"type": "text", "text": "3 Estimation and statistical inference ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here we discuss estimation and statistical inference for the aggregate terms $(\\Lambda_{\\mathrm{W}},\\Lambda_{\\mathrm{Z}}$ , and $\\Lambda_{\\Upsilon}$ ), the value functions $v_{\\mathrm{Z}}(s)$ and $v_{\\Upsilon}(s)$ , and the Shapley-based detailed terms $\\phi_{\\mathrm{Z},j}$ and $\\phi_{\\Upsilon,j}$ for $j\\in(0,\\cdots,m_{2})$ . One approach is to rely on plug-in estimators, which plug in estimates of conditional means (also called outcome models) or density ratios [43], which we collectively refer to as nuisance parameters. For instance, one can estimate the conditional means $\\mu_{\\cdot10}(w)=\\mathbb{E}_{\\cdot10}[\\ell|W]$ and $\\mu._{00}(w)=\\mathbb{E}._{00}[\\ell|W]$ using ML and take the empirical mean of $\\hat{\\mu}_{\\cdot10}\\mathrm{~-~}\\hat{\\mu}_{\\cdot00}$ with respect to the target domain to get a plug-in estimator for $\\Lambda_{2}=\\mathbb{E}_{1\\cdots}[\\mu_{\\cdot10}-\\mu._{00}]$ . However, because estimation of the true nuisance parameters using ML typically converge at a rate slower than $n^{-1/2}$ , plug-in estimators generally fail to be consistent at a rate of $n^{-1/2}$ and cannot be used to construct CIs [25]. ", "page_idx": 4}, {"type": "text", "text": "To this end, we use the method of one-step correction from semiparametric inference to derive debiased ML estimators [45, 7]. The core idea is to subtract the first-order bias of a plug-in estimator, which requires characterizing the canonical gradient (or efficient influence function) of the estimand [25]. The primary technical contribution in this section is the derivation of debiased estimators for the detailed decompositions. (Estimation and inference for the aggregate decomposition is well-studied, as the aggregate terms can be formulated as average causal effects.) Due to space limitations, this section only presents estimators for the detailed decomposition of the conditional outcome shift. This estimand is particularly interesting, as its unique structure is not amenable to standard techniques for debiasing ML estimators. We refer the reader to the Appendix for derivations, pseudocode, and proofs for all the estimators. ", "page_idx": 4}, {"type": "text", "text": "Notation. Let $\\mathbb{P}_{D}$ denote the expectation with respect to domain $D$ . For ease of exposition, suppose the number of IID observations from each domain is the same, denoted by $n$ . We present split-sample estimators, though the results can be readily extended using cross-fitting [7, 25]. Let the data be randomly split into \u201ctraining\u201d and \u201cevaluation\u201d partitions. Let $\\mathbb{P}_{D,n}$ denote the empirical average in the evaluation partition for domain $D$ . All estimated quantities are denoted using hat notation. ", "page_idx": 4}, {"type": "text", "text": "3.1 Value of $s$ -partial conditional outcome shifts ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here we describe the high-level steps for deriving a debiased estimator for $v_{\\Upsilon}(s)$ , the value of a candidate $s$ -partial conditional outcome shift. The following section describes a computationally efficient procedure for combining such estimates to obtain Shapley values. ", "page_idx": 4}, {"type": "text", "text": "Standard recipes for deriving asymptotically normal, nonparametric-efficient estimators rely on pathwise differentiability of the estimand and analyzing its efficient influence function [25]. However, $v_{\\Upsilon}(s)$ is not pathwise differentiable because it is a function of (5), which conditions on the source risk $q(w,z)$ equalling some value $r$ . Taking the pathwise derivative of $v_{\\Upsilon}(s)$ requires taking a derivative of the indicator function $\\mathbb{1}\\{q(w,z)=r\\}$ , which generally does not exist. Given the difficulties in deriving an asymptotically normal estimator for $v_{\\mathtt{Y}}(s)$ , we propose estimating a close alternative that is pathwise differentiable. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The idea is to replace $q$ in (5) with its binned variant $\\begin{array}{r}{q_{\\mathrm{bin}}(w,z)\\,=\\,\\frac{1}{B}\\lfloor q(w,z)B+\\frac{1}{2}\\rfloor}\\end{array}$ for some $B\\in\\mathbb{Z}^{+}$ , which discretizes outputs from $q$ into $B$ disjoint bins. As long as $B$ is sufficiently high, the binned version of the estimand, denoted $v_{\\mathtt{Y,b i n}}(s)$ , is a close approximation to $v_{\\mathtt{Y}}(s)$ . (We use $B=20$ in the empirical analyses, which we believe to be sufficient in practice.) The benefit of this binned variant is that the derivative of the indicator function $\\mathbb{1}\\{q_{\\mathsf{b i n}}(w,\\bar{z}){=}r\\}$ is zero almost everywhere as long as observations with source risks exactly equal to a bin edge have measure zero. More formally, we require the following: ", "page_idx": 5}, {"type": "text", "text": "Condition 3.1. Let \u039e be the set of $(W,Z)$ such that $q(W,Z)$ falls precisely on some bin edge and is not equal to zero or one. The set $\\Xi$ is measure zero. ", "page_idx": 5}, {"type": "text", "text": "Under this condition, $v_{\\mathtt{Y,b i n}}(s)$ is pathwise differentiable and, using one-step correction, we derive a debiased ML estimator that has the unique form of a ${\\mathrm{V}}.$ -statistic (this follows from the integration over \u201cphantom\u201d $\\tilde{z}_{-s}$ in (5)). We represent ${\\mathrm{V}}.$ -statistics using the operator $\\mathbb{P}_{1,n}\\tilde{\\mathbb{P}}_{1,n}$ , which takes the average over all pairs of observations $O_{i}$ with replacement, i.e. $\\textstyle{\\frac{1}{n^{2}}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}g(O_{i},O_{j})$ for some function $g$ . Calculation of this estimator and its theoretical properties are as follows. ", "page_idx": 5}, {"type": "text", "text": "Estimation. Using the training partition, estimate the outcome models $\\mu...{\\cal D}.(W,Z)=\\mathbb{E}..{\\cal D}[\\ell|W,Z]$ for $D\\,=\\,0,1$ , the shifted outcome model $\\mu..._{s}(W,Z)\\,=\\,\\mathbb{E}.._{s}[\\ell|W,Z]$ ; and the density ratio models $\\pi_{110}(W,Z)=p_{1}(W,Z)/p_{0}(W,Z)$ and $\\pi({\\cal W},Z_{s},Z_{-s},Q_{\\mathrm{bin}}^{\\ \\cdot\\cdot})=\\bar{p}_{1}(Z_{-s}|{\\cal W},Z_{s},q_{\\mathrm{bin}}^{\\cdot}({\\cal W},Z)=$ $Q_{\\mathtt{b i n}})/p_{1}(Z_{-s})$ . The outcome and density ratio models can be fti using ML-based regression models and probabilistic classifiers [43], respectively (see Section $\\mathrm{H}$ for details). The estimator for $v_{\\mathtt{Y,b i n}}(s)$ is the ratio $\\hat{v}_{\\mathtt{Y,b i n}}(s)=\\hat{v}_{\\mathtt{Y,n}}^{\\mathtt{n u m}}(s)/\\hat{v}_{\\mathtt{Y,n}}^{\\mathtt{d e n}}$ , where the numerator and denominator are estimated using the evaluation partition as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{v}_{\\Upsilon,n}^{\\mathrm{num}}(s)=\\mathbb{P}_{1,n}\\hat{\\xi}_{s}(W,Z)^{2}+2\\,\\mathbb{P}_{1,n}\\hat{\\xi}_{s}(W,Z)(\\ell-\\hat{\\mu}_{\\cdot1}(W,Z))}\\\\ &{\\phantom{\\hat{v}_{\\Upsilon,n}^{\\mathrm{num}}(s)}-2\\,\\mathbb{P}_{1,n}\\tilde{\\mathbb{P}}_{1,n}\\hat{\\xi}_{s}(W,Z_{s},\\tilde{Z}_{-s})\\ell(W,Z_{s},\\tilde{Z}_{-s},Y)\\hat{\\pi}(W,Z_{s},\\tilde{Z}_{-s},Q_{\\mathrm{bin}})}\\\\ &{\\phantom{\\hat{\\xi}_{\\Upsilon,n}^{\\mathrm{num}}(s)}+2\\,\\mathbb{P}_{1,n}\\tilde{\\mathbb{P}}_{1,n}\\hat{\\xi}_{s}(W,Z_{s},\\tilde{Z}_{-s})\\hat{\\mu}_{\\cdot s}(W,Z_{s},\\tilde{Z}_{-s})\\hat{\\pi}(W,Z_{s},\\tilde{Z}_{-s},Q_{\\mathrm{bin}})}\\\\ &{\\hat{v}_{\\Upsilon,n}^{\\mathrm{den}}=\\mathbb{P}_{1,n}\\,(\\hat{\\mu}_{\\cdot1}(W,Z)-\\hat{\\mu}_{\\cdot0}(W,Z))^{2}+2\\,\\mathbb{P}_{1,n}\\,(\\hat{\\mu}_{\\cdot1}(W,Z)-\\hat{\\mu}_{\\cdot0}(W,Z))\\,(\\ell-\\hat{\\mu}_{\\cdot1}(W,Z))}\\\\ &{\\phantom{\\hat{\\xi}_{\\Upsilon,n}^{\\mathrm{num}}(s)}-2\\,\\mathbb{P}_{0,n}\\,(\\hat{\\mu}_{\\cdot1}(W,Z)-\\hat{\\mu}_{\\cdot0}(W,Z))\\,(\\ell-\\hat{\\mu}_{\\cdot0}(W,Z))\\hat{\\pi}_{110}(W,Z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\xi}_{s}(W,Z)=\\hat{\\mu}_{\\cdot1}(W,Z)-\\hat{\\mu}_{\\cdot s}(W,Z)$ . Note that the first terms in (6) and (7) are the plug-in estimates, followed by additional terms that correct its bias. ", "page_idx": 5}, {"type": "text", "text": "Inference. This estimator is asymptotically normal assuming the estimators for the nuisance parameters converge at a fast enough rate, per the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2. Suppose Condition 3.1 holds. For variable subset $s$ , suppose the density ratios $\\pi(W,Z_{s},Z_{-s},Q_{b i n})$ and $\\pi_{110}(W,Z)$ are bounded; denominator in case of no shift $v_{Y}^{d e n}(\\emptyset)>0,$ ; estimator $\\hat{\\pi}$ is consistent; estimators $\\hat{\\mu}...0\\,,\\,\\hat{\\mu}...1$ and $\\hat{\\mu}..._{s}$ converge at an $o_{p}(n^{-1/4})$ rate, and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}_{1}\\big(\\hat{q}_{b\\,i n}-q_{b\\,i n}\\big)^{2}=o_{p}(n^{-1})}&{}\\\\ {\\mathbb{P}_{1}\\big(\\mu_{\\cdot\\cdot s}-\\hat{\\mu}_{\\cdot s}\\big)(\\pi-\\hat{\\pi})=o_{p}(n^{-1/2}),\\quad\\mathbb{P}_{0}\\big(\\mu_{\\cdot\\cdot0}-\\hat{\\mu}_{\\cdot\\cdot0}\\big)(\\pi_{110}-\\hat{\\pi}_{110})=o_{p}(n^{-1/2})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then the estimator $\\hat{v}_{Y,b i n}(s)$ is asymptotically normal centered at the estimand $v_{Y,b i n}(s)$ . ", "page_idx": 5}, {"type": "text", "text": "Note that the product terms in (9) mean that the estimator converges to normal at $n^{-1/2}$ -rate even if one of the nuisance parameters is estimated at a rate slower than $n^{-1/2}$ . Hence, it is multiply-robust to nuisance model misspecification. A convergence rate of $o_{p}(n^{-1/4})$ can be achieved by ML estimators in a wide variety of conditions, and such assumptions are commonly used to construct debiased ML estimators. The additional requirement in (8) that $\\hat{q}_{\\mathrm{bin}}$ converges at a $o_{p}(n^{-1})$ rate is new, but fast or even super-fast convergence rates of binned risks is achievable under suitable margin conditions [2] such as Condition G.7 in the Appendix. ", "page_idx": 5}, {"type": "text", "text": "3.2 Shapley values ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Calculating the exact Shapley value is computationally intractable as it involves an exponential number of terms. However, Williamson and Feng [47] showed that calculating the exact Shapley value is unnecessary for the purposes of statistical inference. Because there is inherent uncertainty in estimates of the value functions $v(s)$ , one only needs to sample and estimate the values for enough variable subsets such that the uncertainty due to estimation dominates that due to subset sampling. This leads to a drastic reduction in computation time: Williamson and Feng [47] proves that the number of subsets one needs to sample only needs to be linear or super-linear in the total number of observations $n$ . Using this result, Algorithm 4 outlines a computationally efficient procedure for estimation and inference of the detailed decomposition. ", "page_idx": 5}, {"type": "image", "img_path": "nXXwYsARXB/tmp/be1fc496b07f9e65cf01585839a746973dc3bdf9640a15675f892e270bfe0dc2.jpg", "img_caption": ["Figure 2: (a) Coverage rates of $90\\%$ CIs for value of $s$ -partial shifts for the conditional covariate (first column) and outcome shifts (second column) across dataset sizes $n$ . Dashed horizontal line indicates $90\\%$ coverage rate. (b) Comparison of variable importance reported by proposed method HDPD (debiased) versus existing methods for conditional covariate and outcome shift terms. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 Simulation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now present simulations to show that the proposed procedure achieves the desired coverage rates (Section 4.1) and illustrate how the HDPD framework provides more intuitive explanations of performance gaps (Section 4.2). In all empirical analyses, performance of the ML algorithm is quantified in terms of 0-1 accuracy. Below, we briefly describe the simulation settings; full details are provided in Section I in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "4.1 Verifying theoretical properties ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first verify that the inference procedures for the decomposition terms have CIs with coverage close to their nominal rate. We check the coverage of the aggregate decomposition as well as the value of $s$ -partial conditional covariate and partial conditional outcome shifts for $s=\\{Z_{1}\\},\\{Z_{2}\\},\\{Z_{3}\\}$ . $(W,Z_{1},Z_{2},Z_{3})$ are sampled from independent normal distributions with different means in source and target, while $Y$ is simulated from logistic regression models with different coefficients. CIs for the debiased ML estimator converge to the nominal $90\\%$ coverage rate with increasing sample size, whereas those for the na\u00efve plug-in estimator do not (Fig 2a and Fig 6). ", "page_idx": 6}, {"type": "text", "text": "4.2 Comparing explanations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now compare the proposed definitions for the detailed decomposition with existing methods. For the detailed decomposition due to conditional covariate shift, the comparators are: ", "page_idx": 6}, {"type": "text", "text": "\u2022 MeanChange Tests for a difference in means for each feature. Defines importance as $1\\mathrm{-}\\;\\mathfrak{p}.$ -value.   \n\u2022 Oaxaca-Blinder: Fits a linear model of the logit-transformed expected loss with respect to $Z$ in the source domain. Defines importance of $Z_{i}$ as its coefficient multiplied by the difference in the means of $Z_{i}$ [32, 3].   \n\u2022 WuShift [48]: Defines importance of subset $s$ as change in overall performance due to $s$ -partial conditional covariate shifts. Applies Shapley framework to obtain VIs. ", "page_idx": 6}, {"type": "image", "img_path": "nXXwYsARXB/tmp/5f970f0521da2ca12b366cebd574041192e978188bf8e13d96e30d8999fd446b.jpg", "img_caption": ["Figure 3: Aggregate and detailed decompositions for performance gaps of (a) a model predicting readmission risk across patient populations and (b) a model predicting insurance coverage across US states. A subset of VI estimates is shown; see full list in Section J in the Appendix. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "For the detailed decomposition due to conditional outcome shifts, we compare against: ", "page_idx": 7}, {"type": "text", "text": "\u2022 ParametricChange: Fits a logistic model for $Y$ with interaction terms between domain and $Z$ . Defines importance of $Z_{i}$ as the coefficient of its interaction term.   \n\u2022 ParametricAcc: Same as ParametricChange but models the 0-1 loss rather than $Y$ .   \n\u2022 RandomForestAcc: Compares VI of random forest models trained on data from both domains with input features $D,Z$ , and $W$ to predict the 0-1 loss.   \n\u2022 Oaxaca-Blinder: Fits linear models for the logit-transformed expected loss in each domain. Defines importance of $Z_{i}$ as its mean in the target domain multiplied by the difference in its coefficients across domains. ", "page_idx": 7}, {"type": "text", "text": "Although the proposed method may agree with these other methods on the top features in certain data settings, we highlight important situations where the methods differ. ", "page_idx": 7}, {"type": "text", "text": "Conditional covariate. (Fig 2b(i)) We simulate $(W,Z_{1})$ from a standard normal distribution, $Z_{2}$ from a mixture of two Gaussians whose means depend on the value of $Z_{1}$ (i.e. $Z_{1}\\to Z_{2}$ ), and $Y$ from a logistic regression model depending on $(W,Z_{1},Z_{2})$ . We induce a shift from the source domain to the target domain by shifting only the distribution of $Z_{1}$ , so that $p_{1}(Z|W)\\,=$ $p_{0}(Z_{2}|Z_{1},W)p_{1}(Z_{1}|W)$ . Only the proposed estimator correctly recovers that $Z_{1}$ is more important than $Z_{2}$ , as the $\\{1\\}$ -partial conditional covariate shift explains all the variation in performance gaps across strata $W$ (i.e. the corresponding $R^{2}$ -based value function $v_{\\mathrm{z}}(\\{1\\})$ is equal to 1). The other methods incorrectly assign higher importance to $Z_{2}$ . MeanChange only measures shifts but not loss due to shifts, Oaxaca-Blinder uses a misspecified linear model, and WuShift estimates the performance change due to hypothesized $s$ -partial shifts but does not check if the partial shifts are good explanations in the first place. ", "page_idx": 7}, {"type": "text", "text": "Conditional outcome. (Fig 2b(ii)) $W$ and $Z\\in\\mathbb{R}^{4}$ are simulated from the same distribution in both domains. $Y$ is generated from a logistic regression model with coefficients for $(W,Z_{1},\\cdot\\cdot\\cdot\\,,Z_{4})$ as $(0.5,0.5,1,0.3,0.3)$ in the source and $(0.5,0.3,1,1.3,-0.1)$ in the target. Interestingly, none of the methods have the same ranking of the features. ParametricChange identifies $Z_{1}$ as having the largest shift on the logit scale, but this does not mean that it is the most important explanation for changes in the loss. According to our decomposition framework, $Z_{3}$ is actually the most important for explaining changes in model performance due to outcome shifts. Oaxaca-Blinder, ParametricAcc, and RandomForestAcc have odd behavior. Oaxaca-Blinder assigns $Z_{3}$ second to the lowest importance and ParametricAcc assigns $Z_{2}$ the highest importance), likely because they misspecify the outcome models. RandomForestAcc likely ranks $Z_{2}$ highly because its VI values quantify which variables are good predictors of performance, not performance shift. ", "page_idx": 7}, {"type": "text", "text": "A more objective evaluation is to compare the performance of fixes based on the different explanations. To this end, we re-fit the ML algorithm in the target domain with respect to input features $Q,W$ , and the top variables $Z_{s}$ from each explanation. We find that model revisions based on the proposed method achieve the highest performance gain (Table 3 in Appendix). ", "page_idx": 7}, {"type": "text", "text": "5 Real-world data case studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now demonstrate applicability of the framework on two datasets with naturally-occurring shifts. ", "page_idx": 8}, {"type": "text", "text": "Hospital readmission. Using electronic health record data from the Zuckerberg San Francisco General Hospital, we analyzed performance of a Gradient Boosted Tree (GBT) trained on the general patient population (source) to predict 30-day readmission risk but applied to patients diagnosed with heart failure (HF, target). Features include 4 demographic variables $(W)$ and 16 diagnosis codes $(Z)$ . Each domain supplied $n=3750$ observations from which we keep $20\\%$ in the evaluation partition. ", "page_idx": 8}, {"type": "text", "text": "Model accuracy drops from $70\\%$ to $53\\%$ in HF population. From the aggregate decompositions (Fig 3a), we observe that the drop is mainly due to covariate shift. If one performed the standard check to see which variables significantly changed in their mean value (MeanChange), then one would find a significant shift in nearly every variable. Little support is offered to identify main drivers of the performance drop. In contrast, the detailed decomposition from the proposed framework estimates diagnoses \u201cDrug-induced or toxic-related condition\u201d and \u201cMental & substance use disorder in remission\u201d as having the highest estimated contributions to the conditional covariate shift, and most other variables having little to no contribution. Upon discussion with clinicians from this hospital, differences in the top two diagnoses may be explained by (i) substance use being a major cause of HF at this hospital, with over eighty percent of its HF patients reporting current or prior substance use, and (ii) substance use and mental health disorders often occurring simultaneously in this HF patient population. Based on these findings, closing the performance gap may require a mixture of both operational (e.g. care programs centered around substance use) and algorithmic interventions (e.g. reweighting data with respect to the top two features). Finally, CIs from the debiased ML procedure provide valuable information on the uncertainty of the estimates and highlight, for instance, that more data is necessary to determine the true ordering between the top two features. In contrast, existing methods do not provide (asymptotically valid) CIs. ", "page_idx": 8}, {"type": "text", "text": "ACS Public Coverage. We analyze a neural network trained to predict whether a person has public health insurance using data from Nebraska in the American Community Survey (source, $n=3000)$ ), applied to data from Louisiana (target, $n=6000$ ). Baseline variables include 3 demographics (sex, age, race), and covariates $Z$ include 31 variables related to health conditions, employment, marital status, citizenship status, and education. ", "page_idx": 8}, {"type": "text", "text": "Model accuracy drops from $84\\%$ to $66\\%$ across the two states. The main driver is the shift in the outcome distribution per the aggregate decomposition (Fig 3b) and the most important contributor to the outcome shift is annual income, perhaps due to differences in cost of living across the two states. Income is significantly more important than all the other variables; the ranking between the remaining variables is unclear. In comparing the performance of targeted model revisions, we find that revising the model based on top variables identified by the proposed procedure leads to AUCs that are better or as good as those based on RandomForestAcc (Table 4 in the Appendix). ", "page_idx": 8}, {"type": "text", "text": "6 Prior work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Describing distribution shifts. This line of work focuses on detecting and localizing which distributions shift between datasets [29, 39]. Budhathoki et al. [4] identify the main variables contributing to a distribution shift via a Shapley framework, Kulinski and Inouye [28] fits interpretable optimal transport maps, and Liu et al. [30] finds the region with the largest shift in the conditional outcome distribution. However, these works do not quantify how these shifts contribute to changes in performance, the metric of practical importance. ", "page_idx": 8}, {"type": "text", "text": "Explaining loss differences across subpopulations. Understanding differences in model performance across subpopulations in a single dataset is similar to understanding differences in model performance across datasets, but the focus is typically to find subpopulations with poor performance rather than to explain how distribution shifts contributed to the performance change. Existing approaches include slice discovery methods [35, 22, 10, 13] and structured representations of the subpopulation using e.g. Euclidean balls [1]. ", "page_idx": 8}, {"type": "text", "text": "Attributing performance changes. Prior works have described similar aggregate decompositions of the performance change into covariate and conditional outcome shift components [5, 36]. To provide more granular explanations of performance shifts, existing works on causal attribution [50, 38] and mediation analysis [44] quantify the importance of shifts in each variable assuming the causal graph is correctly specified; covariate shifts restricted to variable subsets assuming that the partial shifts follow a particular structure [48]; and conditional shifts in each variable assuming a parametric model [11]. However, the strong assumptions made by these methods make them difficult to apply in practice, and model misspecification can lead to unintuitive interpretations. Furthermore, such methods do not provide hierarchical decompositions, i.e. VIs for each type of shift. Decomposition methods such as Oaxaca-Blinder similarly make strong parametric assumptions [32, 3, 16, 14, 49, 15], which is inappropriate for the complex data settings in ML. In addition, there is no unifying nonparametric framework for decomposing both covariate and outcome shifts, and many methods do not output CIs, which is important when the amount of labeled data from a given domain is limited. A summary of how the proposed framework compares against prior works is shown in Table 1. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "ML algorithms regularly encounter distribution shifts in practice, leading to drops in performance. We present a novel framework that helps ML developers and deployment teams build a more nuanced understanding of the shifts. Compared to past work, the approach provides a nonparametric hierarchical framework for decomposing both conditional covariate and outcome shifts, does not require fine-grained knowledge of the causal relationship between variables, and quantifies the uncertainty of the estimates by constructing confidence intervals. We present real-world case studies to demonstrate how this framework can help diagnose performance drops and guide corrective actions. This framework requires overlapping support of the covariates, which may not always be applicable in practice. In such cases, one solution is to restrict to the common support [5]. ", "page_idx": 9}, {"type": "text", "text": "Important extensions of this work include decompositions of more complex measures of model performance such as AUC and analyzing other factorizations of the data distribution (e.g. label/prior shifts [27]). For unstructured data (e.g. image and text), the current framework can be applied to low-dimensional embeddings or by extracting interpretable concepts [26]; more work is needed to directly analyze unstructured data. Finally, while the focus of this work is to interpret performance gaps, future work may take this work one step further to design optimal interventions for closing the performance gap. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank Lucas Zier for supplying the dataset from the Zuckerberg San Francisco General Hospital and providing their clinical expertise to interpret results. We are grateful to Nicholas Petrick, Berkman Sahiner, Gene Pennello, Mi-Ok Kim, Romain Pirracchio, Julian Hong, Avni Kothari, and the anonymous reviewers for helpful feedback. This work was funded through a Patient-Centered Outcomes Research Institute $^\\mathrm{\\textregistered}$ (PCORI $\\textsuperscript{\\textregistered}$ ) Award (ME-2022C1-25619). The views presented in this work are solely the responsibility of the author(s) and do not necessarily represent the views of the $\\mathrm{PCORI@}$ , its Board of Governors or Methodology Committee. The contents are those of the author(s) and do not necessarily represent the official views of, nor an endorsement, by FDA/HHS, or the U.S. Government. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Alnur Ali, Maxime Cauchois, and John C. Duchi. The lifecycle of a statistical model: Model failure detection, identification, and refitting, 2022. URL https://arxiv.org/abs/2202. 04166.   \n[2] Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. Ann. Stat., 35(2):608\u2013633, April 2007.   \n[3] Alan S. Blinder. Wage discrimination: Reduced form and structural estimates. The Journal of Human Resources, 8(4):436\u2013455, 1973. ISSN 0022166X. URL http://www.jstor.org/ stable/144855.   \n[4] Kailash Budhathoki, Dominik Janzing, Patrick Bloebaum, and Hoiyi Ng. Why did the distribution change? In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 1666\u20131674. PMLR, 13\u201315 Apr 2021. URL https://proceedings.mlr.press/v130/budhathoki21a.html.   \n[5] Tiffany Tianhui Cai, Hongseok Namkoong, and Steve Yadlowsky. Diagnosing model performance under distribution shift. March 2023. URL http://arxiv.org/abs/2303.02011. [6] A Charnes, B Golany, M Keane, and J Rousseau. Extremal principle solutions of games in characteristic function form: Core, chebychev and shapley value generalizations. In Jati K Sengupta and Gopal K Kadekodi, editors, Econometrics of Planning and Efficiency, pages 123\u2013133. Springer Netherlands, Dordrecht, 1988.   \n[7] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. Econom. J., 21(1):C1\u2013C68, February 2018. [8] Juan Correa and Elias Bareinboim. A calculus for stochastic Interventions:Causal effect identification and surrogate experiments. AAAI, 34(06):10093\u201310100, April 2020. URL https://ojs.aaai.org/index.php/AAAI/article/view/6567. [9] A P Dawid. Influence diagrams for causal modelling and inference. Int. Stat. Rev., 70 (2):161\u2013189, August 2002. URL https://onlinelibrary.wiley.com/doi/10.1111/ j.1751-5823.2002.tb00354.x.   \n[10] Greg d\u2019Eon, Jason d\u2019Eon, James R. Wright, and Kevin Leyton-Brown. The spotlight: A general method for discovering systematic errors in deep learning models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201922, page 1962\u20131981, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393522. doi: 10.1145/3531146.3533240. URL https://doi.org/10.1145/3531146.3533240.   \n[11] Lori E Dodd and Margaret Sullivan Pepe. Semiparametric regression for the area under the receiver operating characteristic curve. J. Am. Stat. Assoc., 98(462):409\u2013417, 2003.   \n[12] Christopher Duckworth, Francis P Chmiel, Dan K Burns, Zlatko D Zlatev, Neil M White, Thomas W V Daniels, Michael Kiuber, and Michael J Boniface. Using explainable machine learning to characterise data drift and detect emergent health risks for emergency department admissions during COVID-19. Sci. Rep., 11(1):23017, November 2021.   \n[13] Sabri Eyuboglu, Maya Varma, Khaled Kamal Saab, Jean-Benoit Delbrouck, Christopher LeeMesser, Jared Dunnmon, James Zou, and Christopher Re. Domino: Discovering systematic errors with cross-modal embeddings. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $\\equiv$ FPCMqjI0jXN.   \n[14] Robert W Fairlie. An extension of the blinder-oaxaca decomposition technique to logit and probit models. Journal of economic and social measurement, 30(4):305\u2013316, 2005.   \n[15] Sergio P. Firpo, Nicole M. Fortin, and Thomas Lemieux. Decomposing wage distributions using recentered influence function regressions. Econometrics, 6(2), 2018. ISSN 2225-1146. doi: 10.3390/econometrics6020028. URL https://www.mdpi.com/2225-1146/6/2/28.   \n[16] Nicole Fortin, Thomas Lemieux, and Sergio Firpo. Chapter 1 - decomposition methods in economics. volume 4 of Handbook of Labor Economics, pages 1\u2013102. Elsevier, 2011. doi: https://doi.org/10.1016/S0169-7218(11)00407-2. URL https://www.sciencedirect.com/ science/article/pii/S0169721811004072.   \n[17] Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2242\u20132251. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr. press/v97/ghorbani19c.html.   \n[18] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359\u2013378, 2007. doi: 10.1198/ 016214506000001437. URL https://doi.org/10.1198/016214506000001437.   \n[19] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. International Conference on Machine Learning, 70:1321\u20131330, 2017.   \n[20] Oliver Hines, Karla Diaz-Ordaz, and Stijn Vansteelandt. Variable importance measures for heterogeneous causal effects. April 2022.   \n[21] John W Jackson. Meaningful causal decompositions in health equity research: definition, identification, and estimation through a weighting framework. Epidemiology, 32(2):282\u2013290, 2021.   \n[22] Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry. Distilling model failures as directions in latent space. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=99RpBVpLiX.   \n[23] Yonghan Jung, Shiva Kasiviswanathan, Jin Tian, Dominik Janzing, Patrick Bloebaum, and Elias Bareinboim. On measuring causal contributions via do-interventions. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 10476\u201310501. PMLR, 17\u201323 Jul 2022. URL https: //proceedings.mlr.press/v162/jung22a.html.   \n[24] Joseph D. Y. Kang and Joseph L. Schafer. Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data. Statistical Science, 22(4):523 \u2013 539, 2007. doi: 10.1214/07-STS227. URL https://doi.org/10.1214/ 07-STS227.   \n[25] Edward H Kennedy. Semiparametric doubly robust targeted double machine learning: a review. March 2022.   \n[26] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory sayres. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2668\u20132677. PMLR, 10\u201315 Jul 2018. URL https://proceedings. mlr.press/v80/kim18d.html.   \n[27] Wouter M. Kouw and Marco Loog. An introduction to domain adaptation and transfer learning, 2019.   \n[28] Sean Kulinski and David I. Inouye. Towards explaining distribution shifts. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 17931\u201317952. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/kulinski23a.html.   \n[29] Sean Kulinski, Saurabh Bagchi, and David I Inouye. Feature shift detection: Localizing which features have shifted via conditional distribution tests. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 19523\u201319533. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/e2d52448d36918c575fa79d88647ba66-Paper.pdf.   \n[30] Jiashuo Liu, Tianyu Wang, Peng Cui, and Hongseok Namkoong. On the need for a language describing distribution shifts: Illustrations on tabular datasets. July 2023.   \n[31] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 8a20a8621978632d76c43dfd28b67767-Paper.pdf.   \n[32] Ronald Oaxaca. Male-female wage differentials in urban labor markets. International Economic Review, 14(3):693\u2013709, 1973. ISSN 00206598, 14682354. URL http://www.jstor.org/ stable/2525981.   \n[33] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \n[34] John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61\u201374, 1999.   \n[35] Gregory Plumb, Nari Johnson, Angel Cabrera, and Ameet Talwalkar. Towards a more rigorous science of blindspot discovery in image classification models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id $=$ MaDvbLaBiF. Expert Certification.   \n[36] Hongxiang Qiu, Eric Tchetgen Tchetgen, and Edgar Dobriban. Efficient and multiply robust risk estimation under general forms of dataset shift. June 2023.   \n[37] Joaquin Qui\u00f1onero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. Mit Press, 2022.   \n[38] Victor Quintas-Martinez, Mohammad Taha Bahadori, Eduardo Santiago, Jeff Mu, Dominik Janzing, and David Heckerman. Multiply-robust causal change attribution, 2024.   \n[39] Stephan Rabanser, Stephan G\u00fcnnemann, and Zachary Lipton. Failing loudly: An empirical study of methods for detecting dataset shift. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper/2019/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf.   \n[40] James M Robins, Thomas S Richardson, and Ilya Shpitser. An interventionist approach to mediation analysis. In Probabilistic and Causal Inference: The Works of Judea Pearl. Association for Computing Machinery, August 2020. URL http://arxiv.org/abs/2008. 06019.   \n[41] L S Shapley. 17. a value for n-person games. In Harold William Kuhn and Albert William Tucker, editors, Contributions to the Theory of Games (AM-28), Volume II, pages 307\u2013318. Princeton University Press, Princeton, December 1953.   \n[42] Ewout W Steyerberg. Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating. Springer, New York, NY, 2009.   \n[43] Masash Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul Buenau, and Motoaki Kawanabe. Direct importance estimation with model selection and its application to covariate shift adaptation. Adv. Neural Inf. Process. Syst., 2007.   \n[44] Eric J. Tchetgen Tchetgen and Ilya Shpitser. Semiparametric theory for causal mediation analysis: Efficiency bounds, multiple robustness and sensitivity analysis. The Annals of Statistics, 40(3):1816 \u2013 1845, 2012. doi: 10.1214/12-AOS990. URL https://doi.org/10. 1214/12-AOS990.   \n[45] Anastasios A Tsiatis. Semiparametric Theory and Missing Data. Springer New York, 2006.   \n[46] A W van der Vaart. Asymptotic Statistics. Cambridge University Press, October 1998.   \n[47] Brian D Williamson and Jean Feng. Efficient nonparametric statistical inference on population feature importance using shapley values. International Conference on Machine Learning, 2020. URL https://proceedings.icml.cc/static/paper_files/icml/2020/3042-Paper. pdf.   \n[48] Eric Wu, Kevin Wu, and James Zou. Explaining medical AI performance disparities across sites with confounder shapley value analysis. November 2021. URL http://arxiv.org/ abs/2111.08168.   \n[49] Myeong-Su Yun. Decomposing differences in the first moment. Economics Letters, 82(2): 275\u2013280, 2004. ISSN 0165-1765. doi: https://doi.org/10.1016/j.econlet.2003.09.008. URL https://www.sciencedirect.com/science/article/pii/S0165176503002866. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[50] Haoran Zhang, Harvineet Singh, Marzyeh Ghassemi, and Shalmali Joshi. \"Why did the model fail?\": Attributing model performance changes to distribution shifts. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 41550\u201341578. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/zhang23ai.html. ", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents of the Appendix are as follows. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Table 2 collects all the notation used for reference.   \n\u2022 Section B discusses broader impacts of the work.   \n\u2022 Algorithms 1 and 4 provide the steps required for computing the aggregate and detailed decomposition respectively. Detailed decompositions require computing the value of $s$ -partial conditional outcome and conditional covariate shifts which is described in Algorithms 2 and 3.   \n\u2022 Section C discusses considerations for choosing baseline variables $W$ and conditional covariates $Z$ .   \n\u2022 Section D describes a causal interpretation of the aggregate and detailed decompositions as effects of stochastic (or shift) interventions on a structural causal model.   \n\u2022 Section E explain why value functions in prior work give unintuitive attribution and that the $R^{2}$ -based value functions are proper scoring rules.   \n\u2022 Section F describes the estimation and inference for aggregate decomposition and detailed decomposition of conditional covariate shift.   \n\u2022 Section G provides the derivations of the results.   \n\u2022 Sections H and I describe the implementation and simulation details.   \n\u2022 Section J provides additional details on the two real world datasets and results. ", "page_idx": 14}, {"type": "text", "text": "B Broader impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This work presents a method for understanding failures of ML algorithms when they are deployed in settings or populations different from the ones in development datasets. Therefore, the work can be used to suggest ways of improving the algorithms or mitigating their harms. The method is generally applicable to tabular data settings for any classification algorithm, hence, it can potentially be applied across multiple domains where ML is used including medicine, finance, and online commerce. ", "page_idx": 14}, {"type": "text", "text": "Care must be taken while interpreting the results. As usual, assumptions underlying the decompositions such as the coarse causal ordering between the variables $W,Z$ , and $Y$ should be validated through domain knowledge. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 Aggregate decompositions into baseline, conditional covariate, and conditional outcome shifts Input: Source and target data $\\{(W_{i}^{(d)},Z_{i}^{(d)},Y_{i}^{(d)})\\}_{i=1}^{n_{d}}$ for $d\\in\\{0,1\\}$ , loss function $\\ell(W,Z,Y;f)$ . Output: Performance change due to baseline, conditional covariate, and conditional outcome shifts $\\Lambda_{\\mathsf{W}},\\Lambda_{\\mathsf{Z}},\\Lambda_{\\mathsf{Y}}$ . 1 Split source and target data into training $\\mathbb{T}\\mathbf{r}$ and evaluation $\\mathtt{E v}$ partitions. Let $n^{\\mathtt{E v}}$ be the total number of data points in the Ev partition. 2 Fit nuisance parameters $\\eta_{\\mathrm{w}},\\eta_{\\mathrm{z}},\\eta_{\\mathrm{Y}}$ , defined in Section G.1, on the $\\mathbb{T}\\mathbf{r}$ partition as outlined in Section H.1. 3 Estimate $\\Lambda_{\\mathsf{W}},\\Lambda_{\\mathsf{Z}},\\Lambda_{\\mathsf{Y}}$ using ftited nuisance parameters on the Ev partitions following the equations in Section F.1. 4 Estimate variance of influence functions $\\psi_{\\mathsf{M}}(d,w,z,y;\\hat{\\eta}_{\\mathsf{N}}),\\psi_{\\mathsf{Z}}(d,w,z,y;\\hat{\\eta}_{\\mathsf{N}})$ , and $\\psi_{\\tt Y}(d,w,z,y;\\hat{\\eta}_{\\tt N})$ as defined in (23), (24), and (25), respectively. 5 Compute $\\alpha$ -level confidence intervals as $\\hat{\\Lambda}_{\\mathtt{N}}\\!\\pm z_{1-\\alpha/2}\\sqrt{\\widehat{v a r}(\\psi_{\\mathtt{N}}(d,w,z,y;\\hat{\\eta}_{\\mathtt{N}}))/n^{\\mathtt{E v}}}$ for $\\tt N\\in\\{W,Z,Y\\}$ , where $z$ is the inverse CDF of the standard normal distr ibution. 6 return $\\hat{\\Lambda}_{\\boldsymbol{W}},\\hat{\\Lambda}_{\\boldsymbol{Z}},\\hat{\\Lambda}_{\\boldsymbol{Y}}$ and confidence intervals ", "page_idx": 14}, {"type": "table", "img_path": "nXXwYsARXB/tmp/2e5771f220807b6b890bcaf6dc139b5ac51ecf2b9cf86a7fcd281ca2813d165a.jpg", "table_caption": ["Table 2: Notation "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Algorithm 2 VALUECONDITIONALOUTCOME(S): Value for $s$ -partial conditional outcome shift for a subset s Input: Training Tr and evaluation Ev partitions of source and target data, subset of variables $s$ . Output: Value for $s$ -partial conditional outcome shift for subset $s$ . 1 Fit nuisance parameters $\\eta_{\\mathtt{Y},s}^{\\mathtt{n u m}},\\eta_{\\mathtt{Y}}^{\\mathtt{d e n}}$ , defined in Sections G.3, on the $\\mathbb{T}\\mathbf{r}$ partitions as outlined in H.2. 2 Estimate $v_{\\mathtt{Y}}(s)$ by $\\hat{v}_{\\tt Y}^{\\tt n u m}(s)/\\hat{v}_{\\tt Y}^{\\tt d e n}$ where $\\hat{v}_{\\mathtt{Y}}^{\\mathtt{n u m}}(s)$ is estimated using (6) and $\\hat{v}_{\\Upsilon}^{\\sf d e n}$ is estimated using (7) on the Ev partition. 3 Estimate variance of influence function $\\psi_{\\mathtt{Y,b i n},s}(d,w,z,y;\\hat{\\eta}_{\\mathtt{Y,s}}^{\\mathtt{n u m}},\\hat{\\eta}_{\\mathtt{Y}}^{\\mathtt{d e n}})$ as defined in (74). 4 Compute $\\alpha$ -level confidence interval as $\\hat{v}_{\\Upsilon}(s)\\pm z_{1-\\alpha/2}\\sqrt{\\widehat{v a r}(\\psi_{\\Upsilon,\\mathrm{bin},s}(d,w,z,y;\\hat{\\eta}_{\\Upsilon,s}^{\\mathrm{num}},\\hat{\\eta}_{\\Upsilon}^{\\mathrm{den}}))/n^{\\mathrm{Ev}}}.$ 5 return $\\hat{v}_{Y}(s)$ and confidence interval ", "page_idx": 15}, {"type": "text", "text": "Algorithm 3 VALUECONDITIONALCOVARIATE(S): Value for $s$ -partial conditional covariate shift for a subset $s$ ", "page_idx": 15}, {"type": "text", "text": "Input: Training Tr and evaluation Ev partitions of source and target data, subset of variables $s$ . Output: Value for $s$ -partial conditional covariate shift for subset $s$ . 1 Fit nuisance parameters $\\eta_{\\mathrm{z},s}^{\\mathrm{num}}$ , defined in Sections G.2, on the $\\mathbb{T}\\mathbf{r}$ partition, as outlined in H.3. 2 Estimate $v_{\\mathrm{Z}}(s)$ by $\\hat{v}_{\\mathrm{Z}}^{\\mathrm{num}}(s)/\\hat{v}_{\\mathrm{Z}}^{\\mathrm{num}}(\\emptyset)$ using (21) on the Ev partition. 3 Estimate variance of influence function $\\psi_{\\sf z},s(d,w,z,y;\\hat{\\eta}_{\\sf z}^{\\mathrm{num}})$ as defined in (41). 4 Compute $\\alpha$ -level confidence interval as $\\hat{v}_{\\mathrm{Z}}(s)\\pm z_{1-\\alpha/2}\\sqrt{\\widehat{v a r}(\\psi_{\\mathrm{Z},s}(d,w,z,y;\\hat{\\eta}_{\\mathrm{Z},s}^{\\mathrm{num}}))/n^{\\mathrm{Ev}}}.$ 5 return $\\hat{v}_{Z}(s)$ and confidence interval ", "page_idx": 15}, {"type": "text", "text": "Algorithm 4 Detailed decomposition for conditional outcome and covariate shift Input: Source and target data $\\{(W_{i}^{(d)},Z_{i}^{(d)},Y_{i}^{(d)})\\}_{i=1}^{n_{d}}$ for $d\\in\\{0,1\\}$ , loss function $\\ell(W,Z,Y;f)$ , Output: Detailed decomposition for conditional outcome or covariate shift, $\\{\\phi_{\\Upsilon,j}:j=0,\\cdot\\cdot\\cdot,m_{2}\\}$ or $\\{\\phi_{2,j}:j=1,\\cdot\\cdot\\cdot,m_{2}\\}$ .   \n1 Split source and target data into training $\\mathbb{T}\\mathbf{r}$ and evaluation $\\mathtt{E v}$ partitions. Let $n^{\\mathtt{E v}}$ be the total number of data points in the Ev partition.   \n2 Subsample $\\lfloor\\gamma n^{\\tt E v}\\rfloor$ subsets from $\\mathcal{Z}=\\{1,\\cdot\\cdot\\cdot,m_{2}\\}$ with respect to Shapley weights, including $\\varnothing$ and Z, denoted s1, \u00b7 \u00b7 \u00b7 , sk.   \n3 Estimate $v_{\\mathtt{Y}}(s)\\gets$ VALUECONDITIONALOUTCOME(S) and $v_{2}(s)\\gets$ VALUECONDITIONALCOVARIATE(S) for $s\\in s_{1},\\cdots,s_{k}$ .   \n4 Get estimated Shapley values $\\{\\phi_{\\Upsilon,j}\\}$ and $\\{\\phi_{2,j}\\}$ by solving constrained linear regression problems in (7) in Williamson and Feng [47] with value functions $v_{\\Upsilon}(s)$ and $v_{\\mathrm{Z}}(s)$ , respectively.   \n5 Compute confidence intervals based on the influence functions defined in Theorem 1 in Williamson and Feng [47]. ", "page_idx": 16}, {"type": "text", "text": "6 return Shapley values $\\{\\phi_{Y,j}:j=0,\\ldots,m_{2}\\}$ and $\\{\\phi_{Z,j}:j=1,\\ldots,m_{2}\\}$ and confidence intervals ", "page_idx": 16}, {"type": "text", "text": "C Considerations for the choice of $W$ and $Z$ variables ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We suppose that variables $X$ are partitioned into baseline variables $W$ and conditional covariates $Z$ . We suggest selecting $Z$ to be the variables that may act as mediators of the environment shift and/or variables whose associations with $Y$ are likely to be modified by the environment shift (i.e. effect modification). This selection can be chosen based on a high-level causal graph, where $W$ are variables known to be upstream of $Z$ . For instance, if $Z$ are treatment variables and $W$ are baseline variables, one can interpret a covariate shifts as a change in the treatment policy and an outcome shift as a change in the treatment effect across the two environments. ", "page_idx": 16}, {"type": "text", "text": "In the absence of any prior knowledge, another option is to choose $W$ as the variables for which one would like the expected loss given $W$ to be invariant across the two environments; this can be useful to promote fairness of ML algorithms across environments. When this invariance does not hold, the proposed framework explains how variables $Z$ contribute to these differences, which can inform efforts to eliminate performance gaps. This last option is similar to how variables are typically chosen in disparity analyses [21]. For instance, to understand why income differs between males and females controlling for age, one would set domain to $D=$ gender, $W=\\mathrm{age}$ , and $Z$ as variables that may explain this disparity (e.g. marital status, employment status). ", "page_idx": 16}, {"type": "text", "text": "In general, including more variables in $W$ and $Z$ to explain the performance difference is preferable. Nevertheless, there are tradeoffs. For instance, including more variables in $W$ leads to higher variance of $\\Delta._{10}(W)$ , so it allows one to better distinguish the relative importance of variables in $Z$ for explaining its variability. On the other hand, when more variables are assigned to $W$ , the performance gap with respect to $W\\left(\\Delta._{10}(W)\\right)$ is a more complex function. Thus we may have more uncertainty in our estimate of $\\Delta._{10}(W)$ , which may lead to wider confidence intervals for the variable importance values. ", "page_idx": 16}, {"type": "text", "text": "D Causal interpretation of aggregate and detailed decompositions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Partial distribution shifts that we define in the framework can be equivalently described as stochastic (or shift) interventions in a structural causal model (SCM) respecting a causal directed acyclic graph (DAG) [8]. To represent an intervention on variable $X$ , we use regime indicator $\\sigma_{X}$ which means that the conditional probability distribution for $X$ in the SCM has been updated to a new one [9]. ", "page_idx": 16}, {"type": "text", "text": "Suppose that the source and target data $(W,Z,Y)$ are generated by SCMs respecting the same DAG $\\mathcal{G}$ in Figure 4 with no unmeasured confounders. Intervening on a variable in source SCM sets its ", "page_idx": 16}, {"type": "image", "img_path": "nXXwYsARXB/tmp/53a57512f6add192b426007b291d593ddc1420def5bb0d8ad958940ea9327f1e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 4: Decomposition framework for explaining the performance gap from source to target domain, visualized through causal directed acyclic graphs. Aggregate decompositions describe the incremental effect of stochastic interventions on each aggregate variable\u2019s distribution at the source with that in the target, indicated by regime indicators $\\sigma_{\\mathsf{W}},\\sigma_{\\mathsf{Z}}$ , and $\\sigma_{\\Upsilon}$ . Detailed decompositions quantify how well candidate partial distribution shifts explain the variability of performance gaps across strata. The candidate partial shifts considered in this work are shown in the DAGs on the right. An $s$ -partial conditional covariate shift is a stochastic intervention on variable subset $Z_{s}$ . An $s$ -partial conditional outcome shift is a stochastic intervention on variable $Y$ , in which the conditional outcome distribution fine-tunes the risk in the source domain (indicated by the additional node $Q=p_{0}(Y=1|W,Z))$ with respect to $Z_{s}$ . ", "page_idx": 17}, {"type": "text", "text": "conditional distribution to the corresponding one in the target SCM. Under the assumption of no unmeasured confounders, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{0}(V;\\sigma_{X})=\\prod_{V_{i}\\in V\\backslash X}p_{0}(V_{i}|p a_{\\mathcal{G}}(V_{i});\\sigma_{X})\\prod_{V_{i}\\in X}p_{1}(V_{i}|p a_{\\mathcal{G}}(V_{i}))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any variable set $V$ where $p a_{\\mathcal{G}}$ denotes parents in $\\mathcal{G}$ . Therefore, the target data is obtained by intervening on all the variables $\\bar{p_{0}(W,Z,Y;\\sigma_{\\mathsf{W}},\\sigma_{\\mathsf{Z}},\\sigma_{\\mathsf{Y}})}=p_{1}(W,Z,Y)$ . Expectation $\\mathbb{E}$ is taken over the source distribution or its shifted version based on the specified regime indicator. ", "page_idx": 17}, {"type": "text", "text": "Aggregate decompositions can then be written as causal effect of intervening on $W,Z,Y$ incrementally as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Lambda_{\\mathbb{V}}=\\mathbb{E}[\\ell|\\sigma_{\\mathbb{V}}]-\\mathbb{E}[\\ell]}\\\\ &{\\Lambda_{2}=\\mathbb{E}[\\ell|\\sigma_{\\mathbb{V}},\\sigma_{2}]-\\mathbb{E}[\\ell|\\sigma_{\\mathbb{V}}]}\\\\ &{\\Lambda_{\\mathbb{Y}}=\\mathbb{E}[\\ell|\\sigma_{\\mathbb{V}},\\sigma_{2},\\sigma_{\\mathbb{Y}}]-\\mathbb{E}[\\ell|\\sigma_{\\mathbb{V}},\\sigma_{\\mathbb{Z}}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Due to the factorization (10), the above are equivalent to aggregate decompositions presented in Section 2. ", "page_idx": 17}, {"type": "text", "text": "For detailed decomposition of the conditional covariate shift, assume the causal DAG in Figure 4 with $Z_{s}\\,\\rightarrow\\,Z_{-s}$ for variable subset $s$ . The $s$ -partial conditional covariate shift is represented by $p_{0}(V;\\sigma_{\\mathrm{Z}_{s}})$ and, under the factorization (10), is equivalent to the one considered in Section 2.2. Thus, the performance difference $\\Delta._{s0}(W)$ can be equivalently written as $\\mathbb{E}[\\ell|W;\\sigma_{\\Sigma_{s}}]-\\mathbb{E}[\\ell|W]$ and $\\Delta._{10}(W)$ as $\\mathbb{E}[\\ell|W;\\sigma_{2}]-\\mathbb{E}[\\ell|W]$ . The value function is then ", "page_idx": 17}, {"type": "equation", "text": "$$\nv_{\\mathrm{Z}}(s):=1-\\frac{\\mathbb{E}\\left[\\left(\\Delta_{\\cdot s0}(W)-\\Delta_{\\cdot10}(W)\\right)^{2};\\sigma_{\\mathsf{W}}\\right]}{\\mathbb{E}\\left[\\Delta_{\\cdot10}^{2}(W);\\sigma_{\\mathsf{W}}\\right]}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The $s$ -partial conditional outcome shift corresponds to a stochastic intervention that changes the distribution of $Y$ to (5). Denoting it as $p_{0}(V;\\sigma_{\\Upsilon,2_{s}})$ , the performance difference $\\Delta.._{s}(W,Z)$ can be equivalently written as $\\mathbb{E}[\\ell|W,Z;\\sigma_{\\Upsilon,\\sf Z_{s}}]-\\mathbb{E}[\\ell|W,Z]$ and $\\Delta.._{1}(W,Z)$ as $\\mathbb{E}[\\ell|W,Z;\\sigma_{\\mathbb{Y}}]-\\mathbb{E}[\\ell|W,Z]$ . The value function (4) is then ", "page_idx": 17}, {"type": "equation", "text": "$$\nv_{\\Upsilon}(s):=1-\\frac{{\\mathbb E}\\left[(\\Delta.._{s}(W,Z)-\\Delta.._{1}(W,Z))^{2}\\,;\\sigma_{\\mathsf{W}},\\sigma_{\\mathsf{Z}}\\right]}{{\\mathbb E}\\left[\\Delta_{\\cdot\\cdot1}^{2}(W,Z);\\sigma_{\\mathsf{W}},\\sigma_{\\mathsf{Z}}\\right]}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "E Proper scoring rules for partial distribution shift ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Prior works have considered scoring rules for partial distribution shifts in terms of their change in the average loss [48]. For instance, for conditional covariate shifts, prior works have considered scoring rules of the form ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{1s0}[\\ell]-\\mathbb{E}_{100}[\\ell]=\\int\\ell(Y,f(Z,W))\\left(p_{s}(z|w)-p_{0}(z|w)\\right)p_{1}(w)d z d w.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "However, in the absence of a detailed causal graph, we show that this can lead to unintuitive attributions. Consider the following counterexample. We drop $W$ from the data example for simplicity of exposition. ", "page_idx": 18}, {"type": "text", "text": "Example E.1. Consider the following data-generating process with random variables $Z_{1}$ and $Z_{2}$ , a real-valued outcome $Y$ , and loss function as squared error $\\ell=(f(Z_{1},Z_{2})-Y)^{2}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{Z_{1}\\sim N\\left(D+1,1\\right)}}\\\\ {{Z_{2}=|Z_{1}|}}\\\\ {{\\epsilon\\sim N(0,1)}}\\\\ {{Y=Z_{1}+Z_{2}+\\epsilon\\mathbb{1}\\{Z_{1}\\leq0\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $D\\,=\\,0$ and $D\\,=\\,1$ correspond to the source and target domains, respectively. Suppose the ML model is the optimal model for minimizing the expected loss in the source domain, i.e. $f(Z_{1},Z_{2})=Z_{1}+Z_{2}$ . Consider the following candidate partial distribution shifts for explaining the performance change, where Option 1 hypothesizes $Z_{1}$ causes $Z_{2}$ and Option 2 hypothesizes $Z_{2}$ causes $Z_{1}$ . ", "page_idx": 18}, {"type": "equation", "text": "$)\\,\\operatorname{For}s=\\{1\\},p_{s}(Z)=p_{1}(Z_{1})p_{0}(Z_{2}|Z_{1})$ ", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(Option 2) For $\\mathbf{\\tau}=\\{2\\},p_{s}(Z)=p_{1}(Z_{2})p_{0}(Z_{1}|Z_{2})$ ", "page_idx": 18}, {"type": "text", "text": "Per the given data-generating process, the partial shift in Option 1 exactly corresponds to the true (aggregate) dataset shift; thus we would expect it to have a higher value than Option 2. Nevertheless, the MSE resulting from Option 2\u2019s shift is 0.5 (the marginal distribution of $Z_{1}$ under Option 2 is symmetric around 0). In contrast, the MSE resulting from Option 1\u2019s shift is the probability that a standard normal variable is less than $^-1$ , which is 0.159. ", "page_idx": 18}, {"type": "text", "text": "In Example E.1, scoring rule (12) assigns a higher value to a partial shift that contradicts the true causal graph than even the true aggregate shift, because the rule assumes the hypothesized dataset shift is true. Consequently, (12) is not a proper scoring rule. In contrast, the proposed value functions described in Section 2.1 are. ", "page_idx": 18}, {"type": "text", "text": "More formally, we extend the traditional definition for a proper scoring rule [18] to the context of explaining performance changes as follows. Let a scoring function $\\chi:O_{0}\\times O_{1}\\times Q$ be defined as a function of source observation $O_{0}\\,=\\,(W_{0},Z_{0},Y_{0})$ , target observation $O_{1}\\,=\\,(W_{1},Z_{1},Y_{1})$ , and candidate shift probability model $Q$ . A scoring rule $\\chi$ is proper with respect to the set of distribution shift models $\\mathcal{F}$ if the following holds: for any true model of the post-shift data distribution $p^{*}(W,Z,Y)\\in\\mathcal{F}$ , the expectation of the scoring function $\\chi(O_{0},O_{1},p^{*})$ with respect to $O_{0}\\sim p_{0}$ and $O_{1}\\sim p^{*}$ is always no smaller than the expectation of $\\chi(O_{0},O_{1},p)$ for any $p\\in\\mathcal{F}$ , i.e. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{p_{0},p^{*}}[\\chi(O_{0},O_{1},p^{*})]\\ge\\mathbb{E}_{p_{0},p^{*}}[\\chi(O_{0},O_{1},p)]\\quad\\forall p\\in\\mathcal{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, we say that $\\chi$ is strictly proper if (17) holds with equality iff $p=p^{*}$ ", "page_idx": 18}, {"type": "text", "text": "Let ${\\mathcal{F}}_{\\mathrm{Z}}$ be the set of candidate $s$ -partial conditional covariate shifts $p._{s0}$ and $\\mathcal{F}_{\\mathtt{Y}}$ be the set of candidate $s$ -partial conditional outcome shifts $p.._{s}$ , i.e. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal F}_{\\boldsymbol Z}=\\left\\{p_{\\boldsymbol{s}0}(\\boldsymbol{y},\\boldsymbol{z}|\\boldsymbol{w})=p_{0}(\\boldsymbol{y}|\\boldsymbol{z},\\boldsymbol{w})p_{\\boldsymbol{s}}(\\boldsymbol{z}|\\boldsymbol{w}):\\boldsymbol{s}\\in\\left\\{1,\\cdots,m_{2}\\right\\}\\right\\}}\\\\ &{{\\mathcal F}_{\\boldsymbol\\Upsilon}=\\left\\{p_{\\boldsymbol{s}}(\\boldsymbol{y}|\\boldsymbol{z},\\boldsymbol{w}):\\boldsymbol{s}\\in\\left\\{1,\\cdots,m_{2}\\right\\}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, $v_{Z}$ defined in (3) is a proper scoring rule with respect to ${\\mathcal{F}}_{\\mathrm{z}}$ , as $v_{Z}$ attains its largest value when $\\Delta_{\\cdot s0}\\equiv\\Delta_{\\cdot10}$ , which holds when $p._{s0}$ matches the true covariate-only distribution shift model $p.10$ . Similarly, $v_{\\Upsilon}$ defined in (4) is a proper scoring rule with respect to $\\mathcal{F}_{\\mathtt{Y}}$ . Moreover, these are strictly proper scoring rules as long as the conditional expectation of the losses of candidate distribution shifts are not adversarially aligned. That is, under the assumptions that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\cdot s0}\\left[\\ell|W\\right]\\equiv\\mathbb{E}_{\\cdot10}\\left[\\ell|W\\right]}&{{}\\iff\\quad p_{\\cdot s0}\\equiv p_{\\cdot10}\\quad\\forall p_{\\cdot s0}\\in\\mathcal{F}_{\\mathrm{Z}}}\\\\ {\\mathbb{E}_{\\cdot s}\\left[\\ell|W,Z\\right]\\equiv\\mathbb{E}_{\\cdot1}\\left[\\ell|W,Z\\right]}&{{}\\iff\\quad p_{\\cdot s}\\equiv p_{\\cdot1}\\quad\\forall p_{\\cdot s}\\in\\mathcal{F}_{\\mathrm{Y}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$v_{Z}$ and $v_{\\Upsilon}$ are strictly proper scoring rules with respect to ${\\mathcal{F}}_{\\mathrm{Z}}$ and $\\mathcal{F}_{\\mathtt{Y}}$ , respectively. As the candidate distribution shifts considered in this work are functionals of the observed distribution shift and observed distribution shifts are unlikely to be adversarial in many real-world situations, Assumptions (A.1) and (A.2) are likely reasonable in practice. ", "page_idx": 19}, {"type": "text", "text": "F Estimation and Inference ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Let $\\{(W_{i}^{(d)},Z_{i}^{(d)},Y_{i}^{(d)}):i=1,\\cdot\\cdot\\cdot\\cdot,n_{d}\\}$ denote $n_{d}$ independent and identically distributed (IID) observations from the source and target domains $d=0$ and 1, respectively. Let a fixed fraction of the data be partitioned towards \u201ctraining\u201d $(\\mathbb{T}\\mathbf{r})$ and the remaining to \u201cevaluation\u201d $\\mathtt{(E v)}$ ; let $n_{\\mathrm{Ev}}$ be the number of observations in the evaluation partition. Let $\\mathbb{P}_{d}$ denote the expectation with respect to domain $d$ and $\\mathbb{P}_{d,n}$ denote the empirical average over observations in partition Ev from domain $d=\\{0,1\\}$ . ", "page_idx": 19}, {"type": "text", "text": "F.1 Aggregate decomposition ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The aggregate decomposition terms can be formulated as an average treatment effect, a well-studied estimand in causal inference, where domain $D$ corresponds to treatment. As such, one can use augmented inverse probability weighting (AIPW) to define debiased ML estimators of the aggregate decomposition terms (e.g. Kang and Schafer [24]). We review estimation and inference for these terms below. ", "page_idx": 19}, {"type": "text", "text": "Estimation. Using the training data, estimate outcome models $\\mu._{00}(W)=\\mathbb{E}._{00}[\\ell|W=w]$ and $\\mu.._{0}(W,Z)=\\mathbb{E}[\\ell|\\bar{W},Z]$ and density ratio models $\\pi_{100}(W)=p_{1}(W)/p_{0}(W)$ and $\\pi_{110}(W,Z)=$ $p_{1}(W,Z)/p_{0}(W,Z)$ . The debiased ML estimators for $\\Lambda_{\\mathbb{W}},\\Lambda_{\\mathbb{Z}},\\Lambda_{\\mathbb{Y}}$ are ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\Lambda}_{\\mathsf{u}}=\\!\\mathbb{P}_{0,n}\\left(\\ell-\\hat{\\mu}._{00}(W)\\right)\\hat{\\pi}_{100}(W)+\\mathbb{P}_{1,n}\\hat{\\mu}._{00}(W)-\\mathbb{P}_{0,n}\\ell}\\\\ &{\\hat{\\Lambda}_{\\mathsf{z}}=\\!\\mathbb{P}_{0,n}\\left(\\ell-\\hat{\\mu}._{0}(W,Z)\\right)\\hat{\\pi}_{110}(W,Z)+\\mathbb{P}_{1,n}\\hat{\\mu}._{\\cdot0}(W,Z)}\\\\ &{\\qquad\\quad-\\mathbb{P}_{0,n}\\left(\\ell-\\hat{\\mu}._{00}(W)\\right)\\hat{\\pi}_{100}(W)-\\mathbb{P}_{1,n}\\hat{\\mu}._{\\cdot00}(W)}\\\\ &{\\hat{\\Lambda}_{\\mathsf{Y}}=\\!\\mathbb{P}_{1,n}\\ell-\\mathbb{P}_{0,n}\\left(\\ell-\\hat{\\mu}._{\\cdot0}(W,Z)\\right)\\hat{\\pi}(W,Z)-\\mathbb{P}_{1,n}\\hat{\\mu}_{0}(W,Z)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Inference. Assuming the estimators for the outcome and density ratio models converge at a fast enough rate, the AIPW estimators for the aggregate decomposition terms are asymptotically linear and, thus, facilitate the construction of CIs. ", "page_idx": 19}, {"type": "text", "text": "Theorem F.1. Suppose $\\pi_{100}$ and $\\pi_{110}$ are bounded; estimators $\\hat{\\mu}._{00},\\ \\hat{\\pi}.._{0},\\ \\hat{\\pi}_{100},$ , and $\\hat{\\pi}_{110}$ are consistent; and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{0}\\left(\\hat{\\mu}._{00}-\\mu._{00}\\right)\\left(\\hat{\\pi}_{100}-\\pi_{100}\\right)=o_{p}(n^{-1/2})}\\\\ &{\\mathbb{P}_{0}\\left(\\hat{\\mu}._{\\cdot0}-\\mu._{\\cdot0}\\right)\\left(\\hat{\\pi}_{110}-\\pi_{110}\\right)=o_{p}(n^{-1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then $\\hat{\\Lambda}_{\\boldsymbol{b}},\\hat{\\Lambda}_{\\boldsymbol{Z}}$ , and $\\hat{\\Lambda}_{Y}$ are asymptotically linear estimators of their respective estimands. ", "page_idx": 19}, {"type": "text", "text": "F.2 Value of $s$ -partial conditional covariate shifts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Estimation. Using the training partition, estimate the density ratio $\\begin{array}{r l}{\\pi_{1s0}(z_{s},w)}&{{}=}\\end{array}$ $p_{1}(z_{s},w)/p_{0}(z_{s},w)$ and the outcome models ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mu_{\\cdot0_{-s}0}(z_{s},w)=\\mathbb{E}_{\\cdot00}[\\ell|z_{s},w]=\\int\\int\\ell p_{0}(y|w,z)p_{0}(z_{-s}|z_{s},w)d y d z_{-s}}\\\\ {\\displaystyle\\mu_{\\cdot10}(w)=\\mathbb{E}_{\\cdot10}[\\ell|w]}\\\\ {\\displaystyle\\mu_{\\cdot s0}(w)=\\mathbb{E}_{\\cdot s0}[\\ell|w]=\\int\\int\\ell p_{0}(y|w,z)p_{0}(z_{-s}|z_{s},w)p_{1}(z_{s}|w)d y d z_{-s}d z_{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "in addition to the other nuisance models previously mentioned. We propose the estimator $\\hat{v}_{\\sf Z}(s)=$ $\\hat{v}_{2}^{\\mathrm{num}}(s)/\\hat{v}_{2}^{\\mathrm{den}}$ , where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{v}_{\\mathrm{z}}^{\\mathrm{num}}(s):=\\!\\!\\mathbb{P}_{1,n}(\\hat{\\mu}_{\\cdot\\,s0}(W)-\\hat{\\mu}_{\\cdot\\,10}(W))^{2}}\\\\ &{\\qquad\\qquad+\\,2\\mathbb{P}_{0,n}(\\hat{\\mu}_{\\cdot\\,s0}(W)-\\hat{\\mu}_{\\cdot\\,10}(W))(\\ell-\\hat{\\mu}_{\\cdot\\,0_{-s}0}(W,Z_{s}))\\hat{\\pi}_{1s0}(W,Z_{s})}\\\\ &{\\qquad\\qquad-\\,2\\mathbb{P}_{0,n}(\\hat{\\mu}_{\\cdot\\,s0}(W)-\\hat{\\mu}_{\\cdot\\,10}(W))(\\ell-\\hat{\\mu}_{\\cdot\\,0}(W,Z))\\hat{\\pi}_{110}(W,Z)}\\\\ &{\\qquad\\quad+\\,2\\mathbb{P}_{1,n}(\\hat{\\mu}_{\\cdot\\,s0}(W)-\\hat{\\mu}_{\\cdot\\,10}(W))(\\hat{\\mu}_{\\cdot\\,0_{-s}0}(W,Z_{s})-\\hat{\\mu}_{\\cdot\\,s0}(W))}\\\\ &{\\qquad\\qquad-\\,2\\mathbb{P}_{1,n}(\\hat{\\mu}_{\\cdot\\,0}(W)-\\hat{\\mu}_{\\cdot\\,10}(W))(\\hat{\\mu}_{\\cdot\\,0}(W,Z)-\\hat{\\mu}_{\\cdot\\,10}(W))}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $\\hat{v}_{2}^{\\tt d e n}:=\\hat{v}_{2}^{\\tt n u m}(\\emptyset)$ . ", "page_idx": 20}, {"type": "text", "text": "Inference. The estimator is asymptotically normal as long as the outcome and density ratio models are estimated at a fast enough rate defined formally as follows. ", "page_idx": 20}, {"type": "text", "text": "Condition F.2. For variable subset s, suppose the following holds ", "page_idx": 20}, {"type": "text", "text": "\u2022 $\\mathbb{P}_{0}(\\mu_{001}(W)-\\mu_{01}(W))^{2}$ is bounded   \n\u2022 $\\begin{array}{r l}&{\\mathbb{P}_{0}(\\mu_{\\cdot0_{-s}0}(Z_{s},W)-\\hat{\\mu}_{\\cdot0_{-s}0}(Z_{s},W))(\\hat{\\pi}_{1s0}(Z_{s},W)-\\pi_{1s0}(Z_{s},W))=o_{p}(n^{-1/2})}\\\\ &{\\mathbb{P}_{0}(\\mu_{\\cdot0}(W,Z)-\\hat{\\mu}_{\\cdot0}(W,Z))(\\hat{\\pi}_{110}(W,Z)-\\pi_{110}(W,Z))=o_{p}(n^{-1/2})}\\\\ &{\\mathbb{P}_{1}(\\hat{\\mu}_{\\cdot s0}(W)-\\mu_{\\cdot s0}(W))^{2}=o_{p}(n^{-1/2})}\\\\ &{\\mathbb{P}_{1}(\\hat{\\mu}_{\\cdot10}(W)-\\mu_{\\cdot10}(W))^{2}=o_{p}(n^{-1/2})}\\end{array}$   \n\u2022   \n\u2022   \n\u2022   \n\u2022 (Positivity) $p_{0}(z_{s},w)\\;>\\;0$ and $p_{0}(w,z)\\;>\\;0$ almost everywhere, such that the density ratios $\\pi_{1s0}(w,z_{s})$ and $\\pi_{110}(w,z)$ are well-defined and between $(0,1)$ . ", "page_idx": 20}, {"type": "text", "text": "Theorem F.3. For variable subset $s$ , suppose $v_{Z}^{d e n}(s)>0$ and Condition $F.2$ hold. Then the estimator $\\hat{v}_{Z}(s)$ is asymptotically normal. ", "page_idx": 20}, {"type": "text", "text": "G Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Notation. For all proofs, we will write $\\mathbb{P}$ to mean expectation on the evaluation partition (and likewise for the empirical version) for notational simplicity. ", "page_idx": 20}, {"type": "text", "text": "Overview of derivation strategy. We first present the general strategy for proving asymptotic normality of the estimators for the decompositions. Details on nonparametric debiased estimation can be found in texts such as Tsiatis [45] and Kennedy [25]. ", "page_idx": 20}, {"type": "text", "text": "Let $v(\\mathbb{P})$ be a pathwise differentiable quantity that is a function of the true regular (differentiable in quadratic mean) probability distribution $\\mathbb{P}$ over random variable $O$ . For instance, $v$ in the case of mean is defined as $v(\\mathbb{P}):=\\mathbb{E}_{o\\sim\\mathbb{P}(O)}[o]$ . Let $\\hat{\\mathbb P}$ denote an arbitrary regular estimator of $\\mathbb{P}$ , such as the maximum likelihood estimator. The plug-in estimator is then defined as $v(\\hat{\\mathbb{P}})$ . ", "page_idx": 20}, {"type": "text", "text": "The von-Mises expansion of the functional $v$ (which linearizes $v$ in analogy to the first-order Taylor expansion), given it is pathwise differentiable, gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v(\\hat{\\mathbb{P}})-v(\\mathbb{P})=-\\mathbb{P}\\,\\psi(o;\\hat{\\mathbb{P}})+R(\\hat{\\mathbb{P}},\\mathbb{P}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, the function $\\psi$ is called an influence function (or a functional gradient of $v$ at $\\hat{\\mathbb P}$ ). $R(\\hat{\\mathbb{P}},\\mathbb{P})$ is a second-order remainder term. The one-step corrected estimators we consider have the form of $v({\\hat{\\mathbb{P}}})+\\mathbb{P}_{n}\\psi(o;{\\hat{\\mathbb{P}}})$ where $\\mathbb{P}_{n}$ denotes a sample average. Following the expansion above, the one-step corrected estimator can be analyzed as follows, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big(v(\\hat{\\mathbb{P}})+\\mathbb{P}_{n}\\psi(o;\\hat{\\mathbb{P}})\\Big)-v(\\mathbb{P})}\\\\ &{=(\\mathbb{P}_{n}-\\mathbb{P})\\psi(o;\\hat{\\mathbb{P}})+R(\\hat{\\mathbb{P}},\\mathbb{P})}\\\\ &{=(\\mathbb{P}_{n}-\\mathbb{P})\\psi(o;\\mathbb{P})+(\\mathbb{P}_{n}-\\mathbb{P})(\\psi(o;\\hat{\\mathbb{P}})-\\psi(o;\\mathbb{P}))+R(\\hat{\\mathbb{P}},\\mathbb{P})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Our\u221a goal will be to analyze each of the three terms and to show that they are asymptotically negligible at $\\sqrt{n}$ -rate, such that the one-step corrected estimator satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Big(v(\\hat{\\mathbb{P}})+\\mathbb{P}_{n}\\psi(o;\\hat{\\mathbb{P}})\\Big)-v(\\mathbb{P})=\\mathbb{P}_{n}\\psi(o;\\mathbb{P})+o_{p}\\big(n^{-1/2}\\big),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used the property of influence functions that they have zero mean. Thus the one-step corrected estimator is asymptotically normal with mean $v(\\mathbb{P})$ and variance $v a r(\\psi(o;\\mathbb{P}))/n$ , which allows for the construction of CIs. In the following proofs, we present the influence functions without derivations; see Kennedy [25] and Hines et al. [20] for strategies for deriving influence functions. ", "page_idx": 21}, {"type": "text", "text": "G.1 Aggregate decompositions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Let the nuisance parameters in the one-step estimators $\\Lambda_{\\mathsf{W}},\\Lambda_{\\mathsf{Z}},\\Lambda_{\\mathsf{Y}}$ be denoted by $\\eta_{\\mathsf{W}}\\quad=$ $\\left(\\mu._{00},\\pi_{100}\\right)\\!,\\eta_{\\mathrm{}}=\\left(\\mu._{\\cdot0},\\mu._{00},\\pi_{110}\\right)\\!,\\eta_{\\mathrm{}}=\\left(\\mu._{\\cdot0},\\pi_{110}\\right)$ respectively. Denote the estimated nuisances by $\\hat{\\eta}_{\\!\\mathrm{w}},\\hat{\\eta}_{\\!\\mathrm{z}},\\hat{\\eta}_{\\!\\mathrm{Y}}$ . The canonical gradients for the three estimands are ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c c l}{\\displaystyle\\psi_{\\mathsf{M}}(d,w,z,y;\\eta_{\\mathsf{M}})=\\big[\\ell(w,z,y)-\\mu._{0}(w)\\big)\\,\\pi_{100}(w)-\\ell(w,z,y)\\big]\\,\\frac{\\mathbb{I}\\big\\{d=0\\big\\}}{p(d=0)}}\\\\ {\\displaystyle}&{\\displaystyle+\\,\\mu_{\\cdot0}(w)\\frac{\\mathbb{1}\\,\\{d=1\\}}{p(d=1)}-\\Lambda_{\\mathsf{M}}}\\\\ {\\displaystyle}&{\\displaystyle\\psi_{2}(d,w,z,y;\\eta_{\\mathsf{Z}})=\\big[(\\ell(w,z,y)-\\mu._{\\cdot0}(w,z)\\big)\\,\\pi_{110}(w,z)\\big]\\,\\frac{\\mathbb{1}\\,\\{d=0\\}}{p(d=0)}+\\mu._{\\cdot0}(w,z)\\frac{\\mathbb{1}\\,\\{d=1\\}}{p(d=1)}}\\\\ {\\displaystyle}&{\\displaystyle-\\,\\big[(\\ell(w,z,y)-\\mu._{\\cdot0}(w))\\,\\pi_{100}(w)\\big]\\,\\frac{\\mathbb{I}\\,\\{d=0\\}}{p(d=0)}+\\mu._{\\cdot00}(w)\\frac{\\mathbb{1}\\,\\{d=1\\}}{p(d=1)}-\\Lambda_{\\mathsf{Z}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{\\Upsilon}(d,w,z,y;\\eta_{\\Upsilon})=\\left(\\ell(w,z,y)-\\mu...0(w,z)\\right)\\frac{\\mathbb{1}\\,\\{d=1\\}}{p(d=1)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\left[(\\ell(w,z,y)-\\mu...(w,z)\\right)\\pi_{110}(w,z)\\right]\\frac{\\mathbb{1}\\,\\{d=0\\}}{p(d=0)}-\\Lambda_{\\Upsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Theorem G.1 (Theorem F.1). Under conditions outlined in Theorem $F.l$ , the one-step corrected estimators for the aggregate decomposition terms, baseline, conditional covariate, and conditional outcome $\\hat{\\Lambda}_{k},\\,\\hat{\\Lambda}_{Z}$ , and $\\hat{\\Lambda}_{Y},$ are asymptotically linear, i.e. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\Lambda}_{N}-\\Lambda_{N}=\\mathbb{P}_{n}\\psi_{N}+o_{p}\\bigl(n^{-1/2}\\bigr)\\quad\\forall N\\in\\{h,\\,Z,\\,Y\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. The estimands $\\Lambda_{\\mathsf{W}},\\Lambda_{\\mathsf{Z}},\\Lambda_{\\mathsf{Y}}$ have similarities to the standard average treatment effect (ATE) in the causal inference literature (see [25, Example 2]. Hence, the estimators and their asymptotic properties directly follow. For treatment $T$ , outcome $O$ , and confounders $C$ , the mean outcome under $T=1$ among the population with $T=0$ is identified as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\phi=\\int o p(o|c,t=1)p(c|t=0)d o d c\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and its one-step corrected estimator can be derived from the canonical gradient of $\\phi$ , which takes the following form after plugging in the estimates of the nuisance models: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\phi}_{n}=\\mathbb{P}_{n}\\left\\{\\frac{\\mathbb{1}\\{T=1\\}}{\\operatorname*{Pr}(T=1)}\\hat{\\pi}(c)\\left(O-\\hat{\\mu}_{1}(c)\\right)+\\frac{\\mathbb{1}\\{T=0\\}}{\\operatorname*{Pr}(T=0)}\\hat{\\mu}_{1}(c)\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\phi}_{n}-\\phi=\\mathbb{P}_{n}\\left\\{\\frac{\\mathbb{1}\\{T=1\\}}{\\operatorname*{Pr}(T=1)}\\pi(c)\\left(O-\\mu_{1}(c)\\right)+\\frac{\\mathbb{1}\\{T=0\\}}{\\operatorname*{Pr}(T=0)}\\mu_{1}(c)-\\phi\\right\\}+o_{p}(n^{-1/2})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mu_{1}(c)=\\mathbb{E}[o|c,t=1]$ and $\\pi(c)=p(c|t=0)/p(c|t=1)$ as long as the following conditions hold: ", "page_idx": 21}, {"type": "text", "text": "\u2022 $p(c|t=1)>0$ almost everywhere such that the density ratios $\\pi(c)$ are well-defined and bounded, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bullet\\ \\mathbb{P}_{1}(\\hat{\\mu}_{1}-\\mu_{1})(\\hat{\\pi}-\\pi)=o_{p}(n^{-1/2}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We establish the estimators and their influence functions by showing that they can all be viewed as mean outcomes of the form (27). ", "page_idx": 22}, {"type": "text", "text": "Baseline term $\\Lambda_{\\mathbb{W}}$ . The first term $\\mathbb{E}_{100}\\left[\\ell(w,z,y)\\right]$ is a mean outcome with respect to $p(\\ell(w,z,y)|w,d\\,=\\,0)p(w|d\\,=\\,1)$ , which is the same as that in (27) but with $\\ell(w,z,y)$ as the outcome, $w$ as the confounder, and $d$ as the (filpped) treatment. The second term $\\mathbb{E}_{000}\\left[\\ell(w,z,y)\\right]$ is a simple average over $D=0$ population whose influence function is the $\\ell(w,z,y)$ itself. ", "page_idx": 22}, {"type": "text", "text": "Conditional covariate term $\\Lambda_{z}$ . First term $\\mathbb{E}_{110}\\left[\\ell(w,z,y)\\right]$ is the mean outcome with respect to $p(\\ell(w,z,y)|w,z,d=0)p(w,z|d=1)$ , where the chief difference is $(w,z)$ is the confounder. Second term $\\mathbb{E}_{100}\\left[\\ell(w,z,y)\\right]$ is also a mean outcome, as discussed above. ", "page_idx": 22}, {"type": "text", "text": "Conditional outcome term $\\Lambda_{\\Upsilon}$ . First term $\\mathbb{E}_{111}\\left[\\ell(w,z,y)\\right]$ is a simple average over the $D=1$ population. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "G.2 Value of $s$ -partial conditional covariate shifts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Let nuisance parameters in the one-step estimator $v_{2,s}^{\\mathrm{num}}$ be denoted $\\eta_{\\scriptscriptstyle\\mathrm{Z},s}^{\\tt n u m}\\qquad=$ $(\\mu._{s0},\\mu._{10},\\mu._{0-s}0,\\mu_{001},\\mu.._{0},\\pi_{1s0},\\pi_{110})$ and the set of estimated nuisances by $\\hat{\\eta}_{\\boldsymbol{2},s}^{\\mathrm{num}}$ . The canonical gradient of $v_{\\mathrm{Z}}^{\\mathrm{num}}(s)$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\psi_{\\mathbb{Z},s}^{\\mathrm{man}}(D,W,Z,Y;\\eta_{2,s}^{\\mathrm{man}})=(\\mu_{s0}(W)-\\mu_{10}(W))^{2}\\frac{\\mathbb{I}\\{D=1\\}}{\\operatorname*{Pr}(D=1)}}&{}\\\\ {+\\,2(\\mu_{s0}(W)-\\mu_{10}(W))(\\ell-\\mu_{0-s0}(W,Z_{s}))\\pi_{10}(W,Z_{s})\\frac{\\mathbb{I}\\{D=0\\}}{\\operatorname*{Pr}(D=0)}}&{}\\\\ {-\\,2(\\mu_{s0}(W)-\\mu_{10}(W))(\\ell-\\mu_{-0}(W,Z))\\pi_{10}(W,Z)\\frac{\\mathbb{I}\\{D=0\\}}{\\operatorname*{Pr}(D=0)}}&{}\\\\ {+\\,2(\\mu_{s0}(W)-\\mu_{10}(W))(\\mu_{0-s0}(W,Z_{s})-\\mu_{s0}(W))\\frac{\\mathbb{I}\\{D=1\\}}{\\operatorname*{Pr}(D=1)}}&{}\\\\ {-\\,2(\\mu_{s0}(W)-\\mu_{10}(W))(\\mu_{-0}(W,Z)-\\mu_{10}(W))\\frac{\\mathbb{I}\\{D=1\\}}{\\operatorname*{Pr}(D=1)}}&{}\\\\ {-\\,v_{\\mathbb{Z}}^{\\mathrm{man}}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma G.2. Under Condition F.2, $\\hat{v}_{Z}^{n u m}(s)$ satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\hat{v}_{z}^{n u m}(s)-v_{z}^{n u m}(s)=\\mathbb{P}_{n}\\psi_{z,s}^{n u m}(D,W,Z,Y;\\eta_{Z,s}^{n u m})+o_{p}(n^{-1/2})}\\\\ &{}&{\\hat{v}_{z}^{d e n}-v_{z}^{d e n}=\\mathbb{P}_{n}\\psi_{z,\\emptyset}^{n u m}(D,W,Z,Y;\\eta_{Z,s}^{n u m})+o_{p}(n^{-1/2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Consider the following decomposition ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\hat{v}_{\\mathrm{z}}^{\\mathrm{num}}(s)-v_{\\mathrm{z}}^{\\mathrm{num}}(s)}\\\\ &{=\\!(\\mathbb{P}_{n}-\\mathbb{P})\\psi_{\\mathrm{z}}^{\\mathrm{num}}(D,W,Z,Y;\\eta_{{\\mathrm{z}},s}^{\\mathrm{num}})}\\\\ &{\\quad+\\,(\\mathbb{P}_{n}-\\mathbb{P})\\!\\big(\\psi_{\\mathrm{z}}^{\\mathrm{num}}(D,W,Z,Y;\\hat{\\eta}_{{\\mathrm{z}},s}^{\\mathrm{num}})-\\psi_{\\mathrm{z}}^{\\mathrm{num}}(W,Z,Y;\\eta_{{\\mathrm{z}},s}^{\\mathrm{num}})\\big)}\\\\ &{\\quad+\\,\\mathbb{P}(\\psi_{\\mathrm{z}}^{\\mathrm{num}}(D,W,Z,Y;\\hat{\\eta}_{{\\mathrm{z}},s}^{\\mathrm{num}})-\\psi_{\\mathrm{z}}^{\\mathrm{num}}(D,W,Z,Y;\\eta_{{\\mathrm{z}},s}^{\\mathrm{num}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We note that (32) converges to a normal distribution per CLT assuming the variance of $\\psi_{\\mathrm{{Z}},s}^{\\mathrm{{num}}}$ is finite. The empirical process term (32) is asymptotically negligible, as the nuisance parameters $\\eta_{\\mathrm{Z},s}^{\\mathrm{num}}$ are estimated using a separate training data split from the evaluation data and [25, Lemma 1] states that ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\mathbb{P}_{n}-\\mathbb{P})(\\psi_{\\mathtt{Z},s}^{\\mathtt{n u m}}(D,W,Z,Y;\\hat{\\eta}_{\\mathtt{Z},s}^{\\mathtt{n u m}})-\\psi_{\\mathtt{Z},s}^{\\mathtt{n u m}}(D,W,Z,Y;\\eta_{\\mathtt{Z},s}^{\\mathtt{n u m}})=o_{p}(n^{-1/2})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "as long as estimators for all nuisance parameters are consistent. We now establish that the remainder term (33) is also asymptotically negligible. Integrating with respect to $Y$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathfrak{z})=\\!\\mathbb{P}\\,2(\\hat{\\mu}_{\\cdot\\,s0}(W)-\\hat{\\mu}_{\\cdot\\,10}(W))\\times\\Big((\\mu_{\\cdot\\,0-s0}(Z_{s},W)-\\hat{\\mu}_{\\cdot\\,0-s0}(Z_{s},W))\\hat{\\pi}_{1s0}(W,Z_{s})\\frac{\\mathbb{1}\\{D=0\\}}{p(D=0)}}&{}\\\\ {+\\,(\\hat{\\mu}_{\\cdot\\,0-s0}(Z_{s},W)-\\hat{\\mu}_{\\cdot\\,0}(W))\\frac{\\mathbb{1}\\{D=1\\}}{p(D=1)}}&{}\\\\ {-\\,(\\mu_{\\cdot\\,0}(W,Z)-\\hat{\\mu}_{\\cdot\\,0}(W,Z))\\hat{\\pi}_{110}(W,Z)\\frac{\\mathbb{1}\\{D=0\\}}{p(D=0)}}&{}\\\\ {-\\,(\\hat{\\mu}_{\\cdot\\,0}(W,Z)-\\hat{\\mu}_{\\cdot\\,10}(W))\\frac{\\mathbb{1}\\{D=1\\}}{p(D=1)}\\Big)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad}&{+\\,\\mathbb{P}((\\hat{\\mu}_{\\infty}(W)-\\hat{\\mu}_{10}(W))^{2}-(\\mu,\\mathrm{o}(W)-\\mu_{10}(W))^{2})\\frac{1}{p}(D=1)}\\\\ &{=\\mathbb{P}\\,2(\\hat{\\mu}_{\\infty}(W)-\\hat{\\mu}_{10}(W))\\times\\Big((\\mu_{0,-0}(Z_{*},W)-\\hat{\\mu}_{0,-0}(Z_{*},W))(\\hat{\\pi}_{1,0}(W,Z_{*})-\\pi_{1\\infty}(W,Z_{*}))\\,\\frac{1}{p!}\\Big.}\\\\ &{\\qquad\\Big.\\qquad\\qquad\\qquad\\quad+\\left.\\left(\\mu_{0,-0}(Z_{*},W)-\\hat{\\mu}_{0,-0}(Z_{*},W)\\right)\\pi_{1,0}(W,Z_{*})\\frac{1}{p}(D=0)}\\\\ &{\\qquad\\qquad\\qquad\\quad+\\left.\\left(\\hat{\\mu}_{0,-0}(Z_{*},W)-\\hat{\\mu}_{-0}(W)\\right)\\frac{1}{p}(D=1)\\right.}\\\\ &{\\qquad\\qquad\\quad\\qquad\\quad-\\left.\\left(\\mu_{-0}(W,Z)-\\hat{\\mu}_{-0}(W,Z)\\right)(\\hat{\\pi}_{10}(W,Z)-\\pi_{10}(W,Z))\\,\\frac{1}{p!}(D=0)\\right.}\\\\ &{\\qquad\\qquad\\quad\\left.-\\left(\\mu_{-0}(W,Z)-\\hat{\\mu}_{-0}(W,Z)\\right)\\tau_{10}(W,Z)\\frac{1}{p(D=0)}\\right.\\Big.\\Big.\\frac{1}{p}(D=0)}\\\\ &{\\qquad\\qquad\\quad\\left.\\quad+\\left.\\left(\\hat{\\mu}_{0}(W,Z)-\\hat{\\mu}_{-0}(W,Z)\\right)\\tau_{10}(W,Z)\\right)\\frac{1}{p(D=0)}(E=0)}\\\\ &{\\qquad\\qquad\\quad\\quad\\left.-\\left(\\hat{\\mu}_{-0}(W,Z)-\\hat{\\mu}_{-0}(W)\\right)\\frac{1}{p(D=1)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n+\\,\\mathbb{P}((\\hat{\\mu}._{s0}(W)-\\hat{\\mu}._{10}(W))^{2}-(\\mu._{s0}(W)-\\mu._{10}(W))^{2})\\frac{\\mathbb{1}\\{D=1\\}}{p(D=1)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From convergence conditions in Condition F.2, this simplifies to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\gamma\\log2(\\hat{\\mu}_{\\cdot\\mathrm{{s0}}}(W)-\\hat{\\mu}_{\\cdot\\mathrm{10}}(W))\\times\\Big((\\mu_{\\cdot\\mathrm{0}_{-s}0}(Z_{s},W)-\\hat{\\mu}_{\\cdot\\mathrm{0}_{-s}0}(Z_{s},W))\\pi_{\\mathrm{1s0}}(W,Z_{s})\\frac{\\mathbb{1}\\{D=0\\}}{p(D=0)}}&{}\\\\ {+\\;(\\hat{\\mu}_{\\cdot0_{-s}0}(Z_{s},W)-\\hat{\\mu}_{\\cdot\\mathrm{80}}(W))\\frac{\\mathbb{1}\\{D=1\\}}{p(D=1)}}&{}\\\\ {-\\;(\\mu_{\\cdot\\mathrm{0}}(W,Z)-\\hat{\\mu}_{\\cdot\\mathrm{0}}(W,Z))\\pi_{\\mathrm{110}}(W,Z)\\frac{\\mathbb{1}\\{D=0\\}}{p(D=0)}}&{}\\\\ {-\\;(\\hat{\\mu}_{\\cdot\\mathrm{0}}(W,Z)-\\hat{\\mu}_{\\cdot\\mathrm{10}}(W))\\frac{\\mathbb{1}\\{D=1\\}}{p(D=1)}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle+\\,\\mathbb{P}(({\\hat{\\mu}}_{\\cdot s0}(W)-{\\hat{\\mu}}_{\\cdot10}(W))^{2}-(\\mu_{\\cdot s0}(W)-\\mu_{\\cdot10}(W))^{2})\\frac{\\mathbb{1}\\{D=1\\}}{p(D=1)}}}\\\\ {{\\displaystyle+\\,o_{p}(n^{-1/2}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Given the true density ratios, we can further simplify the expectations over $D=0$ weighted by the density ratios in the expression above to expectations over $D=1$ . By definition of $\\bar{\\mu_{\\cdot0_{-s}0}}(Z_{s},\\bar{W})$ in ", "page_idx": 23}, {"type": "text", "text": "(18) and $\\mu._{s0}(W)$ in (20) and the definition of $\\mu...0\\mathopen{}\\mathclose\\bgroup\\left(W,Z\\aftergroup\\egroup\\right)$ and $\\mu._{10}(W)$ in Section F.1, (33) simplifies to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(33)=\\!\\!\\operatorname*{lp}_{1}\\,2\\big(\\hat{\\mu}._{\\cdot0}(W)-\\hat{\\mu}._{\\cdot10}(W)\\big)(\\mu._{\\cdot0}(W)-\\hat{\\mu}._{\\cdot0}(W)\\big)}\\\\ &{\\qquad\\quad-\\operatorname{\\mathbb{P}}_{1}\\,2\\big(\\hat{\\mu}._{\\cdot0}(W)-\\hat{\\mu}._{\\cdot10}(W)\\big)(\\mu._{\\cdot10}(W)-\\hat{\\mu}._{\\cdot10}(W))}\\\\ &{\\qquad\\quad+\\operatorname{\\mathbb{P}}_{1}\\big(\\hat{\\mu}._{\\cdot0}(W)-\\hat{\\mu}._{\\cdot10}(W)\\big)^{2}-(\\mu._{\\cdot{s0}}(W)-\\mu._{\\cdot10}(W))^{2})}\\\\ &{\\qquad\\quad+\\,o_{p}\\big(n^{-1/2}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is $o_{p}(n^{-1/2})$ as long as the convergence conditions in Condition F.2 hold. ", "page_idx": 24}, {"type": "text", "text": "As the denominator $v_{\\mathrm{Z}}^{\\tt d e n}$ is equal to the numerator $v_{2}^{\\mathrm{num}}(\\varnothing)$ , it follows that the one-step estimator for the denominator $\\hat{v}_{\\mathrm{Z}}^{\\mathsf{d e n}}$ is asymptotically linear with influence function $\\psi_{2,\\emptyset}^{\\mathrm{num}}$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Proof for Theorem F.3. Combining Lemma G.2 and the Delta method [46, Theorem 3.1], the estimator $\\hat{v}_{\\mathrm{Z}}(s)=\\hat{v}_{\\mathrm{Z}}^{\\mathrm{num}}(s)/\\hat{v}_{\\mathrm{Z}}^{\\mathrm{den}}$ is asymptotically linear ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\hat{v}_{\\mathrm{Z}}^{\\mathrm{num}}(s)}{\\hat{v}_{\\mathrm{Z}}^{\\mathrm{den}}}-\\frac{v_{\\mathrm{Z}}^{\\mathrm{num}}(s)}{v_{\\mathrm{Z}}^{\\mathrm{den}}}=\\mathbb{P}_{n}\\psi_{\\mathrm{Z},s}(D,W,Z,Y;\\eta_{\\mathrm{Z},s}^{\\mathrm{num}},\\eta_{\\mathrm{Z}}^{\\mathrm{den}})+o_{p}(n^{-1/2}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with influence function ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\psi_{\\mathrm{Z},s}(D,W,Z,Y;\\eta_{\\mathrm{Z},s}^{\\mathfrak{n u m}},\\eta_{\\mathrm{Z},s}^{\\mathrm{den}})=\\frac{1}{v_{\\mathrm{Z}}^{\\mathrm{den}}}\\psi_{\\mathrm{Z},s}^{\\mathtt{n u m}}(D,W,Z,Y;\\eta_{\\mathrm{Z},s}^{\\mathtt{n u m}})-\\frac{v_{\\mathrm{Z}}^{\\mathtt{n u m}}(s)}{(v_{\\mathrm{Z}}^{\\mathrm{den}})^{2}}\\psi_{\\mathrm{Z}}^{\\mathtt{d e n}}(D,W,Z,Y;\\eta_{\\mathrm{Z}}^{\\mathtt{d e n}}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\psi_{\\mathrm{z},s}^{\\tt n u m}(D,W,Z,Y;\\eta_{\\mathrm{z},s}^{\\tt n u m})$ is defined in (28) and \u03c8Zden(D, W, Z, Y ; \u03b7Zden) $\\psi_{\\mathrm{z},\\emptyset}^{\\mathrm{num}}(D,W,Z,Y;\\eta_{\\mathrm{z},\\emptyset}^{\\mathrm{num}})$ . ", "page_idx": 24}, {"type": "text", "text": "Accordingly, the estimator asymptotically follows the normal distribution, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sqrt{n}\\left(\\hat{v}_{\\mathrm{z}}(s)-v_{\\mathrm{z}}(s)\\right)\\rightarrow_{d}N(0,\\mathrm{var}(\\psi_{\\mathrm{z},s}(D,W,Z,Y;\\eta_{\\mathrm{z},s}^{\\mathrm{num}},\\eta_{\\mathrm{z},s}^{\\mathrm{den}}))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "G.3 Value of $s$ -partial conditional outcome shifts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Let the nuisance parameters in $\\boldsymbol{v}_{\\mathtt{Y,b i n}}^{\\mathtt{n u m}}$ be denoted $\\eta_{\\mathtt{Y},s}^{\\mathtt{n u m}}=(Q_{\\mathtt{b i n}},\\mu..1,\\mu.._{s},\\pi)$ and its estimate as $\\hat{\\eta}_{\\mathtt{Y},s}^{\\mathtt{n u m}}$ We represent the one-step corrected estimator for $v_{\\mathtt{Y,b i n}}^{\\mathtt{n u m}}(s)$ as the ${\\mathrm{V}}.$ -statistic ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{v}_{\\Upsilon,\\mathrm{bin}}^{\\mathrm{num}}(s)=\\mathbb{P}_{1,n}\\tilde{\\mathbb{P}}_{1,n}\\left(\\hat{\\mu}_{\\cdot1}(W,Z)-\\hat{\\mu}_{\\cdot{s}}(W,Z)\\right)^{2}\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad\\qquad+\\,2\\mathbb{P}_{1,n}\\tilde{\\mathbb{P}}_{1,n}\\left(\\hat{\\mu}_{\\cdot1}(W,Z)-\\hat{\\mu}_{\\cdot{s}}(W,Z)\\right)(\\ell-\\mu_{\\cdot1}(W,Z))}\\\\ &{\\qquad\\quad-\\,2\\mathbb{P}_{1,n}\\tilde{\\mathbb{P}}_{1,n}\\bigg[\\left(\\hat{\\mu}_{\\cdot1}(W,Z_{s},\\tilde{Z}_{-s})-\\hat{\\mu}_{\\cdot{s}}(W,Z_{s},\\tilde{Z}_{-s})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad(\\ell(W,Z_{s},\\tilde{Z}_{-s},Y)-\\mu_{\\cdot{s}}(W,Z_{s},\\tilde{Z}_{-s}))\\pi(\\tilde{Z}_{-s},Z_{s},W,q_{\\mathrm{bin}}(W,Z))\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\quad\\qquad\\qquad\\quad=\\mathbb{P}_{1,n}\\tilde{\\mathbb{P}}_{1,n}h\\left(W,Z,Y,\\tilde{W},\\tilde{Z},\\tilde{Y};\\hat{\\eta}_{\\nabla,s}^{\\mathrm{num}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In more detail, the conditions in Theorem 3.2 are as follows. ", "page_idx": 24}, {"type": "text", "text": "Condition G.3. For variable subset s, suppose the following hold ", "page_idx": 24}, {"type": "text", "text": "\u2022 $\\pi(W,Z_{s},Z_{-s},Q_{b i n})$ is bounded \u2022 $\\hat{\\pi}$ is consistent $\\begin{array}{r}{\\bullet\\ \\mathbb{P}_{1}\\left(\\hat{\\mu}.._{0}-\\mu.._{0}\\right)^{2}=o_{p}(n^{-1/2})}\\\\ {\\bullet\\ \\mathbb{P}_{1}\\left(\\hat{\\mu}.._{1}-\\mu.._{1}\\right)^{2}=o_{p}(n^{-1/2})}\\\\ {\\bullet\\ \\mathbb{P}_{1}\\left(\\hat{\\mu}.._{s}-\\mu.._{s}\\right)^{2}=o_{p}(n^{-1/2})}\\end{array}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\cdot\\ \\mathbb{P}_{1}\\left(\\hat{q}_{b i n}-q_{b i n}\\right)^{2}=o_{p}(n^{-1})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\bullet\\;\\mathbb{P}_{1}\\left({\\widehat{\\mu}}..s-\\mu..s\\right)({\\widehat{\\pi}}-\\pi)=o_{p}(n^{-1/2}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma G.4. Assuming Condition $G.3$ holds, $\\hat{v}_{Y,b i n}^{n u m}$ is an asymptotically linear estimator for $\\boldsymbol{v}_{\\boldsymbol{Y},\\,b\\,i n}^{n u m},$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{v}_{Y,b i n}^{n u m}(s)-v_{Y,b i n}^{n u m}(s)=\\mathbb{P}_{1,n}\\psi_{Y,s}^{n u m}(D,W,Z,Y;\\eta_{Y,s}^{n u m})+o_{p}(n^{-1/2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with influence function ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\rho_{Y,s}^{\\mathfrak{n u m}}\\left(d,w,z,y;\\eta_{Y,s}^{\\mathfrak{n u m}}\\right)=\\left(\\mu...\\right)=\\left(\\mu...(w,z)-\\mu...(w,z)\\right)^{2}}}\\\\ &{}&{+\\left.2(\\mu.._{1}(w,z)-\\mu...s(w,z))\\left[\\ell(w,z,y)-\\mu...(w,z)\\right]\\right.}\\\\ &{}&{\\left.-\\;2\\mathbb{P}_{1}\\left[\\left(\\mu...(w,z_{s},Z_{-s})-\\mu..._{s}(w,z_{s},Z_{-s})\\right)\\right.\\right.}\\\\ &{}&{\\left.\\left.\\left[\\ell\\left(w,z_{s},Z_{-s},y\\right)\\right)-\\mu...(w,z_{s},Z_{-s})\\right]\\pi\\left(Z_{-s},z_{s},w,q_{b i n}(w,z)\\right)\\right]}\\\\ &{}&{-\\left.v_{r,b i n}^{\\mathfrak{n u m}}(s).\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Defining the symmetrized version of $h$ in (46) as hsym(W, Z, Y, W\u02dc, Z\u02dc, Y\u02dc ) = h(W,Z,Y, W\u02dc , Z\u02dc, Y\u02dc )+2h( W\u02dc , Z\u02dc, Y\u02dc ,W,Z,Y ), we rewrite the estimator as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{v}_{\\mathtt{Y},\\mathtt{b i n}}^{\\mathtt{n u m}}(s)=\\mathbb{P}_{1,n}\\tilde{\\mathbb{P}}_{1,n}h_{s y m}\\left(W,Z,Y,\\tilde{W},\\tilde{Z},\\tilde{Y};\\hat{\\eta}_{\\mathtt{Y},s}^{\\mathtt{n u m}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Per Theorem 12.3 in [46], the H\u00e1jek projection of v\u02c6Yn,ubmin(s) is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{u}_{\\Upsilon,\\mathrm{bin}}^{n u m}(s)=\\displaystyle\\sum_{i=1}^{n}\\mathbb{P}_{1}\\left[\\mathbb{P}_{1,n}\\mathbb{\\tilde{P}}_{1,n}h_{s y m}\\left(W,Z,Y,\\tilde{W},\\tilde{Z},\\tilde{Y};\\hat{\\eta}_{\\Upsilon,s}^{\\mathrm{num}}\\right)-\\bar{\\hat{\\upsilon}}_{Y}^{n u m}(s)\\mid X_{i},Y_{i}\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i=1}^{n}\\mathbb{P}_{1}\\left[h_{s y m}\\left(X_{i},Y_{i},X^{(2)},Y^{(2)};\\hat{\\eta}_{\\Upsilon,s}^{\\mathrm{num}}\\right)-\\bar{\\hat{\\upsilon}}_{Y}^{n u m}(s)\\mid X_{i},Y_{i}\\right]}\\\\ &{\\qquad=\\displaystyle\\sum_{i=1}^{n}h_{s y m,1}\\left(X_{i},Y_{i};\\hat{\\eta}_{\\Upsilon,s}^{\\mathrm{num}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\bar{\\hat{v}}_{Y}^{n u m}(s)=\\mathbb{P}_{1}\\tilde{\\mathbb{P}}_{1}h_{s y m}\\left(W,Z,Y,\\tilde{W},\\tilde{Z},\\tilde{Y};\\hat{\\eta}_{\\mathtt{Y},s}^{\\mathrm{num}}\\right)\\!.$ ", "page_idx": 25}, {"type": "text", "text": "Consider the decomposition ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\frac{\\underset{\\mathrm{v},\\mathrm{puin}}{\\operatorname*{max}}}{\\Upsilon_{\\mathrm{y},\\mathrm{bin}}}(s)-v_{\\mathbb{Y},\\mathrm{bin}}^{\\mathrm{num}}(s)=\\mathbb{P}_{1,n}\\mathbb{\\tilde{P}}_{1,n}h_{s y m}\\left(W,Z,Y,\\tilde{W},\\tilde{Z},\\tilde{Y};\\hat{\\eta}_{\\mathrm{\\!{\\!\\!Y},s\\!\\!}}^{\\mathrm{num}}\\right)-\\mathbb{P}_{1}\\mathbb{\\tilde{P}}_{1}h_{s y m}\\left(W,Z,Y,\\tilde{W},\\tilde{Z},\\tilde{Y};\\eta_{\\mathrm{\\!{\\!\\!Y},s\\!\\!}}^{\\mathrm{num}}\\right)}&{}\\\\ &{}&{\\quad=\\mathbb{P}_{1,n}\\mathbb{\\tilde{P}}_{1,n}h_{s y m}\\left(W,Z,Y,\\tilde{W},\\tilde{Z},\\tilde{Y};\\hat{\\eta}_{\\mathrm{\\!{\\!\\!Y},s\\!\\!}}^{\\mathrm{num}}\\right)-\\mathbb{P}_{1,n}\\left[h_{s y m,1}\\left(X,Y;\\hat{\\eta}_{\\mathrm{\\!{\\!\\!Y},s\\!\\!}}^{\\mathrm{num}}\\right)+\\bar{\\hat{\\nu}}_{\\mathrm{\\!{\\!\\!Y},s\\!\\!}}^{\\mathrm{num}}(s)\\right.}\\\\ &{}&{\\quad\\left.\\quad\\quad\\quad\\quad\\quad\\quad+\\left(\\mathbb{P}_{1,n}-\\mathbb{P}_{1}\\right)\\left(h_{s y m,1}\\left(X,Y;\\hat{\\eta}_{\\mathrm{\\!{\\!\\!Y},s\\!\\!}}^{\\mathrm{num}}\\right)+\\bar{\\hat{\\nu}}_{\\mathrm{\\!{\\!\\!Y}}}^{\\mathrm{num}}(s)-h_{s y m,1}\\left(X,Y\\right)-v_{\\mathrm{\\!{\\!\\!Y}}}^{\\mathrm{num}}(s)\\right)\\right.}\\\\ &{}&{\\quad\\left.\\quad\\quad\\quad\\quad\\quad+\\left(\\mathbb{P}_{1,n}-\\mathbb{P}_{1}\\right)\\left(h_{s y m,1}\\left(X,Y\\right)+v_{\\mathrm{\\!{\\!\\!{Y}}}}^{\\mathrm{num}}(s)\\right)\\right.}\\\\ &{}&{\\quad\\left.\\quad\\quad\\quad+\\mathbb{P}_{1}\\left(h_{s y m,1}\\left(X,Y;\\hat{\\eta}_{\\mathrm{\\!{\\!\\!Y},s\\!\\!}}^{\\mathrm{num}}\\right)+\\bar{\\hat{\\nu}}_{\\mathrm \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We analyze each term in turn. ", "page_idx": 25}, {"type": "text", "text": "Term (49): Suppose $\\mathbb{P}_{1}h_{s y m}^{2}(W,Z,Y,\\tilde{W},\\tilde{Z},\\tilde{Y};\\hat{\\eta}_{\\mathtt{Y},s}^{\\mathtt{n u m}})<\\infty$ . Via a straightforward extension of the proof in Theorem 12.3 in van der Vaart [46], one can show that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{v a r\\left(\\mathbb{P}_{1,n}\\tilde{\\mathbb{P}}_{1,n}h_{s y m}\\left(W,Z,Y,\\tilde{W},\\tilde{Z},\\tilde{Y};\\hat{\\eta}_{\\mathrm{Y},s}^{\\mathrm{num}}\\right)\\right)}{v a r\\left(\\mathbb{P}_{1,n}h_{s y m,1}\\left(W,Z,Y;\\hat{\\eta}_{\\mathrm{Y},s}^{\\mathrm{num}}\\right)\\right)}\\rightarrow_{p}1.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then by Theorem 11.2 in van der Vaart [46] and Slutsky\u2019s lemma, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n^{\\mathrm{p}}_{1,n}\\tilde{\\mathbb{P}}_{1,n}h_{s y m}\\left(W,Z,Y,\\tilde{W},\\tilde{Z},\\tilde{Y};\\hat{\\eta}_{\\tilde{Y},s}^{\\mathrm{num}}\\right)-\\mathbb{P}_{1,n}\\left[h_{s y m,1}\\left(W,Z,Y;\\hat{\\eta}_{\\mathtt{Y},s}^{\\mathrm{num}}\\right)+\\bar{\\hat{\\nu}}_{\\mathtt{Y}}^{\\mathrm{num}}(s)\\right]=o_{p}\\left(n^{-1/2}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Term (50): We perform sample splitting to estimate the nuisance parameters and calculate the estimator for $\\hat{v}_{\\mathtt{Y}}^{\\mathtt{n u m}}(s)$ . Then by Lemma 1 in Kennedy [25], we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}_{1,n}-\\mathbb{P}_{1}\\left(h_{s y m,1}\\left(W,Z,Y;\\hat{\\eta}_{\\Upsilon,s}^{\\mathrm{num}}\\right)+\\bar{\\hat{\\upsilon}}_{\\Upsilon}^{\\mathrm{num}}(s)-h_{s y m,1}\\left(W,Z,Y;\\eta_{\\Upsilon,s}^{\\mathrm{num}}\\right)-{v}_{\\Upsilon}^{\\mathrm{num}}(s)\\right)=o_{p}(n^{-1/2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "as long as the estimators for the nuisance parameters are consistent. ", "page_idx": 26}, {"type": "text", "text": "Term (51): This term $\\left(\\mathbb{P}_{1,n}-\\mathbb{P}_{1}\\right)\\left(h_{s y m,1}\\left(W,Z,Y;\\eta_{\\mathtt{Y},s}^{\\mathtt{n u m}}\\right)+v_{\\mathtt{Y}}^{\\mathtt{n u m}}(s)\\right)$ $(\\mathbb{P}_{1,n}-\\mathbb{P}_{1})\\,h_{s y m,1}\\left(W,Z,Y;\\eta_{\\mathbb{Y},s}^{\\mathrm{num}}\\right)$ follows an asymptotic normal distribution per CLT. ", "page_idx": 26}, {"type": "text", "text": "Term (52): We will show that this bias term is asymptotically negligible. For notational simplicity, let $\\hat{\\xi}(W,Z_{s},Z_{-s})=\\hat{\\mu}.._{1}(W,Z)-\\hat{\\mu}.._{s}(W,Z)$ . ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\tau}\\Big\\lVert\\bar{h}_{n}\\left(\\partial_{x},\\frac{\\gamma}{\\gamma},\\hat{V}_{t},\\frac{\\gamma}{\\gamma},\\hat{V}_{t}^{2},\\frac{\\gamma}{2},\\hat{V}_{t}^{2},\\frac{\\gamma}{2}\\right)+\\bar{\\mathcal{O}}^{(n)}(-h_{\\infty})\\left(\\mathbb{V}_{\\tau},Z_{T},\\mathbb{V}_{t},\\frac{\\gamma}{2},\\hat{V}_{t}^{2},\\frac{\\gamma}{2},\\sigma_{t}^{2}\\right)-\\sigma_{\\eta}^{2}\\Big(\\eta\\Big(X,\\frac{\\gamma}{2}\\Big)\\Big)}\\\\ &{=\\gamma_{1}\\hat{\\gamma}_{1}\\left(h_{n}^{T},Z_{T},\\frac{\\gamma}{2},\\hat{V}_{t}^{2},\\frac{\\gamma}{2}\\right)-h_{\\infty}^{T}\\Big(\\mathbb{V}_{\\tau},Z_{T},\\mathbb{V}_{t},\\frac{\\gamma}{2},\\hat{V}_{t}^{2},\\frac{\\gamma}{2}\\Big)}\\\\ &{-\\mathbb{P}_{\\tau}\\Big(\\hat{\\mu}_{n}^{T},\\frac{\\gamma}{2},\\hat{V}_{t}^{2},\\frac{\\gamma}{2}-\\mathbb{V}_{\\tau},\\frac{\\gamma}{2},\\hat{V}_{t}^{2},(\\mu_{n}^{T})^{T}-\\mathbb{V}_{\\tau},\\frac{\\gamma}{2},\\hat{V}_{t}^{2},\\sigma_{t}^{2}\\Big)}\\\\ &{+\\frac{\\gamma_{1}}{2}\\Big(\\eta\\frac{\\gamma}{n}\\Big(X,\\frac{\\gamma}{2}\\Big)-\\frac{\\gamma}{2},\\frac{\\gamma}{2},\\frac{\\gamma}{2}\\Big)\\Big[\\frac{\\gamma}{\\eta\\tau},\\Big(h_{n}^{T},Z_{T}^{2}-\\frac{\\gamma}{2},h_{\\infty}^{T},\\bar{\\mathbb{V}}_{t}^{2})}\\\\ &{-\\frac{\\gamma_{2}}{2}\\mathbb{P}_{\\tau}\\Big\\lVert\\bar{h}_{n}^{T},\\frac{\\gamma}{2}\\Big(h_{n}^{T},Z_{T}^{2}-\\frac{\\gamma}{2},h_{\\infty}^{T},\\bar{\\mathbb{V}}_{t}^{2}-\\frac{\\gamma}{2}\\Big)\\Big]\\Big(\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;2\\mathbb{P}_{1}\\tilde{\\mathbb{P}}_{1}\\Big\\{\\hat{\\xi}(W,Z_{s},\\tilde{Z}_{-s})\\left[\\ell(W,Z_{s},\\tilde{Z}_{-s},Y)-\\hat{\\mu}_{\\cdot s}(W,Z_{s},\\tilde{Z}_{-s})\\right]}\\\\ &{\\qquad\\qquad\\Big[\\hat{\\pi}\\left(\\tilde{Z}_{-s},Z_{s},W,\\hat{q}_{\\mathrm{bin}}(W,Z)\\right)-\\hat{\\pi}\\left(\\tilde{Z}_{-s},Z_{s},W,q_{\\mathrm{bin}}(W,Z)\\right)\\Big]\\left.\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that (56) is $o_{p}(n^{-1/2})$ under the assumed convergence rates for $\\hat{q}_{\\mathtt{b i n}}$ . In addition, (54) is $o_{p}(n^{-1/2})$ , under the assumed convergence rates for $\\hat{\\mu}_{\\cdot\\cdot1}$ and $\\hat{\\mu}..._{s}$ . ", "page_idx": 26}, {"type": "text", "text": "Analyzing the remaining summands $(53)+(55)$ , we note that it simplifies as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{1}\\left(\\mu,s_{*}(X)-\\hat{\\mu}._{*}(X)\\right)\\left(\\hat{\\mu}._{*1}(X)-\\hat{\\mu}._{*}(X)+\\mu._{*1}(X)-\\mu._{*3}(X)\\right)}\\\\ &{\\phantom{=}-2\\mathbb{P}_{1}\\tilde{\\mathbb{P}}_{1}\\Big\\{\\hat{\\xi}(W,Z_{s},\\tilde{Z}_{*})\\left[\\mu._{**}(W,Z_{s},\\tilde{Z}_{-s})-\\hat{\\mu}._{*}(W,Z_{s},\\tilde{Z}_{-s})\\right]}\\\\ &{\\phantom{=}\\left(\\hat{\\pi}\\left(\\tilde{Z}_{-s},Z_{s},W,q_{\\mathrm{sin}}(W,Z))\\right)-\\pi\\left(\\tilde{Z}_{-s},Z_{s},W,q_{\\mathrm{sin}}(W,Z))\\right)\\Big\\}}\\\\ &{\\phantom{=}-2\\mathbb{P}_{1}\\tilde{\\mathbb{P}}_{1}\\hat{\\xi}(W,Z_{s},\\tilde{Z}_{-s})\\left[\\mu._{**}(W,Z_{s},\\tilde{Z}_{-s})-\\hat{\\mu}._{**}(W,Z_{s},\\tilde{Z}_{-s})\\right]\\pi\\left(\\tilde{Z}_{-s},Z_{s},W,q_{\\mathrm{sin}}(W,Z))\\right)}\\\\ &{=\\mathbb{P}_{1}\\left(\\mu._{*}(X)-\\hat{\\mu}._{*}(X)\\right)\\left(\\mu._{*1}(X)-\\hat{\\mu}._{*1}(X)-\\mu._{*3}(X)+\\hat{\\mu}._{*3}(X)\\right)}\\\\ &{\\phantom{=}-2\\mathbb{P}_{1}\\tilde{\\mathbb{P}}_{1}\\Big\\{\\hat{\\xi}(W,Z_{s},\\tilde{Z}_{-s})\\left[\\mu._{**}(W,Z_{s},\\tilde{Z}_{-s})-\\hat{\\mu}._{**}(W,Z_{s},\\tilde{Z}_{-s})\\right]}\\\\ &{\\phantom{=}\\Big[\\hat{\\pi}\\left(\\tilde{Z}_{-s},Z_{s},W,q_{\\mathrm{sin}}(W,Z))\\right)-\\pi\\left(\\tilde{Z}_{-s},Z_{s},W,q_{\\mathrm{sin}}(W,Z))\\Big]\\Big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which is $o_{p}(n^{-1/2})$ , under the assumed convergence rates for $\\hat{\\mu}...s,\\hat{\\mu}..1$ , and $\\hat{\\pi}$ . ", "page_idx": 27}, {"type": "text", "text": "Condition G.5 (Convergence conditions for $\\hat{v}_{\\Upsilon}^{\\sf d e n}$ ). Suppose the following holds ", "page_idx": 27}, {"type": "text", "text": "\u2022 $\\mathbb{P}_{1}\\big(\\mu_{\\cdot1}-\\mu_{\\cdot0}-\\big(\\hat{\\mu}_{\\cdot1}-\\hat{\\mu}_{\\cdot0}\\big)\\big)^{2}=o_{p}\\big(n^{-1/2}\\big)$ \u2022 $\\mathbb{P}_{0}\\big(\\mu_{\\cdot0}-\\hat{\\mu}_{\\cdot0}\\big)\\big(\\pi_{110}-\\hat{\\pi}_{110}\\big)=o_{p}\\big(n^{-1/2}\\big)$ \u2022 $\\mathbb{P}_{0}(\\mu_{\\cdot\\cdot1}-\\mu_{\\cdot\\cdot0})^{2}$ is bounded ", "page_idx": 27}, {"type": "text", "text": "\u2022 (Positivity) $p(w,z|d=0)>0$ almost everywhere, such that the density ratios $\\pi_{110}(w,z)$ are well-defined and bounded. ", "page_idx": 27}, {"type": "text", "text": "Let the nuisance parameters in the one-step estimator $v_{\\Upsilon}^{\\mathrm{den}}$ be denoted by $\\eta_{\\mathtt{Y}}^{\\mathtt{d e n}}=\\left(\\mu.._{0},\\mu.._{1},\\pi_{110}\\right)$ and the set of estimated nuisances by $\\hat{\\eta}_{\\mathtt{Y}}^{\\mathtt{d e n}}$ . ", "page_idx": 27}, {"type": "text", "text": "Lemma G.6. Assuming Condition G.5 holds, then $\\hat{v}_{Y}^{d e n}$ is an asymptotically linear estimator for $v_{Y}^{d e n}$ , i.e. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{v}_{Y}^{d e n}-v_{Y}^{d e n}=\\mathbb{P}_{n}\\psi_{Y}^{d e n}(D,W,Z,Y;\\eta_{Y}^{d e n})+o_{p}(n^{-1/2})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with influence function ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\rho_{Y}^{d e n}(D,W,Z,Y;\\eta_{Y}^{d e n})=\\left(\\mu.._{1}(W,Z)-\\mu._{-0}(W,Z)\\right)^{2}\\frac{\\mathbb{I}\\left\\{D=1\\right\\}}{p(D=1)}}}\\\\ &{}&{+\\;2\\left(\\mu._{\\cdot1}(W,Z)-\\mu._{\\cdot0}(W,Z)\\right)\\left(\\ell-\\mu._{\\cdot1}(W,Z)\\right)\\!\\frac{\\mathbb{I}\\left\\{D=1\\right\\}}{p(D=1)}\\begin{array}{c c c}{(57)}&&{}\\\\ &{}&\\\\ &{-\\left2\\left(\\mu._{\\cdot1}(W,Z)-\\mu._{\\cdot0}(W,Z)\\right)\\left(\\ell-\\mu._{\\cdot0}(W,Z)\\right)\\!\\pi_{110}(W,Z)\\frac{\\mathbb{I}\\left\\{D=0\\right\\}}{p(D=0)}}\\\\ &{}&\\\\ &{}&{-\\;v_{r}^{d e n}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Consider the following decomposition of bias in the one-step corrected estimate ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{v}_{\\Upsilon}^{\\mathrm{den}}-v_{\\Upsilon}^{\\mathrm{den}}}\\\\ &{=\\!(\\mathbb{P}_{n}-\\mathbb{P})\\psi_{\\Upsilon}^{\\mathrm{den}}(D,W,Z,Y;\\eta_{\\Upsilon}^{\\mathrm{den}})}\\\\ &{\\phantom{{=}}+(\\mathbb{P}_{n}-\\mathbb{P})\\big(\\psi_{\\Upsilon}^{\\mathrm{den}}(D,W,Z,Y;\\hat{\\eta}_{\\Upsilon}^{\\mathrm{den}})-\\psi_{\\Upsilon}^{\\mathrm{den}}(D,W,Z,Y;\\eta_{\\Upsilon}^{\\mathrm{den}})\\big)}\\\\ &{\\phantom{{=}}+\\mathbb{P}(\\psi_{\\Upsilon}^{\\mathrm{den}}(D,W,Z,Y;\\hat{\\eta}_{\\Upsilon}^{\\mathrm{den}})-\\psi_{\\Upsilon}^{\\mathrm{den}}(D,W,Z,Y;\\eta_{\\Upsilon}^{\\mathrm{den}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We observe that (61) converges to a normal distribution per CLT assuming that the variance of $\\psi_{\\mathsf{Y}}^{\\mathsf{d e n}}$ is finite. The empirical process term (62) is asymptotically negligible since the nuisance parameters $\\eta_{\\mathtt{Y}}^{\\mathtt{d e n}}$ are evaluated on an separate evaluation data split from the training data used for estimation. ", "page_idx": 27}, {"type": "text", "text": "In addition assuming that the estimators for the nuisance parameters are consistent, Kennedy [25, Lemma 1] states that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbb{P}_{n}-\\mathbb{P})(\\psi_{\\Upsilon}^{\\tt d e n}(D,W,Z,Y;\\hat{\\eta}_{\\Upsilon}^{\\tt d e n})-\\psi_{\\Upsilon}^{\\tt d e n}(D,W,Z,Y;\\eta_{\\Upsilon}^{\\tt d e n}))=o_{p}(n^{-1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We now show that the remainder term (63) is also asymptotically negligible. Substituting the influence function and integrating with respect to $Y$ , (63) becomes ", "page_idx": 28}, {"type": "text", "text": "$\\begin{array}{r l}&{=\\mathbb{P}\\left(\\widehat{\\mu}...(W,Z)-\\widehat{\\mu}...(W,Z)-(\\mu...(W,Z)-\\mu...(W,Z))\\right)^{2}\\frac{\\mathbb{1}(D=1)}{p(D=1)}\\qquad\\qquad\\qquad(64)}\\\\ &{\\quad+\\,2\\mathbb{P}\\left(\\widehat{\\mu}...(W,Z)-\\widehat{\\mu}...(W,Z)-(\\mu...(W,Z)-\\mu...(W,Z))\\right)(\\mu...(W,Z)-\\mu...(W,Z))\\frac{\\mathbb{1}(D=1)}{p(D=0)}.}\\end{array}$ p(D = 1) (65)   \n$\\begin{array}{r l}&{\\quad+\\;2\\mathbb{P}\\big(\\widehat{\\mu}_{-1}(W,Z)-\\widehat{\\mu}_{-0}(W,Z)\\big)\\big(\\mu_{-1}(W,Z)-\\widehat{\\mu}_{-1}(W,Z)\\big)\\frac{\\mathbb{I}\\big\\{D=1\\big\\}}{p\\big(D=1\\big)}\\qquad\\qquad\\qquad\\qquad(66)}\\\\ &{\\quad-\\;2\\mathbb{P}\\big(\\widehat{\\mu}_{-1}(W,Z)-\\widehat{\\mu}_{-0}(W,Z)\\big)\\big(\\mu_{-0}(W,Z)-\\widehat{\\mu}_{-0}(W,Z)\\big)\\widehat{\\mu}_{10}(W,Z)\\frac{\\mathbb{I}\\big\\{D=0\\big\\}}{p\\big(D=0\\big)}\\qquad\\qquad(67)}\\\\ &{\\quad-\\mathbb{P}\\big(\\widehat{\\mu}_{-1}(W,Z)-\\widehat{\\mu}_{-0}(W,Z)-\\big(\\mu_{-1}(W,Z)-\\mu_{-0}(W,Z)\\big)\\big)^{2}\\frac{\\mathbb{I}\\big(D=1\\big)}{p\\big(D=1\\big)}\\qquad\\qquad\\qquad(68)}\\\\ &{\\quad+\\;2\\mathbb{P}\\big(\\mu_{-0}(W,Z)-\\widehat{\\mu}_{-0}(W,Z)\\big)\\big(\\mu_{-1}(W,Z)-\\mu_{-0}(W,Z)\\big)\\frac{\\mathbb{I}\\big(D=1\\big)}{p\\big(D=1\\big)}\\qquad\\qquad\\qquad(69)}\\\\ &{\\quad-\\;2\\mathbb{P}\\big(\\widehat{\\mu}_{-1}(W,Z)-\\widehat{\\mu}_{-0}(W,Z)\\big)\\big(\\mu_{-0}(W,Z)-\\widehat{\\mu}_{-0}(W,Z)\\big)\\big(\\widehat{\\mu}_{10}(W,Z)-\\pi_{10}(W,Z)\\big)\\frac{\\mathbb{I}\\big\\{D=0\\big\\}}{p\\big(D=0\\big)}}\\end{array}$ 0} 1) (70)   \n$\\begin{array}{r l}&{\\quad-\\,2\\mathbb{P}\\left(\\widehat{\\mu}_{\\cdot\\cdot1}(W,Z)-\\widehat{\\mu}_{\\cdot\\cdot0}(W,Z)\\right)\\big(\\mu_{\\cdot\\cdot0}(W,Z)-\\widehat{\\mu}_{\\cdot\\cdot0}(W,Z)\\big){\\pi}_{110}(W,Z)\\frac{\\mathbb{1}\\{D=0\\}}{p(D=0)}\\quad\\left(71\\right)}\\\\ &{\\quad-\\mathbb{P}\\left(\\widehat{\\mu}_{\\cdot1}(W,Z)-\\widehat{\\mu}_{\\cdot\\cdot0}(W,Z)-\\big(\\mu_{\\cdot\\cdot1}(W,Z)-\\mu_{\\cdot\\cdot0}(W,Z)\\big)\\right)^{2}\\frac{\\mathbb{1}\\{D=1\\}}{p(D=1)}\\qquad\\qquad\\qquad(72)}\\\\ &{\\quad-\\,2\\mathbb{P}\\big(\\widehat{\\mu}_{\\cdot1}(W,Z)-\\widehat{\\mu}_{\\cdot\\cdot0}(W,Z)\\big)\\big(\\mu_{\\cdot\\cdot0}(W,Z)-\\widehat{\\mu}_{\\cdot\\cdot0}(W,Z)\\big)\\big(\\widehat{\\pi}_{110}(W,Z)-\\pi_{\\cdot10}(W,Z)\\big)\\frac{\\mathbb{1}\\{D==0\\}}{p(D=0)}.}\\end{array}$ 00)} (73) ", "page_idx": 28}, {"type": "text", "text": "Thus the remainder term is $o_{p}(n^{-1/2})$ if Condition G.5 holds. ", "page_idx": 28}, {"type": "text", "text": "Proof for Theorem 3.2. Combining Lemmas G.4, G.6, and the Delta method [46, Theorem 3.1], the estimator $\\hat{v}_{\\mathtt{Y,b i n}}(s)=\\hat{v}_{\\mathtt{Y,b i n}}^{\\mathtt{n u m}}(s)/\\hat{v}_{\\mathtt{Y}}^{\\mathtt{d e n}}$ is asymptotically linear ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\hat{v}_{\\mathrm{Y,bin}}^{\\mathrm{num}}(s)}{\\hat{v}_{\\mathrm{Y}}^{\\mathrm{den}}}-\\frac{v_{\\mathrm{Y,bin}}^{\\mathrm{num}}(s)}{v_{\\mathrm{Y}}^{\\mathrm{den}}}=\\mathbb{P}_{n}\\psi_{\\mathrm{Y,bin},s}(D,W,Z,Y;\\eta_{\\mathrm{Y},s}^{\\mathrm{num}},\\eta_{\\mathrm{Y}}^{\\mathrm{den}})+o_{p}(n^{-1/2}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with influence function ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\nu}_{\\Upsilon,\\mathrm{bin},s}(D,W,Z,Y;\\eta_{\\Upsilon,s}^{\\mathrm{num}},\\eta_{\\Upsilon}^{\\mathrm{den}})=\\frac{1}{v_{\\Upsilon}^{\\mathrm{den}}}\\psi_{\\Upsilon,s}^{\\mathrm{num}}(D,W,Z,Y;\\eta_{\\Upsilon,s}^{\\mathrm{num}})-\\frac{v_{\\Upsilon,\\mathrm{bin}}^{\\mathrm{num}}(s)}{(v_{\\Upsilon}^{\\mathrm{den}})^{2}}\\psi_{\\Upsilon}^{\\mathrm{den}}(D,W,Z,Y;\\eta_{\\Upsilon}^{\\mathrm{den}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where \u03c8Yn,usm and $\\psi_{\\Upsilon}^{\\mathsf{d e n}}$ are defined in (48) and (60). ", "page_idx": 28}, {"type": "text", "text": "Accordingly, the estimator follows a normal distribution asymptotically, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sqrt{n}\\left(\\hat{v}_{\\Upsilon}(s)-v_{\\Upsilon}(s)\\right)\\to_{d}N(0,\\mathrm{var}(\\psi_{\\Upsilon,\\mathrm{bin},s}(D,W,Z,Y;\\eta_{\\Upsilon}^{\\mathrm{num}},\\eta_{\\Upsilon}^{\\mathrm{den}}))\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Remark on super-fast convergence of $\\hat{q}_{b i n}$ . One of the conditions in Condition G.3 states that the $\\hat{q}_{\\mathrm{bin}}$ converges at $\\bar{o}_{p}(n^{-1})$ rate. While this seems restrictive, binned risk converges exponentially under suitable margin conditions presented in Audibert and Tsybakov [2]. In particular, consider the following margin condition which is a relaxation of Condition 3.1. ", "page_idx": 28}, {"type": "text", "text": "Condition G.7. For all bin edges $b$ except $b\\in\\{0,1\\}$ , the set $\\Xi_{\\epsilon}=\\{x:|q(x)-b|\\leq\\epsilon\\}$ is measure zero for some $\\epsilon>0$ . ", "page_idx": 29}, {"type": "text", "text": "Suppose the function $q$ belongs to the H\u00f6lder class $\\Sigma(\\beta,L,\\ensuremath{\\mathbb{R}}^{d})$ for positive integer $\\beta$ and $L>0$ (see definition in Audibert and Tsybakov [2]), the marginal law of $X$ satisfies the strong density assumption, and the margin condition G.7 is satisfied. Then the rate of convergence of $\\hat{q}_{\\mathtt{b i n}}$ is exponential, i.e. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(\\widehat{q}_{\\mathsf{b i n}}(X)-q_{\\mathsf{b i n}}(X)\\right)^{2}\\le C_{1}\\exp(-C_{2}n)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for constants $C_{1},C_{2}>0$ that do not depend on $n$ . ", "page_idx": 29}, {"type": "text", "text": "Remark on value of $s$ -partial conditional outcome shift. Unlike the case of $s$ -partial conditional covariate shift, note that $v_{\\mathtt{Y}}(\\varnothing)$ may be non-zero when the risk in the target domain is a recalibration (i.e. temperature-scaling) of the risk in the source domain [34, 19]. For instance, there may be general environmental factors such that readmission risks in the target domain are uniformly lower. ", "page_idx": 29}, {"type": "text", "text": "H Implementation details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Here we describe how the nuisance parameters can be estimated in each of the decompositions. In general, density ratio models can be estimated via a standard reduction to a classification problem where a probabilistic classifier is trained to discriminate between source and target domains [43]. ", "page_idx": 29}, {"type": "text", "text": "Note on computation time. Shapley value computation can be parallelized over the subsets. For high-dimensional tabular data, grouping together variables can further reduce computation time (and increase interpretability). ", "page_idx": 29}, {"type": "text", "text": "H.1 Aggregate decompositions ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Density ratio models. Using direct importance estimation [43], density ratio models $\\pi_{100}(W)$ and $\\pi_{110}(W,Z)$ can be estimated by fitting classifiers on the combined source and target data to predict $D=0$ or 1 from features $W$ and $(W,Z)$ , respectively. ", "page_idx": 29}, {"type": "text", "text": "Outcome models. The outcome models $\\mu._{00}(W)$ and $\\mu...0\\mathopen{}\\mathclose\\bgroup\\left(W,Z\\aftergroup\\egroup\\right)$ can be fit in a number of ways. One option is to estimate the conditional distribution of the outcome (i.e. $p_{0}(Y|W)$ or $p_{0}(Y|W,Z))$ using binary classifiers, from which one can obtain an estimate of the conditional expectation of the loss. Alternatively, one can estimate the conditional expectations of the loss directly by fitting regression models. ", "page_idx": 29}, {"type": "text", "text": "H.2 Detailed decomposition for $s$ -partial outcome shift ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Density ratio models. The density ratio $\\pi(W,Z_{s},Z_{-s},Q)\\;\\;=\\;\\;p_{1}(Z_{-s}|W,Z_{s},q(W,Z)\\;\\;=$ $Q)/p_{1}(Z_{-s})$ in (6) can be estimated as follows. Create a second (\u201cphantom\u201d) dataset of the target domain in which $Z_{-s}$ is independent of $Z_{s}$ by permuting the original $Z_{-s}$ in the target domain. Compute $q_{\\mathsf{b i n}}$ for all observations in the original dataset and the permuted dataset. Concatenate the original dataset from the target domain with the permuted dataset. Train a classifier to predict if an observation is from the original versus the permuted dataset. ", "page_idx": 29}, {"type": "text", "text": "Outcome models. The outcome models $\\mu..1$ and $\\mu...s(W,Z)$ can be similarly fit by estimating the conditional distributions $p_{0}(Y|W,Z)$ and $p_{s}(Y|W,Z,q_{\\mathsf{b i n}}(W,Z))$ on the target domain, and then taking expectation of the loss. ", "page_idx": 29}, {"type": "text", "text": "Computing U-statistics. Calculating the double average $\\mathbb{P}_{1,n}\\tilde{\\mathbb{P}}_{1,n}$ in the estimator requires evaluating all $n^{2}$ pairs of data points in target domain. This can be computationally expensive, so a good approximation is to subsample the inner average. We take 2000 subsamples. We did not see large changes in the bias of the estimates compared to calculating the exact U-statistics. ", "page_idx": 29}, {"type": "text", "text": "H.3 Detailed decomposition for $s$ -partial conditional outcome shift ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Density ratio models. The ratio $\\pi_{1s0}(W,Z_{s})=p_{1}(W,Z_{s})/p_{0}(W,Z_{s})$ can be similarly fit using direct importance estimation. ", "page_idx": 29}, {"type": "text", "text": "Outcome models. We require the following models. ", "page_idx": 29}, {"type": "text", "text": "\u2022 $\\mu._{0_{-s}0}(z_{s},w)=\\mathbb{E}._{00}[\\ell|z_{s},w]$ , defined in (18), can be estimated by regressing loss against $w,z_{s}$ on the source domain.   \n\u2022 $\\mu._{10}(w)=\\mathbb{E}._{10}[\\ell|w]$ , defined in (19), can be estimated by regressing $\\mu...0\\left(w,z\\right)$ against $w$ in the target domain.   \n\u2022 $\\mu._{s0}(w)=\\mathbb{E}._{s0}[\\ell|z_{s},w]$ , defined in (20), can be estimated by regressing $\\mu._{0_{-s}0}(z_{s},w)$ against $w$ in the target domain. ", "page_idx": 30}, {"type": "text", "text": "For all models, we use cross-validation to select among model types and hyperparameters. Model selection is important so that the convergence rate conditions for the asymptotic normality results are met. ", "page_idx": 30}, {"type": "text", "text": "I Simulation details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Data generation: We generate synthetic data under two settings. For the coverage checks in Section 4.1, all features are sampled independently from a multivariate normal distribution. The mean of the $(W,Z)$ in the source and target domains are $(0,2,0.7,3)$ and $(0,0,0,0)$ , respectively. The outcome in the source and target domains are simulated from a logistic regression model with coefficients $(0.3,1,0.5,1)$ and (0.3, 0.1, 0.5, 1.4). ", "page_idx": 30}, {"type": "text", "text": "In the second setting for baseline comparisons in Figure 2b(ii), each feature in $W\\in\\mathbb{R}$ and $Z\\in\\mathbb{R}^{5}$ is sampled independently from the rest from a uniform distribution over $[-1,1)$ . The binary outcome $Y$ is sampled from a logistic regression model with coefficients $(0.2,0.4,2,0.25,0.1,0.1)$ in source and $(0.2,-0.4,0.8,0.1,0.1,0.1)$ in target. ", "page_idx": 30}, {"type": "text", "text": "In both the settings, we analyze performance gap of logistic regression models fti on a held-out source dataset. ", "page_idx": 30}, {"type": "text", "text": "Sample-splitting: We fit all models on $80\\%$ of the data points from both source and target datasets which is the $\\mathbb{T}\\mathbf{r}$ partition, and keep the remaining $20\\%$ for computing the estimators which is the Ev partition. ", "page_idx": 30}, {"type": "text", "text": "Model types: We use scikit-learn implementations for all models [33]. We use 3-fold cross validation to select models. For density models, we fti random forest classifiers and logistic regression models with polynomial features of degree 3. We clip the predicted probabilities from the density model for $\\pi$ at $1\\dot{0}^{-6}$ to avoid very large density weights. Depending on whether the target outcome in outcome models is binary or real-valued, we fti random forest classifiers or regressors, and logistic regression or linear regression models with ridge penalty. Specific hyperparameter ranges for the grid search are provided in the code. ", "page_idx": 30}, {"type": "text", "text": "Computing time and resources: Computation for the VI estimates can be quite fast, as Shapley value computation can be parallelized over the subsets and the number of unique variable subsets sampled in the Shapley value approximation is often quite small. For instance, for the ACS Public Coverage case study with 34 features, the unique subsets is 131 even when the number of sampled subsets is 3000, and it takes around 160 seconds to estimate the value of a single variable subset. All experiments are run on a $2.60\\:\\mathrm{GHz}$ processor with 8 CPU cores. ", "page_idx": 30}, {"type": "text", "text": "J Data analysis details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Synthetic. We describe accuracy of the ML algorithm after it is retrained with the top $k$ features and predictions from the original model (Table 3). Proposed method results in the revised model with highest gain in accuracy and AUC. Figure 5 shows the aggregate and detailed decompositions for the simulation setup in Section 4.1. Figure 6 shows the coverage rates of $90\\%$ CIs for the aggregate decompositions. ", "page_idx": 30}, {"type": "text", "text": "Hospital readmission. Using data from the electronic health records of a large safety-net hospital in the US, we analyzed the transferability of performance measures of a Gradient Boosted Tree (GBT) trained to predict 30-day readmission risk for the general patient population (source) but applied to patients diagnosed with heart failure (target). Each of the source and target datasets have 3750 observations for analyzing the performance gap. The GBT is trained on a held-out sample of 18,873 points from the general population. Features include 4 demographic variables $(W)$ and 16 diagnosis ", "page_idx": 30}, {"type": "image", "img_path": "nXXwYsARXB/tmp/fa5d1cbc0b9565c753221c876cfd15331b6292c0ca0faab8bb0f8474b801757c.jpg", "img_caption": ["Figure 5: Sample estimates and CIs for simulation from Section 4.1. Proposed is debiased ML estimator for HDPD. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "nXXwYsARXB/tmp/43490b9f94f6d65419942538e7fbaad2f6a38a25e42bc04b55d529913d7c6fc1.jpg", "img_caption": ["Figure 6: Coverage rates of $90\\%$ CIs for the aggregate decomposition terms for the simulation setup in Section 4.1. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Table 3: Accuracy and AUC for the revised model with respect to the top $k=\\{1,2,3\\}$ variables identified by different methods. Proposed method leads to a model with highest improvement in performance, thus, reducing the performance gap. We compare against two additional baselines that are outperformed by the proposed method: retraining a model on the target data with all features (AUC 0.89, Acc 0.84) and retraining on a weighted source-target data where loss for each point in source and target is weighted by a tuned parameter $\\alpha$ and $1-\\alpha$ respectively (AUC 0.89, Acc 0.85). ", "page_idx": 31}, {"type": "table", "img_path": "nXXwYsARXB/tmp/edf819f1bb7067283d5a99d2e293283b4b2819ec01ce8c0ab1ccf2fbf76fe56f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "Table 4: Difference in AUCs between the revised insurance prediction model with respect to the top $k$ variables identified by the proposed versus RandomForestAcc procedures (Diff AUC-k $=$ Proposed \u2212RandomForestAcc). $95\\%$ CIs are shown. ", "page_idx": 32}, {"type": "table", "img_path": "nXXwYsARXB/tmp/d1b22b3493dcc60fe1842334cdacec0a1ebaf272db9ea265d06340fec71d45f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "nXXwYsARXB/tmp/c0fe7339d616519a5f5e2e4ebf9ebda2cad4ad1740e3b017cf006eb8e296f615.jpg", "img_caption": ["Figure 7: Detailed decompositions for the performance gap of a model predicting hospital readmission in HF population. Plot shows values for the full set of 16 variables. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "codes $(Z)$ . While training, we reweigh samples by class weights to address class imbalance. Figure 7 shows the detailed decomposition of conditional covariate shift for the dataset. ", "page_idx": 32}, {"type": "text", "text": "ACS Public Coverage. We extract data from the American Community Survey (ACS) to predict whether a person has public health insurance. The data only contains persons of age less than 65 and having an income of less than $\\mathbb{S}30{,}000$ . We analyze a neural network (MLP) trained on data from Nebraska (source) to data from Louisiana (target) given 3000 and 6000 observations from the source and target domains, respectively. Another 3300 from source for training the model. Figure 8 shows the detailed decomposition of conditional outcome shift for the dataset. Table 4 shows the difference in AUCs for the revised models based on top features from the proposed method versus RandomForestAcc method. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Main contributions are stated in Section 1. Theoretical and experimental results are presented in Sections 3 and 5 respectively. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. ", "page_idx": 32}, {"type": "image", "img_path": "nXXwYsARXB/tmp/e713efb6d439f20dcc4b6eb36936d1d60a9379f621df41bf3744fc4495760d18.jpg", "img_caption": ["Figure 8: Detailed decompositions for the performance gap of a model predicting insurance coverage prediction across two US states $\\mathrm{(NE\\toLA)}$ ). Plot shows values for the full set of 31 covariates. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Limitations are discussed in Section 7. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Assumptions and proofs for the theoretical results are provided in Sections 3, F, and G. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Pseudocode is provided in Algorithms 1, 2, 3, and 4. Implementation details are described in Section H. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \nWhile NeurIPS does not require releasing code, the conference does require all submis  \nsions to provide some reasonable avenue for reproducibility, which may depend on the   \nnature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Code is available at the link https://github.com/jjfeng/HDPD. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Implementation and simulation details are described in Section H and Section I. Full details are in the provided code. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Results are accompanied by confidence intervals. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Compute resources are described in Section I. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We adhere to the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Although we do not envision any direct negative impacts of the work, potential positive and negative impacts are discussed in Section B. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: We do not release a model or a new dataset. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Medical dataset used for the case study is accessed and analyzed under the terms of an IRB. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We do not introduce new assets. The code is provided only for reproducing the experiments. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Study does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Medical dataset was accessed according to the terms of use of an IRB. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]