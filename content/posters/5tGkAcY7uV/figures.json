[{"figure_path": "5tGkAcY7uV/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Comparison of different 3D backbones in terms of detection performance on Waymo [52], nuScenes [4], Argoverse V2 [60] and ONCE [37] datasets. Here, we adopt Mamba [22] as the default operator of our LION. Besides, we present the simplified schematic of DSVT (b) [60] and our LION (c) for implementing feature interaction in 3D backbones.", "description": "This figure compares the performance of different 3D object detection backbones on four benchmark datasets (Waymo, nuScenes, Argoverse V2, and ONCE).  It shows that the proposed LION method (using the Mamba linear group RNN operator) outperforms existing methods like CenterPoint and DSVT.  Subfigures (b) and (c) illustrate the key difference between the DSVT (using transformers with small feature groups) and LION (using linear group RNNs with larger feature groups) approaches for feature interaction.", "section": "1 Introduction"}, {"figure_path": "5tGkAcY7uV/figures/figures_2_1.jpg", "caption": "Figure 2: The illustration of LION, which mainly consists of N LION blocks, each paired with a voxel generation for feature enhancement and a voxel merging for down-sampling features along the height dimension. (H, W, D) indicates the shape of the 3D feature map, where H, W, and D are the length, width, and height of the 3D feature map along the X-axis, Y-axis, and Z-axis. N is the number of LION blocks. In LION, we first convert point clouds to voxels and partition these voxels into a series of equal-size groups. Then, we feed these grouped features into LION 3D backbone to enhance their feature representation. Finally, these enhanced features are fed into a BEV backbone and a detection head for final 3D detection.", "description": "This figure illustrates the overall architecture of the LION model for 3D object detection. It consists of several LION blocks, each containing voxel generation and merging modules.  The input is point clouds, which are first voxelized and then processed through the LION blocks.  The output of the LION 3D backbone is then passed to the BEV (Bird's Eye View) backbone and finally a detection head for generating the final object detection results.", "section": "3.1 Overview"}, {"figure_path": "5tGkAcY7uV/figures/figures_3_1.jpg", "caption": "Figure 3: (a) shows the structure of LION block, which involves four LION layers, two voxel merging operations, two voxel expanding operations, and two 3D spatial feature descriptors. Here, 1\u00d7, \u00d7, and \u00d7 indicate the resolution of 3D feature map as (H, W, D), (H/2, W/2, D/2) and (H/4, W/4, D/4), respectively. (b) is the process of voxel merging for voxel down-sampling and voxel expanding for voxel up-sampling. (c) presents the structure of LION layer. (d) shows the details of the 3D spatial feature descriptor.", "description": "This figure details the architecture of the LION block, a core component of the LION framework for 3D object detection.  Panel (a) shows the overall structure of the LION block, illustrating the sequence of LION layers, voxel merging and expanding operations, and the integration of 3D spatial feature descriptors. Panels (b), (c), and (d) provide more detailed views of the voxel merging and expanding processes, the LION layer's internal structure (involving linear group RNN operations along both the x-axis and y-axis), and the design of the 3D spatial feature descriptor, respectively.  The figure clarifies how the framework uses a hierarchical structure to process features at multiple scales.", "section": "3.2 LION Block"}, {"figure_path": "5tGkAcY7uV/figures/figures_4_1.jpg", "caption": "Figure 5: The details of voxel generation. For input voxels, we first select the foreground voxels and diffuse them along different directions. Then, we initialize the corresponding features of the diffused voxels as zeros and utilize the auto-regressive ability of the following LION block to generate diffused features. Note that we do not present the voxel merging here for simplicity.", "description": "This figure illustrates the voxel generation process in LION.  It starts with input voxels, where foreground voxels are identified and diffused in four directions.  These diffused voxels are initialized with zero features and fed into a LION block, which uses its auto-regressive property to generate features for the diffused voxels. The process enhances feature representation in sparse point clouds, particularly for foreground objects.  The voxel merging step is omitted for simplicity.", "section": "3.3 Voxel Generation"}, {"figure_path": "5tGkAcY7uV/figures/figures_4_2.jpg", "caption": "Figure 4: The illustration of spatial information loss when flattening into 1D sequences. For example, there are two adjacent voxels in spatial position (indexed as 01 and 34) but are far in the 1D sequences along the X order.", "description": "This figure illustrates a limitation of converting 3D spatial data into 1D sequential data for use in linear group RNNs.  Two voxels that are adjacent in 3D space (01 and 34) become distant when the data is flattened into a 1D sequence. This spatial information loss is addressed in the LION framework by incorporating a 3D spatial feature descriptor.", "section": "3 Method"}, {"figure_path": "5tGkAcY7uV/figures/figures_17_1.jpg", "caption": "Figure 6: Visualization of feature map of different blocks. We highlight the foreground annotated by red GT boxes. The color map represents the magnitude of the feature response.", "description": "The figure visualizes feature maps from four different LION blocks.  Red boxes highlight ground truth foreground objects. The color intensity represents the magnitude of the feature response, showing how the foreground features become more prominent and distinct as they pass through subsequent blocks. This visualization supports the claim that the voxel generation effectively enhances foreground feature representation.", "section": "A.6 Visualization for Feature Map"}, {"figure_path": "5tGkAcY7uV/figures/figures_18_1.jpg", "caption": "Figure 7: Comparison of DSVT and LION on the WOD validation set from the BEV perspective. Blue and green boxes are the prediction and ground truth boxes. It can be seen that LION can achieve better results compared to DSVT, demonstrating the superiority of LION.", "description": "This figure compares the qualitative results of DSVT and LION on the Waymo Open Dataset (WOD) validation set.  The BEV (Bird's Eye View) perspective is used, showing both predicted (blue) and ground truth (green) bounding boxes for detected objects.  The visual comparison demonstrates that LION achieves better accuracy and fewer false positives than DSVT, highlighting its superiority.", "section": "A.7 Comparison of Qualitative Results with DSVT"}, {"figure_path": "5tGkAcY7uV/figures/figures_18_2.jpg", "caption": "Figure 7: Comparison of DSVT and LION on the WOD validation set from the BEV perspective. Blue and green boxes are the prediction and ground truth boxes. It can be seen that LION can achieve better results compared to DSVT, demonstrating the superiority of LION.", "description": "This figure compares the qualitative results of DSVT and LION on the Waymo Open Dataset (WOD) validation set.  The Bird's Eye View (BEV) perspective is used, showing both the predicted bounding boxes (blue) and ground truth boxes (green). The comparison highlights LION's improved accuracy in object detection compared to DSVT.", "section": "A.7 Comparison of Qualitative Results with DSVT"}]