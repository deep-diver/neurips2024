[{"figure_path": "CIcMZGLyZW/figures/figures_1_1.jpg", "caption": "Figure 1: The overview of our neuro-symbolic data generation framework. The framework comprises three steps: (1) Formalize the seed problem into its symbolic version. (2) Mutate the symbolic problem to create new variants. (3) Translate the variants in symbolic form back to the natural language version. Additionally, we prompt GPT-4 to generate reasoning paths, which are verified by symbolic solvers, as part of the supervision.", "description": "This figure illustrates the three main steps of the neuro-symbolic data generation framework: formalization of a seed problem into a symbolic representation, mutation of the symbolic problem to create diverse variants, and informalization of the mutated symbolic problems back into natural language.  The process leverages both the strengths of LLMs (intuitive informalization) and math solvers (precise symbolic reasoning). GPT-4 is used to generate reasoning paths, which are then verified by symbolic solvers to ensure validity.", "section": "2 Mutation"}, {"figure_path": "CIcMZGLyZW/figures/figures_1_2.jpg", "caption": "Figure 2: The performance of our proposed mutation mechanism. The first figure illustrates that the generated problems with higher difficulty levels lead to more reasoning steps of GPT-4. The second figure shows that the gradual incorporation of more difficult problems consistently improves the LLM's reasoning capability.", "description": "This figure shows the results of experiments evaluating the proposed mutation mechanism for generating math problems with varying difficulty levels. The left sub-figure displays the distribution of reasoning steps required by GPT-4 to solve problems at different difficulty levels, indicating that more difficult problems necessitate more reasoning steps. The right sub-figure illustrates the relationship between the number of reasoning steps in training and the reasoning error rate during testing. The results demonstrate that incorporating more complex problems into the training data steadily improves the LLM's reasoning capabilities.", "section": "Empirical evaluation"}, {"figure_path": "CIcMZGLyZW/figures/figures_7_1.jpg", "caption": "Figure 3: BLUE scores between the output of our fine-tuned model, versus the ground-truth solution and GPT-4 output. The model is fine-tuned on the Mistral 7B base model, and MetaMath Mistral 7B model is also included as a reference. The results show that our method does not induce data contamination.", "description": "This figure presents the BLEU scores comparing the model's generated solutions against ground truth and GPT-4 solutions on both the training and test sets of the GSM8K and MATH datasets.  The low BLEU scores on the test sets for both our model and MetaMathQA demonstrate that neither model is memorizing training data, thus proving the data generation method doesn't introduce data contamination.", "section": "4.2 Empirical Results"}, {"figure_path": "CIcMZGLyZW/figures/figures_7_2.jpg", "caption": "Figure 4: Performance curves of the LLaMA-2-7B models fine-tuned on various scales of datasets. The two datasets are generated by our approach and MetaMath (MMQA). The performance can be consistently enhanced by increasing the amount of data generated using the proposed framework.", "description": "This figure compares the performance of models fine-tuned using datasets generated by the proposed method and MetaMathQA.  The x-axis represents the size of the training dataset, and the y-axis shows the accuracy on four different benchmark datasets (GSM8K, SVAMP, ASDiv, and MATH).  The lines show that increased training data consistently improves model performance across all datasets, and that the proposed method generally outperforms MetaMathQA.", "section": "4.2 Empirical Results"}, {"figure_path": "CIcMZGLyZW/figures/figures_16_1.jpg", "caption": "Figure 1: The overview of our neuro-symbolic data generation framework. The framework comprises three steps: (1) Formalize the seed problem into its symbolic version. (2) Mutate the symbolic problem to create new variants. (3) Translate the variants in symbolic form back to the natural language version. Additionally, we prompt GPT-4 to generate reasoning paths, which are verified by symbolic solvers, as part of the supervision.", "description": "This figure illustrates the three main steps of the neuro-symbolic data generation framework: formalization of a seed problem into its symbolic representation, mutation of the symbolic problem to generate diverse and valid variants, and informalization of the mutated symbolic problems back into natural language using LLMs.  GPT-4 is used to generate reasoning paths which are verified by symbolic solvers to ensure validity and consistency.", "section": "2 Mutation"}, {"figure_path": "CIcMZGLyZW/figures/figures_21_1.jpg", "caption": "Figure 5: The diversity gain across all difficulty levels. The results indicate that the diversity gain of the Mix version continues to increase and reaches the highest compared with alternatives as the data budget increases.", "description": "This figure shows the diversity gain (a measure of how different the generated data is from the original dataset) at different difficulty levels and data budget sizes.  The diversity is calculated using the BERT model as a feature extractor.  The results show that increasing the size of the dataset consistently enhances the diversity, with a mix of difficulty levels yielding the highest gains.", "section": "E.4 Diversity Gain across Various Difficulty Levels"}]