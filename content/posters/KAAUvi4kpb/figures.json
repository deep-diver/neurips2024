[{"figure_path": "KAAUvi4kpb/figures/figures_1_1.jpg", "caption": "Figure 1: BrainBits bottlenecking framework as applied to BrainDiffuser. The goal of image reconstruction is to generate an image based on brain signal. The brain signal is mapped to a hidden vector (gold) by a compression mapping gr, which is then used to predict VDVAE, CLIP-text, and CLIP-vision latents via a mapping fL. As in [18], these latents are used to produce the final reconstruction. In our studies, we restrict the information available from the brain by varying the dimension of the hidden vector.", "description": "This figure illustrates the BrainBits framework applied to the BrainDiffuser model.  It shows how brain signals are compressed through an adjustable information bottleneck (gL) into a lower-dimensional representation. This reduced representation, along with CLIP-text and CLIP-vision embeddings, is then used to predict VDVAE latents.  Finally, these latents are fed into a Versatile Diffusion model to generate the reconstructed image.  The size of the bottleneck (L) is varied to assess how much brain information is necessary for successful reconstruction. The goal is to understand how much of the reconstruction is due to information from the brain, versus reliance on the generative model's prior knowledge.", "section": "4.1 BrainDiffusers"}, {"figure_path": "KAAUvi4kpb/figures/figures_2_1.jpg", "caption": "Figure 2: High quality stimuli can be reconstructed from a fraction of the data. Shown here are images and text reconstructed for several bottleneck sizes using our BrainBits approach. Images and text are shown for subject 1 for all three methods. Examples where the original methods could reasonably reconstruct the stimuli were chosen; the same images for both visual methods are shown in the appendix. As the bottleneck dimension increases, the accuracy of the reconstruction increases. Although there are differences between the full and bottlenecked (d = 50) results, the reconstructions are surprisingly comparable, despite the fact that the full reconstruction methods have > 14,000 voxels available to them. Text reconstructions are harder to evaluate in this qualitative manner, later we present a quantitative evaluation.", "description": "This figure shows the results of applying the BrainBits method to three different stimulus reconstruction methods: BrainDiffuser, Takagi & Nishimoto 2023, and Tang et al. 2023.  It demonstrates that high-fidelity image and text reconstructions can be achieved even when using only a small fraction of the available neural data (bottleneck size).  The figure visually compares reconstructions at different bottleneck sizes, highlighting that the quality of the reconstructions increases as more neural data is used, but even with a small fraction the reconstructions are surprisingly good, suggesting that model priors play a significant role.", "section": "3 Approach"}, {"figure_path": "KAAUvi4kpb/figures/figures_2_2.jpg", "caption": "Figure 15: Generalization to other reconstruction methods: Shown here are images reconstructed for several bottleneck sizes applying our BrainBits approach to Takagi & Nishimoto et al.", "description": "This figure demonstrates the application of the BrainBits method to Takagi & Nishimoto's image reconstruction approach.  It shows a grid of images reconstructed at various bottleneck sizes (1, 5, 10, 25, 50), comparing them to Takagi's reconstruction ceiling (i.e., reconstruction if the model had perfect access to latent information) and the ground truth images.  The goal is to visually illustrate how much of the image information can be accurately recovered even with a small amount of neural data, highlighting the role of generative model priors in these high-fidelity reconstructions.", "section": "A.6 Takagi Image Grid with BrainDiffuser Image Indices"}, {"figure_path": "KAAUvi4kpb/figures/figures_3_1.jpg", "caption": "Figure 3: Quantifying how the fraction of the data needed to reconstruct stimuli. While different metrics present slightly different pictures of model performance, most performance is reached by about 20 32-bit floating point numbers and essentially all performance is reached by about 50. Vision reconstruction methods (a,b) are significantly better than language reconstruction methods (c). And although language methods appear to use very large bottlenecks, as a fraction of the data available, they are comparable (vision methods presented here use only voxels in the visual cortex). Relative to vision, language methods are much closer to the random baseline and have a longer way to go, as shown by the inset. This also reveals a limitation of the resolution of the BrainBits approach: there is not much room for bottlenecking when performance is near the random baseline and metrics lack a well calibrated scale. Across different metrics, both low level metrics (like pixel correlation and word error rate) and high level metrics (like DreamSim and BERT) the message is the same: models asymptote quickly.", "description": "This figure quantitatively shows how the performance of three different reconstruction methods changes as the amount of brain data used is reduced.  It demonstrates that a surprisingly small amount of neural data is sufficient for high-fidelity reconstruction, with performance reaching a plateau quickly.  The figure uses different evaluation metrics for image (DreamSim, CLIP embedding cosine similarity, SSIM, and pixel correlation) and text (WER, BERT, BLEU, and METEOR) reconstruction. It highlights that vision reconstruction methods perform significantly better than language reconstruction methods, and that language methods rely more heavily on generative model priors than neural data.", "section": "Results"}, {"figure_path": "KAAUvi4kpb/figures/figures_6_1.jpg", "caption": "Figure 4: How large are the bottlenecks? Even though the bottleneck representations have L dimensions, it is not necessarily the case that all dimensions are used by the bottleneck mapping. For both language and vision, we can measure the effective dimensionality to get a sense for how much of the channel capacity is being used. For BrainDiffuser, the effective dimensionality is comparable to the bottleneck size, showing that information is being extracted from the neural recordings up to about 15\u201320 dimensions. For language bottlenecks, effective dimensionality remains low showing that little of the channel capacity, and therefore little of the neural signal, is being used.", "description": "This figure shows two plots visualizing the effective dimensionality of bottlenecks for vision and language reconstruction methods.  The left plot (BrainDiffuser) shows that the effective dimensionality closely follows the bottleneck size for vision, indicating that a substantial amount of information is used from the neural recordings. Conversely, the right plot (language) illustrates that for language tasks, the effective dimensionality remains low even with large bottlenecks, suggesting that only a small fraction of the neural signal is being used. The dashed lines in the plots indicate the expected dimensionality if all the dimensions were used effectively.", "section": "5 Results"}, {"figure_path": "KAAUvi4kpb/figures/figures_7_1.jpg", "caption": "Figure 5: What areas of the brain help reconstruction the most? Models quickly zoom in on useful areas even at low bottleneck sizes. Note that for clarity the color bar cuts off at le-6, values above that are all orange. In this case BrainDiffuser on subject 1 attends to peripheral areas of the early visual system. As the bottleneck size goes up models exploit those original areas but do not meaningfully expand to new areas. Ideally, one would hope to see more of the brain playing an important role with larger bottleneck sizes; this is not what BrainBits uncovers.", "description": "This figure shows the brain regions that contribute the most to the reconstruction of images at different bottleneck sizes using BrainDiffuser.  The color intensity represents the weight of each brain region in the reconstruction. The figure highlights that while models quickly identify and utilize specific brain areas (in the early visual cortex) for reconstruction, they do not significantly expand their focus to other areas even with larger bottleneck sizes. This suggests that improving the generative models might have a bigger impact than improving the brain signal extraction.", "section": "What regions of the brain matter most?"}, {"figure_path": "KAAUvi4kpb/figures/figures_7_2.jpg", "caption": "Figure 6: What information do bottlenecks contain? For the BrainDiffusers approach we compute the decodability of four different features (object class, brightness, RMS contrast, and the average gradient magnitude) as a function of bottleneck size. Object class refers to decoding the class of the largest object in the image; often the focus of the image. The average gradient magnitude is a proxy for the edge energy in the image. Dashed lines in plot (a) indicate 1-out-of-61 classification chance, 1.6%. Dashed lines on plots (b, c, d) indicate the metric's MSE distance from the average metric value on the training set. Larger bottlenecks are needed to extract more object class information above chance. Edge energy, brightness and contrast are mostly exhausted early. Looking at features as a function of bottleneck size can reveal what types of interpretable features models learn, offering some explanation as to why performance goes up as a function of bottleneck size.", "description": "This figure shows the decodability of four different visual features (object class, brightness, RMS contrast, and average gradient magnitude) as a function of bottleneck size for the BrainDiffusers model.  It demonstrates that low-level features like brightness and contrast are easily decoded even with small bottleneck sizes, while high-level features like object class require larger bottlenecks to achieve reliable decoding. This analysis helps understand what types of information are captured at different bottleneck sizes, providing insights into the model's learning process.", "section": "What do the bottlenecks contain?"}, {"figure_path": "KAAUvi4kpb/figures/figures_13_1.jpg", "caption": "Figure 7: Samples for all subjects in BrainDiffuser, for a single image", "description": "This figure shows the image reconstruction results from the BrainDiffuser model for four different subjects at various bottleneck sizes (1, 5, 10, 25, 50). Each row represents a different subject, and each column shows the reconstruction at a specific bottleneck size. The rightmost column shows the ground truth image. The figure demonstrates how the quality of image reconstruction improves with increasing bottleneck size.", "section": "A.2 brain-diffuser bottlenecks"}, {"figure_path": "KAAUvi4kpb/figures/figures_13_2.jpg", "caption": "Figure 2: High quality stimuli can be reconstructed from a fraction of the data. Shown here are images and text reconstructed for several bottleneck sizes using our BrainBits approach. Images and text are shown for subject 1 for all three methods. Examples where the original methods could reasonably reconstruct the stimuli were chosen; the same images for both visual methods are shown in the appendix. As the bottleneck dimension increases, the accuracy of the reconstruction increases. Although there are differences between the full and bottlenecked (d = 50) results, the reconstructions are surprisingly comparable, despite the fact that the full reconstruction methods have > 14,000 voxels available to them. Text reconstructions are harder to evaluate in this qualitative manner, later we present a quantitative evaluation.", "description": "This figure shows examples of image and text reconstructions from brain activity using different bottleneck sizes.  It demonstrates that high-fidelity reconstructions can be achieved even when using a small fraction of the available brain data. The results suggest that generative models rely heavily on their internal priors, and only a small amount of neural information is needed to guide them towards high-quality reconstructions. ", "section": "Experiments"}, {"figure_path": "KAAUvi4kpb/figures/figures_14_1.jpg", "caption": "Figure 2: High quality stimuli can be reconstructed from a fraction of the data. Shown here are images and text reconstructed for several bottleneck sizes using our BrainBits approach. Images and text are shown for subject 1 for all three methods. Examples where the original methods could reasonably reconstruct the stimuli were chosen; the same images for both visual methods are shown in the appendix. As the bottleneck dimension increases, the accuracy of the reconstruction increases. Although there are differences between the full and bottlenecked (d = 50) results, the reconstructions are surprisingly comparable, despite the fact that the full reconstruction methods have > 14,000 voxels available to them. Text reconstructions are harder to evaluate in this qualitative manner, later we present a quantitative evaluation.", "description": "This figure shows examples of images and text reconstructed using different bottleneck sizes for three different reconstruction methods.  The results demonstrate that high-quality reconstructions can be achieved using surprisingly little information from brain recordings, highlighting the power of generative models' priors.", "section": "Results"}, {"figure_path": "KAAUvi4kpb/figures/figures_14_2.jpg", "caption": "Figure 2: High quality stimuli can be reconstructed from a fraction of the data. Shown here are images and text reconstructed for several bottleneck sizes using our BrainBits approach. Images and text are shown for subject 1 for all three methods. Examples where the original methods could reasonably reconstruct the stimuli were chosen; the same images for both visual methods are shown in the appendix. As the bottleneck dimension increases, the accuracy of the reconstruction increases. Although there are differences between the full and bottlenecked (d = 50) results, the reconstructions are surprisingly comparable, despite the fact that the full reconstruction methods have > 14,000 voxels available to them. Text reconstructions are harder to evaluate in this qualitative manner, later we present a quantitative evaluation.", "description": "This figure demonstrates the effectiveness of the BrainBits method in reconstructing high-fidelity images and text from a small fraction of brain data.  It shows examples for three different reconstruction methods (BrainDiffuser, Takagi & Nishimoto 2023, Tang et al. 2023) and several bottleneck sizes. The results indicate that surprisingly little information from the brain is needed to achieve high-quality reconstructions, highlighting the role of generative model priors.", "section": "Results"}, {"figure_path": "KAAUvi4kpb/figures/figures_15_1.jpg", "caption": "Figure 11: Regression weight plots for each subject", "description": "This figure visualizes the weights of the bottleneck mapping projected onto the brain for each subject (1, 2, 5, and 7) at different bottleneck sizes (1, 5, 25, and 50).  The color intensity represents the magnitude of the weights, indicating the relative importance of different brain regions in the reconstruction process at varying levels of information compression.  It shows how the model's focus on specific brain areas changes as the amount of available brain data increases. The areas highlighted are ROIs (Regions Of Interest) in the brain related to visual processing.", "section": "A.3 Projecting bottlenecks onto the brain"}, {"figure_path": "KAAUvi4kpb/figures/figures_16_1.jpg", "caption": "Figure 12: Legend of ROIs for main text salience maps in Figure 5 and Figure 11.", "description": "This figure shows the legend for the brain regions of interest (ROIs) used in Figures 5 and 11 of the paper.  The legend provides abbreviations and full names for several brain regions involved in visual processing, including face-selective regions (aTL-faces), early visual cortex (EarlyVis), extrastriate body area (EBA), fusiform body area (FBA-1 and FBA-2), fusiform face area (FFA-1 and FFA-2), occipital face area (OFA), occipital place area (OPA), parahippocampal place area (PPA), and retrospenial cortex (RSC). The image displays the location of these ROIs within the brain.", "section": "A.3 Projecting bottlenecks onto the brain"}, {"figure_path": "KAAUvi4kpb/figures/figures_20_1.jpg", "caption": "Figure 13: BrainDiffuser identification accuracy. We evaluate the agreement between latent embeddings of of ground-truth and decoded images of the BrainDiffuser method using the identification accuracy protocol described in [26].", "description": "The figure shows the identification accuracy of the BrainDiffuser method as a function of bottleneck size. The identification accuracy is measured using the latent embeddings of ground-truth and decoded images. The chance level and the performance without bottleneck are also shown for comparison. Different image encoders (alexnet5, alexnet12, alexnet18, clip_h6, clip_h12, clip, inception) were used to evaluate the agreement between latent embeddings of the ground-truth and the decoded images. The results show that the identification accuracy increases with the bottleneck size, indicating that more information from the neural recordings is beneficial for improving the quality of the decoded images.", "section": "A.5 Two-Way Evaluation"}, {"figure_path": "KAAUvi4kpb/figures/figures_20_2.jpg", "caption": "Figure 13: BrainDiffuser identification accuracy. We evaluate the agreement between latent embeddings of of ground-truth and decoded images of the BrainDiffuser method using the identification accuracy protocol described in [26].", "description": "This figure shows the identification accuracy of the BrainDiffuser method across different bottleneck sizes.  The identification accuracy is measured by comparing the latent embeddings of ground-truth and decoded images using a protocol described in a cited paper [26].  The graph displays how the accuracy changes as the size of the information bottleneck increases, indicating the amount of brain information needed to achieve a certain level of reconstruction accuracy. Several different models (alexnet5, alexnet12, etc.) are compared.", "section": "Results"}, {"figure_path": "KAAUvi4kpb/figures/figures_21_1.jpg", "caption": "Figure 2: High quality stimuli can be reconstructed from a fraction of the data. Shown here are images and text reconstructed for several bottleneck sizes using our BrainBits approach. Images and text are shown for subject 1 for all three methods. Examples where the original methods could reasonably reconstruct the stimuli were chosen; the same images for both visual methods are shown in the appendix. As the bottleneck dimension increases, the accuracy of the reconstruction increases. Although there are differences between the full and bottlenecked (d = 50) results, the reconstructions are surprisingly comparable, despite the fact that the full reconstruction methods have > 14,000 voxels available to them. Text reconstructions are harder to evaluate in this qualitative manner, later we present a quantitative evaluation.", "description": "This figure shows the results of applying the BrainBits method to three different stimulus reconstruction methods (BrainDiffuser, Takagi & Nishimoto 2023, and Tang et al. 2023).  It demonstrates that high-fidelity reconstructions of images and text can be achieved using only a small fraction of the full brain data. The figure visually compares reconstructions at different bottleneck sizes, highlighting the surprising similarity between reconstructions using a small bottleneck and the full brain data.  It also shows that the reconstruction quality generally improves as the bottleneck size increases.", "section": "Experiments"}, {"figure_path": "KAAUvi4kpb/figures/figures_22_1.jpg", "caption": "Figure 16: Effective dimensionality (dashed line) of fMRI inputs as used in brain-diffuser (a) and Takagi et al. (b). Here, effective dimensionality is computed the same was as in the main text, as the number of dimensions needed to explain 95% of the variance (as determined by PCA).", "description": "This figure shows the effective dimensionality of fMRI inputs for both BrainDiffuser and Takagi et al. methods. The effective dimensionality represents the number of dimensions needed to explain 95% of the variance in the fMRI data, as determined by Principal Component Analysis (PCA).  The figure visually displays the cumulative variance explained by increasing numbers of principal components for each subject in both models, providing a comparison of how efficiently each method utilizes brain data.  The dashed line represents the 95% variance explained threshold, and the intersection of the line and the curve shows the effective dimensionality for each subject.", "section": "A.7 Effective dimensionality of the fMRI inputs"}]