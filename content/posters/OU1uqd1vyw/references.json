{"references": [{"fullname_first_author": "Devansh Arpit", "paper_title": "Ensemble of averages: Improving model selection and boosting performance in domain generalization", "publication_date": "2021-10-26", "reason": "This paper proposes a novel method for improving model selection and boosting performance in domain generalization, which is relevant to the task of selecting high-quality data for LLMs in diverse private domains."}, {"fullname_first_author": "Junbum Cha", "paper_title": "SwAD: Domain generalization by seeking flat minima", "publication_date": "2021-12-01", "reason": "This paper introduces a new approach to domain generalization by seeking flat minima, which is relevant to the problem of handling domain heterogeneity in collaborative settings."}, {"fullname_first_author": "Together Computer", "paper_title": "RedPajama: an open dataset for training large language models", "publication_date": "2023-01-01", "reason": "This paper introduces RedPajama, a large open dataset for training LLMs, which serves as a crucial resource for developing the anchor dataset and evaluating the proposed data quality control technique."}, {"fullname_first_author": "Shachar Don-Yehiya", "paper_title": "Cold fusion: Collaborative descent for distributed multitask finetuning", "publication_date": "2022-12-01", "reason": "This paper proposes a novel method for collaborative fine-tuning of LLMs, which is directly applicable to the collaborative training setting considered in the current work."}, {"fullname_first_author": "Vipul Gupta", "paper_title": "Stochastic weight averaging in parallel: Large-batch training that generalizes well", "publication_date": "2020-01-20", "reason": "This paper investigates the effects of stochastic weight averaging in parallel training, which is relevant to the model merging techniques employed in the proposed collaborative training approach."}]}