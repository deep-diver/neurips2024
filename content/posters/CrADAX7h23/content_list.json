[{"type": "text", "text": "DAGER: Exact Gradient Inversion for Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ivo Petrov\u22171, Dimitar I. Dimitrov\u22171,2, Maximilian Baader2, Mark Niklas M\u00fcller2,3, Martin Vechev1,2 ", "page_idx": 0}, {"type": "text", "text": "1 INSAIT, Sofia University \"St. Kliment Ohridski\" 2 ETH Zurich 3 LogicStar.ai {ivo.petrov, dimitar.iliev.dimitrov}@insait.ai 1 {mbaader, mark.mueller, martin.vechev}@inf.ethz.ch 2 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning works by aggregating locally computed gradients from multiple clients, thus enabling collaborative training without sharing private client data. However, prior work has shown that the data can actually be recovered by the server using so-called gradient inversion attacks. While these attacks perform well when applied on images, they are limited in the text domain and only permit approximate reconstruction of small batches and short input sequences. In this work, we propose DAGER, the first algorithm to recover whole batches of input text exactly. DAGER leverages the low-rank structure of self-attention layer gradients and the discrete nature of token embeddings to efficiently check if a given token sequence is part of the client data. We use this check to exactly recover full batches in the honest-but-curious setting without any prior on the data for both encoderand decoder-based architectures using exhaustive heuristic search and a greedy approach, respectively. We provide an efficient GPU implementation of DAGER and show experimentally that it recovers full batches of size up to 128 on large language models (LLMs), beating prior attacks in speed ( $20\\mathrm{x}$ at same batch size), scalability ( $10\\mathbf{x}$ larger batches), and reconstruction quality (ROUGE- $1/2>0.99)$ ). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While large language models (LLMs) have demonstrated exceptional potential across a wide range of tasks, training them requires large amounts of data. However, this data is sensitive in many cases, leading to privacy concerns when sharing it with third parties for model training. Federated learning (FL) has emerged as a promising solution to addressing this issue by allowing multiple parties to collaboratively train a model by sharing only gradients computed on their private data with the server instead of the data itself. In particular, FL has been used to finetune LLMs while protecting private data [1, 2, 3] in privacy-critical domains, such as law [4] and medicine [5]. ", "page_idx": 0}, {"type": "text", "text": "Gradient Inversion Attacks Unfortunately, recent work has shown that this private data can be recovered from the shared gradients using so-called gradient inversion attacks, raising concerns about the privacy guarantees of federated learning [6]. While most prior work on gradient inversion attacks has focused on image data [7, 8, 9], first works have demonstrated that text can also be recovered [6, 10, 11]. However, as these approaches are optimization-based, the discrete nature of text data poses a major challenge by inducing much harder optimization problems and limiting them to approximate recovery of small batch sizes and short sequences. Therefore, applying existing attacks methods on modern LLMs would be computationally infeasible or yield subpar reconstructions. ", "page_idx": 0}, {"type": "text", "text": "This Work: Exact Recovery of Large Batches and Long Sequences To overcome these limitations, we propose DAGER (Discreteness-Based Attack on Gradients for Exact Recovery), the first exact gradient inversion attack for (transformer-based) LLMs in the challenging honest-but-curious setting. Our key insight is that while discrete inputs pose a challenge for optimization-based attacks, they can be leveraged in combination with the low-rank structure of gradients to enable exact recovery via search-based attacks. Crucially, we show that the gradients of self-attention projection matrices in transformers are i) typically low-rank and ii) linear combinations of input embeddings. This allows us to check whether a given input embedding lies within the span of the gradient and was thus part of the input sequence. We use this to first recover the set of input tokens and then reconstruct the full sequences. For decoder architectures DAGER leverages their causal attention masks for to derive an efficient greedy recovery, while for encoder architectures, DAGER uses several heuristics to make exhaustive search tractable. As DAGER only requires propagating inputs through the first transformer block instead of full gradient computations, it scales to very large models. In fact, the higher internal dimension of these models even allows DAGER to recover more information as our low-rankness assumptions hold for larger batch sizes and longer sequences. We note that this approach is applicable both to the easier next-token prediction and to the harder classification setting. ", "page_idx": 1}, {"type": "text", "text": "Evaluation We demonstrate in an extensive evaluation that DAGER enables the exact recovery of long sequences and large batch sizes for both encoder- and decoder-based architectures, beating prior attacks in terms of speed $20\\mathrm{x}$ at same batch sizes), scalability (10x larger batches), and reconstruction quality (ROUGE- $\\cdot1/2>0.99_{\\cdot}$ ). In particular, we show this for GPT-2 [12], LLaMa-2 [13], and BERT [14] across CoLA[15], SST-2 [16], Rotten Tomatoes [17] and ECHR [18], for batch sizes up to 128. Additionally, we demonstrate that DAGER is versatile and can be applied to a wide range of settings, including FedAvg [19], LoRA [20] finetuning and model quantization [21]. ", "page_idx": 1}, {"type": "text", "text": "Key Contributions Our main contributions are: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We show how the low-rankness of self-attention layer gradients can be leveraged to check whether specific discrete inputs were present in the input (Sec. 4).   \n\u2022 We leverage this key insight to propose DAGER, the first exact gradient inversion attack for transformers (Sec. 5).   \n\u2022 We conduct an extensive empirical evaluation demonstrating that DAGER is not only able to reconstruct inputs exactly but also scales to much larger batch sizes, longer input sequences, and larger models than prior attacks, while also being significantly faster to mount (Sec. 6).   \n\u2022 We provide an efficient GPU implementation of DAGER, that can be publicly accessed at https://github.com/insait-institute/dager-gradient-inversion. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Gradient leakage attacks, first introduced by Zhu et al. [6], generally fall into two categories \u2014 honest-but-curious attacks [6, 22, 23, 7, 8, 24, 9, 10, 11, 25, 26, 27, 28], where the attacker passively observes the client\u2019s federated learning updates and tries to recover the data solely based on them, and malicious server attacks [29, 30, 31, 32, 33] where the attacker is further allowed to modify the federated learning model shared with the client. In this work, we focus on the harder to attack and more realistic honest-but-curious setting. A large body of the gradient leakage literature in this setting focuses on image data [7, 8, 24, 9, 27]. Differently, gradient leakage in the text domain remains successful only in the case of a malicious adversary [31, 32, 34]. In the honest-but-curious setting, the results either remain limited to short sequences and small batch sizes $B$ [6, 10, 11], require large number of gradient updates [25], or cannot recover the order of tokens in client sequences Xu et al. [35]. Further, state-of-the-art attacks require strong data priors [11, 25], and do not scale to realistic decoder-based LLMs. In contrast, DAGER, works on large batches and sequences for both encoderand decoder-based transformers, including LLaMa-2 [13]. Additionally, unlike prior work, our attack works on both token prediction tasks and the harder setting of sentiment analysis [11] where label recovery methods, such as [35], are not applicable. Finally, DAGER has no requirements for the state of the model training. In instance, [25] exploits model memorization of the data, unlike DAGER, which can handle the more realistic setting of being applicable at any point in time. Further, in contrast to [26, 25, 31], we do not require the gradient of the embedding layer, making our setting significantly harder. ", "page_idx": 1}, {"type": "table", "img_path": "CrADAX7h23/tmp/823e1164064e384d9317f481afbd9f12c7b07e9fb4f23feb7a2b6627d32d08a8.jpg", "table_caption": ["Table 1: Table of notations used in the technical description of DAGER. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "While most prior honest-but-curious attacks leverage optimization methods to approximately recover the client inputs [6, 23, 7, 8, 24, 9, 10, 11], several works have shown that exact reconstruction is possible for batch size $B\\,=\\,1$ under various conditions for different architectures [22, 26, 27]. Crucially, Dimitrov et al. [28] recently showed that $B>1$ exact reconstruction from gradients of fully-connected layers is also possible. Our work, builds upon this result to show that exact gradient leakage is also possible for transformer-based LLMs. ", "page_idx": 2}, {"type": "text", "text": "3 Background and Notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce the background and notation required to understand our work. To this end, we first recall the basic operation of the transformer architecture in the context of LLMs, and then describe the result, first introduced in Dimitrov et al. [28] for linear layers, in the context of a self-attention layer showing that the gradients of its linear transformations have a low-rank structure. The notations used throughout this paper are summarized in Table 1 for clarity and ease of reference. ", "page_idx": 2}, {"type": "text", "text": "3.1 Transformers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we consider LLMs based on both encoder and decoder transformer architectures trained using the FedSGD [36] protocol and a loss function $\\mathcal{L}$ . While we mainly focus on the harder-to-attack binary-classification loss typically used for sentiment analysis, we demonstrate that DAGER is equally applicable to the next-token prediction loss in Sec. 6, which contains more gradient information, as suggested by prior work Zhu et al. [6]. We denote the transformer\u2019s context length with $P$ , its hidden dimension with $d$ , the number of transformer blocks with $L$ , the vocabulary of the tokenizer with $\\mathcal{V}$ , and its size with $V$ . We present our approach for single-headed self-attention but it can be directly extended to multi-head self-attention, and we experimentally apply DAGER in this context. ", "page_idx": 2}, {"type": "text", "text": "Transformer Inputs We consider inputs batches consisting of $B$ sequences of tokens, where $n_{j}$ is the length of the $j^{\\mathrm{th}}$ batch element. Sequences with length $<n=\\operatorname*{max}_{j}(n_{j})$ are padded. We denote the total number of non-padding tokens in a batch with $\\begin{array}{r}{\\boldsymbol{b}=\\sum_{j=1}^{B}n_{j}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Token Embeddings The discrete input tokens are usually embedded via a function $f^{0}\\colon[V]\\times[P]\\to$ $\\mathbb{R}^{d}$ mapping a token\u2019s vocabulary index $v$ and its position $i$ in the sequence to an embedding vector $z^{i j}=\\overbar{f^{0}}(\\bar{v_{,}}\\,i)$ . These embeddings $z^{i j}$ are then stacked row-wise to form the input $Z_{1}\\in\\mathbb{R}^{b\\times d}$ to the first self-attention layer. Note that $f^{0}$ is known to the server, as it is part of the model. Further, while $f^{0}$ differs between models, typically it maps token indices to embedding vectors before optionally adding a positional encoding and applying a LayerNorm. Crucially, $f^{0}$ is applied per-token. ", "page_idx": 2}, {"type": "text", "text": "Self-Attention The stacked embeddings $Z_{1}$ are then passed through a series of self-attention layers. We denote the input to the $l^{\\mathrm{th}}$ self-attention layer as $\\dot{\\boldsymbol{Z}}_{l}\\in\\mathbb{R}^{b\\times d}$ , for $1\\leq l\\leq L$ . A self-attention layer is a combination of three linear layers: The query $\\pmb{Q}_{l}=\\pmb{Z}_{l}\\pmb{W}_{l}^{Q}$ , key ${\\pmb K}_{l}={\\pmb Z}_{l}{\\pmb W}_{l}^{K}$ , and value $V_{l}=Z_{l}W_{l}^{V}$ layer, which are then combined to compute the self-attention output: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{attention}(Q_{l},{\\cal K}_{l},{\\cal V}_{l})=\\mathrm{softmax}\\left({\\cal M}\\odot\\frac{Q_{l}{\\cal K}_{l}^{T}}{\\sqrt{d}}\\right){\\cal V}_{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $_M$ is the binary self-attention mask, $\\odot$ is the element-wise product, and the softmax is applied row-wise. $_M$ is chosen to ensure that padding tokens do not affect the layer\u2019s output. Further, for decoders, $_M$ ensures that only preceeding tokens are attended. For notational convenience, we denote as $f_{i}^{l}\\colon\\mathcal{V}^{P}\\to\\mathbb{R}^{d}$ the function that maps any sequence of input tokens to the $i^{\\mathrm{th}}$ input embedding at the $1\\leq l\\leq L$ self-attention layer. Note that $f_{i}^{l}$ is part of the model and, thus, known to the attacker. ", "page_idx": 3}, {"type": "text", "text": "3.2 Low-Rank Decomposition of Self-Attention Gradients ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For a linear layer $\\pmb{Y}=\\pmb{X}\\pmb{W}+(\\pmb{b}|\\leq...\\,|\\pmb{b})^{T}$ with a weight matrix $W\\in\\mathbb{R}^{n\\times m}$ , a bias $\\pmb{b}\\in\\mathbb{R}^{m}$ , and batched inputs $\\pmb{X}\\in\\mathbb{R}^{b\\times n}$ and outputs $\\pmb{Y}\\in\\mathbb{R}^{b\\times m}$ , Dimitrov et al. [28] show that: ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Adapted from Dimitrov et al. [28]). The network\u2019s gradient w.r.t. the weights $W$ can be represented as the matrix product: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}}=\\pmb{X}^{T}\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{Y}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Further, when the batch size $b\\leq n,m_{\\ast}$ , the rank of $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial W}}$ is at most $b$ . ", "page_idx": 3}, {"type": "text", "text": "The rank limit follows directly from the dimensionalities of $\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{Y}}\\in\\mathbb{R}^{b\\times m}$ and $\\b X\\in\\mathbb{R}^{b\\times n}$ in Eq. 1. In this work, we apply Theorem 3.1 to the linear projection matrices $\\pmb{W}_{l}^{\\{Q,K,V\\}}\\in\\mathbb{R}^{d\\times d}$ . As long as the total number of tokens $b<d$ , it states that the gradients \u2202W lQ ,\u2202W lK , and $\\frac{\\partial\\mathcal{L}}{\\partial{W_{l}^{V}}}$ are rank-deficient. Without loss of generality, for the rest of the paper we use $\\begin{array}{r}{\\frac{\\partial\\boldsymbol{\\Dot{\\mathcal{L}}}}{\\partial\\mathbf{W}_{l}^{Q}}=Z_{l}^{T}\\frac{\\partial\\boldsymbol{\\mathcal{L}}}{\\partial\\mathbf{Q}_{l}}}\\end{array}$ to explain our method. ", "page_idx": 3}, {"type": "text", "text": "4 Overview of DAGER ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we provide a high-level overview of our method DAGER, illustrated in Fig. 1. DAGER is an attack that recovers the client input sequences from the shared gradients of a transformer-based LLM. DAGER works for both encoder and decoder-based LLMs, however, for simplicity here we focus on decoder-only LLMs. While, in theory, one could enumerate all possible batches of input sequences, and check whether they produce the desired gradients, this is infeasible in practice as it requires computing $\\stackrel{\\cdot}{V}^{P\\times B}$ dif", "page_idx": 3}, {"type": "image", "img_path": "CrADAX7h23/tmp/14f37854b8fcfdc5914c499042127a9d78c366b58feb99e409a6aa2437c9cdfb.jpg", "img_caption": ["Figure 1: Overview of DAGER. DAGER first recovers the sets of client tokens $\\boldsymbol{\\mathcal{T}}_{i}^{*}$ at each position $i\\in\\mathcal{P}$ by testing each token in the vocabulary $\\nu$ via a span check based on the client gradients of the first self-attention. Then it recursively combines them into partial client sequences $\\boldsymbol{S}_{i}$ with length up to $i$ , flitered to obtain the correct sequences $S_{i}^{*}$ via the gradients of the second self-attention. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "ferent gradients. We reduce the search space by leveraging the rank-deficiency of $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}$ , discussed in Sec. 3.2, combined with the finite number of possible inputs to each self-attention corresponding to one of the V P \u00d7B gradients above. For the rest of the section, we assume rank-deficiency of $\\frac{\\bar{\\partial\\mathcal{L}}}{\\partial{\\pmb W}_{l}^{Q}}$ that is $b<d$ . This assumption is in practice satisfied for reasonable input lengths and batch sizes. ", "page_idx": 3}, {"type": "text", "text": "Leveraging the Rank Deficiency As the gradient matrix $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}$ is rank-deficient, i.e. $b\\,<\\,d$ , the columns of $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}$ form a subspace of $\\mathbb{R}^{d}$ of dimension $b$ . Further, under mild assumptions (see Theorem 5.1), the embedding vectors forming $Z_{l}$ are linear combinations of the columns of \u2202W lQ = ZlT\u2202\u2202QLl . It is unlikely that any incorrect embedding vector, part of one of the V P \u00d7B incorrect inputs $Z_{l}$ , is part of colspan $(\\frac{\\partial\\mathcal{L}}{\\partial W_{l}^{Q}})\\subset\\mathbb{R}^{d}$ , as the hypervolume of this subspace is $0$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Filtering Incorrect Embeddings We can efficiently fliter out all incorrect client embeddings at any layer $l$ without computing their gradient, simply by checking if they are in colspan $\\big(\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}\\big)$ . However, applying this procedure naively still requires us to check all $V^{P\\times B}$ different client batches. Instead, we leverage this flitering in a two-stage recovery algorithm that first recovers the client tokens $\\boldsymbol{\\mathcal{T}_{i}^{*}}$ at position $i$ using the rank deficiency of $\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}_{1}^{Q}}$ (Token Recovery in Fig. 1), and then recovers the client batch of sequences $S^{*}$ based on $\\boldsymbol{\\mathcal{T}}_{i}^{*}$ and the rank deficiency of $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{2}^{Q}}$ (Sequence Recovery in Fig. 1). ", "page_idx": 4}, {"type": "text", "text": "Token Recovery Our token recovery method relies on the observation that $f^{0}$ is computed pertoken. Therefore, the input embeddings in $Z_{1}$ are always part of the set $\\{f^{0}(v,\\dot{i})|v\\in[V],i\\in[\\dot{P}]\\}$ . We apply our span check above to this set for the first layer gradients $\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}_{1}^{Q}}$ to filter the incorrect embeddings and their corresponding client tokens $v$ at position $i$ , thus, constructing the set of correct client tokens $\\boldsymbol{\\mathcal{T}_{i}^{*}}$ at position $i$ . ", "page_idx": 4}, {"type": "text", "text": "Sequence Recovery In our sequence recovery, we leverage the fact that $f_{i}^{1}$ is computed persequence and that the decoder mask $_M$ ensures that the second layer input embeddings at position $i$ do not depend on tokens with position $>i$ , i.e., $f_{i}^{1}(s_{1},\\dots,s_{P})\\stackrel{\\cdot}{=}f_{i}^{1}(\\dot{s_{1}},\\dots,s_{i})$ , for any sequence of tokens $s_{1},\\ldots,s_{P}$ . Crucially, for a correct client partial sequence $s_{1},\\dotsc,s_{i-1}$ of length $i-1$ this allows us to find the correct next token in $\\boldsymbol{\\mathcal{T}_{i}^{*}}$ by simply extending it with all possible token $\\overline{{s}}_{i}\\in T_{i}^{*}$ and then checking which of the resulting embedding vectors $f_{i}^{1}{\\overline{{(s_{1},\\ldots,s_{i-1}},{\\overline{{s}}}_{i})}}$ is correct, i.e, is in $\\mathrm{colspan}(\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}_{2}^{Q}})$ . We apply this procedure iteratively starting with the single token sequences $S_{1}^{*}=\\tau_{1}^{*}$ , extending them one token at a time to produce the partial sequence reconstructions $S_{i}^{*}$ , until the sequences cannot be extended anymore and return the result. ", "page_idx": 4}, {"type": "text", "text": "5 DAGER: Exact Sequence Recovery for Transformers ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present the technical details of DAGER. Specifically, we first theoretically derive of our filtering procedure based on the rank-deficiency of\u2202\u2202WL Q in Sec. 5.1. We then describe how we apply it on the gradients of the first and second self-attention layers to respectively recover the client tokens (Sec. 5.2) and sequences (Sec. 5.3). ", "page_idx": 4}, {"type": "text", "text": "5.1 Efficient Embedding Filtering ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Below, we discuss the technical details of our filtering procedure, outlined in Sec. 4, and prove its correctness. We first show that, under mild assumptions, the embedding vectors forming $Z_{l}$ are linear combinations of the columns o f\u2202\u2202WL Q , restating this in terms of rowspan(Zl) and colspan(\u2202 $\\big(\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}\\big)$ : ", "page_idx": 4}, {"type": "text", "text": "Theorem 5.1. If $b~<~d$ and the matrix $\\frac{\\partial\\mathcal{L}}{\\partial Q_{l}}$ is of full rank (rank b), then rowspan $\\left(Z_{l}\\right)\\;=\\;$ colspan $\\big(\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}\\big)$ . ", "page_idx": 4}, {"type": "text", "text": "Note that the assumption that\u2202\u2202QLl is full-rank holds in practice, as shown empirically in Dimitrov et al. [28], and that further $b<d$ is almost always satisfied, i.e., that the total number of tokens in the input is smaller than the internal dimensionality of the model, for practical LLMs. We discuss the assumptions in further detail in App. B.2. The latter then directly implies the rank-deficiency of \u2202\u2202WL Q , which we leverage to show: ", "page_idx": 4}, {"type": "text", "text": "Theorem 5.2. When $b<d$ , the probability of a random vector $\\in\\mathbb{R}^{d}$ to be part of $\\mathrm{colspan}(\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}_{l}^{Q}})$ is almost surely 0. ", "page_idx": 4}, {"type": "text", "text": "vCeoctmobr tnhga tT ihse poarret mosf  5t.h1e  aclnide n5t. 2s,e lwf-ea tatrernitvieo ant  ionupru tms rbeesluolnt gsst attoi ncgo ltshpata, n $b<d$ ,,  awnh eilme braendddionmg $_{\\textit{z}}$ $Z_{l}$ $\\big(\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}\\big)$ embedding vectors that are not part of almost surely do not. ", "page_idx": 5}, {"type": "text", "text": "Span Check Implementation While the above result holds under real, i.e., infinite precision, arithmetic, for our method to work in practice, we require an implementation that is both fast and robust to numerical errors caused by floating-point arithmetic. We, therefore, introduce the metric $d$ , $_{\\textit{z}}$ $\\mathrm{colspan}(\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}_{l}^{Q}})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d(z,l)=\\Vert z-\\mathrm{proj}(z,\\mathrm{colspan}(\\frac{\\partial\\mathcal{L}}{\\partial W_{l}^{Q}}))\\Vert_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Intuitively, the closer $d(z,l)$ is to 0, the more likely $_{\\textit{z}}$ is part of the span. To allow for efficient computation of the projection in Eq. 2, we first pre-compute an orthonormal basis for $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}$ using an SVD and truncating the eigenvalues below a chosen threshold $\\tau_{l}^{\\mathrm{rank}}$ . We can then trivially compute tchoilss ppraonj ,a si ft thhee s duimst aonf cper tios  ebaecloh wb aas icsh voescetno rp. erF-ilnaaylelry ,t hwree sshaoyl tdh . a vector $_{z}$ is in $\\big(\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}\\big)$ $d(z,l)<\\tau_{l}$ $\\tau_{l}$ ", "page_idx": 5}, {"type": "text", "text": "5.2 Recovering Token Sets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now describe how DAGER leverages the above filtering procedure to recover the input tokens exactly. To this end, we consider the set of all tokens in the model\u2019s vocabulary $v\\in[V]$ at every possible position $i\\ \\in\\ [P]$ and compute their input embeddings at the first layer via the per-token embedding function $f^{0}$ . We then filter out token-position tuples $(v,i)$ whose embedding vectors $f^{0}(v,\\dot{v})$ do not lie in $\\mathrm{colspan}\\big(\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}_{1}^{Q}}\\big)$ n(\u2202\u2202WL Q ) to obtain the set input tokens (across batch elements) at position $i$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nT_{i}^{*}=\\{v\\in[V]\\mid d(f^{0}(v,i),1)<\\tau_{1}\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "CrADAX7h23/tmp/4561ea0b7d1cc21d81cc20fa24ef9904fe771aa4246972fa01ca2359ffca37b6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "We formalize this process in Algorithm 1, where we simply enumerate all token position tuples $(v,i)$ . Additionally, we compute the length of the longest input sentence $n$ as the largest position $i$ of any recovered tuple $(v,i)$ (Line 7). If $f^{0}$ is position-independent, e.g. when rotary instead of absolute positional embeddings are used, we recover the set of all input tokens $\\tau^{*}=\\cup_{i}\\bar{\\tau}_{i}^{*}$ for every position. Our algorithm handles this at the sequence recovery stage at the price of slig htly higher computational costs (see Theorem B.4). ", "page_idx": 5}, {"type": "text", "text": "While conceptionally simple, this approach is exceptionally effective and robust to the distance threshold $\\tau_{1,2}$ , as we demonstrate in Fig. 2 for the GPT-2 model [12] $d=768)$ ) and a batch of $B\\,=\\,32$ sequences consisting of $b\\,=\\,391$ tokens. Despite the total number of tokens $b$ exceeding half the model dimensionality $d$ , even our single layer (L1) flitering approach narrows the set of possible starting tokens, which are independent of the rest of the input, down to less than 300 from GPT-2\u2019s vocabulary of $V\\approx50K$ across a wide range of thresholds. Adding a second filtering stage using second-layer flitering, described next, allows DAGER Figure 2: Effect of L1 and L2 Filtering to recover the exact 32 starting tokens. This is used to exactly narrow down the first token for each sequence, allowing us to inductively reconstruct the whole sentence for decoder-only models. ", "page_idx": 5}, {"type": "image", "img_path": "CrADAX7h23/tmp/fdb4e893f673144714eeb24b263bbe0a640ce1f6583fbecf4f8ce13cf9e267db.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "5.3 Recovering Sequences ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given the set of input tokens, recovered above, we now describe how to recover input sequences by applying our filtering procedure to the inputs of the second self-attention layer $Z_{2}$ . We first define the set $\\bar{\\boldsymbol{S}}=\\boldsymbol{T}_{1}^{*}\\times\\dot{\\boldsymbol{\\cdots}}\\times\\boldsymbol{T}_{P}^{*}$ of all sequences formed using the recovered token sets $\\boldsymbol{\\mathcal{T}}_{i}^{*}$ . As the ", "page_idx": 5}, {"type": "text", "text": "second layer input embeddings $z_{2}=f^{1}(\\pmb{s})$ are computed independently for each sequence $\\pmb{s}$ , one can naively enumerate all $s\\in S$ , compute their second layer embedding vectors $f^{1}(\\pmb{s})$ and apply the span check for every position $i$ to obtain the true set of client sequences: ", "page_idx": 6}, {"type": "equation", "text": "$$\nS^{*}=\\{s\\in S\\mid d(f_{i}^{1}(s),2)<\\tau_{2},\\;\\forall i\\in[P]\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Unfortunately, this naive approach requires $\\mathcal{O}(B^{P})$ span checks. To alleviate this issue, we first show that the causal attention mask of decoder architectures allows us to greedily recover the exact sequences in polynomial time, before discussing heuristics that make an exhaustive search tractable for encoderbased architectures. ", "page_idx": 6}, {"type": "text", "text": "Recovering Decoder Sequences Due to the causal attention mask $_M$ in decoder architectures, the $i^{\\mathrm{th}}$ input of the second-self attention layer $f_{i}^{1}(s)$ depends only on the first $i$ tokens of the input sequence $\\pmb{s}$ . We can thus apply a span check on the results of $f_{i}^{1}$ to check arbitrary sequence prefixes of length $i$ . We leverage this insight in Algorithm 2 to iteratively recover the sets $S_{i}^{*}$ (Line 10) of input sequence prefixes ", "page_idx": 6}, {"type": "table", "img_path": "CrADAX7h23/tmp/187128775a077f6f4ea2a1db7d683ef0a7269b80c0a7ed621bc225bfe8326b5d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "equation", "text": "$$\nS_{i}^{*}=\\{s\\in S_{i-1}^{*}\\times T_{i}^{*}\\mid d(f_{i}^{1}(s),2)<\\tau_{2}\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "starting from the set of empty sequences ${\\mathcal S}_{0}$ and extending them one token at a time (Line 6) until none of our sequences can be extended any further (Line 12). ", "page_idx": 6}, {"type": "text", "text": "For models with a small internal dimension $d$ , or batches with a large number of total tokens $b$ , the weight gradient $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}^{Q}}$ might become full rank and all embeddings would pass our span check. To avoid this we set a maximum rank threshold, $\\tilde{b}=\\operatorname*{min}(b,d-\\Delta_{b})$ for the orthonormal basis computed via SVD (See Sec. 5.1). We visualize the effect of different $\\Delta_{b}$ on GPT-2, in Fig. 3, and observe that $\\Delta_{b}=20$ offers the best trade-off between stability and accuracy, yielding almost perfect reconstruction even for very large inputs with $b$ very close to $d$ . ", "page_idx": 6}, {"type": "image", "img_path": "CrADAX7h23/tmp/5c3c59b986a790391a2697ac3720f32e2d645ed8ac6a19a2ab1564311ca2e8bb.jpg", "img_caption": ["Figure 3: Encoder Ablation Study "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Recovering Encoder Sequences For encoders, all secondlayer embeddings $f_{i}^{1}(\\pmb{s})$ depend on all input tokens. We thus cannot use the greedy reconstruction discussed above but have to enumerate all sequences in $\\boldsymbol{S}$ . To make this search tractable, we leverage the following heuristics. We can determine the positions $i$ of end-of-sequence (EOS) tokens in the input to determine the input sequence lengths $n_{j}$ . This allows us to recover input sequences by increasing length and eliminate the tokens constituting the recovered sequences from the token sets $\\boldsymbol{\\mathcal{T}}_{i}^{*}$ . Additionally, we truncate the proposal token sets $\\boldsymbol{\\mathcal{T}}_{i}^{*}$ to the batch size $B$ token closest to $\\mathrm{colspan}\\big(\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}_{1}^{Q}}\\big)$ . Finally, we ", "page_idx": 6}, {"type": "image", "img_path": "CrADAX7h23/tmp/491f6e37784d031a370e004cb712529d42c1876d3d9f9870c8ddc28e221fbca5.jpg", "img_caption": ["Figure 4: Encoder Ablation Study "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "$\\boldsymbol{S}$ $\\mathrm{colspan}(\\frac{\\partial\\mathcal{L}}{\\partial W_{2}^{Q}})$ We demonstrate the effectiveness of our heuristics in Fig. 4 for the base BERT model and different batch sizes $B$ and note that for $B=1$ we can still recover inputs perfectly, as $|\\mathcal{T}_{i}^{*}|=1$ . We provide more details on DAGER for encoder-architectures in App. B.6. ", "page_idx": 6}, {"type": "text", "text": "6 Experimental Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now describe our extensive experimental evaluation of DAGER. Our results demonstrate significant performance improvements compared to prior methods, on a variety of settings. We also present ablation studies for isolating the effects of each DAGER component. ", "page_idx": 7}, {"type": "text", "text": "6.1 Experimental Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate DAGER on both encoder- and decoder-based models including BERT [14], GPT-2 [12], and variations of LLaMa [13, 37]. We consider three sentiment analysis datasets \u2013 CoLA [15], SST2 [16], and Rotten Tomatoes (RT) [17], featuring sequences of varying lengths between typically 4 and 27 words. Additionally, we consider the ECHR [18] dataset, which contains sentences exceeding 1000 words to demonstrate the scalability of our approach in sequence length. We provide a more detailed description of our architectures and datasets in App. C. Following previous work, we report the mean and error of the ROUGE-1/2 [38] scores, i.e., the overlap rate of unigrams and bigrams, respectively, over 100 batches, excluding padding tokens. We report the error as the $95\\%$ confidence interval given by twice the standard error. Wherever we cannot assume normality of the mean\u2019s distribution, we estimate the interval by generating $10\\,000$ random samples by bootstrapping. Additional details regarding computational requirements and hyperparameters can be found in App. C. ", "page_idx": 7}, {"type": "text", "text": "Comparison against baselines First, we compared our performance against the state-of-the-art algorithms TAG [10] and LAMP [11] with batch sizes ranging between $B=1$ and $B=8$ . We run the two attacks for 2500 iterations per experiment, making use of the hyperparameters described in their respective studies. As Balunovic\u00b4 et al. [11] provide two variations of the LAMP algorithm based on different objective functions \u2014 $\\mathrm{LAMP}_{L2}+L1$ and $\\mathrm{LAMP}_{\\mathrm{Cos}}$ , we only report results from the variation with the higher ROUGE-1 score. In Table 2, we show results on GPT-2BASE and BERTBASE, assessing the performance on decoder-based and encoder-based models respectively. ", "page_idx": 7}, {"type": "text", "text": "The results indicate that for decoder-based models, such as GPT2, DAGER achieves near-perfect reconstructions across all datasets and batch sizes, significantly outperforming the baseline algorithms in every setting. Importantly, as further elaborated in App. C.1, DAGER achieves that while also being significantly more efficient \u2014 100 batches of size 8 on RT took 3.5 hours vs TAG and LAMP which required $\\approx10$ and 50 hours, respectively. Additionally, we confirm the claims made by Balunovi\u00b4c et al. [11] that LAMP outperforms TAG in the majority of settings. Examples of reconstructed sentences can be seen in Table 9 in App. C.3. We note that while we observe non-perfect ROUGE-2 scores on the SST-2 dataset, this is entirely due to an artifact of our metric library that assigns ROUGE-2 score of 0 to the SST-2\u2019s single-word sequences. We kept this behaviour to avoid having to rerun the baseline experiments, that also relied on this. ", "page_idx": 7}, {"type": "text", "text": "Further, Table 2 shows a significant improvement over prior work on encoder-based models like BERT, with near-perfect reconstruction for $B=1,2$ , and an average of $43\\%$ more tokens recovered for larger batch sizes. A significant advantage of DAGER over the baselines is its ability to more accurately recover the sentence structure, as evidenced by the much higher ROUGE-2 scores. ", "page_idx": 7}, {"type": "text", "text": "Main experiments While prior attacks\u2019 performances become very poor for batch sizes as little as 8, we now demonstrate that DAGER is only limited by the embedding dimension of the model. To this end, in Table 3 we compare two decoder-only models, $\\mathrm{GPT}{\\mathrm{-}}2_{\\mathrm{BASE}}$ with $d=768$ , and LLaMa-2 7B with $d=4096$ on $B$ as large as 128. ", "page_idx": 7}, {"type": "text", "text": "The results are consistent with our claims that DAGER produces almost perfect reconstructions in all cases when the total number of client tokens is not extremely close to the embedding dimension $d$ Further, while on LLaMa-2 DAGER achieves near-perfect reconstructions even up to a batch size of 128, for GPT-2 DAGER shows partial or complete failure for $B=64$ , 128. This suggests that despite the significant computational costs of $>2$ hours per batch for $B=128$ on LLaMa-2, larger models have the potential to leak significantly more information. This is especially concerning given the current trend of ever-increasing model sizes. Finally, we observe the effect of attempting a best-effort reconstruction by establishing a rank threshold, as described in Sec. 5.3, when the gradients are of full rank. This allows DAGER to achieve a ROUGE-1 score of 30.3 (instead of 0) for GPT-2 on CoLA $B=128$ . A thorough ablation study on the advantage of this heuristic can be found in App. C.2. ", "page_idx": 7}, {"type": "table", "img_path": "CrADAX7h23/tmp/62fb00f2c31cbac10b17003bea9dec89f9c4b1ccf4c232a72f667aed64c4995f.jpg", "table_caption": ["Table 2: Comparison of sequence reconstruction from gradients between DAGER and the baseline algorithms TAG and LAMP on various batch sizes and datasets. R-1 and R-2 denote the ROUGE-1 and ROUGE-2 scores respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "CrADAX7h23/tmp/b0026a10bf186ad5d562f95aa0b85c1c65710fb7be9a5ba1f98bfd57630ee945.jpg", "table_caption": ["Table 3: Main experiments on the GPT-2BASE and LLaMa-2 (7B) models with higher batch sizes on various datasets. R-1 and R-2 denote the ROUGE-1 and ROUGE-2 scores respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Reconstruction under FedAvg The FedAvg algorithm [19] is among the most widely used protocols in federated learning. It features $E$ training epochs on minibatches of size $B_{m i n i}<B$ with a fixed learning rate $\\eta$ . Despite featuring multiple low-rank gradient updates, this setting it remains vulnerable to our attack, as we elaborate in App. B.3. We show in Table 4 that FedAvg is susceptible to gradient leakage under DAGER for a range of reasonable learning rates and number of epochs. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Experiments on the FedAVG setting on the GPT-2 model with a batch size of 16 on the Rotten Tomatoes dataset. We use default set of hyperparameters of $E=10$ epochs, learning rate $\\eta=10^{-4}$ and mini-batch size $B_{m i n i}=4$ . R-1 and R-2 denote ROUGE-1 and ROUGE-2 respectively. ", "page_idx": 8}, {"type": "table", "img_path": "CrADAX7h23/tmp/0bfd2dbf70071db2da4ae321856b986bc833b2735e1bda045b2e9fe6ea1ade6f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Effect of Fine-tuning Methods We further demonstrate DAGER\u2019s versatility across a range of pretraining paradigms, including quantized models and Low-Rank Adaptation (LoRA) [20] finetuning. For both LLaMa-3 70B with $B\\,=\\,1$ and LLaMa-3.1 8B with $B\\,=\\,32$ at 16-bit quantization, we observed excellent ROUGE-1 and ROUGE-2 scores $(>\\!99\\%)$ (see Table 11 in App. C.5). We also present near-exact reconstructions under LoRA training, as DAGER can be directly applied to the decomposed weight matrix, with further technical specifics detailed in App. B.4. With LoRA updates of rank $r=256$ , which is standard for the LLaMa-2 model as noted by Biderman et al. [39], we observe ROUGE-1 and ROUGE-2 scores in the region of $94-95\\%$ , given in Table 11. These results reaffirm that DAGER is applicable to common fine-tuning methods. ", "page_idx": 8}, {"type": "text", "text": "Effect of Model Size and Training on Reconstruction Prior work [11, 10] suggests that the size of a model, as well as, the degree of pre-training significantly affects the amount of leaked client information. To this end, in Table 5 we evaluate DAGER on the larger (GPT-2LARGE) and ", "page_idx": 8}, {"type": "table", "img_path": "CrADAX7h23/tmp/ff06095e75df3b522e169e1ce36547f2512ec93b5405b9e9a0244ed13ba640e0.jpg", "table_caption": ["Table 5: Experiments on GPT-2 variations in different settings on the Rotten Tomatoes dataset. R-1 and R-2 denote the ROUGE-1 and ROUGE-2 scores respectively. "], "table_footnote": ["pre-trained for 2 epochs (GPT-2FineTuned) variants of GPT-2 on the RT dataset for batch sizes up to 128. We observe very little difference in performance. In fact, the GPT-2LARGE\u2019s larger embedding dimension allows us to approximately reconstruct more tokens at larger batch sizes. We further note that a larger vocabulary does not negatively impact DAGER, as can be seen from the applications on LLaMa3.1-8B and LLaMa3.1-70B (see Table 11), which feature a vocabulary size of 128,256 tokens. "], "page_idx": 9}, {"type": "text", "text": "Reconstruction under Next-Token Prediction Additionally, we evaluate our model on the nexttoken prediction task to demonstrate DAGER\u2019s efficacy under different contexts. We again achieve near-perfect results with ROUGE- $_{1/2}$ scores of $>99$ . DAGER does not reach perfect scores because the last token in each client sequence only acts as a target and it is, thus, masked out from the input. ", "page_idx": 9}, {"type": "text", "text": "Reconstruction of Long Sequences Finally, to demonstrate our robustness to long sequences, we conducted a single experiment with $B=1$ on the ECHR dataset truncated to 512 tokens. We obtain a perfect score of ${\\bf100.0\\pm0.0}$ for ROUGE-1 and ROUGE-2 on GPT-2BASE, emphasizing the general applicability of DAGER. In contrast, in the same setting LAMP achieves a ROUGE-1 of $10.1\\pm2.3$ . ", "page_idx": 9}, {"type": "text", "text": "7 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As discussed in Sec. 5 and demonstrated in Sec. 6, the performance of DAGER on decoder-based models is only constrained by the embedding dimension $d$ . While an exact reconstruction for a number of tokens $b>d$ is unachievable, we showed that the attack\u2019s effectiveness decreases only gradually with $b$ . Given our robust performance in an undefended setting, an interesting avenue for future work is to improve DAGER against different defense mechanisms, including but not limited to using the Differential Privacy SGD optimization process (DPSGD)[40]. ", "page_idx": 9}, {"type": "text", "text": "On the other hand, applying DAGER on encoder-based architectures for larger batches $(B>>8)$ ) becomes challenging due to the high-order polynomial growth of the search space volume with respect to the batch size. These computational constraints make comprehensive exploration of the search space nearly impossible, thereby reducing the likelihood of achieving a feasible reconstruction. This issue extends to longer sequences, where the size of the search space expands exponentially with the maximum sequence length. To mitigate these effects, we propose that future research could focus on exploring further heuristics to efficiently reduce the search space. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced DAGER, the first gradient inversion attack for transformers able to recover large batches of input text exactly. By exploiting the rank-deficiency of self-attention layer gradients and discreteness of the input space, we devised a greedy algorithm and a heuristic search approach for decoder-based and encoder-based architectures, respectively. Our results show that DAGER achieves exact reconstruction for batch sizes up to 128 and sequences up to 512 tokens. We further demonstrate DAGER\u2019s effectiveness across model sizes, architectures, degrees of pre-training, and federated learning algorithms, establishing the widespread applicability of our attack. ", "page_idx": 9}, {"type": "text", "text": "Our work demonstrates that recent decoder-based LLMs are particularly vulnerable to data leakage, allowing adversaries to recover very large batches and sequences in the absence of a robust defense mechanism. This underlying vulnerability highlights the need for increased awareness and development of effective countermeasures in privacy-critical applications. We hope this paper can facilitate further research into creating reliable frameworks for effective and private collaborative learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was partially funded by the Ministry of Education and Science of Bulgaria (support for INSAIT, part of the Bulgarian National Roadmap for Research Infrastructure). ", "page_idx": 10}, {"type": "text", "text": "This work has been done as part of the EU grant ELSA (European Lighthouse on Secure and Safe AI, grant agreement no. 101070617) . Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or European Commission. Neither the European Union nor the European Commission can be held responsible for them. ", "page_idx": 10}, {"type": "text", "text": "The work has received funding from the Swiss State Secretariat for Education, Research and Innovation (SERI). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Yu, Guoyin Wang, and Yiran Chen. Towards building the federatedgpt: Federated instruction tuning. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6915\u20136919. IEEE, 2024.   \n[2] Chaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, and Xiaolin Zheng. Federated large language model: A position paper. arXiv preprint arXiv:2307.08925, 2023.   \n[3] Sara Babakniya, Ahmed Roushdy Elkordy, Yahya H Ezzeldin, Qingfeng Liu, Kee-Bong Song, Mostafa El-Khamy, and Salman Avestimehr. Slora: Federated parameter efficient fine-tuning of language models. arXiv preprint arXiv:2308.06522, 2023.   \n[4] Zhuo Zhang, Xiangjing Hu, Jingyuan Zhang, Yating Zhang, Hui Wang, Lizhen Qu, and Zenglin Xu. Fedlegal: The first real-world federated learning benchmark for legal nlp. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3492\u20133507, 2023.   \n[5] Adam Sadilek, Luyang Liu, Dung Nguyen, Methun Kamruzzaman, Stylianos Serghiou, Benjamin Rader, Alex Ingerman, Stefan Mellem, Peter Kairouz, Elaine O Nsoesie, et al. Privacy-first health research with federated learning. NPJ digital medicine, 4(1):132, 2021. [6] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In NeurIPS, 2019.   \n[7] Jonas Geiping, Hartmut Bauermeister, Hannah Dr\u00f6ge, and Michael Moeller. Inverting gradientshow easy is it to break privacy in federated learning? NeurIPS, 2020.   \n[8] Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M. Alvarez, Jan Kautz, and Pavlo Molchanov. See through gradients: Image batch recovery via gradinversion. In CVPR, 2021.   \n[9] Zhuohang Li, Jiaxin Zhang, Luyang Liu, and Jian Liu. Auditing privacy defenses in federated learning via generative gradient leakage. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10132\u201310142, 2022.   \n[10] Jieren Deng, Yijue Wang, Ji Li, Chenghong Wang, Chao Shang, Hang Liu, Sanguthevar Rajasekaran, and Caiwen Ding. Tag: Gradient attack on transformer-based language models. In Conference on Empirical Methods in Natural Language Processing, 2021.   \n[11] Mislav Balunovi\u00b4c, Dimitar I. Dimitrov, Nikola Jovanovi\u00b4c, and Martin T. Vechev. LAMP: extracting text from gradients with language model priors. In NeurIPS, 2022.   \n[12] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[13] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[15] Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. arXiv preprint 1805.12471, 2018.   \n[16] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew $\\mathrm{Ng}$ , and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP, pages 1631\u20131642, 2013.   \n[17] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL, 2005.   \n[18] Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. Neural legal judgment prediction in English. In Anna Korhonen, David Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4317\u20134323, July 2019.   \n[19] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[21] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2704\u20132713, 2018.   \n[22] Le Trieu Phong, Yoshinori Aono, Takuya Hayashi, Lihua Wang, and Shiho Moriai. Privacypreserving deep learning via additively homomorphic encryption. IEEE Trans. Inf. Forensics Secur., (5), 2018.   \n[23] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. idlg: Improved deep leakage from gradients. arXiv, 2020.   \n[24] Jiahui Geng, Yongli Mou, Feifei Li, Qing Li, Oya Beyan, Stefan Decker, and Chunming Rong. Towards general deep leakage in federated learning. arXiv, 2021.   \n[25] Samyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, and Danqi Chen. Recovering private text in federated learning of language models. In NeurIPS, 2022.   \n[26] Jiahao Lu, Xi Sheryl Zhang, Tianli Zhao, Xiangyu He, and Jian Cheng. April: Finding the achilles\u2019 heel on privacy for vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10051\u201310060, 2022.   \n[27] Junyi Zhu and Matthew B. Blaschko. R-GAP: recursive gradient attack on privacy. In ICLR, 2021.   \n[28] Dimitar I Dimitrov, Maximilian Baader, Mark Niklas M\u00fcller, and Martin Vechev. Spear: Exact gradient inversion of batches in federated learning. arXiv preprint arXiv:2403.03945, 2024.   \n[29] Franziska Boenisch, Adam Dziedzic, Roei Schuster, Ali Shahin Shamsabadi, Ilia Shumailov, and Nicolas Papernot. When the curious abandon honesty: Federated learning is not private. arXiv, 2021.   \n[30] Liam H. Fowl, Jonas Geiping, Wojciech Czaja, Micah Goldblum, and Tom Goldstein. Robbing the fed: Directly obtaining private data in federated learning with modified models. In ICLR, 2022.   \n[31] Liam Fowl, Jonas Geiping, Steven Reich, Yuxin Wen, Wojtek Czaja, Micah Goldblum, and Tom Goldstein. Decepticons: Corrupted transformers breach privacy in federated learning for language models. ICLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[32] Hong-Min Chu, Jonas Geiping, Liam H Fowl, Micah Goldblum, and Tom Goldstein. Panning for gold in federated learning: Targeted text extraction under arbitrarily large-scale aggregation. ICLR, 2023. ", "page_idx": 12}, {"type": "text", "text": "[33] Yuxin Wen, Jonas Geiping, Liam Fowl, Micah Goldblum, and Tom Goldstein. Fishing for user data in large-batch federated learning via gradient magnification. In ICML, 2022. ", "page_idx": 12}, {"type": "text", "text": "[34] Jianwei Li, Sheng Liu, and Qi Lei. Beyond gradient and priors in privacy attacks: Leveraging pooler layer inputs of language models in federated learning. arXiv preprint arXiv:2312.05720, 2023. ", "page_idx": 12}, {"type": "text", "text": "[35] Qiongkai Xu, Jun Wang, Olga Ohrimenko, and Trevor Cohn. Flat-chat: A word recovery attack on federated language model training. 2023. ", "page_idx": 12}, {"type": "text", "text": "[36] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag\u00fcera y Arcas. Communication-efficient learning of deep networks from decentralized data. In AISTATS, 2017. ", "page_idx": 12}, {"type": "text", "text": "[37] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aur\u00e9lien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi\u00e8re, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gr\u00e9goire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783. ", "page_idx": 12}, {"type": "text", "text": "[38] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381, 2004.   \n[39] Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns less and forgets less. arXiv preprint arXiv:2405.09673, 2024.   \n[40] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308\u2013318, 2016.   \n[41] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \n[42] Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, and Jianfeng Gao. Deep learning\u2013based text classification: a comprehensive review. ACM computing surveys (CSUR), 54(3):1\u201340, 2021.   \n[43] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR.   \n[44] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. ", "page_idx": 12}, {"type": "text", "text": "[45] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308\u2013318, 2016. ", "page_idx": 13}, {"type": "text", "text": "A Broader Impact ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this work, we demonstrate that it is possible to exactly reconstruct large batches of textual data from gradients in the honest-but-curious setting. Our findings are widely applicable across different transformer-based LLM architectures, showing that, in contrast to prior belief, language transformers are actually more susceptible to gradient leakage attacks than other architectures. While our work naturally substantially increases the privacy risks posed to federated learning clients training LLMs, we also believe that sharing our work is crucial for finding future solutions to the issues we uncover. ", "page_idx": 14}, {"type": "text", "text": "Importantly, we find that the recent decoder-based models are much more susceptible to gradient leakage attacks due to the causal nature of their self-attention masks. Our work implies that in the absence of proper defense mechanisms, receiving gradients from those models is essentially equivalent to receiving the client data directly. Further, we show that the attacker\u2019s ability to mount DAGER grows with the embedding size $d$ , suggesting the privacy risks posed by DAGER in practical settings will only grow over time. With these considerations in mind, we emphasize the importance of providing privacy safeguards via secure aggregation, larger batch sizes, or gradient perturbations. ", "page_idx": 14}, {"type": "text", "text": "B Additional Technical Details of Our Method ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Deferred Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 5.1. If b < d and the matrix \u2202\u2202QLl is of full rank (rank b), then rowspan $\\left(Z_{l}\\right)\\;=\\;$ colspan $\\big(\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}\\big)$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. We split the proof into two parts. We first prove c $\\begin{array}{r}{\\mathsf{\\rho})\\mathrm{lspan}(\\frac{\\partial\\mathcal{L}}{\\partial W_{l}^{Q}})\\subseteq\\mathrm{rowspan}(Z_{l})}\\end{array}$ and then prove that rank (\u2202\u2202WL Q ) = rank(Zl), thus implying the two spaces are the same. For the first part of the proof, we observe that due to the matrix multiplication in $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial W_{l}^{Q}}=Z_{l}^{T}\\frac{\\partial\\mathcal{L}}{\\partial Q_{l}}}\\end{array}$ all columns of $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}$ are linear combinations of the columns of $Z_{l}^{T}$ with coefficients given by $\\frac{\\partial\\mathcal{L}}{\\partial Q_{l}}$ . Thus, $\\begin{array}{r}{\\mathrm{colspan}(\\frac{\\partial\\dot{\\mathcal{L}}}{\\partial W_{l}^{Q}})\\subseteq\\mathrm{colspan}(Z_{l}^{T})=\\mathrm{rowspan}(Z_{l}).}\\end{array}$ For the second part of the proof, we observe that since\u2202\u2202QLl is of rank $b$ and $\\mathrm{rank}(Z_{l}^{T})$ is at most $b$ , $\\begin{array}{r}{\\operatorname{rank}(\\frac{\\partial\\mathcal{L}}{\\partial W_{l}^{Q}})=\\operatorname*{min}(\\operatorname{rank}(Z_{l}^{T}),\\operatorname{rank}(\\frac{\\partial\\mathcal{L}}{\\partial Q_{l}}))=\\operatorname{rank}(Z_{l}^{T})}\\end{array}$ . This finishes the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Theorem 5.2. When $b<d$ , the probability of a random vector $\\in\\mathbb{R}^{d}$ to be part of $\\mathrm{colspan}(\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}_{l}^{Q}})$ almost surely 0. ", "page_idx": 14}, {"type": "text", "text": "Proof. As $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}$ is rank-deficient (Theorem 3.1), $\\mathrm{colspan}(\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}_{l}^{Q}})$ is a strict linear subspace of $\\mathbb{R}^{d}$ Thus, it has a hypervolume 0 via the Sard\u2019s lemma. This directly implies that the probability of a random vector $\\in\\mathbb{R}^{d}$ to be part of colspan (\u2202\u2202WL Q) is almost surely 0. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B.2 Technical Assumptions of DAGER ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Below we present a brief commentary on the technical assumptions of DAGER. DAGER makes three assumptions: ", "page_idx": 14}, {"type": "text", "text": "\u2022 We assume that $\\frac{\\partial\\mathcal{L}}{\\partial Q_{l}}$ is full-rank.   \n\u2022 We require the total number of tokens $b$ in the batch to be smaller than the embedding dimension $d$ , ensuring that $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}$ is of low-rank.   \n\u2022 We assume a known discrete set of possible inputs to the model, i.e. its vocabulary. ", "page_idx": 14}, {"type": "text", "text": "Importantly, DAGER does not assume any prior knowledge of the labels or the lengths of each sequence in the batch nor access to the gradients of the embedding layers which have been shown to leak significant information [26]. Further, we require no language priors and operate under the honest-but-curious setting which does not allow malicious changes to model weights. Finally, sub-differentiability is sufficient for applying DAGER. ", "page_idx": 15}, {"type": "text", "text": "In practice, DAGER requires much fewer assumptions than existing works in the honest-but-curious setting while being successful in a variety of common LLM tasks, e.g., next-token prediction and sentence classification. While these tasks use the cross-entropy loss, DAGER can be applied to any loss function that non-trivially depends on every input token. This ensures the full-rankness of $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}$ ", "page_idx": 15}, {"type": "text", "text": "To confirm its generality, we apply DAGER with a Frobenius norm-based loss and ReLU activation functions. We use a custom loss function $\\mathcal{L}(s_{1},s_{2},...,s_{P})=\\Vert f^{L}(s_{1},s_{2},...,s_{P})\\Vert_{F}.$ , where $\\|.\\|_{F}$ is the Frobenius norm, which is equivalent to an MSE loss with 0 as a target vector. We report the results of applying DAGER with these modifications on Rotten Tomatoes using GPT-2 with $B=16$ in Table 11, achieving ROUGE-1 and ROUGE-2 scores of ${>}99\\%$ in both cases. ", "page_idx": 15}, {"type": "text", "text": "B.3 DAGER under the FedAvg protocol ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We establish that DAGER can be effectively used to attack clients employing FedAvg updates under mild assumptions. First, we show that it is possible to theoretically apply Theorem 5.1 to the protocol directly on the first layer, and under reasonably low model updates for further layers. We then demonstrate experimentally that DAGER can be successfully applied across a wide range of parameters, namely the number of epochs $E$ , learning rate $\\eta$ and mini-batch size $b_{m i n i}$ . ", "page_idx": 15}, {"type": "text", "text": "Spanchecks for FedAvg updates Let $X_{l.}^{e}=f^{l}(s_{1}^{e},s_{2}^{e},\\ldots,s_{b_{m i n i}}^{e})$ denote the input embedding vectors to the relevant $l$ -th layer query projection matrix for a mini-batch of size $b_{m i n i}$ . Obtaining $X_{l}^{e}$ in the forward pass is equivalent to sampling the corresponding rows in the full-batch representation $X_{l}=f^{l}(s_{1}^{e},s_{2}^{\\bar{e}},\\ldots,s_{B})$ , as each sequence in a batch is independent from all the rest. Therefore, we can rewrite each local gradient update $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{l}^{Q}}=X_{l}^{e T}\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{Q}_{l}^{e}}}\\end{array}$ as $\\begin{array}{r}{\\boldsymbol{X_{l}}^{T}\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{Q_{l}}}}\\end{array}$ , where the $i$ -th column of $\\frac{\\partial\\mathcal{L}}{\\partial Q_{l}}$ is the zero vector 0 if $s_{i}$ is not present in the mini-batch. Therefore, we are able to simply disregard the mini-batch sampling and focus only on the total number of iterations $\\mathcal{E}$ , assuming that every sequence in the original batch is sampled at least once. ", "page_idx": 15}, {"type": "text", "text": "Let the input embedding vectors to the $l$ -th layer at timestep $t<\\mathcal{E}$ be $X_{l}^{t}$ . The final weight $W_{l}^{\\mathcal{E}}$ after $\\mathcal{E}$ steps can be written as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{W}_{l}^{\\mathcal{E}}=\\pmb{W}_{l}^{0}-\\eta\\sum_{t=0}^{\\mathcal{E}}\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}_{l}^{t}}=\\pmb{W}_{l}^{0}-\\eta\\sum_{t=0}^{\\mathcal{E}}\\pmb{X}_{l}^{t}^{T}\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{Q}_{l}^{t}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Under the assumption that the changes in model weights are relatively small, we can approximate $X_{l}^{t}=X_{l}^{0}$ . This is always the case for $l=0$ , as the embeddings before the first layer are independent of the model weights. This allows us to rewrite $W_{\\mathcal{E}}$ as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{W}_{l}^{\\mathcal{E}}=\\pmb{W}_{l}^{0}-\\eta\\sum_{t=0}^{\\mathcal{E}}\\pmb{X}_{l}^{t}\\frac{\\partial\\mathcal{L}}{\\partial Q_{l}^{t}}=\\pmb{W}_{l}^{0}-\\eta\\sum_{t=0}^{\\mathcal{E}}\\pmb{X}_{l}^{0}\\frac{\\partial\\mathcal{L}}{\\partial Q_{l}^{t}}=\\pmb{W}_{l}^{0}-\\pmb{X}_{l}^{0}\\frac{\\partial^{T}}{\\partial Q_{l}^{t}}\\frac{\\partial\\mathcal{L}}{\\partial Q_{l}^{t}}=\\pmb{W}_{l}^{0}-\\pmb{X}_{l}^{0}\\frac{\\partial^{T}}{\\partial Q_{l}^{t}}\\frac{\\partial\\mathcal{L}}{\\partial Q_{l}^{t}}\\frac{\\partial\\mathcal{L}}{\\partial Q_{l}^{t}}=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As the server has knowledge of the starting weight ${W}_{l}^{0}$ and the final weight $W_{l}^{\\mathcal{E}}$ , it is able to compute the sum of all gradient steps, i.e we will be able to apply Theorem 5.1 to $\\begin{array}{r}{\\dot{X_{l}^{0}}^{T}(\\eta\\sum_{t=0}^{\\mathcal{E}}{\\frac{\\partial\\mathcal{L}}{\\partial Q_{l}^{t}}})}\\end{array}$ E \u2202Lt) We . still require tE=0\u2202\u2202QLt to be full-rank, which is satisfied under standard DAGER assumptions. ", "page_idx": 15}, {"type": "text", "text": "Further experimental details We further empircially demonstrate that DAGER can effectively utilise the assumption of consistent feature embeddings across epochs under a reasonable learning rate and number of iterations. As shown in Table 4, we observe near-exact reconstruction rates for most configurations, with metrics only slightly declining as the number of epochs increases. The key factor that influences the success of DAGER is observed to be the learning rate $\\eta$ , as the aforementioned assumption might be invalidated at large $\\eta$ . However, learning rates exceeding $\\eta\\geq10^{-3}$ are typically too high for the model to converge, particularly in multi-client settings. Therefore, we can conclude that DAGER is highly effective in the FedAvg context. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "B.4 DAGER under LoRA training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we discuss how DAGER can be extended to work on LoRA weight decomposition.   \nUnder LoRA, the linear layer weight updates for a weight $W\\in\\mathbb{R}^{d\\times d}$ are performed on a low-rank   \nrepresentation: $W=W_{0}+A B$ , where $A\\in\\mathbb{R}^{d\\times r}$ , $\\bar{B}\\in\\mathbb{R}^{r\\times d}$ . As we obtain the gradient weights   \nfor both A and B, we can apply Theorem 5.1 to A with ZA = XA, namely because \u2202\u2202AL $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial A}=X^{T}\\frac{\\mathbf{\\check{\\partial}}\\mathcal{L}}{\\partial Z_{A}}}\\end{array}$   \nsApsasnucmhiencgk ts htaot $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{X}A}$ r fiso rfumlel-dr aonnk n idn tstheaat $b<r$ ofuorr  weaocrkh  ilsa ydeirr,e catfltye ra pwphliiccha blDeA. GThEiRs  ccaann  rbeep laapcpel itehde $\\pmb{A}$ $W$   \ndirectly. In practice, LoRA finetuning typically initializes $W=W_{0}$ and $_B$ to only contain zeroes   \n8wBh icmho rdeedl uocne t thhee  Rraotntke no fT $\\frac{\\partial\\mathcal{L}}{\\partial A}$ a tfooer st hdea tfairssett  fuesiwn og pat ibmaitzcaht isoizne  sotef p4s . wiWthe e( ftroallion wtihne g L[L3a9]M) af-o3r .31 $r=256$   \nepochs before applying DAGER. We report results in Table 11 and observe an excellent R1 and R2 of about $95\\%$ . ", "page_idx": 16}, {"type": "text", "text": "B.5 Complexity Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A key point of DAGER is the algorithm\u2019s exceptional computational efficiency on decoder-based models. In order to quantify the dependency of runtime on relevant variables, we describe the asymptotic complexity for both decoder- and encoder-based models. Below we list and prove several lemmas that assist us in the complete proof of our assertion for the complexity of DAGER. When not specified, a batch size of $B=1$ is implied for any inputs. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.1. The product of two matrices $M^{1}\\in\\mathbb{R}^{n\\times m}$ and $M^{2}\\in\\mathbb{R}^{m\\times p}$ can be naively computed in $\\mathcal{O}(n m p)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We write down the product: ", "page_idx": 16}, {"type": "equation", "text": "$$\n(M^{1}M^{2})_{i j}=\\sum_{k}M_{i k}^{1}M_{k j}^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To produce the entire matrix we explore all integers $i=1\\ldots n$ , $j=1\\ldots p$ and $k=1\\ldots m$ , and in particular any combination of the 3. This implies that we make nmp iterations, from which a time complexity of $\\mathcal{O}(n m p)$ follows. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma B.2. For any matrix $M\\in\\mathbb{R}^{d\\times n}$ and vector $\\pmb{v}\\in\\mathbb{R}^{d}$ , we can compute the projection of v on the subspace spanned by the columns of $_M$ in $O(d^{2}n)$ time. ", "page_idx": 16}, {"type": "text", "text": "Proof. Projecting onto the column space of a matrix can be done by projecting the vector onto individual columns and then summing all projected components. We compute this using Einstein notation, while denoting the resulting vector as $\\pmb{p}\\in\\mathbb{R}^{d}$ . Then, we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{k}=\\sum_{i,j}M_{k i}M_{j i}\\pmb{v}_{j}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This loop iterates over $i\\,=\\,1\\ldots n,j\\,=\\,1\\ldots d,k\\,=\\,1\\ldots d$ , resulting in a total number of $d^{2}{n}$ iterations, which implies a time complexity of $O(d^{2}n)$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma B.3. For any transformer-based model, which has an embedding dimension $d,$ , square projection weights of dimension $d\\times d,$ and an MLP hidden dimension of $d_{M L P}$ , propagating $a$ sequence of b tokens, takes time of asymptotic complexity $\\mathcal{O}(b d^{2}+b^{2}d+b\\dot{d}_{M L P}d)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We follow the notation defined in Sec. 3.1. The embedding representation of the sequence is denoted as $z\\in\\mathbb{R}^{b\\times d}$ . We then obtain the query, key and value vectors $Q,K,$ , $V$ by multiplying with the weight matrices $\\pmb{W}_{1}^{Q},\\pmb{W}_{1}^{K},\\pmb{W}_{1}^{V}\\in\\mathbb{R}^{d\\times d}$ . According to Lemma B.1, we can accomplish this in a time of $\\mathcal{O}(b d^{2})$ . ", "page_idx": 16}, {"type": "text", "text": "Computing the attention scores is dominated by computing $Q K^{T}$ , which by applying Lemma B.1, can be done in ${\\mathcal{O}}(b^{2}d)$ . ", "page_idx": 17}, {"type": "text", "text": "As a final step to the self-attention component, we compute the multiplication of the scores $A=$ softmax $(M\\overset{\\cdot}{\\odot}\\frac{Q K^{T}}{\\sqrt{d}})\\in\\mathbb{R}^{b\\times b}$ and $V\\in\\mathbb{R}^{b\\times d}$ in time ${\\mathcal{O}}(b^{2}d)$ . We note that computing the row-wise softmax operations can be amortized and take a total of ${\\mathcal{O}}(b^{2})$ time, which is dominated by ${\\mathcal{O}}(b^{2}d)$ . The total computation up until this point can, therefore, be done in $\\mathcal{O}(b d^{2}+b^{2}d)$ . ", "page_idx": 17}, {"type": "text", "text": "To produce the output of the transformer block, we pass the self-attention through an MLP layer, which can be represented by a set of simple matrix multiplications (of the same complexity). Therefore, applying Lemma B.1 once again, this step requires time $\\mathcal{O}(b d_{\\mathrm{MLP}}d)$ . ", "page_idx": 17}, {"type": "text", "text": "We can sum up all components to obtain a total complexity of $\\mathcal{O}(b d^{2}+b^{2}d+b d_{\\mathrm{MLP}}d)$ . If we reasonably assume that $\\bar{d}_{\\mathrm{MLP}}=\\mathcal{O}(d)$ , then the complexity can be simplified to $O(b d^{2}+b^{2}d)$ . ", "page_idx": 17}, {"type": "text", "text": "Having explored in-depth each smaller component of the algorithm, we can now determine the complexity of DAGER. We explore 3 instances of our algorithm, namely separating decoder- and encoder-based models, while also differentiating between absolute positional encodings and RoPE. We summarize our findings in a single theorem: ", "page_idx": 17}, {"type": "text", "text": "Theorem B.4. By considering a training iteration of batch of size $B$ with the longest sentence being of length $P$ , where the sentences are represented as a sequence of tokens from a vocabulary of size $V$ , DAGER can reconstruct the input with an asymptotic time complexity of: ", "page_idx": 17}, {"type": "text", "text": "1. For decoder-based models: ", "page_idx": 17}, {"type": "text", "text": "Proof. We separate our algorithm in 2 different parts: ", "page_idx": 17}, {"type": "text", "text": "1. Recovering the tokens $\\mathcal{T}^{*}$ through the span check on $W_{1}^{Q}$ . ", "page_idx": 17}, {"type": "text", "text": "2. Reconstructing the entire sequences $S^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "We begin by obtaining $\\mathcal{T}^{*}$ by taking $\\begin{array}{r}{T_{i}^{*}\\;=\\;\\{v\\;\\in\\;\\mathcal{V}|f^{0}(v,i)\\;\\in\\;\\mathrm{colspan}(\\frac{\\partial\\mathcal{L}}{\\partial W_{1}^{Q}})\\}}\\end{array}$ . Because we determine the span check via the projection distance $d_{\\mathrm{proj}}$ of a representation vector $z\\in\\mathbb{R}^{d}$ , and truncated gradient $\\frac{\\partial\\mathcal{L}}{\\partial W_{l}^{Q}}\\in\\mathbb{R}^{d\\times r}$ , where $\\begin{array}{r}{r=\\mathrm{rank}(\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}_{1}^{Q}})}\\end{array}$ k(\u2202\u2202WL Q ) according to Lemma B.2, this can be done in time $\\mathcal{O}(d^{2}r)$ . The rank of both $\\frac{\\partial\\mathcal{L}}{\\partial W_{1}^{Q}},\\,\\frac{\\partial\\mathcal{L}}{\\partial W_{2}^{Q}}$ is limited by the total number of tokens $b\\in{\\mathcal{O}}(P B)$ As can be inferred by Theorem 5.1, the complexity of the spancheck is $O(P B d^{2})$ . We perform this for every token in the vocabulary for an additional factor of $V$ , making the total complexity $O(P B V d^{2})$ . We note that for models which employ positional embeddings before the first transformer block, we repeat this step $P$ times, while we only have to do it once for those that don\u2019t. This respectively results in time complexities of $O(P^{2}B{\\dot{V}}d^{2})$ and $O(P B V d^{2})$ for recovering individual tokens. ", "page_idx": 17}, {"type": "text", "text": "Spancheck complexity In practice, we perform all span checks by performing a Singular Value Decomposition on $\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}_{1}^{Q}}$ and then applying the projection distance to the right orthonormal. It is a well known result that SVD for a matrix of size $d\\times d$ takes time $O(d^{3})$ . ", "page_idx": 17}, {"type": "text", "text": "Sequence reconstruction Having recovered the individual tokens, we detail the resulting time complexity of reconstructing the entire sequences. We describe each step for decoder-based models, before proceeding to encoder-based ones. ", "page_idx": 17}, {"type": "text", "text": "\u2022 For decoder-based models we begin by describing the number of tokens we obtain per position. If the model applies positional embedding before the first transformer layer, we assume that for each position $i$ we recover a set of tokens $\\boldsymbol{\\mathcal{T}}_{i}^{*}$ , such that $|T_{i}^{*}|=\\mathcal{O}(T)$ for some variable $T$ that depends on our input parameters and setup. Similarly, for models that do not have positional embeddings before the first transformer layer, we only recover a single set of tokens $\\tau^{*}$ of size $|T^{\\bar{*}}|=\\mathcal{O}(P T)$ .   \n\u2022 We now show the complexity required for performing the forward pass and span check for a single sequence. For a sequence length $n$ , according to Lemma B.3 the forward pass takes time $O(\\dot{n}d^{2}+n^{2}d)$ , and the span check per token is of complexity $O(P B d^{2})$ (as per Lemma B.2). This implies a time complexity of $O(n P B d^{2})$ per sequence for the span check, leading us to an overall complexity of $\\mathcal{O}(n P B d^{2}+n d^{2}+n^{2}d)$ .   \n\u2022 Finally, we apply our observations to the full reconstruction. We further assume that at each step of the greedy reconstruction, we maintain $\\mathcal{O}(B)$ possible sequences, which is usually the case when performing a greedy reconstruction. By repeating the above propagation for every combination per sequence length yields a total runtime of ${\\cal O}(T\\times B\\stackrel{.}{\\times}(\\stackrel{.}{n}\\!d^{2}+n^{2}d+$ $n P\\dot{B}d^{2}))=\\mathcal{O}(P\\dot{T}B^{2}d^{2}\\dot{n})$ and $\\mathcal{O}(\\bar{P T}\\times B\\times(n d^{2}+n^{2}d+n P\\dot{B}d^{2}))=\\mathcal{O}(P^{2}T B^{2}d^{2}n)$ in the cases described in statement 1a) and 1b) respectively.   \n\u2022 Summing over all lengths yields a time of $\\begin{array}{r}{\\mathcal{O}(\\sum_{n=1}^{P}P B^{3}d^{2}n)\\,=\\,\\mathcal{O}(P^{3}T B^{2}d^{2})}\\end{array}$ for the former. Analogically, for the latter we obtain a complexity of $\\mathcal{O}(P^{4}T B^{2}d^{2})$ .   \n\u2022 In practice, we observe that $T\\;=\\;\\alpha B$ for some factor $\\alpha$ between 1 and 10, hence we can assume that $T\\,=\\,\\mathcal{O}(B)$ . This is a practically correct assumption for a reasonably defined threshold $\\tau_{1}$ . This makes the final time complexity for recovering sequences $\\mathcal{O}(P^{3}T B^{2}d^{2})=\\mathcal{O}(P^{3}B^{3}d^{2})$ for case 1a) and $\\mathcal{O}(P^{4}T\\dot{B}^{2}d^{2})\\dot{=}\\mathcal{O}(P^{4}B^{3}d^{2})$ for 1b). ", "page_idx": 18}, {"type": "text", "text": "On the other hand, in the case of encoder-based models, we need to exhaust all possible token combinations over all positions. ", "page_idx": 18}, {"type": "text", "text": "\u2022 We again assume that for each position $i$ we recover a set of tokens $\\boldsymbol{\\mathcal{T}}_{i}^{*}$ , such that $\\vert\\Tilde{T}_{i}^{*}\\vert=$ ${\\mathcal{O}}(T)$ .   \n\u2022 Because we have to explore all possible combinations, that results in a total number of $\\begin{array}{r}{\\prod_{i=1}^{P}|{\\mathcal{T}}_{i}^{*}|=\\prod_{i=1}^{P}{\\mathcal{O}}({\\bar{T}})={\\mathcal{O}}({\\bar{T^{P}}})}\\end{array}$ sequences.   \n\u2022 We now show the cost of the span check on a single sequence. We leverage our finding that one such span check takes time $O(P B d^{2})$ (we again substitute that the rank $r=\\mathcal{O}(P B)$ and apply Lemma B.2), meaning all span checks take time $O(P^{2}B d^{2})$ per sequence.   \n\u2022 Additionally, the forward pass takes $\\mathcal{O}(P d^{2}+P^{2}d)$ time.   \n\u2022 Finally, because we reconstruct each sequence separately, we repeat the reconstruction algorithm $\\mathcal{O}(B)$ times. This leads to the complexity for this step - $\\dot{\\mathcal{O}(T^{P}B(P^{2}B d^{2}\\!+\\!P d^{2}\\!+$ $P^{\\check{2}}d))=\\mathcal{O}(T^{\\check{P}}B^{2}P^{2}d^{2})$ .   \n\u2022 As above, we assume $T=\\mathcal{O}(B)$ , leading us to a final complexity of $\\mathcal{O}(B^{P}B^{2}P^{2}d^{2})=$ $\\mathcal{O}(B^{P}P^{2}d^{2})$ . ", "page_idx": 18}, {"type": "text", "text": "Combining our conclusions for steps 1 and 2 yields the stated complexities for each setting. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "The key points to highlight is that DAGER is polynomial across both sequence length and batch size for decoders, regardless of the type of positional encoding. Meanwhile, for encoders the we observe that DAGER is exponential in length and polynomial with high degree in terms of batch size. To this end, the heuristics we described are crucial to significantly reduce the search space. ", "page_idx": 18}, {"type": "table", "img_path": "CrADAX7h23/tmp/782bcfa8ac9e1aeb60cfba71972351692da9f73e243a3f6b721f112ab72ae780.jpg", "table_caption": ["Table 6: Specifications of models that were used in our work. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "In this section, we provide pseudocode for DAGER when applied to encoderbased LLMs. We provide it in Algorithm 3, where we first find the set of client sequence lengths $n_{j}$ and store them in $\\mathcal{N}$ (Line 4 to Line 8). We then go through the $n_{j}\\mathbf{s}$ from the smallest to the largest (Line 9), enumerating all possible sequences $s\\ \\in\\ S_{i}$ of length $i$ (Line 10). Importantly, when a correct sentence $s\\,\\in\\,S_{i}^{\\mathrm{corr}}$ is found its tokens are removed for the token sets $\\boldsymbol{\\mathcal{T}_{i}^{*}}$ (Line 14). Finally, we return the deduplicated best reconstructions across different sequence lengths $S_{\\mathrm{best}}^{*}$ (Line 15). We note that when ${\\cal{S}}_{i}$ is larger than 10M combinations, we sample 10M random combinations from it instead. ", "page_idx": 19}, {"type": "text", "text": "C Additional experimental details ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "CrADAX7h23/tmp/b73f341efa280bf04536920a8beff334b880308b1256ce3b4e95f34eba2deed2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "In this section we describe additional details regarding the experimental setup for DAGER. ", "page_idx": 19}, {"type": "text", "text": "Models In particular, we explore its feasibility on the base variant $\\mathrm{GPT}{\\mathrm{-}}2_{\\mathrm{BASE}}$ , featuring a 12-layer transformer with a 768-dimensional hidden state, 12 attention heads and a feed-forward filter of size 3072. Additionally, we examine the effects of model size by considering G $\\mathrm{PT}{-}2_{\\mathrm{LARGE}}$ , which contains 36 layers, a 1280-dimensional hidden state, 20 attention heads, and a feed-forward fliter of size 5120. To demonstrate the versatility of our approach, we also evaluate our attack in a next-token prediction context on the GPT-2BASE model. We further demonstrate the efficacy of our attack on LLaMa 2-7B[13] and BERTBASE[14] to illustrate our performance on a state-of-the-art decoder and an encoder-only model, respectively. All models were sourced from HuggingFace[41] in their pre-trained form, following standard practice in language modeling research[42]. Full specification details for all models can be found in Table 6. ", "page_idx": 19}, {"type": "text", "text": "Datasets In our evaluation, we utilize three binary classification datasets with varying sentence lengths, namely the Corpus of Linguistic Acceptability (CoLA)[15], the Stanford Sentiment Treebank (SST-2)[16], which are part of the GLUE benchmark[43], as well as the Rotten Tomatoes[17] sentiment analysis dataset. While our algorithm is data-independent, previous studies have indicated that text size can affect reconstructability[11, 6]. We demonstrate robustness to this factor by selecting the aforementioned datasets, with CoLA featuring text typically ranging from 4 to 9 words, SST-2 - from 4 to 13 words, and Rotten Tomatoes - from 10 to 27 words. We note that the binary classification setting has been shown to be a less vulnerable than next-token prediction which can be attacked via label reconstruction attacks [35], however, we conduct an additional experiment to substantiate the claim in this setting. Finally, to showcase DAGER\u2019s capability to handle arbitrarily long sequences, we leverage the European Court of Human Rights (ECHR)[18] dataset which includes sentences that are over 1000 words long, far exceeding the maximum input length of any of the aforementioned models. ", "page_idx": 19}, {"type": "table", "img_path": "CrADAX7h23/tmp/abdd9f4225e99cb300c2c6484680d98d91f653284c0656049d856c794cddad80.jpg", "table_caption": ["Table 7: Total runtime for all main experiments given in hours. Fields containing N/A represent experiments that were not run. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Computational requirements We implement DAGER in PyTorch [44] and run all experiments on a single GPU. Tests on the LLaMa-2 (7B) architecture were performed on NVIDIA A100 Tensor Core GPUs, which boast $40\\:\\mathrm{GB}$ of memory, while all others were ran on NVIDIA L4 GPUs with $24\\:\\mathrm{GB}$ of memory. In practice, less demanding resources may be used, especially for lower batch sizes on BERT and GPT- $2_{\\mathrm{BASE}}$ . In terms of required RAM, we used between 16 GB and $150\\,\\mathrm{GB}$ per experiment, depending on the batch size and model. ", "page_idx": 20}, {"type": "text", "text": "Hyperparameter details We use a span check acceptance threshold of $\\tau_{1}=10^{-5}$ in the first layer, and $\\tau_{2}=10^{-3}$ in the second, a rank truncation of $\\Delta_{b}=20$ , and for decoder-based models consider at most $10\\,000\\,000$ proposal sentences per recovered EOS token position. We consider pre-trained models with a randomly initialized classification head using a normal distribution with $\\sigma=10^{-3}$ . To manage numerical instabilities within the framework, we tweak the eigenvalue threshold when doing the SVD $\\tau_{l}^{\\mathrm{rank}}$ and decrease with the batch size growing, varying it between $10^{-7}$ and $10^{-9}$ . ", "page_idx": 20}, {"type": "text", "text": "C.1 Runtime of experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We further demonstrate the computational efficiency of DAGER. The runtime summary can be found in Table 7. It is notable that for the experiments on BERT, we have a drastic increase in complexity with respect to the batch size and sequence length, as expected from App. B.5, which do not affect the baselines as much. That said, we still remain within the same order of magnitude, while achieving significantly better results. ", "page_idx": 20}, {"type": "text", "text": "In contrast, we achieve a significant improvement for decoder-based models. We notice that within the attack\u2019s scope for GPT-2, we can reconstruct a batch for less than 8 minutes for any batch size. It is important to highlight that the runtime decreases for the batch sizes of 64 and 128 because we are more often than not unable to recover any tokens due to the embedding dimension limitation described in Sec. 7. ", "page_idx": 20}, {"type": "table", "img_path": "CrADAX7h23/tmp/2d07d6066bd7591ffe02778b11c82873ab40ac2c05ec13ef12805c886ff51bda.jpg", "table_caption": ["Table 8: Ablation study on $\\mathrm{GPT}{\\mathrm{-}}2_{\\mathrm{BASE}}$ with and without the rank threshold heuristic. We report scores of 0 for any example that has full-rank gradients. R-1 and R-2 denote the ROUGE-1 and ROUGE-2 scores respectively. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 9: Example comparison between DAGER and $\\mathrm{LAMP}_{\\mathrm{Cos}}$ for random samples from different datasets, reconstructed at batch size 1. We note that ${\\mathrm{LAMP}}_{\\mathrm{Cos}}$ was selected, as it was the bestperforming baseline model for a batch size of 1. ", "page_idx": 21}, {"type": "table", "img_path": "CrADAX7h23/tmp/ce6059bae163f8ebd703f9690007f9f6329d55da86b8793408c999d0b48603a5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.2 Rank restriction ablation study ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In section Sec. 5.3, we described that for full-rank matrices we attempt a best-effort reconstruction by artificially restricting the rank by a threshold $\\tilde{b}$ . Here we demonstrate the effect of this component by showing the performance of DAGER without attempting to recover any part of the sentence. Any time we observe a sample with full-rank gradients at either the first or second layers, we immediately fail and report a score of 0. The results can be seen in Table 8. ", "page_idx": 21}, {"type": "text", "text": "C.3 Example reconstructions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In Table 9 we show sample reconstructions between the best-performing baseline LAMP_Cos for a batch size $B=1$ . We note that DAGER achieves exact reconstruction, while LAMP_Cos can predict less than half of the sentence correctly. ", "page_idx": 21}, {"type": "text", "text": "C.4 DAGER under differential privacy ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we show how DAGER performs under a defended setting, in particular by adding random Gaussian noise with variance $\\sigma^{2}$ to all gradients. We explore the range $\\dot{\\sigma^{\\circ}}\\in[10^{-5},5^{\\circ}\\!\\times10^{-4}]$ , as for any $\\sigma\\geq10^{-3}$ , the sentiment prediction accuracy of the converged model drops to below $80\\%$ from $>87\\%$ . We apply DAGER on the GPT-2 model for the Rotten Tomatoes dataset at $B=1$ . Due to the highly random nature of this type of defense, we cannot simply filter the sequences by measuring the single span check distance at layer $l=2$ . Instead, we utilise that further layers $l>3$ retain the same property that the input embeddings only depend on previous tokens, and measure thhyep earvpearraagme $\\begin{array}{r}{\\bar{d}=\\sum_{l=2}^{\\bar{L}_{D P}}d(f_{i}^{l-1}(s),\\bar{l})}\\end{array}$ nfooirs ea  wceilrlt amina kneu tmheb egrr aodfi elanty eurps $L_{D P}$ f, ulwl-hriacnhk ,w feo ro cpotimmpisutea taiso na purposes we set a constant rank of $r=100$ , which is much higher than the length of any sentence. While it has been shown [45] that differencial privacy provides provable guarantees for protecting privacy, we provide promising initial results, as seen in Table 10. We believe that there are numerous improvements one could make, but leave these for future work. ", "page_idx": 21}, {"type": "text", "text": "Table 10: Experiments on the differential privacy setting under Gaussian noise on the GPT-2 model with a batch size of 1 on the Rotten Tomatoes dataset. R-1 and R-2 denote the ROUGE-1 and ROUGE-2 scores respectively. ", "page_idx": 22}, {"type": "table", "img_path": "CrADAX7h23/tmp/84f3d7732c92462588245fe225c503286063a578b135f7474fe6a9126a0af1e8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 11: Miscallaneous experiments, referenced in the evaluation section. We applied DAGER on the Rotten Tomatoes dataset for $B=16$ , if not specified otherwise. ", "page_idx": 22}, {"type": "table", "img_path": "CrADAX7h23/tmp/17eaeedf5984a28df1ad944685c836dd9261ef567994f04c78c33ac97ce95c4f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.5 Miscallaneous ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Any other experiments, namely the ones on LLaMA-3 70B, LLaMa-3.1 8B, DAGER under LoRA training, or DAGER using different loss functions are included in Table 11. ", "page_idx": 22}, {"type": "text", "text": "D Licenses ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In our work, we use the publicly available datasets CoLA, SST-2, Rotten Tomatoes and ECHR. CoLA is licensed under the MIT license. No public licensing information was found for SST-2 and Rotten Tomatoes. Furthermore, we use ECHR under the Creative Commons Attribution-NonCommercialShareAlike 4.0 International (CC BY 4.0) license. For privacy concerns, ECHR has issued a statement of protection of personal data that ensures private data was handled appropriately1. ", "page_idx": 22}, {"type": "text", "text": "In terms of Large Language Model architectures, we use GPT-2 under the MIT license and BERT under the Apache License. All aforementioned licenses permit our use of the underlying assets for the purposes of this paper. We obtained access to LLaMa-2 through the Llama 2 Community License Agreement2 which permits the model\u2019s use in commercial and research settings. ", "page_idx": 22}, {"type": "text", "text": "Finally, we obtain the code for the LAMP and TAG attacks through the public repository for LAMP which is licensed under the Apache License 2.0. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The main claim of our abstract and introduction sections is that we demonstrate the first algorithm to recover whole batches of input text exactly in the honest-but-curious setting. To this end, we provide an overview of DAGER in Sec. 4 and delve into further mathematical details in Sec. 5. The discussion encompasses relevant mathematical proofs, as well as a thorough description of the underlying algorithms, featuring a greedy approach for decoder-based architectures and an exhaustive heuristic search for encoder-based ones. Additionally, we claim that the proposed attack outperforms existing attacks under the same threat model in quality, speed and scalability. We substantiate this claim through multiple experiments across batch sizes up to $B=128$ for different datasets and models, as showcased in Sec. 6. Finally, in the same section we support our claim that DAGER can be applied across a variety of settings through empirical evaluation, and through rigorous analysis in App. B. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We detail our limitations in a dedicated section (Sec. 7), discussing both theoretical and computational aspects of DAGER, for the latter of which we present a formal proof of complexity in App. B.5. We further describe any underlying assumptions in the \"Related Work\" and \"Background\" sections (Sec. 2 and Sec. 3 respectively). ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide rigorous proofs for all theorems directly following the statement, or have been deferred to App. B. All assumptions have been explicitly stated, discussed and further elaborated upon in App. B.2. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide code and additional resources that are required to reproduce our results shown in Sec. 6 in our supplementary material. Furthermore, we describe in sufficient detail all specifics of the algorithm that are needed for reproduction in the technical and experimental sections (Sec. 5, Sec. 6 respectively), with additional information found in App. C. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We provide a Python implementation, alongside detailed instructions for both setup and execution. Most of the data and architectures we used belong to the public domain. The only exception is that one must request access to the private HuggingFace repository for all LLaMa models. Instructions on how to approach this have also been included. We discuss the licensing and availability for all assets in App. D. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We have described all relevant hyperparameters as part of the experimental section, details of which can be found in Sec. 6 and App. C. Furthermore, ablation studies on the effect of most hyperparameters have been demonstrated in Fig. 2, Fig. 3, Fig. 4 and App. C.2 to support our choices. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide a $95\\%$ confidence interval for all relevant scores in the experimental section (Sec. 6). To this end, we detail our treatment of Gaussian and non-Gaussian distributions separately. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We list all necessary computational resources in Sec. 6.1, including information regarding the GPU and CPU requirements. We further disclose the necessary runtime for each of the main experiments in App. C.1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We discuss the implications of our work on user privacy in federated learning in our Broader Impact statement in App. A. To prevent malicious uses of our code we plan to release it under a license that prohibits any non-research related uses. We note that our work does not introduce new datasets or architectures and that we rely on only publicly available datasets, whose licenses we appropriately discuss in App. D. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discuss this paper\u2019s implications in the Broader Impact section (App. A), where we describe the necessity of our work and associated privacy considerations. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Upon publicly releasing the code, we plan on attaching a license that prohibits any non-academic use. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All datasets and models used are either publicly available, or have been properly attributed. We have also appropriately credited the authors of the baselines\u2019 code, which is publicly available. We discuss the licenses and terms and conditions explicitly in App. D. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We provide a Python implementation, alongside detailed instructions for both setup and execution. Our work does not introduce new architectures or datasets. We discuss licenses for all external assets in App. D. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing experiments nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing experiments nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]