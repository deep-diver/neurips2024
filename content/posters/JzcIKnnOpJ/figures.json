[{"figure_path": "JzcIKnnOpJ/figures/figures_1_1.jpg", "caption": "Figure 1: An idealized distribution Q is learned to minimizes the loss of a model. We then compare Q with the original data distribution P via a density ratio p = dQ/dP. A rejection criteria is defined via threshold value T.", "description": "The figure shows two distributions, P (data distribution) and Q (idealized distribution).  A density ratio, p(x) = dQ/dP(x), is calculated to compare the two. A rejection criterion is established: if p(x) is below a threshold T, the model abstains from making a prediction. The idealized distribution Q is learned to optimize the model's performance, so regions where Q has significantly less mass than P represent areas where the model is less confident and thus rejection is preferred.", "section": "3 Rejection via Idealized Distributions"}, {"figure_path": "JzcIKnnOpJ/figures/figures_9_1.jpg", "caption": "Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \u03c4\u2208 (0,1] and c\u2208 [0,0.5) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \u00b11 s.t.d. region.", "description": "This figure compares the performance of different rejection methods across three datasets (HAR, Gas Drift, MNIST) at various acceptance coverage levels.  Each method's accuracy is plotted against its acceptance coverage, with the black horizontal line representing the baseline accuracy without rejection.  The shaded areas represent the standard deviation.  Missing points indicate that the model rejected more than 60% of test points or achieved an accuracy below the baseline.", "section": "5 Experiments"}, {"figure_path": "JzcIKnnOpJ/figures/figures_26_1.jpg", "caption": "Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \u03c4\u2208 (0,1] and c\u2208 [0,0.5) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \u00b11 s.t.d. region.", "description": "This figure compares the performance of different rejection methods across multiple datasets, showing the trade-off between accuracy and acceptance coverage.  Each point represents a specific rejection threshold. The black line shows the baseline accuracy without rejection.  The shaded area represents the standard deviation, indicating variability. Missing data points signify that the model rejected more than 60% of the data or performed worse than the baseline.", "section": "5 Experiments"}, {"figure_path": "JzcIKnnOpJ/figures/figures_27_1.jpg", "caption": "Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \u03c4\u2208 (0,1] and c\u2208 [0,0.5) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \u00b11 s.t.d. region.", "description": "The figure shows the accuracy vs. acceptance coverage trade-off for different rejection methods on three datasets (HAR, Gas Drift, MNIST).  Each method is represented by a line showing its performance across different thresholds (\u03c4). The black horizontal line represents the baseline accuracy without rejection. The shaded area indicates the standard deviation. Missing data points indicate that a particular method rejected more than 60% of the data points or had lower accuracy than the baseline model. This figure illustrates the performance of the proposed density ratio rejection method compared to alternative approaches.", "section": "5 Experiments"}, {"figure_path": "JzcIKnnOpJ/figures/figures_28_1.jpg", "caption": "Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \u03c4\u2208 (0,1] and c\u2208 [0,0.5) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \u00b11 s.t.d. region.", "description": "This figure compares the performance of different rejection methods, including the proposed density ratio rejection methods, on three datasets (HAR, Gas Drift, MNIST) under clean and noisy conditions.  The x-axis represents the acceptance coverage (percentage of inputs not rejected), while the y-axis shows the accuracy.  The plots illustrate how each method trades off accuracy for coverage.  The black horizontal line represents the baseline performance without rejection. Shaded areas represent one standard deviation around the mean.", "section": "5 Experiments"}, {"figure_path": "JzcIKnnOpJ/figures/figures_28_2.jpg", "caption": "Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \u03c4\u2208 (0,1] and c\u2208 [0,0.5) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \u00b11 s.t.d. region.", "description": "The figure shows the accuracy vs. acceptance coverage trade-off for different rejection methods on three datasets (HAR, Gas Drift, and MNIST).  Each method is represented by a line, showing how the model's accuracy changes as the acceptance coverage (percentage of instances not rejected) varies.  The black horizontal line indicates the accuracy of the base model without any rejection. The shaded area shows the standard deviation around the mean accuracy for each point.", "section": "5 Experiments"}, {"figure_path": "JzcIKnnOpJ/figures/figures_29_1.jpg", "caption": "Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \u03c4\u2208 (0,1] and c\u2208 [0,0.5) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \u00b11 s.t.d. region.", "description": "This figure shows the accuracy vs. acceptance coverage trade-off for various rejection methods on several datasets.  Each point represents a different threshold for rejection.  The black horizontal line indicates the baseline accuracy without rejection.  Shaded areas represent standard deviations.", "section": "5 Experiments"}, {"figure_path": "JzcIKnnOpJ/figures/figures_29_2.jpg", "caption": "Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \u03c4\u2208 (0,1] and c\u2208 [0,0.5) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \u00b11 s.t.d. region.", "description": "This figure compares the performance of various rejection methods, including the proposed density ratio rejectors, against baselines on three datasets (HAR, Gas Drift, MNIST).  Each method's accuracy is plotted against its acceptance coverage (percentage of instances not rejected), for different threshold values (\u03c4). The black horizontal line shows the accuracy of the base model without rejection. The shaded region represents the standard deviation.  Methods that frequently reject (rejecting > 60% of instances) or perform worse than the base model are omitted from the plots.", "section": "5 Experiments"}, {"figure_path": "JzcIKnnOpJ/figures/figures_30_1.jpg", "caption": "Figure VII: MNIST with different noises for density ratio approaches.", "description": "The figure shows the accuracy versus acceptance coverage plots for MNIST dataset with different levels of label noise (10%, 20%, 25%, 30%, 40%).  The plots compare the performance of two density ratio rejection methods: KL-Rej and (\u03b1=3)-Rej. Each line represents a specific noise level, and the shaded area represents the standard deviation. The figure illustrates how the rejection methods trade-off accuracy and coverage under varying levels of data corruption. ", "section": "5 Experiments"}, {"figure_path": "JzcIKnnOpJ/figures/figures_31_1.jpg", "caption": "Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \u03c4\u2208 (0,1] and c\u2208 [0,0.5) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \u00b11 s.t.d. region.", "description": "This figure compares the accuracy and coverage of different rejection methods, including the proposed density ratio methods and several baselines.  The x-axis represents the acceptance coverage (percentage of inputs not rejected), and the y-axis represents the accuracy. Each point represents a different threshold value, and the lines connect points with similar thresholds. The black horizontal lines show the baseline accuracy without rejection.  The shaded region represents the standard deviation of the accuracy. The plot shows that the density-ratio rejectors generally provide a better tradeoff between accuracy and coverage than the baselines, particularly when a higher coverage is required.  Some methods are missing from certain plots because their rejection rates are too high.", "section": "5 Experiments"}, {"figure_path": "JzcIKnnOpJ/figures/figures_32_1.jpg", "caption": "Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \u03c4\u2208 (0,1] and c\u2208 [0,0.5) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \u00b11 s.t.d. region.", "description": "This figure displays the accuracy vs. acceptance coverage trade-off for different rejection methods on three datasets: HAR, Gas Drift, and MNIST. The x-axis represents acceptance coverage (percentage of inputs not rejected), and the y-axis shows the accuracy of the model on the accepted inputs.  Each method is represented by a line, with the shaded area representing one standard deviation.  The black horizontal line indicates the baseline accuracy without rejection.  The plot demonstrates how different methods balance accuracy and rejection rate.", "section": "5 Experiments"}, {"figure_path": "JzcIKnnOpJ/figures/figures_32_2.jpg", "caption": "Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \u03c4\u2208 (0,1] and c\u2208 [0,0.5) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \u00b11 s.t.d. region.", "description": "This figure displays the accuracy versus acceptance coverage trade-off for different rejection methods on three datasets (HAR, Gas Drift, MNIST).  Each point represents a model trained with a different rejection threshold, and the lines connect points with the same method. The black horizontal line shows the baseline accuracy without rejection. The shaded area represents the standard deviation of the results. The plot shows that the density-ratio rejection method (KL-Rej and \u03b1=3-Rej) achieves high accuracy even at high coverage rates, outperforming other methods in many cases.", "section": "Experiments"}, {"figure_path": "JzcIKnnOpJ/figures/figures_33_1.jpg", "caption": "Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \u03c4\u2208 (0,1] and c\u2208 [0,0.5) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \u00b11 s.t.d. region.", "description": "This figure shows the accuracy vs. acceptance coverage trade-off for different rejection methods on three datasets: HAR, Gas Drift, and MNIST.  Each point represents a different threshold (\u03c4) used for rejection. The black line represents the baseline model without rejection.  The shaded area represents the standard deviation.  The plot illustrates how each method balances accuracy and rejection rate, allowing a model to reject predictions it's less confident about in order to improve overall accuracy.", "section": "5 Experiments"}]