[{"type": "text", "text": "Rejection via Learning Density Ratios ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexander Soen\u2020 ", "page_idx": 0}, {"type": "text", "text": "Hisham Husain\u22c6 ", "page_idx": 0}, {"type": "text", "text": "hisham.husain@protonmail.com ", "page_idx": 0}, {"type": "text", "text": "Amazon The Australian National University alexander.soen@anu.edu.au ", "page_idx": 0}, {"type": "text", "text": "Philip Schulz Amazon phschulz@amazon.com ", "page_idx": 0}, {"type": "text", "text": "Vu Nguyen Amazon vutngn@amazon.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Classification with rejection emerges as a learning paradigm which allows models to abstain from making predictions. The predominant approach is to alter the supervised learning pipeline by augmenting typical loss functions, letting model rejection incur a lower loss than an incorrect prediction. Instead, we propose a different distributional perspective, where we seek to find an idealized data distribution which maximizes a pretrained model\u2019s performance. This can be formalized via the optimization of a loss\u2019s risk with a $\\varphi$ -divergence regularization term. Through this idealized distribution, a rejection decision can be made by utilizing the density ratio between this distribution and the data distribution. We focus on the setting where our $\\varphi$ -divergences are specified by the family of $\\alpha$ - divergence. Our framework is tested empirically over clean and noisy datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Forcing Machine Learning (ML) models to always make a prediction can lead to costly consequences. Indeed, in real-world domains such as automated driving, product inspection, and medical diagnosis, inaccurate prediction can cause significant real-world harm [16, 29, 53, 50]. To deal with such a dilemma, selective prediction and classification with rejection were proposed to modify the standard supervised learning setting [15, 17, 72]. The idea is to allow for a model to explicitly reject making a prediction whenever the underlying prediction would be either inaccurate and / or uncertain. ", "page_idx": 0}, {"type": "text", "text": "In classification, a confidence-based approach can be utilized, where a classifier is trained to output a \u201cmargin\u201d which is used as a confidence score for rejection [7, 31, 71, 59, 53]. The model rejects whenever this confidence score is lower than an assigned threshold value. A key aspect of these approaches is that they rely on good probability estimates $\\operatorname*{Pr}(\\mathsf{Y}\\mid\\mathsf{X}=x)$ [60], i.e., being calibrated [71, 53]. While some approaches avoid explicit probability estimation, these are typically restricted to binary classification [7, 31, 47]. Empirically, approaches utilizing confidence scores have shown to outperform other methods, even with simple plugin estimates for probabilities [26, 39, 58]. ", "page_idx": 0}, {"type": "text", "text": "Another classifier-rejection approach aims to simultaneously train a prediction and rejection model in tandem [16, 17, 53]. These approaches are theoretically driven by the construction of surrogate loss functions, but in the multiclass classification case many of these loss functions have been shown to not be suitable [53]. For the multiclass classification setting, one approach connects classification with rejection to cost-sensitive classification [14]. In practice, these classification-rejection approaches require models to be trained from scratch using their specific loss function and architecture \u2014 if there is an existing classifier for the dataset, it must be discarded. A recent approach proposes to learn a post-hoc rejector on top of a pretrained classifier via surrogate loss functions [48]. ", "page_idx": 0}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/0ba9e1104bc02329a93f47b6dffe5a4d528c4ca48a8aa55b21a9de25346c716a.jpg", "img_caption": ["Figure 1: An idealized distribution $\\mathrm{Q}$ is learned to minimizes the loss of a model. We then compare $\\mathrm{Q}$ with the original data distribution $\\mathrm{P}$ via a density ratio $\\rho=\\mathrm{dQ/dP}$ . A rejection criteria is defined via threshold value $\\tau$ . "], "img_footnote": [], "page_idx": 1}, {"type": "table", "img_path": "JzcIKnnOpJ/tmp/338309f3b8818d9d20c67da4afe4fb4a65b27de9c7269160c8bbcf525e16d8af.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, to learn rejectors we shift from a loss function perspective to a distributional perspective. Given a model and a corresponding loss function, we find a distribution where the model and loss performs \u201cbest\u201d and compare it against the data input distribution to make rejection decisions (see Fig. 1 and Algorithm 1 with equations boxed). As such, the set of rejectors that we propose creates a rejection decision by considering the density ratio [67] between a \u201cbest\u201d case (idealized) distribution and the data distribution, which can be thresholded by different values $\\tau$ to provide different accuracy vs rejection percentage trade-offs. To learn these density ratios for rejection, we consider a risk minimization problem which is regularized by $\\varphi$ -divergences [1, 19]. We study a particular type of $\\varphi$ -divergences as our regularizer: the family of $\\alpha$ -divergences which generalizes the KL-divergence. To this end, one of our core contributions in this work is providing various methods for constructing and approximating idealized distributions, in particular those constructed by $\\alpha$ -divergences. ", "page_idx": 1}, {"type": "text", "text": "The idealized distributions that we consider are connected to adversarial distributions examined in Distributionally Robust Optimization (DRO) [63] and the distributions learned in Generalized Variational Inference (GVI) [42]; and, as such, the closed formed solutions found for our idealized distribution have utility outside of rejection. Furthermore, when utilizing the KL-divergence and Bayes optimal models, we recover the well known optimal rejection policies, i.e., Chow\u2019s rule [15, 72]. Our rejectors are then examined empirically on 6 different datasets and under label noise corruption. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are the following: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We present a new framework for learning with rejection involving the density ratios of idealized distributions which mirrors the distributions learned in DRO and GVI;   \n\u2022 We show that rejection policies learned in our framework can theoretically recover optimal rejection policies, i.e., Chow\u2019s rule;   \n\u2022 We derive optimal idealized distributions generated from $\\alpha$ -divergences;   \n\u2022 We present a set of simplifying assumptions such that our framework can be utilized in practice for post-hoc rejection and verify this empirically. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation Let $\\mathcal{X}$ be a domain of inputs and $\\mathcal{Y}$ be output targets. We primarily consider the case where $\\mathcal{Y}$ is a finite set of $N$ labels $[N]$ , where $[N]\\,\\doteq\\,\\{0,\\dots,N-1\\}$ . We also consider output domains $\\mathcal{Y^{\\prime}}$ which is not necessarily the same as the output data $\\mathcal{Y}$ , e.g., class probabilities estimates in classification. Denote the underlying (usually unknown) input distribution as $\\mathrm{P_{x,y}}\\in\\mathcal{D}(\\mathcal{X}\\times\\mathcal{Y})$ . The marginals of a joint distribution are denoted by subscript, e.g., $\\mathrm{P_{y}}\\in\\mathcal{D}(\\mathcal{Y})$ for the marginal on the label space. We denote conditional distributions, e.g., $\\mathrm{P_{x|y}}$ . For marginals on $\\mathcal{D}(\\mathcal{X})$ , the subscripting of $\\mathrm{x}$ is implicit with $\\mathrm{P=P_{x}}$ . The empirical distributions of $\\mathrm{P}$ are denoted by $\\hat{\\mathrm{P}}_{N}$ , with $N$ denoting the number of samples used to generate it. Denote the Iverson bracket $\\[p]=1$ if the predicate $p$ is true and $\\[p]=0$ otherwise [43]. The maximum $[z]_{+}\\doteq\\operatorname*{max}\\{0,z\\}$ is s h or thanded. ", "page_idx": 1}, {"type": "text", "text": "Learning with Rejection We first recall the standard risk minimization setting for learning. Suppose that we are given a (point-wise) loss function $\\ell\\colon\\mathcal{Y}\\times\\mathcal{Y}^{\\prime}\\to\\mathbb{R}_{\\geq0}$ which measures the level of disagreement between predictions and data. Given a data distribution $\\mathrm{P_{x,y}}$ and a hypothesis set of models $h\\in{\\mathcal{H}}$ , we aim to minimize the expected risk w.r.t. $h\\colon\\mathcal{X}\\rightarrow\\mathcal{Y}^{\\prime}$ , ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{h\\in\\mathcal{H}}{\\mathrm{argmin}}~~\\mathbb{E}_{\\mathrm{P_{x,y}}}[\\ell(\\mathsf{Y},h(\\mathsf{X}))].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "One way of viewing learning with rejection is to augment the risk minimization framework by learning an additional rejection function $r\\colon\\mathcal{X}\\rightarrow\\{\\bar{0,}1\\}$ . This can be formally defined using a regularization term $c\\in\\mathbb{R}_{\\geq0}$ which controls the rate of rejection: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{argmin}_{(h,r)\\in\\mathcal{K}\\times\\mathcal{R}}~~\\mathbb{E}_{\\mathrm{P}_{\\mathrm{x},\\mathrm{y}}}[(1-r(\\mathsf{X}))\\ell(\\mathsf{Y},h(\\mathsf{X}))]+c\\cdot\\mathrm{P}[r(\\mathsf{X})=1],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{R}$ denotes a hypothesis set of rejection functions. Once minimized, a combined model $f$ which can return a rejection token $\\textsuperscript{\\textregistered}$ to abstain from predictions can be defined, ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(x)={\\binom{\\circledast}{h(x)}}\\quad{\\mathrm{if~}}r(x)=1\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Suppose that $\\mathcal{R}$ is complete (contains all possible rejectors). In such a case, one can see that setting $c=\\infty$ reduces Eq. (2) to standard risk minimization setting given by Eq. (1). Furthermore, setting $c=0$ will result in $r$ always rejecting, i.e., the case where there is no rejection cost. Some values of $c\\in\\mathbb{R}_{\\geq0}$ can be redundant. If the loss $\\ell$ is bounded above by $B$ , then any $c>B$ is equivalent to setting $c=B$ \u2014 the optimal $r^{\\star}$ will be to never reject. In classification, where $\\ell$ is taken to be the zero-one-loss, $c$ is typically restricted to values in $(0,0.5)$ as otherwise low confidence prediction can be accepted [53] \u2014 our work only considers this case. [59] explores the $c\\in[0.5,1]$ scenario. ", "page_idx": 2}, {"type": "text", "text": "So far, the learning task has been left general. By considering Class Probability Estimation (CPE) [61], we recover familiar optimal rejection and classifier pairs. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1 (Optimal CPE Rejection / Chow\u2019s Rule). Let us consider the binary CPE setting, where ${\\mathcal{Y}}=\\{0,1\\}$ , $\\ y^{\\prime}=[0,1]$ , and $\\ell$ be any proper1 loss function [60] (e.g., log loss). Then w.r.t. Eq. (2), the optimal classifier is given by $h^{\\star}(\\bar{\\boldsymbol{x}})\\stackrel{\\textstyle-}{=}\\sf P[\\boldsymbol{Y}=\\bar{\\boldsymbol{1}}\\mid\\boldsymbol{\\sf X}=\\boldsymbol{x}]$ and the optimal rejector is given by $r^{\\star}(x)=\\left[\\mathbb{E}_{\\mathsf{Y}\\sim h^{\\star}(\\mathsf{X}=x)}[\\ell(\\mathsf{\\bar{Y}},h^{\\star}(x))]\\geq c\\right]$ . ", "page_idx": 2}, {"type": "text", "text": "The theorem can easily be generalized to non-binary cases. We note that Theorem 2.1 is a generalization of the well known Chow\u2019s rule of classification with rejection [15, 14]. Indeed, taking $\\ell$ to be the zero-one-loss function, we get $r^{\\star}(x)=\\||2\\cdot\\mathrm{P}[\\mathsf{Y}=1\\mid\\mathsf{X}=x]-1|\\geq1-c\\|$ . One can further clarify this by noticing that $\\mathrm{Pr}[r(\\mathsf{X})=1]=\\mathbb{E}_{\\mathrm{P}}[r(\\mathsf{X})]$ . In the general proper loss c ase, the optimal rejector is thresholding a generalized entropy function (known as the conditional Bayes risk [60]). This would correspond to thresholding the class probabilities $\\operatorname{P}[\\mathsf{Y}=y\\mid\\mathsf{X}=x]$ (via the Bayes posterior $h^{\\star}$ ), but with different thresholding values per class $y\\in\\mathcal{Y}$ (unless $\\ell$ is a symmetric loss function). ", "page_idx": 2}, {"type": "text", "text": "Although the objective of Eq. (2) follows the cost-based model of rejection [15], other models of rejection exist in the literature. Alternatively, the bounded improvement model of rejection [54, 29] maximizes coverage (non-rejection rate) whilst maintaining a constraint on the performance of the rejection measured by the selective risk (the first term of Eq. (2) inversely weighted by the coverage). Optimality conditions of the bounded improvement have been explored in [27]. In the bounded abstention model, the constraint and objective is switched \u2013 the selective risk is minimized with a constraint on minimum coverage [54]. Our objective function Eq. (2) (and the cost-based model) can be interpreted as a change in the type of risk considered and a change in the hard coverage constraint to a soft constraint w.r.t. bounded abstention. The optimal strategies of these approaches are explored in [28]. Alongside Section 1, further details about rejection can be found in [34]. ", "page_idx": 2}, {"type": "text", "text": "Generalized Variational Inference Generalized Variational Inference (GVI) [42] provides a framework for a generalized set of entropy regularized risk minimization problems. In particular, GVI generalizes Bayes\u2019 rule by interpreting the Bayesian posterior as the solution to a minimization problem [73]. The Bayes\u2019 rule minimization is given by, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{Q}\\in\\mathcal{D}(\\Theta)}{\\mathrm{argmin}}~\\;\\mathbb{E}_{\\theta\\sim\\mathbf{Q}}\\left[-\\sum_{i=1}^{N}\\log p(z_{i}\\mid\\theta)\\right]+\\mathrm{KL}(\\mathrm{P}\\parallel\\mathrm{Q}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta\\sim\\mathrm{Q}$ denotes a set of model parameters with likelihood function $p(x\\mid\\theta)$ and $\\{z_{i}\\}$ denotes data. Here, $\\mathrm{P}\\in\\mathcal{D}(\\Theta)$ denotes the prior and the optimal $\\mathrm{Q}^{\\star}$ denotes the posterior in Bayes\u2019 rule. ", "page_idx": 2}, {"type": "text", "text": "For GVI, we generalize a number of quantities. For instance, the \u2018loss\u2019 considered can be altered from the log-likelihood $\\log p(z\\mid\\theta)$ to an alternative loss function over samples $\\{z_{i}\\}$ . The divergence function $\\mathrm{KL}(\\mathrm{P}\\parallel\\mathrm{Q})$ can also be altered to change the notion of distributional distance. Furthermore, the set of distributions being minimized $\\mathcal{D}(\\Theta)$ can also be altered to, e.g., reduce the computational cost of the minimization problem. One can thus alter Eq. (4) to give the following: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{\\mathrm{Q}\\in\\Omega}\\;\\;L(\\mathrm{Q})+\\lambda\\cdot D(\\mathrm{P}\\parallel\\mathrm{Q}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda\\,>\\,0$ and $L,\\,D$ , and Q denote the generalized loss, divergence, and set of distribution, respectively. Here $\\lambda$ seeks to act as a regularization constant which can be tuned. ", "page_idx": 3}, {"type": "text", "text": "In our work, we will consider a GVI problem which changes the loss function and divergence to define an entropy regularized risk minimization problem. Our loss function corresponds to the learning setting. For the change in divergence, we consider $\\varphi$ -divergence [1, 19], which are otherwise referred to as $f$ -divergences [62] or the Csisz\u00e1r divergence [18]. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2. Let $\\varphi\\colon\\mathbb{R}\\rightarrow(-\\infty,\\infty]$ be a convex lower semi-continuous function with $\\varphi(1)=0$ then the corresponding $\\varphi$ -divergence over non-negative measures for $\\mathrm{{P,Q}}$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{\\varphi}(\\mathbf{P}\\parallel\\mathbf{Q})\\doteq\\int_{\\mathcal{X}}\\varphi\\left(\\frac{\\mathrm{d}\\mathbf{Q}}{\\mathrm{d}{\\mathbf{P}}}\\right)\\mathrm{d}\\mathbf{P},\\quad\\mathrm{if}\\ Q\\ll\\mathbf{P};\\qquad\\mathrm{otherwise},\\quad D_{\\varphi}(\\mathbf{P}\\parallel\\mathbf{Q})=+\\infty.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We note that for $\\varphi$ -divergences, the regularization constant in Eq. (5) can be absorbed into the $\\varphi$ -divergence generator, i.e., $\\lambda\\cdot D_{\\varphi}(\\mathrm{P}\\parallel\\bar{\\mathrm{Q}})=D_{\\lambda\\cdot\\varphi}(\\mathrm{P}\\parallel\\mathrm{Q})$ . ", "page_idx": 3}, {"type": "text", "text": "Distributionally Robust Optimization A related piece of literature is Distributionally Robust Optimization (DRO) [63], where the goal is to find a distribution that maximizes (or minimizes) the expectation of a function from a prescribed uncertainty set. Popular candidates for these uncertainty sets include all distributions that are a certain radius away from a fixed distribution by some divergence. $\\varphi$ -divergences have been used to define such uncertainty set [8, 22, 45]. Given radius $\\varepsilon>0$ , define $\\dot{B}_{\\varepsilon}^{\\varphi}(\\mathrm{P})\\stackrel{*}{=}\\{\\mathrm{Q}\\in\\mathcal{D}(\\mathcal{X}\\times\\mathcal{Y}):D_{\\varphi}(\\mathrm{P}\\parallel\\mathrm{Q})<\\varepsilon\\}$ . Given a point-wise loss function $\\ell\\colon\\mathcal{Y}\\times\\mathcal{Y}^{\\prime}\\rightarrow\\mathbb{R}$ , DRO alters risk minimization, as per Eq. (1), to solve the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{h\\in\\mathcal{H}}\\;\\operatorname*{max}_{\\bar{\\mathbf Q}_{\\mathbf{x},\\mathbf{y}}\\in B_{\\varepsilon}(\\mathbf P)}\\;\\;\\mathbb{E}_{\\bar{\\mathbf Q}_{\\mathbf{x},\\mathbf{y}}}\\left[\\ell(\\mathbf{Y},h(\\mathbf{X}))\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The max over $\\bar{\\mathrm{Q}}$ is typically over the target space $\\mathcal{Y}$ . That is, ${\\bar{\\mathsf{Q}}}(x,y)=\\mathsf{Q}(y)\\cdot\\mathrm{P}(x\\mid y)$ and the max is adversarial over the marginal label distribution [74]. Note that converting the $\\varepsilon$ -ball constraint into a Lagrange multiplier, the inner optimization over $\\mathrm{Q}$ in Eq. (7) mirrors Eq. (5). The connection between GVI and adversarial robustness has been previously noted [37]. ", "page_idx": 3}, {"type": "text", "text": "Typically in DRO and related learning settings, the construction of the adversarial distribution defined by the inner maximization problem is implicitly solved. For example, when $\\varphi$ is twice differentiable, it has been shown that the inner maximization can be reduced to a variance regularization expression [22, 21]; whereas other choices of divergences such as kernel Maximum Mean Discrepancy (MMD) yields kernel regularization [66] and Integral Probability Metrics (IPM) correspond to general regularizers [36]. Another popular choice is the Wasserstein distance which has shown strong connections to point-wise adversarial robustness [10, 11, 64]. ", "page_idx": 3}, {"type": "text", "text": "The aforementioned work, however, seek only to find the value of the inner maximization in Eq. (7) without considering the form the optimal adversarial distribution takes. ", "page_idx": 3}, {"type": "text", "text": "3 Rejection via Idealized Distributions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We propose learning rejection functions $r\\colon\\mathcal{X}\\rightarrow\\{0,1\\}$ by comparing data distributions $\\mathrm{P}$ to a learned idealized distribution $\\mathrm{Q}$ (as per Fig. 1). An idealized distribution (w.r.t. model $h$ ) is a distribution which when taken as data results in low risk (per Eq. (1)). Q are idealized rather than \u2018ideal\u2019 as they do not solely rely on a model\u2019s performance, but are also regularized by their distance from the data distribution P. Formally, we define our idealized distribution via a GVI minimization problem. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1. Given a data distribution $\\mathrm{P_{x,y}\\in\\Delta}\\mathcal{D}(\\mathcal{X}\\times\\mathcal{Y})$ and a $\\varphi$ -divergence, an idealized distribution $\\mathrm{Q}\\in\\mathcal{D}(\\mathcal{X})$ for a fixed model $h$ and loss $\\ell$ is a distribution given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{arginf}_{\\mathrm{Q}\\in\\mathcal{D}(\\mathcal{X})}~~L(\\mathrm{Q})+\\lambda\\cdot D_{\\varphi}(\\mathrm{P}~||~\\mathrm{Q}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $L(\\mathrm{Q})\\doteq\\mathbb{E}_{\\mathrm{Q}}[L^{\\prime}(\\mathsf{X})]$ and $L^{\\prime}(x)\\doteq\\mathbb{E}_{\\mathrm{P}_{\\mathrm{y|x}}}\\left[\\ell(\\mathsf{Y},h(x))\\right]$ . ", "page_idx": 3}, {"type": "text", "text": "Given the objective of Eq. (8), an idealized distribution Q will have high mass when $L^{\\prime}(x)$ is small and low mass when $L^{\\prime}(x)$ is large. The $\\varphi$ -divergence regularization term prevents the idealized distributions from collapsing to a point mass. Indeed, without regularization the distance from $\\mathrm{P}$ , idealized distributions would simply be Dirac deltas at values of $x\\in\\mathcal X$ which minimize $L^{\\prime}(x)$ . ", "page_idx": 4}, {"type": "text", "text": "With an idealized distribution Q, a rejection can be made via the density ratio [67] w.r.t. P. ", "page_idx": 4}, {"type": "table", "img_path": "JzcIKnnOpJ/tmp/2390b4ff700cc44fdd2d907dfa8ae7089d786343471b9bf5a9dd5a1a9af789a6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Definition 3.2 aims to reject inputs where the idealized rejection distribution has lower mass than the original data distribution. Given Definition 3.1 for idealized distribution, small values of $\\rho(x)$ corresponds to regions of the input space $\\mathcal{X}$ where having lower data probability would decrease the expected risk of the model. Note that we do not reject on regions with high density ratio $\\rho$ as $L^{\\prime}(x)$ would be necessarily small or the likelihood of occurrence $\\mathrm{P}(x)$ would be relatively small. We restrict the value of $\\tau$ to $(0,1]$ to ensure that rejection is focused regions where $L^{\\prime}(x)$ is high with high probability w.r.t. $\\mathrm{P}(x)$ \u2014 further noting that $\\tau=0$ always rejects. ", "page_idx": 4}, {"type": "text", "text": "Although Definitions 3.1 and 3.2 suggests that we should learn distributions $\\mathrm{Q}$ (and P) separately to make our rejection decision, in practice, we can learn the density ratio $\\mathrm{dQ}/\\mathrm{dP}$ directly. Indeed, through the definition of Definition 2.2, we have that equivalent minimization problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{\\rho:\\;\\mathcal{X}\\rightarrow\\mathbb{R}_{+}}\\;\\;\\mathbb{E}_{\\mathrm{P}_{\\mathrm{x},\\mathrm{y}}}\\left[\\rho(\\mathsf{X})\\cdot\\ell(\\mathsf{Y},h(\\mathsf{X}))+\\lambda\\cdot\\varphi(\\rho(\\mathsf{X}))\\right];\\qquad\\mathrm{s.t.}\\quad\\mathbb{E}_{\\mathrm{P}}\\left[\\rho(\\mathsf{X})\\right]=1.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Such an equivalence has been utilized in DRO previously [23, Proof of Theorem 1]. Notice that the learning density ratio $\\rho$ in Eq. (9) is analogous to the acceptor $1-r(x)$ in Eq. (2). Indeed, ignoring the normalization constraint $\\mathbb{E}_{\\mathrm{P}}\\left[\\rho(\\mathsf{X})\\right]=\\bar{\\mathsf{1}}$ , by restricting $\\rho(x)\\in\\{0,1\\}$ , letting $\\lambda=c$ , and letting $\\varphi(z)=\\mathbb{[}z=0]$ , the objective function of Eq. (9) can be reduced to: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathrm{P_{x,y}}}\\left[\\rho({\\sf X})\\cdot\\ell({\\sf Y},h({\\sf X}))\\right]+c\\cdot\\mathrm{P}\\left(\\rho({\\sf X})=0\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given the restriction of $\\rho$ to binary outputs $\\{0,1\\}$ (and Definition 3.2), we have that $r_{\\tau}(x)=1-\\rho(x)$ . As such, Eq. (10) in this setting is equivalent to the minimization of $r$ in Eq. (2) (with $\\mathcal{R}$ as all possible functions ${\\mathcal{X}}\\rightarrow\\{0,1\\}$ ). Through the specific selection of $\\varphi$ and restriction of $r$ , we have shown that rejection via idealized distributions generalizes the typical learning with rejection objective Eq. (2). ", "page_idx": 4}, {"type": "text", "text": "By utilizing form of $\\varphi$ -divergences, we find the form of the idealized rejection distributions Q and their corresponding density ratio $\\rho$ used for rejection. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3. Given Definition 3.1, the optimal density ratio function $\\rho$ of Eq. (9) is of the form, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho_{\\lambda}^{\\varphi}(x)=(\\varphi^{\\prime})^{-1}\\left({\\frac{a(x)-L^{\\prime}(x)+b}{\\lambda}}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $a(x)$ are Lagrange multipliers to ensure non-negativity $\\rho_{\\lambda}^{\\varphi}(\\cdot)\\,\\geq\\,0.$ ; and $b$ is a Lagrange multiplier to ensure the constraint $\\mathbb{E}_{\\mathrm{P}}\\left[\\rho_{\\lambda}^{\\varphi}(\\mathsf{X})\\right]=1$ is satisfied. Furthermore, the optimal idealized rejection distribution is given by: $\\mathrm{Q}^{\\varphi}(x)\\stackrel{\\cdot\\,\\cdot}{=}\\mathrm{P}(x)\\cdot\\rho_{\\lambda}^{\\varphi}(x)$ . ", "page_idx": 4}, {"type": "text", "text": "Taking $h$ as the Bayes posterior $h^{\\star}$ , $L^{\\prime}$ becomes a function of the ground truth posterior $\\operatorname*{Pr}(\\mathsf{Y}\\mid\\mathsf{X}=x)$ . Hence taking the output $h$ as a neural network plugin estimate of the underlying true posterior $\\operatorname*{Pr}(\\mathsf{Y}\\mid\\mathsf{X}=\\mathit{x})$ (see Section 4.3) yields an approach similar to softmax response, i.e., rejection based on the output of $h$ when it outputs softmax probabilities [29]. Theorem 3.3 presents a general approach to generating rejectors from these plugin estimates, as a function of $\\ell$ and $\\varphi$ . ", "page_idx": 4}, {"type": "text", "text": "Connections to GVI and DRO The formulation and solutions to the optimization of idealized distributions has several connections to GVI and DRO. In contrast to the setting of GVI, Eqs. (4) and (5), the support of the idealized distributions being learned in Definition 3.1 is w.r.t. inputs $\\mathcal{X}$ instead of parameters. Furthermore, the inner maximization of the DRO optimization problem, Eq. (7), seeks to solve a similar form of optimization. For idealized distributions, the maximization is switched to minimization and we consider a distribution over inputs $\\mathcal{X}$ instead of targets $\\mathcal{Y}$ . Indeed, notice that the explicit inner optimization of DRO (in Eq. (7)) over $\\mathrm{Q}$ can be expressed as the following via the Fan\u2019s minimax Theorem [25, Theorem 2]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\lambda>0}\\,\\operatorname*{inf}_{\\mathrm{\\ensuremath{~Q_{\\lambda}}\\in\\mathrm{{\\mathcalD}}(\\mathcal X)}}\\;-L(\\mathrm{\\ensuremath{Q_{\\lambda}}})+\\lambda\\cdot(D_{\\varphi}(\\mathrm{\\ensuremath{Q_{\\lambda}}}\\parallel\\mathrm{\\ensuremath{P}})-\\varepsilon).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notably, the loss $L(\\mathrm{Q})$ in Eq. (7) can be simply negated to make the optimization over $\\mathrm{Q}$ in Eq. (12) equivalent to DRO Eq. (8) (noting the only requirement for Eq. (7) to Eq. (12) is that $L(\\mathrm{Q})$ is a linear functional of Q). This shows that switching the sign of the loss function changes idealized distributions of Definition 3.1 to DRO adversarial distributions. Indeed, through the connection between our idealized distributions and DRO adversarial distributions, the distributions $\\mathrm{Q}^{\\varphi}(x)$ will have the same form as the optimal rejection distributions implicitly learned in DRO. ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.4. Suppose $\\mathrm{Q}_{\\lambda}^{\\varphi}$ denotes the optimal idealized distribution in Theorem 3.3 (switching $L(\\mathrm{Q})$ to $-L(\\mathrm{Q}))$ for a fixed $\\lambda\\dot{>}0$ . Further let $\\lambda^{\\star}\\,\\in\\,\\operatorname*{arginf}_{\\lambda>0}\\left\\{-L(\\mathrm{Q}_{\\lambda}^{\\varphi})+\\lambda\\cdot(D_{\\varphi}(\\mathrm{Q}_{\\lambda}^{\\varphi}\\parallel\\mathrm{P})^{\\prime}-\\varepsilon)\\right\\}$ . Then the optimal adversarial distribution in the inner minimization for DRO (Eq. (7)) is $\\mathrm{Q}_{\\lambda^{\\star}}^{\\varphi}$ . ", "page_idx": 5}, {"type": "text", "text": "As such, the various optimal idealized distributions (w.r.t. Definition 3.1) for rejection we will present in the sequel can be directly used to obtain the optimal adversarial distributions for DRO. ", "page_idx": 5}, {"type": "text", "text": "4 Learning Idealized Distributions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the following section, we explore optimal closed-form density ratio rejectors. We first examine the easiest example \u2014 the KL-divergence \u2014 and then consider the more general $\\alpha$ -divergences. ", "page_idx": 5}, {"type": "text", "text": "4.1 KL-Divergence ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let us first consider the KL-divergence [2] for constructing density ratio rejectors via Theorem 3.3. ", "page_idx": 5}, {"type": "table", "img_path": "JzcIKnnOpJ/tmp/07a5e36e4dab1a8b0c5365dd2945ff9cf16a45044abcd28fc8d203c4a7ea23d4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "One will notice that $\\rho_{\\lambda}^{\\mathrm{KL}}$ corresponds to an exponential tilt [24] of $\\mathrm{P}$ to yield a Gibbs distribution $\\mathrm{Q^{KL}}$ . The KL density ratio rejectors are significant in a few ways. First, we obtain a closed-form solution due to the properties of \u2018log\u2019 and complementary slackness, i.e., $a(\\cdot)=0$ . Secondly, due to the properties of exp, the normalization term $b$ has a closed form solution given by the typical log-normalizer term of exponential families. ", "page_idx": 5}, {"type": "text", "text": "Another notable property of utilizing a KL idealized distribution is that it recovers the previously mentioned optimal rejection policies for classical modelling with rejection via cost penalty. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2 (Informal). Given the CPE setting Theorem 2.1, if $h=h^{\\star}$ is optimal, then there exists a r\u03c4 $r_{\\tau}^{\\mathrm{KL}}$ which is equivalent to the optimal rejectors in Theorem 2.1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2 states that the KL density rejectors (with correctly specified $\\lambda$ and $\\tau$ ) with optimal predictors $h^{\\star}$ recovers the optimal rejectors of the typical rejection setting, i.e., Chow\u2019s rule. ", "page_idx": 5}, {"type": "text", "text": "Until now, we have implicitly assumed that the true data distribution $\\mathrm{P}$ is accessible. In practice, we only have access empirical $\\hat{\\mathrm{P}}_{N}$ , defining subsequent rejectors $\\hat{\\rho}_{\\lambda,N}$ . We show that for the KL rejector, $\\hat{\\mathrm{P}}_{N}$ is enough given the following generalization bound. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3. Assume we have bounded loss $|\\ell(\\cdot,\\cdot)|\\,\\le\\,B$ for $B\\,>\\,0;$ , $\\hat{\\mathrm{P}}_{N}\\subset\\mathrm{P}$ with h.p., and $\\mathcal{T}\\subset\\mathrm{Supp}(\\mathrm{P})$ . Suppose $M=|\\mathcal{T}|<+\\infty,$ , then with probability $1-\\delta$ , we have that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathbb{Y}}\\big|\\rho_{\\lambda}^{\\mathrm{KL}}(x)-\\hat{\\rho}_{\\lambda,N}^{\\mathrm{KL}}(x)\\big|\\leq C\\cdot\\sqrt{\\frac{2}{N}\\log\\Big(\\frac{2M}{\\delta}\\Big)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C=\\exp\\left({B}/{\\lambda}\\right)^{3}\\cdot\\sinh\\left({B}/{\\lambda}\\right)$ ", "page_idx": 5}, {"type": "text", "text": "Looking at Theorem 4.3, essentially we pay a price in generalization $M$ for each element $x\\in\\upmathscr{T}$ we are testing for rejection. For generalization, it is useful to consider how $N,M$ changes our rate in Theorem 4.3. If we assume that the test set $\\upsigma$ is small in comparison to the $N$ samples used to generate empirical distribution $\\hat{\\mathrm{P}}$ , then the $\\mathcal{O}(1/\\sqrt{N})$ rate will dominate. A concrete example\u221a of this case is when $|\\mathcal X|$ is finite. A less advantaged scenario is when $M\\approx N$ , yielding $\\mathcal{O}(\\log(N)/\\sqrt{N})-$ the scenario where we test approximately the same number of data points as that used to learn the rejector. This rate will still decrease with $N\\rightarrow\\infty$ , although with a $\\log N$ price. ", "page_idx": 5}, {"type": "text", "text": "4.2 Alpha-Divergences ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Although the general case of finding idealized rejection distributions for $\\varphi$ -divergences is difficult, we examine a specific generalization of the KL case, the $\\alpha$ -divergences [3]. ", "page_idx": 6}, {"type": "text", "text": "Definition 4.4. For $\\alpha\\in\\mathbb R$ , the $\\alpha$ -divergence $D_{\\alpha}$ is defined as the $\\varphi_{\\alpha}$ -divergence, where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\varphi_{\\alpha}(z)\\doteq\\left\\{\\begin{array}{l l}{\\frac{4}{1-\\alpha^{2}}\\left(1-z^{\\frac{1+\\alpha}{2}}\\right)-\\frac{2}{1-\\alpha}(z-1)}&{\\mathrm{if~}\\alpha\\ne\\pm1}\\\\ {-\\log z+(z-1)}&{\\mathrm{if~}\\alpha=-1\\,\\cdot}\\\\ {z\\log z-(z-1)}&{\\mathrm{if~}\\alpha=1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We further define $\\psi_{\\alpha}$ , where $\\psi_{\\alpha}(z)=z^{\\frac{1-\\alpha}{2}}$ when $\\alpha\\neq1$ and $\\psi_{\\alpha}(z)=\\log z$ when $\\alpha=1$ . ", "page_idx": 6}, {"type": "text", "text": "Note that taking $\\alpha\\,=\\,1$ recover the KL-divergence. The $\\alpha$ -divergence covers a wide range of divergences including the Pearson $\\chi^{2}$ divergence $(\\alpha=3)$ ). For the density ratio, $\\alpha$ -divergences with $\\alpha\\neq1$ (i.e. not KL) can be characterized as the following. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.5. Let $\\alpha\\neq1$ and $\\lambda>0$ . For $\\varphi_{\\alpha}$ , the optimal density ratio rejector $\\rho_{\\lambda}^{\\alpha}(x)$ is, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho_{\\lambda}^{\\alpha}(x)=\\psi_{\\alpha}^{-1}\\left(\\frac{2}{\\alpha-1}\\cdot\\left(a(x)-\\frac{L^{\\prime}(x)}{\\lambda}+b\\right)^{-1}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$a(x)$ are Lagrange multipliers for positivity; and $b$ is a Lagrange multiplier for normalization. ", "page_idx": 6}, {"type": "text", "text": "One major downside of using general $\\varphi$ -divergences is that solving the Lagrange multipliers for the idealized rejection distribution is often difficult. Indeed, the \u201clog\u201d and \u201cexp\u201d ensures non-negativity of the idealized distribution when the input data $\\mathrm{P}$ is in the interior of the simplex; and also provides a convenient normalization calculation. For $\\alpha$ -divergences, the non-negative Lagrange multipliers $a(\\cdot)$ can be directly solved given certain conditions. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.6. Suppose $\\alpha>1$ and $\\lambda>0$ , then Eq. (14) simplifies to, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho_{\\lambda}^{\\alpha}(x)=\\left[\\left(\\frac{\\alpha-1}{2}\\cdot\\left(b-\\frac{L^{\\prime}(x)}{\\lambda}\\right)\\right)^{\\frac{2}{\\alpha-1}}\\right]_{+},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we take non-integer powers of negative values as $\\boldsymbol{O}$ . ", "page_idx": 6}, {"type": "text", "text": "On the other hand, for $\\alpha\\leq-1$ , $D_{\\alpha}(\\mathrm{P~}\\|\\mathrm{~Q~})=\\infty$ whenever $\\mathrm{Q}$ is on the boundary whenever $\\mathrm{P}$ is not on the boundary [3, Section 3.4.1]. As such, we can partially simplify Eq. (14) for $\\alpha\\leq-1$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.7. Suppose $\\alpha\\leq-1$ , $\\lambda>0$ , and $\\mathrm{P}$ lies in the simplex interior, then $a(\\cdot)=0$ in Eq. (14). ", "page_idx": 6}, {"type": "text", "text": "Both Corollaries 4.6 and 4.7 can provide a unique rejector policy than the KL-divergence variant. Corollary 4.7 can provide a similar effect when $a(x)\\neq0$ for all $x$ . Nevertheless, having to determine which inputs $a(x)\\neq0$ and solving these values are difficult in practice. As such, we focus on $\\alpha>0$ If there are values of $L^{\\prime}(x)$ with high risk, the max will flatten these inputs to 0 in Corollary 4.6. However, if the original model $h$ performs well and $L^{\\prime}(x)$ is relatively small for all $x$ , then it is possible that the max is not utilized. In such a case, the $\\alpha$ -divergence rejectors can end up being similar \u2014 this follows the fact that (as $\\varphi_{\\alpha}^{\\prime\\prime}(1)>0,$ ) locally all $\\alpha$ -divergences will be similar to the $\\chi^{\\breve{2}}$ $/\\left(\\alpha=3\\right)$ -divergence [56, Theorem 7.20]. This ultimately results in $\\alpha$ -divergences being similar to, e.g., Chow\u2019s rule when $h_{y}(x)\\approx\\operatorname*{Pr}(\\mathsf{Y}=y\\mid\\mathsf{X}=x)$ in classification via Theorem 2.1. ", "page_idx": 6}, {"type": "text", "text": "Among the $\\alpha>0$ cases, we examine the $\\chi^{2}$ -divergence $(\\alpha=3)$ ) which results in closed form for bounded loss functions and sufficient large regularizing parameter $\\lambda$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.8. Suppose that $\\ell(\\cdot,\\cdot)\\le B$ and $\\lambda>\\operatorname*{max}_{x}L^{\\prime}(x)-\\mathbb{E}_{\\mathrm{P}}[L^{\\prime}(x)]$ . Then, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho_{\\lambda}^{\\alpha=3}(x)=1+\\frac{\\mathbb{E}_{\\mathrm{P}}[L^{\\prime}(x)]-L^{\\prime}(x)}{\\lambda}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The condition on $\\lambda$ for Corollary 4.8 is equivalent to rescaling bounded loss functions $\\ell$ . Indeed, by fixing $\\lambda=1$ , we can achieve a similar Theorem with suitable rescaling of $\\ell$ . Nevertheless, Eq. (16) provides a convenient form to allow for generalization bounds to be established. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.9. Assume we have bounded loss $|\\ell(\\cdot,\\cdot)|\\le B$ for $B>0$ , $\\lambda>2B,$ , $\\hat{\\mathrm{P}}_{N}\\subset\\mathrm{P}$ with h.p., and $\\mathcal{T}\\subset\\mathrm{Supp}(\\mathrm{P})$ . Suppose $M=|\\mathcal{T}|<+\\infty$ , then with probability $1-\\delta$ , we have that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathbb{T}}\\left|\\rho_{\\lambda}^{\\alpha=3}(x)-\\hat{\\rho}_{\\lambda}^{\\alpha=3}(x)\\right|\\leq\\frac{B}{\\lambda}\\cdot\\sqrt{\\frac{2}{N}\\log\\left(\\frac{2M}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Notice that Theorem 4.9\u2019s sample complexity is equivalent to Theorem 4.3 up to constant multiplication. Hence, the analysis of Theorem 4.3 regarding the scales of $N,M$ hold for Theorem 4.9. ", "page_idx": 7}, {"type": "text", "text": "A question pertaining to DRO is what would be the generalization capabilities of the corresponding adversarial distributions $\\hat{\\mathrm{Q}}_{\\lambda,N}\\,\\doteq\\,\\hat{\\mathrm{P}}_{N}\\,\\cdot\\,\\hat{\\rho}_{\\lambda,N}$ (through Corollary 3.4). On a finite domain, via Theorems 4.3 and 4.9 and\u221a a simple triangle inequality, one can immediately bound the total variation $\\mathrm{TV}(\\hat{\\mathrm{Q}}_{\\lambda,N},\\mathrm{Q}_{\\lambda})\\leq\\mathcal{O}(1/\\sqrt{N})$ , see Appendix $\\mathbf{M}$ for further details. ", "page_idx": 7}, {"type": "text", "text": "4.3 Practical Rejection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We consider practical concerns for utilizing the KL-or $(\\alpha>1)$ -divergence case, Eqs. (13) and (15), for post-hoc rejection. To do so, we need to estimate the loss $L^{\\prime}(x)$ and rejector normalizer $Z$ or $b$ . ", "page_idx": 7}, {"type": "text", "text": "Loss: The former is tricky, we require an estimate to evaluate $L^{\\prime}(x)=\\mathbb{E}_{\\mathrm{P}_{\\mathrm{y|x}}}\\left[\\ell(\\mathsf{Y},h(x))\\right]$ over any possible $x\\in\\mathscr{X}$ to allow us to make a rejection decision. Implicitly, this requires us to have a high quality estimate of $\\mathrm{P_{y|x}}$ . In a general learning setting, this can be difficult to obtain \u2014 in fact it is just the overall objective that we are trying to learning, i.e., predicting a target $y$ given an input $x$ . However, in the case of CPE (classification), it is not unreasonable to obtain an calibrated estimate of $\\mathrm{P_{y|x}}$ via the classifier $h\\colon\\mathcal{X}\\to\\mathcal{D}(\\mathfrak{Y})$ [55]. In Section 5, we utilize temperature scaling to calibrate the neural networks we learn to provide such an estimate [32]. Hence, we set $L^{\\prime}(x)\\overset{=}{=}\\mathbb{E}_{\\mathsf{Y}\\sim h(x)}\\left[\\ell(\\mathsf{Y},h(x))\\right]$ . For proper CPE loss functions, $\\mathbb{E}_{\\mathsf{Y}\\sim h(x)}\\left[\\ell(\\mathsf{Y},h(x))\\right]$ acts as a generalized entropy function. As such, the rejectors Eqs. (13) and (15) act as functions over said generalized entropy functions. It should be noted that simply considering the softmax outputs of neural networks for probability estimation have seen prior success [29, 39]. This is equivalent to taking the 0-1-loss function [15, 35, 28] and taking a plugin estimate of probabilities via the neural network\u2019s output. The study of using plugin estimates have also been explored in the model cascade literature [40]. ", "page_idx": 7}, {"type": "text", "text": "Normalization: For the latter, we utilize a sample based estimate (over the dataset used to train the rejector) of $\\mathbb{E}_{\\mathrm{P}}\\left[\\rho_{\\lambda}^{\\varphi}(\\mathsf{X})\\right]$ is utilized to solve the normalizers $Z$ and $b$ . In the case of the KL-divergence rejector, this is all that is required due to the convenient function form of the Gibbs distribution, i.e., the normalizer $Z$ can be simply estimated by a sample mean. However, for $\\alpha>1$ -divergences $b$ needs to be found to determine the normalization. Practically, we find $b$ through bisection search of ", "page_idx": 7}, {"type": "table", "img_path": "JzcIKnnOpJ/tmp/41e13d288abf90ea0c0f6259453ed2e370c70f8f44e1d2294be88ee4877aab73.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "This practically works as the optimization $b$ is over a single dimension. Furthermore, we can have that $b>\\operatorname*{min}_{x}\\left\\{L^{\\prime}(x)/\\lambda\\right\\}$ . As an upper bound over the possible values of $b$ , we utilize a heuristic where we multiple the corresponding maximum of $L^{\\prime}(x){\\bar{\\big/}}\\lambda$ with a constant. ", "page_idx": 7}, {"type": "text", "text": "Threshold $\\tau$ : In addition to learning the density ratio, an additional consideration is how to tune $\\tau$ in the rejection decision, Definition 3.2. Given a fixed density estimator $\\rho$ , change $\\tau$ amounts to changing the rate of rejection. We note that this problem is not limited to our density ratio rejectors, where approaches with rejectors $r\\in\\mathcal{R}$ via a (surrogate) minimization of Eq. (2) may be required multiple rounds of training with different rejection costs $c$ to find an accept rate of rejection. In our case, we have a fixed $\\rho$ which allows easy tuning of $\\tau$ given a validation dataset, similar to other confidence based rejection approaches, e.g., tuning a threshold for the margin of a classifier [7]. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate our distributional approach to rejection across a number of datasets. In particular, we consider the standard classification setting with an addition setting where uniform label noise is introduced [4, 30]. For our density ratio rejectors, we evaluate the KL-divergence based rejector (Corollary 4.1) and $(\\alpha{=}3)$ -based rejectors (Corollary 4.6) with 50 equidistant $\\tau\\,\\in\\,(0,1]$ values. For our tests, we fix $\\lambda=1$ . Throughout our evaluation, we assume that a neural network (NN) model without rejection is accessible for all (applicable) approaches. For our density ratio rejectors, we utilize the log-loss, practical considerations in Section 4.3, and Algorithm 1.2 ", "page_idx": 7}, {"type": "table", "img_path": "JzcIKnnOpJ/tmp/da10fc5f7b0815def1ecbb807de063304f4df7183ac53a2112df8646bbaff7d6.jpg", "table_caption": ["Table 1: Summary of rejection methods over all baselines and datasets targeting $80\\%$ coverage. Each cell reports the \u201caccuracy [coverage]\u201d values, bold for most accurate, and s.t.d. reported in Appendix. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "To evaluate our density ratio rejectors and baselines, we compare accuracy and acceptance coverage. Accuracy corresponds to the 0-1 loss in Eq. (1) and the acceptance coverage is the percentage of non-rejections in the test set. Ideally, a rejector smoothly trade-offs between accuracy and acceptance, i.e., a higher accuracy can be achieved by decreasing the acceptance coverage by rejecting more data. We study this trade-off by examining multiple cut-off values $\\tau$ and rejection costs $c$ . ", "page_idx": 8}, {"type": "text", "text": "Dataset and Baselines We consider 6 multiclass classification datasets. For tabular datasets, we consider the gas drift dataset [68] and the human activity recognition (HAR) dataset [5]. Each of these datasets consists of 6 classes to predict. Furthermore, we utilize a two hidden layer MLP NN model for these datasets. We consider the MNIST image dataset [46] (10 classes), where we utilize a convolutional NN. Additionally, we consider 3 larger image datasets with ResNet-18 architecture [33]: CIFAR-10 [44] (10 classes); and OrgMNIST / OrganSMNIST (11 classes) and OctMNIST (4 classes) from the MedMNIST collection [69, 70]. These prediction models are trained utilizing the standard logistic / log-loss without rejection and then are calibrated via temperature scaling [32]. For each of these datasets, we utilize both clean and noisy variants. For the noisy variant, we flip the class labels of the train set with a rate of $25\\%$ . We note that the test set is clean in both cases. All evaluation uses 5-fold cross validation. All implementation use PyTorch and training was done on a p3.2xlarge AWS instance. ", "page_idx": 8}, {"type": "text", "text": "We consider 4 different baseline for comparison. Each is trained with 50 equidistant costs $c,\\tau\\in$ [0, 0.5), except on OctMNIST which uses 10 equidistant costs (selecting $c,\\tau$ discussed in Appendix P). One baseline used corresponds to a modification of [49]\u2019s cross-entropy surrogate approach (DEFER) originally used for the learning to defer literature (see [14, Appendix A.2]). This approach treats the rejection option as a separate class and solves a $|\\y|+1$ classification problem. A generalization of DEFER is considered which utilizes generalized cross-entropy [75] as its surrogate loss (GCE) [13]. We also consider a cost-sensitive classification reduction (CSS) of the classification with rejection problem [14] utilizing the sigmoid loss function. The aforementioned 3 baselines all learn a model with rejection simultaneously, i.e., a pretrained model cannot be utilized. We also consider a two stage predictor-rejector approach (PredRej) which learns a rejector from a pretrained classifier [48]. ", "page_idx": 8}, {"type": "text", "text": "Results Table 1 presents a tabular summary of accuracy and coverage values when targeting $80\\%$ coverage values; and Fig. 2 presents a summary plot of the acceptance coverage versus model accuracy after rejection, focused around the $60\\%$ to $100\\%$ coverage region. This plot is limited to HAR, Gas Drift, and MNIST due to space limitations however the deferred datasets show curves where the density ratio rejector dominate with better accuracy and coverage in the plotted region, with corresponding extended plots for all datasets in Appendix Q . Over all folds for MNIST our density ratio rejectors take approximately $\\approx1/2$ hour to fti. A single baseline (fixed $c$ ) takes upwards of 2 hour for a single fold. Overall, given that the underlying model is calibrated, we find that our density ratio rejector are either competitive or superior to the baselines. One might notice that the aforementioned baselines do not or provide poor trade-offs for coverage values $>95\\%$ (as per Fig. 2). Indeed, to achieve rejection with high coverage (without architecture tuning), approaches which wrap\u2019 a base classifier seem preferable, i.e., PredRej and our density ratios rejectors. Even at lower coverage targets $(80\\%)$ , Table 1 shows that density-ratio methods are comparable or superior in the more complex datasets of CIFAR-10 and the MedMNIST collection. If large models are allowed to be used for the rejector \u2014 as per the MNIST case \u2014 CSS, DEFER, and GCE can provide superior accuracy vs acceptance coverage trade-offs (noisy MNIST). However, this is not always true as per CIFAR-10 where all approaches are similarly effected by noise; or OctMNIST and OrgMNIST where approaches only slightly change with noise. The latter appears to be a consequence of label noise not effecting the Base classifier\u2019s accuracy (using the larger ResNet-18 architecture), as per Table 1. ", "page_idx": 8}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/6ef9e883cabb84604f865b9f0e70f6e662e706848ebc946eed4885fd21745939.jpg", "img_caption": ["Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant $\\tau\\in(0,1]$ and $c\\in[0,0.5)$ values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than $60\\%$ of test points or has accuracy below the base model. Shaded region indicates $\\pm1$ s.t.d. region. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Among the approaches which \u2018wrap\u2019 the base classifier $h$ , we find that these approaches have higher variance ranges than the other approaches. In particular, the randomness of the base model potentially magnifies the randomness after rejection. The variance range of the base model tends to increase as the noise increases (additional ranges of noise for HAR and Gas Drift are presented in the Appendix). The influence on rejection is unsurprising as these \u2018wrapping\u2019 approaches predict via a composition of the original model (and hence inherits its randomness across folds). In general, our density ratio rejector outperforms PredRej. However, it should be noted that PredRej does not require a calibrated classifier. Among the density ratio rejectors, between KL and $\\alpha=3$ , the only variation is in coverage region that the $\\tau\\in(0,1]$ threshold covers. This follows from the fact that for similar distributions, $\\varphi$ -divergences act similarly [56, Theorem 7.20]. We find this pattern holds for other values of $\\alpha$ . ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose a new framework for rejection by learning idealized density ratios. Our proposed rejection framework links typically explored classification with rejection to generalized variational inference and distributionally robust optimization. It should be noted that although we have focused on classification, $L^{\\prime}(\\dot{\\mathrm{Q}})$ could in theory be replaced by any other loss functions. In this sense, one could adapt this approach to other learning tasks such as regression, discussed in Appendix N. Furthermore, although we have focused on $\\varphi$ -divergences, there are many alternative ways idealized distribution can be constructed, e.g., integral probability metrics [51, 9]. One limitation of our distributional way of rejecting is the reliance on approximating $\\mathrm{P}[\\mathsf{Y}\\mid\\mathsf{X}]$ with model $h$ . In future work, one may seek to approximate the density ratio $\\rho$ by explicitly learning densities $\\mathrm{Q}$ and $\\mathrm{P}$ or via gradient based methods (for the latter, see Appendix O). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the reviewers and the area chair for the various suggestions which improved the paper. A majority of the work was done whilst AS was interning at Amazon. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distribution from another. Journal of the Royal Statistical Society: Series B (Methodological), 28(1):131\u2013142, 1966.   \n[2] S.-I. Amari and H. Nagaoka. Methods of Information Geometry. Oxford University Press, 2000.   \n[3] Shun-ichi Amari. Information geometry and its applications, volume 194. Springer, 2016.   \n[4] Dana Angluin and Philip Laird. Learning from noisy examples. Machine learning, 2:343\u2013370, 1988.   \n[5] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge Luis Reyes-Ortiz, et al. A public domain dataset for human activity recognition using smartphones. In Esann, volume 3, page 3, 2013.   \n[6] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.   \n[7] Peter L Bartlett and Marten H Wegkamp. Classification with a reject option using a hinge loss. Journal of Machine Learning Research, 9(8), 2008.   \n[8] Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. Management Science, 59(2):341\u2013357, 2013.   \n[9] Jeremiah Birrell, Paul Dupuis, Markos A Katsoulakis, Yannis Pantazis, and Luc Rey-Bellet. $(f,\\Gamma)$ -Divergences: interpolating between $f$ -divergences and integral probability metrics. The Journal of Machine Learning Research, 23(1):1816\u20131885, 2022.   \n[10] Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport. Mathematics of Operations Research, 44(2):565\u2013600, 2019.   \n[11] Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein profile inference and applications to machine learning. Journal of Applied Probability, 56(3):830\u2013857, 2019.   \n[12] St\u00e9phane Boucheron, G\u00e1bor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymptotic Theory of Independence. Oxford University Press, 2013.   \n[13] Yuzhou Cao, Tianchi Cai, Lei Feng, Lihong Gu, Jinjie Gu, Bo An, Gang Niu, and Masashi Sugiyama. Generalizing consistent multi-class classification with rejection to be compatible with arbitrary losses. Advances in Neural Information Processing Systems, 35:521\u2013534, 2022.   \n[14] Nontawat Charoenphakdee, Zhenghang Cui, Yivan Zhang, and Masashi Sugiyama. Classification with rejection based on cost-sensitive classification. In International Conference on Machine Learning, pages 1507\u20131517, 2021.   \n[15] C Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on information theory, 16(1):41\u201346, 1970.   \n[16] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Boosting with abstention. Advances in Neural Information Processing Systems, 29, 2016.   \n[17] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Learning with rejection. In Algorithmic Learning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016, Proceedings 27, pages 67\u201382. Springer, 2016.   \n[18] Imre Csisz\u00e1r. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der ergodizitaet von markoffschen ketten. Magyer Tud. Akad. Mat. Kutato Int. Koezl., 8:85\u2013108, 1964.   \n[19] Imre Csisz\u00e1r. Information-type measures of difference of probability distributions and indirect observation. studia scientiarum Mathematicarum Hungarica, 2:229\u2013318, 1967.   \n[20] Imre Csisz\u00e1r, Paul C Shields, et al. Information theory and statistics: A tutorial. Foundations and Trends\u00ae in Communications and Information Theory, 1(4):417\u2013528, 2004.   \n[21] John Duchi and Hongseok Namkoong. Variance-based regularization with convex objectives. The Journal of Machine Learning Research, 20(1):2450\u20132504, 2019.   \n[22] John C Duchi, Peter W Glynn, and Hongseok Namkoong. Statistics of robust optimization: A generalized empirical likelihood approach. Mathematics of Operations Research, 46(3): 946\u2013969, 2021.   \n[23] Krishnamurthy Dj Dvijotham, Jamie Hayes, Borja Balle, Zico Kolter, Chongli Qin, Andras Gyorgy, Kai Xiao, Sven Gowal, and Pushmeet Kohli. A framework for robustness certification of smoothed classifiers using f-divergences. In International Conference on Learning Representations, 2019.   \n[24] Bradley Efron. Exponential families in theory and practice. Cambridge University Press, 2022.   \n[25] Ky Fan. Minimax theorems. Proceedings of the National Academy of Sciences of the United States of America, 39(1):42, 1953.   \n[26] Leo Feng, Mohamed Osama Ahmed, Hossein Hajimirsadeghi, and Amir H. Abdi. Towards better selective classification. In The Eleventh International Conference on Learning Representations, 2023.   \n[27] Vojtech Franc and Daniel Prusa. On discriminative learning of prediction uncertainty. In International Conference on Machine Learning, pages 1963\u20131971, 2019.   \n[28] Vojtech Franc, Daniel Prusa, and Vaclav Voracek. Optimal strategies for reject option classifiers. Journal of Machine Learning Research, 24(11):1\u201349, 2023.   \n[29] Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. Advances in neural information processing systems, 30, 2017.   \n[30] Aritra Ghosh, Naresh Manwani, and PS Sastry. Making risk minimization tolerant to label noise. Neurocomputing, 160:93\u2013107, 2015.   \n[31] Yves Grandvalet, Alain Rakotomamonjy, Joseph Keshet, and St\u00e9phane Canu. Support vector machines with a reject option. Advances in neural information processing systems, 21, 2008.   \n[32] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330, 2017.   \n[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[34] Kilian Hendrickx, Lorenzo Perini, Dries Van der Plas, Wannes Meert, and Jesse Davis. Machine learning with a reject option: A survey. Machine Learning, 113(5):3073\u20133110, 2024.   \n[35] Radu Herbei and Marten H Wegkamp. Classification with reject option. The Canadian Journal of Statistics/La Revue Canadienne de Statistique, pages 709\u2013721, 2006.   \n[36] Hisham Husain. Distributional robustness with ipms and links to regularization and gans. Advances in Neural Information Processing Systems, 33:11816\u201311827, 2020.   \n[37] Hisham Husain and Jeremias Knoblauch. Adversarial interpretation of bayesian inference. In International Conference on Algorithmic Learning Theory, pages 553\u2013572, 2022.   \n[38] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448\u2013456, 2015.   \n[39] Paul F Jaeger, Carsten Tim L\u00fcth, Lukas Klein, and Till J. Bungert. A call to reflect on evaluation practices for failure detection in image classification. In The Eleventh International Conference on Learning Representations, 2023.   \n[40] Wittawat Jitkrittum, Neha Gupta, Aditya K Menon, Harikrishna Narasimhan, Ankit Rawat, and Sanjiv Kumar. When does confidence-based cascade deferral suffice? Advances in Neural Information Processing Systems, 36, 2024.   \n[41] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on Learning Representations, 2014.   \n[42] Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. An optimization-centric view on bayes\u2019 rule: Reviewing and generalizing variational inference. The Journal of Machine Learning Research, 23(1):5789\u20135897, 2022.   \n[43] Donald E Knuth. Two notes on notation. The American Mathematical Monthly, 99(5):403\u2013422, 1992.   \n[44] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Toronto, ON, Canada, 2009.   \n[45] Henry Lam. Robust sensitivity analysis for stochastic systems. Mathematics of Operations Research, 41(4):1248\u20131275, 2016.   \n[46] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online], 2, 2010.   \n[47] Naresh Manwani, Kalpit Desai, Sanand Sasidharan, and Ramasubramanian Sundararajan. Double ramp loss based reject option classifier. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 151\u2013163. Springer, 2015.   \n[48] Anqi Mao, Mehryar Mohri, and Yutao Zhong. Predictor-rejector multi-class abstention: Theoretical analysis and algorithms. In International Conference on Algorithmic Learning Theory, pages 822\u2013867, 2024.   \n[49] Hussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. In International Conference on Machine Learning, pages 7076\u20137087, 2020.   \n[50] Hussein Mozannar, Hunter Lang, Dennis Wei, Prasanna Sattigeri, Subhro Das, and David Sontag. Who should predict? exact algorithms for learning to defer to humans. In International Conference on Artificial Intelligence and Statistics, pages 10520\u201310545, 2023.   \n[51] Alfred M\u00fcller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):429\u2013443, 1997.   \n[52] Harikrishna Narasimhan, Wittawat Jitkrittum, Aditya K Menon, Ankit Rawat, and Sanjiv Kumar. Post-hoc estimators for learning to defer to an expert. Advances in Neural Information Processing Systems, 35:29292\u201329304, 2022.   \n[53] Chenri Ni, Nontawat Charoenphakdee, Junya Honda, and Masashi Sugiyama. On the calibration of multiclass classification with rejection. Advances in Neural Information Processing Systems, 32, 2019.   \n[54] Tadeusz Pietraszek. Optimizing abstaining classifiers using roc analysis. In Proceedings of the 22nd international conference on Machine learning, pages 665\u2013672, 2005.   \n[55] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61\u201374, 1999.   \n[56] Yury Polyanskiy and Yihong Wu. Information theory: From coding to learning. Cambridge university press, 2024.   \n[57] Andrea Pugnana and Salvatore Ruggieri. A model-agnostic heuristics for selective classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 9461\u20139469, 2023.   \n[58] Andrea Pugnana, Lorenzo Perini, Jesse Davis, and Salvatore Ruggieri. Deep neural network benchmarks for selective classification. Journal of Data-centric Machine Learning Research, 2024.   \n[59] H. G. Ramaswamy, Ambuj Tewari, and Shivani Agarwal. Consistent algorithms for multiclass classification with an abstain option. Electronic Journal of Statistics, 12:530\u2013554, 2018.   \n[60] Mark D Reid and Robert C Williamson. Composite binary losses. The Journal of Machine Learning Research, 11:2387\u20132422, 2010.   \n[61] Mark D Reid and Robert C Williamson. Information, divergence and risk for binary experiments. Journal of Machine Learning Research, 12:731\u2013817, 2011.   \n[62] Igal Sason and Sergio Verd\u00fa. $f$ -divergence inequalities. IEEE Transactions on Information Theory, 62(11):5973\u20136006, 2016.   \n[63] Herbert E Scarf. A min-max solution of an inventory problem. Technical report, RAND CORP SANTA MONICA CALIF, 1957.   \n[64] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. In International Conference on Learning Representations, 2018.   \n[65] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929\u20131958, 2014.   \n[66] Matthew Staib and Stefanie Jegelka. Distributionally robust optimization and generalization in kernel methods. In Advances in Neural Information Processing Systems, pages 9131\u20139141, 2019.   \n[67] Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine learning. Cambridge University Press, 2012.   \n[68] Alexander Vergara, Shankar Vembu, Tuba Ayhan, Margaret A Ryan, Margie L Homer, and Ram\u00f3n Huerta. Chemical gas sensor drift compensation using classifier ensembles. Sensors and Actuators B: Chemical, 166:320\u2013329, 2012.   \n[69] Jiancheng Yang, Rui Shi, and Bingbing Ni. Medmnist classification decathlon: A lightweight automl benchmark for medical image analysis. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 191\u2013195. IEEE, 2021.   \n[70] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. Scientific Data, 10(1):41, 2023.   \n[71] Ming Yuan and Marten Wegkamp. Classification methods with reject option based on convex risk minimization. Journal of Machine Learning Research, 11(1), 2010.   \n[72] Ahmed Zaoui, Christophe Denis, and Mohamed Hebiri. Regression with reject option and application to knn. Advances in Neural Information Processing Systems, 33:20073\u201320082, 2020.   \n[73] Arnold Zellner. Optimal information processing and bayes\u2019s theorem. American Statistician, pages 278\u2013280, 1988.   \n[74] Jingzhao Zhang, Aditya Krishna Menon, Andreas Veit, Srinadh Bhojanapalli, Sanjiv Kumar, and Suvrit Sra. Coping with label shift via distributionally robust optimisation. In International Conference on Learning Representations, 2021.   \n[75] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. Advances in neural information processing systems, 31, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This is the Supplementary Material to Paper \"Rejection via Learning Density Ratios\". To differentiate with the numberings in the main file, the numbering of Theorems is letter-based (A, B, ...). ", "page_idx": 14}, {"type": "text", "text": "Table of contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "$\\hookrightarrow$ Appendix A: Proof of Theorem 2.1 Pg 16   \n$\\hookrightarrow$ Appendix B: Proof of Theorem 3.3 Pg 16   \n$\\hookrightarrow$ Appendix C: Proof of Corollary 3.4 Pg 17   \n$\\hookrightarrow$ Appendix D: Proof of Corollary 4.1 Pg 17   \n$\\hookrightarrow$ Appendix E: Proof of Theorem 4.2 Pg 17   \n$\\hookrightarrow$ Appendix F: Proof of Theorem 4.3 Pg 18   \n$\\hookrightarrow$ Appendix G: Proof of Theorem 4.5 Pg 19   \n$\\hookrightarrow$ Appendix H: Proof of Corollary 4.6 Pg 21   \n$\\hookrightarrow$ Appendix I: Proof of Corollary 4.7 Pg 21   \n$\\hookrightarrow$ Appendix J: Proof of Corollary 4.8 Pg 21   \n$\\hookrightarrow$ Appendix K: Proof of Theorem 4.9 Pg 22 ", "page_idx": 14}, {"type": "text", "text": "Deferred Content ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "$\\hookrightarrow$ Appendix L: Broader Impact Pg 22   \n$\\hookrightarrow$ Appendix M: Distribution Generalization Bounds Pg 22   \n$\\hookrightarrow$ Appendix N: Rejection for Regression Pg 23   \n$\\hookrightarrow$ Appendix O: Gradient of Density Ratio Objective Pg 24   \n$\\hookrightarrow$ Appendix P: Finding the Best Rejection Cost Pg 24   \n$\\hookrightarrow$ Appendix Q: Additional Experimental Details Pg 25 ", "page_idx": 14}, {"type": "text", "text": "A Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. We first rewrite the coverage probability as an expectation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}}[(1-r(\\mathsf{X}))\\cdot\\ell(\\mathsf{Y},h(\\mathsf{X}))]+c\\cdot\\operatorname*{Pr}[r(\\mathsf{X})=1]=\\mathbb{E}_{\\mathbb{P}}[(1-r(\\mathsf{X}))\\cdot\\ell(\\mathsf{Y},h(\\mathsf{X}))+c\\cdot r(\\mathsf{X})]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We note that point-wise, the proper loss $\\ell$ is minimized by taking the Bayes optimal classifier $\\eta^{\\star}(x)=\\operatorname*{Pr}[\\mathsf{Y}=+1\\mid\\mathsf{X}=x]$ . Thus taking the argmin over all possible CPE classifiers, $h^{\\star}=\\eta^{\\star}$ . We note that the point-wise risk taken by the Bayes optimal classifier is typical denoted as the Bayes point-wise risk $\\underline{{L}}(x)=\\mathbb{E}_{\\mathrm{P}_{\\mathrm{y}\\mid\\mathrm{x}=\\mathrm{x}}}[\\ell(\\mathsf{Y},\\eta^{\\star}(x))]$ [60, 61]. ", "page_idx": 15}, {"type": "text", "text": "As such, we are left to minimize $r$ over, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathrm{P}}[(1-r(\\mathsf{X}))\\cdot\\ell(\\mathsf{Y},\\eta^{\\star}(\\mathsf{X}))+c\\cdot r(\\mathsf{X})]}\\\\ &{=\\mathbb{E}_{\\mathrm{P}_{\\mathrm{x}}}\\mathbb{E}_{\\mathrm{P}_{\\mathrm{y}\\mid\\mathrm{x}=\\mathsf{X}}}[(1-r(\\mathsf{X}))\\cdot\\ell(\\mathsf{Y},\\eta^{\\star}(\\mathsf{X}))+c\\cdot r(\\mathsf{X})]}\\\\ &{=\\mathbb{E}_{\\mathrm{P}_{\\mathrm{x}}}[(1-r(\\mathsf{X}))\\cdot\\underline{{L}}(\\mathsf{X})+c\\cdot r(\\mathsf{X})]}\\\\ &{=\\mathbb{E}_{\\mathrm{P}_{\\mathrm{x}}}[\\underline{{L}}(\\mathsf{X})]+\\mathbb{E}_{\\mathrm{P}_{\\mathrm{x}}}\\left[r(\\mathsf{X})\\cdot(c-\\underline{{L}}(\\mathsf{X}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, we immediately get the optimal $r^{\\star}(x)=\\mathbb{L}(x)\\geq c\\mathbb{I}$ . ", "page_idx": 15}, {"type": "text", "text": "B Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. We use the theory of Lagrange multipliers to make different constraints explicit optimization problems. ", "page_idx": 15}, {"type": "text", "text": "Let us first consider the reduction from learing explicit distributions Eq. (8) to density ratios Eq. (9). First note that the objective Eq. (8) can be written as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{L(\\mathbf{Q})+\\lambda\\cdot D_{\\varphi}(\\mathbf{P}\\parallel\\mathbf{Q})}\\\\ &{=\\int L^{\\prime}(x)\\mathrm{d}\\mathbf{Q}(x)+\\lambda\\cdot\\int\\varphi\\left({\\frac{\\mathrm{d}\\mathbf{Q}}{\\mathrm{d}\\mathbf{P}}}\\right)\\mathrm{d}\\mathbf{P}(x)}\\\\ &{=\\int L^{\\prime}(x){\\frac{\\mathrm{d}\\mathbf{Q}}{\\mathrm{d}\\mathbf{P}}}(x)\\mathrm{d}\\mathbf{P}(x)+\\lambda\\cdot\\int\\varphi\\left({\\frac{\\mathrm{d}\\mathbf{Q}}{\\mathrm{d}\\mathbf{P}}}(x)\\right)\\mathrm{d}\\mathbf{P}(x)}\\\\ &{=\\mathbb{E}_{\\mathbf{P}}\\left[L^{\\prime}(x)\\cdot{\\frac{\\mathrm{d}\\mathbf{Q}}{\\mathrm{d}\\mathbf{P}}}(x)+\\lambda\\cdot\\varphi\\left({\\frac{\\mathrm{d}\\mathbf{Q}}{\\mathrm{d}\\mathbf{P}}}(x)\\right)\\right]}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The reduction to Eq. (9) now follows from a reduction from minimizing over $\\mathrm{Q}$ to $\\mathrm{dQ}/\\mathrm{dP}$ , noting that $\\mathrm{Q}$ is restricted to be on the simplex. As such, the simplex constraints are transfered to: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathrm{Q}}{\\mathrm{d}\\mathrm{P}}\\geq0\\quad\\mathrm{and}\\quad\\int\\mathrm{d}\\mathrm{P}(x)\\cdot\\frac{\\mathrm{d}\\mathrm{Q}}{\\mathrm{d}\\mathrm{P}}(x)=\\mathbb{E}_{\\mathrm{P}}\\left[\\frac{\\mathrm{d}\\mathrm{Q}}{\\mathrm{d}\\mathrm{P}}({\\mathsf{X}})\\right]=1,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the former is the non-negativity of simplex elements and the latter is the normalization requirement. Hence, taking $\\begin{array}{r}{\\rho=\\bar{\\rho}_{\\lambda}\\doteq\\frac{\\mathrm{d}\\mathrm{Q}}{\\mathrm{d}\\mathrm{P}}}\\end{array}$ completes the reduction. (we remove the subscript \u201c $\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ for the rest of the proof) ", "page_idx": 15}, {"type": "text", "text": "As such we have the optimization problem in Eq. (9), where we will convert the constraints into Lagrange multipliers, defining, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\rho;a,b)=\\mathbb{E}_{\\mathrm{P}}\\left[\\rho(\\mathsf{X})\\cdot L^{\\prime}(\\mathsf{X})+\\lambda\\cdot\\varphi(\\rho(\\mathsf{X}))\\right]-\\int a^{\\prime}(x)\\rho(x)\\mathrm{d}x+b\\cdot\\left(1-\\mathbb{E}_{\\mathrm{P}}[\\rho(\\mathsf{X})]\\right)}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\mathrm{P}}\\left[\\rho(\\mathsf{X})\\cdot L^{\\prime}(\\mathsf{X})+\\lambda\\cdot\\varphi\\left(\\rho(\\mathsf{X})\\right)-a(\\mathsf{X})\\cdot\\rho(\\mathsf{X})-b\\cdot\\rho(\\mathsf{X})\\right]+b,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $a(x)=a^{\\prime}(x)\\cdot\\mathrm{P}(x)$ . ", "page_idx": 15}, {"type": "text", "text": "We can obtain the first order optimality conditions by taking the functional derivative. Suppose that $\\delta>0$ and $h\\colon\\mathcal{X}\\rightarrow\\mathbb{R}$ is any function. The functional derivative is given by, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}\\delta}\\mathcal{L}(\\rho+\\delta\\cdot h;a,b)\\Bigg|_{\\delta=0}}\\\\ &{=\\mathbb{E}_{\\mathbb{P}}\\left[h(\\mathsf{X})\\cdot(L^{\\prime}(\\mathsf{X})+\\lambda\\cdot\\varphi^{\\prime}\\left(\\rho(\\mathsf{X})+\\delta\\cdot h(\\mathsf{X})\\right)-a(\\mathsf{X})-b)\\right]\\Bigg|_{\\delta=0}}\\\\ &{=\\mathbb{E}_{\\mathbb{P}}\\left[h(\\mathsf{X})\\cdot(L^{\\prime}(\\mathsf{X})+\\lambda\\cdot\\varphi^{\\prime}\\left(\\rho(\\mathsf{X})\\right)-a(\\mathsf{X})-b)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, the first order condition is, ", "page_idx": 16}, {"type": "equation", "text": "$$\n0=\\frac{\\mathrm{d}}{\\mathrm{d}\\delta}\\mathcal{L}(\\rho+\\delta\\cdot h;a,b)\\Bigg|_{\\delta=0}=\\mathbb{E}_{\\mathrm{P}}\\left[h(\\mathsf{X})\\cdot\\left(L^{\\prime}(\\mathsf{X})+\\lambda\\cdot\\varphi^{\\prime}\\left(\\rho(\\mathsf{X})\\right)-a(\\mathsf{X})-b\\right)\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $h\\colon\\mathcal{X}\\rightarrow\\mathbb{R}$ . As the condition must hold for all $h$ for an optimal $\\rho^{\\star}$ , we have that for all $x\\in\\mathcal X$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{0=L^{\\prime}(x)+\\lambda\\cdot\\varphi^{\\prime}\\left(\\rho^{\\star}(x)\\right)-a(x)-b}}\\\\ {{\\varphi^{\\prime}\\left(\\rho^{\\star}(x)\\right)=\\displaystyle\\frac{b+a(x)-L^{\\prime}(x)}{\\lambda}}}\\\\ {{\\rho^{\\star}(x)=(\\varphi^{\\prime})^{-1}\\left(\\displaystyle\\frac{b-L^{\\prime}(x)+a(x)}{\\lambda}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As required. ", "page_idx": 16}, {"type": "text", "text": "C Proof of Corollary 3.4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. First notice that the inner maximization in the DRO optimization Eq. (7) can be simplified as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbb{Q}\\in B_{\\varepsilon}(\\mathbb{P})}{\\operatorname*{sup}}L(\\mathbb{Q})=-\\left(\\underset{\\mathbb{Q}\\in B_{\\varepsilon}(\\mathbb{P})}{\\operatorname*{inf}}-L(\\mathbb{Q})\\right)}\\\\ &{\\qquad\\qquad\\qquad=-\\left(\\underset{\\mathbb{Q}\\in\\mathcal{D}(\\mathcal{X})}{\\operatorname*{inf}}\\ \\underset{\\lambda\\geq0}{\\operatorname*{sup}}-L(\\mathbb{Q}_{\\lambda})-\\lambda\\cdot\\left(\\varepsilon-D_{\\varphi}(\\mathbb{P}\\parallel\\mathbb{Q}_{\\lambda})\\right)\\right)}\\\\ &{\\qquad\\qquad=-\\left(\\underset{\\lambda\\geq0}{\\operatorname*{sup}}\\ \\underset{\\mathbb{Q}_{\\lambda}\\in\\mathcal{D}(\\mathcal{X})}{\\operatorname*{inf}}-L(\\mathbb{Q}_{\\lambda})-\\lambda\\cdot\\left(\\varepsilon-D_{\\varphi}(\\mathbb{P}\\parallel\\mathbb{Q}_{\\lambda})\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality follows from Fan\u2019s minimax Theorem [25, Theorem 2] noting that $\\mathrm{Q}\\mapsto$ $L(\\mathrm{Q}_{\\lambda})$ is linear and the selected $\\varphi$ per Definition 2.2 makes $\\mathrm{~Q~}\\mapsto\\,D(\\mathrm{~P~}\\parallel\\mathrm{~Q_{\\lambda}})$ a convex lower semi-continuous function. ", "page_idx": 16}, {"type": "text", "text": "Now notice that the inner minimization of this simplification is exactly our idealized rejection distribution objective Eq. (8) when we negate the loss. As such, noticing that solutions to Eq. (8) are exactly given by $\\mathrm{~P~}\\!\\cdot\\!\\;\\rho$ yields the result after optimizing for the \u2018arguments\u2019 in the above DRO objective for both $\\lambda$ and $\\mathrm{Q}_{\\lambda}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "D Proof of Corollary 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We defer the proof of Corollary 4.1 to Appendix G, which covers any $\\alpha$ including KL $(\\alpha=1)$ ). ", "page_idx": 16}, {"type": "text", "text": "E Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We breakdown Theorem 4.2 into two sub-theorems Theorem E.1 for each of the settings. ", "page_idx": 16}, {"type": "text", "text": "Theorem E.1. Given the binary CPE setting presented in Theorem 2.1 and that we are given the optimal classifier $h^{\\star}(x)=\\operatorname{P}(\\Upsilon=1\\mid\\mathsf{X}=x)$ , for any $\\lambda>0$ there exists a $\\tau>0$ such that the $r_{\\tau}^{\\mathrm{KL}}$ rejector generated the optimal density ratio in Corollary 4.1 is equivalent to the optimal rejector in Theorem 2.1. ", "page_idx": 16}, {"type": "text", "text": "The proofs of each are similar. We first make the observation that the rejector function $r_{\\tau}^{\\mathrm{KL}}$ can be simplified as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nr_{\\tau}^{\\mathrm{KL}}=\\left[\\![L^{\\prime}(x)\\leq-\\lambda\\,(\\log Z+\\log\\tau)]\\!\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we note that given a fixed $\\lambda>0$ (which also fixes $Z$ ), the RHS term has a one-to-one mapping from $\\mathbb{R}_{+}$ to $\\mathbb{R}$ . Thus all that is to verify is that thresholding $L^{\\prime}(x)$ is equivalent to the rejectors of Theorems 2.1 and N.1. ", "page_idx": 16}, {"type": "text", "text": "Proof. For the CPE case, from assumptions, we have that $L^{\\prime}(x)=\\mathbb{E}_{\\mathsf{Y}\\sim\\mathrm{P}(\\mathsf{Y}|\\mathsf{X}=x)}[\\ell(\\mathsf{Y},h^{\\star}(x))]=$ $\\mathbb{E}_{\\mathsf{Y}\\sim h^{\\star}(\\mathsf{X}=x)}[\\ell(\\mathsf{Y},h^{\\star}(x))$ as $h^{\\star}$ is optimal. Thus thresholding used in $r_{\\tau}^{\\mathrm{KL}}$ is equivalent to thresholding $L^{\\prime}(x)$ by keeping $\\lambda$ fixed and changine $\\tau$ appropriately. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "F Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To prove the theorem, we will be using the standard Hoeffding\u2019s inequality [12]. ", "page_idx": 17}, {"type": "text", "text": "Theorem F.1 (Hoeffding\u2019s Inequality [12, Theorem 2.8]). Let $\\mathsf{X}_{1},\\ldots,\\mathsf{X}_{n}$ be independent random variables such that $\\mathsf{X}_{i}$ takes values in $[a_{i},b_{i}]$ almost surely for all $i\\leq n$ . Defining $\\begin{array}{r}{\\mathsf{X}\\,{=}\\,\\sum_{i}\\mathsf{X}_{i}-\\mathbb{E}X_{i},}\\end{array}$ , then for every $t>0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left({\\mathsf{X}}\\geq t\\right)\\leq\\exp\\left(-{\\frac{2t^{2}}{\\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Let us denote $Z$ to be the normalizer with the true expectation $\\mathbb{E}_{\\mathrm{P}}$ and $\\hat{Z}$ to be the normalizer with the empirical expectation EP\u02c6. ", "page_idx": 17}, {"type": "text", "text": "As $\\ell$ is bounded, we take note of the following bounds, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\exp(-B/\\lambda)\\leq\\operatorname*{max}\\left\\{\\exp\\left({\\frac{-L^{\\prime}(x)}{\\lambda}}\\right),Z,{\\hat{Z}}\\right\\}\\leq\\exp(B/\\lambda).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This can be simply verified by taking the smallest and largest values of the exp. ", "page_idx": 17}, {"type": "text", "text": "Now we simply have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho^{\\mathrm{KL}}(x)-\\hat{\\rho}^{\\mathrm{KL}}(x)\\vert=\\exp\\left(\\frac{-L^{\\prime}(x)}{\\lambda}\\right)\\cdot\\left\\vert\\frac{1}{Z}-\\frac{1}{\\hat{Z}}\\right\\vert}\\\\ &{\\qquad\\qquad\\quad\\leq\\exp\\left(\\frac{B}{\\lambda}\\right)\\cdot\\left\\vert\\frac{1}{Z}-\\frac{1}{\\hat{Z}}\\right\\vert}\\\\ &{\\qquad\\qquad\\quad=\\exp\\left(\\frac{B}{\\lambda}\\right)\\cdot\\frac{\\vert Z-\\hat{Z}\\vert}{\\vert Z\\cdot\\hat{Z}\\vert}}\\\\ &{\\qquad\\qquad\\quad\\leq\\exp\\left(\\frac{B}{\\lambda}\\right)^{3}\\cdot\\vert Z-\\hat{Z}\\vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Notice that $|Z-\\hat{Z}|$ can be bounded via concentration inequality on (bounded) random variable $\\textstyle\\exp\\left({\\frac{-L^{\\prime}(\\mathbf{X})}{\\lambda}}\\right)$ . ", "page_idx": 17}, {"type": "text", "text": "Thus, we have that by Theorem F.1 ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left|\\mathbb{E}_{\\mathrm{P}}\\left[\\exp\\left(\\frac{-L^{\\prime}(x)}{\\lambda}\\right)\\right]-\\mathbb{E}_{\\mathrm{P}}\\left[\\exp\\left(\\frac{-L^{\\prime}(x)}{\\lambda}\\right)\\right]\\right|>t\\right)\\le2\\exp\\left(\\frac{-2\\cdot N\\cdot t^{2}}{(b-a)^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$(b-a)^{2}=(\\exp(B/\\lambda)-\\exp(-B/\\lambda))^{2}=4\\sinh^{2}(B/\\lambda).$ ", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking a union bound over $\\upsigma$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{s}_{\\mathbf{r}}\\left(\\exists x\\in\\mathbb{T}:\\left|\\mathbb{E}_{\\mathbb{P}}\\left[\\exp\\left({\\frac{-L^{\\prime}(x)}{\\lambda}}\\right)\\right]-\\mathbb{E}_{\\mathbb{P}}\\left[\\exp\\left({\\frac{-L^{\\prime}(x)}{\\lambda}}\\right)\\right]\\right|>t\\right)\\leq2M\\exp\\left({\\frac{-N\\cdot t^{2}}{2\\sinh^{2}(B/\\lambda)}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus taking $\\begin{array}{r}{t=\\sinh(B/\\lambda)\\cdot\\sqrt{\\frac{2}{N}\\cdot\\log\\left(\\frac{2M}{\\delta}\\right)}}\\end{array}$ , we have that with probability $1-\\delta$ for $\\delta>0$ , for any $x\\in\\mathcal{T}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\rho^{\\mathrm{KL}}(x)-\\hat{\\rho}^{\\mathrm{KL}}(x)|\\leq\\exp\\bigg(\\displaystyle\\frac{B}{\\lambda}\\bigg)^{3}\\cdot|Z-\\hat{Z}|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\exp\\bigg(\\displaystyle\\frac{B}{\\lambda}\\bigg)^{3}\\cdot\\sinh(B/\\lambda)\\cdot\\sqrt{\\displaystyle\\frac{2}{N}\\cdot\\log\\bigg(\\displaystyle\\frac{2M}{\\delta}\\bigg)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As required. ", "page_idx": 17}, {"type": "text", "text": "G Proof of Theorem 4.5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Before proving the theorems, we first state some basic properties of $\\psi_{\\alpha}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma G.1. For $\\neq-1,\\,\\psi_{\\alpha}({\\boldsymbol{u}}\\cdot{\\boldsymbol{v}})=\\psi_{\\alpha}({\\boldsymbol{u}})\\cdot\\psi_{\\alpha}({\\boldsymbol{v}})$ and $\\psi_{\\alpha}(1/v)=1/\\psi_{\\alpha}(v)$ . Furthermore, these statements hold when $\\psi_{\\alpha}$ is replaced with $\\psi_{\\alpha}^{-1}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma G.2. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\psi_{\\alpha}^{-1}(z)=\\left\\{z^{\\frac{2}{1-\\alpha}}\\ \\ \\ i f\\alpha\\not=1\\atop{o t h e r w i s e}\\right..\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The above statements follows directly from definition and simple calculation. The next statement directly connects $\\varphi_{\\alpha}^{\\prime}$ to $\\psi_{\\alpha}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma G.3. For $\\alpha\\neq1$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\varphi_{\\alpha}^{\\prime})(z)=\\frac{2}{\\alpha-1}\\cdot\\psi_{\\alpha}(1/z).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We prove this via cases. ", "page_idx": 18}, {"type": "text", "text": "\u2022 $\\alpha=-1$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varphi_{-1}^{\\prime}(z)={\\frac{\\operatorname{d}}{\\operatorname{d}\\!z}}(-\\log z)=-z^{-1}=-1\\cdot z^{-{\\frac{1-\\alpha}{2}}}={\\frac{2}{\\alpha-1}}\\cdot\\psi_{\\alpha}(1/z).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\alpha\\neq\\pm1$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varphi_{\\alpha}^{\\prime}(z)={\\frac{\\mathrm{d}}{\\mathrm{d}z}}\\left({\\frac{4}{1-\\alpha^{2}}}\\cdot\\left(1-z^{\\frac{1+\\alpha}{2}}\\right)\\right)=-{\\frac{2}{1-\\alpha}}z^{\\frac{\\alpha-1}{2}}=-{\\frac{2}{1-\\alpha}}\\cdot z^{-{\\frac{1-\\alpha}{2}}}={\\frac{2}{\\alpha-1}}\\cdot\\psi_{\\alpha}(1/z)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now define the following constants depending on $\\alpha$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nc_{\\alpha}={\\left\\{\\psi_{\\alpha}^{-1}\\left(-{\\frac{2}{1-\\alpha}}\\right)\\qquad{\\mathrm{if~}}\\alpha\\neq1\\quad\\right.}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma G.4. For $\\alpha\\neq1$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\varphi_{\\alpha}^{\\prime})^{-1}(z)=c_{\\alpha}\\cdot\\frac{1}{\\psi_{\\alpha}^{-1}(z)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We prove this via cases. ", "page_idx": 18}, {"type": "text", "text": "\u2022 $\\alpha=-1$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varphi_{-1}^{\\prime}(z)={\\frac{\\mathrm{d}}{\\mathrm{d}z}}(-\\log z)=-z^{-1}=-1\\cdot z^{-{\\frac{1-\\alpha}{2}}}=-1\\cdot\\psi_{\\alpha}(1/z).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\varphi_{-1}^{\\prime})^{-1}=\\frac{1}{\\psi_{\\alpha}^{-1}\\,(-1\\cdot z)}=\\psi_{\\alpha}^{-1}\\left(\\frac{1}{-1\\cdot z}\\right)=\\psi_{\\alpha}^{-1}\\,(-1)\\cdot\\frac{1}{\\psi_{\\alpha}^{-1}\\,(z)}.=c_{\\alpha}\\cdot\\frac{1}{\\psi_{\\alpha}^{-1}\\,(z)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\alpha\\neq\\pm1$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\diamond_{\\alpha}^{\\prime}(z)={\\frac{\\mathrm{d}}{\\mathrm{d}z}}\\left({\\frac{4}{1-\\alpha^{2}}}\\cdot\\left(1-z^{\\frac{1+\\alpha}{2}}\\right)\\right)=-{\\frac{2}{1-\\alpha}}z^{\\frac{\\alpha-1}{2}}=-{\\frac{2}{1-\\alpha}}\\cdot z^{-{\\frac{1-\\alpha}{2}}}=-{\\frac{2}{1-\\alpha}}\\cdot\\psi_{\\alpha}(1/z)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varphi_{\\alpha}^{\\prime})^{-1}={\\frac{1}{\\psi_{\\alpha}^{-1}\\left(-{\\frac{1-\\alpha}{2}}\\cdot z\\right)}}={\\frac{1}{\\psi_{\\alpha}^{-1}\\left(-{\\frac{1-\\alpha}{2}}\\right)\\cdot\\psi_{\\alpha}^{-1}\\left(z\\right)}}=\\psi_{\\alpha}^{-1}\\left(-{\\frac{2}{1-\\alpha}}\\right)\\cdot{\\frac{1}{\\psi_{\\alpha}^{-1}\\left(z\\right)}}=c_{\\alpha}\\cdot{\\frac{1}{\\psi_{\\alpha}^{-1}\\left(z\\right)}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\varphi_{\\alpha}^{\\prime})^{-1}(z)=c_{\\alpha}\\cdot\\psi_{\\alpha}^{-1}(z).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. \u2022 $\\underline{{\\boldsymbol{\\alpha}}}=1$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\varphi_{1}^{\\prime}(z)={\\frac{\\mathrm{d}}{\\mathrm{d}z}}\\left(z\\log z\\right)=\\log z+1\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n(\\varphi_{-1}^{\\prime})^{-1}=\\exp(z-1)=\\exp(-1)\\cdot\\exp(z)=\\psi_{1}^{-1}(-1)\\cdot\\psi_{1}^{-1}(z)=c_{\\alpha}\\cdot\\psi_{\\alpha}^{-1}(z)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus now via Lemmas G.4 and G.5, we can prove the Theorems. ", "page_idx": 19}, {"type": "text", "text": "Proof of Corollary 4.1 and Theorem 4.5. The proof follows from utilizing either Lemmas G.4 and G.5 in conjunction with Theorem 3.3. ", "page_idx": 19}, {"type": "text", "text": "\u2022 $\\underline{{\\alpha}}=1$ : We have that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\rho_{\\lambda}^{\\varphi}(x)=(\\varphi^{\\prime})^{-1}\\left(\\displaystyle\\frac{a(x)-L^{\\prime}(x)+b}{\\lambda}\\right)}\\\\ {\\quad\\quad=c_{\\alpha}\\cdot\\psi_{\\alpha}^{-1}\\left(\\displaystyle\\frac{a(x)-L^{\\prime}(x)+b}{\\lambda}\\right)}\\\\ {\\quad\\quad=\\exp\\left(\\displaystyle\\frac{a(x)-L^{\\prime}(x)+b-1}{\\lambda}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As $\\rho_{\\lambda}^{\\varphi}(x)$ for $\\alpha\\,=\\,1$ is already positive by $\\cdot\\exp^{\\circ}$ , by complementary slackness, the Lagrange multipliers $a(\\cdot)=0$ . Hence, we can further simplify the above, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\rho_{\\lambda}^{\\varphi}(x)=\\exp\\left(\\frac{-L^{\\prime}(x)+b-\\lambda}{\\lambda}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The normalizer then can be easily calculated, renaming $b^{\\prime}=b-\\lambda$ , we simplify have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{\\lambda}^{\\varphi}(x)=\\exp\\left(\\frac{-L^{\\prime}(x)+b^{\\prime}}{\\lambda}\\right)}\\\\ &{\\qquad\\quad=\\frac{1}{\\exp(-b^{\\prime}/\\lambda)}\\cdot\\exp\\left(\\frac{-L^{\\prime}(x)}{\\lambda}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which by normalization condition and setting $Z\\doteq\\exp(-b^{\\prime}/\\lambda)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1=\\mathbb{E}\\left[\\cfrac{1}{Z}\\cdot\\exp\\left(\\cfrac{-L^{\\prime}(x)}{\\lambda}\\right)\\right]}\\\\ {\\iff}&{Z=\\mathbb{E}\\left[\\exp\\left(\\cfrac{-L^{\\prime}(x)}{\\lambda}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which completes the case (Corollary 4.1). ", "page_idx": 19}, {"type": "text", "text": "$\\alpha\\neq1$ : We have that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{\\lambda}^{\\mathcal{O}}(x)=(\\varphi^{\\prime})^{-1}\\left(\\frac{a(x)-L^{\\prime}(x)+b}{\\lambda}\\right)}\\\\ &{\\qquad=c_{\\alpha}\\cdot\\left(\\psi_{\\alpha}^{-1}\\left(\\frac{a(x)-L^{\\prime}(x)+b}{\\lambda}\\right)\\right)^{-1}}\\\\ &{\\qquad\\overset{(a)}{=}\\psi_{\\alpha}^{-1}\\left(\\frac{2}{\\alpha-1}\\right)\\cdot\\left(\\psi_{\\alpha}^{-1}\\left(\\frac{a(x)-L^{\\prime}(x)+b}{\\lambda}\\right)\\right)^{-1}}\\\\ &{\\qquad=\\psi_{\\alpha}^{-1}\\left(\\frac{2}{\\alpha-1}\\right)\\cdot\\psi_{\\alpha}^{-1}\\left(\\left(\\frac{a(x)-L^{\\prime}(x)+b}{\\lambda}\\right)^{-1}\\right)}\\\\ &{\\qquad=\\psi_{\\alpha}^{-1}\\left(\\frac{2}{\\alpha-1}\\cdot\\left(\\frac{a(x)-L^{\\prime}(x)+b}{\\lambda}\\right)^{-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(a)$ we exploit the that $(\\psi_{\\alpha}^{-1}(z))^{-1}=\\psi_{\\alpha}^{-1}(1/z)$ via Lemma G.1. ", "page_idx": 19}, {"type": "text", "text": "H Proof of Corollary 4.6 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. First we simplify the density ratio rejector. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\rho_{\\lambda}^{\\alpha}(\\boldsymbol{x})=\\psi_{\\alpha}^{-1}\\left(\\frac{2}{\\alpha-1}\\cdot\\left(\\boldsymbol{a}(\\boldsymbol{x})-\\frac{L^{\\prime}(\\boldsymbol{x})}{\\lambda}+b\\right)^{-1}\\right)}\\\\ {\\displaystyle=\\left(\\frac{\\alpha-1}{2}\\cdot\\left(\\boldsymbol{a}(\\boldsymbol{x})-\\frac{L^{\\prime}(\\boldsymbol{x})}{\\lambda}+b\\right)\\right)^{\\frac{\\alpha-1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We suppose that it is possible for ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left({\\frac{\\alpha-1}{2}}\\cdot\\left(a(x)-{\\frac{L^{\\prime}(x)}{\\lambda}}+b\\right)\\right)^{\\frac{\\alpha-1}{2}}<0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for values of $a(x),\\,b$ , and $\\lambda$ . Otherwise, $\\alpha(x)\\,=\\,0$ and we are done due to the above equation\u2019s non-negativity. ", "page_idx": 20}, {"type": "text", "text": "Let $x\\in\\mathcal X$ be arbitrary. Suppose that $a(x)=0$ . Then by complementary slackness, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left({\\frac{\\alpha-1}{2}}\\cdot\\left(-{\\frac{L^{\\prime}(x)}{\\lambda}}+b\\right)\\right)^{\\frac{\\alpha-1}{2}}>0\\iff\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\nb-\\frac{L^{\\prime}(x)}{\\lambda}>0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By contra-positive, we have that $b-L^{\\prime}(x)/\\lambda\\leq0$ implies that $a(x)\\neq0$ . By prime feasibility, in this case we also have $\\rho(x)=0$ . We can solve either case by using the maximum as stated. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "I Proof of Corollary 4.7 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Corollary 4.6. The proof directly follows from a property of the $\\alpha$ -divergence when one of the measure have disjoint support. From [3] we have. ", "page_idx": 20}, {"type": "text", "text": "Theorem I.1 ([3, Section 3.4.1 (4)]). For $\\alpha$ -divergences, we have that ", "page_idx": 20}, {"type": "text", "text": "This result immediately gives the result, as otherwise the objective function is $\\infty$ . ", "page_idx": 20}, {"type": "text", "text": "J Proof of Corollary 4.8 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. We first note simplifying $\\rho_{\\lambda}^{\\alpha=3}$ via Corollary 4.6 yields: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\rho_{\\lambda}^{\\alpha=3}(x)=\\operatorname*{max}\\left\\{0,b-\\frac{L^{\\prime}(x)}{\\lambda}\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus our goal is to solve $b$ to give $\\mathbb{E}_{\\mathrm{P}}[\\rho_{\\lambda}^{\\alpha=3}(\\mathsf{X})]=1$ . ", "page_idx": 20}, {"type": "text", "text": "Now, consider the following, ", "page_idx": 20}, {"type": "equation", "text": "$$\n1+\\frac{\\mathbb{E}_{\\mathrm{P}}\\left[L^{\\prime}(\\mathsf{X})\\right]}{\\lambda}-\\frac{L^{\\prime}(x)}{\\lambda}>0\\iff\\lambda>L^{\\prime}(x)-\\mathbb{E}_{\\mathrm{P}}\\left[L^{\\prime}(\\mathsf{X})\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the latter holds uniformly for all $x$ from assumptions on $\\lambda$ . ", "page_idx": 20}, {"type": "text", "text": "Thus setting $\\begin{array}{r}{b=1+\\frac{\\mathbb{E}_{\\mathrm{P}}\\left[L^{\\prime}(\\mathsf{X})\\right]}{\\lambda}}\\end{array}$ , we simplify ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\operatorname*{max}\\left\\{0,1+\\frac{\\mathbb{E}_{\\mathrm{P}}\\left[L^{\\prime}(\\mathsf{X})\\right]}{\\lambda}-\\frac{L^{\\prime}(\\mathsf{X})}{\\lambda}\\right\\}\\mathrm{d}\\mathrm{P}(x)}\\\\ &{=\\displaystyle\\int1+\\frac{\\mathbb{E}_{\\mathrm{P}}\\left[L^{\\prime}(\\mathsf{X})\\right]}{\\lambda}-\\frac{L^{\\prime}(\\mathsf{X})}{\\lambda}\\mathrm{d}\\mathrm{P}(x)}\\\\ &{=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As such, $\\begin{array}{r}{b=1+\\frac{\\mathbb{E}_{\\mathrm{P}}\\left[L^{\\prime}(\\mathsf{X})\\right]}{\\lambda}}\\end{array}$ solves the required normalization. Substituting $b$ back into $\\rho_{\\lambda}^{\\alpha=3}(x)$ yields the Theorem. ", "page_idx": 20}, {"type": "text", "text": "K Proof of Theorem 4.9 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. First we note that for any meas $\\mathrm{R}$ $,\\lambda>2B$ implies that $\\lambda>L^{\\prime}(x)-\\mathbb{E}_{\\mathrm{R}}\\left[L^{\\prime}(\\mathsf{X})\\right]$ (taking largest and smallest values of $\\ell$ . ", "page_idx": 21}, {"type": "text", "text": "As such, for both $\\mathrm{P},{\\hat{\\mathrm{P}}}$ have closed forms Corollary 4.8 ", "page_idx": 21}, {"type": "text", "text": "Thus, the bound can be simply shown to have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\rho_{\\lambda}^{\\alpha=3}(x)-\\hat{\\rho}_{\\lambda}^{\\alpha=3}(x)|=\\frac{1}{\\lambda}\\cdot\\left|\\mathbb{E}_{\\mathbb{P}}\\left[L^{\\prime}(\\mathsf{X})\\right]-\\mathbb{E}_{\\hat{\\mathbf{P}}}\\left[L^{\\prime}(\\mathsf{X})\\right]\\right|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus by Hoeffding\u2019s inequality Theorem F.1 and union bound (see for instance the proof of Theorem 4.3), setting $\\begin{array}{r}{t=B\\cdot{\\sqrt{{\\frac{2}{N}}\\log\\left({\\frac{2M}{\\delta}}\\right)}}}\\end{array}$ , we have that with probability $1-\\delta$ for all $x\\in\\mathcal{T}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\rho_{\\lambda}^{\\alpha=3}(x)-\\hat{\\rho}_{\\lambda}^{\\alpha=3}(x)|\\leq\\frac{B}{\\lambda}\\cdot\\sqrt{\\frac{2}{N}\\log\\left(\\frac{2M}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As required. ", "page_idx": 21}, {"type": "text", "text": "L Broader Impact ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The paper presents work which reinterprets the classification with rejection problem in terms of learning distributions and density ratios. Beyond advancing Machine Learning in general, potential societal consequences include enhancing the understanding of the rejection paradigm and, consequentially, the human-in-the-loop paradigm. The general rejection setting aims to prevent models from making prediction when they are not confident, which can have societal significance when deployed in high stakes real life scenarios \u2014 allowing for human intervention. ", "page_idx": 21}, {"type": "text", "text": "M Distribution Generalization Bounds ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the following, we seek to provide generalization bounds on $\\hat{\\mathrm{Q}}_{\\lambda,N}\\doteq\\hat{\\mathrm{P}}_{N}\\cdot\\hat{\\rho}_{\\lambda,N}$ . That is, one seeks to know that as $N\\rightarrow\\infty$ how and if $\\hat{\\mathrm{Q}}_{\\lambda,N}\\rightarrow\\mathrm{Q}$ . A natural measure of distance for probability measures is total variation [20], ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\mathrm{P},\\mathrm{Q})\\stackrel{.}{=}\\frac{1}{2}\\cdot\\|\\mathrm{P}-\\mathrm{Q}\\|_{1}=\\int|\\mathrm{Q}(x)-\\mathrm{P}(x)|\\;\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let us also define ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\mathrm{P}-\\mathrm{Q}\\|_{\\infty}=\\operatorname*{sup}_{x\\in\\mathcal{X}}|\\mathrm{Q}(x)-\\mathrm{P}(x)|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "One immediately gets a rate if we assume that $|{\\mathcal{X}}|$ is finite and we have a bound for $\\hat{\\rho}_{\\lambda,N}$ . ", "page_idx": 21}, {"type": "text", "text": "Theorem M.1. Suppose that $|{\\mathcal{X}}|=M<\\infty$ is finite and bounded density ratio $|\\hat{\\rho}_{\\lambda,N}|\\le B^{\\prime}$ for $B^{\\prime}>0$ . Then, if we have that with probability $1-\\delta$ that , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\rho(x)-\\hat{\\rho}_{N,\\lambda}(x)\\|_{\\infty}\\leq C\\cdot\\left(\\sqrt{\\frac{1}{N}\\cdot\\log\\frac{2M}{\\delta}+C^{\\prime}}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some $C,C^{\\prime}>0$ , then we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\mathrm{Q},\\hat{\\mathrm{Q}}_{\\lambda,N})=\\mathcal{O}\\left(\\sqrt{\\frac{M^{2}\\log M}{N}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Noting that the empirical distribution is a sum of Dirac deltas $\\begin{array}{r}{\\hat{\\mathrm{P}}_{N}(x)=\\sum_{i\\in[N]}\\delta(x-x_{i})}\\end{array}$ , we can establish a simple bound via a consequence of Hoeffding\u2019s Theorem Theorem F.1 (also noting that $\\mathrm{P}=\\mathbb{E}\\hat{\\mathrm{P}}_{N}\\mathrm{\\~}$ ). We have that, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(|\\mathrm{P}(x)-\\hat{\\mathrm{P}}_{N}(x)|\\geq t)\\leq2\\exp\\left(-2N t^{2}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}(\\forall x\\in\\mathcal{X}:|\\mathrm{P}_{N}(x)-\\hat{\\mathrm{P}}_{N}(x)|\\geq t)\\leq2M\\exp\\left(-2N t^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Setting $\\begin{array}{r}{t=\\sqrt{\\frac{1}{2N}\\log\\frac{2M}{\\delta}}}\\end{array}$ , we have with probability $1-\\delta$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\mathrm{P}-\\hat{\\mathrm{P}}_{N}\\|_{\\infty}\\leq\\sqrt{\\frac{1}{2N}\\log\\frac{2M}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now consider the following: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\mathrm{Q}-\\hat{\\mathrm{Q}}_{\\lambda,N}\\|_{\\infty}=\\|\\mathrm{P}\\cdot\\rho-\\hat{\\mathrm{P}}_{N}\\cdot\\hat{\\rho}_{\\lambda,N}\\|_{\\infty}}}\\\\ &{=\\|\\mathrm{P}\\cdot(\\rho-\\rho\\hat{\\lambda}_{,N})+(\\mathrm{P}-\\hat{\\mathrm{P}}_{N})\\cdot\\hat{\\rho}_{\\lambda,N}\\|_{\\infty}}\\\\ &{\\leq\\|\\mathrm{P}\\cdot(\\rho-\\hat{\\rho}_{\\lambda,N})\\|_{\\infty}+\\|(\\mathrm{P}-\\hat{\\mathrm{P}}_{N})\\cdot\\hat{\\rho}_{\\lambda,N}\\|_{\\infty}}\\\\ &{\\leq\\|(\\rho-\\hat{\\rho}_{\\lambda,N})\\|_{\\infty}+B^{\\prime}\\cdot\\|(\\mathrm{P}-\\hat{\\mathrm{P}}_{N})\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking a union bound of the above inequality and our assumption, we have that, for some $C>0$ , with probability $1-\\delta$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathrm{Q}-\\hat{\\mathrm{Q}}_{\\lambda,N}\\|_{\\infty}\\leq\\|\\big(\\rho-\\hat{\\rho}_{\\lambda,N}\\big)\\|_{\\infty}+B^{\\prime}\\cdot\\|\\big(\\mathrm{P}-\\hat{\\mathrm{P}}_{N}\\big)\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq C\\cdot\\sqrt{\\frac{1}{N}\\log\\frac{4M}{\\delta}+C^{\\prime}}+B^{\\prime}\\cdot\\sqrt{\\frac{1}{2N}\\log\\frac{4M}{\\delta}}}\\\\ &{\\qquad\\qquad=\\mathcal{O}\\left(\\sqrt{\\frac{\\log M}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Converting the bound to TV amounts to simply summing over $\\mathcal{X}$ , which gives $\\mathrm{TV}(\\mathrm{Q},\\hat{\\mathrm{Q}}_{\\lambda,N})\\,=$ $\\mathcal{O}(\\sqrt{(M^{2}\\log M)/N})$ , as required. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Notably, with appropriate assumptions, Theorems 4.3 and 4.9 can be used with Theorem M.1 to get bounds for $\\hat{\\mathrm{Q}}_{\\lambda,N}^{\\mathrm{KL}}$ and $\\hat{\\mathrm{Q}}_{\\lambda,N}^{\\alpha=3}$ . ", "page_idx": 22}, {"type": "text", "text": "N Rejection for Regression ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In the main-text of the paper we have focused on classification. However, many of the idea discussed can be extended for the regression setting. For instance, similar to Chow\u2019s Rule in Theorem 2.1 we can express the regression equivalent to the optimal solution of Eq. (2). ", "page_idx": 22}, {"type": "text", "text": "Theorem N.1 (Optimal Regression Rejection [72]). Let us consider the regression setting, where $\\ y\\ =\\ y^{\\prime}\\ =\\ \\mathbb{R}$ and $\\ell(y,y^{\\prime})^{\\vee}={\\textstyle\\ }_{\\textstyle2}^{1}(y\\,-\\,^{\\bullet}y^{\\prime})^{2}$ . Then w.r.t. Eq. (2), the optimal model is given by $h^{\\star}(x)=\\mathbb{E}_{\\mathrm{P}}[\\mathsf{Y}\\mid\\mathsf{X}=x]$ and the optimal rejector is given by $r^{\\star}(x)=\\mathbb{[}\\sigma^{2}(x)\\leq c].$ , where $\\sigma^{2}(x)$ is the conditional variance of $\\textsf{Y}$ given $\\mathsf X=x$ . ", "page_idx": 22}, {"type": "text", "text": "For regression, there is no clear analogous notion of output \u201cconfidence score\u201d unless the model explicitly outputs probabilities. Indeed, rejection method for regression explicitly requires the estimation of the target variable\u2019s conditional variance [72]. ", "page_idx": 22}, {"type": "text", "text": "Similar to the CPE case, our KL density ratio rejector can provide a rejection policy equivalent to the typical case. ", "page_idx": 22}, {"type": "text", "text": "rTehgeroesrseomr $h^{\\star}(x)\\;=\\;\\mathbb{E}_{\\sf P}[\\sf{Y}\\bar{\\tau}\\mid\\sf{X}=\\tau{x}]$ t,t ifnogr  parnesye $\\lambda\\ >\\ 0$ Tthheeorree emx iNs.t1s  aan $\\tau~>~0$ es uacreh  gtihvaetn  tohpet $r_{\\tau}^{\\mathrm{KL}}$ rejector generated the optimal density ratio in Corollary 4.1 is equivalent to the optimal rejector in Theorem N.1. ", "page_idx": 22}, {"type": "text", "text": "Proof. The proof follows similarly to the proof of Theorem E.1. The regression case follows almost identically, by noticing that $L^{\\prime}(x)=\\sigma^{2}(x)$ is the variance. This is similar to the proof of Theorem N.1 [72]. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Despite the equivalence, there is a difficult in using the density ratio rejectors, as per the closed form equations of Section 4, for regression. Estimating $\\bar{L}^{\\prime}(x)=\\sigma^{2}\\dot{(x)}$ is challenging. Unlike classification where learning calibrated classifiers has a variety of approaches, learning a regression model which explicitly outputs probabilities is quite difficult. As such, approximating $\\operatorname{P}(\\mathsf{Y}\\mid\\mathsf{X}=x)$ with the model $h(x)$ cannot be done. ", "page_idx": 23}, {"type": "text", "text": "O Gradient of Density Ratio Objective ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "As an alternative to the closed-form rejectors explored in the main-text, one may want to explore a method to learn $\\rho$ iteratively. We consider the gradients of the optimization problem in Eq. (9). In practice, we found that we were unable to learn such rejectors via taking gradient updates and thus leave a concrete implementation of the idea for future work. ", "page_idx": 23}, {"type": "text", "text": "The idea comes from utilizing the \u201cvariational\u201d aspect of the GVI formulation (which was not explored in the main-text). We suppose that the rejectors we are interested in come from a parameterized family. In particular, we consider the self-normalizing family $\\rho_{\\vartheta}/Z_{\\vartheta}$ , where $Z_{\\vartheta}=\\bar{\\mathbb{E_{P}}}[\\pi_{\\vartheta}(\\mathsf{X})]$ normalizes the rejector such that $\\mathbb{E}_{\\mathrm{P}}[\\rho_{\\vartheta}(\\mathsf{X})]=1$ . Having the $Z_{\\vartheta}$ normalizing term means that the constraint in Eq. (9) is satisfied for any $\\vartheta$ . The only constraint that we must have for $\\pi_{\\vartheta}$ is non-negativity, i.e., $\\pi_{\\vartheta}$ is a neural network with exponential last activation functions from $\\mathcal{X}\\rightarrow\\mathbb{R}_{+}$ . By setting a parametric form of $\\rho_{\\vartheta}$ , we implicitly restrict the set of idealized distributions to $\\mathrm{Q}_{\\vartheta}=\\mathrm{P}\\cdot\\rho_{\\vartheta}$ . The gradients of such a parametric form can be calculated as follows. ", "page_idx": 23}, {"type": "text", "text": "Corollary O.1. Let $\\rho_{\\vartheta}=\\pi_{\\vartheta}(x)/Z_{\\vartheta}$ . Then the gradient of Eq. (9) w.r.t. $\\vartheta$ is given by, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathrm{Px,y}}\\left[\\nabla_{\\vartheta}\\left(\\frac{\\pi_{\\vartheta}(\\mathsf{X})}{Z_{\\vartheta}}\\right)\\cdot\\left(\\varphi^{\\prime}\\left(\\frac{\\pi_{\\vartheta}(\\mathsf{X})}{Z_{\\vartheta}}\\right)+\\lambda\\cdot\\ell(\\mathsf{Y},h(\\mathsf{X}))\\right)\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. The proof is immediate from differentiation of Eq. (9). ", "page_idx": 23}, {"type": "text", "text": "An alternative form of Eq. (24) can be found by noticing that $\\nabla f=f\\cdot\\nabla\\log f$ . This provides an expression in terms of the log density ratio. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathrm{P_{x,y}}}\\left[\\frac{\\pi_{\\vartheta}(\\mathsf{X})}{Z_{\\vartheta}}\\cdot\\nabla_{\\vartheta}\\log\\left(\\frac{\\pi_{\\vartheta}(\\mathsf{X})}{Z_{\\vartheta}}\\right)\\cdot\\left(\\varphi^{\\prime}\\left(\\frac{\\pi_{\\vartheta}(\\mathsf{X})}{Z_{\\vartheta}}\\right)+\\lambda\\cdot\\ell(\\mathsf{Y},h(\\mathsf{X}))\\right)\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "One will notice that the gradient is in-fact the log-likelihood of the idealized distribution: noting $\\mathrm{P}$ free of $\\vartheta$ , we have $\\nabla_{\\vartheta}\\operatorname{log}(\\pi_{\\vartheta}(\\mathsf{X})/Z_{\\vartheta})=\\nabla_{\\vartheta}\\log(\\mathrm{P}(\\mathsf{X})\\cdot\\pi_{\\vartheta}(\\mathsf{X})/Z_{\\vartheta})=\\nabla_{\\vartheta}\\log\\mathrm{Q}_{\\vartheta}$ . As such, the gradient Eq. (24) is equivalent to the gradient of a weighted log-likelihood. ", "page_idx": 23}, {"type": "text", "text": "Despite the potential nice form of the gradient, we found that learning rejector through this was not possible. One limiting factor of computing such gradients is that we need to estimate $Z_{\\vartheta}$ at each gradient calculation, i.e., this must happen whenever $\\vartheta$ changes. This can be particularly costly when $\\mathcal{X}$ is high-dimensional. Secondly, we suspect that the model capacity of $\\pi_{\\vartheta}$ was not sufficient: we only tested on simple neural networks and convolutions neural networks, mirroring the architecture of the base classifiers used in our experimental setting. ", "page_idx": 23}, {"type": "text", "text": "P Finding the Best Rejection Cost ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Both the baselines and density ratio approaches evaluated in Section 5 require a tuning of hyperparameters $c,\\tau$ . Practically, we are more interested in the rejection rate $\\mathrm{P}[{\\bar{r}}(\\mathsf X)=1]$ rather than the abstract choices of $c,\\tau$ . Hence, one often wishes to select $c,\\tau$ according to a coverage constraint. ", "page_idx": 23}, {"type": "text", "text": "One of the simplest choices of selecting $c,\\tau$ is to utilize a calibration set and approximate the rejection rate / coverage on this data. Our experiments in Section 5 mirrors this in Tables 1 and I by treating the final test set as the calibration data. ", "page_idx": 23}, {"type": "text", "text": "Other works in the literature utilize more sophisticated methods for deriving coverage constraints. [29] picks threshold values which provide a guarantee on a coverage-modified risk requirement. Another approach [57] does not require an additional calibration set and instead ftis threshold values $\\tau$ by exploiting stacking confidences and (stratified) cross-ftiting. Although one could consider such approaches for all baselines, including those which utilize the cost-model of rejection via $c$ , one will have to pay a price in retraining the learned rejector $r$ for each instance of the hyperparameter $c$ that is searched. As a result, approaches which can trade-off accuracy and coverage via a threshold variable $\\tau$ (rather than an expensive retraining) are computationally preferable when exhaustively searching for a good $c,\\tau$ . ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "We leave more sophisticated methods for selecting $\\tau$ for density ratio rejectors for future work. ", "page_idx": 24}, {"type": "text", "text": "Q Additional Experimental Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Q.I Training Settings ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The neural network architecture used to train the base classifiers and baselines are almost identical. For the baseline approaches which have output dimension which is different than the output of the original neural network, we modify the last linear layer of the base classifier\u2019s architecture to fit the baseline\u2019s requirements, e.g., adding an additional output dimension for rejection in DEFER. Our architectures utilize batch normalization [38] and dropout [65] in a variety of places. Training settings are mostly identical, with some baselines requiring slight changes. ", "page_idx": 24}, {"type": "text", "text": "The base model\u2019s architecture is as follows. ", "page_idx": 24}, {"type": "text", "text": "\u2022 HAR (CC BY 4.0): We utilize a two hidden layer neural network with batch normalization. Both hidden layer is 64 neurons and the activation function is the sigmoid function. We take a 64 batch size, 40 training epochs and a 0.0001 learning rate.   \n\u2022 Gas Drift (CC BY 4.0): We utilize a two hidden layer neural network with batch normalization. Both hidden layers are 64 neurons and the activation function is the sigmoid function. We take a 64 batch size, 40 training epochs and a 0.0001 learning rate.   \n\u2022 MNIST (CC BY-SA 3.0): We utilize a convolutional neural network with two convolutional layers and two linear layers. The architecture follows directly from the MNIST example for PyTorch. We utilize the sigmoid function activation function. We take a 256 batch size, 40 training epochs and a 0.0001 learning rate.   \n\u2022 CIFAR-10 (CC BY 4.0): We utilize a ResNet-18 classifier. Random cropping and horizontal flipping data augmentation is utilized in training. We take a 256 batch size, 40 training epochs and a 0.0001 learning rate.   \n\u2022 OrgMNIST (CC BY 4.0): We utilize a ResNet-18 classifier. We take a 256 batch size, 40 training epochs and a 0.0001 learning rate.   \n\u2022 OctMNIST (CC BY 4.0): We utilize a ResNet-18 classifier. We take a 256 batch size, 40 training epochs and a 0.0001 learning rate. ", "page_idx": 24}, {"type": "text", "text": "For CSS, as noted in [53], batch normalization is needed at the final layer to stabilize training. ", "page_idx": 24}, {"type": "text", "text": "Table I presents an extended version of Table 1 over coverage targets of $80\\%$ and $90\\%$ . The standard deviation is additionally reported. One can see the observation from the main text are consistent with this extended table. ", "page_idx": 24}, {"type": "text", "text": "Plots Figs. I to III show Fig. 2 over and extend region of acceptable coverage percentages. In addition, we include a larger range of noise rates for HAR and Gas Drift. For MNIST, we explore a larger range of noise rates for MNIST in Fig. VII for our density ratio rejectors. We find that the findings in the main text are extended to these additional noise rates. In particular, we find that the our density ratio rejectors can be competitive with the various baseline approaches. We find that our density ratio approaches can have more fine-grained trade-offs at higher ranges of acceptance coverage. This is an important region where the budge for rejection may be low (and only a few examples, e.g. $<10\\%$ , can be rejected). Indeed, the baseline approaches which do not \u2018wrap\u2019 a base model require a lower maximum acceptance coverage as the noise rate increases (the approaches require a higher rejection $\\%$ for any type of rejection). Nevertheless, we do see a downside of the density ratio approach: the quality of the density ratio rejectors is dependent on the initial model. As such, at higher levels of noise there can be higher variation in the quality of rejection, see Fig. II. Interestingly, for MNIST and CIFAR-10, Figs. III and IV, the base model the density ratio rejectors are more robust across noise rates than other models. This seems to be due to the default MNIST architecture being robust against higher noise rates (notice that the s.t.d. range is also quite small at $100\\%$ coverage). In OctMNIST and OrganMNIST, Figs. V and VI, we find that there is little to no change in performance, likely due to the Base classifier\u2019s own performance only changing slightly with the addition of noise. ", "page_idx": 24}, {"type": "table", "img_path": "JzcIKnnOpJ/tmp/0ae9a7662160e529de980901bc099877433ed1b447500a3fb4881146aeb3bbd6.jpg", "table_caption": ["Table I: Extended Summary performance of rejection methods over all baselines and datasets targeting 80% and 90% coverage. Each cell reports the \u201caccuracy "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/dc4be2954036dfa381d5dae888ef273ca8dccec0359dfe027cedd99b70566f34.jpg", "img_caption": ["Figure I: Extended plots for HAR of Fig. 2. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Q.III Smaller models case study ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In the following, we consider the Gas Drift dataset when models are switched to a base model with only a single hidden layer. First we make note of the original setting explored in the main text. In Table II, we take note of the number of tunable parameters in all approaches and baselines. Notably, these default parameter / architecture sizes are similar to [14], with the HAR and Gas Drift setting including an additional hidden layer than previously utilized in the literature. ", "page_idx": 26}, {"type": "text", "text": "In Table III, we note the setting we consider in this subsection. The parameter sizes of the Gas Drift dataset is reduced to the originally explored model sizes in [14]. Notice that both the base models and the baseline approaches have reduced parameter sizes. It should be noted that this smaller parameter size setting can be useful in the related learning with deferral setting [52], where having a small model to defer to a larger model is needed. ", "page_idx": 26}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/3ed9cce938d383e794c0a263f9087729ea1b23c278de4eaeb816652d1179c6b5.jpg", "img_caption": ["Figure II: Extended plots for Gas Drift of Fig. 2. "], "img_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "JzcIKnnOpJ/tmp/bc859ba1d908e1cd186c47255e9b2d6b244536e3ff5ed1aeebce4e6ecbd6bf7e.jpg", "table_caption": ["Table II: Default parameter sizes of experiments. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "The results are reported in Figs. VIII and IX. We can see that in this setting, PredRej and our density ratio approaches are more competitive. This might indicate that for simple base models, approaches which \u2018wrap\u2019 a base model for rejection can be quite effective (especially in higher coverage regimes). In general, it seems with this smaller architecture regime, the \u2018non-wrapping\u2019 baseline approaches only provide rejection options when the acceptance coverage is lower than $7\\bar{0}\\%$ . ", "page_idx": 27}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/231dd108d04e1617e98fe0b2bd76400732b11708e0c816e4dcaa2660d5e0d7c4.jpg", "img_caption": ["Figure III: Extended plots for MNIST of Fig. 2. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/381c3d978706ecf222929447913a8b3bc12e57d55468ffa78dbb0694c1af484d.jpg", "img_caption": ["Figure IV: Deferred extended plots for CIFAR-10 of Fig. 2. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Q.IV Parameter sweeps over $\\alpha$ and $\\lambda$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The following shows parameter sweeps over $\\alpha$ and $\\lambda$ for our density ratio rejectors. These are given by Figs. X and XI respectively. We find that increasing $\\alpha$ compresses the trade-off curve from both sides. While decrease $\\lambda$ extends the trade-off curve on the left side. ", "page_idx": 28}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/acc701558436027c6fdf8608b57e5c51cd5fa54adcb843310e420cccde0a6a12.jpg", "img_caption": ["Figure V: Deferred extended plots for OctMNIST of Fig. 2. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/716abe867298bb7c6570d7729ca29b8f32afd068dbac92b84ef89d00f343c943.jpg", "img_caption": ["Figure VI: Deferred extended plots for OrganMNIST of Fig. 2. "], "img_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "JzcIKnnOpJ/tmp/b75749de90afb799096a030fb930f6442a7180b86d5d6ef1837064a0a89ae886.jpg", "table_caption": ["Table III: Alternative parameter sizes of experiments. "], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/71d1a5cde286280ca6ba01d0077cb94197c025dc26c1ae8b4df81d11a7506790.jpg", "img_caption": ["Figure VII: MNIST with different noises for density ratio approaches. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/a158cd54d869978859cedd3f5e51b35d008de7e20496d01bf8386a46a364f2ab.jpg", "img_caption": ["Figure VIII: Plots for HAR with smaller models. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/2236eb6629a35d2a7f4f3635b5764f0ab370d4139034a707585dbf8f56853d56.jpg", "img_caption": ["Figure IX: Plots for Gas Drift with smaller models. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/8ac7f676e737b4430316f4e6cd64100b97e27689e7b63740ad6d68bda0cd459c.jpg", "img_caption": ["Figure X: $\\alpha$ parameter sweep over dataset $^+$ noise combinations. S.t.d. shade is not utilized, but instead end points of the trade-off curves for $\\tau\\in(0,1]$ are shown via vertical bars "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "JzcIKnnOpJ/tmp/e9ab4048f722652d3e19d9b074ba41387cf5ea2350594fa88ee26ab794ec23c8.jpg", "img_caption": ["Figure XI: $\\lambda$ parameter sweep over dataset $^+$ noise combinations for KL rejector. S.t.d. shade is not utilized, but instead end points of the trade-off curves for $\\tau\\in(0,1]$ are shown via vertical bars "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Claims are justified via formal results and experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Limitations are appropriately discussed throughout the paper. Furthermore, the last section of the main-text also explicitly discusses limitations. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Assumptions are stated and complete proofs are provided in Appendix. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Detailed settings of experiments are provided in Appendix. Code is also included. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Code and scripts to create small scale experiments are provided. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All included in the main-text and also the Appendix. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: Error bars are given in plots. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We state the AWS resources utilized and estimated total compute time. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Research conforms with the Code of Ethics. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We provide a broader impact section in the Appendix. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 37}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: No release. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Baselines and data are correctly credited (with licenses given in the Appendix). Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: No new assets. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: No crowdsourcing and research with human subjects. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: No research with human subjects. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]