{"references": [{"fullname_first_author": "Alexander Kirillov", "paper_title": "Segment Anything", "publication_date": "2023-10-01", "reason": "This paper introduces a novel model that achieves state-of-the-art results on various segmentation tasks, providing a strong baseline for comparison in the field of class-agnostic object detection."}, {"fullname_first_author": "Shilong Liu", "paper_title": "Grounding DINO: Marrying DINO with Grounded Pre-training for Open-Set Object Detection", "publication_date": "2023-01-01", "reason": "This paper introduces a new vision-language model that significantly improves the performance of class-agnostic object detection, serving as a foundation for the proposed DiPEx method."}, {"fullname_first_author": "Mathilde Caron", "paper_title": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments", "publication_date": "2020-12-01", "reason": "This foundational paper introduces the contrastive learning approach that has influenced many self-supervised learning methods, making it essential to understand the context of the DiPEx method."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "publication_date": "2021-05-01", "reason": "This paper introduces the Vision Transformer (ViT) architecture, a crucial component in many modern vision models, including those used in the DiPEx method."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2019-06-01", "reason": "This influential paper introduces the BERT model, which is utilized for semantic analysis in the DiPEx method, making it a critical component in the proposed approach."}]}