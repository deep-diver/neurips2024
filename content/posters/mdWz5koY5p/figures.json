[{"figure_path": "mdWz5koY5p/figures/figures_7_1.jpg", "caption": "Figure 1: Evaluation on Maze tasks. (a)-(c): RGMDT (purple bar) completes the tasks in fewer steps than all the baselines. (d)-(f): RGMDT (blue line) achieves a higher mean episode reward than all the baselines in all scenarios with varying complexities, which illustrates its ability to minimize the return gap in hard environments.", "description": "This figure displays the results of experiments conducted on maze tasks with varying complexities. The bar chart (a-c) shows that RGMDT outperforms other methods by completing the tasks in fewer steps, especially in more complex scenarios. The line chart (d-f) shows that RGMDT achieves higher average episode rewards compared to baselines across all complexities, demonstrating its effectiveness in minimizing the return gap. ", "section": "6 Evaluation and Results"}, {"figure_path": "mdWz5koY5p/figures/figures_8_1.jpg", "caption": "Figure 1: Evaluation on Maze tasks. (a)-(c): RGMDT (purple bar) completes the tasks in fewer steps than all the baselines. (d)-(f): RGMDT (blue line) achieves a higher mean episode reward than all the baselines in all scenarios with varying complexities, which illustrates its ability to minimize the return gap in hard environments.", "description": "This figure shows the performance comparison between RGMDT and several baselines on three maze tasks with increasing complexity.  The bar charts (a-c) illustrate the number of steps to complete each task, showing RGMDT's superior efficiency.  The line charts (d-f) display the mean episode rewards, demonstrating RGMDT's ability to achieve higher rewards even in complex scenarios, thus minimizing the return gap.", "section": "6 Evaluation and Results"}, {"figure_path": "mdWz5koY5p/figures/figures_8_2.jpg", "caption": "Figure 1: Evaluation on Maze tasks. (a)-(c): RGMDT (purple bar) completes the tasks in fewer steps than all the baselines. (d)-(f): RGMDT (blue line) achieves a higher mean episode reward than all the baselines in all scenarios with varying complexities, which illustrates its ability to minimize the return gap in hard environments.", "description": "This figure displays the results of experiments conducted on maze tasks with varying difficulty levels. The bar charts (a-c) compare the number of steps taken by RGMDT and several baseline algorithms to complete the tasks. The line charts (d-f) show a comparison of the average episode rewards obtained by these algorithms. The results demonstrate that RGMDT consistently outperforms the baselines in both metrics, highlighting its effectiveness in minimizing the return gap, particularly in complex scenarios.", "section": "6 Evaluation and Results"}, {"figure_path": "mdWz5koY5p/figures/figures_9_1.jpg", "caption": "Figure 3: Comparisons on the n-agent tasks: (a)-(c) 2 agents, (d)-(f) 3 agents. RGMDT with limited leaf nodes can learn these tasks much faster than the baselines and have better final performance, even when most of the baselines fail on the task with 2 and 4 leaf nodes.", "description": "This figure compares the performance of RGMDT against several baselines on multi-agent tasks with varying complexities.  The tasks involve 2 or 3 agents and different numbers of leaf nodes in the decision tree. The results demonstrate that RGMDT significantly outperforms the baselines in terms of both speed and final reward, especially when the number of leaf nodes is limited. This showcases RGMDT's effectiveness in learning complex multi-agent tasks efficiently even under constrained model complexity.", "section": "Evaluation and Results"}, {"figure_path": "mdWz5koY5p/figures/figures_30_1.jpg", "caption": "Figure 5: The return gap (left) is bounded by average cosine distance (right) and diminishes as average cosine distance (right) decreases due to the increase of the maximum number of leaf nodes.", "description": "This figure shows the relationship between the return gap and the average cosine distance for different numbers of leaf nodes in a decision tree.  The return gap represents the difference in performance between an optimal policy and a decision tree policy. The average cosine distance measures the similarity of action-value vectors within each cluster. The figure demonstrates that a smaller return gap is achieved with a smaller average cosine distance, and that this relationship is improved by increasing the number of leaf nodes in the tree.", "section": "F.2 Empirical Performance Comparison between RGMDT and DRL"}, {"figure_path": "mdWz5koY5p/figures/figures_31_1.jpg", "caption": "Figure 6: Interpretable Non-Euclidean Clustering Labels: (a)-(b): The positions are more likely to be labeled as '1' when closer to the higher-reward target (circle), while more likely to be labeled as '2' when closer to the lower-reward target (triangle); (c)-(d): both agents are more likely to take action 'down', 'up', 'right', 'left' conditioned on labels '0', '1', '2', and '3', respectively.", "description": "This figure shows the interpretability of the non-Euclidean clustering labels used in RGMDT.  Subfigures (a) and (b) illustrate how agent positions during training correlate with these labels. Subfigures (c) and (d) show the relationship between agent actions and the clustering labels.  The results indicate that the labels generated by the non-Euclidean clustering effectively capture relevant spatial and action information, making the resulting decision tree more interpretable.", "section": "Evaluation and Results"}, {"figure_path": "mdWz5koY5p/figures/figures_31_2.jpg", "caption": "Figure 6: Interpretable Non-Euclidean Clustering Labels: (a)-(b): The positions are more likely to be labeled as '1' when closer to the higher-reward target (circle), while more likely to be labeled as '2' when closer to the lower-reward target (triangle); (c)-(d): both agents are more likely to take action 'down', 'up', 'right', 'left' conditioned on labels '0', '1', '2', and '3', respectively.", "description": "This figure demonstrates the interpretability of the non-Euclidean clustering labels used in RGMDT.  The top two subfigures (a) and (b) show the relationship between agent positions in the environment and the assigned cluster label. Agents closer to the higher reward target are more likely to be labeled '1', while agents closer to the lower reward target tend to be labeled '2'. The bottom two subfigures (c) and (d) show the relationship between the cluster label and the action taken by the agents. The visualization reveals a clear mapping between labels and actions. For example, label '1' strongly correlates with the 'up' action.", "section": "Evaluation and Results"}, {"figure_path": "mdWz5koY5p/figures/figures_31_3.jpg", "caption": "Figure 6: Interpretable Non-Euclidean Clustering Labels: (a)-(b): The positions are more likely to be labeled as '1' when closer to the higher-reward target (circle), while more likely to be labeled as '2' when closer to the lower-reward target (triangle); (c)-(d): both agents are more likely to take action 'down', 'up', 'right', 'left' conditioned on labels '0', '1', '2', and '3', respectively.", "description": "This figure shows the interpretability of the non-Euclidean clustering labels used in RGMDT.  Subfigures (a) and (b) demonstrate the correlation between agent positions during training and the resulting cluster labels. Agents closer to the higher-reward target tend to be labeled '1', while those near the lower-reward target are labeled '2'. Subfigures (c) and (d) illustrate the relationship between agent actions ('down', 'up', 'right', 'left') and the cluster labels ('0', '1', '2', '3').  The heatmaps show the conditional probabilities of taking each action given a specific cluster label.", "section": "Evaluation and Results"}, {"figure_path": "mdWz5koY5p/figures/figures_31_4.jpg", "caption": "Figure 6: Interpretable Non-Euclidean Clustering Labels: (a)-(b): The positions are more likely to be labeled as '1' when closer to the higher-reward target (circle), while more likely to be labeled as '2' when closer to the lower-reward target (triangle); (c)-(d): both agents are more likely to take action 'down', 'up', 'right', 'left' conditioned on labels '0', '1', '2', and '3', respectively.", "description": "This figure shows the interpretability of the non-Euclidean clustering labels used in RGMDT.  Subfigures (a) and (b) illustrate how agent positions during training correlate with clustering labels. Subfigures (c) and (d) show how the agents take actions conditioned on those labels. This demonstrates that the clustering labels are meaningful and reflect the agents' behavior in the environment.", "section": "Evaluation and Results"}, {"figure_path": "mdWz5koY5p/figures/figures_32_1.jpg", "caption": "Figure 5: The return gap (left) is bounded by average cosine distance (right) and diminishes as average cosine distance (right) decreases due to the increase of the maximum number of leaf nodes.", "description": "This figure shows the relationship between the return gap and the average cosine distance for different numbers of leaf nodes in a decision tree.  The return gap represents the difference in performance between an optimal policy and the decision tree policy, while the average cosine distance measures the similarity of action-value vectors within the same cluster.  As the number of leaf nodes increases, the average cosine distance decreases, indicating that the action-value vectors within each cluster are more similar.  Consequently, the return gap also decreases, showing that the decision tree policy approaches the optimal policy's performance.", "section": "F.2 Empirical Performance Comparison between RGMDT and DRL"}, {"figure_path": "mdWz5koY5p/figures/figures_33_1.jpg", "caption": "Figure 3: Comparisons on the n-agent tasks: (a)-(c) 2 agents, (d)-(f) 3 agents. RGMDT with limited leaf nodes can learn these tasks much faster than the baselines and have better final performance, even when most of the baselines fail on the task with 2 and 4 leaf nodes.", "description": "This figure compares the performance of RGMDT against several baselines on multi-agent tasks with varying numbers of agents and leaf nodes. The results demonstrate that RGMDT significantly outperforms the baselines in terms of both speed and final reward, particularly in more challenging scenarios with limited leaf nodes.", "section": "Evaluation and Results"}, {"figure_path": "mdWz5koY5p/figures/figures_34_1.jpg", "caption": "Figure 5: The return gap (left) is bounded by average cosine distance (right) and diminishes as average cosine distance (right) decreases due to the increase of the maximum number of leaf nodes.", "description": "This figure shows the relationship between the return gap and the average cosine distance using different numbers of leaf nodes in the decision tree.  The return gap, representing the performance difference between the optimal policy and the learned decision tree, decreases as the average cosine distance decreases.  This is consistent with the theoretical findings of the paper, demonstrating that using more leaf nodes reduces the return gap by improving the accuracy of the tree's approximation of the optimal policy.", "section": "F.2 Empirical Performance Comparison between RGMDT and DRL"}, {"figure_path": "mdWz5koY5p/figures/figures_35_1.jpg", "caption": "Figure 5: The return gap (left) is bounded by average cosine distance (right) and diminishes as average cosine distance (right) decreases due to the increase of the maximum number of leaf nodes.", "description": "This figure shows the relationship between the return gap and the average cosine distance for different numbers of leaf nodes in a decision tree.  The return gap, representing the difference in performance between an optimal policy and the decision tree policy, is plotted against the average cosine distance, a measure of the dissimilarity between action-value vectors within clusters. The figure demonstrates that as the number of leaf nodes (and thus the complexity of the tree) increases, the average cosine distance decreases, leading to a smaller return gap. This aligns with the theoretical findings in the paper, supporting the claim that RGMDT effectively minimizes the return gap.", "section": "F.2 Empirical Performance Comparison between RGMDT and DRL"}]