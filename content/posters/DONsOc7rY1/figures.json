[{"figure_path": "DONsOc7rY1/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of HORSE to the state-of-the-arts EquiVSet and INSET in handling subsets. \"S\" represents the supervision, indicating the specific subset of interest. \u201c+\u201d refers to the aggregation of different vectors, which is implemented through concatenation in practice. Unlike EquiVSet and INSET, the HORSE model captures more complex information from V by employing attention mechanisms. Furthermore, HORSE facilitates the division of V into distinct partitions.", "description": "This figure compares three different neural network architectures for subset selection: EquiVSet, INSET, and HORSE.  It illustrates how each model processes the input set (V) and the selected subset (S). EquiVSet and INSET use simple aggregation methods, while HORSE uses an attention mechanism to capture more complex interactions between the input set and the selected subset.  HORSE also partitions the input set into smaller, manageable chunks before aggregation, improving scalability for large datasets.", "section": "1 Introduction"}, {"figure_path": "DONsOc7rY1/figures/figures_5_1.jpg", "caption": "Figure 2: This figure illustrates the HORSE model's capability to achieve Permutation Invariance and satisfy the Identity Property in subset selection tasks. It demonstrates that HORSE maintains consistent output despite the permutation of input set elements and the partition if the ground set.", "description": "This figure shows the architecture of the HORSE model, highlighting its ability to maintain permutation invariance and satisfy the Identity Property.  The left side illustrates how the model processes input sets, showing that regardless of the order of elements (permutation invariance), the model consistently generates the same output.  The Identity Property is represented by the dashed lines showing the consistent output despite the model being fed both the selected subset and its origin set.  The right side provides a legend explaining the symbols used in the diagram, clearly depicting the distinct stages of the model: the input sets, the attention mechanisms, and the final aggregation that ensures the desired properties are achieved.", "section": "3.3 Attention-Based Set Representation"}, {"figure_path": "DONsOc7rY1/figures/figures_8_1.jpg", "caption": "Figure 3: The performance of the methods on the Two-Moons and Gaussian Datasets with respect to the set size is examined in the left two subfigures. These subfigures provide insights into how the performance of the methods varies as the size of the input sets changes. The right subfigure focuses on the influence of the number of partitions on the performance using the BindingDB dataset.", "description": "This figure compares three different methods (HORSE, INSET, and Set Transformer) on their ability to perform subset selection tasks on three different datasets. The left two graphs show how performance changes as the size of the dataset increases for two datasets (Two-Moons and Gaussian). The right graph examines how well the methods perform when the dataset is split into different numbers of partitions, which tests the scalability of the algorithms.", "section": "5 Experiments"}]