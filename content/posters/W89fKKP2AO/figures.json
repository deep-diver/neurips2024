[{"figure_path": "W89fKKP2AO/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the permutation symmetries in the weight space of a recurrent neural network (Example 2.2). Left: Each layer contains feedforward (ff) weights mapping between different layer's activations, and recurrent (rec) weights transforming activations over time. We can permute the hidden activations as illustrated without changing the final outputs h<sup>f</sup>. Right: Permuting the hidden activations induces a permutation on the weights. Here, the rows and columns of the feedforward weights are permuted by (\u03c3<sup>\u2113+1</sup>,\u03c3<sup>\u2113</sup>), while the recurrent weights are permuted by (\u03c3<sup>\u2113</sup>,\u03c3<sup>\u2113</sup>). Our algorithm automatically constructs permutation equivariant models for any collection of weight tensors given a description of its symmetries (Appendix A).", "description": "This figure illustrates how permutation symmetries affect the weights of a recurrent neural network. The left side shows how permuting the hidden activations (h<sup>\u2113</sup>) doesn't change the final output (h<sup>f</sup>), while the right side shows how this permutation affects both feedforward and recurrent weights (W<sup>ff</sup> and W<sup>rec</sup>) by permuting their rows and columns accordingly.  This example demonstrates the type of weight space permutation symmetries the paper's algorithm handles.", "section": "2 Preliminaries"}, {"figure_path": "W89fKKP2AO/figures/figures_7_1.jpg", "caption": "Figure 2: Training loss (negative log-likelihood) curves for different tasks and architectures using meta-learned optimizers. We implement learned optimizers with either universal neural functionals (UNFs), NFNs [Zhou et al., 2023a], or Deep Sets [Zaheer et al., 2017]. Deep Sets are the current standard choice for implementing learned optimizers. Note that NFN is identical to UNF in the MLP case, different for CNN case, and not applicable to RNNs or Transformers. All loss curves are smoothed and averaged over 5 random initializations (3 for Transformer), with shaded regions showing standard error.", "description": "This figure compares the training loss curves of different meta-learned optimizers (UNFs, NFNs, Deep Sets, and SGDM) across four distinct tasks: MLP on FashionMNIST, CNN on CIFAR-10, RNN on LM1B, and Transformer on LM1B.  The results illustrate the performance of each optimizer in terms of convergence speed and final loss.  The shaded regions represent standard errors, and the curves are smoothed and averaged.", "section": "4.2 Learned optimizers"}]