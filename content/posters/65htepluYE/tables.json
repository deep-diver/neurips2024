[{"figure_path": "65htepluYE/tables/tables_6_1.jpg", "caption": "Table 1: A Comparison over the performance of CATOD, in terms of the CLIP score and CMMD score with 100 images sampled at last. This table shows the average result of 5 sub-classes within each category. The overall improvement of our proposed CATOD is provided by \"Imp.\". Methods with the best performance are bold-folded.", "description": "This table compares the performance of CATOD against several baseline methods across different OOD concepts.  The metrics used are CLIP score (higher is better, indicating better image-text alignment) and CMMD score (lower is better, indicating less discrepancy between generated and real images).  The table is broken down by concept category (insect, lizard, penguin, seafish, snake) and shows the average performance across five subclasses within each category.  The \"Imp.\" column shows the improvement achieved by CATOD relative to the baseline methods.  The best-performing methods in each category are highlighted in bold.", "section": "Experiments"}, {"figure_path": "65htepluYE/tables/tables_7_1.jpg", "caption": "Table 1: A Comparison over the performance of CATOD, in terms of the CLIP score and CMMD score with 100 images sampled at last. This table shows the average result of 5 sub-classes within each category. The overall improvement of our proposed CATOD is provided by \"Imp.\". Methods with the best performance are bold-folded.", "description": "This table compares the performance of CATOD against several baseline methods across various metrics.  Specifically, it shows the average CLIP and CMMD scores for different methods (DreamBooth, Textual Inversion, LoRA) combined with different sampling strategies (RAND, CLIP, CATOD) on five insect sub-categories. The \"Imp.\" column highlights the performance improvement achieved by CATOD.  The best performing methods for each sub-category are highlighted in bold.", "section": "5 Experiments"}, {"figure_path": "65htepluYE/tables/tables_8_1.jpg", "caption": "Table 3: Results of Ablating Aesthetic, Concept-Matching Scorer and Weighted Scoring on CATOD. We show the average results conducted on the categories \"penguin\" and \"lizard\" with LoRA.", "description": "This table presents the ablation study results for the CATOD model. By removing either the aesthetic scoring module, the concept-matching scoring module, or the weighted scoring system, the impact on the model's performance in terms of CLIP score and CMMD score is evaluated.  The results are shown separately for the \"penguin\" and \"lizard\" categories, using LoRA as the adaptor module. This allows for a clear understanding of the individual contributions and interplay of each component within the CATOD framework.", "section": "5.3 Ablation Studies"}, {"figure_path": "65htepluYE/tables/tables_8_2.jpg", "caption": "Table 4: Results of CATOD with different types of aesthetic scorers. We show the average results conducted on the categories \"penguin\" and \"lizard\" with LoRA.", "description": "This table presents a comparison of the performance of the CATOD framework using different aesthetic scoring models.  The experiment focuses on the \"penguin\" and \"lizard\" categories, utilizing the LoRA adaptor. The table shows the average CLIP score (higher is better) and CMMD score (lower is better) for each aesthetic scorer.  The \"Ours\" row represents the performance of the proposed aesthetic scorer within the CATOD framework.", "section": "5.3 Ablation Studies"}, {"figure_path": "65htepluYE/tables/tables_20_1.jpg", "caption": "Table 1: A Comparison over the performance of CATOD, in terms of the CLIP score and CMMD score with 100 images sampled at last. This table shows the average result of 5 sub-classes within each category. The overall improvement of our proposed CATOD is provided by \"Imp.\". Methods with the best performance are bold-folded.", "description": "This table compares the performance of CATOD against several baseline methods (DreamBooth, TI, and LoRA) across five different concept categories (insect, lizard, penguin, seafish, snake).  Each category contains five sub-classes, and the average CLIP score (a measure of image-text similarity) and CMMD score (a measure of the discrepancy between generated and real images) are reported for each method.  The \"Imp.\" column shows the improvement achieved by CATOD compared to the baselines.", "section": "5 Experiments"}, {"figure_path": "65htepluYE/tables/tables_20_2.jpg", "caption": "Table 1: A Comparison over the performance of CATOD, in terms of the CLIP score and CMMD score with 100 images sampled at last. This table shows the average result of 5 sub-classes within each category. The overall improvement of our proposed CATOD is provided by \"Imp.\". Methods with the best performance are bold-folded.", "description": "This table compares the performance of CATOD against other methods (DreamBooth, TI, and LORA) using two metrics: CLIP score (higher is better) and CMMD score (lower is better).  It shows the average performance across five sub-categories for each of five main categories of images. The \"Imp.\" column indicates the improvement achieved by CATOD over the best-performing baseline method for each metric and sub-category.", "section": "5 Experiments"}, {"figure_path": "65htepluYE/tables/tables_23_1.jpg", "caption": "Table 7: A Comparison over the performance of CATOD on different types of the initial training data pool, in terms of the CLIP score and CMMD score with 100 images sampled at last. This table shows the average result of 5 sub-classes within each category. The overall improvement of our proposed CATOD is provided by \"Imp.\". Methods with the best performance are bold-folded.", "description": "This table compares the performance of CATOD using different initial training data pools (high-quality vs. random samples, with varying sample sizes) and evaluates the CLIP score and CMMD score.  The \"Imp.\" column shows the improvement of CATOD compared to baselines.  Bold values indicate the best-performing methods in each category.", "section": "5.1 Single-concept Generation Results"}, {"figure_path": "65htepluYE/tables/tables_24_1.jpg", "caption": "Table 1: A Comparison over the performance of CATOD, in terms of the CLIP score and CMMD score with 100 images sampled at last. This table shows the average result of 5 sub-classes within each category. The overall improvement of our proposed CATOD is provided by \"Imp.\". Methods with the best performance are bold-folded.", "description": "This table compares the performance of CATOD against several baseline methods (DreamBooth, TI, and LoRA) across different sampling strategies (RAND, CLIP, and CATOD) for 25 OOD concepts grouped into five categories (insect, lizard, penguin, seafish, snake).  The evaluation metrics are CLIP score (higher is better, measuring image-text alignment) and CMMD score (lower is better, measuring the discrepancy between generated and real images). The \"Imp.\" column shows the improvement achieved by CATOD compared to each baseline method.  The best performing methods in each category are bolded.", "section": "5 Experiments"}, {"figure_path": "65htepluYE/tables/tables_24_2.jpg", "caption": "Table 1: A Comparison over the performance of CATOD, in terms of the CLIP score and CMMD score with 100 images sampled at last. This table shows the average result of 5 sub-classes within each category. The overall improvement of our proposed CATOD is provided by \"Imp.\". Methods with the best performance are bold-folded.", "description": "This table compares the performance of CATOD against several baseline methods (DreamBooth, Textual Inversion, and LoRA) across various metrics, including CLIP score (higher is better) and CMMD score (lower is better).  The results are averaged across five sub-categories for each concept, and the improvement achieved by CATOD is explicitly shown.  The table highlights the best-performing method for each metric.", "section": "5 Experiments"}]