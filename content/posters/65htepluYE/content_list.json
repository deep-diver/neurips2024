[{"type": "text", "text": "Locating What You Need: Towards Adapting Diffusion Models to OOD Concepts In-the-Wild ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jianan Yang1,2 Chenchao Gao4,2 Zhiqing Xiao1,2 Junbo Zhao1,2 Sai Wu1,2 Gang Chen1,2, Haobo Wang3,2\u2217 ", "page_idx": 0}, {"type": "text", "text": "1College of Computer Science and Technology, Zhejiang University 2Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security 3School of Software Technology, Zhejiang University 4International School of Information Science and Engineering, Dalian University of Technology {jianan0115,zhiqing.xiao,j.zhao,wusai,cg,wanghaobo}@zju.edu.cn gccgyllyp@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent large-scale text-to-image generative models have attained unprecedented performance, while people established adaptor modules like LoRA and DreamBooth to extend this performance to even more unseen concept tokens. However, we empirically find that this workflow often fails to accurately depict the out-of-distribution concepts. This failure is highly related to the low quality of training data. To resolve this, we present a framework called Controllable Adaptor Towards Out-of-Distribution Concepts (CATOD). Our framework follows the active learning paradigm which includes high-quality data accumulation and adaptor training, enabling a finer-grained enhancement of generative results. The aesthetics score and concept-matching score are two major factors that impact the quality of synthetic results. One key component of CATOD is the weighted scoring system that automatically balances between these two scores and we also offer comprehensive theoretical analysis for this point. Then, it determines how to select data and schedule the adaptor training based on this scoring system. The extensive results show that CATOD significantly outperforms the prior approaches with an 11.10 boost on the CLIP score and a $33.08\\%$ decrease on the CMMD metric. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The generative modeling for text-to-image has attained unprecedented performance most recently [37, 45, 41, 49]. Notably, by training over billions of text-image data pairs [52, 51], the family of diffusion models has allowed high-fidelity image synthesis directed by the prompt provided in production. Despite their massive successes, these models still fail to generate images with decent quality and matching semantics, when encountering prompts that contain unseen or out-of-distribution concept tokens [60, 12, 21]. Simply put, the reason causing this failure is due to that the training set is not unbounded with limited variations. On the side of production, this limitation could significantly impact the practicality of this technique in real-world applications. ", "page_idx": 0}, {"type": "text", "text": "To deal with such concepts, recent works have resorted to adaptors such as Textual Inversion [18], DreamBooth [47, 58, 48], and LoRA [25, 66, 76], which tunes only a small part of the text-to-image model or insert extra modules. These adaptors largely reduce the training costs, and more importantly, preserve the visual aesthetic information originally learned by the underlying model. ", "page_idx": 0}, {"type": "image", "img_path": "65htepluYE/tmp/3f142951f43660babb1913220c44eac03b5c0c0310599b9dc6871c2d9bf45f61.jpg", "img_caption": ["Figure 1: Comparison of images generated before/after training adaptors over concepts with different CMMD scores. One observation is that concepts with higher CMMD scores are notably more challenging for the underlying model to generate (the second row). Additionally, we also notice that a higher CMMD value leads to a more notable loss of visual details when training adaptors (the third row). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, in this paper, we have found that recent works still struggle to accurately depict the visual details of out-of-distribution concepts (with a CMMD score above 3.5), as illustrated in Figure 1. Adapters like LoRA are able to accurately represent the shape and color of OOD concepts compared to the generative results before adaptor training, but they fall short when it comes to finer details such as texture, contours, and patterns. This issue arises from the fact that current studies predominantly focus on variations of in-distribution (ID) concepts (e.g., humans, dogs, and cats) while ignoring the out-of-distribution (OOD) ones. This failure motivates us to think about what makes the problem of distorted visual details happen when training adaptors. ", "page_idx": 1}, {"type": "text", "text": "In Figure 2, we observed that how an adaptor depicts OOD concepts can be significantly influenced by the quality of the training data. If the model is trained on samples containing disruptive objects, the resulting generative outputs are likely to reflect these disruptive elements. When the training data contains images with vague or very small instances of the OOD concept, the generative results may appear low-quality. In contrast, the high-quality data for adapting OOD concepts usually contains a single and clear object corresponding to the given concept, which is highly distinguishable from the background and other types of objects and helps produce accurate results with high-fidelity. However, manually picking such high-quality data requires much human labor and expertise, which may crucially limit the versatility of text-to-image models. Therefore, an effective paradigm to locate high-quality samples of OOD concepts is important for the practical shipping of this field. ", "page_idx": 1}, {"type": "text", "text": "To this end, we developed a framework called Controllable Adaptor Towards Out-of-Distribution Concepts (referred to as CATOD) which aims to identify high-quality samples to guide the adaptor training. This framework follows the Active Learning (AL) paradigm [54, 44], involving iteratively accumulating training data and updating the adaptor. The profound motivation of this approach is to comprehensively model the interaction between training data and the underlying text-to-image model. Specifically, CATOD includes two interconnected scores: the aesthetic score and the conceptmatching score, following the observation that object clarity and uniqueness largely impact generative results, as illustrated in Figure 2. Based on this, we devised a weighted scoring system that adapts itself according to the adaptor to select high-quality data while also properly balancing the two scores. With the carefully selected high-quality data, we schedule the adaptor training based on the quality of generative results evaluated through this scoring system. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are as follows: (i)-We have identified the challenge of adapting text-to-image models to out-of-distribution (OOD) concepts, where recent studies often struggle to accurately depict them; (ii)-We have introduced a framework called CATOD that iteratively updates training data and the adaptor to generate OOD concepts precisely; (iii)-Our extensive experiments verified that CATOD achieves significant performance gain with up to 11.10 on the CLIP score and $33.08\\%$ on the CMMD metric; (iv)-We have also offered theoretical insights into the key factors: aesthetics and concept-matching, which contribute to the effectiveness of our method. ", "page_idx": 1}, {"type": "image", "img_path": "65htepluYE/tmp/b42ee994a18088abe3d3c51524a8843ee3b5df55f5f0c60669e08df124ecf498.jpg", "img_caption": ["Figure 2: Comparison of synthetic results on data with different quality. Generated images are significantly influenced by the quality of training data. If the training data includes disruptive objects, the generative images may include disruptive visual details (Left). When an object within the image is too small, the results may not accurately represent the intended concepts (Middle). In contrast, if the image contains a high-fidelity object without disruptive elements (Right), the model is more likely to generate the desired result accurately. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 OOD Problem in Latent Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Revisiting Latent Diffusion Models (LDMs). Latent Diffusion Models (LDMs) [45] comprise two components: a diffusion process operating the latent space and an auto-encoder which contains an encoder $\\mathcal{E}$ mapping an image into the latent space and a decoder $\\mathcal{D}$ that reconstruct images from latent codes. Furthermore, the diffusion process can be conditioned on the output of text embedding models, enabling the auto-encoder to integrate the information derived from texts. Let $x$ be the image, the CLIP textual encoder $c_{\\theta}$ that maps the corresponding text $y$ into the latent space, the LDM loss is: ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{L D M}(x,y):=\\mathbb{E}_{z\\sim\\mathcal{E}(x),\\epsilon\\sim\\mathcal{N}(0,1),t}\\left[\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(z_{t},t,c_{\\theta}(y)\\right)\\right\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $t$ denotes the time step, $z_{t}$ denotes the latent code noised at time step $t$ , when $\\epsilon,\\epsilon_{\\theta}$ represents the noised samples and the denoising U-Net [46], respectively. Through this noising-denoising procedure applied to the latent codes, LDM enables the underlying model to integrate information derived from texts into the visual results, while also allowing more flexibility to produce images. ", "page_idx": 2}, {"type": "text", "text": "The OOD Concepts for LDMs. Intuitively, out-of-distribution (OOD) concepts refer to the category of data whose distribution deviates significantly from what the model has learned. This degree of drifting can be quantified by an MMD score [27], which evaluates the discrepancy between ground-truth images and the generative results in the image latent space. Formally, for two probability distributions $P$ and $Q$ , the MMD distance with respect to a positive definite kernel $k$ is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{list}_{M M D}^{2}(P,Q):=\\mathbb{E}_{\\boldsymbol{x}_{p}\\sim P,\\boldsymbol{x}_{p}^{\\prime}\\sim P}\\left[k(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})\\right]+\\mathbb{E}_{\\boldsymbol{x}_{q}\\sim Q,\\boldsymbol{x}_{q}^{\\prime}\\sim Q}\\left[k(\\boldsymbol{x}_{q},\\boldsymbol{x}_{q}^{\\prime})\\right]-2\\mathbb{E}_{\\boldsymbol{x}_{p}\\sim P,\\boldsymbol{x}_{q}\\sim Q}\\left[k(\\boldsymbol{x}_{p},\\boldsymbol{x}_{q})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $x_{p},x_{p}^{\\prime}$ independently follow the distribution $P$ while $x_{q},x_{q}^{\\prime}$ independently follow the distribution $Q$ , with $k$ the Gaussian RBF kernel [17]. In our implementation, we sample two sets of vectors from the distribution $P$ and $Q$ , then use CLIP embeddings [40] to calculate this score, which is also named as CMMD [27]. We set a concept with a high CMMD score (above 3.5) as an OOD concept. ", "page_idx": 2}, {"type": "text", "text": "OOD Concepts are Hard to Adapt. Recent text-to-image LDMs [45, 39] have achieved unprecedented performance on a wide range of concept tokens. However, we have found that there still exist many concepts that make LDMs fail after the adapter is fully trained. As we show in Figure 1, there are several discoveries: (i)-The concepts with higher CMMD scores are much more challenging for the underlying model to generate or adapt. The concepts with a CMMD score above 3.0 show explicit wrong visual details. For Axolotl and Frilled Lizards with CMMD above 4.0, the LDMs even generate the wrong species; (ii)-A higher CMMD score indicates a more severe loss of visual details when training adaptors. The concepts with CMMD scores above 3.5 in Axolotls and Emperor Penguin Chicks, show explicit distorted visual details, like color, texture, and delicate details. For Axolotl, the generative results show a wrong number and wrong positions of amateurs. For Emperor Penguin Chicks, the generative results show the wrong fur color of their heads and wings. To summarize, the higher the CMMD score of a concept, the more difficult it is for LDMs to adapt. ", "page_idx": 2}, {"type": "text", "text": "The High-Quality Matters. We further observe that the generative results are quite sensitive to training data when training adaptors meet OOD concepts. As shown in Figure 2, when the training images contain disruptive elements, the visual features of these disruptive elements will be easily introduced into the generative results. For example, when an adult emperor penguin appears in training data, then the black fur on their back can easily appear when generating their chicks, despite that their chicks have white fluffs as shown in the left part of Figure 2. If the object of the desired concept appears to be too small within the image, then the generative results tend to be bad-looking, since the necessary visual details are not fully identified. In the middle part of Figure 2, we can see that distorted visual details like texture and shape in axolotl and emperor penguin chicks largely harm the aesthetics of the generated image. To generate images correctly and good-looking, we require enough amount of images with objects of high fidelity and do not contain disruptive elements, namely High-Quality samples as shown in the right part of Figure 2. Therefore, we aim to devise an effective data selection strategy for locating those high-quality images for these OOD concepts. ", "page_idx": 2}, {"type": "image", "img_path": "65htepluYE/tmp/2a438590ec585583d24abf69039ed93f46dc24930d1886539386c2898e19da3e.jpg", "img_caption": ["Figure 3: The overall pipeline of CATOD. In brief, CATOD alternatively performs data selection and scheduled OOD concept adaption. In each training cycle, we first generate OOD concepts according to the current adaptor and calculate the weights for the aesthetic score and concept-matching score. Then, we calculate the weighted score for each sample within the data pool $\\mathcal{D}_{p o o l}$ , select the top images accordingly, and add them to the training pool. At last, CATOD fine-tunes the scoring system and training adaptors according to the updated data pool, and proceed to the next cycle. The above three steps alternatively proceed until convergence. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Overall Architecture ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We aim to adapt OOD concepts to LDMs correctly by an iterative data selection criterion that locates high-quality data and training adaptors accordingly as shown in Figure 3. Consequently, our method would alleviate issues introduced by disruptive elements, e.g., irrelevant objects, and blurry images, while maintaining high fidelity of generative results. The core to CATOD is a scoring system, which consists of an aesthetic scorer and a concept-matching scorer, aiming to resolve the problems of incorrectly introduced visual details and distorted objects we have observed in Section 2. By properly trading off between these two scores, CATOD locates the most valuable samples for adapting underlying LDMs to OOD concepts and making the desired OOD concept depicted correctly. ", "page_idx": 3}, {"type": "text", "text": "3.2 The Scoring System ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As described above, we need to estimate the potential impact of real-world samples over LDMs, according to which we select the most high-quality samples for training. In Section 2, we have observed two major factors that significantly impact generated image quality: object clarity and disruptive elements. Diving into the loss term $L_{L D M}(x,y)$ in Eq. (1), we may also observe that the training set $\\mathbf{X}_{T}$ should be optimized towards both the underlying model (including the embedded adaptor) and the conditional text $y$ , which indicates two important factors: object clarity and conceptmatching achieved by preserving aesthetic information originally learned by LDMs and accurate image-text matching, respectively. Therefore, we attribute the quality of images to aesthetic and concept-matching and design two decoupled scorers accordingly. ", "page_idx": 3}, {"type": "text", "text": "The Aesthetic Scorer. Aesthetic evaluation is a long-standing field, which comprehensively considers whether the lighting, contrast, texture, and other photographic factors of an image are consistent with human aesthetics. The general aesthetic scorer can be simply described as $p=S_{a e s}(x)$ , where $S_{a e s}$ indicates the aesthetic scoring model, $p$ denotes the predicted score, and $x$ represents the input image. Following the work of PA-IAA [33], we fine-tune the generic aesthetic model and make it adapt to OOD concepts with personalized preference scores. In general, we assign a high score to the samples within the training set $\\mathbf{X}_{T}$ , assign a low score to samples from irrelevant categories and samples generated by underlying LDMs without an adaptor, and use them to fine-tune the aesthetic scorer. More details of this personalization are given in the Appendix B.3. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The Concept-Matching Scorer. Intuitively, concept-matching describes whether the OOD concepts get perfectly reflected in the generated results. A similar task is image retrieval, which is designed to retrieve images containing objects describing the desired concepts. Since image retrieval also relies on feature maps and some sort of matching score to retrieve images, we adopt matching score from VLAD-related works [28, 64, 55] as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{c o n}(x)=\\frac{1}{|\\mathbf{X}_{T}|}\\sum_{k=1}^{|\\mathbf{X}_{T}|}\\phi\\left(r_{x},r_{k}\\right)\\exp\\left(\\left\\|r_{x}-r_{k}\\right\\|\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi(r_{x},r_{k})=1$ if $r_{k}$ is the closet representation for $r_{x}$ and is set to 0 otherwise. Simply put, VLAD regards the extracted representations for $\\mathbf{X}_{T}$ as a codebook and maps each image to its nearest code. Note that samples within $X_{T}$ mostly consist of clean and clear samples, this score is sufficient to quantify whether the object if exists in the given image distinguishes itself from other photographic components and matches the given concept. ", "page_idx": 4}, {"type": "text", "text": "3.3 Active Data Acquisition ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Optimization with Active Learning (AL). Aiming to mitigate the problems caused by low-quality training samples, we proactively integrate the training data $\\mathbf{X}_{T}$ into our objective as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nA^{*},\\mathbf{X}_{T}^{*}=\\arg\\operatorname*{min}_{A,\\mathbf{X}_{T}}\\mathbb{E}_{x\\sim\\mathbf{X}_{T}}L_{L D M}(x,A,y).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since the optimal set is initially unknown, a one-step optimization can easily lead to convergence to local optima. Therefore, we use an iterative paradigm to optimize adaptor $A$ and training data $\\mathbf{X}_{T}$ , respectively. In our implementation, we adopt the paradigm of AL [44, 54] to perform the optimization of $\\mathbf{\\dot{X}}_{T}^{\\left(t\\right)}$ by data accumulation before training adaptors, with $t$ denotes the time step: ", "page_idx": 4}, {"type": "equation", "text": "$$\nB^{(t)}=\\underset{B\\subset\\mathcal{D}_{p o o l}-\\mathbf{X}_{T}^{(t-1)},\\,|B|=b}{\\arg\\operatorname*{min}}\\mathbb{E}_{x\\sim\\mathbf{X}_{T}^{(t-1)}\\cup B}L_{L D M}(x,A,y),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $b$ is the number of samples added to the training pool at each cycle. The main reason for using AL is its preferred sample efficiency, with better controllable data bias management [15, 50]. Instead of repeatedly selecting data from the whole real-world data pool, AL provides a more efficient procedure to optimize training data by using data selection to accumulate high-quality training data. To this stage, the learning procedure of CATOD is relatively clear: the sample pool is progressively accumulated -by Eq. (5) -and the optimization of the adaptor $A$ is straightforward (Fig. 3). ", "page_idx": 4}, {"type": "text", "text": "Remember that AL involves iteratively updating the adaptor and the training data, it is important to design a training schedule for adaptors and determine how to acquire high quality based on the two scorers mentioned above. The primary objective of this design is to achieve a dynamic trade-off between the two scores. The specific details of these designs are described below. ", "page_idx": 4}, {"type": "text", "text": "The Active Schedule for Training Adaptors. The training schedule has been found crucial for successful adaption [47, 74, 65]. Since our training data continuously expands as the learning cycle of AL proceeds, the training schedule will be even more important. To arrange this schedule, we first calculate the aesthetics score $\\begin{array}{r}{\\gamma_{a e s}(A)=\\frac{1}{\\left|g_{A}(\\mathbf{X}_{T})\\right|}\\sum_{x_{g}\\in g_{A}(\\mathbf{X}_{T})}S_{a e s}(x_{g})}\\end{array}$ and concept-matching score $\\begin{array}{r}{\\gamma_{c o n}(A)=\\frac{1}{|g_{A}(\\mathbf{X}_{T})|}\\sum_{x_{g}\\in g_{A}(\\mathbf{X}_{T})}S_{c o n}\\overset{\\cdot}{(x_{g})}}\\end{array}$ of the adaptor $A$ based on its generative results $g_{A}(\\mathbf{X}_{T})$ , in which both the aesthetic score $\\gamma_{a e s}(A)$ and $\\gamma_{c o n}(A)$ range from 0 to 10. Then, we use a trigonometric indicator that comprehensively measures its performance: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\gamma(A)=10\\sin\\left(\\frac{\\pi}{20}\\gamma_{a e s}(A)\\right)\\sin\\left(\\frac{\\pi}{20}\\gamma_{c o n}(A)\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notably, $\\gamma(A)$ peaks when the two scores get close or around the common value of 5.0 for most samples. Meanwhile, it bottoms when the scoring exhibits a stronger signal of being biased (to either side). The properties emphasize that adapters balancing the two factors are of better quality. In training CATOD, we use $\\bar{\\gamma}(A)$ as a signal to reduce the learning rate and stop training in time. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Trading-off the Two Scores in Data Acquisition. After getting the adaptor $A$ , we continue to select the most suitable samples for the next-cycle training. Notice that whether newly selected images enhance the adaptor depends on both aesthetics and concept-matching, we ought to adjust our preferences according to the adaptor. In more detail, when the adaptor has high aesthetics but does not accurately depict the OOD concept, samples with high concept-matching scores better enhance the adaptor; when the adaptor fails to exhibit photographic attributes consistent with humans, samples with high aesthetics are preferred. To implement this preference, our acquisition score is: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS(x)=\\left(1-\\sin\\left(\\frac{\\pi}{20}\\gamma_{a e s}(A)\\right)\\right)S_{a e s}(x)+\\left(1-\\sin\\left(\\frac{\\pi}{20}\\gamma_{c o n}(A)\\right)\\right)S_{c o n}(x).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This formulation offers some meritable advantages. On one hand, the balancing terms are not prefixed or manually tuned, but dynamically dependent on the scores marginalized over the current generations. Further, when the score of either side gets larger, the corresponding balancing coefficient, in turn, decreases, thus attaining a proper trade-off. As a result, this mechanism encourages an alternation of sample selection towards both scoring metrics by selecting Top- $\\mathcal{K}$ samples to add to the training data $\\mathbf{X}_{T}$ accordingly, which effectively guarantees the sample set diversity. ", "page_idx": 5}, {"type": "text", "text": "4 Theoretical Insights ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section presents our theoretical analyses of why aesthetic/concept-matching scores work in OOD Adaption. Specifically, we derive the distance between real-world data distributions and synthetic data distributions and then induce the important factors that affect this distance. We first introduce the minimum mean square error (MMSE) [22, 11, 6] to measure the discrepancy between distributions: ", "page_idx": 5}, {"type": "text", "text": "Definition 4.1. The minimum mean square error (MMSE) of estimating an input random vector $\\widehat{\\mathbf{X}}\\in\\mathbb{R}^{n}$ from an observation/output $\\mathbf{X}\\in\\mathbb{R}^{k}$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{MMSE}(\\widehat{\\mathbf{X}}|\\mathbf{X})=\\operatorname*{inf}_{f\\in\\mathcal{M}(\\mathbb{R}^{n})}\\mathbb{E}\\left[\\left\\|\\widehat{\\mathbf{X}}-f(\\mathbf{X})\\right\\|^{2}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "in which $\\mathcal{M}(\\mathbb{R}^{n})$ denotes the space consisting of all measurable functions on $\\mathbb{R}^{n}$ . ", "page_idx": 5}, {"type": "text", "text": "Notice that we are trying to produce the best generation results which are initially unknown, our adaption task can be also regarded as estimating the optimal distribution with carefully selected data. By denoting the ideal generative results and the ideal adaptor with random variables $\\hat{\\mathbf{X}}_{G}$ and $A^{*}$ , respectively, we conclude that the LDM loss $L_{L D M}$ is consistent with this MMSE term : ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2. Let $L_{L D M}$ be the LDM loss following Eq. $^{\\,l}$ , and the image space lies within $\\mathbb{R}^{n}$ . Then there always exists an ideal random vector $\\widehat{\\mathbf{X}}_{G}\\in\\mathbb{R}^{\\bar{n}}$ and an adaptor $A^{*}$ for $L D M,$ , such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{\\mathbf{X}_{T}\\in\\mathbb{R}^{n}}\\operatorname{MMSE}(\\widehat{\\mathbf{X}}_{G}|\\mathbf{X}_{T})=\\arg\\operatorname*{min}_{\\mathbf{X}_{T}\\in\\mathbb{R}^{n}}\\mathbb{E}_{x\\sim\\mathbf{X}_{T}}\\left[L_{L D M}\\left(x,A^{*}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Based on the preceding deduction, we have confirmed that the change in MMSE can also indicate the change in LDMs. To dive deep into the MMSE term, we further decompose it as follows: ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3. (Pythagorean Theorem for MMSE [13].) Following Theorem 4.2, by setting $f$ to the generative model $g_{A}$ , the MMSE term in Eq. (8) can be decomposed into two terms as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\widehat{\\mathbf{X}}_{G}-g_{A}(\\mathbf{X}_{T})\\right\\Vert\\right]=\\mathbb{E}\\left[\\left\\Vert\\widehat{\\mathbf{X}}_{G}-g_{A^{\\star}}(\\mathbf{X}_{T})\\right\\Vert\\right]+\\mathbb{E}\\left[\\left\\Vert g_{A^{\\star}}(\\mathbf{X}_{T})-g_{A}(\\mathbf{X}_{T})\\right\\Vert\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "in which $A^{*}$ denotes a potential ideal adaptor containing information for text $y$ . Notice that the generative results $\\mathbf{X}_{G}$ relies on the training set $\\mathbf{X}_{T}$ , $g_{A}(\\mathbf{X}_{T})$ can also be written as $\\mathbf{X}_{G}|\\mathbf{X}_{T}$ . ", "page_idx": 5}, {"type": "text", "text": "Upon revisiting the two terms, we can gain a deeper understanding of the relationship between MMSE and aesthetic/concept-matching score: (i)-The first term is focused on estimating the difference between the generative distribution $\\mathbf{X}_{G}|\\mathbf{X}_{T}$ and the ideal distribution of $\\hat{\\mathbf{X}}_{G}$ . This assessment helps us understand how the introduced samples lose visual information within the underlying LDM, thus we can connect this term to aesthetic preservation, i.e. aesthetic score; (ii)-The second term illustrates the degree to which the training set $\\mathbf{X}_{T}$ distorts the OOD concept information within the ideal adaptor. Hence, a concept-matching score accurately portrays how newly given samples impact this term. At this stage, we have completed the theoretical support showing that both aesthetics and concept-matching are major factors that influence the performance of adaptors. ", "page_idx": 5}, {"type": "table", "img_path": "65htepluYE/tmp/ab5bc33b647844c9a465192b9cdf0feaf2758f0fe0cf2805085168fff9f3bd30.jpg", "table_caption": ["Table 1: A Comparison over the performance of CATOD, in terms of the CLIP score and CMMD score with 100 images sampled at last. This table shows the average result of 5 sub-classes within each category. The overall improvement of our proposed CATOD is provided by \u201cImp.\u201d. Methods with the best performance are bold-folded. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present the main experimental results both qualitatively and quantitatively. To evaluate our proposed CATOD, we combine it with several works for adaption, i.e. DreamBooth [47], Textual Inversion [18], and LoRA [25]. More experimental results can be found in the Appendix. The source code is attached in the Supplementary. ", "page_idx": 6}, {"type": "text", "text": "Datasets. We test our method on datasets with 25 OOD concepts that can hardly be generated through prompt engineering on the text-to-image model. This dataset consists of 5 categories: insect, lizard, penguin, seafish, and snake, and each category contains data from 5 OOD concepts. Each concept has 1,000 examples in total with 100 samples left out for validation. The dataset is collected from publicly available datasets including ImageNet, iNaturalist 2018 [67], IP102 [71]. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We conduct the active generation experiments on our proposed CATOD and three representative adaptors, i.e. DreamBooth [47], Textual Inversion [18] (termed as TI in the paper), and LoRA [25]. Since there are currently no available studies that focus on locating \u201chigh-quality\u201d samples for training, we apply random sampling (RAND), and CLIP-score-based sampling (CLIP) for each baseline in our active learning setting. Each experiment starts with 20 randomly sampled instances, and we conducted 5 cycles of data accumulation in which we selected 20 \u201cgood\u201d samples to add to the training pool. We train 20 epochs for all combinations of adaption techniques and sampling strategies in each active learning cycle, with a batch size of 1. Furthermore, we generate 100 images for each concept for evaluation. We use the commonly adopted Stable Diffusion 2.0 pre-trained on LAION-5B [51] following Rombach\u2019s work [45]. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. We evaluate the quality of our generated images with the widely used CLIP score [20] and the recently proposed CMMD score [27], which quantify the model performance in two aspects. Specifically, the CLIP score measures how generated images match the given text, which is expected to be as high as possible. Meanwhile, the CMMD score evaluates the discrepancy between generated images with the real ones, indicating better generative results with lower values. ", "page_idx": 6}, {"type": "text", "text": "5.1 Single-concept Generation Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To evaluate the performance of CATOD on OOD concepts, we test it on all 25 target concepts one by one separately and report the average performance of 5 concepts within each category in Table 1. We show the superior results of our CATOD with respect to both qualitative and quantitative comparisons. ", "page_idx": 6}, {"type": "text", "text": "Qualitative Comparisons. We qualitatively compare CATOD with other sampling strategies according to their generated images based on LoRA [25]. With random sampling (RAND), we observe that the generative results only partially learned some photographic attributes like color and texture, but failed to make the objects have the correct appearance and shape. CLIP-based sampling (CLIP) somehow produces the corrected shape of the concept but still fails to capture the necessary details for describing the object. In comparison, our proposed CATOD successfully matches all the photographic attributes, while also guaranteeing the image aesthetics. ", "page_idx": 6}, {"type": "table", "img_path": "65htepluYE/tmp/b8f80b0dfac477456c8ad51961b150b8a601db5da4dcdd13903d67a78c9bc5ee.jpg", "table_caption": ["Table 2: A Comparison over the performance of CATOD when training with images from multiple concepts, in terms of the CLIP score and CMMD score with 100 images sampled at last. In each experiment, we sample images from all the sub-classes within each category and check whether the fine-tuned model can generate all 5 concepts. The overall improvement of our proposed CATOD is provided by \u201cImp.\u201d. Methods with the best performance are bold-folded. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "65htepluYE/tmp/ec9a0887f68e18ee86294e19924a75ab4501daed8d9e5614f5d9e546472bb830.jpg", "img_caption": ["Figure 4: A comparison of different sampling strategies with LoRA. Specifically, we compare three lines of works: (1) RAND, in which the model is trained with 100 randomly selected samples; (2) with samples of the highest CLIP scores (100 samples); (3) 100 samples with CATOD. The model trained with randomly sampled data fails to capture the features of out-of-distribution (OOD) concepts, while the ones trained with top CLIP scores contain necessary details but also include disruptive elements. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "To further look at how CATOD learns the photographic attributes that precisely match the concept, we show samples generated from different cycles in Fig. 7. We can see that some attributes like color, and texture are already learned in cycle 1, but the shape of the object does not match the ground-truth ones. From cycle 1 to 3, CATOD shows a clear shape modification, making the objects more like the real ones. From cycles 3 to 5, an iterative refinement on more photographic details like light, contrast, and other minor modifications (like the beak for penguins and antenna for axolotls) is shown in generative results, making them hard to distinguish from the ground-truth ones even with a careful look. At cycle 5 and later cycles, the image quality stabilizes and we can hardly see enhancement apart from image diversity. To conclude, we can see an explicit attribute matching process from easy ones to the finer ones within CATOD, showing the importance and effectiveness of iterative training. ", "page_idx": 7}, {"type": "text", "text": "Quantitative Comparisons. Table 1 reports CLIP scores [20] and CMMD scores [27] from each strategy with models trained on 25 different OOD concepts and evaluated through the generative results. Specifically, we evaluate the performance of CATOD on each concept and average the results within each category in Table 1. We have the following conclusions: (1) the average performance of our strategies outperforms all compared methods by a $0.56{\\sim}11.10$ CLIP score, justifying the effectiveness of locating aesthetic and clean samples over text-image matching. (2) CATOD also brings a $0.12{\\sim}0.54$ CMMD decrease on various frameworks and concepts, indicating better image alignment with proper sampling guided by CATOD. To summarize, both image-matching and textmatching scores exhibit better results compared to the baselines, suggesting that our proposed CATOD significantly outperforms other strategies, which is consistent with the qualitative results in Fig. 4. ", "page_idx": 7}, {"type": "image", "img_path": "65htepluYE/tmp/58fca0114a75abed41165a29955696b46a549b884013751676e42f3af9ccc6e2.jpg", "img_caption": ["Figure 5: Generative results as cycle proceeds. Samples are generated with CATOD on cycles from 1 to 7. To better observe how generated images change as the cycle proceeds, we conduct another 2 cycles here. In each cycle, we select and add 20 high-quality samples. Generative samples start to converge and contain the right details within the original concept after cycle 4 or 5. We can also see that those generative results contain diverse contents within the background based on the few images given. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "65htepluYE/tmp/1baf6ca3b8b8d899145bad30db64424d254fe97cc178252ac957f6863b26cc9e.jpg", "table_caption": ["Table 3: Results of Ablating Aesthetic, ConceptMatching Scorer and Weighted Scoring on CATOD. We show the average results conducted on the categories \u201cpenguin\u201d and \u201clizard\u201d with LoRA. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Multi-concept adaption Results. ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "65htepluYE/tmp/936bd999d32ef4de4a7386790429d583674ae06b6954d08d2df5237473ef479e.jpg", "table_caption": ["Table 4: Results of CATOD with different types of aesthetic scorers. We show the average results conducted on the categories \u201cpenguin\u201d and \u201clizard\u201d with LoRA. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "To further investigate whether CATOD could adapt multiple concepts simultaneously, we group the 25 concepts by category and train the adaptation model on each category. To be specific, we compare our baselines using a generated set consisting of 500 images (100 images per concept) and exhibit our results in Table 2. Following the setting of single-concept adaption, we conduct 10 cycles of data accumulation and get 200 samples at last, since multi-concept adaption requires more data. We still observe a notable performance gain with a $1.56{\\sim}5.07$ CLIP score increase and a $0.12{\\sim}0.86\\$ CMMD score decrease compared to baselines. These results verify that our CATOD still achieves better performance on multi-concept adaption. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We verify the efficacy of all components in our proposed CATOD in Table 3 and 4, including the aesthetical scoring module and the concept-matching mechanism within the weighted scoring system. ", "page_idx": 8}, {"type": "text", "text": "W/O Aesthetic Score. First, we validate the effectiveness of CATOD by removing the aesthetic scores. A significant decrease in performance (up to 8.55) on the CLIP score can be observed in Table 3. We attribute this decline to the fact that matching-based metrics prioritize image representations over the given concept, leading to a deterioration in image-text matching. ", "page_idx": 8}, {"type": "text", "text": "The Type of Aesthetic Scores. To further investigate the impact of aesthetic scores, we have also applied different types of aesthetics with CATOD in Table 4. We observed that recent state-of-theart aesthetic evaluations did not improve OOD adaption and even led to minor performance loss. This could be attributed to the fact that these models were originally designed for general aesthetic assessments, whereas our aesthetic scorer is highly personalized towards specific OOD concepts. ", "page_idx": 8}, {"type": "text", "text": "W/O Concept-Matching Score. After removing the concept-matching scorer, we observed that the adaptor tends to perform better compared to just removing the aesthetic scorers. This might be due to the aesthetic scorer being designed based on CLIP backbones, showing some consistency with the CLIP score. However, it still demonstrates limitations based on the image-matching score CMMD. While these aesthetic qualities partly describe the clarity and accuracy of the object, they do not adequately focus on the image representation space. ", "page_idx": 8}, {"type": "text", "text": "The Weighted Score. We have confirmed the effectiveness of balancing two scores by simply adding them together to guide CATOD (Table 3, Line 5). We observed that this resulted in consistent performance loss for both the CLIP score (up to 1.65) and the CMMD score (up to 0.22). This suggests that both text-image matching and image-to-image matching are affected by the trade-off between aesthetics and concept matching, further emphasizing the importance of these two scores. ", "page_idx": 9}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Personalized Text-to-image Synthesis with Adaptors. The task of text-to-image generation involves creating specific images based on text descriptions [3, 73, 79, 4], and has achieved impressive performance with state-of-the-art diffusion models [41, 45, 39]. Therefore, adapting large-scale text-to-image models to a specific concept while also preserving this amazing performance, i.e. personalization [7], has become another recent research interest. But this is often difficult since re-training a model with an expanded dataset for each new concept is prohibitively expensive while fine-tuning the whole model [12, 32] or transformation modules [83, 20, 60] on few examples typically leads to some rate of forgetting [30]. Therefore, adaptors such as Textual Inversion [18], DreamBooth [47, 48, 5, 58], LoRA [25, 66, 86, 76], along with some other works [70, 19] derived from them, have become more commonly adopted. Typically, they focus on a small but crucial part of the model or extra networks inserted into underlying models, thus more computationally-efficient, while also preserving the efficacy of the underlying models with lower computational costs. For example, textual inversion (TI) [18] represents the newly-given concept with pseudo word [42] and remapping it to another carefully trained embedding in the text-encoding space, guided by few images. Despite their computational efficiency, these approaches are still facing difficulties dealing with out-of-distribution concepts as we observe in Section 2. ", "page_idx": 9}, {"type": "text", "text": "Active Learning and Selection. Active Learning is a machine learning paradigm that involves actively selecting the most suitable data for training models from external data sources [44, 63]. The most crucial part of active learning is the strategy to locate the optimal data batch. Current studies can be roughly categorized as follows: (a) Score-based methods that prefer the samples with the highest information scores [36, 69, 10]; (b) Representation-based methods searching for the samples that are the most representative of the underlying data distribution [53, 1, 62]. The Active Selection paradigm within Active Learning serves as an efficient and powerful dataset curation tool (i.e. adaption), leading to numerous studies adopting this paradigm from a wide range of subjects [68, 24, 8]. Meanwhile, due to the effectiveness of Active Learning in selecting the most suitable training samples, recent studies have utilized similar sampling strategies to address challenges associated with long-tailed distributions [56, 57] and noisy data [72]. Given that Out-Of-Distribution (OOD) concepts in Latent Diffusion Models (LDMs) often involve unseen or long-tailed concept tokens [79], this motivates us to leverage Active Learning for selecting samples that are well-suited for training adaptors. ", "page_idx": 9}, {"type": "text", "text": "In this work, we focus on scored-based strategies in two-fold: aesthetics and concept-matching. Image Aesthetics Assessment (IAA) aims at evaluating image aesthetics computationally and automatically [9], while automatically assessing image aesthetics is useful for many applications [35, 29, 14]. To take a step further, personalized image aesthetics assessment (PIAA) [43, 85, 75] was proposed to capture unique aesthetic preferences, consistent with our goal to adjust our paradigms accordingly different concepts. At the same time, \"Concept-matching\" is adopted from the field of image retrieval, in which we search for relevant images in an image gallery by analyzing the visual content (e.g., objects, colors, textures, shapes etc.), given a query image [61, 31]. To design a paradigm that automatically meets the harsh requirements for adaptor training, we consider both two factors. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose CATOD, an enhanced, data-efficient, and practically useful version of the OOD concept adaptation for AIGC. This method is encapsulated in an active-learned paradigm with carefully designed acquisitional scoring mechanisms. CATOD significantly outperforms the prior approaches in many (if not most) aspects including generation quality, concept matches, technological robustness, data efficiency, etc. In the future, we hope to ship CATOD to the open-source community so as to absorb more OOD concepts that were originally uncovered. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by the Pioneer R&D Program of Zhejiang (No. 2024C01035). This work is also partially supported by the Fundamental Research Funds for the Central Universities (226-2024- 00049) and (226-2024-00145). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.   \n[2] Song Bai, Xiang Bai, Qi Tian, and Longin Jan Latecki. Regularized diffusion process on bidirectional context for object retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(5):1213\u20131226, 2018. [3] Razan Bayoumi, Marco Alfonse, and Abdel-Badeeh M Salem. Text-to-image synthesis: A comparative study. In Digital Transformation Technology: Proceedings of ITAF 2020, pages 229\u2013251. Springer, 2021. [4] Pu Cao, Feng Zhou, Qing Song, and Lu Yang. Controllable generation with text-to-image diffusion models: A survey. arXiv preprint arXiv:2403.04279, 2024. [5] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth: Disentangled parameter-efficient tuning for subject-driven text-to-image generation. arXiv preprint arXiv:2305.03374, 2023. [6] Edwin K. P. Chong. Well-conditioned linear minimum mean square error estimation. IEEE Control. Syst. Lett., 6:2431\u20132436, 2022.   \n[7] Niv Cohen, Rinon Gal, Eli A Meirom, Gal Chechik, and Yuval Atzmon. \u201cthis is my unicorn, fluffy\u201d: Personalizing frozen vision-language representations. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XX, pages 558\u2013577. Springer, 2022. [8] Wupeng Deng, Quan Liu, Feifan Zhao, Duc Truong Pham, Jiwei Hu, Yongjing Wang, and Zude Zhou. Learning by doing: A dual-loop implementation architecture of deep active learning and human-machine collaboration for smart robot vision. Robotics and Computer-Integrated Manufacturing, 86:102673, 2024. [9] Yubin Deng, Chen Change Loy, and Xiaoou Tang. Image aesthetic assessment: An experimental survey. IEEE Signal Processing Magazine, 34(4):80\u2013106, 2017.   \n[10] Francesco Di Fiore, Michela Nardelli, and Laura Mainini. Active learning and bayesian optimization: a unified perspective to learn with a goal. Archives of Computational Methods in Engineering, pages 1\u201329, 2024.   \n[11] Mario D\u00edaz, Peter Kairouz, and Lalitha Sankar. Lower bounds for the minimum mean-square error via neural network-based estimation. CoRR, abs/2108.12851, 2021.   \n[12] Yuxuan Ding, Lingqiao Liu, Chunna Tian, Jingyuan Yang, and Haoxuan Ding. Don\u2019t stop learning: Towards continual learning for the clip model. arXiv preprint arXiv:2207.09248, 2022.   \n[13] Alex Dytso, H Vincent Poor, Ronit Bustin, and Shlomo Shamai. On the structure of the least favorable prior distributions. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 1081\u20131085. IEEE, 2018.   \n[14] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone photography. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3677\u20133686, 2020.   \n[15] Sebastian Farquhar, Yarin Gal, and Tom Rainforth. On statistical bias in active learning: How and when to fix it. arXiv preprint arXiv:2101.11665, 2021.   \n[16] Li Fe-Fei et al. A bayesian approach to unsupervised one-shot learning of object categories. In proceedings ninth IEEE international conference on computer vision, pages 1134\u20131141. IEEE, 2003.   \n[17] Kenji Fukumizu, Arthur Gretton, Bernhard Sch\u00f6lkopf, and Bharath K Sriperumbudur. Characteristic kernels on groups and semigroups. Advances in neural information processing systems, 21, 2008.   \n[18] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.   \n[19] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel CohenOr. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG), 42(4):1\u201313, 2023.   \n[20] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544, 2021.   \n[21] Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, and Philip Torr. A systematic survey of prompt engineering on vision-language foundation models. arXiv preprint arXiv:2307.12980, 2023.   \n[22] Dongning Guo, Shlomo Shamai, and Sergio Verd\u00fa. Estimation of non-gaussian random variables in gaussian noise: Properties of the MMSE. In Frank R. Kschischang and En-Hui Yang, editors, 2008 IEEE International Symposium on Information Theory, ISIT 2008, Toronto, ON, Canada, July 6-11, 2008, pages 1083\u20131087. IEEE, 2008.   \n[23] Shuai He, Yongchang Zhang, Rui Xie, Dongxiang Jiang, and Anlong Ming. Rethinking image aesthetics assessment: Models, datasets and benchmarks. In Proceeding of the Thirty-First International Joint Conference on Artificial Intelligence, 2022.   \n[24] Aral Hekimoglu, Michael Schmidt, and Alvaro Marcos-Ramiro. Monocular 3d object detection with lidar guided semi supervised active learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2346\u20132355, 2024.   \n[25] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021.   \n[26] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269, 2023.   \n[27] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards a better evaluation metric for image generation. arXiv preprint arXiv:2401.09603, 2023.   \n[28] Herv\u00e9 J\u00e9gou, Matthijs Douze, Cordelia Schmid, and Patrick P\u00e9rez. Aggregating local descriptors into a compact image representation. In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2010, San Francisco, CA, USA, 13-18 June 2010, pages 3304\u20133311. IEEE Computer Society, 2010.   \n[29] Shu Kong, Xiaohui Shen, Zhe Lin, Radomir Mech, and Charless Fowlkes. Photo aesthetics ranking network with attributes and content adaptation. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part I 14, pages 662\u2013679. Springer, 2016.   \n[30] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Finetuning can distort pretrained features and underperform out-of-distribution. arXiv preprint arXiv:2202.10054, 2022.   \n[31] Michael S Lew, Nicu Sebe, Chabane Djeraba, and Ramesh Jain. Content-based multimedia information retrieval: State of the art and challenges. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 2(1):1\u201319, 2006.   \n[32] Dingcheng Li, Zheng Chen, Eunah Cho, Jie Hao, Xiaohu Liu, Fan Xing, Chenlei Guo, and Yang Liu. Overcoming catastrophic forgetting during domain adaptation of seq2seq language generation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5441\u20135454, 2022.   \n[33] Leida Li, Hancheng Zhu, Sicheng Zhao, Guiguang Ding, and Weisi Lin. Personality-assisted multi-task learning for generic and personalized image aesthetics assessment. IEEE Transactions on Image Processing, 29:3898\u20133910, 2020.   \n[34] Chundi Liu, Guangwei Yu, Maksims Volkovs, Cheng Chang, Himanshu Rai, Junwei Ma, and Satya Krishna Gorti. Guided similarity separation for image retrieval. Advances in Neural Information Processing Systems, 32, 2019.   \n[35] Yiwen Luo and Xiaoou Tang. Photo and video quality evaluation: Focusing on the subject. In Computer Vision\u2013ECCV 2008: 10th European Conference on Computer Vision, Marseille, France, October 12-18, 2008, Proceedings, Part III 10, pages 386\u2013399. Springer, 2008.   \n[36] Xiaoyi Mai, Salman Avestimehr, Antonio Ortega, and Mahdi Soltanolkotabi. On the effectiveness of active learning by uncertainty sampling in classification of high-dimensional gaussian mixture data. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4238\u20134242. IEEE, 2022.   \n[37] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n[38] Michael A Nielsen. Neural networks and deep learning, volume 25. Determination press San Francisco, CA, USA, 2015.   \n[39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.   \n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[41] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.   \n[42] Natalie Rathvon. Early Reading Assessment. A Practitioner\u2019s Handbook. ERIC, 2004.   \n[43] Jian Ren, Xiaohui Shen, Zhe Lin, Radomir Mech, and David J Foran. Personalized image aesthetics. In Proceedings of the IEEE international conference on computer vision, pages 638\u2013647, 2017.   \n[44] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. ACM computing surveys (CSUR), 54(9):1\u201340, 2021.   \n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n[46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015.   \n[47] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine-tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500\u201322510, 2023.   \n[48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06949, 2023.   \n[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.   \n[50] Hitesh Sapkota and Qi Yu. Balancing bias and variance for active weakly supervised learning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1536\u20131546, 2022.   \n[51] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.   \n[52] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.   \n[53] Silvio Savarese Sener et al. Active learning for convolutional neural networks: A core-set approach. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.   \n[54] Burr Settles. Active learning literature survey. 2009.   \n[55] Shihao Shao, Kaifeng Chen, Arjun Karpur, Qinghua Cui, Andr\u00e9 Ara\u00fajo, and Bingyi Cao. Global features are all you need for image retrieval and reranking. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 11002\u2013 11012. IEEE, 2023.   \n[56] Jiang-Xin Shi, Tong Wei, Yuke Xiang, and Yu-Feng Li. How re-sampling helps for long-tail learning? Advances in Neural Information Processing Systems, 36:75669\u201375687, 2023.   \n[57] Jiang-Xin Shi, Tong Wei, Zhi Zhou, Jie-Jing Shao, Xin-Yan Han, and Yu-Feng Li. Longtail learning with foundation model: Heavy fine-tuning hurts. In Proceedings of the 41st International Conference on Machine Learning, pages 45014\u201345039, 2024.   \n[58] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. arXiv preprint arXiv:2304.03411, 2023.   \n[59] Josef Sivic and Andrew Zisserman. Video google: A text retrieval approach to object matching in videos. In Computer Vision, IEEE International Conference on, volume 3, pages 1470\u20131470. IEEE Computer Society, 2003.   \n[60] Gabriel Skantze and Bram Willemsen. Collie: Continual learning of language grounding from language-image embeddings. Journal of Artificial Intelligence Research, 74:1201\u20131223, 2022.   \n[61] Arnold WM Smeulders, Marcel Worring, Simone Santini, Amarnath Gupta, and Ramesh Jain. Content-based image retrieval at the end of the early years. IEEE Transactions on pattern analysis and machine intelligence, 22(12):1349\u20131380, 2000.   \n[62] Qun Sui and Sujit K Ghosh. Similarity-based active learning methods. Expert Systems with Applications, 251:123849, 2024.   \n[63] Alaa Tharwat and Wolfram Schenck. A survey on active learning: state-of-the-art, practical challenges and research directions. Mathematics, 11(4):820, 2023.   \n[64] Weijie Tu, Weijian Deng, Tom Gedeon, and Liang Zheng. A bag-of-prototypes representation for dataset-level applications. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 2881\u20132892. IEEE, 2023.   \n[65] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1921\u20131930. IEEE, 2023.   \n[66] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. Dylora: Parameterefficient tuning of pre-trained models using dynamic search-free low-rank adaptation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3266\u20133279, 2023.   \n[67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8769\u20138778, 2018.   \n[68] Jinsheng Wang, Guoji Xu, Peng Yuan, Yongle Li, and Ahsan Kareem. An efficient and versatile kriging-based active learning method for structural reliability analysis. Reliability Engineering & System Safety, 241:109670, 2024.   \n[69] Tianyang Wang, Xingjian Li, Pengkun Yang, Guosheng Hu, Xiangrui Zeng, Siyu Huang, Cheng-Zhong Xu, and Min Xu. Boosting active learning via improving test performance. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8566\u20138574, 2022.   \n[70] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15943\u2013 15953, 2023.   \n[71] Xiaoping Wu, Chi Zhan, Yu-Kun Lai, Ming-Ming Cheng, and Jufeng Yang. Ip102: A large-scale benchmark dataset for insect pest recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8787\u20138796, 2019.   \n[72] Zhi-Fan Wu, Tong Wei, Jianwen Jiang, Chaojie Mao, Mingqian Tang, and Yu-Feng Li. Ngc: A unified framework for learning with open-world noisy data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 62\u201371, 2021.   \n[73] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.   \n[74] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. RAPHAEL: text-to-image generation via large mixture of diffusion paths. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[75] Yuzhe Yang, Liwu Xu, Leida Li, Nan Qie, Yaqian Li, Peng Zhang, and Yandong Guo. Personalized image aesthetics assessment with rich attributes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19861\u201319869, 2022.   \n[76] Shin-Ying Yeh, Yu-Guan Hsieh, Zhidong Gao, Bernard BW Yang, Giyeong Oh, and Yanmin Gong. Navigating text-to-image customization: From lycoris fine-tuning to model evaluation. arXiv preprint arXiv:2309.14859, 2023.   \n[77] Ran Yi, Haoyuan Tian, Zhihao Gu, Yu-Kun Lai, and Paul L Rosin. Towards artistic image aesthetics assessment: a large-scale dataset and a new method. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22388\u201322397, 2023.   \n[78] Mariia Zameshina, Olivier Teytaud, and Laurent Najman. Diverse diffusion: Enhancing image diversity in text-to-image generation. arXiv preprint arXiv:2310.12583, 2023.   \n[79] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion model in generative ai: A survey. arXiv preprint arXiv:2303.07909, 2023.   \n[80] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \n[81] Lin Zhao, Meimei Shang, Fei Gao, Rongsheng Li, Fei Huang, and Jun Yu. Representation learning of image composition for aesthetic prediction. Computer Vision and Image Understanding, 199:103024, 2020.   \n[82] Yan Zhao, Lei Wang, Luping Zhou, Yinghuan Shi, and Yang Gao. Modelling diffusion process by deep neural networks for image retrieval. In BMVC, page 161, 2018.   \n[83] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.   \n[84] Hancheng Zhu, Leida Li, Jinjian Wu, Sicheng Zhao, Guiguang Ding, and Guangming Shi. Personalized image aesthetics assessment via meta-learning with bilevel gradient optimization. IEEE Transactions on Cybernetics, 52(3):1798\u20131811, 2020.   \n[85] Hancheng Zhu, Yong Zhou, Leida Li, Yaqian Li, and Yandong Guo. Learning personalized image aesthetics from subjective and objective attributes. IEEE Transactions on Multimedia, 2021.   \n[86] Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, and Lei Zhang. Deltalora: Fine-tuning high-rank parameters with the delta of low-rank matrices. arXiv preprint arXiv:2309.02411, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Theory ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section provides a complete derivation for the analysis given in Section 4. In brief, we first link $L_{L D M}$ to a minimum mean square error (MMSE) term in Theorem A.2 (i.e. Theorem 3.2 within the main context), then decompose the MMSE term to see how we minimize MMSE in different aspects in Theorem A.3 (i.e. Theorem 3.3 within the main context) in the main context. For convenience, we reclaim some of the formulations at the beginning of this section: ", "page_idx": 16}, {"type": "text", "text": "Definition A.1. The minimum mean square error (MMSE) of estimating an input random vector $\\widehat{\\mathbf{X}}\\in\\mathbb{R}^{n}$ from an observation/output $\\mathbf{X}\\in\\mathbb{R}^{k}$ is defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{MMSE}(\\widehat{\\mathbf{X}}|\\mathbf{X})=\\operatorname*{inf}_{f\\in\\mathcal{M}(\\mathbb{R}^{n})}\\mathbb{E}\\left[\\left\\|\\widehat{\\mathbf{X}}-f(\\mathbf{X})\\right\\|^{2}\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "in which $\\mathcal{M}(\\mathbb{R}^{n})$ denotes the space consisting of all measurable functions on $\\mathbb{R}^{n}$ . ", "page_idx": 16}, {"type": "text", "text": "Based on Eq. (1), we can adapt the LDM to an arbitrary concept $y$ with a corresponding image set $X$ that describes this concept. Typical adaptations are mostly based on fine-tuning all the parameters $\\theta$ , which is quite costly and requires a lot of data. However, recent studies have found that training only a small part of the model [18, 47] or inserting extra networks [25, 26] could attain the same performance, which largely alleviates the computational burden with far fewer samples need for training. We call this part of parameters or networks as \u201cadaptors\u201d and denote them by $A$ . Then, the fine-tuning process of the adaptors, also named adaption, can be formulated as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nA^{*}=\\arg\\operatorname*{min}_{A}\\mathbb{E}_{x\\sim X,z\\sim\\mathcal{E}(x),\\epsilon\\sim\\mathcal{N}(0,1),t}\\left[\\|\\epsilon-\\epsilon_{\\theta,A}\\left(z_{t},t,c_{\\theta,A}\\left(y\\right)\\right)\\|_{2}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, $v$ be a conditioning vector corresponding to some given text $y$ , the LDM loss is: ", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{L D M}(x,A,y):=\\mathbb{E}_{x\\sim X,z\\sim\\mathcal{E}(x),\\epsilon\\sim\\mathcal{N}(0,1),t}\\left[\\left\\|\\epsilon-\\epsilon_{\\theta,A}\\left(z_{t},t,c_{\\theta,A}\\left(y\\right)\\right)\\right\\|_{2}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Following the definition of LDM loss, we continue to link LDM loss to the MMSE term in the theorem as follows (in which we omit the parameter $y$ since it is independent of other parameters): ", "page_idx": 16}, {"type": "text", "text": "Theorem A.2. Let $L_{L D M}$ be the LDM loss following Eq. (13), and the image space lies within $\\mathbb{R}^{n}$ . Then there always exists an ideal random vector $\\widehat{\\mathbf{X}}_{G}\\in\\mathbb{R}^{n}$ and a condition vector $v^{*}$ within the text embedding space for LDM, such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{\\mathbf{X}_{T}\\in\\mathbb{R}^{n}}\\operatorname{MMSE}(\\widehat{\\mathbf{X}}_{G}|\\mathbf{X}_{T})=\\arg\\operatorname*{min}_{\\mathbf{X}_{T}\\in\\mathbb{R}^{n}}\\mathbb{E}_{x\\sim\\mathbf{X}_{T}}\\left[L_{L D M}\\left(x,A^{*}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. By denoting an ideal generative distribution with $\\widehat{\\mathbf{X}}_{G}$ , which is produced by the generative model with a minimized LDM loss, we continue our pro of. Set $f_{A}(\\mathbf{X}_{T})=\\mathbb{E}_{x\\sim\\mathbf{X}_{T}}\\left[x,A\\right]$ , we can see that $f_{A}$ is also a functional controlled by the adaptor $A$ . Following the definition of MMSE and a fixed $\\mathbf{X}_{T}$ , we set $A^{*}$ as: ", "page_idx": 16}, {"type": "equation", "text": "$$\nA^{*}=\\underset{\\upsilon}{\\arg\\operatorname*{min}}\\,\\mathbb{E}_{x\\sim\\mathbf{X}_{T}}\\left[L_{L D M}(x,A)\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With this $A^{*}$ , and the Universal Approximation Theorem [38] for all deep learning models, $f_{A^{*}}$ becomes the functional needed to quantify the MMSE term following its definition. Therefore, with proper $\\mathbf{X}_{G},A^{*}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MMSE}(\\widehat{\\mathbf{X}}_{G}|\\mathbf{X}_{T})=\\mathbb{E}_{x\\sim\\mathbf{X}_{T}}\\left[L_{L D M}\\left(x,A^{*}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "thus completing the proof of this theorem. ", "page_idx": 16}, {"type": "text", "text": "Theorem A.3. (Pythagorean Theorem for MMSE [13].) Following Theorem A.2, by setting $f$ to the generative model $g_{A*}$ with an ideal adaptor $A^{*}$ containing sufficient OOD concept information, rewrite $g_{A}(\\mathbf{X}_{T})$ to $\\mathbf{X}_{G}|\\mathbf{X}_{T}$ , and the minimum mean square error (MMSE) in Eq. (11) can be decomposed into two terms: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\widehat{\\mathbf{X}}_{G}-\\mathbb{E}\\left[\\mathbf{X}_{G}|\\mathbf{X}_{T}\\right]\\right\\Vert\\right]=\\mathbb{E}\\left[\\left\\Vert\\widehat{\\mathbf{X}}_{G}-g_{A^{*}}(\\mathbf{X}_{T})\\right\\Vert\\right]+\\mathbb{E}\\left[\\left\\Vert g_{A^{*}}(\\mathbf{X}_{T})-\\mathbb{E}\\left[\\mathbf{X}_{G}|\\mathbf{X}_{T}\\right]\\right\\Vert\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. This equation can be further decomposed into two terms: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\widehat{\\mathbf{X}}_{G}-\\mathbb{E}\\left[\\mathbf{X}_{G}|\\mathbf{X}_{T}\\right]\\right)\\phi(X_{T})\\right]}\\\\ &{~=\\mathbb{E}\\left[(\\widehat{\\mathbf{X}}_{G}-\\mathbf{X}_{G})\\phi(\\mathbf{X}_{T})\\right]+\\mathbb{E}\\left[\\mathbf{X}_{G}-\\mathbb{E}\\left[\\mathbf{X}_{G}|\\mathbf{X}_{T}\\right]\\phi(\\mathbf{X}_{T})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\widehat{\\mathbf{X}}_{G}-\\mathbf{X}_{G}$ represents the variance for generative results and is orthogonal to $\\mathbf{X}_{T}$ , the first term is 0, making it sufficient to consider only the second term. To prove that the second term is also 0, we first prove the orthogonality property, i,e. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\widehat{\\mathbf{X}}_{G}-\\mathbb{E}\\left[\\mathbf{X}_{G}|\\mathbf{X}_{T}\\right]\\right)\\boldsymbol{\\phi}(X_{T})\\right]=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any function $\\phi$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\mathbb{E}\\left[\\mathbb{E}\\left[\\mathbf{X}_{G}|\\mathbf{X}_{T}\\right]\\phi(\\mathbf{X}_{T})\\right]}\\\\ &{=\\displaystyle\\sum_{x_{t}}\\mathbb{E}\\left[\\mathbf{X}_{G}|\\mathbf{X}_{T}=x_{t}\\right]\\phi(x_{t})P(\\mathbf{X}_{T}=x_{t})}\\\\ &{=\\displaystyle\\sum_{x_{g}}\\left[\\sum_{x_{t}}\\frac{P(\\mathbf{X}_{G}=x_{g},\\mathbf{X}_{T}=x_{t})}{P(\\mathbf{X}_{T}=x_{t})}\\right]\\phi(x_{t})P(\\mathbf{X}_{T}=x_{t})}\\\\ &{=\\displaystyle\\sum_{x_{g}}\\sum_{x_{t}}x_{g}\\phi(x_{t})P(\\mathbf{X}_{G}=x_{g},\\mathbf{X}_{T}=x_{t})}\\\\ &{=\\mathbb{E}\\left[\\mathbf{X}_{G}\\phi(\\mathbf{X}_{T})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we have the orthogonal property in Eq. (16). ", "page_idx": 17}, {"type": "text", "text": "Now we continue our proof for the main theorem. First, we can decompose Eq. (15) as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\mathbb{E}\\left[\\left\\Vert\\widehat{\\mathbf{X}}_{G}-\\mathbb{E}\\left[\\mathbf{X}_{G}|\\mathbf{X}_{T}\\right]\\right\\Vert^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\Vert\\widehat{\\mathbf{X}}_{G}-g_{A^{*}}(\\mathbf{X}_{T})+g_{A^{*}}(\\mathbf{X}_{T})-\\mathbb{E}\\left[\\mathbf{X}_{G}|\\mathbf{X}_{T}\\right]\\Vert^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\Vert\\widehat{\\mathbf{X}}_{G}-g_{A^{*}}(\\mathbf{X}_{T})\\Vert^{2}\\right]+\\mathbb{E}\\left[g_{A^{*}}(\\mathbf{X}_{T})-\\mathbb{E}\\left[\\mathbf{X}_{G}|\\mathbf{X}_{T}\\right]\\Vert\\right]}\\\\ &{+\\;2\\mathbb{E}\\left(\\widehat{\\mathbf{X}}_{G}-g_{A^{*}}(\\mathbf{X}_{T})\\right)\\left(g_{A^{*}}(\\mathbf{X}_{T})-\\mathbb{E}\\left[\\mathbf{X}_{G}|\\mathbf{X}_{T}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since the gap between pure generative result guided by the training set $\\mathbf{X}_{T}$ and an ideal text embedding is purely influenced by the training set $\\mathbf{X}_{T}$ , $g_{A^{*}}(\\mathbf{X}_{T})-\\mathbb{E}\\left[\\mathbf{X}_{G}\\vert\\mathbf{X}_{T}\\right]$ can also be regarded as a functional over $\\mathbf{X}_{T}$ , making the last term equal to 0 according to the orthogonal property in Eq. (16). At this step, we obtain the result in Eq. (15). \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B More Details About CATOD. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide more details about our proposed Controllable Adaptor Towards Out-ofDistribution Concepts (CATOD). ", "page_idx": 17}, {"type": "text", "text": "B.1 An Overall Paradigm ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Initially, we have a large data pool $D_{p o o l}$ related to the concept given with different quality and begin with an initial randomly-sampled training dataset $\\mathbf{X}_{T}^{(0)}$ . $\\mathbf{X}_{T}$ might contain distorted images or even mismatch the given concept, which is common in publicly available datasets. Following the typical paradigm of Active Learning, we train all the models we use on the training set $\\mathbf{X}_{T}$ . Then, we select a batch of data $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ from the data pool $D_{p o o l}$ according to the acquisition functions induced from models. Finally, we move this selected batch of data $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ to $\\mathbf{X}_{T}$ , go back to the model training step to re-train or adapt the models, and repeat this cycle until $\\mathbf{X}_{T}$ reaches its maximum capacity or the quality of models cannot be further enhanced by adding new data. An overall Algorithm for a cycle of CATOD is provided in Alg. 1. ", "page_idx": 17}, {"type": "text", "text": "Input: Text-to-image model $g_{A}(\\cdot)$ that can be integrated with adaptor $A$ , real-world data pool $D_{p o o l}$ ,   \ntraining data pool $\\mathbf{X}_{T}$ , budget $b$ , pretrained aesthetic scorer $S_{a e s}(\\cdot)$ , concept-matching scorer $S_{c o n}(\\cdot)$ ,   \nadaptor $A$ , learning rate group $R$ .   \nOutput: Updated training pool $\\mathbf{X}_{T}$ , updated adaptor $A$ .   \n1: Fine-tune the aesthetic scorer $S_{a e s}(\\cdot,\\theta)$ on $\\mathbf{X}_{T}$ following Sec.B.3;   \n2: Calculate the aesthetic score $\\begin{array}{r}{\\gamma_{a e s}(\\dot{A})=\\frac{1}{\\left|g_{A}(\\mathbf{X}_{T})\\right|}\\sum_{x_{g}\\in g_{A}(\\mathbf{X}_{T})}S_{a e s}(x_{g})}\\end{array}$ and concept-matching   \nscore $\\begin{array}{r}{\\gamma_{c o n}(A)=\\frac{1}{|g_{A}(\\mathbf{X}_{T})|}\\sum_{x_{g}\\in g_{A}(\\mathbf{X}_{T})}\\overbar{S}_{c o n}(x_{g})}\\end{array}$ for $A$ ;   \n3: for $x\\in D_{p o o l}$ do   \n4: Calculate the integrated score $S(x)(\\mathrm{Eq.}(7))$ ;   \n5: end for   \n6: $B\\leftarrow\\{\\mathrm{Top}{\\leftarrow}{\\mathrm{b}}$ samples within $D_{p o o l}$ according to $S$ };   \n7: $s\\leftarrow\\gamma(A)$ (Eq.(6));   \n8: $r\\gets$ the largest value within $R$ ;   \n9: while $r\\neq\\operatorname*{min}(R)$ do   \n10: $\\begin{array}{r}{A^{\\prime}\\gets A-r\\nabla_{A}\\sum_{x\\in\\mathbf{X}_{T}\\cup B}L_{L D M}(x,A);}\\end{array}$ .   \n11: Generate a small-scale $\\mathbf{X}_{G}$ with $g(\\cdot)$ conditioned on $A^{\\prime}$ ;   \n12: Calculate score $\\gamma(A)$ for $A$ via Eq.(6)) and $\\mathbf{X}_{G}$ ;   \n13: if $\\gamma(A)>s$ then   \n14: $A\\leftarrow A^{\\prime}$ ;   \n15: $s\\leftarrow\\gamma(A)$ ;   \n16: else   \n17: $r\\leftarrow\\mathbf{a}$ smaller value than $r$ within $R$   \n18: end if   \n19: end while   \n20: $\\mathbf{X}_{T}\\leftarrow\\mathbf{X}_{T}\\cup B$ ", "page_idx": 18}, {"type": "text", "text": "We use a weighted-scoring system as the acquisition module that evaluates the quality of images and adaptors in our proposed CATOD framework. In traditional active learning or data selection, the acquisition module is only conducted on samples. However, we observed that an increase in the number of samples does not necessarily contribute to a performance boost like in a traditional active learning setting (which we will verify in Appendix C). Therefore, we propose this weighted scoring system to help schedule adaptor training, which helps locate the best version among them and select samples according to their quality. ", "page_idx": 18}, {"type": "text", "text": "Our Controllable Adaptor Towards Out-of-Distribution Concepts framework can be divided into 4 steps: (1) Train the scoring system on the current training set $\\mathbf{X}_{T}$ . (2) Train the adaptor on the training set $\\mathbf{X}_{T}$ and schedule the training accordingly to the scoring system; (3) Calculate the acquisition score accordingly to the scoring system and the best adaptor for samples in dataset $D_{p o o l}$ ; (4) Select top- $\\mathbf{\\nabla}\\cdot\\mathbf{K}$ samples from $D_{p o o l}$ and move them to train set $\\mathbf{X}_{T}$ . ", "page_idx": 18}, {"type": "text", "text": "B.2 An Implementation of MMD score. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To give out an quantifiable and unbiased estimation of Eq.(2), we sample two sets of vectors $X\\bar{=}\\left\\{x_{1},x_{2},\\ldots,x_{m}\\right\\}$ from $P$ and $G=\\{g_{1},g_{2},\\ldots,g_{n}\\}$ from $Q$ , an estimation can be give by: d $\\operatorname{list}_{M M D}^{2}(X,G):={\\frac{1}{m(m-1)}}\\sum_{i=1}^{m}\\sum_{j\\neq i}^{m}k(x_{i},x_{j})+{\\frac{1}{n(n-1)}}\\sum_{i=1}^{n}\\sum_{j\\neq i}^{n}k(g_{i},g_{j})-{\\frac{2}{m n}}\\sum_{i=1}^{m}\\sum_{j=1}^{n}k(x_{i},g_{j})\\,.$ (17) ", "page_idx": 18}, {"type": "text", "text": "B.3 Aesthetic Scorer ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Following the description of the aesthetic scorer in Section 3.2, we explain how to personalize it here. Since our primary goal is to make the model distinguish low-quality/high-quality examples for OOD concepts, we design an automatic procedure to assign aesthetic scores to our training set $\\mathbf{X}_{T}$ at each cycle, based on which we attain an aesthetic training set ${\\bf X}_{T}^{a e s}$ . In this work, the aesthetics training comprises three parts: (1) The original training set $\\mathbf{X}_{T}$ ; (2) Generated set $D_{T G}$ without assistance from any adaptors; (3) Randomly sampled set $D_{S I}$ from similar but irrelevant categories. Since we focus on the concepts that the text-to-image model can hardly recognize and visualize, $D_{T G}$ contains lots of plausible, disrupted, or even irrelevant samples, which we assign a zero score. This helps the aesthetic model understand that originally generated samples are low-quality samples. In contrast, $D_{S I}$ somehow describes some attributes for the concept but still does not match the concept, to which we assign the average score of 5.0. We employ the normal regression loss formulated as follows: ", "page_idx": 18}, {"type": "image", "img_path": "65htepluYE/tmp/1c3a605d2f0e0c70e559a0b3aac71ce75bfec32fb7202333baf0ee9c6aca4a01.jpg", "img_caption": ["Figure 6: An intuitive comparison for fixed/dynamic schedules. The active learning paradigm can be viewed as guiding the iterative embedding updating through newly added samples. We can see that a fixed schedule makes the learned embedding heavily biased, which in turn leads to performance fluctuation. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a e s}=\\frac{1}{|\\mathbf{X}_{T}|}\\sum_{x_{t}\\in\\mathbf{X}_{T}}\\|p_{t},\\hat{p}_{t}\\|+\\frac{\\lambda}{|D_{T G}|+|D_{S I}|}\\sum_{x_{t}^{\\prime}\\in D_{T G}\\cup D_{S I}}\\|p_{t}^{\\prime}-\\hat{p}_{t}^{\\prime}\\|,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $p$ denotes score predicted by the original scorer, when $\\hat{p}$ denotes the score we assign, and $\\lambda$ is set to 1.0 in our experiments. Following this paradigm, we train an aesthetic scorer for each category of concepts, and the score personalized for each category from $D_{p o o l}$ can be predicted accordingly. ", "page_idx": 19}, {"type": "text", "text": "After selecting top-K data according to this score based on adaptor quality, we assign a lowered aesthetic score to them: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{p}_{x}=10.0-\\frac{t_{c u r r e n t}}{t o t a l}(10.0-p_{x}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $p_{x}$ denotes the score given by a generic aesthetic scorer, $t o t a l$ represents the number of total active learning cycles, when $t_{c u r r e n t}$ indicates the current cycle. Note that the newly selected images typically have lower quality than those from earlier cycles, we assign progressively lowered scores to them as the learning cycle proceeds. ", "page_idx": 19}, {"type": "text", "text": "B.4 Adaptor Evaluation and Schedules ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Following the content in the second part \u201cThe Active Schedule for Training Adaptors\u201d in Sec. 3.3, we describe our schedule in more detail. In training CATOD, we use this indicator as a signaling proxy throughout the training process. As the number of training samples increases, the quality of selected samples at later cycles might be lower, and a fixed schedule may introduce some unnecessary details within these images (such as watermarks, borders, disturbing objects, etc.). However, these subsequent samples with relatively lower quality do contain some good photographic attributes that can help the embedding evolve. Actually, the potential problems can be alleviated by carefully-designed schedules. An intuitive illustration for this schedule is given in Fig. 6. ", "page_idx": 19}, {"type": "text", "text": "We first train the adaptor for 5 epochs, and save an adaptor version $A_{i}$ per 5 epochs, together with its corresponding generated images, constituting a sub-cycle. When this sub-cycle is completed, we filter out the adaptor with the highest quality based on the score $\\gamma(A)$ (Eq.(6)). If the selected adaptor has the most training epochs across all versions, we keep the learning rate and conduct the next sub-cycle, otherwise, we will reduce the learning rate. Note that if the adaptor quality of this cycle is not as good as that of the previous cycle (because of the quality degradation caused by introducing some unnecessary details), the initial learning rate will be reduced, which helps avoid introducing disturbing elements led by some samples with insufficient quality. The sub-cycles will be continuously conducted until the minimum learning rate is reached or the adaptor quality no longer increases. Our learning rates include $5\\times10^{-4}$ , $2.5\\times10^{-4}$ , $7.5\\times10^{-5}$ , $5\\times\\dot{1}0^{-5}$ , $2.5\\stackrel{.}{\\times}10^{-5}$ . ", "page_idx": 19}, {"type": "table", "img_path": "65htepluYE/tmp/28dcad847301ccda8da7ba9f5a995e548df9f6d17558bba523a72f632b165c9d.jpg", "table_caption": ["Table 5: The statistics of the dataset we use. The test data size ${\\#\\mathcal{D}_{v a l}}$ of all the concepts is fixed to 100 for a fair comparison, while the training data size $\\#\\mathcal{D}_{p o o l}$ varies since different concepts since the number of data samples across publicly available datasets is different. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "65htepluYE/tmp/1d1c8579092a90bcd81fd6ccabe2e8ae5d15810a52333b8bbd0dba21b74d78d7.jpg", "table_caption": ["Table 6: The statistics of the dataset we use. The test data size ${\\#\\mathcal{D}_{v a l}}$ of all the concepts is fixed to 100 for a fair comparison, while the training data size $\\#\\mathcal{D}_{p o o l}$ varies since different concepts since the number of data samples across publicly available datasets is different. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "C Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 How to Automatically Locate OOD Concepts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Since the publicly available dataset, i.e., ImageNet, iNaturalist 2018 [67], IP102 [71], which we adopt in our research contains 9,244 concepts with 14,883,455 images in total, it is infeasible to manually test and decide what concept is the out-of-distribution concept one by one. Therefore, our procedure for figuring out is two-fold: (1) automatically quantifying the distribution drift of the given concept from what the text-to-image model (Stable Diffusion 2.0, a.k.a SD 2.0) learned with the CMMD metric (thanks to the recent work proposed by Google [27]); (2) Manually verifying whether the text-to-image model could generate the right content, according to the rank given by the CMMD metric above. Specifically, in the first step, we randomly sample 10 image data for each concept within the datasets and then generate 10 images with the model with the text prompt \u201cA photo of $S^{*,}$ , in which $S^{*}$ denotes the concept name. Then, we calculate the CMMD discrepancy between these two data batches and record them. The CMMD scores of some concepts (including both Out-of-Distribution concepts in the first 5 rows and In-Distribution in the last 5 rows) are listed in Table 5. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Empirically, we observed that $\\mathrm{SD}\\ 2.0$ is unable to synthesize the concepts with a CMMD metric above 3.5. Therefore, we pick 25 OOD concepts accordingly divide them into 5 categories, and make our dataset as follows: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Insect: Zopherinae, Antlion, Lycorma Delicatula, Parasitic Wasps, Xylotrechus;   \n\u2022 Lizard: Axolotl, Frilled Lizard, Mediterranean House Gecko, Oedura, Opluridae;   \n\u2022 Penguin: Emperor Penguin Chick, Gentoo Penguin, King Penguin Chick, Rock Hopper Penguin, Royal Penguin;   \n\u2022 Sea Fish: Crampfish, Dragonfish, Garfish, Tigerfish, Tuna;   \n\u2022 Snake: Ahaetulla Nasutar, Aipysurus Laevis, Indian Cobra, Pelamis Platurus, Sidewinder. ", "page_idx": 21}, {"type": "text", "text": "Actually, the dataset we collect is not class-balanced, so the number of samples we can collect varies across different concepts. The statistics of each concept are shown in Table 6. For the concepts with a $\\mathcal{D}_{p o o l}$ less than 1,000 images, we also collect some of them from image host websites including Dreamstime, Flicker, istockphoto, pinterest, and shutterstock. The dataset will be released as soon as the code is released. ", "page_idx": 21}, {"type": "text", "text": "C.2 Experimental Settings ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To verify the effectiveness and extensibility of our proposed CATOD framework, we conduct experiments on 25 different OOD concepts that the large-scale text-to-image (i.e. Stable Diffusion 2.0 [45]) implement different versions of CATOD. This section explains some important implementation details for the dataset, the aesthetic scorer, the concept-matching scorer we use, and how we design the learning paradigm. ", "page_idx": 21}, {"type": "text", "text": "The Learning Rate Schedule. As shown in Algorithm 1, we have a learning rate group R, which includes 5 learning rates: $5\\times10^{-4}$ , $2.5\\times10^{-4}$ , $\\bar{7}.5\\times10^{-5}$ , $5\\times10^{-5}$ , $2.5\\stackrel{.}{\\times}10^{-5}$ . Following the indicator $\\gamma(A)$ in Eq. (6), we calculate this value after every epoch. When this indicator falls below the previous evaluation, we reduce the learning rate. If the learning rate cannot be lowered further, we conclude that the model has converged and the training is complete. Note that we choose to train 20 epochs in all experiments as claimed in the main context, it is possible that the training is early stopped before reaching epoch 20. ", "page_idx": 21}, {"type": "text", "text": "Aesthetic Scorers. To offer the basic knowledge for aesthetic evaluation, we use pre-trained generic models, including ReLIC [81], TANet [23], SD-Chad Scorer, and VLAD [59]. Note that the scores given by the generic aesthetic scoring model tend to lie in the majority score range (5 to 6) in our dataset, we ought to personalize this model accordingly to our training set, which we refer to as PIAA [84]. Since PIAA is a typical small sample problem, we adopt similar experimental settings and evaluation criteria by referring to Few-Shot Learning [16] and a previous PIAA research work: PA-IAA [33]. ", "page_idx": 21}, {"type": "text", "text": "Concept-Matching Scorers. Since the first step of general image retrieval is feature extraction, we use the CLIP ViT-L/14 encoder, which is a commonly used and reliable feature extractor [2, 82, 34]. ", "page_idx": 21}, {"type": "text", "text": "C.3 About Compositional Results over Multiple-Concepts. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To further validate the effectiveness of CATOD, we also present compositional results based on the LoRA adaptors we obtained, as illustrated in Figure 7. We compose different types of concepts, including (i) background concepts such as brick wall and grassland for Emperor Penguin Chicks, and desk and beach for Frilled Lizard; (ii) in-distribution and common concepts, like the dog; (iii) in-distribution and similar concepts, like Adelie penguin for Emperor Penguin Chicks, and Alligator Lizard for Frilled Lizard; (iii) out-of-distribution concepts from other adaptors, like King Penguin Chicks for Emperor Penguin Chicks and Opluridae for Frilled Lizard. Our observations reveal that the synthetic results on out-of-distribution concepts with CATOD can seamlessly integrate different background elements, even if they were not present in real-world images (columns 1 to 2). For other in-distribution concepts like dogs, LDMs tend to represent a specific species with similar color and texture, while different concepts remain highly distinguishable within one image (columns 3 to 4). For out-of-distribution concepts, we also note that the creatures are depicted correctly, but may exhibit some confused visual details that negatively impact the aesthetics of the image (column 5). ", "page_idx": 21}, {"type": "image", "img_path": "65htepluYE/tmp/1603063ca6f4365bc4ee6d45cdef4fccb03aa84fc464d94d06f7f5b7f39d0c4d.jpg", "img_caption": ["Figure 7: Generative results with 2 concepts within one image. Experiments are conducted based on the LoRA adaptor fully trained on concepts \u201cFrilled Lizard\u201d and \u201cEmperor Penguin Chick\u201d. We try to compose these creatures with background elements, in-distribution concepts, and out-of-distribution concepts learned by other adaptors. The final results show high quality with minimal disruptive details. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "C.4 More Analysis. ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "65htepluYE/tmp/c2774e6840c924467f24cad0b85df3d9a8ba5baa41319acb764d847d03a45614.jpg", "img_caption": ["Figure 8: A comparison on how the schedule and scores change on RAND(scheduled) and CATOD as cycle proceeds on concept emperor penguin(chick). (a),(b) show how the #epochs for each learning rate in the schedule change as the cycle proceeds, when (c),(d) show how aesthetic/concept-matching/comprehensive score change on RAND (scheduled) and CATOD. The scores for CATOD stop changing at cycle 12 since more added samples do not help boost adaptor quality. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "CATOD is more reliable on schedules. Figure 8(a),(b) show a comparison of how our schedules adjust as the number of cycles increases on RAND(scheduled) and CATOD. Both of them show the tendency that training epochs with larger learning rates decrease when those with small learning rates increase, meaning that the training schedule focuses more on fine-tuning at later cycles. Comparing CATOD with RAND, we can see that the epochs with the maximal learning rate already diminish at cycle 4 on CATOD while RAND still needs large learning rates, indicating that CATOD are better at recognizing the given concept and tends to be more stable. ", "page_idx": 22}, {"type": "image", "img_path": "65htepluYE/tmp/f9017c6bd7248a51199636666c6ebdfa6ef83d1e9960452474dfb4c759a7bdf3.jpg", "img_caption": ["(a) R-precision $(\\%)$ with different strategies as training pool expands. The experiments are performed over concept \"emperor penguin(chick)\" and produced over RAND and CATOD. To further show the impact of the number of samples, we also compare their modified versions that do not use dynamic schedules. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "65htepluYE/tmp/6acd79497bb9d48f488100f528fff54675bfff917a3cdcc707243b76c41a065c.jpg", "img_caption": ["(b) A comparison on real-images with CATODgenerated ones in 3 different shapes of lycorma delicatula. We can see that with a carefully designed selection, the adaptor helps produce all the shapes of the concept \"lycorma delicatula\". "], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "65htepluYE/tmp/b4418b939af6ebe810278c6e84b886a1a5e15dcb8acd7bd66621123046322992.jpg", "table_caption": ["Table 7: A Comparison over the performance of CATOD on different types of the initial training data pool, in terms of the CLIP score and CMMD score with 100 images sampled at last. This table shows the average result of 5 sub-classes within each category. The overall improvement of our proposed CATOD is provided by \u201cImp.\u201d. Methods with the best performance are bold-folded. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "CATOD achieves a good score earlier than other methods. Figure 8(c),(d) shows a comparison of how the aesthetic/concept-matching/comprehensive scores change as the cycle proceeds on RAND and CATOD. We can see that CATOD already achieves a higher value on all scores and cannot be further boosted with 120 training samples on cycle 12 when the performance for RAND still fluctuates. This phenomenon illustrates that our active selection strategy makes our embedding be trained on more high-quality data. ", "page_idx": 23}, {"type": "text", "text": "Furthermore, we also notice that aesthetic/concept-matching scores fluctuate at later cycles when the comprehensive score is enhanced continuously, which exactly corresponds to our proposed dual scoring system that balances the importance of these two factors when ensuring the quality of the adaptor never declines. ", "page_idx": 23}, {"type": "text", "text": "The performance of CATOD on different initial training data pool. We also ablate CATOD over the initial training set size/quality. In detail, we retest CATOD with different numbers of initial samples (10,20,50) and different quality, i.e., high-quality (HQ) and random sampling (RAND), with 100 images sampled at last and 10 images selected per cycle. These experiments are tested with adaptor LoRA. The results are shown in Table 7: ", "page_idx": 23}, {"type": "table", "img_path": "65htepluYE/tmp/d815d0411a3027e1542824d28879a9fed3ec04fdce43d55608009f7249e44f0b.jpg", "table_caption": ["Table 8: A Comparison over the performance of CATOD on different architectures, in terms of the CLIP score and CMMD score with 100 images sampled at last. This table shows the average result of 5 sub-classes within each category. The overall improvement of our proposed CATOD is provided by \u201cImp.\u201d. Methods with the best performance are bold-folded. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "65htepluYE/tmp/6b902c5243b9f3ec8b60282088c51fb4a9ba66a2635461cffc9ffff8fc1a3d58.jpg", "table_caption": ["Table 9: A Comparison over the diversity score of CATOD, in terms of the CLIP score and CMMD score with 100 images sampled at last. This table shows the average result of 5 sub-classes within each category. The overall improvement of our proposed CATOD is provided by \u201cImp.\u201d. Methods with the best performance are bold-folded. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "From these results, we can draw the following conclusions: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Initial batch size has a more significant impact on randomly initialized samples than on high-quality samples. Specifically, the performance change with high-quality samples is at most 1.40 in the CLIP score and 0.10 in the CMMD score concerning different initial numbers of training data. In contrast, the performance change on low-quality is up to 7.65 in the CLIP score and 0.45 in the CMMD score. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The quality of initial samples does have an impact on generative results since we can see a consistent performance loss when changing HQ initial samples to randomly initialized samples. With the initial size increase, this impact tends to be even more significant. ", "page_idx": 24}, {"type": "text", "text": "C.5 Further Extension: Concept with multiple shapes ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Recall that Stable Diffusion 2.0 fails to generate emperor penguin chicks, which accounts for the fact that adult emperor penguin chicks and their chicks have different appearances. We raise another question: can we train an adaptor with a concept that has different shapes? Inspired by this, we also test our proposed CATOD on the concept \u201clycorma delicatula\u201d, which is a kind of pest with 3 different forms in its life-cycle, and surprisingly find that with 300 training samples, the best version of adaptors successfully produces all these 3 shapes, as shown in Figure 9(b). These results further validate the effectiveness of our framework, even in the presence of appearance ambiguity. ", "page_idx": 24}, {"type": "text", "text": "C.6 About Experimental Results on other Architectures ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our experiments in the main content are all conducted on Stable Diffusion 2.0 (SD 2.0). To further validate the superiority of our proposed CATOD, we conduct additional experiments on other architectures (based on LoRA), including Stable Diffusion 1.5 (SD 1.5) and Stable Diffusion XL (SDXL). The results are shown in Table 9. ", "page_idx": 24}, {"type": "text", "text": "This table shows that our proposed CATOD is also compatible with different architectures with notable performance gain compared to the baselines. ", "page_idx": 24}, {"type": "image", "img_path": "65htepluYE/tmp/172dc2889d643c8f1f7b1dac16788ab9bdb944aa982ec383f24c6f07fdde22a2.jpg", "img_caption": ["Figure 9: A comparison of selected and generate samples on different combinations of methods and concepts. We can observe that training samples with different angles selected by CATOD also lead to diverse angle in their generative results. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "C.7 About the diversity ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "CATOD maintains the diversity to produce OOD concepts with different angles or poses in generative results. To validate the diversity of our generative results, we first provide a quantitative evaluation with LPIPS [80, 78]. The results are shown in Table 9. In this table, we can see that CATOD gives out a diversity improvement up to 0.17 in the LPIPS score compared to CLIP-based sampling, and outperforms CLIP over most categories. From this, we conclude that CATOD also preserves diversity. Since the training set contains samples with different angles in CATOD, it is also easy to produce objects with different angles, and we show the examples on concepts \u201cAxolotl\u201d and \u201cEmperor Penguin Chick\u201d in Figure 9. We can observe that training samples with different angles selected by CATOD also lead to diverse angle in their generative results. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have clearly summarized our contribution in the abstract and in the last paragraph of the introduction. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: We believe that the limitation should not be regarded as an integral part of this paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have provided theoretical proof of our theory in Sec.4 in Appendix. A. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have provided our code within the supplementary material. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not provide the dataset regarding the risk of anonymity, despite that the dataset is just a collection of publicly available datasets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All experimental details are provided in Appendix B. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our experiments do not have error bars. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is reported in the Appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Our paper has no risk regarding ethics. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: This has been briefly discussed in our conclusion section. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: There is no such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: MIT License has been added already. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have a readme document for our codes. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: No crowdsourcing experiments were conducted. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: No crowdsourcing experiments were conducted. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]