[{"figure_path": "0HRRNEAQFp/tables/tables_3_1.jpg", "caption": "Table 1: Overview of the datasets and training/evaluation statistics for the properties investigated. For each property, we list the image dataset used, and the number of images for the train, val, and test set. 1000 images are used for testing if the original test set is larger than 1000 images. Regions are selected in each image, and pairs of regions are used for all the probe questions.", "description": "This table presents a summary of the datasets used for evaluating different 3D scene properties. For each property (Same Plane, Perpendicular Plane, Material, Support Relation, Shadow, Occlusion, Depth), it lists the dataset used, the number of images in the training, validation, and test sets, the number of regions selected in each image, and the number of region pairs used for training, validation, and testing.  It shows the data split and size for training the linear classifiers used to evaluate the ability of different large vision models to predict the different properties.", "section": "3 Method - Properties, Datasets, and Classifiers"}, {"figure_path": "0HRRNEAQFp/tables/tables_7_1.jpg", "caption": "Table 2: SVM grid search results of Stable Diffusion features. For each property, we train the linear SVM on the training set and grid search the best combination of time step, layer, and C on the validation set. The ROC AUC score (%) is reported on the validation set using the selected combination.", "description": "This table presents the hyperparameters and performance of a linear Support Vector Machine (SVM) trained on Stable Diffusion features for seven different 3D scene properties.  For each property, the optimal time step, layer, and regularization parameter (C) were determined through grid search on a validation set.  The table shows the resulting validation AUC (Area Under the ROC Curve) scores, indicating the model's performance in classifying the property.", "section": "4.2 Results for Stable Diffusion"}, {"figure_path": "0HRRNEAQFp/tables/tables_8_1.jpg", "caption": "Table 3: SVM grid search results of CLIP/DINO/VQGAN features. We train the linear SVM on the training set, and grid search the best combination of ViT/Transformer layer and C on the validation set. The OpenCLIP and VQGAN models we use have 48 transformer layers, DINOv1 has 12 layers and DINOv2 has 40 layers. The i-th layer means the i-th transformer layer from the input side.", "description": "This table presents the results of a grid search performed to find the optimal hyperparameters for different pre-trained vision models (CLIP, DINOv1, DINOv2, VQGAN) when used for a linear probing task.  The grid search involved selecting the best transformer layer and regularization parameter (C) for an SVM classifier to predict different 3D scene properties.  The table shows the optimal layer and C value found, along with the resulting validation AUC score for each property and model.", "section": "4.3 Results for CLIP/DINO/VQGAN Features"}, {"figure_path": "0HRRNEAQFp/tables/tables_9_1.jpg", "caption": "Table 4: Comparison of different features trained at scale. For each property, we use the best time step, layer and C found in the grid search for Stable Diffusion, and the best layer and C found in the grid search for other features. The performance is the ROC AUC on the test set, and 'Random' means a random classifier.", "description": "This table compares the performance of different large-scale vision models (OpenCLIP, DINOv1, DINOv2, VQGAN, and Stable Diffusion) on various downstream tasks related to 3D physical scene understanding.  For each property (Same Plane, Perpendicular Plane, etc.), the best hyperparameters from a grid search are used for each model. The results are presented as the area under the ROC curve (AUC) on a test set, comparing each model against a random classifier.", "section": "4.4 Comparison of Different Features Trained at Scale"}, {"figure_path": "0HRRNEAQFp/tables/tables_19_1.jpg", "caption": "Table 5: Train/Val AUC of SVM grid search for Stable Diffusion features. For each property, the Train/Val AUC at the best combination of time step, layer and C is reported.", "description": "This table presents the training and validation AUC scores achieved by a Support Vector Machine (SVM) using features extracted from the Stable Diffusion model.  The best performing time step, layer, and regularization parameter (C) were determined through grid search for each of the seven properties investigated (Same Plane, Perpendicular Plane, Material, Support Relation, Shadow, Occlusion, and Depth).  The AUC scores indicate the model's ability to discriminate between positive and negative examples for each property using the Stable Diffusion features.", "section": "4.2 Results for Stable Diffusion"}, {"figure_path": "0HRRNEAQFp/tables/tables_24_1.jpg", "caption": "Table 6: Preliminary results of using the probed feature for the downstream task of normal estimation. Here we show the results of injecting the selected Stable Diffusion feature into iDisc [29]. Please see text for more details.", "description": "This table shows the results of using features from Stable Diffusion, selected by the authors' probing method, for the task of surface normal estimation. The features were injected into the iDisc model [29], and the results are compared to the original iDisc model's performance. The metrics used for comparison include Mean Angular Error, Angular RMSE, and percentages of angles less than 11.25, 22.5, and 30 degrees.", "section": "E Preliminary Results of Applying Probed Feature for Downstream Tasks"}, {"figure_path": "0HRRNEAQFp/tables/tables_24_2.jpg", "caption": "Table 7: Preliminary results of using the probed feature for downstream task of depth estimation. Here we show the results of a comparison between ResNet and SD features on the NYUv2 Depth test dataset. Please see text for more details.", "description": "This table shows the comparison of depth estimation performance using features from ResNet-50 and Stable Diffusion (SD).  The performance is measured by three metrics (\u03b4\u2081, \u03b4\u2082, \u03b4\u2083) representing the percentage of pixels with relative depth errors less than 11.25%, 22.5%, and 30%, respectively, and RMSE (Root Mean Squared Error) representing the overall accuracy. SD features significantly outperform ResNet-50 features across all metrics, indicating the potential of using probed SD features for depth estimation.", "section": "E Preliminary Results of Applying Probed Feature for Downstream Tasks"}]