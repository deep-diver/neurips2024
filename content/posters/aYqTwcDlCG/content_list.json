[{"type": "text", "text": "Learning World Models for Unconstrained Goal Navigation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuanlin Duan Wensen Mao He Zhu Rutgers University Rutgers University Rutgers University yuanlin.duan@rutgers.edu wm300@cs.rutgers.edu hz375@cs.rutgers.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning world models offers a promising avenue for goal-conditioned reinforcement learning with sparse rewards. By allowing agents to plan actions or exploratory goals without direct interaction with the environment, world models enhance exploration efficiency. The quality of a world model hinges on the richness of data stored in the agent\u2019s replay buffer, with expectations of reasonable generalization across the state space surrounding recorded trajectories. However, challenges arise in generalizing learned world models to state transitions backward along recorded trajectories or between states across different trajectories, hindering their ability to accurately model real-world dynamics. To address these challenges, we introduce a novel goal-directed exploration algorithm, MUN (short for \"World Models for Unconstrained Goal Navigation\"). This algorithm is capable of modeling state transitions between arbitrary subgoal states in the replay buffer, thereby facilitating the learning of policies to navigate between any \"key\" states. Experimental results demonstrate that MUN strengthens the reliability of world models and significantly improves the policy\u2019s capacity to generalize across new goal settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Goal-conditioned reinforcement learning (GCRL) has emerged as a powerful framework for learning diverse skills within an environment and subsequently solving tasks based on user-specified goal commands, without requiring further training( Mendonca et al. (2021); Andrychowicz et al. (2017)). Given that specifying dense task rewards for GCRL requires domain expertise, access to object positions, is time-consuming, and is prone to human errors, rewards in GCRL are typically sparse, signaling success only upon reaching goal states. However, sparse rewards pose a challenge for exploration during training. To address this challenge, several previous methods, e.g., Hafner et al. (2019a); Hansen et al. (2023); Mendonca et al. (2021) have proposed learning a generative world model of the environment using a reconstruction (decoder) objective, an instantiation of Model-based Reinforcement Learning (MBRL), visualized in Fig. 1. This approach is appealing because the world model can provide a rich learning signal( Yu et al. (2020); Georgiev et al. (2024)). For example, world models allow agents to plan their actions or exploratory goals without directly interacting with the real environment for more efficient exploration( Hu et al. (2023); Sekar et al. (2020)). ", "page_idx": 0}, {"type": "text", "text": "Existing MBRL techniques train world models to capture the dynamics of the environment from the agent\u2019s past experiences stored in a replay buffer. The richness of the data stored in the agent\u2019s replay buffer directly impacts the quality of a World Model. It is expected that the world model generalizes reasonably well to the state space surrounding the trajectories recorded in the replay buffer. However, the world model may not generalize well to state transitions backward along recorded trajectories or to states across different trajectories, which impedes the world model\u2019s learning of the real-world dynamics. ", "page_idx": 0}, {"type": "text", "text": "To induce a data-rich replay buffer covering a wide range of dynamic transitions, in this paper, we present a novel goal-directed exploration algorithm for effective world modeling and policy learning, MUN (short for \"World Models for Unconstrained Goal Navigation\"). MUN facilitates modeling state transitions between any subgoal states in the replay buffer, whether tracing back along recorded trajectories or transitioning between ", "page_idx": 1}, {"type": "image", "img_path": "aYqTwcDlCG/tmp/f3ebc185d050e8f36cf996161cff8f54e70602e94d07ca411a3d2ff90b9df2d0.jpg", "img_caption": ["Figure 1: The general framework of model-based RL. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "states on separate trajectories. This enhances the reliability of the learned world model and significantly improves the generalizability of the policy derived from the model to real-world environments, thereby boosting the exploration capabilities of the method. Additionally, we introduce a simple and practical strategy for discovering key subgoal states from the replay buffer. The key subgoals precisely mark the milestones necessary for task completion, such as steps like grasping and releasing blocks in the context of block-stacking scenarios. By world modeling and policy learning for unconstrained navigation between these key states, MUN can generalize to new goal settings, such as block unstacking that was not given to the agent at training time. ", "page_idx": 1}, {"type": "text", "text": "Our key contributions are as follows. First, we propose a novel goal-directed exploration algorithm MUN for effective world modeling of state transition between arbitrary subgoal states in replay buffers. As the quality of the world model improves, MUN becomes highly effective at learning goal-conditioned policies that excel at exploration in sparse-reward environments. Second, we present a practical strategy for identifying pivotal subgoal states, which serve as milestones in completing sophisticated tasks. By training world models for unconstrained transition between these milestones, our method enables learning policies that can adapt to novel goal scenarios. Finally, we evaluate MUN in challenging robotics environments, such as guiding a multi-legged ant robot through a maze, maneuvering a robot arm amidst cluttered tabletop objects, and rotating items in the grasp of an anthropomorphic robotic hand. Across these environments, MUN exhibits superior efficiency in training generalizable goal-conditioned policies compared to baseline methods and ablations. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Setup and Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the problem of goal-conditioned reinforcement learning (GCRL) under a Markov Decision Process (MDP) parameterized by $\\left(S,A,P,G,\\eta,R,\\rho_{0}\\right)$ . $S$ and $A$ are the state and action spaces, respectively. The probability distribution of the initial states is given by $\\rho_{0}(s)$ , and $P(s^{\\prime}|s,a)$ is the transition probability. $\\eta:S\\rightarrow G$ is a mapping from the state space to the goal space, which assumes that every state $s$ can be mapped to a corresponding achieved goal $g$ . The reward function $R$ is defined as $R(s,a,s^{\\prime},g)=1\\{\\eta(s^{\\prime})=g\\}$ . We assume that each episode has a fixed horizon $T$ . For ease of presentation, we further assume $S=G$ and $\\eta$ is an identify function in this paper. ", "page_idx": 1}, {"type": "text", "text": "A goal-conditioned policy is a probability distribution $\\pi:S\\times G\\times A\\to\\mathbb{R}^{+}$ , which gives rise to trajectory samples of the form $\\tau=\\{s_{0},a_{0},g,s_{1},\\ldots,s_{T}\\}$ . The purpose of the policy $\\pi$ is to learn how to reach the goals drawn from the goal distribution $p_{g}$ . With a discount factor $\\gamma\\in\\left(0,1\\right)$ , it $\\begin{array}{r}{\\boldsymbol{J}(\\pi)=\\mathbb{E}_{g\\sim p_{g},\\tau\\sim\\pi(g)}\\left[\\sum_{t=0}^{T-1}\\gamma^{t}\\cdot R(s_{t},a_{t},s_{t+1},g)\\right]\\!.}\\end{array}$ ", "page_idx": 1}, {"type": "text", "text": "In the context of model-based reinforcement learning (MBRL), a world model $\\hat{M}$ is trained over trajectories sampled from the agent\u2019s interactions with the real environment, which are stored in a replay buffer, to predict the dynamics of the real environment. Fig. 1 illustrates the general MBRL framework. We use the world model structure $\\hat{M}$ of Dreamer (Hafner et al. (2019a,b, 2020, 2023)) to learn real environment dynamics as a recurrent state-space model (RSSM). We provide a detailed explanation of the network architecture and working principles of the RSSM in Appendix A.1. Our study focuses on tackling the world model learning problem in goal-conditioned model based reinforcement learning settings. Particularly, we consider GC-Dreamer (goal-conditioned Dreamer) as an important baseline with the following learning components: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l}{{\\cal N}\\mathrm{orld~Model}!}&{{}\\hat{M}(s_{t}|s_{t-1},a_{t-1})}&{}&{{\\mathrm{Actor}}!}&{{}\\pi^{G}(a_{t}|s_{t},g)}&{}&{{\\mathrm{Critic}}!}&{{}V(s_{t},g)}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "image", "img_path": "aYqTwcDlCG/tmp/e6745b09fd350e4ae75994efdd4620e0c0735a269e6826cddd2ae817a6ef0bf3.jpg", "img_caption": ["(a) Key subgoal states in a 3-Block Stacking task. "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "aYqTwcDlCG/tmp/9df0a22cfb89c5969ac8b86623cc814d66a8966155e60ac3db4216aa54d22808.jpg", "img_caption": ["Figure 2: In Fig. 2(a), we illustrate the key states involved in completing the task of 3-block stacking. In Fig. 2(b), we demonstrate the significant advantages of the bidirectional replay buffer used in MUN over traditional methods in learning world models. ", "(b) Bidirectional Replay Buffer "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "In GC-Dreamer, the goal-conditioned agent $\\pi^{G}(a|s,g)$ samples goal commands $g\\,\\in\\,G$ from the given environment goal distribution $p_{g}$ to collect trajectories in the real world. These trajectories are used to train the world model $\\hat{M}$ , and subsequently, $\\pi^{G}$ is trained on imagined rollouts generated by $\\hat{M}$ using the model-based actor-critic algorithm in Dreamer Hafner et al. (2020), with these two steps run in alternation. The critic estimates the sum of future rewards $\\textstyle\\sum_{t}r_{t}^{G}$ , and the actor tries to maximize the predicted values from the critic. The goal-reaching reward $r^{G}$ is defined by the self-supervised temporal distance network $D_{t}$ (Mendonca et al. (2021)), i.e. $r^{G}(s,g)=-D_{t}(s,g)$ . $D_{t}$ predicts the anticipated number of action steps needed to transition from $s$ to $g$ . Essentially, $\\pi^{G}$ is reinforced to minimize the action steps required to transition from the current state $s$ to a sampled goal state $g$ . The temporal distance estimator $D_{t}$ is trained by extracting pairs of states $s_{t}$ and $s_{t+k}$ from an imagined rollout generated by running the policy over the world model and predicting the distance $k$ between them as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nD_{t}\\big(\\Psi(s_{t}),\\Psi(s_{t+k})\\big)\\approx k/H\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\Psi$ represents the preprocessing for imagined states, such as transforming them into the world model\u2019s latent space (we assume $S=G$ in the paper). $H$ represents the total length of the imagined rollout. Further details on the training procedure of $D_{t}$ can be found in Appendix A.2. ", "page_idx": 2}, {"type": "text", "text": "3 Training World Models for Unconstrained Goal Navigation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce MUN, our main approach to addressing the core challenge in GCRL: efficient exploration in long-horizon, sparse-reward environments. Our approach focuses on enhancing the agent\u2019s understanding of the real-world environment through improved dynamic (world) modeling and latent space representation. As the quality of the world model improves, the goal-conditioned policy developed from it generalizes more effectively to the real environment. By closing the generalization gap between the policy\u2019s behavior in the real environment and the world model, MUN effectively guides the agent\u2019s exploration towards the desired goal region in the real environment. ", "page_idx": 2}, {"type": "text", "text": "3.1 Training Generalizable World Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Fig 1 illustrates the general framework of Model-based RL, where world models are trained using agent\u2019s experiences stored in a replay buffer populated with observed environment transitions $(s_{t},a_{t},s_{t+1})$ linking the environment\u2019s future states $s_{t+1}$ and past states $s_{t}$ along with the corresponding control actions $a_{t}$ . The richness of the environment space and dynamic transitions captured by the replay buffer define the extent of what a world model can learn about the real environment. Through supervised learning, the model can generalize reasonably well within the state space moving forward along the trajectories recorded in the replay buffer. However, it may be inaccurate for the state transitions moving backward along the recorded trajectories or across different trajectories. Consider the task of stacking blocks using a robot manipulator in Fig. 2(a). When humans learn to stack blocks, they also understand how to reverse the process to unstack the blocks or return to the initial state. In contrast, a world model trained solely on data from policies under training for stacking ", "page_idx": 2}, {"type": "text", "text": "1: Input: Policy $\\pi^{G}$ , World Model $\\hat{M}$ , reward function $r^{G}$ , subgoals transfer number $N_{s}$ , subgoal   \ntime limit $T_{s}$   \n2: Initialize buffers $D,D_{D A D},D_{e g c}$   \n3: for $i=1$ to $N_{t r a i n}$ do   \n4: if Should Plan Subgoals then   \n5: $B_{e g c}\\leftarrow\\mathbf{A}$ batch of episodes from $D_{e g c}$   \n6: $\\bar{G_{s u b g o a l s}}\\leftarrow\\mathrm{DAD}(B_{e g c})$ with Algorithm 2   \n7: Initialize empty trajectory $\\tau$   \n8: for $s=1$ to $N_{s}$ do   \n9: $t_{s}=0$   \n10: $g_{s}=\\mathfrak{g}$ Sample a subgoal randomly from $G_{s u b g o a l s}$   \n11: while agent has not reached $g_{s}$ and $t_{s}<T_{s}$ do   \n12: Append one step in real environment with $\\pi^{G}$ using goal $g_{s}$ to $\\tau$   \n13: $t_{s}\\gets t_{s}+1$   \n14: $D_{D A D}\\leftarrow D_{D A D}\\cup\\{\\tau\\}$   \n15: $\\tau^{\\prime}\\gets$ Trajectory of $\\pi^{G}$ sampled using the environment goal distribution $g\\sim p_{g}$   \n16: Degc \u2190Degc \u222a\u03c4 \u2032   \n17: D \u2190DDAD \u222aDegc   \n18: Update $\\hat{M}$ with D   \n19: Update $\\pi^{G}$ in imagination with $\\hat{M}$ to maximize $r^{G}$ ", "page_idx": 3}, {"type": "text", "text": "is unlikely to accurately model the unstacking process. As a result, the model may yield hallucinated trajectories for training policies, causing a significant discrepancy between the policy\u2019s behavior in the model and in the real world, thereby leading to ineffective exploration. ", "page_idx": 3}, {"type": "text", "text": "To improve model generalizability, in MUN, we proposed to learn world models capable of characterizing state transitions between any states in the replay buffer, whether by tracing back along recorded trajectories or transitioning between states on separate trajectories. Fig. 2(b) visualizes the comparison between the bidirectional replay buffer for learning world models used in MUN and the unidirectional replay buffer in conventional model-based algorithms. The bidirectional replay buffer not only covers a wider observation space but also captures a richer set of dynamic transitions. As discussed in Sec. 2, due to joint optimization, the richer set of dynamic transitions in MUN allows for a more reliable latent representation of the environmental space and consequently a higher quality reward function (Equation 2) for training policies generalizable to the real environment on top of the learned model. ", "page_idx": 3}, {"type": "text", "text": "We depict the learning algorithm in MUN in Algorithm 1. In the algorithm, we maintain $G_{s u b g o a l s}$ as a set of pivot subgoal states sampled from the relay buffer (illustrated in Algorithm 2) and aim to learn world models capable of seamless transitions between these subgoals. At line 6, we periodically update $G_{s u b g o a l s}$ as the training evolves. In the loop starting from line 8, we repeatedly sample $N_{s}$ subgoals from $G_{\\mathrm{subgoals}}$ and direct the agent to sequentially reach these subgoals within a time limit of $T_{s}$ steps for each. In this way, MUN samples a replay buffer that records bidirectional state transitions between the subgoals in $G_{s u b g o a l s}$ . Based on our experience, we find that setting $N_{s}=2$ is sufficient. Further discussion on the setting of $N_{s}$ is provided in Sec 4. At line 18, we train the world model $\\hat{M}$ using trajectories collected by both the goal commands from $G_{s u b g o a l s}$ (stored in $D_{D A D}$ ) and that sampled from the environment goal distribution $p_{g}$ (stored in $D_{e g c.}$ ). Then, we sample imaginary rollouts from the world model for policy training at line 19. ", "page_idx": 3}, {"type": "text", "text": "Comparison with Go-Explore. We highlight the key difference between MUN\u2019s exploration strategy and the recently popular \"Go-Explore\" strategy( Ecoffet et al. (2019); Pislar et al. (2021); Tuyls et al. (2022); Hu et al. (2023)), designed for exploration-extensive long-term GCRL settings. In Go-Explore, each training episode comprises two phases: the \"Go-phase\" and the \"Explore-phase\". During the \"Go-phase,\" the goal-conditioned policy $\\pi_{G}$ directs the agent to an \"interesting\" goal( Pong et al. (2019); Pitis et al. (2020) )(e.g., states with low frequency of occurrence in the replay buffer), resulting in a final state $s_{T_{g}}$ after $T_{g}$ steps. Following this, the \"Explore-phase\" begins, where an undirected exploration policy takes over from $s_{T_{g}}$ for the remaining $T_{e}$ timesteps. This exploration policy is trained to maximize an intrinsic exploration reward( Bellemare et al. (2016); Pathak et al. ", "page_idx": 3}, {"type": "text", "text": "(2017); Burda et al. (2018); Sekar et al. (2020)) (e.g., to visit areas of the real world that the World Model has not yet learned well). This structure of training episodes has been shown to result in richer exploration( Pislar et al. (2021)). In MUN, when $N_{s}=2$ , Algorithm 1 essentially replaces the \"Explore-phase\" in \"Go-Explore\" with another \"Go-phase\". Thus, the algorithm directs the agent to navigate between two \"interesting\" goals selected from the replay buffer. Firstly, MUN is computationally efficient as it eliminates the need to train a separate exploration policy and an ensemble of world models used to generate intrinsic exploration rewards. Secondly, MUN trains the world model for unconstrained navigation between goal states in the replay buffer, thereby improving the model\u2019s generalization to the real-world environment and leveraging the model for exploration. We empirically compare the two strategies in the context of model-based GCRL in Sec. 4. ", "page_idx": 4}, {"type": "text", "text": "3.2 Key Subgoal Generation through Distinct Action Discovery (DAD) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Having set up the main learning algorithm, we seek to address: how should we pick explorationinducing goals from the replay buffer at training time to help learn generalizable world models? A straightforward strategy is to sample trajectories from the replay buffer and select subgoal states at fixed time intervals along these trajectories. We improve this simple approach with a practical method called DAD (Distinct Action Discovery) for identifying key subgoal states, which represent the pivotal milestones necessary to complete a complex task. Consider the block stacking task as an example. The robotic arm must be able to move its gripper to the vicinity of a block, close the gripper, lift the block, move to the top of another block, release the block, and finally open the gripper. These key subgoal states are essential for completing the task. We illustrate the roles of key states in a 3-Block Stacking task in Fig. 2(a). For an agent to learn this task, it must master reaching and navigating between the key states. By training world models for unconstrained transitions between these key states, MUN can develop models that more accurately capture the task structure and learn policies capable of adapting to novel goal scenarios e.g. block unstacking. ", "page_idx": 4}, {"type": "text", "text": "There exist methods for identifying key states( Zhang et al. (2021); Paul et al. (2019)). However, these methods often tend to be overly complex, leading to insufficient generalization across different environments and requiring adjustments to the methods\u2019 components or parameters for various tasks. Our approach is based ", "page_idx": 4}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/e060a6e182de6341ad82de4515e46991daded43ea01ebd3f1063bc4945cdc15e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "on the observation that certain actions are crucial at different stages of task completion. For instance, in a block stacking task, the robotic arm must learn actions such as closing the gripper, grasping the block, lifting it, and releasing the gripper. When the agent performs these key actions, the corresponding states can often be considered key subgoal states. By selecting actions that significantly differ along trajectories and extracting the corresponding states during these actions, we can identify potential key subgoal states. The Farthest Point Sampling (FPS) algorithm(Eldar et al. (1997)) provides a simple and efficient method for selecting $N$ points with maximal differences from a set. We apply FPS(Eldar et al. (1997)) to choose $N$ time steps with the greatest variations in actions from a batch of trajectory data, thereby obtaining the set of key subgoal states corresponding to these time steps. Algorithm 2 shows how MUN finds key subgoal states using the DAD method. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We evaluate MUN across various robotic manipulation and navigation environments, aiming to address the following three research questions: (RQ1) Does MUN outperform other goal-conditioned model-based reinforcement learning baselines with advanced exploration strategies? (RQ2) Can DAD effectively identify key subgoal states along trajectories to the environment goal region? (RQ3) Does MUN successfully leverage the bi-directional replay buffer to train a generalizable policy for navigating effectively between arbitrary subgoals? ", "page_idx": 4}, {"type": "text", "text": "4.1 Environments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We conducted experiments on six challenging goal-conditioned tasks to evaluate MUN. In Ant-Maze, an ant-like robot is tasked to learn complex 4-legged locomotion behavior and navigate around the hallways within a maze structure. The Walker task involves a two-legged robot learning to control its leg joints effectively to achieve stable walking to reach goals along a flat plane forward or backward. In 3-Block Stacking, a robot arm with a two-fingered gripper operates on a tabletop with three blocks. The goal is to stack the blocks into a tower configuration. The agent needs to learn to push, pick, and stack objects while discovering complex action sequences to complete the task in the environment. Previous solutions have relied on methods like demonstrations, curriculum learning, or extensive simulator data, highlighting the task\u2019s difficulty( Ecoffet et al. (2019); Li et al. (2020); Nair et al. (2018); Lanier (2019)). The Block Rotation and Pen Rotation tasks require the agent to manipulate a block and a pen, respectively, to achieve a randomly specified orientation along all axes. Pen Rotation is particularly challenging due to the pen\u2019s thinness, requiring precise control to prevent it from dropping. In Fetch Slide, a manipulator slides a puck to a designated goal area on a slippery table. Unlike tasks that involve direct manipulation, Fetch Slide emphasizes the challenge of accurately controlling the force and direction of the push operation, as the puck must slide across the flat surface to the target. See Appendix. C for more information about environments. ", "page_idx": 4}, {"type": "image", "img_path": "aYqTwcDlCG/tmp/d1766706e698f24ec7482d2b225e0869576032bd8253d1acec3160dd3c0af4d6.jpg", "img_caption": ["Figure 3: We evaluate MUN on 6 environments: Ant Maze, Walker, 3-Block Stacking, Block Rotation, Pen Rotation, Fetch Slide. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.2 Baselines ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We compare MUN with the following baselines. The GC-Dreamer baseline is discussed in Sec. 2. We include two baselines based on the Go-Explore strategy( Ecoffet et al. (2019)) that has been proved efficient in the GCRL setting: MEGA( Pitis et al. (2020)) and PEG( Hu et al. (2023)). A Go-Explore agent firstly uses its goal-conditioned policy \u03c0 to approach a sampled exploration-inducing goal command $g$ , referred to as the Go-phase. In the Explore-phase, it activates an exploration policy $\\pi^{E}$ to explore the environment from the terminal state of the Go-phase. In contrast, MUN improves the generalization of world models to facilitate effective real-world environment exploration. During training, MUN collects trajectories that navigate between two goal states sampled from its candidate subgoal set, essentially replacing the \"Explore-phase\" in \"Go-Explore\" with another \"Go-phase\". MEGA commands the agent to rarely seen states at the frontier by using kernel density estimates (KDE) of state densities and chooses low-density goals from the replay buffer. PEG selects goal commands to guide an agent\u2019s goal-conditioned policy toward states with the highest exploration potential given its current level of training. This potential is defined as the expected accumulated exploration reward during the Explore-phase. Similar to MUN, our baseline methods, named PEG-G and MEGA-G, augment GC-Dreamer with the PEG and MEGA Go-Explore strategies, respectively. In these methods, the replay buffer $D$ contains not only trajectories sampled by the GCRL policy $\\pi_{G}$ commanded by environment goals but also exploratory trajectories sampled using the Go-Explore strategies. The exploration policy $\\pi^{E}$ in PEG-G and MEGA-G is the Plan2Explore policy from Sekar et al. (2020), which encourages the agent to actively search for states that induce disparities among an ensemble of world models. ", "page_idx": 5}, {"type": "text", "text": "We note that MUN and the baselines are all implemented based on the Dreamer framework as realized in GC-Dreamer1. ", "page_idx": 5}, {"type": "text", "text": "4.3 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Fig. 4 shows the evaluation performance of MUN and all baselines across training. MUN demonstrates superior performance compared to the baseline models, excelling in both the final success rate and the speed of learning. MUN outperforms the Go-Explore baselines (MEGA-G and PEG-G) across all tasks, demonstrating the effectiveness of the exploration strategy in MUN over the alternative Go-Explore strategies. In the most challenging tasks\u2014block stacking, block rotation, and pen rotation\u2014MUN shows a significant margin of superiority. For example, MUN achieves over $95\\%$ success rate on 3 block stacking, while all other baselines only manage to achieve around $60\\%$ success rate on this task within $2.5\\mathrm{M}$ steps. MEGA-G and PEG-G heuristically pick exploration-inducing goals to initiate exploration by a separate policy. Since finding a goal state that is optimally aligned with both the goal-conditioned policy and the exploration policy is challenging, these methods can result in suboptimal goals, thereby slowing down exploration. GC-Dreamer lacks a Go-Explore phase, which limits its exploration potential. Despite this, it can still perform comparably to or even better than MEGA-G and PEG-G in certain contexts. This indicates that the Go-Explore strategy does not always guarantee improved exploration, and suboptimal goal-setting during the \"Go-phase\" can hinder exploration (see 3 block stacking). ", "page_idx": 5}, {"type": "image", "img_path": "aYqTwcDlCG/tmp/268ba100bb8c4cb51260b6875de9d1aa21a62d6c0fac1c4d9da2c4b12a62c060.jpg", "img_caption": ["Figure 4: Experiment results comparing MUN with the baselines over 5 random seeds. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Fetch Slide is a non-prehensile manipulation task. This environment has asymmetric state transitions: when the puck is slid outside the robot\u2019s workspace, the manipulator cannot reach the puck\u2019s position to slide it backward due to physical constraints. MUN still outperforms the other baselines in this environment. We found MUN, with the DAD strategy, can discover key subgoals for this task, like contacting the puck, placing the manipulator at different angles around the puck, and stabilizing the manipulator upon reaching the goal (these key states result from distinct actions). MUN enables learning state transitions between these key subgoals to dis", "page_idx": 6}, {"type": "image", "img_path": "aYqTwcDlCG/tmp/068d57a0f52e25cb88e7098f5d504a7a170c1af66ca4386eda6ed1a9568e25e6.jpg", "img_caption": ["Figure 5: The world model prediction error curves throughout the training steps for 3-Block Stacking and Pen Rotation. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "cover the high-level task structure. It learns a generalizable world model that handles sliding the puck between any positions within the workspace and predicts low probabilities for infeasible transitions from puck positions outside the workspace. Particularly, it enables the agent to hit the puck multiple times if it is within its workspace, thereby improving task success rates. That said, the current goal selection mechanism in MUN lacks a process to filter out infeasible goals from the current state, which could adversely affect sample efficiency. We left addressing this limitation and implementing a robust filtering mechanism for infeasible goals as a focus for future work. ", "page_idx": 6}, {"type": "text", "text": "We studied the prediction error of learned world models in MUN and the baselines. Fig. 5 shows the one-step model prediction error throughout the training steps. The world models trained by MUN show a much smaller generalization gap to the real environment compared to the baselines across the training steps. Consequently, MUN can effectively leverage these higher-quality world models to train policies that generalize better to the real environment. We present a quantitative comparison of the world model prediction quality between MUN and the baselines in terms of model prediction compounding error in Appendix F.3. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.4 Can DAD find key subgoals? ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "aYqTwcDlCG/tmp/f50d1b72015737b01576a292f9548aec0344439c85279af50b8f4b5a0ce92fff.jpg", "img_caption": ["Figure 6: Key subgoals found by DAD (Algorithm 2) in three environments: Ant-Maze, Walker, 3-Block Stacking. They present the important landmarks on the path to the task goal regions. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "We visualize several subgoals found by the DAD algorithm during the training process in Fig. 6 for three environments: Ant-Maze, Walker, 3-Block Stacking. In Walker, DAD successfully identifies the crucial joint angles and forces of the Walker robot during its forward locomotion, including standing, striding, jumping, landing, and leg support. In Ant-Maze, DAD recognizes significant motion variations at corridor corners. In 3-Block Stacking, DAD successfully identifies crucial state transitions required during the stacking process. These critical subgoals include block grasping, lifting, horizontal movement, vertical movement, and gripper release. For more discussion about subgoals found by the DAD in other environments, please refer to Appendix F.1. ", "page_idx": 7}, {"type": "text", "text": "4.5 Can MUN navigate between arbitrary subgoals? ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "aYqTwcDlCG/tmp/8d3924cb679feee8085b0dc30230e9a01776a79886c9985f8ca41e523254e49c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 7: Experiment setup and results of navigation between any pair of subgoals in the 3-Block Stacking environment. In the left part, the bottom section of each image depicts the ultimate evaluation goal for one evaluation episode, while the top section illustrates the manually set initial state. The right part shows the evaluation success rates. ", "page_idx": 7}, {"type": "text", "text": "As MUN is capable of identifying pivotal subgoal states necessary for complex tasks and training world models and policies for seamless transitions between these subgoals, we investigate MUN\u2019s capacity to generalize to new task settings concerning important subgoals. We set the initial state of the agent at one random subgoal and command it to reach another random subgoal. Such task setting is not provided to the agent during training. For the 3 Block Stacking task, we employ a set of 15 manually created subgoals representing various critical states in the block-stacking process, resulting in 225 unique combinations of initial states and test goals for evaluation. Each combination undergoes 10 repeated evaluations, totaling 2250 evaluation trajectories. These evaluations encompass both the forward and reverse processes of stacking and unstacking blocks, assessing the agent\u2019s proficiency in both task completion and restoration. For example, in the left portion of Fig. 7, we visualize some subgoals used as initial task state in the upper part and some subgoals used as evaluation test goals in the lower part. The right section of Fig. 7 illustrates MUN\u2019s superiority over the other baselines in these evaluation experiments, achieving the highest success rate through its ability to develop a robust and adaptable world model that generalizes to novel tasks. Additional results in different environments are provided in Appendix F.2. ", "page_idx": 7}, {"type": "image", "img_path": "aYqTwcDlCG/tmp/8dc8825ad9476d4c793438c75be724a4d941b81cf4e5a6a47fac49dbf5e3cbd4.jpg", "img_caption": ["Figure 8: Experiment results comparing MUN with its ablations over 5 random seeds. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.6 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conducted the following ablation studies to investigate MUN\u2019s exploration goal selection mechanism. First, we investigated the effect of the number of subgoal states $(N_{s})$ in our algorithm. MUN sequentially traverses $N_{s}=2$ goal states sampled from the replay buffer to explore the environment during each training episode. We introduced an ablation MUN-Ns-3 that sets $N_{s}=3$ . This ablation aims to investigate whether increasing $N_{s}$ leads to improved learning performance. Second, we considered an ablated version of MUN, named MUN-noDAD, which replaces the goal sampling strategy DAD (Algorithm 2) with a simple method that chooses goal states with fixed time interval in trajectories sampled from the replay buffer. This ablation investigates the importance of identifying key subgoal states, which represent pivotal milestones necessary to complete a complex task. It seeks to determine whether training world models from state transitions between these key states in MUN is essential, or if using any states from the replay buffer would suffice. Lastly, we explored an alternative key subgoal discovery strategy. MUN identifies key subgoals for exploration as states in the replay buffer that result in distinct actions within the action space. We introduced an ablation, MUN-KeyObs, which directly discovers key subgoals from the state space by identifying centroids of (latent) state clusters in the replay buffer, following the strategy in Zhang et al. (2021). ", "page_idx": 8}, {"type": "text", "text": "The results are depicted in Fig. 8. MUN outperforms all ablated versions. Setting $N_{s}=3$ slows down the training performance, supporting our claim it suffices to set $N_{s}=2$ . The performance of MUN-noDAD and MUN-KeyObs does not match MUN, especially in the 3 Block Stacking environment, highlighting that discovering key subgoals in the action space (the DAD strategy) indeed contributes to higher performance and efficiency. It is noteworthy that the ablation methods achieve a relatively small gap in success rates compared to MUN in the challenging Block Rotation and Pen Rotation environments. This suggests that MUN\u2019s approach to learning a world model from state transitions between any states in the replay buffer (whether tracing back along recorded trajectories or transitioning across separate trajectories) alone is effective in bridging the generalization gap between the model and the real environment. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Model-based reinforcement learning (MBRL) is a promising approach to reinforcement learning that learns a model of the environment and uses it to plan actions(Sutton (1991); Deisenroth and Rasmussen (2011); Oh et al. (2017); Chua et al. (2018)). It has achieved remarkable success in numerous control tasks and games, such as chess(Silver et al. (2017); Schrittwieser et al. (2020); Xu et al. (2022)), Atari games(Hafner et al. (2020); Schrittwieser et al. (2020); Oh et al. (2017)), continuous control tasks(Kurutach et al. (2018); Buckman et al. (2018); Hafner et al. (2019b); Janner et al. (2019)), and robotic manipulation tasks(Lowrey et al. (2018); Luo et al. (2018)). The dynamic model serves as a pivotal component of model-based reinforcement learning, primarily fulfilling two key roles: planning actions(Deisenroth and Rasmussen (2011); Oh et al. (2017); Chua et al. (2018); Lowrey et al. (2018); Hafner et al. (2019b)) or generating synthetic data to aid in the training of model-free reinforcement learning algorithms(Janner et al. (2019); Hafner et al. (2020, 2023)). The primary drawback of the former lies in the excessive cost associated with long-term planning. To address this issue, the concept of ensemble has been employed to enhance performance(Chua et al. (2018); Kurutach et al. (2018); Buckman et al. (2018)). Oh et al. (2017); Hansen et al. (2022b) integrate the dynamics model with a value prediction network to improve the accuracy of long-term planning. The latter also suffers from the potential bias of the model, which can result in inaccuracies in the generated data, thereby directly impacting policy learning(Luo et al. (2018); Lai et al. (2021)). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Multi-goal reinforcement learning (RL) agents (Schaul et al. (2015); Plappert et al. (2018); Ghosh et al. (2019)) acquire goal-conditioned behaviors capable of achieving and generalizing across diverse sets of objectives. Researchers have been continuously exploring the integration of Model-based RL and Goal-conditioned RL(Mendonca et al. (2021); Nair et al. (2020); Zhang et al. (2020)), leveraging the capabilities of dynamic models in planning and generating synthetic data to enhance the training efficiency and generalization of GCRL. However, compared to traditional RL problems, GCRL faces more severe challenges regarding reward sparsity and exploration difficulties(Ren et al. (2019); Florensa et al. (2018); Trott et al. (2019)). These challenges often lead to significant biases in the learned World Model, consequently impairing the performance of goal-conditioned policy(Mendonca et al. (2021); Hu et al. (2023)). Pong et al. (2019) propose to learn a maximum-entropy goal distribution, Pitis et al. (2020) encourage the agent to explore goals with low frequency of occurrence in the replay buffer, Sekar et al. (2020) introduce a planning algorithm to pick goals for exploration using world model. ", "page_idx": 9}, {"type": "text", "text": "The World Model holds inherent advantages for GCRL, as it often enables faster exploration and facilitates the training of a more generalized policy(McCarthy et al. (2021); Shyam et al. (2019); Hu et al. (2023); Sekar et al. (2020)). However, within the GCRL framework, learning a reliable World Model is a crucial prerequisite for developing excellent policies(Zhang et al. (2024); Young et al. (2022); Wang et al. (2023); Lai et al. (2021)). Kauvar et al. (2023) propose a curiosity-driven exploration method, which is focused on replay buffer management. Hansen et al. (2022a) use demonstration data as a supplement to the replay buffer to learn a more reliable World Model. Previous work has often focused on devising more appropriate objectives when sampling real trajectory data from the environment to enrich the diversity of dynamic transitions in the replay buffer(Nair et al. (2020); Charlesworth and Montana (2020); Trott et al. (2019); Florensa et al. (2018); Campero et al. (2020)). However, they overlooked the overall direction of dynamic transitions within the data which extremely affects the richness of dynamic transitions to learn a comprehensive World Model. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In summary, we introduce MUN, a novel goal-directed exploration algorithm designed for effective world modeling of seamless transitions between arbitrary states in replay buffers, whether retracing along recorded trajectories or transitioning between states on separate trajectories. As the quality of the world model improves, MUN demonstrates high efficacy in learning goal-conditioned policies in sparse-reward environments. Additionally, we present a practical strategy DAD for identifying pivotal subgoal states, which act as critical milestones in completing complex tasks. The experimental results underscored the effectiveness of MUN in strengthening the reliability of world models and learning policies capable of adapting to novel test goals. ", "page_idx": 9}, {"type": "text", "text": "Reproducibility Statement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The code of MUN is provided on https://github.com/RU-Automated-Reasoning-Group/MUN. For hyperparameter settings and baseline pseudocode, please refer to Appendix D and Appendix E.3. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers for their comments and suggestions. This work was supported by NSF Award #CCF-2124155. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O., and Zaremba, W. (2017). Hindsight experience replay. Advances in neural information processing systems, 30.   \nBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. (2016). Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29.   \nBuckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee, H. (2018). Sample-efficient reinforcement learning with stochastic ensemble value expansion. Advances in neural information processing systems, 31.   \nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894.   \nCampero, A., Raileanu, R., K\u00fcttler, H., Tenenbaum, J. B., Rockt\u00e4schel, T., and Grefenstette, E. (2020). Learning with amigo: Adversarially motivated intrinsic goals. arXiv preprint arXiv:2006.12122.   \nCharlesworth, H. and Montana, G. (2020). Plangan: Model-based planning with sparse rewards and multiple goals. Advances in Neural Information Processing Systems, 33:8532\u20138542.   \nChua, K., Calandra, R., McAllister, R., and Levine, S. (2018). Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Advances in neural information processing systems, 31.   \nDeisenroth, M. and Rasmussen, C. E. (2011). Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pages 465\u2013472.   \nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and Clune, J. (2019). Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995.   \nEldar, Y., Lindenbaum, M., Porat, M., and Zeevi, Y. Y. (1997). The farthest point strategy for progressive image sampling. IEEE transactions on image processing, 6(9):1305\u20131315.   \nFlorensa, C., Held, D., Geng, X., and Abbeel, P. (2018). Automatic goal generation for reinforcement learning agents. In International conference on machine learning, pages 1515\u20131528. PMLR.   \nGeorgiev, I., Giridhar, V., Hansen, N., and Garg, A. (2024). Pwm: Policy learning with large world models. arXiv preprint arXiv:2407.02466.   \nGhosh, D., Gupta, A., Reddy, A., Fu, J., Devin, C., Eysenbach, B., and Levine, S. (2019). Learning to reach goals via iterated supervised learning. arXiv preprint arXiv:1912.06088.   \nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2019a). Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603.   \nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. (2019b). Learning latent dynamics for planning from pixels. In International conference on machine learning, pages 2555\u20132565. PMLR.   \nHafner, D., Lillicrap, T., Norouzi, M., and Ba, J. (2020). Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193.   \nHafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. (2023). Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104.   \nHansen, N., Lin, Y., Su, H., Wang, X., Kumar, V., and Rajeswaran, A. (2022a). Modem: Accelerating visual model-based reinforcement learning with demonstrations. arXiv preprint arXiv:2212.05698.   \nHansen, N., Su, H., and Wang, X. (2023). Td-mpc2: Scalable, robust world models for continuous control. arXiv preprint arXiv:2310.16828.   \nHansen, N., Wang, X., and Su, H. (2022b). Temporal difference learning for model predictive control. arXiv preprint arXiv:2203.04955.   \nHu, E. S., Chang, R., Rybkin, O., and Jayaraman, D. (2023). Planning goals for exploration. arXiv preprint arXiv:2303.13002.   \nJanner, M., Fu, J., Zhang, M., and Levine, S. (2019). When to trust your model: Model-based policy optimization. Advances in neural information processing systems, 32.   \nKauvar, I., Doyle, C., Zhou, L., and Haber, N. (2023). Curious replay for model-based adaptation. arXiv preprint arXiv:2306.15934.   \nKurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. (2018). Model-ensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592.   \nLai, H., Shen, J., Zhang, W., Huang, Y., Zhang, X., Tang, R., Yu, Y., and Li, Z. (2021). On effective scheduling of model-based reinforcement learning. Advances in Neural Information Processing Systems, 34:3694\u20133705.   \nLai, Y., Wang, W., Yang, Y., Zhu, J., and Kuang, M. (2020). Hindsight planner. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems, pages 690\u2013698.   \nLanier, J. B. (2019). Curiosity-driven multi-criteria hindsight experience replay. University of California, Irvine.   \nLi, R., Jabri, A., Darrell, T., and Agrawal, P. (2020). Towards practical multi-object manipulation using relational reinforcement learning. In 2020 ieee international conference on robotics and automation (icra), pages 4051\u20134058. IEEE.   \nLowrey, K., Rajeswaran, A., Kakade, S., Todorov, E., and Mordatch, I. (2018). Plan online, learn offilne: Efficient learning and exploration via model-based control. arXiv preprint arXiv:1811.01848.   \nLuo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T. (2018). Algorithmic framework for modelbased deep reinforcement learning with theoretical guarantees. arXiv preprint arXiv:1807.03858.   \nMcCarthy, R., Wang, Q., and Redmond, S. J. (2021). Imaginary hindsight experience replay: Curious model-based learning for sparse reward tasks. arXiv preprint arXiv:2110.02414.   \nMendonca, R., Rybkin, O., Daniilidis, K., Hafner, D., and Pathak, D. (2021). Discovering and achieving goals via world models. Advances in Neural Information Processing Systems, 34:24379\u2013 24391.   \nNagabandi, A., Konolige, K., Levine, S., and Kumar, V. (2020). Deep dynamics models for learning dexterous manipulation. In Conference on Robot Learning, pages 1101\u20131112. PMLR.   \nNair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P. (2018). Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE international conference on robotics and automation (ICRA), pages 6292\u20136299. IEEE.   \nNair, S., Savarese, S., and Finn, C. (2020). Goal-aware prediction: Learning to model what matters. In International Conference on Machine Learning, pages 7207\u20137219. PMLR.   \nOh, J., Singh, S., and Lee, H. (2017). Value prediction network. Advances in neural information processing systems, 30.   \nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pages 2778\u20132787. PMLR.   \nPaul, S., Vanbaar, J., and Roy-Chowdhury, A. (2019). Learning from trajectories via subgoal discovery. Advances in Neural Information Processing Systems, 32.   \nPislar, M., Szepesvari, D., Ostrovski, G., Borsa, D., and Schaul, T. (2021). When should agents explore? arXiv preprint arXiv:2108.11811.   \nPitis, S., Chan, H., Zhao, S., Stadie, B., and Ba, J. (2020). Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. In International Conference on Machine Learning, pages 7750\u20137761. PMLR.   \nPlappert, M., Andrychowicz, M., Ray, A., McGrew, B., Baker, B., Powell, G., Schneider, J., Tobin, J., Chociej, M., Welinder, P., et al. (2018). Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464.   \nPong, V. H., Dalal, M., Lin, S., Nair, A., Bahl, S., and Levine, S. (2019). Skew-fit: State-covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698.   \nRen, Z., Dong, K., Zhou, Y., Liu, Q., and Peng, J. (2019). Exploration via hindsight goal generation. Advances in Neural Information Processing Systems, 32.   \nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015). Universal value function approximators. In International conference on machine learning, pages 1312\u20131320. PMLR.   \nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al. (2020). Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609.   \nSekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., and Pathak, D. (2020). Planning to explore via self-supervised world models. In International conference on machine learning, pages 8583\u20138592. PMLR.   \nShyam, P., Ja\u00b4skowski, W., and Gomez, F. (2019). Model-based active exploration. In International conference on machine learning, pages 5779\u20135788. PMLR.   \nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al. (2017). Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815.   \nSutton, R. S. (1991). Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160\u2013163.   \nTrott, A., Zheng, S., Xiong, C., and Socher, R. (2019). Keeping your distance: Solving sparse reward tasks using self-balancing shaped rewards. Advances in Neural Information Processing Systems, 32.   \nTuyls, J., Yao, S., Kakade, S., and Narasimhan, K. (2022). Multi-stage episodic control for strategic exploration in text games. arXiv preprint arXiv:2201.01251.   \nWang, X., Wongkamjan, W., Jia, R., and Huang, F. (2023). Live in the moment: Learning dynamics model adapted to evolving policy. In International Conference on Machine Learning, pages 36470\u201336493. PMLR.   \nWilliams, G., Aldrich, A., and Theodorou, E. (2015). Model predictive path integral control using covariance variable importance sampling. arXiv preprint arXiv:1509.01149.   \nXu, Y., Hansen, N., Wang, Z., Chan, Y.-C., Su, H., and Tu, Z. (2022). On the feasibility of cross-task transfer with model-based reinforcement learning. arXiv preprint arXiv:2210.10763.   \nYoung, K., Ramesh, A., Kirsch, L., and Schmidhuber, J. (2022). The benefits of model-based generalization in reinforcement learning. arXiv preprint arXiv:2211.02222.   \nYu, T., Thomas, G., Yu, L., Ermon, S., Zou, J. Y., Levine, S., Finn, C., and Ma, T. (2020). Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129\u201314142.   \nZhang, L., Yang, G., and Stadie, B. C. (2021). World model as a graph: Learning latent landmarks for planning. In International conference on machine learning, pages 12611\u201312620. PMLR.   \nZhang, W., Wang, G., Sun, J., Yuan, Y., and Huang, G. (2024). Storm: Efficient stochastic transformer based world models for reinforcement learning. Advances in Neural Information Processing Systems, 36.   \nZhang, Y., Abbeel, P., and Pinto, L. (2020). Automatic curriculum learning through value disagreement. Advances in Neural Information Processing Systems, 33:7648\u20137659. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Extended Background ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Dreamer World Model ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The RSSM consists of an encoder, a recurrent model, a representation model, a transition predictor, and a decoder, as formulated in Equation 3. And it employs an end-to-end training methodology, where its parameters are jointly optimized based on the loss functions of various components, including dynamic transition prediction, reward prediction, and observation encoding-decoding. These components often operate in a latent space rather than the original observation space, as encoded by the World Model. Therefore, during end-to-end training, the losses of all components indirectly optimize the latent space. ", "page_idx": 13}, {"type": "text", "text": "The encoder $f_{E}$ encodes the input state $x_{t}$ into a embed state $e_{t}$ , which is then fed with the deterministic state $h_{t}$ into the representation model $q_{\\phi}$ to generate the posterior state $z_{t}$ . The transition predictor $p_{\\phi}$ predicts the prior state $\\hat{z}_{t}$ based on the deterministic state $h_{t}$ without access to the current input state $x_{t}$ . Using the concatenation of either $(h_{t},z_{t})$ or $(h_{t},\\hat{z}_{t})$ as input, the recurrent transition function $f\\phi$ iteratively updates the deterministic state $h_{t}$ with given action $a_{t}$ . ", "page_idx": 13}, {"type": "text", "text": "Encoder: $\\begin{array}{r l}&{{\\boldsymbol e}_{t}=f_{E}({\\boldsymbol e}_{t}|x_{t})}\\\\ &{{\\boldsymbol h}_{t}=f_{\\phi}(h_{t-1},z_{t-1},a_{t-1})}\\\\ &{z_{t}\\sim q_{\\phi}(z_{t}|{\\boldsymbol h}_{t},{\\boldsymbol e}_{t})}\\\\ &{\\hat{z}_{t}\\sim p_{\\phi}(\\hat{z}_{t}|{\\boldsymbol h}_{t})}\\\\ &{\\hat{x}_{t}\\sim f_{D}(\\hat{x}_{t}|{\\boldsymbol h}_{t},{\\boldsymbol z}_{t})}\\end{array}$   \nRecurrent model:   \nRepresentation model:   \nTransition predictor: Decoder: ", "page_idx": 13}, {"type": "text", "text": "A.2 Temporal Distance Training in LEXA ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The goal-reaching reward $r^{G}$ is defined by the self-supervised temporal distance objective (Mendonca et al. (2021)) which aims to minimize the number of action steps needed to transition from the current state to a goal state within imagined rollouts. We use $b_{t}$ to denote the concatenate of the deterministic state $h_{t}$ and the posterior state $z_{t}$ at time step $t$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\boldsymbol{b}_{t}=\\left(h_{t},z_{t}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The temporal distance $D_{t}$ is trained by sampling pairs of imagined states $b_{t},b_{t+k}$ from imagined rollouts and predicting the action steps number $k$ between the embedding of them, with a predicted embedding $\\hat{e}_{t}$ from $b_{t}$ to approximate the true embedding $e_{t}$ of the observation $x_{t}$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\ne m b(b_{t})=\\hat{e}_{t}\\approx e_{t},\\qquad\\mathrm{where}\\quad e_{t}=f_{E}(x_{t})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Temporal distance $:D_{t}(\\hat{e}_{t},\\hat{e}_{t+k})\\approx k/H\\qquad\\mathrm{where}\\quad\\hat{e}_{t}=e m b(b_{t})\\quad\\hat{e}_{t+k}=e m b(b_{t+k})$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\boldsymbol{r}_{t}^{G}(b_{t},b_{t+k})=-D_{t}(\\hat{e}_{t},\\hat{e}_{t+k})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B Limitations and Future Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The MUN has provided powerful guidance in enhancing world model learning by repeatedly studying the transitions between various key states. This allows the acquisition of richer dynamic transitions and deepens the world model\u2019s understanding of the real world. However, such a framework requires an efficient strategy for discovering key states, as evidenced by the comparative results of the MUN and MUN-noDAD. We found that although DAD excels in discovering key states with its simple and efficient method, it will identify ineffective and task-irrelevant states in tasks with highly complex action spaces or weak correlations between goal space and action space. This can lead to the degradation of the MUN architecture due to poor-quality subgoals, resulting in a substantial amount of ineffective sampling in the environment. Therefore, for environments with more complex action and goal spaces, we need to develop a more robust and effective method for discovering subgoals than DAD. Only with an efficient and powerful self-supervised subgoal discovery mechanism can the MUN framework be fully utilized. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Meanwhile, MUN autonomously discovers subgoals and learns a more robust and comprehensive world model by randomly navigating between subgoals. Although the MUN has achieved huge success in model-based reinforcement learning (MBRL), we believe it can also be applied to general model-free methods. General model-free methods do not require learning a world model and have a simpler architecture. The MUN can directly guide the goal-conditioned policy to enhance learning in navigation between different subgoals. It can use sampled trajectories to learn this policy directly, bypassing the use of the world model to train policies and value functions through simulated trajectories, thereby enhancing the agent\u2019s ability to reach unconstrained goals. Therefore, we plan to explore the application and effectiveness of the MUN in model-free RL in the future and develop a new robust self-supervised subgoal discovery mechanism to make the MUN applicable to more complex environments. ", "page_idx": 14}, {"type": "text", "text": "C Environments", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 3-Block Stacking ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this task, the robot must stack three blocks in different colors into a tower shape. While PEG assesses goals of varying difficulty levels: 3 easy goals (picking up a single block), 6 medium goals (stacking two blocks), and 6 hard goals (stacking three blocks), our evaluation is focused solely on the 6 hard goals, and we use only 3 hard goals of them as the guiding goals from the training environment. Training and evaluating with only the hardest goals imposes a significant challenge for the MUN. However, we observed that the MUN can spontaneously discover additional easy and medium goals through DAD, as these serve as critical transitional states toward the hard goals. The environment is characterized by a 14-dimensional state and goal space. The first five dimensions capture the gripper\u2019s state, while the remaining nine dimensions correspond to the xyz positions of each block. The action space is 4-dimensional, with three dimensions dedicated to the gripper\u2019s $x y z$ movements and the fourth dimension controlling the gripper\u2019s finger movement. Success is defined by achieving an L2 distance of less than $3\\;\\mathrm{{cm}}$ between each block\u2019s xyz position and its target position. This environment is a modified version of the FetchStack3 environment from Pitis et al. (2020), designed to better test the robot\u2019s precision in stacking. ", "page_idx": 14}, {"type": "text", "text": "C.2 Walker ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this environment, a 2D walker robot is trained and evaluated on its ability to move across a flat surface. The environment\u2019s implementation is based on the code from Mendonca et al. (2021). To thoroughly assess the agent\u2019s capability and precision in covering longer distances, we expanded the evaluation goals to 12 $\\:(\\pm13,\\pm16,\\pm19,\\pm22,\\pm25,\\pm28)\\:$ along the $x$ axis from the initial position. In our training setup for the MUN, we only use the goals at $\\pm13$ and $\\pm16$ provided by the environment, but we evaluate the agent\u2019s performance across all 12 goals. Success is measured by verifying whether the agent\u2019s $x$ position is within a small margin of the target $x$ position. The state and goal space in this environment are nine-dimensional, comprising the walker\u2019s $x z$ positions and its joint angles. This configuration ensures a comprehensive evaluation of the walker\u2019s locomotion capabilities. ", "page_idx": 14}, {"type": "text", "text": "C.3 Ant Maze ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This environment builds upon the Ant Maze from Pitis et al. (2020), incorporating a few modifications. The state and goal spaces in the Ant Maze environment are highly complex, totaling 29 dimensions. These dimensions include the ant\u2019s $x y z$ position, joint angles, and velocities. The first three dimensions account for the $x y z$ position, the next 12 dimensions capture the joint angles of the ant\u2019s limbs, and the remaining 14 dimensions represent the velocities of the joints and the ant\u2019s movements in the $x y$ plane. The action space consists of 8 dimensions, controlling the hip and ankle movements of the ant\u2019s legs.We matched the goal space to the state space, which includes the ant\u2019s $x y z$ coordinates, joint positions, and velocities. We also introduced an additional room in the top left to increase the difficulty like PEG. In this scenario, the ant robot must traverse from the bottom left to the top left of a maze, navigating through various corridors. The task is particularly challenging due to its lengthy duration\u2014each episode lasts 500 timesteps\u2014and the significant distance the ant must cover. Unlike PEG, which evaluates goals in both the central hallway and the top left room, our evaluation focuses exclusively on the four most difficult goals located in the top left room. For training, we utilize all 32 goals throughout the maze. The maze itself measures about 6 by 8 meters. The ant succeeds if its $x y$ position is within 1.0 meter of the goal, roughly the size of a single cell in the maze. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C.4 Fetch Slide ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this task, a robotic arm with a two-fingered gripper must push an object along a flat surface to a specific goal location. We use the \"FetchSlide-v1\" environment from Gymnasium, where the robot operates in a 25-dimensional state space that includes the robot\u2019s joint states, object position, and goal information. The goal space is 3-dimensional, representing the target coordinates for the object. Each episode presents a unique random goal location within a bounded area, requiring the agent to adjust its pushing strategy accordingly. A key challenge in Fetch Slide lies in the indirect manipulation of the object. The agent must accurately control the force and direction of its push while accounting for physical properties like friction, surface irregularities, and object momentum. Unlike grasping or lifting tasks, sliding demands precise force calibration and anticipation of the object\u2019s response to contact. For evaluation, the agent\u2019s learned policy is tested across 50 episodes with different goal locations, assessing its ability to generalize over varied configurations. Training goals are randomly generated from the environment, helping the agent explore diverse sliding trajectories to improve robustness across different scenarios. ", "page_idx": 15}, {"type": "text", "text": "C.5 Block and Pen Rotation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this task, a robotic hand must manipulate either a thin pen or a block to achieve specified target rotations. We use \"HandManipulatePenRotate-v1\" and \"HandManipulateBlockRotateXYZv1\" versions of the gymnasium environments. Both tasks feature a state space of 61 dimensions, encompassing the robot\u2019s joint states, object states, and goal information. The goal space is 7- dimensional, representing the target pose details. Each episode will have randomized target rotations goal for all axes of the block and for the x and y axes of the pen. The pen is more challenging to handle due to its tendency to slip, requiring more precise control compared to the block. For evaluation, the latest policy is tested 50 episodes for each task, with each episode having a unique random goal. In our framework, training goals are also randomly generated from the environment. ", "page_idx": 15}, {"type": "text", "text": "D Baselines ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first present our overall training framework for goal-conditioned model-based reinforcement learning (MBRL). It is important to note that all baselines utilize this training framework, differing only in the strategy employed for collecting trajectories within the real environment. Our training framework is based on the implementation of LEXA paper(Mendonca et al. (2021)). ", "page_idx": 15}, {"type": "text", "text": "Algorithm 3 General MBRL Training Framework   \n1: Input: Policy $\\pi^{G},\\pi^{E}$ , Environment Goal Distribution $G$ , World Model $\\hat{M}$ , reward function $r^{G}$ ,   \nrE   \n2: $\\mathcal{D}\\leftarrow\\{\\}$ Initialize buffer.   \n3: for Episode $i=1$ to $N_{\\mathrm{train}}$ do   \n4: $\\tau\\gets$ Collect trajectories $(\\ldots)$   \n5: $\\mathcal{D}\\leftarrow\\mathcal{D}\\cup\\tau$   \n6: Update world model $\\hat{M}$ with $\\mathcal{D}$   \n7: Update $\\pi^{G}$ in imagination with $\\hat{M}$ to maximize $r^{G}$   \n8: Update $\\pi^{E}$ in imagination with $\\hat{M}$ to maximize $r^{E}$ ", "page_idx": 15}, {"type": "text", "text": "D.1 Go-Explore ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our baselines utilize the state-of-the-art Go-Explore exploration framework, following the implementation detailed in the PEG paper(Hu et al. (2023)). This approach initially employs a goal-conditioned policy $\\pi^{G}$ to get as close as possible to a specified goal $g$ , a process referred to as the \"Go phase.\" Subsequently, an explorer policy $\\pi^{E}$ is used to further explore the environment starting from the final state of the Go phase, known as the \"Explore phase.\" ", "page_idx": 16}, {"type": "text", "text": "The quality of the trajectories generated by the Go-Explore strategy largely depends on the selection of the goal $g$ during the Go phase. Therefore, establishing an effective mechanism for selecting the Go phase goals is crucial. If the chosen goal $g$ is too simple, the explorer will not sufficiently explore the environment. Conversely, if the goal $g$ is too difficult, the goal-achieving policy $\\pi^{G}$ will fail to approach it effectively. Thus, the baselines MEGA-G and PEG-G employ different goal selection strategies to determine $g$ , guiding the agent to areas with high exploration potential during the Go phase. MEGA-G and PEG-G enhance the agent\u2019s exploration efficiency by crafting robust exploration strategies, enabling faster learning of the world model with respect to new dynamic transitions and environmental areas. We present the pseudocode for Go-Explore in Algorithm 4. ", "page_idx": 16}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/7903bed9270546adca69e5e26ce26511bde8057a0890ee8b36d8113c163abd0f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.2 GC-Dreamer ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "GC-Dreamer is the goal-conditioned version of Dreamer(Hafner et al. (2019a,b, 2020)), without incorporating any exploration or goal-directed strategies. It only uses a goal-conditioned policy to collect trajectories, with goals provided by the training environment. ", "page_idx": 16}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/73088a5ac567ea6d40f2d0c5746e4b39dd5ad5e4af6c51920faf94f8abcefcd6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.3 PEG-G ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "PEG uses a world model to simulate exploration trajectories and evaluates the exploration potential $(P^{E}(g))$ to identify areas worth exploring. ", "page_idx": 16}, {"type": "equation", "text": "$$\nP^{E}(g)=\\mathbb{E}_{p_{\\pi^{G}(\\cdot\\,\\mid\\,,g)(s_{T})}}[V^{E}(s_{T})]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nV^{E}(s_{T})=\\mathbb{E}_{\\pi^{E}}[\\sum_{t=T+1}^{T+T_{E}}\\gamma^{t-T-1}r_{t}^{E}]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "PEG set goal $g$ for the goal-conditioned policy and generalize it to $K$ trajectories using world model. $s_{T}$ denotes the final state of the goal-conditioned trajectory from the \"Go phase\" of Go-Explore to reach the $g$ . Since the objective in Equation 8 is not easily computable, as it relies on the final state distribution induced by the target-conditioned policy $\\pi^{g}$ , which may rapidly change throughout the training process, it\u2019s crucial to use the latest estimates for better exploration. PEG achieve this by leveraging the learned world model. PEG utilize the learned exploration value function $V_{E}(s_{k}^{T})$ (Equation 10) from the learned world model to estimate the exploration value of the final state for each trajectory, and average these estimates. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{\\pi^{G}(\\cdot\\,\\cdot\\,,g)(s_{T})}}[V^{E}(s_{T})]=\\frac{1}{K}\\sum_{k}^{K}V^{E}(s_{T}^{k})\\qquad\\mathrm{where~}s_{T}^{k}\\sim\\hat{p}_{\\pi^{G}(\\cdot\\,\\vert\\cdot,g)(\\tau)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{p}_{\\pi^{G}(\\cdot|\\cdot,g)(\\tau)}=p(s_{0})[\\prod_{t=1}^{T}\\hat{M}(s_{t}|s_{t-1},a_{t-1})\\pi^{G}(a_{t-1}|s_{t-1},g)]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The goals sampled for evaluating this exploration potential metric in PEG are drawn from a distribution updated by the MPPI method (Williams et al. (2015); Nagabandi et al. (2020)). For more details of PEG MPPI update, please refer to the appendix of their paper(Hu et al. (2023)).PEG-G not only uses goals obtained by optimizing Equation 10 to guide exploration sampling but also directly samples trajectories using a goal-conditioned policy with goals provided by the environment. The sampling alternate between these two strategies as shown in the pseudocode in Algorithm 6. ", "page_idx": 17}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/d508d77ca3ae90739400b982741473c78a0c7dffd04c61f930fd6889a6636da2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.4 MEGA-G ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "MEGA (Pitis et al. (2020)) employs kernel density estimates (KDE) to assess state densities and selects goals with low densities from the replay buffer. For the implementation of MEGA, we adopt the model-based MEGA methodology described in the PEG paper without modifications. The PEG paper has illustrated that their adaptation of MEGA outperforms the original MEGA implementation. This entails integrating MEGA\u2019s KDE model and incorporating a goal-conditioned value function into the LEXA framework to fliter goals based on reachability. Similar to PEG-G, MEGA-G switches between utilizing goals from the environment and employing the MEGA goal selection strategy. ", "page_idx": 17}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/18be1176a6b5292f3b3ad76750dbc1ac3d649bef83e1c7b5a153ca34846464f1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "We consider the Time-sample Hindsight Waypoints Sampling Strategy from Hindsight Planner (Lai et al. (2020)) as an alternative to the subgoal selection mechanism in MUN. MUN-noDAD selects subgoals at fixed time intervals along trajectories, providing a simple and effective strategy for defining subgoals. MUN-noDAD can still benefit from the framework of MUN in navigating between different subgoals. The pseudocode for this baseline is as follows: ", "page_idx": 18}, {"type": "text", "text": "Algorithm 8 MUN-noDAD Subgoal Picking Strategy ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1: Function: Subgoals_fixed_interval(...)   \n2: Input: A batch of episodes $\\boldsymbol{B_{e g c}}$ , number of subgoals $N_{s u b g o a l s}$   \n3: $S_{s u b g o a l s}\\leftarrow\\mathfrak{p}$ ick $N_{s u b g o a l s}$ states at fixed time intervals from $B_{e g c}$   \n4: $G_{s u b g o a l s}\\gets\\eta(S_{s u b g o a l s})$   \n5: return Gsubgoals ", "page_idx": 18}, {"type": "text", "text": "Algorithm 9 Trainning Frame for MUN-noDAD ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1: Input: Policy $\\pi^{G}$ , World Model $\\hat{M}$ , reward function $r^{G}$ , subgoals transfer number $N_{s}$ , subgoal   \ntime limit $T_{s}$   \n2: Initialize buffers D, DDAD, Degc   \n3: for $i=1$ to $N_{t r a i n}$ do   \n4: if Should Plan Subgoals then   \n5: $B_{e g c}\\leftarrow\\mathbf{A}$ batch of episodes from $D_{e g c}$   \n6: $G_{s u b g o a l s}\\leftarrow\\mathtt{S}$ ubgoals_fixed_interva $(\\ldots)$ with Algorithm 8   \n7: Initialize empty trajectory $\\tau$   \n8: for $s=1$ to $N_{s}$ do   \n9: $t_{s}=0$   \n10: $g_{s}=S$ ample a subgoal randomly from $G_{s u b g o a l s}$   \n11: while agent has not reached $g_{s}$ and $t_{s}<T_{s}$ do   \n12: Append one step in real environment with $\\pi^{G}$ using goal $g_{s}$ to $\\tau$   \n13: $t_{s}\\gets t_{s}+1$   \n14: $D_{D A D}\\leftarrow D_{D A D}\\cup\\{\\tau\\}$   \n15: $D_{e g c}\\leftarrow D_{e g c}\\cup$ Sample a trajectory with $\\pi^{G}$ using goal from training environment   \n16: $D\\overset{\\cdot}{\\leftarrow}D_{D A\\overset{\\cdot}{D}}\\cup D_{e g c}$   \n17: Update $\\hat{M}$ with D   \n18: Update $\\pi^{G}$ in imagination with $\\hat{M}$ to maximize $r^{G}$ ", "page_idx": 18}, {"type": "text", "text": "E Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/9681c4592cf244669f72f229ae2b90b6e0ab4949b7c479d147719c0405219ef2.jpg", "table_caption": ["E.1 Farthest Point Sampling (FPS) Algorithm "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "The pseudocode presented in the Algorithm 10 illustrate the process of Farthest Point Selection (FPS) algorithm. The FPS algorithm begins by initializing an empty list called \u2019sampled_points\u2019 to store the selected points. The process commences by randomly selecting an initial point from the input point set, denoted as \u2019points\u2019, and adding it to \u2019sampled_points\u2019. Subsequently, \u2019min_distances\u2019 is initialized to keep track of the minimum distance from each point to any of the sampled points, with initial values set to infinity. ", "page_idx": 19}, {"type": "text", "text": "The core procedure involves iteratively selecting points until the desired number of samples is reached. At each iteration, the algorithm identifies the point in \u2019points\u2019 with the maximum minimum distance to the previously sampled points and includes it in \u2019sampled_points\u2019. Concurrently, \u2019min_distances\u2019 is updated to reflect the recalculated minimum distance of each point to any of the sampled points. ", "page_idx": 19}, {"type": "text", "text": "E.2 Runtime ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/a5e17be260c19161c296f18e4da0a1335682e1da689f6979a3ac3d2ca7240732.jpg", "table_caption": ["Table 1: Runtimes per experiment. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "We conduct each experiment on GPU Nvidia A100 and require about 3GB of GPU memory. See table in Table 1 for specific running time of MUN for different task. Most of the runtime is consumed by the neural network updates for the policy and the world model, while the time taken by DAD to filter subgoals is minimal. ", "page_idx": 19}, {"type": "text", "text": "E.3 Hyperparameters ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We use the default hyperparameters of the LEXA backbone MBRL agent (e.g., learning rate, optimizer, network architecture) and keep them consistent across all baselines. MUN primarily requires hyperparameter tuning in the following: 1) the number of candidate subgoals stored $N_{s u b g o a l s}$ ; 2) the number of subgoals used for navigation when sampling in the environment $N_{s}$ ; and 3) the total episode length $L$ and the maximum number of timesteps allocated for navigating to a specific subgoal $T_{s}$ . We show these hyperparameters in Table 2. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/f40ac199c8a3a5c05a7983c450673e4dbab133c7111dcb38771ca9b22f5529ac.jpg", "table_caption": ["Table 2: Hyperparameters of MUN. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "F Additional Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "F.1 More subgoals found by DAD ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "aYqTwcDlCG/tmp/33a41d842d9db9f902c830b2984c588aa53b17e24ed5a15e196b3e3c3a9aaae3.jpg", "img_caption": ["Figure 9: More subgoals found by DAD(Algorithm 2) in all six environments "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "We visualize several subgoals found by the DAD algorithm during the training process in Fig. 9. In Walker, the first five images show that DAD successfully identifies the crucial joint angles and forces of the Walker robot during its forward locomotion, including standing, striding, jumping, landing, and leg support. In the subsequent three images, DAD similarly succeeds in recognizing the key movements of the Walker robot during its backward locomotion. In Ant-Maze, DAD recognizes significant motion variations at corridor corners. In Block Rotation and Pen Rotation, DAD is able to identify crucial finger movements subgoals for rotating objects. In 3-Block Stacking, DAD successfully identifies crucial state transitions required during the stacking process. These critical subgoals include block grasping, lifting, horizontal movement, vertical movement, and gripper release. ", "page_idx": 20}, {"type": "text", "text": "We do the extend navigation experiments on 3-Block Stacking, Ant Maze, and Walker environments to see if the MUN can learn a better world model to navigate to unconstrained goals from unconstrained start state compared to other baselines. In the 3-Block Stacking task, we use a set of 15 goals that represent various critical states in the block-stacking process. These goals serve as candidates for both initial states and endpoint goals, resulting in a total of 225 unique combinations of initial states and endpoint goals for each evaluation episode. For each combination, we conduct 10 repeated evaluations, ultimately computing the average success rate across 2250 evaluation trajectories. Our goal is to assess whether MUN can effectively achieve a random goal when the agent starts from an arbitrary state. This evaluation inherently includes both the forward and reverse processes of stacking blocks, determining whether an agent that can stack blocks is also capable of returning the stacked blocks to an intermediate state. In the Ant Maze environment, we use 32 different positions within the maze as a candidate set for starting and goal positions. Evaluating navigation between these positions allows for a comprehensive assessment of the Ant Robot\u2019s world model learning for the maze structure. This evaluation not only measures its ability to reach the final room but also its capability to return to previous rooms from intermediate positions. We evaluate 1024 combinations of starting and goal positions, conducting 10 evaluations for each combination, resulting in an average success rate computed over 10,240 experiments. In the Walker environment, we use all evaluation goals $(\\pm13,\\pm16,\\pm19,\\pm22,\\pm25,\\pm28)$ as a candidate set for starting and goal positions. This set can form a total of 144 different combinations of starting and goal positions, providing a thorough assessment of the Walker robot\u2019s ability to move forward and backward, as well as its precision in position judgment. See Table 3, 4, 5 for specific results of MUN and all baselines. ", "page_idx": 21}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/38e7f30173040eafc49a236d6c28c0c9cc3d21abfee40f0a7b91ac4d64037464.jpg", "table_caption": ["Table 3: Success rate of navigation experiments on 3-Block Stacking "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/0e333a2be2f69f00c06f28277ca8f0b0e96d719cbc9b608eaa061d4a9bf4667e.jpg", "table_caption": ["Table 4: Success rate of navigation experiments on Ant Maze "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/4fd98a1ad8b4f00967198e27c8b345d50a3edb5e89c38424f3f03e9831c35fa7.jpg", "table_caption": ["Table 5: Success rate of navigation experiments on Walker "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "We observe that MUN significantly outperforms other baselines in navigation experiments across all three environments, demonstrating its exceptional contribution to learning comprehensive world models and policies. ", "page_idx": 22}, {"type": "text", "text": "F.3 World Model Assessment ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Table 6 shows the single-step prediction error of learned world models. We randomly sample $1e4$ state transition tuples $(s_{i},a_{i},s_{i+1})$ within the replay buffers from all of our baselines (MUN, MUNnoDAD, GC-Dreamer, MEGA-G, and PEG-G) to form a validation dataset. Table 6 reports the mean squared error on this dataset. ", "page_idx": 22}, {"type": "text", "text": "Table 7 shows the compounding error (multistep prediction error) of learned world models for evaluation when generating the same length simulated trajectories. More specifically, assume a real trajectory of length $h$ is denoted as $\\left(s_{0},a_{0},s_{1},...,s_{h}\\right)$ . For a learned model $M$ , we sample from $s_{0}$ and generate forward rollouts $\\left(\\hat{s}_{0},a_{0},\\hat{s}_{1},...,\\hat{s}_{h}\\right)$ where $\\hat{s}_{0}=s_{0}$ and for $i\\leq0$ , $\\hat{s}_{i+1}=M(\\hat{s}_{0},a_{i})$ tThhee nm tahxei mcuormr ensupomnbdeirn og f ctoimmepsoteupnsd iinn go eurrr eonr voifr $M$ mise ndtes.f inWeed  eavsa $\\begin{array}{r}{\\frac{1}{h}\\sum_{i=1}^{h}\\left\\|\\hat{s}_{i}-s_{i}\\right\\|_{2}^{2}}\\end{array}$ . diWneg  spert $h$ i tcoti obne error of the learned world models by generating 500 trajectories for each benchmark, simulated on both the models and the real environments. ", "page_idx": 22}, {"type": "text", "text": "In Tables 6 and 7, we used the final world models trained by all methods after the same number of environment interaction steps. These results provide a quantitative comparison of the world model prediction quality between MUN and the baselines across our benchmarks. The world models trained by MUN show a much smaller generalization gap to the real environment compared to goal-conditioned Dreamer (and the other baselines). Consequently, MUN can effectively leverage these world models to train control policies that generalize well to the real environment. This explains the superior task success rates of MUN compared to the baselines in our experiment. Fig 10 also provides more information about the world model compound prediction error. ", "page_idx": 22}, {"type": "image", "img_path": "aYqTwcDlCG/tmp/8c7274217b7afc3945d4bba8f1e2e1ac536ef001f8fec36851710a732d512867.jpg", "img_caption": ["(a) Trajectories 3-Block Stacking "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "aYqTwcDlCG/tmp/79fc086548f8274a74f03399e0227a673749172e2b428ec4a6e72d73b3fe3d1b.jpg", "img_caption": ["(b) Trajectories Block Rotation "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 10: Fig(a) and Fig(b) illustrate the imagined and real environment trajectories for 3-Block Stacking and Block Rotation respectively, starting from the same initial state. Among the baselines, MUN demonstrates the smallest compound model error with respect to the ground truth trajectories. The X-axis represents the trajectory steps. In Fig(a), the Y-axis represents the sum of the heights of the three blocks. MUN\u2019s world model outperforms other methods in predicting the correct locations of the three blocks. In Fig(b), the Y-axis represents the position of the block in the x coordinate. MUN\u2019s world model outperforms other methods in predicting the correct position of the block. ", "page_idx": 22}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/5503db3009a83718d4d0a0f6f4961681114de6a9c6b7bd5e358fd0cc5fc8fc5f.jpg", "table_caption": ["Table 6: One-step Model Prediction Error. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "aYqTwcDlCG/tmp/9ab866b3657a6c148c72b5fb3285b695f48e06f43e8b150dea1cad96c68eb0e9.jpg", "table_caption": ["Table 7: Compound Model Prediction Error. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our abstract and introduction accurately represent the primary contributions of the paper, which include the development of the MUN algorithm designed to improve goal-conditioned reinforcement learning through enhanced world modeling and exploration capabilities. The introduction outlines the key challenges in GCRL, specifically with sparse rewards, and how MUN addresses these by facilitating effective state transitions between arbitrary subgoal states in the replay buffer. These claims are well-supported by the theoretical underpinnings and experimental results presented in the paper, reflecting the scope and impact of the proposed method. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper thoroughly discusses the limitations of the MUN framework in Appendix. We highlight the dependency on an efficient strategy for discovering key states, pointing out that while DAD is effective, it will also identify irrelevant states in tasks with complex action spaces or weak correlations between goal space and action space. Additionally, we mention the potential for applying MUN to model-free reinforcement learning methods, which do not require learning a world model and have simpler architectures. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 24}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: In the Experiment section and the Appendix of our paper, we provide a detailed description of our experimental procedures and configurations. This includes all sources and modifications of the test environments, pseudocode and implementation methods for all baselines, the equipment and memory used, as well as the specific values of the required hyperparameters. Additionally, we have open-sourced our code, which can be found in the Reproducibility Statement section. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: In the Experiment section and appendix of our paper, we elaborate on the procedure and configuration of our experiments. This includes the sources and modifications of all testing environments, pseudo code and implementation methods for all baselines, the devices and memory utilized, as well as specific values of hyperparameters employed. Concurrently, we have open-sourced our code; please refer to the Reproducibility Statement section for further details. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submis  \nsions to provide some reasonable avenue for reproducibility, which may depend on the   \nnature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: As we mentioned in the previous justification, we have not only open-sourced our code but also provided detailed steps and settings for reproducing our main experimental results. In the Experiment section and Appendix, we elaborate on the sources and modifications of the environments, baseline implementation details, and MUN implementation specifics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We provide comprehensive details regarding the hyperparameters essential for understanding the experiments, including those specific to our MUN framework. The table presented (Fig 2) outlines these hyperparameters for each task, facilitating reproducibility and comparison. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We conducted each experiment a minimum of five times using different random seeds, and upon plotting the results, as demonstrated in the Experiment section, we incorporated the experimental error. The solid line denotes the average success rate, while the shaded region signifies the standard deviation among the repeated experimental outcomes. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We clearly specifies the computer resources (Nvidia A100 GPU) and the amount of GPU memory required (approximately 3GB). Additionally, we provides detailed information on the runtime of each experiment in Appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The research conducted in our paper aligns with the NeurIPS Code of Ethics. We have thoroughly reviewed the guidelines and ensured that our research adheres to ethical standards. Additionally, we have implemented measures to safeguard anonymity and comply with pertinent laws and regulations. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our research aims to address the exploration problem in Reinforcement Learning (RL) within the GCRL environment. It is currently in the theoretical research stage and has minimal societal impact. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper properly credits the creators or original owners of assets used, including code, data, and models. The licenses and terms of use are explicitly respected. Specifically, we cite the original papers for code packages or datasets used, state the version of the assets, and include URLs where possible. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We have documented our code and provided detailed instructions on its usage, licenses, and permissible scope of use. Additionally, we have included the documentation alongside the assets to ensure accessibility and clarity for users. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our paper not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]