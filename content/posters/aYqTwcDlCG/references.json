{"references": [{"fullname_first_author": "Andrychowicz", "paper_title": "Hindsight experience replay", "publication_date": "2017-00-00", "reason": "This paper introduces Hindsight Experience Replay, a crucial technique for improving sample efficiency in goal-conditioned reinforcement learning, which is directly relevant to the core methodology of the target paper."}, {"fullname_first_author": "Hafner", "paper_title": "Dream to control: Learning behaviors by latent imagination", "publication_date": "2019-12-01", "reason": "This paper introduces Dreamer, a model-based reinforcement learning algorithm that uses a world model to generate imagined rollouts for planning and learning, forming the foundation for the model-based approach used in the target paper."}, {"fullname_first_author": "Ecoffet", "paper_title": "Go-explore: a new approach for hard-exploration problems", "publication_date": "2019-01-00", "reason": "This paper introduces Go-Explore, a popular exploration strategy for reinforcement learning in sparse-reward environments; the target paper compares its performance against this method."}, {"fullname_first_author": "Mendonca", "paper_title": "Discovering and achieving goals via world models", "publication_date": "2021-00-00", "reason": "This paper introduces LEXA, a goal-directed exploration algorithm that uses a temporal distance network and world models for exploration; the target paper extends this approach and compares against it."}, {"fullname_first_author": "Pitis", "paper_title": "Maximum entropy gain exploration for long horizon multi-goal reinforcement learning", "publication_date": "2020-00-00", "reason": "This paper introduces MEGA, an exploration strategy using maximum entropy gain for multi-goal reinforcement learning; the target paper compares against this method."}]}