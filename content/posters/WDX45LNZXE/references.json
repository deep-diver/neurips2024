{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation of the models studied in this paper."}, {"fullname_first_author": "Shivam Garg", "paper_title": "What can transformers learn in-context? A case study of simple function classes", "publication_date": "2022-12-01", "reason": "This paper established a well-defined framework to analyze transformers' in-context learning ability, providing a theoretical lens used in this paper."}, {"fullname_first_author": "Ruiqi Zhang", "paper_title": "Trained transformers learn linear models in-context", "publication_date": "2023-06-01", "reason": "This paper provided a theoretical analysis on the capability of transformers to learn linear models in context, paving a way for the non-parametric analysis in this paper."}, {"fullname_first_author": "Yu Huang", "paper_title": "In-context convergence of transformers", "publication_date": "2023-10-01", "reason": "This paper analyzed the convergence of transformers trained with gradient descent under in-context learning, crucial for the convergence analysis of the current study."}, {"fullname_first_author": "Siyu Chen", "paper_title": "Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality", "publication_date": "2024-02-01", "reason": "This paper studied the optimization dynamics of softmax attention layers in transformers during in-context learning, directly related to the single-layer analysis of this paper."}]}