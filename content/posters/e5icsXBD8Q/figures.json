[{"figure_path": "e5icsXBD8Q/figures/figures_1_1.jpg", "caption": "Figure 1: Using embedding-corrupted prompts to maintain an unlearned state on the LLM subject to unlearning. We first employ a classifier to identify whether the incoming prompt falls within the scope of the unlearning target. We construct embedding-corrupted prompts by selectively corrupting dimensions within the tokens' embeddings. The corruption parameter is learned offline via zeroth order optimization. An unlearned state is imposed during inference and does not require any updates to the original model's weights.", "description": "This figure illustrates the ECO (Embedding-Corrupted) Prompts framework for large language model unlearning.  It shows a two-step process: First, a prompt classifier determines if an incoming prompt is within the scope of knowledge to be forgotten. Second, if the classifier flags the prompt for unlearning, the prompt embedding is corrupted using a learned corruption function (parameterized by \u03c3k) before being fed into the model. This corruption is learned offline via zeroth-order optimization.  The figure highlights that this process does not involve updating the original LLM weights; instead, it modifies the input at inference time to achieve the unlearning effect. The difference between the original output and the unlearned output is displayed to emphasize the result of applying the corrupted prompt.", "section": "3 ECO: Unlearned LLMs via Embedding-Corrupted Prompts"}, {"figure_path": "e5icsXBD8Q/figures/figures_6_1.jpg", "caption": "Figure 2: Model utility versus forget quality (p-value) on three different forget set sizes of the TOFU dataset after unlearning. We show two models, Phi-1.5 (top) and Llama-2-7B-Chat (bottom). For GA, GD, KL, PO, and the prompting baseline, the forget qualities are either too small or come at the cost of a substantial decrease in model utility. Negative preference optimization (NPO) [149] variants achieve a good balance in some cases, but the trade-off in model utility is still non-trivial. ECO-RN (random noise) and ECO-ZO (zero-out) achieve an almost identical distribution to the retained model while incurring no sacrifice in model utility.", "description": "This figure shows the trade-off between model utility and forget quality for two different LLMs (Phi-1.5 and Llama-2-7B-Chat) after unlearning different percentages (1%, 5%, and 10%) of the TOFU dataset.  It compares the performance of the ECO method to several baseline unlearning methods (GA, GD, KL, PO, Prompting, NPO, NPO-KL, NPO-RT). The plot demonstrates that ECO achieves high forget quality with no loss in model utility, unlike most baseline methods which either fail to forget sufficiently or suffer significant utility loss.", "section": "4.2 Entity Unlearning"}, {"figure_path": "e5icsXBD8Q/figures/figures_21_1.jpg", "caption": "Figure 1: Using embedding-corrupted prompts to maintain an unlearned state on the LLM subject to unlearning. We first employ a classifier to identify whether the incoming prompt falls within the scope of the unlearning target. We construct embedding-corrupted prompts by selectively corrupting dimensions within the tokens' embeddings. The corruption parameter is learned offline via zeroth order optimization. An unlearned state is imposed during inference and does not require any updates to the original model's weights.", "description": "This figure illustrates the ECO framework for large language model unlearning.  It shows how a prompt classifier first identifies whether a given prompt should be forgotten (i.e., falls within the scope of the unlearning target).  If it should be forgotten, the framework selectively corrupts dimensions within the tokens' embeddings using a corruption function learned offline (through zeroth-order optimization). This corruption results in an output resembling the model's response had it never been trained on the data to be forgotten. Crucially, this method does not require updating the LLM's weights, just manipulating inputs during the inference stage.", "section": "3 ECO: Unlearned LLMs via Embedding-Corrupted Prompts"}, {"figure_path": "e5icsXBD8Q/figures/figures_29_1.jpg", "caption": "Figure 1: Using embedding-corrupted prompts to maintain an unlearned state on the LLM subject to unlearning. We first employ a classifier to identify whether the incoming prompt falls within the scope of the unlearning target. We construct embedding-corrupted prompts by selectively corrupting dimensions within the tokens' embeddings. The corruption parameter is learned offline via zeroth order optimization. An unlearned state is imposed during inference and does not require any updates to the original model's weights.", "description": "This figure illustrates the ECO prompt unlearning framework.  It shows how a classifier determines if a prompt should be unlearned, and if so, how corruption is applied to the prompt embedding before input to the LLM.  This corruption prevents the model from recalling information without requiring any changes to its internal weights.", "section": "3 ECO: Unlearned LLMs via Embedding-Corrupted Prompts"}, {"figure_path": "e5icsXBD8Q/figures/figures_40_1.jpg", "caption": "Figure 4: The number of parameters of the model subject to unlearning versus the average performance on WMDP benchmark and MMLU subsets. This figure is a visualization of the forget set accuracy in Table 21 and Table 22.", "description": "This figure shows the relationship between the number of parameters in a language model and its performance on two benchmarks (WMDP and MMLU) after applying an unlearning technique. The x-axis represents the number of parameters, and the y-axis shows the average accuracy.  It visualizes data from Tables 21 and 22, demonstrating how the unlearning method's effectiveness changes as model size increases.", "section": "4 Experiments"}, {"figure_path": "e5icsXBD8Q/figures/figures_40_2.jpg", "caption": "Figure 3: Probing results based on model output logits before and after unlearning on the WMDP dataset via ECO. The linear probes' accuracy remains at random chance for all three models, regardless of their size and performance. This indicates that ECO is resistant against linear probes trained on the raw output logits, indicating that the corrupted prompts effectively guard against the risk of inferring the correct answer from the logits.", "description": "This figure shows the results of probing experiments using linear probes trained on the logits of three different LLMs: Zephyr-7B, Yi-34B-Chat, and Mixtral-8x7B-Instruct.  The goal was to assess the model's ability to prevent knowledge recovery after applying ECO (Embedding-Corrupted Prompts) unlearning. The x-axis represents the three sub-categories of the WMDP (Winning at Machine Deception Prediction) benchmark dataset (Bio, Chem, Cyber). The y-axis represents the accuracy of the linear probe.  The dashed line indicates random chance.  The figure demonstrates that before unlearning (Original), the probes achieve relatively high accuracy. After unlearning with ECO (Unlearned), however, the accuracy drops to near random chance, indicating that ECO successfully prevents the extraction of sensitive information directly from the model's logits.", "section": "4.2 Entity Unlearning"}, {"figure_path": "e5icsXBD8Q/figures/figures_55_1.jpg", "caption": "Figure 1: Using embedding-corrupted prompts to maintain an unlearned state on the LLM subject to unlearning. We first employ a classifier to identify whether the incoming prompt falls within the scope of the unlearning target. We construct embedding-corrupted prompts by selectively corrupting dimensions within the tokens' embeddings. The corruption parameter is learned offline via zeroth order optimization. An unlearned state is imposed during inference and does not require any updates to the original model's weights.", "description": "This figure illustrates the ECO (Embedding-Corrupted) prompts method for unlearning in LLMs.  It shows a two-step process: first, a classifier determines if an incoming prompt is related to the target knowledge to be forgotten; second, if it is, the prompt's embedding is corrupted using a learned corruption function before being fed to the LLM. This corruption is learned offline and doesn't require any changes to the LLM's weights, creating an 'unlearned state' during inference.  The figure visually depicts the flow of the prompt through the classifier, the corruption function, and finally into the LLM to generate an unlearned output.", "section": "3 ECO: Unlearned LLMs via Embedding-Corrupted Prompts"}]