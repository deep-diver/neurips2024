{"references": [{"fullname_first_author": "Alekh Agarwal", "paper_title": "On the theory of policy gradient methods: Optimality, approximation, and distribution shift", "publication_date": "2021-00-00", "reason": "This paper provides the theoretical foundation for the optimality of policy gradient methods, which is crucial to the analysis in the current paper."}, {"fullname_first_author": "Jalaj Bhandari", "paper_title": "Global optimality guarantees for policy gradient methods", "publication_date": "2024-00-00", "reason": "This paper establishes global optimality guarantees for policy gradient methods, which is highly relevant to the analysis of global convergence in the current paper."}, {"fullname_first_author": "Audrey Huang", "paper_title": "Reinforcement Learning in Low-Rank MDPs with Density Features", "publication_date": "2023-00-00", "reason": "This paper explores reinforcement learning in low-rank MDPs, which is a related setting with structural assumptions that allow for more efficient algorithms."}, {"fullname_first_author": "Philip Amortila", "paper_title": "Scalable Online Exploration via Coverability", "publication_date": "2024-00-00", "reason": "This paper addresses online exploration, a key aspect of reinforcement learning that is directly relevant to the exploration strategies considered in the current work."}, {"fullname_first_author": "Jinglin Chen", "paper_title": "Offline reinforcement learning under value and density-ratio realizability: the power of gaps", "publication_date": "2022-00-00", "reason": "This paper investigates offline reinforcement learning, which is a closely related setting that is particularly relevant to the offline policy gradient algorithms discussed in the current paper."}]}