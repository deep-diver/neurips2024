[{"type": "text", "text": "$S E(3)$ Equivariant Ray Embeddings for Implicit Multi-View Depth Estimation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yinshuang Xu Dian Chen Katherine Liu University of Pennsylvania Toyota Research Institute Toyota Research Institute xuyin@seas.upenn.edu dian.chen@tri.global katherine.liu@tri.global ", "page_idx": 0}, {"type": "text", "text": "Sergey Zakharov Toyota Research Institute sergey.zakharov@tri.global ", "page_idx": 0}, {"type": "text", "text": "Rares Ambrus Toyota Research Institute ares.ambrus@tri.global ", "page_idx": 0}, {"type": "text", "text": "Kostas Daniilidis University of Pennsylvania kostas@cis.upenn.edu ", "page_idx": 0}, {"type": "text", "text": "Vitor Guizilini Toyota Research Institute vitor.guizilini@tri.global ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Incorporating inductive bias by embedding geometric entities (such as rays) as input has proven successful in multi-view learning. However, the methods adopting this technique typically lack equivariance, which is crucial for effective 3D learning. Equivariance serves as a valuable inductive prior, aiding in the generation of robust multi-view features for 3D scene understanding. In this paper, we explore the application of equivariant multi-view learning to depth estimation, not only recognizing its significance for computer vision and robotics but also addressing the limitations of previous research. Most prior studies have either overlooked equivariance in this setting or achieved only approximate equivariance through data augmentation, which often leads to inconsistencies across different reference frames. To address this issue, we propose to embed $S E(3)$ equivariance into the Perceiver IO architecture. We employ Spherical Harmonics for positional encoding to ensure 3D rotation equivariance, and develop a specialized equivariant encoder and decoder within the Perceiver IO architecture. To validate our model, we applied it to the task of stereo depth estimation, achieving state of the art results on real-world datasets without explicit geometric constraints or extensive data augmentation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Equivariance is a valuable property in computer vision, leveraging various symmetries to reduce sample and model complexity while boosting generalization. It has seen broad application in fields such as 3D shape analysis [48, 52, 13], panoramic image prediction [10, 54, 19], and robotics [46, 25, 42, 2]. In particular, there is an increasing interest in equivariant scene representation from multiple viewpoints [43, 55], as the multi-view setting is a fundamental challenge in the field and equivariant representations are desirable for their robustness and efficiency. ", "page_idx": 0}, {"type": "text", "text": "Meanwhile, multi-view depth estimation has always been a core topic in computer vision. Previous works [27, 31, 26] leverage the explicit geometric constraint to construct the feature cost-volume for depth prediction. Recently, the paradigm of combining implicit representations with generalist architectures has been widely adopted and gaining success. Inserting inductive bias via the embedding of geometric entities (rays) in the multi-view setting [58, 47, 44] has become popular. Notably, in multi-view depth estimation, Yifan et al. [57] effectively combined geometric epipolar embeddings with image features for stereo depth estimation, outperforming traditional methods that depend on explicit geometric constraints. State-of-the-art work by [23] integrated multi-view geometric embeddings with image features for video depth estimation. These methods show that the implicit multi-view geometry learned by the Perceiver IO architecture, which is a more efficient general architecture compared to the vision transformer [14], can improve upon approaches that rely on traditional explicit geometric constraints, such as cost volumes, bundle adjustment, and projective geometry. However, the implicit multi-view geometry promoted by the Perceiver IO architecture lacks equivariance. This limitation becomes apparent when transforming the coordinate frame representing input geometry, such as camera poses, ray directions, or 3D coordinates. These transformations change the input in such a way that non-equivariant architectures are unable to achieve the same results, as shown in Figure 1. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Although [23] have tried to approximate equivariance through extensive data augmentation, achieving true equivariance at an architectural level remains an ongoing challenge. In this paper, we propose to embed equivariance with respect to $S E(3)$ transformation of the global coordinate frame, i.e., gauge equivariance, to the Perceiver IO model. We substitute traditional Fourier positional encodings for the ray embedding with Spherical Harmonics, which are more suitable to represent 3D rotations. We customdevelop a $S\\bar{E}(3)$ equivariant attention module to seamlessly interact with different types of equivariant features. This is achieved using a combination of invariant attention weights and equivariant fundamental layers. During the decoding stage, this equivariant latent space is disentangled into the equivariant frame and invariant global features. Our approach not only simplifies the integration of existing modules ", "page_idx": 1}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/e0de2020a03fcf739776c01beaedacba3e0ee77d919d3167f4ee0d1a96f222aa.jpg", "img_caption": ["Figure 1: Given a sparse set of posed images (red), the task is to estimate depth for a novel viewpoint (blue). The Perceiver IO struggles to accurately predict depth when the reference frame (gray) changes, equivalent to an inverse transformation applied to the object and cameras. In contrast, our model delivers the consistent result due to its equivariant design. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "without requiring a specialized design, but also allows the network to focus on effective scene analysis via an invariant latent space, reducing the effects of global transformations. The equivariant frame is used to \u201cstandardize\u201d the query ray, serving as an invariant input for the decoder. This method ensures that both sets of inputs for the decoder are invariant, leading to an invariant output regardless of the decoder used. Consequently, we can employ the conventional Perceiver IO decoder in our equivariant framework. In summary, our key contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We integrate $S E(3)$ equivariance into a multi-view depth estimation model by design, using spherical harmonics as positional encodings for ray embeddings, as well as a specialized equivariant encoder.   \n\u2022 By leveraging the equivariant learned latent space, we introduce a novel scene representation scheme for multi-view settings, featuring a disentangled equivariance frame and an invariant scene representation.   \n\u2022 We assess our model\u2019s ability to learn 3D structures through wide-baseline stereo depth estimation. Our model delivers state-of-the-art results in this task, significantly surpassing the non-equivariant baseline. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Equivariant Networks Equivariant Networks are garnering interest in vision for their efficiency and powerful inductive bias. These networks can be categorized by the data structures over which they operate, spanning 2D images [15, 39], graphs [45, 38], 3D point clouds [60, 5], manifolds [9, 37], and spherical images [17, 7]. From an architectural perspective, methods can also be classified by the tools they rely on, such as group convolution [8, 16, 36, 20], steerable convolution on homogeneous spaces [53, 51, 52, 11, 54], and recently transformers [41, 40, 33, 24]. In the context of this paper, we highlight significant $S E(3)$ equivariant transformer works. Fuchs et al. [22] first introduced an SE(3) equivariant transformer for point clouds, using steerable kernels for transformers and focusing on local features in point clouds. Liao and Smidt [33], Liao et al. [34] adopted a message-passing architecture for 3D equivariant transformers in point clouds. Xu et al. [55] applied similar techniques for ray space. Our approach differs by using direct input-level positional encodings, rather than modifying the kernel with relative poses. We learn a global, non-hierarchical representation. Safin et al. [43] inserts pairwise relative poses in self-attention with a conventional attention module, requiring quadratic computation and lacking compact scene representation, unlike our method. Closely related to our work, Assaad et al. [1] proposed vector neuron transformers embedded in the Perceiver IO encoder for point clouds. However, they replace the original latent array with a learnable transformation, did not use spherical harmonics for equivariant positional encoding, or design an equivariant decoder within the Perceiver IO framework. We treat the original latent array as invariant, and learn a disentangled representation for the decoder with versatile queries. Esteves et al. [18] uses spherical harmonics for positional encoding, primarily to enhance spherical function learning, not for equivariance. ", "page_idx": 1}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/67568b0dbf1dc7a3096ff22cebe9cfd8d84ade9ae3fa298ba55ea7a5b0c52a32.jpg", "img_caption": ["Figure 2: Our proposed Equivariant Perceiver IO (EPIO) architecture. (a) We take as input the concatenation of per-pixel image, ray, and camera embeddings, the latter two calculated using spherical harmonics. (b) The output of our equivariant encoder is a global latent code, including both global invariant and equivariant components. From those, we extract an equivariant reference frame through an equivariant MLP, while simultaneously obtaining invariant latents through inner product. (c) When a query camera is positioned in this equivariant reference frame, its pose becomes invariant, which enables the use of conventional Fourier basis to encode it. (d) Given an invariant latent and invariant pose, we use a conventional Perceiver IO decoder to generate predictions for each query ray. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Implicit Multi-View Geometry Even in the age of deep learning, traditional multi-view stereo methods like COLMAP [21] are still widely used for structure-from-motion. These methods are accurate but slow due to complex post-processing steps. To speed things up while maintaining accuracy, learning-based methods adapt traditional cost volume-based techniques for depth estimation [29, 3, 26, 27]. Recently, transformers [50] have become prevalent approaches, replacing CNNs in terms of popularity and performance. The Stereo Transformer [32] replaces cost volumes with an attention-based matching procedure inspired by sequence modeling. IIB [57] leverages Perceiver IO [28] for generalized stereo estimation by incorporating the epipolar geometric bias into the model. Liu et al. [35], Chen et al. [4] inject 3D geometry into the transformer akin to IIB for object detection, while Chen et al. [4] learns equivariance in a data-driven way. A closely related study to ours is DeFiNe [23], in which camera information is incorporated into Perceiver IO and used to decode predictions from arbitrary viewpoints. However, their approach relies on data augmentation to approximate equivariance in the Perceiver IO, whereas our design inherently ensures equivariance at an architectural level. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section we start with some preliminaries about Perceiver IO and our baseline, Depth Field Networks (DeFiNe) [23], a state-of-the-art method integrating camera geometries into Perceiver IO for multi-view depth estimation. We then outline the concept of equivariance in multi-view scenarios in Section 3.2. Given these preliminaries, in Section 3.3 we delve into the details of our proposed equivariant positional encoding for rays, in Section 3.4 we elaborate on the attention mechanisms used in our model, and in Section 3.5 we describe our choice of encoder parameterization. Finally, in Section 3.6 we describe our decoder procedure, focusing on the task of depth estimation. The pipeline of our proposed $S E(3)$ equivariant model in multi-view context is shown in Figure 2. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries: Input-level Inductive Biases to Perceiver IO ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The Perceiver IO [28] is a generalist transformer architecture that encodes input data $\\mathcal{T}\\in\\mathbb{R}^{N_{i}\\times C_{i}}$ into a latent space $\\mathcal{R}\\in\\mathbb{R}^{\\breve{N}_{R}\\times C_{R}}$ by cross-attending $\\mathcal{T}$ with $\\mathcal{R}$ . Further refinement of this latent space $\\mathcal{R}$ is achieved using self-attention layers, followed by cross-attention to decode predictions $\\dot{\\mathcal{O}}\\in\\mathbb{R}^{N_{o}\\times C_{o}}$ using queries $\\mathcal{Q}\\in\\mathbb{R}^{N_{o}\\times C_{q}}$ . Many works exploit its generic nature by introducing inductive biases at an input level, namely, providing prior knowledge about the data for implicit rfeoar stohnei nmg.u ltSip-eviceifwic aplrloy,b lDeemF.i NGei v[e2n3 ] $N$ siems acgaems $\\{I_{i}\\}_{i=1}^{N}$ fertroiems  at os ecto nosft rcuacmt e3rDa s pwosiitthi opnoasl ees $\\{T_{i}\\}_{i=1}^{N}$ and intrinsics $\\{K_{i}\\}_{i=1}^{N}$ , DeFiNe calculates 3D rays $\\{r_{u v}^{i}\\}_{u v=(1,1)}^{(H,W)}$ from each camera center $t_{i}$ to each pixel $(u,v)$ on image , and obtains positional encodings $P E(r_{u v}^{i},t_{i})$ with a mapping $P E(\\cdot)$ . These positional encodings are combined with image embeddings $\\mathcal{F}=\\{f_{u v}^{i}\\}$ from a visual feature extractor to be encoded by $\\mathcal{R}$ such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{1}=\\mathrm{cross-attn}(\\mathcal{R}_{0},\\{f_{u v}^{i}\\oplus P E(r_{u v}^{i},t_{i})\\})}\\\\ &{\\mathcal{R}_{k}=\\mathrm{self-attn}(\\mathcal{R}_{k-1}),\\quad k=2,\\ldots,K}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To obtain predictions for a set of $M$ novel viewpoints, we can similarly calculate 3D query rays from poses $\\{T_{j}^{\\prime}\\}_{j=1}^{M}$ and intrinsics $\\{K_{j}\\}_{j=1}^{M}$ and map them to query positional encodings $\\mathcal{Q}~=~\\{P E(r_{u v}^{j},t_{j}\\}\\}$ , which will be used to decode the latent space $\\mathcal{R}$ via cross attention: $\\mathcal{O}\\,=\\,\\mathrm{cross}\u2013\\mathrm{attn}(\\mathcal{Q},\\mathcal{R}_{K})$ . In this way, prior knowledge, i.e., 3D camera geometries, is directly fed into the model as additional input features for the implicit learning of multi-view geometry. ", "page_idx": 3}, {"type": "text", "text": "3.2 Equivariance Definition in Multiview Context ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "After introducing the input-level inductive bias framework, it is worth noting that the poses of the encoding cameras, as well as the query viewpoints, are defined in a global reference frame $T_{\\mathrm{G}}$ . However, this choice of global reference frame is subject to change, and the property of equivariance ensures that predictions remain identical under these changes. Assuming the global reference frame undergoes a transform $T^{-1}\\,\\in\\,S E(3)$ to $T_{\\mathrm{G}}^{\\prime}\\,=\\,T^{-1}\\bar{T_{\\mathrm{G}}}$ , the ray representations become $(R r_{u v}^{i(j)},R t_{i(j)}+t)$ when representing $T=(R,t)$ . The equivariant model $\\Phi$ should satisfy ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi(\\{f_{u v}^{i}\\oplus P E(R r_{u v}^{i},R t_{i}+t)\\},\\{P E(R r_{u v}^{j},R t_{j}+t)\\})}\\\\ &{\\ =\\Phi(\\{f_{u v}^{i}\\oplus P E(r_{u v}^{i},t_{i})\\},\\{P E(r_{u v}^{j},t_{j})\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For further details on the definition of the equivariance, please see Appendix A.2. ", "page_idx": 3}, {"type": "text", "text": "3.3 Equivariant Positional Encoding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To ensure the positional encoding process is equivariant w.r.t a transformation group $G$ , we would like to enforce that $\\Phi(\\cdot,P E(\\rho_{g}^{\\bar{\\mathcal{X}}}\\dot{x}))\\;=\\;\\Phi(\\cdot,\\rho_{g}^{\\bar{\\mathcal{Y}}}P E(x))$ for any $g\\ \\in\\ G$ , where $\\rho^{\\hat{\\mathcal X}}$ is the group representation on coordinate space, and $\\rho^{y}$ is the group representation on the positional encoding space. The traditional Fourier basis is translationally equivariant, as detailed in Appendix B. Kitaev et al. [30] used this to attain translational equivariance, employing a conjugate product for invariant attention. However, this approach lacks rotational equivariance. ", "page_idx": 3}, {"type": "text", "text": "This raises a key question: Are there any basis functions equivariant to both 3D translations and rotations? Unfortunately, none exist. However, a common method in equivariant works for translational equivariance is to subtract the center point, a technique we apply in our context as illustrated in Figure 3. This enables translational invariance, leaving the model to focus solely on achieving rotational equivariance. To address the 3D rotational equivariance, we turn to spherical harmonics (SPH), known for their inherent rotation-equivariant properties. They offer a way to accommodate 3D rotational changes, thereby achieving $S E(3)$ equivariance. ", "page_idx": 3}, {"type": "text", "text": "3.3.1 Spherical Harmonics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We provide a detailed introduction to Spherical Harmonics in Appendix A.4, where we also discuss how their application in previous equivariant transformers differs from their use in our model. Below, we present a brief overview of Spherical Harmonics for clarity. Similar to the varying frequencies of sines and cosines in Fourier series, spherical harmonics are characterized by different degrees (orders), denoted as $l\\in\\mathbb{N}$ . An order- $l$ spherical harmonics, denoted as $Y^{l}:\\mathbb{R}^{3^{\\bullet}}\\!\\rightarrow\\mathbb{R}^{2l+1}$ , follows the transformation rule: $Y^{l}(R r)=D^{l}(\\dot{R})Y^{l}(r),Y^{l}(r)=\\|r\\|^{l}Y^{l}(\\hat{r})$ , where $R\\in S O(3)$ , $\\hat{r}$ is the unit vector, $D^{l}:S O(3)\\rightarrow\\mathbb{R}^{(2l+1)\\times(2l+1)}$ is called the Wigner-D matrix, serving as the irreducible representation of $S O(3)$ corresponding to the order $l$ . The Wigner-D matrix is an orthogonal matrix, that is $D^{l}(R)(D^{l}(R))^{\\bar{T}}=I$ . These important properties allow us to achieve equivariance in the Perceiver IO transformer architecture. ", "page_idx": 3}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/a723354be9fbfff8c2a41c00f462427863807e8d72f096f189a62906a16b8c10.jpg", "img_caption": ["Figure 3: Comparison between an equivariant input embedding in our model (left) and the conventional input embedding in DeFiNe (right). (a) Pipeline used to generate input embeddings for the encoder, resulting in cross-attention keys and values. (b) To generate geometric information, we calculate embeddings for each ray $r_{u v}^{i}$ and camera relative position $t_{i}-\\bar{t}$ ; (c) The final composed embedding format includes both image embeddings, which are invariant, and geometric embeddings, which are equivariant. In contrast, the conventional approach by Perceiver IO, as highlighted in parts (a) and (c), integrates Fourier positional encodings with image embeddings to form the input embeddings. Furthermore, as indicated in (b), Perceiver IO utilizes each ray $r_{u v}^{i}$ and the absolute translation $t_{i}$ for positional encoding purposes. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.3.2 Equivariant Hidden features ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In our model, we embed both camera centers and viewing rays using spherical harmonics. The embedding is given by: $P E(r_{u v}^{i},t_{i})=\\bigoplus_{l\\in L}(Y^{l}(r_{u v}^{i})\\oplus\\check{Y^{l}}(t_{i}^{'}-\\bar{t}))$ , where each part corresponds to the same order of spherical harmonics  (Figure 3). Here, $L=\\{1,2,\\dots,l_{m a x}\\}$ and $\\begin{array}{r}{\\bar{t}=\\frac{1}{N}\\sum_{i=1}^{N}t_{i}}\\end{array}$ highlighting the extraction of the cameras\u2019 central position for translational invariance. Due to the properties of spherical harmonics, the positional encoding of transformed input, $P E(R r_{u v}^{i},R t_{i}+t)$ , is equal to $\\begin{array}{r}{\\bigoplus_{l\\in L}(D^{l}(R)(Y^{l}(r_{u v}^{i})\\oplus Y^{l}(t_{i}-\\bar{t})))=R\\cdot P E(r_{u v}^{i},t_{i})}\\end{array}$ , for any rotation $R\\in S O(3)$ and translation $t\\in\\mathbb{R}^{3}$ . In other words, it guarantees that these embeddings are both rotationally equivariant and translationally invariant. The transformation of these embeddings operates by multiplying each block with its respective Wigner-D matrix. ", "page_idx": 4}, {"type": "text", "text": "The image remains unchanged when the reference frame is transformed, as its contents are unaffected. Mathematically, this property is akin to multiplying by a 0-order Wigner Matrix, equivalent to an identity. Thus, combiniing image fieatures with positio0nal enicodings (Figiure 3) transform as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(\\widehat{f}_{u v}^{i},\\widehat{P E}(\\widehat{R}r_{u v}^{i},\\bar{R}t_{i}+\\bar{t}))\\overset{\\leftarrow}{=}\\left(\\bar{D^{0}(R)}\\widehat{f}_{u v}^{i},\\widehat{R\\cdot P E}(\\widehat{r}_{u v}^{i},t_{i})\\right)}&{}\\\\ {=R\\cdot(f_{u v}^{i},P E(r_{u v}^{i},t_{i}))}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In our model, the equivariant hidden features mirror the structure of our embeddings, composed as $\\bigoplus_{l\\in L}H_{l}$ with subscripts indicating the feature type and $L=\\{0,1,\\cdot\\cdot\\cdot,l_{m a x}\\}$ . The size for each feature type $H_{l}$ follows $(2l+1,C_{l})$ , where $2l+1$ is the intrinsic dimension and $C_{l}$ is the number of channels. For more a intuitive understanding, we visualize these embeddings in Appendix C (Figure 10). Similar to the input embeddings, any rotation $R$ in $S O(3)$ rotates the hidden features as ", "page_idx": 4}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/b8cc773c785cc483f0e3d64d03af62baa31c82c19d08101b12404ef6fe088e21.jpg", "img_caption": ["Figure 4: Left: Our equivariant module is distinct from traditional implementations [50] in its fundamental layers and the key-query product, that are crafted to be respectively equivariant and invariant. Right: Equivariant latent array used as additional input to the encoder. We apply equivariant positional encoding to each camera rotation, which is then averaged. We leverage an equivariant linear layer to get a global geometric latent $\\oplus_{l}G_{l}$ , which is concatenated with the conventional latent array ${\\mathcal R}_{0}$ to compose our proposed equivariant latent array $\\mathcal{R}_{0}^{\\prime}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "equation", "text": "$$\nR\\cdot\\bigoplus_{l\\in L}H_{l}=\\bigoplus_{l\\in L}D^{l}(R)H_{l}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $D^{l}$ are the Wigner-D matrices. We disregard any translation action since the input and queries become translation-invariant after center subtraction. ", "page_idx": 5}, {"type": "text", "text": "3.4 Basic Attention Modules ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section highlights the equivariant attention module, fundamental to ensure encoder equivariance as depicted in Figure 4a. It consists of basic equivariant layers and an invariant multi-head inner product. Our architecture, unlike typical equivariant transformers [22, 33], does not enforce geometric constraints in the equivariant kernel. Instead, it incorporates all geometric features at an input-level. Our module, utilizing the Perceiver IO structure, learns global latent representations, in contrast to other methods that emphasize the hierarchical learning of local features. ", "page_idx": 5}, {"type": "text", "text": "Equivariant Foundamental Layers For the fundamental layers, we use the equivariant linear layer and layer normalization commonly used in previous works [48, 22, 33, 34], and provide additional details in Appendix A.3. For equivariant nonlinear layers, there have been multiple proposed methods for features with the same format as ours: Norm-based Nonlinearity, Gate Nonlinearity, and Fourierbased Nonlinearity. Here, we take inspiration from the nonlinearity of Vector Neuron [13] and adapt a similar vector operation to higher-order features. Please see Appendix E for details of equivariant nonliearity. To better understand the differences between the basic layers in equivariant attention module and those in conventional one, we have visualize them in Appendix A.3 and Appendix E. ", "page_idx": 5}, {"type": "text", "text": "Multi-Head Attention Inner Product As done in previous equivariant transformer works [22, 33], we can obtain the invariant attention matrix through inner product of the same types of features. These transformers that emphasize the hierarchical learning of local features suggest using tensor products of edge feature and node feature to mix different feature types, which is computationally demanding. We discard the tensor product and only calculate attention weights using various feature types and then multiply these weights with multi-type features to efficiently integrate different types of feature. Please see Appendix F for more details. Alternatively, we can mix feature types by treating them as Fourier coefficients for spheres, apply transformers on the sphere, and use Fourier Transform to obtain new coefficients. Please refer to Appendix H for details. ", "page_idx": 5}, {"type": "text", "text": "3.5 Equivariant Encoder ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "3.5.1 Equivariant Cross-attention ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As shown in the left part in Figure 3 and Section 3.3.2, the cross-attention input is in the format $\\bigoplus_{l\\in L}H_{l}$ with $C_{l}=2$ for $l\\geq1$ and $C_{0}$ being the channel number of $f_{u v}^{i}$ . To facilitate a clearer understanding, a comparison of this input embedding with the one used in DeFiNE is also depicted in Figure 3. The latent array $\\mathcal{R}_{0}\\,=\\,\\overline{{(}}(R_{0})_{1},(R_{0})_{2},\\cdot\\cdot\\cdot\\,,(R_{0})_{N_{R}})\\,\\in\\,\\mathbb{R}^{N_{R}\\times C_{R}}$ can be treated as a scalar (0-order) feature, remaining constant during transformations in the reference frame. To make the latent array also learn the geometric information, we apply a technique similar to [1], learning equivariant features from the input\u2019s averaged geometric information. Specifically, we apply the positional encoding (PE) for each camera rotation, with each order being the concatenation of embeddings of the rotation matrix\u2019s three columns. The PE is then averaged over cameras. For the specific formulation please see Appendix I. ", "page_idx": 6}, {"type": "text", "text": "We obtain a global geometric latent $\\mathcal{G}$ using an equivariant linear layer, where the size of the weight matrix $W_{l}$ for each type $l$ is $(3,N_{R}C^{l})$ , with ${\\dot{C}}^{l}$ being the channel count for type- $l$ feature in each latent. We then append this equivariant feature to the latent ${\\mathcal R}_{0}$ , forming a new latent array $\\mathcal{R}_{0}^{\\prime}=((R_{0}^{\\prime})_{1},(R_{0}^{\\prime})_{2},\\cdot\\cdot\\cdot,(R_{0}^{\\prime})_{N_{R}})$ , where $(R_{0}^{\\prime})_{i}=(R_{0})_{i}\\oplus\\bigoplus_{l\\in L}(G_{l})_{i}$ with $\\bar{L}=\\{1,2,\\cdot\\cdot\\cdot l_{m a x}\\}$ and $(G_{l})_{i}\\in\\mathbb{R}^{(2l+1)\\times C^{l}}$ . Figure 4b illustrates the construction of this equivariant latent array. With both an equivariant input embedding and latent array, we apply equivariant cross-attention to get the equivariant latent output. ", "page_idx": 6}, {"type": "text", "text": "3.5.2 Equivariant Self-Attention ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We apply a self-attention equivariant attention mechanism to the equivariant output of cross-attention, producing a conditioned equivariant latent code. For visualization purposes (Figure 5), we can treat the equivariant latent code as the Fourier coefficients of spherical functions. Note that we do not map the features to a 2D color image. Since we have features with type-0, type-1, and type-2, etc, but we randomly select each channel from different types of feature and apply the inverse Fourier transform to get a spherical function and visualize it on a 3D sphere. For a proof of this result (i.e., the visualized sphere is also rotated when the latent code is rotated), please see Appendix D. ", "page_idx": 6}, {"type": "text", "text": "3.6 Decoder ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Figure 2 we show that, before inputting the equivariant latent space and geometric query to the decoder, they are converted into an invariant form by establishing an equivariant frame. Specifically, the equivariant latent space $\\mathcal{R}_{K}$ is represented as $\\bigoplus_{l\\in L}(\\mathcal{R}_{K}\\bar{)}_{l}$ . From its ty pe-1 feature $(\\mathcal{R}_{K})_{1}$ , we employ an equivariant MLP and the Gram-Schmidt orthogonalization [59] to derive an equivariant frame, represented by a rotation matrix $R$ . As depicted in Figure 5, the equivariant frame\u2019s rotation aligns with that of both the equivariant latent ", "page_idx": 6}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/847e5507390861655972cbca9c5131e9f889fa5cc5a34258fe76eba25ac8fec3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 5: Equivariant latent code and predicted frame. For simplicity, we use object rotation to denote the inverse rotation of the reference frame. When the object is rotated, our latent code and predicted canonical frame are also rotated. ", "page_idx": 6}, {"type": "text", "text": "and the $3D$ scene. Applying the inverse of $R$ to $\\mathcal{R}_{K}$ results in a rotation-invariant latent code $\\oplus_{l\\in L}D^{l}(R)^{T}(\\mathcal{R}_{K})_{l}$ , obtaining an invariant representation. See Appendix J for a proof. ", "page_idx": 6}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/f64775443db01b1b608e27cda08526492d7538fffc93f532d0fe474491db4f2b.jpg", "img_caption": ["Figure 6: Stereo depth estimation results on ScanNet, using our proposed EPIO architecture. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "For the embedding of camera center and viewing rays, we apply $R^{T}$ to $r_{u v}^{j}$ and $t_{j}-\\bar{t}$ to obtain invariant coordinates (see Appendix J for a proof), denoted as $R^{T}r_{u v}^{j}$ and $R^{T}(t_{j}-\\Bar{t})$ . We then use traditional sine and cosine positional encoding for these invariant coordinates, which allows us to leverage higher frequency information beyond the dimensional constraints of SPH. Since both the latent hidden state and the query are invariant to the transformation, this enables us to apply conventional cross-attention mechanisms to obtain invariant outputs and predictions, capturing higher frequency details and improving expressiveness. ", "page_idx": 7}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Datasets and Implementation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We use ScanNet [12] and DeMoN [49] to validate our model on the task of stereo depth estimation. For ScanNet, we use the same setting as [31], which downsamples scenes by a factor of 20 and splits them to obtain 94212 training and 7517 test pairs. The DeMoN dataset includes the SUN3D, RGBD-SLAM and Scenes11 datasets, where SUN3D and RGBD-SLAM are real world datasets and Scenes11 is a synthetic dataset. There are a total of 166285 training image pairs from 50420 scenes, and we use the same test split as [31] (80 pairs in SUN3D, 80 pairs in RGBD and 168 pairs in Scenes11). We include details on the network architecture and implementation in Appendix L. ", "page_idx": 7}, {"type": "text", "text": "4.2 Stereo Depth Estimation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare our equivariant model with other state-of-the-art methods on stereo depth estimation, and report quantitative results in Table 1. As we can see, it significantly outperforms competing methods on all real-world datasets and shows comparable results to the state-of-the-art on Scenes11, a synthetic dataset. This superior performance on real-world datasets is evidence of the benefits of using equivariance in multi-view scene representation. Synthetic datasets, unaffected by real-world lighting conditions, camera miscalibration and view-dependent artifacts, might benefit approaches such as DPSNet [27] and NAS [31] that use cost volume to achieve view consistency. It\u2019s also noteworthy that NAS [31] uses additional ground truth surface normals as supervision. ", "page_idx": 7}, {"type": "text", "text": "We denote our model with \u201cEqui\u201d and our baseline, DeFiNe, with \u201cNonequi\u201d to highlight that both use the same architecture, Perceiver IO, with the key difference being the presence of equivariance in our model. ", "page_idx": 7}, {"type": "text", "text": "Additionally, to assess the advantages of incorporating equivariance into our model, we conducted a com", "page_idx": 7}, {"type": "table", "img_path": "yRuJqoWoCs/tmp/64280c8ef9183b0c92f74b06d0322ca56851e13008287975dfe3f7bc58e1677e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Comparison of our EPIO model and DeFiNe on ScanNet regarding the use of data augmentation. VCA stands for virtual camera augmentation, and jitter stands for canonical camera jittering. ", "page_idx": 7}, {"type": "text", "text": "parative analysis of our model against our nonequivariant DeFiNe baseline [23], both with and without data augmentation. We explore two kinds of data augmentation: virtual camera augmentation (VCA), in which novel viewpoints are generated via pointcloud reprojection; and canonical camera jittering (CCJ), in which the reference frame is perturbed with random rotation and translation, reported in Table 2. ", "page_idx": 7}, {"type": "table", "img_path": "yRuJqoWoCs/tmp/2a06aeb47607e182caf37db2bed316a451eb26e5dea58416ee9e5012c0dd48d1.jpg", "table_caption": [], "table_footnote": ["Table 1: Stereo depth estimation results compared with the state-of-the-art: DPSNet [27], NAS [31], IIB [57], DeFine [23], DeMoN [49], DeepMVS [26]. "], "page_idx": 8}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/d90a68b030afeb795f0a813caab0abb2652a96d26b304ae744510f89ad4a4cd4.jpg", "img_caption": ["Figure 7: The figure (a) shows the equivariance of changing reference frame (Red: reference frame): For the same input and varying camera frames as reference, the Perceiver IO\u2019s predictions change, but our model\u2019s predictions stay consistent and the predicted frame is equivariant to the reference frame transformation. The figure (b) shows the approximate equivariance for different camera sets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "To further showcase our equivariant properties, we visualize the predicted canonical frame and reconstructed $3D$ point clouds from depth maps. In Figure 7a, we see that, for the same scene, when we switch the reference frame (in red) between cameras, the output point clouds change when using the standard Perceiver IO architecture, while ours remain constant, since the predicted depth is equivariant to transformations. Furthermore, even when we use different image pairs within the same scene, which theoretically cannot be guaranteed equivariant due to changes in image content, our model still predicts near-consistent canonical frames and point clouds, as illustrated in Figure 7b.In the meanwile, we compare our model with current state-of-art depth estimation model Depth anything [56], and we provide the results in Appendix O.1. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We performed an ablation study on the geometric positional encodings, spherical harmonics encoding, equivariant attention, and the decoder architecture, and report the quantitative results in Table 3. As expected, when positional encoding is not used, results are significantly degraded due to missing geometric information. Our results demonstrate that the model leverages geometric information to learn implicit multi-view geometry. Although using Fourier positional encodings with our method breaks the equivariant properties, we conducted ablations by replacing spherical harmonics with Fourier encodings to assess the specific contribution of spherical harmonics. ", "page_idx": 8}, {"type": "text", "text": "As shown in the table, Fourier encodings, which are not equivariant, are incompatible with an equivariant architecture, resulting in significantly worse performance. Additionally, we replaced the equivariant attention module with a conventional one in another ablation study. Removing the equivariant attention layers also disrupts the equivariance of our architecture, leading to a substantial drop in performance since the model loses its theoretical equivariance. We also explore the impact of the maximum order of spherical harmonics in the positional encodings, indicated by $l_{m a x}$ . For network architecture, we evaluated the impact of not learning the canonical frame, that is, we use the equivariant attention module in the decoder followed by transferring the equivariant output to invariant output via inner product, see Appendix K for details. We noticed that higher order of spherical harmonics improve depth estimation, since high frequency promotes fine-grained learning and differentiate positions in a higher-dimensional space. ", "page_idx": 9}, {"type": "text", "text": "Unlike Fourier basis, the dimension of the spherical harmonics grows two times linearly with increasing order, which is a limitation of our method, and therefore we keep the highest SPH order as 8 in our final model. This is also a reason why learning a equivariant canonical frame for invariant decoding with Fourier basis and a conventional decoder is a better approach ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "yRuJqoWoCs/tmp/81fcc7cadc59b2883c332a55f82069133cfbf10c4ebf78f05e8ee65e6ec36f70.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 3: Ablation study on the choice of positional encoding frequency and decoder architecture. ", "page_idx": 9}, {"type": "text", "text": "than directly using an equivariant decoder. Another factor is that learning a canonical frame enforces all inputs to the decoder to be invariant, which should facilitate 3D reasoning. Moreover, we performed additional small-scale experiments to study the impact of the number of available views, see Appendix O.2. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce an $S E(3)$ equivariant model designed to learn the equivariant $3D$ scene prior across multiple views, utilizing spherical harmonics for positional encoding and specialized equivariant attention mechanisms within the Perceiver IO architecture. Additionally, our design exploits its equivariant latent space to disentangle equivariant frames and invariant scene details, enabling the seamless integration of various existing decoders in conjunction with our specialized encoder. our model\u2019s capability in 3D structure comprehension is showcased through its superior performance in stereo depth estimation, significantly exceeding that of non-equivariant models. Our architecture can be modified to accommodate a wider range of vision tasks, which we leave to future work (for a more detailed discussion please see Appendix M). ", "page_idx": 9}, {"type": "text", "text": "Limitation As discussed in Section 4.3, unlike the Fourier basis, the dimension of spherical harmonics increases linearly at twice the rate with each order. This limits the number of spherical harmonics and the maximum frequency utilized, resulting in an inability to preserve detailed features in cameras and images. Additionally, the presence of different types of features, each with its own linear and nonlinear layers, slightly slows down the forward process compared to traditional methods. Moreover, we observe instability in training the equivariant network, which may be due to the magnitude explosion of high-order spherical harmonics. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was supported by Toyota Research Institute, whose funding and resources were invaluable in advancing this work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Serge Assaad, Carlton Downey, Rami Al-Rfou, Nigamaa Nayakanti, and Ben Sapp. Vn-transformer: Rotation-equivariant attention for vector neurons. arXiv preprint arXiv:2206.04176, 2022.   \n[2] Johann Brehmer, Joey Bose, Pim De Haan, and Taco Cohen. Edgi: Equivariant diffusion for planning with embodied agents. arXiv preprint arXiv:2303.12410, 2023.   \n[3] Joao Carreira, Skanda Koppula, Daniel Zoran, Adria Recasens, Catalin Ionescu, Olivier Henaff, Evan Shelhamer, Relja Arandjelovic, Matt Botvinick, Oriol Vinyals, et al. Hip: Hierarchical perceiver. arXiv preprint arXiv:2202.10890, 2022.   \n[4] Dian Chen, Jie Li, Vitor Guizilini, Rares Andrei Ambrus, and Adrien Gaidon. Viewpoint equivariance for multi-view 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9213\u20139222, 2023.   \n[5] Haiwei Chen, Shichen Liu, Weikai Chen, Hao Li, and Randall Hill. Equivariant point network for 3d point cloud analysis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14514\u201314523, 2021. [6] Yunlu Chen, Basura Fernando, Hakan Bilen, Matthias Nie\u00dfner, and Efstratios Gavves. 3d equivariant graph implicit functions. In European Conference on Computer Vision, pages 485\u2013502. Springer, 2022. [7] Oliver J Cobb, Christopher GR Wallis, Augustine N Mavor-Parker, Augustin Marignier, Matthew A Price, Mayeul d\u2019Avezac, and Jason D McEwen. Efficient generalized spherical cnns. arXiv preprint arXiv:2010.11661, 2020. [8] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on machine learning, pages 2990\u20132999. PMLR, 2016. [9] Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolutional networks and the icosahedral cnn. In International conference on Machine learning, pages 1321\u20131330. PMLR, 2019.   \n[10] Taco S Cohen, Mario Geiger, Jonas K\u00f6hler, and Max Welling. Spherical cnns. arXiv preprint arXiv:1801.10130, 2018.   \n[11] Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on homogeneous spaces. Advances in neural information processing systems, 32, 2019.   \n[12] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828\u20135839, 2017.   \n[13] Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and Leonidas J Guibas. Vector neurons: A general framework for so (3)-equivariant networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12200\u201312209, 2021.   \n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[15] Carlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, and Kostas Daniilidis. Polar transformer networks. arXiv preprint arXiv:1709.01889, 2017.   \n[16] Carlos Esteves, Yinshuang Xu, Christine Allen-Blanchette, and Kostas Daniilidis. Equivariant multi-view networks. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1568\u20131577, 2019.   \n[17] Carlos Esteves, Ameesh Makadia, and Kostas Daniilidis. Spin-weighted spherical cnns. Advances in Neural Information Processing Systems, 33:8614\u20138625, 2020.   \n[18] Carlos Esteves, Tianjian Lu, Mohammed Suhail, Yi-fan Chen, and Ameesh Makadia. Generalized fourier features for coordinate-based learning of functions on manifolds. 2021.   \n[19] Carlos Esteves, Jean-Jacques Slotine, and Ameesh Makadia. Scaling spherical cnns. arXiv preprint arXiv:2306.05420, 2023.   \n[20] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In International Conference on Machine Learning, pages 3165\u20133176. PMLR, 2020.   \n[21] Alex Fisher, Ricardo Cannizzaro, Madeleine Cochrane, Chatura Nagahawatte, and Jennifer L Palmer. Colmap: A memory-efficient occupancy grid mapping framework. Robotics and Autonomous Systems, 142:103755, 2021.   \n[22] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-translation equivariant attention networks. Advances in neural information processing systems, 33:1970\u20131981, 2020.   \n[23] Vitor Guizilini, Igor Vasiljevic, Jiading Fang, Rare Ambru, Greg Shakhnarovich, Matthew R Walter, and Adrien Gaidon. Depth field networks for generalizable multi-view scene representation. In European Conference on Computer Vision, pages 245\u2013262. Springer, 2022.   \n[24] Lingshen He, Yiming Dong, Yisen Wang, Dacheng Tao, and Zhouchen Lin. Gauge equivariant transformer. Advances in Neural Information Processing Systems, 34:27331\u201327343, 2021.   \n[25] Haojie Huang, Dian Wang, Xupeng Zhu, Robin Walters, and Robert Platt. Edge grasp network: A graphbased se (3)-invariant approach to grasp detection. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 3882\u20133888. IEEE, 2023.   \n[26] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2821\u20132830, 2018.   \n[27] Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon. Dpsnet: End-to-end deep plane sweep stereo. arXiv preprint arXiv:1905.00538, 2019.   \n[28] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021.   \n[29] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry. End-to-end learning of geometry and context for deep stereo regression. In Proceedings of the IEEE international conference on computer vision, pages 66\u201375, 2017.   \n[30] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.   \n[31] Uday Kusupati, Shuo Cheng, Rui Chen, and Hao Su. Normal assisted stereo depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2189\u20132199, 2020.   \n[32] Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding, Francis X Creighton, Russell H Taylor, and Mathias Unberath. Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6197\u20136206, 2021.   \n[33] Yi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic graphs. arXiv preprint arXiv:2206.11990, 2022.   \n[34] Yi-Lun Liao, Brandon Wood, Abhishek Das, and Tess Smidt. Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations. arXiv preprint arXiv:2306.12059, 2023.   \n[35] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun. Petr: Position embedding transformation for multi-view 3d object detection. In European Conference on Computer Vision, pages 531\u2013548. Springer, 2022.   \n[36] Lachlan E MacDonald, Sameera Ramasinghe, and Simon Lucey. Enabling equivariance for arbitrary lie groups. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8183\u20138192, 2022.   \n[37] DE Pim, Maurice Weiler, Taco Sebastiaan Cohen, and Max Welling. Gauge equivariant geometric graph convolutional neural network, August 12 2021. US Patent App. 17/169,338.   \n[38] Omri Puny, Derek Lim, Bobak Kiani, Haggai Maron, and Yaron Lipman. Equivariant polynomials for graph neural networks. In International Conference on Machine Learning, pages 28191\u201328222. PMLR, 2023.   \n[39] Md Ashiqur Rahman and Raymond A Yeh. Truly scale-equivariant deep nets with fourier layers. arXiv preprint arXiv:2311.02922, 2023.   \n[40] David Romero, Erik Bekkers, Jakub Tomczak, and Mark Hoogendoorn. Attentive group equivariant convolutional networks. In International Conference on Machine Learning, pages 8188\u20138199. PMLR, 2020.   \n[41] David W Romero and Jean-Baptiste Cordonnier. Group equivariant stand-alone self-attention for vision. arXiv preprint arXiv:2010.00977, 2020.   \n[42] Hyunwoo Ryu, Hong-in Lee, Jeong-Hoon Lee, and Jongeun Choi. Equivariant descriptor fields: Se (3)-equivariant energy-based models for end-to-end visual robotic manipulation learning. arXiv preprint arXiv:2206.08321, 2022.   \n[43] Aleksandr Safin, Daniel Durckworth, and Mehdi SM Sajjadi. Repast: Relative pose attention scene representation transformer. arXiv preprint arXiv:2304.00947, 2023.   \n[44] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Luc\u02c7ic\u00b4, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene representation transformer: Geometryfree novel view synthesis through set-latent scene representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6229\u20136238, 2022.   \n[45] V\u0131ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In International conference on machine learning, pages 9323\u20139332. PMLR, 2021.   \n[46] Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B Tenenbaum, Alberto Rodriguez, Pulkit Agrawal, and Vincent Sitzmann. Neural descriptor fields: Se (3)-equivariant object representations for manipulation. In 2022 International Conference on Robotics and Automation (ICRA), pages 6394\u20136400. IEEE, 2022.   \n[47] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Generalizable patch-based neural rendering. In European Conference on Computer Vision, pages 156\u2013174. Springer, 2022.   \n[48] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018.   \n[49] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learning monocular stereo. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5038\u20135047, 2017.   \n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[51] Maurice Weiler and Gabriele Cesa. General e (2)-equivariant steerable cnns. Advances in neural information processing systems, 32, 2019.   \n[52] Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S Cohen. 3d steerable cnns: Learning rotationally equivariant features in volumetric data. Advances in Neural Information Processing Systems, 31, 2018.   \n[53] Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic networks: Deep translation and rotation equivariance. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5028\u20135037, 2017.   \n[54] Yinshuang Xu, Jiahui Lei, Edgar Dobriban, and Kostas Daniilidis. Unified fourier-based kernel and nonlinearity design for equivariant networks on homogeneous spaces. In International Conference on Machine Learning, pages 24596\u201324614. PMLR, 2022.   \n[55] Yinshuang Xu, Jiahui Lei, and Kostas Daniilidis. $s e(3)$ equivariant convolution and transformer in ray space. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[56] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. arXiv preprint arXiv:2401.10891, 2024.   \n[57] Wang Yifan, Carl Doersch, Relja Arandjelovic\u00b4, Joao Carreira, and Andrew Zisserman. Input-level inductive biases for 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6176\u20136186, 2022.   \n[58] Jason Y Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. arXiv preprint arXiv:2402.14817, 2024.   \n[59] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5745\u20135753, 2019.   \n[60] Minghan Zhu, Maani Ghaffari, William A Clark, and Huei Peng. E2pn: Efficient se (3)-equivariant point network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1223\u20131232, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Perceiver IO ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The Perceiver IO [28] efficiently encodes multi-modality inputs by utilizing cross-attention between the inputs themselves and a learnable, fixed-dimension latent code. This latent code is then refined through a series of self-attention layers. In the decoding phase, the model employs cross-attention between a given query and the refined latent code to generate predictions. ", "page_idx": 14}, {"type": "text", "text": "In DeFiNe [23], this framework is used to address scenarios involving multiple cameras with predetermined relative poses, denoted as $\\{T_{i}\\}_{i=1}^{N}$ , and their respective images $\\{{\\bar{I_{i}}}\\}_{i=1}^{N}$ . Within this context, the system queries arbitrary camera poses, represented as j\u2032}jM=1. Importantly, this queried pose $T_{j}$ can either be one of the already established camera positions (as it is common in stereo depth estimation), or a position outside the range of the input cameras (which is typical in video depth estimation). Based on input data and queried pose, the network generates the corresponding predicted output $\\hat{D}_{j}$ for the query. ", "page_idx": 14}, {"type": "text", "text": "Like other vision tasks that utilize the Perceiver framework, DeFiNe uses as input a composite of geometric information and corresponding image features, while the query utilizes only geometric information. Specifically, the input is formulated as $\\left\\{f_{u v}^{i}\\oplus P E(r_{u v}^{i})\\:{\\stackrel{.}{\\oplus}}\\:{\\dot{P}}E(t_{i})\\right\\}$ , where $\\bar{f_{u v}^{i}}$ represents image features associated with each pixel $(u,v)$ in camera $i$ . $P E(r_{u v}^{i})$ denotes the positional encoding with Fourier cosine and sine series of the ray direction $r_{u v}^{i}$ calculated relative to camera $T_{i}$ , $P E(t_{i})$ refers to the positional encoding of the camera\u2019s translation $t_{i}$ in $T_{i}$ , using Fourier cosine and sine series. The query in this model is represented as $\\left\\{P E(r_{u v}^{j})\\oplus P E(t_{j})\\right\\}$ , and the network is designed to output depth estimationDuv for each query pixel $(u,v)$ of query camera $j$ . ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A.2 Equivariance Definition ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Concretely, assuming the global reference frame undergoes a transform $T^{-1}\\,\\in\\,S E(3)$ to $T_{\\mathrm{{G}}}^{\\prime}=$ $T^{-1}T_{\\mathrm{G}}$ , the set of input and query poses would become $\\{T T_{i}\\}$ and $\\{T T_{j}^{\\prime}\\}$ with respect to $T_{\\mathrm{G}}^{\\prime}$ (note that the corresponding input images $\\{I_{i}\\}_{i=1}^{N}$ remain unchanged). Mathematically, we use $\\Lambda_{T}((\\{T_{i}\\},\\{I_{i}\\}))\\stackrel{*}{=}\\stackrel{*}{(}\\{T T_{i}\\},\\{\\bar{I}_{i}\\})$ and $\\Lambda_{T}(\\{T_{j}^{\\prime}\\})\\,=\\,\\{T T_{j}\\}$ to denote the $S E(3)$ actions on the input and query cameras. An equivariant network satifies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Phi(\\Lambda_{T}(\\{T_{i}\\},\\{I_{i}\\}),\\Lambda_{T}(\\{T_{j}^{\\prime}\\}))\\equiv\\Phi((\\{T_{i}\\},\\{I_{i}\\}),\\{T_{j}^{\\prime}\\}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Readers may recognize the above equation as describing the invariance of the network $\\Phi$ to the transformation of both input and query. In fact, it is also equivalent to the statement that the network $\\Phi$ is equivariant when viewed that it learns an implicit field. To demonstrate this equivalence, let $F(\\cdot)=\\Phi((\\{T_{i}\\},\\{I_{i}\\}),\\cdot)$ , and define the $S E(3)$ operator $\\Lambda^{\\prime}$ on $F$ $:\\Lambda_{T}^{\\prime}F(\\{T_{j}\\})=\\bar{F}(\\Lambda_{T}^{-1}(\\{T_{j}\\}))$ . We then derive ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Phi(\\Lambda_{T}(\\{T_{i}\\},\\{I_{i}\\}))=\\Lambda_{T}^{\\prime}F=\\Lambda_{T}^{\\prime}\\Phi(\\{T_{i}\\},\\{I_{i}\\}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "i.e., that the network $\\Phi$ is equivariant. A similar statement can be found in [6]. ", "page_idx": 14}, {"type": "text", "text": "With input level inductive bias and representing $T=(R,t)$ , the equivariant model $\\Phi$ should satisfy ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi(\\{f_{u v}^{i}\\oplus P E(R r_{u v}^{i},R t_{i}+t)\\},\\{P E(R r_{u v}^{j},R t_{j}+t)\\})}\\\\ &{\\ =\\Phi(\\{f_{u v}^{i}\\oplus P E(r_{u v}^{i},t_{i})\\},\\{P E(r_{u v}^{j},t_{j})\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.3 Fundamental Layers ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.3.1 Equivariant Linear Layer ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "With the transformation acting on the features, we can define the equivariant linear layer $\\mathcal{L}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}(R\\cdot H^{(k)})=R\\cdot\\mathcal{L}(H^{(k)})=R\\cdot H^{(k+1)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/e2adb2a7d8ce3d84f5982212db5b600c8d5dfd1746f8a7ba21b9c22211c5551e.jpg", "img_caption": ["Figure 8: Comparison between Equivariant and Non-Equivariant Linear Layers: The figure above depicts the equivariant linear layer, wherein each type of feature is linearly combined using a specific matrix $W_{l}$ , treating the vector or tensor as a cohesive geometric entity. Conversely, the figure below illustrates the traditional linear approach, where all channels within an intrinsic feature, as well as different types of features, are linearly intermixed, since it vectorizes and concatenates all features and applies a unified weight matrix "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "where the superscript denotes the index of layer H(k) =  l\u2208L Hlk = (H0(k ), H1(k ), \u00b7 and the same for $H^{(k+1)}$ . To achieve equivariance, we use the same linear layer $\\mathcal{L}$ as stated in [48, 52, 33]: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}((H_{0}^{(k)},H_{1}^{(k)},\\cdot\\cdot\\cdot,H_{l_{m a x}}^{(k)}))}\\\\ &{=(H_{0}^{(k)}W_{0},H_{1}^{(k)}W_{1},H_{l_{m a x}}^{(k)}W_{l_{m a x}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The weights $W_{l}$ have the format $(C_{l}^{(k)},C_{l}^{(k+1)})$ , where $C_{l}^{(k)}$ is the number of channels in $H_{l}^{(k)}$ , representing the corresponding input channels, and $C_{l}^{(k+1)}$ is the number of channels in $H_{l}^{(k+1)}$ representing the corresponding output channels. The difference of equivariant linear layers and conventional linear layers are depicted in Figure 8. ", "page_idx": 15}, {"type": "text", "text": "A.3.2 Equivariant LayerNormalization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use the commonly used equivariant layer normalization in equivariant works that apply the layer normalization to the norm of the features, and then multiply those with unit tensor features. The normalization layer $\\mathcal{L N}$ is defined as ", "page_idx": 15}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/06527ecaf7da0665bda853494f77abe823f7f0d52e15889425ffb71ded2cef83.jpg", "img_caption": ["Figure 9: Comparison Between Equivariant and Non-Equivariant Layer Normalization: The figure above illustrates the equivariant nonlinear layer, where layer normalization is applied to the norm of each feature type, followed by multiplication with a unit for each feature. The figure below depicts the traditional nonlinear layer, employing element-wise layer normalization across features, thereby disrupting equivariance by treating each feature type as a concatenation of individual channels rather than a unified geometric entity. "], "img_footnote": [], "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal L N((H_{0}^{(k)},H_{1}^{(k)},\\cdots,H_{l_{m a x}}^{(k)}))}\\\\ &{=(l n(H_{0}^{(k)}),l n(\\|H_{1}^{(k)}\\|)\\cdot\\frac{H_{1}^{(k)}}{\\|H_{1}^{(k)}\\|},}\\\\ &{\\cdots\\cdot,l n(\\|H_{1}^{(k)}\\|)\\cdot\\frac{H_{l_{m a x}}^{(k)}}{\\|H_{l_{m a x}}^{(k)}\\|}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $l n$ is the conventional layer normalization. The difference of equivariant layernormalization and conventional layernormalization are depicted in Figure 9. ", "page_idx": 16}, {"type": "text", "text": "A.4 Spherical Harmonics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The spherical harmonics constitute a complete set of orthogonal functions, making them an orthonormal basis. Any spherical function $f\\,\\in\\,\\bar{\\mathcal{L}}^{2}(\\mathbb{S}^{2})$ can be expressed as a linear combination of these spherical harmonics. In simpler terms, they serve as the Fourier basis for functions defined on a sphere. ", "page_idx": 16}, {"type": "text", "text": "Similar to the varying frequencies of sines and cosines in Fourier series, spherical harmonics are characterized by different degrees (orders), denoted as $l\\in\\mathbb{N}$ . Each degree of spherical harmonics corresponds to a specific pattern or shape on the surface of a sphere. Higher orders (degrees) indicate higher frequencies, resulting in more intricate and complex patterns on the sphere\u2019s surface. To apply spherical harmonics in three-dimensional space $(\\,\\Bar{\\mathbb{R}}^{3})$ , we incorporate a radial component $r^{l}$ into the original spherical harmonics of the corresponding degree (order)- $l$ . This method scales the spherical harmonics for three-dimensional applications, extending their utility beyond the sphere $\\mathbb{S}^{2}$ . This adaptation scales the spherical harmonics for applications beyond the two-dimensional sphere surface, effectively extending their utility to three-dimensional analyses and applications. Despite this adaptation, these functions retain their fundamental characteristics and are still referred to as spherical harmonics. In this work, we utilize spherical harmonics, incorporating 3D Cartesian coordinates, as positional embeddings in transformer models. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "The fascinating properties of spherical harmonics make them crucial in $S O(3)$ group representations, allowing us to harness their power to achieve equivariance in transformers. A order- $l$ spherical harmonics, denoted as $Y^{l}:\\mathbb{R}^{3}\\rightarrow\\mathbb{R}^{2l+1}$ , is a vector function with $2l+1$ dimension, following the transformation rule: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{Y^{l}(R r)=D^{l}(R)Y^{l}(r),}}\\\\ {{Y^{l}(r)=\\|r\\|^{l}Y^{l}(\\hat{r}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $R$ is an arbitrary rotation, $\\begin{array}{r}{\\hat{r}=\\frac{r}{\\lVert r\\rVert}}\\end{array}$ , and $D^{l}:S O(3)\\rightarrow\\mathbb{R}^{(2l+1)\\times(2l+1)}$ , is called the Wigner-D matrix, serving as the irreducible representation of $S O(3)$ corresponding to the order $l$ . The Wigner-D matrix are orthogonal matrix, i.e., $D^{l}(R)D^{l}(R)^{T}=\\stackrel{}{I}$ . To mitigate the impact of the large scaling factor $\\lVert r\\rVert^{l}$ , we explored alternative approaches: one involved substituting the scaling factor $\\lVert r\\rVert^{l}$ with Gaussian radial basis functions, expressed as $e^{-(\\|r\\|-l)^{2}}$ . Another approach entailed employing Fourier sine and cosine series to represent $\\lVert r\\rVert$ , creating an invariant embedding. This embedding was then combined with the positional encoding of the unit vector $\\hat{r}$ using the spherical harmonics for sphere. However, these alternatives did not demonstrate any significant benefits over the use of spherical harmonics adapted for three-dimensional space $(\\mathbb{R}^{3})$ , leading us to continue with our initial methodology. ", "page_idx": 17}, {"type": "text", "text": "A.4.1 The use of Spherical Harmonics in Equivariant Transformer ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We acknowledge that most equivariant transformer works Fuchs et al. [22], Liao and Smidt [33], Liao et al. [34] also uses spherical harmonics in the transformer layers. However, the use of the spherical harmonics is to derive the equivariant kernel basis based on the relitive position for specific geometirc entities. For example, Fuchs et al. [22] first proposes the $S E(3)$ equivariant transformer for point clouds, where spherical harmonics is served as the equivariant kernel as in steerable 3D convolutions. Equiformer[33, 34] is a graph-based architecture that leverages spherical harmonics for the edges, using a depth-wise tensor product to embed it into the node. For this graph embedding (atom $^+$ edge-degree) and graph attention, the use of spherical harmonics could be interpreted more as an equivariant kernel basis. The difference here is that Equiformer uses spherical harmonics to integrate edge information into the equivariant kernel, while ours uses spherical harmonics directly in each token. ", "page_idx": 17}, {"type": "text", "text": "B Equivariance of Conventional Positional Encoding ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The conventional positional encoding leveraging Fourier sine and cosine functions is translational equivariant. When the input is translated by a translation $t$ , the output will be transformed in a specific way, multiplied by the representation of $t$ , i.e., $P E(x+t)=e^{i\\omega t}e^{i\\omega x}$ for specific frequency $\\omega$ in complex format or ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P E(x+t)=\\Big[c o s(\\omega t)}&{-s i n(\\omega t)\\Big]\\Big[c o s(\\omega x)}&{-s i n(\\omega x)\\Big]}\\\\ {s i n(\\omega t)}&{c o s(\\omega t)\\Big]\\Big[s i n(\\omega x)}&{c o s(\\omega x)\\Big]}\\\\ {=\\rho_{\\omega}(t)P E(x)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "in matrix format. ", "page_idx": 17}, {"type": "text", "text": "C Equivariant Hidden Feature Format ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The equivariant latent code is structured as $\\begin{array}{r}{\\boldsymbol{H}\\,=\\,\\bigoplus_{l\\in L}\\boldsymbol{H}_{l}\\,=\\,(\\boldsymbol{H}_{0},\\boldsymbol{H}_{1},\\cdot\\cdot\\cdot\\,,\\boldsymbol{H}_{l})}\\end{array}$ , with each $H_{l}$ having dimensions $(2l+1,C_{l})$ . The action of a rotation $R$ on this latent code is depicted in Figure 10. ", "page_idx": 17}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/265ec5aec268a0f0d5215fcb2ecc1334689af868b7c1ba326ec3d5960efea9c7.jpg", "img_caption": ["Figure 10: Latent code transformation. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Visualization of the Equivariant Latent ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "With the spherical harmonics, we can introduce the Fourier Transform for the sphere. The Fourier coefficient $\\mathcal{F}^{l}$ of a function on the sphere, $f:\\mathbb{S}^{2}\\to\\mathbb{R}$ , corresponding to the order $l$ is obtained by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{F}^{l}=\\int_{\\mathbb{S}^{2}}f(x)Y^{l}(x)d x,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the inverse Fourier Transform without normalization follows the equation: ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(x)=\\sum_{l}(\\mathcal{F}^{l})^{T}Y^{l}(x)d x,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When we rotate the function $f$ with any rotation $R\\,\\in\\,S O(3)$ , i.e., we get a new function $f^{\\prime}=$ $f(R^{-1}x)$ . The Fourier coefficients corresponding to the order $l$ are: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{F}^{\\prime l}=\\int_{\\mathbb{S}^{2}}f(R^{-1}x)Y^{l}(x)d x=\\int_{\\mathbb{S}^{2}}f(y)Y^{l}(R y)d y=\\int_{\\mathbb{S}^{2}}f(y)D^{l}(R)Y^{l}(y)d y=D^{l}(R)\\mathcal{F}^{l},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This indicates that when a spherical function is rotated by any rotation $R$ , its Fourier coefficients will be multiplied by the corresponding Wigner-D matrix. Inversely, we have that when all Fourier coefficients $\\mathcal{\\dot{F}}^{l}$ are multiplied by Wigner-D matrices $D^{l}(R)$ , the obtained spherical function is rotated by $R$ . ", "page_idx": 18}, {"type": "text", "text": "With such preliminary, we can treat our latent code $\\bigoplus_{l\\in L}H_{l}$ as the Fourier coefficients, where type- $l$ features are the $l$ -th order coefficients, enabling us to derive the spherical function. As a result, when our features are transformed \u2014 with each type being multiplied by its respective Wigner-D matrix \u2014 the spherical function undergoes a corresponding rotation. ", "page_idx": 18}, {"type": "text", "text": "E Equivariant Nonlinear Layer ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In our proposed nonlinear layer, we first generate intermediate hidden features $\\bigoplus_{l\\in L}H_{l}^{\\prime k}\\;=$ $(H_{1}^{\\prime(k)},\\cdot\\cdot\\cdot\\,,H_{l_{m a x}}^{\\prime(k)})$ of the same size as the input via an equivariant linear layer. Subsequent to this, we employ the specified nonlinearity: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A((H_{0}^{(k)},H_{1}^{(k)},\\cdots,H_{l_{m a x}}^{(k)}))}\\\\ &{=(a(H_{0}^{(k)}),(a(\\langle H_{1}^{(k)},H_{1}^{\\prime(k)}\\rangle)-\\langle H_{1}^{(k)},H_{1}^{\\prime(k)}\\rangle)\\cdot\\frac{H_{1}^{\\prime(k)}}{n o r m(H_{1}^{\\prime(k)})}+H_{1}^{(k)},}\\\\ &{\\cdots,(a(\\langle H_{l_{m a x}}^{(k)},H_{l_{m a x}}^{\\prime(k)}\\rangle)-\\langle H_{l_{m a x}}^{(k)},H_{l_{m a x}}^{\\prime(k)}\\rangle)\\cdot\\frac{H_{l_{m a x}}^{\\prime(k)}}{n o r m(H_{l_{m a x}}^{\\prime(k)})}+H_{l_{m a x}}^{(k)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/b6c0ac2e9b585ffd1564c9f416d6adf5bf8051b67395f2402a7eb512ad49dc1e.jpg", "img_caption": ["Figure 11: Comparison of Equivariant and Non-Equivariant Nonlinear Layers: The figure above illustrates the equivariant nonlinear layer, similar to those in vector neuron models. This layer establishes equivariant directions using an equivariant linear layer, applies nonlinearity to the projection of the original feature in these directions, and then adds this to the orthogonal component relative to the direction in the input. On the other hand, the figure below shows the conventional nonlinear layer, which applies element-wise nonlinearity to the vectorized feature, thus failing to preserve equivariance. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Here, $\\langle\\cdot,\\cdot\\rangle$ denotes the per-channel inner product, meaning the size of $\\langle H_{l}^{(k)},H_{l}^{\\prime(k)}\\rangle$ is in the format $(1,C_{l}^{(k)})$ . The function $a$ represents a conventional activation operation, such as ReLU, Sigmoid, or LeakyReLU. The symbol $\\cdot$ indicates broadcast multiplication. This approach bears resemblance to gated normalization [52]. However, in our model, the scalar for the \u201cgate\" is derived from the inner product of two outputs of the equivariant linear layer, rather than being a scalar present in the hidden features themselves. The difference of equivariant nonlinear layers and conventional nonlinear layers are depicted in Figure 11. ", "page_idx": 19}, {"type": "text", "text": "F Multi-head Attention Inner Product ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Given the input of the attention module formulated as $\\bigoplus_{l\\in L}H_{l}$ , we generate query $Q$ , key $K$ , and value $V$ by equivariant linear layers. As $Q,K$ , and $V$ are equivariant features, they are expressed as $Q_{i}=\\underline{{\\hat{\\Phi}_{l\\in L}}}(Q_{l})_{i}$ , $\\begin{array}{r}{K_{j}=\\bigoplus_{l\\in L}(K_{l})_{j}}\\end{array}$ , and $V_{j}=\\bigoplus_{l\\in L}(V_{l})_{j}$ , where $i$ and $j$ are the indices of the latents.  The inner product i s calculated between t he equivariant key $K$ and the equivariant query $Q$ , and here we describe how we use it in multi-head attention. With $N_{h}$ multi-heads, we split the features $K,Q$ , and $V$ into $N_{h}$ heads along the channel dimension. Taking $Q$ as an instance for clarity, and denoting $C_{l}$ as the number of channels for type- $l$ feature $(Q_{l})_{i}$ in $Q_{i},Q_{i}$ gets divided into various heads $(Q_{i})^{h}$ with $h$ as the head index. $(Q_{i})^{h}$ maintains the equivariant feature format, represented as $\\begin{array}{r}{(Q_{i})^{h}=\\bigoplus_{l\\in L}(Q_{l})_{i}^{h}}\\end{array}$ , with the channel count for type- $l$ feature $(Q_{l})_{i}^{h}$ being $\\frac{C_{l}}{N_{h}}$ . This division also applies to $K$ and $V$ . The inner product of $(Q_{i})^{h}$ and $(K_{j})^{h}$ is defined as ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\langle(Q_{i})^{h},(K_{j})^{h}\\rangle=\\sum_{l\\in L}\\sum_{c}^{\\frac{C_{l}}{N_{h}}}(((Q_{l})_{i}^{h})_{c})^{T}((K_{l})_{j}^{h})_{c}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "With the defined inner product, we have the output: ", "page_idx": 20}, {"type": "equation", "text": "$$\n(O_{i})^{h}=\\sum_{j}\\frac{e x p(\\langle(Q_{i})^{h},(K_{j})^{h}\\rangle)}{\\sum_{j}e x p(\\langle(Q_{i})^{h},(K_{j})^{h}\\rangle)}(V_{j})^{h}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The final output of the attention mechanism, composed in the channel dimension, can be denoted as $\\bigoplus_{l\\in L}O_{l}$ . For proof of equivariance, refer to Sec. G. ", "page_idx": 20}, {"type": "text", "text": "G Proof of Equivariance for Multi-Head Attention ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The inner product is formulated as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\langle(Q_{i})^{h},(K_{j})^{h}\\rangle=\\sum_{l\\in L}\\sum_{c=1}^{\\frac{C_{l}}{N_{h}}}(((Q_{l})_{i}^{h})_{c})^{T}((K_{l})_{j}^{h})_{c}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Owing to the equivariance of the Linear Layer, when the input undergoes a rotation $R$ , the components $Q,K,V$ are correspondingly transformed, denoted as $R\\!\\cdot\\!Q,R\\!\\cdot\\!K$ , and $R\\cdot V$ . Under these conditions, the corresponding inner product becomes: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle(R\\cdot Q_{i})^{h},(R\\cdot K_{j})^{h}\\rangle=\\displaystyle\\sum_{l\\in L}^{\\frac{c_{l}}{N_{h}}}((D^{l}(R)(Q_{l})_{i}^{h})_{c})^{T}(D^{l}(R)(K_{l})_{j}^{h})_{c}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{l\\in L}^{\\frac{c_{l}}{N_{h}}}(((Q_{l})_{i}^{h})_{c})^{T}D^{l}(R)^{T}D^{l}(R)((K_{l})_{j}^{h})_{c}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\frac{c_{l}}{N_{h}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{l\\in L}^{\\frac{c_{l}}{N_{h}}}(((Q_{l})_{i}^{h})_{c})^{T}(K_{l})_{j}^{h})_{c}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\langle(Q_{i})_{i}^{h},(K_{j})^{h}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which proves the invariance of the defined inner product. Thereby, the output becomes: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{j}\\frac{e x p\\left(\\left\\langle Q_{i}\\right\\rangle^{h},\\left(K_{j}\\right)^{h}\\right)}{\\sum_{j}e x p\\left(\\left\\langle Q_{i}\\right\\rangle^{h},\\left(K_{j}\\right)^{h}\\right)}(R\\cdot V_{j}^{h})}\\\\ &{=\\sum_{j}\\frac{e x p\\left(\\left\\langle(Q_{i})^{h},(K_{j})^{h}\\right\\rangle\\right)}{\\sum_{j}e x p\\left(\\left\\langle(Q_{i})^{h},(K_{j})^{h}\\right\\rangle\\right)}\\bigoplus D^{l}(R)(V_{l})_{j}^{h}}\\\\ &{=\\bigoplus D^{l}(R)(\\sum_{j}\\frac{e x p\\left(\\left\\langle Q_{i}\\right\\rangle^{h},\\left(K_{j}\\right)^{h}\\right)}{\\sum_{j}e x p\\left(\\left\\langle(Q_{i})^{h},(K_{j})^{h}\\right\\rangle\\right)}(V_{l})_{j}^{h})}\\\\ &{=\\bigoplus D^{l}(R)(O_{l})_{j}^{h}}\\\\ &{=R\\cdot O_{j}^{h},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which proves that the whole attention mechanism is equivariant. ", "page_idx": 20}, {"type": "text", "text": "H Alternative Equivariant Attention ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "When it comes to attention mechanisms involving latents with different types of equivariant features, a direct method is to use tensor product to entangle these different types, which is complicated and computationally expensive. However, there is an alternative approach that treats the different types of features as Fourier coefficients to obtain spherical features. By applying the conventional transformer to these spherical features, followed by the Fourier Transform, we can retrieve different types of equivariant features. This method offers a more efficient way to entangle and handle various types of features within the attention mechanism. For latent features $\\bigoplus_{l\\in L}{\\bar{H_{l}}}$ , we apply the Inverse Fourier Transform so that: ", "page_idx": 21}, {"type": "equation", "text": "$$\nS_{l}(x)=Y^{l}(x)^{T}H_{l},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which implies that $S_{l}$ has size $(N_{R},C_{l},N_{S})$ , where $N_{R}$ is the number of latents and $N_{S}$ is the number of samplings for the sphere. From the preliminary in appendix $\\mathrm{D}$ , we know that when the input features are rotated by $R$ , the output becomes $S_{l}(R^{-1}x)$ , which means these spheres are rotated as well. By concatenating the $\\{S_{l}\\}$ on the channel dimension, we get the features after an inverse Fourier Transform with size $(\\bar{N}_{R},\\bar{\\sum_{l}C_{l}},N_{S})$ , i.e., we have $N_{R}$ spheres with $\\sum_{l}C_{l}$ channels and $N_{S}$ number of sampling. We can di rectly apply the self-attention mechanism to t hese spheres without breaking equivariance, resulting in spherical features $F$ with dimensions $(N_{R},\\Sigma_{l}\\,C_{l}\\bar{,}N_{S})$ after the self-attention. Finally, we apply the Fourier Transform as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\nH_{l}=\\sum_{i}S^{l}(x_{i})Y^{l}(x_{i}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $S^{l}(x_{i})=F[:,I n d_{l},i]$ , with $I n d_{l}$ representing the index of channels for spheres that correspond to type- $l$ features $H_{l}$ and $i$ denoting the index of the sample on the sphere. By composing different types of features, we obtain outputs in the format $\\bigoplus_{l\\in L}H_{l}$ . It is evident that the composition of inverse Fourier Transform, transformer on the sp here and the Fourier Transform is equivariant, as confirmed by the preliminary properties of the Fourier Transform in appendix D. In practice, the computational load is increased due to the number of samples and the complexity of spherical convolution. To address this, we can utilize icosahedron sampling and apply equivariant correlation on the icosahedron for the linear layer in attention. Additionally, we can use standard nonlinear layers and typical layer normalization in the self-attention mechanism. ", "page_idx": 21}, {"type": "text", "text": "I Averaged Global Geometric Embedding ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The formula for the averaged global geometric embedding is as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{(\\displaystyle\\frac{1}{N}\\sum_{i}Y^{1}(R_{i}^{1})\\oplus Y^{1}(R_{i}^{2})\\oplus Y^{1}(R_{i}^{3}),}\\\\ {\\displaystyle\\;\\frac{1}{N}\\sum_{i}Y^{2}(R_{i}^{1})\\oplus Y^{2}(R_{i}^{2})\\oplus Y^{2}(R_{i}^{3}),\\cdots,}\\\\ {\\displaystyle\\;\\frac{1}{N}\\sum_{i}Y^{l_{m a x}}(R_{i}^{1})\\oplus Y^{l_{m a x}}(R_{i}^{2})\\oplus Y^{l_{m a x}}(R_{i}^{3})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the superscript denotes the index of the column in the matrix. ", "page_idx": 21}, {"type": "text", "text": "J Proof of Invariant Latent and Query ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Given that the predicted frame $R$ is equivariant and the latent code $\\bigoplus_{l\\in L}(\\mathcal{R}_{K})_{l}$ , is also equivariant, when the input undergoes a transformation by a rotation $(R_{0},t_{0})\\in S E(3)$ , the frame is modified to $R_{0}R$ , and the latent code transforms to $\\begin{array}{r}{R_{0}\\cdot\\mathcal{R}_{K}=\\bigoplus_{l\\in L}D^{l}(R_{0})(\\mathcal{R}_{K})_{l}}\\end{array}$ . Applying the inverse of the equivariant frame to the latent code yields: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bigoplus_{l\\in L}D^{l}(R_{0}R)^{T}D^{l}(R_{0})(\\mathcal{R}_{K})_{l}=\\bigoplus_{l\\in L}D^{l}(R)^{T}D^{l}(R_{0})^{T}D^{l}(R_{0})(\\mathcal{R}_{K})_{l}=\\bigoplus_{l\\in L}D^{l}(R)^{T}(\\mathcal{R}_{K})_{l},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "demonstrating that the transformed latent code is invariant. Furthermore, when the input is subjected to a transformation by a rotation $(R_{0},t_{0})\\in S E(3)$ , the decoded camera $j$ \u2019s pose shifts to $(R_{0}R_{j}^{\\bar{}},R_{0}t_{j}+$ ", "page_idx": 21}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/9a0077c54a37f526dbbd2b2ab2ff221883ead91017b8b277368ba9be2c9fd18e.jpg", "img_caption": ["Figure 12: Equivariant Decoder. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "$t_{0}^{\\ }$ ). After subtracting the center, the pose becomes $(R_{0}R_{j},R_{0}(t_{j}-\\bar{t}))$ . Applying the inverse of the equivariant frame to the query camera pose, we arrive at: ", "page_idx": 22}, {"type": "equation", "text": "$$\n((R_{0}R)^{T}(R_{0}R_{j}),(R_{0}R)^{T}(R_{0}(t_{j}-\\Bar{t})))=(R^{T}R_{j},R^{T}(t_{j}-\\Bar{t})),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which confirms that the transformed query camera pose remains invariant. ", "page_idx": 22}, {"type": "text", "text": "K Alternative Equivariant Decoder ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The pipeline of the equivariant decoder is show in Figure 12. The cross-attention mechanism with equivariance processes two inputs: firstly, the resulting hidden features from self-attention, denoted as $\\bigoplus_{l\\in L}(\\mathcal{R}_{K}\\bar{)}_{l}$ with $L=\\{0,\\bar{1},\\cdot\\cdot\\cdot,l_{m a x}\\}$ , and secondly, the positional encoding of query rays and cameras, represented as $(P E(r_{u v}^{j},t_{j}-\\bar{t}))$ with spherical harmonics. In this context, $r_{u v}^{j}$ signifies the $(u,v)$ -th ray in the $j$ -th query camera, $t_{j}$ refers to the translation of the $j$ -th query camera, and $\\bar{t}$ is the pre-calculated center of the encoded cameras. The positional encoding is structured as $\\oplus_{l\\in L}\\bar{P}E_{l}$ with $L=\\{1,\\cdot\\cdot\\cdot,l_{m a x}\\}$ . Note that this encoding does not include the 0-type (invariant) image features, in contrast to the encoded input. Hence, when generating the $Q,K,V$ features in the attention module, $Q$ features do not include 0-th type features. To achieve invariant attention weights, $K$ features should also exclude 0-th type features, which is accomplished by setting $W_{0}=\\mathbf{0}$ in the equivariant linear layer. The multi-head attention mechanism adheres to the equations in Sec. F: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\langle(Q_{i})^{h},(K_{j})^{h}\\rangle=\\sum_{l\\in L}\\sum_{c}^{\\frac{C_{l}}{N_{h}}}(((Q_{l})_{i}^{h})_{c})^{T}((K_{l})_{j}^{h})_{c},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n(O_{i})^{h}=\\sum_{j}\\frac{e x p(\\langle(Q_{i})^{h},(K_{j})^{h}\\rangle)}{\\sum_{j}e x p(\\langle(Q_{i})^{h},(K_{j})^{h}\\rangle)}(V_{j})^{h},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $L=\\{1,2,\\cdot\\cdot\\cdot,l_{m a x}\\}$ , and $(V_{j})^{h}$ conforms to the $\\oplus_{l\\in\\{0,1,\\cdots,l_{m a x}\\}}(V_{l})_{j}^{h}$ format. The output $O$ is derived as $\\oplus_{l\\in\\{0,1,\\cdots,l_{m a x}\\}}{\\cal O}_{l}$ . ", "page_idx": 22}, {"type": "text", "text": "Since the final prediction value is unchanged when the reference frame is transformed, it is characterized as an invariant type-0 (scalar) feature. To extract the invariant features for prediction, we utilize an \u201cinvariant layer\", as depicted in Figure 13. In this process, we initially generate two intermediate features, $H^{\\prime}$ and $H^{\\prime\\prime}$ , of the same size. Subsequently, for each type- $l$ , we perform an inner product operation between $H_{l}^{\\prime}$ and $H_{l}^{\\prime\\prime}$ , which results in invariant features $I_{l}$ , each with a channel count of $C_{l}$ . By concatenating these invariant features $\\{I_{l}\\}$ , we formulate final invariant features $O_{0}^{\\prime}=I$ with a combined channel count of $\\sum_{l}C_{l}$ , which is then utilized for the final prediction. ", "page_idx": 22}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/676dac98b20894dbdd0bb98d51fc6497e4a6c7e1122ddcd945d607a36fc90a9b.jpg", "img_caption": ["Figure 13: Invariant Layer "], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "yRuJqoWoCs/tmp/08bc8b89087af46700993e7cac22e205ec9e953aa73040793516168ce25a0740.jpg", "table_caption": [], "table_footnote": ["Table 4: Different tasks and their corresponding geometric information in Equivariant Perceiver IO "], "page_idx": 23}, {"type": "text", "text": "L Network Architecture and Implementation Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Regarding architecture, we use a ResNet18 as the visual backbone, resulting in 960-dimensional features. The order of spherical harmonics is [1,2,4,8], resulting in $(3\\!+\\!5\\!+\\!9\\!+\\!17)^{*}2=68.$ -dimensional features. For encoding, visual and geometric features are concatenated to produce $960\\mathrm{~+~}68=$ 1028-dimensional embeddings. For decoding, we use the same Fourier encoding as the standard Perceiver IO, resulting in 186-dimensional embeddings. Our original latent representation $\\mathbf{R}$ is of dimensionality $1024\\mathrm{~x~}512$ . We set the number of channels for each type of the equivariant hidden feature as [512, 64,32, 8]. In the Perceiver IO implementation, we have 1 block of cross-attention with 1 head, 8 self-attention layers with 8 heads, and 1 cross-attention with 1 head for a decoder. ", "page_idx": 23}, {"type": "text", "text": "Our DeFiNe baseline has $73\\mathbf{M}$ parameters, and our EPIO implementation has 147M parameters. This increase is due to: additional parameters for the global geometric latent code as shown in Figure 5; inference for frame prediction as shown in Figure 2; and additional parameters for type-2, type-3, and type-4 features, where we set the channel numbers as 64, 32, and 8 respectively. Regarding runtime, we observed an increase of roughly $2\\mathbf{x}$ in training iteration times and $1.5\\mathrm{x}$ in per-pixel queries during inference. However, we would like to note that our approach converges in roughly $15\\%$ . Training and evaluation was conducted using distributed training (DDP) on 8 A100 GPUs, with 80 GB each. ", "page_idx": 23}, {"type": "text", "text": "Regarding experiments, we used Pytorch to implement our Equivariant Perceiver IO and will opensource our code and pre-trained weights upon acceptance. We used a batch size of 192, the AdamW optimizer with $\\beta=0.9$ , and $\\beta_{2}=0.999$ , weight decay of $10^{-4}$ , and an initial learning rate lr at $2^{^{\\star}}\\!\\times10^{-4}$ . . For ScanNet, the training duration was 200 epochs, with the learning rate being reduced by half every 80 epochs; For DeMon datasets, the training duration was 200 epochs, with the learning rate being reduced by half every 80 epochs. We used the same losses as DeFiNe, i.e., the L1-log loss, with a weight of 1.0 for real views and 0.2 for virtual views. Following standard practice, we used images of size 128x192 for ScanNet, and images of size $240\\mathrm{x}320$ for DeMoN, using two images as input, with corresponding intrinsics and extrinsics, and ground-truth depth maps as supervision (see Section 3.1). This is the standard training and evaluation protocol and was used by our baselines as well, ensuring a fair comparison. ", "page_idx": 23}, {"type": "text", "text": "M Extended Discussion for General Tasks of Equivariant Periceiver IO ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our model, designed as a general architecture, is adaptable to various tasks. This paper focuses on demonstrating the advantages of integrating equivariance into Perceiver IO for scene representation, ", "page_idx": 23}, {"type": "image", "img_path": "yRuJqoWoCs/tmp/cc162b6b981b5ac60dd8d54c98df730d4750f6bbb89b03a7f511f17fb206d403.jpg", "img_caption": ["Figure 14: Qualitative Results "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "primarily evaluated through depth estimation, a core problem in vision. While implementing our model for other tasks is beyond this paper\u2019s scope, we provide a brief overview of its potential extensions to different applications, as shown in table 4. ", "page_idx": 24}, {"type": "text", "text": "N More Qualitative Results for Depth Estimation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Please see Figure 14 for more qualitative results. ", "page_idx": 24}, {"type": "text", "text": "O More Experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "O.1 Comparison with Current Prevalent Depth Estimation Model ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We have evaluated DepthAnything on the same ScanNet stereo benchmark we report results as shown in Table5. we can confidently say that our method outperforms DepthAnything on this benchmark. However, we would like to emphasize that these are not meaningful comparisons. DepthAnything is a monocular depth estimation network that outputs affine-invariant predictions, while ours is a multi-view depth estimation network that outputs metric predictions. Hence, to achieve the reported DepthAnything numbers, we had to artificially shift and scale predictions using ground-truth depth maps (the same thing is done in their paper). We also could not use the second image as input to DepthAnything, since it is a monocular network, while our method can leverage multiple images as input by design (and even benefits from that during training via the virtual camera augmentation procedure). ", "page_idx": 24}, {"type": "table", "img_path": "yRuJqoWoCs/tmp/38d9f2816f4b3950e7c99d0833febecd24bc408ed19f19f548a52a17c63773ff.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "yRuJqoWoCs/tmp/e47f2a1b810a65c5865462a0b7fecef4338c5f8af3c9e1d9d389babe269ccce1.jpg", "table_caption": ["Table 6: Novel View Depth Estimation across a varying number of views. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "O.2 Varying views ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We performed additional small-scale experiments to study the impact of the number of available views. In this setting, we have 500 views of one scene, and we randomly choose N encoding views and 1 different decoding camera viewpoint for novel depth estimation. ", "page_idx": 25}, {"type": "text", "text": "For DeFiNe, we train with jittering augmentation on the reference frames and test with augmentation as well. For our model, we train without jittering augmentation and also test with augmentation. We explored 2, 3, and 4 views, and the Abs. Rel. depth estimation results are reported in Tab. 6 ", "page_idx": 25}, {"type": "text", "text": "As we can see, our method consistently surpasses DeFiNe across a varying number of views, even without employing augmentation during training, with the performance gap remaining similar across different view counts. ", "page_idx": 25}, {"type": "text", "text": "P Impact Statements ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "This work aims to advance $3D$ effective learning, with an application in $3D$ reconstruction. While the direct outcome of our research may not have immediate societal implications, the broader application of 3D reconstruction technologies could have some impact. A primary concern is the potential for privacy violations, particularly in scenarios where 3D reconstruction is used to create detailed representations of real-world environments or individuals without their consent. Such applications could lead to unauthorized surveillance or data collection, posing ethical and privacy challenges that need to be addressed as this technology advances and becomes more accessible. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We discuss the limitations of the work in Section 5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We don\u2019t include propositions or theorems in the paper, but we provide the proofs in the appendix for the statement in the paper for soundness. We provide proof of multi-head attention, invariant latent and query in Appendix. G and Appendix. J. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does our architecture is not limited to specific geometric entities. Safin et al.not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide network architecture and implementation details in Appendix L. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: We will release the code soon. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the training and test details in Section. 4.1 and Appendix. L ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: It was not possible to train independent models enough times to produce enough samples for a statistical analysis. We provide ablations showing the improvements generated by our contributions (all starting from the same random seed), and comparisons to state-of-the-art baselines (which also do not report error bars). ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide the detials of compute resources in the Appendix. L. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper discuss both potential positive societal impacts and negative societal impacts of the work performed in Appendix. P. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We cited the original papers that produce the code package or dataset. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: The paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}]