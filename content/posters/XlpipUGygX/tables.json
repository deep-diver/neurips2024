[{"figure_path": "XlpipUGygX/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of our action-value models against Stockfish 16, variants of Leela Chess Zero and AlphaZero (with and without Monte Carlo tree search), and GPT-3.5-turbo-instruct. Tournament Elo ratings are obtained by making the agents play against each other and cannot be directly compared to the Lichess Elo. Lichess (blitz) Elo ratings result from playing against either human opponents or bots on Lichess. Stockfish 16 with a time limit of 50ms per move is our data-generating oracle. Models operating on the PGN observe the full move history, whereas FENs only contain very limited historical information (sufficient for the fifty-move rule). Unlike all other engines, our policies were trained with supervised learning and use no explicit search at test time (except for GPT-3.5-turbo-instruct, which was trained via self-supervised learning and then instruction tuned).", "description": "This table compares the performance of the authors' action-value models against several other chess engines, including Stockfish, AlphaZero, and Leela Chess Zero.  It shows the tournament Elo ratings (from matches against other engines), Lichess Elo ratings (from games against humans and bots), and puzzle accuracy for each engine.  It highlights the fact that the authors' models use no explicit search at test time, in contrast to most other engines, and were trained via supervised learning.", "section": "3 Case-Study Results"}, {"figure_path": "XlpipUGygX/tables/tables_7_1.jpg", "caption": "Table 2: Ablating the predictor target, loss function, network depth, and number of value bins (see Section 3.4). The best configurations are: action-value prediction (see Appendix B.5 for a detailed discussion), HL-Gauss loss, depth 16, and 128 bins. We conduct further ablations (over the data sampler, the Stockfish time limit, and the model architecture) in Appendix B.1 and Table A2.", "description": "This table presents the ablation study's results, comparing various model parameters and configurations. It shows the impact of different predictor targets (action-value, state-value, behavioral cloning), loss functions (HL-Gauss, log, L2), network depths (2, 4, 8, 16, 32), and the number of value bins (16, 32, 64, 128, 256) on the model's performance.  The performance metrics used are puzzle accuracy, action accuracy, and Kendall's \u03c4, indicating the correlation between predicted and actual action rankings.  The table helps to identify the optimal hyperparameters for the model.", "section": "3.4 Variants and Ablations"}, {"figure_path": "XlpipUGygX/tables/tables_14_1.jpg", "caption": "Table 1: Comparison of our action-value models against Stockfish 16, variants of Leela Chess Zero and AlphaZero (with and without Monte Carlo tree search), and GPT-3.5-turbo-instruct. Tournament Elo ratings are obtained by making the agents play against each other and cannot be directly compared to the Lichess Elo. Lichess (blitz) Elo ratings result from playing against either human opponents or bots on Lichess. Stockfish 16 with a time limit of 50ms per move is our data-generating oracle. Models operating on the PGN observe the full move history, whereas FENs only contain very limited historical information (sufficient for the fifty-move rule). Unlike all other engines, our policies were trained with supervised learning and use no explicit search at test time (except for GPT-3.5-turbo-instruct, which was trained via self-supervised learning and then instruction tuned).", "description": "This table compares the performance of the authors' action-value models against several other chess engines, including Stockfish, AlphaZero, and Leela Chess Zero.  It shows the tournament Elo, Lichess Elo (against bots and humans), and puzzle accuracy for each engine.  The table highlights the performance of the authors' models, which achieve strong results despite not using explicit search at test time.  It also points out key differences in training methods and input data (FEN vs. PGN) between the authors' models and other engines.", "section": "3.1 Main Result"}, {"figure_path": "XlpipUGygX/tables/tables_15_1.jpg", "caption": "Table A2: Ablating the Stockfish time limit (used for dataset annotation), the data sampling method for training, and the model architecture (see Appendix B.1).", "description": "This table presents the results of ablations performed on the 9M parameter model, investigating the impact of Stockfish time limit, data sampling method, and model architecture on the model's performance. The metrics evaluated are puzzle accuracy, action accuracy, and Kendall's \u03c4, which measures the rank correlation between the predicted and ground-truth action distributions.", "section": "B.1 Additional Ablations"}, {"figure_path": "XlpipUGygX/tables/tables_16_1.jpg", "caption": "Table 1: Comparison of our action-value models against Stockfish 16, variants of Leela Chess Zero and AlphaZero (with and without Monte Carlo tree search), and GPT-3.5-turbo-instruct. Tournament Elo ratings are obtained by making the agents play against each other and cannot be directly compared to the Lichess Elo. Lichess (blitz) Elo ratings result from playing against either human opponents or bots on Lichess. Stockfish 16 with a time limit of 50ms per move is our data-generating oracle. Models operating on the PGN observe the full move history, whereas FENs only contain very limited historical information (sufficient for the fifty-move rule). Unlike all other engines, our policies were trained with supervised learning and use no explicit search at test time (except for GPT-3.5-turbo-instruct, which was trained via self-supervised learning and then instruction tuned).", "description": "This table compares the performance of the authors' action-value models against other state-of-the-art chess engines, including Stockfish, AlphaZero, and Leela Chess Zero.  It shows the tournament Elo, Lichess Elo (against both humans and bots), and puzzle accuracy for each engine.  Key differences in training methods and search usage are highlighted, emphasizing the novelty of the authors' searchless approach.", "section": "3.1 Main Result"}, {"figure_path": "XlpipUGygX/tables/tables_16_2.jpg", "caption": "Table A3: Inference times and legal move accuracy.", "description": "This table presents a comparison of inference times and legal move accuracy for various chess-playing agents. Inference times are measured on 1000 random boards from the Encyclopedia of Chess Openings (ECO). Legal move accuracy is assessed on 1000 random boards from three data sources: ECO, Puzzles, and a Test set. The table highlights the significant difference in inference times between agents that use search and those that don't, with searchless agents being considerably faster. It also shows that behavioral cloning models are superior in terms of legal move accuracy compared to action-value models.", "section": "B.3 Predicting Legal Moves"}, {"figure_path": "XlpipUGygX/tables/tables_18_1.jpg", "caption": "Table A4: Ranking the policies that arise from our three different predictors by having them play against each other in a tournament and computing relative Elo rankings (200 games per pairing; i.e., 600 games per column). When constructing the training data for all three predictors based on the same number of games (middle column), the action-value dataset is much larger than the state-value / behavioral cloning set, which leads to a stronger policy. When correcting for this by forcing the same number of training data points for all three (right column), the difference between state- and action-value prediction disappears.", "description": "This table shows the results of a tournament between three different chess playing policies trained using different prediction targets (action-value, state-value, and behavioral cloning).  The Elo ratings demonstrate the relative strength of each policy. The results are shown under two conditions: first, using the same number of games to create the training datasets, and second, using the same number of training data points for all three models. The table highlights how the larger dataset size for action-value prediction leads to a significant performance advantage. When the dataset size is equalized across methods, the difference between state-value and action-value prediction disappears.", "section": "B.5 Predictor-Target Comparison"}]