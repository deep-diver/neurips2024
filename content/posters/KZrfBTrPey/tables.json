[{"figure_path": "KZrfBTrPey/tables/tables_4_1.jpg", "caption": "Table 1: The performance comparison on Stereotypes. \u2193 means the lower the better alignment.", "description": "This table compares the performance of various LLMs (Large Language Models) on stereotype bias detection tasks across different evaluation settings.  The settings include zero-shot evaluations, untargeted and targeted system prompts, and evaluations using the ALI-Agent framework. Lower scores indicate better alignment, meaning the model is less likely to perpetuate harmful stereotypes. The table helps to demonstrate the effectiveness of the ALI-Agent framework in identifying model misalignment related to stereotypes.", "section": "3.1 Performance Comparison (RQ1)"}, {"figure_path": "KZrfBTrPey/tables/tables_6_1.jpg", "caption": "Table 1: The performance comparison on Stereotypes. \u2193 means the lower the better alignment.", "description": "This table compares the performance of several LLMs on the DecodingTrust dataset, which evaluates models' alignment with human values regarding stereotypes.  The table shows the average model agreeability scores for each LLM across three different evaluation settings: Zero-shot, Untargeted System Prompt, and Targeted System Prompt. A lower score indicates better alignment (less agreement with stereotypical statements).  The ALI-Agent results are also included, demonstrating its effectiveness in revealing misalignment.", "section": "3.1 Performance Comparison (RQ1)"}, {"figure_path": "KZrfBTrPey/tables/tables_6_2.jpg", "caption": "Table 2: The performance comparison on Morality", "description": "This table compares the performance of various LLMs (GPT-4, GPT-3.5, Gemini-Pro, ChatGLM3, Vicuna-7B, Vicuna-13B, Vicuna-33B, Llama 2-7B, Llama 2-13B, Llama 2-70B) across different evaluation settings on the ETHICS and Social Chemistry 101 datasets.  The evaluation settings include zero-shot, untargeted system prompts, evasive sentences, in-context attacks, and the proposed ALI-Agent method. The metric used is model agreeability, where a lower score indicates better alignment with human values.  The table shows the average misalignment rate for each LLM across all the evaluation settings for both datasets.", "section": "3.1 Performance Comparison (RQ1)"}, {"figure_path": "KZrfBTrPey/tables/tables_7_1.jpg", "caption": "Table 3: The performance comparison on Singapore Rapid Transit Systems Regulations (legality)", "description": "This table compares the performance of different LLMs on the task of evaluating their alignment with legal principles as defined by Singapore's Rapid Transit Systems Regulations.  The comparison uses various prompt engineering techniques, including zero-shot prompting, untargeted system prompts, evasive sentences, and in-context attacks. The model agreeability metric, where a lower score indicates better alignment, is used to evaluate the LLMs' responses.  The results show how ALI-Agent performs against existing evaluation methodologies in revealing misalignment.", "section": "3.1 Performance Comparison (RQ1)"}, {"figure_path": "KZrfBTrPey/tables/tables_7_2.jpg", "caption": "Table 4: The performance comparison on AdvBench (legality)", "description": "This table compares the performance of various LLMs on the AdvBench dataset, specifically focusing on the attack success rate (ASR).  A lower ASR indicates better alignment with human values. The table shows the ASR for different LLMs across six evaluation settings: Zero-shot, Evasive Sentences, In-Context Attack, Jailbreak Prompts, GPTFuzzer, ALI-Agent, and ALI-Agent combined with GPTFuzzer.  This allows for a comparison of how different LLMs and evaluation methods perform in identifying and mitigating harmful instructions.", "section": "3.1 Performance Comparison (RQ1)"}, {"figure_path": "KZrfBTrPey/tables/tables_17_1.jpg", "caption": "Table 5: The performance comparison between ALI-Agent and a Red LM on the CrowS-Pairs dataset. Model agreeability (%) is reported.", "description": "This table compares the performance of ALI-Agent and a Red LM (a fine-tuned GPT-3.5-turbo-1106 model trained on ALI-Agent's memory) in terms of model agreeability on the CrowS-Pairs dataset.  Model agreeability represents the percentage of times a model agrees with a test scenario containing misconduct; a higher percentage indicates greater misalignment.  The table shows the agreeability scores for various LLMs across different evaluation settings. The comparison aims to highlight ALI-Agent's ability to identify more misalignment cases compared to a simpler red team approach.", "section": "3.1 Performance Comparison (RQ1)"}, {"figure_path": "KZrfBTrPey/tables/tables_24_1.jpg", "caption": "Table 10: The performance comparison of different evaluators", "description": "This table compares the performance of three different methods for evaluating the alignment of LLMs with human values. The methods are: Rule Match, GPT-3.5, GPT-4, and the authors' proposed method (Ours). The metrics used for comparison are True Positive Rate (TPR), Accuracy, and F1 score. Higher values for TPR, Accuracy, and F1 score indicate better performance. The table shows that the authors' proposed method outperforms the other three methods in terms of TPR and F1 score, and is comparable to GPT-4 in terms of Accuracy.", "section": "D.4 Comparison of evaluators"}, {"figure_path": "KZrfBTrPey/tables/tables_31_1.jpg", "caption": "Table 1: The performance comparison on Stereotypes. \u2193 means the lower the better alignment.", "description": "This table presents the performance comparison of various LLMs across different evaluation settings on the DecodingTrust and CrowS-Pairs datasets, focusing on the aspect of stereotypes.  The metrics used are model agreeability, which reflects the percentage of times a model agrees with a test scenario containing misconduct (lower scores indicate better alignment), and Attack Success Rate (ASR), representing the percentage of times a model follows a harmful instruction. The table compares the results of different prompting methods (Zero-shot, Untargeted System Prompt, Targeted System Prompt, and ALI-Agent).", "section": "3.1 Performance Comparison (RQ1)"}, {"figure_path": "KZrfBTrPey/tables/tables_33_1.jpg", "caption": "Table 20: Performance comparison between parametric and textual memory on the CrowS-Pairs dataset. Model agreeability (%) is reported.", "description": "This table compares the performance of using parametric memory versus textual memory in the ALI-Agent framework.  The performance metric used is model agreeability (lower is better), showing the percentage of times a model agrees with a scenario containing misconduct. The table presents the model agreeability scores for several LLMs (GPT-4, GPT-3.5, Gemini-Pro, ChatGLM3, Vicuna-7B, Vicuna-13B, Vicuna-33B, Llama 2-7B, Llama 2-13B, Llama 2-70B) when using either parametric or textual memory.", "section": "3.3 Study on ALI-Agent (RQ3)"}]