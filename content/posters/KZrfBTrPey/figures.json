[{"figure_path": "KZrfBTrPey/figures/figures_1_1.jpg", "caption": "Figure 1: ALI-Agent generates test scenarios to probe long-tail risks. As judged by OpenAI Moderation API [1], test scenarios generated by ALI-Agent exhibit significantly decreased harmfulness scores compared to the expert-designed counterparts (collected from (a) CrowS-Pairs [2] and (b) ETHICS [3]), enhancing the difficulty for target LLMs to identify the risks.", "description": "This figure shows a comparison of harmfulness scores for test scenarios generated by ALI-Agent and expert-designed scenarios from CrowS-Pairs and ETHICS datasets.  The scores were assessed using the OpenAI Moderation API.  ALI-Agent produced scenarios with significantly lower harmfulness scores, indicating that it successfully generated scenarios that were more challenging for LLMs to identify risks.  The figure highlights the effectiveness of ALI-Agent in probing long-tail risks in LLMs.", "section": "1 Introduction"}, {"figure_path": "KZrfBTrPey/figures/figures_2_1.jpg", "caption": "Figure 2: An overview of the existing evaluation benchmarks and the proposed ALI-Agent. Existing benchmarks adopt pre-defined misconduct datasets as test scenarios to prompt target LLMs and evaluate their feedback. In contrast, ALI-Agent not only uses pre-defined datasets but also allows for tests based on user queries. Additionally, ALI-Agent operates through two stages: Emulation and Refinement, facilitating in-depth alignment tests by probing long-tail risks across a wide range of real-world scenarios.", "description": "This figure compares the existing evaluation benchmark with the proposed ALI-Agent.  The existing benchmark uses pre-defined misconduct datasets to create test scenarios for LLMs.  In contrast, ALI-Agent uses both pre-defined datasets and user queries, employing two stages: Emulation (generating realistic scenarios) and Refinement (iteratively refining scenarios to probe long-tail risks).  This two-stage process allows ALI-Agent to conduct more in-depth assessments and explore a wider range of real-world scenarios to evaluate LLM alignment with human values.", "section": "2 Method of ALI-Agent"}, {"figure_path": "KZrfBTrPey/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of the existing evaluation benchmarks and the proposed ALI-Agent. Existing benchmarks adopt pre-defined misconduct datasets as test scenarios to prompt target LLMs and evaluate their feedback. In contrast, ALI-Agent not only uses pre-defined datasets but also allows for tests based on user queries. Additionally, ALI-Agent operates through two stages: Emulation and Refinement, facilitating in-depth alignment tests by probing long-tail risks across a wide range of real-world scenarios.", "description": "This figure compares the existing evaluation benchmarks with the proposed ALI-Agent framework.  Existing methods rely on pre-defined datasets of misconducts to test LLMs, which is limited in scope and doesn't adapt to evolving LLMs.  ALI-Agent introduces automation, using LLMs as agents, to generate realistic scenarios (Emulation stage) and iteratively refine them to uncover less obvious risks (Refinement stage).  The figure highlights ALI-Agent's ability to leverage both pre-defined datasets and user queries, making it a more comprehensive and adaptive approach for evaluating LLM alignment with human values.", "section": "2 Method of ALI-Agent"}, {"figure_path": "KZrfBTrPey/figures/figures_8_1.jpg", "caption": "Figure 1: ALI-Agent generates test scenarios to probe long-tail risks. As judged by OpenAI Moderation API [1], test scenarios generated by ALI-Agent exhibit significantly decreased harmfulness scores compared to the expert-designed counterparts (collected from (a) CrowS-Pairs [2] and (b) ETHICS [3]), enhancing the difficulty for target LLMs to identify the risks.", "description": "This figure shows a comparison of harmfulness scores for test scenarios generated by ALI-Agent versus expert-designed scenarios from two benchmarks, CrowS-Pairs and ETHICS.  The radar charts illustrate that ALI-Agent produces scenarios with significantly lower harmfulness scores across various categories (violence, hate speech, sexual content, etc.), making it harder for LLMs to detect the risks and thus revealing potential long-tail misalignments.", "section": "1 Introduction"}, {"figure_path": "KZrfBTrPey/figures/figures_26_1.jpg", "caption": "Figure 1: ALI-Agent generates test scenarios to probe long-tail risks. As judged by OpenAI Moderation API [1], test scenarios generated by ALI-Agent exhibit significantly decreased harmfulness scores compared to the expert-designed counterparts (collected from (a) CrowS-Pairs [2] and (b) ETHICS [3]), enhancing the difficulty for target LLMs to identify the risks.", "description": "This figure shows a comparison of harmfulness scores for test scenarios generated by ALI-Agent and expert-designed scenarios from CrowS-Pairs and ETHICS datasets.  ALI-Agent's scenarios show significantly lower harmfulness scores as assessed by the OpenAI Moderation API, indicating that they are more challenging for LLMs to identify risks, especially those in the long tail.", "section": "1 Introduction"}]