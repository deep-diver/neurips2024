[{"figure_path": "HbIBqn3grD/figures/figures_2_1.jpg", "caption": "Figure 1: The NM-RNN consists of two subnetworks: a low-rank subnetwork that generates the output (bottom) and a smaller, full-rank neuromodulatory sub-network (top).", "description": "The figure illustrates the architecture of the Neuromodulated Recurrent Neural Network (NM-RNN).  It comprises two main components: a neuromodulatory subnetwork and an output-generating subnetwork. The neuromodulatory subnetwork is a smaller, fully connected RNN that generates a time-varying neuromodulatory signal. This signal then modulates the weights of the larger, low-rank output-generating subnetwork.  The output-generating subnetwork is a low-rank recurrent neural network that produces the final output of the system. The neuromodulatory signal dynamically scales the low-rank recurrent weights of the output-generating network, providing flexibility and adaptability.", "section": "3 Neuromodulated recurrent neural networks"}, {"figure_path": "HbIBqn3grD/figures/figures_3_1.jpg", "caption": "Figure 2: A. Illustration of how neuromodulatory signals s(t) affect decay rates of the state w(t) in a 1-D, simplified model (derived in Section 3.1), calculated with prespecified s(t) signal. As s(t) approaches 0, w(t) decays more rapidly. B. & C. Visual comparison of an LSTM and an NM-RNN. Corresponding parts of the networks are highlighted with shaded rectangles. Blue: a forget gate computation. Green: an input gate to recurrent dynamics. Purple: recurrent feedback onto the modulatory state variable.", "description": "This figure compares the NM-RNN and LSTM networks, highlighting their similarities. Panel A shows how neuromodulation affects the decay rate of a simplified model's state variable. Panels B and C provide a visual comparison of the NM-RNN and LSTM, emphasizing the structural similarities between them by highlighting corresponding components with color-coded rectangles.", "section": "3 Neuromodulated recurrent neural networks"}, {"figure_path": "HbIBqn3grD/figures/figures_5_1.jpg", "caption": "Figure 3: A. Visualization of Measure-Wait-Go task. B. Theoretical predictions (dashed lines) match closely with empirical results (solid lines) for rank-1 network. C. L2 loss comparsion of four model types on MWG task. We trained 10 randomly initialized parameter-matched low-rank RNNs, NM-RNNs, vanilla RNNs, and LSTMs. Median losses shown by bars. Performance of visualized LR-RNN and NM-RNN are starred (example visualizations for all four models shown in Supp. Fig. 1). D. Comparison of model-generated output ramps for both trained (purple) and extrapolated (red, blue) intervals. Dashed lines show target outputs. E.Three-dimensional neuromodulatory signal s(t) for trained/extrapolated intervals. Left, traces are aligned to start of trial. Right, traces are aligned to 'go' cue. F. Resulting output traces when ablating each component of s(t). In all panels, colors reflect trained/extrapolated intervals (see legend). For output plots, dashed grey lines are targets. Additional model visualizations in Supp. Fig. 1-2.", "description": "This figure shows the results of applying the NM-RNN model to a timing task, comparing its performance to other RNN architectures. Panel A shows a schematic of the task. Panel B shows the close match between theoretical predictions and empirical results obtained using a rank-1 NM-RNN. Panel C compares the performance (measured by L2 loss) of NM-RNNs, low-rank RNNs, vanilla RNNs and LSTMs on the task. Panel D visualizes the output ramps generated by these models for trained and extrapolated intervals. Panel E shows the three-dimensional neuromodulatory signal (s(t)) during the task. Panel F shows the impact of ablating each component of the neuromodulatory signal on the model's performance.", "section": "4 Time interval reproduction"}, {"figure_path": "HbIBqn3grD/figures/figures_7_1.jpg", "caption": "Figure 3: A. Visualization of Measure-Wait-Go task. B. Theoretical predictions (dashed lines) match closely with empirical results (solid lines) for rank-1 network. C. L2 loss comparsion of four model types on MWG task. We trained 10 randomly initialized parameter-matched low-rank RNNs, NM-RNNs, vanilla RNNs, and LSTMs. Median losses shown by bars. Performance of visualized LR-RNN and NM-RNN are starred (example visualizations for all four models shown in Supp. Fig. 1). D. Comparison of model-generated output ramps for both trained (purple) and extrapolated (red, blue) intervals. Dashed lines show target outputs. E. Three-dimensional neuromodulatory signal s(t) for trained/extrapolated intervals. Left, traces are aligned to start of trial. Right, traces are aligned to 'go' cue. F. Resulting output traces when ablating each component of s(t). In all panels, colors reflect trained/extrapolated intervals (see legend). For output plots, dashed grey lines are targets. Additional model visualizations in Supp. Fig. 1-2.", "description": "This figure shows the results of the Measure-Wait-Go (MWG) task. It compares the performance of four different RNN models: low-rank RNNs, NM-RNNs, vanilla RNNs, and LSTMs. The figure includes plots showing the theoretical predictions vs. empirical results for a rank-1 network, a comparison of L2 losses for the four models, a comparison of the generated output ramps for trained and extrapolated intervals, the three-dimensional neuromodulatory signal, and the resulting output traces when ablating each component of the neuromodulatory signal. The results show that the NM-RNN outperforms other models in both accuracy and generalization.", "section": "Time interval reproduction"}, {"figure_path": "HbIBqn3grD/figures/figures_8_1.jpg", "caption": "Figure 5: A. Visualization of the Element Finder Task. B. MSE losses attained across multiple runs in different classes of models trained on the EFT (median is indicated by black lines). C. Training loss curves for selected parameter-matched models. The NM-RNNs have hyperparameter counts 1: (M=5, N=18, R=8), 2: (M=5, N=13, R=12), and 3: (M=10, N=12, R=7). D. Visualization of selected components of s(t) for an example NM-RNN, shown across different query indices. E. Trajectories for the top two PCs of z(t) across different query indices. The different trajectories converge to an approximate line attractor (black) encoding query index. The time at which the queried element arrives is marked in red. F. Top two PCs of x(t), visualized for different query indices and target element values. Each trajectory converges to a fixed point on an approximate line attractor encoding element value. Each curve shown in D, E, and F is averaged over 100 trials. Additional visualizations in Supp. Fig. 5-6.", "description": "This figure shows the results of the Element Finder Task (EFT). Panel A describes the EFT. Panel B compares the performance of different RNN models (LSTM, NM-RNNs, LR-RNNs, and RNNs) on the EFT in terms of Mean Squared Error (MSE) loss. Panel C displays the training dynamics (MSE loss over iterations) for different NM-RNN models and other models. Panels D-F provide visualizations of the NM-RNN's internal dynamics, showing how the neuromodulatory signal (s(t)) and the latent states of the neuromodulatory (z(t)) and output-generating (x(t)) subnetworks evolve across different query indices and target values during the EFT.", "section": "6 Capturing long-term dependencies via neuromodulation"}]