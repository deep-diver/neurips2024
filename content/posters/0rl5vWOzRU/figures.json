[{"figure_path": "0rl5vWOzRU/figures/figures_1_1.jpg", "caption": "Figure 1: Density estimates of the mollified Cantor distribution (left) using a DDM with schedule T = {t}100 generated with 100 linearly spaces discretisation times t\u2081 = i/100 (middle), compared to the optimised schedule T* = {t}500 with 50 discretisation times t generated by Algorithm 1 (right). The eight modes present in our true mollified distribution are shown in grey on each plot.", "description": "This figure compares density estimations of a mollified Cantor distribution generated using three different methods. The leftmost plot shows the actual density of the mollified Cantor distribution. The middle plot shows the density estimate generated by a Denoising Diffusion Model (DDM) using a linear schedule, which clearly fails to capture the eight modes of the distribution. The rightmost plot shows the density estimate generated by a DDM using an optimized schedule generated by the proposed Algorithm 1, which accurately captures the eight modes.", "section": "1 Introduction"}, {"figure_path": "0rl5vWOzRU/figures/figures_7_1.jpg", "caption": "Figure 2: Comparison of Linear and Learned Schedules over Training Iterations for the bimodal example. Each point corresponds to 500 training iterations.", "description": "This figure compares the performance of a linear schedule and a learned schedule for training a diffusion model on a bimodal Gaussian distribution.  The left panel shows the average likelihood of samples generated by the model over training iterations. The right panel displays the model's score error, which measures the discrepancy between the model's predicted score and the true score of the data distribution.  The results show that the learned schedule leads to higher likelihoods and lower score errors compared to the linear schedule.", "section": "4.2 Adaptive Schedule Learning for Bimodal Example"}, {"figure_path": "0rl5vWOzRU/figures/figures_8_1.jpg", "caption": "Figure 3: (Left) Incremental costs \u221aL(tj+1, tj) for the cosine schedule and our online adaptive algorithm. Higher learning rates enforce equalisation of costs more quickly. (Right) Progression of the learned schedule during 40k training iterations, depicted through the standard-deviation \u221a1 \u2013 \u0101tn.", "description": "This figure shows two plots. The left plot compares the incremental costs for the cosine schedule and the online adaptive algorithm. The right plot shows the progression of the learned schedule over 40,000 training iterations, visualized by the standard deviation of the noise level.", "section": "4.3 Scalable Schedule Learning Diffusion"}, {"figure_path": "0rl5vWOzRU/figures/figures_16_1.jpg", "caption": "Figure 5: Schedules and density estimates for: linear (blue); Stein score optimised (green); and predictor optimised (red) schedules. The predictor optimised schedule identifies a bump along the diffusion path where the reference Gaussian density splits into two modes. In the regions where the score is evaluated (around \u00b16), our trained score is accurate compared to the linear schedule score, which fails to match the slope of the true score, resulting in a wider variance density estimate.", "description": "This figure compares three different schedules and their resulting density estimates for a bimodal Gaussian distribution.  The top panel shows the three schedules: a linear schedule (blue), a corrector-optimized schedule (green), and a predictor-optimized schedule (red). The middle panel displays density estimates for each schedule, along with the true density of the bimodal Gaussian distribution. The bottom panel shows the score estimates for each schedule. The results show that the optimized schedules yield more accurate density estimates than the linear schedule, particularly near the modes of the distribution.", "section": "4.2 Adaptive Schedule Learning for Bimodal Example"}, {"figure_path": "0rl5vWOzRU/figures/figures_17_1.jpg", "caption": "Figure 5: Schedules and density estimates for: linear (blue); Stein score optimised (green); and predictor optimised (red) schedules. The predictor optimised schedule identifies a bump along the diffusion path where the reference Gaussian density splits into two modes. In the regions where the score is evaluated (around \u00b16), our trained score is accurate compared to the linear schedule score, which fails to match the slope of the true score, resulting in a wider variance density estimate.", "description": "This figure compares three different schedules for a bimodal Gaussian distribution: linear, Stein score optimized, and predictor optimized. The predictor optimized schedule shows improved accuracy in estimating the true score compared to the linear schedule, leading to a more precise density estimation.", "section": "C Experiment Details"}, {"figure_path": "0rl5vWOzRU/figures/figures_18_1.jpg", "caption": "Figure 7: Evolution of the estimated score for the mollified Cantor distribution Section 4.1 with a Corrector Optimised Schedule. In this case the linear schedule fails to evenly progress the progression of the score, see Figure 6 showing the terminal score estimate in this case. The estimated score exhibits a self-similar nature of interweaving roots around the centers of mass of the mollified Cantor distribution. Identification of these roots amounts to estimated modes in our density estimate, see Figure 1.", "description": "This figure shows the evolution of the estimated score for the mollified Cantor distribution across different time steps (t = 1 to t = 39). Each subplot displays the estimated score as a function of x for a specific time step.  The figure highlights how the linear schedule fails to capture the multimodal nature of the distribution, unlike the corrector-optimized schedule.  The self-similar fractal structure of the score is also shown, indicating the complexity of the distribution.", "section": "4 Computational Experiments"}, {"figure_path": "0rl5vWOzRU/figures/figures_19_1.jpg", "caption": "Figure 8: Corrector optimised and predictor optimised schedules for the 4 image datasets, CIFAR10, FFHQ, AFHQv2 and ImageNet.", "description": "This figure displays the corrector and predictor optimized schedules for four different image datasets: CIFAR-10, FFHQ, AFHQv2, and ImageNet.  The x-axis represents the timestep in the diffusion process, and the y-axis represents the noise level (sigma). Each dataset has two lines: one solid line representing the corrector optimized schedule and one dashed line representing the predictor optimized schedule.  The schedules show how the noise level decreases over time during the sampling process.  The different lines illustrate the variations in optimal scheduling strategies across different datasets with varying complexities and data characteristics. The curves show the decay of noise level in the diffusion process to yield high quality samples.", "section": "4.3 Scalable Schedule Learning Diffusion"}, {"figure_path": "0rl5vWOzRU/figures/figures_19_2.jpg", "caption": "Figure 8: Corrector optimised and predictor optimised schedules for the 4 image datasets, CIFAR10, FFHQ, AFHQv2 and ImageNet.", "description": "This figure displays the corrector and predictor optimized schedules for four image datasets: CIFAR10, FFHQ, AFHQv2, and ImageNet.  The x-axis represents the timestep in the generative process, and the y-axis represents the noise level (\u03c3). Each dataset has two lines: one for the corrector-optimized schedule and one for the predictor-optimized schedule. The schedules show how the noise level changes over time during the sampling process.  The shapes of the curves reveal the different strategies employed to navigate the diffusion process for each dataset and optimization method.", "section": "4.3 Scalable Schedule Learning Diffusion"}, {"figure_path": "0rl5vWOzRU/figures/figures_20_1.jpg", "caption": "Figure 10: Progression of the length and energy Equation (18) over training of MNIST. Both models are trained from initialisation, one with adaptive schedule learning (red) and one without (blue). We can see that the energy and length quantities increase during training. Recall that for a fixed path of scorest log pt that the length A is constant. As we are learning the score, this value is not constant during training. Interestingly, by optimising the schedule during training we observe a larger length value, possibly indicating that the diffusion path learned with the optimised schedule differs greatly from the path learned without.", "description": "This figure shows the progression of the length and energy of the diffusion path during the training of a MNIST model, comparing a model with adaptive schedule learning and one without.  The length and energy are metrics derived from the cost function used to optimize the schedule. The figure indicates that the optimized schedule leads to a longer diffusion path, suggesting that it explores the data distribution differently compared to the fixed cosine schedule.", "section": "3.1 Diffusion Schedule Path Length and Energy"}, {"figure_path": "0rl5vWOzRU/figures/figures_20_2.jpg", "caption": "Figure 11: Sample progression of MNIST digits for the standard cosine schedule with \\(\\epsilon = 0.008\\) (top) against our optimised schedule (bottom). As we can see, the cosine schedule spends more time near the Gaussian reference distribution whereas the optimised schedule quickly determines large scale features and spends more time toward the data distribution.", "description": "This figure compares the sample generation process using a standard cosine schedule and the optimized schedule proposed in the paper. The top row shows samples generated using the cosine schedule, which spends more time near the Gaussian reference distribution.  The bottom row shows samples generated using the optimized schedule, which quickly identifies large-scale features and focuses more on the data distribution.", "section": "4.3 Scalable Schedule Learning Diffusion"}, {"figure_path": "0rl5vWOzRU/figures/figures_20_3.jpg", "caption": "Figure 10: Progression of the length and energy Equation (18) over training of MNIST. Both models are trained from initialisation, one with adaptive schedule learning (red) and one without (blue). We can see that the energy and length quantities increase during training. Recall that for a fixed path of scorest log pt that the length A is constant. As we are learning the score, this value is not constant during training. Interestingly, by optimising the schedule during training we observe a larger length value, possibly indicating that the diffusion path learned with the optimised schedule differs greatly from the path learned without.", "description": "This figure shows the progression of length and energy during the training process of a MNIST model with and without adaptive schedule learning. The length and energy increase during training.  The optimized schedule results in a larger length value, suggesting a difference in the learned diffusion path compared to the fixed schedule.", "section": "3.1 Diffusion Schedule Path Length and Energy"}]