[{"figure_path": "0rl5vWOzRU/tables/tables_6_1.jpg", "caption": "Table 1: Sample quality measured by Fr\u00e9chet Inception Distance (FID) versus schedule on CIFAR10 (32 \u00d7 32), FFHQ, AFHQv2, ImageNet (64 \u00d7 64). Pretrained models are used from Karras et al. (2022). All FIDs are calculated using 50000 samples. We highlight the best FID in bold. The ImageNet model lacks second-order differentiation, so no predictor optimised schedule is shown.", "description": "This table compares the Fr\u00e9chet Inception Distance (FID) scores, a measure of sample quality, for different schedules used in sampling from pre-trained diffusion models on four image datasets: CIFAR-10, FFHQ, AFHQv2, and ImageNet.  The schedules compared include various previously proposed methods (Eq (22) with \u03c1=3 and \u03c1=7, LogLinear, Convex), and the novel corrector and predictor-optimized schedules proposed in the paper.  The best FID score for each dataset is highlighted in bold.  Note that the ImageNet results only include the corrector-optimized schedule because the model lacks second-order differentiation, which is needed for the predictor-optimized schedule.", "section": "4.4 Sampling Pre-Trained Models"}, {"figure_path": "0rl5vWOzRU/tables/tables_8_1.jpg", "caption": "Table 1: Sample quality measured by Fr\u00e9chet Inception Distance (FID) versus schedule on CIFAR10 (32 \u00d7 32), FFHQ, AFHQv2, ImageNet (64 \u00d7 64). Pretrained models are used from Karras et al. (2022). All FIDs are calculated using 50000 samples. We highlight the best FID in bold. The ImageNet model lacks second-order differentiation, so no predictor optimised schedule is shown.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores, a measure of sample quality, for different sampling schedules on four image datasets: CIFAR-10, FFHQ, AFHQv2, and ImageNet.  The schedules compared include those from prior work (Equation (22) with p=3 and p=7, LogLinear) and the novel corrector and predictor-optimized schedules proposed in the paper. Lower FID scores indicate better sample quality.  The results show that the proposed optimized schedules achieve FID scores competitive with or better than the best-performing schedule from prior work.  The ImageNet results exclude the predictor-optimized schedule due to the model's limitations.", "section": "4.4 Sampling Pre-Trained Models"}, {"figure_path": "0rl5vWOzRU/tables/tables_8_2.jpg", "caption": "Table 1: Sample quality measured by Fr\u00e9chet Inception Distance (FID) versus schedule on CIFAR10 (32 \u00d7 32), FFHQ, AFHQv2, ImageNet (64 \u00d7 64). Pretrained models are used from Karras et al. (2022). All FIDs are calculated using 50000 samples. We highlight the best FID in bold. The ImageNet model lacks second-order differentiation, so no predictor optimised schedule is shown.", "description": "This table compares the Fr\u00e9chet Inception Distance (FID) scores, a metric for evaluating the quality of generated images, across different scheduling methods for four image datasets: CIFAR-10, FFHQ, AFHQv2, and ImageNet.  The schedules compared include those derived from the paper's proposed methods (Corrector and Predictor Optimized) as well as existing methods (LogLinear, and schedules from Karras et al. 2022). Lower FID scores indicate higher quality images.  The results show that the proposed methods achieve competitive performance compared to the existing schedules.", "section": "4.4 Sampling Pre-Trained Models"}, {"figure_path": "0rl5vWOzRU/tables/tables_9_1.jpg", "caption": "Table 2: Comparison of FID across different amounts of discretisation points for different schedules on CIFAR10. CO stands for our corrector optimised schedule.", "description": "This table shows how the number of discretisation points, T, used during sampling affects the quality of generated samples for different schedules.  It demonstrates that the FID (Fr\u00e9chet Inception Distance, a metric for image quality) decreases with increasing T for all schedules. However, when T is small (low resolution), only the optimised schedules maintain stable performance, indicating improved efficiency.", "section": "4.4 Sampling Pre-Trained Models"}, {"figure_path": "0rl5vWOzRU/tables/tables_19_1.jpg", "caption": "Table 3: Comparison of sFID across different amounts of discretisation points for different schedules on CIFAR10. CO stands for our corrector optimised schedule.", "description": "This table presents the sFID (a measure of sample quality) achieved by different sampling schedules on the CIFAR-10 dataset.  Different numbers of discretization points (T) were used with each schedule, to analyze how the number of discretization points affects the quality of the generated samples. The schedules include the corrector-optimized schedule from the proposed algorithm, and three other schedules based on varying values of the parameter \u03c1 (3, 7, and 100).", "section": "4.4 Sampling Pre-Trained Models"}]