[{"figure_path": "umukvCdGI6/figures/figures_1_1.jpg", "caption": "Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1", "description": "This figure presents the evaluation results of various models on the Tabular Benchmark.  The x-axis represents the number of random search iterations, and the y-axis represents the normalized test accuracy (for classification tasks) or R2 score (for regression tasks). Each line represents a different model, and their relative positions indicate their performance.  The average performance across multiple datasets within each benchmark (medium-sized, large-sized, classification and regression) is shown.", "section": "4.2 Performance Evaluation"}, {"figure_path": "umukvCdGI6/figures/figures_4_1.jpg", "caption": "Figure 2: (a) Condition Generation: For each column xi, Ncond conditions are generated through an individual sub-network \u03941i. The aggregate of the conditions of all columns is denoted by the matrix M. (b) Relaxed ODT Construction: The condition matrix M is shuffled (i.e. permutation with \u03c0) and reshape into O, representing NrODT rODTs each with depth d. (c) Forest Construction: To compute the weights wi, an individual sub-networks \u03942i is applied to each rODT. In addition, each wi is paired with a learnable embedding vector ei. The aggregate of all weights and their corresponding embedding vectors are denoted as w and E, respectively.", "description": "This figure illustrates the three main modules of the DOFEN model: Condition Generation, Relaxed ODT Construction, and Forest Construction.  The Condition Generation module uses sub-networks (\u03941i) to generate conditions for each column (xi) of the input data, resulting in a matrix M. The Relaxed ODT Construction module shuffles and reshapes this matrix M into a matrix O representing NrODT relaxed oblivious decision trees (rODTs), each with depth d. Finally, the Forest Construction module uses sub-networks (\u03942i) to calculate weights (wi) for each rODT, paired with embedding vectors (ei), which are then aggregated to form w and E respectively. This process creates differentiable counterparts to ODTs which are then combined into a final model. ", "section": "3.2 DOFEN Model"}, {"figure_path": "umukvCdGI6/figures/figures_5_1.jpg", "caption": "Figure 3: (a) Forest Construction: First, Nestimator pairs of (wi, \u0113i) are randomly sampled to form w' and E'. Secondly, w' is transformed through a softmax function, and is used for computing the weighted sum of E' to form forest embedding f. (b) Forest Ensemble: a shared-weight sub-network A3 is employed to make a prediction \u0177 for each embedding. The final prediction is the average of all \u0177 values, and the total loss is the sum of their individual losses.", "description": "This figure illustrates the two-level ensemble process in DOFEN.  (a) shows how Nestimator pairs of weights (wi) and embedding vectors (ei) are randomly sampled from a larger pool to construct an rODT forest. These weights are then processed through a softmax function to obtain a weighted sum of the embeddings, resulting in a forest embedding (f). (b) shows how multiple rODT forests are created, each with its own embedding (f), and then combined via a shared sub-network (\u03943) to produce a final prediction. The final prediction is the average of the individual predictions from each forest, and the loss is the sum of the individual losses.", "section": "3.2.3 Two-level Relaxed ODT Ensemble"}, {"figure_path": "umukvCdGI6/figures/figures_6_1.jpg", "caption": "Figure 3: (a) Forest Construction: First, Nestimator pairs of (wi, \u0113i) are randomly sampled to form w' and E'. Secondly, w' is transformed through a softmax function, and is used for computing the weighted sum of E' to form forest embedding f. (b) Forest Ensemble: a shared-weight sub-network A3 is employed to make a prediction \u0177 for each embedding. The final prediction is the average of all \u0177 values, and the total loss is the sum of their individual losses.", "description": "This figure illustrates the two-level ensemble process used in DOFEN. (a) shows how individual rODT forests are constructed by randomly sampling weights and embeddings and using a softmax function to obtain the final forest embedding. (b) shows how these forest embeddings are used to make predictions using a shared sub-network, and the final prediction is the average of all predictions.", "section": "3.2.3 Two-level Relaxed ODT Ensemble"}, {"figure_path": "umukvCdGI6/figures/figures_7_1.jpg", "caption": "Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1", "description": "This figure shows the performance of DOFEN and other baseline models on the Tabular Benchmark.  The x-axis represents the number of random search iterations, while the y-axis shows the normalized test accuracy (for classification) or R2 score (for regression).  The lines represent the different models, with their names ordered by performance after hyperparameter optimization.  The results are averaged across multiple datasets within each benchmark category (medium classification, medium regression, large classification, large regression). The detailed number of datasets in each category can be found in Appendix B.1 of the paper.", "section": "4.2 Performance Evaluation"}, {"figure_path": "umukvCdGI6/figures/figures_9_1.jpg", "caption": "Figure 5: In the covertype dataset, Figure 5a shows that the average weights of true positives differ significantly from those of true negatives. Conversely, Figure 5b reveals a contrasting result for rODTs with small weight variation.", "description": "This figure shows the average weights of true positives (TP) and true negatives (TN) for the top 25 rODTs, sorted by the standard deviation of their weights.  Figure 5a demonstrates a clear separation between TP and TN weights, indicating that rODTs with high weight variation are more influential in classification. Figure 5b contrasts this, showing little difference in average weights between TP and TN for rODTs with low weight variation, meaning these are less impactful in the classification process.", "section": "Additional Analysis"}, {"figure_path": "umukvCdGI6/figures/figures_9_2.jpg", "caption": "Figure 5: In the covertype dataset, Figure 5a shows that the average weights of true positives differ significantly from those of true negatives. Conversely, Figure 5b reveals a contrasting result for rODTs with small weight variation.", "description": "This figure visualizes the average weights of true positives (TP) and true negatives (TN) for relaxed oblivious decision trees (rODTs) in the covertype dataset.  Figure 5a shows a significant difference in average weights between TP and TN for rODTs with large weight variations, indicating that these rODTs play a more crucial role in classification.  In contrast, Figure 5b demonstrates that this difference is minimal for rODTs with small weight variations.", "section": "4.3.2 Weights of Individual Relaxed ODT"}, {"figure_path": "umukvCdGI6/figures/figures_14_1.jpg", "caption": "Figure 2: (a) Condition Generation: For each column xi, Ncond conditions are generated through an individual sub-network A1i. The aggregate of the conditions of all columns is denoted by the matrix M. (b) Relaxed ODT Construction: The condition matrix M is shuffled (i.e. permutation with \u03c0) and reshape into O, representing NrODT rODTs each with depth d. (c) Forest Construction: To compute the weights wi, an individual sub-networks A2i is applied to each rODT. In addition, each wi is paired with a learnable embedding vector e\u00bf. The aggregate of all weights and their corresponding embedding vectors are denoted as w and E, respectively.", "description": "This figure illustrates the three main modules of the DOFEN model: Condition Generation, Relaxed ODT Construction, and Forest Construction.  The Condition Generation module uses sub-networks (\u03941) to generate soft conditions for each column. These are aggregated into matrix M.  The Relaxed ODT Construction module shuffles and reshapes M into matrix O, creating a pool of relaxed oblivious decision trees (rODTs).  Finally, the Forest Construction module uses sub-networks (\u03942) and embedding vectors (ei) to compute weights for each rODT and combines them into a forest embedding.", "section": "3.2 DOFEN Model"}, {"figure_path": "umukvCdGI6/figures/figures_14_2.jpg", "caption": "Figure 2: (a) Condition Generation: For each column xi, Ncond conditions are generated through an individual sub-network A1i. The aggregate of the conditions of all columns is denoted by the matrix M. (b) Relaxed ODT Construction: The condition matrix M is shuffled (i.e. permutation with \u03c0) and reshape into O, representing NrODT rODTs each with depth d. (c) Forest Construction: To compute the weights wi, an individual sub-networks A2i is applied to each rODT. In addition, each wi is paired with a learnable embedding vector ei. The aggregate of all weights and their corresponding embedding vectors are denoted as w and E, respectively.", "description": "This figure illustrates the three main modules of DOFEN model: Condition Generation, Relaxed ODT Construction, and Forest Construction.  (a) shows how individual sub-networks process each column to generate multiple soft conditions, which are then aggregated into a matrix M. (b) demonstrates how the conditions in M are randomly shuffled and reshaped to create a pool of relaxed oblivious decision trees (rODTs). (c) details how a two-level ensemble process uses sub-networks to compute weights and embedding vectors for each rODT, which are used in subsequent layers to enhance predictive performance.", "section": "3.2 DOFEN Model"}, {"figure_path": "umukvCdGI6/figures/figures_14_3.jpg", "caption": "Figure 2: (a) Condition Generation: For each column xi, Ncond conditions are generated through an individual sub-network \u03941i. The aggregate of the conditions of all columns is denoted by the matrix M. (b) Relaxed ODT Construction: The condition matrix M is shuffled (i.e. permutation with \u03c0) and reshape into O, representing NrODT rODTs each with depth d. (c) Forest Construction: To compute the weights wi, an individual sub-networks \u03942i is applied to each rODT. In addition, each wi is paired with a learnable embedding vector ei. The aggregate of all weights and their corresponding embedding vectors are denoted as w and E, respectively.", "description": "This figure illustrates the three main modules of the DOFEN model: Condition Generation, Relaxed ODT Construction, and Forest Construction.  The Condition Generation module uses sub-networks (\u03941i) to generate soft conditions for each column (xi) in the input data. These conditions are aggregated into a matrix (M). The Relaxed ODT Construction module shuffles and reshapes the condition matrix (M) to create a pool of relaxed oblivious decision trees (rODTs), each with a depth (d). Finally, the Forest Construction module uses sub-networks (\u03942i) and embedding vectors (ei) to compute weights (wi) for each rODT. These weights and embeddings are combined into vectors (w and E) for the next stage of the DOFEN model.", "section": "3.2 DOFEN Model"}, {"figure_path": "umukvCdGI6/figures/figures_23_1.jpg", "caption": "Figure 9: Overfitting arises when not using sampling in the forest ensemble, affecting both (a) classification and (b) regression tasks. \"Train\" refers to training performance, and \"Test\" refers to testing performance. \"w/\" indicates the use of sampling to construct multiple forests, while \"w/o\" indicates the use of all constructed rODTs to form a single forest.", "description": "This figure compares the training and testing performance of DOFEN with and without sampling in the forest ensemble. The left panel shows the results for classification tasks, while the right panel shows the results for regression tasks. Both panels show that when sampling is not used, the model overfits the training data, leading to a significant gap between training and testing performance. In contrast, when sampling is used, the training and testing performance are more closely aligned, indicating that the ensemble of forests helps to mitigate overfitting.", "section": "F More Analysis"}, {"figure_path": "umukvCdGI6/figures/figures_25_1.jpg", "caption": "Figure 5: In the covertype dataset, Figure 5a shows that the average weights of true positives differ significantly from those of true negatives. Conversely, Figure 5b reveals a contrasting result for rODTs with small weight variation.", "description": "This figure shows the average weights of true positives (TP) and true negatives (TN) for relaxed oblivious decision trees (rODTs) in the covertype dataset.  Figure 5a displays a significant difference in average weights between TP and TN rODTs with large weight variation, indicating that these rODTs are crucial for classification. Figure 5b, in contrast, shows little to no difference for rODTs with small weight variation, suggesting that these rODTs play a less crucial role in the classification process.", "section": "Additional Analysis"}, {"figure_path": "umukvCdGI6/figures/figures_25_2.jpg", "caption": "Figure 5: In the covertype dataset, Figure 5a shows that the average weights of true positives differ significantly from those of true negatives. Conversely, Figure 5b reveals a contrasting result for rODTs with small weight variation.", "description": "This figure visualizes the average weights assigned to relaxed oblivious decision trees (rODTs) in a binary classification task (covertype dataset). Two sub-figures are presented: (a) shows rODTs with large weight variation, where the average weights of true positives (TP) are distinctly different from those of true negatives (TN); (b) shows rODTs with small weight variation, where the difference in average weights between TP and TN is less pronounced. This highlights how the variability of rODT weights correlates with their importance in classification, where rODTs with higher weight variation play a more crucial role.", "section": "4.3.2 Weights of Individual Relaxed ODT"}, {"figure_path": "umukvCdGI6/figures/figures_26_1.jpg", "caption": "Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1", "description": "This figure shows the performance comparison of DOFEN and other state-of-the-art models on the Tabular Benchmark. The x-axis represents the number of iterations during the hyperparameter random search, and the y-axis represents the normalized test accuracy or R2 score. Each line represents a different model.  The figure is divided into four subfigures, each representing a different combination of dataset size (medium or large) and task type (classification or regression).  The results demonstrate that DOFEN achieves state-of-the-art performance on the Tabular Benchmark, particularly for medium-sized datasets.", "section": "4.2 Performance Evaluation"}, {"figure_path": "umukvCdGI6/figures/figures_27_1.jpg", "caption": "Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1", "description": "This figure shows the performance comparison of DOFEN with other state-of-the-art models on the Tabular Benchmark.  The models are sorted by their performance after hyperparameter tuning using a random search. The results are averaged across multiple datasets within each benchmark category (medium classification, medium regression, large classification, and large regression).  The figure helps illustrate DOFEN's competitive performance against established methods, especially Gradient Boosting Decision Trees (GBDTs), on tabular data.", "section": "4.2 Performance Evaluation"}, {"figure_path": "umukvCdGI6/figures/figures_28_1.jpg", "caption": "Figure 4: Results on medium-sized classification and regression datasets.", "description": "The figure shows the performance of DOFEN and other models on the medium-sized datasets in the Tabular Benchmark.  The models are grouped by their performance after hyperparameter tuning, with tree-based models generally performing best, followed by DOFEN and a group of other DNN models. The figure shows that DOFEN is highly competitive with the tree-based methods in numerical datasets, and also shows the struggle that DNNs have when compared to tree-based methods in heterogeneous datasets.", "section": "4.2 Performance Evaluation"}, {"figure_path": "umukvCdGI6/figures/figures_29_1.jpg", "caption": "Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1", "description": "This figure presents the evaluation results of various machine learning models on the Tabular Benchmark.  The models are ranked based on their average performance across multiple datasets within each benchmark category (medium classification, medium regression, large classification, and large regression).  The graph visually compares the performance of DOFEN against other tree-based models (e.g., CatBoost, XGBoost, LightGBM) and various deep learning models (e.g., Trompt, SAINT, FT-Transformer).  The detailed number of datasets used in each benchmark is available in Appendix B.1.  This is a key result showing DOFEN's state-of-the-art performance on tabular data.", "section": "4.2 Performance Evaluation"}, {"figure_path": "umukvCdGI6/figures/figures_30_1.jpg", "caption": "Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1", "description": "This figure shows the performance comparison of DOFEN with other state-of-the-art models on the Tabular Benchmark.  The benchmark includes 73 datasets covering a wide variety of domains and tasks (classification and regression). The models are ranked based on their average performance across the datasets after hyperparameter tuning using random search. DOFEN achieves state-of-the-art results, outperforming other deep neural networks (DNNs) and competitive with gradient boosting decision tree (GBDT) models.", "section": "1 Introduction"}, {"figure_path": "umukvCdGI6/figures/figures_31_1.jpg", "caption": "Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1", "description": "This figure presents the results of the model evaluation on the Tabular Benchmark. The models are compared based on their performance after a random hyperparameter search. The results are presented as average performance across multiple datasets within each benchmark category (medium classification, medium regression, large classification, large regression). Each plot displays the normalized test accuracy (or R-squared for regression) as a function of the number of random search iterations.  The model names on the x-axis are sorted according to their performance at the end of the random search process, providing a clear visualization of the ranking.", "section": "4.2 Performance Evaluation"}, {"figure_path": "umukvCdGI6/figures/figures_31_2.jpg", "caption": "Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1", "description": "This figure shows the performance comparison of different models on the Tabular Benchmark.  The models are grouped by their performance, with the best performing models listed first.  The results are averaged across multiple datasets within each benchmark category (medium classification, medium regression, large classification, large regression). Appendix B.1 provides the exact number of datasets in each category.", "section": "4.2 Performance Evaluation"}, {"figure_path": "umukvCdGI6/figures/figures_32_1.jpg", "caption": "Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1", "description": "The figure shows the performance of various machine learning models on the Tabular Benchmark dataset.  The models include tree-based models (Random Forest, Extra Trees, Gradient Boosting Decision Trees like XGBoost, LightGBM, and CatBoost) and deep neural network models (Deep Forest, NODE, TabNet, SAINT, FT-Transformer, Trompt, MLP, ResNet). The models are compared across four sub-benchmarks: medium and large datasets for both classification and regression tasks.  The y-axis represents the normalized test accuracy (for classification) and R-squared score (for regression).  The x-axis shows the number of random search iterations for hyperparameter tuning.  The figure demonstrates that DOFEN outperforms other DNNs and is comparable to GBDTs.", "section": "4.2 Performance Evaluation"}, {"figure_path": "umukvCdGI6/figures/figures_32_2.jpg", "caption": "Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1", "description": "The figure shows the performance of various models (DOFEN, XGBoost, Catboost, etc.) on the Tabular Benchmark dataset.  The models are sorted by performance after hyperparameter tuning.  The results are averaged across multiple datasets within each benchmark (medium classification, medium regression, large classification, large regression).  Appendix B.1 provides details on the number of datasets in each benchmark category.", "section": "4.2 Performance Evaluation"}, {"figure_path": "umukvCdGI6/figures/figures_32_3.jpg", "caption": "Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1", "description": "The figure displays the performance of various models on the Tabular Benchmark dataset. The models are ranked by their performance after hyperparameter tuning via random search.  The results presented are averages across multiple datasets within each benchmark (medium-sized, large-sized, and categorized by task type). Appendix B.1 provides a detailed breakdown of the number of datasets in each benchmark category.", "section": "4.2 Performance Evaluation"}, {"figure_path": "umukvCdGI6/figures/figures_33_1.jpg", "caption": "Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1", "description": "This figure presents the performance comparison of various models on the Tabular Benchmark across four different tasks: medium-sized classification, medium-sized regression, large-sized classification, and large-sized regression.  Each plot shows the normalized test accuracy or R-squared score achieved by each model as a function of the number of random search iterations. This provides an overview of the relative performance of different models on diverse tabular datasets, showcasing DOFEN's strong performance across the board.", "section": "4.2 Performance Evaluation"}]