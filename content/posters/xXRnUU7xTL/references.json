{"references": [{"fullname_first_author": "L. Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper is foundational for instruction tuning, a key technique used and improved upon in SelfCodeAlign."}, {"fullname_first_author": "M. Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-00-00", "reason": "This paper is a benchmark for evaluating code generation models, and SelfCodeAlign is evaluated using these benchmarks."}, {"fullname_first_author": "A. Lozhkov", "paper_title": "StarCoder 2 and the stack v2: The next generation", "publication_date": "2024-00-00", "reason": "StarCoder2 is a model created using SelfCodeAlign and is a state-of-the-art model for code generation, showing the effectiveness of the method."}, {"fullname_first_author": "N. Muennighoff", "paper_title": "Octopack: Instruction tuning code large language models", "publication_date": "2023-00-00", "reason": "OctoPack is a prior state-of-the-art method for instruction tuning without human annotations, which SelfCodeAlign improves upon."}, {"fullname_first_author": "Y. Wang", "paper_title": "Self-instruct: Aligning language models with self-generated instructions", "publication_date": "2023-07-00", "reason": "This paper introduces self-instruction tuning, a concept that is the basis of the SelfCodeAlign approach."}]}