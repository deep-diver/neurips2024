[{"figure_path": "xXRnUU7xTL/tables/tables_3_1.jpg", "caption": "Table 1: Pass@1 (%) of different LLMs on EvalPlus computed using greedy decoding.", "description": "This table compares the performance (Pass@1) of various Large Language Models (LLMs) on the HumanEval+ and MBPP+ benchmarks for function-level code generation.  It includes models trained with different methods (proprietary data, GPT distillation, self-generated data), highlighting the performance of SelfCodeAlign-CQ-7B in comparison to other models. The \"Transparent\", \"Non-proprietary\", and \"Non-distilled\" columns indicate whether the model's training data and methods are publicly accessible and free of distillation techniques.", "section": "Main Evaluation"}, {"figure_path": "xXRnUU7xTL/tables/tables_4_1.jpg", "caption": "Table 2: Pass@1 (%) of LLMs on LiveCodeBench. Newer start dates imply lower contamination risk.", "description": "This table presents the pass@1 scores achieved by various LLMs on the LiveCodeBench benchmark.  LiveCodeBench is a benchmark designed to mitigate data contamination issues. The table shows pass@1 scores for three different start dates (2023-09-01, 2023-07-01, 2023-05-01).  A newer start date implies a lower risk of contamination because the data used in training the models was less likely to overlap with data from the benchmark tasks. This allows for a fairer comparison between models and assessment of their ability to generate code without relying on data from the same source as the benchmark.", "section": "3.1 Function-level Code Generation"}, {"figure_path": "xXRnUU7xTL/tables/tables_4_2.jpg", "caption": "Table 3: Pass@1 (%) of code LLMs on EvoEval.", "description": "This table presents the performance of various code LLMs on the EvoEval benchmark, broken down by different task subcategories (Difficult, Creative, Subtle, Combine, Tool use).  It compares the pass@1 scores (percentage of tasks where the model's top-ranked code passed all tests) for different models.  The models are evaluated on code generation tasks designed to assess various facets of code generation capability.", "section": "3.1 Function-level Code Generation"}, {"figure_path": "xXRnUU7xTL/tables/tables_5_1.jpg", "caption": "Table 4: Ranking of model code efficiency based on the EvalPerf win rates, which are computed over the common set of passing tasks for each model pair. Each model generates 100 samples per task at a temperature 1.0. To exemplify differential performance score (DPS) with SelfCodeAlign-CQ-7B, it means its generations if correct can match the efficiency of 79.9% LLM samples.", "description": "This table presents the ranking of different LLMs based on their code efficiency using the EvalPerf metric.  The ranking is determined by comparing the performance of each model pair across tasks where both models produced correct code. The table includes the differential performance score (DPS), the pass@1 rate (percentage of correct code generation), and the win rate (percentage of times one model's correct submissions outperform the other's) for each model.", "section": "3.1 Function-level Code Generation"}, {"figure_path": "xXRnUU7xTL/tables/tables_6_1.jpg", "caption": "Table 5: Pass@1 (%) of code LLMs on ClassEval using greedy decoding.", "description": "This table presents the performance of various code LLMs on the ClassEval benchmark, specifically focusing on class-level and method-level pass@1 scores using greedy decoding.  The results are useful for comparing the effectiveness of different models in generating complete and correct class-level code.", "section": "3.2 Class-level Code Generation"}, {"figure_path": "xXRnUU7xTL/tables/tables_6_2.jpg", "caption": "Table 1: Pass@1 (%) of different LLMs on EvalPlus computed using greedy decoding.", "description": "This table presents the pass@1 scores achieved by various large language models (LLMs) on the HumanEval+ and MBPP+ benchmarks for function-level code generation.  The models are evaluated using greedy decoding. The table includes information about the type of instruction data used for training each model (proprietary, public, self-generated, distilled from GPT), and whether the model is transparent and non-proprietary. This allows for a comparison of different approaches to instruction tuning in code generation and highlights the relative performance of the self-aligned model compared to other methods.", "section": "Main Evaluation"}, {"figure_path": "xXRnUU7xTL/tables/tables_6_3.jpg", "caption": "Table 7: Pass@1 (%) of code LLMs on CanItEdit.", "description": "This table presents the performance of various code LLMs on the CanItEdit benchmark, specifically focusing on code editing tasks categorized as corrective, adaptive, and perfective.  The \"Average\" column shows the overall performance across all three categories.  It allows for a comparison of the effectiveness of different LLMs in handling various types of code editing changes.", "section": "3.4 Code Editing"}, {"figure_path": "xXRnUU7xTL/tables/tables_7_1.jpg", "caption": "Table 8: HumanEval+ pass@1 when finetuning the base models on different data (37k seeds).", "description": "This table presents the HumanEval+ pass@1 scores achieved when finetuning various base language models (StarCoder2-3B, Llama-3-8B, StarCoder2-15B, DeepSeek-Coder-33B, and CodeQwen1.5-7B) using data generated by different data-generation models. Each row represents a different base model, and each column represents a different data-generation model. The values in the table show the pass@1 scores achieved when the base model is finetuned on the data generated by the corresponding data-generation model. The diagonal values represent the results of self-alignment (i.e., finetuning the base model on data generated by the same base model).", "section": "4.1 Self-Alignment with Different Models"}, {"figure_path": "xXRnUU7xTL/tables/tables_7_2.jpg", "caption": "Table 9: Pass@1 on HumanEval+ with different response selection strategies.", "description": "This table shows the results of four different experiments on HumanEval+, comparing four different response selection strategies.  The four strategies are: Random Selection (all), Random Selection (subset), Failures only, and Passes only. For each strategy, the table shows the data size, execution pass rate, and Pass@1 score. The results demonstrate the importance of execution filtering and code correctness for self-alignment.  The Passes Only strategy has the highest Pass@1 score (65.2).", "section": "4.2 Effectiveness of Execution-based Filtering"}, {"figure_path": "xXRnUU7xTL/tables/tables_8_1.jpg", "caption": "Table 10: Pass@1 on HumanEval+ using different seeds and pipelines.", "description": "This table shows the performance of SelfCodeAlign on HumanEval+ using different seeds and pipelines. The first row shows the results of directly generating instructions from filtered functions, while the second row shows the results of generating instructions from random snippets after mining concepts. The last row shows the performance of using the original pipeline with filtered functions. The results show that the original pipeline performs best.", "section": "4 Component Analysis"}, {"figure_path": "xXRnUU7xTL/tables/tables_8_2.jpg", "caption": "Table 11: SelfCodeAlign versus distillation using CodeQwen1.5-7B as the base model.", "description": "This table compares the performance of SelfCodeAlign against several state-of-the-art distillation methods for code instruction tuning.  It shows the dataset size, the teacher model used for distillation (GPT-3.5-Turbo or GPT-40), whether execution filtering was used, and the resulting pass@1 score on HumanEval+.  SelfCodeAlign achieves the best performance without relying on a stronger, external model for distillation.", "section": "4.4 Comparing Self-Alignment to Distillation"}, {"figure_path": "xXRnUU7xTL/tables/tables_18_1.jpg", "caption": "Table 12: End-to-end data generation time cost on 4\u00d7A100", "description": "This table shows the estimated computational cost for end-to-end data generation using different base models.  The cost is broken down into the time required for seed data generation, data production, overall data generation, and the subsequent finetuning process.  Note that the amount of seed data used was fixed at 37k examples across all experiments. ", "section": "3.1 Function-level Code Generation"}, {"figure_path": "xXRnUU7xTL/tables/tables_19_1.jpg", "caption": "Table 1: Pass@1 (%) of different LLMs on EvalPlus computed using greedy decoding.", "description": "This table compares the performance (Pass@1) of various large language models (LLMs) on the HumanEval+ and MBPP+ benchmarks for function-level code generation.  It shows the pass@1 scores for each model, indicating the percentage of times the model generated a correct solution on the first attempt using greedy decoding.  The table also indicates whether each model used proprietary data, knowledge distillation, or whether its training was fully transparent and non-proprietary.", "section": "3.1 Function-level Code Generation"}]