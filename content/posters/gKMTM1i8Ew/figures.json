[{"figure_path": "gKMTM1i8Ew/figures/figures_8_1.jpg", "caption": "Figure 1: Empirical cost complexity for 1000 runs times with \u03b4 = 0.01 on the 4 \u00d7 5 multi-fidelity bandit.", "description": "The figure shows the empirical cost complexity results for three different algorithms (GRAD, MF-GRAD, and IISE) across 1000 runs on a 4 \u00d7 5 multi-fidelity bandit problem with a risk parameter \u03b4 set to 0.01. The boxplots visually represent the distribution of the cost complexities for each algorithm, allowing for a comparison of their performance in terms of the cost required to identify the best arm.", "section": "5 Numerical experiments"}, {"figure_path": "gKMTM1i8Ew/figures/figures_8_2.jpg", "caption": "Figure 4: Empirical cost proportions of MF-GRAD for 100000 iterations on the 5 \u00d7 2 bandit model of Section 5. Results are average over 100 runs and shaded area report 95% confidence intervals. Empirical cost proportions of each arm are plotted with the same color. Cost proportions at fidelity 1, 2, 3, 4 and 5 are visualized with circle, squared, cross, triangle, and diamond respectively.", "description": "This figure shows the evolution of empirical cost proportions for each arm and fidelity over 100000 iterations of the MF-GRAD algorithm on a 5 \u00d7 2 bandit problem. The shaded areas represent 95% confidence intervals, and different colors represent different arms. The plot visualizes how the algorithm dynamically allocates resources across different arms and fidelities, gradually converging towards a sparse optimal allocation.", "section": "Numerical experiments"}, {"figure_path": "gKMTM1i8Ew/figures/figures_33_1.jpg", "caption": "Figure 4: Empirical cost proportions of MF-GRAD for 100000 iterations on the 5 \u00d7 2 bandit model of Section 5. Results are average over 100 runs and shaded area report 95% confidence intervals. Empirical cost proportions of each arm are plotted with the same color. Cost proportions at fidelity 1, 2, 3, 4 and 5 are visualized with circle, squared, cross, triangle, and diamond respectively.", "description": "This figure shows the evolution of empirical cost proportions of MF-GRAD algorithm over 100000 iterations for a 5-arm, 2-fidelity bandit problem.  The shaded regions represent 95% confidence intervals, and different colors represent different arms.  The plot illustrates how the algorithm dynamically allocates its sampling budget across arms and fidelities, converging towards an optimal allocation.", "section": "Numerical experiments"}, {"figure_path": "gKMTM1i8Ew/figures/figures_33_2.jpg", "caption": "Figure 1: Empirical cost complexity for 1000 runs times with \u03b4 = 0.01 on the 4 \u00d7 5 multi-fidelity bandit.", "description": "This figure shows the empirical cost complexity (total cost to identify the best arm) for three different algorithms: GRAD, MF-GRAD, and IISE.  The experiment was conducted 1000 times on a 4 \u00d7 5 multi-fidelity bandit problem with a risk parameter (\u03b4) of 0.01. The box plot visualization displays the median, quartiles, and range of the cost complexity values for each algorithm.  The figure highlights the performance comparison of the algorithms in terms of cost efficiency in solving a multi-fidelity best-arm identification problem.", "section": "5 Numerical experiments"}, {"figure_path": "gKMTM1i8Ew/figures/figures_33_3.jpg", "caption": "Figure 1: Empirical cost complexity for 1000 runs times with \u03b4 = 0.01 on the 4 \u00d7 5 multi-fidelity bandit.", "description": "The figure shows the empirical cost complexity (i.e., the total cost incurred until the algorithm stops) for three different algorithms: GRAD, MF-GRAD, and IISE.  The box plot shows the distribution of the cost complexity across 1000 independent runs of each algorithm on a 4 \u00d7 5 multi-fidelity bandit problem with a risk parameter \u03b4 = 0.01. MF-GRAD, the algorithm proposed in this paper, demonstrates significantly lower cost complexity compared to GRAD and IISE.", "section": "5 Numerical experiments"}, {"figure_path": "gKMTM1i8Ew/figures/figures_34_1.jpg", "caption": "Figure 7: Empirical cost complexity for 1000 runs times with \u03b4 = 0.01 on the 4 \u00d7 5 multi-fidelity bandit of Section 5.", "description": "This boxplot compares the cost complexity of MF-GRAD and MF-GRAD-CONST on a 4x5 multi-fidelity bandit problem. MF-GRAD-CONST uses a constant learning rate instead of the theoretical one used in MF-GRAD.  The plot shows that MF-GRAD-CONST has a lower median cost complexity and a smaller interquartile range, indicating improved performance with the constant learning rate.", "section": "5 Numerical experiments"}, {"figure_path": "gKMTM1i8Ew/figures/figures_34_2.jpg", "caption": "Figure 7: Empirical cost complexity for 1000 runs times with \u03b4 = 0.01 on the 4 \u00d7 5 multi-fidelity bandit of Section 5.", "description": "This figure compares the empirical cost complexity of MF-GRAD and MF-GRAD-CONST on the 4 \u00d7 5 multi-fidelity bandit problem described in Section 5 of the paper.  MF-GRAD-CONST uses a constant learning rate instead of the theoretically derived learning rate used by MF-GRAD. The box plot shows the distribution of costs across the 1000 runs of each algorithm. The results indicate that using a constant learning rate improves the performance of the algorithm, resulting in lower cost complexity.", "section": "5 Numerical experiments"}, {"figure_path": "gKMTM1i8Ew/figures/figures_34_3.jpg", "caption": "Figure 4: Empirical cost proportions of MF-GRAD for 100000 iterations on the 5 \u00d7 2 bandit model of Section 5. Results are average over 100 runs and shaded area report 95% confidence intervals. Empirical cost proportions of each arm are plotted with the same color. Cost proportions at fidelity 1, 2, 3, 4 and 5 are visualized with circle, squared, cross, triangle, and diamond respectively.", "description": "This figure shows the evolution of the empirical cost proportions of the MF-GRAD algorithm over 100000 iterations on a 5x2 multi-fidelity bandit problem.  The shaded areas represent 95% confidence intervals, illustrating the variability in the cost proportions across multiple runs.  The plot reveals how the algorithm allocates its budget across different arms and fidelities, highlighting the sparsity pattern of optimal fidelity allocation.", "section": "Numerical experiments"}, {"figure_path": "gKMTM1i8Ew/figures/figures_35_1.jpg", "caption": "Figure 4: Empirical cost proportions of MF-GRAD for 100000 iterations on the 5 \u00d7 2 bandit model of Section 5. Results are average over 100 runs and shaded area report 95% confidence intervals. Empirical cost proportions of each arm are plotted with the same color. Cost proportions at fidelity 1, 2, 3, 4 and 5 are visualized with circle, squared, cross, triangle, and diamond respectively.", "description": "This figure shows the evolution of the empirical cost proportions of the MF-GRAD algorithm over 100000 iterations for a 5 \u00d7 2 bandit problem. The results are averaged over 100 runs, and 95% confidence intervals are shown as shaded areas. Each color represents a different arm, and different shapes represent different fidelities.", "section": "5 Numerical experiments"}, {"figure_path": "gKMTM1i8Ew/figures/figures_35_2.jpg", "caption": "Figure 1: Empirical cost complexity for 1000 runs times with \u03b4 = 0.01 on the 4 \u00d7 5 multi-fidelity bandit.", "description": "The figure is a box plot showing the empirical cost complexity results for three algorithms (GRAD, MF-GRAD, IISE) on a 4 \u00d7 5 multi-fidelity bandit problem, with a risk parameter \u03b4 set to 0.01.  Each algorithm is run 1000 times and the box plots summarize the distribution of cost complexities across those runs.  The plot visualizes the median, quartiles, and range of the cost complexities for each algorithm, offering a comparison of their performance in terms of the cost required to identify the best arm with the specified confidence.", "section": "5 Numerical experiments"}, {"figure_path": "gKMTM1i8Ew/figures/figures_35_3.jpg", "caption": "Figure 1: Empirical cost complexity for 1000 runs times with \u03b4 = 0.01 on the 4 \u00d7 5 multi-fidelity bandit.", "description": "This figure shows the empirical cost complexity (total cost until the algorithm stops) for three different algorithms: GRAD, MF-GRAD, and IISE.  Each algorithm is run 1000 times on the same 4x5 multi-fidelity bandit problem (4 arms, 5 fidelities) with a risk parameter \u03b4 set to 0.01.  The box plots visually represent the distribution of the cost complexities obtained across the 1000 runs for each algorithm, showing the median, quartiles, and outliers. This allows for a comparison of the algorithms' performance in terms of the total cost required to identify the best arm with a given level of confidence.", "section": "5 Numerical experiments"}, {"figure_path": "gKMTM1i8Ew/figures/figures_36_1.jpg", "caption": "Figure 1: Empirical cost complexity for 1000 runs times with \u03b4 = 0.01 on the 4 \u00d7 5 multi-fidelity bandit.", "description": "The figure shows the empirical cost complexity of three algorithms (GRAD, MF-GRAD, IISE) for a 4 \u00d7 5 multi-fidelity bandit problem, with a risk parameter of \u03b4 = 0.01. The boxplot displays the distribution of cost complexities across 1000 independent runs of each algorithm, providing a visual comparison of their performance in terms of cost efficiency.  The algorithms differ in their approach to handling multiple fidelities; GRAD uses only the highest fidelity, MF-GRAD is the proposed algorithm which uses a gradient-based approach, and IISE is a previously published algorithm.  The plot shows that MF-GRAD is the most cost-effective.", "section": "5 Numerical experiments"}, {"figure_path": "gKMTM1i8Ew/figures/figures_37_1.jpg", "caption": "Figure 15: Visualization of the non-stopping behavior of LUCBExploreA and LUCBExploreB.", "description": "This figure shows the cost complexity of three algorithms, LUCB, LUCBExploreA, and LUCBExploreB.  The algorithms were tested on a multi-fidelity bandit problem where the optimal fidelity for each arm is known.  The results show that LUCBExploreA and LUCBExploreB fail to terminate within a reasonable time frame (10^7 samples), unlike LUCB which terminates quickly. This highlights a limitation of the LUCBExplore algorithms, specifically their inability to effectively identify the correct fidelity for each arm in all cases, thus leading to non-termination.", "section": "D.6 On LUCB-ExploreA and LUCB-ExploreB"}]