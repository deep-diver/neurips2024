[{"figure_path": "3vHfwL2stG/figures/figures_3_1.jpg", "caption": "Figure 1: Policy Parameter Change and Detour on policy paths obtained by TD3 on MuJoCo/Hopper and RAD on DMC/walker-walk. We use the colors () to denote Layer 1,2,3 respectively. Each subfigure contains: (left) the CDF histogram of the elements in the vectors (Layer k) and (right) the CDF histograms of the elements in the vectors (rjeLayer k) for k \u2208 {1,2,3}. Only upper 80% values according to (APC) are taken to plot the histogram for (ru) for meaningful analysis; extreme elements in (ru) are neglected for clarity. See Phenomenon 1.1 and 2.1 for conclusions.", "description": "This figure displays the results of an empirical investigation on the policy parameter change and detour. The left panels show the CDF histograms of the accumulated parameter change for each layer (1, 2, 3) of the policy network in the MuJoCo Hopper and DMC walker environments. The right panels show the CDF histograms of the detour ratio for each layer, indicating the extent of detours during the parameter update process. The results highlight the asymmetry in parameter changes and the significant detours observed in many parameters.", "section": "3 Phenomena on DRL Policy Learning Path"}, {"figure_path": "3vHfwL2stG/figures/figures_3_2.jpg", "caption": "Figure 2: Temporal SVD Analysis of policy paths obtained by RAD on DMC/walker-walk. (a) Curves of D(3) against various threshold \u03b2 for the three layers and three periods (i.e., early, middle, later) of the learning process for a temporal view. X-axis represents D(3) and y-axis represents different \u03b2. (b) The second and third panels depict the curves of u\u2217,k, with each curve illustrating the evolving path of the k-th coordinate. The fourth and fifth panels display the curves of detour ratio (rpud) and final change (\u2206PC). The X-axis represents the index i of the singular values.", "description": "This figure presents the results of a temporal singular value decomposition (SVD) analysis applied to policy learning paths generated by the RAD algorithm on the DMC walker-walk task. Panel (a) shows how the dimensionality of the policy learning path changes depending on the information threshold. Panel (b) illustrates the evolving paths of different dimensions of the policy (major and minor directions) along with their detour ratios and final parameter changes.", "section": "3.2 Temporal SVD Analysis of Policy Learning Path"}, {"figure_path": "3vHfwL2stG/figures/figures_5_1.jpg", "caption": "Figure 3: A conceptual illustration of Policy Path Trimming and Boosting (PPTB). The 2D view of an exemplary policy learning path in policy parameter space (left) and the corresponding policy learning path improved by PPTB (right).", "description": "This figure conceptually illustrates the Policy Path Trimming and Boosting (PPTB) method. The left panel shows a 2D representation of a typical policy learning path in the policy parameter space. The path is depicted as a sequence of points (\u03b8t) connected by lines, showing the evolution of policy parameters during training. Major directions are represented by green arrows, while minor directions are shown in red. The right panel shows how PPTB improves the learning path. PPTB trims the updates in minor directions (red crosses), effectively smoothing the path. Additionally, it boosts the updates in major directions (yellow arrows), enhancing the learning progress. The trimmed and boosted path is now more direct and efficient, highlighting the improvement achieved by PPTB.", "section": "4 Policy Path Trimming and Boosting"}, {"figure_path": "3vHfwL2stG/figures/figures_9_1.jpg", "caption": "Figure 4: The parameter and Temporal SVD analysis for Behavior Cloning with Adam and SGD optimizers in D4RL halfcheetah-medium-replay.", "description": "This figure presents the results of an empirical investigation on policy learning paths using behavior cloning with Adam and SGD optimizers. It shows the parameter analysis (CDFs of accumulated parameter change) and temporal SVD analysis (singular values and left unitary matrix) for both Adam and SGD.  The goal was to examine how parameters change during training and identify the dominant directions of parameter evolution using temporal SVD.  This allows for a comparison of learning dynamics under different optimizers (Adam and SGD) in an offline reinforcement learning setting.", "section": "5.2 Investigation on Policy Learning Path of Behavior Clone"}, {"figure_path": "3vHfwL2stG/figures/figures_16_1.jpg", "caption": "Figure 1: Policy Parameter Change and Detour on policy paths obtained by TD3 on MuJoCo/Hopper and RAD on DMC/walker-walk. We use the colors () to denote Layer 1,2,3 respectively. Each subfigure contains: (left) the CDF histogram of the elements in the vectors (Layer k) and (right) the CDF histograms of the elements in the vectors (rjeLayer k) for k \u2208 {1,2,3}. Only upper 80% values according to (APC) are taken to plot the histogram for (ru) for meaningful analysis; extreme elements in (ru) are neglected for clarity. See Phenomenon 1.1 and 2.1 for conclusions.", "description": "This figure displays the results of an empirical investigation into how policy parameters change and detour during the training process of two different deep reinforcement learning (DRL) agents: TD3 and RAD.  It uses CDF histograms to visualize the accumulated parameter changes and detour ratios for each layer (1, 2, and 3) of the policy network across multiple trials in the Hopper (MuJoCo) and walker-walk (DMC) environments. The asymmetry in parameter changes and the significant detours observed highlight key phenomena discussed in the paper.", "section": "3 Phenomena on DRL Policy Learning Path"}, {"figure_path": "3vHfwL2stG/figures/figures_17_1.jpg", "caption": "Figure 1: Policy Parameter Change and Detour on policy paths obtained by TD3 on MuJoCo/Hopper and RAD on DMC/walker-walk. We use the colors () to denote Layer 1,2,3 respectively. Each subfigure contains: (left) the CDF histogram of the elements in the vectors (Layer k) and (right) the CDF histograms of the elements in the vectors (rjeLayer k) for k \u2208 {1,2,3}. Only upper 80% values according to (APC) are taken to plot the histogram for (ru) for meaningful analysis; extreme elements in (ru) are neglected for clarity. See Phenomenon 1.1 and 2.1 for conclusions.", "description": "This figure shows the results of an empirical investigation on the policy parameter change and detour.  It presents Cumulative Distribution Function (CDF) histograms of the accumulated parameter change and the detour ratio for each layer (1, 2, and 3) of the policy network in two different environments (MuJoCo Hopper and DMC walker-walk).  The left panels show the CDF of accumulated change, while the right panels illustrate the CDF of the detour ratio. The analysis reveals asymmetries in parameter changes and significant detours during the policy learning process.", "section": "3 Phenomena on DRL Policy Learning Path"}, {"figure_path": "3vHfwL2stG/figures/figures_18_1.jpg", "caption": "Figure 1: Policy Parameter Change and Detour on policy paths obtained by TD3 on MuJoCo/Hopper and RAD on DMC/walker-walk. We use the colors () to denote Layer 1,2,3 respectively. Each subfigure contains: (left) the CDF histogram of the elements in the vectors (Layer k) and (right) the CDF histograms of the elements in the vectors (rjeLayer k) for k \u2208 {1,2,3}. Only upper 80% values according to (APC) are taken to plot the histogram for (ru) for meaningful analysis; extreme elements in (ru) are neglected for clarity. See Phenomenon 1.1 and 2.1 for conclusions.", "description": "This figure presents the cumulative distribution functions (CDFs) of the accumulated parameter change and the detour ratio for each layer (1,2,3) of the policy networks in the Hopper (MuJoCo) and walker-walk (DMC) environments.  The left panels show the distribution of the total parameter change during training, highlighting the asymmetry in changes across layers and parameters. The right panels illustrate the distribution of the detour ratio, revealing that many parameters deviate significantly from their initial values during training.  These results visually support Phenomena 1.1 and 2.1 in the paper, showing asymmetric parameter changes and substantial detours during policy learning.", "section": "3 Phenomena on DRL Policy Learning Path"}, {"figure_path": "3vHfwL2stG/figures/figures_19_1.jpg", "caption": "Figure 8: Complete results of the empirical investigation on SVD left unitary matrix of policy learning path in MuJoCo environments.", "description": "This figure shows the results of an empirical investigation on SVD left unitary matrix of policy learning paths in four MuJoCo environments (HalfCheetah-v4, Hopper-v4, Walker2d-v4, and Ant-v4). For each environment, it presents five subfigures: (1) The learning curve, showing the average return over time steps. (2-5) The temporal evolution of the k-th coordinate (u*,k), with each curve illustrating the evolving path of the k-th coordinate, where k represents the index of the singular value. The detour ratio (rpud) and final change (\u2206PC) of the policy parameters along the learning path are also shown. This figure helps illustrate the phenomena observed in the paper, specifically Phenomenon 2.2 regarding the behavior of policy parameter updates in major and minor SVD directions.", "section": "3.2 Temporal SVD Analysis of Policy Learning Path"}, {"figure_path": "3vHfwL2stG/figures/figures_19_2.jpg", "caption": "Figure 2: Temporal SVD Analysis of policy paths obtained by RAD on DMC/walker-walk. (a) Curves of D(3) against various threshold \u03b2 for the three layers and three periods (i.e., early, middle, later) of the learning process for a temporal view. X-axis represents D(3) and y-axis represents different \u03b2. (b) The second and third panels depict the curves of u\u2217,k, with each curve illustrating the evolving path of the k-th coordinate. The fourth and fifth panels display the curves of detour ratio (rpud) and final change (\u2206PC). The X-axis represents the index i of the singular values.", "description": "This figure shows the results of a temporal singular value decomposition (SVD) analysis of policy learning paths from the RAD algorithm on the DMC walker-walk environment.  Panel (a) shows how the dimensionality of the policy path changes as a function of an information threshold (\u03b2), separately for three layers of the network and three different learning periods. Panel (b) presents a detailed analysis of the dominant and less significant singular vectors (u\u2217,k), showing how parameters evolve along those directions, along with their detour ratios and final changes.", "section": "3.2 Temporal SVD Analysis of Policy Learning Path"}, {"figure_path": "3vHfwL2stG/figures/figures_22_1.jpg", "caption": "Figure 10: The learning curves for TD3 (Gray) and TD3-PPTB (Red) in four MuJoCo tasks, with means and standard errors across 6 seeds.", "description": "This figure displays the learning curves for both TD3 and TD3-PPTB across four different MuJoCo tasks. The gray lines represent the performance of the TD3 algorithm, while the red lines represent the performance of the TD3 algorithm enhanced with the PPTB method. Error bars indicating standard errors across six different seeds are included for each task. This visualization helps in comparing the performance and stability of both algorithms across various environments.", "section": "5 Experiments"}, {"figure_path": "3vHfwL2stG/figures/figures_22_2.jpg", "caption": "Figure 11: The learning curves for RAD (Gray) and RAD-PPTB (Red) in four DMC tasks, with means and standard errors across 6 seeds.", "description": "This figure shows the learning curves for the RAD algorithm and the improved RAD-PPTB algorithm across four different DeepMind Control Suite (DMC) environments.  The x-axis represents the number of timesteps, and the y-axis represents the average return.  The shaded regions indicate the standard error across six independent runs with different random seeds. The curves show that RAD-PPTB generally outperforms RAD in terms of both final performance and learning speed, indicating the effectiveness of the proposed method.", "section": "5 Experiments"}, {"figure_path": "3vHfwL2stG/figures/figures_22_3.jpg", "caption": "Figure 5: Complete results of empirical investigation on policy parameter change amount in MuJoCo environments.", "description": "This figure shows the complete results of an empirical investigation on policy parameter change amount in MuJoCo environments. It includes CDF histograms of the accumulated parameter change and the detour ratio of the parameters for different layers (Layer 1, 2, 3) of the network in six different MuJoCo environments (HalfCheetah-v4, Hopper-v4, Walker2d-v4, Ant-v4, Humanoid-v4). The results reveal the asymmetry in parameter changes across layers and the significant detours present in parameter updates.", "section": "3 Phenomena on DRL Policy Learning Path"}]