[{"figure_path": "3vHfwL2stG/tables/tables_7_1.jpg", "caption": "Table 1: Performance evaluation of Policy Path Trimming and Boosting (PPTB) for TD3 [Fujimoto et al., 2018] in four MuJoCo environments. The learning curves are in Figure 10.", "description": "This table presents the performance evaluation of the proposed method, PPTB, when applied to the TD3 algorithm across four MuJoCo environments.  It compares the performance of TD3 with and without PPTB using two metrics: Score (best average return across multiple runs) and AUC (mean of average returns over the learning process). The results demonstrate that PPTB improves the learning performance of TD3 in terms of both effectiveness (Score) and efficiency/stability (AUC).  The Aggregate rows show the relative improvement across all environments.", "section": "5.1 Performance Evaluation of PPTB"}, {"figure_path": "3vHfwL2stG/tables/tables_8_1.jpg", "caption": "Table 2: Performance evaluation of Policy Path Trimming and Boosting (PPTB) for RAD [Laskin et al., 2020] in four DeepMind Control (DMC) environments. The learning curves are in Figure 11.", "description": "This table presents the performance evaluation results of the RAD algorithm with and without PPTB in four different DeepMind Control Suite environments.  The metrics used are Score (the best average return across multiple runs) and AUC (the mean average return over the learning process).  The table shows the improvement in both metrics achieved by using PPTB, indicating its effectiveness in improving the performance of the RAD algorithm. Figure 11 visualizes the learning curves for a better understanding.", "section": "5.1 Performance Evaluation of PPTB"}, {"figure_path": "3vHfwL2stG/tables/tables_8_2.jpg", "caption": "Table 3: Performance evaluation regarding Score of Policy Path Trimming and Boosting (PPTB) for DoubleDQN [van Hasselt et al., 2016] in three MinAtar environments.", "description": "This table presents the results of experiments evaluating the performance of the Policy Path Trimming and Boosting (PPTB) method when applied to the DoubleDQN algorithm on three MinAtar environments: SpaceInvaders, Seaquest, and Breakout.  The table shows the average scores achieved by the standard DoubleDQN algorithm and the DoubleDQN algorithm enhanced with PPTB, along with standard deviations across six independent runs for each environment. It demonstrates the effectiveness of PPTB in value-based reinforcement learning.", "section": "Results for Value-based Methods"}, {"figure_path": "3vHfwL2stG/tables/tables_20_1.jpg", "caption": "Table 4: Statistics for Temporal SVD reconstruction of RAD policies in cartpole-wingup (the first period) with varying number of major dimensions kept.", "description": "This table presents the results of an experiment evaluating the performance of a temporal singular value decomposition (SVD) reconstruction method applied to RAD policies on the cartpole-wingup task.  The experiment varied the number of major dimensions retained during the reconstruction, and the table shows the average return (AVG(AR(\u03b8i))), average absolute return difference (AVG(|\u2206R(\u03b8i)|)), maximum return (MAX(AR(\u03b8i))), and minimum return (MIN(AR(\u03b8i))) for each number of major dimensions.  These metrics provide insights into the effectiveness of the reconstruction method and how the number of retained dimensions impacts performance.", "section": "D.3 Empirical Investigation on Temporal SVD Reconstruction of DRL Policies"}, {"figure_path": "3vHfwL2stG/tables/tables_20_2.jpg", "caption": "Table 5: Statistics for Temporal SVD reconstruction of RAD policies in cartpole-wingup (the second period) with varying number of major dimensions kept.", "description": "This table presents the results of reconstructing RAD policies using Temporal SVD with varying numbers of major dimensions retained.  The data is specifically for the second period of the cartpole-wingup task.  The table shows the average absolute return (AVG({AR(\u03b8i)}i)), the average absolute change in return (AVG({|\u2206R(\u03b8i)|}i)), the maximum absolute return (MAX({AR(\u03b8i)}i)), and the minimum absolute return (MIN({AR(\u03b8i)}i)) for different numbers of major dimensions.", "section": "D.3 Empirical Investigation on Temporal SVD Reconstruction of DRL Policies"}, {"figure_path": "3vHfwL2stG/tables/tables_20_3.jpg", "caption": "Table 6: Statistics for Temporal SVD reconstruction of RAD policies in finger-spin (the first period) with varying number of major dimensions kept.", "description": "This table presents the results of an experiment evaluating the performance of a temporal singular value decomposition (SVD) reconstruction method on RAD policies in the finger-spin environment.  The experiment varied the number of major dimensions kept during reconstruction (1, 2, 4, 8, 16, 32, 64, 128). For each number of major dimensions, the table shows the average and standard deviation of the average return (AVG({AR(\u03b8)}i)), the average absolute return (AVG({|\u2206R(\u03b8)|}i)), the maximum average return (MAX({AR(\u03b8)}i)), and the minimum average return (MIN({AR(\u03b8)}i)). This data helps quantify the impact of dimensionality reduction on policy performance. The first period refers to an early stage of the training process.", "section": "D.3 Empirical Investigation on Temporal SVD Reconstruction of DRL Policies"}, {"figure_path": "3vHfwL2stG/tables/tables_21_1.jpg", "caption": "Table 7: Statistics for Temporal SVD reconstruction of RAD policies in finger-spin (the second period) with varying number of major dimensions kept.", "description": "This table presents the results of a temporal singular value decomposition (SVD) reconstruction experiment on RAD policies in the finger-spin environment during the second learning period.  It shows the average absolute return (AVG({AR(\u03b8i)}i)), average absolute return change (AVG({|\u2206R(\u03b8i)|}i)), maximum absolute return (MAX({AR(\u03b8i)}i)), and minimum absolute return (MIN({AR(\u03b8i)}i)) obtained by reconstructing the policies using different numbers of major dimensions (from 1 to 128). The results are used to study the effects of dimensionality reduction on policy reconstruction in the context of deep reinforcement learning (DRL).", "section": "D.3 Empirical Investigation on Temporal SVD Reconstruction of DRL Policies"}, {"figure_path": "3vHfwL2stG/tables/tables_21_2.jpg", "caption": "Table 8: Statistics for Temporal SVD reconstruction of RAD policies in walker-walk (the first period) with varying number of major dimensions kept.", "description": "This table presents the results of a temporal singular value decomposition (SVD) reconstruction experiment on RAD policies in the walker-walk environment.  The experiment varied the number of major dimensions kept during reconstruction (from 1 to 128). For each number of major dimensions, the table shows the average and standard deviation of the absolute return (AVG({AR(\u03b8i)}i)), average and standard deviation of the absolute change in return (AVG({|\u2206R(\u03b8i)|}i)), maximum return (MAX({AR(\u03b8i)}i)), and minimum return (MIN({AR(\u03b8i)}i)). This data helps to evaluate the effectiveness of reducing the dimensionality of the policy learning path using temporal SVD.", "section": "D.3 Empirical Investigation on Temporal SVD Reconstruction of DRL Policies"}, {"figure_path": "3vHfwL2stG/tables/tables_21_3.jpg", "caption": "Table 9: Statistics for Temporal SVD reconstruction of RAD policies in walker-walk (the second period) with varying number of major dimensions kept.", "description": "This table presents the results of a temporal singular value decomposition (SVD) reconstruction of RAD policies on the walker-walk task in the DeepMind Control Suite (DMC).  The reconstruction is performed using different numbers of major dimensions (1, 2, 4, 8, 16, 32, 64, 128) to assess the impact on reconstruction accuracy.  The average absolute return (AVG({AR(\u03b8i)}i)), average absolute return difference (AVG({|\u2206R(\u03b8)|}i)), maximum absolute return (MAX({\u2206R(\u03b8)}i)), and minimum absolute return (MIN({\u2206R(\u03b8)}i)) are reported for each number of major dimensions.  This analysis helps evaluate the method's effectiveness in capturing the essential aspects of the policy path during learning. The data shown is for the second period of the learning process.", "section": "D.3 Empirical Investigation on Temporal SVD Reconstruction of DRL Policies"}]