[{"figure_path": "eTu6kvrkSq/figures/figures_3_1.jpg", "caption": "Figure 1: Empirical verification of the theoretical equilibrated energy of deep linear networks (Theorem 1). For different datasets, we plot the energy (Eq. 2) at the numerical inference equilibrium \u2202F/\u2202z \u2248 0 for DLNs with different number of hidden layers H \u2208 {2, 5, 10} (see \u00a7A.4 for more details), observing an excellent match with the theoretical prediction (Eq. 5).", "description": "This figure empirically validates Theorem 1, which provides a closed-form solution for the equilibrated energy of deep linear networks (DLNs). The plot shows the energy at inference equilibrium (where \u2202F/\u2202z \u2248 0) for DLNs trained on three different datasets (MNIST, Fashion-MNIST, and CIFAR-10) and with three different numbers of hidden layers (H=2, 5, and 10).  The experimental results closely match the theoretical predictions, demonstrating the accuracy of the derived formula for the equilibrated energy.", "section": "Theoretical results"}, {"figure_path": "eTu6kvrkSq/figures/figures_4_1.jpg", "caption": "Figure 2: Toy examples illustrating the (Theorem 2) result that the saddle at the origin of the equilibrated energy is strict independent of network depth. We plot the MSE loss L(0) (top) and equilibrated energy landscape F*(0) (middle) around the origin for 3 linear networks trained with SGD on a toy problem (see \u00a7A.4 for details). We also show the training losses for a representative run with initialisation close to the origin (bottom). For the one-dimensional networks, we visualise the landscape around the origin as well as the SGD updates. For the wide network, we project the landscape onto the maximum and minimum eigenvectors of the Hessian, following [7]. Note that in this case the loss is flat because the Hessian at the origin is zero for H > 1 (Eq. 6).", "description": "This figure shows that the origin saddle of the loss function is not strict for deep linear networks (DLNs) while it is a strict saddle for the equilibrated energy. The figure shows 3 examples of linear networks with varying depths, from 1 to 3 hidden layers. The plots of the MSE loss L(0) and the equilibrated energy F*(0) around the origin are presented. In the bottom row, the training losses of SGD with initialisation close to the origin are presented for each network. One-dimensional networks allow for a complete visual representation of the landscape, while the 2-D wide network is projected onto the max and min eigenvectors of the Hessian for visualization.  It demonstrates that even though the loss landscape has degenerate saddles, the equilibrated energy does not.", "section": "Theoretical results"}, {"figure_path": "eTu6kvrkSq/figures/figures_5_1.jpg", "caption": "Figure 3: Empirical verification of the Hessian at the origin of the equilibrated energy for DLNs tested on toy data. We show the Hessian and its eigenspectrum at the origin of the MSE loss (top) and equilibrated energy (middle) for DLNs with Gaussian target y = x where x ~ N(1,0.1) (see \u00a7A.4 for details). Note that purple bars show overlapping loss and energy Hessian eigendensity. In the right panel, we vary one of the output dimensions to be y2 = x2. We confirm the strictness of the origin saddle in the equilibrated energy and observe an excellent numerical validation of our theoretical Hessian (Eq. 8). Figure 8 shows the same results for one-dimensional networks, and Figure 4 shows similar results for more realistic datasets.", "description": "This figure empirically validates the theoretical results about the Hessian at the origin of the equilibrated energy for deep linear networks.  It shows a comparison of the Hessian and its eigenspectrum for both the MSE loss and the equilibrated energy, demonstrating a perfect match between theoretical predictions and numerical results. The plots include both 2D heatmaps of the Hessian and histograms of its eigenvalues.  A variation on the experiment is also shown, altering the target variable to highlight the robustness of the results.", "section": "3 Theoretical results"}, {"figure_path": "eTu6kvrkSq/figures/figures_6_1.jpg", "caption": "Figure 4: Empirical verification of the Hessian eigenspectrum at the origin of the equilibrated energy for DLNs tested on more realistic datasets. This shows similar results to Figure 3 for the more realistic datasets MNIST and MNIST-1D [16] (see \u00a7A.4 for details). We again find a perfect match between theory and experiment for DLNs with different number of hidden layers \u0397 \u2208 {1, 2, 4}, confirming the strictness of the origin saddle of the equilibrated energy.", "description": "This figure empirically validates the theoretical findings about the Hessian at the origin of the equilibrated energy for deeper linear networks.  It compares the numerically computed Hessian eigenspectrum to the theoretical one for MNIST and MNIST-1D datasets and for networks with 1, 2 and 4 hidden layers. The strong agreement supports the theory that the origin saddle of the equilibrated energy is strict, even for realistic datasets.", "section": "3 Theoretical results"}, {"figure_path": "eTu6kvrkSq/figures/figures_7_1.jpg", "caption": "Figure 5: PC escapes the origin saddle much faster than BP with SGD on non-linear networks. We plot the training loss for a representative run of BP and PC for linear and non-linear networks trained on standard image classification tasks (see \u00a7A.4 for details). All networks were initialised close to the origin with scale \u03c3 = 5e-3), and trained with SGD and learning rate \u03b7 = 1e-3. The networks trained on MNIST and Fashion-MNIST had 5 fully connected layers, while those trained on CIFAR-10 had a convolutional architecture. Figure 11 shows the corresponding weight gradient norms during training. Results were consistent across different random seeds.", "description": "This figure compares the training loss dynamics of backpropagation (BP) and predictive coding (PC) on various network architectures (linear, Tanh, ReLU) and datasets (MNIST, Fashion-MNIST, CIFAR-10).  It demonstrates that PC escapes the origin saddle significantly faster than BP.  The consistent performance across different datasets and network types supports the claim that PC inference improves the loss landscape's geometry.", "section": "4 Experiments"}, {"figure_path": "eTu6kvrkSq/figures/figures_8_1.jpg", "caption": "Figure 5: PC escapes the origin saddle much faster than BP with SGD on non-linear networks. We plot the training loss for a representative run of BP and PC for linear and non-linear networks trained on standard image classification tasks (see \u00a7A.4 for details). All networks were initialised close to the origin with scale \u03c3 = 5e-3), and trained with SGD and learning rate \u03b7 = 1e-3. The networks trained on MNIST and Fashion-MNIST had 5 fully connected layers, while those trained on CIFAR-10 had a convolutional architecture. Figure 11 shows the corresponding weight gradient norms during training. Results were consistent across different random seeds.", "description": "This figure compares the training loss dynamics of linear and non-linear networks (with linear, Tanh, and ReLU activations) trained using BP and PC, respectively, with SGD initialised near the origin.  The results show that PC escapes the origin saddle significantly faster than BP for all network types and datasets (MNIST, Fashion-MNIST, and CIFAR-10). The figure also references a supplementary Figure (Figure 11) showing weight gradient norms that support the faster convergence observed with PC.", "section": "4 Experiments"}, {"figure_path": "eTu6kvrkSq/figures/figures_24_1.jpg", "caption": "Figure 7: Training and test statistics for linear networks of Figure 2. For each network, we plot the mean and \u00b11 standard deviation of the training loss, test loss and gradient norm over 5 random initialisations. For the wide network, the test loss is evaluated once every epoch (rather than for each batch), and the training metrics are plotted on a log axis for easier visualisation. For the chain with two hidden units, the multiple loss plateaus and corresponding gradient spikes are due to different escape times from the saddle for different runs.", "description": "This figure shows the training and testing loss, along with the gradient norm for linear networks with different architectures.  The results are averaged over five runs with different random initializations. The figure highlights the faster convergence of Predictive Coding (PC) compared to Backpropagation (BP), particularly noticeable in the wide network. The plot also demonstrates how PC avoids vanishing gradients.", "section": "A.5 Supplementary results"}, {"figure_path": "eTu6kvrkSq/figures/figures_25_1.jpg", "caption": "Figure 3: Empirical verification of the Hessian at the origin of the equilibrated energy for DLNs tested on toy data. We show the Hessian and its eigenspectrum at the origin of the MSE loss (top) and equilibrated energy (middle) for DLNs with Gaussian target y = -x where x ~ N(1, 0.1) (see \u00a7A.4 for details). Note that purple bars show overlapping loss and energy Hessian eigendensity. In the right panel, we vary one of the output dimensions to be y2 = 12. We confirm the strictness of the origin saddle in the equilibrated energy and observe an excellent numerical validation of our theoretical Hessian (Eq. 8). Figure 8 shows the same results for one-dimensional networks, and Figure 4 shows similar results for more realistic datasets.", "description": "This figure empirically validates the theoretical Hessian at the origin of the equilibrated energy.  It compares the Hessian and its eigenspectrum for the MSE loss and the equilibrated energy of deep linear networks (DLNs) on toy Gaussian data. The results confirm the strictness of the origin saddle in the equilibrated energy and demonstrate a high level of agreement between the numerical and theoretical results, further strengthened by showing consistent findings in one-dimensional networks and more realistic datasets.", "section": "3 Theoretical results"}, {"figure_path": "eTu6kvrkSq/figures/figures_25_2.jpg", "caption": "Figure 9: Empirical verification of a strict zero-rank saddle of the equilibrated energy other than the origin for DLNs tested on a toy dataset. We show the Hessian eigenspectrum of the MSE loss and equilibrated energy at a strict saddle other than the origin covered by Theorem 3, specifically for the critical point where all weight matrices except the penultimate are zero \u03b8* (Wl = 0, \u2200l \u2260 L \u2212 1). We do not show the loss Hessians because they are zero for H > 1 (Eq. 6). The target is the same as used for Figure 3, and in the right panel one of the output dimensions is varied to be y2 = x2. Figure 10 shows results for the same critical point on MNIST and MNIST-1D.", "description": "This figure empirically validates Theorem 3 of the paper, which states that zero-rank saddles of the MSE loss become strict saddles in the equilibrated energy.  It shows the Hessian eigenspectra for both the MSE loss and the equilibrated energy at a zero-rank saddle (other than the origin) for deep linear networks (DLNs).  The experiment is performed on a simple toy dataset and the results are then compared to results from experiments on the MNIST and MNIST-1D datasets (shown in Figure 10). The results strongly support the theoretical findings.", "section": "Analysis of other saddles"}, {"figure_path": "eTu6kvrkSq/figures/figures_26_1.jpg", "caption": "Figure 10: Empirical verification of a strict zero-rank saddle of the equilibrated energy other than the origin for DLNs tested on more realistic datasets. This shows similar results to Figure 9 for the more realistic datasets MNIST and MNIST-1D.", "description": "This figure empirically validates the theoretical findings about the strictness of zero-rank saddles in the equilibrated energy landscape. It compares the Hessian eigenspectrum of the MSE loss and the equilibrated energy at a specific zero-rank saddle point (other than the origin) for deep linear networks trained on MNIST and MNIST-1D datasets. The results demonstrate that this zero-rank saddle is strict in the equilibrated energy but not in the MSE loss.", "section": "Supplementary results"}, {"figure_path": "eTu6kvrkSq/figures/figures_26_2.jpg", "caption": "Figure 5: PC escapes the origin saddle much faster than BP with SGD on non-linear networks. We plot the training loss for a representative run of BP and PC for linear and non-linear networks trained on standard image classification tasks (see \u00a7A.4 for details). All networks were initialised close to the origin with scale \u03c3 = 5e-3), and trained with SGD and learning rate \u03b7 = 1e-3. The networks trained on MNIST and Fashion-MNIST had 5 fully connected layers, while those trained on CIFAR-10 had a convolutional architecture. Figure 11 shows the corresponding weight gradient norms during training. Results were consistent across different random seeds.", "description": "This figure compares the training loss dynamics of linear and non-linear networks trained with backpropagation (BP) and predictive coding (PC) using stochastic gradient descent (SGD). The results demonstrate that PC escapes the origin saddle significantly faster than BP across various network architectures and datasets (MNIST, Fashion-MNIST, CIFAR-10). The figure also shows that the weight gradient norms during training using PC do not exhibit vanishing gradients, unlike BP.", "section": "4 Experiments"}, {"figure_path": "eTu6kvrkSq/figures/figures_27_1.jpg", "caption": "Figure 5: PC escapes the origin saddle much faster than BP with SGD on non-linear networks. We plot the training loss for a representative run of BP and PC for linear and non-linear networks trained on standard image classification tasks (see \u00a7A.4 for details). All networks were initialised close to the origin with scale \u03c3 = 5e-3), and trained with SGD and learning rate \u03b7 = 1e-3. The networks trained on MNIST and Fashion-MNIST had 5 fully connected layers, while those trained on CIFAR-10 had a convolutional architecture. Figure 11 shows the corresponding weight gradient norms during training. Results were consistent across different random seeds.", "description": "This figure compares the training loss dynamics of linear and non-linear networks trained with BP and PC using SGD. The networks were initialized close to the origin saddle. The results show that PC escapes the origin saddle significantly faster than BP, and no vanishing gradients are observed in PC.", "section": "4 Experiments"}]