[{"type": "text", "text": "ET-Flow: Equivariant Flow-Matching for Molecular Conformer Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Majdi Hassan\u22171 Nikhil Shenoy\u22172,4 Jungyoon Lee\u22171 Hannes St\u00e4rk3 Stephan Thaler4 ", "page_idx": 0}, {"type": "text", "text": "Dominique Beaini1,4 ", "page_idx": 0}, {"type": "text", "text": "1Mila & Universit\u00e9 de Montr\u00e9al 2University of British-Columbia 3Massachusetts Institute of Technology 4Valence Labs ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Predicting low-energy molecular conformations given a molecular graph is an important but challenging task in computational drug discovery. Existing stateof-the-art approaches either resort to large scale transformer-based models that diffuse over conformer fields, or use computationally expensive methods to generate initial structures and diffuse over torsion angles. In this work, we introduce Equivariant Transformer Flow (ET-Flow). We showcase that a well-designed flow matching approach with equivariance and harmonic prior alleviates the need for complex internal geometry calculations and large architectures, contrary to the prevailing methods in the field. Our approach results in a straightforward and scalable method that directly operates on all-atom coordinates with minimal assumptions. With the advantages of equivariance and flow matching, ET-Flow significantly increases the precision and physical validity of the generated conformers, while being a lighter model and faster at inference. Code is available https://github.com/shenoynikhil/ETFlow. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generating low-energy 3D representations of molecules, called conformers, from the molecular graph is a fundamental task in computational chemistry as the 3D structure of a molecule is responsible for several biological, chemical and physical properties (Guimar\u00e3es et al., 2012; Sch\u00fctt et al., 2018, 2021; Gasteiger et al., 2020; Axelrod and Gomez-Bombarelli, 2023). Conventional approaches to molecular conformer generation consist of stochastic and systematic methods. While stochastic methods such as Molecular Dynamics (MD) accurately generate conformations, they can be slow, cost-intensive, and have low sample diversity (Shim and MacKerell Jr, 2011; Ballard et al., 2015; De Vivo et al., 2016; Hawkins, 2017; Pracht et al., 2020). Systematic (rule-based) methods (Hawkins et al., 2010; Bolton et al., 2011; Li et al., 2007; Miteva et al., 2010; Cole et al., 2018; Lagorce et al., 2009) that rely on torsional proflies and knowledge base of fragments are much faster but become less accurate with larger molecules. Therefore, there has been an increasing interest in developing scalable and accurate generative modeling methods in molecular conformer generation. ", "page_idx": 0}, {"type": "image", "img_path": "avsZ9OlR60/tmp/193bd5590dcb06f3a202185e2b276db246ddb6888b44fe71f2af7543519545b3.jpg", "img_caption": ["Figure 1: (a) Overview of ET-Flow. The model predicts a conditional vector field $\\vec{v_{\\theta}}$ using interpolated positions $\\left(\\boldsymbol{x}_{t}\\right)$ , molecular structure $(G)$ , and time-step $(t)$ . Samples are drawn from the harmonic prior $(x_{0}\\sim p_{0})$ ) and then rotationally aligned with the samples from data $(x_{1}\\sim p_{1})$ ). A conditional probability path is constructed between pairs of $x_{0}$ and $x_{1}$ , and $x_{t}$ is then sampled from this path at a random time $t$ . (b) The ET-Flow architecture consists of a representation module based on the TorchMD-NET architecture (Th\u00f6lke and De Fabritiis, 2022) and an equivariant vector output module. For detailed architecture and input preprocessing information, see Section A.1. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Existing machine learning based approaches use diffusion models (Ho et al., 2020; Song and Ermon, 2019) to sample diverse and high quality samples given access to low-energy conformations. Prior methods typically fall into two categories: diffusing the atomic coordinates in the Cartesian space (Xu et al., 2022; Wang et al., 2024) or diffusing along the internal geometry such as pairwise distances, bond angles, and torsion angles (Ganea et al., 2021; Jing et al., 2022). ", "page_idx": 1}, {"type": "text", "text": "Early approaches based on diffusion (Shi et al., 2021; Luo et al., 2021; Xu et al., 2022) faced challenges such as lengthy inference and training times as well as having lower accuracy compared to cheminformatics methods. Torsional Diffusion (Jing et al., 2022) was the first to outperform cheminformatics methods by diffusing only on torsion angles after producing an initial conformer with the chemoinformatics tool RDKiT. This reliance on RDKiT structures instead of employing an end-to-end approach comes with several limitations, such as restricting the tool to applications where the local structures produced by RDKiT are of sufficient accuracy. Unlike prior approaches, the current state-of-the-art MCF (Wang et al., 2024) proposes a domain-agnostic approach by learning to diffuse over functions by scaling transformers and learning soft inductive bias from the data (Zhuang et al., 2022). Consequently, it comes with drawbacks such as high computational demands due to large number of parameters, limited sample efficiency from a lack of inductive biases like euclidean symmetries, and potential difficulties in scenarios with sparse data \u2014 a common challenge in this field. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose Equivariant Transformer Flow (ET-Flow), a simple yet powerful flowmatching model designed to generate low-energy 3D structures of small molecules with minimal assumptions. We utilize flow matching (Lipman et al., 2022; Albergo et al., 2023; Liu et al., 2022), which enables the learning of arbitrary probability paths beyond diffusion paths, enhancing both training and inference efficiency compared to conventional diffusion generative models. Departing from traditional equivariant architectures like EGNN (Satorras et al., 2021), we adopt an Equivariant Transformer (Th\u00f6lke and De Fabritiis, 2022) to better capture geometric features. Additionally, our method integrates a Harmonic Prior (Jing et al., 2023; Stark et al., 2023), leveraging the inductive bias that atoms connected by a bond should be in close proximity. We further optimize our flow matching objective by initially conducting rotational alignment on the harmonic prior, thereby constructing shorter probability paths between source and target distributions at minimal computational cost. ", "page_idx": 1}, {"type": "text", "text": "1. We obtain state-of-the-art precision for molecule conformer prediction, resulting in more physically realistic and reliable molecules for practitioners. We improve upon the previous methods by a large margin on ensemble property prediction.   \n2. We highlight the effectiveness of incorporating equivariance and more informed priors in generating physically-grounded molecules in our simple yet well-engineered method.   \n3. Our parameter-efficient model requires orders of magnitude fewer sampling steps than GeoDiff (Xu et al., 2022) and has significantly fewer parameters than MCF (Wang et al., 2024). ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion Generative Models. Diffusion models (Song and Ermon, 2019; Song et al., 2020; Ho et al., 2020) enables a high-quality and diverse sampling from an unknown data distribution by approximating the Stochastic Differential Equation(SDE) that maps a simple density i.e. Gaussian to the unknown data density. Concretely, it involves training a neural network to learn the score, represented as $\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})$ of the diffused data. During inference, the model generates sample by iteratively solving the reverse SDE. However, diffusion models have inherent drawbacks, as they (i) require on longer training times (ii) are restricted to specific probability paths and (iii) depend on the use of complicated tricks to speed up sampling (Song et al., 2020; Zhang and Chen, 2022). ", "page_idx": 2}, {"type": "text", "text": "Flow Matching. Flow Matching (Albergo et al., 2023; Lipman et al., 2022; Liu et al., 2022) provides a general framework to learn Continuous normalizing flows (CNFs) while improving upon diffusion models in simplicity, generality, and inference speed in several applications. Through simple regression against the vector field reminiscent of the score-matching objective in diffusion models, Flow matching has enabled a fast, simulation-free training of CNFs. Several subsequent studies have then expanded the scope of flow matching objective to manifolds (Chen and Lipman, 2024), arbitrary sources (Pooladian et al., 2023), and conditional flow matching with arbitrary transport maps and optimal couplings between source and target samples (Tong et al., 2023). ", "page_idx": 2}, {"type": "text", "text": "Molecular Conformer Generation. Various machine learning (ML) based approaches (Kingma and Welling, 2013; Liberti et al., 2014; Dinh et al., 2016; Simm and Hern\u00e1ndez-Lobato, 2019; Shi et al., 2021; Luo et al., 2021; Xu et al., 2021; Ganea et al., 2021; Xu et al., 2022; Jing et al., 2022; Wang et al., 2024) have been developed to improve upon the limitations of conventional methods, among which the most advanced are TorsionDiff (Jing et al., 2022) and Molecular Conformer Fields (MCF) (Wang et al., 2024). TorsionDiff designs a diffusion model on the torsion angles while incorporating the local structure from RDKiT ETKDG (Riniker and Landrum, 2015). MCF trains a diffusion model over functions that map elements from the molecular graph to points in 3D space. ", "page_idx": 2}, {"type": "text", "text": "Equivariant Architectures for Atomistic Systems. Inductive biases play an important role in generalization and sample efficiency. In the case of 3D atomistic modelling, one example of a useful inductive bias is the euclidean group $S O(3)$ which represents rotation equivariance in 3D space. Recently, various equivariant architectures (Duval et al., 2023) have been developed that act on both Cartesian (Satorras et al., 2021; Th\u00f6lke and De Fabritiis, 2022; Simeon and De Fabritiis, 2024; Du et al., 2022; Frank et al., 2022) and spherical basis (Musaelian et al., 2023; Batatia et al., 2022; Fuchs et al., 2020; Liao et al., 2023; Passaro and Zitnick, 2023; Anderson et al., 2019; Thomas et al., 2018). For molecular conformer generation, initial methods like ConfGF, DGSM utilize invariant networks as they act upon inter-atomic distances, whereas the use of equivariant GNNs have been used in GeoDiff (Xu et al., 2022) and Torsional Diffusion (Jing et al., 2022). GeoDiff utilizes EGNN (Satorras et al., 2021), a Cartesian basis equivariant architecture while Torsional Diffusion uses Tensor Field Networks (Thomas et al., 2018) to output pseudoscalars. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We design ET-Flow, a scalable equivariant model that generates energy-minimized conformers given a molecular graph. In this section, we layout the framework to achieve this objective by detailing the generative process in flow matching, the rotation alignment between distributions, stochastic sampling, and finally the architecture details. ", "page_idx": 2}, {"type": "text", "text": "Preliminaries We define notation that we use throughout this paper. Inputs are continuous atom positions $\\mathbf{x}\\in\\mathbb{R}^{N\\times3}$ where $N$ is the number of atoms. We use the notation $v_{t}(\\mathbf{x})$ interchangeably with $v(t,\\bf{x})$ for vector field. ", "page_idx": 3}, {"type": "text", "text": "3.1 Flow Matching ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The aim is to learn a time-dependent vector field $v_{t}(x):\\mathbb{R}^{N\\times3}\\times[0,1]\\to\\mathbb{R}^{N\\times3}$ associated with the transport map $X_{t}:\\mathbb{R}^{N\\times3}\\stackrel{\\cdot}{\\times}[0,1]\\rightarrow\\mathbb{R}^{N\\times3}$ that pushes forward samples from a base distribution $\\rho_{0}$ , often an easy-to-sample distribution, to samples from a more complex target distribution $\\rho_{1}$ , the low-energy conformations of a molecule. This can be defined as an ordinary differential equation (ODE), ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\dot{X}_{t}({\\bf x})=v_{t}(X_{t}({\\bf x})),\\qquad\\quad X_{t=0}={\\bf x}_{0},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x_{0}\\sim\\rho_{0}$ . We can construct the $v_{t}$ via a time-differentiable interpolation between samples from $\\rho_{0}$ and $\\rho_{1}$ that gives rise to a probability path $\\rho_{t}$ that we can easily sample (Lipman et al., 2022; Liu et al., 2022; Albergo and Vanden-Eijnden, 2023; Tong et al., 2023). The general interpolation between samples $x_{0}\\sim\\rho_{0}$ and $x_{1}\\sim\\rho_{1}$ can be defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nI_{t}(\\mathbf{x}_{0},\\mathbf{x}_{1})=\\alpha_{t}\\mathbf{x}_{1}+\\beta_{t}\\mathbf{x}_{0}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given this interpolant that couples $\\mathbf{x}_{\\mathrm{0}}$ and $\\mathbf{x}_{1}$ , we can define the conditional probability path as $\\rho_{t}(\\mathbf{x}|\\mathbf{x}_{0},\\mathbf{x}_{1})\\ =\\ \\mathcal{N}(\\mathbf{x}|I_{t}(\\mathbf{x}_{0},\\mathbf{x}_{1}),\\sigma_{t}^{2}\\mathbf{I})$ , and the vector field can be computed as $v_{t}(\\mathbf{x})\\;=\\;$ $\\partial_{t}\\rho_{t}(\\mathbf{x}|\\mathbf{x}_{0},\\mathbf{x}_{1})$ which has the following form ", "page_idx": 3}, {"type": "equation", "text": "$$\nv_{t}(\\mathbf{x})=\\dot{\\alpha}_{t}\\mathbf{x}_{1}+\\dot{\\beta}_{t}\\mathbf{x}_{0}+\\dot{\\sigma}_{t}\\mathbf{z}\\qquad\\mathbf{\\beta}\\mathbf{z}\\sim\\mathcal{N}(0,\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here we use $\\dot{\\alpha}_{t}$ as a shorthand notation for $\\partial_{t}\\alpha_{t}$ , and similarly we apply the same notation to $\\beta$ and $\\sigma$ . In our work, we use linear interpolation where $\\alpha_{t}=t$ , $\\beta_{t}=1-t$ , and $\\sigma_{t}=\\sigma\\sqrt{t(1-t)}$ , resulting in the vector field ", "page_idx": 3}, {"type": "equation", "text": "$$\nv_{t}(\\mathbf{x})=\\mathbf{x}_{1}-\\mathbf{x}_{0}+\\frac{1-2t}{2\\sqrt{t(1-t)}}\\mathbf{z}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Now, we can define the objective function for learning a vector field $v_{\\theta}(\\mathbf{x})$ that generates a probability path $\\rho_{t}$ between a base density $\\rho_{0}$ and the target density $\\rho_{1}$ as, ", "page_idx": 3}, {"type": "equation", "text": "$$\n=\\mathbb{E}_{t\\sim\\mathcal{U}(0,1),\\mathbf{x}\\sim\\rho_{t}(\\mathbf{x}_{0},\\mathbf{x}_{1})}\\|v(t,\\mathbf{x})-v_{\\theta}(t,\\mathbf{x})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For training, we sample (i) $\\mathbf{x}_{0}\\;\\sim\\;\\rho_{0},\\;\\mathbf{x}_{1}\\;\\sim\\;\\rho_{1}$ , and $t\\,\\sim\\,\\mathcal{U}(0,1)$ , (ii) interpolate according to Equation 2, (iii) add noise from a standard Gaussian, and (iv) minimize the loss defined in Equation 5. For sampling, we sample $\\mathbf{x}_{0}\\sim\\rho_{0}$ and integrate from $t=0$ to $t=1$ using the Euler\u2019s method. At each time-step, the Euler solver iteratively predicts the vector field for $\\mathbf{x}_{t}$ and updates its position $\\mathbf{x}_{t+\\Delta t}=\\mathbf{x}_{t}+v_{\\theta}(t,\\mathbf{x})\\Delta t$ . More details on the training and sampling algorithms are provided in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "3.2 Alignment ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Several previous works (Tong et al., 2023; Klein et al., 2024; Jing et al., 2024; Song et al., 2024) demonstrate that constructing a straighter path between base distribution $\\rho_{0}$ and target distribution $\\rho_{1}$ minimizes the transport costs and improves performance. In our work, we reduce the transport costs between samples from the harmonic prior $\\rho_{0}$ and samples from the data distribution $\\rho_{1}$ by rotationally aligning them using the Kabsch algorithm (Kabsch, 1976) similar to (Klein et al., 2024; Jing et al., 2024). This approach leads to faster convergence and reduces the path length between atoms by leveraging the similarity in \"shape\" of the samples as seen in Figure 1a without incurring high computational cost. ", "page_idx": 3}, {"type": "text", "text": "3.3 Stochastic Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We employ a variant of the stochastic sampling technique inspired by (Karras et al., 2022). Specifically, we inject noise at each time step to construct an intermediate state, evaluate the vector field from the intermediate state, and then perform the deterministic ODE step from the noisy state. The original method utilizes a second-order integration, which averages the denoiser output at the noisy intermediate state and the state at the next time step after integration. ", "page_idx": 3}, {"type": "text", "text": "In our experiment, we use the stochastic sampler without this second-order correction term, which empirically provided a performance boost comparable to the second-order method. We apply stochastic sampling only during the final part of the integration steps, specifically within the range $t\\in[0.8,1.0]$ . This helps prevent drifting towards overpopulated density regions and improves the quality of the samples (Karras et al., 2022). Stochastic sampling has improved both diversity and accuracy of the generated conformers, measured by Coverage and Average Minimum RMSD (AMR) respectively as shown in Table 1. Detailed information on the stochastic sampling algorithm is provided in algorithm B. ", "page_idx": 4}, {"type": "text", "text": "3.4 Chirality Correction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While generating conformations, it is necessary to take account of the stereochemistry of atoms bonded to four distinct groups also referred to as tetrahedral chiral centers. To generate conformations with the correct chirality, we propose a simple post hoc trick as done in GeoMol (Ganea et al., 2021). We compare the oriented volume (OV) (Equation 6) of the generated conformation and the required orientation with the RDKit tags. In the case of a mismatch, we simply flip the conformation against the ${\\bf Z}$ -axis. This correction step can be efficiently performed as a batched operation since it involves a simple comparison with the required RDKit tags and an inversion of position if necessary. ", "page_idx": 4}, {"type": "image", "img_path": "avsZ9OlR60/tmp/97ce33700c9e603b85d73e277910f49c653081348090331368f1433a026f1df5.jpg", "img_caption": ["Figure 2: Stochastic sampling procedure used in inference. Noise is added to the positions $x_{t}$ indicated by the purple line, resulting in $\\hat{x}_{t}$ . Then, the model predicts the vector field $\\hat{v}_{t}$ from $\\hat{x}_{t}$ instead of $x_{t}$ indicted by the yellow line and updates $\\hat{x}_{t}$ using $\\hat{v}_{t}$ to get $x_{t+1}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{OV}(p_{1},p_{2},p_{3},p_{4})=s i g n\\left(\\left|\\begin{array}{l l l l}{{1}}&{{1}}&{{1}}&{{1}}\\\\ {{x_{1}}}&{{x_{2}}}&{{x_{3}}}&{{x_{4}}}\\\\ {{y_{1}}}&{{y_{2}}}&{{y_{3}}}&{{y_{4}}}\\\\ {{z_{1}}}&{{z_{2}}}&{{z_{3}}}&{{z_{4}}}\\end{array}\\right|\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We also consider an alternative approach for chirality correction. Instead of using the post hoc correction with our $O(3)$ equivariant architecture, we slightly tweak our architecture to make it $S O(3)$ equivariant by introducing a cross product term in the update layers. We compare these methods on both the GEOM-DRUGS and GEOM-QM9 dataset in Table 1 and Table 2. Our base method (ET-Flow) corresponds to using the post hoc correction whereas the $S O(3)$ variant is referred by ET-Flow- $.S O(3)$ . We empirically observe that using an additional chirality correction step is not only computationally efficient, but also performs better. We provide details on the architectural modification and proof of $S O(3)$ equivariance in Section A.1 and Section C.1 respectively. ", "page_idx": 4}, {"type": "text", "text": "3.5 Architecture ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "ET-Flow (Figure 1b) consists of two main components: (1) a representation module based on the equivariant transformer architecture from TorchMD-NET (Th\u00f6lke and De Fabritiis, 2022) and (2) the equivariant vector output module. In the representation module, an embedding layer encodes the inputs (atomic positions, atomic numbers, atom features, bond features and the time-step) into a set of invariant features. Initial equivariant features are constructed using normalized edge vectors where the edges are constructed using a radius graph of 10 angstrom and the bonds from the 2D molecular graph. Then, a series of equivariant attention-based layers update both the invariant and equivariant features using a multi-head attention mechanism. Finally, the vector field is produced by the output layer, which updates the equivariant features using gated equivariant blocks (Sch\u00fctt et al., 2018). Given that TorchMD-NET was originally designed for modeling neural network potentials, we implement several modifications to its architecture to better suit generative modeling, as detailed in Section A.1. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We empirically evaluate ET-Flow by comparing the generated and ground-truth conformers in terms of distance-based RMSD (Section 4.2) and chemical property based metrics (Section 4.4). We ", "page_idx": 4}, {"type": "table", "img_path": "avsZ9OlR60/tmp/8b8df3f0d5f6a9cf83dde8d55ce6a0955ccf018c5f69d7c5dc7354495d33ac10.jpg", "table_caption": ["Table 1: Molecule conformer generation results on GEOM-DRUGS $\\mathit{\\dot{\\Phi}}(\\delta=0.75\\mathring{\\mathrm{A}})$ . ET-Flow - SS is ET-Flow with stochastic sampling and ET-Flow - $S O(3)$ is ET-Flow using the $S O(3)$ architecture for chirality correction. For ET-Flow, ET-Flow-SS and ET-Flow- $S O(3)$ , we sample conformations over 50 time-steps. "], "table_footnote": ["present the general experimental setups in Section 4.1. The implementation details are provided in Appendix A. "], "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Dataset: We conduct our experiments on the GEOM dataset (Axelrod and Gomez-Bombarelli, 2022), which offers curated conformer ensembles produced through meta-dynamics in CREST (Pracht et al., 2024). Our primary focus is on GEOM-DRUGS, the most extensive and pharmacologically relevant subset comprising $304\\mathrm{k}$ drug-like molecules, each with an average of 44 atoms. We use a train/validation/test (243473/30433/1000) split as provided in (Ganea et al., 2021) Additionally, we train and test model on GEOM-QM9, a subset of smaller molecules with an average of 11 atoms. Finally, in order to assess the model\u2019s ability to generalize to larger molecules, we evaluate the model trained on GEOM-DRUGS on a GEOM-XL dataset, a subset of large molecules with more than 100 atoms. The results for GEOM-QM9 and GEOM-XL can be found in the Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Evaluation: Our evaluation methodology is similar to that of (Jing et al., 2022). First, we look at RMSD based metrics like Coverage and Average Minimum RMSD (AMR) between generated and ground truth conformer ensembles. For this, we generate $2K$ conformers for a molecule with $K$ ground truth conformers. Second, we look at chemical similarity using properties like Energy $(E)$ , dipole moment $(\\mu)$ , HOMO-LUMO gap $(\\Delta\\epsilon)$ and the minimum energy $\\mathrm{\\Delta}E_{\\mathrm{min}})$ calculated using xTB (Bannwarth et al., 2019). ", "page_idx": 5}, {"type": "text", "text": "Baselines: We benchmark ET-Flow against leading approaches outlined in Section 2. Specifically, we assess the performance of GeoMol (Ganea et al., 2021), GeoDiff (Xu et al., 2022), Torsional Diffusion (Jing et al., 2022), and MCF (Wang et al., 2024). Notably, the most recent among these, MCF, has demonstrated superior performance across evaluation metrics compared to its predecessors. It\u2019s worth mentioning that GeoDiff initially utilized a limited subset of the GEOM-DRUGS dataset; thus, for a fair comparison, we consider its re-evaluated performance as presented in (Jing et al., 2022). ", "page_idx": 5}, {"type": "text", "text": "4.2 Ensemble RMSD ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As shown in Table 1 and Table 2, ET-Flow outperforms all preceding methodologies and demonstrates competitive performance with the previous state-of-the-art, MCF (Wang et al., 2024). Despite being significantly smaller with only 8.3M parameters, ET-Flow shows a substantial improvement in the quality of generated conformers, as evidenced by superior Precision metrics across all MCF models, including the largest MCF-L. When compared to MCF-S, which is closer in size, ET-Flow achieves markedly better Precision while the impact on Recall is less significant and limited to Recall Coverage. Notably, our Recall AMR remains competitive with much bigger MCF-B, underscoring the inherent advantage of our method in accurately predicting overall structures. ", "page_idx": 5}, {"type": "text", "text": "Table 2: Molecule conformer generation results on GEOM-QM9 $\\!\\!\\!\\delta=0.5\\mathring{\\mathrm{A}}\\!\\!\\!$ . ET-Flow - $S O(3)$ is ETFlow using the $S O(3)$ architecture for chirality correction. For both ET-Flow and ET-Flow- $\\cdot S O(3)$ , we sample conformations over 50 time-steps. ", "page_idx": 6}, {"type": "table", "img_path": "avsZ9OlR60/tmp/f4ecaa63802c9f9d9cc4fc7bc96a5ac48c25f79d7b11f647305106ff1fcbf1ee.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 Coverage Threshold Plots ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare the coverage metrics of ET-Flow against Torsional diffusion (Jing et al., 2022) and MCF (Wang et al., 2024) against a wide range of thresholds on the GEOM DRUGS dataset in Figure 3. ET-Flow consistently outperforms previous methods in precision-based metrics. In terms of recall, our approach demonstrates better performance than Torsional Diffusion across all thresholds. Despite MCF performing better at higher thresholds, ET-Flow outperforms in the lower thresholds, underscoring its proficiency in generating accurate conformer predictions. ", "page_idx": 6}, {"type": "image", "img_path": "avsZ9OlR60/tmp/eaed7a2b5427af1f86f901d1454b162ad44cb96a23e6a488f784328f3cd6ac20.jpg", "img_caption": ["Figure 3: Recall and Precision Coverage result on GEOM-DRUGS as a function of the threshold distance. ET-Flow outperforms TorsionDiff by a large margin especially in a lower threshold region. We emphasize the better performance of ET-Flow at lower thresholds in both Recall and Precision metrics. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.4 Ensemble Properties ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "RMSD provides a geometric measure for assessing ensemble quality, but it is also essential to consider the chemical similarity between generated and ground truth ensembles. For a random 100-molecule subset of the test set of GEOM-DRUGS, if a molecule has $K$ ground truth conformers, we generate a minimum of $2K$ and a maximum of 32 conformers per molecule. These conformers are then relaxed using GFN2-xTB (Bannwarth et al., 2019), and the Boltzmann-weighted properties of the generated and ground truth ensembles are compared. Specifically, using xTB (Bannwarth et al., ", "page_idx": 6}, {"type": "text", "text": "2019), we compute properties such as energy $(E)$ , dipole moment $(\\mu)$ , HOMO-LUMO gap $(\\Delta\\epsilon)$ , and the minimum energy $\\left(E_{m i n}\\right)$ . Table 3 illustrates the median errors for ET-Flow and the baselines, highlighting our method\u2019s capability to produce chemically accurate ensembles. Notably, we achieve significant improvements over both TorsionDiff and MCF across all evaluated properties. ", "page_idx": 7}, {"type": "table", "img_path": "avsZ9OlR60/tmp/06bd8d55516b86aef0960340d7a379bd7a586e00153306780cbc597fc0efa3d1.jpg", "table_caption": ["Table 3: Median averaged errors of ensemble properties between sampled and generated conformers $(E,\\Delta\\varepsilon,E_{m i n}$ in kcal/mol, and $\\mu$ in debye). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.5 Inference Steps Ablation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Table 1, our sampling process with ET-Flow utilizes 50 inference steps. To evaluate the method\u2019s performance under constrained computational resources, we conducted an ablation study by progressively reducing the number of inference steps. Specifically, we sample for 5, 10 and 20 time-steps. The results on GEOM-DRUGS are presented in Table 4. We observed minimal performance degradation with a decrease in the number of steps. Notably, ET-Flow demonstrates high efficiency, maintaining performance across all precision and recall metrics even with as few as 5 inference steps. Interestingly, ET-Flow with 5 steps still achieves superior precision metrics compared to all existing methods. This underscores ET-Flow\u2019s ability to generate high-quality conformations while operating within limited computational budgets. ", "page_idx": 7}, {"type": "text", "text": "4.6 Sampling Efficiency ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "avsZ9OlR60/tmp/0b5278dbe1ae8d634479f808a220ec5da499b15d71d0f0fbca60704a5e499ea5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Sampling efficiency as a measure of the quality of Inference time with respect to the number of time steps on GEOM-DRUGS. ", "page_idx": 7}, {"type": "text", "text": "We demonstrate the ability of ET-Flow to generate samples efficiently. We evaluate the inference time per molecule over varying number of time steps and report the average time across 1000 random samples from the test set of GEOM-DRUGS. Figure 4 shows that ET-Flow outperforms Torsional diffusion (Jing et al., 2022) in inference across all time steps. While ET-Flow may not achieve the fastest raw inference times (potentially due to MCF variants benefiting from optimized CUDA kernels for attention), it maintains competitive speeds while ensuring higher precision. We suspect that concurrent work on improving equivariant operations with optimized CUDA kernels (Lee et al., 2024) should lead to similar efficiency gains as seen in transformer-based architectures. ", "page_idx": 7}, {"type": "table", "img_path": "avsZ9OlR60/tmp/8ae34a2c4200006d3f1c102b579e637d594bb277898c1b38685b96a15298476c.jpg", "table_caption": ["Table 4: Ablation over number of inference steps on GEOM-DRUGS $\\mathit{\\dot{\\delta}}=0.75\\mathring{\\mathrm{A}};$ ). Performance of ET-Flow at 5 steps is competent across all metrics while also retaining state-of-the-art performance on precision metrics when compared with previous methods. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "ET-Flow effectively balances performance and speed, making it ideal for tasks that require high sample quality with efficient computation. With the ability to generate high-quality samples in fewer time steps, e.g., 5 time steps, as indicated in Table 4, ET-Flow is well-suited for scenarios demanding a large number of samples, as fewer steps lead to lower inference time per molecule. Additionally, we encountered difficulties running MCF-L for 20 and 50 steps, so those results have not been included. In summary, ET-Flow demonstrates efficient sampling, balancing precision and speed, making it highly effective for generating high-quality molecular samples while remaining competitive in inference time. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we present our simple and scalable method ET-Flow, which utilizes an equivariant transformer with flow matching to achieve state-of-the-art performance on multiple molecular conformer generation benchmarks. By incorporating inductive biases, such as equivariance, and enhancing probability paths with a harmonic prior and RMSD alignment, we significantly improve the precision of the generated molecules, and consequently generate more physically plausible molecules. Importantly, our approach maintains parameter and speed efficiency, making it not only effective but also accessible for practical high-throughput applications. ", "page_idx": 8}, {"type": "text", "text": "6 Limitations And Future Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While ET-Flow demonstrates competitive performance in molecular conformer generation, there are areas where it can be enhanced. One such area is the recall metrics, which capture the diversity of generated conformations. Another area is the use of an additional chirality correction step that is used to predict conformations with the desired chirality. Moreover, although our performance on the GEOM-XL dataset is comparable to MCF-S and TorsionDiff, there is still room for improvement. ", "page_idx": 8}, {"type": "text", "text": "We propose three future directions here. First, we observe during experiments that a well-designed sampling process incorporating stochasticity can enhance the quality and diversity of generated samples. An extension of our current approach could involve using Stochastic Differential Equations (SDEs), which utilize both vector field and score in the integration process, potentially improving the diversity of samples. Second, we propose to scale the number of parameters of ET-Flow, which has not only been shown to be useful across different domains of deep learning, but has also shown to be useful in molecular conformer generation for MCF (Wang et al., 2024). Third, to better handle the chirality problem, we aim to explore alternatives for incorporating $S O(3)$ -equivariance into the model in the future. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The authors sincerely thank Cristian Gabellini, Jiarui Ding, and the NeurIPS reviewers for the insightful discussions and feedback. Resources used in completing this research were provided by Valence Labs. Furthermore, we acknowledge a grant for student supervision received by Mila - Quebec\u2019s AI institute - and financed by the Quebec ministry of Economy. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Michael S. Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants, 2023.   \nMichael S Albergo, Nicholas M Boff,i and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.   \nBrandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural networks. Advances in neural information processing systems, 32, 2019.   \nSimon Axelrod and Rafael Gomez-Bombarelli. Geom, energy-annotated molecular conformations for property prediction and molecular generation. Scientific Data, 9(1):185, 2022.   \nSimon Axelrod and Rafael Gomez-Bombarelli. Molecular machine learning with conformer ensembles. Machine Learning: Science and Technology, 4(3):035025, 2023.   \nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.   \nAndrew J Ballard, Stefano Martiniani, Jacob D Stevenson, Sandeep Somani, and David J Wales. Exploiting the potential energy landscape to sample free energy. Wiley Interdisciplinary Reviews: Computational Molecular Science, 5(3):273\u2013289, 2015.   \nChristoph Bannwarth, Sebastian Ehlert, and Stefan Grimme. Gfn2-xtb\u2014an accurate and broadly parametrized self-consistent tight-binding quantum chemical method with multipole electrostatics and density-dependent dispersion contributions. Journal of Chemical Theory and Computation, 15 (3):1652\u20131671, 2019. doi: 10.1021/acs.jctc.8b01176. URL https://doi.org/10.1021/acs. jctc.8b01176. PMID: 30741547.   \nIlyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and G\u00e1bor Cs\u00e1nyi. Mace: Higher order equivariant message passing neural networks for fast and accurate force fields. Advances in Neural Information Processing Systems, 35:11423\u201311436, 2022.   \nEvan E Bolton, Sunghwan Kim, and Stephen H Bryant. Pubchem3d: conformer generation. Journal of cheminformatics, 3:1\u201316, 2011.   \nRicky T. Q. Chen and Yaron Lipman. Flow matching on general geometries, 2024.   \nJason C Cole, Oliver Korb, Patrick McCabe, Murray G Read, and Robin Taylor. Knowledge-based conformer generation using the cambridge structural database. Journal of Chemical Information and Modeling, 58(3):615\u2013629, 2018.   \nMarco De Vivo, Matteo Masetti, Giovanni Bottegoni, and Andrea Cavalli. Role of molecular dynamics and related methods in drug discovery. Journal of medicinal chemistry, 59(9):4035\u20134061, 2016.   \nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480\u20137512. PMLR, 2023.   \nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016.   \nWeitao Du, He Zhang, Yuanqi Du, Qi Meng, Wei Chen, Nanning Zheng, Bin Shao, and Tie-Yan Liu. Se (3) equivariant graph neural networks with complete local frames. In International Conference on Machine Learning, pages 5583\u20135608. PMLR, 2022.   \nAlexandre Duval, Simon V Mathis, Chaitanya K Joshi, Victor Schmidt, Santiago Miret, Fragkiskos D Malliaros, Taco Cohen, Pietro Li\u00f2, Yoshua Bengio, and Michael Bronstein. A hitchhiker\u2019s guide to geometric gnns for 3d atomic systems. arXiv preprint arXiv:2312.07511, 2023.   \nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024.   \nThorben Frank, Oliver Unke, and Klaus-Robert M\u00fcller. So3krates: Equivariant attention for interactions on arbitrary length-scales in molecular systems. Advances in Neural Information Processing Systems, 35:29400\u201329413, 2022.   \nFabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d rototranslation equivariant attention networks. Advances in neural information processing systems, 33: 1970\u20131981, 2020.   \nOctavian Ganea, Lagnajit Pattanaik, Connor Coley, Regina Barzilay, Klavs Jensen, William Green, and Tommi Jaakkola. Geomol: Torsional geometric generation of molecular 3d conformer ensembles. Advances in Neural Information Processing Systems, 34:13757\u201313769, 2021.   \nJohannes Gasteiger, Janek Gro\u00df, and Stephan G\u00fcnnemann. Directional message passing for molecular graphs. arXiv preprint arXiv:2003.03123, 2020.   \nCristiano RW Guimar\u00e3es, Alan M Mathiowetz, Marina Shalaeva, Gilles Goetz, and Spiros Liras. Use of 3d properties to characterize beyond rule-of-5 property space for passive permeation. Journal of chemical information and modeling, 52(4):882\u2013890, 2012.   \nPaul CD Hawkins. Conformation generation: the state of the art. Journal of chemical information and modeling, 57(8):1747\u20131756, 2017.   \nPaul CD Hawkins, A Geoffrey Skillman, Gregory L Warren, Benjamin A Ellingson, and Matthew T Stahl. Conformer generation with omega: algorithm and validation using high quality structures from the protein databank and cambridge structural database. Journal of chemical information and modeling, 50(4):572\u2013584, 2010.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \nBowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for molecular conformer generation. Advances in Neural Information Processing Systems, 35:24240\u201324253, 2022.   \nBowen Jing, Ezra Erives, Peter Pao-Huang, Gabriele Corso, Bonnie Berger, and Tommi Jaakkola. Eigenfold: Generative protein structure prediction with diffusion models. arXiv preprint arXiv:2304.02198, 2023.   \nBowen Jing, Bonnie Berger, and Tommi Jaakkola. Alphafold meets flow matching for generating protein ensembles, 2024.   \nW. Kabsch. A solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section A, 32(5):922\u2013923, Sep 1976. doi: 10.1107/S0567739476001873. URL https://doi. org/10.1107/S0567739476001873.   \nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models, 2022.   \nDiederik $\\mathbf{P}$ Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \nLeon Klein, Andreas Kr\u00e4mer, and Frank No\u00e9. Equivariant flow matching. Advances in Neural Information Processing Systems, 36, 2024.   \nDavid Lagorce, Tania Pencheva, Bruno O Villoutreix, and Maria A Miteva. Dg-ammos: A new tool to generate 3d conformation of small molecules using d istance g eometry and a utomated m olecular m echanics o ptimization for in silico s creening. BMC Chemical Biology, 9:1\u201310, 2009.   \nGreg Landrum et al. Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. Greg Landrum, 8(31.10):5281, 2013.   \nKin Long Kelvin Lee, Mikhail Galkin, and Santiago Miret. Deconstructing equivariant representations in molecular systems. In AI for Accelerated Materials Design - NeurIPS 2024, 2024. URL https://openreview.net/forum?id=pshyLoyzRn.   \nJiabo Li, Tedman Ehlers, Jon Sutter, Shikha Varma-O\u2019Brien, and Johannes Kirchmair. Caesar: a new conformer generation algorithm based on recursive buildup and local rotational symmetry consideration. Journal of chemical information and modeling, 47(5):1923\u20131932, 2007.   \nYi-Lun Liao, Brandon Wood, Abhishek Das, and Tess Smidt. Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations. arXiv preprint arXiv:2306.12059, 2023.   \nLeo Liberti, Carlile Lavor, Nelson Maculan, and Antonio Mucherino. Euclidean distance geometry and applications. SIAM review, 56(1):3\u201369, 2014.   \nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.   \nXingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.   \nShitong Luo, Chence Shi, Minkai Xu, and Jian Tang. Predicting molecular conformation via dynamic graph score matching. Advances in Neural Information Processing Systems, 34:19784\u201319795, 2021.   \nMaria A Miteva, Frederic Guyon, and Pierre Tuff\u00ef\u00bf 1/2ry. Frog2: Efficient 3d conformation ensemble generator for small compounds. Nucleic acids research, 38(suppl_2):W622\u2013W627, 2010.   \nAlbert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai Kornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. Nature Communications, 14(1):579, 2023.   \nSaro Passaro and C Lawrence Zitnick. Reducing so (3) convolutions to so (2) for efficient equivariant gnns. In International Conference on Machine Learning, pages 27420\u201327438. PMLR, 2023.   \nAram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky T. Q. Chen. Multisample flow matching: Straightening flows with minibatch couplings, 2023.   \nPhilipp Pracht, Fabian Bohle, and Stefan Grimme. Automated exploration of the low-energy chemical space with fast quantum chemical methods. Physical Chemistry Chemical Physics, 22(14):7169\u2013 7192, 2020.   \nPhilipp Pracht, Stefan Grimme, Christoph Bannwarth, Fabian Bohle, Sebastian Ehlert, Gereon Feldmann, Johannes Gorges, Marcel M\u00fcller, Tim Neudecker, Christoph Plett, et al. Crest\u2014a program for the exploration of low-energy molecular chemical space. The Journal of Chemical Physics, 160(11), 2024.   \nSereina Riniker and Gregory A Landrum. Better informed distance geometry: using what we know to improve conformation generation. Journal of chemical information and modeling, 55(12): 2562\u20132574, 2015.   \nV\u0131ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In International conference on machine learning, pages 9323\u20139332. PMLR, 2021.   \nKristof Sch\u00fctt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In International Conference on Machine Learning, pages 9377\u20139388. PMLR, 2021.   \nKristof T Sch\u00fctt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R M\u00fcller. Schnet\u2013a deep learning architecture for molecules and materials. The Journal of Chemical Physics, 148(24), 2018.   \nChence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. In International conference on machine learning, pages 9558\u20139568. PMLR, 2021.   \nJihyun Shim and Alexander D MacKerell Jr. Computational ligand-based rational design: role of conformational sampling and force fields in model development. MedChemComm, 2(5):356\u2013370, 2011.   \nGuillem Simeon and Gianni De Fabritiis. Tensornet: Cartesian tensor representations for efficient learning of molecular potentials. Advances in Neural Information Processing Systems, 36, 2024.   \nGregor NC Simm and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. A generative model for molecular distance geometry. arXiv preprint arXiv:1909.11459, 2019.   \nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \nYuxuan Song, Jingjing Gong, Minkai Xu, Ziyao Cao, Yanyan Lan, Stefano Ermon, Hao Zhou, and Wei-Ying Ma. Equivariant flow matching with hybrid probability transport for 3d molecule generation. Advances in Neural Information Processing Systems, 36, 2024.   \nHannes Stark, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Harmonic prior self-conditioned flow matching for multi-ligand docking and binding site design. In NeurIPS 2023 AI for Science Workshop, 2023.   \nPhilipp Th\u00f6lke and Gianni De Fabritiis. Torchmd-net: Equivariant transformers for neural network based molecular potentials. arXiv preprint arXiv:2202.02541, 2022.   \nNathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018.   \nAlexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Conditional flow matching: Simulation-free dynamic optimal transport. arXiv preprint arXiv:2302.00482, 2(3), 2023.   \nOliver T Unke and Markus Meuwly. Physnet: A neural network for predicting energies, forces, dipole moments, and partial charges. Journal of chemical theory and computation, 15(6):3678\u20133693, 2019.   \nYuyang Wang, Ahmed A. Elhag, Navdeep Jaitly, Joshua M. Susskind, and Miguel Angel Bautista. Swallowing the bitter pill: Simplified scalable conformer generation, 2024.   \nMinkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative dynamics for molecular conformation generation. arXiv preprint arXiv:2102.10240, 2021.   \nMinkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. arXiv preprint arXiv:2203.02923, 2022.   \nQinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022.   \nPeiye Zhuang, Samira Abnar, Jiatao Gu, Alex Schwing, Joshua M Susskind, and Miguel Angel Bautista. Diffusion probabilistic fields. In The Eleventh International Conference on Learning Representations, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "avsZ9OlR60/tmp/cd38648efd0c5a44c3182802b4a60e868654d3d6d7e20af1849449b43fdc6fb2.jpg", "img_caption": ["A Implementation Details "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 5: (a) Overall Architecture of ET-Flow consisting of 2 components, (1) Representation Layer based on TorchMD-NET Th\u00f6lke and De Fabritiis (2022) and (2) Equivariant Output Layer from (Sch\u00fctt et al., 2018). (b) Equivariant Attention Layer with all the operations involved, (c) Multi-Head Attention block modified with the LayerNorm. ", "page_idx": 13}, {"type": "text", "text": "A.1 Architecture ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The ET-Flow architecture (Figure 5) consists of 2 major components, a representation layer and an output layer. For the representation layer, we use a modified version of the embedding and equivariant attention-based update layers from the equivariant transformer architecture of TorchMD-NET (Th\u00f6lke and De Fabritiis, 2022). The output layer utilizes the gated equivariant blocks from (Sch\u00fctt et al., 2018). We highlight our modifications over the original TorchMD-NET architecture with this color. These modifications enable stabilized training since we use a larger network than the one proposed in the TorchMD-NET (Th\u00f6lke and De Fabritiis, 2022) paper. Additionally, since our input structures are interpolations between structures sampled from a prior and actual conformations, it is important to ensure our network is numerically stable when the interpolations contain two atoms very close to each other. ", "page_idx": 13}, {"type": "text", "text": "Embedding Layer: The embedding layer maps each atom\u2019s physical and chemical properties into a learned representation space, capturing both local atomic features and geometric neighborhood information. For the $i$ -th atom in a molecule with $N$ atoms, we compute an invariant embedding $x_{i}$ through the following process: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{z_{i}=\\mathrm{embed}^{\\mathrm{int}}(z_{i})}\\\\ {h_{i}=\\mathrm{MLP}(h_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $z_{i}$ is the atomic number and $h_{i}$ represents atomic attributes (detailed in Appendix A). The MLP projects atomic attributes into a feature vector of dimension $d_{h}$ . ", "page_idx": 13}, {"type": "text", "text": "Next, we compute a neighborhood embedding $n_{i}$ that captures local atomic environment: ", "page_idx": 13}, {"type": "equation", "text": "$$\nn_{i}=\\sum_{j=1}^{N}\\mathrm{embed}^{\\mathrm{nbh}}(z_{j})\\cdot g(d_{i j},l_{i j}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here, embed $^\\mathrm{nbh}(z_{j})$ provides a separate embedding for neighboring atomic numbers, $d_{i j}$ is the distance between atoms $i$ and $j$ , and $l_{i j}$ encodes edge features (either from a radius-based graph or molecular bonds). The interaction function $g(d_{i j},l_{i j})$ combines distance and edge information: ", "page_idx": 13}, {"type": "equation", "text": "$$\ng(d_{i j},l_{i j})=W^{F}\\left[\\phi(d_{i j})e_{1}^{\\mathrm{RBF}}(d_{i j}),\\dots,\\phi(d_{i j})e_{K}^{\\mathrm{RBF}}(d_{i j}),l_{i j}\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $e_{k}^{\\mathrm{RBF}}$ are $K$ exponential radial basis functions following (Unke and Meuwly, 2019), and $\\phi(d_{i j})$ is a smooth cutoff function: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\phi(d_{i j})=\\left\\{\\begin{array}{l l}{\\frac{1}{2}\\left(\\cos(\\frac{\\pi d_{i j}}{d_{\\mathrm{cutoff}}}+1)\\right),}&{\\mathrm{if}\\;d_{i j}\\leq d_{\\mathrm{cutoff}}}\\\\ {0,}&{\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, we combine all features into the atom\u2019s embedding through a linear projection: ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{i}=W^{C}\\left[{\\mathrm{embed}}^{\\mathrm{int}}(z_{i}),h_{i},t,n_{i}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $t$ represents the time-step, and $[\\cdot,\\cdot]$ denotes concatenation. The resulting embedding $\\boldsymbol{x}_{i}\\in\\mathbb{R}^{d}$ serves as input to subsequent layers of the network. ", "page_idx": 14}, {"type": "text", "text": "Attention Mechanism: The multi-head dot-product attention operation uses atom features $x_{i}$ , atom attributes $h_{i}$ , time-step $t$ and inter-atomic distances $d_{i j}$ to compute attention weights. The input atom-level features $x_{i}$ are mixed with the atom attributes $h_{i}$ and the time-step $t$ using an MLP and then normalized using a LayerNorm (Ba et al., 2016). To compute the attention matrix, the inter-atomic distances $d_{i j}$ are projected into two dimensional filters $D^{K}$ and $D^{V}$ as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{D^{K}=\\sigma\\left({W^{D^{K}}e^{R B F}(d_{i j})+b^{D^{K}}}\\right)}}\\\\ {{D^{V}=\\sigma\\left({W^{D^{V}}e^{R B F}(d_{i j})+b^{D^{V}}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The atom level features are then linearly projected along with a LayerNorm operation to derive the query $Q$ and key $K$ vectors. The value vector $V$ is computed with only the linear projection of atom-level features. Applying LayerNorm on Q, K vectors (also referred to as QK-Norm) has proven to stabilize un-normalized values in the attention matrix (Dehghani et al., 2023; Esser et al., 2024) when scaling networks to large number of parameters. The $Q$ and $K$ vectors are then used along with the distance fliter $D^{K}$ for a dot-product operation over the feature dimension: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle{Q=\\mathrm{LayerNorm}(W^{Q}x_{i}),\\quad K=\\mathrm{LayerNorm}(W^{K}x_{i}),\\quad V=W^{V}x_{i}}}\\\\ {\\displaystyle{\\mathrm{dot}(Q,K,D^{K})=\\sum_{k}^{F}Q_{k}\\cdot K_{k}\\cdot D_{k}^{K}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The attention matrix is derived by passing the above dot-product operation matrix through a non-linearity and weighting it using a cosine cutoff $\\phi(d_{i j})$ (similar to the embedding layer) which ensures the attention weights are non-zero only when two atoms are within a specified cutoff: ", "page_idx": 14}, {"type": "equation", "text": "$$\nA=\\mathrm{SiLU}(\\operatorname*{dot}(Q,K,D^{K}))\\cdot\\phi(d_{i j}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using the value vector $V$ and the distance filter $D_{V}$ , we derive 3 equally sized filters by splitting along the feature dimension, ", "page_idx": 14}, {"type": "equation", "text": "$$\ns_{i j}^{1},s_{i j}^{2},s_{i j}^{3}=\\mathrm{split}(V_{j}\\cdot D_{i j}^{V}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A linear projection is then applied to combine the attention matrix and the vectors $s_{i j}^{3}$ to derive an atom level feature $\\begin{array}{r}{y_{i}=W^{O}\\left(\\sum_{j}^{N}{A_{i j}\\cdot s_{i j}^{3}}\\right)}\\end{array}$ . The output of the attention operation are $y_{i}$ (an atom level feature) and two scalar filters $s_{i j}^{1}$ and $s_{i j}^{2}$ (edge-level features). ", "page_idx": 14}, {"type": "text", "text": "Update Layer: The update layer computes interactions between atoms in the attention block and uses the outputs to update the scalar feature $x_{i}$ and the vector feature $\\vec{v_{i}}$ . First, the scalar feature output $y_{i}$ from the attention mechanism is split into three features $(q_{i}^{1},q_{i}^{2},q_{i}^{3})$ , out of which $q_{i}^{1}$ and $q_{i}^{2}$ are used for the scalar feature update as, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta x_{i}=q_{i}^{1}+q_{i}^{2}\\cdot\\langle U_{1}\\vec{v}_{i}\\cdot U_{2}\\vec{v}_{i}\\rangle,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\left\\langle U_{1}\\vec{v}_{i}\\cdot U_{2}\\vec{v}_{i}\\right\\rangle$ is the inner product between linear projections of vector features $\\vec{v_{i}}$ with matrices $U_{1},U_{2}$ ", "page_idx": 14}, {"type": "text", "text": "The edge vector update consists of two components. First, we compute a vector $\\overrightarrow{w}i$ , which for each atom is computed as a weighted sum of vector features and a clamped-norm of the edge vectors over all neighbors: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\vec{w}_{i}=\\sum_{j}^{N}s_{i j}^{1}\\cdot\\vec{v}_{j}+s_{i j}^{2}\\cdot\\frac{\\vec{r}_{i}-\\vec{r}_{j}}{\\operatorname*{max}(\\|\\vec{r}_{i}-\\vec{r}_{j}\\|,\\epsilon)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta\\vec{v}_{i}=\\vec{w}_{i}+q_{i}^{3}\\cdot U_{3}\\vec{v}_{i}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $U_{1}$ and $U_{3}$ are projection matrices over the feature dimension of the vector feature $\\vec{v_{i}}$ . In this layer, we clamp the minimum value of the norm (to $\\epsilon=0.01$ ) to prevent numerically large values in cases where positions of two atoms are sampled too close from the prior. ", "page_idx": 14}, {"type": "text", "text": "$S O(3)$ Update Layer: We also design an $S O(3)$ equivariant architecture by adding an additional cross product term in Equation 19 as follows, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\vec{w}_{i}=\\sum_{j}^{N}s_{i j}^{1}\\cdot\\vec{v}_{j}+s_{i j}^{2}\\cdot\\frac{\\vec{r_{i}}-\\vec{r_{j}}}{\\operatorname*{max}(\\lVert\\vec{r_{i}}-\\vec{r_{j}}\\rVert,\\epsilon)}+s_{i j}^{4}\\cdot\\left(\\vec{v}_{j}\\times\\frac{\\vec{r_{i}}-\\vec{r_{j}}}{\\operatorname*{max}(\\lVert\\vec{r_{i}}-\\vec{r_{j}}\\rVert,\\epsilon)}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $s i j^{4}$ is derived by modifying the split operation Equation 17 in the attention layer where the value vector $V$ and distance filter $D_{V}$ is projected into 4 equally sized filters instead of 3. ", "page_idx": 15}, {"type": "text", "text": "Output Layer: The output layer consists of Gated Equivariant Blocks from (Sch\u00fctt et al., 2018). Given atom scalar $x_{i}$ and vector features $\\vec{v_{i}}$ , the updates in each block is defined as, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{i,\\mathrm{updated}},\\vec{w}_{i}=\\mathrm{split}(\\mathbf{MLP}([x_{i},U_{1}\\vec{v}_{i})]))}\\\\ {\\vec{v}_{i,\\mathrm{updated}}=(U_{2}\\vec{v}_{i})\\cdot\\vec{w}_{i}\\,~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, $U_{1}$ and $U_{2}$ are linear projection matrices that act along feature dimension. Our modification is to use LayerNorm in the MLP to improve training stability. ", "page_idx": 15}, {"type": "text", "text": "A.2 Input Featurization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Atomic features (or Node Features) are computed using RDKit (Landrum et al., 2013) features as described in Table 5. For computing edge features and edge index, we use a combination of global (radius based edges) and local (molecular graph edges) similar to (Jing et al., 2022). ", "page_idx": 15}, {"type": "table", "img_path": "avsZ9OlR60/tmp/3ebe5fa308cabe126f9aa7405dd575c92b43f623e80eea7b8c1fd596d53a7785.jpg", "table_caption": ["Table 5: Atomic features included in ET-Flow. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.3 Evaluation Metrics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Following the approaches of (Ganea et al., 2021; Xu et al., 2022; Jing et al., 2022), we utilize Average Minimum RMSD (AMR) and Coverage (COV) to assess the performance of molecular conformer generation. Here, $C_{g}$ denotes the set of generated conformations, and $C_{r}$ denotes the set of reference conformations. For both AMR and COV, we calculate and report Recall (R) and Precision (P). Recall measures the extent to which the generated conformers capture the ground-truth conformers, while Precision indicates the proportion of generated conformers that are accurate. The specific formulations for these metrics are detailed in the following equations: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{AMR-R}(C_{g},C_{r})={\\frac{1}{|C_{r}|}}\\sum_{\\mathbf{R}\\in C_{r}}\\operatorname*{min}_{\\mathbf{R}\\in C_{g}}\\operatorname{RMSD}(\\mathbf{R},{\\hat{\\mathbf{R}}})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{COV-R}(C_{g},C_{r})=\\frac{1}{|C_{r}|}|\\{{\\bf R}\\in C_{r}|\\mathrm{RMSD}({\\bf R},\\hat{\\bf R})<\\delta,\\hat{\\bf R}\\in C_{g}\\}|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{AMR-P}(C_{r},C_{g})=\\frac{1}{|C_{g}|}\\sum_{\\hat{\\mathbf{R}}\\in C_{g}}\\operatorname*{min}_{\\mathbf{R}\\in C_{r}}\\mathrm{RMSD}(\\hat{\\mathbf{R}},\\mathbf{R})}\\\\ {\\displaystyle\\mathrm{COV-P}(C_{r},C_{g})=\\frac{1}{|C_{g}|}|\\{\\hat{\\mathbf{R}}\\in C_{g}|\\mathrm{RMSD}(\\hat{\\mathbf{R}},\\mathbf{R})<\\delta,\\mathbf{R}\\in C_{r}\\}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A lower AMR score signifies improved accuracy, while a higher COV score reflects greater diversity in the generative model. Following (Jing et al., 2022), the threshold $\\delta$ is set to 0.5\u00c5 for GEOM-QM9 and $0.{\\dot{7}}5\\mathring\\mathrm{A}$ for GEOM-DRUGS. ", "page_idx": 15}, {"type": "text", "text": "A.4 Training Details and Hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For GEOM-DRUGS, we train ET-Flow for a fixed 250 epochs with a batch size of 64 and 5000 training batches per epoch per GPU on 8 A100 GPUs. For the learning rate, we use the Adam Optimizer with a cosine annealing learning rate which goes from a maximum of $10^{-3}$ to a minimum $10^{-7}$ over 250 epochs with a weight decay of $10^{-10}$ . For GEOM-QM9, we train ET-Flow for 200 epochs with a batch size of 128, and use all of the training dataset per epoch on 4 A100 GPUs. We use the cosine annealing learning rate schedule with maximum of $8\\cdot10^{-4}$ to minimum of $10^{-7}$ over 100 epochs, post which the maximum is reduced by a factor of 0.05. We select checkpoints based on the lowest validation error. ", "page_idx": 15}, {"type": "table", "img_path": "avsZ9OlR60/tmp/aaf525f4ae3fb025e48867738fb7a61db6ef3184e2ee3528054251ed603b7929.jpg", "table_caption": ["Table 6: Hyperparameters for ET-Flow "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Training and Sampling Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The following algorithms go over the pseudo-code for the training and sampling procedure. For each molecule, we use up to 30 conformations with the highest boltzmann weights as provided by CREST (Pracht et al., 2024) similar to that of (Jing et al., 2022) ", "page_idx": 16}, {"type": "text", "text": "Algorithm 1: Training procedure   \nInput: molecules $[G_{0},...,G_{N}]$ each with true conformers $[C_{G,1},...C_{G,K_{G}}]$ , the harmonic prior $\\rho_{0}$ , learning rate $\\alpha$ , number of epochs $N_{e}$ , initialized vector field $v_{\\theta}$   \nOutput: trained flow matching model $v_{\\theta}$   \nfor $i\\gets1$ to $N_{e}$ do for $G$ in $[G_{0},...,G_{N}]$ do Sample $t\\sim\\mathcal{U}[0,1]$ and $C_{1}\\in[C_{G,1},...C_{G,K_{G}}]$ ; Sample prior $C_{0}\\sim\\rho_{0}(G)$ ; Align $C_{0}\\gets\\mathsf{R M S D A l i g n}(C_{0},C_{1})$ ; Sample $C_{t}=t C_{1}+(1-t)C_{0}+\\sigma^{2}t(1-t)z,\\;\\;\\;\\;\\;z\\sim{\\mathcal N}(0,{\\bf I});$ Construct vector field $\\begin{array}{r}{u_{t}\\gets x_{1}-x_{0}+\\frac{1-2t}{2\\sqrt{t(1-t)}}z}\\end{array}$ Compute loss $\\leftarrow\\|v_{\\theta}(t,C_{t})-u_{t}\\|^{2}$ ; Take gradient step $\\theta\\leftarrow\\theta-\\alpha\\nabla_{\\theta}$ ;   \nAlgorithm 2: Inference procedure   \nInput: molecular graph $G$ , number conformers $K$ , number of sampling steps $N$   \nOutput: predicted conformers $\\left[C_{1},...C_{K}\\right]$   \nfor $C$ in $[C_{1},...C_{K}]$ do sample prior $\\hat{C}\\sim\\rho_{0}(G)$ ; for $n\\leftarrow0$ to $N-1$ do Set $t\\leftarrow\\frac{n}{N}$ ; Set $\\Delta t\\gets\\frac{1}{N}$ ; Predict $\\hat{v}=v_{\\theta}(t,\\hat{C})$ ; Update $\\hat{C}=\\hat{C}+\\hat{v}\\Delta t$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Input: molecular graph $G$ , number conformers $K$ , number of sampling steps $N$ , stochasticity level churn, stochastic sampling range $[t_{m i n},t_{m a x}]$ ", "page_idx": 17}, {"type": "text", "text": "Output: predicted conformers $[{C_{1}},...{\\bar{C}}_{K}]$   \nfor $C$ in $[C_{1},...C_{K}]$ do sample prior $\\hat{C}\\sim\\rho_{0}(G)$ ; for $n\\leftarrow0$ to $N-1$ do SSeett $t\\leftarrow\\frac{n}{N}$ ; $\\begin{array}{r}{\\Delta t\\leftarrow\\frac{1}{N}}\\end{array}$ Set $\\gamma\\leftarrow{\\frac{c h u r n}{N}}$ ; if $t\\in[t_{m i n},t_{m a x}]$ then Sample $\\epsilon\\sim\\dot{N}(0,I)$ ; $\\Delta\\hat{t}\\gets\\gamma(1-t)$ ; $\\hat{t}\\gets m a x(t-\\Delta\\hat{t},0)$ ; $\\hat{C}\\gets\\hat{C}+\\Delta\\hat{t}\\sqrt{t^{2}-\\hat{t}^{2}}\\epsilon;$ Predict $\\hat{v}=v_{\\theta}(\\hat{t},\\hat{C})$ ; Set $\\Delta t\\gets\\Delta t+\\Delta\\hat{t}$ ; else Predict v\u02c6 = v\u03b8(t, C\u02c6); Update $\\hat{C}=\\hat{C}+\\hat{v}\\Delta t$ ", "page_idx": 17}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Designing SO(3) Equivariance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We show that we can modify the architecture in Section A.1 (Equation 18) to produce a final vector output that satisfies rotation equivariance and reflection asymmetry. Let $\\vec{v}_{1}$ and $\\scriptstyle{\\vec{v}}_{2}$ be linearly independent non-zero vectors $\\|\\vec{v}_{1}\\|>0,\\|\\vec{v}_{2}\\|>0$ , and $s$ be a scalar. We implement SO(3) equivariance by adding a vector with a cross product. We show that vector $\\vec{v}=\\vec{v}_{1}+s(\\vec{v}_{1}\\times\\vec{v}_{2})$ , where $\\vec{v}_{1}\\times\\vec{v}_{2}$ denotes cross product of $\\vec{v}_{1}$ and $\\vec{v}_{2}$ , satisfies anti-symmetry while maintaining rotation equivariance as follows, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{R\\vec{v}_{1}+s(R\\vec{v}_{1}\\times R\\vec{v}_{2})=R(\\vec{v}_{1})+s R(\\vec{v}_{1}\\times\\vec{v}_{2})}\\\\ &{}&{\\qquad=R(\\vec{v}_{1}+s(\\vec{v}_{1}\\times\\vec{v}_{2}))}\\\\ &{}&{-\\vec{v}_{1}+s(-\\vec{v}_{1}\\times-\\vec{v}_{2})=-\\vec{v}_{1}+s(\\vec{v}_{1}\\times\\vec{v}_{2})}\\\\ &{}&{\\qquad\\neq-(\\vec{v}_{1}+s(\\vec{v}_{1}\\times\\vec{v}_{2}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This concludes the proof for rotation equivariance and reflection anti-symmetry. ", "page_idx": 17}, {"type": "text", "text": "D Additional Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Design Choice Ablations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conduct a series of ablation studies to assess the influence of each component in the ET-Flow. Particularly, we re-run the experiments with (1) $O(3)$ equivariance without chirality correction, (2) Absence of Alignment, (3) Gaussian Prior as a base distribution. We demonstrate that improving probability paths and utilizing an expressive equivariant architecture with correct symmetries are key components for ET-Flow to achieve state of the art performance. The ablations were ran with reduced settings (50 epochs; $\\mathrm{lr}=1e-4$ ; 4 A100 gpus). Results are shown in Table D.1. ", "page_idx": 17}, {"type": "text", "text": "D.2 Results on GEOM-XL ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We now assess how well a model trained on GEOM-DRUGS generalises to unseen molecules with large numbers of atoms, using the GEOM-XL dataset containing a total of 102 molecules. This provides insights into the model\u2019s capacity to tackle larger molecules and out-of-distribution tasks. Upon executing the checkpoint provided by Torsional Diffusion, we encountered 27 failed cases for generation likely due to RDKit failures, similar to the observations in MCF albeit with slightly different exact numbers. In both experiments involving all 102 molecules and a subset of 75 molecules, ET-Flow achieves performance comparable to Torsional Diffusion and MCF-S, but falls short of matching the performance of MCF-B and MCF-L. It\u2019s worth noting that MCF-B and MCF-L are significantly larger models, potentially affording them an advantage in generalization tasks. As part of our future work, we plan to scale up our model and conduct further tests to explore its performance in this regard. ", "page_idx": 17}, {"type": "table", "img_path": "avsZ9OlR60/tmp/f9a7aac8b940fca42013b8a2e170b4e678f6c5a41ac9640626706c5d063e3355.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "avsZ9OlR60/tmp/57db65ded7c1030b2a3002a426d31069cd24633b6a7afdd95dc7e07300d1316e.jpg", "table_caption": ["Table 8: Generalization results on GEOM-XL. AMR-P \u2193 AMR-R \u2193 # mols "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "D.3 Additional Out-of-Distribution Results ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "avsZ9OlR60/tmp/4c79e4edbd5615637e72d3ede384f9e12975cf90b1cd756ebf990e23a07de685.jpg", "table_caption": ["Table 9: Additional OOD results. We use RS and SS to indicate Random Split and Scaffold Split respectively. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "To further evaluate the generalization performance of ET-Flow, we conduct two more out-of-distribution experiments in addition to GEOM-XL. First, we test the model on scaffold-based splits of the GEOM-QM9 and GEOM-DRUGS dataset, which offers a more challenging alternative to the standard random split. We split the datasets based on Murcko scaffolds of the molecules into an 80:10:10 ratio for train, validation, and test sets. We evaluate our method on 1000 randomly sampled molecules from the resulting test set. The second experiment involves training the model on GEOM-DRUGS and assessing its performance on GEOM-QM9, a dataset with significantly smaller molecules. This experiment complements the generalization task to larger molecules in ", "page_idx": 18}, {"type": "text", "text": "GEOM-XL by assessing ability for ET-Flow to generalize to smaller molecules. The results, presented in Table 9, indicate that the model\u2019s performance degrades only marginally on the scaffold-based split. Furthermore, the model demonstrates robust performance on GEOM-QM9 even when trained on GEOM-DRUGS. ", "page_idx": 19}, {"type": "text", "text": "E Visualizations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure 6 shows randomly selected examples of sampled conformers from ET-Flow for GEOM-DRUGS. The left column is the reference molecule from the ground truth, and the remaining columns are samples generated with 50 sampling steps. Figure 7 showcases the ability for ET-Flow to generate quality samples with fewer sampling steps. ", "page_idx": 19}, {"type": "table", "img_path": "avsZ9OlR60/tmp/cd389711dfa72fa8ca5a88e141cef02e0d902cdeaf87f433cdf5599ffd7dfe84.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "avsZ9OlR60/tmp/e8df1d20c33977057d1e3ab261e73d91cae14bbb343328406d34f68b01f033e1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 7: Examples of conformers of ground truth and ET-Flow for different number of sampling steps. ", "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The paper claims to give State-of-the-Art on the precision of the generated conformers while being competitive in recall to another State-of-the-Art model of similar size. Claims are supported by results. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Yes, the paper discussed the limitations in the Section 6. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All assumptions and proofs are stated as needed in the manuscript. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, the experimental detail can be found in Section 4 Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We provide links to our our code and data. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Data splits is detailed in Section 4, and the hyperparameters are detailed in Section A.4. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: We follow the same data splits as previous works (Ganea et al., 2021; Jing et al., 2022;   \nWang et al., 2024). ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Justification: We provide these details in Section A.4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We follow the the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of AI for Science. There are many potential societal consequences of our work, but none that we feel must be specifically acknowledged. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of AI for Science. There are many potential risks for misuse of our work, but none that we feel must be specifically safeguarded. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we cite all creators and original owners of work referenced in this manuscript. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We intend to open-source our code and data with proper documentation. These are currently under preparation, and will be ready with a later revision of the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects is done in our work. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No IRB Approvals or equivalent for research with human subjects are done in our work. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]