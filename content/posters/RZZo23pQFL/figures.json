[{"figure_path": "RZZo23pQFL/figures/figures_1_1.jpg", "caption": "Figure 1: A example of vanilla pixel-level classifiers, where the SeaFormer-L [45] is the baseline and the feature distribution is visualized with t-SNE. (a) is a test image of the ADE20K dataset, and (b) denotes the feature distributions in the semantic domain of (a), with purple and gray dots denoting the pixel features on the test image of the door and other categories, respectively. Blue star denotes the fixed prototype trained on training set of the door category. It shows that vanilla pixel-level classifiers directly interact pixel features with the fixed semantic prototypes, which leads to feature deviation in the semantic domain and information loss in the spatial domain problems. In contrast, SSA-Seg makes classification decisions based on adaptive semantic and spatial prototypes by prompting the prototypes to offset toward the center of the semantic domain and the spatial domain, as shown in (c) and (d). Visual comparison of the baseline and SSA-Seg can be found in Fig. 5.", "description": "This figure illustrates the limitations of vanilla pixel-level classifiers and how SSA-Seg addresses them.  (a) shows a test image. (b) visualizes feature distribution, highlighting the large intra-class variance and information loss.  (c) and (d) demonstrate how SSA-Seg adapts semantic and spatial prototypes for improved classification, moving them toward the center of semantic and spatial domains.  A visual comparison of results using the baseline and SSA-Seg is shown in Figure 5.", "section": "1 Introduction"}, {"figure_path": "RZZo23pQFL/figures/figures_3_1.jpg", "caption": "Figure 2: SSA-Seg overview. For the semantic features Sf output from the backbone and decode head, we first generate spatial features Pf by position encode. Then we retain the original 1 \u00d7 1 convolution to generate the coarse mask Mc. Guided by Mc, we generate the center of the semantic domain and spatial domain in the pre-classified representations and fused them with the fixed semantic prototypes S and the prototype position basis P to generate the semantic prototypes Sp and the spatial prototype Pp. Finally, we consider simultaneously semantic and spatial prototypes to perform classification decisions. The right figure shows an online teacher classifier only for training, where the coarse mask is replaced with ground-truth mask to participate in model training, and constrains the prototype adaption and transfer accurate semantic and spatial knowledge to the primary classifier based on multi-domain distillation learning.", "description": "This figure illustrates the overall architecture of SSA-Seg, showing how semantic and spatial information is integrated for improved semantic segmentation. The left side depicts the core SSA-Seg classifier, which uses a coarse mask to guide the adaptation of semantic and spatial prototypes. The right side details the online multi-domain distillation process that refines these prototypes using ground truth information.  The figure uses various visual elements such as arrows, boxes, and mathematical symbols to clearly represent the flow of information and the different processes involved.", "section": "3.2 Semantic and Spatial Adaptive Classifier"}, {"figure_path": "RZZo23pQFL/figures/figures_5_1.jpg", "caption": "Figure 3: Visualization of the inter-class relation matrix for the semantic prototypes Sp and Sp, and the latter possesses better inter-class separability. This motivates us to add semantic domain distillation loss to constrain the adaption of the semantic prototypes. The results show that after semantic domain distillation, the semantic prototypes have better separability, which facilitates category recognition.", "description": "This figure visualizes the inter-class relation matrices for semantic prototypes with and without semantic domain distillation.  The matrices show the relationships between different semantic prototypes.  The results demonstrate that after applying semantic domain distillation, the prototypes exhibit better separability, which improves category recognition.", "section": "3.2.1 Semantic Prototype Adaptation"}, {"figure_path": "RZZo23pQFL/figures/figures_9_1.jpg", "caption": "Figure 4: mIoU of the validation set on (a) ADE20K and (b) COCO-Stuff-10K with iterations.", "description": "The figure shows two line graphs, one for ADE20K and one for COCO-Stuff-10K, illustrating the validation set mIoU (mean Intersection over Union) over the number of training iterations.  Each graph compares the performance of the baseline SeaFormer model (blue line) with the performance of the proposed SSA-Seg model (orange line).  The results demonstrate that SSA-Seg achieves a higher mIoU and reaches its peak performance with significantly fewer training iterations.", "section": "4.1 Main Results"}, {"figure_path": "RZZo23pQFL/figures/figures_9_2.jpg", "caption": "Figure 5: Comparison of SSA-Seg and Baseline (SeaFormer-L) results. Purple and gray indicate pixel features in the door and other categories, respectively. Orange star indicates the initial fixed prototype of wall category, and red star indicates the adapted semantic prototype.", "description": "This figure compares the results of SSA-Seg and the baseline model (SeaFormer-L) on a specific image from the ADE20K dataset. The visualization uses t-SNE to show the distribution of pixel features in the feature space. Purple and gray dots represent features from the door and other categories, respectively. The orange star marks the original fixed prototype for the \"wall\" category, while the red star represents the adapted prototype generated by SSA-Seg. The figure demonstrates how SSA-Seg adjusts the fixed prototypes toward the center of the semantic and spatial domains, leading to improved segmentation accuracy.", "section": "Visual comparison of SSA-Seg and Baseline results"}, {"figure_path": "RZZo23pQFL/figures/figures_9_3.jpg", "caption": "Figure 6: Examples of extreme offsets without distillation.", "description": "This figure demonstrates a scenario where, without distillation, the offset of the prototype is uncontrollable and in extreme cases moves away from the semantic features of the corresponding image, leading to more pixels being misclassified.  The image shows how the prototype offsets without distillation (red star) versus with distillation (purple star).  The t-SNE plots show the improved clustering and separation of features when distillation is applied.", "section": "Visual comparison of SSA-Seg and Baseline results"}, {"figure_path": "RZZo23pQFL/figures/figures_15_1.jpg", "caption": "Figure 7: t-SNE of some example images, which are randomly selected from the ADE20K dataset. The first row represents the distribution of pixel features in the door class, and the second row represents table class. it can be observed that due to the complex scenarios and varying object distributions, pixel features of the same class tend to exhibit larger intra-class variance when the trained model on the training set is applied to the test set.", "description": "This figure uses t-SNE to visualize the pixel feature distributions of \"door\" and \"table\" classes from the ADE20K dataset.  It demonstrates that even within the same class, pixel features exhibit high intra-class variance due to diverse imaging conditions and object appearances.  This variance highlights a key challenge addressed by the proposed SSA-Seg method which adapts to variations in the test image.", "section": "D Extra analysis"}, {"figure_path": "RZZo23pQFL/figures/figures_16_1.jpg", "caption": "Figure 8: Visualization of segmentation predictions and class activation maps [41] for features output from the backbone on the ADE20K dataset. SeaFormer-L is the baseline.", "description": "This figure visualizes the segmentation results and class activation maps (CAMs) for different methods on the ADE20K dataset.  It compares the ground truth (GT) segmentations with those produced by SeaFormer, CAC, and SSA-Seg. The CAMs, generated using Grad-CAM [41], highlight the regions of the feature maps that are most important for making classification decisions. The figure shows that SSA-Seg produces more accurate and complete segmentations, particularly in resolving ambiguous boundaries and handling complex scenes, compared to SeaFormer and CAC.  The CAM visualization further supports this conclusion, demonstrating that SSA-Seg focuses its attention on more relevant features leading to more accurate results.", "section": "C More qualitative visualization"}]