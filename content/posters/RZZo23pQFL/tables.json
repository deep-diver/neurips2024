[{"figure_path": "RZZo23pQFL/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison of SSA-Seg on state-of-the-art general (top) and light weight (bottom) methods. The number of FLOPs (G) is calculated on the input size of 512 \u00d7 512 for ADE20K and COCO-Stuff-10K, and 480 x 480 for PASCAL-Context. The latency (ms) is calculated on the input size of 512 \u00d7 512 on V100 GPU. The green number indicates the increase from the baseline.", "description": "This table presents a comparison of the performance of SSA-Seg against other state-of-the-art models for semantic segmentation on three benchmark datasets (ADE20K, COCO-Stuff-10K, and PASCAL-Context).  It shows the FLOPs (floating point operations), latency (in milliseconds), number of parameters, and mean Intersection over Union (mIoU) for each method. The table is divided into two parts: general methods and lightweight methods. For each method, both baseline results and results after applying SSA-Seg are provided for comparison. The improvements achieved by SSA-Seg are highlighted in green.", "section": "4.1 Main Results"}, {"figure_path": "RZZo23pQFL/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparison of SSA-Seg on state-of-the-art general (top) and light weight (bottom) methods. The number of FLOPs (G) is calculated on the input size of 512 \u00d7 512 for ADE20K and COCO-Stuff-10K, and 480 x 480 for PASCAL-Context. The latency (ms) is calculated on the input size of 512 \u00d7 512 on V100 GPU. The green number indicates the increase from the baseline.", "description": "This table presents a comparison of the performance of SSA-Seg against other state-of-the-art semantic segmentation methods.  It shows the mIoU (mean Intersection over Union), FLOPs (floating-point operations), and latency for various backbones on three benchmark datasets (ADE20K, COCO-Stuff-10K, and PASCAL-Context).  The results demonstrate SSA-Seg's improvements in mIoU with minimal increases in computational cost.  The table is separated into general and lightweight methods sections for easier comparison.", "section": "4 Main Results"}, {"figure_path": "RZZo23pQFL/tables/tables_7_2.jpg", "caption": "Table 3: Comparison with state-of-the-art mask classification models. FLOPS (G) are calculated using the input size of 512 \u00d7 512. The latency (ms) is measured on a single V100 GPU with input size 512x512.", "description": "This table compares the performance of SSA-Seg integrated with different mask classification methods on three benchmark datasets (ADE20K, COCO-Stuff-10K, and PASCAL-Context).  It shows the number of parameters, FLOPs, latency, and mIoU achieved by each method.  The results highlight SSA-Seg's ability to improve the performance of existing mask-based methods.", "section": "4.1 Main Results"}, {"figure_path": "RZZo23pQFL/tables/tables_7_3.jpg", "caption": "Table 4: Ablation of the position basis.", "description": "This table presents the ablation study of the position basis in the SSA-Seg model.  The baseline is SeaFormer-L.  It shows a comparison between the full SSA-Seg model and a version that only uses the randomly initialized position basis, demonstrating the impact of spatial prototype adaptation on model performance. The results highlight the importance of the spatial prototype adaptation component in achieving the best results.", "section": "4.2 Ablation Study"}, {"figure_path": "RZZo23pQFL/tables/tables_8_1.jpg", "caption": "Table 5: Ablation experiments of the response domain distillation loss Lrd.", "description": "This table presents the ablation study of the response domain distillation loss (Lrd) in the SSA-Seg model. It shows the effect of removing different components from the Lrd calculation on the final mIoU (mean Intersection over Union) performance. Specifically, it evaluates the impact of removing the entropy (H), boundary mask (B), and a term denoted by \u03b5 from the Lrd formula. The results demonstrate the importance of each component for the overall performance of the SSA-Seg model.", "section": "4.2 Ablation Study"}, {"figure_path": "RZZo23pQFL/tables/tables_8_2.jpg", "caption": "Table 6: Ablation experiments of the generation of Pc. SoftmaxHw denotes apply softmax operation on spatial dimension.", "description": "This table shows the ablation study on the generation of the spatial domain center Pc. It compares the performance of using only softmax on the channel dimension (SoftmaxK) versus using softmax on both the channel and spatial dimensions (SoftmaxHW). The results indicate that applying softmax on both dimensions leads to a significant improvement in mIoU, suggesting that considering the spatial distribution of features is crucial for accurate segmentation.", "section": "4.2 Ablation Study"}, {"figure_path": "RZZo23pQFL/tables/tables_8_3.jpg", "caption": "Table 7: Ablation experiments of position encoding (PE) methods on the ADE20K dataset.", "description": "This table presents the ablation study of different position encoding methods used in the SSA-Seg model on the ADE20K dataset. It compares the performance (mIoU) of three different position encoding techniques: Sinusoidal, Learnable, and CPVT.  The results show the impact of the choice of position encoding on the overall segmentation accuracy.", "section": "4.2 Ablation Study"}, {"figure_path": "RZZo23pQFL/tables/tables_8_4.jpg", "caption": "Table 8: Ablation experiment of SEPA and SPPA.", "description": "This table shows the ablation study of the proposed Semantic and Spatial Adaptive Classifier (SSA-Seg). It compares the performance of different components of SSA-Seg in terms of mIoU and FLOPs. The baseline uses a 1x1 convolution with softmax. Adding SEPA (Semantic Prototype Adaptation) and SPPA (Spatial Prototype Adaptation) improves mIoU, and the inclusion of the distillation losses (Lg, Lsd, and Lpd) further enhances the performance.", "section": "4.2 Ablation Study"}, {"figure_path": "RZZo23pQFL/tables/tables_8_5.jpg", "caption": "Table 1: Performance comparison of SSA-Seg on state-of-the-art general (top) and light weight (bottom) methods. The number of FLOPs (G) is calculated on the input size of 512 \u00d7 512 for ADE20K and COCO-Stuff-10K, and 480 x 480 for PASCAL-Context. The latency (ms) is calculated on the input size of 512 \u00d7 512 on V100 GPU. The green number indicates the increase from the baseline.", "description": "This table compares the performance of SSA-Seg against other state-of-the-art semantic segmentation models, both general and lightweight, across three benchmark datasets: ADE20K, COCO-Stuff-10K, and PASCAL-Context.  It presents metrics such as mIoU (mean Intersection over Union), FLOPs (floating point operations), latency (in milliseconds), and the number of parameters for each model. The table highlights the improvements achieved by incorporating SSA-Seg into various baseline models, demonstrating its effectiveness in enhancing segmentation performance without significant computational overhead.", "section": "4 Main Results"}, {"figure_path": "RZZo23pQFL/tables/tables_8_6.jpg", "caption": "Table 1: Performance comparison of SSA-Seg on state-of-the-art general (top) and light weight (bottom) methods. The number of FLOPs (G) is calculated on the input size of 512 \u00d7 512 for ADE20K and COCO-Stuff-10K, and 480 x 480 for PASCAL-Context. The latency (ms) is calculated on the input size of 512 \u00d7 512 on V100 GPU. The green number indicates the increase from the baseline.", "description": "This table compares the performance of SSA-Seg against several state-of-the-art semantic segmentation models, both general and lightweight, across three benchmark datasets: ADE20k, COCO-Stuff-10K, and PASCAL-Context.  It shows the model's mIoU (mean Intersection over Union), FLOPs (floating point operations), latency (in milliseconds), and the number of parameters. The comparison highlights SSA-Seg's ability to improve baseline models' performance while maintaining efficiency. The increase in FLOPs and latency introduced by SSA-Seg is minimal compared to the improvements achieved in mIoU. ", "section": "4 Main Results"}, {"figure_path": "RZZo23pQFL/tables/tables_14_1.jpg", "caption": "Table 1: Performance comparison of SSA-Seg on state-of-the-art general (top) and light weight (bottom) methods. The number of FLOPs (G) is calculated on the input size of 512 \u00d7 512 for ADE20K and COCO-Stuff-10K, and 480 x 480 for PASCAL-Context. The latency (ms) is calculated on the input size of 512 \u00d7 512 on V100 GPU. The green number indicates the increase from the baseline.", "description": "This table compares the performance of SSA-Seg against other state-of-the-art semantic segmentation models, both general and lightweight, across three benchmark datasets (ADE20K, COCO-Stuff-10K, and PASCAL-Context).  Metrics include mIoU (mean Intersection over Union), FLOPs (floating point operations), and latency (inference time).  The table highlights SSA-Seg's improvements in mIoU while showing minimal increases in computational cost.  Green numbers denote the performance gain from applying SSA-Seg to a baseline model.", "section": "4 Main Results"}, {"figure_path": "RZZo23pQFL/tables/tables_15_1.jpg", "caption": "Table 1: Performance comparison of SSA-Seg on state-of-the-art general (top) and light weight (bottom) methods. The number of FLOPs (G) is calculated on the input size of 512 \u00d7 512 for ADE20K and COCO-Stuff-10K, and 480 x 480 for PASCAL-Context. The latency (ms) is calculated on the input size of 512 \u00d7 512 on V100 GPU. The green number indicates the increase from the baseline.", "description": "This table compares the performance of SSA-Seg against other state-of-the-art models on three common semantic segmentation datasets (ADE20K, COCO-Stuff-10K, and PASCAL-Context).  It shows the model's mIoU (mean Intersection over Union), FLOPs (floating point operations), parameters, and latency. The comparison is broken down into general and lightweight models, highlighting the efficiency gains achieved by SSA-Seg.  Green numbers indicate performance improvements over the baseline models after applying SSA-Seg.", "section": "4.1 Main Results"}, {"figure_path": "RZZo23pQFL/tables/tables_15_2.jpg", "caption": "Table 1: Performance comparison of SSA-Seg on state-of-the-art general (top) and light weight (bottom) methods. The number of FLOPs (G) is calculated on the input size of 512 \u00d7 512 for ADE20K and COCO-Stuff-10K, and 480 x 480 for PASCAL-Context. The latency (ms) is calculated on the input size of 512 \u00d7 512 on V100 GPU. The green number indicates the increase from the baseline.", "description": "This table compares the performance of SSA-Seg against several state-of-the-art semantic segmentation models, both general and lightweight.  It shows the model's performance in terms of mean Intersection over Union (mIoU), FLOPS (floating point operations per second), and latency (in milliseconds) on three benchmark datasets: ADE20K, COCO-Stuff-10K, and PASCAL-Context.  The increase in performance achieved by adding SSA-Seg to existing models is highlighted.", "section": "4.1 Main Results"}]