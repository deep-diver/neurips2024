{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces Llama, a foundational large language model used extensively in the target paper's experiments and comparisons, making it highly relevant to the research."}, {"fullname_first_author": "Rohan Taori", "paper_title": "Stanford alpaca: An instruction-following llama model", "publication_date": "2023-03-30", "reason": "The Stanford Alpaca dataset, a key dataset in instruction tuning, is directly used and analyzed in this research, forming a crucial basis for the paper's contributions."}, {"fullname_first_author": "Jason Wei", "paper_title": "Finetuned language models are zero-shot learners", "publication_date": "2022-04-25", "reason": "This paper is foundational to the field of instruction tuning and zero-shot learning which underpins the core methodology of the target paper."}, {"fullname_first_author": "Chunting Zhou", "paper_title": "LIMA: Less is more for alignment", "publication_date": "2023-11-28", "reason": "This paper presents LIMA, a dataset emphasizing data quality over quantity in instruction tuning, and is directly relevant to the core argument and methodology of the current paper."}, {"fullname_first_author": "Lichang Chen", "paper_title": "Alpagasus: Training a better alpaca with fewer data", "publication_date": "2024-01-01", "reason": "The AlpaGasus method, a competing approach to data selection, serves as a direct comparison and benchmark for the proposed SelectIT method, making it a highly relevant and important reference."}]}