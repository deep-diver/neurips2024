[{"figure_path": "QNieOPt4fg/tables/tables_5_1.jpg", "caption": "Table 1: Overall results on IT. \u201cCodeX\u201d and \u201cAE\u201d mean HumanEval and AlpacaEval benchmarks. All the scores are averages of three independent runs with different random seeds.", "description": "This table presents the overall performance comparison of different instruction tuning methods on two base LLMs (LLaMA-2-7B and LLaMA-2-13B).  It contrasts the performance of Alpaca-GPT4, LIMA, AlpaGasus, Q2Q, Instruction Mining, and three individual components of the proposed SelectIT method (Token-R, Sentence-R, Model-R) against the full SelectIT approach.  The results are shown for various downstream tasks (MMLU, BBH, GSM, TyDiQA, CodeX, AE) and an overall average, offering a comprehensive evaluation of each method's effectiveness in improving LLM abilities.  All scores represent the average of three independent runs with varying random seeds.", "section": "4.2 Main Results"}, {"figure_path": "QNieOPt4fg/tables/tables_6_1.jpg", "caption": "Table 2: Effect of different K.", "description": "This table shows the overall performance of LLMs on the open-instruct benchmark with different values of K (number of rating prompts).  The results are averages across three independent runs with different random seeds. The table helps determine the optimal value of K by showing how variations in the parameter impact the performance of LLMs.", "section": "4 Experiments"}, {"figure_path": "QNieOPt4fg/tables/tables_6_2.jpg", "caption": "Table 3: Effect of different \u03b1.", "description": "This table presents the results of experiments conducted to determine the optimal value of the uncertainty factor (\u03b1) used in the SelectIT method.  Different values of \u03b1 were tested (0.2, 0.4, 0.6, 0.8), and the resulting performance of LLMs on various benchmarks (MMLU, BBH, GSM, Tydiqa, CodeX, AE) is shown. The average performance across all benchmarks is also reported. This allows for the evaluation of how \u03b1 influences the balance between the mean and standard deviation of scores from Token-R, thereby affecting the overall quality assessment of IT data. The results help in determining the optimal value for \u03b1 which gives the best balance for high-quality IT data selection.", "section": "4.2 Main Results"}, {"figure_path": "QNieOPt4fg/tables/tables_6_3.jpg", "caption": "Table 4: The relationship between the SelectIT and the individual selection strategy. Sentence-R plays the most significant impact on the final rating of the IT data. IDs 6, 7, and 8 correspond to the system of the same IDs in Table 1.", "description": "This table shows the contribution of each submodule (Token-R, Sentence-R, Model-R) of SelectIT to the final data selection.  It highlights the percentage of samples uniquely selected by each method and the overall percentage of samples selected by each method in the final set of high-quality data from Selective Alpaca.  Sentence-R is clearly the most influential component.", "section": "5 Analysis"}, {"figure_path": "QNieOPt4fg/tables/tables_6_4.jpg", "caption": "Table 1: Overall results on IT. \u201cCodeX\u201d and \u201cAE\u201d mean HumanEval and AlpacaEval benchmarks. All the scores are averages of three independent runs with different random seeds.", "description": "This table presents the overall performance comparison of different instruction tuning methods on two LLaMA-2 base models (7B and 13B parameters).  It compares the performance using the original Alpaca dataset with several high-quality data selection methods (AlpaGasus, Q2Q, Instruction Mining) and the proposed SelectIT method (using individual components Token-R, Sentence-R, Model-R and all three combined). The evaluation is performed across multiple benchmarks assessing various LLM capabilities:  MMLU (factual knowledge), BBH and GSM (reasoning), TyDiQA (multilingual), HumanEval (CodeX - coding), and AlpacaEval (AE - open-ended generation).  The results show the average score across three independent runs and highlight the improvement achieved by SelectIT compared to baselines and individual components.", "section": "4.2 Main Results"}, {"figure_path": "QNieOPt4fg/tables/tables_7_1.jpg", "caption": "Table 1: Overall results on IT. \u201cCodeX\u201d and \u201cAE\u201d mean HumanEval and AlpacaEval benchmarks. All the scores are averages of three independent runs with different random seeds.", "description": "This table presents the overall performance of different models and methods on various instruction tuning benchmarks. It compares the performance of the base Llama-2 models (7B and 13B parameters) with various instruction tuning methods, including Alpaca-GPT4, LIMA, AlpaGasus, Q2Q, Instruction Mining, and the proposed SelectIT method, across different evaluation metrics such as MMLU, BBH, GSM, TydiQA, CodeX (HumanEval), and AE (AlpacaEval). The table showcases the average scores across three independent runs, demonstrating the effectiveness of each method in improving the LLM's performance.", "section": "4.2 Main Results"}, {"figure_path": "QNieOPt4fg/tables/tables_7_2.jpg", "caption": "Table 7: Results of IT with various IT datasets.", "description": "This table presents the results of instruction tuning (IT) experiments conducted on various instruction tuning datasets. It compares the performance of LLMs fine-tuned with the original datasets (WizardLM and Orca-GPT4) against the performance of LLMs fine-tuned with datasets curated using the SelectIT method.  The table includes performance metrics across multiple benchmarks (MMLU, BBH, GSM, Tydiqa, Codex, and AE), the size of each dataset, and the overall average performance improvement achieved by SelectIT. The results show that SelectIT consistently improves performance across various datasets and benchmarks.", "section": "5.2 Robustness across Models, Datasets and Domains"}, {"figure_path": "QNieOPt4fg/tables/tables_8_1.jpg", "caption": "Table 8: The overall results on MT LLMs.", "description": "This table compares the performance of various machine translation (MT) large language models (LLMs) on the task of translating between four language pairs: English to German, German to English, English to Chinese, and Chinese to English.  The models are evaluated using two metrics: COMET and BLEU scores.  The table includes state-of-the-art (SOTA) models as baselines and several existing methods.  The key focus is on showing how the ALMA model, when enhanced with the SelectIT method for data selection, outperforms other models, especially in the COMET scores.", "section": "5.2 Robustness across Models, Datasets and Domains"}, {"figure_path": "QNieOPt4fg/tables/tables_8_2.jpg", "caption": "Table 1: Overall results on IT. \u201cCodeX\u201d and \u201cAE\u201d mean HumanEval and AlpacaEval benchmarks. All the scores are averages of three independent runs with different random seeds.", "description": "This table presents the overall performance of different instruction tuning methods on various downstream tasks.  It compares the baseline performance of LLaMA-2 models (7B and 13B parameters) with instruction tuning using Alpaca-GPT4, LIMA, AlpaGasus, Q2Q, Instruction Mining, and the proposed SelectIT method.  The evaluation metrics include MMLU, BBH, GSM, TyDiQA, HumanEval (CodeX), and AlpacaEval (AE), which cover factual knowledge, reasoning, multilinguality, and code generation. The table shows that SelectIT outperforms existing methods, yielding significant improvements across all benchmarks.  Results are averages from three independent runs for robustness.", "section": "4.2 Main Results"}, {"figure_path": "QNieOPt4fg/tables/tables_8_3.jpg", "caption": "Table 10: Comparasion with variants.", "description": "This table compares the performance of LLMs (LLaMA-2 and ALMA) trained with different data selection strategies.  The \"Full Dataset\" row shows the baseline performance using the complete Alpaca dataset. The other rows show the performance when using only a subset of the data selected by different methods: randomly selecting 20% of the full dataset, randomly selecting 20% of the *unselected* data (i.e., the data not selected by SelectIT), selecting 20% of the data based on sample length, and finally, using the data selected by SelectIT. The \u0394 column shows the improvement in performance compared to the baseline (Full Dataset).  The results indicate that SelectIT significantly outperforms the other methods.", "section": "5 Analysis"}, {"figure_path": "QNieOPt4fg/tables/tables_15_1.jpg", "caption": "Table 1: Overall results on IT. \u201cCodeX\u201d and \u201cAE\u201d mean HumanEval and AlpacaEval benchmarks. All the scores are averages of three independent runs with different random seeds.", "description": "This table presents the overall performance comparison of different instruction tuning methods on various benchmarks including MMLU, BBH, GSM, TyDiQA, CodeX, and AE.  It shows the average scores across three independent runs using different random seeds, allowing for a statistically robust evaluation of each method.  The table distinguishes between baseline models (LLaMA-2 7B and 13B), existing methods (Alpaca-GPT4, LIMA, AlpaGasus, Q2Q, Instruction Mining), and the proposed SelectIT method, with and without its individual components (Token-R, Sentence-R, Model-R). The results highlight SelectIT's superior performance compared to existing methods.", "section": "4.2 Main Results"}, {"figure_path": "QNieOPt4fg/tables/tables_17_1.jpg", "caption": "Table 1: Overall results on IT. \u201cCodeX\u201d and \u201cAE\u201d mean HumanEval and AlpacaEval benchmarks. All the scores are averages of three independent runs with different random seeds.", "description": "This table presents the overall performance of different instruction tuning methods on various downstream tasks.  It compares the baseline LLaMA-2 models (7B and 13B parameters) with several existing instruction tuning data selection methods (Alpaca-GPT4, LIMA, AlpaGasus, Q2Q, Instruction Mining) and the proposed SelectIT method, both individually (Token-R, Sentence-R, Model-R) and combined.  The evaluation metrics include performance on MMLU, BBH, GSM, TyDiQA, HumanEval (CodeX), and AlpacaEval (AE) benchmarks.  The scores represent the average of three independent runs, using different random seeds, highlighting the robustness of the findings.  SelectIT demonstrates consistent improvement across all benchmarks, particularly in the combined model.", "section": "4.2 Main Results"}, {"figure_path": "QNieOPt4fg/tables/tables_17_2.jpg", "caption": "Table 1: Overall results on IT. \u201cCodeX\u201d and \u201cAE\u201d mean HumanEval and AlpacaEval benchmarks. All the scores are averages of three independent runs with different random seeds.", "description": "This table presents the overall results of instruction tuning (IT) experiments using different methods and baselines on various benchmarks.  The benchmarks cover different aspects of LLM abilities, including factual knowledge (MMLU), reasoning (BBH and GSM), multilingual capabilities (TyDiQA), and coding proficiency (CodeX). Alpaca-GPT4, LIMA, AlpaGasus, Q2Q and Instruction mining are also shown for comparison.  The results are the average scores across three runs, demonstrating the performance improvement achieved by the proposed SelectIT method on LLAMA-2 (7B and 13B).", "section": "4.2 Main Results"}, {"figure_path": "QNieOPt4fg/tables/tables_17_3.jpg", "caption": "Table 11: Overall results on machine translation LLMs. \u201c\u2020\u201d the improvement is significant by contrast to the ALMA model (p < 0.05).", "description": "This table presents the overall results of different machine translation models on various metrics such as COMET and BLEU scores.  It compares the performance of several state-of-the-art models against ALMA, both with and without SelectIT.  The table highlights the significant improvement achieved by ALMA when using the SelectIT method for data selection, particularly for the English-to-other language translation tasks.", "section": "A.1 Applying SelectIT on Machine Translation LLMs"}, {"figure_path": "QNieOPt4fg/tables/tables_17_4.jpg", "caption": "Table 1: Overall results on IT. \u201cCodeX\u201d and \u201cAE\u201d mean HumanEval and AlpacaEval benchmarks. All the scores are averages of three independent runs with different random seeds.", "description": "This table presents the overall results of instruction tuning (IT) experiments using different methods on two base models, LLaMA-2-7B and LLaMA-2-13B.  It compares the performance of several methods for selecting high-quality instruction data, including existing methods like Alpaca-GPT4, LIMA, AlpaGasus, Q2Q, and Instruction Mining, and the proposed SelectIT method.  The evaluation is conducted across multiple benchmarks: MMLU (Massive Multitask Language Understanding), BBH (Big-Bench-Hard), GSM (Grade School Math), TyDiQA (multilingual question answering), CodeX (HumanEval coding), and AE (AlpacaEval).  The table shows the average score across three independent runs for each method and benchmark, allowing for a comparison of the effectiveness of different data selection techniques in improving the performance of LLMs on various tasks.", "section": "4.2 Main Results"}, {"figure_path": "QNieOPt4fg/tables/tables_17_5.jpg", "caption": "Table 1: Overall results on IT. \u201cCodeX\u201d and \u201cAE\u201d mean HumanEval and AlpacaEval benchmarks. All the scores are averages of three independent runs with different random seeds.", "description": "This table presents the overall performance comparison of different instruction tuning methods on various benchmark tasks using LLAMA-2-7B and LLAMA-2-13B as base models.  It compares the baseline performance of Alpaca-GPT4, LIMA, and other existing data selection methods with the proposed SelectIT approach. The results are broken down by benchmark (MMLU, BBH, GSM, TydiQA, CodeX, AE), showing the average scores across three independent runs for each method. This allows for a comprehensive comparison of the effectiveness of the different approaches to instruction tuning.", "section": "4.2 Main Results"}, {"figure_path": "QNieOPt4fg/tables/tables_17_6.jpg", "caption": "Table 1: Overall results on IT. \u201cCodeX\u201d and \u201cAE\u201d mean HumanEval and AlpacaEval benchmarks. All the scores are averages of three independent runs with different random seeds.", "description": "This table presents the overall results of instruction tuning (IT) experiments on various LLMs using different datasets and methods. It compares the performance of LLMs on multiple benchmark tasks, including factual knowledge (MMLU), reasoning (BBH, GSM), multilingual question answering (TyDiQA), coding (CodeX), and open-ended generation (AlpacaEval). The table shows the average performance scores across three independent runs, with various data selection methods compared against baselines.", "section": "4.2 Main Results"}]