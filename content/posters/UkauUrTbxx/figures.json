[{"figure_path": "UkauUrTbxx/figures/figures_1_1.jpg", "caption": "Figure 1: Various attack mechanisms on language models. Classic text attacks modify the input content using typos or synonyms; Prompt attacks perturb the prompt template within the input; and Jailbreaks append adversarial, non-semantic suffixes to manipulate the model into producing malicious outputs.", "description": "This figure showcases three main types of attacks against language models: classic text attacks, prompt attacks, and jailbreaks.  Classic text attacks involve subtle modifications to the input text (e.g., typos or synonyms) to change the model's prediction. Prompt attacks manipulate the prompt itself to elicit a different response.  Jailbreaks append seemingly innocuous phrases to prompts that trick the model into generating unsafe or unexpected outputs.", "section": "1 Introduction"}, {"figure_path": "UkauUrTbxx/figures/figures_1_2.jpg", "caption": "Figure 2: Overview of ProTransformer. ProAttention can be plugged into pretrained transformers without additional training. The ProTransformer is versatile and can be applied across various domains, including language, image, and graph.", "description": "This figure illustrates the architecture of ProTransformer, highlighting its plug-and-play nature.  ProAttention, a novel robust attention mechanism, is integrated into existing transformer models (BERT, ViT, GAT, LLaMA, GPT, etc.) without requiring additional training or fine-tuning.  The diagram shows how ProAttention improves upon vanilla attention by incorporating a robust token estimation process, enhancing the resilience of transformers to various attacks across multiple data domains (text, image, graph). The iterative nature of ProAttention is shown with the 'x K' indicating multiple iterations of the process. ", "section": "3 ProTransformer"}, {"figure_path": "UkauUrTbxx/figures/figures_3_1.jpg", "caption": "Figure 3: Different p(z).", "description": "This figure shows different penalty functions used in robust weighted least squares token estimators. The x-axis represents the residual, while the y-axis represents the penalty. The figure illustrates the quadratic loss (l2), the absolute loss (l1), the Huber loss, the Minimax Concave Penalty (MCP), and the Huber-MCP loss.  Each function is plotted to show how it handles outliers (large residuals). The Huber, MCP and Huber-MCP are robust loss functions.", "section": "3 ProTransformer"}, {"figure_path": "UkauUrTbxx/figures/figures_7_1.jpg", "caption": "Figure 4: Ablation studies.", "description": "Four ablation studies are presented to support the claims of the paper. (a) Convergence: The convergence speed of the proposed Newton-IRLS algorithm is compared to the first-order method. (b) Adversarial Training: The training curves of adversarial fine-tuning under TextFooler attack are presented. (c) Attack Constraints: The performance of the proposed algorithm under various attack constraints (maximum perturbation percentage) is presented. (d) Different Penalties: The performance of the proposed algorithm under different penalties (l2, l1, Huber, MCP) is presented.", "section": "4.2.2 Ablation Study"}, {"figure_path": "UkauUrTbxx/figures/figures_8_1.jpg", "caption": "Figure 6: Attack success rates (ASRs) under transfer jailbreak.", "description": "This figure shows the attack success rates (ASRs) under transfer jailbreak attacks.  The heatmap displays ASRs for the Vicuna model and its Pro-Vicuna (Huber) variant, across various values of the smoothing parameter (q) and the Huber loss parameter (\u03b4).  The results demonstrate the effectiveness of the Pro-Vicuna model in reducing ASRs, particularly at lower smoothing values. The data highlights that the effectiveness of the Pro-Vicuna model is comparatively good even without random smoothing (q=0).", "section": "4.3 Adversarial Prompting Attacks on LLMs"}, {"figure_path": "UkauUrTbxx/figures/figures_25_1.jpg", "caption": "Figure 7: Loss Curve of Algorithms", "description": "The figure shows the loss curves of the proposed Newton-IRLS algorithm with different penalty functions (l1, MCP, and Huber). The x-axis represents the number of layers, and the y-axis represents the loss value. The plots demonstrate that the algorithm converges quickly to the optimal solution within a few iterations, regardless of the penalty function used.", "section": "4.2.2 Ablation Study"}, {"figure_path": "UkauUrTbxx/figures/figures_25_2.jpg", "caption": "Figure 8: Optimization trajectory.", "description": "This figure visualizes the trajectories of the updated vectors in a 2D plane during the optimization process.  It uses L1 penalty and shows how the updated vectors (Trajectory-1, Trajectory-2, Trajectory-3) converge towards their respective ground truth values (Ground-Truth-1, Ground-Truth-2, Ground-Truth-3) within just three steps. This demonstrates the efficient and effective convergence of the Newton-IRLS algorithm.", "section": "F.1 Convergence Guarantee"}, {"figure_path": "UkauUrTbxx/figures/figures_26_1.jpg", "caption": "Figure 9: Different estimators in simulations.", "description": "The figure visualizes different estimators' performance in the presence of outliers.  It uses synthetic data with varying percentages of outliers (15%, 30%, 45%).  The plot compares the performance of the least squares estimator (l2), the least absolute deviations estimator (l1), and the minimax concave penalty estimator (MCP) against the ground truth mean. The results demonstrate the robustness of l1 and MCP, particularly MCP, against outliers, showcasing their ability to accurately estimate the mean even with a significant number of contaminating data points.", "section": "F.2 Robust Estimation"}, {"figure_path": "UkauUrTbxx/figures/figures_28_1.jpg", "caption": "Figure 4: Ablation studies.", "description": "This figure presents four ablation studies to demonstrate the effectiveness of the proposed ProTransformer.  (a) Convergence:  Shows the convergence behavior of Newton-IRLS, comparing it to a first-order method. (b) Adversarial Training: Illustrates the effect of adversarial training on model robustness, showcasing how Pro-BERT improves upon standard BERT with TextFooler attacks. (c) Attack Constraints: Demonstrates the impact of various attack constraints (maximum perturbation, minimum cosine similarity, sentence similarity threshold) on model performance for the TextFooler attack. (d) Different Penalties: Compares the performance using different robust penalties (L1, Huber, MCP, Huber-MCP) for the ProTransformer model, highlighting the superior robustness of the MCP penalty.", "section": "4.2.2 Ablation Study"}, {"figure_path": "UkauUrTbxx/figures/figures_29_1.jpg", "caption": "Figure 11: Ablation studies on attack constraints.", "description": "This figure presents ablation studies on three attack constraints: sentence similarity threshold, maximum perturbation percentage, and synonym cosine similarity.  The studies were performed on the AGNEWS dataset using the TextFooler attack with the ALBERT model as the backbone. The results demonstrate the consistent improvement of the proposed method (ALBERT+MCP) over the baseline model (ALBERT) across various settings of the constraints.", "section": "G.4 Ablation Study on Attack constraints"}, {"figure_path": "UkauUrTbxx/figures/figures_30_1.jpg", "caption": "Figure 2: Overview of ProTransformer. ProAttention can be plugged into pretrained transformers without additional training. The ProTransformer is versatile and can be applied across various domains, including language, image, and graph.", "description": "This figure illustrates the ProTransformer architecture, which involves integrating the ProAttention module into various pre-trained transformer models. It highlights the plug-and-play nature of ProAttention, its adaptability to different domains (language, image, graph), and its ability to enhance the robustness of transformer models without requiring additional training or fine-tuning.", "section": "3 ProTransformer"}, {"figure_path": "UkauUrTbxx/figures/figures_31_1.jpg", "caption": "Figure 4: Ablation studies.", "description": "This figure presents several ablation studies conducted to validate the effectiveness of the proposed method.  Subfigure (a) shows the convergence analysis of the Newton-IRLS algorithm compared to the standard IRLS method. Subfigure (b) compares the effectiveness of adversarial fine-tuning. Subfigure (c) shows how the performance of the model changes under different attack constraints. Finally, subfigure (d) compares the performance of the model with different penalty functions.", "section": "4.2.2 Ablation Study"}, {"figure_path": "UkauUrTbxx/figures/figures_32_1.jpg", "caption": "Figure 4: Ablation studies.", "description": "This figure presents four ablation studies. (a) shows the convergence comparison between the first-order method and the proposed Newton-IRLS algorithm, demonstrating the superior efficiency of the latter. (b) illustrates the impact of adversarial fine-tuning on model robustness. (c) analyzes the influence of different attack constraints on model performance, highlighting the effectiveness of the proposed method under various constraints. Finally, (d) compares the performance of the proposed method with different penalties (l1, Huber, MCP, Huber-MCP) on the AGNEWS dataset.", "section": "4.2.2 Ablation Study"}, {"figure_path": "UkauUrTbxx/figures/figures_33_1.jpg", "caption": "Figure 4: Ablation studies.", "description": "This figure presents the results of several ablation studies conducted to analyze the impact of different factors on the performance of the proposed ProTransformer. (a) Convergence: This plot illustrates the convergence behavior of the Newton-IRLS algorithm used in ProAttention, comparing it against a standard first-order method.  (b) Adversarial Training: This shows the effect of adversarial training on model robustness, comparing ProTransformer with standard BERT. (c) Attack Constraints: This demonstrates the influence of various attack parameters (like the maximum perturbation percentage) on the model's robustness. (d) Different Penalties: This illustrates the impact of different penalty functions (L1, Huber, MCP) within ProAttention on the model's robustness.", "section": "4.2.2 Ablation Study"}, {"figure_path": "UkauUrTbxx/figures/figures_34_1.jpg", "caption": "Figure 17: Accuracy (%) under prompt attack on SST2 (TextFooler, T5)", "description": "The figure shows the accuracy of different robust attention mechanisms under prompt attack using TextFooler on the SST2 dataset.  The x-axis represents the number of perturbed words in the prompt, and the y-axis represents the accuracy. The results show that the proposed ProTransformer with MCP loss consistently outperforms other methods, especially with a larger number of perturbed words, demonstrating its improved robustness against prompt attacks.", "section": "H.1 Experiments on T5"}, {"figure_path": "UkauUrTbxx/figures/figures_35_1.jpg", "caption": "Figure 17: Ablation study on Huber on T5", "description": "This ablation study explores the effect of different delta values (\u03b4) on the performance of the Huber-based Pro-T5 model against the TextFooler attack on the SST2 dataset. The x-axis represents the number of perturbed words, and the y-axis shows the accuracy.  The figure shows how different values of delta affect the robustness of the model against adversarial attacks with varying intensity. The optimal delta value is found to provide the best balance between robustness and clean accuracy.", "section": "H.1.2 Ablation on Huber"}, {"figure_path": "UkauUrTbxx/figures/figures_36_1.jpg", "caption": "Figure 4: Ablation studies.", "description": "This figure presents several ablation studies on the proposed ProTransformer. Specifically, it includes (a) Convergence: showing the loss descent curves for the Newton-IRLS algorithm, (b) Adversarial Training: visualizing the training curves of adversarial fine-tuning, (c) Attack Constraints: presenting the performance under different attack constraints such as maximum perturbation, minimum cosine similarity, and sentence similarity, and (d) Different Penalties: comparing the performance using different penalties (l2, l1, Huber, MCP). These results provide insights into various aspects of the ProTransformer and its robustness.", "section": "4.2.2 Ablation Study"}, {"figure_path": "UkauUrTbxx/figures/figures_37_1.jpg", "caption": "Figure 1: Various attack mechanisms on language models. Classic text attacks modify the input content using typos or synonyms; Prompt attacks perturb the prompt template within the input; and Jailbreaks append adversarial, non-semantic suffixes to manipulate the model into producing malicious outputs.", "description": "This figure illustrates three main types of adversarial attacks against language models: classic text attacks, prompt attacks, and jailbreaks. Classic text attacks involve subtle modifications to the input text, such as typos or synonym replacements, to cause misclassification. Prompt attacks focus on manipulating the prompt or instructions given to the model, leading to unintended or harmful outputs. Finally, jailbreaks use adversarial suffixes added to the input to elicit malicious behavior from the model, bypassing safety mechanisms.", "section": "Introduction"}, {"figure_path": "UkauUrTbxx/figures/figures_38_1.jpg", "caption": "Figure 20: LLaMA (Textfooler)", "description": "This figure shows the results of the TextFooler attack on the LLaMA model. Subfigures (a) through (d) present the main results and ablation studies on the hyperparameters of Huber and Huber-MCP loss functions.  The results demonstrate that Pro-LLaMA with the Huber-MCP loss function offers improved robustness against TextFooler attacks, especially when compared to other methods (l1, MCP, Huber) under various attack budgets.", "section": "H.2.2 TextBugger"}, {"figure_path": "UkauUrTbxx/figures/figures_39_1.jpg", "caption": "Figure 20: LLaMA (Textfooler)", "description": "This figure shows the results of the textual entailment task on the SST2 dataset under the TextFooler attack for the LLaMA model.  The left-hand plot shows the main results comparing the performance of the baseline LLaMA model against versions incorporating different robust penalties (l1, Huber, MCP, and Huber-MCP).  The right-hand plots show ablation studies for the Huber and Huber-MCP robust penalty functions, varying the parameters delta (\u03b4) and gamma (\u03b3), respectively, to analyze their impact on model accuracy under the TextFooler attack.", "section": "H.2 Experiments on LLaMA"}, {"figure_path": "UkauUrTbxx/figures/figures_39_2.jpg", "caption": "Figure 20: LLaMA (Textfooler)", "description": "This figure presents the results of textual entailment on SST2 under the TextFooler attack for the LLaMA model.  It compares the performance of the baseline LLaMA model against versions enhanced with different robust attention mechanisms (l1, Huber, MCP, and Huber-MCP). Subplots (a) shows the main results, while (b), (c), and (d) show ablation studies on the hyperparameters (\u03b4 and \u03b3) of the Huber and Huber-MCP methods respectively. The results demonstrate that l1 and MCP-based models suffer from significant performance drops, while the Huber and Huber-MCP methods achieve better robustness. ", "section": "H.2.2 TextBugger"}, {"figure_path": "UkauUrTbxx/figures/figures_40_1.jpg", "caption": "Figure 20: LLaMA (Textfooler)", "description": "This figure presents the results of textual entailment experiments on the SST2 dataset using the TextFooler attack against the LLaMA language model. It compares the performance of the original LLaMA model against various robust versions using different penalty functions (l1, Huber, MCP, and Huber-MCP).  The subfigures (a) to (d) show the main results and ablation studies on the delta and gamma parameters of the Huber and Huber-MCP penalty functions respectively. The results demonstrate the impact of different penalty functions on the model's robustness under different attack strengths, represented by the number of perturbed words.", "section": "H.2.2 TextBugger"}, {"figure_path": "UkauUrTbxx/figures/figures_40_2.jpg", "caption": "Figure 20: LLaMA (Textfooler)", "description": "This figure displays the results of textual entailment on SST2 under TextFooler attack for LLaMA model. It presents the main results, and ablation studies on delta and gamma in Huber and Huber-MCP respectively. The results show that l1 and MCP-based methods sacrifice performance, while Pro-LLaMA (Huber) outperforms other baselines.", "section": "H.2.2 TextBugger"}, {"figure_path": "UkauUrTbxx/figures/figures_42_1.jpg", "caption": "Figure 23: Adaptive JailBreak", "description": "This figure shows the attack success rates (ASRs) of Vicuna and Pro-Vicuna under adaptive jailbreaking attacks on the Behaviors dataset.  It demonstrates the effectiveness of Pro-Vicuna in mitigating these attacks, showing a significant improvement in robustness compared to the baseline Vicuna model, even with varying numbers of attack attempts. The different lines represent different values of the delta parameter (\u03b4) in the Huber loss function used in Pro-Vicuna, demonstrating the impact of this parameter on the model's robustness.", "section": "I Additional Experiments on Jailbreak"}, {"figure_path": "UkauUrTbxx/figures/figures_44_1.jpg", "caption": "Figure 4: Ablation studies.", "description": "This figure presents the results of several ablation studies conducted to evaluate the effectiveness of the proposed ProTransformer model. The studies explore different aspects of the model, including its convergence properties, the effects of adversarial fine-tuning, the impact of attack constraints, and the influence of different penalty functions used in the robust token estimators.  Subfigure (a) shows convergence curves, (b) illustrates the impact of adversarial training on model robustness, (c) analyzes the effect of attack constraints, and (d) compares the performance of different penalty functions.", "section": "4.2.2 Ablation Study"}]