[{"type": "text", "text": "Instance-Optimal Private Density Estimation in the Wasserstein Distance ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vitaly Feldman Audra McMillan Satchit Sivakumar\u2217 Kunal Talwar Apple Apple Boston University Apple ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Estimating the density of a distribution from samples is a fundamental problem in statistics. In many practical settings, the Wasserstein distance is an appropriate error metric for density estimation. For example, when estimating population densities in a geographic region, a small Wasserstein distance means that the estimate is able to capture roughly where the population mass is. In this work we study differentially private density estimation in the Wasserstein distance. We design and analyze instance-optimal algorithms for this problem that can adapt to easy instances. ", "page_idx": 0}, {"type": "text", "text": "For distributions $P$ over $\\mathbb{R}$ , we consider a strong notion of instance-optimality: an algorithm that uniformly achieves the instance-optimal estimation rate is competitive with an algorithm that is told that the distribution is either $P$ or $Q_{P}$ for some distribution $Q_{P}$ whose probability density function (pdf) is within a factor of 2 of the pdf of $P$ . For distributions over $\\mathbb{R}^{\\bar{2}}$ , we use a different notion of instance optimality. We say that an algorithm is instance-optimal if it is competitive with an algorithm that is given a constant-factor multiplicative approximation of the density of the distribution. We characterize the instance-optimal estimation rates in both these settings and show that they are uniformly achievable (up to polylogarithmic factors). Our approach for $\\mathbb{R}^{2}$ extends to arbitrary metric spaces as it goes via hierarchically separated trees. As a special case our results lead to instance-optimal private learning in TV distance for discrete distributions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Distribution estimation is a fundamental problem in statistics. In this work, we focus on the problem of learning the density of a distribution over a low-dimensional real space. Our motivation for studying this problem comes from practical problems such as estimating the population density in a geographical area (defined by bounded two dimensional space, for e.g. $[\\hat{0},\\hat{\\ell}]^{2})$ , learning the distribution of accuracy of a machine learning model (i.e. a distribution over $[0,1]\\rangle$ , estimating the average temperature across latitude, longitude, and altitude (i.e. a distribution over $[0,\\ell]^{3})$ etc. ", "page_idx": 0}, {"type": "text", "text": "In this work, we are interested in the non-parametric version of this question, where we make no assumptions on the form of the distribution we are learning. This is frequently of interest in practice, where population densities for example may change over time (become more or less concentrated), and it is difficult to specify a meaningful parametric class that will simultaneously capture all densities of interest. Given estimation is often done using sensitive data (for e.g. health data), our interest in this question is in, and consequently all our results are for, the differentially private version of this question. While we believe our results in the non-private setting are also novel and interesting, we view the private results as our main contribution. ", "page_idx": 0}, {"type": "text", "text": "Any statistical algorithm learning from samples is inexact. The appropriate gauge to measure the (in)accuracy of a density estimation algorithm depends on how this density estimate is used. In this work, we focus on the Wasserstein distance between the original distribution and the learnt distribution as our measure of accuracy. Known by many names (Earthmover distance, Kantorovich distance, Optimal Transport distance), this distance is defined over any distance metric $d$ as the minimum over all couplings $\\pi$ from $P$ to $Q$ of the quantity $\\mathbb{E}_{x\\sim P}[d(x,\\overbar{\\pi}(x))]$ . It is arguably one of the most natural ways to define distances between distributions over a metric space and has been extensively studied (see Appendix C) . We note that Wasserstein distance is particularly salient in many practical applications of density estimation where the geometry of the space is significant. As a simple example, when creating population density estimates, if the population is concentrated in a few cities, then outputting a distribution concentrated close to these cities (even if not exactly at the cities) is intuitively better than outputting a distribution that is more spread out. Metrics such as TV distance that do not incorporate the geometry of the space do not capture this nuance. Additionally, Wasserstein distance is versatile and can be adapted to the setting of interest by varying the metric. In the case of the metric being a discrete metric with $d(x,y)=\\mathbf{1}(x\\neq y)$ , it reduces to the commonly used total variation (TV) distance. Our focus in this work is on the case of Euclidean distance metric on $[0,1]$ or $[0,1]^{2}$ , though our results apply to both to higher-dimensional Euclidean space as well as to any finite metric. In the $[0,1]$ case (with the standard Euclidean metric), the Wasserstein distance is equivalent to the total area between the cumulative distribution functions. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The problem of learning a distribution under Wasserstein distance has a long history, starting with [Dud69] proving worst-case bounds on the rate of convergence of the Wasserstein distance between the empirical distribution $\\hat{P}_{n}$ and the target distribution $P$ over $\\mathbb{R}^{d}$ . Similarly, this question for the case of the discrete metric $(d(x,y)=\\mathbf{1}(x\\neq y))$ has been very well studied. However, most known results for this problem look at it from the point of view of worst-case analysis. This can paint a rather pessimistic picture. For example, the minimax rate of $\\varepsilon$ -privately learning a discrete distribution over $\\{0,\\ldots,\\bar{k}\\}$ in TV distance (i.e. Wasserstein with the discrete metric described above) scales linearly with $k$ , which can be prohibitive for large support size $k$ . For Wasserstein distance with $\\ell_{2}$ norm, the rate of convergence of the empirical distribution suffers a curse of dimensionality, with the worst-case error between the distribution and the empirical distribution being $\\Theta(n^{-}{\\textstyle{\\frac{1}{d}}})$ for distributions over $[0,1]^{d}$ . For the differentially private version of this question, recent works [BSV22, HVZ23] have shown that the optimal Wasserstein minimax error between the sample and the private estimate is $\\left(\\!\\left(\\varepsilon n\\right)^{-\\frac{1}{d}}\\!\\right)$ . This worst-case analysis viewpoint fails to distinguish between algorithms that perform very differently on the types of instances one may see in practice. In particular, many practical distributions may be more feasible to estimate than suggested by the minimax rate. As an example, Figure 1 shows the cumulative distribution function of a bimodal distribution on $[0,1]$ with very sparse support, and the cdf learnt by a minimax optimal algorithm, as well as an algorithm we present in this work (See Appendix F for details on this experiment). As is clear from the figure, the minimax optimal algorithm is easily outperformed. This phenomenon only gets worse in higher dimensions. Similarly, if the distribution in $\\mathrm{Re}^{d}$ lies on a $k$ -dimensional subspace, the worst-case error scaling with $\\tilde{O}((\\varepsilon n)^{-\\frac{1}{d}})$ is significantly larger than our algorithm\u2019s scaling of $\\tilde{O}((\\varepsilon n)^{-\\frac{1}{k}})$ . ", "page_idx": 1}, {"type": "text", "text": "This motivates the problem of viewing this question through the lens of instance optimality. 2 Briefly, instance optimal algorithms are those that on any given instance of the problem, are able to perform competitively with what any algorithm can do on this instance. Let $\\mathcal{M}$ be a class of algorithms of interest (e.g. all $(\\varepsilon,\\delta)$ -differentially private algorithms) and $c o s t(\\cdot,P)$ be a cost measure for an instance $P$ . In our setting, we have a distribution $P$ over a metric space, and given a set $\\hat{P}_{n}$ of $n$ samples from $P$ , we want to learn an estimate $\\boldsymbol{A}(\\hat{P}_{n})$ for the distribution. Our measure of performance is the Wasserstein distance $\\mathcal{W}$ , so $c o s t(A,P)=\\mathbb{E}[\\mathcal{W}(P,\\mathcal{A}(\\hat{P}_{n}))]$ . We would ideally like to say that an algorithm $\\boldsymbol{\\mathcal{A}}$ is $\\alpha$ -instance optimal in a class $\\mathcal{M}$ if for all instances $P$ , and all ${\\mathcal{A}}^{\\prime}\\in M$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\nc o s t(A(\\hat{P}_{n})),P)\\leq\\alpha\\cdot c o s t(A^{\\prime}(\\hat{P}_{n})),P).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "(InstanceOptimality-Ideal) ", "page_idx": 1}, {"type": "text", "text": "The reader would have noticed that this definition is however impossible to achieve except for trivial classes $\\mathcal{M}$ . The algorithm $\\mathcal{A^{\\prime}}$ that ignores its input and always outputs $P$ makes the right hand side 0. However, this algorithm performs poorly on any distributions far from $P$ and so is not a reasonable benchmark. A common approach in many works is to measure the performance of the competing algorithm $\\mathcal{A}^{\\prime}$ not just on the given instance, but on a small neighborhood around it. Thus we say that that an algorithm $\\boldsymbol{\\mathcal{A}}$ is $\\alpha$ -instance optimal amongst a class $\\mathcal{M}$ with respect to a neighborhood function $\\mathcal{N}$ if for all instances $P$ , and all ${\\mathcal{A}}^{\\prime}\\in M$ ", "page_idx": 1}, {"type": "image", "img_path": "Apq6corvfZ/tmp/ad28914ea37dae70e6bb55a983828fdfa8c971e5d42d2e790229a658d3044af7.jpg", "img_caption": ["Figure 1: (Left) A sparsely supported distribution on integers [0,999] (pdf). (Right) CDF for the same distribution (green, solid line), along with a (non-private) minimax optimal learnt distribution (blue, dashed line), as well as 1-DP instance-optimal algorithm (red, dotted), both learnt from the same 1600 samples. The $W_{1}$ error for the minimax optimal algorithm is 13.4, whereas the DP estimated distribution has $W_{1}$ error of 0.86. While this example is artificial, it demonstrates the large potential gap between minimax optimal and instance optimal algorithms on specific instances. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nc o s t(\\mathcal{A}(\\hat{P}_{n})),P)\\leq\\alpha\\cdot\\operatorname*{sup}_{P^{\\prime}\\in\\mathcal{N}(P)}c o s t(\\mathcal{A}^{\\prime}(\\hat{P}^{\\prime}{}_{n})),P^{\\prime}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In other words, the benchmark we evaluate against is the cost of the best algorithm for a neighborhood ${\\mathcal{N}}(P)$ that knows this neighborhood. We would like our algorithm $\\boldsymbol{\\mathcal{A}}$ , that is not tailor-made for ${\\mathcal{N}}(P)$ , to nevertheless be competitive against this benchmark. ", "page_idx": 2}, {"type": "text", "text": "This definition is general, and captures most notions of instance optimality that have been studied in the literature. The set ${\\mathcal{N}}(P)$ must be carefully defined for this notion to be meaningful; we can always define ${\\mathcal{N}}(P)$ to be the set of all instances whence this notion reduces to worst-case analysis. In many previous works, this neighborhood map has been defined to capture the belief that any natural algorithm must not have significantly different performance on different members of ${\\mathcal{N}}({\\dot{P}})$ . For example, [FLN01, ABC17, VV16, OS15, GKN20] include in ${\\mathcal{N}}(P)$ appropriate renamings of $P$ to capture some kind of permutation invariance of natural algorithms. In statistics, one often enforces that the cardinality of ${\\mathcal{N}}(P)$ is 2, often called the hardest one-dimensional subproblem [CL15, AD20, DLSV23]. Some recent works in privacy [HLY21, DKSS23] have defined instance optimality w.r.t. neighboring datasets obtained by deleting a small number of data points. Any reasonable definition of instance optimality for a problem must justify its choice of the neighborhood map; similar choices must be justifiable in every other notion of beyond worst case analysis [Rou21]. In instance-optimality definitions, this choice of neighborhood is what encapsulates what class of domain-specific algorithms our algorithm competes against. A good definition thus depends on the context and on the kind of domain knowledge we imagine an expert designing a custom algorithm for an application may have. Ideally, the definition is broad (i.e. the neighborhoods $\\mathcal{N}(P)$ are sufficiently contained) so that in a large class of applications, we expect the domain knowledge to not be enough to rule out any member of ${\\mathcal{N}}(P)$ . We discuss this general definition of instance optimality further in Appendix B. We remark that for reasonable neighborhood maps, this is an extremely strong requirement: an instance-optimal algorithm must simultaneously do well on every single input, in fact as well as any other algorithm that is given this neighborhood ${\\mathcal{N}}(P)$ in advance! ", "page_idx": 2}, {"type": "text", "text": "Instance optimality guarantees are most useful when there is a big difference between achievable utility guarantees for typical cases and the worst-case utility guarantees. Wasserstein estimation is an example of such a problem. We will see that achievable utility bounds for, for example, concentrated distributions are a lot better than worse case distributions. Our definition of instance optimality is particularly suitable for metric spaces, and our notion of neighborhood allows the target utility bound to adapt to the distribution. We note that for estimation in Wasserstein distance with practically important metrics such as $\\ell_{1}$ and $\\ell_{2}$ norms, it is unclear if existing instance optimality definitions (using notions of neighborhood discussed above) capture this. For example, for discrete distributions, setting the neighborhood to be all permutations of the distribution destroys all structure of the distribution (for e.g. concentration), and hence performance on this neighborhood may not capture the relative ease of estimation of a concentrated distribution. Similar problems apply to other previously studied definitions of instance optimality, which are not well-suited to density estimation with error metrics that incorporate the geometry of the metric space. See Appendix B and Appendix $\\textrm{C}$ for further discussion on the inadequacy of existing instance optimality definitions for our setting of interest. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Our notion of neighborhood will correspond to small balls in one of the strictest notions of distance between distributions. Recall that for distributions $P,Q$ on $X$ , $D_{\\infty}(P,Q)$ is defined as $\\begin{array}{r}{\\operatorname*{sup}_{x\\in X}\\operatorname*{max}\\left(\\ln\\frac{P(x)}{Q(x)},\\ln\\frac{Q(x)}{P(x)}\\right)}\\end{array}$ . Our neighborhood map $\\mathcal{N}$ will have the property that for all $P$ , and for all $Q\\,\\in\\mathcal{N}(P),\\,D_{\\infty}(P,Q)\\,\\leq\\,\\ln2$ . This corresponds to the benchmark algorithm $\\mathcal{A^{\\prime}}$ being given as auxiliary input a multiplicative constant factor approximation to the probability density function $P(x)$ (and we can replace the constant 2 by any constant). In particular, an algorithm that knows the support of the distribution $P$ will not be able to do much better than our algorithm that gets no such information. Notice that this implicitly implies that our algorithm is able to exploit sparsity in the data distribution since it is competitive with an algorithm that is told the support. In the one-dimensional real case we can achieve an even stronger notion of instance-optimality. In this case ${\\mathcal{N}}(P)$ is defined to be $\\{P,Q\\}$ where $Q$ is a distribution with $D_{\\infty}(P,Q)\\ \\stackrel{!}{\\leq}\\ \\ln2$ . This is a strengthening of the rate defined by the hardest one-dimensional subproblem. ", "page_idx": 3}, {"type": "text", "text": "We also give a definition that captures another aspect of instance optimality, related to the notion of super efficiency, that we term local minimality in Appendix B. Informally, local minimality says that if any comparator algorithm does better than $\\boldsymbol{\\mathcal{A}}$ on $P$ , then there is a distribution $Q$ in the neighborhood of $P$ where $\\boldsymbol{\\mathcal{A}}$ does better than the comparator. Approximate local minimality relaxes the latter condition to being better than some constant times the comparator. The two definitions of approximate local minimality and instance optimality are in general incomparable (see Appendix B) but for suitable smooth algorithms, we show that these definitions are equivalent. Our algorithms, both for the 1-dimensional and the case of general metric spaces approximately satisfy both these definitions. ", "page_idx": 3}, {"type": "text", "text": "In order to show that the instance optimality definition is achievable, we give both algorithmic upper bounds and matching, up to logarithmic factors, theoretical lower bounds. The algorithms we use in our upper bounds are built largely from ingredients previously used for similar problems. We see this as an asset since these algorithms are implementable in practice. A key ingredient that we do introduce is the use of randomised HST approximation of finite metric spaces. This replaces deterministic hierarchical decompositions that were used in prior work, allowing us to gain tighter utility guarantees. Our main conceptual contribution is to introduce what we believe to the right notion of instance optimality for this problem, including the definition of a meaningful neighbourhood function. The main technical challenge is in the lower bounds, which require carefully building nets of distributions within each neighborhood ${\\mathcal{N}}(P)$ that allow us to use a slight generalisation of DP Assoud\u2019s Lemma to give a lower bound on the target estimation rate for each distribution $P$ . ", "page_idx": 3}, {"type": "text", "text": "Preliminaries: First, we define differential privacy. Further discussion on differential privacy can be found in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Definition 1.1 (Differential Privacy [DMNS17, $\\mathrm{DKM^{+}06}]$ ). A randomized algorithm $A:\\mathcal{X}^{n}\\to\\mathcal{Y}$ is $(\\varepsilon,\\delta)$ -differentially private if for every pair of datasets $\\mathbf{x},\\mathbf{x}^{\\prime}\\in\\mathcal{X}^{n}$ that differ in at-most one data entry, and for all events $Y\\subseteq\\mathcal{Y}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}[A(\\mathbf{x})\\in Y]\\le e^{\\varepsilon}\\cdot\\operatorname*{Pr}[A(\\mathbf{x}^{\\prime})\\in Y]+\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given an estimation algorithm $A:\\mathcal{X}^{n}\\to\\mathcal{M}$ , the estimation rate of $\\boldsymbol{\\mathcal{A}}$ for distribution $P$ is: ", "page_idx": 3}, {"type": "text", "text": "$R_{A,n}(P)=\\operatorname*{inf}_{t\\in\\mathbb{R}}\\{t:\\mathbf{w.p.}\\geq0.75$ over $\\mathbf{x}\\sim P^{n}$ and the randomness of the algorithm, $\\mathcal{W}_{d}(\\mathcal{A}(\\mathbf{x}),P)\\leq t\\}$ ", "page_idx": 3}, {"type": "text", "text": "1.1 Our Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We start by stating an informal version of our result in the one-dimensional real case. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1.2 (Informal 1-dimensional result). Let $\\varepsilon,\\gamma\\in(0,1]$ . There is an $\\varepsilon$ -differentially private algorithm $\\boldsymbol{\\mathcal{A}}$ such that, for all distributions $P$ supported in $[0,1]$ , for all natural numbers $n\\ >$ polylo\u03b5g 1/\u03b3, there exists a distribution Q (with D\u221e(P, Q) \u2264ln 2) such that the following is satisfied. For any $\\varepsilon$ -DP algorithm $\\mathcal{A^{\\prime}}$ , with probability at least 0.75 over the randomness of $\\mathbf{x}\\,\\sim\\,P^{n}$ and additional randomness of the algorithm, ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}(\\mathcal{A}(\\hat{P}_{n}),P)\\leq\\mathrm{polylog}\\,n\\cdot\\underset{P^{\\prime}\\in\\{P,Q\\}}{\\operatorname*{sup}}R_{\\mathcal{A}^{\\prime},n^{\\prime}}(P^{\\prime})+\\gamma}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{n^{\\prime}\\approx\\frac{n}{\\mathrm{polylog}\\,n/\\gamma}}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "In this one-dimensional case, our algorithm is based on DP quantile estimation. The additive $\\gamma$ term can be made polynomially small. The lower bound is based on (differentially private) simple hypothesis testing where for each distribution $P$ , we find a distribution in ${\\mathcal{N}}(P)$ that is indistinguishable from $P$ given $n$ samples but also sufficiently far from $P$ in Wasserstein distance. ", "page_idx": 4}, {"type": "text", "text": "Extending the quantiles based approach from the one dimensional setting to even the two dimensional setting is challenging, as there is no \u201cright\u201d way to generalize quantiles to dimensions 2 or beyond. Several previous works on Wasserstein density estimation (e.g. [BNNR09]) have used a hierarchical decomposition approach to address this question. A hierarchical approach has also been used in various more practical works on private density estimation (e.g. [CB22, QYL12, $\\mathrm{BKM}^{+}21$ , $\\mathrm{MJT}^{+}22$ , ZXX16]). These works focus on practical performance and do not offer tight theoretical bounds. A hierarchical approach was also used by $[\\bar{\\mathrm{GHK}}^{+}23]$ , who proved theoretical bounds for a related problem, but not through the lens of instance optimality. We compare our results to theirs in more detail later in this section. ", "page_idx": 4}, {"type": "text", "text": "The use of deterministic hierarchical decompositions in all these papers means that some points that are very close (but on opposite sides of the boundaries of the hierarchical decomposition) get mapped to relatively far points, resulting in high distortion factors that are not appropriate for instance optimality. ", "page_idx": 4}, {"type": "text", "text": "Inspired by the above approaches but noting their constraints, we use a randomized embedding into hierarchically separated trees instead of a deterministic one. We define our algorithm on any hierarchically separated tree metric and use the fact that there is a randomized embedding of $[0,1]^{\\frac{\\l}{2}}$ on a hierarchically separated tree metric space with low distortion. This, along with some other important technical modifications (such as truncating low values to 0), allows us to analyze a variant of the above practical algorithms theoretically and show that it satisfies our strong notion of instance optimality, up to polylogarithmic factors in the number of samples. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1.3 (Informal two-dimensional result). There is a polynomial time $\\varepsilon$ -differentially private algorithm $\\boldsymbol{\\mathcal{A}}$ that for any distribution $P$ on $[0,1]^{2}$ , any integer $n$ , and any $\\varepsilon$ -DP algorithm $\\mathcal{A^{\\prime}}$ with probability at least 0.75, satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{W}_{2}(A(\\hat{P}_{n}),P)\\leq(\\log n)^{O(1)}\\operatorname*{sup}_{P^{\\prime}:D_{\\infty}(P,P^{\\prime})\\leq\\ln2}\\mathbb{E}[\\mathcal{W}_{2}(A^{\\prime}(\\hat{P}^{\\prime}{}_{n^{\\prime}}),P^{\\prime})],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where polynlog n. Here, the expectation is taken over the internal coin tosses of A as well as over the choice of the i.i.d. samples $\\hat{P}_{n}$ . ", "page_idx": 4}, {"type": "text", "text": "In fact, since our algorithm is defined on any hierarchically separated tree metric space, it has the added bonus of giving instance optimality results for any finite metric space (since powerful results [Bar96, FRT03] show that any finite metric space can be embedded in a hierarchically separated tree metric space with a distortion factor at most logarithmic in the size of the metric space). ", "page_idx": 4}, {"type": "text", "text": "Theorem 1.4 (Informal finite metric result). Let $(\\mathcal{X},d)$ be an arbitrary metric space with diameter 1. There is a polynomial time $\\varepsilon$ -differentially private algorithm $\\boldsymbol{\\mathcal{A}}$ such that for any distribution $P$ on $X$ any integer n and any $\\varepsilon$ -DP algorithm $\\mathcal{A}^{\\prime}$ with probability at least 0.75, satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{W}(\\mathcal{A}(\\hat{P}_{n}),P)\\leq(\\log|\\mathcal{X}|\\cdot\\log n)^{O(1)}\\operatorname*{sup}_{P^{\\prime}:D_{\\infty}(P,P^{\\prime})\\leq\\ln2}\\mathbb{E}[\\mathcal{W}(\\mathcal{A}^{\\prime}(\\hat{P^{\\prime}}_{n^{\\prime}}),P^{\\prime})],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{n^{\\prime}\\approx\\frac{n}{\\mathrm{polylog}\\,n}}\\end{array}$ polynlog n. Here, the expectation is taken over the internal coin tosses of A as well as over the choice of the i.i.d. samples $\\hat{P}_{n}$ . ", "page_idx": 4}, {"type": "text", "text": "Our lower bound result is actually slightly stronger than stated in Theorem 1.4 since it holds not only for $\\varepsilon$ -DP, but also for $(\\varepsilon,\\delta)$ -DP. At this point, we also compare specifically to the paper of $[\\mathrm{GHK}^{+}23]$ who give an algorithm for obtaining two-dimensional heatmaps and analyze it theoretically. They focus on the empirical version of a variant of this problem as opposed to the population version, and aim to compete with the best $k$ -sparse distribution. Their algorit\u221ahm takes the sparsity parameter $k$ as input in order to set parameters and achieves additive error ${\\sqrt{k}}/n$ (and a constant multiplicative factor). On the other hand, our algorithm also performs better for sparse distributions but is automatically adaptive to the sparsity (and hence doesn\u2019t need to take it as an input). Additionally the additive term in our work can be made polynomially small (for any polynomial) in $n$ at a logarithmic cost to the multiplicative error (regardless of the sparsity of th\u221ae distribution). On the other hand, for large $k$ their results have additive error that scales with $1/\\sqrt{n}$ . Their use of a deterministic hierarchical decomposition makes their algorithm unsuitable for our notion of instance optimality (as discussed earlier), and it is unclear if their algorithm can be directly extended to all finite metric spaces. ", "page_idx": 5}, {"type": "text", "text": "Note that instance optimality for all finite metric spaces implies instance optimality results for a wide variety of applications not addressed in prior work. For example, our results immediately extend to other low-dimensional real spaces with arbitrary metrics (for e.g. $\\ell_{p}$ norms). They also give non-trivial improvements on worst-case analysis for higher-dimension\u221aal spaces that are not the main focus of our work (for $[0,1]^{d}$ , we can use a fine grid of size $(1/(\\eta/\\sqrt{d}))^{d}$ at an additive cost of $\\eta$ in the Wasserstein distance in order to create a finite metric space to apply our result on. Since tohvee rdheepaedn dteernmce  roenpl $|{\\mathcal{X}}|$ n ign  tthhee bfaocvteo ri s alboogvaer.it hmWihci,l eth tish itsr ains slstaitlels  at os iag $\\begin{array}{r}{\\bar{d}\\log\\frac{d}{\\eta}}\\end{array}$ movueltrihpeliacda,t iavlel $\\log\\left|\\lambda\\right|$   \nprevious results on density estimation in the Wasserstein distance (in both the private and nonprivate literature) are worst case, where the sample complexity is exponential in $d$ . Since our results only have a polynomial dependence in $d$ over the optimal error, this is a non-trivial improvement over worst-case error, even when $d$ is large. ", "page_idx": 5}, {"type": "text", "text": "Another immediate application of our results is to give (to the best of our knowledge) new bounds for private estimation of discrete distributions in TV distance. Generally, for learning a discrete distribution defined by probabilities $\\{p_{1},\\ldots,p_{k}\\}$ , our results lead to a rate (up to polylogarithmic factors) of $\\begin{array}{r}{\\sum_{i}\\operatorname*{min}\\left\\{p_{i}(1-p_{i}),\\sqrt{\\frac{p_{i}(1-p_{i})}{n}}\\right\\}+\\sum_{i}\\operatorname*{min}\\left(p_{i},(1-p_{i}),\\frac{1}{\\varepsilon n}\\right).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "This can give significant improvements over the worst case bounds for practically important distributions. The minimax rate is linear in the support size $k$ , namely $\\Theta(k/\\varepsilon n)$ (for sufficiently small $\\varepsilon$ ). Now, consider the following power-law distribution over support size $k$ : $p(i)\\,\\propto\\,i^{-2}$ . (Power law distributions arise frequently in practice for e.g. frequencies of family names, sizes of power outages etc. all follow power law distributions.) Applying our result above gives a bound that is $\\begin{array}{r}{\\tilde{O}\\left(\\operatorname*{min}\\{\\frac{k}{\\varepsilon n},\\frac{1}{\\sqrt{\\varepsilon n}}\\}\\right)}\\end{array}$ , which is much better than the worst case bound for large support distributions. Our result also applies to other practically important settings such as building lists of popular sequences such as $\\mathbf{n}$ -grams over words. We leave open the questions of designing instance-optimal algorithms for other practically important questions in private learning and statistics, and of designing better instance optimal algorithms for higher dimensional spaces. We also leave open the question of removing the polylogarithmic factors in our instance optimality bounds. ", "page_idx": 5}, {"type": "text", "text": "1.2 Techniques ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1.2.1 Distributions over $\\mathbb{R}$ : ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We start by describing the rate we obtain for distributions $P$ over $\\mathbb{R}$ .In order to state the rate, we will use $q_{\\alpha}$ to represent the $\\alpha$ -quantile of the distribution $P$ and use $P|_{a,b}$ to define a certain restricted distribution described below. The rate consists of three terms and roughly looks as follows\u2014 we suppress logarithmic factors in $n$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{\\mathcal{A},n}(P)=\\tilde{O}\\left(\\mathbb{E}\\left[\\mathcal{W}\\left(P,\\hat{P}_{n}\\right)\\right]+\\frac{1}{\\varepsilon n}\\left(q_{1-\\frac{1}{\\varepsilon n}}-q_{\\frac{1}{\\varepsilon n}}\\right)+\\mathcal{W}(P,P|_{q_{\\frac{1}{\\varepsilon n}},q_{1-\\frac{1}{\\varepsilon n}}})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The first term is $\\mathbb{E}[\\mathcal{W}(P,\\hat{P}_{n})]$ , the expected Wasserstein distance between the true distribution and the empirical distribution over $n$ samples, and is the non-private term. The remaining two terms represent the cost of privacy- the first is a specific interquantile distance, roughly $\\begin{array}{r}{\\frac{1}{\\varepsilon n}(q_{1-\\frac{1}{\\varepsilon n}}-q_{\\frac{1}{\\varepsilon n}})}\\end{array}$ , and the second can be thought of as capturing the weight of the tails- represented by the Wasserstein distance between $P$ and a \u2018restricted\u2019 version of $P$ with its tails chopped off (i.e. the cumulative distribution function is 0 below $q_{1/\\varepsilon n}$ and 1 above $q_{1-1/\\varepsilon n}$ and identical to $P$ otherwise). Observe that all 3 of the terms above are smaller for distributions with small support or greater concentration, and hence the rate adapts to the hardness of the distributions. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Upper Bounds: The upper bound involves estimating roughly $\\varepsilon n$ equally spaced quantiles of the empirical distribution differentially privately (using a known private CDF estimation algorithm), and placing roughly $1/\\varepsilon n$ mass at each of the estimated quantile points. For the analysis, the intuition for each of the terms is as follows: since we only have access to the empirical distribution, the non-private term $\\mathbb{E}[\\mathcal{W}(P,\\hat{P}_{n})]$ comes from that. Next, if the quantile estimates are good, then the pointwise CDF differences between the empirical distribution and the estimated distribution are at most $1/\\varepsilon n$ (due to the discretization), and so we will pay $1/\\varepsilon n$ multiplied by the interquantile distance of the empirical distribution. This aligns with the accuracy of state-of-the-art DP quantile estimation algorithms. Finally, since the distribution is restricted to the estimated quantiles, the distribution is 0 before the first estimated quantile and 1 above the last estimated quantile and so we pay the Wasserstein distance between the empirical distribution and a restricted version of the empirical distribution. Some care needs to be taken while reasoning about expectation versus high probability (for various terms), and in relating population quantities to empirical quantities (which we do using various concentration inequalities). Details can be found in Section E.2. ", "page_idx": 6}, {"type": "text", "text": "Lower Bounds: We prove that the private and non-private terms are lower bounds separately. Both proofs follow the same framework. The idea is that given knowledge of two distributions $P$ and $Q$ , we can use a (private) Wasserstein estimation algorithm to construct a hypothesis test distinguishing $P$ from $Q$ . If the (private) estimate for $P$ and $Q$ with $n$ samples gives error smaller than $\\textstyle{\\frac{1}{2}}\\bar{\\mathcal{W}}(P,Q)\\bar{\\mathcal{$ , we can use this to distinguish $P$ from $Q$ . This would give a contradiction if $P$ and $Q$ are (privately) indistinguishable with $n$ samples. Hence, this would give a lower bound of ${\\textstyle\\frac{1}{2}}\\mathcal{W}(P,Q)$ on the error of the Wasserstein estimation algorithm on $P$ or $Q$ . ", "page_idx": 6}, {"type": "text", "text": "Thus the task reduces to constructing a distribution $Q$ that satisfies three properties: 1) it is (privately) indistinguishable from $P$ given $n$ samples, 2) the Wasserstein distance between $P$ and $Q$ is sufficiently large, 3) $D_{\\infty}(P,Q)\\leq\\ln2$ . The main technical work is in identifying a distribution $Q$ that satisfies these properties. ", "page_idx": 6}, {"type": "text", "text": "For the privacy term, we construct the distribution $Q$ by taking half the mass from the first $1/\\varepsilon n$ - quantile of $P$ (scaling the density function by half) and moving it to the last $1/\\varepsilon n$ -quantile of $P$ (scaling the density function by $3/2$ ). The third property is satisfied by definition, so we reason about the other two. Intuitively, since the Wasserstein distance captures how hard it is to \u2018move\u2019 $P$ to $Q$ , this mass needs to move at least the interquantile distance to change $P$ to $Q$ . This implies that the Wasserstein distance is at least the interquantile distance scaled by $1/\\varepsilon n$ , as described in the rate. Additionally, mass that is further out in the tail needs to move more, which is captured by the Wasserstein distance between the distribution $P$ and its \u2018restriction\u2019. Hence, the Wasserstein distance between $P$ and $Q$ is lower bounded by these two terms of interest. The intuition behind Property 2 is that it is hard for any $\\varepsilon$ -DP algorithm to pinpoint the location of an $\\textstyle{\\frac{1}{\\varepsilon n}}$ -fraction of the points in the dataset. Overall, this shows the privacy lower bound. ", "page_idx": 6}, {"type": "text", "text": "The non-private lower bound requires a more careful construction of $Q$ . We divide $P$ into various scales and carefully adjust them differently to obtain the desired properties. Formally, to construct $Q$ from $P$ , we consider $q_{1/2}$ and all quantiles of the form $q_{1/2^{i}}$ and $q_{1-1/2^{i}}$ for $i>1$ . For $1\\leq i<$ $\\log n$ , we add mass to $\\left[q_{1/2^{i+1}},q_{1/2^{i}}\\right)$ , by setting the density $f_{Q}$ to be $(1+{\\sqrt{2^{i}/n}})f_{P}$ and balance out the extra mass by setting $f_{Q}$ to be $(1-{\\sqrt{2^{i}/n}})f_{P}$ between $\\left[q_{1-1/2^{i}},q_{1-1/2^{i+1}}\\right)$ . For $i\\geq\\log n$ (i.e. the tail), we add mass to $\\left[q_{1/2^{i+1}},q_{1/2^{i}}\\right)$ , by setting $f_{Q}$ to be $(1+\\textstyle{\\frac{1}{2}})f_{P}$ and balance out the extra mass by setting $f_{Q}$ to be $(1-{\\textstyle\\frac{1}{2}})f_{P}$ between $\\left[q_{1-1/2^{i}},q_{1-1/2^{i+1}}\\right)$ . ", "page_idx": 6}, {"type": "text", "text": "The third property is again trivially satisfied. For the first property, observe that to \u2018move\u2019 $P$ to $Q$ the extra $\\left.{\\frac{1}{\\sqrt{2^{i}n}}}\\right|$ mass between $\\left[q_{1/2^{i+1}},q_{1/2^{i}}\\right)$ has to \u2018travel\u2019 between $q_{1/2^{i}}$ and $q_{1-1/2^{i}}$ , and so the Wasserstein distance between $P$ and $Q$ can be lower bounded by a sum of various scaled interquantile distances. We attempt to upper bound the expected Wasserstein distance between $P$ and $\\hat{P}_{n}$ by a similar term. It is more intuitive to reason about this using an alternative (equivalent) formulation of Wasserstein distance as the area between the CDF curves of $P$ and $Q$ . The intuition is that the expected pointwise CDF difference between $P$ and $\\hat{P}_{n}$ in the interval $\\left[q_{1/2^{i+1}},q_{1/2^{i}}\\right)$ would be roughly $\\textstyle{\\frac{1}{\\sqrt{2^{i}n}}}$ (by properties of a Binomial) and hence the contribution of this interval to the area would be roughly $\\begin{array}{r}{\\frac{1}{\\sqrt{2^{i}n}}\\,\\left(q_{1/2^{i}}-q_{1/2^{i+1}}\\right)}\\end{array}$ and similarly for the corresponding interval $[q_{1-1/2^{i}}$ , $q_{1/2^{i+1}}$ . Hence, the expected Wasserstein distance would be a sum of these scaled quantile interval distances. We formalize this intuition using a result of Bobkov and Ledoux [BL19] that characterizes the expected Wasserstein distance between $P$ and $\\hat{P}_{n}$ as an integral of a function of the CDF of $P$ . We now have a bound in terms of the sum of scaled quantile interval distances, but we want to bound it by a sum of scaled interquantile distances. We can telescope the sum to indeed bound it by a sum of scaled interquantile distances. This establishes that $\\mathcal{W}(P,Q)\\ge\\mathbb{E}[\\mathcal{W}(P,\\hat{P}_{n}]$ . Next, we show that $P$ is indistinguishable from $Q$ by analyzing the KL divergence between $P$ and $Q$ . The main idea is that high density intervals are modified by a small multiplicative factor of roughly $1+{\\frac{1}{\\sqrt{n}}}$ , but low density intervals (with mass less than $1/n$ ) are modified by a constant multiplicative factor, so overall the contribution of each interval to the KL divergence is sufficiently small. This establishes indistinguishability with $n$ samples. For formal details we refer the reader to Section E.1. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "1.2.2 Distributions on HSTs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Since the main technical challenge of proving Theorem 1.4 is proving the equivalent result for distributions on HST metric spaces, we focus on that problem in this section. Standard results on low distortion embeddings of metric spaces into HST metric spaces can be used to translate the HST result to $[0,1]^{2}$ and to general metric spaces $X$ with log $|X|$ overhead. ", "page_idx": 7}, {"type": "text", "text": "Definition 1.5 (Hierarchically Separated Tree). $A$ hierarchically separated tree (HST) is a rooted weighted tree such that the edges between level $\\ell$ and $\\ell-1$ all have the same weight (denoted $r_{\\ell.}$ ) and the weights are geometrically decreasing so $r_{\\ell+1}=(1/2)r_{\\ell}$ . Let $D_{T}$ be the depth of the tree. ", "page_idx": 7}, {"type": "text", "text": "HSTs can be defined with any geometric scaling but we will only need a factor of 2 in this work. HSTs may also have arbitrary degree. A HST defines a metric on its leaf nodes by defining the distance between any two leaf nodes to be the weight of the minimum weight path between them. ", "page_idx": 7}, {"type": "text", "text": "HST metric spaces are particularly well-behaved when working with the Wasserstein distance since the Wasserstein distance on a HST has a simple closed form. A distribution $P$ on the the underlying metric space in a HST induces a function ${\\mathfrak{G}}_{P}$ on the nodes of the tree where the value of a node $\\nu$ is given by the weight in $P$ of the leaf nodes in the subtree rooted at $\\nu$ . For every level $\\ell\\in[D_{T}]$ of the tree, let $P_{\\ell}$ be the distribution induced on the nodes at level $\\ell$ where the probability of node $\\nu$ is ${\\mathfrak{G}}_{P}(\\nu)$ . Thus $P_{\\ell}$ is a discrete distribution on a domain of size $N_{\\ell}$ , where $N_{\\ell}$ is the number of nodes in level $\\ell$ of the tree. ", "page_idx": 7}, {"type": "text", "text": "Lemma 1.6 (Closed form Wasserstein distance formula). Given two distributions $P$ and $Q$ defined in a HST metric space, the Wasserstein distance between $P$ and $Q$ has the closed formula: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{W}(P,Q)=\\frac{1}{2}\\sum_{\\nu}r_{\\nu}|\\mathfrak{G}_{P}(\\nu)-\\mathfrak{G}_{Q}(\\nu)|=\\sum_{\\ell}r_{\\ell}\\mathrm{TV}(P_{\\ell},Q_{\\ell}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $r_{\\nu}$ is the weight of the edge connecting $\\nu$ to its parent, and the sum is over all nodes in the tree. ", "page_idx": 7}, {"type": "text", "text": "We will call a node $\\nu~\\alpha$ -active under the distribution $P$ if ${\\mathfrak{G}}_{P}(\\nu)\\,\\geq\\,\\alpha$ . Let $\\gamma_{P}\\left(\\alpha\\right)$ be the set of $\\alpha$ -active nodes under $P$ and $\\gamma_{P_{\\ell}}\\left(\\alpha\\right)$ be the set of $\\alpha$ -active nodes at level $\\ell$ . Then there exists an algorithm $\\boldsymbol{\\mathcal{A}}$ such that given a distribution $P,\\varepsilon>0$ , and $n\\in\\mathbb N$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{A,n}(P)=\\tilde{O}\\left(\\operatorname*{max}_{\\ell}r_{\\ell}\\sum_{x\\in[N_{\\ell}]}\\operatorname*{min}_{x\\in[N_{\\ell}]}\\left\\{P_{\\ell}(x)(1-P_{\\ell}(x)),\\sqrt{\\frac{P_{\\ell}(x)(1-P_{\\ell}(x))}{n}}\\right\\}+\\sum_{x\\notin\\gamma_{P_{\\ell}}(2\\kappa)}P_{\\ell}(x)+(|\\gamma_{P_{\\ell}}(2\\kappa)|)\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the max is over all the levels of the tree and $\\kappa=\\Theta(\\frac{\\log(n)}{\\varepsilon n})$ . Further, this bound matches (up to logarithmic factors) the lower bound $\\operatorname*{min}_{\\varepsilon}$ -DP $_{\\mathcal{A}^{\\prime}}\\operatorname*{sup}_{P^{\\prime}:D_{\\infty}(P,P^{\\prime})\\leq\\ln2}\\mathbb{E}[\\mathcal{W}(A^{\\prime}(\\hat{P^{\\prime}}_{n^{\\prime}}),P^{\\prime})]$ where $\\begin{array}{r}{n^{\\prime}\\approx\\frac{n}{\\mathrm{polylog}\\,n}}\\end{array}$ . The error rate $\\mathcal{R}_{\\mathcal{A},n}$ does indeed adapt to easy instances as we expected. The error decomposes into three components. The first component is the non-private sampling error; the error that would occur even if privacy was not required. The second component indicates that we can not privately estimate the value of nodes that have probability less than $\\approx1/(\\varepsilon n)$ . The third component is the error due to privacy on the active nodes. If $P$ is highly concentrated then we expect most nodes to either be $\\kappa$ -active or have weight 0, so the first two terms in $\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}(P)$ are small. There should also be few active nodes, making the last term also small. Conversely, if $P$ has a large region of low density then we expect a large number of inactive nodes, as well as non-zero inactive nodes that are at higher levels of the tree and hence contribute more to the final term. Thus, in distributions with high dispersion we expect the right hand side to be large. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Upper Bounds: As in the one-dimensional setting, we want to restrict to only privately estimating the density at a small number $(\\approx\\varepsilon n)$ of points. While we could try to mimic the one-dimensional solution by privately estimating a solution to the $\\varepsilon n$ -median problem, it\u2019s not clear how to prove such an approach is instance-optimal. It turns out that a simpler solution more amenable to analysis will suffice. Our algorithm has two stages; first we attempt to find the set of $\\kappa$ -active nodes, then we estimate the weight of these active nodes. Since these nodes have weight greater than $\\textstyle{\\frac{\\log(n)}{\\varepsilon n}}$ , we can privately estimate them to within constant multiplicative error. Any nodes that are not detected as active, are initially ascribed a weight of 0. The error due to not estimating the non-active nodes is absorbed into the third error term. The final step is to project the noisy density function into the space of distributions on the underlying metric space. The error of the upper bound algorithm is summed over all levels of the tree, although since the depth of the tree is logarithmic in the size of the metric space, this is within a logarithmic factor of the maximum over the levels. ", "page_idx": 8}, {"type": "text", "text": "Lower Bound: We first observe that in order to estimate the distribution well in Wasserstein distance, an algorithm must estimate each level of the tree well in TV distance. This is derived from Lemma 1.6. This allows us to reduce to the problem of lower bounding the error of density estimation of discrete distributions in TV distance. The main tool we use is a differentially private version of Assouad\u2019s method. Similar to how the technique in the previous section allowed us to relate lower bounding estimation rates to simple hypothesis testing, Assouad\u2019s lemma allows us to relate lower bounding estimation rates to multiple hypothesis testing. Note that unlike the technique in the previous section, Assouad\u2019s lemma allows us to prove lower bounds on the expected error, rather than lower bounds on high probability error bounds. It involves constructing nets of distributions in ${\\mathcal{N}}(P)$ that are pairwise far in the relevant metric of interest (which for us in the TV distance) but the multiple hypothesis testing problem between the distributions is sufficiently hard. For proving the third term belongs in the lower bound, the standard statement of DP Assouad\u2019s lemma [ASZ21] suffices, where one builds a set of distributions indexed by a hypercube. For the first and second terms, we need to slightly generalise the statement to allow for sets of distributions indexed by a product of hypercubes. We use the approximate DP version of DP Assouad\u2019s so while our upper bounds are for pure differential privacy, our lower bounds hold for both pure and approximate differential privacy. ", "page_idx": 8}, {"type": "text", "text": "Let us start with the third term. Suppose the number of active nodes is even (a small tweak is made if there is an odd number of active nodes). We pair up the active nodes and index each pair by a coordinate of the hypercube. For each corner of the hypercube, $(u^{0},u^{1},\\cdot\\cdot\\cdot\\,,u^{k})\\in\\{\\pm1\\}^{k}$ , for each coordinate $j~\\in[k]$ , if $u^{j}\\,=\\,+1$ , we move $\\tilde{O}(\\kappa)$ mass from one node in the $j$ th pair to the other node. If $u^{j}=-\\bar{1}$ then we leave the $j$ th pair of nodes alone. Since each active node has mass $>\\kappa$ , it\u2019s clear that each resulting distribution belongs in ${\\mathcal{N}}(P)$ . We can also show that these distributions form a sufficiently hard multiple hypothesis testing problem. By DP Assouad\u2019s (Lemma D.8), this allows us to lower bound the estimation error by $\\Omega(k\\kappa)$ , which is within $\\tilde{\\Omega}$ of the third term when the number of active nodes is $\\geq2$ . We treat the case where there is a single active node separately. ", "page_idx": 8}, {"type": "text", "text": "For the second term, we want to pair up the inactive nodes in a similar manner and move half their mass from one node to the other. However, since we want to remain within ${\\mathcal{N}}(P)$ , we can\u2019t pair any two inactive nodes together. Thus, we divide the inactive nodes into scales, where nodes within a certain scale all have weight within a multiplicative factor of two. We then pair up nodes within each scale and have a different hypercube for each scale. Again, it\u2019s clear that these distributions are all in ${\\mathcal{N}}(P)$ and we can show that these distributions form a sufficiently hard multiple hypothesis testing problem. The proof for the first term follows similarly. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "[AAK21] Ishaq Aden-Ali, Hassan Ashtiani, and Gautam Kamath. On the sample complexity of privately learning unbounded high-dimensional gaussians. In Vitaly Feldman, Katrina ", "page_idx": 8}, {"type": "text", "text": "Ligett, and Sivan Sabato, editors, Algorithmic Learning Theory, 16-19 March 2021, Virtual Conference, Worldwide, volume 132 of Proceedings of Machine Learning Research, pages 185\u2013216. PMLR, 2021. ", "page_idx": 9}, {"type": "text", "text": "[AAL23a] Mohammad Afzali, Hassan Ashtiani, and Christopher Liaw. Mixtures of gaussians are privately learnable with a polynomial number of samples. CoRR, abs/2309.03847, 2023.   \n[AAL23b] Jamil Arbas, Hassan Ashtiani, and Christopher Liaw. Polynomial time and private learning of unbounded gaussian mixture models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1018\u20131040. PMLR, 2023. [ABC17] Peyman Afshani, Je\u00b4re\u00b4my Barbay, and Timothy M. Chan. Instance-optimal geometric algorithms. J. ACM, 64(1), mar 2017. [AD20] Hilal Asi and John C. Duchi. Instance-optimality in differential privacy via approximate inverse sensitivity mechanisms. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n$[\\mathrm{ADJ^{+}}11]$ Jayadev Acharya, Hirakendu Das, Ashkan Jafarpour, Alon Orlitsky, and Shengjun Pan. Competitive closeness testing. In Sham M. Kakade and Ulrike von Luxburg, editors, COLT 2011 - The 24th Annual Conference on Learning Theory, June 9-11, 2011, Budapest, Hungary, volume 19 of JMLR Proceedings, pages 47\u201368. JMLR.org, 2011.   \n$[\\mathrm{ADJ^{+}}12]$ Jayadev Acharya, Hirakendu Das, Ashkan Jafarpour, Alon Orlitsky, Shengjun Pan, and Ananda Theertha Suresh. Competitive classification and closeness testing. In Shie Mannor, Nathan Srebro, and Robert C. Williamson, editors, COLT 2012 - The 25th Annual Conference on Learning Theory, June 25-27, 2012, Edinburgh, Scotland, volume 23 of JMLR Proceedings, pages 22.1\u201322.18. JMLR.org, 2012.   \n[AJOS13a] Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. A competitive test for uniformity of monotone distributions. In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2013, Scottsdale, AZ, USA, April 29 - May 1, 2013, volume 31 of JMLR Workshop and Conference Proceedings, pages 57\u201365. JMLR.org, 2013.   \n[AJOS13b] Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Optimal probability estimation with applications to prediction and classification. In Shai Shalev-Shwartz and Ingo Steinwart, editors, COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, volume 30 of JMLR Workshop and Conference Proceedings, pages 764\u2013796. JMLR.org, 2013.   \n$[\\mathrm{AKT}^{+}23]$ ] Daniel Alabi, Pravesh K. Kothari, Pranay Tankala, Prayaag Venkat, and Fred Zhang. Privately estimating a gaussian: Efficient, robust, and optimal. In Barna Saha and Rocco A. Servedio, editors, Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June 20-23, 2023, pages 483\u2013496. ACM, 2023. [AL22] Hassan Ashtiani and Christopher Liaw. Private and polynomial time algorithms for learning gaussians and beyond. In Po-Ling Loh and Maxim Raginsky, editors, Conference on Learning Theory, 2-5 July 2022, London, UK, volume 178 of Proceedings of Machine Learning Research, pages 1075\u20131076. PMLR, 2022.   \n[ALMM19] Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite littlestone dimension. In Moses Charikar and Edith Cohen, editors, Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019, pages 852\u2013860. ACM, 2019.   \n[ASSU24] Maryam Aliakbarpour, Rose Silver, Thomas Steinke, and Jonathan R. Ullman. Differentially private medians and interior points for non-pathological data. In Venkatesan Guruswami, editor, 15th Innovations in Theoretical Computer Science Conference, ITCS 2024, January 30 to February 2, 2024, Berkeley, CA, USA, volume 287 of LIPIcs, pages 3:1\u20133:21. Schloss Dagstuhl - Leibniz-Zentrum fu\u00a8r Informatik, 2024. [ASZ17] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private testing of identity and closeness of discrete distributions. CoRR, abs/1707.05128, 2017. [ASZ20] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private assouad, fano, and le cam. CoRR, abs/2004.06830, 2020. [ASZ21] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private Assouad, Fano, and Le Cam. In Vitaly Feldman, Katrina Ligett, and Sivan Sabato, editors, Proceedings of the 32nd International Conference on Algorithmic Learning Theory, volume 132 of Proceedings of Machine Learning Research, pages 48\u201378. PMLR, 16\u2013 19 Mar 2021. [BA20] Victor-Emmanuel Brunel and Marco Avella-Medina. Propose, test, release: Differentially private estimation with high probability. CoRR, abs/2002.08774, 2020. [Bar96] Yair Bartal. Probabilistic approximations of metric spaces and its algorithmic applications. In 37th Annual Symposium on Foundations of Computer Science, FOCS \u201996, Burlington, Vermont, USA, 14-16 October, 1996, pages 184\u2013193. IEEE Computer Society, 1996.   \n[BBDS13] Jeremiah Blocki, Avrim Blum, Anupam Datta, and Or Sheffet. Differentially private data analysis of social networks via restricted sensitivity. In Robert D. Kleinberg, editor, Innovations in Theoretical Computer Science, ITCS \u201913, Berkeley, CA, USA, January 9-12, 2013, pages 87\u201396. ACM, 2013. [BG14] Emmanuel Boissard and Thibaut Le Gouic. On the mean speed of convergence of empirical and occupation measures in Wasserstein distance. Annales de l\u2019Institut Henri Poincar\u00b4e, Probabilit\u00b4es et Statistiques, 50(2):539 \u2013 563, 2014.   \n$[\\mathbf{BGS}^{+}21]$ Gavin Brown, Marco Gaboardi, Adam D. Smith, Jonathan R. Ullman, and Lydia Zakynthinou. Covariance-aware private mean estimation without private covariance estimation. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 7950\u20137964, 2021. [BHS23] Gavin Brown, Samuel B. Hopkins, and Adam Smith. Fast, sample-efficient, affineinvariant private mean and covariance estimation for subgaussian distributions, 2023.   \n$[{\\mathbf{B}}{\\mathbf{K}}{\\mathbf{M}}^{+}21]$ Eugene Bagdasaryan, Peter Kairouz, Stefan Mellem, Adria\\` Gasco\u00b4n, Kallista Bonawitz, Deborah Estrin, and Marco Gruteser. Towards sparse federated analytics: Location heatmaps under distributed differential privacy with secure aggregation. arXiv preprint arXiv:2111.02356, 2021.   \n[BKSW21] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypothesis selection. IEEE Trans. Inf. Theory, 67(3):1981\u20132000, 2021. [BL19] Sergey G. Bobkov and Michel Ledoux. One-dimensional empirical measures, order statistics, and kantorovich transport distances. Memoirs of the American Mathematical Society, 2019. [BM23] Daniel Bartl and Shahar Mendelson. On a variance dependent Dvoretzky-KieferWolfowitz inequality. arXiv e-prints, page arXiv:2308.04757, August 2023.   \n[BNNR09] Khanh Do Ba, Huy L. Nguyen, Huy Ngoc Nguyen, and Ronitt Rubinfeld. Sublinear time algorithms for earth mover\u2019s distance. Theory of Computing Systems, 48:428\u2013 442, 2009. [BNS16] Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. Theory Comput., 12(1):1\u201361, 2016.   \n[BNSV15] Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Differentially private release and learning of threshold functions. CoRR, abs/1504.07553, 2015. [BS19] Mark Bun and Thomas Steinke. Average-case averages: Private algorithms for smooth sensitivity and mean estimation. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alche\u00b4-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 181\u2013191, 2019. [BSV22] March Boedihardjo, Thomas Strohmer, and Roman Vershynin. Private measures, random walks, and synthetic data, 2022. [BUV18] Mark Bun, Jonathan R. Ullman, and Salil P. Vadhan. Fingerprinting codes and the price of approximate differential privacy. SIAM J. Comput., 47(5):1888\u20131938, 2018. [BY02] Z. Bar-Yossef. The Complexity of Massive Data Set Computations. University of California, Berkeley, 2002. [Can17] Cle\u00b4ment L. Canonne. A short note on distinguishing discrete distributions., 2017. [CB22] Graham Cormode and Akash Bharadwaj. Sample-and-threshold differential privacy: Histograms and applications. In International Conference on Artificial Intelligence and Statistics, pages 1420\u20131431. PMLR, 2022.   \n$[\\mathbf{CCD}^{+}23]$ ] Karan Chadha, Junye Chen, John Duchi, Vitaly Feldman, Hanieh Hashemi, Omid Javidbakht, Audra McMillan, and Kunal Talwar. Differentially private heavy hitter detection using federated analytics, 2023. [CD20] Rachel Cummings and David Durfee. Individual sensitivity preprocessing for data privacy. In Shuchi Chawla, editor, Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA 2020, Salt Lake City, UT, USA, January 5-8, 2020, pages 528\u2013547. SIAM, 2020. [CDK17] Bryan Cai, Constantinos Daskalakis, and Gautam Kamath. Priv\u2019it: Private and sample efficient identity testing. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 635\u2013644. PMLR, 2017.   \n[CKM+19] Cle\u00b4ment L. Canonne, Gautam Kamath, Audra McMillan, Adam D. Smith, and Jonathan R. Ullman. The structure of optimal private tests for simple hypotheses. In Moses Charikar and Edith Cohen, editors, Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019, pages 310\u2013321. ACM, 2019. [CL15] T. Tony Cai and Mark G. Low. A framework for estimation of convex functions. Statistica Sinica, 25(2):423\u2013456, 2015.   \n$[\\mathrm{CLN}^{+}23]$ Edith Cohen, Xin Lyu, Jelani Nelson, Tam\u00b4as Sarl\u00b4os, and Uri Stemmer. Optimal differentially private learning of thresholds and quasi-concave optimization. In Barna Saha and Rocco A. Servedio, editors, Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June 20-23, 2023, pages 472\u2013482. ACM, 2023. [CR12] Guillermo D. Can\u02dcas and Lorenzo Rosasco. Learning probability measures with respect to optimal transport metrics. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Le\u00b4on Bottou, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pages 2501\u20132509, 2012. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[CSS11] T.-H. Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. ACM Trans. Inf. Syst. Secur., 14(3):26:1\u201326:24, 2011. ", "page_idx": 12}, {"type": "text", "text": "[CWZ19] T. Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy: Optimal rates of convergence for parameter estimation with differential privacy. CoRR, abs/1902.04495, 2019. [CZ13] Shixi Chen and Shuigeng Zhou. Recursive mechanism: towards node differential privacy and unrestricted joins. In Kenneth A. Ross, Divesh Srivastava, and Dimitris Papadias, editors, Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2013, New York, NY, USA, June 22-27, 2013, pages 653\u2013664. ACM, 2013. [DHS15] Ilias Diakonikolas, Moritz Hardt, and Ludwig Schmidt. Differentially private learning of structured discrete distributions. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2566\u20132574, 2015.   \n$[\\mathrm{DKM}^{+}06]$ Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In International Conference on the Theory and Applications of Cryptographic Techniques, EUROCRYPT \u201906, pages 486\u2013503, St. Petersburg, Russia, 2006.   \n[DKSS23] Travis Dick, Alex Kulesza, Ziteng Sun, and Ananda Theertha Suresh. Subsetbased instance optimality in private estimation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 7992\u20138014. PMLR, 2023. [DL91] David L. Donoho and Richard C. Liu. Geometrizing Rates of Convergence, II. The Annals of Statistics, 19(2):633 \u2013 667, 1991. [DL09] Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In Michael Mitzenmacher, editor, Proceedings of the 41st Annual ACM Symposium on Theory of Computing, STOC 2009, Bethesda, MD, USA, May 31 - June 2, 2009, pages 371\u2013380. ACM, 2009.   \n[DLSV23] Trung Dang, Jasper C.H. Lee, Maoyuan Song, and Paul Valiant. Optimality in mean estimation: Beyond worst-case, beyond sub-gaussian, and beyond ${\\mathfrak{S}}1+{\\mathrm{\\alpha}}{\\mathfrak{S}}$ moments. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[DMNS17] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. Journal of Privacy and Confidentiality, 7(3):17\u201351, 2017.   \n[DNPR10] Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N. Rothblum. Differential privacy under continual observation. In Leonard J. Schulman, editor, Proceedings of the 42nd ACM Symposium on Theory of Computing, STOC 2010, Cambridge, Massachusetts, USA, 5-8 June 2010, pages 715\u2013724. ACM, 2010. [DR18] John C. Duchi and Feng Ruan. The right complexity measure in locally private estimation: It is not the fisher information. CoRR, abs/1806.05756, 2018. [DSS11] Steffen Dereich, Michael Scheutzow, and Reik Schottstedt. Constructive quantization: Approximation by empirical measures. Annales De L Institut Henri Poincareprobabilites Et Statistiques, 49:1183\u20131203, 2011. [Dud69] R. M. Dudley. The speed of mean glivenko-cantelli convergence. The Annals of Mathematical Statistics, 40(1):40\u201350, 1969. [DY95] Vladimir Dobric and Joseph E. Yukich. Asymptotics for transportation cost in high dimensions. Journal of Theoretical Probability, 8:97\u2013118, 1995. [FG15] Nicolas Fournier and Arnaud Guillin. On the rate of convergence in Wasserstein distance of the empirical measure. Probability Theory and Related Fields, 162(3-4):707, August 2015. [FLN01] Ronald Fagin, Amnon Lotem, and Moni Naor. Optimal aggregation algorithms for middleware. In Proceedings of the Twentieth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, PODS $\\mathord{\\left.\\kern-\\nulldelimiterspace}01\\right.$ , page 102\u2013113, New York, NY, USA, 2001. Association for Computing Machinery. [Fou23] Nicolas Fournier. Convergence of the empirical measure in expected wasserstein distance: non asymptotic explicit bounds in $\\bar{\\mathbb{R}}^{d}$ , 2023. [FRT03] Jittat Fakcharoenphol, Satish Rao, and Kunal Talwar. A tight bound on approximating arbitrary metrics by tree metrics. In Proceedings of the Thirty-Fifth Annual ACM Symposium on Theory of Computing, STOC $\\cdot_{03}$ , page 448\u2013455, New York, NY, USA, 2003. Association for Computing Machinery.   \n$[\\mathrm{GHK}^{+}23]$ Badih Ghazi, Junfeng He, Kai Kohlhoff, Ravi Kumar, Pasin Manurangsi, Vidhya Navalpakkam, and Nachiappan Valliappan. Differentially private heatmaps. In Brian Williams, Yiling Chen, and Jennifer Neville, editors, Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 7696\u20137704. AAAI Press, 2023. [GJK21] Jennifer Gillenwater, Matthew Joseph, and Alex Kulesza. Differentially private quantiles. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 3713\u20133722. PMLR, 2021.   \n[GKN20] Tomer Grossman, Ilan Komargodski, and Moni Naor. Instance Complexity and Unlabeled Certificates in the Decision Tree Model. In Thomas Vidick, editor, 11th Innovations in Theoretical Computer Science Conference (ITCS 2020), volume 151 of Leibniz International Proceedings in Informatics (LIPIcs), pages 56:1\u201356:38, Dagstuhl, Germany, 2020. Schloss Dagstuhl \u2013 Leibniz-Zentrum fu\u00a8r Informatik.   \n[HKM22] Samuel B. Hopkins, Gautam Kamath, and Mahbod Majid. Efficient mean estimation with pure differential privacy via a sum-of-squares exponential mechanism. In Stefano Leonardi and Anupam Gupta, editors, STOC \u201922: 54th Annual ACM SIGACT Symposium on Theory of Computing, Rome, Italy, June 20 - 24, 2022, pages 1406\u20131417. ACM, 2022.   \n[HKMN23] Samuel B. Hopkins, Gautam Kamath, Mahbod Majid, and Shyam Narayanan. Robustness implies privacy in statistical estimation. In Barna Saha and Rocco A. Servedio, editors, Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June 20-23, 2023, pages 497\u2013506. ACM, 2023. [HLY21] Ziyue Huang, Yuting Liang, and Ke Yi. Instance-optimal mean estimation under differential privacy. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 25993\u201326004, 2021. [HO19] Yi Hao and Alon Orlitsky. Doubly-competitive distribution estimation. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2614\u2013 2623. PMLR, 2019. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "[HVZ23] Yiyun He, Roman Vershynin, and Yizhe Zhu. Algorithmically effective differentially private synthetic data, 2023. ", "page_idx": 14}, {"type": "text", "text": "[KDH23] Rohith Kuditipudi, John C. Duchi, and Saminul Haque. A pretty fast algorithm for adaptive private mean estimation. In Gergely Neu and Lorenzo Rosasco, editors, The Thirty Sixth Annual Conference on Learning Theory, COLT 2023, 12-15 July 2023, Bangalore, India, volume 195 of Proceedings of Machine Learning Research, pages 2511\u20132551. PMLR, 2023.   \n$[\\mathrm{KLM}^{+}20]$ ] Haim Kaplan, Katrina Ligett, Yishay Mansour, Moni Naor, and Uri Stemmer. Privately learning thresholds: Closing the exponential gap. In Jacob D. Abernethy and Shivani Agarwal, editors, Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria], volume 125 of Proceedings of Machine Learning Research, pages 2263\u20132285. PMLR, 2020.   \n[KLSU19] Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan R. Ullman. Privately learning high-dimensional distributions. In Alina Beygelzimer and Daniel Hsu, editors, Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, volume 99 of Proceedings of Machine Learning Research, pages 1853\u20131902. PMLR, 2019.   \n[KMS22a] Gautam Kamath, Argyris Mouzakis, and Vikrant Singhal. New lower bounds for private estimation and a generalized fingerprinting lemma. In NeurIPS, 2022.   \n$[\\mathrm{KMS}^{+}22\\mathrm{b}]$ ] Gautam Kamath, Argyris Mouzakis, Vikrant Singhal, Thomas Steinke, and Jonathan R. Ullman. A private and computationally-efficient estimator for unbounded gaussians. In Po-Ling Loh and Maxim Raginsky, editors, Conference on Learning Theory, 2-5 July 2022, London, UK, volume 178 of Proceedings of Machine Learning Research, pages 544\u2013572. PMLR, 2022.   \n[KMV22] Pravesh Kothari, Pasin Manurangsi, and Ameya Velingker. Private robust estimation by stabilizing convex relaxations. In Po-Ling Loh and Maxim Raginsky, editors, Conference on Learning Theory, 2-5 July 2022, London, UK, volume 178 of Proceedings of Machine Learning Research, pages 723\u2013777. PMLR, 2022.   \n[KNRS13] Shiva Prasad Kasiviswanathan, Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. Analyzing graphs with node differential privacy. In Amit Sahai, editor, Theory of Cryptography - 10th Theory of Cryptography Conference, TCC 2013, Tokyo, Japan, March 3-6, 2013. Proceedings, volume 7785 of Lecture Notes in Computer Science, pages 457\u2013476. Springer, 2013. [KSS22] Haim Kaplan, Shachar Schnapp, and Uri Stemmer. Differentially private approximate quantiles. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesva\u00b4ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 10751\u201310761. PMLR, 2022.   \n[KSSU19] Gautam Kamath, Or Sheffet, Vikrant Singhal, and Jonathan R. Ullman. Differentially private algorithms for learning mixtures of separated gaussians. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alche\u00b4-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 168\u2013180, 2019. [KSU20] Gautam Kamath, Vikrant Singhal, and Jonathan R. Ullman. Private mean estimation of heavy-tailed distributions. In Jacob D. Abernethy and Shivani Agarwal, editors, Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria], volume 125 of Proceedings of Machine Learning Research, pages 2204\u2013 2235. PMLR, 2020. [KU20] Gautam Kamath and Jonathan R. Ullman. A primer on private statistics. CoRR, abs/2005.00010, 2020. [KV18] Vishesh Karwa and Salil P. Vadhan. Finite sample differentially private confidence intervals. In Anna R. Karlin, editor, 9th Innovations in Theoretical Computer Science Conference, ITCS 2018, January 11-14, 2018, Cambridge, MA, USA, volume 94 of LIPIcs, pages 44:1\u201344:9. Schloss Dagstuhl - Leibniz-Zentrum fu\u00a8r Informatik, 2018. [Lei20] Jing Lei. Convergence and concentration of empirical measures under Wasserstein distance in unbounded functional spaces. Bernoulli, 26(1):767 \u2013 798, 2020.   \n[LKO22] Xiyang Liu, Weihao Kong, and Sewoong Oh. Differential privacy and robust statistics in high dimensions. In Po-Ling Loh and Maxim Raginsky, editors, Conference on Learning Theory, 2-5 July 2022, London, UK, volume 178 of Proceedings of Machine Learning Research, pages 1167\u20131246. PMLR, 2022.   \n$[\\mathbf{M}\\mathbf{J}\\mathbf{T}^{+}22]$ Audra McMillan, Omid Javidbakht, Kunal Talwar, Elliot Briggs, Mike Chatzidakis, Junye Chen, John Duchi, Vitaly Feldman, Yusuf Goren, Michael Hesse, Vojta Jina, Anil Katti, Albert Liu, Cheney Lyford, Joey Meyer, Alex Palmer, David Park, Wonhee Park, Gianni Parsa, Paul Pelzl, Rehan Rishi, Congzheng Song, Shan Wang, and Shundong Zhou. Private federated statistics in an interactive setting. arXiv preprint arXiv:2211.10082, 2022.   \n[MSU22] Audra McMillan, Adam D. Smith, and Jonathan R. Ullman. Instance-optimal differentially private estimation. CoRR, abs/2210.15819, 2022. [Nar23] Shyam Narayanan. Better and simpler lower bounds for differentially private statistical estimation. CoRR, abs/2310.06289, 2023. [NRS07] Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. Smooth sensitivity and sampling in private data analysis. In David S. Johnson and Uriel Feige, editors, Proceedings of the 39th Annual ACM Symposium on Theory of Computing, San Diego, California, USA, June 11-13, 2007, pages 75\u201384. ACM, 2007.   \n[NWB19] Jonathan Niles-Weed and Quentin Berthet. Minimax estimation of smooth densities in wasserstein distance. The Annals of Statistics, 2019. [OS15] Alon Orlitsky and Ananda Theertha Suresh. Competitive distribution estimation: Why is good-turing good. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2143\u20132151, 2015.   \n[QYL12] Wahbeh Qardaji, Weining Yang, and Ninghui Li. Differentially private grids for geospatial data. Proceedings - International Conference on Data Engineering, 09 2012. [Rou21] Tim Roughgarden. Beyond the Worst-Case Analysis of Algorithms. Cambridge University Press, 2021. [RS16] Sofya Raskhodnikova and Adam D. Smith. Lipschitz extensions for node-private graph statistics and the generalized exponential mechanism. In Irit Dinur, editor, IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 495\u2013504. IEEE Computer Society, 2016. [Sin23] Vikrant Singhal. A polynomial time, pure differentially private estimator for binary product distributions. CoRR, abs/2304.06787, 2023. [SP19] Shashank Singh and Barnaba\u00b4s Po\u00b4czos. Minimax distribution estimation in wasserstein distance, 2019.   \n$[\\mathrm{TCK}^{+}22]$ Eliad Tsfadia, Edith Cohen, Haim Kaplan, Yishay Mansour, and Uri Stemmer. Friendlycore: Practical differentially private aggregation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesva\u00b4ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 21828\u201321863. PMLR, 2022. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "[vdV97] A. W. van der Vaart. Superefficiency, pages 397\u2013410. Springer New York, New York, NY, 1997. ", "page_idx": 16}, {"type": "text", "text": "[Vov09] Vladimir Vovk. Superefficiency from the Vantage Point of Computability. Statistical Science, 24(1):73 \u2013 86, 2009. [VV16] Gregory Valiant and Paul Valiant. Instance optimal learning of discrete distributions. In Daniel Wichs and Yishay Mansour, editors, Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 142\u2013155. ACM, 2016. [WB19] Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of empirical measures in wasserstein distance. Bernoulli, 25(4 A):2620\u20132648, 2019. [Wol65] J. Wolfowitz. Asymptotic efficiency of the maximum likelihood estimator. Theory of Probability & Its Applications, 10(2):247\u2013260, 1965. $[Z\\mathrm{KM}^{+}20]$ ] Wennan Zhu, Peter Kairouz, Brendan McMahan, Haicheng Sun, and Wei Li. Federated heavy hitters discovery with differential privacy. In International Conference on Artificial Intelligence and Statistics, pages 3837\u20133847. PMLR, 2020. [ZXX16] Jun Zhang, Xiaokui Xiao, and Xing Xie. Privtree: A differentially private algorithm for hierarchical decompositions. In Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916, page 155\u2013170, New York, NY, USA, 2016. Association for Computing Machinery. ", "page_idx": 16}, {"type": "text", "text": "Organization of Appendices ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A Preliminaries 19   \nA.1 Distribution Distances 19   \nA.2 Differential Privacy 20 ", "page_idx": 17}, {"type": "text", "text": "B On Instance Optimality 20 ", "page_idx": 17}, {"type": "text", "text": "B.1 Local Estimation Rates 21   \nB.2 Locally Minimal Algorithms 23   \nB.3 Relaxed Definitions . 25 ", "page_idx": 17}, {"type": "text", "text": "C Additional Related Work 25 ", "page_idx": 17}, {"type": "text", "text": "D Distribution Estimation on Hierarchically Separated Trees 27 ", "page_idx": 17}, {"type": "text", "text": "D.1 Preliminaries on Hierarchically Separated Trees . 27 ", "page_idx": 17}, {"type": "text", "text": "D.2 The Target Estimation Rate . 28   \nD.2.1 Reduction to Estimation in TV distance of Discrete Distributions . 29   \nD.2.2 Characterizing Target Estimation Rate for Discrete Distributions 29   \nD.3 An $\\varepsilon$ -DP Distribution Estimation Algorithm 33 ", "page_idx": 17}, {"type": "text", "text": "E Instance Optimal Density Estimation on $\\mathbb{R}$ in Wasserstein distance 36 ", "page_idx": 17}, {"type": "text", "text": "E.1 General Lower Bound . 37   \nE.1.1 The Privacy Term . 37   \nE.1.2 Empirical Term 39   \nE.2 Upper Bound 41   \nE.2.1 Algorithm for density estimation 42 ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "F Experiment Details 44 ", "page_idx": 17}, {"type": "text", "text": "G Proofs for Section D 44 ", "page_idx": 17}, {"type": "text", "text": "H Local Minimality in the High Dimensional Setting 50 ", "page_idx": 17}, {"type": "text", "text": "I Differentially Private Quantiles 52 ", "page_idx": 17}, {"type": "text", "text": "J.1 Omitted Proofs in Section E.1.1 54   \nJ.2 Omitted proofs in Section E.1.2 55   \nJ.3 Omitted Proofs in Section E.2 58   \nJ.4 Local Minimality in the One-Dimensional Setting . 61 ", "page_idx": 17}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For all distributions $P$ , we will use $f_{P}$ to denote the density of $P$ (when it exists) and $F_{P}$ to denote the cumulative distribution function of $P$ . Given a space $\\mathcal{X}$ , let $\\Delta(\\mathcal{X})$ be the set of distributions on the space $\\mathcal{X}$ . Given a logical statement $a$ , let $\\chi_{a}\\,=\\,0$ if $a$ is false and 1 if $a$ is true. For example, $\\chi_{0=0}=1$ and $\\chi_{0=1}=0$ . ", "page_idx": 18}, {"type": "text", "text": "A number of distances between distributions are important in this work. We start by defining the infinity divergence, which is important in the notion of instance optimality we use. ", "page_idx": 18}, {"type": "text", "text": "Definition A.1 ( $D_{\\infty}$ -divergence). Given two distributions $P$ and $Q$ with the same support, the $\\infty$ -R\u00b4enyi divergence $\\begin{array}{r l r}{D_{\\infty}(P,Q)}&{=}&{\\ln\\operatorname*{sup}_{t}\\operatorname*{max}\\left\\{\\frac{P(t)}{Q(t)},\\frac{Q(t)}{P(t)}\\right\\}}\\end{array}$ PQ ((tt)), PQ ((tt)) , if P and Q are discrete, and $\\begin{array}{r}{D_{\\infty}(P,Q)=\\ln\\operatorname*{sup}_{t}\\operatorname*{max}\\left\\{\\frac{f_{P}(t)}{f_{Q}(t)},\\frac{f_{Q}(t)}{f_{P}(t)}\\right\\}}\\end{array}$ , if $P$ and $Q$ are continuous distributions on $\\mathbb{R},$ and have density functions. If $P$ and $Q$ don\u2019t have the same support, then $D_{\\infty}(P,Q)=\\infty$ . ", "page_idx": 18}, {"type": "text", "text": "We will use $\\mathrm{KL}(P,Q)$ to denote the KL-divergence, $H^{2}(P,Q)$ to denote the squared Hellinger divergence and $\\mathrm{TV}(P,Q)$ to denote the total variation distance, defined later. ", "page_idx": 18}, {"type": "text", "text": "Wasserstein Distance: The error metric that we use to judge our performance on the density estimation task is 1-Wasserstein distance (that we will call just Wasserstein distance where it is clear from context). In this subsection, we define Wasserstein distance. ", "page_idx": 18}, {"type": "text", "text": "Definition A.2. For any separable metric space $(E,D)$ , let $P,Q$ represent Borel measures on $E$ . Then, the 1-Wasserstein distance between $P,Q$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal W(P,Q)=\\operatorname*{inf}_{\\pi}\\int_{E}\\int_{E}D(t,t_{0})\\pi(x,x_{0}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the infimum is over all measures $\\pi$ on the product space $E\\times E$ with marginals $P$ and $Q$ respectively. ", "page_idx": 18}, {"type": "text", "text": "Finally, for one dimensional real spaces where the metric of interest is $\\ell_{1}$ norm, we will use the following equivalent formulation of Wasserstein distance extensively. ", "page_idx": 18}, {"type": "text", "text": "Lemma A.3 (Wasserstein formula over $\\mathbb{R}$ ). Let $P,Q$ represent probability distributions on $\\mathbb{R}$ with finite expectation. Then, the 1-Wasserstein distance between $P,Q$ is equal to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal W(P,Q)=\\int_{\\infty}^{\\infty}|F_{P}(t)-F_{Q}(t)|d t,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the $F(\\cdot)$ represents the cumulative distribution function. ", "page_idx": 18}, {"type": "text", "text": "Given an metric space $\\mathcal{X}$ , the Wasserstein metric is a well-defined metric on the set of the probability distributions over $\\mathcal{X}$ . ", "page_idx": 18}, {"type": "text", "text": "A.1 Distribution Distances ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A number of other distances between distributions are used in this work. ", "page_idx": 18}, {"type": "text", "text": "Definition A.4 ( $K L$ -divergence). Given two distributions $P$ and $Q$ with $\\L{s u p p}(P)\\subseteq\\L{s u p p}(Q),$ , the $K L$ divergence $\\begin{array}{r}{K L(P,Q)=\\sum_{t\\in s u p p(P)}P(t)\\ln\\frac{P(t)}{Q(t)}}\\end{array}$ , if $P$ and $Q$ are discrete, and $K L(P,Q)=$ $\\begin{array}{r}{\\int_{t\\in\\mathbb{R}:f_{P}(t)>0}f_{P}(t)\\ln\\frac{f_{P}(t)}{f_{Q}(t)}d t}\\end{array}$ if $P$ and $Q$ are distributions on $\\mathbb{R}$ , and have density functions. If $s u p p(P)\\nsubseteq s u p p(Q)$ , then $K L(P,Q)=\\infty$ . ", "page_idx": 18}, {"type": "text", "text": "Definition A.5 (Hellinger\u221a distance). Given two distributions $P$ and $Q$ , the Hellinger distance $\\begin{array}{c c l}{{H(P,Q)}}&{{=}}&{{\\frac{1}{\\sqrt{2}}\\|\\sqrt{P}\\;-\\;\\sqrt{Q}\\|_{2}}}\\end{array}$ (where we think of $P$ and $Q$ as vectors representing the probability masses, and the square root being component-wise.), if $P$ and $Q$ are discrete. If $P$ and $Q$ are distributions on $\\mathbb{R}$ , and have density functions, then $\\begin{array}{r l}{H(P,Q)}&{{}=}\\end{array}$ $\\begin{array}{r}{\\frac{1}{\\sqrt{2}}\\sqrt{\\int_{t\\in\\mathbb{R}:f_{P}(t)>0}(\\sqrt{f_{P}(t)}-\\sqrt{f_{Q}(t)})^{2}d t}.}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Note that we use $H^{2}(P,Q)$ to represent the squared Hellinger distance. Next, we define total variation distance, which will come up in our high-dimensional results. ", "page_idx": 18}, {"type": "text", "text": "Definition A.6 (Total Variation distance). Given two discrete distributions $P$ and $Q$ , the Total Variation distance $\\begin{array}{r}{\\dot{T V}(P,Q)=\\frac{1}{2}\\|P-Q\\|_{1}}\\end{array}$ , (where we think of $P$ and $Q$ as vectors representing the probability masses). More generally, for any two probability measures $P$ and $Q$ defined on $(\\Omega,{\\mathcal{F}})$ , the total variation distance is defined as $\\operatorname*{sup}_{A\\in{\\mathcal{F}}}|P(A)-Q(A)|$ where $P(A)$ represents the probability of $A$ under measure $P$ and likewise for $Q$ . ", "page_idx": 19}, {"type": "text", "text": "We use the following relationship between Hellinger distance and $\\mathrm{KL}$ divergence. ", "page_idx": 19}, {"type": "text", "text": "Lemma A.7. For all distributions $P,Q$ such that $K L$ -divergence of $P,Q$ is well defined, we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\nH^{2}(P,Q)\\leq K L(P,Q),\\;\\;H^{2}(P,Q)\\leq T V(P,Q)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.2 Differential Privacy ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma A.8 (Post-Processing [DMNS17]). If Algorithm $A:\\mathcal{X}^{n}\\to\\mathcal{Y}$ is $(\\varepsilon,\\delta)$ -differentially private, and $B:\\mathcal{V}\\rightarrow\\mathcal{Z}$ is any randomized function, then the algorithm $B\\circ A$ is $(\\varepsilon,\\delta)$ -differentially private. ", "page_idx": 19}, {"type": "text", "text": "Secondly, differential privacy is robust to adaptive composition. ", "page_idx": 19}, {"type": "text", "text": "Lemma A.9 (Composition of $(\\varepsilon,\\delta)$ -differential privacy [DMNS17]). If $\\boldsymbol{\\mathcal{A}}$ is an adaptive composition of $m$ differentially private algorithms $A_{1},\\ldots,A_{m}.$ , where $\\mathcal{A}_{j}$ is $(\\varepsilon_{j},\\delta_{j})$ differentially private, then $\\boldsymbol{\\mathcal{A}}$ is $\\left(\\sum_{j}\\varepsilon_{j},\\sum_{j}\\delta_{j}\\right)$ -differentially private. ", "page_idx": 19}, {"type": "text", "text": "Finally, we discuss the Laplace mechanism, which we will use in one of our algorithms. ", "page_idx": 19}, {"type": "text", "text": "Definition A.10 ( $\\ell_{1}$ -Sensitivity). The $\\ell_{1}$ -sensitivity of a function $f:\\mathcal{X}^{n}\\to\\mathbb{R}^{d}$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{f}=\\operatorname*{max}_{\\mathbf{x},\\mathbf{x}^{\\prime}\\in\\mathcal{X}^{n}}\\|f(\\mathbf{x})-f(\\mathbf{x}^{\\prime})\\|_{1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma A.11 (Laplace Mechanism). Let $f:\\mathcal{X}^{n}\\to\\mathbb{R}^{d}$ be a function with $\\ell_{1}$ -sensitivity $\\Delta_{f}$ . Then the Laplacian mechanism is algorithm ", "page_idx": 19}, {"type": "equation", "text": "$$\nA_{f}(\\mathbf{x})=f(\\mathbf{x})+(Z_{1},\\ldots,Z_{d}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{Z_{i}\\sim L a p\\left(\\frac{\\Delta_{f}}{\\varepsilon}\\right)}\\end{array}$ (and $Z_{1},\\ldots,Z_{d}$ are mutually independent). Algorithm $\\boldsymbol{\\mathcal{A}}_{f}$ is $\\varepsilon$ -DP. ", "page_idx": 19}, {"type": "text", "text": "B On Instance Optimality ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we discuss the notion of instance optimality, and argue that it provides a useful benchmark that captures the idea of going beyond the worst case. The notion of instance optimality we propose can be see as a generalisation of the hardest one-dimensional subproblem, or hardest local alternative introduced by [CL15]. Suppose we have a family of distributions $\\mathcal{P}\\subset\\Delta(\\mathcal{X})$ on a space $\\mathcal{X}$ and our goal is to learn the parameter $\\theta:\\mathcal{P}\\rightarrow\\mathcal{M}$ where $\\mathcal{M}$ is a metric space with metric $d$ . Given an estimation algorithm $A:\\mathcal{X}^{n}\\to\\mathcal{M}$ , we can define the estimation rate $3$ of $\\boldsymbol{\\mathcal{A}}$ to be the function $\\mathcal{R}_{\\mathcal{A},n}:\\mathcal{P}\\rightarrow\\mathbb{R}_{+}$ where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{R}_{{A},n}(P)=\\mathbb{E}_{D\\sim P^{n}}[d(\\theta(P),\\mathcal{A}(D))].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the estimation rate is a function of the distribution $P$ , the estimation rate of an algorithm may be lower at \u201ceasy\u201d distributions and larger at \u201charder\u201d distributions. As a classic example, consider the estimation rate of Bernoulli parameter estimation where $\\boldsymbol{\\mathcal{A}}$ simply outputs the empirical mean. Then $\\mathcal{R}_{\\mathcal{A},n}(8\\mathbf{e}\\mathbf{r}(p))=\\operatorname*{min}\\{p(1-p),\\sqrt{p(1-p)/n}\\},$ , so this algorithm performs better when the Bernoulli parameter is close to 0 or 1, and has it\u2019s worst case error when $p=1/2$ . ", "page_idx": 19}, {"type": "text", "text": "Cai and Low [CL15] proposed three desiderata that a target estimation rate $\\mathcal{R}_{n}:\\Delta(\\mathcal{X})\\,\\rightarrow\\,\\mathbb{R}_{+}$ should satisfy in order to be a meaningful benchmark; ", "page_idx": 19}, {"type": "text", "text": "1. $\\mathcal{R}_{n}(P)$ varies significantly across $\\mathcal{P}$ ", "page_idx": 20}, {"type": "text", "text": "2. ${\\mathcal{R}}_{n}$ is an achievable estimation rate; there exists an algorithm $\\boldsymbol{\\mathcal{A}}$ and constant $\\alpha$ such that $\\mathcal{R}_{{A},n}(P)\\leq\\alpha\\mathcal{R}_{n}(P)$ for all $P\\in\\mathcal P$   \n3. Outperforming the benchmark ${\\mathcal{R}}_{n}$ at one distribution leads to worse performance at another distribution. ", "page_idx": 20}, {"type": "text", "text": "In this section we will discuss the definition of instance optimality we will use in this work by defining the target estimation rate that will serve as our benchmark estimation rate. The main theorems of this paper establish that our chosen benchmark achieves desiderata 1 and 2 above. It is not immediately obvious that desiderata 3 holds. We will show in Section B.2, through the introduction of a related notion of instance optimality which we call local minimality, that desiderata 3 holds in many important settings, including the problem studied in this paper. ", "page_idx": 20}, {"type": "text", "text": "B.1 Local Estimation Rates ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We will start by defining a target estimation rate. We\u2019ll say an algorithm is $\\alpha$ -instance optimal if it uniformly achieves this target estimation rate up to a multiplicative $\\alpha$ factor. For each distribution $P\\in\\mathcal P$ , we define a neighbourhood ${\\mathcal{N}}(P)$ . ", "page_idx": 20}, {"type": "text", "text": "Definition B.1. Given a function ${\\mathcal{N}}:{\\mathcal{P}}\\to\\mathfrak{P}({\\mathcal{P}})$ , where $\\mathfrak{P}(\\mathcal{P})$ is the power set of $\\mathcal{P}$ , we define the optimal estimation rate with respect to $\\mathcal{N}$ to be: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{N},n}(P)=\\operatorname*{min}_{A}\\operatorname*{sup}_{Q\\in\\mathcal{N}(P)}\\mathcal{R}_{A,n}(Q).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "An algorithm $\\boldsymbol{\\mathcal{A}}$ is $\\alpha$ -instance optimal with respect to $\\mathcal{N}$ if for all $P\\in\\mathcal P$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}_{{A},n}(P)\\leq\\alpha\\mathcal{R}_{{N},n}(P)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If an algorithm $\\boldsymbol{\\mathcal{A}}$ uniformly achieves the optimal estimation rate wrt a function $\\mathcal{N}$ , then this implies that for all distributions $P$ , the error of the algorithm $\\boldsymbol{\\mathcal{A}}$ on $P$ is competitive with an algorithm that is told the additional information that the distribution is in ${\\mathcal{N}}(P)$ . Given a function $\\mathcal{N}$ , it is possible that there does not exist an algorithm that uniformly achieves ${\\mathcal{R}}_{{\\mathcal{N}},n}$ . For example, as discussed in the introduction, if $\\mathcal{N}(P)=\\bar{\\{}P\\}$ , then ${\\mathcal{R}}_{{\\mathcal{N}},n}$ is not uniformly achievable. Conversely, if ${\\mathcal{N}}(P)$ is not chosen carefully, then the estimation rate ${\\mathcal{R}}_{{\\mathcal{N}},n}$ may not define a meaningful benchmark; e.g. an estimation rate that adapts to easy instances. ", "page_idx": 20}, {"type": "text", "text": "A different formalization may be more probabilistic: the algorithm designer may have in mind a distribution $\\mathcal{D}$ over distributions that they care about, and their objective may be to minimize $\\mathbb{E}_{P\\sim\\mathcal{D}}[\\mathcal{R}_{{A},{n}}(P)]$ . Suppose that for the $\\mathcal{A}^{\\star}$ chosen by the algorithm designer, and for our neighborhood map $\\mathcal{N}$ , the function $\\mathcal{R}_{\\mathcal{A}^{\\star},n}(P)$ does not vary too much over ${\\mathcal{N}}(P)$ on average. Formally, let ", "page_idx": 20}, {"type": "equation", "text": "$$\nd i s c_{A^{\\star}}^{N}(P)=\\operatorname*{sup}_{P^{\\prime}\\in\\mathcal{N}(P)}(\\mathcal{R}_{A^{\\star},n}(P^{\\prime})-\\mathcal{R}_{A^{\\star},n}(P)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and let $\\overline{{d i s c}}_{\\mathcal{A}^{\\star}}^{N}(P)=\\mathbb{E}_{P\\sim\\mathcal{D}}[d i s c_{\\mathcal{A}^{\\star}}^{N}(P)]$ . Then for any algorithm $\\boldsymbol{\\mathcal{A}}$ that is $\\alpha$ -instance optimal with respect to $\\mathcal{N}$ , we can write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{P\\sim\\mathcal{D}}{\\mathbb{E}}[\\mathcal{R}_{A,n}(P)]\\leq\\alpha\\cdot\\underset{P\\sim\\mathcal{D}}{\\mathbb{E}}[\\underset{P^{\\prime}\\in\\mathcal{N}(P)}{\\operatorname*{sup}}\\mathcal{R}_{A^{\\star},n}(P^{\\prime})]}\\\\ &{\\qquad\\qquad\\qquad=\\alpha\\cdot\\underset{P\\sim\\mathcal{D}}{\\mathbb{E}}[\\mathcal{R}_{A^{\\star},n}(P)+d i s c_{A^{\\star}}^{\\mathcal{N}}(P))]}\\\\ &{\\qquad\\qquad=\\alpha\\cdot\\underset{P\\sim\\mathcal{D}}{\\mathbb{E}}[\\mathcal{R}_{A^{\\star},n}(P)]+\\alpha\\cdot\\underset{P\\sim\\mathcal{D}}{\\mathbb{E}}[d i s c_{A^{\\star}}^{\\mathcal{N}}(P))]}\\\\ &{\\qquad\\qquad=\\alpha\\cdot\\left(\\underset{P\\sim\\mathcal{D}}{\\mathbb{E}}[\\mathcal{R}_{A^{\\star},n}(P)]+\\overline{{d i s c_{A^{\\star}}^{\\mathcal{N}}(P)}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In other words, as long as the algorithm $\\mathcal{A}^{\\star}$ \u2019s performance is relatively constant over ${\\mathcal{N}}(P)$ on average over the distribution of interest, the instance optimal algorithm (that is not tailored to $\\mathcal{D}$ ) is competitive with $\\mathcal{A}^{\\star}$ . A similar result holds for a multiplicative definition of disc. ", "page_idx": 20}, {"type": "text", "text": "This discussion can help guide the choice of the neighborhood function that is appropriate for a particular application. In the case of density estimation in the Wasserstein distance, we will define $\\bar{\\mathcal{N}}(P)$ be a small $D_{\\infty}$ ball around $P$ . We believe this captures the kind of domain information an algorithm designer may have. E.g. one may have a small amount of public data samples, in which case the posterior over distributions in a $D_{\\infty}$ ball will be relatively constant. If the algorithm designer\u2019s custom algorithm needs to do well for all distributions in this set, an instance-optimal algorithm will be competitive with this custom algorithm. ", "page_idx": 21}, {"type": "text", "text": "Previous work in instance optimality has largely focused on two notions of neighborhood. In [FLN01, ABC17, VV16, OS15], where the objects of interest are discrete subsets with no a priori structure, it is natural to ask that the algorithm work well for any permutation of the inputs. For example, if the goal is to compute the set of maximal points from a 2-d point set, the algorithm designer would typically want an algorithm that works well for any permutation of the set of input points. In our setting where the points of interest have a metric structure, this is not an appropriate notion. In fact, even for the discrete case studied in Appendix D.2.2, permutation invariance cannot capture natural prior beliefs that may arise in practice. For example, for power-law distributions that one often sees in private learning applications $[Z\\mathrm{KM}^{+}20$ , CB22, $\\mathrm{CCD}^{+}23]$ , a small number of samples are sufficient to get a good estimate of the heavy bins, and rule out a large fraction of permutations of the input space. ", "page_idx": 21}, {"type": "text", "text": "A second line of work arising from the statistics literature [CL15] has looked at defining instanceoptimality with respect to neighborhoods of size 2. While this approach has been very successful for many problems, we find it inappropriate for density estimation (outside of density estimation on $\\mathbb{R}$ ) as neighborhoods of size two are too weak to capture the difficulty of problems of interest. Even in the simple case of discrete distributions, this neighborhood is provably insufficient to get instanceoptimality results with any $o(K)$ competitive ratio. Indeed, for any two given distributions on $[K]$ with TV distance $\\alpha$ , $\\tilde{O}\\bigl(\\scriptstyle{\\frac{1}{\\alpha^{2}}}\\bigr)$ samples suffice to distinguish them, whereas learning a near uniform distribution on $K$ atoms requires $\\Omega(K)$ samples. In the private setting, the need to use multiple distributions to prove lower bounds is well-studied. Our approach shares this similarity of using a multi-instance lower bounding argument with packing lower bounds in privacy, and local Fano\u2019s and Le Cam\u2019s methods in statistics. Our work shows that some of the same lower bounding techniques can be used to prove instance-optimality results with respect to natural neighborhood maps, going well beyond the the worst-case results those works prove. ", "page_idx": 21}, {"type": "text", "text": "In the special case of density estimation in the Wasserstein distance on $\\mathbb{R}$ , instance optimality with respect to neighborhoods of size 2 is achievable. In the standard version of this benchmark metric, $\\mathcal{N}(P)\\,=\\,\\bar{\\{}P,Q_{P}\\}$ where $Q_{P}$ can be any distribution and is chosen to maximise $\\mathcal{R}_{\\mathcal{N},n}(P)$ . However, this notion may not be an appropriate notion of instance optimality by itself. To see this, consider a distribution $P$ supported on an interval $[a,b]$ . Moving a small amount of mass from one end of the interval to the other would create an indistinguishable distribution that is far from $P$ in Wasserstein distance, and a hypothesis testing argument can be used to show that the target estimation rate defined above (for the hardest one-d sub problem) depends on the interval size $b-a$ . This implies that the adaptivity of algorithms to support size of the distribution (crucial in Wasserstein estimation) is not captured by this notion of instance optimality. Instead, we add a further restriction to the definition to make it more appropriate for our setting; we only consider distributions $Q$ that are in a small $D_{\\infty}$ ball around $P$ $\\bar{(D_{\\infty}}(P,Q)\\,\\leq\\,\\ln2)$ , and ask that an algorithm is competitive with an algorithm that is told the additional information that $P\\,\\in\\,\\{P,Q\\}$ (in the worst case over distributions $Q$ that are in this $D_{\\infty}$ ball). That is, we define the benchmark estimation rate to be ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathrm{loc},n}(P)=\\underset{Q:D_{\\infty}(P,Q)\\leq\\ln2}{\\operatorname*{sup}}\\underset{A}{\\!\\!\\operatorname*{min}}\\{\\mathcal{R}_{{\\mathcal{A}},n}(Q),\\mathcal{R}_{{\\mathcal{A}},n}(P)\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that all such distributions $Q$ have the same support as $P$ , which allows us to capture the adaptivity of algorithms to the support size of the distribution. Specifically, we define the following target estimation rate in the one-dimensional setting. In the case of estimating distributions on a bounded subset of $\\mathbb{R}$ , we will show that this error rate is achievable, up to logarithmic factors. ", "page_idx": 21}, {"type": "text", "text": "We also note that our notion of instance optimality more naturally captures the accuracy of algori\u221athms even for basic tasks. Note that for the Bernoulli case, our technique achieves a bound of $\\begin{array}{r}{\\frac{\\sqrt{p(1-p)}}{\\sqrt{n}}+\\operatorname*{min}\\{p,1-p,\\frac{1}{\\varepsilon n}\\}}\\end{array}$ which also appear to be better than the instance-optimal lower bounds in [MSU22], which take the form $\\textstyle{\\frac{\\sqrt{p(1-p)}}{\\sqrt{n}}}+{\\frac{1}{\\varepsilon n}}$ . This apparent contradiction can be explained by the the use in [MSU22] of the hardest-one dimensional sub-problem to define the instance-optimal rate, i.e., ${\\mathcal{N}}(P)$ is $\\{P,Q\\}$ for a worst-case Bernoulli $Q$ . On the other hand, the notion of instanceoptimality we use would only consider Bernoullis $Q$ such that $D_{\\infty}(P,Q)\\leq\\ln2$ . When $p$ is close to 0, the lower bound in [MSU22] would on this instance consider $Q$ to be $\\begin{array}{r}{B e r n(p+\\frac{1}{\\varepsilon n})}\\end{array}$ , which can have a large $D_{\\infty}$ -distance from $P$ , and so isn\u2019t in the neighborhood used in our notion of instanceoptimality. Hence, the target rate one would obtain from our definition is smaller when $p$ is close to 0. Our algorithm can achieve this improved rate, as it is likely to output 0 as an estimate of $p$ in this case, pushing small counts down to zero. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Recent differentially private algorithms such as those in [HLY21, DKSS23] have shown instanceoptimality for problems such as mean estimation. Relatedly, other works have designed algorithms that adapt to the local/smooth/deletion sensitivity of the underlying function. An instance in these works in a dataset rather than a distribution, and it is not clear how to extend the corresponding notion of neighborhood to our setting. Our neighborhood notion perhaps comes closest to the deletion neighborhoods considered in some of these works. ", "page_idx": 22}, {"type": "text", "text": "Finally, we remark that while we have stated our results as being competitive with the worst-case instance in ${\\mathcal{N}}(P)$ , they apply for the average case over a specific distribution over ${\\mathcal{N}}(P)$ . Since that specific distribution is adversarial, we don\u2019t view this version as more natural than the worst case. ", "page_idx": 22}, {"type": "text", "text": "Given that we are focusing on private estimation, we will use use $\\mathcal{R}_{\\mathrm{loc},n,\\varepsilon}$ to denote the version of Eqn 4 where the minimum is taken over all $\\varepsilon$ -DP mechanisms, and $\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}$ to define the optimal $\\varepsilon$ -DP estimation rate, i.e. Eqn 3 where the minimum is taken over all $\\varepsilon$ -DP mechanisms. ", "page_idx": 22}, {"type": "text", "text": "B.2 Locally Minimal Algorithms ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we address the third desiderata of [CL15]. An important concept in statistics is that of efficiency of an estimator, which informally compares the rate of convergence of the estimator with a benchmark that in general is not beatable. This idea has been used to argue that for some fundamental estimation problems, the Maximum Likelihood Estimator (MLE) is the best possible. Hodge showed an example of a superefficient estimator that is asymptotically as good as the MLE everywhere, but beats the MLE on a certain set of inputs. The statistics community has argued in multiple ways that these superefficient estimators do not limit our ability to argue that MLE is \u201coptimal\u201d. We refer the reader to [vdV97, Wol65, Vov09] for a discussion of superefficiency. One of the more compelling arguments here is a result saying that the set of points where superefficiency is achieved has Lebesgue measure zero. This in particular implies that in a small neighborhood around any point, there is a point (in fact many points) where the superefficient estimator does no better than the MLE. In the partial order on estimators, the MLE is thus minimal and this is true even when looking at the performance of the estimator only on a small neighborhood around a given point. ", "page_idx": 22}, {"type": "text", "text": "This motivates a slightly different notion capturing the goodness of the algorithm locally. ", "page_idx": 22}, {"type": "text", "text": "Definition B.2. Let $\\mathcal{M}$ be a class of algorithms. We say that an algorithm $\\boldsymbol{\\mathcal{A}}$ is $\\alpha$ -locally minimal with respect to a neighborhood map $\\mathcal{N}$ , if for all instance $P$ , and all $A^{\\prime}\\in\\mathcal{M}$ , there is a $\\dot{Q}\\in\\mathcal{N}(P)$ such that $\\mathcal{R}_{{A},n}(Q)\\leq\\alpha\\cdot\\mathcal{R}_{{A}^{\\prime},n}(Q)$ . ", "page_idx": 22}, {"type": "text", "text": "In words, local minimality says that for any other $\\mathcal{A^{\\prime}}$ , the algorithm $\\boldsymbol{\\mathcal{A}}$ is competitive with $\\mathcal{A^{\\prime}}$ for some instance in the neighborhood of $P$ . Put differently, no $\\mathcal{A}^{\\prime}$ can be uniformly much better than $\\boldsymbol{\\mathcal{A}}$ on the neighborhood, even one that knows $P$ . ", "page_idx": 22}, {"type": "text", "text": "We show that in general, this notion is incomparable to our notion of instance optimality. Nevertheless, under reasonable assumptions, the two notions are closely related. ", "page_idx": 22}, {"type": "text", "text": "Example B.3 (Local Minimality $\\nRightarrow$ Instance Optimality). Consider a pair of instances $\\{P,Q\\}$ with $\\mathcal{N}(P)=\\mathcal{N}(Q)=\\{P,Q\\}$ . Let $\\mathcal{M}$ contain two algorithms $\\boldsymbol{\\mathcal{A}}$ , and $\\mathcal{A}^{\\star}$ with ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{R}_{\\mathcal{A},n}(P)=1;\\;\\;}&{{}\\mathcal{R}_{\\mathcal{A},n}(Q)=0;}\\\\ {\\mathcal{R}_{\\mathcal{A}^{\\star},n}(P)=0;\\;\\;}&{{}\\mathcal{R}_{\\mathcal{A}^{\\star},n}(Q)=0;}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then one can verify that $\\boldsymbol{\\mathcal{A}}$ is (1-)locally minimal in $\\mathcal{M}$ . However, it is not $\\alpha$ -instance optimal for any finite $\\alpha$ as it fails to satisfy the definition at $P$ . ", "page_idx": 22}, {"type": "text", "text": "Example B.4 (Instance Optimality $\\nRightarrow$ Local Minimality). Consider a set of instances $\\{P_{1},P_{2},P_{3}\\}$ with $\\bar{\\mathcal{N}}(P_{1})=\\{P_{1},P_{2}\\},\\bar{\\mathcal{N}}(P_{2})\\stackrel{}{=}\\{P_{1},P_{2},P_{3}\\},\\mathcal{N}(\\dot{P}_{3})=\\{P_{2},P_{3}\\}$ . Let $\\mathcal{M}$ contain algorithms ", "page_idx": 22}, {"type": "text", "text": "A, $\\mathcal{A}^{\\star}$ with ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{R}_{\\mathcal{A}^{\\star},n}(P_{1})=1;\\quad}&{{}\\mathcal{R}_{\\mathcal{A},n}(P_{1})=2\\alpha;}\\\\ {\\mathcal{R}_{\\mathcal{A}^{\\star},n}(P_{2})=2\\alpha;\\quad}&{{}\\mathcal{R}_{\\mathcal{A},n}(P_{2})=4\\alpha^{2};}\\\\ {\\mathcal{R}_{\\mathcal{A}^{\\star},n}(P_{3})=4\\alpha^{2};}&{{}\\mathcal{R}_{\\mathcal{A},n}(P_{3})=4\\alpha^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then one can verify that $\\boldsymbol{\\mathcal{A}}$ is (1-)instance optimal in $\\mathcal{M}$ . However, it is not $\\alpha$ -locally minimal at $P_{1}$ . ", "page_idx": 23}, {"type": "text", "text": "Under smoothness assumptions on $\\boldsymbol{\\mathcal{A}}$ with respect to $\\mathcal{N}$ , one can argue that the two notions are essentially equivalent. ", "page_idx": 23}, {"type": "text", "text": "Proposition B.5. Let $\\boldsymbol{\\mathcal{A}}$ be such that for all instances $P$ and for all $Q\\,\\in\\,{\\mathcal{N}}(P),\\;{\\mathcal{R}}_{A,n}(Q)\\,\\leq$ $\\beta\\cdot\\mathcal{R}_{{A,n}}(P)$ . Further, suppose that ${\\mathcal{N}}(P)$ is compact for any $P$ . If $\\boldsymbol{\\mathcal{A}}$ is $\\alpha$ -instance optimal in $\\mathcal{M}$ with respect to $\\mathcal{N}$ , then it is $\\alpha\\beta$ -locally minimal. ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $P$ be an instance and let $\\mathcal{A}^{\\prime}$ be a competing algorithm. By definition of $\\alpha$ -instance optimality, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{A},n}(P)\\leq\\alpha\\cdot\\operatorname*{sup}_{Q\\in\\mathcal{N}(P)}\\mathcal{R}_{\\mathcal{A}^{\\prime},n}(Q).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By compactness, this implies that there is a $Q$ achieving the supremum. In other words, there exists $\\bar{Q^{\\star}}\\in\\bar{\\mathcal{N}}(P)$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{R}_{{A},n}(P)\\leq\\alpha\\cdot\\mathcal{R}_{{A}^{\\prime},n}(Q^{\\star}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $Q^{\\star}\\in\\mathcal{N}(P)$ , our smoothness assumption implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{A},n}(Q^{\\star})\\leq\\beta\\cdot\\mathcal{R}_{\\mathcal{A},n}(P).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining the last two inequalities, this $Q^{\\star}$ satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{A},n}(Q^{\\star})\\leq\\alpha\\beta\\cdot\\mathcal{R}_{\\mathcal{A}^{\\prime},n}(Q^{\\star}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $P$ and $\\mathcal{A^{\\prime}}$ were arbitrary, this implies that $\\boldsymbol{\\mathcal{A}}$ is $\\alpha\\beta$ -locally minimal. ", "page_idx": 23}, {"type": "text", "text": "Proposition B.6. Let $\\boldsymbol{\\mathcal{A}}$ be such that for all instances $P$ and for all $Q\\,\\in\\,\\mathcal{N}(P),\\;\\mathcal{R}_{{A,n}}(Q)\\;\\geq\\;$ $\\beta^{-1}\\cdot\\mathcal{R}_{\\mathcal{A},n}(P)$ . If $\\boldsymbol{\\mathcal{A}}$ is $\\alpha$ -locally minimal in $\\mathcal{M}$ with respect to $\\mathcal{N}$ , then it is $\\alpha\\beta$ -instance optimal. ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $P$ be an instance and let $\\mathcal{A^{\\prime}}$ be a competing algorithm. By definition of $\\alpha$ -local minimality, there is a $Q^{\\star}\\in\\mathcal{N}(P)$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{\\mathcal{A},n}(Q^{\\star})\\leq\\alpha\\cdot\\mathcal{R}_{\\mathcal{A}^{\\prime},n}(Q^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $Q^{\\star}\\in\\mathcal{N}(P)$ , our smoothness assumption implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{\\mathcal{A},n}(P)\\leq\\beta\\cdot\\mathcal{R}_{\\mathcal{A},n}(Q^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining the last two inequalities, this $Q^{\\star}$ satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{\\mathcal{A},n}(P)\\leq\\alpha\\beta\\cdot\\mathcal{R}_{\\mathcal{A}^{\\prime},n}(Q^{\\star})}\\\\ &{\\qquad\\qquad\\leq\\alpha\\beta\\cdot\\operatorname*{sup}_{Q\\in\\mathcal{N}(P)}c o s t(\\mathcal{A}^{\\prime}(Q),Q).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $P$ and $\\mathcal{A^{\\prime}}$ were arbitrary, this implies that $\\boldsymbol{\\mathcal{A}}$ is $\\alpha\\beta$ -instance optimal. ", "page_idx": 23}, {"type": "text", "text": "A similar pair of results hold when the comparator algorithm $\\mathcal{A}^{\\prime}$ is smooth with respect to the neighborhood map. ", "page_idx": 23}, {"type": "text", "text": "B.3 Relaxed Definitions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We finish by noting relaxations of the above definitions that share the same semantic meaning (our algorithms will achieve these relaxed notions). ", "page_idx": 24}, {"type": "text", "text": "Definition B.7. Given a function ${\\mathcal{N}}:{\\mathcal{P}}\\to\\mathfrak{P}({\\mathcal{P}})$ , where $\\mathfrak{P}(\\mathcal{P})$ is the power set of $\\mathcal{P}$ , we define the optimal estimation rate with respect to $\\mathcal{N}$ to $b e$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{N},n}(P)=\\operatorname*{min}_{A}\\operatorname*{sup}_{Q\\in\\mathcal{N}(P)}\\mathcal{R}_{A,n}(Q).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "An algorithm $\\boldsymbol{\\mathcal{A}}$ is $(\\alpha,\\beta,\\gamma)$ -instance optimal with respect to $\\mathcal{N}$ if for all $P\\in\\mathcal P$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{A},n}(P)\\le\\alpha\\mathcal{R}_{\\mathcal{N},\\beta n}(P)+\\gamma\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Definition B.8. Let $\\mathcal{M}$ be a class of algorithms. We say that an algorithm $\\boldsymbol{\\mathcal{A}}$ is $(\\alpha,\\beta,\\gamma)$ -locally minimal with respect to a neighborhood map $\\mathcal{N}$ , if for all instance $P$ , and all $\\mathcal{A}^{\\prime}\\in\\mathcal{M}$ , there is $a$ $Q\\in{\\mathcal{N}}(P)$ such that $\\mathcal{R}_{\\mathcal{A},n}(Q)\\leq\\alpha\\cdot\\mathcal{R}_{\\mathcal{A}^{\\prime},\\beta n}(Q)+\\gamma.$ . ", "page_idx": 24}, {"type": "text", "text": "Note that we think of $\\beta\\in(0,1]$ and $\\gamma$ as non-negative. The reason these are relaxed definitions is because we allow for an additive approximation factor in addition to a multiplicative factor, and also compare to a benchmark rate that depends on a potentially smaller number of samples (and is hence easier to achieve). The original definition of instance optimality (Definition B.1) can be obtained by setting $\\beta=1$ and $\\gamma=0$ . ", "page_idx": 24}, {"type": "text", "text": "In our work, for most settings of interest, we roughly achieve $\\beta\\,=\\,1/(\\log n)^{O(1)}$ and $\\gamma$ to be an arbitrarily small polynomial in the inverse of the number of samples $1/n$ at a $\\log(1/\\gamma)$ cost to the multiplicative factor. We don\u2019t view this as a significant issue since we expect the benchmark rate with ${\\tilde{O}}(n/\\log n)$ samples to behave asymptotically similarly to that with $n$ samples in most cases. We leave it as an open question as to whether the original definition of instance optimality can be achieved. ", "page_idx": 24}, {"type": "text", "text": "C Additional Related Work ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Instance Optimality for Differentially Private Statistics: Several recent works have focused on formulating and giving \u2018instance optimal\u2019 differentially private algorithms for various statistical tasks. The work of McMillan, Smith and Ullman [MSU22] is most directly related to our work; they gave locally minimax optimal algorithms for parameter estimation for one-dimensional exponential families in the central model of differential privacy. The work of Duchi and Ruan [DR18] also gives locally-minimax optimal algorithms for various one-dimensional parameter estimation problems under the stronger constraint of local differential privacy. The notion of local minimax optimality both these papers use is based on the hardest one-dimensional sub-problem described in Section B.1. While our results for density estimation in $\\mathbb{R}^{1}$ satisfy this notion, they also satisfy a stronger notion described in Section B.1. Additionally, as discussed in [MSU22], this definition is provably unsuitable for higher dimensions; we instead suggest a looser definition of instance optimality that is more promising in higher dimensions. More importantly, our paper is primarily focused on the non-parametric setting, and hence our techniques are different than the ones used in those papers, which focused primarily on parameter estimation. ", "page_idx": 24}, {"type": "text", "text": "Other Beyond Worse-Case Results in Central Differential Privacy: Several additional works in the differential privacy literature study algorithms with accuracy that varies with the input dataset. Nearly all of them look at the empirical setting where we are concerned with the specific input dataset, rather than a distribution it may be drawn from. While initial algorithms in differential privacy added noise based on a worse case notion of global sensitivity, these works give various algorithmic frameworks that help develop algorithms with guarantees that adapt to the hardness of the input dataset. These include algorithms based on smooth sensitivity [NRS07, BS19], the propose-test-release framework [DL09, BA20], Lipschitz extensions [BBDS13, KNRS13, CZ13, RS16], and sensitivity pre-processing [CD20]. However, none of these works study a formal notion of instance optimality. ", "page_idx": 24}, {"type": "text", "text": "In contrast, some more recent work do study definitions of instance optimality in the empirical setting. A work of Asi and Duchi [AD20] studies two notions of instance optimality: one by comparing the performance of an algorithm on a dataset against the performance of the best unbiased algorithm on that dataset, and another based on an analogue of the \u2018hardest one-dimensional sub-problem\u2019 for the empirical setting (they compare the performance of an algorithm on a dataset with all benchmark algorithms that know that the input dataset is either of two possible datasets but whose performance is evaluated as the worse over the two datasets). They give a general mechanism known as the inverse sensitivity mechanism that they show is nearly instance optimal under these definitions for various problems such as median and mean estimation. Our work is focused on population quantities as opposed to empirical quantities\u2014while these are related, they can be very different. For example, as pointed out in McMillan, Smith and Ullman [MSU22], using the inverse sensitivity mechanism in [AD20] to estimate the mean of a Gaussian (by using a locally minimax optimal algorithm for empirical mean) will result in infinite mean squared error, whereas other approaches that reason directly about the population quantities can get much better error. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "In [DKSS23] and [HLY21], different notions of instance optimality are defined. Roughly, they compare the performance of an algorithm on a dataset with a benchmark algorithm that knows the input dataset but whose performance is evaluated as the worst-case performance over large subsets of the input dataset. While the details of the definitions in these papers vary slightly, both papers give instance-optimal algorithms for mean estimation under their respective definitions. For one-dimensional distributions, our algorithmic technique at a high level shares ideas with these algorithms\u2014the algorithms in their papers try to adapt to the range of values in the dataset, whereas we try to adapt to the level of concentration of the distribution. However, the details of how this is done and the associated analyses vary. Our algorithm for general metric spaces uses different techniques. Our work differs from these works in a few other prominent ways: firstly, they are primarily concerned with estimating functionals of the underlying dataset, whereas we are concerned with density estimation in Wasserstein distance\u2014these are problems with different output types and different error metrics. Finally, it is not clear if notions such as subset-based instance optimality that are well defined in the empirical setting transfer meaningfully to the distributional setting. ", "page_idx": 25}, {"type": "text", "text": "Instance-Optimal Statistical Estimation without Privacy Constraints: Donoho and Liu [DL91] formulated the notion of the \u2018hardest one-dimensional sub-problem\u2019 as a way of capturing instance optimality for statistical estimation and gave non-private instance optimal algorithms for some one-dimensional parameter estimation problems. Cai and Low [CL15] formulated an instance-optimality type definition for non-parameteric estimation problems. Our results for Wasserstein density estimation over $\\mathbb{R}$ use a stronger version of this notion of instance optimality. In higher dimensions, this notion is provably unachievable, and so we define a different notion. ", "page_idx": 25}, {"type": "text", "text": "The other line of work most related to ours is on instance-optimal learning of discrete distributions [OS15, VV16, HO19]. In their setting, instance optimality is defined by comparing the performance of an algorithm on a discrete distribution $P$ to the minimax error of any algorithm on the class of discrete distributions with probability vectors that are permutations of the probability vector of $P$ . We note that this notion is not well suited to many metric spaces, because permutations may not preserve properties such as concentration of the distribution, and hence this notion of instance optimality may provide an overly pessimistic view of the performance of an algorithm. Our notion of instance optimality (in terms of $D_{\\infty}$ neighborhood) compares against algorithms with a different type of prior knowledge- i.e., the location of where the distribution concentrates, and approximate values of the probabilities at each point. We note that these are technically incomparable, and may be useful in different settings. For estimation in Wasserstein distance, knowledge of where the distribution is concentrated could be very useful in algorithm design, and so comparing to algorithms with this type of knowledge is more appropriate. See Appendix B for more discussion. ", "page_idx": 25}, {"type": "text", "text": "Finally, there is another line of work on getting similar instance optimal guarantees for other statistical problems $[\\mathrm{ADJ^{+}11}$ , $\\mathrm{ADJ^{+}}12$ , AJOS13b, AJOS13a]. For the closeness testing problem (given two sequences, determine if they are produced by the same distribution, or different distributions), Acharya, Das, Jafarpour, Orlitsky, Pan and Suresh $[\\mathrm{ADJ^{+}11}$ , $\\mathrm{ADJ^{+}}12]$ developed a test (without any knowledge about the generating distributions) that achieves the same error with $O(n^{3/2})$ samples that an optimal label-invariant test that knows the distributions $p$ and $q$ would achieve with $n$ samples. ", "page_idx": 25}, {"type": "text", "text": "Other work on Differentially Private Statistics: There is a lot of other work on private statistical estimation, and we survey the most relevant parts of the literature here. There is a long line of work on minimax parameter/distribution estimation on various parametric distribution families: product distributions [BUV18, KLSU19, ASZ20, CWZ19, Sin23], Gaussian, sub-Gaussian distributions (and more generally exponential families) [KV18, KLSU19, AAK21, $\\mathrm{BGS}^{+}21$ , ${\\mathrm{KMS}}^{+}22\\mathrm{b}$ , KMS22a, HKM22, KMV22, AL22, LKO22, TCK $^{+}22$ , HKMN23, $\\mathrm{AKT}^{+}23$ , BHS23, KDH23], mixtures of Gaussian distributions [KSSU19, AAL23b, AAL23a], heavy-tailed distributions [KSU20, Nar23], discrete distributions with finite support [DHS15, ASZ20], distributions with finite covers [BKSW21] and more. This line of work focuses on minimax guarantees in the parametric setting, i.e. optimizing the worst-case error of an algorithm over the entire class of distributions. Our work, on the other hand works in the non-parametric setting where we do not make assumptions about the distribution the dataset is drawn from, but instead give \u2018instance-optimal\u2019 algorithms that adapt to the hardness of the distribution the input dataset is drawn from. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "There is also a line of work on differentially private CDF estimation [DNPR10, CSS11, BNS16, BNSV15, ALMM19, ${\\mathrm{KLM}}^{+}20$ , $\\mathrm{CLN}^{+}23]$ , and quantile estimation [KSS22, GJK21, ASSU24]. Our algorithm for density estimation over $\\mathbb{R}$ uses a quantile estimation algorithm (based on a CDF estimator) as a subroutine. Finally, there is a line of work on differentially private testing [ASZ17, CDK17, $\\mathrm{CKM^{+}19}]$ , and the work characterizing the sample complexity of simple hypothesis tests forms an important part of our analysis of the instance-optimal rate for distributions over $\\mathbb{R}$ . ", "page_idx": 26}, {"type": "text", "text": "Work on Estimation in Wasserstein Distance: In addition to the recent works [BSV22, HVZ23] on private Wasserstein learning on $[0,1]^{d}$ , there is a plethora of works studying it in the non-private setting. ", "page_idx": 26}, {"type": "text", "text": "One line of work studies the convergence in Wasserstein distance of the empirical measure (on $n$ samples) to the true measure, as a function of the measure and the number of samples $n$ [Dud69, DY95, CR12, DSS11, BG14, FG15, BL19, WB19, Lei20, Fou23]. Some of the later works above can be viewed as studying this problem from a beyond worst-case analysis viewpoint. They give upper and lower bounds for the expected value of this quantity, in terms of various notions of \u2018dimension\u2019 of the underlying measure, such as the covering number of the support of the distribution, the upper and lower \u2018Wasserstein dimensions\u2019 of the measure, and others. Our work shows that the empirical measure, appropriately massaged, is approximately instance-optimal for density estimation without privacy constraints (for the notions of instance optimality we consider), and hence these works give us a handle on the instance-optimal rate as a function of the distribution and sample size $n$ . Some more recent work studies minimax estimation in Wasserstein distance [SP19, NWB19], and show that without additional assumptions on the distribution, the empirical measure is minimax optimal. Our work extends this result to show that in the general nonparametric setting, the empirical measure is also approximately instance-optimal; to the best of our knowledge, instance optimal estimation in Wasserstein distance (even without privacy constraints) has not been previously studied. ", "page_idx": 26}, {"type": "text", "text": "D Distribution Estimation on Hierarchically Separated Trees ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let us now turn to distribution estimation on arbitrary finite metric spaces. We will use the fact that any metric on a finite space can be embedding in a hierarchically separated tree (HST) metric to reduce the problem of density estimation in Wasserstein distance on an arbitrary metric space to density estimation in Wasserstein distance on an HST. In Section D.2 we\u2019ll characterise the target estimation rate ${\\mathcal{R}}_{{\\mathcal{N}},n}$ . In Section D.3, we\u2019ll then provide an $\\varepsilon$ -DP algorithm and prove that it achieves this target estimation rate up to logarithmic factors. ", "page_idx": 26}, {"type": "text", "text": "D.1 Preliminaries on Hierarchically Separated Trees ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "A key component of our proof strategy is the reduction to Hierarchically Separated Trees (HSTs). HSTs are special class of tree metrics that are able to embed arbitrary metric spaces with low distortion. They are particularly well-behaved when working with the Wasserstein distance since the Wasserstein distance on an HST has a simple closed form. ", "page_idx": 26}, {"type": "text", "text": "Definition D.1 (Hierarchically Separated Tree). A hierarchically separated tree (HST) is a rooted weighted tree such that the edges between level $\\ell$ and $\\ell-1$ all have the same weight (denoted $r_{\\ell.}$ ) and the weights are geometrically decreasing so $r_{\\ell+1}=(1/2)r_{\\ell}$ . Let $D_{T}$ be the depth of the tree. ", "page_idx": 26}, {"type": "text", "text": "An HST defines a metric on its leaf nodes by defining the distance between any two leaf nodes to be the weight of the minimum weight path between the two nodes. We will rely on two main facts about HSTs in this work. ", "page_idx": 27}, {"type": "text", "text": "Lemma D.2 (Low distortion metric embeddings [FRT03]). Let $(V,d)$ be a metric space with $M$ points. There exists a randomized, polynomial time algorithm that produces an HST where the leaf nodes of the tree correspond to the elements of the metric space and the induced tree metric $d_{T}$ is such that for all $u,v\\in V$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ d(u,v)\\leq d_{T}(u,v)}\\\\ &{\\bullet\\ \\mathbb{E}[d_{T}(u,v)]\\leq O(\\log M)\\cdot d(u,v)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The depth of the HST is logarithmic in the size of the metric space, $D_{T}=\\log M$ . ", "page_idx": 27}, {"type": "text", "text": "An immediate consequence of the $O(\\log M)$ metric distortion in Lemma D.2 is that the Wasserstein distance in the original metric space is also preserved up to a $O(\\log M)$ factor in expectation. Thus, Lemma D.2 allows us to translate the problem of learning densities on an arbitrary metric space in Wasserstein distance to learning densities in Wasserstein distance on an HST. This is a useful tool since HST metrics are generally easier to work with and, as we\u2019ll see below, the Wasserstein distance is particularly well-behaved on an HST. In order to use Lemma D.2 to translate the problem of density estimation on a bounded ball in Rdinto density estimation on an HST, one discretizes the metric, paying a small additive term. ", "page_idx": 27}, {"type": "text", "text": "Corollary D.3. Given $\\alpha>0$ , there is a probabilistic embedding $f$ of $[0,1]^{d}$ into an HST such that for all $x,y\\in[0,1]^{d}$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ d(x,y)-\\alpha\\leq d_{T}(f(x),f(y))}\\\\ &{\\bullet\\ \\mathbb{E}[d_{T}(f(x),f(y)]\\leq O(d\\cdot\\log\\frac{1}{\\alpha}])\\cdot(d(x,y)+\\alpha)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The distortion is logarithmic in $\\frac{1}{\\alpha}$ , so taking $\\alpha$ to be polynomially small, one gets the distortion to be $O(d\\log n)$ . It is easy to see that this implies that the Wasserstein distance is preserved in both directions up to $O(d\\log{\\frac{1}{\\alpha}}$ , up to an $\\alpha$ additive error. ", "page_idx": 27}, {"type": "text", "text": "A distribution $P$ on the the underlying metric space in an HST induces a function ${\\mathfrak{G}}_{P}$ on the nodes of the tree where the value of a node $\\nu$ is given by the weight in $P$ of the leaf nodes in the subtree rooted at $\\nu$ . For every level $\\ell\\in[D_{T}]$ of the tree, let $P_{\\ell}$ be the distribution induced on the nodes at level $\\ell$ where the probability of node $\\nu$ is ${\\mathfrak{G}}_{P}(\\nu)$ . Thus $P_{\\ell}$ is a discrete distribution on a domain of size $N_{\\ell}$ , where $N_{\\ell}$ is the number of nodes in level $\\ell$ of the tree. ", "page_idx": 27}, {"type": "text", "text": "Lemma D.4 (Closed form Wasserstein distance formula). Given two distributions $P$ and $Q$ defined on an HST metric space, the Wasserstein distance between $P$ and $Q$ has the closed formula: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{W}(P,Q)=\\frac{1}{2}\\sum_{\\nu}r_{\\nu}|\\mathfrak{G}_{P}(\\nu)-\\mathfrak{G}_{Q}(\\nu)|=\\sum_{\\ell}r_{\\ell}\\mathrm{TV}(P_{\\ell},Q_{\\ell}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $r_{\\nu}$ is the length of the edge connecting $\\nu$ to its parent, and the sum is over all nodes in the tree. ", "page_idx": 27}, {"type": "text", "text": "D.2 The Target Estimation Rate ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Recall the definition of our neighbourhood. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{N}(P)=\\{Q\\in\\mathcal{P}\\mid D_{\\infty}(P,Q)\\leq\\ln2\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We will call a node $\\nu$ , $\\alpha$ -active node under the distribution $P$ if the weight in $P$ of the sub-tree rooted at $\\nu$ is greater than $\\alpha$ . Let $\\gamma_{P}\\left(\\alpha\\right)$ be the set of $\\alpha$ -active nodes under $P$ and $\\gamma_{P_{\\ell}}\\left(\\alpha\\right)$ be the $\\alpha$ -active nodes at level $\\ell$ . ", "page_idx": 27}, {"type": "text", "text": "Theorem D.5. Given a distribution $P$ on $[N],\\ \\varepsilon\\ >\\ 0,\\ \\delta\\ \\in\\ [0,1],$ , and $\\textit{n}\\in\\mathbb{N}$ , let $\\kappa\\,=$ $\\begin{array}{r}{\\frac{1}{10\\varepsilon n}\\operatorname*{min}\\{W\\left(\\frac{0.45\\varepsilon}{\\delta}\\right),0.6\\}}\\end{array}$ where $W(x)$ is the Lambert W function so $W(x)e^{W(x)}=x$ , then 1 $\\mathfrak{L}_{N,n,\\varepsilon}(P)=\\Omega\\left(\\operatorname*{max}_{\\ell}r_{\\ell}\\sum_{x\\in[N_{\\ell}]}\\operatorname*{min}\\left\\{P_{\\ell}(x)(1-P_{\\ell}(x)),\\sqrt{\\frac{P_{\\ell}(x)(1-P_{\\ell}(x))}{n}}\\right\\}+\\sum_{x\\in\\gamma_{P_{\\ell}}(2\\kappa)}P_{\\ell}(x)+(|\\gamma_{P_{\\ell}}(1-P_{\\ell}(x)|))|,\\varepsilon\\right)$ (2\u03ba) | \u22121)\u03ba ", "page_idx": 27}, {"type": "text", "text": "where the max is over all the levels of the tree. ", "page_idx": 27}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\kappa\\,\\approx\\,\\frac{1}{\\varepsilon n}\\operatorname*{min}\\{\\log(1/\\delta),1\\}}\\end{array}$ so the dependence on $\\varepsilon$ and $n$ in Theorem D.5 matches the upper bound in Theorem D.13. The error rate $\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}$ does indeed adapt to easy instances as we expected. The error decomposes into three components. The first component is the non-private sampling error; the error that would occur even if privacy was not required. The second component indicates that we can not estimate the value of nodes that have probability less than $1/(\\varepsilon n)$ . The third component is the error due to privacy on the active nodes. If $P$ is highly concentrated then we expect most nodes to either be $\\frac{1}{\\varepsilon n}$ -active or have weight 0, so the first two terms in $\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}(P)$ are small. There should also be few active nodes, making the last term smaller as well. Conversely, if $P$ has a large region of low density then we expect a large number of inactive nodes, as well as non-zero inactive nodes that are at higher levels of the tree and hence contribute more to the final term. Thus, in distributions with high dispersion we expect the right hand side to be large. ", "page_idx": 28}, {"type": "text", "text": "The proof of Theorem 4.1 will involve two main steps. First, we will reduce the lower bound on the HST to a lower bound on a star metric, or equivalently estimation of a discrete distribution in TV distance. We\u2019ll then use a variant of Assouad\u2019s inequality to prove the lower bounds on estimating discrete distributions in TV distance. ", "page_idx": 28}, {"type": "text", "text": "D.2.1 Reduction to Estimation in TV distance of Discrete Distributions ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The key observation is that in order to estimate the distribution well in Wasserstein distance, an algorithm must estimate each level of the tree well in TV distance. Any estimate of $P$ also induces an estimate of $P_{\\ell}$ ; let $\\hat{P}$ be an estimate of the distribution $P$ and $\\hat{P}_{\\ell}$ be the induced estimate of the distribution at level $\\ell$ . Then for any distribution $P$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{W}(P,\\hat{P})=\\sum_{\\ell\\in[D_{T}]}r_{\\ell}T V(P_{\\ell},\\hat{P}_{\\ell}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The following observation ensures that our notions of instance optimality in both the Wasserstein metric and the per-level TV distance are compatible at every level $\\ell$ . ", "page_idx": 28}, {"type": "text", "text": "Theorem D.6. For every level $\\ell\\ \\in\\ [D_{T}],$ , define the neighborhood of $P_{\\ell}$ as $\\mathcal{N}_{\\ell}\\;:\\;\\Delta([N_{\\ell}])\\;\\rightarrow$ $\\mathfrak{P}(\\Delta([N_{\\ell}]))$ by $\\mathcal{N}_{\\ell}(P_{\\ell})=\\{Q_{\\ell}\\mid D_{\\infty}(\\dot{P}_{\\ell},\\dot{Q}_{\\ell})\\leq\\ln2\\}$ . Then, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}(P)\\ge\\operatorname*{max}_{\\ell\\in[D_{T}]}r_{\\ell}\\cdot\\mathcal{R}_{\\mathcal{N}_{\\ell},n,\\varepsilon}(P_{\\ell}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the error of $P$ is measured in the Wasserstein distance and $P_{\\ell}$ is measured in the TV distance. ", "page_idx": 28}, {"type": "text", "text": "Recall that $\\mathcal{R}_{\\mathcal{N}_{\\ell},n,\\varepsilon}(P_{\\ell})$ is the optimal estimation rate with respect to $\\mathcal{N}_{\\ell}$ where the error is measured with respect to the total variation error. The proof of Theorem D.6 can be found in Appendix G. ", "page_idx": 28}, {"type": "text", "text": "D.2.2 Characterizing Target Estimation Rate for Discrete Distributions ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In light of Theorem D.6, we will focus on characterizing the difficulty of estimating the distribution at a single level of the tree for the remainder of this section. Since this is fundamentally a statement about estimating discrete distributions in TV distance, we will state everything in this section in terms of general discrete distributions. Let $N\\;\\in\\;\\mathbb{N}$ , and let $P$ be a distribution on $[N]$ . Define $\\mathcal{N}(P)\\,=^{*}\\!\\{Q\\,\\mid\\,D_{\\infty}(P,Q)\\,\\leq\\,\\ln2\\}$ . Our goal is to give a lower bound for $\\mathcal{R}_{f,n,\\varepsilon}(P)$ , where the metric is the TV distance. ", "page_idx": 28}, {"type": "text", "text": "Theorem D.7. Given $\\varepsilon>0$ and $\\delta\\in[0,1]$ , let $\\begin{array}{r}{\\kappa=\\frac{1}{10\\varepsilon n}\\operatorname*{min}\\{W\\left(\\frac{0.45\\varepsilon}{\\delta}\\right),0.6\\}}\\end{array}$ where $W(x)$ is the Lambert W function so $W(x)e^{W(x)}=x$ . Given a distribution $P$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{N,n,\\varepsilon}(P)=\\Omega\\left(\\sum_{x\\in[N]}\\operatorname*{min}\\left\\{P(x)(1-P(x)),\\sqrt{\\frac{P(x)(1-P(x))}{n}}\\right\\}+\\sum_{x\\notin\\gamma_{P}(2\\kappa)}P(x)+(|\\gamma_{P}\\left(2\\kappa\\right)|)\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Theorem D.5 follows immediately from Theorem D.6 and Theorem D.7. The main tool we will use is a differentially private version of Assouad\u2019s method. This gives us a method for lower bounding the error by constructing nets of distributions that are pairwise far in the relevant metric of interest, which for us in the TV distance. The following is a slight variant on the differentially private variant of Assouad\u2019s lemma given in [ASZ21]. Rather than building a set of distributions indexed by a hypercube, we will build a set of distributions over a product of hypercubes. Since this is an extension of the version that appears in [ASZ21], we include a proof in Appendix $\\mathrm{G}$ for completeness. ", "page_idx": 28}, {"type": "text", "text": "Lemma D.8. $I A$ extension of $(\\varepsilon,\\delta)$ -DP Assouad\u2019s method [ASZ21]] Let $k_{0},k_{1},\\ldots$ be a sequence of natural numbers such that $\\sum_{s}k_{s}<\\infty,$ , $\\varepsilon\\,>\\,0$ and $\\delta\\,\\in\\,[0,1]$ . Given a family of distributions $\\dot{\\mathcal{P}}\\,\\subset\\,\\Delta(\\mathcal{X})$ on a space $\\mathcal{X}$ ,  a parameter $\\theta\\,:\\,\\mathcal{P}\\,\\rightarrow\\,\\mathcal{M}$ where $\\mathcal{M}$ is a metric space with metric $d,$ , suppose that there exists a set $\\mathcal{V}\\subset\\mathcal{P}$ of distributions indexed by the product of hypercubes $\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdot\\cdot\\cdot$ where $\\mathcal{E}_{k}:=\\{\\pm1\\}^{k}$ such that for a sequence $\\tau_{0},\\tau_{1},\\dots$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\forall(u^{0},u^{1},\\cdots),(v^{0},v^{1},\\cdots)\\in\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdots,\\quad d(\\theta(p_{u}),\\theta(p_{v}))\\geq2\\sum_{s}\\tau_{s}\\sum_{j=1}^{k_{s}}\\chi_{u_{j}^{s}\\neq v_{j}^{s}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For each coordinate $s\\in\\mathbb{N},\\ j\\in[k_{s}],$ , consider the mixture distributions obtained by averaging over all distributions with a fixed value at the $(s,j)t h$ coordinate: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathfrak{s}_{+(s,j)}=\\frac{2}{|\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdot\\cdot\\cdot\\vert}\\sum_{\\substack{u\\in\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdot\\cdot\\cdot u_{j}^{s}=+1}}p_{u},\\,\\ p_{-(s,j)}=\\frac{2}{|\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdot\\cdot\\cdot\\cdot|}\\sum_{\\substack{u\\in\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdot\\cdot\\cdot\\cdot u_{j}^{s}=+1}}\\sum_{\\substack{u\\in\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdot\\cdot\\cdot\\cdot u_{j}^{s}=+2}}p_{u},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and let $\\phi_{s,j}:\\mathcal{X}^{n}\\to\\{-1,+1\\}$ be a binary classifier. Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{4\\,i s\\,(\\varepsilon,\\delta)\\cdot D P}}\\operatorname*{max}_{p\\in\\mathcal{V}}\\mathcal{R}_{A,n}(p)\\geq\\frac{1}{2}\\sum_{s}\\tau_{s}\\sum_{\\substack{j=1}}^{k_{s}}\\operatorname*{min}_{\\substack{\\phi_{s,j}\\,i s\\,(\\varepsilon,\\delta)\\cdot D P}}\\bigr(\\operatorname*{Pr}_{X\\sim p_{+\\,(s,j)}^{\\!\\!r}}\\bigl(\\phi_{s,j}(X)\\neq1\\bigr)+\\operatorname*{Pr}_{X\\sim p_{-\\,(s,j)}^{\\!\\!r}}\\bigl(\\phi_{s,j}(X)\\neq1\\bigr)\\operatorname*{max}_{\\substack{j=1\\,i s\\,(\\varepsilon,\\delta)\\cdot D P}}\\bigr)\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the min on the LHS is over all $(\\varepsilon,\\delta)$ -DP mechanisms, and on the right hand side is over all $(\\varepsilon,\\delta)$ -DP binary classifiers. Moreover, if for all $s\\,\\in\\,\\mathbb{N}$ , $j\\ \\in[k_{s}],$ , there exists a coupling $(X,Y)$ between $p_{+(s,j)}^{n}$ and $p_{-(s,j)}^{n}$ with $\\mathbb{E}[d_{H a m}(X,Y)]\\leq D_{s}$ , then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{A\\,i s\\,(\\varepsilon,\\delta)\\cdot D P\\,\\,p\\in\\mathcal{V}}}\\mathcal{R}_{A,n}(p)\\geq\\sum_{s}\\frac{k_{s}\\tau_{s}}{2}\\big(0.9e^{-10\\varepsilon D_{s}}-10D_{s}\\delta\\big)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that an upper bound on $T V(P_{i},P_{j})\\le\\gamma$ implies there exists a coupling $(X,Y)$ between $P_{i}^{n}$ and $P_{j}^{n}$ such that $\\mathbb{E}[d_{H a m}(X,Y)]\\leq n\\dot{\\gamma}$ . ", "page_idx": 29}, {"type": "text", "text": "We will separately prove that each of the three terms in Theorem D.7 belong in the lower bound. Each proof will follow the same underlying structure. Given a distribution $P$ , the main technical step is carefully designing a family of distributions in ${\\mathcal{N}}(P)$ that satisfy the conditions of Lemma D.8. Lemma D.9 and Lemma D.10 give lower bounds on the noise due to privacy. Lemma D.11 gives lower bounds based on the error due to sampling. ", "page_idx": 29}, {"type": "text", "text": "Let ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\kappa=\\frac{1}{10\\varepsilon}\\operatorname*{min}\\{W\\left(\\frac{0.45\\varepsilon}{\\delta}\\right),0.6\\},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $W(x)\\approx\\ln x-(1-o(1))\\ln\\ln x$ is the Lambert W function satisfying $W(x)e^{W(x)}=x$ . In both lemma proofs we will use the inequality that if $D\\leq\\kappa$ , then ", "page_idx": 29}, {"type": "equation", "text": "$$\n1.9e^{-10\\varepsilon D}-10D\\delta\\geq e^{-10\\varepsilon D}\\left(0.9-W\\left(\\frac{0.45\\varepsilon}{\\delta}\\right)e^{W\\left(\\frac{0.45\\varepsilon}{\\delta}\\right)}\\frac{\\delta}{\\varepsilon}\\right)=e^{-10\\varepsilon D}\\left(0.9-\\frac{0.45\\varepsilon}{\\delta}\\frac{\\delta}{\\varepsilon}\\right)\\geq e^{-1}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma D.9. Given a distribution $P,\\,\\varepsilon>0,\\,\\delta\\in[0,1]$ and $n\\in\\mathbb{N}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}(P)\\geq0.1\\left(\\left|\\gamma_{P}\\left(\\frac{2\\kappa}{n}\\right)\\right|-1\\right)\\frac{\\kappa}{n}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Let $L=|\\gamma_{P}\\left(2\\kappa/n\\right)|$ be the number of active nodes. If $L=1$ then the RHS is 0 and so we are done. Otherwise, assume $L>1$ and let $k=\\lfloor L/2\\rfloor\\geq1$ . Using the notation from Lemma D.8, let $k_{0}=k$ and $k_{s}=0$ for all $s>0$ . We will drop the reference to $s$ in the notation since only $s=0$ is significant. ", "page_idx": 29}, {"type": "text", "text": "Pair up the active nodes to form $k$ pairs of active nodes denoted by $(a_{1}^{+},a_{1}^{-}),\\cdot\\cdot\\cdot\\;,(a_{k}^{+},a_{k}^{-})$ . Given $u\\in\\mathcal{E}_{k}$ , define the distribution $p_{u}$ as follows: for all $a_{j}^{b}\\in\\gamma_{P}\\left(2\\kappa/n\\right),P_{u}(a_{j}^{+})=P(a_{j}^{+})\\!+\\!(\\kappa/n)$ and $P_{u}(a_{j}^{-})=P(a_{j}^{-})\\!-\\!(\\kappa/n)$ if $u_{j}=+1$ and $P_{u}(a_{j}^{+})=P(a_{j}^{+})\\!-\\!(\\kappa/n)$ and $P_{u}(a_{j}^{-})=P(a_{j}^{-}){+}(\\kappa/n)$ ", "page_idx": 29}, {"type": "text", "text": "if $u_{j}=-1$ . For all other $x$ , $P_{u}(x)=P(x)$ . It is immediate that for all $u$ , $P_{u}\\in\\mathcal{N}(P)$ . For any pair $u,v$ $,d(\\theta(p_{u}),\\theta(p_{v}))=\\mathrm{TV}(p_{u},p_{v})=d_{H a m}(u,v)(\\kappa/n),$ , so that Equation (6) is satisfied with $\\tau\\,=\\,{\\textstyle{\\frac{1}{2}}}(\\kappa/n)$ . Further, given $j\\in[k]$ , $p_{+j}$ and $p_{-j}$ only differ on the probability of $a_{j}^{+}$ and $a_{j}^{-}$ , so $D/n=\\operatorname*{max}_{j}\\mathrm{TV}(p_{+j},p_{-j})=\\kappa/n$ and by Equation (7), $0.9e^{-10\\varepsilon D}-10D\\delta\\geq0.\\dot{2}$ . Noting that $k\\geq(1/2)(\\gamma_{P}\\left(2\\kappa/n\\right)-1)$ completes the proof. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Lemma D.10. For all $\\varepsilon>0,\\,\\delta\\in[0,1],\\,n\\in\\mathbb{N}$ and distributions $P$ on $[N],\\,i f\\,\\kappa<n/2$ , then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}(P)\\geq\\Omega\\left(\\sum_{x\\notin\\gamma_{P}(2\\kappa)}P(x)\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\begin{array}{r}{\\kappa\\leq\\frac{1}{\\varepsilon n}}\\end{array}$ , the condition that $\\kappa<n/2$ is a mild condition. For example, it is satisfied whenever $\\varepsilon>2/n^{2}$ . ", "page_idx": 30}, {"type": "text", "text": "Similar to the proof of Lemma D.9, we are going to pair up the coordinates and move mass between the coordinates to create the distributions indexed by the product of hypercubes. Since we want all the distributions we create to be in ${\\mathcal{N}}(P)$ , we will divide the space into scales such that all elements in the same scale have approximately the same probability of occurring. We\u2019ll then move mass within these scales. For $s\\in\\mathbb{N}$ , let $\\begin{array}{r}{\\dot{S_{s}}\\dot{=}\\left\\{x\\in[N]\\mid P(x)\\in(2^{-s-1},2^{-s}\\right]\\right\\}}\\end{array}$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. Given $s\\in\\mathbb{N}$ , let $S_{s}^{\\prime}=S_{s}\\cap\\{x\\mid P(x)\\leq2\\kappa/n\\}$ and $d_{s}=|S_{s}^{\\prime}|$ . ", "page_idx": 30}, {"type": "text", "text": "Let us first consider the case that there exists a scale $s^{*}$ with $d_{s^{*}}=\\mathrm{~\\,~1~}$ and $P(x^{*})\\quad\\geq$ $\\begin{array}{r}{{\\frac{1}{8}}\\sum_{x\\notin\\gamma_{P}(2\\kappa)}P(x)}\\end{array}$ where $x^{*}$ is the element in $\\mathcal{S}_{s^{*}}^{\\prime}$ . Define $P^{\\prime}$ by $P^{\\prime}(x^{*})\\,=\\,(1/2)P(x^{*})$ and for all $x\\neq x^{*}$ , $\\begin{array}{r}{P^{\\prime}(x)\\,=\\,\\frac{1-(1/2)P(x^{*})}{1-P(x^{*})}P(x)}\\end{array}$ . Since $(1/2)P(x^{*})\\,\\leq\\,2\\kappa/n\\,\\leq\\,1/2,$ , $P^{\\prime}\\,\\in\\,\\mathcal{N}(P)$ . In this case we will use Lemma D.8 with $k_{0}~=~1$ , $k_{s}~=~0$ otherwise, and $\\mathcal{E}_{k_{0}}$ corresponds to the set of distributions $\\{P,P^{\\prime}\\}$ . Then noting that $\\mathrm{TV}(P,P^{\\prime})\\;=\\;(1/2)P(x^{*})$ and using eqn (7) we have that $\\begin{array}{r}{\\tau\\,=\\,\\frac{1}{4}P(\\dot{x}^{*})}\\end{array}$ and $D\\,=\\,(1/2)P(x^{*})n\\,\\leq\\,\\kappa$ so that $\\begin{array}{r l}{\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}(P)\\;\\ge\\;(1/8)P(x^{*})(0.2)\\,=\\,}&{{}}\\end{array}$ $\\begin{array}{r}{\\Omega\\left(\\sum_{x\\notin\\gamma_{P}(2\\kappa)}P(x)\\right)}\\end{array}$ and we are done. ", "page_idx": 30}, {"type": "text", "text": "Next suppose that for all scales $s$ such that $d_{s}=1$ we have $\\begin{array}{r}{P(x^{*})<\\frac{1}{8}\\sum_{x\\notin\\gamma_{P}(2\\kappa)}P(x)}\\end{array}$ . Let $s^{*}$ be the smallest $s$ such that $d_{s}=1$ . Since the scales $2^{-s-1}$ are geometrically decreasing, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{s:d_{s}=1}\\sum_{x\\in S_{s}\\cap\\{x|P(x)\\leq2\\kappa/n\\}}P(x)\\leq2\\sum_{s:d_{s}=1}2^{-s-1}\\leq4\\cdot2^{-s^{*}-1}\\leq\\frac{1}{2}\\sum_{x\\notin\\gamma_{P}(2\\kappa)}P(x).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "It follows that $\\begin{array}{r}{\\sum_{s:d_{s}>1}\\sum_{x\\in S_{s}\\cap\\{x|P(x)<2\\kappa/n\\}}P(x)\\ge(1/2)\\sum_{x\\notin\\gamma_{P}(2\\kappa)}P(x)}\\end{array}$ . Further, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{\\substack{\\phi\\in\\gamma_{P}(2\\kappa)}}P(x)\\le2\\sum_{\\substack{s:d_{s}>1\\,x\\in{\\cal S}_{s}\\cap\\{x|P(x)<2\\kappa/n\\}}}P(x)\\le4\\sum_{\\substack{s:d_{s}>1}}2^{-s-1}d_{s}\\le16\\sum_{\\substack{s:d_{s}>1}}2^{-s-1}\\lfloor d_{s}/2\\rfloor.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus we can (up to constants) ignore scales such that $d_{s}\\,\\leq\\,1\\$ and assume that $d_{s}$ is even for all scales. ", "page_idx": 30}, {"type": "text", "text": "Let $k_{s}=\\lfloor d_{s}/2\\rfloor$ . Now, within each scale $\\mathcal{S}_{s}$ , pair the elements to form $k_{s}$ distinct pairs $(a_{s,j}^{+},a_{s,j}^{-})$ . Given $(u^{0},u^{1},\\cdot\\cdot\\cdot)\\in\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdot\\cdot\\cdot$ , define $p_{u}$ by $p_{u}(a_{s,j}^{+})=p(a_{s,j})+2^{-s-2}$ and $p_{u}(a_{s,j}^{-})=$ $p(a_{s,j})\\,-\\,2^{-s-2}$ if $u_{j}^{s}\\,=\\,+1$ and $p_{u}(a_{s,j}^{+})\\,=\\,p(a_{s,j})\\,-\\,2^{-s-2}$ and $p_{u}(a_{s,j}^{-})\\,=\\,p(a_{s,j})\\,+\\,2^{-s-2}$ if $u_{j}^{s}\\,=\\,-1$ . For all other elements, $p_{u}(x)\\,=\\,p(x)$ . Then, it is easy to see that for all u, $p_{u}~\\in$ ${\\mathcal{N}}(P)$ . Further, using notation from Lemma D.8, Equation (6) is satisfied with $\\begin{array}{r}{\\tau_{s}=\\frac{1}{2}2^{-s-2}}\\end{array}$ since $\\begin{array}{r}{d(\\dot{\\theta}(p_{u}),\\theta(p_{v}))=\\mathrm{T}\\bar{\\mathbf{V}}(p_{u},p_{v})=2\\sum_{s}\\tau_{s}d_{H a m}(u^{s},\\bar{v}^{s})}\\end{array}$ and $\\begin{array}{r}{D_{s}/n=\\operatorname*{max}_{j}\\mathrm{TV}\\bigl(p_{+(s,\\bar{j})},p_{-(s,j)}\\bigr)=}\\end{array}$ $2^{-s-2}$ , which is less than $\\kappa$ whenever $k_{s}\\,>\\,0$ . By eqn (7), $0.9e^{-10\\varepsilon D_{s}}-10D_{s}\\delta\\geq0.2$ whenever $k_{s}>0$ so by Lemma D.8 we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}(P)\\geq\\sum_{s}\\frac{1}{2}2^{-s-2}k_{s}(0.2)=\\frac{0.2}{4}\\sum_{s:d_{s}>1}2^{-s-1}|d_{s}/2|\\geq\\frac{0.2}{4\\times16}\\sum_{x\\notin\\gamma_{P}(2\\kappa)}P(x)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which completes the proof. ", "page_idx": 30}, {"type": "text", "text": "Next we lower bound the statistical term. ", "page_idx": 31}, {"type": "text", "text": "Lemma D.11. For all $n\\in\\mathbb{N},\\,\\varepsilon>0,\\,\\delta\\in[0,1]$ and distributions $P$ , if $n\\geq2$ and $\\varepsilon>2/n,$ then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}(P)\\geq\\mathcal{R}_{N,n}(P)\\geq\\Omega\\left(\\sum_{x\\in[N]}\\operatorname*{min}\\left\\{P(x)(1-P(x)),\\sqrt{\\frac{P(x)(1-P(x)}{n}}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To streamline the notation, we will use $L(x)$ to denote $\\begin{array}{r}{\\operatorname*{min}\\left\\{x(1-x),\\sqrt{\\frac{x(1-x)}{n}}\\right\\}}\\end{array}$ In order to prove Lemma D.11, we will need the following standard result from the statistics literature which allows us to lower bound the performance of any simple classifier distinguishing two distributions $P$ and $Q$ by the KL divergence between $P$ and $Q$ . We give a specific result for distinguishing Bernoulli random variables since we\u2019ll use this in the proof of Lemma D.11. ", "page_idx": 31}, {"type": "text", "text": "Lemma D.12. Given any pair of distributions $P$ and $Q$ on the same domain, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\left(\\operatorname*{Pr}_{X\\sim P^{n}}(\\phi(X)=1)+\\operatorname*{Pr}_{X\\sim Q^{n}}(\\phi(X)=-1)\\right)\\geq\\frac{1}{2}(1-\\sqrt{n\\mathrm{KL}(P,Q)}),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the minimum is over all binary classifiers. In particular, if $P\\,=\\,B e r n o u l\\,\\l2\\,i(p\\,-\\,\\alpha)$ and $Q=B e r n o u l\\,\\!\\!\\wr i(p+\\alpha)$ where $0\\leq{\\dot{\\alpha}}\\leq{\\textstyle\\frac{1}{2}}L{\\dot{(}}p{\\bigr)}$ then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\left(\\operatorname*{Pr}_{X\\sim P^{n}}(\\phi(X)=1)+\\operatorname*{Pr}_{X\\sim Q^{n}}(\\phi(X)=-1)\\right)\\geq1/4,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where again the minimum is over all binary classifiers. ", "page_idx": 31}, {"type": "text", "text": "The proof of Lemma D.12 can be found in Appendix G ", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma $D.I I$ . As in the proof of Lemma D.10, first suppose there exists a scale $s^{*}$ with $d_{s^{*}}=1$ and there exists $x^{*}\\in S_{s^{*}}$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\n{\\frac{1}{2}}L(P(x^{*}))\\geq{\\frac{1}{60}}\\sum_{x\\in[N]}L(P(x)).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then define a distribution $P^{\\prime}$ by $P^{\\prime}(x^{*})\\;=\\;P(x^{*})\\,-\\,{\\textstyle\\frac{1}{2}}L(P(x^{*}))$ and for all $x\\neq x^{*}$ , $P^{\\prime}(x)\\,=$ 1\u2212P (x1\u2217\u2212)+P  (12 xL\u2217()P (x\u2217))P(x). Then P \u2032 \u2208 N(P) since 12L(P(x\u2217)) < 12 min{P(x\u2217), (1 \u2212P(x\u2217))}. Then we will use Lemma D.8 with $k_{0}=1$ and $k_{s}=0$ for $s>0$ , and $\\mathcal{E}_{k_{0}}$ corresponds to $\\{P,P^{\\prime}\\}$ . Now, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathrm{\\dot{U}}(P^{\\prime},P)=\\displaystyle(P(x^{*})-\\frac{1}{2}L(P(x^{*})))\\ln\\frac{P(x^{*})-\\frac{1}{2}L(P(x^{*}))}{P(x^{*})}+\\left(1-P(x^{*})+\\frac{1}{2}L(P(x^{*}))\\right)\\ln\\frac{1-P(x^{*})}{P(x^{*})}}\\\\ {\\leq\\frac{1}{4n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "(for more detail on the proof of this inequality see the proof of Lemma D.12) so ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\Big(\\mathrm{Pr}_{X\\sim P^{n}}(\\phi(X)=1)+\\mathrm{\\tiny~Pr}_{X\\sim P^{\\prime}^{n}}(\\phi(X)=-1)\\Big)\\geq\\frac{1}{2}(1-\\sqrt{n\\mathrm{KL}(P,P^{\\prime})})\\geq1/4\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and $\\begin{array}{r}{\\tau_{0}=\\mathrm{TV}(P,P^{\\prime})=\\frac{1}{2}L(P(x^{*}))}\\end{array}$ . Thus by Lemma D.8, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}(P)\\geq\\mathcal{R}_{\\mathcal{N},n}(P)\\geq\\frac{1}{2}L(P(x^{*}))\\frac{1}{4}\\geq\\frac{1}{480}\\sum_{x\\in[N]}\\frac{1}{2}L(P(x)),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and we are done. ", "page_idx": 31}, {"type": "text", "text": "On the other hand, suppose that for all scales $s$ such that $d_{s}=1$ we have ", "page_idx": 31}, {"type": "equation", "text": "$$\nL(P(x_{s}))\\;\\leq\\;{\\frac{1}{30}}\\sum_{x\\in[N]}L(P(x)),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $S_{s}=\\{x_{s}\\}$ . As in the proof of Lemma D.10, we will argue that we can ignore any singleton scales, and assume that $d_{s}$ is even for all scales. Let $s^{*}=\\operatorname*{min}\\{s>0\\mid d_{s}=1\\}$ so ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s\\to1}L(P(x_{s}))\\leq\\chi_{d_{0}=1}L(P(x_{0}))+\\displaystyle\\sum_{s>0\\to d_{s}=1}^{\\infty}\\operatorname*{min}\\left\\{2^{-s},\\sqrt{\\frac{2^{-s}}{n}}\\right\\}}\\\\ {\\displaystyle}&{\\leq\\chi_{d_{0}=1}L(P(x_{0}))+(2+\\sqrt{2})\\operatorname*{min}\\left\\{2^{-s^{*}},\\sqrt{\\frac{2^{-s^{*}}}{n}}\\right\\}}\\\\ {\\displaystyle}&{\\leq\\chi_{d_{0}=1}L(P(x_{0}))+2(2+\\sqrt{2})\\operatorname*{min}\\left\\{P(x_{s^{*}}),\\sqrt{\\frac{P(x_{s^{*}})}{n}}\\right\\}}\\\\ {\\displaystyle}&{\\leq\\chi_{d_{0}=1}L(P(x_{0}))+4(2+\\sqrt{2})L(P(x_{s^{*}}))}\\\\ {\\displaystyle}&{\\leq\\frac{1+4(2+\\sqrt{2})}{30}\\sum_{x\\in[N]}L(P(x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{\\sum_{s:d_{s}>1}\\sum_{x\\in{\\cal{S}}_{s}}L(P(x))\\geq(1/2)\\sum_{x\\in[N]}L(P(x))}\\end{array}$ and so ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s}L(2^{-s-1})\\lfloor d_{s}/2\\rfloor\\ge\\displaystyle\\sum_{s:d_{s}>1}L(2^{-s-1})\\frac{1}{3}d_{s}}\\\\ &{\\quad\\quad\\quad\\quad\\ge\\displaystyle\\sum_{s:d_{s}>1}\\sum_{x\\in\\mathcal{S}_{s}}L(2^{-s-1})\\frac{1}{3}}\\\\ &{\\quad\\quad\\quad\\quad\\ge\\displaystyle\\frac{1}{3\\sqrt{2}}\\sum_{x\\in[N]}L(P(x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the first inequality follows from $\\lfloor d_{s}/2\\rfloor\\geq(1/3)d_{s}$ whenever $d_{s}>1$ , and the second follows because $2^{-s-1}\\leq\\bar{P}(x)\\overset{\\cdot}{\\leq}1/2$ for all $x\\in S_{s}$ such that $d_{s}>1$ . ", "page_idx": 32}, {"type": "text", "text": "Assume that $d_{s}$ is even for all $s$ . Within each scale $\\ensuremath{\\mathcal{S}}_{s}$ , pair the elements to form $k_{s}=d_{s}/2$ distinct pairs $(a_{s,j}^{+},a_{s,j}^{-})$ per scale. For all $s\\,\\in\\,\\mathbb{N}$ , let $\\begin{array}{r}{\\alpha_{s}\\,=\\,\\frac{1}{2}L(2^{-s-1})}\\end{array}$ , and note that for all $x\\in S_{s}$ and $s>0$ , $\\begin{array}{r}{\\alpha_{s}\\leq\\frac{1}{2}L(P(x))}\\end{array}$ . Given $\\left(u^{0},u^{1},\\cdot\\cdot\\cdot\\right)\\in\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdot\\cdot\\cdot$ , define $p_{u}$ by $p_{u}(a_{s,j}^{+})=p(a_{s,j}^{+})\\!+\\!\\alpha_{s}$ and $p_{u}(a_{s,j}^{-})=p(a_{s,j}^{-})-\\alpha_{s}$ if $u_{j}^{s}=+1$ and $p_{u}(a_{s,j}^{+})=p(a_{s,j}^{+})-\\alpha_{s}$ and $p_{u}(a_{s,j}^{-})=p(a_{s,j}^{-})+\\alpha_{s}$ if $u_{j}^{s}=-1$ . For all other elements, $p_{u}(x)\\,=\\,p(x)$ . Then, for all $u$ , $p_{u}\\,\\in\\mathcal{N}(P)$ . Further, using notation from Lemma D.8, we have $\\tau_{s}=\\alpha_{s}$ . Also, for any $(s,j),p_{+(s,j)}$ and $\\ensuremath{\\boldsymbol{p}}_{-(s,j)}$ only differ on $a_{s,j}^{+}$ and $a_{s,j}^{-}$ where $p_{+(s,j)}(a_{s,j}^{+})=P(a_{s,j}^{+})+\\alpha_{s}$ and $p_{-(s,j)}(a_{s,j}^{+})=P(a_{s,j}^{+})-\\alpha_{s}$ . Therefore, by Lemma D.12, and the post-processing inequality, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\binom{\\operatorname*{Pr}}{X\\sim p_{+(s,j^{\\prime}}}(\\phi(X)=1)+\\operatorname*{Pr}_{X\\sim p_{-(s,j^{n}}}(\\phi(X)=-1)\\right)\\geq1/4.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma D.8 then implies the result. ", "page_idx": 32}, {"type": "text", "text": "Theorem D.7 follows immediately from Lemma D.9, Lemma D.10 and Lemma D.11. ", "page_idx": 32}, {"type": "text", "text": "D.3 An $\\varepsilon$ -DP Distribution Estimation Algorithm ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Now, let us return to HSTs and designing an estimation algorithm that achieves the target estimation rate, up to logarithmic factors. As in the one-dimensional setting, we want to restrict to only privately estimating the density at a small number $(\\approx\\varepsilon n)$ of points. While we could try to mimic the onedimensional solution by privately estimating a solution to the $\\varepsilon n$ -median problem, it\u2019s not clear how to prove that such an approach is instance-optimal. It turns out that a simpler solution more amenable to analysis will suffice. Our algorithm has two stages; first we attempt to find the set of $\\frac{\\log(1/\\delta)}{\\varepsilon n}$ -active nodes, then we estimate the weight of these active nodes. Since these nodes have weight greater than log\u03b5(1n/\u03b4), we can privately estimate them to within constant multiplicative error. ", "page_idx": 32}, {"type": "text", "text": "Let $\\mathcal{X}$ be the underlying metric space so $P\\in\\Delta(\\mathcal X)$ . For any set $S$ of nodes and a function $F$ defined on the nodes, define the function $F|_{S}$ as $F|_{S}(\\nu)=F(\\nu)$ if $\\nu\\in S$ and $F|_{S}(\\nu)=0$ otherwise. Given two functions $F$ and $G$ defined on the nodes, we define ", "page_idx": 33}, {"type": "equation", "text": "$$\n2\\mathfrak{N}(F,G)=\\sum_{\\nu}r_{\\nu}|F(\\nu)-G(\\nu)|,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $r_{\\nu}$ is the length of the edge connecting $\\nu$ to its parent, and the sum is over all nodes in the tree. So by Lemma D.4, $\\mathcal{W}(P,Q)=\\mathfrak{W}(\\mathfrak{G}_{P},\\mathfrak{G}_{Q})$ . Note that $\\mathfrak{W}$ satisfies the triangle inequality. ", "page_idx": 33}, {"type": "text", "text": "Algorithm 1 PrivDensityEstTree ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1: Input: $D\\in\\mathcal{X}^{n},\\varepsilon$ ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "2: $\\widehat{\\mathfrak{G}_{P}}=\\mathtt{E m p D i s t}(D)$ \u25b7Compute empirical distribution.   \n3: $\\hat{\\gamma}_{\\varepsilon}=$ LocateActiveNodes $(\\widehat{\\mathfrak{G}_{P}};\\varepsilon)$ \u25b7Privately approximate set of active nodes. $\\widetilde{\\mathfrak{G}_{\\hat{P}_{n},\\hat{\\gamma}_{\\varepsilon}}}(x)=\\left\\{\\frac{0}{\\mathfrak{G}_{P}}_{P}(x)+\\mathsf{L a p}(\\frac{1}{\\varepsilon n})\\right\\}$ if x \u2208/\u03b3\u02c6\u03b5   \n4: Define $\\hat{\\mathfrak{G}}_{\\hat{P}_{n},\\hat{\\gamma}_{\\varepsilon}}$ by \u25b7Approximate densities. otherwise.   \n5: P\u02c6n,\u03b5 = Projection( G P\u02c6 n,\u03b3\u02c6\u03b5) \u25b7Project noisy densities onto space of distributions.   \n6: return $\\hat{P}_{n,\\varepsilon}$ ", "page_idx": 33}, {"type": "text", "text": "A high-level outline of the proposed algorithm is given in Algorithm 1. Now, we state the main theorem of this section. ", "page_idx": 33}, {"type": "text", "text": "Theorem D.13. Given any $\\varepsilon>0$ , PrivDensityEstTree is $(D_{T}+1)\\varepsilon$ -DP. Given a distribution $P$ , with probability $1-(D_{T}\\log n+4D_{T}\\varepsilon n)\\beta,$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{W}(P,\\hat{P}_{\\varepsilon})=O\\left(\\sum_{\\ell\\in[D_{T}]}\\sum_{x\\in[N_{\\ell}]}\\operatorname*{min}\\left\\{P_{\\ell}(x),1-P_{\\ell}(x)\\sqrt{\\frac{P_{\\ell}(x)\\log(n/\\beta)}{n}}\\right\\}\\right.\\qquad\\qquad\\qquad\\left.}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This bound has the same three terms as our lower bound on $\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}$ in Theorem D.5 corresponding again to the empirical error (the error inherent even in the absence of a privacy requirement), the error from the private algorithm not being able to estimate the probability of events that occur with probability less than $\\approx\\log(1/\\delta)/\\varepsilon n$ , and the error due to the noise added to the active nodes. The maximum over the levels that appeared in the lower bound is replaced with a sum over the levels in the upper bound, so, up to logarithmic factors, the upper bound is within a factor of $D_{T}$ of the lower bound. Since we can not hope to locate the set of $\\log(1/\\delta)/(\\varepsilon n)$ -active nodes exactly with a private algorithm, we find a set $\\hat{\\gamma}_{n}$ that is guaranteed to satisfy ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\gamma_{P}\\left(\\operatorname*{max}\\left\\{\\frac{2}{\\varepsilon n}+2\\frac{\\log(2/\\beta)}{n},\\frac{192\\log(n/\\beta)}{n}\\right\\}\\right)\\subset\\hat{\\gamma}_{\\varepsilon}\\subset\\gamma_{P}\\left(\\frac{1}{2\\varepsilon n}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that max $\\begin{array}{r}{\\left\\{\\frac{2}{\\varepsilon n}+2\\frac{\\log(2/\\beta)}{n},\\frac{192\\log(n/\\beta)}{n}\\right\\}\\ \\leq\\ \\frac{C\\log(n/\\beta)}{\\varepsilon n}}\\end{array}$ so the error introduced here by not estimating $\\textstyle\\gamma_{P}\\left({\\frac{1}{\\varepsilon n}}\\right)$ perfectly is at most a logarithmic multiplicative factor. ", "page_idx": 33}, {"type": "text", "text": "The first step of our algorithm is to estimate the empirical distribution. We use a truncated version of the standard empirical distribution. This allows us to achieve an error rate of $\\operatorname*{min}\\{P(x),{\\sqrt{P(x)/n}}\\}$ even when $P(x)$ is small. ", "page_idx": 33}, {"type": "text", "text": "The proof of the following lemma is contained in Appendix G. ", "page_idx": 33}, {"type": "text", "text": "Lemma D.14. For any distribution $P$ , $i f\\mathrm{log}(n/\\beta)>1$ then with probability $1-3D_{T}\\beta$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathfrak{M}(\\widehat{\\mathfrak{G}_{P}},\\mathfrak{G}_{P})\\leq\\sum_{\\ell\\in[D_{T}]}\\sum_{x\\in[N_{\\ell}]}\\operatorname*{min}\\left\\{P_{\\ell}(x)(1-P_{\\ell}(x)),4\\sqrt{3\\frac{P_{\\ell}(x)(1-P_{\\ell}(x))\\log(n/\\beta)}{n}}\\right\\}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Algorithm 2 EmpDist ", "page_idx": 34}, {"type": "text", "text": "1: Input: $D\\in{\\mathcal{X}}^{n},A$   \n2: Let $\\hat{P}_{n}$ be the empirical distribution.   \n3: for all node $\\nu$ do 0 G \u02c6 (\u03bd) < log(n/\u03b2)   \n4: G P (\u03bd) = 1 G \u02c6 (\u03bd) > 1 \u2212 log n/\u03b2) G P\u02c6 (\u03bd) otherwise ", "page_idx": 34}, {"type": "text", "text": "Algorithm 3 LocateActiveNodes ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1: Input: $\\widehat{\\mathfrak{G}_{P}},\\varepsilon$   \n2: Let $\\ell=0$ and $\\hat{\\gamma}_{\\varepsilon,0}=\\{\\nu\\}$ where $\\nu$ is the root node.   \n3: while $\\hat{\\gamma}_{\\varepsilon,\\ell}\\neq\\varnothing$ and $\\ell<D_{T}$ do   \n4: $\\hat{\\gamma}_{\\varepsilon,\\ell+1}=\\mathcal{O}$   \n5: for all $\\nu\\in\\hat{\\gamma}_{\\varepsilon,\\ell}$ do   \n6: for all children $\\nu^{\\prime}$ of $\\nu$ do   \n7: if $\\begin{array}{r}{\\widehat{\\mathfrak{G}_{P}}(\\nu^{\\prime})+\\mathsf{L a p}(\\frac{1}{\\varepsilon n})>2\\kappa+\\frac{\\log(2/\\beta)}{\\varepsilon n}\\,\\mathfrak{t}}\\end{array}$ log(2/\u03b2)then   \n8: $\\hat{\\gamma}_{\\varepsilon,\\ell+1}=\\hat{\\gamma}_{\\varepsilon,\\ell+1}+\\left\\{\\nu^{\\prime}\\right\\}$   \n9: \u2113= \u2113+ 1   \n10: return \u222a\u03b3\u02c6\u03b5,\u2113 ", "page_idx": 34}, {"type": "text", "text": "The goal of Algorithm 3 is to estimate the set of $1/(\\varepsilon n)$ -active nodes. ", "page_idx": 34}, {"type": "text", "text": "The next lemma allows us to bound how close to the goal we get. The proof is contained in Appendix G. ", "page_idx": 34}, {"type": "text", "text": "Lemma D.15. Let $\\hat{\\gamma}_{\\varepsilon}$ be the set of active nodes found in Algorithm 1. Then with probability 1 \u2212 $D_{T}(\\log n+4\\varepsilon n)\\beta$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\gamma_{P}\\left(\\operatorname*{max}\\left\\{\\frac{2}{\\varepsilon n}+4\\frac{\\log(2/\\beta)}{\\varepsilon n},\\frac{192\\log(n/\\beta)}{n}\\right\\}\\right)\\subset\\hat{\\gamma}_{\\varepsilon}\\subset\\gamma_{P}\\left(\\frac{1}{2\\varepsilon n}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We also prove the following lemma relating the error due to estimating the active nodes to a quantity depending on the true active nodes. ", "page_idx": 34}, {"type": "text", "text": "Lemma D.16. If \u03b3P max{ \u03b52n + 4 log(\u03b52n/\u03b2), 192 logn(n/\u03b2)} ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathfrak{M}\\widehat{(\\mathfrak{G}_{P},\\mathfrak{G}_{P}|_{\\hat{\\gamma}_{\\varepsilon}})}\\le\\mathfrak{M}\\bigl(\\mathfrak{G}_{P},\\widehat{\\mathfrak{G}_{P}}\\bigr)+\\mathfrak{M}\\bigl(\\mathfrak{G}_{P},\\mathfrak{G}_{P}\\big|_{\\gamma_{P}}\\bigl(\\operatorname*{max}\\{\\textstyle\\frac{2}{\\varepsilon n}+4\\frac{\\log(2/\\beta)}{\\varepsilon n},\\frac{192\\log(n/\\beta)}{n}\\}\\bigr)\\bigr)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The key component of this proof is that any discrepancy between the weight of the nodes on $P$ and that assigned by $\\widehat{\\mathfrak{G}_{P}}$ was already paid for in $\\widehat{\\displaystyle\\mathscr{W}(P,\\widehat{\\mathfrak{G}_{P}})}$ . The final step in Algorithm 1 is to project the noisy function $\\widetilde{\\mathfrak{G}_{\\hat{P_{n}},\\hat{\\gamma_{\\varepsilon}}}}$ into the space of distributions on the underlying metric space. We\u2019d like to do this in a way that preserves, up to a constant, the $\\mathfrak{W}$ distance between $P$ and $\\overline{{\\mathfrak{G}_{\\hat{P}_{n},\\hat{\\gamma}_{\\varepsilon}}}}$ . We will do this iteratively starting from the root node, by ensuring that the sum of each node\u2019s children add up to it\u2019s assigned value. Since we know the root node has value 1, this results in a valid distribution. We start from the top of the tree since errors in higher nodes of the contribute more to the Wasserstein distance. While errors in higher nodes of the tree propagate can propagate to lower levels, the predominant influence on the overall error is retained at the top level due to the geometric nature of the edge weights. ", "page_idx": 34}, {"type": "text", "text": "Lemma D.17. For any real-valued function $\\mathfrak{G}$ on the nodes of the HST such that $\\mathfrak{G}(\\nu_{0})=1$ where $\\nu_{0}$ is the root node and given any distribution $P$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}(P,P r o j e c t i o n(\\mathfrak{G}))\\overset{}{\\leq}4\\mathfrak{W}(\\mathfrak{G}_{P},\\mathfrak{G}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining the above lemmas appropriately gives the proof of Theorem D.13 (see Appendix G). ", "page_idx": 34}, {"type": "text", "text": "1: Input: $\\mathfrak{G}$ , a real-valued function on the nodes of the HST such that $\\mathfrak{G}(\\nu_{0})=1$ where $\\nu_{0}$ is the   \nroot node.   \n2: ${\\bar{\\mathfrak{G}}}={\\mathfrak{G}}$   \n3: for $\\ell=0:D_{T}-1$ do   \n4: for all nodes $\\nu$ at level $\\ell$ do   \n5: Let $A_{\\nu}=\\sum\\mathfrak{G}\\mathopen{}\\mathclose\\bgroup\\left(\\nu^{\\prime}\\aftergroup\\egroup\\right)$ where the sum is over the children of $\\nu$ .   \n6: Let $d_{\\nu}$ be  the number of children of $\\nu$   \n7: if $A_{\\nu}=0$ then   \n8: for all children $\\nu^{\\prime}$ of $\\nu$ do   \n9: $\\begin{array}{r}{\\bar{\\mathfrak{G}}(\\nu^{\\prime})=\\frac{1}{d_{\\nu}}\\bar{\\mathfrak{G}}(\\nu)}\\end{array}$   \n10: else   \n11: for all children $\\nu^{\\prime}$ of $\\nu$ do   \n12: $\\begin{array}{r}{\\bar{\\mathfrak{G}}(\\nu^{\\prime})=\\frac{\\bar{\\mathfrak{G}}(\\nu)}{A_{\\nu}}\\mathfrak{G}(\\nu^{\\prime})}\\end{array}$   \n13: return G\u00af ", "page_idx": 35}, {"type": "text", "text": "Proof of Theorem D.13. The privacy follows from the fact that each user contributes to at most $D_{T}$ queries in LocateActiveNodes and at most one coordinate in the computation of $\\widetilde{\\mathfrak{G}_{\\hat{P}_{n},\\hat{\\gamma}_{n}}}$ in line 4 in PrivDensityEstTree. ", "page_idx": 35}, {"type": "text", "text": "For the utility, we will consider each level $\\ell$ individually. First suppose that $\\left|\\gamma_{P_{\\ell}}\\left(1/(2\\varepsilon n)\\right)\\right|>1$ . ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}(P_{\\ell},(\\hat{P}_{\\ell})_{\\ell})\\leq2\\mathfrak{M}((\\mathfrak{G}_{P})_{\\ell},(\\widehat{\\mathfrak{G}_{P_{n},\\hat{\\gamma}_{\\ell}}})_{\\ell})}\\\\ &{\\qquad\\qquad\\leq2\\left(\\mathfrak{M}((\\mathfrak{G}_{P})_{\\ell},(\\widehat{\\mathfrak{G}_{P}})_{\\ell})+\\mathfrak{M}((\\widehat{\\mathfrak{G}_{P}})_{\\ell},(\\widehat{\\mathfrak{G}_{P}}|\\hat{\\gamma}_{\\ell})_{\\ell})+\\mathfrak{M}((\\widehat{\\mathfrak{G}_{P}}|\\hat{\\gamma}_{\\ell})_{\\ell},(\\widehat{\\mathfrak{G}_{P_{n},\\hat{\\gamma}_{\\ell}}})_{\\ell})\\right)~(9)}\\\\ &{\\qquad\\qquad\\leq2\\left(2\\mathfrak{M}((\\mathfrak{G}_{P})_{\\ell},(\\widehat{\\mathfrak{G}_{P}})_{\\ell})+\\mathfrak{M}((\\mathfrak{G}_{P})_{\\ell},(\\mathfrak{G}_{P}|_{\\gamma P}(\\operatorname*{max}\\{\\frac{2}{\\mathfrak{c}_{n}+2\\frac{\\log(2/\\beta)}{n},\\frac{\\log(n/\\beta)}{n}\\})})^{)}\\ell\\right)+\\mathfrak{M}((\\mathfrak{G}_{P})_{\\ell},~(\\mathfrak{G}_{P})_{\\ell})}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the first inequality follow from Lemma D.17, the second inequality follows from the triangle inequality and Lemma D.4, and the third follows from Lemma D.16 and Lemma D.15. Finally, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathfrak{M}(\\widehat{\\mathfrak{G}_{P}}|_{\\hat{\\gamma}_{\\varepsilon}})_{\\ell},(\\widehat{\\mathfrak{G}_{\\hat{P}_{n},\\hat{\\gamma}_{\\varepsilon}}})_{\\ell})\\leq\\sum_{\\nu\\in\\gamma P\\left(\\frac{1}{2\\varepsilon n}\\right)}r_{\\nu}|\\mathsf{L a p}(\\frac{1}{\\varepsilon n})|\\leq\\frac{1}{2}\\sum_{\\nu\\in\\gamma P\\left(\\frac{1}{2\\varepsilon n}\\right)}r_{\\nu}|\\mathsf{L a p}(\\frac{1}{\\varepsilon n})|\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The final statement then follows from Lemma D.14 and basic concentration bounds on the Laplacian distribution. ", "page_idx": 35}, {"type": "text", "text": "If $\\left|\\gamma_{P_{\\ell}}\\left(1/(2\\varepsilon n)\\right)\\right|=1$ , then the proof goes through for all except the final term related to the noise due to privacy. We consider two cases. Let $x\\in\\gamma_{P_{\\ell}}\\left(1/(2\\varepsilon n)\\right)$ . First suppose that $\\begin{array}{r}{P_{\\ell}(x)>1-\\frac{1}{2\\varepsilon n}}\\end{array}$ then no node that is in a level above $x$ , but is not a direct ancestor of $x$ is in $\\gamma_{P_{\\ell}}$ $(1/(2\\varepsilon n))$ . Therefore, since the projection algorithm is top-down, $(\\hat{P}_{n,\\varepsilon})_{\\ell}$ will be concentrated on $x$ . Therefore, the error of level $\\ell$ is simply $(1-P(x))$ , which can be charged to the first term plus the sum of the weight of the inactive nodes, which is in the second term. Next, suppose that P\u2113(x) < 1 \u22122\u03b51n then sum of the inactive nodes (in term two) dominates the error due to adding noise to $P(x)$ \u53e3 ", "page_idx": 35}, {"type": "text", "text": "E Instance Optimal Density Estimation on $\\mathbb{R}$ in Wasserstein distance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Let us now consider the setting of estimating distributions $P$ on $\\mathcal{X}=\\mathbb{R}$ . In this setting, the target estimation rate is that of an algorithm that knows that the distribution is either $P$ or $Q_{P}$ for a distribution $Q_{P}$ such that $D_{\\infty}(P,Q_{P})\\leq\\ln2$ . This definition of instance-optimality strengthens that corresponding to the so-called hardest-one dimensional subproblem [DL91], since this is a harder estimation rate to achieve. A formal description of the target estimation rate is given in Appendix B.1 and Appendix B.3. In Appendix E.1, we lower bound this estimation rate using hypothesis testing techniques. Then, in Appendix E.2, we give an algorithm that up to polylogarithmic factors, uniformly achieves the lower bound, and hence approximately achieves the instance-optimal estimation rate. Our instance optimality results apply to all continuous distributions in a bounded interval with density functions (though it is likely that they apply more generally). All omitted proofs can be found in Appendix J. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "E.1 General Lower Bound ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "To state the main theorem in this section, we will introduce some notation. We start by defining the restriction of a distribution. ", "page_idx": 36}, {"type": "text", "text": "Definition E.1. For any distribution $P$ over $\\mathbb{R}$ with a density function, the restriction $P|_{u,v}$ of $P$ with respect to $u\\leq v\\in\\mathbb{R}$ is defined as the distribution with the following CDF function $F'$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\nF_{P_{u,v}}^{\\prime}(t)=\\left\\{\\!\\!\\begin{array}{l l}{0}&{t<u}\\\\ {F_{P}(x)}&{u\\leq t<v}\\\\ {1}&{t\\geq v}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "If $u=v$ , then $F^{\\prime}$ is a step function that goes to 1 at that point and is 0 prior to that point. ", "page_idx": 36}, {"type": "text", "text": "Also recall the following definition of quantiles. ", "page_idx": 36}, {"type": "text", "text": "Definition E.2. For $0<\\alpha\\le1$ , the $\\alpha$ -quantile of a distribution $P$ over $\\mathbb{R}$ is defined as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\nq_{\\alpha}(P)=\\arg\\operatorname*{min}_{t}\\{\\operatorname*{Pr}_{y\\sim P}(y\\leq t)\\geq\\alpha\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "When the distribution $P$ is clear from context, we will sometimes abuse notation and use $q_{\\alpha}$ when we mean $q_{\\alpha}(P)$ . The main theorem we will prove in this section is the following: ", "page_idx": 36}, {"type": "text", "text": "Theorem E.3. There exists a constant $C$ such that given a continuous distribution $P$ on $\\mathbb{R}$ with bounded expectation and $\\varepsilon\\in(0,1],n\\in\\mathbb{N},$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{l o c,n,\\varepsilon}(P)=\\Omega\\Bigg(\\frac{1}{\\varepsilon n}\\left(q_{1-\\frac{1}{C\\varepsilon n}}-q_{\\frac{1}{C\\varepsilon n}}\\right)+\\mathscr{W}(P,P|_{q_{\\frac{1}{C\\varepsilon n}},q_{1-\\frac{1}{C\\varepsilon n}}})}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{1}{\\sqrt{\\log n}}\\mathbb{E}\\left[\\mathscr{W}(P|_{q_{\\frac{1}{C\\varepsilon n}},q_{1-\\frac{1}{C\\varepsilon n}},\\hat{P}_{n}|_{q_{\\frac{1}{C\\varepsilon n}},q_{1-\\frac{1}{C\\varepsilon n}}})}\\right]\\Bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\hat{P}_{n}$ is the empirical distribution on n samples drawn independently from $P$ . ", "page_idx": 36}, {"type": "text", "text": "The same result can be extended to $(\\varepsilon,\\delta)$ -DP algorithms as well for $\\delta=o\\big(\\frac{1}{n}\\big)$ ", "page_idx": 36}, {"type": "text", "text": "We discuss each of the terms in turn. Note that the final term is related to the expected Wasserstein distance between the empirical distribution and the true distribution. There is now a long line of work characterizing this quantity in terms of the distribution (See Section C), but essentially, if the distribution is more concentrated, this term is smaller. The first term is a very particular inter-quantile distance that is also much smaller for concentrated distributions, and can be large for relatively dispersed distributions. The second term characterizes the length of the tails of the distribution\u2014 longer tails make this Wasserstein distance larger. Overall, this rate is significantly lower for more concentrated distributions with small support, and relatively large for more dispersed distributions. We prove this theorem over the following couple of sections; in Section E.1.1 we characterize the cost of private instance optimality, and in Section E.1.2 we characterize the cost of achieving instance optimality without privacy (this non-private characterization is also new to our work, to the best of our knowledge). Combining the theorems in those sections gives the above result. ", "page_idx": 36}, {"type": "text", "text": "E.1.1 The Privacy Term ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The main theorem we will prove in this section is the following. ", "page_idx": 36}, {"type": "text", "text": "Theorem E.4. Fix $\\varepsilon\\in(0,1]$ , $n\\in\\mathbb N$ . For all distributions $P$ over $\\mathbb{R}$ that have a density function and finite expectation, there exists another distribution $Q^{\\prime\\prime}$ such that $D_{\\infty}(P,Q)\\le2$ , that is indistinguishable from $P$ given $O(n)$ samples such that for all $\\varepsilon$ -DP algorithms $A:\\mathbb{R}^{n}\\rightarrow\\Delta(\\mathbb{R})$ , with probability at least 0.25 over the draws $\\mathbf{x}\\sim P^{n}$ , $\\mathbf{x}^{\\prime}\\sim Q^{\\prime\\prime n}$ , the following holds for some constant $C$ . ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{max}(\\mathcal{W}(P,A(\\mathbf{x})),\\mathcal{W}(Q^{\\prime\\prime},A(\\mathbf{x}^{\\prime})))\\ge\\frac{1}{4C\\varepsilon n}\\left(q_{1-\\frac{1}{C\\varepsilon n}}-q_{\\frac{1}{C\\varepsilon n}}\\right)+\\frac{1}{4}\\mathcal{W}(P,P|_{q_{\\frac{1}{C\\varepsilon n}},q_{1-\\frac{1}{C\\varepsilon n}}}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We start with some notation. For any distribution $P$ with a density, let $f_{P}$ denote its density function. Throughout this section, we will use $q_{\\alpha}$ to represent the $\\alpha$ -quantile of distribution $P$ . Let $L(P)$ be the \u2018starting point\u2019 of distribution $P$ (defined as $\\operatorname*{inf}_{t\\in\\mathbb{R}}\\{t:\\^{\\cdot}F_{P}(t)>0\\}$ if the infimum exists, and $-\\infty$ otherwise. ", "page_idx": 37}, {"type": "text", "text": "Next, we describe some results on differentially private testing that we will use. We say that a testing algorithm $A_{t e s t}$ distinguishes two distributions $P$ and $Q$ with $n$ samples, if given the promise that a dataset of size $n$ is drawn from either $P^{n}$ or $Q^{n}$ , with probability at least $\\textstyle{\\frac{2}{3}}$ , it outputs $P$ if the dataset was drawn from $P^{n}$ and $Q$ if it was drawn from $Q^{n}$ . We now state a theorem lower bounding the sample complexity of differentially private hypothesis testing. ", "page_idx": 37}, {"type": "text", "text": "Theorem E.5 $[\\mathrm{CKM^{+}19}$ , Theorem 1.2]). Fix $n\\in\\mathbb{N},\\varepsilon>0.$ . For every pair of distributions $P,Q$ over $\\mathbb{R}$ , if there exists an $\\varepsilon{-}D P$ testing algorithm4 $A_{t e s t}$ that distinguishes $P$ and $Q$ with $n$ samples, then ", "page_idx": 37}, {"type": "equation", "text": "$$\nn=\\Omega\\left(\\frac{1}{\\varepsilon\\tau(P,Q)+(1-\\tau(P,Q))H^{2}(P^{\\prime},Q^{\\prime})}\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tau(P,Q)=\\operatorname*{max}\\Big\\{\\int_{\\mathbb{R}}\\operatorname*{max}\\{e^{\\varepsilon}f_{P}(t)-f_{Q}(t),0\\}d t,\\int_{\\mathbb{R}}\\operatorname*{max}\\{e^{\\varepsilon}f_{Q}(t)-f_{P}(t),0\\}d t\\Big\\},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and $H^{2}(\\cdot,\\cdot)$ is the squared Hellinger distance between $\\begin{array}{r}{P^{\\prime}\\,=\\,\\frac{\\operatorname*{min}(e^{\\varepsilon}Q,P)}{1-\\tau(P,Q)}}\\end{array}$ ) , and Q\u2032 = min(e\u03b5P,Q ), where $0\\leq\\varepsilon^{\\prime}\\leq\\varepsilon$ is such that if $\\begin{array}{r}{\\tau(P,Q)=\\int_{\\mathbb{R}}\\operatorname*{max}\\{f_{P}(t)-e^{\\varepsilon}f_{Q}(t),0\\}d t}\\end{array}$ , then $\\varepsilon^{\\prime}$ is the maximum value such that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tau(P,Q)=\\int_{\\mathbb{R}}\\operatorname*{max}\\{f_{Q}(t)-e^{\\varepsilon^{\\prime}}f_{P}(t),0\\}d t,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "else $\\varepsilon^{\\prime}$ is the maximum value such that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tau(P,Q)=\\int_{\\mathbb{R}}\\operatorname*{max}\\{f_{P}(t)-e^{\\varepsilon^{\\prime}}f_{Q}(t),0\\}d t.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We now are ready to start proving our main theorem. ", "page_idx": 37}, {"type": "text", "text": "Proof. (of Theorem E.4) The idea is to construct $Q$ from $P$ by moving mass from the leftmost quantiles to the rightmost quantile. We do this such that $Q$ is statistically close enough to $P$ such that the two distributions can not be distinguished with $n$ samples, but is also far from $P$ in Wasserstein distance. This produces a lower bound of $(1/2)\\mathcal{W}(P,Q)$ on how well an algorithm can simultaneously estimate $P$ and $Q$ since if there was an algorithm that produced good estimates of $P$ and $Q$ in Wasserstein distance with $n$ samples, then we could tell them apart, and this would give a contradiction. ", "page_idx": 37}, {"type": "text", "text": "Let $k$ be a quantity to be set later. Formally, we define $Q$ as the distribution with the following density function. ", "page_idx": 37}, {"type": "equation", "text": "$$\nf_{Q}(t)=\\left\\{\\begin{array}{l r}{\\frac{1}{2}f_{P}(t),}&{\\mathrm{for}\\;t<q_{1/k}}\\\\ {f_{P}(t),}&{\\mathrm{for}\\;q_{1/k}\\leq t<q_{1-\\frac{1}{k}}}\\\\ {\\frac{3}{2}f_{P}(t)}&{\\mathrm{for}\\;q_{1-\\frac{1}{k}}\\leq t}\\end{array}\\right\\}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Note that by the definition of $Q$ , we have that $D_{\\infty}(P,Q)\\le2$ . ", "page_idx": 37}, {"type": "text", "text": "We will prove that the sample complexity of telling apart $P$ and $Q$ under $(\\varepsilon,\\delta)$ -DP is $\\Omega(k/\\varepsilon)$ , using known results on hypothesis testing. Then, we will argue that the Wasserstein distance between $P$ and $Q$ is sufficiently large. Setting $k$ appropriately will complete the proof. ", "page_idx": 37}, {"type": "text", "text": "Define $S C_{\\varepsilon,\\delta}(P,Q)$ to be the smallest $n$ such that there exists an $(\\varepsilon,\\delta)$ -DP testing algorithm that distinguishes $P$ and $Q$ ; called the sample complexity of privately distinguishing $P$ and $Q$ . ", "page_idx": 37}, {"type": "text", "text": "Lemma E.6. $S C_{\\varepsilon,\\delta}(P,Q)=\\Omega(k/\\varepsilon)$ . ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The proof of this lemma is in Appendix J. We next argue that $P$ and $Q$ are sufficiently far away in Wasserstein distance. ", "page_idx": 38}, {"type": "text", "text": "Lemma E.7. W(P, Q) \u226521k(q1\u22121 \u2212q1/k) + 21W(P, P|q 1,q1\u22121).", "page_idx": 38}, {"type": "text", "text": "The proof of this lemma is also in Appendix J. ", "page_idx": 38}, {"type": "text", "text": "Finally, we are ready to prove the theorem. Assume that with probability larger than 0.75 over the draw of two datasets $\\mathbf{x}\\sim P^{n}$ , $\\mathbf{x}^{\\prime}\\sim Q^{n}$ , and the randomness used by invocations of algorithm $A$ we have that $\\begin{array}{r}{\\operatorname*{max}(\\mathcal{W}(P,A(\\mathbf{x})),\\mathcal{W}(Q,A(\\mathbf{x}^{\\prime}))\\,<\\,\\frac{1}{2}\\mathcal{W}(P,Q)}\\end{array}$ . Then, given a dataset $\\mathbf{x}^{\\prime\\prime}$ of size $n$ , we can perform the following test: run the differentially private algorithm $A$ on the dataset $\\mathbf{x}^{\\prime\\prime}$ and compute $\\mathcal{W}(P,A(\\mathbf{x}^{\\prime\\prime}))$ and $\\bar{\\mathcal{W}}(Q,A(\\mathbf{x}^{\\prime\\prime}))$ and output the distribution with lower distance. Then, note that $\\begin{array}{r}{\\mathcal{W}(P,Q)\\leq\\mathcal{W}(P,A(\\mathbf{x}^{\\prime\\prime}))+\\mathcal{W}(Q,A(\\mathbf{x}^{\\prime\\prime}))}\\end{array}$ which implies that with probability at least 0.75, $\\begin{array}{r}{\\mathcal{W}(Q,A(\\mathbf{x}^{\\prime\\prime}))\\,>\\,\\frac{1}{2}\\mathcal{W}(P,Q)}\\end{array}$ if the dataset $\\mathbf{x}^{\\prime\\prime}$ was sampled from $P^{n}$ (by the accuracy guarantee). A similar argument shows that with probability at least 0.75, $\\begin{array}{r}{\\mathcal{W}(P,A(\\mathbf{x}^{\\prime\\prime}))\\;>\\;\\frac{1}{2}\\mathcal{W}(P,Q)}\\end{array}$ if the dataset $\\mathbf{x}^{\\prime\\prime}$ was sampled from $Q^{n}$ . Hence, with $n$ samples we have defined a test that distinguishes $P$ and $Q$ . However, for $k=C\\varepsilon n$ for some constant $C$ , by Lemma E.6 we get that any differentially private test distinguishing $P$ and $Q$ requires more than $n$ samples, which is a contradiction. Hence, with probability at least 0.25 over the draw of two datasets $\\mathbf{x}\\,\\sim\\,P^{n}$ , $\\mathbf{x}^{\\prime}\\sim\\mathcal{Q}^{n}$ , and the randomness used by invocations of algorithm $A$ we have that $\\operatorname*{max}(\\mathcal{W}(P,A(\\mathbf{x})),\\mathcal{W}(Q,A(\\mathbf{x}^{\\prime}))\\,\\geq$ $\\begin{array}{r}{\\frac{1}{2}\\mathcal{W}(P,Q)\\geq\\frac{1}{4C\\varepsilon n}(q_{1-\\frac{1}{C\\varepsilon n}}-q_{1/C\\varepsilon n}\\widecheck{)}+\\frac{1}{4}\\mathcal{W}(P,P|_{q_{\\frac{1}{C\\varepsilon n}},q_{1-\\frac{1}{C\\varepsilon\\varepsilon n}}})}\\end{array}$ C\u03b5n ,where the last inequality is by invoking Lemma E.7 with $k=C\\varepsilon n$ . ", "page_idx": 38}, {"type": "text", "text": "E.1.2 Empirical Term ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this section, we prove the following result. ", "page_idx": 38}, {"type": "text", "text": "Theorem E.8. Fix sufficiently large natural numbers $n,k>0$ and let $C,C^{\\prime}>0$ be sufficiently small constants. For all algorithms $A:\\mathbb{R}^{n}\\rightarrow\\Delta_{\\mathbb{R}}$ , the following holds. For all continuous distributions $P$ over $\\mathbb{R}$ with a density and with bounded expectation, there exists another distribution $Q$ (with $D_{\\infty}(P,Q)\\leq\\ln2)$ , that is indistinguishable from $P$ given $O(n)$ samples, such that with probability at least 0.25 over the draws $\\mathbf{x}\\sim P^{n}$ , $\\mathbf{x}^{\\prime}\\sim Q^{n}$ , the following holds. ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{max}(\\mathcal W(P,A(\\mathbf x)),\\mathcal W(Q,A(\\mathbf x^{\\prime})))\\ge\\frac{C^{\\prime}}{\\sqrt{\\log n}}\\mathbb{E}_{\\mathbf x^{\\prime\\prime}\\sim P^{n}}\\left[\\mathcal W\\left(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $q_{\\alpha}$ is the $\\alpha$ -quantile of $P$ . ", "page_idx": 38}, {"type": "text", "text": "Before going into the proof, we state the following result on the sample complexity of testing. This is a folklore result but for a proof of the lower bound see [BY02] and the upper bound see [Can17]. ", "page_idx": 38}, {"type": "text", "text": "Theorem E.9. $F i x\\,n\\in\\mathbb{N},\\varepsilon>0$ . For every pair of distributions $P,Q$ over $\\mathbb{R}$ , if there exists a testing algorithm $A_{t e s t}$ that distinguishes $P$ and $Q$ with $n$ samples, then ", "page_idx": 38}, {"type": "equation", "text": "$$\nn=\\Omega\\left(\\frac{1}{H^{2}(P,Q)}\\right),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "wherer $H^{2}(\\cdot,\\cdot)$ represents the squared Hellinger distance between $P$ and $Q$ . ", "page_idx": 38}, {"type": "text", "text": "Throughout the proof, we will use $q_{\\alpha}$ to represent the $\\alpha$ -quantile of distribution $P$ . ", "page_idx": 38}, {"type": "text", "text": "Proof of Theorem E.8. $Q$ is constructed by adding progressively more mass to $P$ up until $q_{1/2}$ and subtracting proportionate amounts of mass from $P$ afterwards. Intuitively, this is done in such a way that to \u2018change\u2019 $P$ to $Q$ , for all $i\\geq2$ one has to move roughly $\\operatorname*{min}\\{\\frac{{\\mathfrak{q}}}{\\sqrt{2^{i}n}},\\frac{1}{2^{i}}\\}$ mass from $q_{1/2^{i}}$ to $q_{1-1/2^{i}}$ . This ensures that the Wasserstein distance between $P$ and $Q$ is larger than the expected Wasserstein distance between $P$ and its empirical distribution on $n$ samples $\\hat{P}_{n}$ . This is carefully done to ensure that $P$ is indistinguishable from $Q$ . ", "page_idx": 38}, {"type": "text", "text": "consider $i$ in the range $[\\log n,\\infty)$ . For all $t\\in(q_{1/2^{i}},q_{1/2^{i-1}}]$ , we set $\\begin{array}{r}{f_{Q}(t)=f_{P}(t)\\left[1+\\frac{1}{2}\\right]}\\end{array}$ . For all $t\\in(q_{1-1/2^{i-1}},q_{1-1/2^{i}}]$ , we set $\\begin{array}{r}{f_{Q}(t)=f_{P}(t)\\left[1-\\frac{1}{2}\\right]}\\end{array}$ . Note that $P$ has bounded expectation by assumption, and hence, so does $Q$ . Additionally, note that $D_{\\infty}(P,Q)\\leq\\ln2$ . ", "page_idx": 39}, {"type": "text", "text": "There are two key considerations balanced in the design of $Q$ . On one hand, we need $Q$ to be indistinguishable from $P$ given ${\\tilde{O}}(n)$ samples. On the other hand, we need $Q$ to be sufficiently far away from $P$ in Wasserstein distance. This ensures that given an accurate algorithm for estimating the density of the distribution (in Wasserstein distance) given access to $\\tilde{O(n)}$ samples from it, we can design a test distinguishing $P$ and $Q$ with that many samples, thereby contradicting their indistinguishability. ", "page_idx": 39}, {"type": "text", "text": "Detailed proofs of claims below can be found in Appendix J. First, we show that $P$ is indistinguishable from $Q$ . ", "page_idx": 39}, {"type": "text", "text": "Lemma E.10. ", "text_level": 1, "page_idx": 39}, {"type": "equation", "text": "$$\nK L(P,Q)=O(\\log n/n).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Next, we establish a lower bound on the Wasserstein distance between $P$ and $Q$ . ", "page_idx": 39}, {"type": "text", "text": "Lemma E.11. ", "text_level": 1, "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{W}(P,Q)\\geq\\frac{1}{4}\\left[\\sum_{j=2}^{\\log n-1}\\frac{1}{\\sqrt{2^{j}n}}\\left[q_{1-1/2^{j}}-q_{1/2^{j}}\\right]+\\sum_{j=\\log n}^{\\infty}\\frac{1}{2^{j}}\\left[q_{1-1/2^{j}}-q_{1/2^{j}}\\right]\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Next, we upper bound the expected Wasserstein distance between the distribution $P$ and its empirical distribution on $n$ samples. ", "page_idx": 39}, {"type": "text", "text": "Lemma E.12. ", "text_level": 1, "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{W}(P,\\hat{P}_{n})]\\leq8\\left[\\sum_{i=2}^{\\log n-1}\\frac{1}{\\sqrt{2^{i}n}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]+\\sum_{i=\\log n}^{\\infty}\\frac{1}{2^{i}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]\\right]\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We now prove a simple claim regarding restrictions. ", "page_idx": 39}, {"type": "text", "text": "Claim E.13 (Restrictions preserve Wasserstein distance). For all datasets $\\mathbf{x}$ , and any natural number $k>1$ we have that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})\\leq\\mathcal{W}(P,\\hat{P}_{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Finally, we are ready to put the above lemmas together to prove Theorem E.8. Fix n\u2032 = C long n. Assume, for sake of contradiction, that with probability larger than 0.75 over the draw of two datasets $\\mathbf{x}\\,\\sim\\,P^{n^{\\prime}}$ , $\\mathbf{x}^{\\prime}\\sim Q^{n^{\\prime}}$ , and the randomness used by invocations of algorithm $A$ we have that $\\begin{array}{r}{\\operatorname*{max}(\\mathcal{W}(P,A(\\mathbf{x})),\\mathcal{W}(Q,A(\\mathbf{x}^{\\prime}))\\ \\leq\\ \\frac{1}{2}W_{1}(P,Q)}\\end{array}$ . Then, given a dataset $\\mathbf{x}^{\\prime\\prime}$ of size $n^{\\prime}$ , we perform the following test: run the differentially private algorithm $A$ on the dataset $\\mathbf{x}^{\\prime\\prime}$ and compute $\\mathcal{W}(P,A(\\mathbf{x}^{\\prime\\prime}))$ and $\\mathcal{W}(Q,A(\\mathbf{x}^{\\prime\\prime}))$ and output the distribution with lower distance. Then, note that $\\begin{array}{r}{\\mathcal{W}(P,Q)\\leq\\mathcal{W}(P,A(\\mathbf{x}^{\\prime\\prime}))+\\mathcal{W}(Q,A(\\mathbf{x}^{\\prime\\prime}))}\\end{array}$ which implies that with probability at least 0.75, $\\begin{array}{r}{\\mathcal{W}(Q,A(\\mathbf{x}^{\\prime\\prime}))\\ge\\frac{1}{2}\\mathcal{W}(P,Q)}\\end{array}$ if $\\mathbf{x}^{\\prime\\prime}\\sim P^{n^{\\prime}}$ (by the accuracy guarantee). A similar argument shows that with probability at least 0.75, $\\begin{array}{r}{\\mathcal{W}(P,A(\\mathbf{x}^{\\prime\\prime}))\\geq\\frac{1}{2}\\mathcal{W}(P,Q)}\\end{array}$ if $\\mathbf{x}^{\\prime\\prime}\\sim Q^{n^{\\prime}}$ . Hence, with $n^{\\prime}$ samples we have defined a test that distinguishes $P$ and $Q$ . However, by Lemma E.10 bounding the $K L$ divergence between $P$ and $Q$ , Theorem E.9 on sample complexity lower bounds for testing, and Lemma A.7 on the relationship between KL and Hellinger distance, we get that any statistical test distinguishing $P$ and $Q$ requires more than $n^{\\prime}$ samples, which is a contradiction. Hence, with probability at least 0.25 over the draw of two datasets $\\mathbf{x}\\sim P^{n^{\\prime}}$ , $\\mathbf{x}^{\\prime}\\sim Q^{n^{\\prime}}$ , and the randomness used by invocations of algorithm $A$ we must have that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{max}(\\mathcal{W}(P,A(\\mathbf{x})),\\mathcal{W}(Q,A(\\mathbf{x}^{\\prime}))\\geq\\frac12\\mathcal{W}(P,Q).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Next, note that by Lemma E.12 (with value $n^{\\prime}$ ), we have that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}[\\mathcal{W}(P,\\hat{P}_{n^{\\prime}})]\\leq8\\left[\\overset{\\log n^{\\prime}-1}{\\underset{i=2}{\\sum}}\\frac{1}{\\sqrt{2i}n^{\\prime}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]+\\underset{i=\\log n^{\\prime}}{\\sum}\\frac{1}{2^{i}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ =8\\left[\\overset{\\log\\underset{\\sum=0}{\\sum}}{\\underset{i=2}{\\sum}}\\frac{1}{\\sqrt{2i}n}\\frac{\\sqrt{C\\log n}}{\\sqrt{2i}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]+\\underset{i=\\log\\underset{\\tau_{\\mathrm{Ron}}}{\\sum}}{\\sum}\\frac{1}{2^{i}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ =8\\left[\\sqrt{C\\log n}\\underset{i=2}{\\overset{\\log n-\\log(C\\log n)-1}{\\sum}}\\frac{1}{\\sqrt{2^{i}n}}\\left[\\underset{\\sqrt{2i}n}{\\prod}[q_{1-1/2^{i}}-q_{1/2^{i}}]+\\underset{i=\\log n-\\log(C\\log n)}{\\sum}\\frac{1}{2^{i}}\\left[q_{1}\\right.}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots\\underset{i=\\log n}{\\sum}\\frac{1}{2^{i}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Analyzing the middle term in the above sum, we have that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\stackrel{\\mathrm{log}\\,n-1}{=\\log n-\\log(C\\log n)}}\\frac{1}{2^{i}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]\\leq\\sum_{\\stackrel{\\mathrm{log}\\,n-1}{i=\\log n-\\log(C\\log n)}}\\frac{1}{\\sqrt{2^{i}}}\\frac{1}{\\sqrt{2^{\\log n-\\log(C\\log n)}}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]}}\\\\ &{}&{\\leq\\sum_{\\stackrel{\\mathrm{log}\\,n-1}{i=\\log n-\\log(C\\log n)}}^{\\log n-1}\\frac{1}{\\sqrt{2^{i}}}\\frac{\\sqrt{C\\log n}}{\\sqrt{n}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]}\\\\ &{}&{=\\sqrt{C\\log n}\\underbrace{\\log n-1}_{i=\\log n-\\log(C\\log n)}\\frac{1}{\\sqrt{2^{i}n}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Substituting this back in the previous sum, we have that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\xi[\\mathcal{W}(P,\\hat{P}_{n^{\\prime}})]\\leq8\\Bigg[\\sqrt{C\\log n}\\sum_{i=2}^{\\log n-\\log(C\\log n)-1}\\frac{1}{\\sqrt{2^{i n}}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle\\sum_{i=\\log n-\\log(C\\log n)}\\frac{1}{\\sqrt{2^{i n}}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]+\\sum_{i=\\log n}^{\\infty}\\frac{1}{2^{i}}\\left[q_{1-1/2^{i}}\\right]}\\\\ {\\displaystyle\\leq8\\sqrt{C\\log n}\\left[\\sum_{i=2}^{\\log n-1}\\frac{1}{\\sqrt{2^{i n}}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]+\\sum_{i=\\log n}^{\\infty}\\frac{1}{2^{i}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]\\right]}\\\\ {\\displaystyle\\leq16\\sqrt{C\\log n}\\left[\\sum_{i=2}^{\\log n-1}\\frac{1}{\\sqrt{2^{i n}}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]+\\sum_{i=\\log n}^{\\infty}\\frac{1}{2^{i}}\\left[q_{1-1/2^{i}}-q_{1/2^{i}}\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where in the last inequality we use the fact that $n^{\\prime}\\geq{\\sqrt{n}}$ . Hence, by Lemma E.11 (which gives a lower bound on $\\mathcal{W}(P,Q))$ in conjunction with the above equation, we have that $\\mathcal{W}(P,\\bar{Q})\\;\\geq$ $\\frac{C^{\\prime}}{\\sqrt{\\log n^{\\prime}}}\\mathbb{E}[P,\\hat{P}_{n^{\\prime}}]$ for some sufficiently small constant $C^{\\prime}$ . Substituting back in Equation 10, we have that with probability at least 0.25 over the draw of two datasets $\\mathbf{x}\\,\\sim\\,P^{n^{\\prime}}$ , $\\mathbf{x}^{\\prime}\\sim\\,Q^{n^{\\prime}}$ , and the randomness used by invocations of algorithm $A$ we have that ", "page_idx": 40}, {"type": "equation", "text": "$$\n^{\\gamma}(P,A(\\mathbf{x})),\\mathcal{W}(Q,A(\\mathbf{x}^{\\prime}))\\geq\\frac12\\frac{C^{\\prime}}{\\sqrt{\\log n^{\\prime}}}\\mathbb{E}[\\mathcal{W}(P,\\hat{P}_{n^{\\prime}})]\\geq\\frac12\\frac{C^{\\prime}}{\\sqrt{\\log n^{\\prime}}}\\mathbb{E}\\left[\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n^{\\prime}}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})\\right]\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "as required. ", "page_idx": 40}, {"type": "text", "text": "E.2 Upper Bound ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In this section, we describe an algorithm that achieves the instance optimal rate described in the previous section (up to polylogarithmic factors in some of the terms). ", "page_idx": 40}, {"type": "text", "text": "We will be looking at distributions $P$ supported on a discrete, ordered interval $\\{a,a\\!+\\!\\gamma,\\ldots,b\\!-\\!\\gamma,b\\}$ . Note that by a simple coupling argument, any continuous distribution $P^{c o n t}$ on $[a,b]$ is at most $\\gamma$ away in Wasserstein distance from a distribution on this grid. The dependence on $\\gamma$ in our bounds for discrete distributions will be inverse polylogarithmic (or better), and so our algorithms for estimating distributions $P$ in the interval $\\{a,a\\!+\\!\\gamma,\\ldots,b\\!-\\!\\gamma,b\\}$ also work to give similar bounds for continuous distributions on $[a,b]$ , up to a small additive factor of $\\gamma$ , which can be set to any inverse polynomial in the dataset size without significantly affecting our bounds. ", "page_idx": 41}, {"type": "text", "text": "Formally, we will prove the following theorem (See Theorem E.15 for a more detailed statement). ", "page_idx": 41}, {"type": "text", "text": "Theorem E.14. Fix $\\varepsilon,\\beta\\,\\in\\,(0,1]$ , $a,b\\in\\mathbb{R},$ , and $\\gamma<b-a\\in\\mathbb{R}$ such that $\\frac{b\\!-\\!a}{\\gamma}$ is an integer. Let $n\\in\\mathbb{N}>c_{2}\\frac{\\log^{4}\\frac{b-a}{\\beta\\gamma}}{\\varepsilon}$ c2log4\u03b5 b\u03b2\u2212\u03b3a for some sufficiently large constant c2. There exists an \u03b5-DP algorithm A that for any distribution on satisfies the following. When run with input a random sample $\\mathbf{x}\\sim P^{n}$ , $A$ outputs a distribution $P^{D P}$ such that with probability at least $1-\\beta$ over the randomness of $\\mathbf{x}$ and the algorithm, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathit{W}(P,P^{D P})=O\\left(\\frac{1}{k}\\left(q_{1-\\frac{1}{k}}-q_{\\frac{1}{k}}\\right)+\\mathcal{W}(P,P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})+\\sqrt{\\log\\frac{n}{\\beta}}\\mathbb{E}\\left[\\mathcal{W}\\left(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}}\\right)\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\hat{P}_{n}$ is the empirical distribution on n samples drawn independently from $P$ , $q_{\\alpha}$ represents the $\\alpha$ -quantile of distribution $P$ , and $\\begin{array}{r}{k=\\lceil\\frac{\\varepsilon n}{4c_{3}\\log^{3}\\frac{b-a}{\\beta\\gamma}\\log\\frac{n}{\\beta}}\\rceil}\\end{array}$ for a sufficiently large constant $c_{3}$ . ", "page_idx": 41}, {"type": "text", "text": "Since $k\\approx\\varepsilon n/\\log(n)$ , this upper bound matches the lower bound in Theorem E.3 in its dependence on $\\varepsilon$ and its dependence on $n$ (up to logarithmic factors in $n$ ). The algorithm that we will analyze proceeds by estimating sufficiently many quantiles from the empirical distribution and distributing mass evenly between the chosen quantiles. The number of quantiles is chosen carefully to ensure that the estimated $\\alpha$ -quantiles are also approximately $\\alpha$ -quantiles for the empirical distribution (and hence also approximately for the true distribution), and to ensure that the CDF of the output distribution closely tracks the CDF of the empirical distribution. Through a careful analysis, we are able to leverage these properties to give instance optimality guarantees for the accuracy of the algorithm. ", "page_idx": 41}, {"type": "text", "text": "E.2.1 Algorithm for density estimation ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Algorithm 5 is our algorithm for density estimation, and proceeds by differentially privately estimating sufficiently many quantiles of the distribution and placing equal mass on each of them. We argue that a simple CDF based differentially private quantiles estimator $A_{q u a n t}$ satisfies a specific guarantee that will be key to our analysis. See Appendix I for more details about the quantiles algorithm and formal statements and proofs therein. ", "page_idx": 41}, {"type": "text", "text": "Algorithm 5 Algorithm $A$ for estimating a distribution on $\\mathbb{R}$ Input: $\\mathbf{x}\\,=\\,(x_{1},.\\,.\\,.\\,,x_{n})\\,\\sim\\,P^{n}$ , privacy parameter $\\varepsilon$ , interval end-points $a,b$ , granularity $\\gamma$ , access to algorithm $A_{q u a n t}$ Output: Distribution $P^{D P}$ on $\\mathbb{R}$ .   \n1: Let $k$ be set to $\\left\\lceil{\\frac{\\varepsilon n}{4c_{3}\\log^{3}{\\frac{b-a}{\\beta\\gamma}}\\log{\\frac{n}{\\beta}}}}\\right\\rceil$ for a sufficiently large constant $c_{3}$ .   \n2: Use Algorithm $A_{q u a n t}$ referenced in Theorem I.2 with inputs interval end points $a,b$ , granularity $\\gamma$ , ${\\bf x}\\;=\\;(\\dot{x_{1}},\\dot{},\\dots,x_{n})\\;\\in\\;\\{a,a\\,+\\,\\gamma,\\dots,b\\,-\\,\\gamma,b\\}^{n}$ , and desired quantile values $\\alpha=$ $\\{1/2k,3/2k,5/2k,...,(2k-1)/2k\\}$ , and let the outputs be $\\tilde{q}_{1}\\ldots,\\tilde{q}_{k}$ .   \n3: for $j\\in[k]$ do   \n4: Set $\\begin{array}{r}{\\dot{P}^{\\dot{D}P}(\\tilde{q}_{j})=\\frac{1}{k}}\\end{array}$ .   \n5: Output P DP . ", "page_idx": 41}, {"type": "text", "text": "Observe that Algorithm 5 inherits the privacy of $A_{q u a n t}$ , since it simply postprocesses the quantiles it receives from that subroutine, and hence is also $\\varepsilon$ -DP. ", "page_idx": 41}, {"type": "text", "text": "Now, we are in a position to state our main theorem, which bounds the Wasserstein distance between the distribution output by our algorithm, and the underlying probability distribution $P$ . ", "page_idx": 41}, {"type": "text", "text": "Theorem E.15. $\\mathit{\\ddot{i}x}\\;\\varepsilon,\\beta\\,\\in\\,(0,1],\\;a,b\\,\\in\\,\\mathbb{R},$ , and $\\gamma<b-a\\in\\mathbb{R}$ such that $\\frac{b\\!-\\!a}{\\gamma}$ is an integer. Let $n\\in\\mathbb{N}>c_{2}\\frac{\\log^{4}\\frac{b-a}{\\gamma\\beta\\varepsilon}}{\\varepsilon}$ 2log4\u03b5 b\u03b3\u2212\u03b2\u03b5a for some sufficiently large constant c2. Let P be any distribution supported on $\\{a,a+\\gamma,a+\\bar{2}\\gamma,\\ldots,b-\\gamma,b\\}$ , and . ", "page_idx": 42}, {"type": "text", "text": "Then, Algorithm 5, when given inputs $\\mathbf{x}$ , privacy parameter $\\varepsilon$ , interval end points $a,b$ , and granularity $\\gamma_{;}$ , outputs a distribution $P^{D P}$ such that with probability at least $1-O(\\beta)$ over the randomness of $\\mathbf{\\dot{x}}$ and the algorithm, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\hat{P}_{n}$ is the uniform distribution on x, $q_{\\alpha}$ represents the $\\alpha$ -quantile of distribution $P,\\,c,C^{\\prime\\prime}$ are sufficiently large constants, and k = \u23084c3 log3 \u03b5bn\u03b2\u2212\u03b3a log \u03b2n \u2309 , where $c_{3}$ is a sufficiently large constant. ", "page_idx": 42}, {"type": "text", "text": "We note that using more sophisticated differentially private CDF estimators to estimate quantiles i(smuacthe  adsi fofenrees nitni a[l BpNriSvaVc1y,5 , $\\mathrm{CLN}^{+}23]$ e),r  wdee pceannd aelnscoe  oobnt atihne a  sivzeer soiof nt hoef  tdhoem saaimn $\\frac{b\\!-\\!a}{\\gamma}$ o(roenmly f $\\log^{*}(\\frac{b-a}{\\gamma})$ as opposed to poly $\\scriptstyle\\log\\left({\\frac{b-a}{\\gamma}}\\right)$ , where $\\log^{*}t$ is the number of times log has to be applied to $t$ to get it to be $\\leq1$ ). 5 ", "page_idx": 42}, {"type": "text", "text": "To prove Theorem E.15, we first relate the Wasserstein distance of interest (between the true distribution $P$ and the algorithm\u2019s output distribution $P^{D P}$ to a quantity related to an appropriately chosen restriction. Let $q_{\\alpha}$ represent the $\\alpha$ -quantile of $P$ and $\\hat{q}_{\\alpha}$ represent the $\\alpha$ -quantile of $\\hat{P}_{n}$ and $\\tilde{q}_{\\alpha}$ represent the $\\alpha$ -quantiles of $P^{D P}$ . We also note that all these distributions (and others that will come up in the proof) are bounded distributions over the real line and so we can freely apply the triangle inequality for Wasserstein distance, and the cumulative distribution formula for Wasserstein distance (Lemma A.3). The proof of the main theorem will follow from the following lemmas (all proved in Appendix J). ", "page_idx": 42}, {"type": "text", "text": "Lemma E.16. Let $C^{\\prime\\prime}>0$ be a sufficiently large constant, and let $n>0$ be sufficiently large. With probability at least $1-O(\\beta)$ over the randomness in data samples and Algorithm $5$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}(P,P^{D P})\\leq\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},P^{D P}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})+C^{\\prime\\prime}\\mathcal{W}(P,P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Lemma E.17 (Wasserstein in terms of quantiles). For all datasets $\\mathbf{x}$ (with data entries in $[a,b])$ , with probability at least $1-\\beta$ over the randomness of Algorithm $5$ , we have that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left.\\mathcal{W}(\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},P^{D P}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})\\leq\\frac{2}{k}\\left(q_{1-1/k}-q_{1/k}\\right),\\right.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\hat{P}_{n}$ is the uniform distribution over x. ", "page_idx": 42}, {"type": "text", "text": "Now, we argue about the concentration of the Wasserstein distance between restrictions of the empirical distribution and restrictions of the true distribution. ", "page_idx": 42}, {"type": "text", "text": "Claim E.18. Fix $\\beta\\in(0,1)$ and sufficiently large constants $c_{3},c_{6}$ . Let $n\\,>\\,0$ be sufficiently large such that n > log n/\u03b2 (as in Theorem E.15). For all k such that k1 > c3logn \u03b2 , with probability at least $1-O(\\beta)$ over the randomness in the data, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathcal W(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})\\leq\\sqrt{c_{6}\\log\\,\\frac{n}{\\beta}}\\cdot\\mathbb E[\\mathcal W(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Now, we give the proof of our main theorem. ", "page_idx": 42}, {"type": "text", "text": "5The theorem would be of the same form as Theorem E.15, \u221aexcept that Algorithm 5 would be $(\\varepsilon,\\delta)$ -DP, with the lower bound on $n$ instead being $\\begin{array}{r}{n=\\Omega\\left(\\frac{\\mathrm{polylog}^{*}\\;\\frac{b-a}{\\gamma\\varepsilon\\delta}\\sqrt{\\log{1/\\delta}}\\log(1/\\beta)}{\\varepsilon}\\right)}\\end{array}$ , and $k$ being set instead to $\\begin{array}{r}{O\\left(\\frac{\\varepsilon n}{\\log^{*}\\frac{b-a}{\\gamma}\\operatorname{polylog}\\frac{n}{\\beta}}\\right).}\\end{array}$ ", "page_idx": 42}, {"type": "text", "text": "Theorem $E.I5$ . Using Lemma E.16, Claim E.18 and the triangle inequality, we have that with probability at least $1-O(\\beta)$ over the randomness of the data and the algorithm, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^3\\rangle\\leq\\mathcal W(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},P^{D P}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})+C^{\\prime\\prime}\\mathcal W(P,P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})}\\\\ &{\\ \\ \\ \\leq\\mathcal W(\\hat{P}_{n|q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},P^{D P}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})+\\mathcal W(\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})+C^{\\prime\\prime}\\mathcal W(P,P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})}\\\\ &{\\ \\ \\ \\leq\\mathcal W(\\hat{P}_{n|q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},P^{D P}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})+\\sqrt{c_{6}\\log\\frac{n}{\\beta}}\\mathbb E\\left[\\mathcal W(\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})\\right]+C^{\\prime\\prime}\\mathcal W(P,P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Finally, applying Lemma E.17 and taking a union bound over failure probabilities, we get that with probability at least $1-O(\\beta)$ over the randomness of the data and the algorithm, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathit{N}(P,P^{D P})\\leq\\frac{2}{k}\\left(q_{1-1/k}-q_{1/k}\\right)+\\sqrt{c_{6}\\log\\frac{n}{\\beta}}\\mathbb{E}\\left[\\mathcal{W}(\\hat{P}_{n}|_{q_{1},q_{1-\\frac{1}{k}},P}|_{q_{1},q_{1-\\frac{1}{k}}})\\right]+C^{\\prime\\prime}\\mathcal{W}(P,P|_{q_{1},q_{1-\\frac{1}{k}},P}|_{q_{1},q_{1-\\frac{1}{k}}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "as required. ", "page_idx": 43}, {"type": "text", "text": "F Experiment Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Below we describe the experiment referenced in the introduction. ", "page_idx": 43}, {"type": "text", "text": "The distribution: We have taken a distribution on $[0,999]$ , which is concentrated on two points 430 and 440, with $\\begin{array}{r}{p_{430}\\,=\\,\\frac{1}{3}}\\end{array}$ and $\\begin{array}{r}{p_{440}\\,=\\,\\frac{2}{3}}\\end{array}$ . These algorithms have been run with $n\\,=\\,1600$ samples from this distribution. ", "page_idx": 43}, {"type": "text", "text": "Minimax Optimal Algorithm: The minimax-optimal algorithm here is the algorithm PSMM from [HVZ23] that considers a fixed partitioning of the interval into $\\Omega(m^{\\frac{1}{d}})$ equal intervals and places the empirical mass in each interval on an arbitrary point in each interval. Here we consider this algorithm with $\\varepsilon=\\infty$ , so that no noise is added. We have run it here with $K=40$ buckets. ", "page_idx": 43}, {"type": "text", "text": "Instance-optimal Algorithm: The instance-optimal algorithm finds $k$ quantiles as in Algorithm 5. In this particular implementation, we used the recursive exponential mechanism of [KSS22], but we expect other quantile algorithms would work similarly. In this particular case, we use $k=10$ quantiles with $\\varepsilon=1$ . ", "page_idx": 43}, {"type": "text", "text": "G Proofs for Section $\\mathbf{D}$ ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Theorem D.6. For every level $\\ell~\\in~[D_{T}]$ , define the neighborhood of $P_{\\ell}$ as $\\mathcal{N}_{\\ell}\\;:\\;\\Delta([N_{\\ell}])\\;\\rightarrow$ $\\mathfrak{P}(\\Delta([N_{\\ell}]))$ by $\\mathcal{N}_{\\ell}(P_{\\ell})=\\{Q_{\\ell}\\mid D_{\\infty}(\\dot{P}_{\\ell},\\dot{Q}_{\\ell})\\leq\\ln2\\}$ . Then, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}(P)\\ge\\operatorname*{max}_{\\ell\\in[D_{T}]}r_{\\ell}\\cdot\\mathcal{R}_{\\mathcal{N}_{\\ell},n,\\varepsilon}(P_{\\ell}),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the error of $P$ is measured in the Wasserstein distance and $P_{\\ell}$ is measured in the TV distance. ", "page_idx": 43}, {"type": "text", "text": "Proof of Theorem $D.6.$ . Given a distribution $P$ , let ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathcal{A}_{P}^{*}=\\arg\\operatorname*{min}_{\\mathcal{A}^{\\mathrm{is}}\\,\\varepsilon\\mathrm{-DP}}\\operatorname*{max}_{Q\\in{N(P)}}\\mathbb{E}_{D\\sim P^{n}}[\\mathcal{W}(P,\\mathcal{A}(D))]\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "so $\\begin{array}{r}{\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}(P)=\\operatorname*{max}_{Q\\in\\mathcal{N}(P)}\\mathbb{E}_{D\\sim Q^{n}}[\\mathcal{W}(P,\\mathcal{A}_{P}^{*}(D))]}\\end{array}$ . Let $\\ell\\in[D_{T}]$ . We want to define an algorithm $A_{P_{\\ell}}^{*}$ on the distributions in $\\mathcal{N}_{\\ell}(P_{\\ell})$ that achieves maximum error rate $\\scriptstyle{\\frac{1}{r\\ell}}{\\mathcal{R}}_{{\\mathcal{N}},n,\\varepsilon}(P)$ . Define a randomised function $g_{P}$ which given a node $\\nu_{\\ell}$ at level $\\ell$ , $g_{P}(\\nu_{\\ell})$ is sampled from the distribution $P$ restricted to the leaf nodes that are children of $\\nu_{\\ell}$ . Given a set of nodes at level $\\ell$ , define $g_{P}(D)$ to be the set where $g_{D}$ is applied to each set element individually. Then define $\\begin{array}{r}{\\mathcal{A}_{P_{\\ell}}^{*}(D)=(\\mathcal{A}_{P}^{*}(g_{P}(D)))_{\\ell}}\\end{array}$ . Since $g_{P}$ is applied individually to each element in $D$ , $A_{P_{\\ell}}^{\\ast}$ is $\\varepsilon$ -DP. ", "page_idx": 43}, {"type": "text", "text": "Given a distribution $Q^{\\ell}\\in\\mathcal{N}_{\\ell}(P_{\\ell})$ , define a distribution $Q$ on the leaves of the tree as follows: ", "page_idx": 43}, {"type": "equation", "text": "$$\nQ(\\nu)=\\frac{Q^{\\ell}(\\nu_{\\ell})}{P_{\\ell}(\\nu_{\\ell})}*P(\\nu),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\nu_{\\ell}$ is the parent node of $\\nu$ at level $\\ell$ . Note $Q\\in{\\mathcal{N}}(P)$ , $g_{P}(Q^{\\ell})=Q$ and $Q_{\\ell}=Q^{\\ell}$ . Now, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{T V(Q^{\\ell},\\mathcal{A}_{P_{\\ell}}^{*}(D))=T V(Q_{\\ell},(\\mathcal{A}_{P}^{*}(g_{P}(D))_{\\ell})}}\\\\ &{\\le\\frac{1}{r_{\\ell}}\\sum_{\\ell^{\\prime}\\in[D_{T}]}r_{\\ell^{\\prime}}T V(Q_{\\ell^{\\prime}},(\\mathcal{A}_{P}^{*}(g_{P}(D))_{\\ell^{\\prime}})}\\\\ &{=\\frac{1}{r_{\\ell}}\\mathcal{W}(Q,\\mathcal{A}_{P}^{*}(g_{P}(D)))}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the first inequality follows by definition of $A_{P_{\\ell}}^{\\ast}$ and the fact $Q_{\\ell}=Q^{\\ell}$ . Since $g_{P}(Q^{\\ell})=Q$ , this implies that for all distributions in $\\mathcal{N}_{\\ell}(P_{\\ell})$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D\\sim Q^{\\ell}}\\left[T V(Q^{\\ell},A_{P_{\\ell}}^{*}(D))\\right]\\leq\\mathbb{E}_{D\\sim Q}\\left[\\frac{1}{r_{\\ell}}\\mathcal{W}(Q,A_{P}^{*}(D))\\right]\\leq\\frac{1}{r^{\\ell}}\\mathcal{R}_{N,n,\\varepsilon}(P),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "which implies for all levels $\\ell$ , $\\begin{array}{r}{\\mathcal{R}_{\\mathcal{N}_{\\ell},n,\\varepsilon}(P_{\\ell})\\leq\\frac{1}{r^{\\ell}}\\mathcal{R}_{\\mathcal{N},n,\\varepsilon}(P)}\\end{array}$ and so we are done. ", "page_idx": 44}, {"type": "text", "text": "Lemma G.1. $I A$ extension of $(\\varepsilon,\\delta)$ -DP Assouad\u2019s method [ASZ21]] Let $k_{0},k_{1},\\cdot\\cdot\\cdot$ be a sequence of natural numbers such that $\\sum_{s}k_{s}<\\infty,$ , $\\varepsilon\\,>\\,0$ and $\\delta\\,\\in\\,[0,1]$ . Given a family of distributions $\\mathcal{P}\\,\\subset\\,\\Delta(\\mathcal{X})$ on a space $\\mathcal{X}$ ,  a parameter $\\theta\\,:\\,\\mathcal{P}\\,\\rightarrow\\,\\mathcal{M}$ where $\\mathcal{M}$ is a metric space with metric $d,$ , suppose that there exists a set $\\mathcal{V}\\subset\\mathcal{P}$ of distributions indexed by the product of hypercubes $\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdot\\cdot\\cdot$ where $\\mathcal{E}_{k}:=\\{\\pm1\\}^{k}$ such that for a sequence $\\tau_{0},\\tau_{1},\\dots$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\forall(u^{0},u^{1},\\cdots),(v^{0},v^{1},\\cdots)\\in\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdots,\\quad d(\\theta(p_{u}),\\theta(p_{v}))\\geq2\\sum_{s}\\tau_{s}\\sum_{j=1}^{k_{s}}\\chi_{u_{j}^{s}\\neq v_{j}^{s}}^{s}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For each coordinate $s\\in\\mathbb{N},\\,j\\in[k_{s}],$ , consider the mixture distributions obtained by averaging over all distributions with a fixed value at the $(s,j)t h$ coordinate: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathfrak{s}_{+(s,j)}=\\frac{2}{|\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdots\\cdot|}\\sum_{\\substack{u\\in\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdots\\cdot u_{j}^{s}=+1}}p_{u},\\;\\;p_{-(s,j)}=\\frac{2}{|\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdots\\cdot|}\\sum_{\\substack{u\\in\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\cdots\\cdot u_{j}^{s}=+1}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and let $\\phi_{s,j}:\\mathcal{X}^{n}\\to\\{-1,+1\\}$ be a binary classifier. Then ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{1\\,i\\,s\\,(\\varepsilon,\\delta)\\cdot D P}}\\operatorname*{max}_{p\\in\\mathcal{V}}\\mathcal{R}_{A,n}(p)\\geq\\frac{1}{2}\\sum_{s}\\tau_{s}\\sum_{\\substack{j=1}}^{k_{s}}\\operatorname*{min}_{\\substack{i\\,s,j\\,i\\,b\\,(\\varepsilon,\\delta)\\cdot D P}}\\bigr(\\operatorname*{Pr}_{X\\sim p_{+\\,(s,j)}^{\\!\\varepsilon}}(\\phi_{s,j}(X)\\neq1)+\\operatorname*{Pr}_{X\\sim p_{-\\,(s,j)}^{\\!n}}(\\phi_{s,j}(X)\\neq1)\\big)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the min on the LHS is over all $(\\varepsilon,\\delta)$ -DP mechanisms, and on the right hand side is over all $(\\varepsilon,\\delta)$ -DP binary classifiers. Moreover, if for all $s\\,\\in\\,\\mathbb{N},$ , $j\\ \\in[k_{s}],$ , there exists a coupling $(X,Y)$ between $p_{+(s,j)}^{n}$ and $p_{-(s,j)}^{n}$ with $\\mathbb{E}[d_{H a m}(X,Y)]\\leq D_{s}$ , then ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{A\\,i s\\,(\\varepsilon,\\delta)\\cdot D P\\mathrm{\\scriptsize~}p\\in\\mathcal{V}}}\\mathcal{R}_{A,n}(p)\\geq\\sum_{s}\\frac{k_{s}\\tau_{s}}{2}\\big(0.9e^{-10\\varepsilon D_{s}}-10D_{s}\\delta\\big)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof of Lemma $D.8$ . We will follow the proof of Theorem 3 in [ASZ21]. Given an estimator $\\boldsymbol{\\mathcal{A}}$ , define a classifier $\\mathcal{A}^{*}$ by projecting on the product of hypercubes so ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathcal{A}^{*}(X)=\\arg\\operatorname*{min}_{u\\in(\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\dots)}d({\\cal A}(X),\\theta(p_{u})).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "By the triangle inequality and the definition of $A^{*}$ , for any $p\\in\\mathcal{V}$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\nd\\big(\\theta(p_{\\mathcal{A}^{\\ast}(X)}),\\theta(p)\\big)\\leq d\\big(\\mathcal{A}(X),\\theta\\big(p_{\\mathcal{A}^{\\ast}(X)}\\big)\\big)+d\\big(\\mathcal{A}(X),\\theta(p)\\big)\\leq2d\\big(\\mathcal{A}(X),\\theta(p)\\big).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Therefore, we can restrict to a lower bound on the performance of DP classifiers: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{A\\,\\mathrm{is}\\,(\\varepsilon,\\delta)\\cdot\\mathrm{DP}\\,\\,p\\in\\mathcal{V}}}\\mathcal{R}_{A,n}(p)\\geq\\frac{1}{2}\\operatorname*{min}_{\\substack{A^{*}\\,\\mathrm{is}\\,(\\varepsilon,\\delta)\\cdot\\mathrm{DP}\\,\\,p\\in\\mathcal{V}}}\\mathbb{m}_{X\\sim p^{n}}[d(\\theta(p_{A^{*}(X)}),\\theta(p))].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Also, for any $(\\varepsilon,\\delta)$ -DP classifier $A^{*}$ , ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{p\\in\\mathcal{V}}\\mathbb{E}_{X\\sim p^{n}}\\big[d\\big(\\theta\\big(p_{A^{*}(X)}\\big),\\theta\\big(p)\\big)\\big]\\ge\\frac{1}{|\\mathcal{V}|}\\sum_{u\\in(\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\dots)}\\mathbb{E}_{X\\sim p_{u}^{n}}\\big[d\\big(\\theta\\big(p_{A^{*}(X)}\\big),\\theta\\big(p_{u}\\big)\\big)\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\ge\\displaystyle\\frac{2}{|\\mathcal{V}|}\\sum_{s}\\tau_{s}\\sum_{j=1}^{k_{s}}\\sum_{u\\in\\mathcal{E}_{k_{0}}\\times\\mathcal{E}_{k_{1}}\\times\\dots}\\operatorname*{Pr}_{\\mathcal{P}_{u}^{n}}\\!\\big(A^{*}(X)_{j}^{s}\\neq u_{j}^{s}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the first inequality follows from the fact that the max is greater than the average, and the second follows from assumption (6). For each $(s,j)$ pair, we divide $\\mathcal{E}_{k_{0}}\\,\\times\\,\\mathcal{E}_{k_{1}}\\,\\times\\,\\cdot\\,\\cdot\\,.$ into two groups; ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{p\\in\\mathbb{V}}{\\operatorname*{max}}\\mathbb{E}_{X\\sim p^{n}}[d(\\theta(p_{\\pm}*(X)),\\theta(p))]}\\\\ &{\\geq\\frac{2}{|\\mathcal{V}|}\\sum_{s}\\frac{k_{s}}{s_{j+1}}\\Bigg[\\underset{u\\in(\\mathcal{E}_{\\theta_{0}}\\times\\mathcal{E}_{h_{1}}\\times\\dots)\\;|u_{j}^{\\prime}=+1}{\\sum_{\\substack{u\\in(\\mathcal{E}_{\\theta_{0}}\\times\\mathcal{E}_{h_{1}}\\times\\dots)\\;|u_{j}^{\\prime}=+1}}}\\underset{u\\in(\\mathcal{E}_{\\theta_{0}}\\times\\mathcal{E}_{h_{1}}\\times\\dots)\\;|u_{j}^{\\prime}=-1}{\\operatorname*{Pr}}+\\underset{u\\in(\\mathcal{E}_{\\theta_{0}}\\times\\mathcal{E}_{h_{1}}\\times\\dots)\\;|u_{j}^{\\prime}=-1}{\\sum_{\\substack{u\\in(\\mathcal{E}_{\\theta_{0}}\\times\\mathcal{E}_{h_{1}}\\times\\dots)\\;|\\ u_{j}^{\\prime}=-1}}}}\\\\ &{\\geq\\frac{2}{|\\mathcal{V}|}\\sum_{s}\\frac{k_{s}}{s_{j}}\\Bigg[\\underset{u\\in(\\mathcal{E}_{\\theta_{0}}\\times\\mathcal{E}_{h_{1}}\\times\\dots)\\;|u_{j}^{\\prime}=+1}{\\sum_{\\substack{u\\in(\\mathcal{E}_{\\theta_{0}}\\times\\mathcal{E}_{h_{1}}\\times\\dots)\\;|u_{j}^{\\prime}=+1}}}\\underset{u\\in(\\mathcal{E}_{\\theta_{0}}\\times\\mathcal{E}_{h_{1}}\\times\\dots)\\;|u_{j}^{\\prime}=-1}{\\sum_{\\substack{u\\in(\\mathcal{E}_{\\theta_{0}}\\times\\mathcal{E}_{h_{1}}\\times\\dots)\\;|u_{j}^{\\prime}=-1}}}\\underset{u\\in(\\mathcal{E}_{\\theta_{0}}\\times\\mathcal{E}_{\\theta_{1}}\\times\\dots)\\;|u_{j}^{\\prime}=-1}{\\sum_{\\substack{u\\in(\\mathcal{E}_{\\theta_{0}}\\times\\mathcal{E}_{\\theta_{1}}\\times\\dots)\\;|u_{j}^{\\prime}=-1}}}\\underset{u\\in(\\mathcal{E}_{\\theta_{0}}\\times\\mathcal{E}_{\\theta_{1}}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Combining with eqn 11 we have the first statement. Next, since for each pair $(s,j)$ , there exists a coupling $(X,Y)$ between $p_{+(s,j)}$ and $\\boldsymbol{p}_{-(s,j)}$ such that $\\mathbb{E}[d_{H a m}(X,Y)]\\leq D_{s}$ , we can use the DP version of Le Cam\u2019s method from [ASZ21] to give for any classifier $\\phi_{s,j}$ , ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{X\\sim p_{+(s,j)}^{n}}(\\phi_{s,j}(X)\\neq+1)+\\operatorname*{Pr}_{X\\sim p_{-(s,j)}^{n}(\\phi_{s,j}(X)\\neq-1)}\\ge\\frac{1}{2}(0.9e^{-10\\varepsilon D_{s}}-10D_{s}\\delta),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "which implies the final result. ", "page_idx": 45}, {"type": "text", "text": "Lemma G.2. Given any pair of distributions $P$ and $Q$ on the same domain, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\left(\\operatorname*{Pr}_{X\\sim P^{n}}(\\phi(X)=1)+\\operatorname*{Pr}_{X\\sim Q^{n}}(\\phi(X)=-1)\\right)\\geq\\frac{1}{2}(1-\\sqrt{n\\mathrm{KL}(P,Q)}),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the minimum is over all binary classifiers. In particular, if $P\\,=\\,B e r n o u l\\,\\l2\\,i(p\\,-\\,\\alpha)$ and $Q=B e r n o u l\\,\\!\\!\\wr i(p+\\alpha)$ where $\\begin{array}{r}{0\\leq\\dot{\\alpha}\\leq\\frac12L\\dot{(p)}}\\end{array}$ then ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\left(\\operatorname*{Pr}_{X\\sim P^{n}}(\\phi(X)=1)+\\operatorname*{Pr}_{X\\sim Q^{n}}(\\phi(X)=-1)\\right)\\geq1/4,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where again the minimum is over all binary classifiers. ", "page_idx": 45}, {"type": "text", "text": "Proof of Lemma $D.I2$ . A standard result in the statistics literature states that for any pair of distributions $P$ and $Q$ , ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{sin}_{\\phi}\\left(\\sum_{X\\sim P^{n}}(\\phi(X)=1)+\\operatorname*{Pr}_{X\\sim Q^{n}}(\\phi(X)=-1)\\right)=\\frac{1}{2}(1-\\mathrm{TV}(P^{n},Q^{n}))\\geq\\frac{1}{2}(1-\\sqrt{n\\mathrm{KL}(P,Q)}),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the minimum is over all binary classifiers. If $P\\ =\\ \\mathrm{Bernoulli}(p\\\\\\mathrm{~-~}\\alpha)$ and ${\\textit{Q}}=$ Bernoulli $(p+\\alpha)$ where $0\\leq\\alpha\\leq{\\frac{1}{2}}L{\\dot{(}}p{\\Biggr)}$ then ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{KL}(Q,P)=(p+\\alpha)\\ln\\frac{p+\\alpha}{p-\\alpha}+(1-p-\\alpha)\\ln\\frac{1-p-\\alpha}{1-p+\\alpha}}&{}\\\\ {=(p+\\alpha)\\ln\\left(1+\\frac{2\\alpha}{p-\\alpha}\\right)+(1-p-\\alpha)\\ln\\left(1-\\frac{2\\alpha}{1-p+\\alpha}\\right)}&{}\\\\ {\\leq(p+\\alpha)\\frac{2\\alpha}{p-\\alpha}-(1-p-\\alpha)\\frac{2\\alpha}{1-p+\\alpha}}&{}\\\\ {=\\frac{4\\alpha^{2}}{p-\\alpha}+\\frac{4\\alpha^{2}}{1-p+\\alpha}}&{}\\\\ {=\\frac{\\alpha^{2}}{(p-\\alpha)(1-p+\\alpha)}}&{}\\\\ {\\leq\\frac{1}{4\\alpha}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the first inequality holds since $\\ln(1+x)\\,<\\,x$ for $x\\in[-1,1]$ and by assumption $2\\alpha/(p-$ $\\alpha),2\\alpha/(1-p+\\alpha)\\in[0,1]$ and the second follows again because of the constraint on $\\alpha$ . ", "page_idx": 46}, {"type": "text", "text": "Lemma G.3. For any distribution $P,\\,i f\\log(n/\\beta)>1$ then with probability $1-3D_{T}\\beta_{i}$ , ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathfrak{M}(\\widehat{\\mathfrak{G}_{P}},\\mathfrak{G}_{P})\\leq\\sum_{\\ell\\in[D_{T}]}\\sum_{x\\in[N_{\\ell}]}\\operatorname*{min}\\left\\{P_{\\ell}(x)(1-P_{\\ell}(x)),4\\sqrt{3\\frac{P_{\\ell}(x)(1-P_{\\ell}(x))\\log(n/\\beta)}{n}}\\right\\}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Lemma D.14 is an immediate corollary of the following lemma. ", "page_idx": 46}, {"type": "text", "text": "Lemma G.4. For any distribution $P$ , $i f\\log(n/\\beta)>1$ then with probability $1-3D_{T}\\beta_{i}$ , ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\widehat{\\mathfrak{H}_{P}},\\mathfrak{G}_{P})\\le\\sum_{\\ell\\in[D_{T}]}\\sum_{x\\in[N_{\\ell}]}\\operatorname*{min}\\left\\{P_{\\ell}(x),1-P_{\\ell}(x),4\\sqrt{3\\frac{P_{\\ell}(x)\\log(n/\\beta)}{n}},4\\sqrt{3\\frac{(1-P_{\\ell}(x))\\log(n/\\beta)}{n}}\\right\\}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof of Lemma $D.I4$ . We\u2019ll consider each level of the tree individually then use a union bound over all the levels to obtain our final bound. Let $(\\hat{P}_{\\ell})_{n}$ be the empirical distribution without truncation. The following conditions are sufficient to ensure that the bounds hold for a single level $\\ell$ : ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{sup}_{\\nu\\ s.\\ \\ P_{\\varepsilon}(\\nu)\\leq\\frac{3\\ln(n/\\beta)}{n}(\\hat{P_{\\ell}})_{n}(\\nu)\\leq\\frac{7\\ln(n/\\beta)}{n}}}\\\\ {\\displaystyle\\operatorname*{sup}_{\\nu\\ s.\\ \\ P_{\\varepsilon}(\\nu)\\geq1-\\frac{3\\ln(n/\\beta)}{n}}(\\hat{P_{\\ell}})_{n}(\\nu)\\geq1-\\frac{7\\ln(n/\\beta)}{n}}\\\\ {\\displaystyle\\forall\\left(\\nu\\ \\mathrm{s.t.}\\ P_{\\ell}(\\nu)\\in\\left[\\frac{3\\ln(n/\\beta)}{n},1-\\frac{3\\ln(n/\\beta)}{n}\\right]\\right),}\\\\ {\\displaystyle|(\\hat{P_{\\ell}})_{n}(x)-P_{\\ell}(\\nu)|\\leq\\operatorname*{min}\\left\\{\\sqrt{\\frac{3P_{\\ell}(\\nu)\\ln(n/\\beta)}{n}},\\sqrt{\\frac{3(1-P_{\\ell}(x))\\ln(n/\\beta)}{n}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We will begin by showing these conditions are sufficient. If $\\begin{array}{r}{P_{\\ell}(\\nu)\\notin\\big[\\frac{3\\ln(n/\\beta)}{n},1-\\frac{3\\ln(n/\\beta)}{n}\\big]}\\end{array}$ then these conditions imply that the empirical density for node $\\nu$ is truncated, and hence the error that that node is either $P_{\\ell}(\\nu)$ or $1-P_{\\ell}(\\nu)$ (when $P_{\\ell}(\\nu)\\,<\\,1/2$ and $P_{\\ell}(\\nu)\\;>\\;1/2$ , respectively), as required. If [ 3 ln(nn/\u03b2), 1 \u22123 ln(nn/\u03b2)] then either the estimate is not truncated and the error is less than $\\begin{array}{r}{\\operatorname*{sin}\\left\\{\\sqrt{\\frac{3P_{\\ell}(\\nu)\\ln(2n/\\beta)}{n}},\\sqrt{\\frac{3(1-P_{\\ell}(x))\\ln(2n/\\beta)}{n}}\\right\\}\\leq\\operatorname*{min}\\{P_{\\ell}(\\nu),1-P_{\\ell}(\\nu)\\}.}\\end{array}$ , as required. Or the estimate is truncated and the error is $\\operatorname*{min}\\{P_{\\ell}(\\nu),1-P_{\\ell}(\\nu)\\}$ . Under the above conditions, if $P_{\\ell}(\\nu)\\leq1/2$ then truncation will only occur if ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\?\\!\\!\\!\\slash(\\nu)-\\sqrt{\\frac{3p\\ln(2n/\\beta)}{n}}\\leq\\frac{7\\ln(n/\\beta)}{n}\\leq\\sqrt{\\frac{7\\ln(n/\\beta)}{n}}\\frac{7}{3}\\frac{3\\ln(n/\\beta)}{n}\\leq\\sqrt{\\frac{7\\ln(n/\\beta)}{n}}\\frac{7}{3}p=\\frac{7}{3}\\sqrt{\\frac{3\\ln(n/\\beta)}{n}}\\leq\\frac{\\sqrt{3\\ln(n/\\beta)}}{3}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "in which case $\\begin{array}{r}{P_{\\ell}(n u)\\le4\\sqrt{\\frac{3P_{\\ell}(\\nu)\\ln(n/\\beta)}{n}}}\\end{array}$ 3P\u2113(\u03bd) nln(n/\u03b2), as required. Similarly, if P\u2113(\u03bd) > 1/2 then truncation will only occur if $\\begin{array}{r}{1-P_{\\ell}(\\nu)\\le4\\sqrt{\\frac{3(1-P_{\\ell}(\\nu))\\ln(n/\\beta)}{n}}}\\end{array}$ 3(1\u2212P\u2113(\u03bd)) ln(n/\u03b2), as required. ", "page_idx": 47}, {"type": "text", "text": "We will now show that these conditions hold simultaneously with probability at least $1-3\\beta$ for all the nodes at level $\\ell$ . If $\\begin{array}{r}{P_{\\ell}(\\nu)\\leq\\frac{1}{e n}}\\end{array}$ then using the multiplicative form of Chernoff bound, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Pr}\\bigg((\\hat{P}_{\\ell})_{n}(\\nu)\\geq\\frac{3\\ln(n/\\beta)}{n}\\bigg)=\\mathrm{Pr}\\bigg((\\hat{P}_{\\ell})_{n}(\\nu)\\geq\\bigg(1+\\frac{3\\ln(n/\\beta)}{P_{\\ell}(\\nu)n}-1\\bigg)\\,P_{\\ell}(\\nu)\\bigg)}&{}\\\\ {\\leq}&{\\left(\\frac{e^{\\frac{3\\ln(n/\\beta)}{n\\mathcal{F}_{\\ell}(\\nu)}}-1}{(\\frac{3\\ln(n/\\beta)}{n P_{\\ell}(\\nu)})^{\\frac{3\\ln(n/\\beta)}{n\\mathcal{F}_{\\ell}(\\nu)}}}\\right)^{P_{\\ell}(\\nu)n}}\\\\ &{\\leq\\left(\\frac{e n P_{\\ell}(\\nu)}{3\\ln(n/\\beta)}\\right)^{3\\ln(n/\\beta)}}\\\\ &{\\leq P_{\\ell}(\\nu)n\\left(\\frac{e}{3\\ln(n/\\beta)}\\right)^{3\\ln(n/\\beta)}(n P_{\\ell}(\\nu))^{3\\ln(n/\\beta)-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Firstly, since ln(n/\u03b2) \u22651, 3 ln(en/\u03b2) $n/\\beta)\\ge1,\\left(\\frac{e}{3\\ln(n/\\beta)}\\right)^{3\\ln(n/\\beta)}\\le1$ . Further, $n P_{\\ell}(\\nu)\\le1/e$ and $3\\ln(n/\\beta)-1\\geq$ $\\ln(n/\\beta)$ so $(n P_{\\ell}(\\nu))^{3\\ln(n/\\beta)-1}\\leq(1/e)^{\\ln(n/\\beta)}=\\beta/n.$ . Therefore, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\biggl((\\hat{P}_{\\ell})_{n}(\\nu)\\geq\\frac{3\\ln(n/\\beta)}{n}\\biggr)\\leq P_{\\ell}(\\nu)\\beta.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Let $S=\\{x\\in[N_{\\ell}]\\mid P_{\\ell}(x)<1/(e n)\\}$ then using a union bound and Eqn (12) we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\exists x\\in S{\\mathrm{~s.t.~}}({\\hat{P}}_{\\ell})_{n}(x)\\geq{\\frac{2{\\sqrt{2}}\\log(n/\\beta)}{n}}\\right)\\leq\\sum_{x\\in S}P_{\\ell}(\\nu)\\beta\\leq\\beta\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "There exist at most $n$ elements in $[N_{\\ell}]$ that do not belong in $\\boldsymbol{S}$ . We will prove that, independently, each of these elements satisfy the required condition with probability $\\leq{\\bar{2}}\\beta/n$ then a union bound proves the final result. If $\\begin{array}{r}{P_{\\ell}(\\nu)\\,\\in\\,[\\frac{3\\ln(n/\\beta)}{n},1-\\,\\frac{3\\ln(n/\\beta)}{n}]}\\end{array}$ then using the multiplicative form of Chernoff bound (If $X_{i}$ are all i.i.d. and $0\\textless\\delta\\textless1$ , then $\\begin{array}{r}{\\operatorname*{Pr}(|\\sum_{i=1}^{n}X_{i}-n\\mathbb{E}[X_{1}]|\\geq\\delta n\\mathbb{E}[X_{1}])\\leq}\\end{array}$ $2e^{-\\delta^{2}n\\mathbb{E}[X_{1}]/3};$ ), ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{sr}\\biggr(|(\\hat{P}_{\\ell})_{n}(x)-P_{\\ell}(x)|\\geq\\sqrt{\\frac{3P_{\\ell}(x)\\log(n/\\beta)}{n}}\\biggr)=\\operatorname*{Pr}\\biggr(|(\\hat{P}_{\\ell})_{n}(x)-P_{\\ell}(x)|\\geq\\sqrt{\\frac{3\\log(n/\\beta)}{P_{\\ell}(x)n}}P_{\\ell}(x)\\biggr)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2e^{\\frac{-\\left(\\frac{3\\log(n/\\beta)}{P_{\\ell}(x)n}\\right)P_{\\ell}(x)n}{3}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=2\\beta/n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Next, if $\\begin{array}{r}{P_{\\ell}(\\nu)\\,\\le\\,\\frac{3\\ln(n/\\beta)}{n}}\\end{array}$ then using the additive form of Chernoff bound (If $X_{i}$ are all i.i.d. and $\\varepsilon\\ge0$ , then $\\begin{array}{r}{\\operatorname*{Pr}\\bigl(\\frac{1}{n}\\sum_{i=1}^{n^{-}}X_{i}\\geq\\mathbb{E}[X_{1}]+\\varepsilon\\bigr)\\leq e^{-\\varepsilon^{2}n/(2(p+\\varepsilon))})}\\end{array}$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}\\bigg((\\hat{P}_{\\ell})_{n}(\\nu)\\geq\\frac{7\\ln(n/\\beta)}{n}\\bigg)\\leq\\mathrm{Pr}\\bigg((\\hat{P}_{\\ell})_{n}(\\nu)\\geq p+(7\\frac{\\ln(n/\\beta)}{n}-p)\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq e^{-\\frac{(\\frac{\\lceil\\ln(n/\\beta)}{n}-p)^{2}n}{14\\frac{\\ln(n/\\beta)}{n}}}}\\\\ &{\\qquad\\qquad\\qquad\\leq e^{-\\frac{(4\\frac{\\ln(n/\\beta)}{n/\\beta})^{2}n}{14\\frac{\\ln(n/\\beta)}{n}}}}\\\\ &{\\qquad\\qquad\\qquad\\leq e^{-\\ln(n/\\beta)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\beta/n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "By symmetry, if $\\begin{array}{r}{P\\ell(\\nu)\\ge1-\\frac{3\\ln(n/\\beta)}{n}}\\end{array}$ then ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\bigg((\\hat{P}_{\\ell})_{n}(\\nu)\\leq1-\\frac{7\\ln(n/\\beta)}{n}\\bigg)\\leq\\beta/n.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Lemma G.5. Let $\\hat{\\gamma}_{\\varepsilon}$ be the set of active nodes found in Algorithm 1. Then with probability 1 \u2212 $D_{T}(\\log n+4\\varepsilon n)\\beta$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\gamma_{P}\\left(\\operatorname*{max}\\left\\{\\frac{2}{\\varepsilon n}+4\\frac{\\log(2/\\beta)}{\\varepsilon n},\\frac{192\\log(n/\\beta)}{n}\\right\\}\\right)\\subset\\hat{\\gamma}_{\\varepsilon}\\subset\\gamma_{P}\\left(\\frac{1}{2\\varepsilon n}\\right).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proof of Lemma $D.I5$ . First notice that if a node $\\nu$ is an $\\alpha$ -active node, then all of it\u2019s ancestor nodes are also $\\alpha$ -active. So, it suffices to show that (with high probability) if at any stage a node makes to it Line 7 of Algorithm 3, then if \u03bd \u2208/ \u03b3P (2\u03ba) then G P (\u03bd) + Lap( \u03b51n) \u22642\u03ba + log(\u03b52n/\u03b2) and if $\\begin{array}{r}{\\nu\\in\\gamma_{P}\\left(\\operatorname*{max}\\left\\{\\frac{2}{\\varepsilon n}+4\\frac{\\log(2/\\beta)}{\\varepsilon n},\\frac{\\log(n/\\beta)}{n}\\right\\}\\right)}\\end{array}$ then $\\begin{array}{r}{\\widehat{\\mathfrak{G}_{P}}(\\nu)+\\mathsf{L a p}(\\frac{1}{\\varepsilon n}))>2\\kappa+\\frac{\\log(2/\\beta)}{\\varepsilon n}}\\end{array}$ . ", "page_idx": 48}, {"type": "text", "text": "By Lemma D.14, with probability $1-3D_{T}\\beta$ , all nodes $\\nu$ satisfy ", "page_idx": 48}, {"type": "equation", "text": "$$\n|\\widehat{\\mathfrak{G}_{P}}(\\nu)-\\mathfrak{G}_{P}(\\nu)|\\leq\\operatorname*{min}\\left\\{\\mathfrak{G}_{P}(\\nu)(1-\\mathfrak{G}_{P}(\\nu)),4\\sqrt{\\frac{3\\mathfrak{G}_{P}(\\nu)(1-\\mathfrak{G}_{P}(\\nu))\\log(n/\\beta)}{n}}\\right\\}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Further, if one samples $X$ independent samples from $\\textstyle\\mathsf{L a p}\\big(\\frac{1}{\\varepsilon n}\\big)$ then with probability $1-X\\beta$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname*{sup}|\\mathsf{L a p}(\\frac{1}{\\varepsilon n})|\\leq\\frac{\\ln(2/\\beta)}{\\varepsilon n}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "So conditioning on both these events if $\\begin{array}{r}{x\\notin\\gamma_{P}\\left(\\frac{1}{2\\varepsilon n}\\right)}\\end{array}$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\widehat{\\mathfrak{G}_{P}}(\\nu)+\\mathsf{L a p}(\\frac{1}{\\varepsilon n})\\leq\\mathfrak{G}_{P}(\\nu)+\\frac{\\ln(2/\\beta)}{\\varepsilon n}\\leq\\frac{1}{2\\varepsilon n}+\\frac{\\ln(2/\\beta)}{\\varepsilon n},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "so they will not survive Line $^{7}$ of Algorithm 3. If $\\begin{array}{r}{x\\in\\gamma_{P}\\left(\\operatorname*{max}\\lbrace\\frac{2}{\\varepsilon n}+4\\frac{\\log(2/\\beta)}{\\varepsilon n},\\frac{192\\log(n/\\beta)}{n}\\rbrace\\right)}\\end{array}$ then ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathfrak{G_{P}}}(\\nu)+\\mathsf{L a p}(\\frac{1}{\\varepsilon n})\\geq\\mathfrak{G_{P}}(\\nu)-4\\sqrt{3\\frac{\\mathfrak{G_{P}}(\\nu)\\log(n/\\beta)}{n}}-\\frac{\\ln(2/\\beta)}{\\varepsilon n}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{2}\\mathfrak{G_{P}}(\\nu)-\\frac{\\log(2/\\beta)}{\\varepsilon n}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{\\varepsilon n}+\\frac{\\log(2/\\beta)}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Each level has at most $2\\varepsilon n$ in $\\textstyle\\gamma_{P}\\left({\\frac{1}{2\\varepsilon n}}\\right)$ so we query at most $4\\varepsilon n$ nodes in the tree when running LocateActiveNodes since each node has at most 2 children. Therefore, we can set $X=4\\varepsilon n D_{T}$ . \u4e00 ", "page_idx": 48}, {"type": "text", "text": "Lemma G.6. If \u03b3P max{ \u03b52n + 4 log(\u03b52n/\u03b2), 192 logn(n/\u03b2)} then ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathfrak{M}\\widehat{(\\mathfrak{G}_{P},\\mathfrak{G}_{P}|_{\\hat{\\gamma}_{\\varepsilon}})}\\leq\\mathfrak{M}\\big(\\mathfrak{G}_{P},\\widehat{\\mathfrak{G}_{P}}\\big)+\\mathfrak{M}\\big(\\mathfrak{G}_{P},\\mathfrak{G}_{P}\\big|_{\\gamma_{P}}\\big(\\operatorname*{max}\\{\\textstyle\\frac{2}{\\varepsilon n}+4\\frac{\\log(2/\\beta)}{\\varepsilon n},\\frac{192\\log(n/\\beta)}{n}\\}\\big)\\big)\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proof of Lemma $D.16$ . The key component of this proof is that any discrepancy between the weight of the nodes on $P$ and that assigned by $\\widehat{\\mathfrak{G}_{P}}$ was already paid for in $\\widehat{\\nu(P,\\widehat{\\mathfrak{G}_{P}})}$ . ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{D}(\\widehat{\\mathbf e}_{P},\\widehat{\\mathbf e}_{P}|\\mathbf e_{i})=\\displaystyle\\sum_{s^{\\prime}\\neq\\widehat{\\mathbf e}_{P}}r_{i}|\\widehat{\\mathbf e}_{P}(\\nu)|}\\\\ {\\leq}&{\\displaystyle r_{i}|\\widehat{\\mathbf e}_{P}(r)\\sum_{s^{\\prime}=0}^{\\infty}\\sum_{u=1}^{r_{i}(\\widehat{\\mathbf e}_{P}(\\nu)|)}r_{i}|\\widehat{\\mathbf e}_{P}(\\nu)|}\\\\ &{=\\left.\\nu_{i}r_{i}\\left(\\operatorname*{max}\\Big\\{\\frac{2}{r_{i}+4\\ln^{2/2}(\\frac{\\lambda}{r_{i}})},\\frac{102\\ln^{2}(\\alpha/2\\beta)}{r_{i}}\\right\\}\\right.}\\\\ &{\\left.\\quad\\nu_{i}r_{P}(\\operatorname*{max}\\{\\frac{2}{r_{i}+4\\ln^{2/2}(\\frac{\\lambda}{r_{i}})},\\frac{102\\ln^{2}(\\alpha/2\\beta)}{r_{i}}\\})\\right)r_{i}|\\widehat{\\mathbf e}_{P}(\\nu)-\\Phi_{P}(\\nu)+\\Phi_{P}(\\nu)|}\\\\ {\\leq}&{\\displaystyle r_{i}|\\widehat{\\mathbf e}_{P}(r)\\operatorname*{max}\\{\\frac{2}{r_{i}+4\\ln^{2/2}(\\frac{\\lambda}{r_{i}})},\\frac{102\\ln^{2}(\\alpha/2\\beta)}{r_{i}}\\})\\right.}\\\\ &{\\quad\\left.\\quad\\nu_{i}r_{P}(\\operatorname*{max}\\{\\frac{r_{i}}{r_{i}+4\\ln^{2/2}(\\frac{\\lambda}{r_{i}})},\\frac{102\\ln^{2}(\\alpha/2\\beta)}{r_{i}}\\})\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.+\\frac{\\sum_{i}^{r}}{r_{i}r_{P}\\left(\\operatorname*{max}\\{\\frac{2}{r_{i}+2\\ln^{2/2}(\\frac{\\lambda}{r_{i}})},\\frac{102\\ln^{2}(\\alpha/2\\beta)}{r_{i}}\\}\\right)}r_{i}|\\Phi_{P}(\\nu)\\rangle}\\\\ &{\\leq\\mathfrak{M}(\\Phi_{P},\\widehat{\\Phi_{P}})+\\mathfrak{M}(\\Phi_{P},\\Phi_{P}|\\log(\\mathbf{x}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "as required. ", "page_idx": 49}, {"type": "text", "text": "Lemma G.7. For any real-valued function $\\mathfrak{G}$ on the nodes of the HST such that $\\mathfrak{G}(\\nu_{0})=1$ where $\\nu_{0}$ is the root node and given any distribution $P$ , ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}(P,P r o j e c t i o n(\\mathfrak{G}))\\overset{}{\\leq}4\\mathfrak{W}(\\mathfrak{G}_{P},\\mathfrak{G}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proof of Lemma $D.I7.$ . We first note that for any pair of sequences of real values $a_{1},\\cdot\\cdot\\cdot,a_{k}$ and $b_{1},\\cdot\\cdot\\cdot,b_{k}$ , and constant $A$ such that $\\textstyle\\sum_{i}a_{i}\\neq0$ , ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sum|{\\frac{A}{\\sum a_{i}}}a_{i}-b_{i}|\\leq\\sum|{\\frac{A}{\\sum a_{i}}}a_{i}-a_{i}|+|a_{i}-b_{i}|=|A-\\sum a_{i}|+\\sum|a_{i}-b_{i}|\\leq|A-\\sum b_{i}|+2\\sum|a_{i}-a_{i}|.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Also if $\\sum a_{i}=0$ then ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sum|{\\frac{A}{k}}-b_{i}|\\leq\\sum|{\\frac{A}{k}}-{\\frac{\\sum_{i}b_{i}}{k}}|+|{\\frac{\\sum_{i}b_{i}}{k}}-b_{i}|=|A-\\sum b_{i}|+2\\sum|b_{i}|=|A-\\sum b_{i}|+2\\sum|a_{i}-b_{i}|\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Let $\\bar{\\mathfrak{G}}^{\\ell}$ be the function $\\bar{\\mathfrak{G}}$ after only levels $0,\\cdots,\\ell$ have been updated. So $\\bar{\\mathfrak{G}}^{\\ell}$ matches $\\bar{\\mathfrak{G}}^{\\ell-1}$ on all levels except $\\ell$ . Let $\\nu$ be a node in the \u2113th level of the HST. If we suppose the sum is over the normalised children of a node $\\nu$ , $A\\,=\\,{\\bar{\\mathfrak{G}}}^{\\ell-1}(\\nu)$ , and for all the children $\\nu^{\\prime}$ of $\\nu$ , $a_{i}\\,=\\,{\\mathfrak{G}}(\\nu^{\\prime})$ and $b_{i}=\\mathfrak{G}_{P}(\\nu^{\\prime})$ , we can see that the contribution to the Wasserstein distance by the children increases by an additive factor of $|\\mathfrak{G}^{\\ell-1}(\\nu)-\\mathfrak{G}_{P}(\\nu)|$ . Iterating, we can see that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\ensuremath{\\mathcal{N}}(P,\\ensuremath{\\mathtt{P r o j e c t i o n}}(\\mathfrak{G}))\\le2\\sum_{\\ell=0}^{D_{T}}\\sum_{\\tiny{\\mathrm{~a~lecel}}\\;\\ell}(r_{\\ell}+r_{\\ell+1}\\cdot\\cdot\\cdot r_{D_{T}})|\\mathfrak{G}(\\nu)-\\mathfrak{G}_{P}(\\nu)|\\le4\\sum_{\\ell=0}^{D_{T}}\\sum_{\\tiny{\\mathrm{~a~lecel}}\\;\\ell}r_{\\ell}|\\mathfrak{G}(\\nu)-\\mathfrak{G}_{P}(\\nu)|\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "which is 4 times the wasserstein distance. ", "page_idx": 49}, {"type": "text", "text": "H Local Minimality in the High Dimensional Setting ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Theorem H.1. Given any $\\varepsilon\\:>\\:0,$ , and a distribution $P$ , and let $\\begin{array}{r}{n^{\\prime}\\,=\\,\\frac{5}{4\\operatorname*{min}\\{W(\\frac{0.45\\varepsilon}{\\delta}),0.6\\}}n,}\\end{array}$ , then for all $(\\varepsilon,\\delta)$ -DP algorithms $\\mathcal{A}^{\\prime}$ , there exists a distribution $Q\\,\\in\\mathcal{N}(P)$ such that with probability $1-(D_{T}\\log n+4D_{T}\\varepsilon n)\\beta$ , ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}(Q,\\hat{Q_{\\varepsilon,n^{\\prime}}})\\leq\\tilde{O}(\\mathbb{E}_{X\\sim Q^{n},\\mathcal{A}^{\\prime}}(\\mathcal{W}({\\cal A}^{\\prime}(X),Q))),}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\hat{Q_{\\varepsilon,n^{\\prime}}}$ is the output of PrivDensityEstTree(Q) with $n^{\\prime}$ samples. ", "page_idx": 49}, {"type": "text", "text": "Proof. First, let us obtain a slightly simpler upper bound on $\\mathcal{W}(P,\\hat{P}_{\\varepsilon})$ . From eqn (9) in the proof of Theorem D.13 we have that for each level $\\ell$ , ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathcal{W}(P_{\\ell},(\\hat{P}_{\\ell})_{\\ell})\\leq2\\left(\\mathfrak{M}((\\mathfrak{G}_{P})_{\\ell},(\\widehat{\\mathfrak{G}_{P}})_{\\ell})+\\mathfrak{M}((\\widehat{\\mathfrak{G}_{P}})_{\\ell},(\\widehat{\\mathfrak{G}_{P}}|\\hat{\\gamma}_{\\ell})_{\\ell})+\\mathfrak{M}((\\widehat{\\mathfrak{G}_{P}}|\\hat{\\gamma}_{\\ell})_{\\ell},(\\widehat{\\mathfrak{G}_{P_{n},\\hat{\\gamma}_{\\ell}}})_{\\ell})\\right),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "from Lemma D.15 we have that with probability $1-D_{T}(\\log n+4\\varepsilon n)\\beta$ , ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\gamma_{P}\\left(\\operatorname*{max}\\left\\{\\frac{2}{\\varepsilon n}+4\\frac{\\log(2/\\beta)}{\\varepsilon n},\\frac{192\\log(n/\\beta)}{n}\\right\\}\\right)\\subset\\hat{\\gamma}_{\\varepsilon}\\subset\\gamma_{P}\\left(\\frac{1}{2\\varepsilon n}\\right),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and if one samples $4\\varepsilon n D_{T}$ independent samples from $\\textstyle\\mathsf{L a p}\\big(\\frac{1}{\\varepsilon n}\\big)$ then we have that with probability $1-4\\varepsilon n D_{T}\\beta$ , ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\operatorname*{sup}|\\mathsf{L a p}(\\frac{1}{\\varepsilon n})|\\leq\\frac{\\ln(2/\\beta)}{\\varepsilon n}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Therefore, for all \u03bd \u2208/ \u03b3\u02c6\u03b5 we have P\u2113(\u03bd) \u2264 max \u03b52n + 4 log(\u03b52n/\u03b2), 192 logn(n/\u03b2) for some constant $C$ therefore, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathfrak{N}((\\widehat{\\mathfrak{G_{P}}})_{\\ell},(\\widehat{\\mathfrak{G_{P}}}|\\hat{\\gamma}_{\\ell})_{\\ell})+\\mathfrak{M}((\\widehat{\\mathfrak{G_{P}}}|\\hat{\\gamma}_{\\ell})_{\\ell},(\\widehat{\\mathfrak{G_{P_{n},\\hat{\\gamma}_{\\ell}}}})_{\\ell})\\leq\\displaystyle\\sum_{\\nu\\notin\\hat{\\gamma}_{\\ell}}P_{\\ell}(\\nu)+\\displaystyle\\sum_{\\nu\\in\\hat{\\gamma}_{\\ell}}\\displaystyle\\frac{\\ln(2/\\beta)}{\\varepsilon n}}&{}\\\\ {\\leq\\displaystyle\\sum_{\\nu\\notin\\gamma_{P}\\left(\\frac{1}{2\\varepsilon n}\\right)}P_{\\ell}(\\nu)+\\sum_{\\nu\\in\\gamma_{P}\\left(\\frac{1}{2\\varepsilon n}\\right)\\backslash\\gamma_{\\ell}}C\\displaystyle\\frac{\\ln(n/\\beta)}{\\varepsilon n}+\\sum_{\\nu\\in\\gamma_{P}\\left(\\frac{1}{2\\varepsilon n}\\right)}P_{\\ell}(\\nu)}&{}\\\\ {\\leq\\displaystyle\\sum_{\\nu\\notin\\gamma_{P}\\left(\\frac{1}{2\\varepsilon n}\\right)}P_{\\ell}(\\nu)+C\\ln(n/\\beta)\\sum_{\\nu\\in\\gamma_{P}\\left(\\frac{1}{2\\varepsilon n}\\right)}\\frac{1}{\\varepsilon n}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "For the same reason as in the proof of Theorem D.13, we can upper bound $\\sum_{\\nu\\in\\gamma_{P}}\\!\\left({\\frac{1}{2\\varepsilon n}}\\right)\\,{\\frac{1}{\\varepsilon n}}$ by $\\begin{array}{r}{\\left(\\left|\\gamma_{P}\\left(\\frac{1}{2\\varepsilon n}\\right)\\right|-1\\right)\\!\\frac{1}{\\varepsilon n}}\\end{array}$ by dealing with the $\\textstyle|\\gamma_{P}\\left({\\frac{1}{2\\varepsilon n}}\\right)|=1$ case separately. Therefore, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\leq2C\\ln(n/\\beta)\\left(\\sum_{\\nu}\\operatorname*{min}\\left\\{\\mathfrak{G}_{P}(\\nu)(1-\\mathfrak{G}_{P}(\\nu)),\\sqrt{\\frac{\\mathfrak{G}_{P}(\\nu)(1-\\mathfrak{G}_{P}(\\nu))}{n}}\\right\\}+\\sum_{\\nu\\notin\\gamma_{P}\\left(\\frac{1}{2\\varepsilon n}\\right)}\\mathfrak{G}_{P}(\\nu)+(|\\gamma|)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Further, by Theorem D.7 and Theorem D.6, given $\\ensuremath{\\varepsilon}\\mathrm{~\\ensuremath~{~\\sum~}~}\\ensuremath{0}$ and $\\delta\\ \\ \\in\\ \\ [0,1]$ , let $\\kappa\\quad=$ $\\begin{array}{r}{\\frac{1}{10\\varepsilon n}\\operatorname*{min}\\{W\\left(\\frac{0.45\\varepsilon}{\\delta}\\right),0.6\\}}\\end{array}$ where $W(x)$ is the Lambert $\\mathrm{W}$ function so $W(x)e^{W(x)}\\,=\\,x$ . Given a distribution $P$ , there exists a constant $C^{\\prime}$ such that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\therefore\\geq\\frac{C^{\\prime}}{D_{T}}\\left(\\sum_{\\nu}\\operatorname*{min}\\left\\{\\mathfrak{G}_{P}(\\nu)(1-\\mathfrak{G}_{P}(\\nu)),\\sqrt{\\frac{\\mathfrak{G}_{P}(\\nu)(1-\\mathfrak{G}_{P}(\\nu))}{n}}\\right\\}+\\sum_{\\nu\\notin\\gamma_{P}(2\\kappa)}\\mathfrak{G}_{P}(\\nu)+(|\\gamma_{P}(2\\kappa)|-\\mathfrak{G}_{P}(\\nu))\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Let $Q\\in{\\mathcal{N}}(P)$ , then $\\begin{array}{r}{\\gamma_{P}\\left(\\frac{1}{\\varepsilon n}\\right)\\subset\\gamma_{Q}\\left(\\frac{1}{2\\varepsilon n}\\right)\\subset\\gamma_{P}\\left(\\frac{1}{4\\varepsilon n}\\right)}\\end{array}$ so ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{\\stackrel{(\\frac{1}{2\\varepsilon n})}{\\scriptstyle2\\left(\\frac{1}{2\\varepsilon n}\\right)}}\\mathfrak{G}_{Q}(\\nu)+(|\\gamma_{Q}\\left(\\frac{1}{2\\varepsilon n}\\right)|-1)\\frac{1}{\\varepsilon n}=\\sum_{\\stackrel{\\nu\\notin\\gamma_{P}\\left(\\frac{1}{4\\varepsilon n}\\right)}{\\scriptstyle\\nu\\notin\\gamma_{P}\\left(\\frac{1}{4\\varepsilon n}\\right)}}\\mathfrak{G}_{Q}(\\nu)+\\sum_{\\stackrel{\\nu\\in\\gamma_{P}\\left(\\frac{1}{4\\varepsilon n}\\right)\\setminus\\gamma_{Q}\\left(\\frac{1}{2\\varepsilon n}\\right)}{\\scriptstyle\\nu\\in\\gamma_{P}\\left(\\frac{1}{4\\varepsilon n}\\right)\\setminus\\gamma_{Q}\\left(\\frac{1}{2\\varepsilon n}\\right)}}\\mathfrak{G}_{Q}(\\nu)+(|\\gamma_{Q}\\left(\\frac{1}{2\\varepsilon n}\\right)|-1)\\frac{1}{\\varepsilon n}}}&{}\\\\ &{\\leq\\sum_{\\stackrel{\\nu\\notin\\gamma_{P}\\left(\\frac{1}{4\\varepsilon n}\\right)}{\\scriptstyle\\nu\\notin\\gamma_{P}\\left(\\frac{1}{4\\varepsilon n}\\right)}}2\\mathfrak{G}_{P}(\\nu)+\\sum_{\\stackrel{\\nu\\in\\gamma_{P}\\left(\\frac{1}{4\\varepsilon n}\\right)\\setminus\\gamma_{Q}\\left(\\frac{1}{2\\varepsilon n}\\right)}{\\scriptstyle\\nu\\in\\gamma_{P}\\left(\\frac{1}{4\\varepsilon n}\\right)\\setminus\\gamma_{Q}\\left(\\frac{1}{2\\varepsilon n}\\right)}}\\frac{1}{\\varepsilon n}+(|\\gamma_{Q}\\left(\\frac{1}{2\\varepsilon n}\\right)|-1)\\frac{1}{\\varepsilon n}}&{}\\\\ &{\\leq\\sum_{\\stackrel{\\nu\\notin\\gamma_{P}\\left(\\frac{1}{4\\varepsilon n}\\right)}{\\scriptstyle\\nu\\notin\\gamma_{P}\\left(\\frac{1}{4\\varepsilon n}\\right)}}2\\mathfrak{G}_{P}(\\nu)+(|\\gamma_{P}\\left(\\frac{1}{4\\varepsilon n}\\right)|-1)\\frac{1}{\\varepsilon n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Now, let 4 min{W ( 50.45\u03b5),0.6}n \u2265n so for all Q \u2208N(P), ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{*}\\lefteqn{\\widetilde{\\sigma}(\\xi)\\leqslant\\frac{\\mathcal{O}}{\\delta}\\left(\\sum_{\\nu}\\operatorname*{min}\\bigg\\{\\mathfrak{S}_{Q}(\\nu)(1-\\mathfrak{S}_{Q}(\\nu)),\\sqrt{\\frac{\\mathfrak{S}_{Q}(\\nu)(1-\\mathfrak{S}_{Q}(\\nu))}{n^{\\prime}}}\\right\\}+\\sum_{\\nu\\notin\\mathcal{I}_{\\left(\\frac{1}{2\\pi}\\right)}}\\mathfrak{S}_{Q}(\\nu)+(|\\gamma\\varrho_{\\mathcal{I}}(\\frac{1}{2\\pi\\pi^{n}})|-\\mathfrak{S}_{Q}(\\nu)|)}}\\\\ &{\\le\\tilde{\\mathcal{O}}\\left(\\sum_{\\nu}2\\operatorname*{min}\\bigg\\{\\mathfrak{S}_{P}(\\nu)(1-\\mathfrak{S}_{P}(\\nu)),\\sqrt{\\frac{\\mathfrak{S}_{P}(\\nu)(1-\\mathfrak{S}_{P}(\\nu))}{n^{\\prime}}}\\right\\}+\\sum_{\\nu\\notin\\mathcal{I}_{\\left(\\frac{1}{2\\pi}\\right)}}2\\mathfrak{S}_{P}(\\nu)+(|\\gamma\\rho_{\\mathcal{I}}(\\frac{1}{4\\pi})|-\\mathfrak{S}_{P}(\\nu))}\\\\ &{=\\tilde{\\mathcal{O}}\\left(\\sum_{\\nu}2\\operatorname*{min}\\bigg\\{\\mathfrak{S}_{P}(\\nu)(1-\\mathfrak{S}_{P}(\\nu)),\\sqrt{\\frac{\\mathfrak{S}_{P}(\\nu)(1-\\mathfrak{S}_{P}(\\nu))}{n}}\\right\\}+\\sum_{\\nu\\notin\\mathcal{I}_{\\left(\\frac{1}{2\\pi}\\right)}}2\\mathfrak{S}_{P}(\\nu)+(|\\gamma\\rho_{\\mathcal{I}}(\\mathtt{k})|-\\mathfrak{S}_{P}(\\nu))}\\\\ &{\\le\\tilde{\\mathcal{O}}\\left(\\operatorname*{min}\\operatorname*{max}_{4\\nu}\\mathbb{E}_{X\\sim Q^{\\prime\\prime},\\mathcal{M}}(W(\\mathcal{A}^{\\prime}(X),Q^{\\prime}))\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "As in Proposition B.5, since ${\\mathcal{N}}(P)$ is compact, for all $\\mathcal{A^{\\prime}}$ , there exists a specific $Q^{*}\\in\\mathcal{N}(P)$ such that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}(Q^{*},Q_{\\varepsilon,n^{\\prime}}^{*})\\leq\\tilde{O}(\\mathbb{E}_{X\\sim(Q^{*})^{n},A^{\\prime}}(\\mathcal{W}({A^{\\prime}(X)},Q^{*})))}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "I Differentially Private Quantiles ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Estimating appropriately chosen quantiles is the main part of our algorithm for approximating the distribution over $\\mathbb{R}$ in Wasserstein distance, and so in this section, we describe some known differentially private algorithms for this task and derive some corollaries that we use extensively in our application. We will use $F$ to represent CDF functions, with $F_{P}$ representing the CDF of distribution $P$ . We start by stating an important theorem on private CDF estimation. This follows from a use of the binary tree mechanism [CSS11, DNPR10]. A version of this theorem for approximate differential privacy is described in a survey by Kamath and Ullman [KU20, Theorem 4.1]. The version presented here for pure differential privacy follows from a very similar argument, except using Laplace Noise instead of Gaussian noise (and basic composition instead of advanced composition to analyze privacy). Their accuracy was also in expectation, but a similar analysis yields a high probability bound, as in the theorem below. ", "page_idx": 51}, {"type": "text", "text": "Theorem I.1. [KU20, Theorem 4.1] ", "page_idx": 51}, {"type": "text", "text": "Let $\\varepsilon,\\beta\\in(0,1],$ , let $D$ be an ordered, finite domain, and let $\\mathbf{x}\\,\\in\\,D^{n}$ be a dataset. Let $\\hat{P}_{n}$ be the uniform distribution on x. Then, there exists an $\\varepsilon{-}D P$ algorithm $A^{C D F}$ that on input $\\mathbf{x}$ and the domain $D$ outputs a vector $G$ over $D$ such that with probability at least $1-\\beta$ over the randomness of ACDF : ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\|G-F_{\\hat{P}_{n}}\\|_{\\infty}=O\\left(\\frac{\\log^{3}\\frac{|D|}{\\beta}}{\\varepsilon n}\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "CDF estimation is intimately related to quantile estimation, and we use the following quantitative statement that will follow from a simple application of Theorem I.1. ", "page_idx": 51}, {"type": "text", "text": "Theorem I.2. Fix any $n\\,>\\,0,\\;\\varepsilon,\\beta\\,\\in\\,(0,1],\\;a,b\\,\\in\\,\\mathbb{R},$ and $\\gamma\\,<\\,b\\,-\\,a\\,\\in\\,\\mathbb{R}$ such that $\\frac{b\\!-\\!a}{\\gamma}$ is an integer. Let $C$ be a sufficiently large constant. Then, there exists an $\\varepsilon$ -DP algorithm $A_{q u a n t}$ , that on input interval end points $a,b,$ , granularity $\\gamma,$ , $\\mathbf{x}=(x_{1},\\ldots,x_{n})\\in\\{a,a+\\gamma,\\ldots,b-\\bar{\\gamma},b\\}^{n}$ , and desired quantile values $\\alpha\\in(0,1)^{k}$ , outputs quantiles $\\tilde{q}\\in\\{a,a+\\gamma,\\dots,b-\\gamma,b\\}^{k}$ such that with probability at least $1-\\beta$ over the randomness of $A_{q u a n t}$ , for all $r\\in[k]$ , ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\alpha_{r}-F_{\\hat{P}_{n}}(\\tilde{q}_{r})\\leq C\\frac{\\log^{3}\\frac{b-a}{\\beta\\gamma}}{\\varepsilon n},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{y\\sim\\hat{P}_{n}}(y<\\tilde{q}_{r})<\\alpha_{r}+C\\frac{\\log^{3}\\frac{b-a}{\\beta\\gamma}}{\\varepsilon n},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $\\hat{P}_{n}$ is the uniform distribution on the entries of $\\mathbf{x}$ . ", "page_idx": 52}, {"type": "text", "text": "Proof. Algorithm $A_{q u a n t}$ operates by running the algorithm $A^{C D F}$ referenced in Theorem I.1 on $\\mathbf{x}$ and domain $\\{a,a\\,\\dot{+}\\,\\gamma,\\dots,b-\\gamma,b\\}$ , and postprocessing its outputs to get quantile estimates as follows. For every quantile $\\alpha_{r}$ that we are asked to estimate, $A_{q u a n t}$ simply scans the vector $G$ output by algorithm $A^{C D F}$ in order, and outputs the first domain element whose CDF estimate in $G$ crosses $\\alpha_{r}$ . Conditioned on the accuracy of the CDF estimation algorithm $G$ , we have that this output $\\tilde{q}_{r}$ satisfies ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\alpha_{r}-F_{\\hat{P}_{n}}(\\tilde{q}_{r})\\leq C\\frac{\\log^{3}\\frac{b-a}{\\beta\\gamma}}{\\varepsilon n}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Additionally, since $\\tilde{q}_{r}$ is the first domain element whose estimate in $G$ crosses $\\alpha_{r}$ , we also have that ", "page_idx": 52}, {"type": "equation", "text": "$$\nP r_{y\\sim\\hat{P}_{n}}(y<\\tilde{q}_{r})<\\alpha_{r}+C\\frac{\\log^{3}\\frac{b-a}{\\beta\\gamma}}{\\varepsilon n}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Hence, with probability at least $1-\\beta$ , we have this property for all $r\\in[k]$ . ", "page_idx": 52}, {"type": "text", "text": "We now state a corollary of this theorem that we will use extensively in our presentation. ", "page_idx": 52}, {"type": "text", "text": "Corollary I.3. Fix any $\\varepsilon,\\beta\\in(0,1]$ , $a,b\\in\\mathbb{R},$ , and $\\gamma<b-a\\in\\mathbb{R}$ such that $\\frac{b\\!-\\!a}{\\gamma}$ is an integer. Let n \u2208N > 4c2 log4( b\u03b2\u2212\u03b3\u03b5a ), such that $k$ set to $\\int_{\\frac{\\varepsilon n}{4c_{3}l o g^{3}\\frac{b-a}{\\beta\\gamma}\\log\\frac{n}{\\beta}}}\\rceil$ is an integer greater than or equal to 1, where $c_{2}$ and $c_{3}$ are sufficiently large constants. 6 ", "page_idx": 52}, {"type": "text", "text": "Then, there exists an $\\varepsilon$ -DP algorithm $A_{q u a n t}$ (the same one referenced in Theorem I.2), that on input interval end points $a,b$ , granularity $\\gamma$ , $\\mathbf{\\dot{x}}=(x_{1},\\dots,x_{n})\\in\\{a,a+\\gamma,\\dots,b-\\gamma,b\\}^{n}$ , and desired quantile values $\\alpha=\\{1/2k,3/2k,5/2k,\\ldots,(2k-1)/2k\\}$ , outputs quantiles $\\tilde{q}\\in\\{a,a\\!+\\!\\gamma,\\ldots,b\\!-$ $\\gamma,b\\}^{k}$ such that with probability at least $1-\\beta$ , for all $r\\in[k]$ , ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{q}_{\\frac{2r-1}{2k}-\\frac{1}{4k}}\\leq\\tilde{q}_{r}\\leq\\hat{q}_{\\frac{2r-1}{2k}+\\frac{1}{4k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $\\hat{P}_{n}$ is the uniform distribution on the entries of x and for all $p\\in(0,1)$ , ${\\hat{q}}_{p}$ is the $p$ -quantile of $\\hat{P}_{n}$ . ", "page_idx": 52}, {"type": "text", "text": "Proof. First, note that k is set such that41k \u2265Clog\u03b53 nb\u03b2\u2212\u03b3 ", "page_idx": 52}, {"type": "text", "text": "Hence, by Theorem I.2, we have that with probability at least 0.99, ", "page_idx": 52}, {"type": "text", "text": "for all $r\\in[k]$ , ", "page_idx": 52}, {"type": "text", "text": "and ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{2r-1}{2k}-F_{\\hat{P}_{n}}(\\tilde{q}_{r})\\leq\\frac{1}{4k},}}\\\\ {{\\displaystyle\\mathrm{Pr}~_{\\hat{P}_{n}}(y<\\tilde{q}_{r})<\\frac{2r-1}{2k}+\\frac{1}{4k}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Condition the event above for the rest of the proof. Note that the first equation implies that for all $r\\in[k]$ , ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{y\\sim\\hat{P}_{n}}(y\\leq\\tilde{q_{r}})\\geq\\frac{2r-1}{2k}-\\frac{1}{4k},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "which implies that q\u02dcr \u2265q\u02c6 2r2k\u22121 \u221241k . ", "page_idx": 52}, {"type": "text", "text": "Next, note that we also have that for all $r\\in[k]$ , ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{y\\sim\\hat{P}_{n}}(y<\\tilde{q}_{r})<\\frac{2r-1}{2k}+\\frac{1}{4k}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "This implies that for all r \u2208[k], q\u02dcr \u2264q\u02c6 2r\u22121 + 1 . ", "page_idx": 53}, {"type": "text", "text": "J Proofs in Section $\\mathbf{E}$ ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "J.1 Omitted Proofs in Section E.1.1 ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Proof of Lemma E.6. We evaluate the various terms in Theorem E.5. ", "page_idx": 53}, {"type": "text", "text": "We start by evaluating $\\begin{array}{r l r}{\\tau(P,Q)}&{=}&{\\operatorname*{max}\\{\\int_{\\mathbb{R}}\\operatorname*{max}(f_{P}(t)\\ -\\ e^{\\varepsilon}f_{Q}(t),0)d t,\\int_{\\mathbb{R}}\\operatorname*{max}(f_{Q}(t)\\ -\\ e^{\\varepsilon}f_{Q}(t),0)d t\\}}\\end{array}$ $e^{\\varepsilon}f_{P}(t),0)d t\\ \\}$ . Consider the first term in the outer maximum. For all $t\\,\\in\\,[L(P),q_{1/k})$ , we have that $\\begin{array}{r}{f_{Q}(t)\\,=\\,\\frac{1}{2}f_{P}(t)}\\end{array}$ . For all other $t$ , one can see that the value of the integrand is 0. Hence, the value of the first term is $\\begin{array}{r}{\\int_{L(P)}^{q_{1/k}}\\operatorname*{max}(f_{P}(t)\\,-\\,\\frac{e^{\\varepsilon}}{2}f_{P}(t),0)d t\\,=\\,\\operatorname*{max}\\{\\left(1-\\frac{e^{\\varepsilon}}{2}\\right)\\frac{1}{k},0\\}\\,\\le\\,\\frac{1}{2k}}\\end{array}$ . Now, consider the second term in the outer maximum. For all $t<q_{1-\\frac{1}{k}}$ , the value of the integrand is 0. For all $q_{1-\\frac{1}{k}}\\,\\leq\\,t\\,\\leq\\,q_{1}$ , the value of the integrand is $\\operatorname*{max}\\{\\left(\\frac{3}{2}-e^{\\varepsilon}\\right)f_{P}(t),0\\}$ . Hence, the second term is $\\begin{array}{r}{\\operatorname*{max}\\\"\\{\\left(\\frac{3}{2}-e^{\\varepsilon}\\right)\\frac{1}{k},0\\}\\le\\frac{1}{2k}}\\end{array}$ . Put together, we get that $\\begin{array}{r}{\\tau(P,Q)\\le\\frac{1}{2k}}\\end{array}$ . ", "page_idx": 53}, {"type": "text", "text": "When $\\varepsilon\\ge\\ln2$ , we have that $\\begin{array}{r}{1-\\frac{e^{\\varepsilon}}{2}\\leq0}\\end{array}$ , and so we have that the largest value of $\\varepsilon^{\\prime}\\in[0,\\varepsilon]$ that makes $\\begin{array}{r}{\\int_{\\mathbb{R}}\\operatorname*{max}(f_{Q}(t)-e^{\\varepsilon^{\\prime}}f_{P}(t),0)d t=\\tau(P,Q)=0}\\end{array}$ , is $\\varepsilon^{\\prime}\\,=\\,\\varepsilon$ . When $\\varepsilon<\\ln{2}$ , we have that the value of $\\varepsilon^{\\prime}$ that makes $\\begin{array}{r}{\\int_{\\mathbb{R}}\\operatorname*{max}(f_{Q}(t)-e^{\\varepsilon^{\\prime}}f_{P}(t),0)d t=\\operatorname*{max}\\{\\left(\\frac{3}{2}-e^{\\varepsilon}\\right)\\frac{1}{k},0\\}=\\left(1-\\frac{e^{\\varepsilon}}{2}\\right)\\frac{1}{k}}\\end{array}$ , is $\\begin{array}{r}{\\varepsilon^{\\prime}=\\ln\\left(\\frac{1+e^{\\varepsilon}}{2}\\right)}\\end{array}$ . ", "page_idx": 53}, {"type": "text", "text": "Finally, we describe the distributions $P^{\\prime}$ and $Q^{\\prime}$ and compute the squared Hellinger distance between them. There are two cases, based on the range of $\\varepsilon$ . First, consider $\\varepsilon\\ge\\ln2$ . First, we calculate $\\tilde{P}\\,\\equiv\\,\\operatorname*{min}\\{e^{\\varepsilon}Q,P\\}$ . This value is equal to $\\operatorname*{min}\\{e^{\\varepsilon}/2,1\\}f_{P}(t)\\,=\\,f_{P}(t)$ for $t\\,<\\,q_{\\frac{1}{k}}(P)$ , and is also equal to $f_{P}(t)$ for $q_{\\frac{1}{k}}\\,\\leq\\,t\\,\\leq\\,q_{1}$ . Similarly, consider $\\tilde{Q}\\,\\equiv\\,\\mathrm{min}\\{e^{\\varepsilon^{\\prime}}P,Q\\}\\,=\\,\\mathrm{min}\\{e^{\\varepsilon}P,Q\\}$ ; it is equal to $\\frac{f_{P}(t)}{2}$ for $t\\,<\\,q_{\\frac{1}{k}}(P)$ , and is equal to $f_{P}(t)$ for $q_{\\frac{1}{k}}\\,\\leq\\,t\\,\\leq\\,q_{1-\\frac{1}{k}}$ . It is also equal to $\\begin{array}{r}{\\operatorname*{min}(e^{\\varepsilon},\\frac{3}{2})f_{P}(t)=\\frac{3}{2}f_{P}(t)}\\end{array}$ for $q_{\\frac{1}{k}}\\leq t\\leq q_{1}$ . Since $\\tau(P,Q)=0$ , and by the above calculations, we have that $P^{\\prime}=P$ , and $Q^{\\prime}=Q$ . Upper bounding the squared Hellinger distance between $P^{\\prime}$ and $Q^{\\prime}$ by the TV distance (See Lemma A.7), we get that $\\begin{array}{r}{H^{2}\\Dot{(}P^{\\prime},Q^{\\prime})=\\check{H^{2}}(P,Q)\\leq T V(P,Q)=\\frac{1}{2k}\\overset{\\cdot}{\\leq}}\\end{array}$ 2(ln\u03b5 2)k (where we have used that \u03b5 \u2265ln 2). ", "page_idx": 53}, {"type": "text", "text": "Next, consider $\\varepsilon\\ \\ <\\ \\ \\ln{2}$ . First, consider $\\tilde{P}\\;\\;\\equiv\\;\\;\\operatorname*{min}\\{e^{\\varepsilon}Q,P\\}$ . This value is equal to $\\mathrm{min}\\{e^{\\varepsilon}/2,1\\}f_{P}(t)\\;=\\;\\frac{e^{\\varepsilon}}{2}f_{P}(t)$ for $t\\ <\\ q_{\\frac{1}{k}}(P)$ , and is also equal to $f_{P}(t)$ for $q_{\\frac{1}{k}}~\\leq~t~\\leq~q_{1}$ . Similarly, consider $\\begin{array}{r}{\\tilde{Q}\\equiv\\operatorname*{min}\\{e^{\\varepsilon^{\\prime}}P,Q\\}=\\operatorname*{min}\\{\\frac{1+e^{\\varepsilon}}{2}P,Q\\}}\\end{array}$ ; it is equal to $\\scriptstyle{\\frac{1}{2}}f_{P}(t)$ for $t\\,<\\,q_{\\frac{1}{k}}(P)$ , and is equal to $f_{P}(t)$ at $q_{\\frac{1}{k}}~\\leq~t\\,\\leq~q_{1-\\frac{1}{k}}$ . It is also equal to $\\begin{array}{r}{\\operatorname*{min}\\{\\frac{1+e^{\\varepsilon}}{2},\\frac{3}{2}\\}f_{P}(t)\\,=\\,\\frac{1+e^{\\varepsilon}}{2}f_{P}(t)}\\end{array}$ at $q_{1-\\frac{1}{k}}\\ \\leq\\ t\\ \\leq\\ q_{1}$ . Note that $\\begin{array}{r}{\\tau(P,Q)\\stackrel{\\sim}{=}\\left(1-\\frac{e^{\\varepsilon}}{2}\\right)\\frac{1}{k}}\\end{array}$ . $P^{\\prime}$ and $Q^{\\prime}$ are the distributions created by normalizing $\\tilde{P}$ and $\\tilde{Q}$ by dividing by a factor of $1\\mathrm{~-~}\\tau(P,Q)$ . Now, we upper bound the squared Hellinger distance between $P^{\\prime}$ and $Q^{\\prime}$ by the TV distance (See Lemma A.7), to get that $\\begin{array}{r}{\\dot{H^{2}}(P^{\\prime},Q^{\\prime})\\leq\\bar{T}V(P^{\\prime},Q^{\\prime})=O(\\frac{\\varepsilon}{k})}\\end{array}$ . ", "page_idx": 53}, {"type": "text", "text": "Substituting into the lower bound for sample complexity of distinguishing $P$ and $Q$ , this tells us that for all $\\begin{array}{r}{\\varepsilon\\in(0,1],S C_{\\varepsilon}(P,Q)=\\Omega\\left(\\frac{1}{\\varepsilon\\cdot\\frac{1}{k}}\\right)=\\Omega(k/\\varepsilon)}\\end{array}$ . ", "page_idx": 53}, {"type": "text", "text": "Proof of Lemma $E.7.$ . Note that $P$ has bounded expectation (and hence, so does $Q$ ). Hence, we can use the following form of the Wasserstein distance: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal{W}(P,Q)=\\int_{\\mathbb{R}}|F_{P}(t)-F_{Q}(t)|d t.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Now, given the settings of $P$ and $Q$ , we can precisely write the forms of their cumulative distribution function as follows. Note that for $L(P)\\leq\\bar{t}<q_{1/k}(P)$ , we have that $\\begin{array}{r}{|F_{P}(t)-F_{Q}(t)|=\\frac{1}{2}F_{p}(t)}\\end{array}$ . For $q_{1/k}\\le t\\le q_{1-\\frac{1}{k}}$ , we have $\\begin{array}{r}{|F_{P}(t)-F_{Q}(t)|=\\frac{1}{2k}}\\end{array}$ . Finally, for $q_{1-\\frac{1}{k}}\\leq t\\leq q_{1}$ , we have that $\\begin{array}{r l}&{F_{P}(t)=1-\\frac{1}{k}+\\int_{q_{1-1/k}}^{\\tilde{t}}f_{P}(t)d t\\mathrm{~and~}F_{Q}(t)=1-\\frac{3}{2k}+\\frac{3}{2}\\int_{q_{1-1/k}}^{t}f_{P}(t)d t,\\sqrt{2}f_{P}(t)d t,}\\\\ &{F_{P}(t)-F_{Q}(t)=\\frac{1}{2k}-\\frac{1}{2}\\int_{q_{1-1/k}}^{t}f_{P}(t)d t=\\frac{1}{2}[1-F_{P}(t)].}\\end{array}$ which gives us that ", "page_idx": 54}, {"type": "text", "text": "Hence, we have that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{V}(P,Q)=\\int_{\\mathbb{R}}|F_{P}(t)-F_{Q}(t)|d t}\\\\ {\\displaystyle=\\frac{1}{2}\\int_{L(P)}^{q_{1}/k}F_{P}(t)d t+\\int_{q_{1}/k}^{q_{1}}|F_{P}(t)-F_{Q}(t)|d t+\\int_{q_{1}-\\frac{1}{k}}^{q_{1}}|F_{P}(t)-F_{Q}(t)|d t}\\\\ {\\displaystyle\\geq\\frac{1}{2}\\int_{L(P)}^{q_{1}/k}F_{P}(t)d t+\\frac{1}{2}\\int_{q_{1}-\\frac{1}{k}}^{q_{1}}\\big[1-F_{P}(t)\\big]d t+\\frac{1}{2k}(q_{1-\\frac{k}{k}}-q_{\\frac{k}{k}})}\\\\ {\\displaystyle=\\frac{1}{2}\\int_{q_{1-\\frac{1}{k}}}^{q_{1}}\\Big|F_{P}(t)-F_{P|_{q_{1}-\\frac{k}{k}}}(t)\\Big|d t+\\frac{1}{2}\\int_{L(P)}^{q_{1}/k}\\Big|F_{P}(t)-F_{P|_{q_{1}-\\frac{k}{k}}}(t)\\Big|d t+\\frac{1}{2k}(q_{1-\\frac{1}{k}})}\\\\ {\\displaystyle=\\frac{1}{2k}(q_{1-\\frac{k}{k}}-q_{1/k})+\\frac{1}{2}\\mathcal{W}(P,P|_{q_{1},q_{1-\\frac{k}{k}}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "J.2 Omitted proofs in Section E.1.2 ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Proof of Lemma $E.I O$ . The $\\mathrm{KL}$ divergence is defined as $\\begin{array}{r}{\\int_{t:f_{Q}(t)>0}f_{P}(t)\\log f_{P}(t)/f_{Q}(t)d t}\\end{array}$ . This can be broken up into a sum over the dyadic quantiles as: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{K L(P,Q)=\\displaystyle\\sum_{i=1}^{m-1}\\int_{0}^{\\infty}r_{i}\\omega^{i}r_{j}\\,\\mathrm{d}t)\\log\\int_{\\int\\hat{\\cal Q}(i)}^{r(i)}\\,\\mathrm{d}t\\int_{\\hat{\\cal Q}(i)}^{r(i)-1/r}\\int_{r_{i}(1)}^{r(i)}\\log\\frac{f_{\\theta}(r)}{f_{\\theta}(i)}d t}\\\\ {+\\displaystyle\\sum_{i=1}^{m-1}\\int_{0}^{\\infty}r_{i}\\omega^{i}r_{j}\\,\\mathrm{d}t)\\log\\int_{\\hat{\\cal Q}(i)}^{r(i)}\\,\\mathrm{d}t\\int_{\\hat{\\cal Q}(i)}^{r(i)-1/r}\\int_{r_{i}(1)}^{r(i)}\\log\\frac{f_{\\theta}(r)}{f_{\\theta}(i)}d t}\\\\ {+\\displaystyle\\sum_{i=1}^{m-1}\\int_{0}^{\\infty}r_{i}\\omega^{i}r_{i}\\,\\mathrm{d}t\\Big\\}\\Big|_{r_{i}(1)}^{r}\\,\\mathrm{d}\\hat{\\cal P}_{r}^{\\theta}\\Big|_{r_{i}(1)}^{r}}&{=\\displaystyle\\sum_{i=0}^{m-1}f_{r_{i}(1)}^{r(i)-1}\\int_{0}^{\\infty}\\hat{f}_{\\theta}(r)\\log\\frac{1}{1-r}\\int_{0}^{\\hat{\\cal Q}}{\\hat{\\theta}}}\\\\ {+\\displaystyle\\sum_{i=1}^{m}\\int_{0}^{\\infty}r_{i}\\omega^{i}r_{j}\\,\\mathrm{d}t\\Big\\}\\Big|\\omega_{1}\\frac{1}{\\displaystyle r_{i}(1)}\\hat{\\cal Q}+\\int_{r_{i}(1)-r_{i}(2)}^{r(i)}\\,f_{\\theta}(r)\\Big\\|_{r_{i}(1)}\\frac{1}{\\displaystyle r_{i}(1-r)}d t}\\\\ {+\\displaystyle\\sum_{i=1}^{m}\\int_{0}^{\\infty}\\int_{0}^{\\infty}\\Big|\\omega_{1}\\frac{1}{\\displaystyle r_{i}(1-r_{i}^{\\frac{1}{r_{i}^{\\frac{1}{r_{i}^{\\frac{1}{r_{i}^{\\frac{1}{r}}}}}}})}}+\\int_{r_{i}}^{r}\\frac{1}{\\displaystyle r_{i}(1)}\\Big|\\omega_{1}\\\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where the third inequality from last is by the fact that the geometric series $\\textstyle\\sum_{i=\\log n}^{\\infty}{\\frac{1}{2^{i}}}$ converges to $O(\\textstyle{\\frac{1}{n}})$ , the second inequality from last is from the fact that $\\frac{2^{i}}{n}<1/2$ , and $\\log(1/(1-y))<2y$ for $0<y<1/2$ . \u53e3 ", "page_idx": 54}, {"type": "text", "text": "Proof of Lemma $E.I I$ . First, we recall the definition of the 1-Wasserstein distance in terms of the cumulative distribution function. ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathcal{W}(P,Q)=\\int_{\\mathbb{R}}|F_{P}(t)-F_{Q}(t)|d t\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Fix any $2\\,\\leq\\,i\\,<\\,\\log n\\,-\\,1$ . Observe that by construction, for all $t\\,\\in\\,[q_{1/2^{i}},q_{1-1/2^{i}})$ and for all t \u2208[q1\u22121/2i\u22121, q1\u22121/2i), |FP (t) \u2212FQ(t)| \u2265 ljo=g in+\u221211\u221a21jn + 12 j\u221e=log n21j . Similarly, fix any $\\log n-1\\leq i<\\infty$ . Observe that for all $t\\in[q_{1/2^{i}},q_{1-1/2^{i}})$ , and for all $t\\in[q_{1-1/2^{i-1}},q_{1-1/2^{i}}).$ , we have that $\\begin{array}{r}{|F_{P}(t)-F_{Q}(t)|\\geq\\frac{1}{2}\\sum_{j=i+1}^{\\infty}\\frac{1}{2^{j}}}\\end{array}$ . Substituting the above bounds in the formula for the Wasserstein distance, we get that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathpalette{W}(P,Q)\\geq\\sum_{i=2}^{\\log n-2}\\int_{q_{1/2^{i}}}^{q_{1/2^{i-1}}}\\left[\\overset{\\log n-2}{\\underset{j=i+1}{\\sum_{j=i+1}^{\\infty}}}\\frac{1}{\\sqrt{2^{j}n}}+\\frac{1}{2}\\sum_{j=\\log n-1}^{\\infty}\\frac{1}{2^{j}}\\right]\\,d t+\\int_{q_{1-1/2^{i-1}}}^{q_{1-1/2^{i}}}\\left[\\overset{\\log n-2}{\\underset{j=i+1}{\\sum_{j=i+1}^{\\infty}}}\\frac{1}{\\sqrt{2^{j}n}}+\\frac{1}{2}\\right]\\,d t}\\\\ &{\\displaystyle\\qquad+\\sum_{i=\\log n-1}^{\\infty}\\int_{q_{1/2^{i}}}^{q_{1/2^{i-1}}}\\frac{1}{2}\\sum_{j=i+1}^{\\infty}\\frac{1}{2^{j}}d t+\\int_{q_{1-1/2^{i}}}^{q_{1-1/2^{i}}}\\frac{1}{2}\\sum_{j=i+1}^{\\infty}\\frac{1}{2^{j}}d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Pulling the summation over $j$ outside the integral and grouping terms, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathcal{W}(P,Q)\\geq}&{\\displaystyle\\sum_{i=2}^{\\log n-2}\\left[\\sum_{j=i+1}^{\\log n-2}\\int_{q_{1/2^{i}}}^{q_{1/2^{i}-1}}\\frac{1}{\\sqrt{2^{j}n}}d t+\\int_{q_{1-1/2^{i}-1}}^{q_{1-1/2^{i}}}\\frac{1}{\\sqrt{2^{j}n}}d t+\\frac{1}{2}\\sum_{j=\\log n-1}^{\\infty}\\int_{q_{1/2^{i}}}^{q_{1/2^{i}-1}}\\frac{1}{2^{j}}d t+\\right.}\\\\ &{+\\left.\\frac{1}{2}\\sum_{i=\\log n-1}^{\\infty}\\sum_{j=i+1}^{\\infty}\\left[\\int_{q_{1/2^{i}}}^{q_{1/2^{i-1}}}\\frac{1}{2^{j}}d t+\\int_{q_{1-1/2^{i-1}}}^{q_{1-1/2^{i}}}\\frac{1}{2^{j}}d t\\right]}\\\\ &{=\\displaystyle\\sum_{i=2}^{\\log n-2}\\left[(q_{1/2^{i-1}}-q_{1/2^{i}})+(q_{1-1/2^{i}}-q_{1-1/2^{i-1}})\\right]\\left[\\sum_{j=i+1}^{\\log n-2}\\frac{1}{\\sqrt{2^{j}n}}+\\frac{1}{2}\\sum_{j=\\log n-1}^{\\infty}\\frac{1}{2^{j}}\\right]}\\\\ &{+\\displaystyle\\sum_{i=\\log n-1}^{\\infty}\\left[(q_{1/2^{i-1}}-q_{1/2^{i}})+(q_{1-1/2^{i}}-q_{1-1/2^{i-1}})\\right]\\frac{1}{2}\\sum_{j=i+1}^{\\infty}\\frac{1}{2^{j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Switching the order of summation (summing over $j$ first), and grouping terms, we get ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{W}(P,Q)\\geq\\sum_{j=3}^{\\log n-2}\\frac{1}{\\sqrt{2^{j}n}}\\sum_{i=2}^{j-1}\\left[\\left(q_{1/2^{i-1}}-q_{1/2^{i}}+\\left(q_{1-1/2^{i}}-q_{1-1/2^{i-1}}\\right)\\right]\\right.}}\\\\ &{}&{\\left.+\\,\\frac{1}{2}\\sum_{j=\\log n-1}^{\\infty}\\frac{1}{2^{j}}\\sum_{i=2}^{j-1}\\left[\\left(q_{1/2^{i-1}}-q_{1/2^{i}}+\\left(q_{1-1/2^{i}}-q_{1-1/2^{i-1}}\\right)\\right]\\right.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Telescoping the inner sums over $i$ we get that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathcal{W}(P,Q)\\geq\\sum_{j=3}^{\\log n-2}\\frac{1}{\\sqrt{2^{j}n}}\\left[q_{1-1/2^{j-1}}-q_{1/2^{j-1}}\\right]+\\frac{1}{2}\\sum_{j=\\log n-1}^{\\infty}\\frac{1}{2^{j}}\\left[q_{1-1/2^{j-1}}-q_{1/2^{j-1}}\\right]\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "A change of variables (where we now set $j$ to $j-1)$ then gives ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{W}(P,Q)\\ge\\frac{1}{\\sqrt{2}}\\sum_{j=2}^{\\log n-3}\\frac{1}{\\sqrt{2^{j}n}}\\left[q_{1-1/2^{j}}-q_{1/2^{j}}\\right]+\\frac{1}{4}\\sum_{j=\\log n-2}^{\\infty}\\frac{1}{2^{j}}\\left[q_{1-1/2^{j}}-q_{1/2^{j}}\\right]}}\\\\ &{}&{\\ge\\frac{1}{4}\\sum_{j=2}^{\\log n-1}\\frac{1}{\\sqrt{2^{j}n}}\\left[q_{1-1/2^{j}}-q_{1/2^{j}}\\right]+\\frac{1}{4}\\sum_{j=\\log n}^{\\infty}\\frac{1}{2^{j}}\\left[q_{1-1/2^{j}}-q_{1/2^{j}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where the last inequality is by pulling the first two terms from the summation in second term to the summation in the first term, and using the fact that for $j=\\log n-2,j=\\log n-1$ , we have that $\\begin{array}{r}{\\frac{1}{4\\cdot2^{j}}\\geq\\frac{1}{2\\sqrt{2}}\\frac{1}{\\sqrt{2^{j}n}}}\\end{array}$ ", "page_idx": 56}, {"type": "text", "text": "Proof of Lemma E.12. We first state a theorem of Bobkov and Ledoux [BL19]. ", "page_idx": 56}, {"type": "text", "text": "Theorem J.1 (Theorem 3.5, [BL19]). There is an absolute constant $c>0$ , such that for all distributions $P$ over $\\mathbb{R}$ , for every $n\\geq1$ , ", "page_idx": 56}, {"type": "equation", "text": "$$\nc(A_{n}+B_{n})\\leq\\mathbb{E}[\\mathcal{W}(P,\\hat{P}_{n}]\\leq A_{n}+B_{n}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where ", "page_idx": 56}, {"type": "equation", "text": "$$\nA_{n}=2\\int_{F(t)[1-F(t)]\\leq\\frac{1}{4n}}F(t)[1-F(t)]d t,\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "and ", "page_idx": 56}, {"type": "equation", "text": "$$\nB_{n}=\\frac{1}{\\sqrt{n}}\\int_{F(t)[1-F(t)]\\geq\\frac{1}{4n}}\\sqrt{F(t)[1-F(t)]}d t.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Now, we are ready to prove the main theorem. Fix natural number $i\\;\\geq\\;2$ . Restricted to $t\\ \\leq$ $q_{1/2}$ , $F_{P}(t)(1-\\dot{F_{P}}(t)\\dot{)}$ is an increasing function, and hence for $t\\,\\in\\,[q_{1/2^{i}},q_{1/2^{i-1}}]$ , we have that $\\begin{array}{r}{F_{P}(t)(1-F_{P}(t))\\leq\\frac{1}{2^{i-1}}[1-\\frac{1}{2^{i-1}}]}\\end{array}$ . ", "page_idx": 56}, {"type": "text", "text": "Similarly, restricted to $t\\,>\\,q_{1/2}$ , $F_{P}(t)(1-F_{P}(t))$ is a decreasing function, and hence for $t\\ \\in$ $[1-q_{1/2^{i-1}},q_{1-1/2^{i}}]$ , we have that $\\begin{array}{r}{F_{P}(t)(1-F_{P}(t))\\leq\\frac{1}{2^{i-1}}[1-\\frac{1}{2^{i-1}}]}\\end{array}$ . ", "page_idx": 56}, {"type": "text", "text": "Using this, we can now upper bound the expected Wasserstein distance between $P$ and its empirical distribution using Theorem J.1. Hence, we upper bound the terms $B_{n}$ and $A_{n}$ . We start by upper bounding $B_{n}$ . Note that for all $t\\notin[q_{\\frac{1}{4n}},q_{1-\\frac{1}{4n}}]$ , we have that $\\begin{array}{r}{F_{P}(t)(1-F_{P}(t))\\leq\\frac{1}{4n}}\\end{array}$ . Hence, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\allowdisplaybreaks}&{\\mathbf{\\Phi}_{n}=\\frac{1}{\\sqrt{n}}\\int_{m_{0}(1)}^{\\infty}\\exp\\Big[\\!\\exp\\Big(\\!1\\!-\\!P_{t}(\\mathbf{\\Phi})[\\mathbf{i}]\\!\\Big)\\mathbf{a}^{n}}\\\\ &{\\quad\\leq\\frac{1}{\\sqrt{n}}\\int_{a_{0}(1)}^{\\infty}\\mathrm{d}v\\!+\\!\\mathrm{d}v\\!\\int_{0}^{\\infty}(v\\!+\\!1\\!-\\!P_{t}(v))\\mathbf{i}\\mathbf{a}^{n}}\\\\ &{\\quad\\leq\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}\\Bigg\\{\\int_{0}^{\\infty}\\!\\exp\\Big(\\!1\\!-\\!P_{t}(v)[\\mathbf{i}]\\!-\\!F_{t}(v)\\!\\Big)\\mathbf{i}\\mathbf{a}\\!+\\!\\int_{\\alpha_{0}(1)}^{v\\!+\\!v\\!-\\!v\\!}\\!\\mathrm{d}v\\!\\int_{0}^{\\infty}\\!\\mathrm{d}v\\!\\Big[\\!(1\\!-\\!P_{t}(v)\\!)\\!\\Big]\\mathbf{a}^{n}}\\\\ &{\\quad\\leq\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}\\Bigg\\{\\int_{0}^{\\infty}\\!\\mathrm{d}v\\!+\\!\\sqrt{\\frac{1}{2}\\!-\\!\\left[1\\!-\\!\\frac{1}{2}\\right]}\\mathbf{i}\\!\\Big\\}\\alpha\\int_{v\\in\\mathbb{Z}_{0}}^{v\\!-\\!1\\!-\\!v\\!}\\Bigg\\{\\frac{1}{2}\\!\\Bigg[1\\!-\\!\\frac{1}{2}\\frac{1}{v^{2}}\\!\\Bigg]u\\!}\\\\ &{\\quad=\\frac{1}{\\sqrt{n}}\\ln\\Bigg[\\frac{1}{2}\\!-\\!\\left[1\\!-\\frac{1}{2^{n}}\\right]\\Big[u\\!_{0}v\\!-\\!\\alpha_{0}v\\!+\\!\\Phi_{t}\\!-\\!\\alpha_{1}\\!-\\!\\alpha_{1}\\!-\\!\\alpha_{1}\\!-\\!\\alpha_{1}\\!-\\!\\alpha_{1}\\!}\\\\ &{\\quad\\leq\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}\\Big(v\\!-\\!1\\!-\\!\\frac{v\\!}{2^{n}}\\!-\\!\\Phi_{t}v\\!\\Big)}\\\\ &{\\quad=\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}\\Big(\\frac{1}{2^{n}}\\!-\\!\\frac{1}{2 \n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where the last inequality is because for $i\\leq\\log(4n)$ , we have that $\\textstyle{\\frac{1}{n}}\\leq{\\frac{4}{2^{i}}}$ . ", "page_idx": 56}, {"type": "text", "text": "Next, we bound $A_{n}$ . Note that for all $t\\,\\geq\\,q_{1/2n}$ and for all $t\\leq q_{1-\\frac{1}{2n}}$ , we have that $F_{P}(t)(1-$ $\\begin{array}{r}{F_{P}(t))\\not\\leq\\frac{1}{4n}}\\end{array}$ . Hence, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{n}=2\\int_{V_{r}(t)[1-P_{r}(t)]\\leq\\frac{1}{\\alpha}}F_{r}(t)[1-F_{P}(t)]d t}\\\\ &{\\quad\\leq2\\left[\\int_{-\\infty}^{q_{1}}F_{P}(t)[1-F_{P}(t)]d t+\\int_{q_{1}-\\frac{1}{\\alpha}}^{\\infty}F_{P}(t)[1-F_{P}(t)]d t\\right]}\\\\ &{\\quad=\\underset{i=1+\\nu_{0}\\geq0}{\\leq\\infty}\\left[\\int_{q_{1},\\ldots}^{q_{1}\\ldots}F_{P}(t)[1-F_{P}(t)]d t+\\int_{q_{1}-\\frac{1}{\\alpha},\\ldots}^{q_{1}\\ldots}F_{P}(t)[1-F_{P}(t)]d t\\right]}\\\\ &{\\quad\\leq\\underset{i=1+\\nu_{0}\\geq0}{\\leq\\infty}\\left[\\int_{q_{1},\\ldots}^{q_{1}\\ldots}\\frac{1}{2^{i-1}}\\left[1-\\frac{1}{2^{i-1}}\\right]d t+\\int_{q_{1}-\\frac{1}{\\alpha},\\ldots}^{q_{1}-\\frac{1}{\\alpha}}\\frac{1}{2^{i-1}}\\left[1-\\frac{1}{2^{i-1}}\\right]d t\\right]}\\\\ &{\\quad=\\underset{i=1+\\nu_{0}\\geq0}{\\leq\\infty}\\frac{2}{2^{i-1}}\\left[1-\\frac{1}{2^{i-1}}\\right]\\left[q_{1}/2\\cdots-q_{1}/2+q_{1}-q_{2}-q_{1}-1/2-q_{1}\\right]}\\\\ &{\\quad\\leq\\underset{i=1+\\nu_{0}\\geq0}{\\leq\\infty}\\frac{4}{2^{i}}\\left[q_{1-1/2}-q_{1/2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Then, using the upper bound in Theorem $\\mathrm{J}.1$ , substituting in the bounds for $A_{n}$ and $B_{n}$ , and simplifying, we get the claim. \u53e3 ", "page_idx": 57}, {"type": "text", "text": "Proof of Claim $E.I3$ . By the definition of Wasserstein distance and restrictions of distributions, we have that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}(\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})=\\displaystyle\\int_{a}^{b}\\left|F_{\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}}}(t)-F_{P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}}}(t)\\right|d t}\\\\ &{\\phantom{\\mathcal{W}(\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})=}=\\displaystyle\\int_{q_{\\frac{1}{k}}}^{q_{1-\\frac{1}{k}}}\\left|F_{\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}}}(t)-F_{P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}}}(t)\\right|d t}\\\\ &{\\phantom{\\mathcal{W}(\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})=}=\\displaystyle\\int_{q_{\\frac{1}{k}}}^{q_{1-\\frac{1}{k}}}\\left|F_{\\hat{P}_{n}}(t)-F_{P}(t)\\right|d t\\leq\\mathcal{W}(P,\\hat{P}_{n})}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "J.3 Omitted Proofs in Section E.2 ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Before going into the proofs, we state the standard Chernoff concentration bound that we will use multiple times. ", "page_idx": 57}, {"type": "text", "text": "Theorem J.2 (Binomial Concentration). Let $X\\,\\sim\\,B i n(n,p)$ with expectation $\\mu\\,=\\,n p_{i}$ , and $0<$ $\\delta<1$ . Then, ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(|X-\\mu|\\geq\\delta\\mu)\\leq2e^{\\frac{-\\delta^{2}\\mu}{3}}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Proof of Lemma $E.I6$ . ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\displaystyle\\mathcal{N}(P,P^{D P})=\\int_{t}|F_{P}(t)-F_{P^{D P}}(t)|d t\\eqno(13)}}\\\\ {{\\displaystyle\\leq\\int_{t=a}^{q_{1/k}}|F_{P}(t)-F_{P^{D P}}(t)|d t+\\int_{t=q_{1/k}}^{q_{1-1/k}}|F_{P}(t)-F_{P^{D P}}(t)|d t+\\int_{t=q_{1-1/k}}^{b}|F_{P}(t)-F_{P^{D P}}(t)|d t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Note that for all $t\\in[q_{1/k},q_{1-1/k}]$ , we have that the cumulative distribution functions of $P$ and its restricted version are identical and likewise for $P^{D P}$ . Additionally, the cumulative density functions ", "page_idx": 57}, {"type": "text", "text": "for the restricted versions of the two distributions are identical to each other outside of this interval. Hence, we can simplify the middle term in the RHS of the inequality above as follows: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\int_{t=q_{1/k}}^{q_{1-1/k}(P)}|F_{P}(t)-F_{P^{D P}}(t)|d t=\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},P^{D P}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Next, we reason about the remaining terms. ", "page_idx": 58}, {"type": "text", "text": "Consider the term $\\begin{array}{r}{\\int_{t=a}^{q_{1/k}}|F_{P}(t)-F_{P_{\\textsl{P P}}^{D P}}(t)|d t}\\end{array}$ . First, condition on the event in Theorem I.3 (on the accuracy of the private quantiles for the empirical distribution), which tells us that with probability at least $1-\\beta$ , we have for all $r\\in[k]$ , that ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\hat{q}_{\\frac{2r-1}{2k}-\\frac{1}{4k}}\\leq\\tilde{q}_{\\frac{2r-1}{2k}}\\leq\\hat{q}_{\\frac{2r-1}{2k}+\\frac{1}{4k}},\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "which implies in particular that $\\hat{q}_{1/4k}\\leq\\tilde{q}_{1/2k}\\leq\\hat{q}_{3/4k}$ . ", "page_idx": 58}, {"type": "text", "text": "Next, we argue that $\\hat{q}_{1/4k}\\,\\geq\\,q_{1/8k}$ with high probability. By the definition of quantiles, we have that $\\begin{array}{r}{P r_{y\\sim P}(y\\,<\\,q_{1/8k})\\,<\\,\\frac{1}{8k}}\\end{array}$ . The number of entries in the dataset $\\mathbf{x}$ less than $q_{1/8k}$ is hence a Binomial with mean less than $\\textstyle{\\frac{n}{8k}}$ , and hence, we have by Theorem J.2 (with $\\delta$ set to 0.9) that with probability at least $1-\\beta$ , the number of entries in the dataset less than $q_{1/8k}$ is at most $\\begin{array}{r}{1.9\\frac{n}{8k}<\\frac{n}{4k}}\\end{array}$ , which means the total mass less than $q_{\\frac{1}{8k}}$ in the empirical distribution is less than $\\textstyle{\\frac{1}{4k}}$ . This implies that $\\hat{q}_{1/4k}\\ge q_{1/8k}$ by the definition of quantiles. ", "page_idx": 58}, {"type": "text", "text": "Additionally, note that for all $t\\,<\\,q_{1/k}$ , $\\begin{array}{r}{F_{P}(t)\\,<\\,\\frac{1}{k}}\\end{array}$ . The number of entries in the dataset $\\mathbf{x}$ that are less than $q_{1/k}$ is hence a Binomial with success probability less than $\\textstyle{\\frac{1}{k}}$ . By Theorem J.2, we can again argue that with probability at least $1-\\beta$ , there is a constant $c^{\\prime}$ such that the total mass of the empirical distribution on values less than $q_{1/k}$ is less than $\\frac{c^{\\prime}}{k}$ . Hence, $q_{1/k}\\,\\leq\\,\\hat{q}_{c^{\\prime}/k}$ . This implies by Equation 15, that $q_{1/k}\\le\\tilde{q}_{c/k}$ for some constant $c$ . Hence, for all $t<q_{1/k}$ , we have that $\\begin{array}{r}{F_{P D P}(t)\\le\\frac{c}{k}}\\end{array}$ . ", "page_idx": 58}, {"type": "text", "text": "Hence, taking a union bound, with probability at least $1-O(\\beta)$ , ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{t=a}^{(\\mathfrak{N}_{\\ell})/b}|F_{P}(t)-F_{P^{\\Delta_{P}}}(t)|d t=\\int_{t=a}^{\\mathfrak{N}_{\\ell}/b}|F_{P}(t)-F_{P^{\\Delta_{P}}}(t)|d t+\\int_{\\hat{\\eta}_{1}/b}^{\\mathfrak{N}_{\\ell}/b}|F_{P}(t)-F_{P^{\\Delta_{P}}}(t)|d t}}\\\\ &{\\leq\\int_{t=a}^{\\hat{\\eta}_{1}/b}|F_{P}(t)-F_{P^{\\Delta_{\\ell}}}(t)|d t+\\int_{\\eta_{1}/b}^{\\mathfrak{N}_{\\ell}}|F_{P}(t)-\\frac{c}{b}|d t}\\\\ &{\\leq\\int_{t=a}^{\\hat{\\eta}_{1}/b}|F_{P}(x)-F_{P^{\\Delta_{\\ell}}}(t)|d t+\\int_{\\eta_{1}/b}^{\\mathfrak{N}_{\\ell}}|F_{P}(t)-8c F_{P}(t)|d t}\\\\ &{\\leq(1-8c)\\left[\\int_{t=a}^{\\hat{\\eta}_{1}/b}|F_{P}(t)-F_{P^{\\Delta_{\\ell}}}(t)|d t+\\int_{\\eta_{1}/b}^{\\mathfrak{N}_{\\ell}}|F_{P}(t)|d t\\right]}\\\\ &{\\leq(1-8c)\\left[\\int_{t=a}^{\\hat{\\eta}_{1}/b}|F_{P}(t)-F_{P^{\\Delta_{\\ell}}}(t_{1},\\eta_{1},\\eta_{1},t)|d t+\\int_{\\eta_{1}/b}^{\\mathfrak{N}_{\\ell}/b}|F_{P}(t)|d t\\right]}\\\\ &{\\leq(1-8c)\\left[\\int_{t=a}^{\\hat{\\eta}_{1}/b}|F_{P}(t)-F_{P^{\\Delta_{\\ell}}}(t_{1},\\eta_{1},\\eta_{1},t)|d t+\\int_{\\eta_{1}/b}^{\\mathfrak{N}_{\\ell}/b}|F_{P}(t)-F_{P}|_{0}}\\\\ &{\\leq2(1-8c)\\mathcal{N}(P,P|_{\\ell},\\eta_{1},\\eta_{1},\\eta_{1},t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "By a symmetric argument, we also have that with probability at least $1-O(\\beta)$ , ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\int_{t=q_{1-1/k}}^{b}|F_{P}(t)-F_{P^{D P}}(t)|d t\\leq2(1-8c)\\mathcal{W}(P,P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}}).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Taking a union bound to ensure that all terms in Equation 14 are bounded as required, the proof is complete. \u53e3 ", "page_idx": 58}, {"type": "text", "text": "Proof of Lemma $E.I7$ . First, we condition on the event in Corollary I.3 (on the accuracy of differentially private quantile estimates) that for all $r\\in[k]$ , ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{q}_{\\frac{2r-1}{2k}-\\frac{1}{4k}}\\leq\\tilde{q}_{r}\\leq\\hat{q}_{\\frac{2r-1}{2k}+\\frac{1}{4k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "note that this event happens with probability at least $1-\\beta$ over the randomness of the algorithm. ", "page_idx": 59}, {"type": "text", "text": "Observe that this implies that FDP increases by k1 somewhere in the range [q\u02c6 2r2k\u22121 \u221241k , q\u02c6 2r2k\u22121 + 41k ] (for all $r\\in[k])$ and remains constant outside these intervals. ", "page_idx": 59}, {"type": "text", "text": "Now, we show that for all $t\\in[a,b]$ , we have that $\\begin{array}{r}{|F_{P^{D P}}(t)-F_{\\hat{P}_{n}}(t)|\\leq\\frac{2}{k}}\\end{array}$ . ", "page_idx": 59}, {"type": "text", "text": "If there exists $t\\;\\in\\;[a,\\hat{q}_{\\frac{1}{4k}})$ , we have that $F_{P_{D P}}(t)\\,=\\,0$ , and $\\textstyle F_{\\hat{P}_{n}}(t)\\ \\leq\\ {\\frac{1}{4k}}$ , which implies that $\\begin{array}{r}{|F_{D P}(t)\\!-\\!F_{\\hat{P}_{n}}(t)|\\leq\\frac{1}{4k}}\\end{array}$ . If there exists no such $t$ , then we have that $a=\\hat{q}_{\\frac{1}{4k}}$ , and the corresponding interval collapses to a single point (which will fall in another interval considered below). ", "page_idx": 59}, {"type": "text", "text": "Next, fix any $r\\in[k-1]$ . Note that if there exists $\\begin{array}{r}{t\\,\\in\\,[\\hat{q}_{\\frac{2r-1}{2k}-\\frac{1}{4k}},\\hat{q}_{\\frac{2r+1}{2k}-\\frac{1}{4k}})}\\end{array}$ , we have for all such $t$ that $\\begin{array}{r}{\\frac{r-1}{k}\\,\\le\\,F_{D P}(t)\\,<\\,\\frac{r}{k}}\\end{array}$ , and $\\begin{array}{r}{\\frac{2r-1}{2k}-\\frac{1}{4k}\\leq F_{\\hat{P}_{n}}(t)\\leq\\frac{\\tilde{2r}+1}{2k}+\\frac{1}{4k}}\\end{array}$ . 2kThis4 kimplies that for all such $t$ , $\\begin{array}{r}{|F_{D P}(t)-F_{\\hat{P}_{n}}(t)|\\leq\\frac{2}{k}}\\end{array}$ . If there exists no such $t$ , then we have that $\\hat{q}_{\\frac{2r-1}{2k}-\\frac{1}{4k}}=\\hat{q}_{\\frac{2r+1}{2k}-\\frac{1}{4k}}$ , and this $r$ is not relevant since the corresponding interval collapses to a single point (that is considered in another interval). ", "page_idx": 59}, {"type": "text", "text": "Finally, for $t\\in[\\hat{q}_{\\frac{2k-1}{2k}},b]$ , we have that $F_{P_{D P}}(t)\\ge1-\\frac{1}{k}$ , and $\\begin{array}{r}{F_{\\hat{P}_{n}}(t)\\ge1-\\frac{1}{2k}}\\end{array}$ , so we have that $\\begin{array}{r}{|F_{D P}(t)-F_{\\hat{P}_{n}}(t)|\\leq\\frac{1}{k}}\\end{array}$ . ", "page_idx": 59}, {"type": "text", "text": "Note that every $t\\in[a,b]$ is considered in some interval above and hence we have shown that for all $t\\in[a,b]$ , we have that $\\begin{array}{r}{|\\dot{F}_{P^{D P}}(t)-F_{\\hat{P}_{n}}(t)|\\leq\\frac{2}{k}}\\end{array}$ . ", "page_idx": 59}, {"type": "text", "text": "Finally, using the formula for Wasserstein distance (and the definition of a restriction), we have that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}(\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},P^{D P}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})=\\displaystyle\\int_{a}^{b}\\left|F_{\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}}}(t)-F_{P^{D P}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}}}(t)\\right|d t}\\\\ &{\\phantom{\\mathcal{0}}=\\displaystyle\\int_{q_{\\frac{1}{k}}}^{q_{1-\\frac{1}{k}}}|F_{P}(t)-F_{P^{D P}}(t)|\\,d t}\\\\ &{\\phantom{\\mathcal{0}}\\leq\\displaystyle\\int_{q_{\\frac{1}{k}}}^{q_{1-\\frac{1}{k}}}\\frac{2}{k}d t}\\\\ &{\\phantom{\\mathcal{0}}\\leq\\frac{2}{k}\\left(q_{1-1/k}-q_{1/k}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Before the proof of Claim E.18, we state the following variance-dependent version of the DKW inequality that uniformly bounds the absolute difference in CDFs between the true and empirical distribution. ", "page_idx": 59}, {"type": "text", "text": "Theorem J.3 (See for example Theorem 1.2 in [BM23]). Fix $n>0$ . There are absolute constants c0, c1 such that for all \u2206\u2265 c0 logn log n, ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\operatorname*{sup}_{\\substack{t:F_{P}(t)(1-F_{P}(t))\\geq\\Delta}}\\left|F_{P}(t)-F_{\\hat{P}_{n}}(t)\\right|\\geq\\sqrt{\\Delta\\cdot F(t)(1-F(t)}\\right]\\leq2e^{-c_{1}\\Delta n}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "We also state the following lemma on Binomial random variables, which is a simple consequence of a Lemma by Bobkov and Ledoux [BL19]. ", "page_idx": 59}, {"type": "text", "text": "Lemma J.4 (Lemma 3.8 in [BL19]). Let $\\textstyle S_{n}\\,=\\,\\sum_{i=1}^{n}\\eta_{i}$ be the sum of $n$ independent Bernoulli random variables with $\\mathrm{Pr}[\\eta_{i}=1]\\,=\\,p$ and $\\mathrm{Pr}[\\eta_{i}\\stackrel{.}{=}\\bar{0}]\\;=\\;q\\;=\\;1\\,-\\,p$ (for all $i$ ). Also assume $\\textstyle p\\in[{\\frac{1}{n}},1-{\\frac{1}{n}}]$ . Then, for some sufficiently small constant $c$ , ", "page_idx": 59}, {"type": "equation", "text": "$$\nc\\sqrt{n p q}\\leq\\mathbb{E}[|S_{n}-n p|]\\leq\\sqrt{n p q}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Proof of Claim $E.l8$ . Now, by the formula for Wasserstein distance, the definition of restriction, and Fubini\u2019s theorem, we have that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\mathfrak{L}[\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})]=\\mathbb{E}\\Big[\\int_{q_{\\frac{1}{k}}}^{q_{1-\\frac{1}{k}}}\\Big|F_{P}(t)-F_{\\hat{P}_{n}}(t)\\Big|d t\\Big]=\\int_{q_{\\frac{1}{k}}}^{q_{1-\\frac{1}{k}}}\\mathbb{E}\\Big[\\Big|F_{P}(t)-F_{\\hat{P}_{n}}(t)\\Big|\\Big]d t\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "By Lemma J.4, using the fact that $\\begin{array}{r}{F_{\\hat{P}_{n}}(t)\\,=\\,\\sum_{i=1}^{n}1[x_{i}\\,\\le\\,t]}\\end{array}$ , where each term in the sum is an independent Bernoulli random variable with expectation $F_{P}(t)$ , with $q_{\\frac{1}{k}}\\leq t<q_{1-\\frac{1}{k}}$ (ensuring that the conditions of the lemma are met), we get that $\\begin{array}{r}{\\mathbb{E}\\Big[\\Big|F_{P}(t)-F_{\\hat{P}_{n}}(t)\\Big|\\Big]\\geq c\\sqrt{\\frac{F_{P}(t)[1-F_{P}(t)]}{n}}}\\end{array}$ gives ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})]\\geq c\\int_{q_{\\frac{1}{k}}}^{q_{1-\\frac{1}{k}}}{\\sqrt{\\frac{F_{P}(t)[1-F_{P}(t)]}{n}}d t}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Now, consider the random variable $\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})$ . Note that $\\begin{array}{r}{\\frac{1}{k}\\,\\geq\\,\\frac{c_{3}\\log\\frac{n}{\\beta}}{n}}\\end{array}$ (for an appropriately chosen $c_{3}$ ), and so we are in the regime where we can apply Theorem J.3 for an appropriately chosen $\\Delta$ . ", "page_idx": 60}, {"type": "text", "text": "In particular, we have that for $\\begin{array}{r}{t\\in[q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}),F_{P}(t)\\in[\\frac{1}{k},1-\\frac{1}{k}).}\\end{array}$ ", "page_idx": 60}, {"type": "text", "text": "Setting $\\begin{array}{r}{\\Delta=\\frac{\\log\\frac{n}{\\beta}}{c_{1}n}}\\end{array}$ , we have that $\\Delta\\geq c_{0}{\\frac{\\log\\log n}{n}}$ , and $\\begin{array}{r}{\\Delta\\le\\frac{1}{2k}}\\end{array}$ (the second inequality for sufficiently large $c_{3}$ ). In particular, this implies for $t\\in[q_{\\frac{1}{k}},q_{1-\\frac{1}{k}})$ , $F_{P}(t)\\in[2\\Delta,1-2\\Delta)$ , which implies that $F_{P}(t)(1-F_{P}(t))\\geq\\Delta$ , as long as $\\begin{array}{r}{n>c_{4}\\log{\\frac{n}{\\beta}}}\\end{array}$ for some sufficiently large constant $c_{4}$ . ", "page_idx": 60}, {"type": "text", "text": "Now, using Theorem J.3, we have that with probability at least $1-2e^{-c_{1}\\frac{\\log\\frac{n}{\\beta}}{c_{1}n}n}\\geq1-O(\\beta),$ ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\in[q_{\\frac{1}{k}},q_{1-\\frac{1}{k}})}\\left|F_{P}(t)-F_{\\hat{P}_{n}}(t)\\right|\\leq\\sqrt{\\frac{\\log\\frac{n}{\\beta}}{c_{1}n}}F_{P}(t)(1-F_{P}(t))\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Condition on this for the rest of the proof. Then, we can write the following set of equations. ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})=\\displaystyle\\int_{q_{\\frac{1}{k}}}^{q_{1-\\frac{1}{k}}}|F P(t)-F_{\\hat{P}_{n}}(t)|d t}\\\\ &{\\phantom{\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})}\\leq\\displaystyle\\int_{q_{\\frac{1}{k}}}^{q_{1-\\frac{1}{k}}}\\sqrt{\\frac{\\log\\frac{n}{\\beta}}{c_{1}n}}F_{P}(t)(1-F_{P}(t))d t}\\\\ &{\\phantom{\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})}\\leq\\sqrt{c_{5}\\log\\frac{n}{\\beta}}\\int_{q_{\\frac{1}{k}}}^{q_{1-\\frac{1}{k}}}\\sqrt{\\frac{F_{P}(t)(1-F_{P}(t))}{n}}d t}\\\\ &{\\phantom{\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})}\\leq\\sqrt{c_{6}\\log\\frac{n}{\\beta}}\\mathbb{E}[\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "as required. ", "page_idx": 60}, {"type": "text", "text": "J.4 Local Minimality in the One-Dimensional Setting ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "In this subsection, we argue that the instance-optimal algorithm discussed in Section E.2 is also locally-minimal (See Appendix B.2 for a discussion of local minimality). ", "page_idx": 60}, {"type": "text", "text": "First, we state a corollary of our upper bound for continuous distributions, Theorem E.14. This corollary follows by discretizing the distribution and applying the previous upper bound to the discretized distribution. The parameters of the discretized distribution are related to that of the original distribution via simple coupling arguments. ", "page_idx": 60}, {"type": "text", "text": "Corollary J.5. Fix $\\varepsilon,\\beta\\in(0,1]$ , $a,b\\in\\mathbb{R},$ , $n\\in\\mathbb N$ . Let $P$ be any continuous distribution supported soon $[a,b]$ f.f icCieonntsliyd learr gaen yc $\\gamma\\,<\\,b\\,-\\,a\\,\\in\\,\\mathbb{R}$ t (hseurce he xtihsatts $\\gamma$ n dailvgidoerist $b-a)$ a,t  awnhde lne tg inve n> i ncp2uts $n\\,>\\,c_{2}\\frac{\\log^{4}\\frac{b-a}{\\gamma\\beta\\varepsilon}}{\\varepsilon}$ r, $c_{2}$ $\\mathbf{\\bar{x}}\\sim P^{n}$ privacy parameter \u03b5, interval end points $a,b,$ , granularity $\\gamma_{;}$ , and access to algorithm $A_{q u a n t}$ , outputs a distribution $P^{D P}$ such that with probability at least $1-O(\\beta)$ over the randomness of $\\mathbf{x}$ and the algorithm, ", "page_idx": 60}, {"type": "text", "text": "", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathit{W}(P,P^{D P})=O\\left(\\sqrt{\\log n}\\mathbb{E}\\left[\\mathcal{W}(P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}},\\hat{P}_{n}|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}}\\right]+\\mathcal{W}(P,P|_{q_{\\frac{1}{k}},q_{1-\\frac{1}{k}}})+\\frac{1}{k}\\left(q_{1-1/k}-q_{1/k}\\right)\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "kw h=e r\u2308e $\\hat{P}_{n}$ is \u03b5tnhe unifo\u2309r, mw hdeisrter uitsi oan s uofnf $\\mathbf{x}$ ,e $q_{\\alpha}$ y  rlearpgree sceontnss ttahnet. $\\alpha$ -quantile of distribution $P$ , and 4c3 log3 b\u03b2\u2212\u03b3a log \u03b2n $c_{3}$ ", "page_idx": 61}, {"type": "text", "text": "We state a lemma of Ledoux and Bobkov that we will use in the main proof of this section. ", "page_idx": 61}, {"type": "text", "text": "Lemma J.6 (Lemma 3.8 in [BL19]). Let $\\textstyle S_{n}\\,=\\,\\sum_{i=1}^{n}\\eta_{i}$ be the sum of $n$ independent Bernoulli random variables with $\\mathrm{Pr}[\\eta_{i}=1]\\,=\\,p$ and $\\operatorname*{Pr}[\\eta_{i}=0]\\,=\\,q\\,=\\,1\\,-\\,p$ (for all ). Then, for some sufficiently small constant $c$ , ", "page_idx": 61}, {"type": "equation", "text": "$$\nc\\operatorname*{min}\\{2n p q,\\sqrt{n p q}\\}\\leq\\mathbb{E}[|S_{n}-n p|]\\leq\\operatorname*{min}\\{2n p q,\\sqrt{n p q}\\}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Now, we are ready to state and prove the local minimality result. Note that the statement will reference the rates defined by Equation 1 in the introduction. ", "page_idx": 61}, {"type": "text", "text": "Theorem J.7. Let $a,b\\in\\mathbb{R},\\,\\gamma\\in\\mathbb{R}$ . For any continuous distribution $P$ over $[a,b]$ with a density, let $N(P)\\,=\\,\\{Q\\,:\\,D_{\\infty}(P,Q)\\,\\leq\\,\\log2\\}$ . Fix $\\beta,\\gamma,\\varepsilon\\ \\in\\ (0,1].$ , and let $n\\;=\\;\\Omega\\;\\biggl(\\frac{\\log^{4}\\frac{b-a}{\\gamma\\varepsilon}}{\\varepsilon}\\biggr)$ with $\\begin{array}{r}{n^{\\prime}=\\frac{n}{c_{7}\\log n\\log^{3}\\frac{b-a}{\\gamma\\varepsilon}}}\\end{array}$ for some constant $c_{7}$ . There exists an algorithm $\\boldsymbol{\\mathcal{A}}$ such that for all continuous distributions $P$ , for all algorithms $\\mathcal{A^{\\prime}}$ , there exists a distribution $Q\\in N(P)$ such that ", "page_idx": 61}, {"type": "equation", "text": "$$\nR_{\\mathcal{A},n}(Q)\\leq O(\\mathrm{polylog}\\,n)\\cdot\\operatorname*{max}\\{R_{\\mathcal{A}^{\\prime},\\lceil n^{\\prime}\\rceil}(Q),R_{\\mathcal{A}^{\\prime},\\lfloor n^{\\prime}/4\\rfloor}(Q)\\}+\\gamma,\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Proof. Let k = \u23084c3 log3 b\u03b2\u2212\u03b3a log \u03b2n , and set $\\begin{array}{r}{n^{\\prime}=\\frac{2n}{c_{4}\\log^{3}\\frac{b-a}{\\beta\\gamma}\\log\\frac{n}{\\beta}}}\\end{array}$ for a sufficiently large constant $c_{4}$ . Then, by Corollary J.5 with appropriately chosen $\\beta$ we have that with probability at least 0.95, for any distribution $Q$ (and hence particularly any distribution $Q\\in N(P)$ , ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}(Q,\\mathcal{A}(\\hat{Q}_{n})=O\\bigg(\\frac{1}{\\varepsilon n^{\\prime}}\\left(q_{1-\\frac{2}{C\\varepsilon n^{\\prime}}}(Q)-q_{\\frac{2}{C\\varepsilon n^{\\prime}}}(Q)\\right)+\\mathcal{W}(Q,Q|_{q_{\\frac{2}{C\\varepsilon n^{\\prime}}(Q)},q_{1-\\frac{2}{C\\varepsilon n^{\\prime}}}(Q)})}\\\\ &{\\quad\\quad\\quad\\quad+\\sqrt{\\log n}\\mathbb{E}\\left[\\mathcal{W}\\left(Q|_{q_{\\frac{2}{C\\varepsilon n^{\\prime}}}(Q),q_{1-\\frac{2}{C\\varepsilon n^{\\prime}}}(Q),\\hat{Q}_{n}|_{q_{\\frac{2}{C\\varepsilon n^{\\prime}}}(Q),q_{1-\\frac{2}{C\\varepsilon n^{\\prime}}}(Q)}\\right)\\right]\\bigg)+\\gamma,}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where $C$ is the constant referenced in Theorem E.3. We will show that for distribution $P$ , each of the corresponding distribution-dependent terms is closely related to the terms for $Q$ . ", "page_idx": 61}, {"type": "text", "text": "First, consider $\\begin{array}{r}{\\frac{1}{\\varepsilon n^{\\prime}}\\left(q_{1-\\frac{2}{C\\varepsilon n^{\\prime}}}(Q)-q_{\\frac{2}{C\\varepsilon n^{\\prime}}}(Q)\\right)}\\end{array}$ . Firstly, note that for all $\\alpha~\\in~(0,1)$ , $q_{\\alpha}(P)~\\ge$ $q_{\\alpha/2}(Q)$ , and $q_{\\alpha}(P)\\leq q_{2\\alpha}(Q)$ , since $D_{\\infty}(P,Q)\\leq\\ln2$ , which implies that $\\textstyle{\\frac{1}{2}}F_{Q}(t)\\leq F_{P}(t)\\leq$ $2F_{Q}(t)$ for all $t~\\in~\\mathbb{R}$ . Similarly, note that for all $\\alpha~\\in~(0,1)$ , $q_{1-\\alpha}(P)\\ \\geq\\ q_{1-2\\alpha}(Q)$ , and $q_{1-\\alpha}(P)\\leq q_{1-\\frac{1}{2}\\cdot\\alpha}(Q)$ . Hence, we have that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\frac{1}{\\varepsilon n^{\\prime}}\\left(q_{1-\\frac{2}{C\\varepsilon n^{\\prime}}}(Q)-q_{\\frac{2}{C\\varepsilon n^{\\prime}}}(Q)\\right)\\leq\\frac{1}{\\varepsilon n^{\\prime}}\\left(q_{1-\\frac{1}{C\\varepsilon n^{\\prime}}}(P)-q_{\\frac{1}{C\\varepsilon n^{\\prime}}}(P)\\right)\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Next, consider $\\mathcal{W}(P,P|_{q_{\\frac{1}{C\\varepsilon n}}(P),q_{1-\\frac{1}{C\\varepsilon n}}(P)})$ . Recall that $\\begin{array}{r}{q_{\\frac{1}{C\\varepsilon n}}(P)\\leq q_{\\frac{2}{C\\varepsilon n}}(Q)}\\end{array}$ , and $q_{1-\\frac{1}{C\\varepsilon n}}(P)\\geq$ $\\scriptstyle q_{1-{\\frac{2}{C\\varepsilon n}}}(Q)$ . Then, (noting that $L(P)=L(Q)$ and $q_{1}(P)=q_{1}(Q))$ , we have that ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{W}(Q,Q|_{q_{\\frac{2}{C\\epsilon n^{\\prime}}}(Q),q_{1_{-\\frac{2}{C\\epsilon n^{\\prime}}}}(Q)})=\\int_{L(Q)}^{q_{\\frac{2}{C\\epsilon n^{\\prime}}}(Q)}F_{Q}(t)d t+\\int_{q_{1_{-\\frac{2}{C\\epsilon n^{\\prime}}}(Q)}}^{q_{1}(Q)}|1-F_{Q}(t)|d t}\\\\ {\\leq2\\int_{L(Q)}^{q_{\\frac{2}{C\\epsilon n^{\\prime}}}(Q)}F_{P}(t)d t+2\\int_{q_{1_{-\\frac{1}{C\\epsilon n^{\\prime}}}(Q)}}^{q_{1}(Q)}|1-F_{P}(t)|d t}\\\\ {\\leq2\\int_{L(P)}^{q_{\\frac{4}{C\\epsilon n^{\\prime}}}(P)}F_{P}(t)d t+2\\int_{q_{1_{-\\frac{1}{4C\\epsilon n^{\\prime}}}(P)}}^{q_{1}(P)}|1-F_{P}(t)|d t}\\\\ {=2\\mathcal{W}(P,P|_{q_{\\frac{4}{C\\epsilon n^{\\prime}}}(P),q_{1_{-\\frac{4}{C\\epsilon n^{\\prime}}}(P)}}(P))}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Finally, consider $\\begin{array}{r}{\\frac{1}{\\sqrt{\\log n}}\\mathbb E\\left[\\mathcal W(P|_{q_{\\frac{1}{C\\varepsilon n}(P)},q_{1-\\frac{1}{C\\varepsilon n}}(P)},\\hat{P}_{n}|_{q_{\\frac{1}{C\\varepsilon n}(P)},q_{1-\\frac{1}{C\\varepsilon n}}(P)})\\right]}\\end{array}$ and applying both inequalities in Lemma J.6, we have that ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\langle\\mathbf{W}(q_{1},\\mathbf{r}_{1},\\cdots,q_{n},\\theta(\\cdot),\\cdot)\\rangle_{\\mathbb{R}_{n}^{n}}\\right]}\\\\ &{=\\int_{\\mathbb{R}_{n}^{n}\\times\\mathbb{R}_{n}^{n}}[\\langle\\mathbf{hat{z}}_{1},\\mathbf{r}_{2},\\hat{\\mathbf{z}}_{2}^{\\top}\\rangle]\\langle\\mathbf{f}_{n}|\\cdot\\hat{\\mathbf{z}}_{1}^{\\top}\\rangle\\langle\\mathbf{f}_{n}|\\cdot\\hat{\\mathbf{z}}_{2}^{\\top}\\rangle}\\\\ &{\\le\\int_{\\mathbb{R}_{n}^{n}\\times\\mathbb{R}_{n}^{n}}[\\langle\\mathbf{hat{z}}_{1}^{\\top}\\rangle_{\\mathbb{R}_{n}^{n}}[\\langle\\mathbf{1}-F_{q_{1}}(\\cdot)|\\mathbf{hat{z}}_{1}^{\\top}\\rangle^{\\mathbb{A}}}\\\\ &{\\le\\int_{\\mathbb{R}_{n}^{n}\\times\\mathbb{R}_{n}^{n}}[\\langle\\mathbf{hat{z}}_{1}^{\\top}\\rangle_{\\mathbb{R}_{n}^{n}}\\langle\\mathbf{1}-F_{q_{1}}(\\cdot)|\\mathbf{\\hat{z}}_{1}^{\\top}\\rangle\\langle\\mathbf{1}-F_{q_{1}}(\\cdot)|\\mathbf{\\hat{z}}_{1}^{\\top}\\rangle]\\rangle_{\\mathbb{R}_{n}^{n}}}\\\\ &{\\le\\int_{\\mathbb{R}_{n}^{n}\\times\\mathbb{R}_{n}^{n}}[\\langle\\mathbf{hat{z}}_{1}^{\\top}\\rangle_{\\mathbb{R}_{n}^{n}}\\langle\\mathbf{z}_{1}^{\\top}\\rangle\\langle\\mathbf{1}-F_{q_{1}}(\\cdot)|\\mathbf{\\hat{z}}_{1}^{\\top}\\rangle\\langle\\mathbf{1}-F_{q_{1}}(\\cdot)|\\mathbf{\\hat{z}}_{1}^{\\top}\\rangle]\\rangle_{\\mathbb{R}_{n}^{n}}}\\\\ &{\\le\\int_{\\mathbb{R}_{n}^{n}\\times\\mathbb{R}_{n}^{n}}[\\langle\\mathbf{x}_{1}^{\\top}\\rangle_{\\mathbb{R}_{n}^{n}}\\langle\\mathbf{r}_{1}^{\\top}\\rangle\\langle\\mathbf{1}-F_{q}(\\cdot)|\\mathbf{\\hat{z}}_{1}^{\\top}\\rangle\\langle\\mathbf{\\hat{z}}_{1}^\n$$$\\lceil n^{\\prime}\\rceil\\leq n$ ", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where $c_{5}$ is a sufficiently large constant and the fourth inequality holds since . ", "page_idx": 62}, {"type": "text", "text": "By the above observations connecting the distribution-dependent terms with the corresponding terms for $P$ , we have that for all $Q$ , with probability at least 0.95, ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V(Q,A(\\hat{Q}_{n})=O\\bigg(\\frac{1}{\\varepsilon n^{\\prime}}\\left(q_{1-\\frac{1}{C\\varepsilon n^{\\prime}}}(P)-q_{\\frac{1}{C\\varepsilon n^{\\prime}}}(P)\\right)+\\mathcal{W}(P,P|_{q_{\\frac{4}{C\\varepsilon n^{\\prime}}}(P)},q_{1-\\frac{4}{C\\varepsilon n^{\\prime}}}(P))}\\\\ &{\\qquad\\qquad+\\sqrt{\\log n}\\mathbb{E}\\left[\\mathcal{W}\\left(P|_{q_{\\frac{1}{C\\varepsilon n^{\\prime}}}(P),q_{1-\\frac{1}{C\\varepsilon n^{\\prime}}}(P)},\\hat{P}_{7\\pi^{\\prime}}|_{q_{\\frac{1}{C\\varepsilon n^{\\prime}}}(P),q_{1-\\frac{1}{C\\varepsilon n^{\\prime}}}(P)}\\right)\\right]\\bigg)+\\gamma}\\\\ &{\\qquad\\qquad=O(\\log n)\\left(\\frac{1}{\\varepsilon\\left[n^{\\prime}\\right]}\\left(q_{1-\\frac{1}{C\\varepsilon|n^{\\prime}|}}(P)-q_{\\frac{1}{C\\varepsilon|n^{\\prime}|}}(P)\\right)+\\mathcal{W}(P,P|_{q_{\\frac{1}{C\\varepsilon n^{\\prime}/4}}(P)},q_{1-\\frac{1}{C\\varepsilon|n^{\\prime}|}}(P))\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.(20)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\times O\\left(\\mathcal{W}\\left(P|_{q_{\\frac{1}{C\\varepsilon|n^{\\prime}|}}(P),q_{1-\\frac{1}{C\\varepsilon|n^{\\prime}|}}(P)},\\hat{P}_{[n^{\\prime}]}|_{q_{\\frac{1}{C\\varepsilon n^{\\prime}|}}(P),q_{1-\\frac{1}{C\\varepsilon|n^{\\prime}|}}(P)}\\right)\\right]\\bigg)+\\gamma}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Now, we proceed with the analysis in two cases. Firstly, consider the case when the first and third terms inside the bracket on the RHS of equation 20 are larger than the second term inside the bracket. Then, we have that for all $Q$ , with probability at least 0.95, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}(Q,\\boldsymbol{A}(\\hat{Q}_{n})=O(\\log n)\\left(\\frac{1}{\\varepsilon\\lceil n^{\\prime}\\rceil}\\left(q_{1-\\frac{1}{C\\varepsilon\\lceil n^{\\prime}\\rceil}}(P)-q_{\\frac{1}{C\\varepsilon\\lceil n^{\\prime}\\rceil}}(P)\\right)\\right.}\\\\ &{\\qquad\\qquad\\left.+\\left.\\frac{1}{\\sqrt{\\log n}}\\mathbb E\\left[\\mathcal{W}\\left(P|_{q_{\\frac{1}{C\\varepsilon\\lceil n^{\\prime}\\rceil}}(P),q_{1-\\frac{1}{C\\varepsilon\\lceil n^{\\prime}\\rceil}}(P)},\\hat{P}_{\\lceil n^{\\prime}\\rceil}|_{q_{\\frac{1}{C\\varepsilon\\lceil n^{\\prime}\\rceil}}(P),q_{1-\\frac{1}{C\\varepsilon\\lceil n^{\\prime}\\rceil}}(P)}\\right)\\right]\\right)+\\gamma}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "By Theorem E.3 and the fact that $n^{\\prime}<n$ , for all algorithms $\\mathcal{A^{\\prime}}$ , there exists a distribution $Q\\in N(P)$ such that , ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{R_{Q}}(A^{\\prime},\\lceil n^{\\prime}\\rceil)=\\Omega\\Bigg(\\frac{1}{\\varepsilon\\lceil n^{\\prime}\\rceil}\\left({q_{1-\\frac{1}{C\\varepsilon\\lceil n^{\\prime}\\rceil}}(P)-q_{\\frac{1}{C\\varepsilon\\lceil n^{\\prime}\\rceil}}(P)}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\frac{1}{\\sqrt{\\log n}}{\\mathbb{E}}\\left[\\mathcal{W}\\left(P|_{q\\frac{1}{C\\varepsilon\\lceil n^{\\prime}\\rceil}(P),q_{1-\\frac{1}{C\\varepsilon\\lceil n^{\\prime}\\rceil}}(P)},\\hat{P}_{\\lceil n^{\\prime}\\rceil}|_{q\\frac{1}{C\\varepsilon\\lceil n^{\\prime}\\rceil}(P),q_{1-\\frac{1}{C\\varepsilon\\lceil n^{\\prime}\\rceil}}(P)}\\right)\\right]\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Hence, for all algorithms $A^{\\prime}$ and the corresponding distribution $Q$ , with probability at least 0.95, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathcal{W}(Q,\\mathcal{A}(\\hat{Q}_{n})\\leq O(\\log n)R_{Q}(A^{\\prime},\\lceil n^{\\prime}\\rceil)+\\gamma.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Next, consider the case where the first and third terms inside the bracket on the RHS of equation 20 are smaller than the second term inside the bracket. Then, we have that for all $Q$ , with probability at least 0.95, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}(Q,\\mathcal{A}(\\hat{Q}_{n})=O(\\log n)\\mathcal{W}(P,P|_{q_{\\frac{1}{C\\varepsilon\\{n^{\\prime}/4\\}}(P)},q_{1-\\frac{1}{C\\varepsilon\\{n^{\\prime}/4\\}}}(P)})+\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "By Theorem E.3, for all algorithms $\\mathcal{A^{\\prime}}$ , there exists a distribution $Q\\in N(P)$ such that ", "page_idx": 63}, {"type": "equation", "text": "$$\nR_{Q}(\\mathcal{A}^{\\prime},\\lfloor n^{\\prime}/4\\rfloor)=\\Omega\\left(\\mathcal{W}(P,P|_{q_{\\frac{1}{C\\varepsilon\\lfloor n^{\\prime}/4\\rfloor}(P)},q_{1-\\frac{1}{C\\varepsilon\\lfloor n^{\\prime}/4\\rfloor}(P)}})\\right).\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Hence, we have that for all algorithms $A^{\\prime}$ and for the corresponding distribution $Q$ , with probability at least 0.95, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}(Q,\\mathcal{A}(\\hat{Q}_{n})=O(\\log n)R_{Q}(\\mathcal{A}^{\\prime},\\lfloor n^{\\prime}/4\\rfloor)+\\gamma,}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "as required. This completes the proof. ", "page_idx": 63}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: [TODO]The abstract and introduction clearly explain the main claims of the paper; the informal theorems provided in \u2018Our Results\u2019 section of the introduction make these explicit. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 64}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Justification: [TODO]We explain where our bounds are suboptimal, for example in the case of high dimensional distributions over $\\mathbb{R}^{d}$ , we explain how our bounds improve over previous work but still involve significant overhead. As we indicate in both our theorem statements and the discussion in the introduction, we achieve instance optimality upto polylogarithmic factors, and getting rid of these is an open question we leave for future work.) ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 64}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: [TODO]The informal theorems provided in the introduction give simplified versions of our main results (and explain the high level techniques used to obtain them). In the supplementary material, we discuss these results in more generality and give complete proofs of these results. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 65}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: [TODO]Experimental contributions are not a main focus of this work since they have been extensively addressed in previous work, as discussed in the introduction and related work sections. For the experiment we do, we give a detailed description of the distribution used, the methods we compare to and appropriate parameters (along with citations)- the experiment can be reproduced with this information. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 65}, {"type": "text", "text": "", "page_idx": 66}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 66}, {"type": "text", "text": "Answer: [No] ", "page_idx": 66}, {"type": "text", "text": "Justification: [TODO]Our experiment is not a main contribution of our work (we are focused on theoretical bounds) and simply shows how different instance optimal and worstcase bounds can be. We provide the distribution we use precisely, which corresponds to the exact data we use. We also explain the method and hyperparameters we use, but don\u2019t release the code- however, the complete descriptions of the algorithms are publicly available in previous work. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 66}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 66}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 66}, {"type": "text", "text": "Justification: [TODO]As described in the experiment details section in the appendix, we give all the details necessary to understand the experiment and its results. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 66}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 67}, {"type": "text", "text": "Answer: [No] ", "page_idx": 67}, {"type": "text", "text": "Justification: [TODO] The experiment is not the focus of our paper (rather, we are focused on theoretical analysis) and is just used to demonstrate one drawback of worst case bounds (which we also discuss theoretically) - hence, we don\u2019t comment on statistical significance in detail. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 67}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 67}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 67}, {"type": "text", "text": "Justification: [TODO]We do not explicitly address this since the experiment can be performed on essentially any laptop since it is simple (both time and memory efficient) and does not require any significant compute. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 67}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 67}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 67}, {"type": "text", "text": "Justification: [TODO]We have reviewed the code of ethics in detail- both societal impact guidelines and impact-mitigation measures and are confident that our paper conforms to the code of ethics in every respect. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 68}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 68}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 68}, {"type": "text", "text": "Justification: [TODO]Our paper is motivated by theoretically explaining the performance of differentially private algorithms. As touched upon in the introduction, differential privacy is important because estimation is frequently done on sensitive data and so developing methods to better understand the privacy-utility tradeoff is societally valuable. We don\u2019t anticipate any negative societal effects from this work. ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 68}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 68}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 68}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 68}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 69}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 69}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 69}, {"type": "text", "text": "Justification: [TODO]In the experiment conducted, we cite the subroutine we use in another paper, as well as the method from another paper we compare to. We however implement them ourselves and so official licenses are not needed. ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 69}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 69}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 69}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 69}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 69}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 69}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 69}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 70}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 70}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 70}, {"type": "text", "text": "Justification: [TODO] Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 70}]