{"references": [{"fullname_first_author": "Robert A. Jacobs", "paper_title": "Adaptive mixtures of local experts", "publication_date": "1991-01-01", "reason": "This paper introduced the foundational concept of Mixture of Experts (MoE), a crucial architecture for the current work."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-01-01", "reason": "This work pioneered the application of MoE in deep learning, introducing the sparsely-gated MoE layer, which is a key antecedent to this paper's approach."}, {"fullname_first_author": "William Fedus", "paper_title": "Switch Transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-01-01", "reason": "This highly influential paper introduced Switch Transformers, a significant advancement in scaling language models that directly relates to the current research on efficient model scaling."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-01", "reason": "This paper established crucial scaling laws for neural language models, providing a theoretical foundation for understanding and improving model efficiency at scale, a central theme of this research."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This seminal paper introduced the Transformer architecture, the basis of many modern large language models and the underlying architecture upon which this paper's MoT model is built."}]}