[{"figure_path": "Osh7u2E1kC/figures/figures_0_1.jpg", "caption": "Figure 1: Comparison of traditional URL methods using a single world model (left) versus our separated world model (right).", "description": "The figure compares traditional unsupervised reinforcement learning (URL) methods with a single world model to the proposed SeeX method with a separated world model.  In the traditional approach (left), a single model encodes both task-relevant and task-irrelevant information in the latent space. This can lead to inflated uncertainty estimates due to irrelevant information. In SeeX (right), the world model distinctly separates task-relevant and task-irrelevant information. This allows for more accurate uncertainty estimation, which is crucial for effective exploration in visually distracted environments.", "section": "1 Introduction"}, {"figure_path": "Osh7u2E1kC/figures/figures_4_1.jpg", "caption": "Figure 2: (a) Our separated world model comprises task-relevant and task-irrelevant branches, with ensemble predictive heads providing reward signals. (b) Imaginary trajectories within the endogenous branch enhance sample efficiency by reducing real-world interactions.", "description": "This figure shows the architecture of the proposed Separation-Assisted Explorer (SeeX) model.  (a) illustrates the inner optimization process, where the world model is trained to separate task-relevant (endogenous) and task-irrelevant (exogenous) information.  The model uses separate encoders and transition dynamics for each, with multiple predictive heads providing reward signals for uncertainty estimation. (b) shows the outer optimization process, which uses imaginary trajectories generated within the endogenous state space to train a policy that maximizes task-relevant uncertainty, improving sample efficiency by reducing reliance on real-world interactions.", "section": "4 Practical Implementation of SeeX"}, {"figure_path": "Osh7u2E1kC/figures/figures_5_1.jpg", "caption": "Figure 3: We show fine-tuning (FT) performance curves of SeeX and baselines across two domains and eight tasks. Pre-training (PT) used 2M frames, FT 100K. Normalized returns are benchmarked against the expert baseline [33], with mean (solid line) and variance (shaded area).", "description": "This figure displays the fine-tuning performance results for SeeX and several baseline methods across two domains (Walker and Quadruped) and eight different tasks.  The pre-training phase utilized 2 million frames, while the fine-tuning phase used 100,000 frames.  The normalized return for each method is plotted against the number of frames used in fine-tuning. Shaded areas represent the variance of the performance, and the solid lines show the mean.  The results are benchmarked against an expert baseline, showing SeeX's competitive performance across various tasks.", "section": "5 Experiments"}, {"figure_path": "Osh7u2E1kC/figures/figures_6_1.jpg", "caption": "Figure 3: We show fine-tuning (FT) performance curves of SeeX and baselines across two domains and eight tasks. Pre-training (PT) used 2M frames, FT 100K. Normalized returns are benchmarked against the expert baseline [33], with mean (solid line) and variance (shaded area).", "description": "This figure displays the fine-tuning performance of SeeX and various baseline methods across two domains (Walker and Quadruped) and their respective eight tasks.  The pre-training phase used 2 million frames, while the fine-tuning phase used 100,000 frames.  The normalized return for each task is shown, benchmarked against an expert baseline. Solid lines represent the mean performance, while shaded areas indicate the variance.", "section": "5.1 Evaluation on URLB with Distractors"}, {"figure_path": "Osh7u2E1kC/figures/figures_7_1.jpg", "caption": "Figure 5: Ablation results of SeeX with 500k fixed pretraining frames: (a) separation design, Exo-Rec term, and predictive head values; (b) different policy design: \u03c0(s+) (SeeX) and \u03c0(s+, s\u00af) (SeeX-both-branch); (c) impact of different \u03b1 (Exo-Rec weight) across three domains. Due to Jaco\u2019s task complexity, we ran 50 trials per seed and reported the top 30 mean returns.", "description": "This figure shows the ablation study on SeeX model. The (a) part shows the ablation study of different modules by removing either separated world model, exo-rec, or reducing the number of predictive heads. The (b) part shows ablation study of different policy design by comparing \u03c0(s+) and \u03c0(s+, s\u00af). The (c) part shows ablation study of \u03b1 (weight of Exo-Rec term) and its impact on three different environments.", "section": "4 Practical Implementation of SeeX"}, {"figure_path": "Osh7u2E1kC/figures/figures_15_1.jpg", "caption": "Figure 7: We present visualization results for the three domains we employed, following the introduction of driving car distractors. Notably, we retain the floor for Walker while removing the floor for Quadruped. This is due to the fact that Quadruped's floor occupies the entire observation space, precluding the addition of moving distractions without floor removal.", "description": "This figure shows example frames from the three simulated environments used in the paper: Walker, Quadruped, and Jaco. Each environment has driving-car distractors added in the background.  The caption notes a key difference in how distractors were added to the Quadruped environment versus the Walker environment due to the floor being a significant part of the Quadruped's observation space.", "section": "A Environment"}, {"figure_path": "Osh7u2E1kC/figures/figures_15_2.jpg", "caption": "Figure 8: To facilitate a clear understanding of the differences between the Driving-car and Random-video datasets, we present visualizations of both datasets. Additionally, we showcase the DMC observations of various distractor types. A detailed description of the datasets employed can be found in Section 5.", "description": "This figure shows examples of videos from the Driving-car dataset, the Random-video dataset, and also the DMC (DeepMind Control Suite) environments with those videos as distractors.  The Driving-car dataset shows driving-related scenes. The Random-video dataset contains more general scenes. The DMC examples show simulated environments with these video distractors added as backgrounds. The figure highlights the difference in visual characteristics between these datasets, illustrating the visual diversity used in experiments to test model robustness to distraction.", "section": "A Environment"}, {"figure_path": "Osh7u2E1kC/figures/figures_17_1.jpg", "caption": "Figure 9: Presentation of reconstruction results from the observation model and endo-decoder. The first row shows the original observations, while  represents the output of the observation model and  represents the output of the endo-decoder. Each corresponding index pair indicates a one-to-one relationship between the original observation and the reconstructed observation.", "description": "This figure visualizes the reconstruction results from the observation model and endo-decoder of the SeeX model. The top row displays the original observations, while the bottom row shows the reconstructions.  Each pair of corresponding images (one original, one reconstructed) demonstrates the model's ability to reconstruct the observations. This visualization helps to understand how well the model captures the relevant information from the observations, separating out task-relevant features from distracting information.", "section": "C.1 Visualization of observational trajectories"}, {"figure_path": "Osh7u2E1kC/figures/figures_17_2.jpg", "caption": "Figure 10: We present the following: the original observation o, images reconstructed with both the endogenous and exogenous branches \u00f4, images reconstructed using only the endogenous branch \u00f4\u207a, images reconstructed using only the exogenous branch \u00f4\u00af, and images with masking applied.", "description": "This figure visualizes the results of the separated world model in SeeX. It shows the original observation, the reconstruction using both endogenous and exogenous information, the reconstruction using only endogenous information, the reconstruction using only exogenous information, and the mask used in the reconstruction process.  The purpose is to demonstrate SeeX's ability to separate task-relevant (endogenous) and task-irrelevant (exogenous) information from the observations, allowing it to make decisions without being influenced by irrelevant factors. The left-hand side shows results from the Walker environment while the right-hand side shows results from the driving-car environment.", "section": "4 Practical Implementation of SeeX"}, {"figure_path": "Osh7u2E1kC/figures/figures_18_1.jpg", "caption": "Figure 3: We show fine-tuning (FT) performance curves of SeeX and baselines across two domains and eight tasks. Pre-training (PT) used 2M frames, FT 100K. Normalized returns are benchmarked against the expert baseline [33], with mean (solid line) and variance (shaded area).", "description": "This figure compares the fine-tuning performance of SeeX and several baseline algorithms across various locomotion and manipulation tasks.  The results are shown as curves illustrating the normalized return (performance) over the fine-tuning phase (100,000 frames), following a pre-training phase of 2 million frames.  Shaded areas represent the variance, providing a measure of uncertainty in the results. The performance of each algorithm is benchmarked against an expert baseline.", "section": "5.1 Evaluation on URLB with Distractors"}, {"figure_path": "Osh7u2E1kC/figures/figures_19_1.jpg", "caption": "Figure 3: We show fine-tuning (FT) performance curves of SeeX and baselines across two domains and eight tasks. Pre-training (PT) used 2M frames, FT 100K. Normalized returns are benchmarked against the expert baseline [33], with mean (solid line) and variance (shaded area).", "description": "This figure presents the fine-tuning performance results of SeeX and several baseline methods across two domains (Walker and Quadruped) and eight tasks.  The pre-training phase used 2 million frames, and the fine-tuning phase used 100,000 frames.  The performance is normalized and shown relative to an expert baseline. The solid lines represent the mean performance, while the shaded areas represent the variance across multiple trials.", "section": "5.1 Evaluation on URLB with Distractors"}, {"figure_path": "Osh7u2E1kC/figures/figures_19_2.jpg", "caption": "Figure 3: We show fine-tuning (FT) performance curves of SeeX and baselines across two domains and eight tasks. Pre-training (PT) used 2M frames, FT 100K. Normalized returns are benchmarked against the expert baseline [33], with mean (solid line) and variance (shaded area).", "description": "This figure compares the fine-tuning performance of SeeX against several baseline methods across eight different tasks in two domains (walker and quadruped).  The x-axis represents the number of frames used in the fine-tuning phase (100,000 frames), and the y-axis shows the normalized return, which is a measure of performance relative to an expert agent. The solid line indicates the average performance, and the shaded area represents the variance across multiple runs.  The pre-training phase used 2 million frames for all methods.", "section": "5.1 Evaluation on URLB with Distractors"}, {"figure_path": "Osh7u2E1kC/figures/figures_20_1.jpg", "caption": "Figure 3: We show fine-tuning (FT) performance curves of SeeX and baselines across two domains and eight tasks. Pre-training (PT) used 2M frames, FT 100K. Normalized returns are benchmarked against the expert baseline [33], with mean (solid line) and variance (shaded area).", "description": "This figure displays the fine-tuning performance curves for SeeX and seven baseline methods across two domains (Walker and Quadruped) and eight tasks.  The pre-training phase used 2 million frames, while the fine-tuning phase used 100,000 frames.  The normalized returns are benchmarked against an expert baseline, showing SeeX's superior performance and lower variance, indicating higher consistency across multiple trials.", "section": "5.1 Evaluation on URLB with Distractors"}]