[{"figure_path": "UddVRqTrjt/figures/figures_1_1.jpg", "caption": "Figure 1: Hierarchical decomposition of the minimum-MSE predictor into prototypes in the task of mouth inpainting. The predicted tree explores the different options of bigger/smaller lips, mouth opening/closing, round/square jawline, etc.", "description": "This figure shows a hierarchical decomposition of the minimum mean squared error (MMSE) predictor into prototypes for the task of mouth inpainting. The model predicts a tree structure where each node represents a cluster of plausible solutions, and each branch represents the probability of transitioning between clusters. The tree explores various options for mouth appearance, including lip size, mouth shape, and jawline shape, providing a hierarchical and probabilistic representation of the uncertainty in the inpainting task.", "section": "1 Introduction"}, {"figure_path": "UddVRqTrjt/figures/figures_3_1.jpg", "caption": "Figure 2: Method overview. Our model T(y; \u03b8) receives a degraded image y and predicts {xk1,...,kak1,...,kd=1, the bottom Kd leaves, and their probabilities {\u03b1k1,...,kak1,...,kd=1 (faint blue box; illustrated here for K = 3 and d = 2). Next, the tree is iteratively constructed from the bottom up using weighted averaging, until we reach the root node which is the minimum MSE predictor xMMSE. During training, starting from the root, the ground truth x is propagated through the tree until it reaches the leaves (dashed red lines). At tree level d, x is compared to its immediate K children nodes, and the MSE loss to the nearest child is added to the loss trajectory.", "description": "This figure illustrates the method's architecture and training process.  The model takes a degraded input image (y) and produces Kd leaf predictions (bottom-level nodes of the tree) and their associated probabilities. It iteratively builds a tree from the bottom-up, combining predictions at each level based on weighted averaging. During training, it starts from the root node (the minimum MSE prediction) and propagates the ground truth (x) down the tree, comparing it to its immediate children and accumulating a loss based on the MSE.", "section": "3 Method"}, {"figure_path": "UddVRqTrjt/figures/figures_5_1.jpg", "caption": "Figure 3: 2D Gaussian mixture denoising. (a) Underlying signal prior px(x) (blue heatmap), and training samples (xi, Yi) ~ Px,y(x, y). (b) K-means with K = 4 applied to 10K samples Xi ~Pxy(xYt), for a given test point yt (red circle). The resulting cluster centers (blue markers) partition the underlying posterior px|y (xyt) (red heatmap), resulting in cluster probabilities p(yt). (c) Hierarchical K-means applied twice with K = 2 on 10K samples xi ~ Px|y(X|Yt). At depth d = 1, the posterior is partitioned by the dashed blue line (blue triangles mark cluster centers). The resulting half spaces are subsequently halved by the dashed orange and green lines respectively. (d) Posterior trees (ours) with degree K = 2 and depth d = 2. Note that in all cases the estimated posterior mean (x|yt) (black star) coincides with the analytical mean \u03bc(x|yt) (red star), while in (c)-(d) the lowest density mode is better represented. T(yt)/p(yt) are drawn at the bottom of (b)-(d).", "description": "This figure shows a comparison of different methods for visualizing the posterior distribution in a 2D Gaussian mixture denoising task.  It compares standard K-means, hierarchical K-means, and the proposed 'Posterior Trees' method.  The figure illustrates how each method partitions the posterior distribution and estimates cluster probabilities, highlighting the advantages of the hierarchical approach in representing multi-modal posteriors.", "section": "Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_7_1.jpg", "caption": "Figure 4: Diverse applications of posterior trees. The predicted trees represent inherent task uncertainty: e.g., (a) Refining the mean estimate by color, grouping similar colors, while still depicting unlikely ones (e.g., the blue boot); (b) Presenting various plausible colorizations varying by hat color, skin tone, and background; and (c) Exploring the diverse options of eyebrows/eyeglasses.", "description": "This figure demonstrates the application of posterior trees to three different image restoration tasks: Edges to Shoes, Face Colorization, and Eyes Inpainting.  Each row shows a different task, with the input image on the left, the minimum mean squared error (MMSE) estimate in the center, and a tree representing the hierarchical decomposition of the posterior distribution on the right. The leaves of the tree show diverse and plausible solutions. This demonstrates the ability of the method to handle different kinds of uncertainty in diverse tasks.", "section": "4 Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_8_1.jpg", "caption": "Figure 5: Bioimage translation. Here we explored posterior trees for the task of translating the image of a tissue from one fluorescent dye to another. The resulting trees expose important information regarding uncertain cells (yellow/red arrows), e.g., ones that do not consistently appear in all branches, and additionally explore different plausible cellular morphology consistent with the input.", "description": "This figure shows an example of applying the Posterior Trees method to a bioimage translation task. The input is a microscopy image of cells stained with one fluorescent dye, and the goal is to predict what the image would look like if stained with a different fluorescent dye. The figure shows the results of the prediction, where the tree structure represents different possible interpretations of the image, with the probabilities of each interpretation shown on the branches. The red and yellow arrows highlight cells where the uncertainty is particularly high. The results demonstrate the ability of the method to quantify and communicate uncertainty, and to explore the space of plausible solutions in a visually intuitive way.", "section": "Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_16_1.jpg", "caption": "Figure 1: Hierarchical decomposition of the minimum-MSE predictor into prototypes in the task of mouth inpainting. The predicted tree explores the different options of bigger/smaller lips, mouth opening/closing, round/square jawline, etc.", "description": "This figure shows a hierarchical decomposition of the minimum mean squared error (MSE) predictor for mouth inpainting.  The model predicts a tree structure, where each branch represents a plausible solution (e.g., different lip sizes or shapes) with an associated probability.  This tree visualization helps users explore the uncertainty associated with the prediction, offering various plausible options instead of a single reconstruction.", "section": "1 Introduction"}, {"figure_path": "UddVRqTrjt/figures/figures_16_2.jpg", "caption": "Figure 2: Method overview. Our model T(y; \u03b8) receives a degraded image y and predicts {xk1,...,kak1,...,ka=1, the bottom Kd leaves, and their probabilities {\u03b1k1,...,ka}k1,...,ka=1 (faint blue box; illustrated here for K = 3 and d = 2). Next, the tree is iteratively constructed from the bottom up using weighted averaging, until we reach the root node which is the minimum MSE predictor xMMSE. During training, starting from the root, the ground truth x is propagated through the tree until it reaches the leaves (dashed red lines). At tree level d, x is compared to its immediate K children nodes, and the MSE loss to the nearest child is added to the loss trajectory.", "description": "This figure shows the overall method used in the paper. The model receives a degraded image as input and predicts Kd leaves and their probabilities. Then it constructs a tree from bottom up using weighted averaging until it reaches the root node which is the minimum MSE predictor. During training, the ground truth is propagated through the tree to reach the leaves and the MSE loss is compared with its children nodes.", "section": "3 Method"}, {"figure_path": "UddVRqTrjt/figures/figures_18_1.jpg", "caption": "Figure 3: 2D Gaussian mixture denoising. (a) Underlying signal prior px(x) (blue heatmap), and training samples (xi, Yi) ~ Px,y(x, y). (b) K-means with K = 4 applied to 10K samples xi ~Pxy(xYt), for a given test point yt (red circle). The resulting cluster centers (blue markers) partition the underlying posterior px|y (xyt) (red heatmap), resulting in cluster probabilities p(yt). (c) Hierarchical K-means applied twice with K = 2 on 10K samples xi ~ Px|y(X|Yt). At depth d = 1, the posterior is partitioned by the dashed blue line (blue triangles mark cluster centers). The resulting half spaces are subsequently halved by the dashed orange and green lines respectively. (d) Posterior trees (ours) with degree K = 2 and depth d = 2. Note that in all cases the estimated posterior mean (x|yt) (black star) coincides with the analytical mean \u03bc(x|yt) (red star), while in (c)-(d) the lowest density mode is better represented. T(yt)/p(yt) are drawn at the bottom of (b)-(d).", "description": "This figure demonstrates the comparison of different methods for denoising a 2D Gaussian mixture. It shows how K-means, hierarchical K-means, and the proposed method (posterior trees) partition the posterior distribution and visualize the uncertainty. The figure highlights that the proposed method better represents the lower-density modes compared to the other methods.", "section": "Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_21_1.jpg", "caption": "Figure 3: 2D Gaussian mixture denoising. (a) Underlying signal prior px(x) (blue heatmap), and training samples (xi, Yi) ~ Px,y(x, y). (b) K-means with K = 4 applied to 10K samples Xi ~Pxy(xYt), for a given test point yt (red circle). The resulting cluster centers (blue markers) partition the underlying posterior px|y (xyt) (red heatmap), resulting in cluster probabilities p(yt). (c) Hierarchical K-means applied twice with K = 2 on 10K samples xi ~ Px|y(X|Yt). At depth d = 1, the posterior is partitioned by the dashed blue line (blue triangles mark cluster centers). The resulting half spaces are subsequently halved by the dashed orange and green lines respectively. (d) Posterior trees (ours) with degree K = 2 and depth d = 2. Note that in all cases the estimated posterior mean (x|yt) (black star) coincides with the analytical mean \u03bc(x|yt) (red star), while in (c)-(d) the lowest density mode is better represented. T(yt)/p(yt) are drawn at the bottom of (b)-(d).", "description": "This figure shows a comparison of different methods for visualizing uncertainty in a 2D Gaussian mixture denoising task.  It compares standard K-means, hierarchical K-means, and the proposed 'Posterior Trees' method.  Each method's ability to capture the underlying posterior distribution, including multiple modes, is visualized. The figure highlights that the proposed Posterior Trees method effectively visualizes the uncertainty across multiple levels of granularity.", "section": "Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_21_2.jpg", "caption": "Figure 3: 2D Gaussian mixture denoising. (a) Underlying signal prior px(x) (blue heatmap), and training samples (xi, Yi) ~ Px,y(x, y). (b) K-means with K = 4 applied to 10K samples xi ~ Pxy(xYt), for a given test point yt (red circle). The resulting cluster centers (blue markers) partition the underlying posterior px|y (xyt) (red heatmap), resulting in cluster probabilities p(yt). (c) Hierarchical K-means applied twice with K = 2 on 10K samples xi ~ Px|y(X|Yt). At depth d = 1, the posterior is partitioned by the dashed blue line (blue triangles mark cluster centers). The resulting half spaces are subsequently halved by the dashed orange and green lines respectively. (d) Posterior trees (ours) with degree K = 2 and depth d = 2. Note that in all cases the estimated posterior mean (x|yt) (black star) coincides with the analytical mean \u03bc(x|yt) (red star), while in (c)-(d) the lowest density mode is better represented. T(yt)/p(yt) are drawn at the bottom of (b)-(d).", "description": "This figure compares different methods for visualizing the posterior distribution of a 2D Gaussian mixture model.  It shows the underlying signal prior, the results of applying K-means, hierarchical K-means, and the proposed method (Posterior Trees).  The figure highlights how the proposed method effectively captures multimodality and uncertainty in the posterior.", "section": "Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_23_1.jpg", "caption": "Figure A1: Leaf weight sharing strategy. (a) Fully shared architecture, with all leaves predicted jointly. (b) Leaves only share encoder (see Fig. A2).", "description": "This figure compares two different architectures for the proposed method, which is called \"posterior trees\". Both architectures use a U-Net, but differ in how they share parameters across leaves of the tree.  The fully shared architecture (a) shares parameters across all leaves, potentially leading to less diversity in the predictions.  The encoder shared architecture (b) only shares the encoder part of the U-Net, leading to separate decoder parameters for each leaf. This approach balances between computational efficiency and prediction diversity, which is crucial for accurately representing multi-modal posterior distributions.", "section": "A Experimental Details"}, {"figure_path": "UddVRqTrjt/figures/figures_27_1.jpg", "caption": "Figure 3: 2D Gaussian mixture denoising. (a) Underlying signal prior px(x) (blue heatmap), and training samples (xi, Yi) ~ Px,y(x, y). (b) K-means with K = 4 applied to 10K samples xi ~ Pxy(xYt), for a given test point yt (red circle). The resulting cluster centers (blue markers) partition the underlying posterior px|y (xyt) (red heatmap), resulting in cluster probabilities p(yt). (c) Hierarchical K-means applied twice with K = 2 on 10K samples xi ~ Px|y(X|Yt). At depth d = 1, the posterior is partitioned by the dashed blue line (blue triangles mark cluster centers). The resulting half spaces are subsequently halved by the dashed orange and green lines respectively. (d) Posterior trees (ours) with degree K = 2 and depth d = 2. Note that in all cases the estimated posterior mean (x|yt) (black star) coincides with the analytical mean \u03bc(x|yt) (red star), while in (c)-(d) the lowest density mode is better represented. T(yt)/p(yt) are drawn at the bottom of (b)-(d).", "description": "This figure compares different methods for visualizing the posterior distribution of a 2D Gaussian mixture model.  It shows how K-means clustering (flat and hierarchical) and the proposed 'Posterior Trees' method perform in capturing the multi-modal nature of the posterior. The figure highlights that the hierarchical methods, especially the proposed approach, better represent low-density modes in the posterior.", "section": "Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_28_1.jpg", "caption": "Figure 3: 2D Gaussian mixture denoising. (a) Underlying signal prior px(x) (blue heatmap), and training samples (xi, Yi) ~ Px,y(x, y). (b) K-means with K = 4 applied to 10K samples Xi ~Pxy(xYt), for a given test point yt (red circle). The resulting cluster centers (blue markers) partition the underlying posterior px|y (xyt) (red heatmap), resulting in cluster probabilities p(yt). (c) Hierarchical K-means applied twice with K = 2 on 10K samples xi ~ Px|y(X|Yt). At depth d = 1, the posterior is partitioned by the dashed blue line (blue triangles mark cluster centers). The resulting half spaces are subsequently halved by the dashed orange and green lines respectively. (d) Posterior trees (ours) with degree K = 2 and depth d = 2. Note that in all cases the estimated posterior mean (x|yt) (black star) coincides with the analytical mean \u03bc(x|yt) (red star), while in (c)-(d) the lowest density mode is better represented. T(yt)/p(yt) are drawn at the bottom of (b)-(d).", "description": "This figure demonstrates the results of applying different clustering methods to a 2D Gaussian mixture denoising task. It compares K-means, hierarchical K-means, and the proposed posterior trees method.  The results show how the different methods partition the posterior distribution and their strengths and weaknesses in representing the underlying modes, especially low-density modes. ", "section": "Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_29_1.jpg", "caption": "Figure 1: Hierarchical decomposition of the minimum-MSE predictor into prototypes in the task of mouth inpainting. The predicted tree explores the different options of bigger/smaller lips, mouth opening/closing, round/square jawline, etc.", "description": "This figure shows an example of how the model predicts a tree-structured representation of the posterior distribution for a mouth inpainting task.  The root node represents the minimum mean squared error (MMSE) prediction, which is a single image.  However, the model also predicts a tree where each branch represents a plausible alternative reconstruction (e.g., different lip sizes and shapes, mouth openness), with associated probabilities. This hierarchical structure helps visualize uncertainty across multiple levels of granularity.", "section": "1 Introduction"}, {"figure_path": "UddVRqTrjt/figures/figures_30_1.jpg", "caption": "Figure 3: 2D Gaussian mixture denoising. (a) Underlying signal prior px(x) (blue heatmap), and training samples (xi, Yi) ~ Px,y(x, y). (b) K-means with K = 4 applied to 10K samples xi ~ Pxy(xYt), for a given test point yt (red circle). The resulting cluster centers (blue markers) partition the underlying posterior px|y (xyt) (red heatmap), resulting in cluster probabilities p(yt). (c) Hierarchical K-means applied twice with K = 2 on 10K samples xi ~ Px|y(X|Yt). At depth d = 1, the posterior is partitioned by the dashed blue line (blue triangles mark cluster centers). The resulting half spaces are subsequently halved by the dashed orange and green lines respectively. (d) Posterior trees (ours) with degree K = 2 and depth d = 2. Note that in all cases the estimated posterior mean (x|yt) (black star) coincides with the analytical mean \u03bc(x|yt) (red star), while in (c)-(d) the lowest density mode is better represented. T(yt)/p(yt) are drawn at the bottom of (b)-(d).", "description": "This figure shows a comparison of different methods for visualizing the posterior distribution of a 2D Gaussian mixture denoising task.  It compares the results of K-means clustering, hierarchical K-means clustering, and the proposed 'Posterior Trees' method.  The figure highlights how the proposed method effectively captures multi-modal aspects of the posterior distribution, representing both high- and low-density regions more accurately than simpler clustering techniques.", "section": "Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_31_1.jpg", "caption": "Figure 1: Hierarchical decomposition of the minimum-MSE predictor into prototypes in the task of mouth inpainting. The predicted tree explores the different options of bigger/smaller lips, mouth opening/closing, round/square jawline, etc.", "description": "The figure shows a hierarchical decomposition of the minimum mean squared error (MMSE) predictor into prototypes for mouth inpainting.  The predicted tree structure visually represents multiple plausible solutions (prototypes) and their probabilities, branching out to explore variations in lip size, mouth shape, and jawline.  This illustrates how the model captures uncertainty by providing not just a single best guess, but a range of possibilities.", "section": "1 Introduction"}, {"figure_path": "UddVRqTrjt/figures/figures_32_1.jpg", "caption": "Figure A11: Tree comparison in colorization.", "description": "This figure compares the results of three different methods for image colorization: DDNM, DDRM, and the proposed method. Each method's results are presented as a tree, where the root node is the minimum MSE predictor (MMSE) and the leaf nodes are the different colorized images. The probabilities of each leaf node are also shown. The figure demonstrates that the proposed method produces more diverse and higher-quality results compared to the baselines.", "section": "J Visual Comparison to Baselines"}, {"figure_path": "UddVRqTrjt/figures/figures_33_1.jpg", "caption": "Figure A12: Tree comparison in mouth inpainting.", "description": "This figure compares the results of different posterior sampling methods with the proposed method for the task of mouth inpainting. The different methods are compared in terms of the generated samples and their respective probabilities. The proposed method shows a better representation of the underlying uncertainty by providing a more diverse set of samples. ", "section": "J Visual Comparison to Baselines"}, {"figure_path": "UddVRqTrjt/figures/figures_35_1.jpg", "caption": "Figure 3: 2D Gaussian mixture denoising. (a) Underlying signal prior px(x) (blue heatmap), and training samples (xi, Yi) ~ Px,y(x, y). (b) K-means with K = 4 applied to 10K samples xi ~ Pxy(xYt), for a given test point yt (red circle). The resulting cluster centers (blue markers) partition the underlying posterior px|y (xyt) (red heatmap), resulting in cluster probabilities p(yt). (c) Hierarchical K-means applied twice with K = 2 on 10K samples xi ~ Px|y(X|Yt). At depth d = 1, the posterior is partitioned by the dashed blue line (blue triangles mark cluster centers). The resulting half spaces are subsequently halved by the dashed orange and green lines respectively. (d) Posterior trees (ours) with degree K = 2 and depth d = 2. Note that in all cases the estimated posterior mean (x|yt) (black star) coincides with the analytical mean \u03bc(x|yt) (red star), while in (c)-(d) the lowest density mode is better represented. T(yt)/p(yt) are drawn at the bottom of (b)-(d).", "description": "This figure demonstrates the 2D Gaussian mixture denoising task. It compares different methods for visualizing the posterior distribution: K-means, hierarchical K-means, and the proposed posterior trees method. The results show that the posterior trees method effectively visualizes the uncertainty in the posterior distribution across multiple levels of granularity.", "section": "Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_36_1.jpg", "caption": "Figure 1: Hierarchical decomposition of the minimum-MSE predictor into prototypes in the task of mouth inpainting. The predicted tree explores the different options of bigger/smaller lips, mouth opening/closing, round/square jawline, etc.", "description": "This figure shows a hierarchical decomposition of the minimum mean squared error (MMSE) predictor into prototypes for the task of mouth inpainting.  The inpainting task is inherently ambiguous, and the posterior distribution contains many plausible solutions. The figure shows a tree structure, where each node represents a set of similar inpainted images, and the probabilities at each node represent the likelihood of each branch. The tree structure allows for efficient exploration of the uncertainty in the predictions, allowing a user to see different inpainting options (e.g., bigger/smaller lips, mouth opening/closing, jawline shapes) and their probabilities.", "section": "1 Introduction"}, {"figure_path": "UddVRqTrjt/figures/figures_37_1.jpg", "caption": "Figure 4: Diverse applications of posterior trees. The predicted trees represent inherent task uncertainty: e.g., (a) Refining the mean estimate by color, grouping similar colors, while still depicting unlikely ones (e.g., the blue boot); (b) Presenting various plausible colorizations varying by hat color, skin tone, and background; and (c) Exploring the diverse options of eyebrows/eyeglasses.", "description": "This figure demonstrates the applications of posterior trees on different image tasks, including shoes inpainting, face colorization, and eyes inpainting. The results showcase how posterior trees effectively capture and represent uncertainty by providing multiple plausible predictions at different levels of granularity.", "section": "4 Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_38_1.jpg", "caption": "Figure 3: 2D Gaussian mixture denoising. (a) Underlying signal prior px(x) (blue heatmap), and training samples (xi, Yi) ~ Px,y(x, y). (b) K-means with K = 4 applied to 10K samples xi ~ Px|y(x|Yt), for a given test point yt (red circle). The resulting cluster centers (blue markers) partition the underlying posterior px|y (x|yt) (red heatmap), resulting in cluster probabilities p(yt). (c) Hierarchical K-means applied twice with K = 2 on 10K samples xi ~ Px|y(x|Yt). At depth d = 1, the posterior is partitioned by the dashed blue line (blue triangles mark cluster centers). The resulting half spaces are subsequently halved by the dashed orange and green lines respectively. (d) Posterior trees (ours) with degree K = 2 and depth d = 2. Note that in all cases the estimated posterior mean (x|yt) (black star) coincides with the analytical mean \u03bc(x|yt) (red star), while in (c)-(d) the lowest density mode is better represented. T(yt)/p(yt) are drawn at the bottom of (b)-(d).", "description": "This figure demonstrates the results of 2D Gaussian mixture denoising using different methods: K-means, hierarchical K-means, and the proposed posterior trees.  It shows how each method partitions the posterior distribution and estimates cluster probabilities, highlighting the advantages of the hierarchical approach in representing multi-modal posteriors.", "section": "Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_39_1.jpg", "caption": "Figure 1: Hierarchical decomposition of the minimum-MSE predictor into prototypes in the task of mouth inpainting. The predicted tree explores the different options of bigger/smaller lips, mouth opening/closing, round/square jawline, etc.", "description": "This figure shows a hierarchical decomposition of the minimum mean squared error (MMSE) predictor for mouth inpainting.  The prediction is not a single image, but a tree where each branch represents a different plausible reconstruction of the mouth, with associated probabilities.  The tree structure allows for exploring different variations in lip size, mouth shape, and jawline, providing a more comprehensive understanding of the uncertainty in the prediction than a single MMSE estimate would allow.", "section": "1 Introduction"}, {"figure_path": "UddVRqTrjt/figures/figures_40_1.jpg", "caption": "Figure 4: Diverse applications of posterior trees. The predicted trees represent inherent task uncertainty: e.g., (a) Refining the mean estimate by color, grouping similar colors, while still depicting unlikely ones (e.g., the blue boot); (b) Presenting various plausible colorizations varying by hat color, skin tone, and background; and (c) Exploring the diverse options of eyebrows/eyeglasses.", "description": "This figure shows three examples of how posterior trees can be applied to different image tasks.  In each case, the predicted tree reveals multiple plausible solutions (represented as images) and their likelihoods (represented as probabilities). (a) shows edge-to-shoe generation, illustrating how the tree encompasses a range of color variations. (b) demonstrates face colorization, with the tree exploring different possibilities for skin tones, hats, and backgrounds. Finally, (c) illustrates eye inpainting, showcasing variations in eye opening, eyebrows, and the presence/absence of glasses.  In essence, it highlights the tree's utility in representing various solutions and their probability.", "section": "4 Experiments"}, {"figure_path": "UddVRqTrjt/figures/figures_41_1.jpg", "caption": "Figure 1: Hierarchical decomposition of the minimum-MSE predictor into prototypes in the task of mouth inpainting. The predicted tree explores the different options of bigger/smaller lips, mouth opening/closing, round/square jawline, etc.", "description": "This figure shows a hierarchical decomposition of the minimum mean squared error (MSE) predictor into prototypes for the task of mouth inpainting.  The model predicts a tree structure where each node represents a cluster of similar mouth shapes, and the branches represent the probability of transitioning between different mouth configurations. This tree allows for exploration of the different possible solutions for mouth inpainting, ranging from smaller or larger lips, different degrees of mouth opening or closing, and various jawline shapes.  The probability of each path (combination of choices down the tree) reflects the relative likelihood of the corresponding mouth configuration given the input image.", "section": "1 Introduction"}, {"figure_path": "UddVRqTrjt/figures/figures_42_1.jpg", "caption": "Figure 3: 2D Gaussian mixture denoising. (a) Underlying signal prior px(x) (blue heatmap), and training samples (xi, Yi) ~ Px,y(x, y). (b) K-means with K = 4 applied to 10K samples Xi ~Pxy(xYt), for a given test point yt (red circle). The resulting cluster centers (blue markers) partition the underlying posterior px|y (xyt) (red heatmap), resulting in cluster probabilities p(yt). (c) Hierarchical K-means applied twice with K = 2 on 10K samples xi ~ Px|y(X|Yt). At depth d = 1, the posterior is partitioned by the dashed blue line (blue triangles mark cluster centers). The resulting half spaces are subsequently halved by the dashed orange and green lines respectively. (d) Posterior trees (ours) with degree K = 2 and depth d = 2. Note that in all cases the estimated posterior mean (x|yt) (black star) coincides with the analytical mean \u03bc(x|yt) (red star), while in (c)-(d) the lowest density mode is better represented. T(yt)/p(yt) are drawn at the bottom of (b)-(d).", "description": "This figure compares different methods for visualizing uncertainty in a 2D Gaussian mixture denoising task.  It shows the underlying signal distribution, results from K-means clustering (both flat and hierarchical), and the results from the proposed 'posterior trees' method. The comparison highlights how the posterior trees method effectively visualizes the posterior distribution across multiple levels of granularity, capturing even low-density modes.", "section": "Experiments"}]