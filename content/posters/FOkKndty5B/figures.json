[{"figure_path": "FOkKndty5B/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Trade-off between video sampling frequency and frame token number. The horizontal axis represents the ratio (log-transformed) of these two factors. Each curve corresponds to a fixed total number of tokens (e.g., 256 for a 1-minute video). (b) Deficiency of existing Vid-LLMs, such as LLaMA-VID, when facing fine-grained video understanding, and the efficacy of our approach.", "description": "This figure illustrates the trade-off between video sampling frequency and the number of tokens per frame in video LLMs.  Subfigure (a) shows that with a fixed number of total tokens, increasing the sampling frequency (more frames) reduces the number of tokens per frame, and vice versa.  This creates a limitation for fine-grained understanding because too few frames miss crucial details, while compressing frames too much reduces the quality of visual information. Subfigure (b) demonstrates how the proposed SlowFocus method addresses this limitation by focusing on relevant temporal segments with high-frequency sampling while maintaining sufficient global context.", "section": "1 Introduction"}, {"figure_path": "FOkKndty5B/figures/figures_3_1.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework, which consists of two main stages. In Stage 1: Relevant segment grounding, a low-frequency sampling of the video is fed to an LLM to identify the relevant temporal segment based on the query. Stage 2: Mixed-Frequency Sampling then performs high-frequency sampling on the identified segment, combining it with the low-frequency sampling to generate mixed-frequency visual tokens for the LLM to answer the query more accurately.  The diagram visually represents the process flow, showing the input video, query, processing stages, and the final output.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_4_1.jpg", "caption": "Figure 3: The training strategy of SlowFocus, including data distribution and parameter updating in each stage. <image> and <video> denote the tokens for image and video, respectively.", "description": "This figure illustrates the three-stage training process for the SlowFocus model. Stage 1 involves modality alignment using image-text and video-text data to align visual and text embeddings. Stage 2 focuses on boundary enhancement by fine-tuning on tasks such as dense video captioning, segment captioning, and temporal grounding, using a large video dataset. Finally, stage 3 introduces the SlowFocus adaptation, focusing on fine-grained temporal reasoning using specific datasets. Each stage updates the parameters of different model components, including visual encoder, temporal encoder, and LLM.", "section": "3 Method"}, {"figure_path": "FOkKndty5B/figures/figures_5_1.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "The figure illustrates the SlowFocus framework, which involves two stages: 1) identifying query-relevant temporal segments and performing dense sampling (high-frequency) within those segments, and 2) combining these high-frequency features with low-frequency features from the entire video using a multi-frequency mixing attention module. This approach aims to improve fine-grained temporal understanding by preserving both local and global temporal context.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative examples. Our proposed SlowFocus can effectively leverages the segmented temporal clues to accurately answer the posed question.", "description": "This figure shows two example scenarios where the SlowFocus model successfully answers fine-grained temporal questions by leveraging segmented temporal information. In the first example, the model correctly identifies that a shot at a specific time in a sports video is not scored, despite only being provided with a sparse set of frames. The second example shows the model correctly predicting the next action in a music video (switching from keyboard to drums) after being provided with contextual cues, also from a limited number of input frames. These examples highlight SlowFocus's ability to effectively focus on and reason about relevant temporal segments for enhanced fine-grained video understanding, even with sparse input data.", "section": "5 Experiments"}, {"figure_path": "FOkKndty5B/figures/figures_14_1.jpg", "caption": "Figure 6: Video Statistics in FineAction-CGR. It contains a diverse distribution of action types and tasks", "description": "This figure shows two donut charts visualizing the data distribution in the FineAction-CGR benchmark. The left chart displays the distribution of action types per video, indicating that most videos contain only one type of action.  The right chart illustrates the distribution of different tasks within the benchmark, revealing that temporal reasoning and captioning tasks are the most prevalent.", "section": "C Data construction"}, {"figure_path": "FOkKndty5B/figures/figures_14_2.jpg", "caption": "Figure 7: Distribution of clips number in FineAction-CGR", "description": "This figure shows the distribution of the number of video clips per video in the FineAction-CGR dataset.  The x-axis represents the number of clips, and the y-axis shows the number of videos containing that many clips.  The distribution is heavily skewed to the left, indicating that most videos are composed of a small number of clips, while a much smaller number of videos have a large number of clips.  This visualization helps to understand the distribution of video lengths and complexities within the dataset.", "section": "C Data construction"}, {"figure_path": "FOkKndty5B/figures/figures_15_1.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework, which consists of two main stages. The first stage is relevant segment grounding, where the model identifies the query-related temporal segment. The second stage is mixed-frequency sampling, where the model performs dense sampling on the identified segment and combines it with low-frequency sampling across the entire video to generate mixed-frequency visual tokens. These tokens are then used to accurately answer the query.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_15_2.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "The figure illustrates the SlowFocus framework, which involves two stages. First, relevant temporal segments are identified based on the user query.  These segments undergo dense, high-frequency sampling to extract detailed visual features. Second, these high-frequency features are combined with low-frequency samples from the entire video. This mixed-frequency approach is used to generate visual tokens fed into an LLM for accurate query answering. The figure highlights the integration of high-frequency and low-frequency sampling to enhance temporal understanding.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_15_3.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework.  It consists of two main stages: relevant segment grounding and mixed-frequency sampling.  First, a temporal segment relevant to the user's query is identified.  Then, high-frequency sampling is performed on that segment, while low-frequency sampling is done on the whole video. These mixed-frequency visual tokens are then used to generate a more precise and complete answer.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_15_4.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "The figure illustrates the SlowFocus framework's two-stage process. Stage 1 involves identifying query-relevant temporal segments using a low-frequency sampling of the entire video and a temporal grounding model. Stage 2 performs dense sampling (high-frequency) within these segments, combining these high-frequency details with the global low-frequency context via a multi-frequency mixing attention module. The output is a set of mixed-frequency visual tokens used by the LLM for accurate query answering.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_15_5.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework, which involves two main stages. In the first stage, a relevant temporal segment is identified based on the user query.  Then, high-frequency sampling is performed densely on this segment to capture fine-grained details. Simultaneously, low-frequency sampling is done for the entire video to provide broader context. These high and low-frequency features are combined using a multi-frequency mixing attention module to generate a unified representation that accurately answers the query.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_15_6.jpg", "caption": "Figure 1: (a) Trade-off between video sampling frequency and frame token number. The horizontal axis represents the ratio (log-transformed) of these two factors. Each curve corresponds to a fixed total number of tokens (e.g., 256 for a 1-minute video). (b) Deficiency of existing Vid-LLMs, such as LLaMA-VID, when facing fine-grained video understanding, and the efficacy of our approach.", "description": "This figure illustrates the trade-off between video sampling frequency and the number of tokens per frame in video LLMs.  The left subplot (a) shows curves representing different total token numbers, demonstrating that increasing either sampling frequency or tokens per frame (while maintaining a constant total token number) leads to higher accuracy. The right subplot (b) shows how the proposed SlowFocus approach can overcome the limitations of existing models (like LLaMA-VID) in fine-grained video understanding.", "section": "1 Introduction"}, {"figure_path": "FOkKndty5B/figures/figures_15_7.jpg", "caption": "Figure 1: (a) Trade-off between video sampling frequency and frame token number. The horizontal axis represents the ratio (log-transformed) of these two factors. Each curve corresponds to a fixed total number of tokens (e.g., 256 for a 1-minute video). (b) Deficiency of existing Vid-LLMs, such as LLaMA-VID, when facing fine-grained video understanding, and the efficacy of our approach.", "description": "This figure illustrates the trade-off between video sampling frequency and the number of tokens per frame in video LLMs.  Part (a) shows that with a fixed number of total tokens, increasing the sampling frequency (more frames) decreases the number of tokens per frame, and vice versa. This creates a challenge for Vid-LLMs because high token counts per frame are important for accurate frame-level understanding, while a sufficient number of frames are crucial for capturing temporal information. Part (b) demonstrates how existing models like LLaMA-VID struggle with fine-grained video understanding due to this trade-off, highlighting the proposed SlowFocus approach to overcome this limitation.", "section": "1 Introduction"}, {"figure_path": "FOkKndty5B/figures/figures_15_8.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework, which consists of two stages. Stage 1 involves identifying the relevant temporal segments based on the query using low-frequency sampling of the entire video.  Stage 2 then performs dense, high-frequency sampling on those identified segments.  The mixed-frequency visual tokens from both stages are then used to answer the query more accurately.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_15_9.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework.  It shows a two-stage process: first, relevant temporal segments are identified based on the user's query; second, mixed-frequency sampling (high-frequency on the identified segments and low-frequency across the whole video) is applied to generate visual tokens. These tokens are then used to answer the query, improving fine-grained temporal understanding. The diagram highlights the interplay between query understanding, segment identification, high-frequency sampling, low-frequency sampling, and final answer generation.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_15_10.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "The figure illustrates the SlowFocus framework.  It begins by identifying query-relevant temporal segments within a video.  High-frequency sampling is then applied to these segments to capture fine-grained details, while low-frequency sampling covers the entire video to provide context.  The resulting mixed-frequency visual tokens are fed into an LLM for question answering.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_15_11.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework's two stages: relevant segment grounding and mixed-frequency sampling.  First, the system identifies the relevant temporal segment using a query. Second, it performs dense sampling (high frequency) on that segment and combines it with sparse sampling (low frequency) of the entire video. This approach aims to maintain high-quality frame-level information while also capturing comprehensive video-level temporal information.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_15_12.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework. It starts by identifying query-relevant temporal segments. Then, it performs dense sampling (high-frequency) within those segments and combines it with sparse sampling (low-frequency) across the entire video. This mixed-frequency approach aims to provide sufficient high-quality temporal information for accurate query answering.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_16_1.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework, which consists of two stages. In the first stage, the system identifies query-relevant temporal segments within a video. In the second stage, the system performs high-frequency sampling within those segments and combines it with low-frequency sampling of the entire video to create mixed-frequency visual tokens. The mixed-frequency tokens are then used to accurately answer the query.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_16_2.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework, which involves two main stages.  First, relevant temporal segments are identified based on the user query using a low-frequency sampling of the video. Second, high-frequency sampling is performed on the identified segment to capture more details. Both high and low-frequency visual tokens are combined to generate an answer.  This allows the model to efficiently handle the trade-off between video sampling frequency and frame token number, improving the accuracy of fine-grained temporal understanding.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_16_3.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "The figure illustrates the SlowFocus framework, showing a two-stage process.  Stage 1 involves identifying the query-relevant temporal segment within the video using a low-frequency sampling of the video frames. Stage 2 performs high-frequency sampling within the identified segment to extract detailed temporal information.  These high and low-frequency features are then combined to provide the model with a mixed-frequency representation for accurate query answering.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_16_4.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework, which consists of two stages. In the first stage, relevant temporal segments are identified based on the user query.  In the second stage, mixed-frequency sampling is performed: dense sampling on the identified segments to extract local high-frequency features, and low-frequency sampling across the whole video. This method maintains mixed frequency visual tokens for accurate query answering.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_16_5.jpg", "caption": "Figure 5: Qualitative examples. Our proposed SlowFocus can effectively leverages the segmented temporal clues to accurately answer the posed question.", "description": "This figure shows two example queries and how the SlowFocus model uses segmented temporal information to answer them more accurately than models that only use low-frequency sampling.  The first example involves a query about a woman applying lipstick; the low-frequency sampling approach misses crucial details within the relevant segment (the application itself), while SlowFocus identifies and utilizes the high-frequency features, leading to a correct answer. The second query focuses on predicting what action will occur next after a person is playing the keyboard. Again, SlowFocus' ability to zoom into a relevant segment (the transition to playing the drums) results in a better prediction than the low-frequency sampling method.", "section": "5 Experiments"}, {"figure_path": "FOkKndty5B/figures/figures_16_6.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "The figure illustrates the SlowFocus framework, which involves two stages. First, it identifies query-relevant temporal segments using low-frequency sampling of the entire video.  Then, it performs dense, high-frequency sampling within those identified segments. Finally, a multi-frequency mixing attention module combines these high and low-frequency features to generate an answer. This approach aims to improve fine-grained temporal understanding by focusing on relevant segments while maintaining context.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_16_7.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework.  It starts by identifying the relevant temporal segment using a query. Then, it performs dense sampling (high frequency) within that segment and combines these high-frequency features with low-frequency features from the whole video using a multi-frequency mixing attention module. The combined features are fed to the LLM to answer the query. The process is designed to maintain high-quality frame-level information by focusing on the relevant parts of the video.", "section": "3.2 SlowFocus"}, {"figure_path": "FOkKndty5B/figures/figures_16_8.jpg", "caption": "Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.", "description": "This figure illustrates the SlowFocus framework. It shows how the model first identifies the query-relevant temporal segment in the video. Then, it performs dense sampling on this segment to extract high-frequency features. These high-frequency features are combined with global low-frequency features through a multi-frequency mixing attention module to improve temporal understanding. Finally, these combined features are used by the LLM to generate the accurate answer.", "section": "3.2 SlowFocus"}]