[{"type": "text", "text": "SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ming Nie1 Dan Ding1 Chunwei Wang2 Yuanfan Guo2 Jianhua Han2 Hang $\\mathbf{X}\\mathbf{u}^{2}$ Li Zhang1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1School of Data Science, Fudan University 2Noah\u2019s Ark Lab, Huawei ", "page_idx": 0}, {"type": "text", "text": "https://github.com/fudan-zvg/SlowFocus ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have garnered significant attention due to their exceptional text understanding capabilities. Building on the strengths of LLMs, video large language models (VidLLMs) [15, 18, 42] adapt them to the video modality, extending their reasoning and interactive skills to video data. By training on video-level tasks like captioning and question answering, they establish coarse-grained video-language correspondence and acquire the capabilities of video understanding. ", "page_idx": 0}, {"type": "text", "text": "While Vid-LLMs have demonstrated the promising performance in video understanding, they still face a significant challenge. To embed video features into LLMs under computing resource constraints, Vid-LLMs typically need to sparsely sample the original video (e.g., retaining one frame every second) into a collection of low-frequency frames. Subsequently, the token number of each frame are also compressed through a visual adapter like average pooling [16] or Q-former [14]. This leads to a dilemma: Vid-LLMs have to choose between compromised frame-level features and video-level features, each resulting in a reduction of video information. As illustrated in Figure 1 (a), under the premise of a constant total token number, we investigate the performance of Vid-LLMs by dynamically adjusting the sampling frequency and frame token number. The results clearly show that both low sampling frequency and low frame token number lead to performance degradation. Low-frequency sampling results in a sparse image collection as input, omitting crucial temporal details. On the other hand, excessive frame-feature compression degrades the semantic and spatial contexts of each frame. When confronted with fine-grained video understanding tasks, this limitation becomes significantly evident, as depicted in Figure 1 (b). Existing models (e.g., LLaMA-VID [17]) often overlook crucial details early in the input due to low-frequency sampling, leading to inaccuracy. ", "page_idx": 0}, {"type": "image", "img_path": "FOkKndty5B/tmp/8137d6d17f8ba0b4c99661975fe97d923813f84f6790a74e48ac73313982cbf5.jpg", "img_caption": ["Figure 1: (a) Trade-off between video sampling frequency and frame token number. The horizontal axis represents the ratio (log-transformed) of these two factors. Each curve corresponds to a fixed total number of tokens (e.g., 256 for a 1-minute video). (b) Deficiency of existing Vid-LLMs, such as LLaMA-VID, when facing fine-grained video understanding, and the efficacy of our approach. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address this challenge, we introduce SlowFocus, which is designed to pinpoint relevant temporal segments in response to questions, and subsequently maintains high-quality temporal details to enrich fine-grained video comprehension. For video understanding tasks, we assume that the relevant details are concentrated within one or several clips. SlowFocus initially identifies these segments based on the provided questions. It then densely samples the segmented temporal clips at a high-frequency to extract local temporal features highly pertinent to the questions. To effectively model the temporal relationships between frames and capture inter-frame contexts, we propose a specialized temporal encoder and a multi-frequency mixing attention module for enhanced temporal comprehension. ", "page_idx": 1}, {"type": "text", "text": "To enhance Vid-LLMs\u2019 ability to perform SlowFocus based on mixed frequencies, we propose a set of training and inference strategies to improve their temporal localization and fine-grained temporal reasoning capability. Following VTimeLLM [10], we fine-tune our Vid-LLM in the second stage on dense video captioning and temporal grounding tasks. This process enhances the model\u2019s ability to predict discrete bins that define the relevant temporal segments. In the third stage, we adapt the Vid-LLM to the SlowFocus mechanism for high-quality, fine-grained temporal-related tasks, which enables our model to reason precisely based on high-frequency temporal details. ", "page_idx": 1}, {"type": "text", "text": "In addition to the scarcity of methods for fine-grained video understanding, existing benchmarks [37, 40] also fall short in providing adequate challenges for specific temporal-related tasks. To bridge this gap and evaluate our proposed framework, we introduce FineAction-CGR, a newly dedicated benchmark that focuses on fine-grained video understanding, especially reasoning tasks based on temporal details. Our method demonstrates superior performance on the proposed benchmark, offering a promising solution to high-quality video understanding. ", "page_idx": 1}, {"type": "text", "text": "The contributions of this paper are summarized as follows: (i) We introduce SlowFocus, a straightforward yet effective framework designed to resolve the prevalent trade-off in existing Vid-LLMs between capturing limited frame-level details and overarching video-level contexts. SlowFocus adeptly maintains high-frequency local details alongside low-frequency global contexts, facilitating the identification of pertinent temporal segments and precise reasoning on video contents. (ii) We present a novel training strategy specifically designed to enhance the temporal localization abilities of Vid-LLMs, and seamlessly adapt them to our newly proposed SlowFocus approach. (iii) We establish a comprehensive new benchmark and carry out extensive experiments to rigorously assess the fine-grained video understanding capabilities of Vid-LLMs. The empirical evidence strongly indicates that our SlowFocus approach significantly outperforms existing models, particularly in tasks requiring detailed temporal understanding and reasoning within videos. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Vision large language models. Researchers have made significant efforts to enable Large Language Models(LLMs) to comprehend visual information. BLIP-2 [14] aligns vision-language representation with a lightweight Querying Transformer by concept of Q-Former. MiniGPT-4 [45] aligns detailed image descriptions with advanced LLM which significantly enhances its multi-modal abilities. Exploring diverse multi-modal instruction-following data, LLaVA [20] has demonstrated impressive multi-model chat abilities. Recent LLMs, like Kosmos-2 [26] and VisionLLM [33], probed into more specific aspects of image comprehension, including referring and grounding [27], remarkably enhancing the capability to describe intricate image details. ", "page_idx": 2}, {"type": "text", "text": "Video large language models. The exploration of LLMs\u2019 potential has been extended from images to videos, which contributes to emergence of Video LLMs. VideoChat [15] combines fine-tuning and LLM-based video agents and is fine-tuned using a specially designed video-centric instruction dataset. Video-ChatGPT [23] proposes a novel human assisted and semi-automatic annotation framework for generation high quality instruction data for videos. Video-LLaMA [42], trained on vision-language branch and audio-language branch separately with same visual data and process, has demonstrated impressive abilities in understanding both visual and auditory content in videos. Video-LLaVA [18] learns united visual representation by alignment before projection and conducts joint training on images and videos simultaneously. With efforts, Video LLMs have exhibited powerful task-handling capabilities in downstream tasks, like text-video retrieval and video captioning. However, these models remain limited in ability to comprehend fine-grained content. To solve this problem, we introduce a model with powerful capability of fine-grained video understanding. ", "page_idx": 2}, {"type": "text", "text": "Fine-grained video understanding. Comprehending videos in fine-grained aspects requires precisely locating and understanding specific events within a video. It is roughly divided by previous works [10] into temporal grounding [2, 7] and dense video captioning [13, 31] tasks. Temporal grounding demands the model to precisely identify start and end timestamps of video segment according to a given text query. Dense video captioning requires both temporal localization and captioning for all events within an untrimmed video. Both tasks are constrained by the labor-intensive nature of annotation, resulting in relatively small datasets. Moreover, fine-grained video understanding demands a deep comprehension of how objects change and interact over time, particularly at a detailed level. This complexity necessitates temporal reasoning tasks to effectively analyze these dynamics. However, current research in this field falls short of adequately addressing these requirements. To solve this problem, our proposed large-scale dataset provides mass temporal annotations and captions in different dimensions, supplying a wealth of information for training in temporal video grounding and reasoning tasks. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we begin with a preliminary overview of video LLMs (Vid-LLMs) in Section 3.1. Following that, we offer a comprehensive explanation of our proposed SlowFocus in Section 3.2, followed by a detailed introduction to our innovative training strategy in Section 3.3. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Contemporary Vid-LLMs typically feature a modular architecture, which includes a visual encoder $E_{V}$ , a series of visual adapters $Q$ , and a large language model $L$ . For a given video $V=\\{V(t)\\in\\{$ $\\mathbb{R}^{H\\times W\\times3}|t=0,...,T\\}$ that consists of $T$ frames, along with its associated question $q$ , Vid-LLMs generally perform downsampling on the original video $V$ at a fixed interval $M_{L}$ . This process results in sparsely distributed frames $V_{L}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nV_{L}=\\{V(M_{L}t)\\in\\mathbb{R}^{H\\times W\\times3}|t=0,...,T\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $M_{L}\\gg1$ . Consequently, only a very limited number of frames $(T/M_{L}\\ll T)$ are selected as the actual input for the Vid-LLM, a technique we refer to as low-frequency sampling in this study. ", "page_idx": 2}, {"type": "image", "img_path": "FOkKndty5B/tmp/e4d59ee46f9b28c192fbd0de9e1ebea63aa563d38a1eb7d21f92590fd4da52d7.jpg", "img_caption": ["Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Subsequently, the visual encoder processes the downsampled frames $V_{L}$ and encodes them into a series of visual tokens denoted as $\\bar{z}_{L}=E_{V}(V_{L})$ . These visual tokens are then transformed to align with the embedding space of the language model through the visual adapter $Q$ , resulting in aligned visual tokens $h_{L}\\doteq\\bar{Q}(z_{L})$ . Concurrently, the input text query $q$ is encoded into linguistic tokens $h_{q}$ by the textual encoder. These visual and text tokens are concatenated into a unified sequence $[\\dot{h_{L}},h_{q}]$ , which then serves as the input for the decoder component of the large language model $L$ . The model leverages this combined representation to infer the appropriate answer $a n s=L([h_{L},h_{q}])$ , showcasing its ability to perform cross-modal reasoning and respond to human queries. ", "page_idx": 3}, {"type": "text", "text": "Although this paradigm is well developed in the field of video understanding, it encounters significant limitations in tasks requiring fine-grained temporal reasoning. By retaining only a small, discontinuous subset of frames $t,$ where $t\\ll T$ ), substantial information loss can occur, as depicted in Figure 1 (b). Fine-grained temporal video understanding demands that the model concentrate on one or more specific temporal intervals, which could be potentially brief. However, the low-frequency sampling method predominantly captures overarching information while neglecting crucial local details, leading to inaccuracies such as incorrect responses or hallucinations. To address these shortcomings, we develop the SlowFocus strategy. ", "page_idx": 3}, {"type": "text", "text": "3.2 SlowFocus ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To improve Vid-LLMs with the SlowFocus mechanism, we first expand the traditional training and inference paradigm by introducing Mixed-Frequency Sampling (MFS). Subsequently, we explicitly model the temporal relationships among the sampled frames. Finally, we enhance the visual tokens with our newly proposed multiple-frequency mixing attention, designed to capture long-term contexts. We now provide a detailed elaboration on each of these components. ", "page_idx": 3}, {"type": "text", "text": "Relevant segment grounding. To better mimic human cognition in our Vid-LLM, we transform its inference paradigm into a multi-round dialogue format that integrates both the query and temporal awareness elements. The methodology of this transformation is illustrated in Figure 2. Initially, we sample the entire video at a fixed interval to capture the low-frequency frames $V_{L}$ , which are then encoded into visual embeddings $h_{L}$ . Following that, we reformat the original question $q$ into temporal grounding questions $q_{1}$ , such as \u201c<video>\\nPlease provide the temporal segment help to reason the question: <question>\u201d, where <question> refers to the original question $q$ . ", "page_idx": 3}, {"type": "image", "img_path": "FOkKndty5B/tmp/13cd9db5ac98a80cf9786888fdef72c68eb8ffd227c5d81146cd55b1afbe29cd.jpg", "img_caption": ["Figure 3: The training strategy of SlowFocus, including data distribution and parameter updating in each stage. <image> and <video> denote the tokens for image and video, respectively. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "With the augmented question $q_{1}$ , we direct the Vid-LLM to identify the relevant temporal segments $\\tau$ within the video that are pertinent to the original question $q$ . This is accomplished by providing the model with low-frequency frames containing global information: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tau=L([h_{L},h_{q_{1}}]),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $h_{q_{1}}$ represents the textual embedding of question $q_{1}$ and $\\tau\\subset[0,T]$ . The identified segment $\\tau$ is designed to encompass query-related details that are crucial for addressing the question $q$ . ", "page_idx": 4}, {"type": "text", "text": "Mixed-frequency sampling. Subsequently, we perform dense sampling on the identified segment $\\tau$ to obtain high-frequency frames $V_{H}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{H}=\\{V(M_{H}t)\\in\\mathbb{R}^{H\\times W\\times3}|t\\in\\tau\\},M_{H}=m a x(\\frac{|\\tau|}{N_{H}},1).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We dynamically adjust the sampling interval $M_{H}$ based on the temporal length of segment $|\\tau|$ and to ensure an adequate number of samples $N_{H}$ are taken from the relevant segment. The densely sampled frames $V_{H}$ are then encoded into high-frequency visual tokens $h_{H}$ by visual encoder. ", "page_idx": 4}, {"type": "text", "text": "With the inclusion of local details from high-frequency frames, we augment the initial question $q$ with prompts, such as \u201cAdditional temporal clues to focus on: ...\u201d, to form $q_{2}$ , as illustrated in Figure 2. We then combine the mixed-frequency visual tokens to predict the final answer: ", "page_idx": 4}, {"type": "equation", "text": "$$\na n s=L([h_{L},\\pi(h_{L},h_{H}),h_{q_{2}}]),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pi$ is the multiple-frequency mixing attention, which will be detailed subsequently. ", "page_idx": 4}, {"type": "text", "text": "Multiple-frequency mixing attention. Because of the roughness of the low-frequency and the detailed specificity of the high-frequency visual features, simply concatenating these two directly into the language model may not yield optimal results. Fine-grained temporal understanding often requires modeling relationships between multiple events, necessitating the integration of global contexts into high-frequency local features. From this perspective, we introduce the Multiple-frequency Mixing Attention (MMA): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi(h_{L},h_{H})=s o f t m a x(\\frac{h_{H}h_{L}^{T}}{\\sqrt{d}})h_{L},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where, for simplicity, we omit the process of applying FFN to $h_{L}$ and $h_{H}$ . The resulting output is then fed into LLM to predict the textual response, as described in Equation 4. ", "page_idx": 4}, {"type": "text", "text": "Temporal relationship modeling. While LLM can implicitly capture temporal relationships among input visual embeddings based on their sequential positioning, it faces difficulties when the visual tokens are not uniformly distributed along the timeline. To address this challenge, we propose a temporal encoder that encodes the relative positions of frames into a set of discretized temporal tokens $\\mathcal{E}\\in\\mathbb{R}^{N\\times C}$ , where $N$ represents the discrete temporal space. For frames sampled at multiple frequencies $V(t)$ , the corresponding temporal token is $\\epsilon_{i}$ , where $i=\\lfloor N*t/T\\rfloor$ . These temporal embeddings are then incorporated into the visual tokens by directly adding them to the frame features. ", "page_idx": 4}, {"type": "image", "img_path": "FOkKndty5B/tmp/df8788fb0e3251e85ad64f66bdc8ec4843b068e073e8ff22eb348e99380f3d7a.jpg", "img_caption": ["Figure 4: Pipeline of instruction-following data generation. Split the filtered FineAction videos into clips and extract time segments. Use the fine-tuned Video Recaptioner Model and GPT-4V to generate captions for both video clips and full videos. Integrate the ground truth data, new captions, and time segments to create comprehensive annotations. Finally, generate QA pairs for various tasks using different prompts via GPT-4. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.3 Training strategy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Considering training efficiency, we introduce a novel training procedure which is structured into three distinct phases in this work: (i) pre-training for modality alignment, (ii) fine-tuning for enhancing temporal grounding, and (iii) SlowFocus adaption, as illustrated in Figure 3. We now provide detailed elaborations on the training strategies and datasets employed for each of these stages. ", "page_idx": 5}, {"type": "text", "text": "Modality alignment. In the pre-training stage, our primary focus is to optimize the visual adapters and temporal encoders, while the visual encoder and language model are frozen to ensure that the visual features align effectively with the language space. Following the approach used in LLaMA-VID [16], we utilize the image-text LCS-558K dataset from LLaVA [20], along with 232K video-caption samples from the WebVid 2.5M [3]. ", "page_idx": 5}, {"type": "text", "text": "Boundary enhancement. After the pre-training stage, the Vid-LLM gains proficiency in processing visual information. During the fine-tuning stage, we focus on enhancing the model\u2019s ability to comprehend sequences of video frames, thereby improving its temporal localization capabilities. In alignment with practices from VTimeLLM [10], we employ the InternVid-10M-FLT dataset [34], which is specifically designed for temporal-awareness training. The tasks within this dataset include: (i) dense video captioning, which requires detailed descriptions of all events along with their corresponding timestamps, and (ii) segment captioning and temporal video grounding, which involve generating descriptions based on timestamps or determining timestamps based on descriptions. Throughout this stage, we continue to train the visual adapters and temporal encoders. Additionally, the LLM is further trained using LoRA [9] to refine its capabilities. ", "page_idx": 5}, {"type": "text", "text": "SlowFocus adaption. Following the fine-tuning stage, our model has demonstrated the ability to comprehensively understand activities within the video and accurately align them with their respective timestamps. In the final stage, to further enhance multi-modality comprehension and integration with the MMF mechanism, we construct an instruction-tuning dataset using samples from ActivityNet Captions [13] and FineAction [21]. We transform the annotations into over 100K high-quality QA dialogues, which will be detailed in the subsequent section. In alignment with our SlowFocus mechanism, during training with fine-grained captioning and reasoning tasks, the model is provided with the ground-truth temporal segments $\\tau_{G T}$ and subjected to mixed-frequency sampling. In this stage, we freeze all the visual adapters and attention modules, only training the language model as well as the MMA module. The parameters of visual encoder are frozen all over the stages. ", "page_idx": 5}, {"type": "table", "img_path": "FOkKndty5B/tmp/61a37760cd3c49caa31b8da62237528491e5861a64842c2fd7a6f005fff437a6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "FOkKndty5B/tmp/8cacb15db66ca877246874a8eb0f42b0bcac8c03095dbf5bc463388e21e60d02.jpg", "table_caption": ["Table 1: Main results on FineAction-CGR benchmark. The column LoRA represents whether the LLM is fine-tuned fully or using LoRA. $\\dagger$ : Model is re-trained on the stage 3\u2019s data. B: B $@4$ . M: METEOR. R: ROUGE. C: CIDEr. "], "table_footnote": ["Table 2: Comparison with existing methods on coarse-grained video understanding benchmarks. Our method achieve on par performance with state-of-the-art models. "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 FineAction-CGR benchmark ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In addition to the scarcity of previous methods for fine-grained video understanding, existing benchmarks [37, 40] do not adequately challenge specific temporal-related tasks. To address this gap and evaluate our proposed framework, we introduce FineAction-CGR, a comprehensive and highquality instruction-following benchmark designed for evaluating the capabilities of Video LLMs in fine-grained video understanding. FineAction [21] is a large-scale and fine-grained video dataset encompassing diverse action categories with detailed annotations of action instances and time segments. For the purpose of SlowFocus adaption and model evluation, we divide the FineAction dataset into training and testing sets based on videos, allocating $75\\%$ to the training set and $25\\%$ to the test set, ensuring these is no overlap between them. Building on this foundation, we probe time information and content information from FineAction and expand it into multi-tasks. This section outlines the data construction procedures, which consist of three main steps: (i) video preprocessing, (ii) annotation construction, and (iii) formation of downstream tasks. More details are included in the appendix. ", "page_idx": 6}, {"type": "text", "text": "Video preprocessing. As depicted in Step I of Figure 4, we employ a modified two-stage splitting algorithm from Panda70M [5] to segment videos into clips, thereby providing raw materials for fine-grained information at the clip level. This preprocessing step yields 62,912 video clips. ", "page_idx": 6}, {"type": "text", "text": "Annotation construction. Step II in Figure 4 outlines our approach to creating new annotations, which include time segments, captions, and action labels. Given the high cost of using GPT-4V [1] for generating large-scale, clip-level captions, we generate detailed captions for entire videos with GPT4V and then use a fine-tuned Video Recaptioner Model to produce large-scale, clip-level captions. By integrating action labels and time segments from the ground truth with clip segments and the generated captions, we create comprehensive new annotations. ", "page_idx": 6}, {"type": "text", "text": "Formation of downstream tasks. Utilizing the new annotations, we design four types of video instruction-following tasks, as illustrated in Step III of Figure 4: (i) segmented captioning, (ii) temporal video grounding, (iii) temporal video reasoning and (iv) multi-turn QA. Detailed information on construction procedures, task formats, prompts, and case studies can be found in Appendix C- E. ", "page_idx": 6}, {"type": "table", "img_path": "FOkKndty5B/tmp/16ea61f3604ddc96219d25af784bc44a08696085d5ed7f1bd9653c948663e547.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "FOkKndty5B/tmp/f8622df5e44f9659f7e244bbee261a3c28a96467caa9682a18ae38a8a0f9c63c.jpg", "table_caption": ["Table 3: Comparison with existing methods on MovieChat-1K.Table 4: Comparison with existing methods on EgoSchema. "], "table_footnote": ["Table 5: Components analysis. $V_{L}$ means only low-frequency frames are sampled. $V_{L}+V_{H}$ represents performing mixed-frequency sampling and $N_{H}$ denotes the number of high-frequency frames. "], "page_idx": 7}, {"type": "text", "text": "Evaluation protocol. To assess the capability of video language model in comprehending fine-grained video understanding tasks, we integrate multiple evaluation metrics in comprehensively evaluate the model\u2019s performance against our proposed benchmark. For temporal grounding task, we calculate the Intersection over Union (IoU) between the temporal segments generated by the model and the corresponding ground truth, and we report meanIoU (mIoU) metric. For segmented captioning, we employ commonly accepted captioning-based metric, including BLEU [25], METEOR [4], ROUGE [19] and CIDEr [30]. For temporal reasoning task, we utilize GPT-4 [1] to evaluate the accuracy and score of the generated answers. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Experiment setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In our experiments, we implement LLaMA-VID [17] as baseline and utilize Vicuna-7B v1.5 [44] as our foundational LLM. More implementation details are provided in the appendix. We adjust the resolution of input videos to $224\\,\\times\\,224$ , and each frame is condensed into 64 tokens. The low-frequency sampling interval $M_{L}$ is set to match the original video fps, ensuring one frame is sampled every second. We define the dense sampling number $N_{H}$ as 20 and the size of temporal token space $N$ as 1000. The AdamW [22] optimizer is applied with a cosine learning rate and decay and a warm-up period. We train our Vid-LLM for three stages. During the initial pre-training stage, the learning rate is set to $1\\times10^{-3}$ . For the subsequent fine-tuning stages, the learning rate is adjusted to $2\\times10^{-4}$ . Additionally, the LoRA parameters are configured with $r=64$ and $a l p h a=128$ . All experiments are conducted on 8 V100 GPUs. ", "page_idx": 7}, {"type": "text", "text": "5.2 Main results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Results on fine-grained video understanding. We evaluate the performance of existing VidLLMs on our proposed fine-grained video understanding benchmark FineAction-CGR, as detailed in Table 1. Further specifics are available in the appendix. Our method significantly outperforms other counterparts, achieving a mIoU of 66.68 for temporal grounding and an accuracy of $53.10\\%$ for the reasoning task. Notably, most other models, with the exception of VTimeLLM [10], exhibit subpar performance in both the temporal grounding and reasoning tasks on the benchmark. We propose several possible explanations for this observation: (i) These models lack sensitivity to precise time boundaries, making it challenging for them to accurately predict temporal segments. (ii) Due to their reliance on low-frequency sampling, these models struggle to capture fine-grained temporal details, which adversely affects their performance on tasks requiring fine-grained temporal reasoning. ", "page_idx": 7}, {"type": "image", "img_path": "FOkKndty5B/tmp/c0c3e6313049116766a002ad6988d2dc1df210f3fc7ff778aeda7569aa1d5dbc.jpg", "img_caption": ["Figure 5: Qualitative examples. Our proposed SlowFocus can effectively leverages the segmented temporal clues to accurately answer the posed question. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We also fine-tune the baseline using stage 3. To ensure fairness, the implementation details for baseline fine-tuning are kept consistent with those of our method. The baseline\u2019s performance improves because stage 3 includes tasks focused on temporal grounding and is specifically designed for fine-grained analysis. Furthermore, the remaining performance gap further supports our explanations. ", "page_idx": 8}, {"type": "table", "img_path": "FOkKndty5B/tmp/af030d72c41683cbc5b096af332d2fe52c7653a7326777a4f7a7f8b9abe23757.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 6: Ablation of training stages. The first stageTable 7: The results of dynamically adjusting samitself yields poor result. Integrating these stagespling frequency (fps) and frame token number $N_{V}$ together results in the optimal performance. when maintaining constant total token number. ", "page_idx": 8}, {"type": "text", "text": "Results on coarse-grained video-based benchmarks. In Table 2, we present a comparative evaluation of our method against various state-of-the-art methods across three zero-shot videoQA benchmarks: MSVD-QA [36], MSRVTT-QA [37], and ActivityNet-QA [40]. We also conduct experiments on the video-based generative performance benchmark [23]. The results demonstrate that the proposed SlowFocus mechanism not only enhances fine-grained video understanding but also delivers competitive performance in coarse-grained video understanding tasks, achieving results on par with state-of-the-art models. ", "page_idx": 8}, {"type": "text", "text": "Results on long video benchmarks. To further investigate the effectiveness of the proposed method on more challenging scenarios, we provide evaluations on long video benchmarks, including MovieChat-1K [28] and EgoSchema [24]. As shown in Table 3, we evaluate our method on MovieChat-1K. The results show that although our method is not specifically trained on long video benchmarks (in contrast, MovieChat [28] has undergone targeted training for long videos), it still achieves competitive results $58.6\\%$ accuracy in global mode and $48.1\\%$ in breakpoint mode). ", "page_idx": 8}, {"type": "text", "text": "Additionally, we also conduct experiments on EgoSchema [24] benchmark, as detailed in Table 4. The results further demonstrate that, despite not being specifically trained on long video datasets, our method still achieves competitive performance. ", "page_idx": 8}, {"type": "text", "text": "Qualitative results. Figure 5 illustrates the qualitative results of our method on different videos. Our SlowFocus comprehensively analyzes the entire video, accurately identifies relevant temporal details based on the posed question, and provides precise answer within the context of videos. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we provide detailed ablation studies of our method conducted utilizing Vicuna-7B as the foundation model. ", "page_idx": 8}, {"type": "text", "text": "Component-wise analysis. We first investigate the impact of each proposed component in Table 5. Importantly, we observe that the baseline model, which relies solely on low-frequency frames $(V_{L})$ , achieves limited performance, with an mIoU of 32.54 and an accuracy of 30.25. This highlights the challenges faced by current Vid-LLMs in addressing fine-grained temporal tasks using only lowfrequency sampling. Additionally, the results in the second row indicate that the inclusion of MFS significantly improves reasoning capabilities, resulting in an accuracy increase of $+8.87$ . Furthermore, the temporal encoder, which models temporal relationships effectively, boosts both grounding and reasoning capabilities, with an increase of $+22.45$ in mIoU and $+7.25$ in accuracy. Increasing the high-frequency sampling number $N_{H}$ from 10 to 20 leads to a noticeable performance gain $_{(+7.66}$ in mIoU and $+5.31$ in accuracy), although further increasing $N_{H}$ to 40 offers only marginal benefits. Lastly, the MMA module contribute an accuracy enhancement of $+1.42$ . ", "page_idx": 8}, {"type": "table", "img_path": "FOkKndty5B/tmp/2a2eeda000bf493a8bc8d18633f5ef9c3b321178b74a87b45a46d671f7f88c67.jpg", "table_caption": ["Table 8: Ablation study on temporal token space $N$ . "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Necessity of fine-tuning strategy. We also analyze the impact of each training stage, as reported in Table 6. Directly evaluating the pre-trained model after stage 1 yields poor performance. Based on necessary pre-training stage 1, when only utilizing stage 2 for boundary enhancement, there is a significant improvement by 51.56 in mIoU in temporal grounding ability. Furthermore, implementing only stage 3 for SlowFocus adaptation boosts the performance in temporal reasoning by 25.14 in accuracy. Integrating these stages results in the highest performance in both mIoU and accuracy metrics, highlighting the irreplaceability of our proposed comprehensive training strategy. ", "page_idx": 9}, {"type": "text", "text": "What contributes more to fine-grained video understanding? In Table 7, we conduct further experiments to explore how changes in sampling frequency and the number of frame tokens influence our method. For meaningful comparison, we maintain constant total token numbers while gradually decreasing the sampling frequency. The results indicate that within proposed SlowFocus, performance improves with an increase in frame tokens and remains relatively unaffected by a decrease in global sampling frequency. This demonstrates the efficacy of our approach in high-frequency sampling and effectively alleviates the trade-off dilemma between sampling frequency and frame tokens. ", "page_idx": 9}, {"type": "text", "text": "Ablation on discretized temporal space. We investigate the influence of temporal token space $N$ in Table 8. The results demonstrate that when the token space increase from 0.1K to 1K, a significant improvement $(+22.87$ in mIoU and $+13.69\\$ in accuracy) occurs. While further increasing $N$ from 1K to 10K, the performance drops instead. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusion. We introduce SlowFocus, a straightforward yet potent mechanism that significantly enhances fine-grained video understanding. Our approach improves the temporal localization capabilities of Vid-LLMs, enabling them to precisely identify relevant temporal segments based on the given query. Moreover, with our newly developed temporal encoder and multi-frequency mixing attention module, our method effectively models the temporal relationships among frames and captures inter-frame context. Demonstrating superior performance on our newly established fine-grained video understanding benchmarks, we hope that our work can propel the development of Vid-LLMs in achieving advanced video understanding. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Despite significant advancements made in this work to enhance Vid-LLM\u2019s access to high-frequency temporal details and its temporal reasoning capabilities, challenges persist due to the limited existing research on maintaining high resolution in video. Consequently, our method may still encounter inaccurate predictions stemming from the ambiguity of spatial details. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by National Natural Science Foundation of China (Grant No.   \n62106050 and 62376060) and Natural Science Foundation of Shanghai (Grant No. 22ZR1407500). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint, 2023. 7, 8   \n[2] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In ICCV, 2017. 3 [3] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, 2021. 6 [4] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In ACL workshop, 2005. 8 [5] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. arXiv preprint, 2024. 7, 14 [6] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint, 2021. 8   \n[7] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In ICCV, 2017. 3 [8] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020. 8   \n[9] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint, 2021. 6   \n[10] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. arXiv preprint, 2023. 2, 3, 6, 7, 8, 14   \n[11] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint, 2024. 8   \n[12] Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo Park, and Michael S Ryoo. Language repository for long video understanding. arXiv preprint, 2024. 8   \n[13] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Densecaptioning events in videos. In ICCV, 2017. 3, 6   \n[14] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 1, 3   \n[15] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint, 2023. 1, 3, 7, 8   \n[16] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint, 2023. 1, 6, 7   \n[17] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint, 2023. 2, 8   \n[18] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint, 2023. 1, 3   \n[19] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, 2004. 8   \n[20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 2024. 3, 6, 14   \n[21] Yi Liu, Limin Wang, Yali Wang, Xiao Ma, and Yu Qiao. Fineaction: A fine-grained video dataset for temporal action localization. TIP, 2022. 6, 7   \n[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint, 2017. 8   \n[23] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint, 2023. 3, 7, 8, 9   \n[24] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 2023. 9   \n[25] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002. 8   \n[26] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint, 2023. 3   \n[27] Kanchana Ranasinghe, Satya Narayan Shukla, Omid Poursaeed, Michael S Ryoo, and Tsung-Yu Lin. Learning to localize objects improves spatial reasoning in visual-llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12977\u201312987, 2024. 3   \n[28] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In CVPR, 2024. 8, 9   \n[29] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint, 2023. 8   \n[30] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015. 8   \n[31] Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and Yong Xu. Bidirectional attentive fusion with context gating for dense video captioning. In CVPR, 2018. 3   \n[32] Shijie Wang, Qi Zhao, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, and Chen Sun. Vamos: Versatile action models for video understanding. arXiv preprint, 2023. 8   \n[33] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. Advances in Neural Information Processing Systems, 2024. 3   \n[34] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal understanding and generation. arXiv preprint, 2023. 6   \n[35] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint, 2022. 8   \n[36] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In ACM MM, 2017. 9   \n[37] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR, 2016. 2, 7, 9   \n[38] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. Advances in Neural Information Processing Systems, 2022. 7   \n[39] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. Advances in Neural Information Processing Systems, 2022. 8   \n[40] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In AAAI, 2019. 2, 7, 9   \n[41] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. A simple llm framework for long-range video question-answering. arXiv preprint, 2023. 8   \n[42] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint, 2023. 1, 3, 7, 8   \n[43] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint, 2023. 7   \n[44] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. 8   \n[45] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint, 2023. 3 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Overview ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this appendix we present: ", "page_idx": 13}, {"type": "text", "text": "\u2022 More implementation details (Section B).   \n\u2022 Data construction details (Section C).   \n\u2022 Data statistics (Section D).   \n\u2022 Prompts used for instruction generation (Section E).   \n\u2022 Discussion on broader impacts (Section F). ", "page_idx": 13}, {"type": "text", "text": "B More implementation details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In our work, we employ CLIP-ViT-L-14 for the frozen visual encoder, which accepts image resolutions at $224\\times224$ as input. For visual adapters, we follow LLaVA [20] to train a projector layer, aligning visual features with the pre-trained LLM word embedding. ", "page_idx": 13}, {"type": "text", "text": "We also elaborate on how we enable LLM to predict temporal segments during stages 2 and 3 fine-tuning. Following VTimeLLM [10], we utilize the textual format \u201cfrom s to e\u201d to denote a video clip, where $s$ and $e$ represent the starting and ending points, respectively. These time points are normalized and range from 000 to 999, corresponding to the discretized temporal token space $N$ . During training, Vid-LLM is exposed to supervisory signals as described above, which enable it to develop the capability to accurately locate temporal segments. ", "page_idx": 13}, {"type": "text", "text": "C Data construction ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Step I: video preprocessing. We split videos into video clips to provide raw materials for fine-grained information in video clip level. ", "page_idx": 13}, {"type": "text", "text": "A proper splitting and stitching strategy is necessary. For one thing, the content of each clip should be semantically consistent. Without this consistency, actions within the same video clip obtained by erroneous segmentation can\u2019t be inferred reasonably and in chronological order, which negatively impacts the performance of downstream tasks such as action recognition and temporal reasoning. For another, the video clips with insufficient information or that are too short are not suitable for subsequent tasks, such as temporal grounding. Therefore, we adopt a modified two-stage splitting algorithm in Panda70M [5] to obtain semantically consistent video clips. ", "page_idx": 13}, {"type": "text", "text": "In first stage, We collect time boundaries of events in the video. Instead of detecting abrupt changes in pixels of adjacent frames, we detect lens switching so that actions in the same video clip can be inferred reasonably. In second stage, we stitch adjacent events based on time boundaries in first stage. Our stitching strategy could be depicted as merging short video clips and that are semantically similar. To be specific, we merge current event with the previous, if one of the following two conditions is met: 1)the video clip duration is less than five seconds, 2) the feature distance between the start frame of the current event and the end frame of the previous event does not exceed 0.1. Finally, we split videos into clips according to time boundaries from second stage. ", "page_idx": 13}, {"type": "text", "text": "Our adjusted splitting strategy has two advantages: 1) it preserves the information density of each segment, ensuring that subsequently generated captions are reasonable; 2) it maintains logical temporal consistency of actions within each segment, which facilitates the generation of questionanswer pairs in downstream tasks. Following the preprocessing step, we obtained 62,912 video clips. ", "page_idx": 13}, {"type": "text", "text": "Step II: annotation construction. As Figure 4 illustrates, new annotations we construct consist of time segments, captions and actions. (1) Action labels are from FineAction ground truth. (2) Time segments cover clip\u2019s time segments from above segmentation step and the action time segments from ground truth. (3) Captions cover video captions and clip captions. We first utilzie GPT-4V to generate detailed captions for entire videos. However, due to the cost of GPT-4V for generating large-scale, clip-level video captions, we fine tune a Video Recaptioner Model based on the captions generated by GPT-4V, which is then utilized to generate large-scale, clip-level captions. With actions and time segments from ground truth, we integrate clip segments from splitting stage and captions generated by GPT-4V and Video Recaptioner Model to get new, comprehensive annotations. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Step III: formation of downstream tasks. We design various types of instruction-following tasks with different prompts to comprehensively train and evaluate Vid-LLMs. FineAction-CGR encompasses 4 video tasks: (1) captioning, (2) temporal video grounding, (3) temporal video reasoning and (4) multi-turn QA. A few examples from different tasks are shown in Figure 8-9. ", "page_idx": 14}, {"type": "text", "text": "Captioning task needs model to detects what actions occur in given period. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Action recognition: recognize action in level of clip which may contain a kind of action or several kinds of actions. ", "page_idx": 14}, {"type": "text", "text": "Temporal grounding tasks aims to predict the boundary in the video given an instruction in which the start and end time of clip are mentioned to attract model\u2019s attention to target segment. We design two types of question to achieve fine-grained temporal localization. ", "page_idx": 14}, {"type": "text", "text": "\u2022 First/last time grounding: identify the fist/last time a certain action appears. ", "page_idx": 14}, {"type": "text", "text": "Temporal reasoning tasks requires model to understand relationship between actions. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Action sequence reasoning: infer possible action before/after an action over a period of time.   \n\u2022 Count of times: counts times a certain action appears in a clip. ", "page_idx": 14}, {"type": "text", "text": "Multi-turn QA task contains multiple rounds of dialogues involving the above three tasks. The problems are progressive from the entire video to the sub-segment. ", "page_idx": 14}, {"type": "image", "img_path": "FOkKndty5B/tmp/a49585565f10be68440b4ba57ac0e7f111917a820e5628a19bb9df4d1822295b.jpg", "img_caption": ["Figure 6: Video Statistics in FineAction-CGR. It contains a diverse distribution of action types and tasks "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "FOkKndty5B/tmp/b65aacd118ceed19517519b22ac38aa72edadab686642a7885466ad30ba1088e.jpg", "img_caption": ["Figure 7: Distribution of clips number in FineAction-CGR "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "D Data statistics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "After filtering, we obtained 11,188 videos with different number of action types. The distribution is illustrated in Figure 6 left. After splitting, videos are segmented into clips. The amount of videos with different clip number is presented in Figure 7, exhibiting a long-tailed distribution. The largest ", "page_idx": 14}, {"type": "text", "text": "Actions Recognition ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Review the video from 147.347s to 147.647s Can you tell me what the basketball player in white jersey is doing during this time section? ", "page_idx": 15}, {"type": "text", "text": "dunk basketball ", "page_idx": 15}, {"type": "image", "img_path": "FOkKndty5B/tmp/e0e41520530067e25e64a312bc727b978586b846b37a30743128a00b76c50adc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "First/Last Time Grounding ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Identify the first time the athletes dribble basketball in this clip. ", "page_idx": 15}, {"type": "image", "img_path": "FOkKndty5B/tmp/85952d817d973377dadb737e31b34d9ec55de037eead598cf55ac62365dc3c7d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "FOkKndty5B/tmp/f4cc5d26fe3f31323fd13cfe78fa686d7c305a84cb4981f8b1f48315e1c13ab4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "From 36.003s to 37.104s ", "page_idx": 15}, {"type": "text", "text": "Action Sequence Reasoning ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Based on the segment from 36.003s to 42.342s, what is likely to happen after a player dribbles and casts the basketball? ", "page_idx": 15}, {"type": "image", "img_path": "FOkKndty5B/tmp/a5554a578986f87d476852191c32940cd7d0def37fe291111c199a242399f817.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "The player is likely to dunk the basketball. ", "page_idx": 15}, {"type": "text", "text": "Count of Times ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "How many times do the athletic basketball players dunk the basketball from 150.684s to 162.629s in the video? ", "page_idx": 15}, {"type": "image", "img_path": "FOkKndty5B/tmp/b4b5d1cac19571162bc7ea7ead1523e493947d22c15394c4f1af1b75a1704cd8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "FOkKndty5B/tmp/9c29b1736db82bbb118b8177b4b66986805e35e8274b4821d628dd32c04ecc71.jpg", "img_caption": ["Figure 8: Example of different tasks in FineAction-CGR "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Multi-turn QA ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "FOkKndty5B/tmp/0482aea1b9263619e486658e9e45d1ecb971727517bae783603dfcce8e2d2058.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Can you briefly describe the video's content? ", "page_idx": 15}, {"type": "text", "text": "The video captures a high-energy basketball game featuring players in blue and white uniforms. There are also segments showcasing abstract shapes and letters, a dynamic basketball practice session, and a group activity involving athletes in dark blue uniforms. ", "page_idx": 15}, {"type": "image", "img_path": "FOkKndty5B/tmp/a5c0549b049ae7272b5fe4ce4d1170ec2fb46c98bac539606b55c3bcfa3e6506.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "What are the players doing from 36.003s to 146.913s? ", "page_idx": 15}, {"type": "text", "text": "From 36.003s  to 146.913s, the players in team uniforms are diligently involved in a basketball game. They are seen dribbling, passing, and shooting the basketball including performing dynamic slam dunks. Could you tell me when the first slam dunk was performed from 36.003s  to 146.913s? ", "page_idx": 15}, {"type": "image", "img_path": "FOkKndty5B/tmp/54c1d5874632d4b45eab77eecca4a02ce3265c2958f43127dbeebeab18092ee8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "FOkKndty5B/tmp/ceba476c7dc0b37d8fc1f6ddc237969265230e9403e9bfef93a7691822b1c370.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "FOkKndty5B/tmp/78e66335b5bbcfc99c765f80daf5fe2839d9a3f1bf1306a12ff0a197025bd098.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "The first slam dunk was performed from 41.842 s to 42.342s ", "page_idx": 15}, {"type": "text", "text": "What is the last action a player performed from 214.447s to 236.669s? ", "page_idx": 15}, {"type": "image", "img_path": "FOkKndty5B/tmp/4ab7dead1314847aa9a973297e959edcaa80d9fdcd3648fc7b2aaa8989b838df.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "The last action a player performed is casting a basketball ", "page_idx": 15}, {"type": "text", "text": "Between the periods from 36.003s to 146.913s and from 150.684s to 182.649s, which had more slam dunks? ", "page_idx": 15}, {"type": "text", "text": "The period from 36.003s to 146.913s had more slam dunks ", "page_idx": 15}, {"type": "text", "text": "Actions Recognition ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "What were the performers doing from 0s to 35.33s in the video? ", "page_idx": 16}, {"type": "image", "img_path": "FOkKndty5B/tmp/6269fa3bbab466cbf7cf3d874d98461ae3fb140bfe646b7091eb9840a9696e1b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "They were playing the keyboard ", "page_idx": 16}, {"type": "image", "img_path": "FOkKndty5B/tmp/d6747a7bb8635b68225d4d48a4c3f89ceb00e2e89be07f50920f24ef87d9f301.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "First/Last Time Grounding ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "When was the first time one of the people played the keyboard in the performance? ", "page_idx": 16}, {"type": "image", "img_path": "FOkKndty5B/tmp/922e97c0ef4e23709793fc4656607956ae894e2a4bbc402101c00d5bae2999fa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "FOkKndty5B/tmp/b95c33c7e1795568c9e4e1550dec36f4177c9c1d516bed664a04893e404c311f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "From 0s to 35.33s ", "page_idx": 16}, {"type": "text", "text": "Action Sequence Reasoning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Look at this video from 50.33s to 103.66s, if a person starts with playing the keyboard, can you predict what he will do next? ", "page_idx": 16}, {"type": "image", "img_path": "FOkKndty5B/tmp/7d5b5e137ae17a61b470c31237163ccdf58820c8e84ce95e6b4fef00734b2758.jpg", "img_caption": ["Figure 9: Example of different tasks in FineAction-CGR "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "FOkKndty5B/tmp/a0ff58f4350b6ca739a62a7ee9d08e8ede39bdf313e89fef97d7fc662f2c0184.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "FOkKndty5B/tmp/8c68b525f12af4cbbb9ee03e1e64d2ecf0f740768086e5782454a244cfd9c46f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "After playing the keyboard, the individual switches to playing the drums. ", "page_idx": 16}, {"type": "text", "text": "Count of Times ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "How many times does one individual play the keyboard from 0s to 142.933s in the video? ", "page_idx": 16}, {"type": "image", "img_path": "FOkKndty5B/tmp/9bc53953f0f870dce35673625faf4289f72e38dbe80437adabd4f42931ca14f7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Multi-turn QA ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Can you describe the event showed in this video? ", "page_idx": 16}, {"type": "text", "text": "Two people are performing music on a stage. One is playing the keyboard and the other one is playing the drums. ", "page_idx": 16}, {"type": "text", "text": "The keyboard playing occurs from 0s to 35.33s, from 50.33s to 57.66s, and from 104.33s to 141.66s When does the first scene of playing the drums take place from 0s to 142.933s? ", "page_idx": 16}, {"type": "text", "text": "The first scene of playing the drums takes place from 57.66s to 103.66s What is the first action that occurs in the time period from 0s to 142.933s? ", "page_idx": 16}, {"type": "text", "text": "Typically, playing the drums follows after the keyboard performance clip number is more than 68. The horizontal coordinate in the figure is only displayed to 35. After generation step, we obtain 131,984 pairs of QA. The distribution in different tasks are illustrated in Figure 6 right. Explanation of the legend 1) \u2019ar\u2019: Captioning task where the model needs to recognize a single action in a clip, 2) \u2019asr\u2019: Captioning task where the model needs to recognize multiple actions in a clip, 3) \u20191st\u2019: Temporal grounding task where the model needs to localize the first/last occurrence of a given action in a clip, 4) \u2019sequence\u2019: Temporal reasoning task where the model needs to identify what action happens before or after a given action in a clip, 5) \u2019times\u2019: Temporal reasoning task where the model needs to count the occurrences of a given action in a clip, 6) \u2019mtqa\u2019: Multi-turn QA task where the model engages in a multi-turn question and answer dialogue about the entire video. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "E Prompt ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Prompts used for generation of different kinds of instruction data are shown in Figure 10-13. Due to page length constraints, we have omitted some in-context examples in certain tasks. ", "page_idx": 17}, {"type": "text", "text": "F Broader impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The proposed SlowFocus has the potential to greatly enhance the capability of Video LLMs for video content analysis. Therefore it could be beneficial for applications such as video analytics, surveillance and automated content moderation. It could also be used in educational settings to analyze educational videos to provide a better understanding of complex contents in videos for educational purposes. ", "page_idx": 17}, {"type": "text", "text": "As for the potential negative impacts, there is a risk that the technology could be misused to analyze private videos and spreading disinformation. ", "page_idx": 17}, {"type": "text", "text": "Prompts for Action Recognition ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Generate a concise dialogue  with  questions and answers based on the following video caption, clip captions, action label and time segments with sub segments. All the sub-segments in a segment represent all the time periods in which an action occurs. Below are a few examples: ", "page_idx": 18}, {"type": "text", "text": "<Example 1> ", "page_idx": 18}, {"type": "text", "text": "[Video Caption] ", "page_idx": 18}, {"type": "text", "text": "{A spirited basketball game unfolds on an indoor court, where teams in contrasting uniforms compete, witnessed by an excited audience under the glare of bright lights.} ", "page_idx": 18}, {"type": "text", "text": "[Time] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "[\"0:00:00.000\", \"0:00:20.520\"] ", "page_idx": 18}, {"type": "text", "text": "[Clip Caption] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "{Players in yellow and purple jerseys engage in an intense basketball game, running, passing, and dribbling on a well-lit court, captured in a wide-angle static shot.} ", "page_idx": 18}, {"type": "text", "text": "[Action Label] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "dribble basketball ", "page_idx": 18}, {"type": "text", "text": "[Time Segments with Sub-segments] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "[\"0:00:00.000\", \"0:00:20.520\"]: {[5.014, 5.983],  [6.953, 7.989], [9.025, 9.794],  [10.997, 11.966], [15.008, 16.245], [17.382, 19.454]} ", "page_idx": 18}, {"type": "text", "text": "[Q & A] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "[{\"from\":\"human\",   \n\"value\":\"Look at from 15.008 to 16.245 of the video. What the player is doing?\"},   \n{\"from\":\"gpt\",   \n\"value\":\"dribble basketball\"}] ", "page_idx": 18}, {"type": "text", "text": "Now given the following video caption, clip captions, action label and time segments with sub segments, please generate a question to test whether the deep learning model can give an action label based on the information provided by asking a question like \"What does the person do from a to b in this video?\" or \"What do the people do from c to d in this video?\". ", "page_idx": 18}, {"type": "text", "text": "{video_caption}   \n{time_segment}   \n{clip_caption}   \n{action_label}   \n{time_segment : sub_segments} ", "page_idx": 18}, {"type": "text", "text": "The people mentioned must be from [Video Caption] and [Clip Caption]. And the time mentioned should refer to [Time]. The answer must be [Action Label]. ", "page_idx": 18}, {"type": "text", "text": "Just return the list as your response, like following format: ", "page_idx": 18}, {"type": "text", "text": "[{\"from\":\"human\", \"value\":\"... from 0 to 20 ...\"}, {\"from\":\"gpt\", \"value\":\"...\"}]   \nJust return the list. ", "page_idx": 18}, {"type": "text", "text": "Prompts for Temporal Grounding ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Generate a concise dialogue with questions, answers based on the following clip caption, action labels and their time sgements. The action labels are in chronological order. Below are a few examples: ", "page_idx": 19}, {"type": "text", "text": "<Example 1> ", "page_idx": 19}, {"type": "text", "text": "[Action Label] ", "page_idx": 19}, {"type": "text", "text": "dribble basketball, cast basketball, dunk basketball ", "page_idx": 19}, {"type": "text", "text": "[Clip Caption and Time Segment] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "[\"0:00:00.000\", \"0:00:20.520\"] Players in yellow and purple jerseys engage in an intense basketball game, running, passing, and dribbling on a well-lit court, captured in a wide-angle static shot. ", "page_idx": 19}, {"type": "text", "text": "[Action Labels and Time Segments] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "[0.33, 1.301] dribble basketball, [1.668, 2.636] cast basketball, [4.33, 4.638] dribble basketball, [5.0, 5.639] cast basketball, [11.011, 12.679] dribble basketball, [13.013, 13.981] dunk basketball ", "page_idx": 19}, {"type": "text", "text": "[Q & A] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "[{\"from\":\"human\", \"value\":\"Identify the first time the player dribbles basketball in this clip.\"}, {\"from\":\"gpt\", \"value\":\"from 0.33 to 1.301\"},{\"from\":\"human\", \"value\":\"Identify the last time a player casts basketball in this clip.\"},   \n{\"from\":\"gpt\", \"value\":\"from 5 to 5.639\"}] ", "page_idx": 19}, {"type": "text", "text": "Now given the following segment caption, action labels and their time sgements, please generate questions and their answers, please generate three questions related to time and action to test whether the deep learning model can recognition fine-grained action  based on the information provided by asking question like \"identify the first/last time the person/people does/do a certain action during this clip.\" ", "page_idx": 19}, {"type": "text", "text": "{action_label} {clip_caption} : {time_segment} {actions_and_segments_list} ", "page_idx": 19}, {"type": "text", "text": "Don't mention time segments in questions. The people/person in questions must refer to [Clip Caption]. The actions mentioned in the questions are selected from [Action Label]. The answers should be directly inferred from [Action Labels and Time Segments]. Find the time period of the first/last occurrence of an action based on the time given in the question. Keep the answers brief, with no more than 4 words each. ", "page_idx": 19}, {"type": "text", "text": "Just return the list as your response, the format must be like following example: ", "page_idx": 19}, {"type": "text", "text": "[{\"from\":\"human\", \"value\":\"...\"},   \n{\"from\":\"gpt\", \"value\":\"from $<\\!\\mathrm{s}3\\!>$ to $<\\!\\mathrm{e}3\\!>$ \"},{\"from\":\"human\", \"value\":\"...\"}, {\"from\":\"gpt\", \"value\":\"from $\\mathsf{<}\\mathsf{s}4\\mathsf{>}$ to ${\\tt c e4>}$ \"},{\"from\":\"human\", \"value\":\"...\"}, {\"from\":\"gpt\", \"value\":\"from $\\mathrm{<s5>}$ to <e5>\"}] ", "page_idx": 19}, {"type": "text", "text": "The value from 'human' is question. The value from 'gpt' is answer. Just return the list. ", "page_idx": 19}, {"type": "text", "text": "Prompts for Temporal Reasoning ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Generate a concise dialogue  with questions and answers based on the following clip caption, action labels and time sgements. The action labels are in chronological order. Below is an example: ", "page_idx": 20}, {"type": "text", "text": "<Example $1>$ ", "page_idx": 20}, {"type": "text", "text": "[Clip Caption and Time Segment] ", "page_idx": 20}, {"type": "text", "text": "[\"0:00:00.000\", \"0:00:20.520\"] Players in yellow and purple jerseys engage in an intense basketball game, running, passing, and dribbling on a well-lit court, captured in a wide-angle static shot. ", "page_idx": 20}, {"type": "text", "text": "[Action Label] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "dribble basketball, cast basketball, dunk basketball ", "page_idx": 20}, {"type": "text", "text": "[Action Labels and Time Segments] ", "page_idx": 20}, {"type": "text", "text": "[0.33, 1.301] dribble basketball, ", "page_idx": 20}, {"type": "text", "text": "[13.013, 13.981] dunk basketball ", "page_idx": 20}, {"type": "text", "text": "[Q & A] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "[{\"from\":\"human\", \"value\":\"infer from 0.33 to 2.636, what the player would most possibly do after dribble basketball?\"},{\"from\":\"gpt\", \"value\":\"Cast basketball\"}, ", "page_idx": 20}, {"type": "text", "text": "{\"from\":\"human\", \"value\":\"Look at this clip from 0 to 20, what players usually do in a basketball game?\"},{\"from\":\"gpt\", \"value\":\"Players in a basketball game usually dribble basketball and cast basketball.\"}, ", "page_idx": 20}, {"type": "text", "text": "{\"from\":\"human\", \"value\":\"Look at this clip from 11.011 to 13.981, a player dribbles basketball. Guess what would he do after that?\"},{\"from\":\"gpt\", \"value\":\"The player may dunk basketball.\"}, {\"answer_segment\":[[1.668, 2.636],[0,20],[13.013.13.981]]}] ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Now given the following clip caption, action labels and time segments, please generate questions and their answers, please generate three questions related to time and action to test whether the deep learning model can recognition fine-grained action  based on the information provided by asking question like \"Based on one sub-segment of this segment and infer what will most possibly happen after an action.\" or \"Look this video from a to b, What's the most possible action before/after this step?\" ", "page_idx": 20}, {"type": "text", "text": "time_segment: clip_caption ", "page_idx": 20}, {"type": "text", "text": "action_list ", "page_idx": 20}, {"type": "text", "text": "actions_segments_list ", "page_idx": 20}, {"type": "text", "text": "The actions mentioned in the questions are selected from [Action Label]. The answers should be inferred from the provided time and action sequence in [Action Labels and Time Segments]. ", "page_idx": 20}, {"type": "text", "text": "Just return the list as your response, please follow below format: ", "page_idx": 20}, {"type": "text", "text": "[{\"from\":\"human\", \"value\":\" ...\"},{\"from\":\"gpt\", \"value\":\"...\"},{\"answer_segment\": $[<\\!\\mathrm{s}3>,\\allowbreak<\\!\\mathrm{e}3\\!>\\!]\\}]$ ", "page_idx": 20}, {"type": "text", "text": "The value from 'human' is question. The value from 'gpt' is answer. The time in every question must be in format 'from ... to ...' and be the maximum period containing question's and answer's actions' but not others. \"answer_segment\" is a list of time segments pointing to minimum time priod of the answer's action and each is a subset of time in its corrresponding question. Dont't write what you imagine. Just return the list. ", "page_idx": 20}, {"type": "text", "text": "Prompts for Multi-turn QA ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "You are an excellent assistant for generating multi-round conversations. I need you to generate a dialogue with multi-turn questions and answers based on the information of a video. The information is a complete annotation of a video which contains one or more clips in chronological order. It mainly has captions, time segments and actions. Below is an example: ", "page_idx": 21}, {"type": "text", "text": "<Example 1> ", "page_idx": 21}, {"type": "text", "text": "<Information> ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "{\u201csummary\u201d: \u201cA fast-paced basketball game \u2026 \u201d, \u201caction_list\u201d: [\u201ccast basketball\u201d, .. ],   \n\u201cmeta\u201d: [{   \u201cclipid\u201d: 0,  \u201cclip_caption\u201d: \u201cAn intense basketball game .. indoor court ..\u201d, \u201cclip_action\u201d: [\u2026], \u201ctime_segment\u201d: [\u2026], \u201cannotations\u201d: [{\u201csegment\u201d: [\u2026], \u201clabel\u201d: \u201c\u2026\u201d},\u2026]} .. ]} ", "page_idx": 21}, {"type": "text", "text": "[Q & A] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "[{\"from\" $\\because$ \"human\", \"value\":\"Can you briefly describe the video's content?\"}, {\"from\":\"gpt\",   \n\"value\":\"The well-lit indoor arena hosts an energetic basketball game...\"},   \n{\"from\":\"human\", \"value\":\"What the players is doing from 0 to 11.878?\"}, {\"from\":\"gpt\",   \n\"value\":\"The players are competing. They dribble and cast basketball on the court.\"},   \n{\"from\":\"human\", \"value\":\"Please capture the first time a player is dribbling basketball from 0 to   \n11.878.\"}, {\"from\":\"gpt\", \"value\":\"from 0.033 to 1.969.\"},   \n{\"from\":\"human\", \"value\":\"What is the last action before dunking basketball from 52.686 to   \n52.986?\"}, {\"from\":\"gpt\", \"value\":\"An player casts basketball.\"},   \n{\"from\":\"human\", \"value\":\"Please infer what players usually do in a basketball game.\"},{\"from\":\"gpt\"   \n\"value\":\"They usually dribble basketball or cast basketball.\"}, \"task\":['cp','cp','grd','rsn','rsn']] ", "page_idx": 21}, {"type": "text", "text": "Now given the following <Information>, please generate a dialogue with five pairs of questions and answers to test whether the deep learning model has capability of captioning, temporal grounding and especially temporal reasoning based on the information provided. ", "page_idx": 21}, {"type": "text", "text": "Captioning task contains video captioning, clip captioning and action recognition during a time peiord. Temporal grounding task needs to find out target time period according to requirements whose answer is a time period. Temporal reasoning task needs to logically infer behavior motive from video, such as time sequence between actions, action sequence in an activity or reasons why people in video do such an action. Questions must be designed according to information provided and must contain all three tasks. You can design temporal reasoning questions different from examples I give you, but don't write what you imagine. ", "page_idx": 21}, {"type": "text", "text": "{all_information} ", "page_idx": 21}, {"type": "text", "text": "Just return the list as your response, following is format example: ", "page_idx": 21}, {"type": "text", "text": "[{\"from\":\"human\", \"value\":\"Can you briefly describe ...?\"},{\"from\":\"gpt\", \"value\":\"...\"}, {\"from\":\"human\", \"value\":\"... from ... to ...?\"},{\"from\":\"gpt\", \"value\":\"...\"}, {\"from\":\"human\", \"value\":\"... from ... to ....\"},{\"from\":\"gpt\", \"value\":\"from ... to ... .\"}, {\"from\":\"human\", \"value\":\"... from ... to ...?\"},{\"from\":\"gpt\", \"value\":\"...\"}, {\"from\":\"human\", \"value\":\"...\"},{\"from\":\"gpt\", \"value\":\"...\"},\"task\":['..','..','..','..','..']] The value from 'human' is question. The value from 'gpt' is answer. 'task' is a list of string in which 'cp means captioning, 'grd' means grounding, 'rsn' means reasoning. The length of 'task' list are equal to the number of questions. Just return the list. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We make the main claims clearly in the abstract and introduction. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the limitation of this paper in Section 6. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We discuss the experimental setup in Section 5 ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: Code and benchmark would be released after submission and review. It does include a new open-source benchmark. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Details of training and testing are specified in Section 5. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: Experiments in this paper are conducted in multiple iterations, yielding stable results. We reported the average of these results for consistency and reliability. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The computational cost and resources are specified in Section 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research conducted in this paper conforms in every respect with the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The Broader Impacts are discussed in Appendix F. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: We follow the safeguards of base LLM Vicuna-7B v1.5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: All models and datasets used in this paper have been properly credited. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Benchmark would be released after submission and review. The paper does provide well-structured details of the dataset. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]