{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper provides a comprehensive technical report on GPT-4, a highly influential large language model, offering insights into its architecture and capabilities, which are relevant to the current paper's exploration of LLMs for long-context tasks."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This foundational paper established the surprising few-shot learning capabilities of large language models, directly impacting the current research on efficient long-context processing."}, {"fullname_first_author": "Yushi Bai", "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding", "publication_date": "2023-08-14", "reason": "This paper introduces LongBench, a comprehensive benchmark dataset specifically designed for evaluating long-context understanding capabilities in LLMs, providing a crucial evaluation framework for the current work."}, {"fullname_first_author": "Rohan Anil", "paper_title": "PaLM 2 technical report", "publication_date": "2023-05-10", "reason": "This paper offers a detailed technical description of PaLM 2, a leading LLM used in the experiments, giving valuable context to the model's performance within the chain-of-agents framework."}, {"fullname_first_author": "AI Anthropic", "paper_title": "The claude 3 model family: Opus, sonnet, haiku", "publication_date": "2024-01-01", "reason": "This paper introduces the Claude 3 family of LLMs, another prominent model used in the experiments, providing essential background on its architecture and capabilities, which are key to evaluating performance within the proposed framework."}]}