[{"figure_path": "1vPqOmqSfO/figures/figures_1_1.jpg", "caption": "Figure 1: OoD detection performance () on a ResNet.", "description": "The figure shows the AUROC (Area Under the Receiver Operating Characteristic curve) scores for out-of-distribution (OoD) detection on a ResNet model.  It compares the performance of different uncertainty quantification methods: Randomized SVD, Vanilla Lanczos, and Sketched Lanczos (with two different sketch sizes). The x-axis represents the memory cost in the number of neural networks, and the y-axis shows the AUROC score. The results indicate that Sketched Lanczos, particularly with a larger sketch size (s=100000), achieves higher AUROC scores while using less memory compared to the other methods. This demonstrates the memory efficiency and effectiveness of the proposed Sketched Lanczos algorithm for uncertainty quantification.", "section": "1 Introduction"}, {"figure_path": "1vPqOmqSfO/figures/figures_2_1.jpg", "caption": "Figure 2: GGN eigenvalues exponential decay. Average and standard deviation over 5 seeds. Details are in Appendix C.1.", "description": "This figure shows the exponential decay of Generalized Gauss-Newton (GGN) matrix eigenvalues for three different neural network architectures trained on different datasets. The x-axis represents the index of the eigenvalue, and the y-axis represents the eigenvalue value.  The figure demonstrates that the eigenvalues decay rapidly, supporting the use of low-rank approximations of the GGN matrix.  The standard deviation is shown for each eigenvalue across 5 different random seeds, indicating the stability of the observed decay.", "section": "2 Background"}, {"figure_path": "1vPqOmqSfO/figures/figures_8_1.jpg", "caption": "Figure 3: Sketch sizes s comparison for: LeNet p = 40K on FASHIONMNIST vs MNIST (left), ResNet p = 300K on CIFAR-10 vs CIFAR-corrupted with defocus blur (center), and VisualAttentionNet p = 4M on CELEBA vs FOOD101 (right). The lower the ratio s/p, the stronger the memory efficiency.", "description": "This figure compares the performance of the Sketched Lanczos algorithm with different sketch sizes (s) on three different model architectures (LeNet, ResNet, VisualAttentionNet) and datasets.  It demonstrates how the memory efficiency of Sketched Lanczos improves as the ratio of sketch size to the number of model parameters (s/p) decreases. Each subfigure displays the AUROC (Area Under the Receiver Operating Characteristic curve) for out-of-distribution (OoD) detection as a function of memory cost (in the number of neural networks).", "section": "5 Experiments"}, {"figure_path": "1vPqOmqSfO/figures/figures_9_1.jpg", "caption": "Figure 3: Sketch sizes s comparison for: LeNet p = 40K on FASHIONMNIST vs MNIST (left), ResNet p = 300K on CIFAR-10 vs CIFAR-corrupted with defocus blur (center), and VisualAttentionNet p = 4M on CELEBA vs FOOD101 (right). The lower the ratio s/p, the stronger the memory efficiency.", "description": "This figure compares the performance of SKETCHED LANCZOS with different sketch sizes (s) for three different model architectures and datasets.  The x-axis represents the memory cost in the number of neural networks, and the y-axis shows the AUROC (Area Under the Receiver Operating Characteristic curve), a measure of the model's ability to distinguish between in-distribution and out-of-distribution samples. The figure demonstrates that smaller sketch sizes lead to more memory-efficient uncertainty quantification. The lower the ratio of sketch size (s) to the number of parameters (p), the better the memory efficiency, meaning less memory is used to achieve the same uncertainty prediction quality.", "section": "5 Experiments"}, {"figure_path": "1vPqOmqSfO/figures/figures_17_1.jpg", "caption": "Figure 5: We study the GGN of a LeNet model with 44,000 parameters trained on MNIST. We run 40 iterations of hi-memory Lanczos and low-memory Lanczos. Let H = [H1|...|H40], \u039bH, L = [L1|...|L40], and AL be the eigenvectors and eigenvalues computed by the two algorithms respectively. We sort both sets of eigenvectors in decreasing order of corresponding eigenvalues. In position (i, j) we plot (Hi, Lj). It is apparent that multiple eigenvectors Lj correspond to the same eigenvector Hi.", "description": "This figure shows the comparison between eigenvectors obtained from hi-memory Lanczos and low-memory Lanczos on the Generalized Gauss-Newton (GGN) matrix of a LeNet model trained on MNIST.  The plot visualizes the dot product between eigenvectors from each method, revealing that several low-memory Lanczos eigenvectors correspond to the same high-memory Lanczos eigenvector. This observation supports the idea that low-memory Lanczos, despite its lack of strict orthogonality, provides a good approximation of the top eigenvectors.", "section": "C Lanczos algorithm"}, {"figure_path": "1vPqOmqSfO/figures/figures_18_1.jpg", "caption": "Figure 6: Eigenspectrum obtained from hi-memory Lanczos on: ResNet model (p = 300K) trained on CIFAR-10 (top), LeNet model (p = 40K) trained on FashionMNIST (middle) and MLP model (p = 20K) trained on MNIST (bottom). Standard deviations over 5 Lanczos random seeds.", "description": "This figure shows the eigenspectrum obtained from the hi-memory Lanczos algorithm for three different neural network models trained on different datasets.  The top panel shows results for a ResNet model trained on CIFAR-10, the middle panel for a LeNet model trained on FashionMNIST, and the bottom panel for an MLP model trained on MNIST. The plot displays the eigenvalues (y-axis) against their index (x-axis), illustrating the distribution of eigenvalues. Standard deviations are shown across five different random initializations of the Lanczos algorithm, indicating variability in the results. The rapid decay of eigenvalues suggests that the GGN matrix can be well approximated by a low-rank matrix.", "section": "C Lanczos algorithm"}, {"figure_path": "1vPqOmqSfO/figures/figures_22_1.jpg", "caption": "Figure 3: Sketch sizes s comparison for: LeNet p = 40K on FASHIONMNIST vs MNIST (left), ResNet p = 300K on CIFAR-10 vs CIFAR-corrupted with defocus blur (center), and VisualAttentionNet p = 4M on CELEBA vs FOOD101 (right). The lower the ratio s/p, the stronger the memory efficiency.", "description": "This figure compares the performance of the Sketched Lanczos algorithm with different sketch sizes (s) across three different model architectures (LeNet, ResNet, and VisualAttentionNet) and datasets.  It shows that reducing the ratio of sketch size to the number of parameters (s/p) improves the memory efficiency of the algorithm while maintaining reasonable performance.  The lower the s/p ratio, the better the memory efficiency.", "section": "Experiments"}, {"figure_path": "1vPqOmqSfO/figures/figures_23_1.jpg", "caption": "Figure 8: Preconditioning comparison for LeNet p = 40K on FASHIONMNIST vs FASHIONMNIST-rotated-30\u00b0 (left), and ResNet p = 300K on CIFAR-10 vs SVHN (center) or CIFAR-pixelated (right).", "description": "This figure compares the performance of vanilla Lanczos, sketched Lanczos, and sketched Lanczos with preconditioning on three different datasets. The x-axis represents the memory cost in the number of neural networks. The y-axis represents the AUROC (Area Under the Receiver Operating Characteristic curve), a measure of the accuracy of out-of-distribution detection.  The figure demonstrates that preconditioning, a technique that improves the numerical stability of the Lanczos algorithm, significantly enhances the memory efficiency of sketched Lanczos, allowing for higher-rank approximations within the same memory budget.", "section": "D.3.2 Effect of different preconditioning sizes"}, {"figure_path": "1vPqOmqSfO/figures/figures_24_1.jpg", "caption": "Figure 3: Sketch sizes s comparison for: LeNet p = 40K on FASHIONMNIST vs MNIST (left), ResNet p = 300K on CIFAR-10 vs CIFAR-corrupted with defocus blur (center), and VisualAttentionNet p = 4M on CELEBA vs FOOD101 (right). The lower the ratio s/p, the stronger the memory efficiency.", "description": "This figure compares the performance of SKETCHED LANCZOS with different sketch sizes (s) for three different neural network architectures (LeNet, ResNet, VisualAttentionNet) on out-of-distribution (OoD) detection tasks.  The x-axis represents the memory cost in the number of neural networks, and the y-axis represents the AUROC score.  The results demonstrate that smaller s/p ratios lead to greater memory efficiency.  Each plot shows the performance with varying sketch sizes and highlights the trade-off between accuracy and memory usage.", "section": "5 Experiments"}]