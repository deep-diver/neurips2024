[{"figure_path": "cRlQHncjwT/tables/tables_2_1.jpg", "caption": "Table 1: Left: domain circgauss. Center: density learned by a generative forest (GF) consisting of a single tree, boosted for a small number (50) of iterations. Right: density learned by a GF consisting of 50 boosted tree stumps (Center and Right learned using GF.BOOST). In a domain X with dimension d, a single tree with n splits can only partition the domain in n + 1 parts. On the other hand, a set of n stumps in a GF can boost this number to n\u00d1(d) (the tilda omits log dependencies), which explains why the right density appears so much better than the central one, even when each tree is just a stump.", "description": "This table compares the density learned by three different generative forest models on the circgauss domain. The first model uses a single tree boosted for 50 iterations, the second model uses 50 boosted tree stumps, and the third model is the ground truth. The table shows that a set of tree stumps can learn a much more accurate density than a single tree, even if each tree in the set of stumps is very simple. This is because the set of stumps can partition the domain into many more parts than a single tree.", "section": "Generative forests: models and data generation"}, {"figure_path": "cRlQHncjwT/tables/tables_3_1.jpg", "caption": "Table 1: Left: domain circgauss. Center: density learned by a generative forest (GF) consisting of a single tree, boosted for a small number (50) of iterations. Right: density learned by a GF consisting of 50 boosted tree stumps (Center and Right learned using GF.BOOST). In a domain X with dimension d, a single tree with n splits can only partition the domain in n + 1 parts. On the other hand, a set of n stumps in a GF can boost this number to n^\u00d5(d) (the tilda omits log dependencies), which explains why the right density appears so much better than the central one, even when each tree is just a stump.", "description": "This table compares the density learned by generative forests with different structures.  The leftmost image shows the original data distribution (circgauss). The center image shows the density learned by a generative forest (GF) using a single tree boosted for 50 iterations. The rightmost image displays the density learned using a GF of 50 boosted tree stumps, trained with GF.BOOST. The table highlights the advantage of using multiple stumps over a single tree, even when the trees are simple stumps, in terms of better approximating the underlying data density.", "section": "Generative forests: models and data generation"}, {"figure_path": "cRlQHncjwT/tables/tables_4_1.jpg", "caption": "Table 1: Left: domain circgauss. Center: density learned by a generative forest (GF) consisting of a single tree, boosted for a small number (50) of iterations. Right: density learned by a GF consisting of 50 boosted tree stumps (Center and Right learned using GF.BOOST). In a domain X with dimension d, a single tree with n splits can only partition the domain in n + 1 parts. On the other hand, a set of n stumps in a GF can boost this number to n~(d) (the tilda omits log dependencies), which explains why the right density appears so much better than the central one, even when each tree is just a stump.", "description": "This table showcases the density learned by generative forests with different configurations.  The leftmost image displays the original data distribution (circgauss). The center image shows the density learned by a generative forest consisting of a single tree after 50 iterations of boosting. The rightmost image presents the density learned by a generative forest composed of 50 boosted tree stumps. The table highlights the improved performance of the model when using multiple stumps compared to a single tree, demonstrating the combinatorial advantage of the generative forest approach.", "section": "Generative forests: models and data generation"}, {"figure_path": "cRlQHncjwT/tables/tables_5_1.jpg", "caption": "Table 1: Left: domain circgauss. Center: density learned by a generative forest (GF) consisting of a single tree, boosted for a small number (50) of iterations. Right: density learned by a GF consisting of 50 boosted tree stumps (Center and Right learned using GF.BOOST). In a domain X with dimension d, a single tree with n splits can only partition the domain in n + 1 parts. On the other hand, a set of n stumps in a GF can boost this number to n\u00d1(d) (the tilda omits log dependencies), which explains why the right density appears so much better than the central one, even when each tree is just a stump.", "description": "This table compares the density learned by generative forests with different structures.  It shows a single tree boosted for 50 iterations versus a generative forest of 50 tree stumps, both trained using the GF.BOOST algorithm. The key takeaway is that the generative forest with multiple stumps (a simpler model) achieves a significantly more accurate density estimation compared to a single, more complex tree, highlighting the power of the proposed method.", "section": "Generative forests: models and data generation"}, {"figure_path": "cRlQHncjwT/tables/tables_9_1.jpg", "caption": "Table 5: LIFELIKE: comparison of Generative Forests (us, GF), with T = 500 trees, J =2000 total splits to contenders: Adversarial Random Forests (ARF, 200 trees [44]), CT-GAN (CT, 1000 epochs [47]), Forest Flows (FF, 50 trees [17]) and Vine copulas autoencoders (VCAE, VC-* where *=G for Gaussian, D for Direct and R for Regular [41]). Metrics used are regularized OT (Sinkhorn), Coverage, Density and F1 measure. Values shown = average over the 5-folds + std dev.. The best average for us vs contender is shown with a star \"*\". Convention for p-values: computed using a Student paired t-test; if p < 0.01, value is replaced by  (we beat the contender) or  (the contender beats us). See text for details.", "description": "This table presents a comparison of the performance of Generative Forests against four other generative models on several datasets.  The metrics used include Sinkhorn distance (a measure of the difference between probability distributions), coverage and density (measures of how well the generated data covers the real data), and the F1-score (a classification metric showing the accuracy of distinguishing between real and generated data). The results are averaged over 5-fold cross-validation, and the best-performing model for each metric is indicated with a star.  P-values from paired t-tests indicate statistical significance of differences.", "section": "Experiments"}, {"figure_path": "cRlQHncjwT/tables/tables_15_1.jpg", "caption": "Table A1: Public domains considered in our experiments (m = total number of examples, d = number of features), ordered in increasing m \u00d7 d. \"Cat.\" is a shorthand for categorical (nominal / ordinal / binary); \"Num.\" stands for numerical (integers / reals). (*) = simulated, URL for OpenML: https://www.openml.org/search?type=data&sort=runs&id=504&status=active; SOP = Stanford Open Policing project, https://openpolicing.stanford.edu/ (see text). \"Tag\" refers to tag names used in Tables A7 and A8.", "description": "This table lists the 21 public and simulated datasets used in the experiments, including the source, size, number of categorical and numerical features, and presence of missing data.  A tag is assigned to each dataset for easier referencing in other tables.", "section": "V.1 Domains"}, {"figure_path": "cRlQHncjwT/tables/tables_15_2.jpg", "caption": "Table 4: IMPUTE: results on two domains (left and right pane). Since both domain contain categorical and numerical variables, we compute for each perr (categorical variables) and the rmse (numerical variables). In each plot, we display both the corresponding results of GF.BOOST, the results of MICE.RF (indicating the total number of trees used to impute the dataset) and the result of a fast imputation baseline using the marginals computed from the training sample. The x axis displays the number of trees T in GF.BOOST and each curve is displayed with its average + standard deviation in shaded color. The result for T = 1 equivalently indicates the performance of a single generative tree (GT) with J splits [31], shown with an arrow (see text).", "description": "This table presents the results of missing data imputation experiments on two datasets, comparing the performance of Generative Forests (GF), MICE with random forests (MICE.RF), and a simple marginal imputation baseline.  The results are displayed in terms of prediction error rate (perr) for categorical variables and root mean squared error (rmse) for numerical variables.  The number of trees (T) in the GF model is varied, showing the effect on imputation accuracy.", "section": "IMPUTE"}, {"figure_path": "cRlQHncjwT/tables/tables_16_1.jpg", "caption": "Table A1: Public domains considered in our experiments (m = total number of examples, d = number of features), ordered in increasing m \u00d7 d. \"Cat.\" is a shorthand for categorical (nominal / ordinal / binary); \"Num.\" stands for numerical (integers / reals). (*) = simulated, URL for OpenML: https://www.openml.org/search?type=data&sort=runs&id=504&status=active; SOP = Stanford Open Policing project, https://openpolicing.stanford.edu/ (see text). \"Tag\" refers to tag names used in Tables A7 and A8.", "description": "This table lists the 21 public and simulated datasets used in the paper's experiments.  For each dataset, it provides the source, whether missing data was present, the total number of examples (m), the number of features (d), and a breakdown of categorical and numerical features.  The table also includes a tag used to reference the dataset in subsequent tables.", "section": "V.1 Domains"}, {"figure_path": "cRlQHncjwT/tables/tables_24_1.jpg", "caption": "Table A1: Public domains considered in our experiments (m = total number of examples, d = number of features), ordered in increasing m \u00d7 d. \"Cat.\" is a shorthand for categorical (nominal / ordinal / binary); \"Num.\" stands for numerical (integers / reals). (*) = simulated, URL for OpenML: https://www.openml.org/search?type=data&sort=runs&id=504&status=active; SOP = Stanford Open Policing project, https://openpolicing.stanford.edu/ (see text). \"Tag\" refers to tag names used in Tables A7 and A8.", "description": "This table lists the 21 public and simulated datasets used in the experiments.  Each dataset is described by its name, a short tag, source, whether missing data is present, the number of examples (m), the number of features (d), and the count of categorical and numerical features.", "section": "V.1 Domains"}, {"figure_path": "cRlQHncjwT/tables/tables_30_1.jpg", "caption": "Table 5: LIFELIKE: comparison of Generative Forests (us, GF), with T = 500 trees, J =2000 total splits to contenders: Adversarial Random Forests (ARF, 200 trees [44]), CT-GAN (CT, 1 000 epochs [47]), Forest Flows (FF, 50 trees [17]) and Vine copulas autoencoders (VCAE, VC-* where *=G for Gaussian, D for Direct and R for Regular [41]). Metrics used are regularized OT (Sinkhorn), Coverage, Density and F1 measure. Values shown = average over the 5-folds + std dev.. The best average for us vs contender is shown with a star \"*\". Convention for p-values: computed using a Student pared t-test; if p < 0.01, value is replaced by  (we beat the contender) or  (the contender beats us). See text for details.", "description": "This table compares the performance of Generative Forests (GF) against four other generative models (ARF, CT-GAN, ForestFlow, and VCAE) across multiple datasets.  The metrics used are Sinkhorn distance (a measure of distance between distributions), coverage, density, and F1 score. The results indicate the average performance across 5 folds of stratified cross validation, along with standard deviations.  Statistically significant differences (p<0.01) between GF and the other methods are highlighted.", "section": "Experiments"}, {"figure_path": "cRlQHncjwT/tables/tables_31_1.jpg", "caption": "Table A7: LIFELIKE: comparison of Generative Forests (us, GF), with T = 500 trees, J =2000 total splits to contenders: Adversarial Random Forests (ARF, 200 trees [44]), CT-GAN (CT, 1000 epochs [47]), Forest Flows (FF, 50 trees [17]) and Vine copulas autoencoders (VCAE, VC-* where *=G for Gaussian, D for Direct and R for Regular [41]). Metrics used are regularized OT (Sinkhorn), Coverage, Density and F1 measure. Values shown = average over the 5-folds + std dev.. The best average for us vs contender is shown with a star \"*\". Convention for p-values: computed using a Student pared t-test; if p < 0.01, value is replaced by (we beat the contender) or (the contender beats us). See text for details.", "description": "This table presents the results of the LIFELIKE experiment, comparing the performance of Generative Forests against several state-of-the-art generative models on various metrics such as Sinkhorn distance, coverage, density, and F1 measure.  The results are averages over five stratified folds, with the best performance indicated by a star.  P-values from paired t-tests indicate statistical significance.", "section": "Experiments"}, {"figure_path": "cRlQHncjwT/tables/tables_33_1.jpg", "caption": "Table 5: LIFELIKE: comparison of Generative Forests (us, GF), with T = 500 trees, J =2000 total splits to contenders: Adversarial Random Forests (ARF, 200 trees [44]), CT-GAN (CT, 1000 epochs [47]), Forest Flows (FF, 50 trees [17]) and Vine copulas autoencoders (VCAE, VC-* where *=G for Gaussian, D for Direct and R for Regular [41]). Metrics used are regularized OT (Sinkhorn), Coverage, Density and F1 measure. Values shown = average over the 5-folds + std dev.. The best average for us vs contender is shown with a star \"*\". Convention for p-values: computed using a Student paired t-test; if p < 0.01, value is replaced by (we beat the contender) or (the contender beats us). See text for details.", "description": "This table compares the performance of Generative Forests against several state-of-the-art generative models on various datasets using four metrics: Sinkhorn distance, Coverage, Density, and F1 score.  The p-values indicate statistical significance of the results, showing whether the Generative Forest outperforms the other models or vice-versa.", "section": "Experiments"}, {"figure_path": "cRlQHncjwT/tables/tables_34_1.jpg", "caption": "Table A7: LIFELIKE: comparison of Generative Forests (us, GF), with T = 500 trees, J =2000 total splits to contenders: Adversarial Random Forests (ARF, 200 trees [44]), CT-GAN (CT, 1000 epochs [47]), Forest Flows (FF, 50 trees [17]) and Vine copulas autoencoders (VCAE, VC-* where *=G for Gaussian, D for Direct and R for Regular [41]). Metrics used are regularized OT (Sinkhorn), Coverage, Density and F1 measure. Values shown = average over the 5-folds + std dev.. The best average for us vs contender is shown with a star \"*\". Convention for p-values: computed using a Student pared t-test; if p < 0.01, value is replaced by (we beat the contender) or (the contender beats us). See text for details.", "description": "This table compares the performance of Generative Forests against four other generative models (ARF, CT-GAN, Forest Flows, and VCAE) across several datasets.  The metrics used are Sinkhorn distance, coverage, density, and F1-score.  The results show the average and standard deviation across 5-fold stratified cross-validation experiments.  Statistically significant wins for Generative Forests are indicated using a star and p-values are provided.", "section": "Experiments"}, {"figure_path": "cRlQHncjwT/tables/tables_35_1.jpg", "caption": "Table A8: Comparison of results with respect to CT-GANs [47] trained with different number of epochs E. * means p-val < 10<sup>-4</sup>. As already mentioned in Table A8, in some domains, indicated with a *, CT-GANs crashed on some folds. For such domains, we indicate in index to CT-GANs results the number of folds (out of 5) for which this did not happen. In such cases, we restricted the statistical comparisons with us to the folds for which CT-GANs did not crash: the statistical tests take this into account; instead of providing all corresponding average performances for us, we keep giving the average performance for us on all five folds (leftmost column), which is thus indicative. Other conventions follow Table A7.", "description": "This table compares the performance of Generative Forests against CT-GANs on various datasets.  The Generative Forest model uses 500 trees and 2000 total splits.  CT-GANs are trained with different numbers of epochs (10, 100, 300, 1000). The table shows the Sinkhorn distance, coverage, density and F1 measure for each model and dataset, along with p-values indicating statistical significance.  Note that some CT-GAN runs crashed on certain folds, so the comparisons are restricted to the successful folds.", "section": "V.V.3.5 Results vs CT-GANs"}]