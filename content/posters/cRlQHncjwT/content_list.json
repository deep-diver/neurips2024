[{"type": "text", "text": "Generative Forests ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Richard Nock Google Research richardnock@google.com ", "page_idx": 0}, {"type": "text", "text": "Mathieu Guillame-Bert Google gbm@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We focus on generative AI for a type of data that still represent one of the most prevalent form of data: tabular data. Our paper introduces two key contributions: a new powerful class of forest-based models fit for such tasks and a simple training algorithm with strong convergence guarantees in a boosting model that parallels that of the original weak / strong supervised learning setting. This algorithm can be implemented by a few tweaks to the most popular induction scheme for decision tree induction (i.e. supervised learning) with two classes. Experiments on the quality of generated data display substantial improvements compared to the state of the art. The losses our algorithm minimize and the structure of our models make them practical for related tasks that require fast estimation of a density given a generative model and an observation (even partially specified): such tasks include missing data imputation and density estimation. Additional experiments on these tasks reveal that our models can be notably good contenders to diverse state of the art methods, relying on models as diverse as (or mixing elements of) trees, neural nets, kernels or graphical models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "There is a substantial resurgence of interest in the ML community around tabular data, not just because it is still one of the most prominent kind of data available [6]: it is recurrently a place of sometimes heated debates on what are the best model architectures to solve related problems. For example, even for well-posed problems with decades of theoretical formalization like supervised learning [39], after more than a decade of deep learning disruption [22], there is still much ink spilled in the debate decision forests vs neural nets [28]. Generative $\\mathrm{AI^{*}}$ makes no exception. Where the consensus has long been established on the best categories of architectures for data like image and text (neural nets), tabular data still flourishes with a variety of model architectures building up \u2013 or mixing \u2013 elements from knowledge representation, logics, kernel methods, graph theory, and of course neural nets [4, 9, 12, 31, 34, 41, 43, 44] (see Section 2). Because of the remarkable nature of tabular data where a single variable can bring considerable information about a target to model, each of these classes can be a relevant choice at least in some cases (such is is the conclusion of [28] in the context of supervised learning). A key differentiator between model classes is then training and the formal guarantees it can provide [31, 44]. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we introduce new generative models based on sets of trees that we denote as generative forests (GF), along with a training algorithm which has two remarkable features: it is extremely simple and brings strong convergence guarantees in a weak / strong learning model that parallels that of the original boosting model of Valiant\u2019s PAC learning [18]. These guarantees improve upon the best state of the art guarantees [44, 31]. Our training algorithm, GF.BOOST, supports training from data with missing values and is simple enough to be implementable by a few tweaks on the popular induction scheme for decision tree induction with two classes, which is supported by a huge number of repositories / ML software implementing algorithms like CART or C4.5 [1, 35, 3, 36, 45]. From the model standpoint, generative forests bring a sizeable combinatorial advantage over its closest competitors, generative trees [31] (see Table 1 for an example) and adversarial random forests [44]. Experiments on a variety of simulated or readily available domains display that our models can substantially improve upon state of the art, with models of ours as simple as a set of stumps potentially competing with other approaches building much more complex models. ", "page_idx": 0}, {"type": "image", "img_path": "cRlQHncjwT/tmp/757da718a4f8384163f013a4e858ab7d800ba70cb0ba56e3b19a74cbd19f3702.jpg", "img_caption": ["Table 1: Left: domain circgauss. Center: density learned by a generative forest (GF) consisting of a single tree, boosted for a small number (50) of iterations. Right: density learned by a GF consisting of 50 boosted tree stumps (Center and Right learned using GF.BOOST). In a domain $\\mathcal{X}$ with dimension $d$ , a single tree with $n$ splits can only partition the domain in $n+1$ parts. On the other hand, a set of $n$ stumps in a GF can boost this number to $n^{\\tilde{\\Omega}(d)}$ (the tilda omits log dependencies), which explains why the right density appears so much better than the central one, even when each tree is just a stump. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The models we build have an additional benefti: it is computationally easy to compute the full density given an observation, even partially specified; hence, our generative models can also be used for side tasks like missing data imputation or density estimation. Additional experiments clearly display that our approach can be a good contender to the state of the art. To save space and preserve readability, all proofs and additional experiments and results are given in an Appendix. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "It would not do justice to the large amount of work in the field of \"Generative AI\" for tabular data to just sample a few of them, so we devote a part of the Appendix to an extensive review of the state of the art. Let us just mention that, unlike for unstructured data like images, there is a huge variety of model types, based on trees [7, 31, 44], neural networks [12, 14, 20, 47], probabilistic circuits [5, 43, 38], kernel methods [4, 34], graphical models [41] (among others: note that some are in fact hybrid models). The closest approaches to ours are [44] and [31], because the models include trees with a stochastic activation of edges to pick leaves, and a leaf-dependent data generation process. While [31] learn a single tree, [44] use a way to generate data from a set of trees \u2013 called an adversarial random forest \u2013 which is simple: sample a tree, and then sample an observation from the tree. The model is thus simple but not economical: each observation is generated by a single tree only, each of them thus having to represent a good sampler in its own. In our case, generating one observation makes use of all trees (Figure 1, see also Section 4). We also note that the theory part of [44] is limited because it resorts to statistical consistency (infinite sample) and Lipschitz continuity of the target density, with second derivative continuous, square integrable and monotonic. Similarly, [31] resort to a wide set of proper losses, but with a symmetry constraint impeding in a generative context. As we shall see, our theoretical framework does not suffer from such impediments. ", "page_idx": 1}, {"type": "text", "text": "3 Basic definitions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Perhaps surprisingly at first glance, this Section introduces generative and supervised loss functions. Indeed, our algorithm, which trains a data generator and whose overall convergence shall be shown on a generative loss, turns out to locally optimize a supervised loss. For the interested reader, the key link, which is of independent interest since it links in our context losses for supervised and generative learning, is established in Lemma A (Appendix). ", "page_idx": 1}, {"type": "text", "text": "$\\forall k\\in\\mathbb{N}_{>0}$ , let $[k]\\,\\doteq\\,\\{1,2,...,k\\}$ . Our notations follow [37]. Let $\\mathfrak{B}\\doteq(\\pi,\\mathrm{A},\\mathrm{B})$ denote a binary task, where A, B (and any other measure defined hereafter) are probability measures with the same support, also called domain, $\\mathcal{X}$ , and $\\pi\\in[0,1]$ is a prior. $\\mathrm{M}\\doteq\\pi\\,{\\cdot}\\,\\mathrm{A}+(1-\\pi)\\,{\\cdot}\\,\\mathrm{B}$ is the corresponding mixture measure. For the sake of simplicity, we assume $\\mathcal{X}$ bounded hereafter, and note that tricks can be used to remove this assumption [31, Remark 3.3]. In tabular data, each of the $\\dim({\\mathcal{X}})$ features ", "page_idx": 1}, {"type": "table", "img_path": "cRlQHncjwT/tmp/eec91707c16e41a2519e2141b89f37bf795791b8f76bda87cc11fa5fed2ace64.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Sketch of comparison of two approaches to generate one observation, using Adversarial Random Forests [44] (left) and using generative forests, GF (right, this paper). In the case of Adversarial Random Forests, a tree is sampled uniformly at random, then a leaf is sampled in the tree and finally an observation is sampled according to the distribution \"attached\" to the leaf. Hence, only one tree is used to generate an observation. In our case, we leverage the combinatorial power of the trees in the forest: all trees are used to generate one observation, as each is contributing to one leaf. Figure 3 provides more details on generation using GF. ", "page_idx": 2}, {"type": "text", "text": "can be of various types, including categorical, numerical, etc., and associated to a natural measure (counting, Lebesgue, etc.) so we naturally associate $\\mathcal{X}$ to the product measure, which can thus be of mixed type. We also write $\\mathcal{X}\\doteq\\times_{i=1}^{d}\\dot{\\mathcal{X}}_{i}$ , where $\\mathcal{X}_{i}$ is the set of values that can take on variable . Several essential measures will be used in this paper, including U, the uniform measure, G, the measure associated to a generator that we learn, R, the empirical measure corresponding to a training sample of observations. Like in [31], we do not investigate generalisation properties. ", "page_idx": 2}, {"type": "text", "text": "Loss functions There is a natural problem associated to binary task $\\mathfrak{B}$ , that of estimating the probability that an arbitrary observation $\\pmb{x}\\in\\mathcal{X}$ was sampled from A \u2013 call such positive \u2013 or B \u2013 call these negative \u2013. To learn a supervised model $\\mathcal{X}\\rightarrow[\\bar{0},1]$ for such a class probability estimation (CPE) problem, one usually has access to a set of examples where each is a couple (observation, class), the class being in set $y\\doteq\\{-1,1\\}$ $\\lceil=\\{$ negative, positiveu). Examples are drawn i.i.d. according to $\\mathfrak{B}$ . Learning a model is done by minimizing a loss function: when it comes to CPE, any such CPE loss [2] is some $\\ell:y\\times[0,1]\\to\\mathbb{R}$ whose expression can be split according to partial losses $\\ell_{1},\\ell_{-1}$ , $\\ell(y,u)\\,\\doteq\\,[y\\,=\\,1]\\!]\\cdot\\bar{\\ell_{1}(u)\\big^{\\prime}}+[\\![y\\,=\\,-1]\\!]\\cdot\\ell_{-1}(u).$ . Its (pointwise) Bayes risk function is the best achievable lo s s when  labels are  d rawn wit h a particular positive base-rate, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underline{{L}}(p)\\doteq\\operatorname*{inf}_{u}\\,\\mathsf{E}_{\\mathsf{Y}\\sim\\mathsf{B}(p)}\\ell(\\mathsf{Y},u),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\textbf{B}\\dot{=}$ Bernoulli random variable for class 1. A fundamental property for a CPE loss is properness, encouraging to guess ground truth: $\\ell$ is proper iff $\\underline{{L}}(p)=\\mathsf{E}_{\\mathsf{Y}\\sim\\mathsf{B}(p)}\\ell(\\mathsf{\\bar{Y}},p),\\forall p\\in[0,1].$ , and strictly proper if $\\underline{{L}}(p)<\\mathsf{E}_{\\mathsf{Y}\\sim\\mathsf{B}(p)}\\ell(\\mathsf{Y},u),\\forall u\\neq p$ . Strict properness implies strict concavity of Bayes risk. For example, the square loss has $\\ell_{1}^{\\mathrm{sq}}(u)\\doteq(1-u)^{2}$ , $\\ell_{-1}^{\\mathrm{sq}}(u)\\doteq u^{2}$ , and, being strictly proper, Bayes risk $\\underline{{L}}^{\\mathrm{sq}}(u)\\doteq u(1-u)$ . Popular strictly proper ML include the log and Matusita\u2019s losses. All these losses are symmetric since $\\bar{\\ell}_{1}^{\\mathrm{so}}(u)=\\ell_{-1}^{\\mathrm{so}}(\\bar{1}-u),\\forall u\\in(0,1)$ and differentiable because both partial losses are differentiable. ", "page_idx": 2}, {"type": "text", "text": "In addition to CPE losses, we introduce a set of losses relevant to generative approaches, that are popular in density ratio estimation [29, 40]. For any differentiable and convex $F:\\mathbb{R}\\rightarrow\\mathbb{R}$ , the Bregman divergence with generator $F$ is $D_{F}(z\\|z^{\\prime})\\doteq\\bar{F}(z)\\!-\\!F(z^{\\prime})\\!-\\!(z\\!-\\!z^{\\prime})F^{\\prime}(z^{\\prime})$ . Given function $g:\\mathbb{R}\\to\\mathbb{R}$ , the generalized perspective transform of $F$ given $g$ is ${\\check{F}}(z)\\doteq g(z)\\cdot F\\left(z/g(z)\\right)$ , $g$ being implicit in notation $\\check{F}$ [26, 27, 32]. The Likelihood ratio risk of B with respect to A for loss $\\ell$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{D}_{\\ell}\\left(\\mathrm{A},\\mathrm{B}\\right)}&{\\doteq}&{\\pi\\cdot\\mathbb{E}_{\\mathrm{U}}\\left[D_{\\left(\\frac{\\widetilde{-}\\underline{{L}}}{-\\underline{{L}}}\\right)}\\left(\\frac{\\mathrm{d}\\mathrm{A}}{\\mathrm{dU}}\\left\\|\\frac{\\mathrm{d}\\mathrm{B}}{\\mathrm{dU}}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with $g(z)\\doteq z+(1-\\pi)/\\pi$ in the generalized perspective transfo\u203arm. The prior multiplication is for technical convenience. $\\mathbb{D}_{\\ell}$ is non-negative; strict properness is necessary for a key property of $\\mathbb{D}_{\\ell}$ : $\\mathbb{D}_{\\ell}=0$ iff $\\mathrm{A}=\\mathrm{B}$ almost everywhere [31]. ", "page_idx": 2}, {"type": "text", "text": "4 Generative forests: models and data generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Architecture We first introduce the basic building block of our models, trees. ", "page_idx": 2}, {"type": "text", "text": "Definition 4.1. A tree $\\Upsilon$ is a binary directed tree whose internal nodes are labeled with an observation variable and arcs are consistently labeled with subsets of their tail node\u2019s variable domain. ", "page_idx": 2}, {"type": "text", "text": "Consistency is an important generative notion; informally, it postulates that the arcs\u2019 labels define a partition of the measure\u2019s support. To make this notion formal, we proceed need a key definition. ", "page_idx": 2}, {"type": "image", "img_path": "cRlQHncjwT/tmp/59272935364bf461dd0666c4733199653d98019cd2b04bf166ea327eb0af579f.jpg", "img_caption": ["Figure 2: A GF $T=2$ ) associated to UCI German Credit. Constraint (C) (see text) implies that the domain of \"Number existing credits\" is $\\{0,1,...,8\\}$ , that of \"Job\" is tA171, A172, A173, A174u, etc. "], "img_footnote": [], "page_idx": 3}, {"type": "table", "img_path": "cRlQHncjwT/tmp/6970de7e62b0ab1da242cb359b0a0645b0164a40063540a18595afeee3f7d21e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "For any node $\\nu\\in\\mathcal{N}(\\Upsilon)$ (the whole set of nodes of $\\Upsilon$ , including leaves), we denote $\\mathcal{X}_{\\nu}\\subseteq\\mathcal{X}$ the support of the node. The root has $\\mathcal{X}_{\\nu}=\\mathcal{X}$ . To get $\\mathcal{X}_{\\nu}$ for any other node, we initialize it to $\\mathcal{X}$ and then descend the tree from the root, progressively updating $\\mathcal{X}_{\\nu}$ by intersecting an arc\u2019s observation variable\u2019s domain in $\\mathcal{X}_{\\nu}$ with the sub-domain labelling the arc until we reach $\\nu$ . Then, a labeling of arcs in a tree is consistent iff it complies with one constraint (C): ", "page_idx": 3}, {"type": "text", "text": "(C) for each internal node $\\nu$ and its left and right children $\\nu_{\\mathtt{f}},\\nu_{\\mathtt{t}}$ (respectively), $\\mathfrak{X}_{\\nu}=\\mathfrak{X}_{\\nu_{\\mathrm{f}}}\\cup\\mathfrak{X}_{\\nu_{\\mathrm{t}}}$ and the measure of $\\mathfrak{X}_{\\nu_{\\mathrm{f}}}\\cap\\mathfrak{X}_{\\nu_{\\mathrm{t}}}$ with respect to $\\mathrm{G}$ is zero. ", "page_idx": 3}, {"type": "text", "text": "For example, the first split at the root of a tree is such that the union of the domains at the two arcs equals the domain of the feature labeling the split (see Figure 2). We define our generative models. ", "page_idx": 3}, {"type": "text", "text": "Definition 4.2. A generative forest (GF), G, is a set of trees, $\\{\\Upsilon_{t}\\}_{t=1}^{T}$ , associated to measure R (implicit in notation). ", "page_idx": 3}, {"type": "text", "text": "Figure 2 shows an example of GF. Following the consistency requirement, any single tree defines a recursive partition of $\\mathcal{X}$ according to the splits induced by the inner nodes. Such is also the case for a set of trees, where intersections of the supports of tuples of leaves (1 for each tree) define the subsets: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathrm{G})\\doteq\\left\\{\\cap_{i=1}^{T}\\mathcal{X}_{\\lambda_{i}}\\mathrm{~s.t.~}\\lambda_{i}\\in\\Lambda(\\Upsilon_{i}),\\forall i\\right\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$(\\Lambda(\\Upsilon)\\subseteq\\mathcal{N}(\\Upsilon)$ is the set of leaves of $\\Upsilon$ ). Notably, we can construct the elements of $\\mathcal{P}(\\mathrm{G})$ using the same algorithm that would compute it for 1 tree. First, considering a first tree $\\Upsilon_{1}$ , we compute the support of a leaf, say $\\mathcal{X}_{\\lambda_{1}}$ , using the algorithm described for the consistency property above. Then, we start again with a second tree $\\Upsilon_{2}$ but replacing the initial $\\mathcal{X}$ by $\\mathcal{X}_{\\lambda_{1}}$ , yielding $\\mathcal{X}_{\\lambda_{1}}\\cap\\mathcal{X}_{\\lambda_{2}}$ . Then we repeat with a third tree $\\Upsilon_{3}$ replacing $\\mathcal{X}$ by $\\mathcal{X}_{\\lambda_{1}}\\cap\\mathcal{X}_{\\lambda_{2}}$ , and so on until the last tree is processed. This yields one element of $\\mathcal{P}(\\mathrm{G})$ . ", "page_idx": 3}, {"type": "text", "text": "Generating one observation Generating one observation relies on a stochastic version of the procedure just described. It ends up in an element of $\\mathcal{P}(\\mathrm{G})$ of positive measure, from which we sample uniformly one observation, and then repeat the process for another observation. To describe the process at length, we make use of two key routines, INIT and STARUPDATE, see Algorithms 1 and 2. INIT initializes \"special\" nodes in each tree, that are called star nodes, to the root of each tree (notation for a variable $v$ relative to a tree $\\Upsilon$ is $\\Upsilon.v\\bigr)$ ). Stochastic activation, performed in STARUPDATE, progressively makes star nodes descend in trees. When all star nodes have reached a leaf in their respective tree, an observation is sampled from the intersection of the leaves\u2019 domains (which is an element of $\\mathcal{P}(\\mathrm{G})$ ). A Boolean flag, done takes value true when the star node is in the leaf set of the tree, indicating no more STARUPDATE calls for the tree $\\left(\\left[\\!\\!\\left[.\\right]\\right]\\!\\!\\right)^{\\!}$ is Iverson\u2019s bracket, [21]). ", "page_idx": 3}, {"type": "text", "text": "STARUPDATE is called with a tree of the GF for which do ne  is false, a subset $\\mathcal{C}$ of the whole domain and measure R. The first call of this procedure is done with $\\mathcal{C}\\,=\\,\\mathcal{X}$ . When all trees are marked done, C has been \"reduced\" to some $\\ensuremath{\\mathcal{C}}_{\\mathrm{s}}\\in\\mathcal{P}(\\mathrm{G})$ , where the index reminds that this is the last C we obtain, from which we sample an observation, uniformly at random in $\\mathcal{C}_{\\mathrm{s}}$ . Step 1 in STARUPDATE is fundamental: it relies on tossing an unfair coin (a Bernoulli event noted $\\mathbf{B}(p))$ , where the head probability $p$ is just the mass of $\\mathrm{R}$ in $\\mathcal{X}_{\\Upsilon.\\nu_{\\sf t}^{\\star}}\\cap\\mathcal{C}$ relative to $\\mathcal{C}$ . Hence, if $\\mathfrak{X}_{\\Upsilon.\\nu_{\\mathrm{t}}^{\\star}}\\cap\\mathcal{C}=\\mathcal{C}$ , $p=1$ . There is a simple but important invariant (proof omitted). ", "page_idx": 3}, {"type": "table", "img_path": "cRlQHncjwT/tmp/718aefc2a5a43e033c3a3545cd43060fa990560d7b0edb457e75854020763388.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Lemma 4.3. In STARUPDATE, it always holds that the input C satisfies ${\\mathcal{C}}\\subseteq{\\mathcal{X}}_{\\Upsilon,\\nu^{\\star}}$ ", "page_idx": 4}, {"type": "text", "text": "We have made no comment about the sequence of tree choices over which STARUPDATE is called. Let us call admissible such a sequence that ends up with some $\\mathcal{C}_{\\mathrm{s}}\\,\\subseteq\\,\\mathcal{X}$ . $T$ being the number of trees (see INIT), for any sequence $\\pmb{v}\\in[T]^{D(\\mathcal{C}_{\\mathrm{s}})}$ , where $D(\\mathcal{C}_{\\mathrm{s}})$ is the sum of the depths of all the star leaves whose support intersection is $\\mathcal{C}_{\\mathrm{s}}$ , we say that $\\pmb{v}$ is admissible for $\\mathcal{C}_{\\mathrm{s}}$ if there exits a sequence of branchings in Step 1 of STARUPDATE, whose corresponding sequence of trees follows the indexes in $\\pmb{v}$ , such that at the end of the sequence all trees are marked done and the last $\\mathcal{C}=\\mathcal{C}_{\\mathrm{{s}}}$ . Crucially, the probability to end up in $\\mathcal{C}_{\\mathrm{s}}$ using STARUPDATE, given any of its admissible sequences, is the same and equals its mass with respect to $\\mathrm{R}$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 4.4. For any $\\mathcal{C}_{s}\\in\\mathcal{P}(\\mathrm{G})$ and admissible sequence $\\pmb{v}\\in[T]^{D(\\mathcal{C}_{s})}$ for $\\begin{array}{r}{\\mathcal{C}_{s},\\;p_{\\mathrm{G}}\\left[\\mathcal{C}_{s}|v\\right]=p_{\\mathrm{R}}\\left[\\mathcal{C}_{s}\\right]\\!.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "The Lemma is simple to prove but fundamental in our context as the way one computes the sequence \u2013 and thus the way one picks the trees \u2013 does not bias generation: the sequence of tree choices could thus be iterative, randomized, concurrent (e.g. if trees were distributed), etc., this would not change generation\u2019s properties from the standpoint of Lemma 4.4. We defer to Appendix (Section II) three examples of sequence choice. Figure 3 illustrates a sequence and the resulting $\\mathcal{C}_{\\mathrm{s}}$ . ", "page_idx": 4}, {"type": "text", "text": "Missing data imputation and density estimation with GFs A generative forest is not just a generator: it models a density and the exact value of this density at any observation is easily available. Figure 4 (top row) shows how to compute it. Note that if carried out in parallel, the complexity to get this density is cheap, of order $\\bar{O(\\operatorname*{max}_{t}\\mathrm{depth}(\\Upsilon_{t}))}$ . If one wants to prevent predicting zero density, one can stop the descent if the next step creates a support with zero empirical measure. A GF thus also models an easily tractable density, but this fact alone is not enough to make it a good density estimator. To get there, one has to factor the loss minimized during training. In our case, as we shall see in the following Section, we train a GF by minimizing an information-theoretic loss directly formulated on this density (2). So, using a GF also for density estimation can be a reasonable additional benefit of training GFs. ", "page_idx": 4}, {"type": "text", "text": "The process described above that finds leaves reached by an observation can be extended to missing values in the observation using standard procedures for classification using decision trees. We obtain a simple procedure to carry out missing data imputation instead of density estimation: once a tuple of leaves is reached, one uniformly samples the missing features in the corresponding support. This is fast, but at the expense of a bit more computations, we can have a much better procedure, as explained in Figure 4 (bottom row). In a first step, we compute the density of each support subset corresponding to all (not just 1 as for density estimation) tuples of leaves reached in each tree. This provides us with the full density over the missing values given (i) a partial assignment of the tabular domain\u2019s variables and (ii) the GF. We then keep the elements corresponding to the maximal density value and finally simultaneously sample all missing features uniformly in the corresponding domain. Overall, the whole procedure is $\\begin{array}{r}{O(\\bar{d}\\cdot(\\sum_{t}\\mathrm{depth}(\\Upsilon_{t}))^{2})}\\end{array}$ q. ", "page_idx": 4}, {"type": "image", "img_path": "cRlQHncjwT/tmp/2025fac7839460e597486b49b29a1b1879490f8bf6aa3f5cf011eaea1e2b516d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: (Top row) Density estimation using a GF, on an observation indicated by $\\bullet\\ (L e f t)$ . In each tree, the leaf reached by the observation is found and the intersection of all leaves\u2019 supports is computed. The estimated density at $\\bullet$ is computed as the empirical measure in this intersection over its volume $(R i g h t)$ . (Bottom row) Missing data imputation using the same GF, and an observation with one missing value (if $\\mathcal{X}\\subset\\mathbb{R}^{2}$ , then $y$ is missing). We first proceed like in density estimation, finding in each tree all leaves potentially reached by the observation if $y$ were known $(L e f t)$ ; then, we compute the density in each non-empty intersection of all leaves\u2019 supports; among the corresponding elements with maximal density, we get the missing value(s) by uniform sampling $(R i g h t)$ . ", "page_idx": 5}, {"type": "table", "img_path": "cRlQHncjwT/tmp/d5636ae8a360dac9b900bebef809901e36a58f778586c6d3146efcbb3b45b602.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "5 Learning generative forests using supervised boosting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "From the GAN framework to a fully supervised training of generative models It can appear quite unusual to train generative models using a supervised learning framework, so before introducing our algorithm, we provide details on its filiation in the generative world, starting from the popular GAN training framework [12]. In this framework, one trains a generative model against a discriminator model which has no purpose other than to parameterize the generative loss optimized. As shown in [33], there is a generally inevitable slack between the generator and the discriminator losses with neural networks, which translates into uncertainty for training. [31] show that the slack disappears for calibrated models, a property satisfied by their generative trees (and also by our generative forests). Moreover, [31] also show that the GAN training can be simplified and made much more efficient for generative trees by having the discriminator (a decision tree) copy the tree structure of the generator, a setting defined as copycat. Hence, one gets reduced uncertainty and more efficient training. Training still involves two models but it becomes arguably very close to the celebrated boosting framework for top-down induction of decision trees [19], up to the crucial detail that the generator turns out to implements boosting\u2019s leveraged \"hard\" distribution. This training gives guarantees on the likelihood ratio risk we define in (2). Our paper closes the gap with boosting: copycat training can be equivalently simplified to training a single generative model in a supervised (2 classes) framework. The two classes involved are the observed data and the uniform distribution. Using the uniform distribution is allowed by the assumption that the domain is closed, which is reasonable for tabular data but can also be alleviated by reparameterization [31, Remark 3.3]. The (supervised) loss involved for training reduces to the popular concave \"entropic\"-style losses used in CART, C4.5 and in popular packages like scikit-learn. And of course, minimizing such losses still provides guarantee on the generative loss (2). While it is out of scope to show how copycat training does ultimately simplify, we provide all components of the \"end product\": algorithm, loss optimized and the link between the minimization of the supervised and generative losses via boosting. ", "page_idx": 5}, {"type": "text", "text": "The algorithm To learn a GF, we just have to learn its set of trees. Our training algorithm, GF.BOOST (Algorithm 3), performs a greedy top-down induction. In Step 1, we initialize the set of $T$ trees to $T$ roots. Steps 2.1 and 2.2 choose a tree $\\left(\\Upsilon_{\\ast}\\right)$ and a leaf to split $\\left(\\lambda_{*}\\right)$ in the tree. In our implementation, we pick the leaf among all trees which is the heaviest with respect to R. Hence, we merge Steps 2.1 and 2.2. Picking the heaviest leaf is standard to grant boosting in decision tree induction [19]; in our case, there is a second benefit: we tend to learn size-balanced models. For example, during the first $J=T$ iterations, each of the $T$ roots gets one split because each root has a larger mass (1) than any leaf in a tree with depth $>0$ . Step 2.4 splits $\\Upsilon_{\\ast}$ by replacing $\\lambda_{*}$ by a stump whose corresponding splitting predicate, p, is returned in Step 2.3 using a weak splitter oracle called splitPred. \"weak\" refers to boosting\u2019s weak/strong learning setting [18] and means that we shall only require lightweight assumptions about this oracle; in decision tree induction, this oracle is the key to boosting from such weak assumptions [19]. This will also be the case for our generative models. We now investigate Step 2.3 and splitPred. ", "page_idx": 6}, {"type": "text", "text": "The weak splitter oracle splitPred In decision tree induction, a splitting predicate is chosen to reduce an expected Bayes risk (1) (e.g. that of the log loss [36], square loss [1], etc.). In our case, splitPred does about the same with a catch in the binary task it adresses, which is\u2020 $\\mathfrak{B}_{\\mathrm{GEN}}\\,\\doteq\\,(\\pi,\\mathrm{R},\\mathrm{U})$ (our mixture measure is thus $\\mathrm{~M~}\\dot{=}\\mathrm{\\scriptsize~\\pi~}\\cdot\\mathrm{R}+\\left(1-\\pi\\right)\\cdot\\mathrm{U})$ . The corresponding expected Bayes risk that splitPred seeks to minimize is just: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{L}(\\mathcal{T})}&{\\doteq}&{\\displaystyle\\sum_{\\mathcal{C}\\in\\mathcal{P}(\\mathcal{T})}p_{\\textup{M}}[\\mathcal{C}]\\cdot\\underline{{L}}\\left(\\frac{\\pi p_{\\mathrm{R}}[\\mathcal{C}]}{p_{\\textup{M}}[\\mathcal{C}]}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The concavity of $\\underline{{L}}$ implies $\\underline{{\\mathbb{L}}}(\\mathcal{T})\\leqslant\\underline{{L}}(\\pi)$ . Notation $\\mathcal{P}(.)$ overloads that in (3) by depending explicitly on the set of trees of $\\mathrm{G}$ instead of G itself. Regarding the minimization of (4), there are three main differences with classical decision tree induction. The first is computational: in the latter case, (4) is optimized over a single tree: there is thus a single element in $\\mathcal{P}(\\bar{\\mathcal{T}})$ which is affected by the split, $\\mathfrak{X}_{\\lambda_{*}}$ . In our case, multiple elements in $\\mathcal{P}(\\mathcal{T})$ can be affected by one split, so the optimisation is more computationally demanding but a simple trick allows to keep it largely tractable: we do not need to care about keeping in $\\mathcal{P}(\\mathcal{T})$ support elements with zero empirical measure since they will generate no data. Keeping only elements with strictly positive empirical measure guarantees a size $|\\mathcal{P}(\\mathcal{T})|$ never bigger than the number of observations defining R. The second difference plays to our advantage: compared to classical decision tree induction, a single split generally buys a substantially bigger slack in $\\underline{{\\mathbb{L}}}(\\mathcal{T})$ in our case. To see this, remark that for the candidate leaf $\\lambda_{*}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\sum_{\\mathrm{~e~e~}\\mathcal{P}(\\mathcal{C})\\ }}&{p_{\\mathrm{\\scriptsize{M}}}[\\mathcal{C}]\\cdot\\underline{{L}}\\left(\\frac{\\pi p_{\\mathrm{\\tiny{R}}}[\\mathcal{C}]}{p_{\\mathrm{\\scriptsize{M}}}[\\mathcal{C}]}\\right)}&{=}&{\\displaystyle p_{\\mathrm{\\scriptsize{M}}}[\\mathcal{X}_{\\lambda_{*}}]\\cdot\\sum_{\\mathrm{~e~}\\in\\mathcal{P}(\\mathcal{T})\\ }\\frac{p_{\\mathrm{\\scriptsize{M}}}[\\mathcal{C}]}{p_{\\mathrm{\\scriptsize{M}}}[\\mathcal{X}_{\\lambda_{*}}]}\\cdot\\underline{{L}}\\left(\\frac{\\pi p_{\\mathrm{\\tiny{R}}}[\\mathcal{C}]}{p_{\\mathrm{\\scriptsize{M}}}[\\mathcal{C}]}\\right)}\\\\ &&{\\displaystyle\\in\\varkappa_{\\lambda_{*}}}\\\\ &{\\leqslant}&{p_{\\mathrm{\\scriptsize{M}}}[\\mathcal{X}_{\\lambda_{*}}]\\cdot\\underline{{L}}\\left(\\frac{\\pi p_{\\mathrm{\\tiny{R}}}[\\mathcal{X}_{\\lambda_{*}}]}{p_{\\mathrm{\\scriptsize{M}}}[\\mathcal{X}_{\\lambda_{*}}]}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "(because $\\underline{{L}}$ is concave and $\\begin{array}{r}{\\sum_{\\mathcal{\\mathrm{e}}\\in\\mathcal{P}(\\mathcal{T}),\\mathcal{\\mathrm{e}}\\subseteq\\mathcal{X}_{\\lambda_{*}}}p_{\\mathrm{R}}[\\mathcal{\\mathrm{C}}]=p_{\\mathrm{R}}[\\mathcal{X}_{\\lambda_{*}}])}\\end{array}$ . The top-left term is the contribution of $\\lambda_{*}$ to $\\underline{{\\mathbb{L}}}(\\mathcal{T})$ , the botto m-right one its contribution to $\\underline{{\\mathbb{L}}}(\\{\\Upsilon_{*}\\})$ (the decision tree case), so the slack gives a proxy to our potential advantage after splitting. The third difference with decision tree induction is the possibility to converge much faster to a good solution in our case. Scrutinizing the two terms, we indeed get that in the decision tree case, the split only gets two new terms. In our case however, there can be up to $2\\cdot\\operatorname{Card}(\\{\\mathcal{C}\\in\\mathcal{P}(\\mathcal{T}):\\mathcal{C}\\subseteq\\mathcal{X}_{\\lambda_{*}}\\})$ q new elements in $\\mathcal{P}(\\mathcal{T})$ . ", "page_idx": 6}, {"type": "text", "text": "Boosting Two questions remain: can we quantify the slack in decrease and of course what quality guarantee does it bring for the generative model $\\mathrm{G}$ whose set of trees is learned by GF.BOOST ? We answer both questions in a single Theorem, which relies on a weak learning assumption that parallels the classical weak learning assumption of boosting: ", "page_idx": 6}, {"type": "text", "text": "Definition 5.1. $(W L A(\\gamma,\\kappa))$ There exists $\\gamma>0,\\kappa>0$ such that at each iteration of GF.BOOST, the couple $(\\lambda_{\\ast},p)$ chosen in Steps 2.2, 2.3 of GF.BOOST satisfies the following properties: (a) $\\lambda_{*}$ is not skinny: $p_{\\,\\mathrm{M}}[\\dot{\\mathcal{X}}_{\\lambda_{\\ast}}]\\geqslant1/\\mathrm{Card}(\\Lambda(\\Upsilon_{\\ast}))$ , $(b)$ truth values of $\\pmb{\\mathcal{P}}$ moderately correlates with $\\mathfrak{B}_{\\mathrm{GEN}}$ at $\\lambda_{*}$ $_{*}\\colon\\left|p_{\\mathrm{R}}[p|\\mathcal{X}_{\\lambda_{*}}]-p_{\\mathrm{U}}[p|\\mathcal{X}_{\\lambda_{*}}]\\right|\\geqslant\\gamma$ , and finally (c) there is a minimal proportion of real data at $\\lambda_{*}$ : $\\pi p_{\\mathrm{R}}[\\mathcal{X}_{\\lambda_{*}}]/p_{\\mathrm{\\Theta\\mathbb{M}}}[\\mathcal{X}_{\\lambda_{*}}]\\geqslant\\kappa$ . ", "page_idx": 6}, {"type": "text", "text": "The convergence proof of [19] reveals that both (a) and (b) are jointly made at any split, where our ${\\bf(b)}$ is equivalent to their weak hypothesis assumption where their $\\gamma$ parameter is twice ours. (c) ", "page_idx": 6}, {"type": "image", "img_path": "cRlQHncjwT/tmp/cbbe66107654680bcc1bc0f06c132eff4a3fd9cc00136a9eb2563c41606a51eb.jpg", "img_caption": ["Table 2: 2D density plots generated on OpenML kc1 $\\overline{{x=d}}$ and $\\overline{{y=l}}$ ) with GF, for varying number of trees $T$ and number of splits $J$ (columns). \"\u02da\" $=$ all trees are stumps. The rightmost column recalls the domain ground truth for comparison. Each generated dataset contains $m=2000$ observations. The two variables (among $d=22$ ) were chosen because of their deterministic dependence. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "cRlQHncjwT/tmp/f2c0ce9785f6b5c6233c9c74478ca3a3da297f484a581c13994e4fb961bec3c0.jpg", "img_caption": ["Table 3: DENSITY: comparison between us and KDE (summary), counting the number of domains for which we are statistically better (left), or worse (right). The central column counts the remaining domains, for which no statistical significance ever holds. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "postulates that the leaf split has empirical measure at least a fraction of its uniform measure \u2013 thus, of its relative volume. (c) is important to avoid splitting leaves that would essentially be useless for our tasks: a leaf for which (c) is invalid would indeed locally model comparatively tiny density values. Theorem 5.2. Suppose the loss $\\ell$ is strictly proper and differentiable. Let $\\mathrm{G_{0}}$ $\\mathit{\\Pi}^{\\prime}=\\mathrm{\\DeltaU}_{\\mathit{c}}$ ) denote the initial GF with $T$ roots in its trees (Step $^{\\,l}$ , GF.BOOST) and $\\mathrm{G}_{J}$ the final GF, assuming wlog that the number of boosting iterations $J$ is a multiple of $T$ . Under $W L A(\\gamma,\\kappa)$ , we get the following on likelihood ratio risks: $\\begin{array}{r}{\\mathbb{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{J}\\right)\\leqslant\\mathbb{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{0}\\right)-\\frac{\\kappa\\gamma^{2}\\kappa^{2}}{8}\\cdot T\\log\\left(1+\\frac{J}{T}\\right)}\\end{array}$ , for some constant $\\kappa>0$ . It comes from [25, Remark 1] that we can choose $\\kappa=\\operatorname*{inf}\\{\\ell_{-1}^{\\prime}-\\ell_{1}^{\\prime}\\}$ , which is $>0$ if $\\ell$ is strictly proper and differentiable. Strict properness is also essential for the loss to guarantee that minimization gets to a good generator (Section 3). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our code is provided and commented in Appendix, Section V.2. The main setting of our experiments is realistic data generation (\u2019LIFELIKE\u2019), but we have also tested our method for missing data imputation (\u2019IMPUTE\u2019) and density estimation (\u2019DENSITY\u2019): for this reason, we have selected a broad panel of state of the art approaches to test against, relying on models as diverse as (or mixing elements of) trees, neural nets, kernels or graphical models, with MICE [42], adversarial random forests (ARFs [44]), CT-GANs [47], Forest Flows [17], Vine copulas auto-encoders (VCAE, [41]) and Kernel density estimation (KDE, [4, 34]). All algorithms not using neural networks were ran on a low-end CPU laptop \u2013 this was purposely done for our technique. Neural network-based algorithms are run on a bigger machine (technical specs in Appendix, Section V.2). We carried out experiments on a total of 21 datasets, from UCI [10], Kaggle, OpenML, the Stanford Open Policing Project, or simulated. All are presented in Appendix, Section V.1. We summarize results that are presented in extenso in Appendix. Before starting, we complete the 2D heatmap of Table 1 by another one showing our models can also learn deterministic dependences in real-world domains (Table 2). The Appendix also provides an example experiment on interpreting our models for a sensitive domain (in Section V.V.3.1). ", "page_idx": 7}, {"type": "text", "text": "Generation capabilities of our models: LIFELIKE The objective of the experiment is to evaluate whether a generative model is able to create \"realistic\" data. The evaluation pipeline is simple: we create for each domain a 5-fold stratified experiment. Evaluation is done via four metrics: a regularized optimal transport (\"Sinkhorn\", [8]), coverage and density [30] and the F1 score [17]. All metrics are obtained after generating a sample of the same size as the test fold. Sinkhorn evaluates an optimal transport distance between generated and real and F1 estimates the error of a classifier (a 5-nearest neighbor) to distinguish generated vs real (smaller is better for both). Coverage and density are refinements of precision and recall for generative models (the higher, the better). Due to size constraint, we provide results on one set of parameters for \"medium-sized\" generative forests with $T=500$ trees, $J=\\!2\\,000$ total splits (Table 5), thus learning very small trees with an average 4 splits per tree. In the Appendix, we provide additional results on even smaller models $T=200$ , $J=500$ ) ", "page_idx": 7}, {"type": "image", "img_path": "cRlQHncjwT/tmp/9b417c384b647976f23d80bd6e229c7fd1ef7d6e05e63297201b6868b28a097f.jpg", "img_caption": ["Table 4: IMPUTE: results on two domains (left and right pane). Since both domain contain categorical and numerical variables, we compute for each perr (categorical variables) and the rmse (numerical variables). In each plot, we display both the corresponding results of GF.BOOST, the results of MICE.RF (indicating the total number of trees used to impute the dataset) and the result of a fast imputation baseline using the marginals computed from the training sample. The $x$ axis displays the number of trees $T$ in GF.BOOST and each curve is displayed with its average $\\pm$ standard deviation in shaded color. The result for $T=1$ equivalently indicates the performance of a single generative tree (GT) with $J$ splits [31], shown with an arrow (see text). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "and additional results on one metric (Sinkhorn) against contenders with various parameterizations on more domains. In Table 5, contenders are parameterized as follows: ARFs learn sets of 200 trees. Since each tree has to be separately a good generative model, we end up with big models in general. CT-GANs are trained for 1 000 epochs. Forest Flows and VCAEs are run with otherwise default parameters. Table 5 contains just a subset of all our domains, furthermore not the same for each contender. This benchmark was crafted to compare against Forest Flows, our best LIFELIKE contender but a method requiring to re-encode categorical variables. Since our method and ARFs process data natively, we selected only domains with numerical or binary variables (8 domains). On some of these domains, CT-GANs or VCAE crashed on some folds so we do not provide the results for the related domains. Globally, Generative Forests are very competitive against each contender, significantly beating all VCAEs on all metrics and CT-GANs on coverage and density. Tree-based contender methods appear to perform substantially better than neural networks, with FFs performing better than ARFs. Ultimately, our models compare favorably or very favorably against both FFs and ARFs, while on average being much smaller \u2013 for example, each FF model can contain up to 6 750 total splits. As our experiments demonstrate (Appendix, Table A6), learning much smaller generative forests with 500 total splits can still buy an advantage in most cases, a notable counterexample being density for FFs. From the memory standpoint, our code (Java) is not optimized yet managed to crunch domains of up to $m\\times d\\approx\\!1.5\\;\\mathrm{M}$ while always using less than 1.5Gb memory to train our biggest models. Finally, it clearly emerges from our experiments that there is a domain-dependent \"ideal\" size $(T,J)$ for the best generative forests. \"Guessing\" the right size is a prospect for future work. ", "page_idx": 8}, {"type": "text", "text": "Missing data imputation with our models: IMPUTE We compared our method against a powerful contender, MICE [42], which relies on round-robin prediction of missing values using supervised models. We optimized MICE by choosing as supervised models trees (CART) and random forests (RFs, we increased the number of trees to 100 for better results). We create a 5-fold experiment; in each fold, we remove a fixed $\\%$ of observed values (data missing completely at random, MCAR). We then use the data with missing values as input to MICE or us to learn models which are then used to predict the missing values. We compute a per-feature discrepancy, the average error probability (perr, for categorical variables), and the root mean square error (rmse, numerical variables). We also compute one of the simplest baselines, which consists in imputing each variable from its empirical marginal. Results summary Table 4 puts the spotlight on two domains. The Appendix provides many more plots. From all results, we conclude that generative forests can compete or beat MICE.RF while using hundred times less trees (eventually using just stumps, when $J=T=20$ ). From all our results, we also conclude that there is a risk of overfitting if $J$ and / or $T$ are too large. This could motivate further work on pruning generative models. For GF, an unexpected pattern is that the pattern \"small number of small trees works well\" can work on real-world domains as well (see also Appendix), thereby demonstrating that the nice result displayed in Table 1 generalises to bigger domains. ", "page_idx": 8}, {"type": "text", "text": "Density estimation: DENSITY We compared GF vs kernel density estimation (KDE) [4, 24, 34]. The experimental setting is the same as for LIFELIKE: in each of the 5-fold stratified cross validation fold, we use the training sample to learn a model (with KDE or GF.BOOST) which is then used to predict the observation\u2019s density in the the test sample. The higher the density, the better the model. The ", "page_idx": 8}, {"type": "table", "img_path": "cRlQHncjwT/tmp/7b0bd6eb878bc9c6a756a6451faed87c4ce8859c26577c70637cde2efc175e18.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 5: LIFELIKE: comparison of Generative Forests (us, GF), with $T=500$ trees, $J=2\\:000$ total splits to contenders: Adversarial Random Forests (ARF, 200 trees [44]), CT-GAN (CT, 1 000 epochs [47]), Forest Flows (FF, 50 trees [17]) and Vine copulas autoencoders (VCAE, VC-\\* where $^{*}{=}\\mathbf{G}$ for Gaussian, D for Direct and R for Regular [41]). Metrics used are regularized OT (Sinkhorn), Coverage, Density and F1 measure. Values shown $=$ average over the 5-folds $\\pm$ std dev. . The best average for us vs contender is shown with a star $\"\\star\"$ . Convention for $p$ -values: computed using a Student pared $t$ -test; if $p<0.01$ , value is replaced by \u25b2(we beat the contender) or $\\blacktriangledown$ (the contender beats us). See text for details. ", "page_idx": 9}, {"type": "text", "text": "GF models we consider are the same as in LIFELIKE ( $T=500$ , $J=2\\,000)$ ). ", "page_idx": 9}, {"type": "text", "text": "Results summary Table (3, right) summarizes the results (plots: Appendix, Section V.V.3.7). The leftmost column $^{1\\prime}\\mathrm{GF}\\gg\\mathrm{KDE\"}$ ) counts domains for which there exists an iteration $j$ for GF after which we tend to be statistically better (and never statistically worse) than KDE. The rightmost column $^{\\prime\\prime}\\mathrm{KDE}\\gg\\mathrm{GF}^{\\prime\\prime}$ ) counts domains for which KDE is always statistically better than GF. The last type of plots appears to be those for which there is no such statistical difference (central column). The results support the conclusion that GF can also be good models to carry out density estimation. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have introduced new generative models based on ensemble of trees and a top-down boosting-compliant training algorithm which casts training in a supervised 2-classes framework. Those generative forests allow not just efficient data generation: they also allow efficient missing data imputation and density estimation. Experiments have been carried out on all three problems against a variety of contenders, demonstrating the ability of small generative forests to perform well against potentially much more complex contenders. A responsible use of such models necessarily includes restriction to tabular data (Section 1). Experiments clearly show that there is a domain-dependent \"best fti\" size for our models; estimating it can be done via cross-validation but an important question, left for future work, is how to directly \"guess\" the right model size. A pruning algorithm with good generalization seems like a very promising direction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors thank the reviewers for engaging discussions and many suggestions that helped to improve the paper\u2019s content. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] L. Breiman, J. H. Freidman, R. A. Olshen, and C. J. Stone. Classification and regression trees. Wadsworth, 1984.   \n[2] A. Buja, W. Stuetzle, and Y. Shen. Loss functions for binary class probability estimation ans classification: structure and applications, 2005. Technical Report, University of Pennsylvania.   \n[3] T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In $22^{n d}\\ K D D$ , pages 785\u2013794, 2016.   \n[4] Y.-C. Chen. A tutorial on kernel density estimation and recent advances. Biostatistics & Epidemiology, 1:161\u2013187, 2017.   \n[5] Y. Choi, A. Vergari, and G. Van den Broeck. Probabilistic circuits: a unifying framework for tractable probabilistic models, 2020. http://starai.cs.ucla.edu/papers/ProbCirc20. pdf.   \n[6] M. Chui, J. Manyika, M. Miremadi, N. Henke, R. Chung P. Nel, and S. Malhotra. Notes from the AI frontier. McKinsey Global Institute, 2018.   \n[7] A.-H.-C. Correia, R. Peharz, and C.-P. de Campos. Joints in random forests. In NeurIPS\u201920, 2020.   \n[8] M. Cuturi. Sinkhorn distances: lightspeed computation of optimal transport. In $N I P S^{\\ast}26$ , pages 2292\u20132300, 2013.   \n[9] A. Darwiche. A logical approach to factoring belief networks. In Dieter Fensel, Fausto Giunchiglia, Deborah L. McGuinness, and Mary-Anne Williams, editors, KR\u201902, pages 409\u2013 420. Morgan Kaufmann, 2002.   \n[10] D. Dua and C. Graff. UCI machine learning repository, 2021.   \n[11] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A.-C. Courville. Adversarially learned inference. In ICLR\u201917. OpenReview.net, 2017.   \n[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS\\*27, pages 2672\u20132680, 2014.   \n[13] L. Grinsztajn, E. Oyallon, and G. Varoquaux. Why do tree-based models still outperform deep learning on tabular data? In NeurIPS\u201922 Datasets and Benchmarks, 2022.   \n[14] A. Grover and S. Ermon. Boosted generative models. In AAAI\u201918, pages 3077\u20133084. AAAI Press, 2018.   \n[15] G.-E. Hinton. The forward-forward algorithm: Some preliminary investigations. CoRR, abs/2212.13345, 2022.   \n[16] R.C. Holte. Very simple classification rules perform well on most commonly used datasets. MLJ, 11:63\u201391, 1993.   \n[17] A. Jolicoeur-Martineau, K. Fatras, and T. Kachman. Generating and imputing tabular data via diffusion and flow-based gradient-boosted trees. In AISTATS\u201924, volume 238, pages 1288\u20131296. PMLR, 2024.   \n[18] M.J. Kearns. Thoughts on hypothesis boosting, 1988. ML class project.   \n[19] M.J. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms. In Proc. of the $2\\&^{t h}$ ACM STOC, pages 459\u2013468, 1996.   \n[20] D.-P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR\u201914, 2014.   \n[21] D.-E. Knuth. Two notes on notation. The American Mathematical Monthly, 99(5):403\u2013422, 1992.   \n[22] A. Krizhevsky, I. Sutskever, and G.-E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS\\*25, pages 1106\u20131114, 2012.   \n[23] S. Lang, M. Mundt, F. Ventola, R. Peharz, and K. Kersting. Elevating perceptual sample quality in pcs through differentiable sampling. In NeurIPS 2021 Workshop on Pre-Registration in Machine Learning, 13 December 2021, Virtual, volume 181 of Proceedings of Machine Learning Research, pages 1\u201325. PMLR, 2021.   \n[24] Q. Li and J.-S. Racine. Nonparametric estimation of distributions with categorical and continuous data. Journal of Multivariate Analysis, 86:266\u2013292, 2003.   \n[25] Y. Mansour, R. Nock, and R.-C. Williamson. Random classification noise does not defeat all convex potential boosters irrespective of model choice. In ICML\u201923, 2023.   \n[26] P. Mar\u00e9chal. On a functional operation generating convex functions, part 1: duality. J. of Optimization Theory and Applications, 126:175\u2013189, 2005.   \n[27] P. Mar\u00e9chal. On a functional operation generating convex functions, part 2: algebraic properties. J. of Optimization Theory and Applications, 126:375\u2013366, 2005.   \n[28] D. McElfresh, S. Khandagale, J. Valverde, V. Prasad C, G. Ramakrishnan, M. Goldblum, and C. White. When do neural nets outperform boosted trees on tabular data? In NeurIPS\u201923 Datasets and Benchmarks, 2023.   \n[29] A. Menon and C.-S. Ong. Linking losses for density ratio and class-probability estimation. In $33^{r d}$ ICML, pages 304\u2013313, 2016.   \n[30] M.-F. Naeem, S.-J. Oh, Y. Uh, Y. Choi, and J. Yoo. Reliable fidelity and diversity metrics for generative models. In ICML\u201920, volume 119 of Proceedings of Machine Learning Research, pages 7176\u20137185. PMLR, 2020.   \n[31] R. Nock and M. Guillame-Bert. Generative trees: Adversarial and copycat. In $39^{t h}$ ICML, pages 16906\u201316951, 2022.   \n[32] R. Nock, A.-K. Menon, and C.-S. Ong. A scaled Bregman theorem with applications. In $N I P S^{\\ast}29$ , pages 19\u201327, 2016.   \n[33] S. Nowozin, B. Cseke, and R. Tomioka. $f$ -GAN: training generative neural samplers using variational divergence minimization. In $N I P S^{\\ast}29$ , pages 271\u2013279, 2016.   \n[34] E. Parzen. On estimation of a probability density function and mode. The Annals of Mathematical Statistics, 33:1065\u20131076, 1962.   \n[35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \n[36] J. R. Quinlan. C4.5 : programs for machine learning. Morgan Kaufmann, 1993.   \n[37] M.-D. Reid and R.-C. Williamson. Information, divergence and risk for binary experiments. JMLR, 12:731\u2013817, 2011.   \n[38] R. S\u00e1nchez-Cauce, I. Par\u00eds, and F.-J. D\u00edez. Sum-product networks: A survey. IEEE Trans.PAMI, 44(7):3821\u20133839, 2022.   \n[39] L.-J. Savage. Elicitation of personal probabilities and expectations. J. of the Am. Stat. Assoc., pages 783\u2013801, 1971.   \n[40] M. Sugiyama and M. Kawanabe. Machine Learning in Non-Stationary Environments - Introduction to Covariate Shift Adaptation. Adaptive computation and machine learning. MIT Press, 2012.   \n[41] N. Tagasovska, D. Ackerer, and T. Vatter. Copulas as high-dimensional generative models: Vine copula autoencoders. In NeurIPS\\*32, pages 6525\u20136537, 2019.   \n[42] S. van Buuren and K. Groothuis-Oudshoorn. mice: Multivariate Imputation by Chained Equations in R. Journal of Statistical Software, 45(3):1\u201367, 2011.   \n[43] A. Vergari, Y. Choi, and R. Peharz. Probabilistic circuits: Representations, inference, learning and applications, 2022. NeurIPS\u201922 tutorials.   \n[44] D.-S. Watson, K. Blesch, J. Kapar, and M.-N. Wright. Adversarial random forests for density and generative modeling. In AISTATS\u201923, Proceedings of Machine Learning Research. PMLR, 2023.   \n[45] I. Witten and E. Frank. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann, 1999.   \n[46] C. Xiao, P. Zhong, and C. Zheng. BourGAN: Generative networks with metric embeddings. In NeurIPS\u201918, pages 2275\u20132286, 2018.   \n[47] L. Xu, M. Skoularidou, A. Cuesta-Infante, and K. Veeramachaneni. Modeling tabular data using conditional GAN. In NeurIPS\\*32, pages 7333\u20137343, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To differentiate with the numberings in the main flie, the numbering of Theorems, etc. is letter-based (A, B, ...). ", "page_idx": 13}, {"type": "text", "text": "Table of contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Related work Pg 15 ", "page_idx": 13}, {"type": "text", "text": "Additional content Pg 16 ", "page_idx": 13}, {"type": "text", "text": "Supplementary material on proofs Pg 17   \n$\\hookrightarrow$ Proof of Lemma 4.4 Pg 17   \n$\\hookrightarrow$ Proof of Theorem 5.2 Pg 17 ", "page_idx": 13}, {"type": "text", "text": "Simpler models: ensembles of generative trees Pg 22 ", "page_idx": 13}, {"type": "text", "text": "Supplementary material on experiments Pg 25   \n$\\hookrightarrow$ Domains Pg 25   \n$\\hookrightarrow$ Algorithms configuration and choice of parameters Pg 25   \n$\\hookrightarrow$ Interpreting our models: \u2019SCRUTINIZE\u2019 Pg 27   \n$\\hookrightarrow$ More examples of Table 1 (MF) Pg 28   \n$\\hookrightarrow$ The generative forest of Table 1 (MF) developed further Pg 30   \n$\\hookrightarrow$ Full comparisons with MICE on missing data imputation Pg 35   \n$\\hookrightarrow$ Experiment LIFELIKE in extenso Pg 31   \n$\\hookrightarrow$ Comparison with \"the optimal generator\": GEN-DISCRIM Pg 33 ", "page_idx": 13}, {"type": "text", "text": "I Related work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The typical Machine Learning (ML) problem usually contains at least three parts: (i) a training algorithm minimizes (ii) a loss function to output an object whose key part is (iii) a model. The main problem we are interested in is data generation as generally captured by \"Generative AI\". The type of data we are interested in still represents one of the most prevalent form of data: tabular data [6]. When it comes to tabular data, a singular phenomenon of the data world can be observed: there is a fierce competition on part (iii) above, the models. When data has other forms, like images, the ML community has generally converged to a broad idea of what the best models look like at a high-level for many generic tasks: neural networks\u2021. Tabular data offers no such consensus yet, even on well-defined tasks like supervised learning [13]. In fact, even on such \"simple tasks\" the consensus is rather that there is no such consensus [28]. It is an understatement to state that in the broader context of all tasks of interest to us, a sense of a truly intense competition emerges, whose \"gradient\" clearly points towards simultaneously the most complex / expressive / tractable models, as shown in [43, Slides 27, 53]. One can thus end up finding models based on trees [7, 31, 44], neural networks [12, 14, 20, 47], probabilistic circuits [5, 43, 38], kernel methods [4, 34], graphical models [41] (among others: note that some are in fact hybrid models). ", "page_idx": 14}, {"type": "text", "text": "So the tabular data world is blessed with a variety of possible models to solve problems like the ones we are interested in, but \u2013 and this is another singular phenomenon of the tabular data world \u2013, getting the best solutions is not necessarily a matter of competing on size or compute. In fact, it can be the opposite: striving for model simplicity or (non exclusive) lightweight tuning can substantially pay off [28]. In relative terms, this phenomenon is not new in the tabular data world: it has been known for more than two decades [16]. That it has endured all major revolutions in ML points to the fact that lasting solutions can be conveniently addressing all three parts (i\u2013iii) above at once on models, algorithms and losses. ", "page_idx": 14}, {"type": "text", "text": "From this standpoint, the closest approaches to ours are [44] and [31], first and foremost because the models include trees with a stochastic activation of edges to pick leaves, and a leaf-dependent data generation process. While [31] learn a single tree, [44] use a way to generate data from a set of trees \u2013 called an adversarial random forest \u2013 which is simple: sample a tree, and then sample an observation from the tree. Hence, the distribution is a convex combination of the trees\u2019 density. This is simple but it suffers several drawbacks: (i) each tree has to be accurate enough and thus big enough to model \"tricky\" data for tree-based models (tricky can be low-dimensional, see the 2D data of Table 1, main flie); (ii) if leaves\u2019 samplers are simple, which is the case for [31] and our approach, it makes it tricky to learn sets of simple models, such as when trees are stumps (we do not have this issue, see Table 1). In our case, while our models include sets of trees, generating one observation makes use of leaves in all trees instead of just one as in [44] (Figure 1, main file). We note that the primary goal of that latter work is in fact not to generate data [44, Section 4]. ", "page_idx": 14}, {"type": "text", "text": "Theoretical results that are relevant to data generation are in general scarce compared to the flurry of existing methods if we omit the independent convergence rates of the toolbox often used, such as for (stochastic) gradient descent. Specific convergence results are given in [44], but they are relevant to statistical consistency (infinite sample) and they also rely on assumptions that are not realistic for real world domains, such as Lipschitz continuity of the target density, with second derivative continuous, square integrable and monotonic. The assumption made on models, that splitting probabilities on trees is lowerbounded by a constant, is also impeding to model real world data. ", "page_idx": 14}, {"type": "text", "text": "In the generative trees of [31], leaf generation is the simplest possible: it is uniform. This requires big trees to model real-world or tricky data. On the algorithms side, [31] introduce two training algorithms in the generative adversarial networks (GAN) framework [12], thus involving the generator to train but also a discriminator, which is a decision tree in [31]. The GAN framework is very convenient to tackle (ii) above in the context of our tasks because it allows to tie using first principles the problem of learning a density or a sampler and that of training a model (a \"discriminator\") to distinguish fakes from real, model that parameterizes the loss optimized. An issue with neural networks is that this parameterization has an uncontrollable slack unless the discriminator is extremely complex [33]. The advantage of using trees as in [31] is that for such classifiers, the slack disappears because the models are calibrated, so there is a tight link between training the generator and the discriminator. [31] go further, showing that one can replace the adversarial training by a copycat training, involving copying parts of the discriminator in the generator to speed-up training (also discussed in [15] for neural nets), with strong rates on training in the original boosting model. There is however a limitation in the convergence analysis of [31] as losses have to be symmetric, a property which is not desirable for data generation since it ties the misclassification costs of real and fakes with no argument to do so in general. ", "page_idx": 14}, {"type": "table", "img_path": "cRlQHncjwT/tmp/06d4139ee617e38552844b29be2673a202ce0cef668aa0d030b4ee56fa6ab2ec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "cRlQHncjwT/tmp/be71810f193a3ba104289a07eddb203359407deefaa9e41d30cf95828894f1a0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Our paper lifts the whole setting of [31] to models that can fti more complex densities using simpler models (Table 1), using sets of trees that can be much smaller than those of [44] (Figure 1); training such models is achieved by merging the two steps of copycat training into a single one where only the generator is trained, furthermore keeping strong rates on training via the original boosting model, all this while getting rid of the undesirable symmetry assumption of [31] for the loss at hand. ", "page_idx": 15}, {"type": "text", "text": "II Additional content ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this additional content, we provide the three ways to pick the trees in a Generative Forest to generate one observation (sequential, concurrent, randomized), and then give a proof that the optimal splitting threshold on a continuous variable when training a generative forest using GF.BOOST is always an observed value if there is one tree, but can be another value if there are more (thus highlighting some technical difficulties of one wants to stick to the optimal choice of splitting). ", "page_idx": 15}, {"type": "text", "text": "II.1 Sequentially choosing trees in a GF for the generation of observations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "II.2 Concurrent generation of observations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide a concurrent generation using Algorithm 5, which differs from Algorithm 4 (main file). In concurrent generation, each tree runs concurrently algorithm UPDATESUPPORT (hence the use of the Java-style this handler), with an additional global variables (in addition to $\\mathcal{C}_{\\mathrm{s}}$ , initialized to $\\mathcal{X}$ ): a Boolean semaphore accessible implementing a lock, whose value 1 means C is available for an update (and otherwise it is locked by a tree in Steps $1.2/1.3$ in the set of trees of the GF). We assume that INIT has been run beforehand (eventually locally). ", "page_idx": 15}, {"type": "text", "text": "II.3 Randomized generation of observations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide a general randomized choice of the sequence of trees for generation, in Algorithm 6. ", "page_idx": 15}, {"type": "table", "img_path": "cRlQHncjwT/tmp/e1aafb6b0084f1ab9e1c95bbe110678533fc47541996fac72f44195fb289187c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "III Supplementary material on proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "III.1 Proof of Lemma 4.4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given sequence $\\pmb{v}$ of dimension $\\dim(v)$ , denote $\\{\\mathcal{C}_{j}\\}_{j\\in[1+\\mathrm{dim}(\\pmb{v})]}$ the sequence of subsets of the domain appearing in the parameters of UPDATESUPPORT through sequence $\\pmb{v}$ , to which we add a last element, $\\mathcal{C}_{\\mathrm{s}}$ (and its first element is $\\mathcal{X}$ ). If we let $\\mathcal{X}_{j}$ denote the support of the corresponding star node whose Bernoulli event is triggered at index $j$ in sequence $\\pmb{v}$ (for example, $\\mathcal{X}_{1}=\\mathcal{X})$ ), then we easily get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{C}_{j}}&{{}\\subseteq}&{\\mathcal{X}_{j},\\forall j\\in[\\mathrm{dim}(\\pmb{v})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "indicating the Bernoulli probability in Step 1.1 of STARUPDATE is always defined. We then compute ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{p_{\\mathbb{G}}\\left[\\mathbb{e}_{s}\\right]\\!}&{=}&{p_{\\mathbb{G}}(\\gamma_{\\mathbb{G}})\\,}\\\\ &{=}&{\\displaystyle\\prod_{j=1}^{N}p_{\\mathbb{G}}\\left[\\varrho_{j+1}|\\varrho_{j}\\right]}\\\\ &&{=}&{\\displaystyle\\prod_{j=1}^{N}\\frac{p_{\\mathbb{B}}\\left[\\varrho_{\\mathbb{t}+1}\\right]}{p_{\\mathbb{B}}\\left[\\varrho_{\\mathbb{t}}\\right]}}\\\\ &{=}&{\\displaystyle\\frac{p_{\\mathbb{B}}\\left[\\varrho_{N+1}\\right]}{p_{\\mathbb{B}}\\left[\\varrho_{0}\\right]}}\\\\ &{=}&{\\displaystyle\\frac{p_{\\mathbb{B}}\\left[\\varrho_{\\mathbb{t}}\\right]}{p_{\\mathbb{B}}\\left[\\varrho_{1}\\right]}}\\\\ &&{=}&{p_{\\mathbb{B}}\\left[\\varrho_{\\mathbb{t}}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(7) holds because updating the support generation is Markovian in a GF, (8) holds because of Step 3.2 and (9) is a simplification through cancelling terms in products. ", "page_idx": 16}, {"type": "text", "text": "III.2 Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Notations: we iteratively learn generators $\\mathrm{G}_{0},\\mathrm{G}_{1},...,\\mathrm{G}_{J}$ where $\\mathrm{G_{0}}$ is just a root. We let $\\mathcal{P}(\\mathrm{G}_{j})$ denote the partition induced by the set of trees of ${\\mathrm{G}}_{j}$ , recalling that each element is the (non-empty) intersection of the support of a set of leaves, one for each tree (for example, $\\mathcal{P}(\\mathrm{G}_{0})\\,=\\,\\{\\mathcal{X}\\})$ ). The criterion minimized to build $\\mathrm{G}_{j+1}$ from ${\\mathrm{G}}_{j}$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underline{{\\mathbb{L}}}(\\mathrm{G}_{j})}&{\\doteq}&{\\displaystyle\\sum_{\\mathrm{\\tiny\\mathrm{{ec}\\cdot\\mathrm{\\tiny{P}(G}_{j})}}}p_{\\mathrm{\\tiny{M}}}[\\mathrm{{e}}]\\cdot\\underline{{L}}\\left(\\frac{\\pi p_{\\mathrm{{R}}}[\\mathrm{{e}}]}{p_{\\mathrm{\\tiny{~M}}}[\\mathrm{{e}}]}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The proof entails fundamental notions on loss functions, models and boosting. We start with loss functions. A key function we use is ", "page_idx": 16}, {"type": "equation", "text": "$$\ng^{\\pi}(t)\\doteq(\\pi t+1-\\pi)\\cdot\\underline{{L}}\\left(\\frac{\\pi t}{\\pi t+1-\\pi}\\right),\\forall t\\in\\mathbb{R}_{+},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is concave [37, Appendix A.3]. ", "page_idx": 16}, {"type": "text", "text": "Lemma A. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{J-1}\\right)-\\mathbb{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{J}\\right)}&{=}&{\\underline{{\\mathbb{L}}}\\big(\\mathrm{G}_{J-1}\\big)-\\underline{{\\mathbb{L}}}\\big(\\mathrm{G}_{J}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We make use of the following important fact about GFs: ", "page_idx": 16}, {"type": "text", "text": "(F1) For any $\\mathcal{X}^{\\prime}\\in\\mathcal{P}_{J},\\,\\int_{\\mathcal{X}^{\\prime}}\\mathrm{dG}_{J}=\\int_{\\mathcal{X}^{\\prime}}\\mathrm{dR}.$ ", "page_idx": 17}, {"type": "text", "text": "We have from the proof of [31, Lemma 5.6], ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{J}\\right)}&{=}&{\\displaystyle\\sum_{x^{\\prime}\\in\\mathcal{P}_{J}}\\left\\{g^{\\pi}\\left(\\mathbb{E}_{\\mathrm{U}|x^{\\prime}}\\left[\\frac{\\mathrm{d}\\mathrm{G}_{J}}{\\mathrm{dU}}\\right]\\right)-\\mathbb{E}_{\\mathrm{U}|x^{\\prime}}\\left[g^{\\pi}\\left(\\frac{\\mathrm{d}\\mathrm{R}}{\\mathrm{dU}}\\right)\\right]\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We note that (F1) implies (13) is just a sum of slacks of Jensen\u2019s inequality. We observe ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{J-1}\\right)-\\mathbb{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{J}\\right)}}\\\\ {=}&{\\sum_{v^{\\prime}\\in\\mathcal{P}_{J-1}}\\left\\{g^{\\pi}\\left(\\mathbb{E}_{\\cup\\vert x^{\\prime}}\\left[\\frac{\\mathrm{d}\\mathrm{G}_{J-1}}{\\mathrm{d}\\mathrm{U}}\\right]\\right)-\\mathbb{E}_{\\cup\\vert x^{\\prime}}\\left[g^{\\pi}\\left(\\frac{\\mathrm{d}\\mathrm{R}}{\\mathrm{d}\\mathrm{U}}\\right)\\right]\\right\\}}\\\\ &{-\\underbrace{\\sum_{x^{\\prime}\\in\\mathcal{P}_{J}}\\left\\{g^{\\pi}\\left(\\mathbb{E}_{\\cup\\vert x^{\\prime}}\\left[\\frac{\\mathrm{d}\\mathrm{G}_{J}}{\\mathrm{d}\\mathrm{U}}\\right]\\right)-\\mathbb{E}_{\\cup\\vert x^{\\prime}}\\left[g^{\\pi}\\left(\\frac{\\mathrm{d}\\mathrm{R}}{\\mathrm{d}\\mathrm{U}}\\right)\\right]\\right\\}}_{=\\!\\!\\!\\!\\mathrm{C}_{\\mathrm{U}}\\left(\\mathrm{S}_{\\mathrm{U}}\\right)\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathcal{P}_{J}^{\\mathrm{split}}$ contains all couples $(\\mathcal X_{\\mathrm{t}},\\mathcal X_{\\mathrm{f}})$ such that $\\mathcal{X}_{\\mathrm{t}},\\mathcal{X}_{\\mathrm{f}}\\in\\mathcal{P}_{J}$ and $\\mathcal{X}_{\\mathrm{t}}\\cup\\mathcal{X}_{\\mathrm{t}}\\in\\mathcal{P}_{J-1}$ . These unions were subsets of the partition of $\\mathcal{X}$ induced by the set of trees of $\\mathrm{G}_{J-1}$ and that were cut by the predicate $\\mathtt{p}$ put at the leaf $\\lambda_{*}$ that created $\\mathrm{G}_{J}$ from $\\mathrm{G}_{J-1}$ . To save space, denote $\\mathcal{X}_{\\mathrm{tf}}\\doteq\\mathcal{X}_{\\mathrm{t}}\\cup\\dot{\\mathcal{X}_{\\mathrm{f}}}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{g^{\\pi}\\left(\\mathbb{E}_{\\mathrm{U}|\\mathcal{X}_{\\mathrm{t}}}\\left[\\frac{\\mathrm{d}\\mathbf{G}_{J-1}}{\\mathrm{d}{\\mathbf{U}}}\\right]\\right)-\\underset{\\mathrm{v}\\in\\{\\mathbf{t},\\mathbf{t}\\}}{\\sum}g^{\\pi}\\left(\\mathbb{E}_{\\mathrm{U}|\\mathcal{X}_{\\mathrm{v}}}\\left[\\frac{\\mathrm{d}\\mathbf{G}_{J}}{\\mathrm{d}{\\mathbf{U}}}\\right]\\right)}\\\\ {=}&{\\left(\\pi\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathbf{G}_{J-1}+(1-\\pi)\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathrm{U}\\right)\\cdot\\underline{{L}}\\left(\\frac{\\pi\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathbf{G}_{J-1}}{\\pi\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathbf{G}_{J-1}+(1-\\pi)\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathrm{U}}\\right)}\\\\ &{-\\left(\\pi\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathbf{G}_{J}+(1-\\pi)\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathrm{U}\\right)\\cdot\\underline{{L}}\\left(\\frac{\\pi\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathbf{G}_{J}}{\\pi\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathbf{G}_{J}+(1-\\pi)\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathrm{U}}\\right)}\\\\ &{-\\left(\\pi\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathbf{G}_{J}+(1-\\pi)\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathrm{U}\\right)\\cdot\\underline{{L}}\\left(\\frac{\\pi\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathbf{G}_{J}}{\\pi\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathbf{G}_{J}+(1-\\pi)\\int_{\\mathcal{X}_{\\mathrm{t}}}\\mathrm{d}\\mathrm{U}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now work on (14). Using (F1), we note $\\int_{\\mathcal{X}_{\\mathrm{tf}}}\\mathrm{d}\\mathrm{G}_{J-1}\\,=\\,\\int_{\\mathcal{X}_{\\mathrm{tf}}}$ dR since $\\mathcal{X}_{\\mathrm{tf}}\\,\\in\\,\\mathcal{P}_{J-1}$ . Similarly, $\\int_{\\mathcal{X}_{v}}\\mathrm{dG}_{J}=\\int_{\\mathcal{X}_{v}}$ dR, $\\forall\\mathtt{v}\\in\\{\\mathtt{t},\\mathtt{f}\\}$ , so we make a\u015fppear $\\mathrm{R}$ in (14) \u015fand get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g^{\\pi}\\left(\\mathbb{E}_{\\mathrm{U}|\\mathcal{X}_{\\mathrm{tt}}}\\left[\\frac{\\mathrm{d}\\mathrm{G}_{J-1}}{\\mathrm{~d}\\mathrm{U}}\\right]\\right)-\\displaystyle\\sum_{\\mathrm{v}\\in\\{\\mathrm{t},\\mathrm{f}\\}}g^{\\pi}\\left(\\mathbb{E}_{\\mathrm{U}|\\mathcal{X}_{\\mathrm{v}}}\\left[\\frac{\\mathrm{d}\\mathrm{G}_{J}}{\\mathrm{~d}\\mathrm{U}}\\right]\\right)}\\\\ {=}&{p(\\mathbf{t}\\mathbf{f})\\cdot\\left\\{\\displaystyle\\underline{{L}}\\left(\\frac{\\pi\\,\\mathrm{\\{\\sum_{\\boldsymbol{x}}}}_{\\mathrm{tt}}\\,\\mathrm{d}\\mathrm{R}}{p(\\mathbf{t}\\mathbf{f})}\\right)-\\frac{p(\\mathbf{t})}{p(\\mathbf{t}\\mathbf{f})}\\cdot\\displaystyle\\underline{{L}}\\left(\\frac{\\pi\\,\\mathrm{\\sum_{\\boldsymbol{x}}}_{\\mathrm{tt}}\\,\\mathrm{d}\\mathrm{R}}{p(\\mathbf{t})}\\right)-\\frac{p(\\mathbf{f})}{p(\\mathbf{t}\\mathbf{f})}\\cdot\\displaystyle\\underline{{L}}\\left(\\frac{\\pi\\,\\mathrm{\\{\\sum_{\\boldsymbol{x}}}}_{\\mathrm{tt}}\\,\\mathrm{d}\\mathrm{R}}{p(\\mathbf{f})}\\right)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we let for short ", "page_idx": 17}, {"type": "equation", "text": "$$\np(\\mathbf{v})\\quad\\doteq\\quad\\pi\\int_{\\mathcal{X}_{\\mathbf{v}}}\\mathrm{d}\\mathrm{R}+\\left(1-\\pi\\right)\\int_{\\mathcal{X}_{\\mathbf{v}}}\\mathrm{d}\\mathrm{U},\\forall\\mathbf{v}\\in\\{\\mathbf{t},\\mathbf{f},\\mathbf{t}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We finally get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{J-1}\\right)-\\mathfrak{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{J}\\right)}\\\\ {=}&{\\displaystyle\\sum_{\\mathrm{e}\\in\\mathfrak{P}_{J-1}}p_{\\mathrm{\\scriptscriptstyle{M}}}[\\mathfrak{C}]\\cdot\\underline{{L}}\\left(\\frac{\\pi p_{\\mathrm{R}}[\\mathfrak{C}]}{p_{\\mathrm{\\scriptscriptstyle{M}}}[\\mathfrak{C}]}\\right)-\\sum_{\\mathrm{e}\\in\\mathfrak{P}_{J-1}}p_{\\mathrm{\\scriptscriptstyle{M}}}[\\mathfrak{C}]\\cdot\\underbrace{\\sum_{\\mathfrak{p}\\mathrm{\\scriptscriptstyle{M}}}[\\mathfrak{C}_{\\mathfrak{p}_{J-1},\\mathfrak{v}}]}_{\\mathfrak{p}_{\\mathrm{\\scriptscriptstyle{M}}}[\\mathfrak{C}]}\\cdot\\underline{{L}}\\left(\\frac{\\pi p_{\\mathrm{\\scriptscriptstyle{R}}}[\\mathfrak{C}_{\\mathfrak{p}_{J-1},\\mathfrak{v}}]}{p_{\\mathrm{\\scriptscriptstyle{M}}}[\\mathfrak{C}_{\\mathfrak{p}_{J-1},\\mathfrak{v}}]}\\right)}\\\\ {=}&{\\displaystyle\\mathbb{L}\\big(\\mathrm{G}_{J-1}\\big)-\\underline{{\\mathbb{L}}}\\big(\\mathrm{G}_{J}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as claimed. The last identity comes from the fact that the contribution to $\\underline{{\\mathbb{L}}}(.)$ is the same outside $\\mathcal{P}_{J-1}$ for both $\\mathrm{G}_{J-1}$ and $\\mathrm{G}_{J}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Input: measure R, SPD loss $\\ell$ , weak learner WL, iteration number $J\\geqslant1$ , number of trees $T$ ;   \nOutput: PLM (partition-linear model) $H_{J}$ with $T$ trees;   \nStep $1:\\Gamma_{0}\\doteq\\dot{\\{\\Upsilon_{t}\\}}_{t=1}^{T}\\gets\\{\\mathrm{root},\\mathrm{root},...,\\mathrm{root}\\}$ ; $\\mathcal{P}(\\Gamma_{0})\\gets\\{\\mathcal{X}\\}$ ;   \nStep 2 : for $j=1,2,...,J$ Step 2.1 : pick $t\\in[T]$ ; Step $2.2:$ pick $\\lambda\\in\\Lambda(\\Upsilon_{t})$ ; Step $2.3:h_{j}\\gets\\mathrm{wL}(\\dot{\\mathcal{X}}_{\\lambda})$ ; // call to the weak learner for some $h_{j}$ of the type $h_{j}\\doteq1_{\\mathtt{p}(.)}\\cdot K_{j}$ , // $K_{j}$ constant, $\\mathsf{p}(.)$ a splitting predicate (e.g. $x_{i}\\geqslant a$ ) Step $2.4:$ for $\\mathring{\\mathrm{C}}\\in\\mathcal{P}_{\\lambda}\\big(\\Gamma_{j-1}\\big)$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathfrak{C P}}&{\\leftarrow}&{\\mathfrak{C}\\cap\\{x:\\mathfrak{p}(x)\\mathrm{~is~true}\\},}\\\\ {\\alpha(\\mathfrak{C P})}&{\\leftarrow}&{(-\\underline{{L}}^{\\prime})\\left(\\frac{\\pi{p}_{\\mathrm{R}}[\\mathfrak{C P}]}{p_{\\mathrm{\\scriptscriptstyle{M}}}[\\mathfrak{C P}]}\\right),\\ \\ \\ }\\\\ {\\mathfrak{C}^{\\rightarrow\\mathrm{p}}}&{\\leftarrow}&{\\mathfrak{C}\\cap\\{x:\\mathfrak{p}(x)\\mathrm{~is~}\\mathfrak{f a l s e}\\},}\\\\ {\\alpha(\\mathfrak{C}^{\\rightarrow\\mathrm{p}})}&{\\leftarrow}&{-(-\\underline{{L}}^{\\prime})\\left(\\frac{\\pi{p}_{\\mathrm{R}}[\\mathfrak{C}^{\\rightarrow\\mathrm{p}}]}{p_{\\mathrm{\\scriptscriptstyle{M}}}[\\mathfrak{C}^{\\rightarrow\\mathrm{p}}]}\\right).\\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Step $2.5://$ update $\\mathcal{P}(\\Gamma_{.})$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{P}(\\Gamma_{j})}&{\\leftarrow}&{\\big(\\mathcal{P}(\\Gamma_{j-1})\\backslash\\mathcal{P}_{\\lambda}(\\Gamma_{j-1})\\big)\\cup\\big(\\cup_{\\Theta\\in\\mathcal{P}_{\\lambda}(\\Gamma_{j-1}):\\mathbb{C}^{\\flat}\\ne\\mathcal{Q}}\\mathbb{C}^{\\mathbb{P}}\\big)\\cup\\big(\\cup_{\\Theta\\in\\mathcal{P}_{\\lambda}(\\Gamma_{j-1}):\\mathbb{C}^{-\\flat}\\ne\\mathcal{Q}}\\mathbb{C}^{-\\mathbb{P}\\backslash\\mathcal{P}_{1}}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Step $2.6:$ split leaf $\\lambda$ at $\\Upsilon_{t}$ using predicate $\\mathtt{p}$ ; ", "page_idx": 18}, {"type": "text", "text": "return $H_{J}(\\pmb{x})\\doteq\\alpha\\left(\\mathbb{C}(\\pmb{x})\\right)$ with $\\mathcal{C}(\\pmb{x})\\doteq\\bar{\\mathbb{C}}\\in\\mathcal{P}(\\Gamma_{J}\\bar{\\right)}$ such that $\\pmb{x}\\in\\mathcal{C}$ ; ", "page_idx": 18}, {"type": "text", "text": "We now come to models and boosting. Suppose we have $t$ trees $\\{\\Upsilon_{1},\\Upsilon_{2},...,\\Upsilon_{t}\\}$ in ${\\mathrm{G}}_{j}$ . We want to split leaf $\\lambda\\in\\Upsilon_{t}$ into two new leaves, $\\lambda_{\\mathrm{f}},\\lambda_{\\mathrm{t}}$ . Let $\\mathcal{P}_{\\lambda}(\\mathrm{G}_{j})$ denote the subset of $\\mathcal{P}(\\mathrm{G}_{j})$ containing only the subsets defined by intersection with the support of $\\lambda$ , $\\mathcal{X}_{\\lambda}$ . The criterion we minimize can be reformulated as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{L}({\\mathrm G}_{j})}&{=}&{\\displaystyle\\sum_{\\lambda\\in\\Lambda(\\Upsilon)}p_{\\mathrm{\\,M}}[{\\mathcal X}_{\\lambda}]\\cdot\\sum_{\\mathrm{\\,~e}\\in{\\mathcal P}_{\\lambda}({\\mathrm G}_{j})}\\frac{p_{\\mathrm{\\,M}}[\\mathcal C]}{p_{\\mathrm{\\,M}}[{\\mathcal X}_{\\lambda}]}\\cdot\\underline{{L}}\\left(\\frac{\\pi p_{\\mathrm{\\,R}}[\\mathcal C]}{p_{\\mathrm{\\,M}}[\\mathcal C]}\\right),\\Upsilon\\in\\{\\Upsilon_{1},\\Upsilon_{2},...,\\Upsilon_{t}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, $\\Upsilon$ is any tree in the set of trees, since $\\cup_{\\lambda\\in\\Lambda(\\Upsilon)}\\mathcal{P}_{\\lambda}(\\mathrm{G}_{j})$ covers the complete partition of $\\mathcal{X}$ induced by $\\left\\{\\Upsilon_{1},\\Upsilon_{2},...,\\Upsilon_{t}\\right\\}$ . If we had a single tree, the inner sum would disappear since we would have $\\mathcal{P}_{\\lambda}(\\mathrm{G}_{j})=\\{\\mathcal{X}_{\\lambda}\\}$ and so one iteration would split one of these subsets. In our case however, with a set of trees, we still split $\\mathcal{X}_{\\lambda}$ but the impact on the reduction of $\\mathbb{L}$ can be substantially better as we may simultaneously split as many subsets as there are in $\\mathcal{P}_{\\lambda}(\\mathrm{G}_{j})$ . The reduction in $\\mathbb{L}$ can be obtained by summing all reductions to which contribute each of the subsets. ", "page_idx": 18}, {"type": "text", "text": "To analyze it, we make use of a reduction to the MODABOOST algorithm of [25]. We present the algorithm in Algorithm 7. The original MODABOOST algorithm trains partition-linear models, i.e. models whose output is defined by a sum of reals over a set of partitions to which the observation to be classified belongs to. Training means then both learning the organisation of partitions and the reals \u2013 which are just the output of a weak classifier given by a weak learner, times a leveraging constant computed by MODABOOST. As in the classical boosting literature, the original MODABOOST includes the computation and updates of weights. ", "page_idx": 18}, {"type": "text", "text": "In our case, the structure of partition learned is that of a set of trees, each weak classifier $h$ is of the form ", "page_idx": 18}, {"type": "equation", "text": "$$\nh(\\pmb{x})\\quad\\doteq\\quad K\\cdot\\mathtt{p}(\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $K$ is a real and $\\mathtt{p}$ is a Boolean predicate, usually doing an axis-parallel split of $\\mathcal{X}$ on an observed feature, such as ppxq \u201d 1xi\u011ba. ", "page_idx": 18}, {"type": "text", "text": "From now on, it will be interesting to dissociate our generator ${\\mathrm{G}}_{j}$ \u2013 which includes a model structure and an algorithm to generate data from this structure\u2013 from its set of trees \u2013 i.e. its model structure \u2013 which we denote $\\Gamma_{j}\\ \\stackrel{\\cdot}{=}\\ \\{\\Upsilon_{t}\\}_{t=1}^{T}$ . MODABOOST learns both $\\Gamma_{j}$ but also predictions for each possible outcome, predictions that we shall not use since the loss we are minimizing can be made to depend only on the model structure $\\Gamma_{j}$ . Given the simplicity of the weak classifiers $h$ and the particular nature of the partitions learned, the original MODABOOST of [25] can be simplified to our presentation in Algorithm 7. Among others, the simplification discards the weights from the presentation of the algorithm. What we show entangles two objectives on models and loss functions as we show that ", "page_idx": 18}, {"type": "image", "img_path": "cRlQHncjwT/tmp/b0ddf058be367fb55715ffd739346705d49e7e73c8412a53c624550e2c927482.jpg", "img_caption": ["Figure III.1: Learning a set of two trees $\\Gamma$ with MODABOOST. If we want a split at leaf $\\lambda$ indicated, then it would takes two steps (but a single application of the weak learning assumption, [25, Lemma 5]) of the original MODABOOST to learn $\\Upsilon_{1}$ alone [25, Section 4]. Each step, figured with a plain arrow, ends with adding a new leaf. In our case however, we have to take into account the partition induced by the leaves of $\\Upsilon_{2}$ , which results in considering not one but three subsets in $\\mathcal{P}_{\\lambda}$ , thus adding not one but three weak hypotheses simultaneously (one for each of the dashed arrows on $\\Upsilon_{2}$ ). Thanks to Jensen\u2019s inequality, a guaranteed decrease of the loss at hand can be obtained by a single application of the weak learning assumption at $\\lambda$ , just like in the original MODABOOST. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "MODABOOST learns the same structure $\\Gamma$ as our GF.BOOST, and yields a guaranteed decrease of LpGq, ", "page_idx": 19}, {"type": "text", "text": "and it is achieved via the following Theorem. ", "page_idx": 19}, {"type": "text", "text": "Theorem B. MODABOOST greedily minimizes the following loss function: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{L}(\\Gamma_{j})}&{=}&{\\displaystyle\\sum_{\\lambda\\in\\Lambda(\\Upsilon)}p_{\\mathrm{\\scriptsize~M}}[\\mathcal{X}_{\\lambda}]\\cdot\\sum_{\\mathbb{C}\\in\\mathcal{P}_{\\lambda}(\\Gamma_{j})}\\frac{p_{\\mathrm{\\scriptsize~M}}[\\mathcal{C}]}{p_{\\mathrm{\\scriptsize~M}}[\\mathcal{X}_{\\lambda}]}\\cdot\\underline{{L}}\\left(\\frac{\\pi p_{\\mathrm{\\scriptsize{R}}}[\\mathcal{C}]}{p_{\\mathrm{\\scriptsize~M}}[\\mathcal{C}]}\\right),\\Upsilon\\in\\Gamma_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Furthermore, suppose there exists $\\gamma>0$ , $\\kappa>0$ such that at each iteration $j$ , the predicate p splits $\\mathcal{X}_{\\lambda}$ of leaf $\\lambda\\in\\Lambda(\\Upsilon)$ into $\\mathcal{X}_{\\lambda}^{p}$ and $\\mathfrak{X}_{\\lambda}^{\\lnot p}$ (same nomenclature as in (17), (19)) such that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{p_{\\mathrm{\\Theta}}[\\mathfrak{X}_{\\lambda}]}&{\\geqslant\\;\\;\\frac{1}{\\mathrm{Card}(\\Lambda(\\Upsilon))},}\\\\ {\\left|\\displaystyle\\sum_{\\overline{{\\mathrm{X}_{\\lambda}}}}\\mathrm{d}\\mathrm{R}-\\frac{\\displaystyle\\sum_{x_{\\lambda}^{\\mathrm{p}}}\\mathrm{d}\\mathrm{U}}{\\displaystyle\\sum_{x_{\\lambda}}\\mathrm{d}\\mathrm{U}}\\right|}&{\\geqslant\\;\\;\\gamma,}\\\\ {\\operatorname*{min}\\left\\{\\frac{\\pi p_{\\mathrm{R}}[\\mathcal{X}_{\\lambda}]}{p_{\\mathrm{\\Theta}}[\\mathcal{X}_{\\lambda}]},1-\\frac{\\pi p_{\\mathrm{R}}[\\mathcal{X}_{\\lambda}]}{p_{\\mathrm{\\Theta}}[\\mathcal{X}_{\\lambda}]}\\right\\}}&{\\geqslant\\;\\;\\kappa.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then we have the following guaranteed slack between two successive models: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underline{{\\mathbb{L}}}(\\Gamma_{j+1})-\\underline{{\\mathbb{L}}}(\\Gamma_{j})}&{\\leqslant}&{-\\frac{\\kappa\\gamma^{2}\\kappa^{2}}{8\\mathrm{Card}(\\Lambda(\\Upsilon))},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where \u03ba is such that $0<\\kappa\\leqslant\\operatorname*{inf}\\{\\ell_{-1}^{\\prime}-\\ell_{1}^{\\prime}\\}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof sketch The loss function comes directly from [25, Lemma 7]. At each iteration, MODABOOST makes a number of updates that guarantee, once Step 2.4 is completed (because ", "page_idx": 19}, {"type": "text", "text": "the elements in ${\\mathcal{P}}_{\\lambda}(\\Gamma_{j})$ are disjoint) ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathbb{L}\\big(\\Gamma_{j+1}\\big)-\\underline{{\\mathbb{L}}}\\big(\\Gamma_{j}\\big)}&{\\leqslant}&{-\\displaystyle\\frac{\\kappa}{2}\\cdot\\sum_{\\ell\\in\\mathcal{P}_{\\lambda}(\\Gamma_{j})}p_{\\mathrm{\\scriptsize~M}}[\\mathcal{C}]\\cdot\\mathbb{E}_{\\mathrm{\\scriptsize~M}|\\mathcal{C}}\\left[(w_{j+1}-w_{j})^{2}\\right]}\\\\ &&{=}&{-\\displaystyle\\frac{\\kappa}{2}\\cdot p_{\\mathrm{\\scriptsize~M}}[\\mathcal{X}_{\\lambda}]\\mathbb{E}_{\\mathrm{\\scriptsize~M}|\\mathcal{X}_{\\lambda}}\\left[(w_{j+1}-w_{j})^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the weights are given in [25]. Each expression in the summand of (26) is exactly the guarantee of [25, ineq. before (69)]; all such expressions are not important; what is more important is (26): all steps occurring in Step 2.4 are equivalent to a single step of MODABOOST carried out over the whole $\\mathcal{X}_{\\lambda}$ . Overall, the number of \"aggregated\" steps match the counter $j$ in MODABOOST. We then just have to reuse the proof of [25, Theorem B], which implies, in lieu of their [25, eq. (74)] ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{L}(\\Gamma_{j+1})-\\mathbb{L}(\\Gamma_{j})}&{\\leqslant}&{-2\\kappa\\cdot p_{\\mathrm{\\scriptscriptstyleM}}[\\mathcal{X}_{\\lambda}]\\cdot\\left(\\frac{1}{2}\\cdot\\left(\\frac{\\displaystyle\\sum_{\\gamma_{\\lambda}^{\\mathrm{p}}}\\mathrm{d}\\mathrm{R}}{\\displaystyle\\sum_{\\gamma_{\\lambda}}\\mathrm{d}\\mathrm{R}}-\\frac{\\displaystyle\\sum_{\\gamma_{\\lambda}^{\\mathrm{p}}}\\mathrm{d}\\mathrm{U}}{\\displaystyle\\sum_{\\gamma_{\\lambda}}\\mathrm{d}\\mathrm{U}}\\right)\\right)^{2}\\cdot\\underline{{L}}^{\\mathrm{s0}}\\left(\\frac{\\displaystyle\\pi p_{\\mathrm{\\scriptscriptstyleR}}[\\mathcal{X}_{\\lambda}]}{\\displaystyle p_{\\mathrm{\\scriptscriptstyleM}}[\\mathcal{X}_{\\lambda}]}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with $\\underline{{L}}^{\\mathrm{sq}}(u)\\doteq u(1-u)$ . Noting $2\\underline{{L}}^{\\mathrm{sq}}(u)\\geqslant\\operatorname*{min}\\{u,1-u\\}$ , we then use (22) \u2013 (24), which yields the statement of the Theorem. ", "page_idx": 20}, {"type": "text", "text": "As a consequence, if we assume that the total number of boosting iterations $J$ is a multiple of the number of trees $T$ , it comes from [25, eq. (29)] that after $J$ iterations, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\underline{{\\mathbb{L}}}(\\Gamma_{J})-\\underline{{\\mathbb{L}}}(\\Gamma_{0})\\;\\;\\leqslant\\;\\;-\\frac{\\kappa\\gamma^{2}\\kappa^{2}}{8}\\cdot T\\log\\left(1+\\frac{J}{T}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using Lemma A and the fact that the induction of the sets of trees in MODABOOST is done in the same way as the induction of the set of trees of our generator $\\mathrm{G}$ in GF.BOOST, we get: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\pi\\cdot\\mathbb{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{J}\\right)}&{=}&{\\pi\\cdot\\mathbb{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{0}\\right)+\\left(\\mathbb{L}(\\mathrm{G}_{J})-\\mathbb{L}(\\mathrm{G}_{0})\\right)}\\\\ &{=}&{\\pi\\cdot\\mathbb{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{0}\\right)+\\left(\\mathbb{L}(\\Gamma_{J})-\\mathbb{L}(\\Gamma_{0})\\right)}\\\\ &{\\leqslant}&{\\pi\\cdot\\mathbb{D}_{\\ell}\\left(\\mathrm{R},\\mathrm{G}_{0}\\right)-\\frac{\\kappa\\gamma^{2}\\kappa^{2}}{8}\\cdot T\\log\\left(1+\\displaystyle\\frac{J}{T}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as claimed. ", "page_idx": 20}, {"type": "text", "text": "Remark C. One may wonder what is the relationship between the loss that we minimize and \"conventional\" losses used to train generative models. The advantage of our Theorem 5.2 is that by choosing different proper losses, one can get convergence guarantees for different \"conventional\" losses. Let us illustrate this with the KL divergence between measures A and B, noted $\\mathrm{{KL}(A\\parallel B)}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma D. Suppose the prior satisfies $\\pi>0$ and R absolutely continuous with respect to G. Then for the choice $\\ell\\doteq\\ell^{\\mathrm{LOG}}=l o g$ -loss, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{D}_{\\ell^{\\mathrm{LoG}}}\\left(\\mathrm{R},\\mathrm{G}\\right)}&{=}&{\\pi\\cdot\\mathrm{KL}\\left(\\mathrm{R}\\|\\mathrm{G}\\right)-\\mathrm{KL}\\left(\\pi\\mathrm{d}\\mathrm{R}+(1-\\pi)\\mathrm{d}\\mathrm{U}\\|\\pi\\mathrm{d}\\mathrm{G}+(1-\\pi)\\mathrm{d}\\mathrm{U}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We first recall that for any convex $F$ , we have with our choice of $g$ (assuming $\\pi>0$ ), ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{{\\check{F}(z)}}&{{=}}&{{\\displaystyle\\frac{\\pi z+1-\\pi}{\\pi}\\cdot F\\left({\\frac{\\pi z}{\\pi z+1-\\pi}}\\right),}}\\\\ {{\\check{F}^{\\prime}(z)}}&{{=}}&{{F\\left({\\frac{\\pi z}{\\pi z+1-\\pi}}\\right)+\\displaystyle{\\frac{1-\\pi}{\\pi z+1-\\pi}}\\cdot F^{\\prime}\\left({\\frac{\\pi z}{\\pi z+1-\\pi}}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The log-loss has $\\ell_{1}^{\\mathrm{{LoG}}}(u)=-\\log u,\\ell_{-1}^{\\mathrm{{LoG}}}(u)=-\\log(1-u)$ . It is strictly proper and so it comes ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left(\\overbrace{-\\underline{{L}}}\\right)(z)\\quad=\\quad z\\cdot\\log\\left(\\frac{\\pi z}{\\pi z+1-\\pi}\\right)+\\frac{1-\\pi}{\\pi}\\cdot\\log\\left(\\frac{1-\\pi}{\\pi z+1-\\pi}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We thus get, in our case ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left(\\widecheck{-\\underline{{L}}}\\right)\\left(\\frac{\\mathrm{d}\\mathrm{R}}{\\mathrm{d}\\mathrm{U}}\\right)}&{=}&{\\frac{\\mathrm{d}\\mathrm{R}}{\\mathrm{d}\\mathrm{U}}\\cdot\\log\\left(\\frac{\\pi\\mathrm{d}\\mathrm{R}}{\\pi\\mathrm{d}\\mathrm{R}+(1-\\pi)\\mathrm{d}\\mathrm{U}}\\right)+\\frac{1-\\pi}{\\pi}\\cdot\\log\\left(\\frac{(1-\\pi)\\mathrm{d}\\mathrm{U}}{\\pi\\mathrm{d}\\mathrm{R}+(1-\\pi)\\mathrm{d}\\mathrm{U}}\\right)+}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "image", "img_path": "cRlQHncjwT/tmp/b1c221e889b0c2bde23dea08d668e4b0002fdfec945264d78e7ac9811169d7f4.jpg", "img_caption": ["Figure IV.1: A generative tree (GT) associated to UCI German Credit. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "and using (32) and the fact that for the log-loss $(-\\underline{{L}})^{\\prime}(u)=\\log(u/(1-u)).$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\left(\\widecheck{-\\underline{{L}}}\\right)^{\\prime}\\left(\\frac{\\mathrm{d}\\mathrm{G}}{\\mathrm{d}\\mathrm{U}}\\right)}&{=}&{\\frac{\\pi\\mathrm{d}\\mathrm{G}}{\\pi\\mathrm{d}\\mathrm{G}+(1-\\pi)\\mathrm{d}\\mathrm{U}}\\cdot\\log\\left(\\frac{\\pi\\mathrm{d}\\mathrm{G}}{\\pi\\mathrm{d}\\mathrm{G}+(1-\\pi)\\mathrm{d}\\mathrm{U}}\\right)}\\\\ &&{+\\frac{(1-\\pi)\\mathrm{d}\\mathrm{U}}{\\pi\\mathrm{d}\\mathrm{G}+(1-\\pi)\\mathrm{d}\\mathrm{U}}\\cdot\\log\\left(\\frac{(1-\\pi)\\mathrm{d}\\mathrm{U}}{\\pi\\mathrm{d}\\mathrm{G}+(1-\\pi)\\mathrm{d}\\mathrm{U}}\\right)}\\\\ &&{+\\frac{(1-\\pi)\\mathrm{d}\\mathrm{U}}{\\pi\\mathrm{d}\\mathrm{G}+(1-\\pi)\\mathrm{d}\\mathrm{U}}\\cdot\\log\\left(\\frac{\\pi\\mathrm{d}\\mathrm{G}}{(1-\\pi)\\mathrm{d}\\mathrm{U}}\\right)=\\log\\left(\\frac{\\pi\\mathrm{d}\\mathrm{G}}{\\pi\\mathrm{d}\\mathrm{G}+(1-\\pi)\\mathrm{d}\\mathrm{U}}\\right)\\!\\!\\3\\mathrm{4})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, we get, assuming $\\mathrm{R}$ absolutely continuous with respect to G, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\doteq}&{\\pi\\cdot\\int_{\\mathcal{X}}\\mathrm{d}\\mathrm{U}\\cdot\\left(\\left(\\widetilde{-\\frac{d}{d}}\\right)\\left(\\frac{\\mathrm{d}\\mathrm{R}}{\\mathrm{d}\\mathrm{U}}\\right)-\\left(\\widetilde{-\\frac{d}{d}}\\right)\\left(\\frac{\\mathrm{d}\\mathrm{G}}{\\mathrm{d}\\mathrm{U}}\\right)-\\left(\\frac{\\mathrm{d}\\mathrm{R}-\\mathrm{d}\\mathrm{G}}{\\mathrm{d}\\mathrm{U}}\\right)\\cdot\\left(\\widetilde{-\\frac{d}{d}}\\right)^{\\prime}\\left(\\frac{\\mathrm{d}\\mathrm{G}}{\\mathrm{d}\\mathrm{U}}\\right)\\right)}\\\\ {=}&{\\int_{\\mathcal{X}}\\left\\{\\begin{array}{l l}{\\pi\\mathrm{d}\\mathrm{R}\\cdot\\log\\left(\\frac{\\pi\\mathrm{d}\\mathrm{R}}{\\pi\\mathrm{d}\\mathrm{R}+(1-\\pi)\\mathrm{d}\\mathrm{U}}\\right)+(1-\\pi)\\mathrm{d}\\mathrm{U}\\cdot\\log\\left(\\frac{(1-\\pi)\\mathrm{d}\\mathrm{U}}{\\pi\\mathrm{d}\\mathrm{R}+(1-\\pi)\\mathrm{d}\\mathrm{U}}\\right)}\\\\ {-\\pi\\mathrm{d}\\mathrm{G}\\cdot\\log\\left(\\frac{\\pi\\mathrm{d}\\mathrm{G}}{\\pi\\mathrm{d}\\mathrm{G}-(1-\\pi)\\mathrm{d}\\mathrm{U}}\\right)-(1-\\pi)\\mathrm{d}\\mathrm{U}\\cdot\\log\\left(\\frac{(1-\\pi)\\mathrm{d}\\mathrm{U}}{\\pi\\mathrm{d}\\mathrm{G}+(1-\\pi)\\mathrm{d}\\mathrm{U}}\\right)}\\end{array}\\right.}\\\\ {=}&{\\int_{\\mathcal{X}}\\left\\{\\begin{array}{l l}{\\pi\\mathrm{d}\\mathrm{R}\\cdot\\log\\left(\\frac{\\pi\\mathrm{d}\\mathrm{G}}{\\mathrm{d}\\mathrm{G}\\mathrm{d}(1-\\pi)\\mathrm{d}\\mathrm{U}}\\right)+\\pi\\mathrm{d}\\mathrm{G}\\cdot\\log\\left(\\frac{\\pi\\mathrm{d}\\mathrm{G}}{\\pi\\mathrm{d}\\mathrm{G}+(1-\\pi)\\mathrm{d}\\mathrm{U}}\\right)}\\\\ {-\\left(\\pi\\mathrm{d}\\mathrm{R}\\cdot\\log\\left(\\frac{\\pi\\mathrm{d}}{\\mathrm{d}\\mathrm{G}}\\right)\\right)\\cdot\\log\\left(\\pi\\mathrm{d}\\mathrm{R}+(1-\\pi)\\mathrm{d}\\mathrm{U}\\right)}\\end{array}\\right.}\\\\ {=}&{\\pi\\left\\{\\begin{array}{l l}{\\pi\\mathrm{d}\\mathrm{R}+(1-\\pi)\\mathrm{d}\\mathrm{U})\\cdot\\log\\left(\\pi\\mathrm{d}\\mathrm{R}+(1-\\pi)\\mathrm{d}\\mathrm{U}\\\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as claimed (end of the proof of Lemma D). ", "page_idx": 21}, {"type": "text", "text": "Because of the joint convexity of $K L$ divergence, we always have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{KL}\\left(\\pi\\mathrm{d}\\mathrm{R}+(1-\\pi)\\mathrm{d}\\mathrm{U}\\right\\|\\pi\\mathrm{d}\\mathrm{G}+(1-\\pi)\\mathrm{d}\\mathrm{U}\\right)}&{\\leqslant}&{\\pi\\mathrm{KL}\\left(\\mathrm{R}\\|\\mathrm{G}\\right)+(1-\\pi)\\mathrm{KL}\\left(\\mathrm{d}\\mathrm{U}\\|\\mathrm{d}\\mathrm{U}\\right)}\\\\ &{}&{=\\pi\\mathrm{KL}\\left(\\mathrm{R}\\|\\mathrm{G}\\right),\\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which yields (another proof) that our loss, D\u2113LOG $(\\mathrm{R},\\mathrm{G})$ , is lowerbounded by $\\boldsymbol{O}$ . In general, if we let $\\gamma\\,>\\,0$ such that KL $(\\pi\\mathrm{d}\\mathrm{R}+(1-\\pi)\\mathrm{d}\\mathrm{U}\\|\\pi\\mathrm{d}\\mathrm{G}+(1-\\pi)\\mathrm{d}\\mathrm{U})\\leqslant(1-\\nonumber$ \u03b3q \u00a8 \u03c0KL pR}Gq (The strict positivity of $\\gamma$ is also a weak assumption as long as R, G sufficiently differ from the uniform distribution), then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{D}_{\\ell^{\\mathrm{LOG}}}\\left(\\mathrm{R,G}\\right)}&{\\geqslant}&{\\gamma\\pi\\cdot\\mathrm{KL}\\left(\\mathrm{R}\\|\\mathrm{G}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "so any upperbound on $\\mathbb{D}_{\\ell^{\\mathrm{LOG}}}$ $(\\mathrm{R},\\mathrm{G})$ (such as obtained from Theorem 5.2) translates to an upperbound on the $K L$ divergence. Note that the condition for absolute continuity in Lemma $D$ is always met with the models we learn (both generative forests and ensembles of generative trees). ", "page_idx": 21}, {"type": "text", "text": "IV Simpler models: ensembles of generative trees ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Storing a GF requires keeping information about the empirical measure $\\mathrm{R}$ to compute the branching probability in Step 1 of STARUPDATE. This does not require to store the full training sample, but requires at least an index table recording the $\\{{\\mathrm{leaves}}\\}\\times\\{{\\mathrm{observations}}\\}$ association, for a storage cost in between $\\Omega(m)$ and $O(m T)$ where $m$ is the size of the training sample. There is a simple way to get rid of this constraint and approximate the GF by a set of generative trees (GTs) of [31]. ", "page_idx": 21}, {"type": "text", "text": "Models Simply put, a GT is a simplified equivalent representation of a generative forest with 1 tree only. In this case, the branching probabilities in Step 1 of STARUPDATE depend only on the tree\u2019s star node position. Thus, instead of recomputing them from scratch each time an observation needs to be generated, we compute them beforehand and use them to label the arcs in addition to the features\u2019 domains, as shown in Figure IV.1. This representation is equivalent to that of generative trees [31]. If we have several trees, we do this process independently for each tree and end up with an ensemble of generative trees (EOGT). The additional memory footprint for storing probabilities $\\begin{array}{r}{(O(\\sum_{i}|\\Lambda(\\Upsilon_{i})|))}\\end{array}$ is offset by the fact that we do not have anymore to store associations between leaves and \u0159observations for generative forests, for a potentially significant reduction is storing size. However, we cannot use anymore STARUPDATE as is for data generation since we cannot compute exactly the probabilities in Step 1. Two questions need to be addressed: is it possible to use an ensemble of generative trees to generate data with good approximation guarantees (and if so, how) and of course how do we train such models. ", "page_idx": 22}, {"type": "text", "text": "Data generation We propose a simple solution based on how well an EOGT can approximate a generative forest. It relies on a simple assumption about $\\mathrm{R}$ and $\\mathcal{X}$ . Taking as references the parameters in STARUPDATE, we now make two simplifications on notations, first replacing $\\Upsilon.\\nu_{\\mathrm{v}}^{\\star}$ by $\\nu_{\\mathrm{v}}^{\\star}$ (tree implicit), and then using notation $\\mathcal{C}_{\\mathtt{v}}\\doteq\\mathcal{X}_{\\nu_{\\mathtt{v}}^{\\star}}\\cap\\mathcal{C}$ for any ${\\tt v}\\in\\{{\\tt t},{\\tt f}\\}$ , the reference to the tree / star node being implicit. The assumption is as follows. ", "page_idx": 22}, {"type": "text", "text": "Assumption A. There exists $\\varkappa\\in(0,\\infty)$ such that at any Step $^{\\,l}$ of STARUPDATE, for any ${\\pmb v}\\in\\{\\,t,f\\}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{p_{\\mathrm{R}}\\left[\\mathfrak{C}_{v}|\\mathcal{X}_{\\nu_{v}^{\\star}}\\right]}{p_{\\mathrm{U}}\\left[\\mathfrak{C}_{v}|\\mathcal{X}_{\\nu_{v}^{\\star}}\\right]}\\quad\\in\\quad\\left[\\exp(-\\varkappa),\\exp(\\varkappa)\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "A simple condition to ensure the existence of $\\varkappa$ is a requirement weaker than (c) in Definition 5.1: at all Steps 1 of STARUPDATE, $p_{\\mathrm{R}}\\left[\\mathcal{C}_{\\sf t}|\\mathcal{C}\\right]\\in(0,1)$ , which also guarantees $p_{\\mathrm{R}}\\left[\\mathcal{C}_{\\mathrm{f}}\\left|\\mathcal{C}\\right]\\in\\left(0,1\\right)$ and thus postulates that the branching in Step 1 of STARUPDATE never reduces to one choice only. The next Lemma shows that it is indeed possible to combine the generation of an ensemble of generative trees with good approximation properties, and provides a simple algorithm to do so, which reduces to running STARUPDATE with a specific approximation to branching probabilities in Step 1. For any GF $\\mathrm{G}$ and ${\\mathcal{C}}\\in{\\mathcal{P}}(\\mathrm{G})$ , depthGpCq is the sum of depths of the leaves in each tree whose support intersection is $\\mathcal{C}$ . We need the definition of the expected depth of G. ", "page_idx": 22}, {"type": "text", "text": "Definition B. The expected depth of GF G $\\begin{array}{r}{i s\\,\\overline{{\\mathrm{depth}}}(\\mathrm{G})\\doteq\\sum_{\\mathcal{C}\\in\\mathcal{P}(\\mathrm{G})}p_{\\mathrm{G}}[\\mathcal{C}]\\cdot\\mathrm{depth}_{\\mathrm{G}}(\\mathcal{C}).}\\end{array}$ $\\overline{{\\mathrm{depth}}}(\\mathrm{G})$ represents the expected complexity to sample an observation (see also Lemma C). Lemma C. In Step 1 of STARUPDATE, suppose $p_{\\mathrm{R}}\\left[\\mathcal{C}_{t}\\cap\\mathcal{C}|\\mathcal{C}\\right]$ is replaced by ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{p}_{\\nu^{\\star}}\\doteq\\frac{p_{\\mathrm{U}}\\left[\\mathbb{C}_{t}|\\mathcal{X}_{\\nu_{t}^{\\star}}\\right]\\cdot p_{\\nu^{\\star}}}{p_{\\mathrm{U}}\\left[\\mathbb{C}_{t}|\\mathcal{X}_{\\nu_{t}^{\\star}}\\right]\\cdot p_{\\nu^{\\star}}+p_{\\mathrm{U}}\\left[\\mathbb{C}_{f}|\\mathcal{X}_{\\nu_{f}^{\\star}}\\right]\\cdot(1-p_{\\nu^{\\star}})},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with $p_{\\nu^{\\star}}\\doteq p_{\\mathrm{R}}\\left[\\mathcal{X}_{\\nu_{t}^{\\star}}|\\mathcal{X}_{\\nu^{\\star}}\\right]$ the probability labeling arc $(\\nu^{\\star},\\nu_{t})$ in its generative tree. Under Assumption $A$ , if we denote $\\mathrm{G}$ the initial GF and $\\hat{\\mathrm{G}}$ the EOGT.P using (40), the following bound holds on the KL divergence between G and $\\hat{\\mathrm{G}}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\mathrm{G}\\|\\hat{\\mathrm{G}})\\leqslant2\\varkappa\\cdot\\overrightarrow{\\mathrm{depth}}(\\mathrm{G}),\\quad w h e r e\\,\\overrightarrow{\\mathrm{depth}}(\\mathrm{G})\\;i s\\;g i\\nu e n\\;i n\\;D e f i n i t i o n\\;B.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Because ${\\mathcal{C}}\\subseteq{\\mathcal{X}}_{\\Upsilon,\\nu^{\\star}}$ at any call of STARUPDATE, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{p_{\\mathrm{R}}\\left[\\mathcal{X}_{\\Upsilon,\\nu_{\\mathrm{t}}^{*}}\\cap\\mathcal{C}|\\mathcal{C}\\right]}\\\\ {=}&{p_{\\mathrm{R}}\\left[\\mathcal{C}_{\\mathrm{t}}|\\mathcal{C}\\right]}\\\\ {=}&{\\frac{p_{\\mathrm{R}}\\left[\\mathcal{C}_{\\mathrm{t}}\\right]}{p_{\\mathrm{R}}\\left[\\mathcal{C}_{\\mathrm{t}}\\right]+p_{\\mathrm{R}}\\left[\\mathcal{C}_{\\mathrm{t}}\\right]}}\\\\ {\\leqslant}&{\\frac{\\exp\\left(\\varkappa\\right)p_{\\mathrm{U}}\\left[\\mathcal{C}_{\\mathrm{t}}\\right]\\mathcal{C}_{\\mathrm{t}}\\left[\\mathcal{C}_{\\mathrm{t}}\\right]\\mathcal{X}_{\\Upsilon,\\nu_{\\mathrm{t}}^{*}}\\right]p_{\\mathrm{R}}\\left[\\mathcal{X}_{\\Upsilon,\\nu_{\\mathrm{t}}^{*}}\\right]}{\\exp\\left(-\\varkappa\\right)p_{\\mathrm{U}}\\left[\\mathcal{C}_{\\mathrm{t}}|\\mathcal{X}_{\\Upsilon,\\nu_{\\mathrm{t}}^{*}}\\right]+\\exp\\left(-\\varkappa\\right)p_{\\mathrm{U}}\\left[\\mathcal{C}_{\\mathrm{t}}\\right]\\mathcal{X}_{\\Upsilon,\\nu_{\\mathrm{t}}^{*}}\\right]p_{\\mathrm{R}}\\left[\\mathcal{X}_{\\Upsilon,\\nu_{\\mathrm{t}}^{*}}\\right]}}\\\\ &{=\\exp(2\\varkappa)\\cdot\\frac{p_{\\mathrm{U}}\\left[\\mathcal{C}_{\\mathrm{t}}\\right]\\mathcal{X}_{\\Upsilon,\\nu_{\\mathrm{t}}^{*}}\\right]p_{\\mathrm{R}}\\left[\\mathcal{C}_{\\mathrm{t}}|\\mathcal{X}_{\\Upsilon,\\nu_{\\mathrm{t}}^{*}}\\right]p_{\\mathrm{R}}\\left[\\mathcal{X}_{\\Upsilon,\\nu_{\\mathrm{t}}^{*}}\\right]\\mathcal{X}_{\\Upsilon,\\nu_{\\mathrm{t}}^{*}}\\right]p_{\\mathrm{R}}\\left[\\mathcal{X}_{\\Upsilon,\\nu_{\\mathrm{t}}^{*}}\\right]\\mathcal{X}_{\\Upsilon,\\nu_{\\mathrm{t}}^{*}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "(43) is due to the fact that, since $\\begin{array}{r}{\\mathcal{\\textsf{C}}\\subseteq\\mathcal{X}_{\\Upsilon.\\nu^{\\star}},p_{\\mathrm{R}}\\left[\\mathcal{\\mathrm{C}}_{\\mathfrak{v}}\\right]=p_{\\mathrm{R}}\\left[\\mathcal{\\mathrm{C}}_{\\mathfrak{v}}\\cap\\mathcal{X}_{\\Upsilon.\\nu_{\\mathfrak{v}}^{\\star}}\\right]=p_{\\mathrm{R}}\\left[\\mathcal{\\mathrm{C}}_{\\mathfrak{v}}|\\mathcal{X}_{\\Upsilon.\\nu_{\\mathfrak{v}}^{\\star}}\\right]p_{\\mathrm{R}}\\left[\\mathcal{X}_{\\Upsilon.\\nu_{\\mathfrak{v}}^{\\star}}\\right]}\\end{array}$ , and then using (39). (44) is obtained by dividing num\u201cerator and d\u2030enomin\u201cator by $p_{\\mathrm{R}}\\left[\\mathcal{X}_{\\Upsilon.\\nu_{\\mathrm{t}}^{\\star}}\\right]+$ $p_{\\mathrm{R}}\\left[\\mathcal{X}_{\\Upsilon.\\nu_{\\mathrm{f}}^{\\star}}\\right]=p_{\\mathrm{R}}\\left[\\mathcal{X}_{\\Upsilon.\\nu^{\\star}}\\right]$ . ", "page_idx": 23}, {"type": "text", "text": "G and $\\hat{\\mathrm{G}}$ satisfy $\\mathcal{P}(\\mathrm{G})=\\mathcal{P}(\\hat{\\mathrm{G}})$ so ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathrm{KL}(\\mathrm{G}\\|\\hat{\\mathrm{G}})}&{=}&{\\displaystyle\\int\\mathrm{d}\\mathrm{G}\\log\\frac{\\mathrm{d}\\mathrm{G}}{\\mathrm{d}\\hat{\\mathrm{G}}}}\\\\ &{=}&{\\displaystyle\\sum_{\\mathrm{\\tiny{e}}\\in\\mathrm{\\mathcal{P}(G)}}p_{\\mathrm{G}}[\\mathrm{\\mathcal{C}}]\\log\\frac{p_{\\mathrm{G}}[\\mathrm{\\mathcal{C}}]}{p_{\\hat{\\mathrm{G}}}[\\mathrm{\\mathcal{C}}]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, $p_{\\mathrm{G}}[\\mathcal{C}]$ and $p_{\\hat{\\mathrm{G}}}[\\mathcal{C}]$ are just the product of the branching probabilities in any admissible sequence. If we use the same admissible sequence in both generators, we can write $\\begin{array}{r}{p_{\\mathrm{G}}[\\mathring{\\mathrm{e}}]=\\prod_{j=1}^{n(\\mathrm{e})}p_{j}}\\end{array}$ and $\\begin{array}{r}{p_{\\hat{\\mathrm{G}}}[\\mathcal{C}]\\,=\\,\\prod_{j=1}^{n(\\mathcal{C})}\\hat{p}_{j}}\\end{array}$ , and (44) directly yields $p_{j}\\,\\leqslant\\,\\exp(2\\varkappa)\\,\\cdot\\,\\hat{p}_{j}\\,,\\forall j\\,\\in\\,[n(\\mathcal{C})],\\,n(\\mathcal{C})$ being a shorthand for $\\mathrm{\\depth_{G}(\\mathcal{C})=\\mathrm{depth_{\\hat{G}}(\\mathcal{C})}}$ (see main file). So, for any ${\\mathcal{C}}\\in{\\mathcal{P}}(\\mathrm{G})$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{p_{\\mathrm{G}}[\\Theta]}{p_{\\hat{\\mathrm{G}}}[\\Theta]}\\;\\;\\leqslant\\;\\;\\exp(2\\varkappa\\cdot\\mathrm{depth}_{\\mathrm{G}}(\\mathcal{C})),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and finally, replacing back $n(\\mathcal{C})$ by notation depthpCq, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathrm{{KL}(G\\|\\hat{G})}}&{\\leqslant}&{2\\varkappa\\cdot\\sum_{\\mathrm{{e}\\in\\mathcal{P}(\\mathrm{G})}}p_{\\mathrm{{G}}}[\\mathfrak{C}]\\cdot\\mathrm{depth}(\\mathfrak{C})}\\\\ &&{=2\\varkappa\\cdot\\overline{{\\mathrm{depth}}}(\\mathrm{{G}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "as claimed. ", "page_idx": 23}, {"type": "text", "text": "Note the additional leverage for training and generation that stems from Assumption A, not just in terms of space: computing $p_{\\mathrm{U}}\\left[\\mathcal{C}_{\\mathrm{v}}|\\mathcal{X}_{\\nu_{\\mathrm{v}}^{\\star}}\\right]$ is $O(\\bar{d})$ and does not necessitate data so the computation of key conditional probabilities (4\u201c0) drop\u2030s from $\\Omega(m)$ to $O(d)$ for training and generation in an EOGT. Lemma C provides the change in STARUPDATE to generate data. ", "page_idx": 23}, {"type": "text", "text": "Training To train an EOGT, we cannot rely on the idea that we can just train a GF and then replace each of its trees by generative trees. To take a concrete example of how this can be a bad idea in the context of missing data imputation, we have observed empirically that a generative forest (GF) can have many trees whose node\u2019s observation variables are the same within the tree. Taken independently of the forest, such trees would only model marginals, but in a GF, sampling is dependent on the other trees and Lemma 4.4 guarantees the global accuracy of the forest. However, if we then replace the GF by an EOGT with the same trees and use them for missing data imputation, this results in imputation at the mode of the marginal(s) (exclusively), which is clearly suboptimal. To avoid this, we have to use a specific training for an EOGT, and to do this, it is enough to change a key part of training in splitPred, the computation of probabilities $p_{\\mathrm{{R}}}[.]$ used in (4). Suppose $\\varkappa$ very small in Assumption A. To evaluate a split at some leaf, we observe (suppose we have a single tree) ", "page_idx": 23}, {"type": "equation", "text": "$$\np_{\\mathrm{R}}[\\mathcal{X}_{\\lambda_{t}}]=p_{\\mathrm{R}}[\\mathcal{X}_{\\lambda}]\\cdot p_{\\mathrm{R}}[\\mathcal{X}_{\\lambda_{t}}|\\mathcal{X}_{\\lambda}]\\approx p_{\\mathrm{R}}[\\mathcal{X}_{\\lambda}]\\cdot p_{\\mathrm{U}}[\\mathcal{X}_{\\lambda_{t}}|\\mathcal{X}_{\\lambda}]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(and the same holds for $\\lambda_{\\mathrm{t}}$ ). Extending this to multiple trees and any $\\mathcal{C}\\in\\mathcal{P}(\\mathcal{T})$ , we get the computation of any $p_{\\mathrm{{R}}}[\\mathcal{C}_{\\mathrm{{f}}}]$ and $p_{\\mathrm{{R}}}[\\mathcal{C}_{\\mathbf{t}}]$ needed to measure the new (4) for a potential split. Crucially, it does not necessitate to split the empirical measure at $\\mathcal{C}$ but just relies on computing $\\bar{p}_{\\mathrm{U}}[\\mathcal{C}_{\\mathrm{v}}|\\mathcal{C}]$ , $\\mathtt{v}\\in\\{\\mathtt{f},\\mathtt{t}\\}$ : with the product measure and since we make axis-parallel splits, it can be done in $O(1)$ , thus substantially reducing training time. ", "page_idx": 23}, {"type": "text", "text": "More with ensembles of generative trees we can follow the exact same algorithmic steps as for generative trees to perform missing data imputation and density estimation, but use branching probabilities in the trees to decide branching, instead of relying on the empirical measure at tuples of nodes, which we do not have access to anymore. For this, we rely on the approximation (40) in Lemma C. A key difference with a GF is that no tuple can have zero density because branching probabilities are in $(0,1)$ in each generative tree. ", "page_idx": 23}, {"type": "table", "img_path": "cRlQHncjwT/tmp/c47290adbd2668fd3a413d05b9a5f6bcaaca3bc8aa13e855dd24e64bccc59899.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table A1: Public domains considered in our experiments ( ${\\boldsymbol{m}}=$ total number of examples, $d=\\mathrm{num}-$ ber of features), ordered in increasing $m\\times d$ . \"Cat.\" is a shorthand for categorical (nominal / ordinal / binary); \"Num.\" stands for numerical (integers $/$ reals). ${\\mathfrak{(}}^{*})=$ simulated, URL for OpenML: https://www.openml.org/search?type $=$ data&sort $=$ runs&id $\\mathtt{=}50\\$ 4&status $=$ active; SOP $=$ Stanford Open Policing project, https://openpolicing.stanford.edu/ (see text). \"Tag\" refers to tag names used in Tables A7 and A8. ", "page_idx": 24}, {"type": "text", "text": "V Appendix on experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "V.1 Domains ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "ringGauss is the seminal 2D ring Gaussians appearing in numerous GAN papers [46]; those are eight (8) spherical Gaussians with equal covariance, sampling size and centers located on regularly spaced (2-2 angular distance) and at equal distance from the origin. gridGauss was generated as a decently hard task from [11]: it consists of 25 2D mixture spherical Gaussians with equal variance and sampled sizes, put on a regular grid. circGauss is a Gaussian mode surrounded by a circle, from [46]. randGauss is a substantially harder version of ringGauss with 16 mixture components, in which covariance, sampling sizes and distances on sightlines from the origin are all random, which creates very substantial discrepancies between modes. ", "page_idx": 24}, {"type": "text", "text": "V.2 Algorithms configuration and choice of parameters ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "GF.BOOST We have implemented GF.BOOST in Java, following Algorithm 3\u2019s blueprint. Our implementation of tree and leaf in Steps 2.1, 2.2 is simple: we pick the heaviest leaf among all trees (with respect to R). The search for the best split is exhaustive unless the variable is categorical with more than a fixed number of distinct modalities (22 in our experiments), above that threshold, we pick the best split among a random subset. We follow [31]\u2019s experimental setting: in particular, the input of our algorithm to train a generator is a .csv flie containing the training data without any further information. Each feature\u2019s domain is learned from the training data only; while this could surely and trivially be replaced by a user-informed domain for improved results (e.g. indicating a proportion\u2019s domain as $[0\\bar{\\%},100\\%]$ , informing the complete list of socio-professional categories, etc.) \u2014 and is in fact standard in some ML packages like weka\u2019s ARFF flies, we did not pick this option to alleviate all side information available to the GT learner. Our software automatically recognizes three types of variables: nominal, integer and floating point represented. ", "page_idx": 24}, {"type": "text", "text": "Comments on implementation For the interested reader, we give here some specific implementation details regarding choices mades for two classical bottlenecks on tree-based methods (this also applies to the supervised case of learning decision trees): handling features with large number of modalities and handling continuous features. ", "page_idx": 25}, {"type": "text", "text": "We first comment the case where the number of modalities of a feature is large. The details can be found in the code in File Algorithm.java (class Algorithm), as follows: ", "page_idx": 25}, {"type": "text", "text": "\u2022 method public GenerativeModelBasedOnEnsembleOfTrees learn_geot() implements both GF.Boost (for generative forests) and its extension to training ensembles of generative trees (Appendix) as a single algorithm.   \n\u2022 method Node choose_leaf(String how_to_choose_leaf) is the method to choose a leaf (Step 2.2). Since it picks the heaviest leaf among all trees, it also implements Step 2.1 (we initialize all trees to their roots; having a tree $=$ root for generation does not affect generation).   \n\u2022 method public boolean split_generative_forest(Node leaf, HashSet <MeasuredSupportAtTupleOfNodes> tol) finds the split for a Generative Forest, given a node chosen for the split. Here is how it works: 1. It first computes the complete description of possible splits (not their quality yet), using method public static Vector<FeatureTest> ALL_FEATURE_TESTS(Feature f, Dataset ds) in Feature.java. The method is documented: for continuous features, the list of splits is just a list of evenly spaced splits. 2. then it calls public SplitDetails split_top_down_boosting_generative_forest_fast(Node leaf, boolean [] splittable_feature, FeatureTest [][] all_feature_tests, HashSet <MeasuredSupportAtTupleOfNodes> tol), which returns the best split as follows: (i) it shuffles the potential list of splits and then (ii) picks the best one in the sublist containing the first Algorithm.MAXIMAL_NUMBER_OF_SPLIT_TESTS_TRIES_PER_BOOSTING_ITERATION elements (if the list is smaller, the search for the best is exhaustive); this class variable is currently $=1000$ . ", "page_idx": 25}, {"type": "text", "text": "Last, we comment how we handle continuous variables: the boosting theory tells us that finding a moderately good split (condition (b) Definition 5.1, main file) is enough to get boosting-compliant convergence (Theorem 5.2, main file), so we have settled for a trivial and efficient mechanism: cut-points are evenly spaced and the number is fixed beforehand. See method public static Vector<FeatureTest> ALL_FEATURE_TESTS(Feature f, Dataset ds) in Feature.java for the details. It turns out that this works very well and shows our theory was indeed concretely used in the design/implementation of the training algorithm. ", "page_idx": 25}, {"type": "text", "text": "MICE We have used the R MICE package V 3.13.0 with two choices of methods for the round robin (column-wise) prediction of missing values: CART [1] and random forests (RF) [42]. In that last case, we have replaced the default number of trees (10) by a larger number (100) to get better results. We use the default number of round-robin iterations (5). We observed that random forests got the best results so, in order not to multiply the experiments reported and perhaps blur the comparisons with our method, we report only MICE\u2019s results for random forests. ", "page_idx": 25}, {"type": "text", "text": "TENSORFLOW To learn the additional Random Forests involved in experiments GEN-DISCRIM, we used Tensorflow Decision Forests library\u00a7. We use 300 trees with max depth 16. Attribute sampling: sqrt(number attributes) for classification problems, number attributes / 3 for regression problems (Breiman rule of thumb); the min #examples per leaf is 5. ", "page_idx": 25}, {"type": "text", "text": "CT-GAN We used the Python implementation\u00b6 with default values [47]. ", "page_idx": 25}, {"type": "image", "img_path": "cRlQHncjwT/tmp/f4a8ea2173de08946771a3892a4d6227dddc16a1f7bf0efd0e1ab3580e84306c.jpg", "img_caption": ["Table A2: SCRUTINIZE: Tree in an example of Generative Forest on sensitive domain compas, with 17 nodes and part of its graph sketched following Figure 2\u2019s convention, modeling a bias learned from data. Ensemble of such small trees can be very accurate: on missing data imputation, they compete with or beat MICE comparatively using 7 000 trees for imputation (see text for details). "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Adversarial Random Forests We used the R code of the generator FORGE made available from the paper [44]||, learning forests containing a variable number of trees in $\\{10,50,100,200\\}$ . We noted that the code does not run when the dataset has missing values and we also got an error when trying to run the code on kc1. ", "page_idx": 26}, {"type": "text", "text": "Vine Copulas AutoEncoders We used the Python code available from the paper $[41]^{**}$ , which processes only fully numerical datasets. We got a few errors when trying to run the code on compas. ", "page_idx": 26}, {"type": "text", "text": "Forest Flows We used the R code available from the paper $[17]^{\\dagger\\dagger}$ . Hyperparameters used were default parameters\u2021\u2021. ", "page_idx": 26}, {"type": "text", "text": "Kernel Density Estimation We used the R code available in the package npudens with default values following the approach of [24]. We tried several kernels but ended up with sticking to the default choices, that seemed to provide overall some of the best results. Compared to us with GF, KDE\u2019s code is extremely compute-intensive as on domains below tictactoe in Table A1: it took orders of magnitude more than ours to get the density values on all folds, so we did not run it on the biggest domains. Notice that in our case, computation time includes not just training the generative model, but also, at each applicable iteration $J$ in GF.BOOST, the computation of the same number of density values as for KDE. ", "page_idx": 26}, {"type": "text", "text": "Computers used We ran part of the experiments on a Mac Book Pro 16 Gb RAM w/ 2 GHz Quad-Core Intel Core i5 processor, and part on a desktop Intel(R) Xeon(R) 3.70GHz with 12 cores and 64 Gb RAM. CT-GANs and VCAEs were run on the desktop, the other algorithms on the laptop. ", "page_idx": 26}, {"type": "text", "text": "V.3 Supplementary results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Due to the sheer number of tables to follow, we shall group them according to topics ", "page_idx": 26}, {"type": "text", "text": "V.V.3.1 Interpreting our models: \u2019SCRUTINIZE\u2019 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Table A2 provides experimental results on sensitive real world domain compas. It shows that it is easy to flag potential bias / fairness issues in data by directly estimating conditional probabilities: considering the GF of this example and ignoring variable age for simplicity, conditionally to having 0 felony count and race being African-American, the probability of generating an observation with two_year_recid $=$ true is .562 (=1138/2024). ", "page_idx": 26}, {"type": "image", "img_path": "cRlQHncjwT/tmp/43599585f128ccb75453310d8dafbf2742d475cfc133a4fe0283af029e9fbcb9.jpg", "img_caption": ["Table A3: 2D density plots ( $x=$ Life Style Index and $\\overline{{y=\\mathbf{C}}}$ ustomer Rating) for data generated on sigma_cabs, from models learned with $5\\%$ missing features (MCAR), using using ensembles of generative trees (top) and generative forests (bottom), for varying total number of trees $T$ and total number of splits $J$ (columns). $\"*\"={\\mathrm{all}}$ trees are stumps. The rightmost column recalls the domain ground truth for comparison. Each generated dataset contains $m=2000$ observations. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "V.V.3.2 More examples of Table 1 (MF) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Tables A3 completes Tables 2 and A2 (main file), displaying that even a stochastic dependence can be accurately modeled. ", "page_idx": 27}, {"type": "text", "text": "We provide in Table A4 the density learned on all our four 2D (for easy plotting) simulated domains and not just circgauss as in Table 1 (MF). We see that the observations made in Table 1 (MF) can be generalized to all domains. Even when the results are less different between 50 stumps in a generative forest and 1 generative tree with 50 splits for ringgauss and randgauss, the difference on gridgauss is stark, 1 generative tree merging many of the Gaussians. ", "page_idx": 27}, {"type": "image", "img_path": "cRlQHncjwT/tmp/04e59d92ff3e2655ff6f99d5fee311ccb79f8f49f43aa7fa6ba9d7ad35730fe8.jpg", "img_caption": ["Table A4: Comparison of the density learned by a Generative Forest (GF) consisting of a single tree learned by GF.BOOST with $n$ total splits $(r i g h t)$ and a set of $n$ GF stumps (1 split per tree) learned by GF.BOOST (center). The left picture is the domain, represented using the same heatmap. On each domain, $5\\%$ of the data is missing $\\mathbf{(MCAR=}$ missing completely at random). "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "cRlQHncjwT/tmp/69577bd338ad0a88bc729cf93791e76a0e49cfb8e43be2729242d6e93041023c.jpg", "img_caption": ["Table A5: Densities learned on circgauss by GF.BOOST with generative forests consisting of stumps, for values of $T\\,=\\,J$ in $\\{1,2,...70\\}$ . The models quickly capture the ring and the middle dense Gaussian. The boxy-shape of the densities, due to the axis-parallel splits, get less noticeable as $T=J$ exceeds a few dozen. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "V.V.3.3 The generative forest of Table 1 (MF) developed further ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "One might wonder how a set of stumps gets to accurately fit the domain as the number of stumps increases. Table A5 provides an answer. From this experiment, we can see that 16 stumps are enough to get the center mode. The ring shape takes obviously more iterations to represent but still, is takes a mere few dozen stumps to clearly get an accurate shape, the last iterations just finessing the fitting. ", "page_idx": 29}, {"type": "table", "img_path": "cRlQHncjwT/tmp/3f652197b5168e85261c07b57039fbeeebb20998e9ff1233fb5dea11e4aa0732.jpg", "table_caption": ["V.V.3.4 Experiment LIFELIKE in extenso "], "table_footnote": ["Table A6: LIFELIKE: comparison of Generative Forests (us, GF), using smaller generative forest models ( $T=200$ trees, $J=\\!500$ total splits) to the same contenders as in Table 5 (main flie, we also add the option Center for VCAE; we did not push it in the main file for space constraints but its results are on par with the other VCAE results). Conventions are the same as in Table 5. See text for details. "], "page_idx": 30}, {"type": "text", "text": "The objective of the experiment is to evaluate whether a generative model is able to create \"realistic\" data. Tabular data is hard to evaluate visually, unlike $e,g$ . images or text, so we have considered a simple evaluation pipeline: we create for each domain a 5-fold stratified experiment. After a generative model has been trained, we generate the same number of observations as in the test fold and compute the optimal transport (OT) distance between the generated sample and the fold\u2019s test sample. To fasten the computation of OT costs, we use Sinkhorn\u2019s algorithm [8] with an $\\varepsilon$ -entropic regularizer, for some $\\varepsilon=0.5$ which we observed was the smallest in our experiments to run with all domains without leading to numerical instabilities. To balance the importance of categorical features (for which the cost is binary, depending on whether the guess is right or not) and numerical features, we normalize numerical features with the domain\u2019s mean and standard deviation prior to computing OT costs. We then compare our method to three possible contenders: Adversarial Random Forests (ARF) [44], CT-GANs [47], Forest Flows [17] and Vine copula autoencoders (VCAE) [41]. All these approaches rely on models that are very different from each other. Adversarial Random Forests (ARF) and Forest Flows (FFs) are tree-based generators. ARFs work as follows to generate one observation: one first sample uniformly at random a tree, then samples a leaf in the tree. The leaf is attached to a distribution which is then used to sample the observation. As we already pointed out in Section 2, there are two main differences with our models. From the standpoint of the model\u2019s distribution, if one takes the (non-empty) support from a tuple of leaves, its probability is obtained from a weighted average of the tree\u2019s distributions in [44] while it is obtained from a product of theirs in our case. Hence, at similar tree sizes, the modelling capability of the set of trees tips in our favor, but it is counterbalanced by the fact that their approach equip leaves with potentially complex distributions (e.g. truncated Gaussians) whereas we stick to uniform distributions at the leaves. A consequence of the modeling of ARFs is that each tree has to separately code for a good generator: hence, it has to be big enough or the distributions used at the leaves have to be complex enough. In our case, as we already pointed out, a small number of trees (sometimes even stumps) can be enough to get a fairly good generator, even on real-world domains. Forest Flows use tree-based models to estimate parameters of diffusion models. Their tree-based models are not necessarily big but they need a lot of them to carry out training and generation, which is a big difference with us. Our next contender, CT-GAN [47], fully relies on neural networks so it is de facto substantially different from ours. The last, VCAE [41] relies on a subtle combination of deep nets and graphical models learned in the latent space. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "Table 5 (main file) with smaller generative forests Table A6 provides the equivalent of Table 5 (with one additional contender, which did not fti in the space allotted for the main flie), but in which our generative forests are much smaller: $T=\\!200$ trees, $j=500$ total splits (so each tree is on average barely bigger than a decision stump). One can check that our models are still competitive with respect to all contenders. The best contender, Forest Flows, beat our models on density, but it must be remembered that Forest Flows uses many tree-based models to carry out both training and generation. ", "page_idx": 31}, {"type": "text", "text": "Focus on Sinkhorn for different parameterizations of contenders We now investigate how changing the parameterization of various contenders affect their performance against our approach. Here, ARFs are trained with a number of trees $T\\,\\in\\,\\{10,50,100,200\\}$ (ARFs include an algorithm to select the tree size so we do not have to select it). CT-GANs are trained with a number of epochs $E\\in\\{10,100,300,1000\\}$ . ", "page_idx": 31}, {"type": "text", "text": "We compare those models with generative forests with $T\\,=\\,500$ trees and trained for a total of $J=2000$ iterations, for all domains considered. Compared to the trees learned by ARFs, the total number of splits we use can still be small compared to theirs, in particular when they learn $T\\geqslant100$ trees. Each table against ARFs and CT-GANs has a different size and contains only a subset of the whole 21 domains: ARFs do not process domains with missing values and we also got issues running contenders on several domains: those are discussed in Appendix, Section V.2. ", "page_idx": 31}, {"type": "table", "img_path": "cRlQHncjwT/tmp/5dbaeec11cb0a1e41778821738838649b5b3065cf6f9c0be3024cdce28c0cc73.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "Table A7: LIFELIKE: comparison of ARFs [44] with different number $T$ of trees, and meduim-sized Generative Forests (us, GF) where the number of trees and number of iterations are fixed $T=500$ trees, total of $J=2000$ splits in trees). Values shown $=$ average over the 5-folds $\\pm$ std dev. . The $p$ -values for the comparison of our results (\"us\") and ARFs are shown. Values appearing in green in $p$ -vals mean (i) the average regularized OT cost for us (GF) is smaller than that of ARFs and (ii) $p<0.1$ , i.e. our method outperforms ARFs (there is no instance of us being statistically significantly beaten by ARFs). $\\varepsilon$ means $p$ -val $<10^{-4}$ . Domain details in Table A1. ", "page_idx": 31}, {"type": "text", "text": "Results vs Adversarial Random Forests Table A7 digs into results obtained against adversarial random forests on the Sinkhorn metric. A first observation, not visible in the table, is that ARFs indeed tend to learn big models, typically with dozens of nodes in each tree. For the largest ARFs with 100 or 200 trees, this means in general a total of thousands of nodes in models. A consequence, also observed experimentally, is that there is sometimes little difference in performance in general between models with a different number of trees in ARFs as each tree is in fact an already good generator. In our case, this is obviously not the case. Noticeably, the performance of ARFs is not monotonic in the number of trees, so there could be a way to find the appropriate number of trees in ARFs to buy the slight (but sometimes significant) increase in performance in a domain-dependent way. In our case, there is obviously a dependency in the number of trees chosen as well. From the standpoint of the optimal transport metric, even when ARFs make use of distributions at their tree leaves that are much more powerful than ours and in fact fit to some of our simulated domains (Gaussians used in circgauss, randgauss and gridgauss), we still manage to compete or beat ARFs on those simulated domains. ", "page_idx": 32}, {"type": "text", "text": "Globally, we manage to beat ARFs on almost all runs, and very significantly on many of them. Since training generative forests does not include a mechanism to select the size of models, we have completed this experiment by another one on which we learn much smaller generative forests. The corresponding Table is in Appendix, Section V.V.3.4. Because we clearly beat ARFs on student_performance_mat and student_performance_por in Table A7 but are beaten by ARFs for much smaller generative forests, we conclude that there could exist mechanisms to compute the \"right size\" of our models, or to prune big models to get models with the right size. Globally however, even with such smaller models, we still manage to beat ARFs on a large majority of cases, which is a good figure given the difference in model sizes. The ratio \"results quality over model size\" tips in our favor and demonstrates the potential in using all trees in a forest to generate an observation (us) vs using a single of its trees (ARFs), see Figure 1. ", "page_idx": 32}, {"type": "text", "text": "Results vs CT-GANs Table A8 summarizes our results against CT-GAN using various numbers of training epochs $(E)$ . In our case, our setting is the same as versus ARFs: we stick to $T=500$ trees trained for a total number of $J=2000$ iterations in the generative forest, for all domains. We see that generative forests consistently and significantly outperform CT-GAN on nearly all cases. Furthermore, while CT-GANs performances tend to improve with the number of epochs, we observe that on a majority of datasets, performances at 1 000 epochs are still far from those of generative forests and already induce training times that are far bigger than for generative forests: for example, more than 6 hours per fold for stm while it takes just a few seconds to train a generative forest. Just like we did for ARFs, we repeated the experiment vs CT-GAN using much smaller generative forests $T=200$ , $J=500$ ). The results are presented in Appendix, Section V.V.3.4. There is no change in the conclusion: even with such small models, we still beat CT-GANs, regardless of the number of training epochs, on all cases (and very significantly on almost all of them). ", "page_idx": 32}, {"type": "text", "text": "More results with small generative forests Tables A9 and A10 present the results of generative forests vs adversarial random forests and CT-GANs, when the size of our models is substantially smaller than in the main file $(T=200,J=500)$ . We still manage to compete or beat ARFs on simulated domains for which modelling the leaf distributions in ARFs should represent an advantage (Gaussians used in circgauss, randgauss and gridgauss), even more using comparatively very small models compared to ARFs. We believe this could be due to two factors: (i) the fact that in our models all trees are used to generate each observation (which surely facilitates learning small and accurate models) and (ii) the fact that we do not generate data during training but base our splitting criterion on an exact computation of the reduction of Bayes risk in growing trees. If we compute aggregated statistics comparing, for each number of trees in ARFs, the number of times we win / lose versus ARFs, either considering only significant $p$ -values or disregarding them, our generative forests always beat ARFs on a majority of domains. Globally, this means we win on 34 out of 52 cases. The results vs CT-GANs are of the same look and feel as in the main file with larger generative forests: generative forests beat them in all cases, and very significantly in most of them. ", "page_idx": 32}, {"type": "text", "text": "V.V.3.5 Comparison with \"the optimal generator\": GEN-DISCRIM ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Rather than compare our method with another one, the question we ask here is \"can we generate data that looks like domain\u2019s data\" ? We replicate the experimental pipeline of [31]. In short, we shuffle a 3-partition (say $\\mathrm{R_{1}},\\mathrm{R_{2}},\\mathrm{R_{3}})$ of the training data in a $3!=6$ -fold CV, then train a generator $\\mathrm{G_{1}}$ from $\\mathrm{R_{1}}$ (we use generative forests with GF.BOOST), then train a random forest (RF) to distinguish between $\\mathrm{G_{1}}$ and $\\mathrm{R_{2}}$ , and finally estimate its test accuracy on $\\mathrm{G_{1}}$ vs $\\mathrm{R_{3}}$ . The lower this accuracy, the less distinguishable are fakes from real and thus the better the generator. We consider 3 competing baselines to our models: (i) the \"optimal\" one, COPY, which consists in replacing $\\mathrm{G_{1}}$ by a set of real data, (ii) the \"worst\" one, UNIF(orm), which uniformly samples data, and (iii) GF.BOOST for $T=1$ , which learns a generative tree (GT). We note that this pipeline is radically different from the one used in [44]. In this pipeline, domains used are supervised (the data includes a variable to predict). A generator is trained from some training data, then generates an amount of data equal to the size of the training data. Then, two classifiers are trained, one from the original training data, one from the generated data, to predict for the variable to predict and their performances are compared on this basis (the higher the accuracy, the better). There is a risk in this pipeline that we have tried to mitigate with our sophisticated pipeline: if the generator consists just in copying its training data, it is guaranteed to perform well according to [44]\u2019s metric. Obviously, this would not be a good generator however. Our pipeline would typically prevent this, $\\mathrm{R_{2}}$ being different from $\\mathrm{R_{3}}$ . ", "page_idx": 32}, {"type": "table", "img_path": "cRlQHncjwT/tmp/0f42be466bf4a5831fbad7c242f8e6e93565082afe58aa3544c72ba82b4f1813.jpg", "table_caption": [], "table_footnote": ["Table A8: Comparison of results with respect to CT-GANs [47] trained with different number of epochs $E$ . On some domains, indicated with a \"\u02da\", CT-GANs crashed on some folds. For such domains, we indicate in index to CT-GANs results the number of folds (out of 5) for which this did not happen. In such cases, we restricted the statistical comparisons with us to the folds for which CT-GANs did not crash: the statistical tests take this into account; instead of providing all corresponding average performances for us, we keep giving the average performance for us on all five folds (leftmost column), which is thus indicative. Other conventions follow Table A7. "], "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "Results Tables A11 and A12 provide the results we got, including statistical $p$ -values for the test of comparing our method to COPY. They display that it is possible to get GFs generating realistically looking data: for iris, the Student $t$ -tests show that we can keep $H_{0}={}^{\\prime\\prime}\\mathrm{GFs}$ perform identically to COPY\" for $J\\,=\\,T\\,=\\,40\\;(p\\,>\\,.2)$ . For mat, the result is quite remarkable not because of the highest $p$ -val, which is not negligible $(>0.001)$ , but because to get it, it suffices to build a GF in which the total number of feature occurrences $J=80$ ) is barely three times as big as the domain\u2019s dimension $\\mathit{\\Delta}d=33$ ). The full tables display a clear incentive to grow further the GFs as domains increase in size. However, our results show, pretty much like the experiment against Adversarial Random Forests, that there could be substantial value of learning the right model size: iris and student_performance_por clearly show nontrivial peaks of $p$ -values, for which we would get the best models compared to COPY. ", "page_idx": 33}, {"type": "table", "img_path": "cRlQHncjwT/tmp/0b3585688e7f8cabcdb90b4207f74cd03e736aaa0dd8ade82afc78839457899a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "V.V.3.6 Full comparisons with MICE on missing data imputation ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "The main file provides just two examples of domains for the comparison, abalone and analcatdata_supreme. We provide here more complete results on the domain used in the main file and results on more domains in the form of one table for each additional domain: ", "page_idx": 34}, {"type": "text", "text": "\u2022 Table A13: experiments on analcatdata_supreme completing the results shown in Table 4 (MF);   \n\u2022 Table A14: experiments on abalone completing the results shown in Table 4 (MF);   \n(tables below are ordered in increasing domain size, see Table A1) \u2022 Table A15: experiments on iris;   \n\u2022 Table A16: experiments on ringgauss;   \n\u2022 Table A17: experiments on circgauss;   \n\u2022 Table A18: experiments on gridgauss;   \n\u2022 Table A19: experiments on randgauss;   \n\u2022 Table A20: experiments on student_performance_mat;   \n\u2022 Table A21: experiments on student_performance_por;   \n\u2022 Table A22: experiments on kc1;   \n\u2022 Table A23: experiments on sigma_cabs;   \n\u2022 Table A24: experiments on compas;   \n\u2022 Table A25: experiments on open_policing_hartford; ", "page_idx": 34}, {"type": "text", "text": "The large amount of pictures to process may make it difficult to understand at first glance how our approaches behave against MICE, so we summarize here the key points: ", "page_idx": 34}, {"type": "table", "img_path": "cRlQHncjwT/tmp/61e34d63131e55a9cf1d8e14bb39d4bb9dd397a1284ddae4b94c87facc5e0b5c.jpg", "table_caption": [], "table_footnote": ["Table A10: Comparison of results with respect to CT-GANs [47] trained with different number of epochs $E$ . $\\varepsilon$ means $p$ -val $<10^{-4}$ . As already mentioned in Table A8, in some domains, indicated with a \"\u02da\", CT-GANs crashed on some folds. For such domains, we indicate in index to CT-GANs results the number of folds (out of 5) for which this did not happen. In such cases, we restricted the statistical comparisons with us to the folds for which CT-GANs did not crash: the statistical tests take this into account; instead of providing all corresponding average performances for us, we keep giving the average performance for us on all five folds (leftmost column), which is thus indicative. Other conventions follow Table A9. "], "page_idx": 35}, {"type": "text", "text": "\u2022 first and most importantly, there is not one single type of our models that perform better than the other. Both Generative Forests and Ensembles of Generative Trees can be useful for the task of missing data imputation. For example, analcatdata_supreme gives a clear advantage to Generative Forests: they even beat MICE with random forests having 4 000 trees (that is hundreds of times more than our models) on both categorical variables (perr) and numerical variables (rmse). However, on circgauss, student_performance_por and sigma_cabs, Ensembles of Generative Trees obtain the best results. On sigma_cabs, they can perform on par with MICE if the number of tree is limited (which represents $^{100+}$ times less trees than MICE\u2019s models); ", "page_idx": 35}, {"type": "text", "text": "\u2022 as is already noted in the main flie (MF) and visible on several plots (see e,g, sigma_cabs), overfitting can happen with our models (both Generative Forests and Ensembles of Generative Trees), which supports the idea that a pruning mechanism or a mechanism to stop training would be a strong plus to training such models. Interestingly, in several cases where overftiting seems to happen, increasing the number of splits can reduce the phenomenon, at fixed number of trees: see for example iris $50\\to100$ iterations for EOGTs), gridgauss, randgauss and kc1 (GFs); ", "page_idx": 35}, {"type": "text", "text": "\u2022 some domains show that our models seem to be better at estimating continuous (regression) rather than categorical (prediction) variables: see for example abalone, student_performance_mat, student_performance_por and our biggest domain, open_policing_hartford. Note from that last domain that a larger number of iterations or models with a larger number of trees may be required for the best results, in particular for Ensembles of Generative Trees; ", "page_idx": 35}, {"type": "text", "text": "\u2022 small models may be enough to get excellent results on real world domains whose size would, at first glance, seem to require much bigger ones: on compas, we can compete (rmse) or beat (perr) MICE whose models contain 7 000 trees with just 20 stumps; ", "page_idx": 35}, {"type": "text", "text": "\u2022 it is important to remember that our technique does not just offer the capability to do missing data imputation: our models can also generate data and compute the full density conditional to observed values, which is not the case of MICE\u2019s models, crafted with the sole purpose of solving the task of missing data imputation. Our objective was not to beat MICE, but rather show that Generative Forests and Ensembles of Generative Trees can also be useful for this task. ", "page_idx": 35}, {"type": "image", "img_path": "cRlQHncjwT/tmp/a47f510db047a76290d81a7df0ed8d0f6070b3ad4330882244d3e3849aee046f.jpg", "img_caption": ["Table A11: Results of generative forests on GEN-DISCRIM for four UCI domains, ordered, from top to bottom, in increasing domain size. In each row, the accuracy of distinguishing fakes from real is plotted (the lower, the better the technique) for four contenders: UNIFORM (generates observation uniformly at random), COPY (\"optimal\" contender using real data as generated), GF.BOOST inducing generative forests and generative trees (indicated for $T=1$ ). A clear pattern emerges, that as the domain size grows, increasing $J$ buys improvements and, if $J$ is large enough, increasing $T$ also improves results. See Table A12 for comparisons (us vs COPY) in terms of $p_{\\|}$ -values. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "cRlQHncjwT/tmp/235f9a0fc6197672ac43747037e370984fcd5175ac2cef990e9805e994d193b7.jpg", "img_caption": ["Table A13: Missing data imputation: results on analcatdata_supreme with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests) with 100 "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "cRlQHncjwT/tmp/6207d9cac79115381178d57621a9ae06eaaf37cf2d5d662fd88d700ce1ed7ca9.jpg", "img_caption": ["Table A12: $p$ -values for Student tests with $H_{0}$ being \"GF and COPY perform identically on GENDISCRIM\u201d (for the domains in Table A11). Large values indicate we can keep $H_{0}$ and thus consider that GF is successful at generating \"realistic\" data. Clearly, GF are successful for iris $\\mathit{\\Delta}_{p}$ increases with $T$ up to $p\\sim0.3$ for $J\\,=\\,40,$ ). For the other domains, Table A11 suggests increasing $J$ or $T$ . The curves for abalone reveal a dramatic relative increase of $p$ as $(J,T)$ increases, starting from $p\\sim0,\\forall T$ for $J=20$ . "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "cRlQHncjwT/tmp/44f8335fc16732d9ee748b4fc64d74446a96f254124eff15a1995b6c92f94c25.jpg", "img_caption": ["Table A14: Missing data imputation: results on abalone with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests). Conventions follow Table A13. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "cRlQHncjwT/tmp/1dfe796f3a574fc5ed7a3db5108c7099b51027cd3d40860c1d1dc32dbf93be9c.jpg", "img_caption": ["Table A15: Missing data imputation: results on iris with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests). Conventions follow Table A13. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "cRlQHncjwT/tmp/f06580b7462c4f58d9876602e9c5e9e465c670b9acc0ae3ced3f1cb3233f187d.jpg", "img_caption": ["Table A16: Missing data imputation: results on ringgauss with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests). Since there are no nominal attributes in the domain, we have removed column perr. Conventions otherwise follow Table A13. "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "cRlQHncjwT/tmp/5491da1ed401b754b513998c53e0b3d0dc2e3bd894ab13f6ccd3273c82e4f052.jpg", "img_caption": ["Table A17: Missing data imputation: results on circgauss with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests). Since there are no nominal attributes in the domain, we have removed column perr. Conventions otherwise follow Table A13. "], "img_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "cRlQHncjwT/tmp/2303e41ed3a2aa40e18dd9468cdd844c57a38f778578cf4e162d9f8efee943ed.jpg", "img_caption": ["Table A18: Missing data imputation: results on gridgauss with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests). Since there are no nominal attributes in the domain, we have removed column perr. Conventions otherwise follow Table A13. "], "img_footnote": [], "page_idx": 41}, {"type": "image", "img_path": "cRlQHncjwT/tmp/a814afddc0e87351c59167ed2589210a303c6fd26c0e85a9555fc84f4b4278f4.jpg", "img_caption": ["Table A19: Missing data imputation: results on randgauss with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests). Since there are no nominal attributes in the domain, we have removed column perr. Conventions otherwise follow Table A13. "], "img_footnote": [], "page_idx": 42}, {"type": "image", "img_path": "cRlQHncjwT/tmp/c03d1161e5d70de69fe4672090d9be0503b418625acdad0ee42d8beb987c2b48.jpg", "img_caption": ["Table A20: Missing data imputation: results on student_performance_mat with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests). Conventions follow Table A13. "], "img_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "cRlQHncjwT/tmp/7bb44260126333630147db6bcaa7243c41530ac32c0a4b7198f86daa5c537366.jpg", "img_caption": ["Table A21: Missing data imputation: results on student_performance_por with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests). Conventions follow Table A13. "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "cRlQHncjwT/tmp/901c9e87897ce3514596faf7af790869165685f33f5cc7dfd8e61790edd71904.jpg", "img_caption": ["Table A22: Missing data imputation: results on kc1 with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests). Conventions follow Table A13. "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "cRlQHncjwT/tmp/f5e14b4b1c4426c28da978b63750711686919313dff4078e3a0f06de18b7c2f8.jpg", "img_caption": ["Table A23: Missing data imputation: results on sigma_cabs with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests). Conventions follow Table A13. "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "cRlQHncjwT/tmp/ba1495eafc65c17f9aa4f0bf72c97a40c7836d784f0d611bc8eacaa2fc919a97.jpg", "img_caption": ["Table A24: Missing data imputation: results on compas with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests). Conventions follow Table A13. "], "img_footnote": [], "page_idx": 47}, {"type": "image", "img_path": "cRlQHncjwT/tmp/9f145b7e2964fec46c612eee28b7eb201a6604d0aa06350ae7f367e520fc07fe.jpg", "img_caption": ["Table A25: Missing data imputation: results on open_policing_hartford with $5\\%$ missing features (MCAR), for GF.BOOST learning GFs and EOGTs, vs mice using RF (random forests). Conventions follow Table A13. "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "cRlQHncjwT/tmp/b1f6407cdfe9e062b109bd72838c3a2d212e904b5507bb87bd8ec33f5ea1a253.jpg", "img_caption": ["Table A26: DENSITY: zoom on the gridGauss domain: the GF is grown by GF.BOOST in a way that first learns a complete set of stumps (there are a total of $T\\,=\\,500$ trees in the final model) and then grows some of the trees. The plateau for $j\\leqslant500$ leaves displays that we quickly reach a limit in terms of maximal performances of stumps, this being probably dues to the fact that the dimension of the domain is small (2). We can however remark that with just stumps, we still attain close performances to KDE (note that we plot the expected log-densities; see text for details). "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "cRlQHncjwT/tmp/d02e0546be4693a7cf658705567af4dc941a3e318beda578ff8d5d13817cd90e.jpg", "img_caption": ["Table A27: DENSITY: zoom on the abalone domain: with $T\\,=\\,500$ trees, total of $J\\,=\\,2000$ as in Tables A28 and A29, we are clearly beaten by KDE (left). A much smaller of bigger trees $(T\\ =\\ 15,J\\ =\\ 10000)$ is enough to become competitive and beat KDE, albeit not statistically significantly (right). The right picture also displays the interest into getting pruning or early stopping algorithms to prevent eventual overfitting. "], "img_footnote": [], "page_idx": 49}, {"type": "text", "text": "V.V.3.7 Full comparisons with KDE on density estimation ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "For each domain we either use the expected density, or the expected negative log-density, as the metric to be maximized. We consider the expected density to cope with the eventuality of zero (pointwise) density predictions. Tables A28 and A29 present the results against KDE (details in Table A28). Table A26 singles out a result on the domain gridGauss, which displays an interesting pattern. The leaf chosen for a split in GF.BOOST is always the heaviest leaf; hence, as long as the iteration number $j\\leqslant T$ (the maximum number of trees, which is 500 in this experiment), GF.BOOST grows stumps. In the case of gridGauss, like in a few other domains, there is a clear plateau in performances for $j\\leqslant500$ , which does not hold anymore as $j>500$ . This, we believe, characterizes the fact that on these domains, which all share the property that their dimension is small, training stumps attains a limit in terms of performances. Table A27 shows that by changing basic parameters \u2013 here, decreasing the number of trees, increasing the total number of iterations \u2013, one can obtain substantially better results. ", "page_idx": 49}, {"type": "image", "img_path": "cRlQHncjwT/tmp/e3c8cccfaf686967a45e0522a6be1e37f77e66f0c9f67eca1b359a884b43970e.jpg", "img_caption": ["Table A28: Comparison of results for Kernel Density Estimation (KDE) and Generative Forests (us, GF) with parameters $T=500$ trees, total of $J=2000$ splits in trees, on a first batch of domains, put in increasing size according to Table A1. For each domain, we compute for GF at regular intervals the average $\\pm$ standard deviation of either (i) density values or (ii) negative log density values for the test fold (sometimes, KDE gives 0 estimated density which prevents the computation of (ii)). Warning: some $y$ scales are logscale. We then compute the baseline of KDE and display its average $\\pm$ standard deviation in blue on each picture. We perform, at each applicable iteration of GF.BOOST (i.e. each iteration for many domains, which can be seen from the plots), a Student paired $t$ -test over the five folds to compare the results with KDE. If we are statistically better $(p=0.1)$ ), our plot is displayed in green; if KDE is better, our plot is displayed in red; otherwise it is displayed in gray. When standard deviations are not displayed, it means average $\\pm$ standard deviation exceeds the plot\u2019s $y$ scale. "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "cRlQHncjwT/tmp/87de5004771bc7b8d12884fe8fdc9165b5e557b81e226d283eecfdef560de64f.jpg", "img_caption": ["Table A29: Comparison of results for Kernel Density Estimation and Generative Forests (us, GF) where the number of trees and number of iterations are $T=500$ trees, total of $J=2000$ splits in trees), on a second batch of domains. Conventions follow Table A28. "], "img_footnote": [], "page_idx": 51}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: Claims are supported by both the theoretical results presented in the paper and extensive experimental results, summarized in the main paper but otherwise presented in extenso in the Supplement. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 52}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: Limitations are discussed, in particular in the experimental section vs the SOTA (see e.g. experiment Lifelike). ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 52}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: Assumptions are detailed, formal results given and full proofs are provided in the Appendix. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 53}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: (i) full code provided, (ii) extensive README.txt, (iii) coding choices discussed in Appendix, (iv) all scripts provided for easy running of each experiments, (v) special visualization routines (e.g. heatmaps) also included in the code, (vi) example domains (public and simulated) provided for quick assessment. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 53}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: (i) full code provided, (ii) extensive README.txt, (iii) coding choices discussed in Appendix, (iv) all scripts provided for easy running of each experiments, (v) special visualization routines (e.g. heatmaps) also included in the code, (vi) example domains (public and simulated) provided for quick assessment. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 54}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Justification: Details are provided in main text and Appendix. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 54}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Justification: standard devs and inferential statistics test done (details in main file $^+$ appendix). ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 54}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 55}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 55}, {"type": "text", "text": "Justification: Our code runs on any standard computer. Details of codes, computer and SOTA used (version, etc.) in appendix. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 55}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: The research of the paper follows the code of ethics. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 55}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: See conclusion. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 55}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 56}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: No release of data or models. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 56}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: Appropriate credit / referencesse are provided. Licenses listed in Appendix. Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 56}, {"type": "text", "text": "", "page_idx": 57}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: No new assets provided. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 57}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 57}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: No research with human subjects. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 58}]